version 3: an introduction to

data science

jeffrey stanton, syracuse university

(with a contribution by robert w. de graaf)

introduction to data science

   2012, 2013 by jeffrey stanton,    
portions    2013, by robert de graaf

this book is distributed under the creative commons attribution-
noncommercial-sharealike 3.0 license. you are free to copy, dis-
tribute, and transmit this work. you are free to add or adapt the 
work. you must attribute the work to the author(s) listed above. 
you may not use this work or derivative works for commercial pur-
poses. if you alter, transform, or build upon this work you may dis-
tribute the resulting work only under the same or similar license. 

for additional details, please see:
http://creativecommons.org/licenses/by-nc-sa/3.0/ 

this book was developed for the certi   cate of data science pro-
gram at syracuse university   s school of information studies. if 
you    nd errors or omissions, please contact the author, jeffrey stan-
ton, at jmstanto@syr.edu. a pdf version of this book and code ex-
amples used in the book are available at:

http://jsresearch.net/wiki/projects/teachdatascience 

the material provided in this book is provided "as is" with no war-
ranty or guarantees with respect to its accuracy or suitability for 
any purpose.

thanks to ashish verma for help with revisions to chapter 10!

i

data science: many skills

data science refers to an emerging area of work concerned with the collection, preparation, analysis, 
visualization, management, and preservation of large collections of information. although the name 
data science seems to connect most strongly with areas such as databases and computer science, 
many different kinds of skills - including non-mathematical skills - are needed.  

ii

section 1

data science: many skills

overview
1. data science includes data 
analysis as an important 
component of the skill set 
required for many jobs in 
this area, but is not the only 
necessary skill.

2. a brief case study of a 

supermarket point of sale 
system illustrates the many 
challenges involved in data 
science work.

3. data scientists play active 
roles in the design and 
implementation work of 
four related areas: data 
architecture, data 
acquisition, data analysis, 
and data archiving.

4. key skills highlighted by the 
brief case study include 
communication skills, data 
analysis skills, and ethical 
reasoning skills.

word frequencies from the de   nitions in a shakespeare glossary. while professional data scientists do need 
skills with mathematics and statistics, much of the data in the world is unstructured and non-numeric. 
for some, the term "data science" evokes images 
of statisticians in white lab coats staring    xedly 
at blinking computer screens    lled with scrolling 
numbers. nothing could be further from the 
truth. first of all, statisticians do not wear lab 
coats: this fashion statement is reserved for biolo-
gists, doctors, and others who have to keep their 

clothes clean in environments    lled with unusual 
   uids. second, much of the data in the world is 
non-numeric and unstructured. in this context, 
unstructured means that the data are not ar-
ranged in neat rows and columns. think of a 
web page full of photographs and short mes-
sages among friends: very few numbers to work 

3

with there. while it is certainly true that companies, schools, and 
governments use plenty of numeric information - sales of prod-
ucts, grade point averages, and tax assessments are a few examples 
- there is lots of other information in the world that mathemati-
cians and statisticians look at and cringe. so, while it is always use-
ful to have great math skills, there is much to be accomplished in 
the world of data science for those of us who are presently more 
comfortable working with words, lists, photographs, sounds, and 
other kinds of information.
in addition, data science is much more than simply analyzing data. 
there are many people who enjoy analyzing data and who could 
happily spend all day looking at histograms and averages, but for 
those who prefer other activities, data science offers a range of 
roles and requires a range of skills. let   s consider this idea by think-
ing about some of the data involved in buying a box of cereal. 
whatever your cereal preferences - fruity, chocolaty,    brous, or 
nutty - you prepare for the purchase by writing "cereal" on your 
grocery list. already your planned purchase is a piece of data, al-
beit a pencil scribble on the back on an envelope that only you can 
read. when you get to the grocery store, you use your data as a re-
minder to grab that jumbo box of fruitychocoboms off the shelf 
and put it in your cart. at the checkout line the cashier scans the 
barcode on your box and the cash register logs the price. back in 
the warehouse, a computer tells the stock manager that it is time to 
request another order from the distributor, as your purchase was 
one of the last boxes in the store. you also have a coupon for your 
big box and the cashier scans that, giving you a predetermined dis-
count. at the end of the week, a report of all the scanned manufac-
turer coupons gets uploaded to the cereal company so that they 

can issue a reimbursement to the grocery store for all of the coupon 
discounts they have handed out to customers. finally, at the end of 
the month, a store manager looks at a colorful collection of pie 
charts showing all of the different kinds of cereal that were sold, 
and on the basis of strong sales of fruity cereals, decides to offer 
more varieties of these on the store   s limited shelf space next 
month.
so the small piece of information that began as a scribble on your 
grocery list ended up in many different places, but most notably on 
the desk of a manager as an aid to decision making. on the trip 
from your pencil to manager   s desk, the data went through many 
transformations. in addition to the computers where the data may 
have stopped by or stayed on for the long term, lots of other pieces 
of hardware - such as the barcode scanner - were involved in col-
lecting, manipulating, transmitting, and storing the data. in addi-
tion, many different pieces of software were used to organize, ag-
gregate, visualize, and present the data. finally, many different "hu-
man systems" were involved in working with the data. people de-
cided which systems to buy and install, who should get access to 
what kinds of data, and what would happen to the data after its im-
mediate purpose was ful   lled. the personnel of the grocery chain 
and its partners made a thousand other detailed decisions and ne-
gotiations before the scenario described above could become real-
ity.
obviously data scientists are not involved in all of these steps. 
data scientists don   t design and build computers or barcode read-
ers, for instance. so where would the data scientists play the most 
valuable role? generally speaking, data scientists play the most ac-
tive roles in the four a   s of data: data architecture, data acquisition, 

4

data analysis, and data archiving. using our cereal example, let   s 
look at them one by one. first, with respect to architecture, it was 
important in the design of the "point of sale" system (what retailers 
call their cash registers and related gear) to think through in ad-
vance how different people would make use of the data coming 
through the system. the system architect, for example, had a keen 
appreciation that both the stock manager and the store manager 
would need to use the data scanned at the registers, albeit for some-
what different purposes. a data scientist would help the system ar-
chitect by providing input on how the data would need to be 
routed and organized to support the analysis, visualization, and 
presentation of the data to the appropriate people.
next, acquisition focuses on how the data are collected, and, impor-
tantly, how the data are represented prior to analysis and presenta-
tion. for example, each barcode represents a number that, by itself, 
is not very descriptive of the product it represents. at what point 
after the barcode scanner does its job should the number be associ-
ated with a text description of the product or its price or its net 
weight or its packaging type? different barcodes are used for the 
same product (for example, for different sized boxes of cereal). 
when should we make note that purchase x and purchase y are 
the same product, just in different packages? representing, trans-
forming, grouping, and linking the data are all tasks that need to 
occur before the data can be pro   tably analyzed, and these are all 
tasks in which the data scientist is actively involved. 
 the analysis phase is where data scientists are most heavily in-
volved. in this context we are using analysis to include summariza-
tion of the data, using portions of data (samples) to make infer-
ences about the larger context, and visualization of the data by pre-

5

senting it in tables, graphs, and even animations. although there 
are many technical, mathematical, and statistical aspects to these 
activities, keep in mind that the ultimate audience for data analysis 
is always a person or people. these people are the "data users" and 
ful   lling their needs is the primary job of a data scientist. this 
point highlights the need for excellent communication skills in 
data science. the most sophisticated statistical analysis ever devel-
oped will be useless unless the results can be effectively communi-
cated to the data user. 
finally, the data scientist must become involved in the archiving of 
the data. preservation of collected data in a form that makes it 
highly reusable - what you might think of as "data curation" - is a 
dif   cult challenge because it is so hard to anticipate all of the fu-
ture uses of the data. for example, when the developers of twitter 
were working on how to store tweets, they probably never antici-
pated that tweets would be used to pinpoint earthquakes and tsu-
namis, but they had enough foresight to realize that "geocodes" - 
data that shows the geographical location from which a tweet was 
sent - could be a useful element to store with the data.
all in all, our cereal box and grocery store example helps to high-
light where data scientists get involved and the skills they need. 
here are some of the skills that the example suggested:
    learning the application domain - the data scientist must 

quickly learn how the data will be used in a particular context.
    communicating with data users - a data scientist must possess 

strong skills for learning the needs and preferences of users. 
translating back and forth between the technical terms of com-

puting and statistics and the vocabulary of the application do-
main is a critical skill.

and must be able to communicate the limitations of data to try to 
prevent misuse of data or analytical results.

    seeing the big picture of a complex system - after developing an 
understanding of the application domain, the data scientist must 
imagine how data will move around among all of the relevant 
systems and people.

    knowing how data can be represented - data scientists must 
have a clear understanding about how data can be stored and 
linked, as well as about "metadata" (data that describes how 
other data are arranged).

    data transformation and analysis - when data become available 
for the use of decision makers, data scientists must know how to 
transform, summarize, and make id136s from the data. as 
noted above, being able to communicate the results of analyses 
to users is also a critical skill here.

    visualization and presentation - although numbers often have 
the edge in precision and detail, a good data display (e.g., a bar 
chart) can often be a more effective means of communicating re-
sults to data users.

    attention to quality - no matter how good a set of data may be, 
there is no such thing as perfect data. data scientists must know 
the limitations of the data they work with, know how to quan-
tify its accuracy, and be able to make suggestions for improving 
the quality of the data in the future.

    ethical reasoning - if data are important enough to collect, they 
are often important enough to affect people   s lives. data scien-
tists must understand important ethical issues such as privacy, 

the skills and capabilities noted above are just the tip of the ice-
berg, of course, but notice what a wide range is represented here. 
while a keen understanding of numbers and mathematics is impor-
tant, particularly for data analysis, the data scientist also needs to 
have excellent communication skills, be a great systems thinker, 
have a good eye for visual displays, and be highly capable of think-
ing critically about how data will be used to make decisions and 
affect people   s lives. of course there are very few people who are 
good at all of these things, so some of the people interested in data 
will specialize in one area, while others will become experts in an-
other area. this highlights the importance of teamwork, as well.
in this introduction to data science ebook, a series of data prob-
lems of increasing complexity is used to illustrate the skills and ca-
pabilities needed by data scientists. the open source data analysis 
program known as "r" and its graphical user interface companion 
"r-studio" are used to work with real data examples to illustrate 
both the challenges of data science and some of the techniques 
used to address those challenges. to the greatest extent possible, 
real datasets re   ecting important contemporary issues are used as 
the basis of the discussions.
no one book can cover the wide range of activities and capabilities 
involved in a    eld as diverse and broad as data science. through-
out the book references to other guides and resources provide the 
interested reader with access to additional information. in the open 
source spirit of "r" and "r studio" these are, wherever possible, 
web-based and free. in fact, one of guides that appears most fre-

6

quently in these pages is "wikipedia," the free, online, user sourced 
encyclopedia. although some teachers and librarians have legiti-
mate complaints and concerns about wikipedia, and it is admit-
tedly not perfect, it is a very useful learning resource. because it is 
free, because it covers about 50 times more topics than a printed en-
cyclopedia, and because it keeps up with fast moving topics (like 
data science) better than printed encyclopedias, wikipedia is very 
useful for getting a quick introduction to a topic. you can   t become 
an expert on a topic by only consulting wikipedia, but you can cer-
tainly become smarter by starting there. 
another very useful resource is khan academy. most people think 
of khan academy as a set of videos that explain math concepts to 
middle and high school students, but thousands of adults around 
the world use khan academy as a refresher course for a range of 
topics or as a quick introduction to a topic that they never studied 
before. all of the lessons at khan academy are free, and if you log 
in with a google or facebook account you can do exercises and 
keep track of your progress.
at the end of each chapter of this book, a list of wikipedia sources 
and khan academy lessons (and other resources too!) shows the 
key topics relevant to the chapter. these sources provide a great 
place to start if you want to learn more about any of the topics that 
chapter does not explain in detail.
obviously if you are reading this book you probably have access to 
an appropriate reader app, probably on an ipad or other apple de-
vice. you can also access this book as a pdf on the book   s website: 
http://jsresearch.net/wiki/projects/teachdatascience/teach_data
_science.html. it is valuable to have access to the internet while 

you are reading, so that you can follow some of the many links this 
book provides. also, as you move into the sections in the book 
where open source software such as the r data analysis system is 
used, you will sometimes need to have access to a desktop or lap-
top computer where you can run these programs. 
one last thing: the book presents topics in an order that should 
work well for people with little or no experience in computer sci-
ence or statistics. if you already have knowledge, training, or expe-
rience in one or both of these areas, you should feel free to skip 
over some of the introductory material and move right into the top-
ics and chapters that interest you most. there   s something here for 
everyone and, after all, you can   t beat the price!
sources
http://en.wikipedia.org/wiki/e-science 
http://en.wikipedia.org/wiki/e-science_librarianship 
http://en.wikipedia.org/wiki/wikipedia:size_comparisons 
http://en.wikipedia.org/wiki/statistician 
http://en.wikipedia.org/wiki/visualization_(computer_graphics) 
http://www.khanacademy.org/ 
http://www.r-project.org/
http://www.readwriteweb.com/hack/2011/09/unlocking-big-dat
a-with-r.php 
http://rstudio.org/

7

chapter 1

about data

data comes from the latin word, "datum," meaning a "thing given."  although the term "data" has 
been used since as early as the 1500s, modern usage started in the 1940s and 1950s as practical 
electronic computers began to input, process, and output data. this chapter discusses the nature of 
data and introduces key concepts for newcomers without computer science experience.

8

the inventor of the world wide web, tim berners-lee, is often 
quoted as having said, "data is not information, information is not 
knowledge, knowledge is not understanding, understanding is not 
wisdom." this quote suggests a kind of pyramid, where data are 
the raw materials that make up the foundation at the bottom of the 
pile, and information, knowledge, understanding and wisdom rep-
resent higher and higher levels of the pyramid. in one sense, the 
major goal of a data scientist is to help people to turn data into in-
formation and onwards up the pyramid. before getting started on 
this goal, though, it is important to have a solid sense of what data 
actually are. (notice that this book treats the word "data" as a plu-
ral noun - in common usage you may often hear it referred to as 
singular instead.) if you have studied computer science or mathe-
matics, you may    nd the discussion in this chapter a bit redun-
dant, so feel free to skip it. otherwise, read on for an introduction 
to the most basic ingredient to the data scientist   s efforts: data.
a substantial amount of what we know and say about data in the 
present day comes from work by a u.s. mathematician named 
claude shannon. shannon worked before, during, and after world 
war ii on a variety of mathematical and engineering problems re-
lated to data and information. not to go crazy with quotes, or any-
thing, but shannon is quoted as having said, "the fundamental 
problem of communication is that of reproducing at one point ei-
ther exactly or approximately a message selected at another point." 
this quote helpfully captures key ideas about data that are impor-
tant in this book by focusing on the idea of data as a message that 
moves from a source to a recipient. think about the simplest possi-
ble message that you could send to another person over the phone, 
via a text message, or even in person. let   s say that a friend had 
asked you a question, for example whether you wanted to come to 

their house for dinner the next day. you can answer yes or no. you 
can call the person on the phone, and say yes or no. you might 
have a bad connection, though, and your friend might not be able 
to hear you. likewise, you could send them a text message with 
your answer, yes or no, and hope that they have their phone 
turned on so that they can receive the message. or you could tell 
your friend face to face, hoping that she did not have her earbuds 
turned up so loud that she couldn   t hear you. in all three cases you 
have a one "bit" message that you want to send to your friend, yes 
or no, with the goal of "reducing her uncertainty" about whether 
you will appear at her house for dinner the next day. assuming 
that message gets through without being garbled or lost, you will 
have successfully transmitted one bit of information from you to 
her. claude shannon developed some mathematics, now often re-
ferred to as "id205," that carefully quanti   ed how 
bits of data transmitted accurately from a source to a recipient can 
reduce uncertainty by providing information. a great deal of the 
computer networking equipment and software in the world today 
- and especially the huge linked worldwide network we call the 
internet - is primarily concerned with this one basic task of getting 
bits of information from a source to a destination.
once we are comfortable with the idea of a "bit" as the most basic 
unit of information, either "yes" or "no," we can combine bits to-
gether to make more complicated structures. first, let   s switch la-
bels just slightly. instead of "no" we will start using zero, and in-
stead of "yes" we will start using one. so we now have a single 
digit, albeit one that has only two possible states: zero or one 
(we   re temporarily making a rule against allowing any of the big-
ger digits like three or seven). this is in fact the origin of the word 
"bit," which is a squashed down version of the phrase "binary 

9

digit." a single binary digit can be 0 or 1, but there is nothing stop-
ping us from using more than one binary digit in our messages. 
have a look at the example in the table below:

meaning 2nd digit 1st digit

no

maybe
probably
de   nitely

0
0
1
1

0
1
0
1

here we have started to use two binary digits - two bits - to create 
a "code book" for four different messages that we might want to 
transmit to our friend about her dinner party. if we were certain 
that we would not attend, we would send her the message 0 0. if 
we de   nitely planned to attend we would send her 1 1. but we 
have two additional possibilities, "maybe" which is represented by 
0 1, and "probably" which is represented by 1 0. it is interesting to 
compare our original yes/no message of one bit with this new 
four-option message with two bits. in fact, every time you add a 
new bit you double the number of possible messages you can send. 
so three bits would give eight options and four bits would give 16 
options. how many options would there be for    ve bits?
when we get up to eight bits - which provides 256 different combi-
nations - we    nally have something of a reasonably useful size to 
work with. eight bits is commonly referred to as a "byte" - this 
term probably started out as a play on words with the word bit. 
(try looking up the word "nybble" online!) a byte offers enough dif-

ferent combinations to encode all of the letters of the alphabet, in-
cluding capital and small letters. there is an old rulebook called 
"ascii" - the american standard code for information interchange 
- which matches up patterns of eight bits with the letters of the al-
phabet, punctuation, and a few other odds and ends. for example 
the bit pattern 0100 0001 represents the capital letter a and the next 
higher pattern, 0100 0010, represents capital b. try looking up an 
ascii table online (for example, http://www.asciitable.com/) and 
you can    nd all of the combinations. note that the codes may not 
actually be shown in binary because it is so dif   cult for people to 
read long strings of ones and zeroes. instead you may see the 
equivalent codes shown in hexadecimal (base 16), octal (base 8), or 
the most familiar form that we all use everyday, base 10. although 
you might remember base conversions from high school math 
class, it would be a good idea to practice this a little bit - particu-
larly the conversions between binary, hexadecimal, and decimal 
(base 10). you might also enjoy vi hart   s "binary hand dance" 
video at khan academy (search for this at 
http://www.khanacademy.org or follow the link at the end of the 
chapter). most of the work we do in this book will be in decimal, 
but more complex work with data often requires understanding 
hexadecimal and being able to know how a hexadecimal number, 
like 0xa3, translates into a bit pattern. try searching online for "bi-
nary conversion tutorial" and you will    nd lots of useful sites.
combining bytes into larger structures
now that we have the idea of a byte as a small collection of bits 
(usually eight) that can be used to store and transmit things like let-
ters and punctuation marks, we can start to build up to bigger and 
better things. first, it is very easy to see that we can put bytes to-

10

gether into lists in order to make a "string" of letters, what is often 
referred to as a "character string." if we have a piece of text, like 
"this is a piece of text" we can use a collection of bytes to represent 
it like this: 
011101000110100001101001011100110010000001101001011100110010
000001100001001000000111000001101001011001010110001101100101
001000000110111101100110001000000111010001100101011110000111
0100
now nobody wants to look at that, let alone encode or decode it by 
hand, but fortunately, the computers and software we use these 
days takes care of the conversion and storage automatically. for ex-
ample, when we tell the open source data language "r" to store 
"this is a piece of text" for us like this:
mytext <- "this is a piece of text"
...we can be certain that inside the computer there is a long list of 
zeroes and ones that represent the text that we just stored. by the 
way, in order to be able to get our piece of text back later on, we 
have made a kind of storage label for it (the word "mytext" above). 
anytime that we want to remember our piece of text or use it for 
something else, we can use the label "mytext" to open up the 
chunk of computer memory where we have put that long list of bi-
nary digits that represent our text. the left-pointing arrow made 
up out of the less-than character ("<") and the dash character ("-") 
gives r the command to take what is on the right hand side (the 
quoted text) and put it into what is on the left hand side (the stor-
age area we have labeled "mytext"). some people call this the as-
signment arrow and it is used in some computer languages to 

make it clear to the human who writes or reads it which direction 
the information is    owing.
from the computer   s standpoint, it is even simpler to store, remem-
ber, and manipulate numbers instead of text. remember that an 
eight bit byte can hold 256 combinations, so just using that very 
small amount we could store the numbers from 0 to 255. (of 
course, we could have also done 1 to 256, but much of the counting 
and numbering that goes on in computers starts with zero instead 
of one.) really, though, 255 is not much to work with. we couldn   t 
count the number of houses in most towns or the number of cars in 
a large parking garage unless we can count higher than 255. if we 
put together two bytes to make 16 bits we can count from zero up 
to 65,535, but that is still not enough for some of the really big num-
bers in the world today (for example, there are more than 200 mil-
lion cars in the u.s. alone). most of the time, if we want to be    exi-
ble in representing an integer (a number with no decimals), we use 
four bytes stuck together. four bytes stuck together is a total of 32 
bits, and that allows us to store an integer as high as 4,294,967,295. 
things get slightly more complicated when we want to store a 
negative number or a number that has digits after the decimal 
point. if you are curious, try looking up "two's complement" for 
more information about how signed numbers are stored and "   oat-
ing point" for information about how numbers with digits after the 
decimal point are stored. for our purposes in this book, the most 
important thing to remember is that text is stored differently than 
numbers, and among numbers integers are stored differently than 
   oating point. later we will    nd that it is sometimes necessary to 
convert between these different representations, so it is always im-
portant to know how it is represented.

11

so far we have mainly looked at how to store one thing at a time, 
like one number or one letter, but when we are solving problems 
with data we often need to store a group of related things together. 
the simplest place to start is with a list of things that are all stored 
in the same way. for example, we could have a list of integers, 
where each thing in the list is the age of a person in your family. 
the list might look like this: 43, 42, 12, 8, 5. the    rst two numbers 
are the ages of the parents and the last three numbers are the ages 
of the kids. naturally, inside the computer each number is stored 
in binary, but fortunately we don   t have to type them in that way 
or look at them that way. because there are no decimal points, 
these are just plain integers and a 32 bit integer (4 bytes) is more 
than enough to store each one. this list contains items that are all 
the same "type" or "mode." the open source data program "r" re-
fers to a list where all of the items are of the same mode as a "vec-
tor." we can create a vector with r very easily by listing the num-
bers, separated by commas and inside parentheses:
c(43, 42, 12, 8, 5)
the letter "c" in front of the opening parenthesis stands for concate-
nate, which means to join things together. slightly obscure, but 
easy enough to get used to with some practice. we can also put in 
some of what we learned a above to store our vector in a named lo-
cation (remember that a vector is list of items of the same mode/
type):
myfamilyages <- c(43, 42, 12, 8, 5)
we have just created our    rst "data set." it is very small, for sure, 
only    ve items, but also very useful for illustrating several major 
concepts about data. here   s a recap:

    in the heart of the computer, all data are represented in binary. 

one binary digit, or bit, is the smallest chunk of data that we can 
send from one place to another.

    although all data are at heart binary, computers and software 
help to represent data in more convenient forms for people to 
see. three important representations are: "character" for repre-
senting text, "integer" for representing numbers with no digits 
after the decimal point, and "   oating point" for numbers that 
may have digits after the decimal point. the list of numbers in 
our tiny data set just above are integers.

    numbers and text can be collected into lists, which the open 

source program "r" calls vectors. a vector has a length, which is 
the number of items in it, and a "mode" which is the type of data 
stored in the vector. the vector we were just working on has a 
length of 5 and a mode of integer.

    in order to be able to remember where we stored a piece of data, 
most computer programs, including r, give us a way of labeling 
a chunk of computer memory. we chose to give the 5-item vector 
up above the name "myfamilyages." some people might refer to 
this named list as a "variable," because the value of it varies, de-
pending upon which member of the list you are examining.
    if we gather together one or more variables into a sensible 

group, we can refer to them together as a "data set." usually, it 
doesn   t make sense to refer to something with just one variable 
as a data set, so usually we need at least two variables. techni-
cally, though, even our very simple "myfamilyages" counts as a 
data set, albeit a very tiny one.  

12

later in the book we will install and run the open source "r" data 
program and learn more about how to create data sets, summarize 
the information in those data sets, and perform some simple calcu-
lations and transformations on those data sets.
chapter challenge
discover the meaning of "boolean logic" and the rules for "and", 
"or", "not", and "exclusive or". once you have studied this for a 
while, write down on a piece of paper, without looking, all of the 
binary operations that demonstrate these rules. 
sources
http://en.wikipedia.org/wiki/claude_shannon
http://en.wikipedia.org/wiki/information_theory 
http://cran.r-project.org/doc/manuals/r-intro.pdf 
http://www.khanacademy.org/math/vi-hart/v/binary-hand-dan
ce 
http://www.khanacademy.org/science/computer-science/v/intr
oduction-to-programs-data-types-and-variables 
http://www.asciitable.com/ 

test yourself

review 1.1 about data

question 1 of  3
the smallest unit of information com-
monly in use in today   s computers is 
called:

a. a bit
b. a byte
c. a nybble
d. an integer

13

check answerchapter 2

identifying data problems

data science is different from other areas such as mathematics or statistics. data science is an applied 
activity and data scientists serve the needs and solve the problems of data users. before you can solve 
a problem, you need to identify it and this process is not always as obvious as it might seem. in this 
chapter, we discuss the identi   cation of data problems.

14

apple farmers live in constant fear,    rst for their blossoms and 
later for their fruit. a late spring frost can kill the blossoms. hail or 
extreme wind in the summer can damage the fruit. more generally, 
farming is an activity that is    rst and foremost in the physical 
world, with complex natural processes and forces, like weather, 
that are beyond the control of humankind. 
in this highly physical world of unpredictable natural forces, is 
there any role for data science? on the surface there does not seem 
to be. but how can we know for sure?  having a nose for identify-
ing data problems requires openness, curiosity, creativity, and a 
willingness to ask a lot of questions. in fact, if you took away from 
the    rst chapter the impression that a data scientist sits in front a of 
computer all day and works a crazy program like r, that is a mis-
take. every data scientist must (eventually) become immersed in 
the problem domain where she is working. the data scientist may 
never actually become a farmer, but if you are going to identify a 
data problem that a farmer has, you have to learn to think like a 
farmer, to some degree.
to get this domain knowledge you can read or watch videos, but 
the best way is to ask "subject matter experts" (in this case farmers) 
about what they do. the whole process of asking questions de-
serves its own treatment, but for now there are three things to 
think about when asking questions. first, you want the subject mat-
ter experts, or smes, as they are sometimes called, to tell stories of 
what they do. then you want to ask them about anomalies: the un-
usual things that happen for better or for worse. finally, you want 
to ask about risks and uncertainty: what are the situations where it 
is hard to tell what will happen next - and what happens next 
could have a profound effect on whether the situation ends badly 

or well. each of these three areas of questioning re   ects an ap-
proach to identifying data problems that may turn up something 
good that could be accomplished with data, information, and the 
right decision at the right time.
the purpose of asking about stories is that people mainly think in 
stories. from farmers to teachers to managers to ceos, people 
know and tell stories about success and failure in their particular 
domain. stories are powerful ways of communicating wisdom be-
tween different members of the same profession and they are ways 
of collecting a sense of identity that sets one profession apart from 
another profession. the only problem is that stories can be wrong.
if you can get a professional to tell the main stories that guide how 
she conducts her work, you can then consider how to verify those 
stories. without questioning the veracity of the person that tells the 
story, you can imagine ways of measuring the different aspects of 
how things happen in the story with an eye towards eventually 
verifying (or sometimes debunking) the stories that guide profes-
sional work.
for example, the farmer might say that in the deep spring frost 
that occurred    ve years ago, the trees in the hollow were spared 
frost damage while the trees  around the ridge of the hill had more 
damage. for this reason, on a cold night the farmer places most of 
the smudgepots (containers that hold a fuel that creates a smoky 
   re) around the ridge. the farmer strongly believes that this strat-
egy works, but does it? it would be possible to collect time-series 
temperature data from multiple locations within the orchard on 
cold and warm nights, and on nights with and without smudge-
pots. the data could be used to create a model of temperature 

15

changes in the different areas of the orchard and this model could 
support, improve, or debunk the story. 
a second strategy for problem identi   cation is to look for the excep-
tion cases, both good and bad. a little later in the book we will 
learn about how the core of classic methods of statistical id136 
is to characterize "the center" - the most typical cases that occur - 
and then examine the extreme cases that are far from the center for 
information that could help us understand an intervention or an 
unusual combination of circumstances. identifying unusual cases 
is a powerful way of understanding how things work, but it is nec-
essary    rst to de   ne the central or most typical occurrences in or-
der to have an accurate idea of what constitutes an unusual case. 
coming back to our farmer friend, in advance of a thunderstorm 
late last summer, a powerful wind came through the orchard, tear-
ing the fruit off the trees. most of the trees lost a small amount of 
fruit: the dropped apples could be seen near the base of the tree.  
one small grouping of trees seemed to lose a much larger amount 
of fruit, however, and the drops were apparently scattered much 
further from the trees. is it possible that some strange wind condi-
tions made the situation worse in this one spot? or is it just a mat-
ter of chance that a few trees in the same area all lost a bit more 
fruit than would be typical.
a systematic count of lost fruit underneath a random sample of 
trees would help to answer this question. the bulk of the trees 
would probably have each lost about the same amount, but more 
importantly, that "typical" group would give us a yardstick against 
which we could determine what would really count as unusual. 
when we found an unusual set of cases that was truly beyond the 

limits of typical, we could rightly focus our attention on these to 
try to understand the anomaly.
a third strategy for identifying data problems is to    nd out about 
risk and uncertainty. if you read the previous chapter you may re-
member that a basic function of information is to reduce uncer-
tainty. it is often valuable to reduce uncertainty because of how 
risk affects the things we all do. at work, at school, at home, life is 
full of risks: making a decision or failing to do so sets off a chain of 
events that may lead to something good or something not so good. 
it is dif   cult to say, but in general we would like to narrow things 
down in a way that maximizes the chances of a good outcome and 
minimizes the chance of a bad one. to do this, we need to make bet-
ter decisions and to make better decisions we need to reduce uncer-
tainty. by asking questions about risks and uncertainty (and deci-
sions) a data scientist can zero in on the problems that matter. you 
can even look at the previous two strategies - asking about the sto-
ries that comprise professional wisdom and asking about 
anomalies/unusual cases - in terms of the potential for reducing 
uncertainty and risk.
in the case of the farmer, much of the risk comes from the weather, 
and the uncertainty revolves around which countermeasures will 
be cost effective under prevailing conditions. consuming lots of ex-
pensive oil in smudgepots on a night that turns out to be quite 
warm is a waste of resources that could make the difference be-
tween a pro   table or an unpro   table year. so more precise and 
timely information about local weather conditions might be a key 
focus area for problem solving with data. what if a live stream of 
national weather service doppler radar could appear on the 
farmer   s smart phone? let   s build an app for that...

16

chapter 3

getting started with r

"r" is an open source software program, developed by volunteers as a service to the community of 
scientists, researchers, and data analysts who use it. r is free to download and use. lots of advice and 
guidance is available online to help users learn r, which is good because it is a powerful and complex 
program, in reality a full featured programming language dedicated to data.

17

if you are new to computers, programming, and/or data science 
welcome to an exciting chapter that will open the door to the most 
powerful free data analytics tool ever created anywhere in the uni-
verse, no joke. on the other hand, if you are experienced with 
spreadsheets, statistical analysis, or accounting software you are 
probably thinking that this book has now gone off the deep end, 
never to return to sanity and all that is good and right in user inter-
face design. both perspectives are reasonable. the "r" open source 
data analysis program is immensely powerful,    exible, and espe-
cially "extensible" (meaning that people can create new capabilities 
for it quite easily). at the same time, r is "command line" oriented, 
meaning that most of the work that one needs to perform is done 
through carefully crafted text instructions, many of which have 
tricky syntax (the punctuation and related rules for making a com-
mand that works). in addition, r is not especially good at giving 
feedback or error messages that help the user to repair mistakes or 
   gure out what is wrong when results look funny. 
but there is a method to the madness here. one of the virtues of r 
as a teaching tool is that it hides very little. the successful user 
must fully understand what the "data situation" is or else the r 
commands will not work. with a spreadsheet, it is easy to type in a 
lot of numbers and a formula like =forecast() and a result pops 
into a cell like magic, whether it makes any sense or not. with r 
you have to know your data, know what you can do with it, know 
how it has to be transformed, and know how to check for prob-
lems. because r is a programming language, it also forces users to 
think about problems in terms of data objects, methods that can be 
applied to those objects, and procedures for applying those meth-
ods. these are important metaphors used in modern programming 
languages, and no data scientist can succeed without having at 

least a rudimentary understanding of how software is pro-
grammed, tested, and integrated into working systems. the extensi-
bility of r means that new modules are being added all the time by 
volunteers: r was among the    rst analysis programs to integrate 
capabilities for drawing data directly from the twitter(r) social me-
dia platform. so you can be sure that whatever the next big devel-
opment is in the world of data, that someone in the r community 
will start to develop a new "package" for r that will make use of it. 
finally, the lessons one learns in working with r are almost univer-
sally applicable to other programs and environments. if one has 
mastered r, it is a relatively small step to get the hang of the sas(r) 
statistical programming language and an even smaller step to be-
ing able to follow spss(r) syntax. (sas and spss are two of the 
most widely used commercial statistical analysis programs). so 
with no need for any licensing fees paid by school, student, or 
teacher it is possible to learn the most powerful data analysis sys-
tem in the universe and take those lessons with you no matter 
where you go. it will take a bit of patience though, so please hang 
in there!
let   s get started. obviously you will need a computer. if you are 
working on a tablet device or smartphone, you may want to skip 
forward to the chapter on r-studio, because regular old r has not 
yet been recon   gured to work on tablet devices (but there is a 
workaround for this that uses r-studio).  there are a few experi-
ments with web-based interfaces to r, like this one - 
http://dssm.unipa.it/r-php/r-php-1/r/ - but they are still in a 
very early stage. if your computer has the windows(r), mac-os-
x(r) or a linux operating system, there is a version of r waiting for 
you at http://cran.r-project.org/. download and install your own 
copy. if you sometimes have dif   culties with installing new soft-

18

ware and you need some help, there is a wonderful little book by 
thomas p. hogan called, bare bones r: a brief introductory guide 
that you might want to buy or borrow from your library. there are 
lots of sites online that also give help with installing r, although 
many of them are not oriented towards the inexperienced user. i 
searched online using the term  "help installing r" and i got a few 
good hits. one site that was quite informative for installing r on 
windows was at "readthedocs.org," and you can try to access it at 
this tinyurl: http://tinyurl.com/872ngtt.   for mac users there is a 

video by jeremy taylor at vimeo.com, 
http://vimeo.com/36697971, that outlines both the initial installa-
tion on a mac and a number of other optional steps for getting 
started. youtube also had four videos that provide brief tutorials 
for installing r. try searching for "install r" in the youtube search 
box. the rest of this chapter assumes that you have installed r and 
can run it on your computer as shown in the screenshot above. 
(note that this screenshot is from the mac version of r: if you are 
running windows or linux your r screen may appear slightly dif-
ferent from this.) just for fun, one of the    rst things you can do 
when you have r running is to click on the color wheel and cus-
tomize the appearance of r. this screen shot uses syracuse orange 
as a background color. the screenshot also shows a simple com-
mand to type that shows the most basic method of interaction with 
r. notice near the bottom of the screenshot a greater than (">") 
symbol. this is the command prompt: when r is running and it is 
the active application on your desktop, if you type a command it 
appears after the ">" symbol. if you press the "enter" or "return" 
key, the command is sent to r for processing. when the processing 
is done, a result may appear just under the ">." when r is done 
processing, another command prompt (">") appears and r is ready 
for your next command. in the screen shot, the user has typed 
"1+1" and pressed the enter key. the formula 1+1 is used by ele-
mentary school students everywhere to insult each other   s math 
skills, but r dutifully reports the result as 2. if you are a careful ob-
server, you will notice that just before the 2 there is a "1" in brack-
ets, like this: [1]. that [1] is a line number that helps to keep track 
of the results that r displays. pretty pointless when only showing 
one line of results, but r likes to be consistent, so we will see quite 
a lot of those numbers in brackets as we dig deeper.

19

remember the list of ages of family members from the about data 
chapter? no? well, here it is again: 43, 42, 12, 8, 5, for dad, mom, 
sis, bro, and the dog, respectively. we mentioned that this was a list 
of items, all of the same mode, namely "integer." remember that 

you can tell that they are ok to be integers because there are no 
decimal points and therefore nothing after the decimal point. we 
can create a vector of integers in r using the "c()" command. take a 
look at the screen shot just above.

20

this is just about the last time that the whole screenshot from the r 
console will appear in the book. from here on out we will just look 
at commands and output so we don   t waste so much space on the 
page. the    rst command line in the screen shot is exactly what ap-
peared in an earlier chapter:
c(43, 42, 12, 8, 5)
you may notice that on the following line, r dutifully reports the 
vector that you just typed. after the line number "[1]", we see the 
list 43, 42, 12, 8, and 5. r "echoes" this list back to us, because we 
didn   t ask it to store the vector anywhere. in contrast, the next com-
mand line (also the same as in the previous chapter), says: 
myfamilyages <- c(43, 42, 12, 8, 5)
we have typed in the same list of numbers, but this time we have 
assigned it, using the left pointing arrow, into a storage area that 
we have named "myfamilyages." this time, r responds just with 
an empty command prompt. that   s why the third command line 
requests a report of what myfamilyages contains (look after the 
yellow ">". the text in blue is what you should type.) this is a sim-
ple but very important tool. any time you want to know what is in 
a data object in r, just type the name of the object and r will report 
it back to you. in the next command we begin to see the power of 
r:
sum(myfamilyages)
this command asks r to add together all of the numbers in 
myfamilyages, which turns out to be 110 (you can check it your-
self with a calculator if you want). this is perhaps a bit of a weird 
thing to do with the ages of family members, but it shows how 

with a very short and simple command you can unleash quite a bit 
of processing on your data. in the next line we ask for the "mean" 
(what non-data people call the average) of all of the ages and this 
turns out to be 22 years. the command right afterwards, called 
"range," shows the lowest and highest ages in the list. finally, just 
for fun, we tried to issue the command "   sh(myfamilyages)." 
pretty much as you might expect, r does not contain a "   sh()" func-
tion and so we received an error message to that effect. this shows 
another important principle for working with r: you can freely try 
things out at anytime without fear of breaking anything. if r can   t 
understand what you want to accomplish, or you haven   t quite    g-
ured out how to do something, r will calmly respond with an error 
message and will not make any other changes until you give it a 
new command. the error messages from r are not always super 
helpful, but with some strategies that the book will discuss in fu-
ture chapters you can break down the problem and    gure out how 
to get r to do what you want. 
let   s take stock for a moment. first, you should de   nitely try all of 
the commands noted above on your own computer. you can read 
about the commands in this book all you want, but you will learn a 
lot more if you actually try things out. second, if you try a com-
mand that is shown in these pages and it does not work for some 
reason, you should try to    gure out why. begin by checking your 
spelling and punctuation, because r is very persnickety about how 
commands are typed. remember that capitalization matters in r: 
myfamilyages is not the same as myfamilyages. if you verify that 
you have typed a command just as you see in the book and it still 
does not work, try to go online and look for some help. there   s lots 
of help at http://stackover   ow.com, at https://stat.ethz.ch, and 
also at  http://www.statmethods.net/. if you can    gure out what 

went wrong on your own you will probably learn something very 
valuable about working with r. third, you should take a moment 
to experiment a bit with each new set of commands that you learn. 
for example, just using the commands discussed earlier in the 
chapter you could do this totally new thing:
myrange <- range(myfamilyages)
what would happen if you did that command, and then typed 
"myrange" (without the double quotes) on the next command line 
to report back what is stored there ? what would you see? then 
think about how that worked and try to imagine some other experi-
ments that you could try. the more you experiment on your own, 
the more you will learn. some of the best stuff ever invented for 
computers was the result of just experimenting to see what was 
possible. at this point, with just the few commands that you have 
already tried, you already know the following things about r (and 
about data):
    how to install r on your computer and run it.
    how to type commands on the r console.
    the use of the "c()" function. remember that "c" stands for con-
catenate, which just means to join things together. you can put a 
list of items inside the parentheses, separated by commas.

    that a vector is pretty much the most basic form of data storage 

in r, and that it consists of a list of items of the same mode.

    that a vector can be stored in a named location using the assign-
ment arrow (a left pointing arrow made of a dash and a less than 
symbol, like this: "<-").

21

https://plus.google.com/u/0/104922476697914343874/posts (jer-
emy taylor   s blog: stats make me cry)
http://stackover   ow.com
https://stat.ethz.ch 
http://www.statmethods.net/

    that you can get a report of the data object that is in any named 

location just by typing that name at the command line.

    that you can "run" a function, such as mean(), on a vector of 
numbers to transform them into something else. (the mean() 
function calculates the average, which is one of the most basic 
numeric summaries there is.)

    that sum(), mean(), and range() are all legal functions in r 

whereas    sh() is not.

in the next chapter we will move forward a step or two by starting 
to work with text and by combining our list of family ages with the 
names of the family members and some other information about 
them.
chapter challenge
using logic and online resources to get help if you need it, learn 
how to use the c() function to add another family member   s age on 
the end of the myfamilyages vector.
sources
http://a-little-book-of-r-for-biomedical-statistics.readthedocs.org/
en/latest/src/installr.html 
http://cran.r-project.org/
http://dssm.unipa.it/r-php/r-php-1/r/ (unipa experimental 
web interface to r)
http://en.wikibooks.org/wiki/r_programming 

22

concatenates data elements together
assignment arrow
adds data elements

r functions used in this chapter 
c()!
!
<- !
!
sum()!
range()! min value and max value
mean()!

the average

test yourself

review 3.1 getting started with r

question 1 of  3
what is the cost of each software license for the r open 
source data analysis program?

a. r is free
b. 99 cents in the itunes store
c. $10
d. $100

 

23

check answerchapter 4

follow the data

an old adage in detective work is to, "follow the money." in data science, one key to success is to 
"follow the data." in most cases, a data scientist will not help to design an information system from 
scratch. instead, there will be several or many legacy systems where data resides; a big part of the 
challenge to the data scientist lies in integrating those systems.

24

hate to nag, but have you had a checkup lately? if you have been 
to the doctor for any reason you may recall that the doctor   s of   ce 
is awash with data. first off, the doctor has loads of digital sensors, 
everything from blood pressure monitors to ultrasound machines, 
and all of these produce mountains of data. perhaps of greater con-
cern in this era of debate about health insurance, the doctors of   ce 
is one of the big jumping off points for    nancial and insurance 
data. one of the notable "features" of the u.s. healthcare system is 
our most common method of healthcare delivery: paying by the 
procedure. when you experience a "procedure" at the doctor   s of-
   ce, whether it is a consultation, an examination, a test, or some-
thing else, this initiates a chain of data events with far reaching con-
sequences.
if your doctor is typical, the starting point of these events is a pa-
per form. have you ever looked at one of these in detail? most of 
the form will be covered by a large matrix of procedures and 
codes. although some of the better equipped places may use this 
form digitally on a tablet or other computer, paper forms are still 
ubiquitous. somewhere either in the doctor   s of   ce or at an out-
sourced service company, the data on the paper form are entered 
into a system that begins the insurance reimbursement and/or bill-
ing process.
where do these procedure data go? what other kinds of data (such 
as patient account information) may get attached to them in a sub-
sequent step? what kinds of networks do these linked data travel 
over, and what kind of security do they have? how many steps are 
there in processing the data before they get to the insurance com-
pany? how does the insurance company process and analyze the 
data before issuing the reimbursement? how is the money "trans-

mitted" once the insurance company   s systems have given ap-
proval to the reimbursement? these questions barely scratch the 
surface: there are dozens or hundreds of processing steps that we 
haven   t yet imagined.
it is easy to see from this example, that the likelihood of being able 
to throw it all out and start designing a better or at least more stan-
dardized system from scratch is nil. but what if you had the job of 
improving the ef   ciency of the system, or auditing the insurance 
reimbursements to make sure they were compliant with insurance 
records, or using the data to detect and predict outbreaks and epi-
demics, or providing feedback to consumers about how much they 
can expect to pay out of pocket for various procedures? 
the critical starting point for your project would be to follow the 
data. you would need to be like a detective,    nding out in a sub-
stantial degree of detail the content, format, senders, receivers, 
transmission methods, repositories, and users of data at each step 
in the process and at each organization where the data are proc-
essed or housed.
fortunately there is an extensive area of study and practice called 
"data modeling" that provides theories, strategies, and tools to help 
with the data scientist   s goal of following the data. these ideas 
started in earnest in the 1970s with the introduction by computer 
scientist ed yourdon of a methodology called data flow diagrams. 
a more contemporary approach, that is strongly linked with the 
practice of creating id208, is called the entity-
relationship model. professionals using this model develop entity-
relationship diagrams (erds) that describe the structure and 
movement of data in a system.

25

the art lies in understanding the users    current information needs 
and anticipating how those needs may change in the future. if an 
organization is redesigning a system, adding to a system, or creat-
ing brand new systems, they are doing so in the expectation of a 
future bene   t. this bene   t may arise from greater ef   ciency, reduc-
tion of errors/inaccuracies, or the hope of providing a new product 
or service with the enhanced information capabilities. 
whatever the goal, the data scientist has an important and dif   cult 
challenge of taking the methods of today - including paper forms 
and manual data entry - and imagining the methods of tomorrow. 
follow the data!
in the next chapter we look at one of the most common and most 
useful ways of organizing data, namely in a rectangular structure 
that has rows and columns. this rectangular arrangement of data 
appears in spreadsheets and databases that are used for a variety 
of applications. understanding how these rows and columns are 
organized is critical to most tasks in data science.

sources
http://en.wikipedia.org/wiki/data_modeling 
http://en.wikipedia.org/wiki/entity-relationship_diagram 

entity-relationship modeling occurs at different levels ranging 
from an abstract conceptual level to a physical storage level. at the 
conceptual level an entity is an object or thing, usually something 
in the real world. in the doctor   s of   ce example, one important "ob-
ject" is the patient. another entity is the doctor. the patient and the 
doctor are linked by a relationship: in modern health care lingo 
this is the "provider" relationship. if the patient is mr. x and the 
doctor is dr. y, the provider relationship provides a bidirectional 
link: 
    dr. y is the provider for mr. x
    mr. x   s provider is dr. y
naturally there is a range of data that can represent mr. x: name 
address, age, etc. likewise, there are data that represent dr. y: 
years of experience as a doctor, specialty areas, certi   cations, li-
censes. importantly, there is also a chunk of data that represents 
the linkage between x and y, and this is the relationship.
creating an erd requires investigating and enumerating all of the 
entities, such as patients and doctors, as well as all of the relation-
ships that may exist among them. as the beginning of the chapter 
suggested, this may have to occur across multiple organizations 
(e.g., the doctor   s of   ce and the insurance company) depending 
upon the purpose of the information system that is being designed.  
eventually, the erds must become detailed enough that they can 
serve as a speci   cation for the physical storage in a database.
in an application area like health care, there are so many choices 
for different ways of designing the data that it requires some expe-
rience and possibly some "art" to create a workable system. part of 

26

chapter 5

rows and columns

one of the most basic and widely used methods of representing data is to use rows and columns, 
where each row is a case or instance and each column is a variable and attribute. most spreadsheets 
arrange their data in rows and columns, although spreadsheets don   t usually refer to these as cases or 
variables. r represents rows and columns in an object called a data frame. 

27

although we live in a three dimensional world, where a box of ce-
real has height, width, and depth, it is a sad fact of modern life that 
pieces of paper, chalkboards, whiteboards, and computer screens 
are still only two dimensional. as a result, most of the statisticians, 
accountants, computer scientists, and engineers who work with 
lots of numbers tend to organize them in rows and columns. 
there   s really no good reason for this other than it makes it easy to 
   ll a rectangular piece of paper with numbers. rows and columns 
can be organized any way that you want, but the most common 
way is to have the rows be "cases" or "instances" and the columns 
be "attributes" or "variables." take a look at this nice, two dimen-
sional representation of rows and columns:

name
dad
mom
sis
bro
dog

age
43
42
12
8
5

gender weight

male
female
female
male
female

188
136
83
61
44

pretty obvious what   s going on, right? the top line, in bold, is not 
really part of the data. instead, the top line contains the attribute or 
variable names. note that computer scientists tend to call them at-
tributes while statisticians call them variables. either term is ok. 
for example, age is an attribute that every living thing has, and 
you could count it in minutes, hours, days, months, years, or other 
units of time. here we have the age attribute calibrated in years. 
technically speaking, the variable names in the top line are "meta-

28

data" or what you could think of as data about data. imagine how 
much more dif   cult it would be to understand what was going on 
in that table without the metadata. there   s lot of different kinds of 
metadata: variable names are just one simple type of metadata.
so if you ignore the top row, which contains the variable names, 
each of the remaining rows is an instance or a case. again, com-
puter scientists may call them instances, and statisticians may call 
them cases, but either term is    ne. the important thing is that each 
row refers to an actual thing. in this case all of our things are living 
creatures in a family. you could think of the name column as "case 
labels" in that each one of these labels refers to one and only one 
row in our data. most of the time when you are working with a 
large dataset, there is a number used for the case label, and that 
number is unique for each case (in other words, the same number 
would never appear in more than one row). computer scientists 
sometimes refer to this column of unique numbers as a "key." a key 
is very useful particularly for matching things up from different 
data sources, and we will run into this idea again a bit later. for 
now, though, just take note that the "dad" row can be distin-
guished from the "bro" row, even though they are both male. even 
if we added an "uncle" row that had the same age, gender, and 
weight as "dad" we would still be able to tell the two rows apart 
because one would have the name "dad" and the other would have 
the name "uncle."
one other important note: look how each column contains the 
same kind of data all the way down. for example, the age column 
is all numbers. there   s nothing in the age column like "old" or 
"young." this is a really valuable way of keeping things organized. 
after all, we could not run the mean() function on the age column 

if it contained a little piece of text, like "old" or "young." on a re-
lated note, every cell (that is an intersection of a row and a column, 
for example, sis   s age) contains just one piece of information. al-
though a spreadsheet or a word processing program might allow 
us to put more than one thing in a cell, a real data handling pro-
gram will not. finally, see that every column has the same number 
of entries, so that the whole forms a nice rectangle. when statisti-
cians and other people who work with databases work with a data-
set, they expect this rectangular arrangement.
now let   s    gure out how to get these rows and columns into r. 
one thing you will quickly learn about r is that there is almost al-
ways more than one way to accomplish a goal. sometimes the 
quickest or most ef   cient way is not the easiest to understand. in 
this case we will build each column one by one and then join them 
together into a single data frame. this is a bit labor intensive, and 
not the usual way that we would work with a data set, but it is 
easy to understand. first, run this command to make the column 
of names:
myfamilynames <- c("dad","mom","sis","bro","dog")
one thing you might notice is that every name is placed within 
double quotes. this is how you signal to r that you want it to treat 
something as a string of characters rather than the name of a stor-
age location. if we had asked r to use dad instead of "dad" it 
would have looked for a storage location (a data object) named 
dad. another thing to notice is that the commas separating the dif-
ferent values are outside of the double quotes. if you were writing 
a regular sentence this is not how things would look, but for com-
puter programming the comma can only do its job of separating 
the different values if it is not included inside the quotes. once you 

have typed the line above, remember that you can check the con-
tents of myfamilynames by typing it on the next command line:
myfamilynames
the output should look like this:
[1] "dad" "mom" "sis" "bro" "dog"
next, you can create a vector of the ages of the family members, 
like this:
myfamilyages <- c(43, 42, 12, 8, 5)
note that this is exactly the same command we used in the last 
chapter, so if you have kept r running between then and now you 
would not even have to retype this command because 
myfamilyages would still be there. actually, if you closed r since 
working the examples from the last chapter you will have been 
prompted to "save the workspace" and if you did so, then r re-
stored all of the data objects you were using in the last session. you 
can always check by typing myfamilyages on a blank command 
line. the output should look like this:
[1] 43 42 12  8  5
hey, now you have used the c() function and the assignment arrow 
to make myfamilynames and myfamilyages. if you look at the 
data table earlier in the chapter you should be able to    gure out the 
commands for creating myfamilygenders and myfamilyweights. 
in case you run into trouble, these commands also appear on the 
next page, but you should try to    gure them out for yourself before 
you turn the page. in each case after you type the command to cre-
ate the new data object, you should also type the name of the data 

29

object at the command line to make sure that it looks the way it 
should. four variables, each one with    ve values in it. two of the 
variables are character data and two of the variables are integer 
data. here are those two extra commands in case you need them:
myfamilygenders <- c("male","female","female","male","female")
myfamilyweights <- c(188,136,83,61,44)
now we are ready to tackle the dataframe. in r, a dataframe is a 
list (of columns), where each element in the list is a vector. each 
vector is the same length, which is how we get our nice rectangular  
row and column setup, and generally each vector also has its own 
name. the command to make a data frame is very simple:
myfamily <- data.frame(myfamilynames, + 
myfamilyages, myfamilygenders, myfamilyweights)
look out! we   re starting to get commands that are long enough 
that they break onto more than one line. the + at the end of the 
   rst line tells r to wait for more input on the next line before trying 
to process the command. if you want to, you can type the whole 
thing as one line in r, but if you do, just leave out the plus sign. 
anyway, the data.frame() function makes a dataframe from the 
four vectors that we previously typed in. notice that we have also 
used the assignment arrow to make a new stored location where r 
puts the data frame. this new data object, called myfamily, is our 
dataframe. once you have gotten that command to work, type   
myfamily at the command line to get a report back of what the 
data frame contains. here   s the output you should see:
  myfamilynames myfamilyages myfamilygenders myfamilyweights
1           dad           43            male             188
2           mom           42          female             136

30

3           sis           12          female              83
4           bro            8            male              61
5           dog            5          female              44
this looks great. notice that r has put row numbers in front of 
each row of our data. these are different from the output line num-
bers we saw in brackets before, because these are actual "indices" 
into the data frame. in other words, they are the row numbers that 
r uses to keep track of which row a particular piece of data is in.
with a small data set like this one, only    ve rows, it is pretty easy 
just to take a look at all of the data. but when we get to a bigger 
data set this won   t be practical. we need to have other ways of sum-
marizing what we have. the    rst method reveals the type of "struc-
ture" that r has used to store a data object. 
> str(myfamily)
'data.frame':	 5 obs. of  4 variables:
 $ myfamilynames  : factor w/ 5 levels 
"bro","dad","dog",..: 2 4 5 1 3
	
 $ myfamilyages   : num  43 42 12 8 5
 $ myfamilygenders: factor w/ 2 levels 
	
 $ myfamilyweights: num  188 136 83 61 44
take note that for the    rst time, the example shows the command 
prompt ">" in order to differentiate the command from the output 
that follows. you don   t need to type this: r provides it whenever it 
is ready to receive new input. from now on in the book, there will 

"female","male": 2 1 1 2 1

	

	

be examples of r commands and output that are mixed together, 
so always be on the lookout for ">" because the command after 
that is what you have to type.
ok, so the function "str()" reveals the structure of the data object 
that you name between the parentheses. in this case we pretty well 
knew that myfamily was a data frame because we just set that up 
in a previous command. in the future, however, we will run into 
many situations where we are not sure how r has created a data 
object, so it is important to know str() so that you can ask r to re-
port what an object is at any time.
in the    rst line of output we have the con   rmation that myfamily 
is a data frame as well as an indication that there are    ve observa-
tions ("obs." which is another word that statisticians use instead of 
cases or instances) and four variables. after that    rst line of output, 
we have four sections that each begin with "$". for each of the four 
variables, these sections describe the component columns of the 
myfamily dataframe object.
each of the four variables has a "mode" or type that is reported by 
r right after the colon on the line that names the variable:
$ myfamilygenders: factor w/ 2 levels 
for example, myfamilygenders is shown as a "factor." in the termi-
nology that r uses, factor refers to a special type of label that can 
be used to identify and organize groups of cases. r has organized 
these labels alphabetically and then listed out the    rst few cases 
(because our dataframe is so small it actually is showing us all of 
the cases). for myfamilygenders we see that there are two "lev-
els," meaning that there are two different options: female and male. 

r assigns a number, starting with one, to each of these levels, so 
every case that is "female" gets assigned a 1 and every case that is 
"male" gets assigned a 2 (because female comes before male in the 
alphabet, so female is the    rst factor label, so it gets a 1). if you 
have your thinking cap on, you may be wondering why we started 
out by typing in small strings of text, like "male," but then r has 
gone ahead and converted these small pieces of text into numbers 
that it calls "factors." the reason for this lies in the statistical ori-
gins of r. for years, researchers have done things like calling an ex-
perimental group "exp" and a control, group "ctl" without intend-
ing to use these small strings of text for anything other than labels. 
so r assumes, unless you tell it otherwise, that when you type in a 
short string like "male" that you are referring to the label of a 
group, and that r should prepare for the use of male as a "level" of 
a "factor."  when you don   t want this to happen you can instruct r 
to stop doing this with an option on the data.frame() function: 
stringsasfactors=false. we will look with more detail at options 
and defaults a little later on.
phew, that was complicated! by contrast, our two numeric vari-
ables, myfamilyages and myfamilyweights, are very simple. you 
can see that after the colon the mode is shown as "num" (which 
stands for numeric) and that the    rst few values are reported:
$ myfamilyages   : num  43 42 12 8 5
putting it all together, we have pretty complete information about 
the myfamily dataframe and we are just about ready to do some 
more work with it. we have seen    rsthand that r has some pretty 
cryptic labels for things as well as some obscure strategies for con-
verting this to that. r was designed for experts, rather than nov-

31

	
	
	
	

ices, so we will just have to take our lumps so that one day we can 
be experts too.
next, we will examine another very useful function called sum-
mary(). summary() provides some overlapping information to str() 
but also goes a little bit further, particularly with numeric vari-
ables. here   s what we get:
> summary(myfamily)
 myfamilynames  myfamilyages
 bro: 1		
: 5
min.	
1st qu.	: 8
 dad: 1		
 dog: 1		
median	 : 12
: 22	
mean	
 mom: 1		
 sis: 1         3rd qu.	: 42 
 
myfamilygenders myfamilyweights
 female : 3		
 male	 : 2 	
	
	
	
	
	
	
	
	

min.	
1st qu. : 61.0
median	 : 83.0
mean	
: 102.4
3rd qu.	: 136.0
max		

: 188.0		

	
	
	
	

	
	
	
	

: 44

	

	

32

in order to    t on the page properly, these columns have been reor-
ganized a bit. the name of a column/variable, sits up above the in-
formation that pertains to it, and each block of information is inde-
pendent of the others (so it is meaningless, for instance, that "bro: 
1" and "min." happen to be on the same line of output). notice, as 
with str(), that the output is quite different depending upon 
whether we are talking about a factor, like myfamilynames or 
myfamilygenders, versus a numeric variable like myfamilyages 
and myfamilyweights. the columns for the factors list out a few 
of the factor names along with the number of occurrences of cases 
that  are coded with that factor. so for instance, under 
myfamilygenders it shows three females and two males. in con-
trast, for the numeric variables we get    ve different calculated 
quantities that help to summarize the variable. there   s no time like 
the present to start to learn about what these are, so here goes:
    "min." refers to the minimum or lowest value among all the 

cases. for this dataframe, 5 is the age of the dog and it is the low-
est age of all of the family members.

    "1st qu." refers to the dividing line at the top of the    rst quartile. 
if we took all the cases and lined them up side by side in order 
of age (or weight) we could then divide up the whole into four 
groups, where each group had the same number of observations. 

1st 

quartile
25% of cases 

with the 
smallest 
values here

2nd 

3rd 

quartile
25% of cases 
just below 
the median 

quartile
25% of cases 
just above 
the mean 

here

here

4th 

quartile
25% of cases 

with the 
largest 
values here

just like a number line, the smallest cases would be on the left 
with the largest on the right. if we   re looking at myfamilyages, 
the leftmost group, which contains one quarter of all the cases, 
would start with    ve on the low end (the dog) and would have 
eight on the high end (bro). so the "   rst quartile" is the value of 
age (or another variable) that divides the    rst quarter of the 
cases from the other three quarters. note that if we don   t have a 
number of cases that divides evenly by four, the value is an ap-
proximation.

    median refers to the value of the case that splits the whole group 
in half, with half of the cases having higher values and half hav-
ing lower values. if you think about it a little bit, the median is 
also the dividing line that separates the second quartile from the 
third quartile. 

    mean, as we have learned before, is the numeric average of all of 
the values. for instance, the average age in the family is reported 
as 22.

    "3rd qu." is the third quartile. if you remember back to the    rst 
quartile and the median, this is the third and    nal dividing line 
that splits up all of the cases into four equal sized parts. you may 
be wondering about these quartiles and what they are useful for. 
statisticians like them because they give a quick sense of the 
shape of the distribution. everyone has the experience of sorting 
and dividing things up - pieces of pizza, playing cards into 
hands, a bunch of players into teams - and it is easy for most peo-
ple to visualize four equal sized groups and useful to know how 
high you need to go in age or weight (or another variable) to get 
to the next dividing line between the groups. 

    finally, "max" is the maximum value and as you might expect 
displays the highest value among all of the available cases. for 
example, in this dataframe dad has the highest weight: 188. 
seems like a pretty trim guy.

just one more topic to pack in before ending this chapter: how to 
access the stored variables in our new dataframe. r stores the data-
frame as a list of vectors and we can use the name of the dataframe 
together with the name of a vector to refer to each one using the "$" 
to connect the two labels like this:
> myfamily$myfamilyages
[1] 43 42 12  8  5
if you   re alert you might wonder why we went to the trouble of 
typing out that big long thing with the $ in the middle, when we 
could have just referred to "myfamilyages" as we did earlier when 
we were setting up the data. well, this is a very important point. 
when we created the myfamily dataframe, we copied all of the in-
formation from the individual vectors that we had before into a 
brand new storage space. so now that we have created the my-
family dataframe, myfamily$myfamilyages actually refers to a 
completely separate (but so far identical) vector of values. you can 
prove this to yourself very easily, and you should, by adding some 
data to the original vector, myfamilyages:
> myfamilyages <- c(myfamilyages, 11)
> myfamilyages
[1] 43 42 12  8  5 11
> myfamily$myfamilyages

33

[1] 43 42 12  8  5
look very closely at the    ve lines above. in the    rst line, we use 
the c() command to add the value 11 to the original list of ages that 
we had stored in myfamilyages (perhaps we have adopted an 
older cat into the family). in the second line we ask r to report 
what the vector myfamilyages now contains. dutifully, on the 
third line above, r reports that myfamilyages now contains the 
original    ve values and the new value of 11 on the end of the list. 
when we ask r to report myfamily$myfamilyages, however, we 
still have the original list of    ve values only. this shows that the da-
taframe and its component columns/vectors is now a completely 
independent piece of data. we must be very careful, if we estab-
lished a dataframe that we want to use for subsequent analysis, 
that we don   t make a mistake and keep using some of the original 
data from which we assembled the dataframe.
here   s a puzzle that follows on from this question. we have a nice 
dataframe with    ve observations and four variables. this is a rec-
tangular shaped data set, as we discussed at the beginning of the 
chapter. what if we tried to add on a new piece of data on the end 
of one of the variables? in other words, what if we tried something 
like this command:
myfamily$myfamilyages<-c(myfamily$myfamilyages, 11)
if this worked, we would have a pretty weird situation: the vari-
able in the dataframe that contained the family members    ages 
would all of a sudden have one more observation than the other 
variables: no more perfect rectangle! try it out and see what hap-
pens. the result helps to illuminate how r approaches situations 
like this. 

so what new skills and knowledge do we have at this point? here 
are a few of the key points from this chapter:
    in r, as in other programs, a vector is a list of elements/things 
that are all of the same kind, or what r refers to as a mode. for 
example, a vector of mode "numeric" would contain only num-
bers. 

    statisticians, database experts and others like to work with rec-
tangular datasets where the rows are cases or instances and the 
columns are variables or attributes. 

    in r, one of the typical ways of storing these rectangular struc-
tures is in an object known as a dataframe. technically speaking 
a dataframe is a list of vectors where each vector has the exact 
same number of elements as the others (making a nice rectan-
gle). 

    in r, the data.frame() function organizes a set of vectors into a 
dataframe. a dataframe is a conventional, rectangular shaped 
data object where each column is a vector of uniform mode and 
having the same number of elements as the other columns in the 
dataframe. data are copied from the original source vectors into 
new storage space. the variables/columns of the dataframe can 
be accessed using "$" to connect the name of the dataframe to the 
name of the variable/column.

    the str() and summary() functions can be used to reveal the 

structure and contents of a dataframe (as well as of other data ob-
jects stored by r). the str() function shows the structure of a data 
object, while summary() provides numerical summaries of nu-
meric variables and overviews of non-numeric variables.

34

    a factor is a labeling system often used to organize groups of 
cases or observations. in r, as well as in many other software 
programs, a factor is represented internally with a numeric id 
number, but factors also typically have labels like "male" and 
"female" or "experiment" and "control." factors always have 
review 5.1 rows and columns

question 1 of  7
what is the name of the data object that r uses to store a rec-
tangular dataset of cases and variables? 

a. a list
b. a mode
c. a vector
d. a dataframe

"levels," and these are the different groups that the factor signi-
   es. for example, if a factor variable called gender codes all 
cases as either "male" or "female" then that factor has exactly 
two levels. 

    quartiles are a division of a sorted vector into four evenly sized 
groups. the    rst quartile contains the lowest-valued elements, 
for example the lightest weights, whereas the fourth quartile con-
tains the highest-valued items. because there are four groups, 
there are three dividing lines that separate them. the middle di-
viding line that splits the vector exactly in half is the median. 
the term "   rst quartile" often refers to the dividing line to the 
left of the median that splits up the lower two quarters and the 
value of the    rst quartile is the value of the element of the vector 
that sits right at that dividing line. third quartile is the same 
idea, but to the right of the median and splitting up the two 
higher quarters.

    min and max are often used as abbreviations for minimum and 
maximum and these are the terms used for the highest and low-
est values in a vector. bonus: the "range" of a set of numbers is 
the maximum minus the minimum.

    the mean is the same thing that most people think of as the aver-

age. bonus: the mean and the median are both measures of 
what statisticians call "central tendency."

chapter challenge
create another variable containing information about family mem-
bers (for example, each family member   s estimated iq; you can 
make up the data). take that new variable and put it in the existing 

35

check answermyfamily dataframe. rerun the summary() function on myfamily 
to get descriptive information on your new variable.
sources
http://en.wikipedia.org/wiki/central_tendency 
http://en.wikipedia.org/wiki/median 
http://en.wikipedia.org/wiki/relational_model 
http://msenux.redwoods.edu/math/r/dataframe.php
http://stat.ethz.ch/r-manual/r-devel/library/base/html/data.fr
ame.html
http://www.burns-stat.com/pages/tutor/hints_r_begin.html 
http://www.khanacademy.org/math/statistics/v/mean-median-
and-mode 

!
!

!
!

concatenates data elements together
assignment arrow

r functions used in this chapter 
c()!
<-!
data.frame()! makes a dataframe from separate vectors
str()! !
!
summary()!

reports the structure of a data object
reports data modes/types and a data overview

36

chapter 6

beer, farms, and peas

many of the simplest and most practical methods for summarizing collections of numbers come to us 
from four guys who were born in the 1800s at the start of the industrial revolution. a considerable 
amount of the work they did was focused on solving real world problems in manufacturing and 
agriculture by using data to describe and draw id136s from what they observed.

37

the end of the 1800s and the early 1900s were a time of astonishing 
progress in mathematics and science. given enough time, paper, 
and pencils, scientists and mathematicians of that age imagined 
that just about any problem facing humankind -  including the limi-
tations of people themselves - could be measured, broken down, 
analyzed, and rebuilt to become more ef   cient. four englishmen 
who epitomized both this scienti   c progress and these idealistic be-
liefs were francis galton, karl pearson, william sealy gosset, and 
ronald fisher. 
first on the scene was francis galton, a half-cousin to the more 
widely known charles darwin, but quite the intellectual force him-
self. galton was an english gentleman of independent means who 
studied latin, greek, medicine, and mathematics, and who made a 
name for himself as an african explorer. he is most widely known 
as a proponent of "eugenics" and is credited with coining the term. 
eugenics is the idea that the human race could be improved 
through selective breeding. galton studied heredity in peas, rab-
bits, and people and concluded that certain people should be paid 
to get married and have children because their offspring would im-
prove the human race. these ideas were later horribly misused in 
the 20th century, most notably by the nazis as a justi   cation for kill-
ing people because they belonged to supposedly inferior races. set-
ting eugenics aside, however, galton made several notable and 
valuable contributions to mathematics and statistics, in particular 
illuminating two basic techniques that are widely used today: corre-
lation and regression. 
for all his studying and theorizing, galton was not an outstanding 
mathematician, but he had a junior partner, karl pearson, who is 
often credited with founding the    eld of mathematical statistics. 

pearson re   ned the math behind correlation and regression and 
did a lot else besides to contribute to our modern abilities to man-
age numbers. like galton, pearson was a proponent of eugenics, 
but he also is credited with inspiring some of einstein   s thoughts 
about relativity and was an early advocate of women   s rights.  
next to the statistical party was william sealy gosset, a wizard at 
both math and chemistry. it was probably the latter expertise that 
led the guinness brewery in dublin ireland to hire gosset after col-
lege. as a forward looking business, the guinness brewery was on 
the lookout for ways of making batches of beer more consistent in 
quality. gosset stepped in and developed what we now refer to as 
small sample statistical techniques - ways of generalizing from the 
results of a relatively few observations. of course, brewing a batch 
of beer is a time consuming and expensive process, so in order to 
draw conclusions from experimental methods applied to just a few 
batches, gosset had to    gure out the role of chance in determining 
how a batch of beer had turned out. guinness frowned upon aca-
demic publications, so gosset had to publish his results under the 
modest pseudonym, "student." if you ever hear someone discuss-
ing the "student   s t-test," that is where the name came from. 
last but not least among the born-in-the-1800s bunch was ronald 
fisher, another mathematician who also studied the natural sci-
ences, in his case biology and genetics. unlike galton, fisher was 
not a gentleman of independent means, in fact, during his early 
married life he and his wife struggled as subsistence farmers. one 
of fisher   s professional postings was to an agricultural research 
farm called rothhamsted experimental station. here, he had ac-
cess to data about variations in crop yield that led to his develop-
ment of an essential statistical technique known as the analysis of 

38

variance. fisher also pioneered the area of experimental design, 
which includes matters of factors, levels, experimental groups, and 
control groups that we noted in the previous chapter.
of course, these four are certainly not the only 19th and 20th cen-
tury mathematicians to have made substantial contributions to 
practical statistics, but they are notable with respect to the applica-
tions of mathematics and statistics to the other sciences (and "beer, 
farms, and peas" makes a good chapter title as well).
one of the critical distinctions woven throughout the work of these 
four is between the "sample" of data that you have available to ana-
lyze and the larger "population" of possible cases that may or do 
exist. when gosset ran batches of beer at the brewery, he knew 
that it was impractical to run every possible batch of beer with 
every possible variation in recipe and preparation. gosset knew 
that he had to run a few batches, describe what he had found and 
then generalize or infer what might happen in future batches. this 
is a fundamental aspect of working with all types and amounts of 
data: whatever data you have, there   s always more out there. 
there   s data that you might have collected by changing the way 
things are done or the way things are measured. there   s future 
data that hasn   t been collected yet and might never be collected. 
there   s even data that we might have gotten using the exact same 
strategies we did use, but that would have come out subtly differ-
ent just due to randomness. whatever data you have, it is just a 
snapshot or "sample" of what might be out there. this leads us to 
the conclusion that we can never, ever 100% trust the data we have. 
we must always hold back and keep in mind that there is always 
uncertainty in data. a lot of the power and goodness in statistics 
comes from the capabilities that people like fisher developed to 

help us characterize and quantify that uncertainty and for us to 
know when to guard against putting too much stock in what a sam-
ple of data have to say. so remember that while we can always de-
scribe the sample of data we have, the real trick is to infer what 
the data may mean when generalized to the larger population of 
data that we don   t have. this is the key distinction between de-
scriptive and inferential statistics. 
we have already encountered several descriptive statistics in previ-
ous chapters, but for the sake of practice here they are again, this 
time with the more detailed de   nitions:
    the mean (technically the arithmetic mean), a measure of central 
tendency that is calculated by adding together all of the observa-
tions and dividing by the number of observations.

    the median, another measure of central tendency, but one that 
cannot be directly calculated. instead, you make a sorted list of 
all of the observations in the sample, then go halfway up that 
list. whatever the value of the observation is at the halfway 
point, that is the median.

    the range, which is a measure of "dispersion" - how spread out a 
bunch of numbers in a sample are - calculated by subtracting the 
lowest value from the highest value.

to this list we should add three more that you will run into in a va-
riety of situations:
    the mode, another measure of central tendency. the mode is the 

value that occurs most often in a sample of data. like the me-
dian, the mode cannot be directly calculated. you just have to 

39

count up how many of each number there are and then pick the 
category that has the most. 

    the variance, a measure of dispersion. like the range, the vari-
ance describes how spread out a sample of numbers is. unlike 
the range, though, which just uses two numbers to calculate dis-
persion, the variance is obtained from all of the numbers 
through a simple calculation that compares each number to the 
mean. if you remember the ages of the family members from the 
previous chapter and the mean age of 22, you will be able to 
make sense out of the following table:

who

age

dad
mom
sis
bro
dog

43
42
12
8
5

age - 
mean
43-22 = 21
42-22=20
12-22=-10
8-22=-14
5-22=-17
total:
total/4:

(age-
mean)2
21*21=441
20*20=400
-10*-10=100
-14*-14=196
-17*-17=289

1426
356.5

this table shows the calculation of the variance, which begins by 
obtaining the "deviations" from the mean and then "squares" them 
(multiply each times itself) to take care of the negative deviations 
(for example, -14 from the mean for bro). we add up all of the 
squared deviations and then divide by the number of observations 
to get a kind of "average squared deviation." note that it was not a 

40

mistake to divide by 4 instead of 5 - the reasons for this will be-
come clear later in the book when we examine the concept of de-
grees of freedom. this result is the variance, a very useful mathe-
matical concept that appears all over the place in statistics. while it 
is mathematically useful, it is not too nice too look at. for instance, 
in this example we are looking at the 356.5 squared-years of devia-
tion from the mean. who measures anything in squared years? 
squared feet maybe, but that   s a different discussion. so, to address 
this weirdness, statisticians have also provided us with:
    the standard deviation, another measure of dispersion, and a 
cousin to the variance. the standard deviation is simply the 
square root of the variance, which puts us back in regular units 
like "years." in the example above, the standard deviation would 
be about 18.88 years (rounding to two decimal places, which is 
plenty in this case).

now let   s have r calculate some statistics for us:
> var(myfamily$myfamilyages)
[1] 356.5 
> sd(myfamily$myfamilyages)
[1] 18.88121
note that these commands carry on using the data used in the pre-
vious chapter, including the use of the $ to address variables 
within a dataframe. if you do not have the data from the previous 
chapter you can also do this:
> var(c(43,42,12,8,5))
[1] 356.5

> sd(c(43,42,12,8,5))
[1] 18.88121 
this is a pretty boring example, though, and not very useful for the 
rest of the chapter, so here   s the next step up in looking at data. we 
will use the windows or mac clipboard to cut and paste a larger 
data set into r. go to the u.s. census website where they have 
stored population data:
http://www.census.gov/popest/data/national/totals/2011/inde
x.html
assuming you have a spreadsheet program available, click on the 
xls link for "annual estimates of the resident population for the 
united states." when the spreadsheet is open, select the population 
estimates for the    fty states. the    rst few looked like this in the 
2011 data:

.alabama
.alaska
.arizona
.arkansas

4,779,736
710,231
6,392,017
2,915,918

to make use of the next r command, make sure to choose just the 
numbers and not the text. before you copy the numbers, take out 
the commas by switching the cell type to "general." this can usu-
ally be accomplished under the format menu, but you might also 
have a toolbar button to do the job. copy the numbers to the clip-
board with ctrl+c (windows) or command+c (mac). on a win-
dows machine use the following command:

41

read.dif("clipboard",transpose=true)
on a mac, this command does the same thing:
read.table(pipe("pbpaste"))
it is very annoying that there are two different commands for the 
two types of computers, but this is an inevitable side effect of the 
different ways that the designers at microsoft and apple set up the 
clipboard, plus the fact that r was designed to work across many 
platforms. anyway, you should have found that the long string of 
population numbers appeared on the r output. the numbers are 
not much use to us just streamed to the output, so let   s assign the 
numbers to a new vector.
windows, using read.dif:
> usstatepops <- + 
read.dif("clipboard",transpose=true)
> usstatepops
           v1
1   4779736
2    710231
3   6392017
...
or mac, using read.table:
> usstatepops <- read.table(pipe("pbpaste"))
> usstatepops

           v1
1   4779736
2    710231
3   6392017
...
only the    rst three observations are shown in order to save space 
on this page. your output r should show the whole list. note that 
the only thing new here over and above what we have done with r 
in previous chapters is the use of the read.dif() or read.table() func-
tions to get a bigger set of data that we don   t have to type our-
selves. functions like read.table() are quite important as we move 
forward with using r because they provide the usual way of get-
ting data stored in external    les into r   s storage areas for use in 
data analysis. if you had trouble getting this to work, you can cut 
and paste the commands at the end of the chapter under "if all 
else fails" to get the same data going in your copy of r.
note that we have used the left pointing assignment arrow ("<-") to 
take the results of the read.dif() or read.table() function and place 
it in a data object. this would be a great moment to practice your 
skills from the previous chapter by using the str() and summary() 
functions on our new data object called usstatepops. did you no-
tice anything interesting from the results of these functions? one 
thing you might have noticed is that there are 51 observations in-
stead of 50. can you guess why? if not, go back and look at your 
original data from the spreadsheet or the u.s. census site. the 
other thing you may have noticed is that usstatepops is a data-
frame, and not a plain vector of numbers. you can actually see this 

in the output above: in the second command line where we request 
that r reveal what is stored in usstatepops, it responds with a col-
umn topped by the designation "v1". because we did not give r 
any information about the numbers it read in from the clipboard, it 
called them "v1", short for variable one, by default. so anytime we 
want to refer to our list of population numbers we actually have to 
use the name usstatepops$v1. if this sounds unfamiliar, take an-
other look at the previous "rows and columns" chapter for more 
information on addressing the columns in a dataframe. 
now we   re ready to have some fun with a good sized list of num-
bers. here are the basic descriptive statistics on the population of 
the states:
> mean(usstatepops$v1)
[1] 6053834
> median(usstatepops$v1)
[1] 4339367
> mode(usstatepops$v1)
[1] "numeric"
> var(usstatepops$v1)
[1] 4.656676e+13
> sd(usstatepops$v1)
[1] 6823984
some great summary information there, but wait, a couple things 
have gone awry:

42

    the mode() function has returned the data type of our vector of 
numbers instead of the statistical mode. this is weird but true: 
the basic r package does not have a statistical mode function! 
this is partly due to the fact that the mode is only useful in a 
very limited set of situations, but we will    nd out in later chap-
ters how add-on packages can be used to get new functions in r 
including one that calculates the statistical mode.

    the variance is reported as 4.656676e+13. this is the    rst time 

that we have seen the use of scienti   c notation in r. if you 
haven   t seen this notation before, the way you interpret it is to 
imagine 4.656676 multiplied by 10,000,000,000,000 (also known 
as 10 raised to the 13th power). you can see that this is ten tril-
lion, a huge and unwieldy number, and that is why scienti   c no-
tation is used. if you would prefer not to type all of that into a 
calculator, another trick to see what number you are dealing 
with is just to move the decimal point 13 digits to the right.

other than these two issues, we now know that the average popula-
tion of a u.s. state is 6,053,834 with a standard deviation of 
6,823,984. you may be wondering, though, what does it mean to 
have a standard deviation of almost seven million?  the mean and 
standard deviation are ok, and they certainly are mighty precise, 
but for most of us, it would make much more sense to have a pic-
ture that shows the central tendency and the dispersion of a large 
set of numbers. so here we go. run this command:
hist(usstatepops$v1)
here   s the output you should get: 

histogram of usstatepops$v1

0
3

5
2

0
2

5
1

0
1

5

0

y
c
n
e
u
q
e
r
f

0e+00

1e+07

2e+07

3e+07

4e+07

usstatepops$v1

a histogram is a specialized type of bar graph designed to show 
"frequencies." frequencies means how often a particular value or 
range of values occurs in a dataset. this histogram shows a very 
interesting picture. there are nearly 30 states with populations un-
der    ve million, another 10 states with populations under 10 mil-
lion, and then a very small number of states with populations 
greater than 10 million. having said all that, how do we glean this 
kind of information from the graph? first, look along the y-axis 
(the vertical axis on the left) for an indication of how often the data 

43

occur. the tallest bar is just to the right of this and it is nearly up to 
the 30 mark. to know what this tall bar represents, look along the 
x-axis (the horizontal axis at the bottom) and see that there is a tick 
mark for every two bars. we see scienti   c notation under each tick 
mark. the    rst tick mark is 1e+07, which translates to 10,000,000. 
so each new bar (or an empty space where a bar would go) goes 
up by    ve million in population. with these points in mind it 
should now be easy to see that there are nearly 30 states with popu-
lations under    ve million.
if you think about presidential elections, or the locations of schools 
and businesses, or how a single u.s. state might compare with 
other countries in the world, it is interesting to know that there are 
two really giant states and then lots of much smaller states. once 
you have some practice reading histograms, all of the knowledge is 
available at a glance. 
on the other hand there is something unsatisfying about this dia-
gram. with over forty of the states clustered into the    rst couple of 
bars, there might be some more details hiding in there that we 
would like to know about. this concern translates into the number 
of bars shown in the histogram. there are eight shown here, so 
why did r pick eight? 
the answer is that the hist() function has an algorithm or recipe for 
deciding on the number of categories/bars to use by default. the 
number of observations and the spread of the data and the amount 
of empty space there would be are all taken into account. fortu-
nately it is possible and easy to ask r to use more or fewer 
categories/bars with the "breaks" parameter, like this:
hist(usstatepops$v1, breaks=20)

histogram of usstatepops$v1

y
c
n
e
u
q
e
r
f

5
1

0
1

5

0

0e+00

1e+07

2e+07

3e+07

usstatepops$v1

this gives us    ve bars per tick mark or about two million for each 
bar. so the new histogram above shows very much the same pat-
tern as before: 15 states with populations under two million. the 
pattern that you see here is referred to as a distribution. this is a 
distribution that starts off tall on the left and swoops downward 
quickly as it moves to the right. you might call this a "reverse-j" dis-
tribution because it looks a little like the shape a j makes, although 
   ipped around vertically. more technically this could be referred to 
as a pareto distribution (named after the economist vilfredo pa-

44

reto). we don   t have to worry about why it may be a pareto distri-
bution at this stage, but we can speculate on why the distribution 
looks the way it does. first, you can   t have a state with no people 
in it, or worse yet negative population. it just doesn   t make any 
sense. so a state has to have at least a few people in it, and if you 
look through u.s. history every state began as a colony or a terri-
tory that had at least a few people in it. on the other hand, what 
does it take to grow really large in population? you need a lot of 
land,    rst of all, and then a good reason for lots of people to move 
there or lots of people to be born there. so there are lots of limits to 
growth: rhode island is too small too have a bazillion people in it 
and alaska, although it has tons of land, is too cold for lots of peo-
ple to want to move there. so all states probably started small and 
grew, but it is really dif   cult to grow really huge. as a result we 
have a distribution where most of the cases are clustered near the 
bottom of the scale and just a few push up higher and higher. but 
as you go higher, there are fewer and fewer states that can get that 
big, and by the time you are out at the end, just shy of 40 million 
people, there   s only one state that has managed to get that big. by 
the way, do you know or can you guess what that humongous 
state is? 
there are lots of other distribution shapes. the most common one 
that almost everyone has heard of is sometimes called the "bell" 
curve because it is shaped like a bell. the technical name for this is 
the normal distribution. the term "normal" was    rst introduced by 
carl friedrich gauss (1777-1855), who supposedly called it that in 
a belief that it was the most typical distribution of data that one 
might    nd in natural phenomena. the following histogram depicts 
the typical bell shape of the normal distribution. 

histogram of rnorm(51, 6053834, 6823984)

y
c
n
e
u
q
e
r
f

5
1

0
1

5

0

-1e+07

0e+00

1e+07

2e+07

3e+07

rnorm(51, 6053834, 6823984)

if you are curious, you might be wondering how r generated the 
histogram above, and, if you are alert, you might notice that the his-
togram that appears above has the word "rnorm" in a couple of 
places. here   s another of the cool features in r: it is incredibly easy 
to generate "fake" data to work with when solving problems or giv-

45

ing demonstrations. the data in this histogram were generated by 
r   s rnorm() function, which generates a random data set that    ts 
the normal distribution (more closely if you generate a lot of data, 
less closely if you only have a little. some further explanation of 
the rnorm() command will make sense if you remember that the 
state population data we were using had a mean of 6,053,834 and a 
standard deviation of 6,823,984. the command used to generate 
this histogram was:
hist(rnorm(51, 6043834, 6823984))
there are two very important new concepts introduced here. the 
   rst is a nested function call: the hist() function that generates the 
graph "surrounds" the rnorm() function that generates the new 
fake data. (pay close attention to the parentheses!) the inside func-
tion, rnorm(), is run by r    rst, with the results of that sent directly 
and immediately into the hist() function.
the other important thing is the "arguments that" were "passed" to 
the rnorm() function. we actually already ran into arguments a lit-
tle while ago with the read.dif() and read.table() functions but we 
did not talk about them then. "argument" is a term used by com-
puter scientists to refer to some extra information that is sent to a 
function to help it know how to do its job. in this case we passed 
three arguments to rnorm() that it was expecting in this order: the 
number of observations to generate in the fake dataset, the mean of 
the distribution, and the standard deviation of the distribution. 
the rnorm() function used these three numbers to generate 51 ran-
dom data points that, roughly speaking,    t the normal distribu-
tion. so the data shown in the histogram above are an approxima-
tion of what the distribution of state populations might look like if, 

instead of being reverse-j-shaped (pareto distribution), they were 
normally distributed.
the normal distribution is used extensively through applied statis-
tics as a tool for making comparisons. for example, look at the 
rightmost bar in the previous histogram. the label just to the right 
of that bar is 3e+07, or 30,000,000. we already know from our real 
state population data that there is only one actual state with a 
population in excess of 30 million (if you didn   t look it up, it is cali-
fornia). so if all of a sudden, someone mentioned to you that he or 
she lived in a state, other than california, that had 30 million peo-
ple, you would automatically think to yourself, "wow, that   s un-
usual and i   m not sure i believe it." and the reason that you found 
it hard to believe was that you had a distribution to compare it to. 
not only did that distribution have a characteristic shape (for exam-
ple, j-shaped, or bell shaped, or some other shape), it also had a 
center point, which was the mean, and a "spread," which in this 
case was the standard deviation.  armed with those three pieces of 
information - the type/shape of distribution, an anchoring point, 
and a spread (also known as the amount of variability), you have a 
powerful tool for making comparisons. 
in the next chapter we will conduct some of these comparisons to 
see what we can infer about the ways things are in general, based 
on just a subset of available data, or what statisticians call a sam-
ple.
chapter challenge
in this chapter, we used rnorm() to generate random numbers that 
closely    t a normal distribution. we also learned that the state 
population data was a "pareto" distribution. do some research to 

46

review 6.1 beer, farms, and peas

a bar graph that displays the frequencies of occurrence for a 
numeric variable is called a

a. histogram
b. pictogram
c. bar graph
d. bar chart

   nd out what r function generates random numbers using the pa-
reto distribution. then run that function with the correct parame-
ters to generate 51 random numbers (hint: experiment with differ-
ent id203 values). create a histogram of these random num-
bers and describe the shape of the distribution.

sources
http://en.wikipedia.org/wiki/carl_friedrich_gauss 
http://en.wikipedia.org/wiki/francis_galton
http://en.wikipedia.org/wiki/pareto_distribution 
http://en.wikipedia.org/wiki/karl_pearson 
http://en.wikipedia.org/wiki/ronald_fisher 
http://en.wikipedia.org/wiki/william_sealy_gosset 
http://en.wikipedia.org/wiki/normal_distribution 
http://stat.ethz.ch/r-manual/r-devel/library/utils/html/read.t
able.html 
http://www.census.gov/popest/data/national/totals/2011/inde
x.html 
http://www.r-tutor.com/elementary-statistics/numerical-measur
es/standard-deviation 

47

check answerv1 <- c(4779736,710231,6392017,2915918,37253956,    
5029196,3574097,897934,601723,18801310,9687653,    
1360301,1567582,12830632,6483802,3046355,2853118,
4339367,4533372,1328361,5773552,6547629,9883640,    
5303925,2967297,5988927,989415,1826341,2700551,    
1316470,8791894,2059179,19378102,9535483,672591,    
11536504,3751351,3831074,12702379,1052567,    
4625364,814180,6346105,25145561,2763885,625741,    
8001024,6724540,1852994,5686986,563626)
usstatepops <- data.frame(v1)

r functions used in this chapter 
read.dif()!!
read.table()!
!
mean()!
!
median()!
mode()!
!
!
!
!
var()!!
!
!
sd()! !
hist()!!
!
test yourself

reads data in interchange format
reads data table from external source
calculate arithmetic mean
locate the median
tells the data type/mode of a data object   
note: this is not the statistical mode
calculate the sample variance
calculate the sample standard deviation
produces a histogram graphic

if all else fails
in case you have dif   culty with the read.dif() or read.table() func-
tions, the code shown below can be copied and pasted (or, in the 
worst case scenario, typed) into the r console to create the data set 
used in this chapter.

48

chapter 7

sample in a jar

sampling distributions are the conceptual key to statistical id136. many approaches to 
understanding sampling distributions use examples of drawing marbles or gumballs from a large jar 
to illustrate the in   uences of randomness on sampling. using the list of u.s. states illustrates how a 
non-normal distribution nonetheless has a normal sampling distribution of means.

49

imagine a gum ball jar full of gumballs of two different colors, red 
and blue. the jar was    lled from a source that provided 100 red 
gum balls and 100 blue gum balls, but when these were poured 
into the jar they got all mixed up. if you drew eight gumballs from 
the jar at random, what colors would you get? if things worked out 
perfectly, which they never do, you would get four red and four 
blue. this is half and half, the same ratio of red and blue that is in 
the jar as a whole. of course, it rarely works out this way, does it? 
instead of getting four red and four blue you might get three red 
and    ve blue or any other mix you can think of. in fact, it would be 
possible, though perhaps not likely, to get eight red gumballs. the 
basic situation, though, is that we really don   t know what mix of 
red and blue we will get with one draw of eight gumballs. that   s 
uncertainty for you, the forces of randomness affecting our sample 
of eight gumballs in unpredictable ways.
here   s an interesting idea, though, that is no help at all in predict-
ing what will happen in any one sample, but is great at showing 
what will occur in the long run. pull eight gumballs from the jar, 
count the number of red ones and then throw them back. we do 
not have to count the number of blue because 8 - #red = #blue. mix 
up the jar again and then draw eight more gumballs and count the 
number of red. keeping doing this many times. here   s an example 
of what you might get:

draw

# red

1
2
3
4

5
3
6
2

50

notice that the left column is just counting up the number of sam-
ple draws we have done. the right column is the interesting one 
because it is the count of the number of red gumballs in each par-
ticular sample draw. in this example, things are all over the place. 
in sample draw 4 we only have two red gumballs, but in sample 
draw 3 we have 6 red gumballs. but the most interesting part of 
this example is that if you average the number of red gumballs over 
all of the draws, the average comes out to exactly four red gumballs 
per draw, which is what we would expect in a jar that is half and 
half. now this is a contrived example and we won   t always get 
such a perfect result so quickly, but if you did four thousand draws 
instead of four, you would get pretty close to the perfect result.
this process of repeatedly drawing a subset from a "population"  is 
called "sampling," and the end result of doing lots of sampling is a 
sampling distribution. note that we are using the word population 
in the previous sentence in its statistical sense to refer to the total-
ity of units from which a sample can be drawn. it is just a coinci-
dence that our dataset contains the number of people in each state 
and that this value is also referred to as "population." next we will 
get r to help us draw lots of samples from our u.s. state dataset. 
conveniently, r has a function called sample(), that will draw a ran-
dom sample from a data set with just a single call. we can try it 
now with our state data:
> sample(usstatepops$v1,size=16,replace=true)
[1] 4533372 19378102 897934 1052567 672591 
18801310  2967297
[8]  5029196

as a matter of practice, note that we called the sample() function 
with three arguments. the    rst argument was the data source. for 
the second and third arguments, rather than rely on the order in 
which we specify the arguments, we have used "named argu-
ments" to make sure that r does what we wanted. the size=16 ar-
gument asks r to draw a sample of 16 state data values. the repla-
ce=true argument speci   es a style of sampling which statisticians 
use very often to simplify the mathematics of their proofs. for us, 
sampling with or without replacement does not usually have any 
practical effects, so we will just go with what the statisticians typi-
cally do.
when we   re working with numbers such as these state values, in-
stead of counting gumball colors, we   re more interested in    nding 
out the average, or what you now know as the mean. so we could 
also ask r to calculate a mean() of the sample for us:
> mean(sample(usstatepops$v1,size=16, +   
replace=true))
[1] 8198359
there   s the nested function call again. the output no longer shows 
the 16 values that r sampled from the list of 51. instead it used 
those 16 values to calculate the mean and display that for us. if you 
have a good memory, or merely took the time to look in the last 
chapter, you will remember that the actual mean of our 51 observa-
tions is 6,053,834. so the mean that we got from this one sample of 
16 states is really not even close to the true mean value of our 51 
observations. are we worried? de   nitely not! we know that when 
we draw a sample, whether it is gumballs or states, we will never 
hit the true population mean right on the head. we   re interested 

not in any one sample, but in what happens over the long haul. so 
now we   ve got to get r to repeat this process for us, not once, not 
four times, but four hundred times or four thousand times. like 
most programming languages, r has a variety of ways of repeating 
an activity. one of the easiest ones to use is the replicate() function. 
to start, let   s just try four replications:
> replicate(4, mean(sample(usstatepops$v1,+ 
size=16,replace=true)),simplify=true)
[1] 10300486 11909337  8536523  5798488
couldn   t be any easier. we took the exact same command as be-
fore, which was a nested function to calculate the mean() of a ran-
dom sample of 16 states (shown above in bold). this time, we put 
that command inside the replicate() function so we could run it 
over and over again. the simplify=true argument asks r to re-
turn the results as a simple vector of means, perfect for what we 
are trying to do. we only ran it four times, so that we would not 
have a big screen full of numbers. from here, though, it is easy to 
ramp up to repeating the process four hundred times. you can try 
that and see the output, but for here in the book we will encapsu-
late the whole replicate function inside another mean(), so that we 
can get the average of all 400 of the sample means. here we go:
> mean(replicate(400, mean( +    
sample(usstatepops$v1,size=16,replace=true)),+   
simplify=true))
[1] 5958336
in the command above, the outermost mean() command is bolded 
to show what is different from the previous command. so, put into 

51

histogram of replicate(4000, mean(sample(usstatepops$v1, size = 16, replace = true)), simplify = true)

words, this deeply nested command accomplishes the following: a)  
draw 400 samples of size n=8 from our full data set of 51 states; b) 
calculate the mean from each sample and keep it in a list; c) when 
   nished with the list of 400 of these means, calculate the mean of 
that list of means. you can see that the mean of four hundred sam-
ple means is 5,958,336. now that is still not the exact value of the 
whole data set, but it is getting close. we   re off by about 95,000, 
which is roughly an error of about 1.6% (more precisely, 95,498/
6,053,834 = 1.58%. you may have also noticed that it took a little 
while to run that command, even if you have a fast computer. 
there   s a lot of work going on there! let   s push it a bit further and 
see if we can get closer to the true mean for all of our data:
> mean(replicate(4000, mean( +   
sample(usstatepops$v1,size=16,replace=true)),+   
simplify=true))
[1] 6000972
now we are even closer! we are now less than 1% away from the 
true population mean value. note that the results you get may be a 
bit different, because when you run the commands, each of the 400 
or 4000 samples that is drawn will be slightly different than the 
ones that were drawn for the commands above. what will not be 
much different is the overall level of accuracy. 
we   re ready to take the next step. instead of summarizing our 
whole sampling distribution in a single average, let   s look at the 
distribution of means using a histogram. 
the histogram displays the complete list of 4000 means as frequen-
cies. take a close look so that you can get more practice reading fre-
quency histograms. this one shows a very typical con   guration 

that is almost bell-shaped, but still has a bit of "skewness" off to the 
right. the tallest, and therefore most frequent range of values is 
right near the true mean of 6,053,834.

0
0
0
1

0
0
8

0
0
6

0
0
4

0
0
2

0

y
c
n
e
u
q
e
r
f

2.0e+06

4.0e+06

6.0e+06

8.0e+06

1.0e+07

1.2e+07

1.4e+07

replicate(4000, mean(sample(usstatepops$v1, size = 16, replace = true)), simplify = true)

by the way, were you able to    gure out the command to generate 
this histogram on your own? all you had to do was substitute 
hist() for the outermost mean() in the previous command. in case 
you struggled, here it is:

52

hist(replicate(4000, mean( +   
sample(usstatepops$v1,size=16,replace=true)), +   
simplify=true))
this is a great moment to take a deep breath. we   ve just covered a 
couple hundred years of statistical thinking in just a few pages. in 
fact, there are two big ideas, "the law of large numbers" and    
the central limit theorem" that we have just partially demonstrated. 
these two ideas literally took mathematicians like gerolamo car-
dano (1501-1576) and jacob bernoulli (1654-1705) several centuries 
to    gure out. if you look these ideas up, you may    nd a lot of be-
wildering mathematical details, but for our purposes, there are two 
really important take-away messages. first, if you run a statistical 
process a large number of times, it will converge on a stable result. 
for us, we knew what the average population was of the 50 states 
plus the district of columbia. these 51 observations were our 
population, and we wanted to know how many smaller subsets, or 
samples, of size n=16 we would have to draw before we could get 
a good approximation of that true value. we learned that drawing 
one sample provided a poor result. drawing 400 samples gave us a 
mean that was off by 1.5%. drawing 4000 samples gave us a mean 
that was off by less than 1%. if we had kept going to 40,000 or 
400,000 repetitions of our sampling process, we would have come 
extremely close to the actual average of 6,053,384. 
second, when we are looking at sample means, and we take the 
law of large numbers into account, we    nd that the distribution of 
sampling means starts to create a bell-shaped or normal distribu-
tion, and the center of that distribution, the mean of all of those 
sample means gets really close to the actual population mean. it 
gets closer faster for larger samples, and in contrast, for smaller 

samples you have to draw lots and lots of them to get really close. 
just for fun, lets illustrate this with a sample size that is larger than 
16. here   s a run that only repeats 100 times, but each time draws a 
sample of n=51 (equal in size to the population):
> mean(replicate(100, mean( +    
sample(usstatepops$v1,size=51,replace=true)),+   
simplify=true))
[1] 6114231
now, we   re only off from the true value of the population mean by 
about one tenth of one percent. you might be scratching your head 
now, saying, "wait a minute, isn   t a sample of 51 the same thing as 
the whole list of 51 observations?" this is confusing, but it goes 
back to the question of sampling with replacement that we exam-
ined a couple of pages ago (and that appears in the command 
above as replace=true). sampling with replacement means that 
as you draw out one value to include in your random sample, you 
immediately chuck it back into the list so that, potentially, it could 
get drawn again either immediately or later. as mentioned before, 
this practice simpli   es the underlying proofs, and it does not cause 
any practical problems, other than head scratching. in fact, we 
could go even higher in our sample size with no trouble:
> mean(replicate(100, mean( +   
sample(usstatepops$v1,size=120,replace=true)), +   
simplify=true))
[1] 6054718
that command runs 100 replications using samples of size n=120. 
look how close the mean of the sampling distribution is to the 

53

population mean now! remember that this result will change a lit-
tle bit every time you run the procedure, because different random 
samples are being drawn for each run. but the rule of thumb is that 
the bigger your sample size, what statisticians call n, the closer 
your estimate will be to the true value. likewise, the more trials 
you run, the closer your population estimate will be. 
so, if you   ve had a chance to catch your breath, let   s move on to 
making use of the sampling distribution. first, let   s save one distri-
bution of sample means so that we have a    xed set of numbers to 
work with:
samplemeans <- replicate(10000, mean(sample(us-
statepops$v1,size=5,+   
replace=true)),simplify=true)
the bolded part is new. we   re saving a distribution of sample 
means to a new vector called "samplemeans". we should have 
10,000 of them:
> length(samplemeans)
[1] 10000
and the mean of all of these means should be pretty close to our 
population mean of 6,053,384:
> mean(samplemeans)
[1] 6065380
you might also want to run a histogram on samplemeans and see 
what the frequency distribution looks like. right now, all we need 
to look at is a summary of the list of sample means:

> summary(samplemeans)
min.  1st qu.   median     mean  3rd qu.     max. 
799100  3853000  5370000 6065000 7622000 25030000 
if you need a refresher on the median and quartiles, take a look 
back at chapter 3 - rows and columns. 
this summary is full of useful information. first, take a look at the 
max and the min. the minimum sample mean in the list was 
799,100. think about that for a moment. how could a sample have 
a mean that small when we know that the true mean is much 
higher? rhode island must have been drawn several times in that 
sample! the answer comes from the randomness involved in sam-
pling. if you run a process 10,000 times you are de   nitely going to 
end up with a few weird examples. its almost like buying a lottery 
ticket. the vast majority of tickets are the usual - not a winner. 
once in a great while, though, there is a very unusual ticket - a win-
ner. sampling is the same: the extreme events are unusual, but 
they do happen if you run the process enough times. the same 
goes for the maximum: at 25,030,000 the maximum sample mean is 
much higher than the true mean. 
at 5,370,000 the median is quite close to the mean, but not exactly 
the same because we still have a little bit of rightward skew (the 
"tail" on the high side is slightly longer than it should be because of 
the reverse j-shape of the original distribution). the median is very 
useful because it divides the sample exactly in half: 50%, or exactly 
5000 of the sample means are larger than 5,370,000 and the other 
50% are lower. so, if we were to draw one more sample from the 
population it would have a    fty-   fty chance of being above the me-
dian. the quartiles help us to cut things up even more    nely. the 

54

third quartile divides up the bottom 75% from the top 25%. so only 
25% of the sample means are higher than 7,622,000. that means if 
we drew a new sample from the population that there is only a 
25% chance that it will be larger than that. likewise, in the other 
direction, the    rst quartile tells us that there is only a 25% chance 
that a new sample would be less than 3,853,000. 
there is a slightly different way of getting the same information 
from r that will prove more    exible for us in the long run. the 
quantile() function can show us the same information as the me-
dian and the quartiles, like this:
 > quantile(samplemeans, probs=c(0.25,0.50,0.75))
    25%     50%     75% 
3853167 5370314 7621871
you will notice that the values are just slightly different, by less 
than one tenth of one percent, than those produced by the sum-
mary() function. these are actually more precise, although the less 
precise ones from summary() are    ne for most purposes. one rea-
son to use quantile() is that it lets us control exactly where we 
make the cuts. to get quartiles, we cut at 25% (0.25 in the com-
mand just above), at 50%, and at 75%. but what if we wanted in-
stead to cut at 2.5% and 97.5%? easy to do with quantile():
> quantile(samplemeans, probs=c(0.025,0.975))
     2.5%    97.5% 
 2014580 13537085 
so this result shows that, if we drew a new sample, there is only a 
2.5% chance that the mean would be lower than 2,014,580. like-

wise, there is only a 2.5% chance that the new sample mean would 
be higher than 13,537,085 (because 97.5% of the means in the sam-
pling distribution are lower than that value).   
now let   s put this knowledge to work. here is a sample of the num-
ber of people in a certain area, where each of these areas is some 
kind of a unit associated with the u.s.:
3,706,690   
159,358   
106,405   
55,519   
53,883
we can easily get these into r and calculate the sample mean:
> mysterysample <- c(3706690, 159358, 106405, +   
   55519, 53883)
> mean(mysterysample)
[1] 816371
the mean of our mystery sample is 816,371. the question is, is this 
a sample of u.s. states or is it something else? just on its own it 
would be hard to tell. the    rst observation in our sample has more 
people in it than kansas, utah, nebraska, and several other states. 
we also know from looking at the distribution of raw population 
data from our previous example that there are many, many states 
that are quite small in the number of people. thanks to the work 
we   ve done earlier in this chapter, however, we have an excellent 
basis for comparison. we have the sampling distribution of means, 
and it is fair to say that if we get a new mean to look at, and the 
new mean is way out in the extreme areas of the sample distribu-

55

tion, say, below the 2.5% mark or above the 97.5% mark, then it 
seems much less likely that our mysterysample is a sample of 
states. 
in this case, we can see quite clearly that 816,371 is on the extreme 
low end of the sampling distribution. recall that when we ran the 
quantile() command we found that only 2.5% of the sample means 
in the distribution were smaller than 2,014,580. 
in fact, we could even play around with a more stringent criterion:
> quantile(samplemeans, probs=c(0.005,0.995))
     0.5%    99.5% 
 1410883 16792211
this quantile() command shows that only 0.5% of all the sample 
means are lower than 1,410,883. so our mysterysample mean of 
816,371 would de   nitely be a very rare event, if it were truly a sam-
ple of states. from this we can infer, tentatively but based on good 
statistical evidence, that our mysterysample is not a sample of 
states. the mean of mysterysample is just too small to be very 
likely to be a sample of states.
and this is in fact correct: mysterysample contains the number of 
people in    ve different u.s. territories, including puerto rico in the 
caribbean and guam in the paci   c. these territories are land 
masses and groups of people associated with the u.s., but they are 
not states and they are different in many ways than states. for one 
thing they are all islands, so they are limited in land mass. among 
the u.s. states, only hawaii is an island, and it is actually bigger 
than 10 of the states in the continental u.s. the important thing to 
take away is that the characteristics of this group of data points, no-

tably the mean of this sample, was suf   ciently different from a 
known distribution of means that we could make an id136 that 
the sample was not drawn from the original population of data.
this reasoning is the basis for virtually all statistical id136. you 
construct a comparison distribution, you mark off a zone of ex-
treme values, and you compare any new sample of data you get to 
the distribution to see if it falls in the extreme zone. if it does, you 
tentatively conclude that the new sample was obtained from some 
other source than what you used to create the comparison distribu-
tion. 
if you feel a bit confused, take heart. there   s 400-500 years of 
mathematical developments represented in that one preceding 
paragraph. also, before we had cool programs like r that could be 
used to create and analyze actual sample distributions, most of the 
material above was taught as a set of formulas and proofs. yuck! 
later in the book we will come back to speci   c statistical proce-
dures that use the reasoning described above. for now, we just 
need to take note of three additional pieces of information.
first, we looked at the mean of the sampling distribution with 
mean() and we looked at its shaped with hist(), but we never quan-
ti   ed the spread of the distribution:
> sd(samplemeans)
[1] 3037318
this shows us the standard deviation of the distribution of sam-
pling means. statisticians call this the "standard error of the mean." 
this chewy phrase would have been clearer, although longer, if it 
had been something like this: "the standard deviation of the distri-

56

bution of sample means for samples drawn from a population." un-
fortunately, statisticians are not known for giving things clear la-
bels. suf   ce to say that when we are looking at a distribution and 
each data point in that distribution is itself a representation of a 
sample (for example, a mean), then the standard deviation is re-
ferred to as the standard error. 
second, there is a shortcut to    nding out the standard error that 
does not require actually constructing an empirical distribution of 
10,000 (or any other number) of sampling means. it turns out that 
the standard deviation of the original raw data and the standard 
error are closely related by a simple bit of algebra:
> sd(usstatepops$v1)/sqrt(5)
[1] 3051779
the formula in this command takes the standard deviation of the 
original state data and divides it by the square root of the sample 
size. remember three of four pages ago when we created the sam-
plemeans vector by using the replicate() and sample() commands, 
that we used a sample size of n=5. that   s what you see in the for-
mula above, inside of the sqrt() function. in r, and other software 
sqrt() is the abbreviation for "square root" and not for "squirt" as 
you might expect. so if you have a set of observations and you cal-
culate their standard deviation, you can also calculate the standard 
error for a distribution of means (each of which has the same sam-
ple size), just by dividing by the square root of the sample size. you 
may notice that the number we got with the shortcut was slightly 
larger than the number that came from the distribution itself, but 
the difference is not meaningful (and only arrises because of ran-
domness in the distribution). another thing you may have noticed 

is that the larger the sample size, the smaller the standard error. 
this leads to an important rule for working with samples: the big-
ger the better.
the last thing is another shortcut. we found out the 97.5% cut 
point by constructing the sampling distribution and then using 
quantile to tell us the actual cuts. you can also cut points just using 
the mean and the standard error. two standard errors down from 
the mean is the 2.5% cut point and two standard errors up from the 
mean is the 97.5% cut point.
> stderror<-sd(usstatepops$v1)/sqrt(5)
> cutpoint975<-mean(usstatepops$v1)+(2 * stder-
ror)
> cutpoint975
[1] 12157391
you will notice again that this value is different from what we cal-
culated with the quantile() function using the empirical distribu-
tion. the differences arise because of the randomness in the distri-
bution that we constructed. the value above is an estimate that is 
based on statistical proofs, whereas the empirical samplemeans list 
that we constructed is just one of a nearly in   nite range of such 
lists that we could create. we could easily reduce the discrepancy 
between the two methods by using a larger sample size and by hav-
ing more replications included in the sampling distribution.
to summarize, with a data set that includes 51 data points with the 
numbers of people in states, and a bit of work using r to construct 
a distribution of sampling means, we have learned the following:

57

    run a statistical process a large number of times and you get a 

consistent pattern of results.

    taking the means of a large number of samples and plotting 
them on a histogram shows that the sample means are fairly 
well normally distributed and that the center of the distribution 
is very, very close to the mean of the original raw data.

    this resulting distribution of sample means can be used as a ba-

sis for comparisons. by making cut points at the extreme low 
and high ends of the distribution, for example 2.5% and 97.5%, 
we have a way of comparing any new information we get.

    if we get a new sample mean, and we    nd that it is in the ex-
treme zone de   ned by our cut points, we can tentatively con-
clude that the sample that made that mean is a different kind of 
thing than the samples that made the sampling distribution.

    a shortcut and more accurate way of    guring the cut points in-
volves calculating the "standard error" based on the standard de-
viation of the original raw data.

we   re not statisticians at this point, but the process of reasoning 
based on sampling distributions is at the heart of inferential statis-
tics, so if you have followed the logic presented in this chapter, you 
have made excellent progress towards being a competent user of 
applied statistics.

chapter challenge
collect a sample consisting of at least 20 data points and construct 
a sampling distribution. calculate the standard error and use this 

to calculate the 2.5% and 97.5% distribution cut points. the data 
points you collect should represent instances of the same phenome-
non. for instance, you could collect the prices of 20 textbooks, or 
count the number of words in each of 20 paragraphs.
sources
http://en.wikipedia.org/wiki/central_limit_theorem 
http://en.wikipedia.org/wiki/gerolamo_cardano 
http://en.wikipedia.org/wiki/jacob_bernoulli 
http://en.wikipedia.org/wiki/law_of_large_numbers
http://en.wikipedia.org/wiki/list_of_u.s._states_and_territories
_by_population 
http://www.khanacademy.org/math/statistics/v/central-limit-th
eorem 

r commands used in this chapter
length() - the number of elements in a vector
mean() - the arithmetic mean or average of a set of values
quantile() - calculates cut points based on percents/proportions
replicate() - runs an expression/calculation many times
sample() - chooses elements at random from a vector
sd() - calculates standard deviation

58

sqrt() - calculates square root
summary() - summarizes contents of a vector

59

chapter 8

big data? big deal!

in 2012 the technology press contained many headlines about big data. what makes data big, and 
why is this bigness important? in this chapter, we discuss some of the real issues behind these 
questions. armed with information from the previous chapter concerning sampling, we can give 
more thought to how the size of a data set affects what we do with the data. 

60

marketwatch (a wall street journal service) recently published an 
article with the title, "big data equals big business opportunity 
say global it and business professionals," and the subtitle, "70 per-
cent of organizations now considering, planning or running big 
data projects according to new global survey." the technology 
news has been full of similar articles for several years. given the 
number of such articles it is hard to resist the idea that "big data" 
represents some kind of revolution that has turned the whole 
world of information and technology topsy-turvy. but is this really 
true? does "big data" change everything?
business analyst doug laney suggested that three characteristics 
make "big data" different from what came before: volume, velocity, 
and variety. volume refers to the sheer amount of data. velocity fo-
cuses on how quickly data arrives as well as how quickly those 
data become "stale." finally, variety re   ects the fact that there may 
be many different kinds of data. together, these three characteris-
tics are often referred to as the "three vs" model of big data. note, 
however, that even before the dawn of the computer age we   ve had 
a variety of data, some of which arrives quite quickly, and that can 
add up to quite a lot of total storage over time (think, for example, 
of the large variety and volume of data that has arrived annually at 
library of congress since the 1800s!). so it is dif   cult to tell, just 
based on someone saying that they have a high volume, high veloc-
ity, and high variety data problem, that big data is fundamentally a 
brand new thing. 
with that said, there are certainly many changes afoot that make 
data problems qualitatively different today as compared with a 
few years ago. let   s list a few things which are pretty accurate:

1. the decline in the price of sensors (like barcode readers) and 
other technology over recent decades has made it cheaper and 
easier to collect a lot more data.

2. similarly, the declining cost of storage has made it practical to 

keep lots of data hanging around, regardless of its quality or use-
fulness.

3. many people   s attitudes about privacy seem to have accommo-
dated the use of facebook and other platforms where we reveal 
lots of information about ourselves.

4. researchers have made signi   cant advances in the "machine 
learning" algorithms that form the basis of many data mining 
techniques.

5. when a data set gets to a certain size (into the range of thou-

sands of rows), conventional tests of statistical signi   cance are 
meaningless, because even the most tiny and trivial results (or 
effect sizes, as statisticians call them) are statistically signi   cant.

keeping these points in mind, there are also a number of things 
that have not changed throughout the years:
a. garbage in, garbage out: the usefulness of data depends heav-
ily upon how carefully and well it was collected. after data were 
collected, the quality depends upon how much attention was paid 
to suitable pre-processing: data cleaning and data screening.
b. bigger equals weirder: if you are looking for anomalies - rare 
events that break the rules - then larger is better. low frequency 
events often do not appear until a data collection goes on for a long 

61

time and/or encompasses a large enough group of instances to con-
tain one of the bizarre cases.
c. linking adds potential: standalone datasets are inherently lim-
ited by whatever variables are available. but if those data can be 
linked to some other data, all of a sudden new vistas may open up. 
no guarantees, but the more you can connect records here to other 
records over there, the more potential    ndings you have.
items on both of the lists above are considered pretty common-
place and uncontroversial. taken together, however, they do shed 
some light on the question of how important "big data" might be. 
we have had lots of historical success using conventional statistics 
to examine modestly sized (i.e., 1000 rows or less) datasets for sta-
tistical regularities. everyone   s favorite basic statistic, the student   s 
t-test, is essential a test for differences in the central tendency of 
two groups. if the data contain regularities such that one group is 
notably different from another group, a t-test shows it to be so. 
big data does not help us with these kinds of tests. we don   t even 
need a thousand records for many conventional statistical compari-
sons, and having a million or a hundred million records won   t 
make our job any easier (it will just take more computer memory 
and storage). think about what you read in the previous chapter: 
we were able to start using a basic form of statistical id136 with 
a data set that contained a population with only 51 elements. in 
fact, many of the most commonly used statistical techniques, like 
the student   s t-test, were designed speci   cally to work with very 
small samples. 
on the other hand, if we are looking for needles in haystacks, it 
makes sense to look (as ef   ciently as possible) through the biggest 

possible haystack we can    nd, because it is much more likely that a 
big haystack will contain at least one needle and maybe more. 
keeping in mind the advances in machine learning that have oc-
curred over recent years, we begin to have an idea that good tools 
together with big data and interesting questions about unusual pat-
terns could indeed provide some powerful new insights.
let   s couple this optimism with three very important cautions. the 
   rst caution is that the more complex our data are, the more dif   -
cult it will be to ensure that the data are "clean" and suitable for the 
purpose we plan for them. a dirty data set is worse in some ways 
than no data at all because we may put a lot of time and effort into 
   nding an insight and    nd nothing. even more problematic, we 
may put a lot of time and effort and    nd a result that is simply 
wrong! many analysts believe that cleaning data - getting it ready 
for analysis, weeding out the anomalies, organizing the data into a 
suitable con   guration - actually takes up most of the time and ef-
fort of the analysis process.
the second caution is that rare and unusual events or patterns are 
almost always by their nature highly unpredictable. even with the 
best data we can imagine and plenty of variables, we will almost 
always have a lot of trouble accurately enumerating all of the 
causes of an event. the data mining tools may show us a pattern, 
and we may even be able to replicate the pattern in some new data, 
but we may never be con   dent that we have understood the pat-
tern to the point where we believe we can isolate, control, or under-
stand the causes. predicting the path of hurricanes provides a great 
example here: despite decades of advances in weather instrumenta-
tion, forecasting, and number crunching, meteorologists still have 
great dif   culty predicting where a hurricane will make landfall or 

62

how hard the winds will blow when it gets there. the complexity 
and unpredictability of the forces at work make the task exceed-
ingly dif   cult. 
the third caution is about linking data sets. item c above suggests 
that linkages may provide additional value. with every linkage to 
a new data set, however, we also increase the complexity of the 
data and the likelihood of dirty data and resulting spurious pat-
terns. in addition, although many companies seem less and less 
concerned about the idea, the more we link data about living peo-
ple (e.g., consumers, patients, voters, etc.) the more likely we are to 
cause a catastrophic loss of privacy. even if you are not a big fan of 
the importance of privacy on principle, it is clear that security and 
privacy failures have cost companies dearly both in money and 
reputation. today   s data innovations for valuable and acceptable 
purposes maybe tomorrow   s crimes and scams. the greater the 
amount of linkage between data sets, the easier it is for those peo-
ple with malevolent intentions to exploit it.
putting this altogether, we can take a sensible position that high 
quality data, in abundance, together with tools used by intelligent 
analysts in a secure environment, may provide worthwhile bene-
   ts in the commercial sector, in education, in government, and in 
other areas. the focus of our efforts as data scientists, however, 
should not be on achieving the largest possible data sets, but rather 
on getting the right data and the right amount of data for the pur-
pose we intend. there is no special virtue in having a lot of data if 
those data are unsuitable to the conclusions that we want to draw. 
likewise, simply getting data more quickly does not guarantee 
that what we get will be highly relevant to our problems. finally, 
although it is said that variety is the spice of life, complexity is of-

ten a danger to reliability and trustworthiness: the more complex 
the linkages among our data the more likely it is that problems 
may crop up in making use of those data or keeping them safe.
the tools of data science
over the past few chapters, we   ve gotten a pretty quick jump start 
on an analytical tool used by thousands of data analysts world-
wide - the open source r system for data analysis and visualiza-
tion. despite the many capabilities of r, however, there are hun-
dreds of other tools used by data scientists, depending on the par-
ticular aspects of the data problem they focus on. 
the single most popular and powerful tool, outside of r, is a pro-
prietary statistical system called sas (pronounced "sass"). sas con-
tains a powerful programming language that provides access to 
many data types, functions, and language features. learning sas 
is arguably as dif   cult (or as easy, depending upon your perspec-
tive) as learning r, but sas is used by many large corporations be-
cause, unlike r, there is extensive technical and product support 
on offer. of course, this support does not come cheap, so most sas 
users work in large organizations that have suf   cient resources to 
purchase the necessary licenses and support plans.
next in line in the statistics realm is spss, a package used by many 
scientists (the acronym used to stand for statistical package for the 
social sciences). spss is much friendlier than sas, in the opinion 
of many analysts, but not quite as    exible and powerful.
r, spss, and sas grew up as statistics packages, but there are also 
many general purpose programming languages that incorporate 
features valuable to data scientists. one very exciting development 

63

http://en.wikipedia.org/wiki/big_data 
http://en.wikipedia.org/wiki/data.gov 
http://www.marketwatch.com/story/big-data-equals-big-busines
s-opportunity-say-global-it-and-business-professionals-2012-05-14 

in programming languages has the odd name of "processing." proc-
essing is a programming language speci   cally geared toward creat-
ing data visualizations.  like r, processing is an open source pro-
ject, so it is freely available at http://processing.org/. also like r, 
processing is a cross-platform program, so it will run happily on 
mac, windows, and linux. there are lots of books available for 
learning processing (unfortunately, no open source books yet) and 
the website contains lots of examples for getting started. besides r, 
processing might be one of the most important tools in the data sci-
entist   s toolbox, at least for those who need to use data to draw con-
clusions and communicate with others.
chapter challenge
look over the various websites connected with "data.gov" to    nd 
the largest and/or most complex data set that you can. think 
about (and perhaps write about) one or more of the ways that 
those data could potentially be misused by analysts. download a 
data set that you    nd interesting and read it into r to see what you 
can do with it.
for a super extra challenge, go to this website: 
http://teamwpc.co.uk/products/wps 
and download a trial version of the "world programming system" 
(wps). wps can read sas code, so you could easily look up the 
code that you would need in order to read in your data.gov data-
set. 
sources
http://aqua.nasa.gov/doc/pubs/wx_forecasting.pdf 

64

chapter 9

onward with r-studio

as an open source program with an active user community, r enjoys constant innovation thanks to 
the dedicated developers who work on it. one useful innovation was the development of r-studio, a 
beautiful frame to hold your copy of r. this chapter walks through the installation of r-studio and 
introduces "packages," the key to the extensibility of r.

65

joseph j. allaire is a serial entrepreneur, software engineer, and the 
originator of some remarkable software products including "cold-
fusion," which was later sold to the web media tools giant mac-
romedia and windows live writer, a microsoft blogging tool. start-
ing in 2009, allaire began working with a small team to develop an 
open source program that enhances the usability and power of r.
as mentioned in previous chapters, r is an open source program, 
meaning that the source code that is used to create a copy of r to 
run on a mac, windows, or linux computer is available for all to 
inspect and modify. as with many open source projects, there is an 
active community of developers who work on r, both on the basic 
program itself and the many pieces and parts that can be added 
onto the basic program. one of these add-ons is r-studio. r-studio 
is an integrated development environment, abbreviated as ide. 
every software engineer knows that if you want to get serious 
about building something out of code, you must use an ide. if you 
think of r as a piece of canvas rolled up and laying on the    oor, r-
studio is like an elegant picture frame. r hangs in the middle of r 
studio, and like any good picture frame, enhances our appreciation 
of what is inside it.
the website for r-studio is http://www.rstudio.org/ and you can 
inspect the information there at any time. for most of the rest of 
this chapter, if you want to follow along with the installation and 
use of r-studio, you will need to work on a mac, windows, or 
linux computer. 
before we start that, let   s consider why we need an ide to work 
with r. in the previous chapters, we have typed a variety of com-
mands into r, using what is known as the "r console." console is 

an old technology term that dates back to the days when comput-
ers were so big that they each occupied their own air conditioned 
room. within that room there was often one "master control sta-
tion" where a computer operator could do just about anything to 
control the giant computer by typing in commands. that station 
was known as the console. the term console is now used in many 
cases to refer to any interface where you can directly type in com-
mands. we   ve typed commands into the r console in an effort to 
learn about the r language as well as to illustrate some basic princi-
ples about data structures and statistics.
if we really want to "do" data science, though, we can   t sit around 
typing commands every day. first of all, it will become boring very 
fast. second of all, whoever is paying us to be a data scientist will 
get suspicious when he or she notices that we are retyping some of 
the commands we typed yesterday. third, and perhaps most impor-
tant, it is way too easy to make a mistake - to create what computer 
scientists refer to as a bug - if you are doing every little task by 
hand. for these reasons, one of our big goals within this book is to 
create something that is reusable: where we can do a few clicks or 
type a couple of things and unleash the power of many processing 
steps. using an ide, we can build these kinds of reusable pieces. 
the ide gives us the capability to open up the process of creation, 
to peer into the component parts when we need to, and to close the 
hood and hide them when we don   t. because we are working with 
data, we also need a way of closely inspecting the data, both its con-
tents and its structure. as you probably noticed, it gets pretty tedi-
ous doing this at the r console, where almost every piece of output 
is a chunk of text and longer chunks scroll off the screen before you 
can see them. as an ide for r, r-studio allows us to control and 

66

ately in order to get started with the activities in the later parts of 
this chapter. unlike other introductory materials, we will not walk 
through all of the different elements of the r-studio screen. rather, 
as we need each feature we will highlight the new aspect of the ap-
plication. when you run r-studio, you will see three or four sub-
windows. use the file menu to click "new" and in the sub-menu 
for "new" click "r script." this should give you a screen that looks 
something like this:

monitor both our code and our text in a way that supports the crea-
tion of reusable elements.
before we can get there, though, we have to have r-studio in-
stalled on a computer. perhaps the most challenging aspect of in-
stalling r-studio is having to install r    rst, but if you   ve already 
done that in chapter 2, then r-studio should be a piece of cake. 
make sure that you have the latest version of r installed before 
you begin with the installation of r-studio. there is ample docu-
mentation on the r-studio website, 
http://www.rstudio.org/, so if you follow the instructions 
there, you should have minimal dif   culty. if you reach a 
page where you are asked to choose between installing r-
studio server and installing r-studio as a desktop applica-
tion on your computer, choose the latter. we will look into r-
studio server a little later, but for now you want the 
desktop/single user version. if you run into any dif   culties 
or you just want some additional guidance about r-studio, 
you may want to have a look at the book entitled, getting 
started with r-studio, by john verzani (2011, sebastopol, ca: 
o   reilly media). the    rst chapter of that book has a general 
orientation to r and r-studio as well as a guide to installing 
and updating r-studio. there is also a youtube video that 
introduces r-studio here:     
http://www.youtube.com/watch?v=7samqkz3be8    
be aware if you search for other youtube videos that there is 
a disk recovery program as well a music group that share the 
r-studio name: you will get a number of these videos if you 
search on "r-studio" without any other search terms.
once you have installed r-studio, you can run it immedi-

67

the upper left hand "pane" (another name for a sub-window) dis-
plays a blank space under the tab title "untitled1." click in that 
pane and type the following:
mymode <- function(myvector)
{
  return(myvector)
}
you have just created your    rst "function" in r. a function is a bun-
dle of r code that can be used over and over again without having 
to retype it. other programming languages also have functions. 
other words for function are "procedure" and "subroutine," al-
though these terms can have a slightly different meaning in other 
languages. we have called our function "mymode." you may re-
member from a couple of chapters that the basic setup of r does 
not have a statistical mode function in it, even though it does have 
functions for the two other other common central tendency statis-
tics, mean() and median(). we   re going to    x that problem by creat-
ing our own mode function. recall that the mode function should 
count up how many of each value is in a list and then return the 
value that occurs most frequently. that is the de   nition of the statis-
tical mode: the most frequently occurring item in a vector of num-
bers.
a couple of other things to note: the    rst is the "myvector" in pa-
rentheses on the    rst line of our function.  this is the "argument" or 
input to the function. we have seen arguments before when we 
called functions like mean() and median(). next, note the curly 
braces that are used on the second and    nal lines. these curly 

braces hold together all of the code that goes in our function. fi-
nally, look at the return() right near the end of our function. this 
return() is where we send back the result of what our function ac-
complished. later on when we "call" our new function from the r 
console, the result that we get back will be whatever is in the paren-
theses in the return().
based on that explanation, can you    gure out what mymode() 
does in this primitive initial form? all it does is return whatever 
we give it in myvector, completely unchanged. by the way, this is a 
common way to write code, by building up bit by bit. we can test 
out what we have each step of the way. let   s test out what we have 
accomplished so far. first, let   s make a very small vector of data to 
work with. in the lower left hand pane of r-studio you will notice 
that we have a regular r console running. you can type commands 
into this console, just like we did in previous chapters just using r:
> tinydata <- c(1,2,1,2,3,3,3,4,5,4,5)
> tinydata
 [1] 1 2 1 2 3 3 3 4 5 4 5
then we can try out our new mymode() function:
> mymode(tinydata)
error: could not find function "mymode"
oops! r doesn   t know about our new function yet. we typed our 
mymode() function into the code window, but we didn   t tell r 
about it. if you look in the upper left pane, you will see the code 
for mymode() and just above that a few small buttons on a tool 
bar. one of the buttons looks like a little right pointing arrow with 

68

the word "run" next to it. first, use your mouse to select all of the 
code for mymode(), from the    rst m all the way to the last curly 
brace. then click the run button. you will immediately see the 
same code appear in the r console window just below. if you have 
typed everything correctly, there should be no errors or warnings. 
now r knows about our mymode() function and is ready to use it. 
now we can type:
> mymode(tinydata)
 [1] 1 2 1 2 3 3 3 4 5 4 5
this did exactly what we expected: it just echoed back the contents 
of tinydata. you can see from this example how parameters work, 
too. in the command just above, we passed in tinydata as the input 
to the function. while the function was working, it took what was 
in tinydata and copied it into myvector for use inside the function. 
now we are ready to add the next command to our function:
mymode <- function(myvector)
{
  uniquevalues <- unique(myvector)
  return(uniquevalues)
}
because we made a few changes, the whole function appears again 
above. later, when the code gets a little more complicated, we will 
just provide one or two lines to add. let   s see what this code does. 
first, don   t forget to select the code and click on the run button. 
then, in the r console, try the mymode() command again:

> mymode(tinydata)
[1] 1 2 3 4 5
pretty easy to see what the new code does, right? we called the 
unique() function, and that returned a list of unique values that ap-
peared in tinydata. basically, unique() took out all of the redundan-
cies in the vector that we passed to it. now let   s build a little more:
mymode <- function(myvector)
{
  uniquevalues <- unique(myvector)
  uniquecounts <- tabulate(myvector)
  return(uniquecounts)
}
don   t forget to select all of this code and run it before testing it 
out. this time when we pass tinydata to our function we get back 
another list of    ve elements, but this time it is the count of how 
many times each value occurred:
> mymode(tinydata)
[1] 2 2 3 2 2
now we   re basically ready to    nish our mymode() function, but 
let   s make sure we understand the two pieces of data we have in 
uniquevalues and uniquecounts:
in the table below we have lined up a row of the elements of 
uniquevalues just above a row of the counts of how many of each 
of those values we have. just for illustration purposes, in the top/

69

index

uniquevalues
uniquecounts

1
1
2

2
2
2

3
3
3

4
4
2

5
5
2

label row we have also shown the "index" number. this index num-
ber is the way that we can "address" the elements in either of the 
variables that are shown in the rows. for instance, element number 
4 (index 4) for uniquevalues contains the number four, whereas ele-
ment number four for uniquecounts contains the number two. so 
if we   re looking for the most frequently occurring item, we should 
look along the bottom row for the largest number. when we get 
there, we should look at the index of that cell. whatever that index 
is, if we look in the same cell in uniquevalues, we will have the 
value that occurs most frequently in the original list. in r, it is easy 
to accomplish what was described in the last sentence with a single 
line of code:
uniquevalues[which.max(uniquecounts)]
the which.max() function    nds the index of the element of unique-
counts that is the largest. then we use that index to address 
uniquevalues with square braces. the square braces let us get at 
any of the elements of a vector. for example, if we asked for 
uniquevalues[5] we would get the number 5. if we add this one list 
of code to our return statement, our function will be    nished:
mymode <- function(myvector)
{
  uniquevalues <- unique(myvector)

  uniquecounts <- tabulate(myvector)
  return(uniquevalues[which.max(uniquecounts)])
}
we   re now ready to test out our function. don   t forget to select the 
whole thing and run it! otherwise r will still be remembering our 
old one. let   s ask r what tinydata contains, just to remind our-
selves, and then we will send tinydata to our mymode() function:
> tinydata
 [1] 1 2 1 2 3 3 3 4 5 4 5
> mymode(tinydata)
[1] 3
hooray! it works. three is the most frequently occurring value in 
tinydata. let   s keep testing and see what happens:
> tinydata<-c(tinydata,5,5,5)
> tinydata
 [1] 1 2 1 2 3 3 3 4 5 4 5 5 5 5
> mymode(tinydata)
[1] 5

70

it still works! we added three more    ves to the end of the tinydata 
vector. now tinydata  contains    ve    ves. mymode() properly re-
ports the mode as    ve. id48, now let   s try to break it:
> tinydata
 [1] 1 2 1 2 3 3 3 4 5 4 5 5 5 5 1 1 1
> mymode(tinydata)
[1] 1
this is interesting: now tinydata contains    ve ones and    ve    ves. 
mymode() now reports the mode as one. this turns out to be no 
surprise. in the documentation for which.max() it says that this 
function will return the    rst maximum it    nds. so this behavior is 
to be expected. actually, this is always a problem with the statisti-
cal mode: there can be more than one mode in a data set. our my-
mode() function is not smart enough to realize this, not does it give 
us any kind of warning that there are multiple modes in our data. 
it just reports the    rst mode that it    nds.
here   s another problem:
> tinydata<-c(tinydata,9,9,9,9,9,9,9)
> mymode(tinydata)
[1] na
> tabulate(tinydata)
 [1] 5 2 3 2 5 0 0 0 7 
in the    rst line, we stuck a bunch of nines on the end of tinydata. 
remember that we had no sixes, sevens, or eights. now when we 

run mymode() it says "na," which is r   s way of saying that some-
thing went wrong and you are getting back an empty value. it is 
probably not obvious why things went whacky until we look at the 
last command above, tabulate(tinydata). here we can see what 
happened: when it was run inside of the mymode() function, tabu-
late() generated a longer list than we were expecting, because it 
added zeroes to cover the sixes, sevens, and eights that were not 
there. the maximum value, out at the end is 7, and this refers to 
the number of nines in tinydata. but look at what the unique() 
function produces:
> unique(tinydata)
[1]  1  2  3  4  5  9
there are only six elements in this list, so it doesn   t match up as it 
should (take another look at the table on the previous page and 
imagine if the bottom row stuck out further than the row just 
above it). we can    x this with the addition of the match() function 
to our code:
mymode <- function(myvector)
{
  uniquevalues <- unique(myvector)
  uniquecounts <- tabulate( +    
               match(myvector,uniquevalues))
  
  return(uniquevalues[which.max(uniquecounts)])
}

71

the new part of the code is in bold. now instead of tabulating 
every possible value, including the ones for which we have no 
data, we only tabulate those items where there is a "match" be-
tween the list of unique values and what is in myvector. now 
when we ask mymode() for the mode of tinydata we get the cor-
rect result:
> mymode(tinydata)
[1] 9
aha, now it works the way it should. after our last addi-
tion of seven nines to the data set, the mode of this vector is 
correctly reported as nine. 
before we leave this activity, make sure to save your work. 
click anywhere in the code window and then click on the 
file menu and then on save. you will be prompted to 
choose a location and provide a    lename. you can call the 
   le mymode, if you like. note that r adds the "r" extension 
to the    lename so that it is saved as mymode.r. you can 
open this    le at any time and rerun the mymode() function 
in order to de   ne the function in your current working ver-
sion of r. 
a couple of other points deserve attention. first, notice that 
when we created our own function, we had to do some test-
ing and repairs to make sure it ran the way we wanted it to. 
this is a common situation when working on anything re-
lated to computers, including spreadsheets, macros, and pretty 
much anything else that requires precision and accuracy. second, 
we introduced at least four new functions in this exercise, includ-
ing unique(), tabulate(), match(), and which.max(). where did 

these come from and how did we know? r has so many functions 
that it is very dif   cult to memorize them all. there   s almost always 
more than one way to do something, as well. so it can be quite con-
fusing to create a new function, if you don   t know all of the ingredi-
ents and there   s no one way to solve a particular problem. this is 
where the community comes in. search online and you will    nd 

dozens of instances where people have tried to solve similar prob-
lems to the one you are solving, and you will also    nd that they 
have posted the r code for their solutions. these code fragments 
are free to borrow and test. in fact, learning from other people   s ex-

72

amples is a great way to expand your horizons and learn new tech-
niques.
the last point leads into the next key topic. we had to do quite a 
bit of work to create our mymode function, and we are still not 
sure that it works perfectly on every variation of data it might en-
counter. maybe someone else has already solved the same prob-
lem. if they did, we might be able to    nd an existing "package" to 
add onto our copy of r to extend its functions. in fact, for the statis-
tical mode, there is an existing package that does just about every-
thing you could imagine doing with the mode. the package is 
called modeest, a not very good abbreviation for mode-estimator. 
to install this package look in the lower right hand pane of r-
studio. there are several tabs there, and one of them is "packages." 
click on this and you will get a list of every package that you al-
ready have available in your copy of r (it may be a short list) with 
checkmarks for the ones that are ready to use. it is unlikely that 
modeest is already on this list, so click on the button that says "in-
stall packages. this will give a dialog that looks like what you see 
on the screenshot above. type the beginning of the package name 
in the appropriate area, and r-studio will start to prompt you with 
matching choices. finish typing modeest or choose it off of the list. 
there may be a check box for "install dependencies," and if so 
leave this checked. in some cases an r package will depend on 
other packages and r will install all of the necessary packages in 
the correct order if it can. once you click the "install" button in this 
dialog, you will see some commands running on the r console (the 
lower left pane). generally, this works without a hitch and you 
should not see any warning messages. once the installation is com-
plete you will see modeest added to the list in the lower right pane 
(assuming you have clicked the "packages" tab). one last step is to 

click the check box next to it. this runs the library() function on the 
package, which prepares it for further use.
let   s try out the mfv() function. this function returns the "most fre-
quent value" in a vector, which is generally what we want in a 
mode function: 
> mfv(tinydata)
[1] 9
so far so good! this seems to do exactly what our mymode() func-
tion did, though it probably uses a different method. in fact, it is 
easy to see what strategy the authors of this package used just by 
typing the name of the function at the r command line:
> mfv
function (x, ...) 
{
    f <- factor(x)
    tf <- tabulate(f)
    return(as.numeric(levels(f)[tf == max(tf)]))
}
<environment: namespace:modeest>
this is one of the great things about an open source program: you 
can easily look under the hood to see how things work. notice that 
this is quite different from how we built mymode(), although it too 
uses the tabulate() function. the    nal line, that begins with the 
word "environment" has importance for more complex feats of pro-

73

gramming, as it indicates which variable names mfv() can refer to 
when it is working. the other aspect of this function which is 
probably not so obvious is that it will correctly return a list of multi-
ple modes when one exists in the data you send to it:
> multidata <- c(1,5,7,7,9,9,10)
> mfv(multidata)
[1] 7 9
> mymode(multidata)
[1] 7
in the    rst command line above, we made a small new vector that 
contains two modes, 7 and 9.  each of these numbers occurs twice, 
while the other numbers occur only once. when we run mfv() on 
this vector it correctly reports both 7 and 9 as modes. when we use 
our function, mymode(), it only reports the    rst of the two modes.
to recap, this chapter provided a basic introduction to r-studio, an 
integrated development environment (ide) for r. an ide is useful 
for helping to build reusable components for handling data and 
conducting data analysis. from this point forward, we will use r-
studio, rather than plain old r, in order to save and be able to re-
use our work. among other things, r-studio makes it easy to man-
age "packages" in r, and packages are the key to r   s extensibility. 
in future chapters we will be routinely using r packages to get ac-
cess to specialized capabilities. 
these specialized capabilities come in the form of extra functions 
that are created by developers in the r community. by creating our 
own function, we learn that functions take "arguments" as their in-

puts and provide a return value. a return value is a data object, so 
it could be a single number (technically a vector of length one) or it 
could be a list of values (a vector) or even a more complex data ob-
ject. we can write and reuse our own functions, which we will do 
quite frequently later in the book, or we can use other people   s 
functions by installing their packages and using the library() func-
tion to make the contents of the package available. once we have 
used library() we can inspect how a function works by typing its 
name at the r command line. (note that this works for many func-
tions, but there are a few that were created in a different computer 
language, like c, and for those we will not be able to inspect the 
code as easily.)
chapter challenge
write and test a new function called mysamplingdistribution() 
that creates a sampling distribution of means from a numeric input 
vector. you will need to integrate your knowledge of creating new 
functions from this chapter with your knowledge of creating sam-
pling distributions from the previous chapter in order to create a 
working function. make sure to give careful thought about the pa-
rameters you will need to pass to your function and what kind of 
data object your function will return.
sources
http://en.wikipedia.org/wiki/r_(programming_language)
http://en.wikipedia.org/wiki/joseph_j._allaire  
http://stats.lse.ac.uk/penzer/st419materials/cschpt3.pdf 

74

http://www.use-r.org/downloads/getting_started_with_rstudio
.pdf 
http://www.statmethods.net/interface/packages.html 
http://www.youtube.com/watch?v=7samqkz3be8 

review 9.1 onward with r-studio

question 1 of  5
one common de   nition for the statistical mode is:

r commands used in this chapter
function() - creates a new function
return() - completes a function by returning a value
tabulate() - counts occurrences of integer-valued data in a vector
unique() - creates a list of unique values in a vector
match() - takes two lists and returns values that are in each
mfv() - most frequent value (from the modeest package)

a. the sum of all values divided by the 

number of values.

b. the most frequently occurring value 

in the data.

c. the halfway point through the data.
d. the distance between the smallest 

value and the largest value.

75

check answerchapter 10

tweet, 

tweet!

we   ve come a long way already: basic skills in controlling r, some exposure to r-studio, knowledge 
of how to manage add-on packages, experience creating a function, essential descriptive statistics, and 
a start on sampling distributions and inferential statistics. in this chapter, we use the social media 
service twitter to grab some up-to-the minute data and begin manipulating it. 

76

prior to this chapter we only worked with toy data sets: some 
made up data about a    ctional family and the census head 
counts for the 50 states plus the district of columbia. at this 
point we have practiced a suf   cient range of skills to work 
with some real data. there are data sets everywhere, thou-
sands of them, many free for the taking, covering a range of 
interesting topics from psychology experiments to    lm actors. 
for sheer immediacy, though, you can   t beat the twitter so-
cial media service. as you may know from direct experience, 
twitter is a micro-blogging service that allows people all over 
the world to broadcast brief thoughts (140 characters or less) 
that can then be read by their "followers" (other twitter users 
who signed up to receive the sender   s messages). the devel-
opers of twitter, in a stroke of genius, decided to make these 
postings, called tweets, available to the general public 
through a web page on the twitter.com site, and additional 
through what is known as an application programming inter-
face or api. 
here   s where the natural extensibility of r comes in. an indi-
vidual named jeff gentry who, at this writing, seems to be a 
data professional in the    nancial services industry, created an 
add-on package for r called twitter (not sure how it is pro-
nounced, but "twit-are" seems pretty close). the twitter pack-
age provides an extremely simple interface for downloading 
a list of tweets directly from the twitter service into r. using 
the interface functions in twitter, it is possible to search 
through twitter to obtain a list of tweets on a speci   c topic. 
every tweet contains the text of the posting that the author 
wrote as well as lots of other useful information such as the 
time of day when a tweet was posted. put it all together and 

77

it makes a fun way of getting up-to-the-minute data on what 
people are thinking about a wide variety of topics. 
the other great thing about working with twitter is that we 
will use many, if not all of the skills that we have developed 
earlier in the book to put the interface to use.
a token of your esteem: using oauth
before we move forward with creating some code in r-
studio, there   s an important set of steps we need to accom-
plish at the twitter website.
in 2013, twitter completed a transition to a new version of 
their application programming interface, or api. this new 
api requires the use of a technique for authorization - a way 
of proving to twitter that you are who you are when you 
search for (or post) tweets from a software application. the 
folks at twitter adopted an industry standard for this process 
known as oauth. oauth provides a method for obtaining 
two pieces of information - a "secret" and a "key" - without 
which it will be dif   cult if not downright impossible to work 
with twitter (as well as twitter). here are the steps:
1.!get a twitter account at twitter.com if you don   t already 

have one.

2.!go to the development page at twitter 

(https://dev.twitter.com) and sign in with your twitter cre-
dentials.

3.!click on "my applications." the location of this may vary 

over time, but look for in a drop down list that is under 
your pro   le picture on the top right corner of the screen.
4.!click on "create a new application." fill in the blanks 

with some sensible answers. where it asks for a    website    
you can give your own home page. this is a required re-
sponse, so you will have to have some kind of web page to 
point to. in contrast, the    callback url    can be left blank. 
click submit.

5.!check the checkbox speci   ed in the image below under set-
tings. your application should be set so that it can be used 
to sign in with twitter.

6.!you will get a screen containing a whole bunch of data. 
make sure to save it all, but the part that you will really 
need is the "consumer key" and the "consumer secret," 
both of which are long strings of letters and numbers. 
these strings will be used later to get your application run-
ning in r. the reason these are such long strings of gibber-
ish is that they are encrypted.

7.!also take note of the request token url and the author-

ize url. for the most part these are exactly the same 
across all uses of twitter, but they may change over time, 
so you should make sure to stash them away for later. you 
do not need to click on the    create my access token    but-
ton.

8.!go to the settings tab and make sure that "read, write and 

access direct messages" is set. 

you may notice on the home->my applications screen in the 
dev.twitter.com interface that there are additional tabs along 
the top for different activities and tasks related to oauth. 
there is a tab called "oauth tool" where you can always 
come back to get your consumer key and consumer secret 
information. later in the chapter we will come back to the us-
age of your consumer key and your consumer secret but be-
fore we get there we have to get the twitter package ready to 
go.
working with twitter
to begin working with twitter, launch your copy of r-studio. 
the    rst order of business is to create a new r-studio "pro-
ject". a project in r-studio helps to keep all of the different 
pieces and parts of an activity together including the datasets 
and variables that you establish as well as the functions that 
you write. for professional uses of r and r-studio, it is impor-
tant to have one project for each major activity: this keeps dif-
ferent data sets and variable names from interfering with 
each other. click on the "project" menu in r-studio and then 
click on "new project." you will usually have a choice of three 
kinds of new projects, a brand new "clean" project, an existing 
directory of    les that will get turned into a project folder, or a 
project that comes out of a version control system. (later in 
the book we will look at version control, which is great for 
projects involving more than one person.) choose "new di-
rectory" to start a brand new project. you can call your project 

78

whatever you want, but because this project uses the twitter 
package, you might want to just call the project "twitter". you 
also have a choice in the dialog box about where on your com-
puter r-studio will create the new directory.
r-studio will respond by showing a clean console screen and 
most importantly an r "workspace" that does not contain any 
of the old variables and data that we created in previous chap-
ters. in order to use twitter, we need to load several packages 
that it depends upon. these are called, in order "bitops", 
"rcurl", "rjsonio", and once these are all in place "twitter" 
itself. rather than doing all of this by hand with the menus, 
let   s create some functions that will assist us and make the ac-
tivity more repeatable. first, here is a function that takes as 
input the name of a package. it tests whether the package has 
been downloaded - "installed" - from the r code repository. if 
it has not yet been downloaded/installed, the function takes 
care of this. then we use a new function, called require(), to 
prepare the package for further use. let   s call our function 
"ensurepackage" because it ensures that a package is ready 
for us to use. if you don   t recall this step from the previous 
chapter, you should click the "file" menu and then click 
"new" to create a new    le of r script. then, type or copy/
paste the following code:
ensurepackage<-function(x)
{
  x <- as.character(x)
  if (!require(x,character.only=true))
  {

    install.packages(pkgs=x,   
         repos="http://cran.r-project.org")
    require(x,character.only=true)
  }
}
on windows machines, the folder where new r packages are 
stored has to be con   gured to allow r to put new    les there 
(   write    permissions). in windows explorer, you can right 
click on the folder and choose    properties->security    then 
choose your username and user group, click edit, enable all 
permissions, and click ok. if you run into trouble, check out 
the windows faq at cran by searching or using this web 
address: 
http://cran.r-project.org/bin/windows/base/rw-faq.html . 
the require() function on the fourth line above does the same 
thing as library(), which we learned in the previous chapter, 
but it also returns the value "false" if the package you re-
quested in the argument "x" has not yet been downloaded. 
that same line of code also contains another new feature, the 
"if" statement. this is what computer scientists call a condi-
tional. it tests the stuff inside the parentheses to see if it evalu-
ates to true or false. if true, the program continues to 
run the script in between the curly braces (lines 4 and 8). if 
false, all the stuff in the curly braces is skipped. also in the 
third line, in case you are curious, the arguments to the re-
quire() function include "x," which is the name of the package 
that was passed into the function, and "character.only=true" 
which tells the require() function to expect x to be a character 

79

string. last thing to notice about this third line: there is a "!" 
character that reverses the results of the logical test. techni-
cally, it is the boolean function not. it requires a bit of men-
tal gyration that when require() returns false, the "!" inverts 
it to true, and that is when the code in the curly braces 
runs.
once you have this code in a script window, make sure to se-
lect the whole function and click run in the toolbar to make r 
aware of the function. there is also a checkbox on that same 
toolbar called, "source on save," that will keep us from hav-
ing to click on the run button all the time. if you click the 
checkmark, then every time you save the source code    le, r-
studio will rerun the code. if you get in the habit of saving af-
ter every code change you will always be running the latest 
version of your function.
now we are ready to put ensurepackage() to work on the 
packages we need for twitter. we   ll make a new function, 
"preparetwitter," that will load up all of our packages for us. 
here   s the code:
preparetwitter<-function()
{
  ensurepackage("bitops")
  ensurepackage("rcurl")
  ensurepackage("rjsonio")
  ensurepackage("twitter")
  ensurepackage("roauth")

}
this code is quite straightforward: it calls the ensurepack-
age() function we created before    ve times, once to load each 
of the packages we need. you may get some warning mes-
sages and these generally won   t cause any harm. if you are 
on windows and you get errors about being able to write to 
your library remember to check the windows faq as noted 
above. 
make sure to save your script    le once you have typed this 
new function in. you can give it any    le name that make 
sense to you, such as "twittersupport." now is also a good 
time to start the habit of commenting: comments are human 
readable messages that software developers leave for them-
selves and for others, so that everyone can remember what a 
piece of code is supposed to do. all computer languages have 
at least one "comment character" that sets off the human read-
able stuff from the rest of the code. in r, the comment charac-
ter is #. for now, just put one comment line above each func-
tion you created, brie   y describing it, like this:
# ensurepackage(x) - installs and loads a package 
# if necessary
and this:
# preparetwitter() - load packages for working 
# with twitter
later on we will do a better job a commenting, but this gives 
us the bare minimum we need to keep going with this pro-
ject. before we move on, you should run the preparetwitter() 

80

function on the console command line to actually load the 
packages we need:
> preparetwitter()
note the parentheses after the function name, even though 
there is no argument to this function. what would happen if 
you left out the parentheses? try it later to remind yourself of 
some basic r syntax rules. 
you may get a lot of output from running preparetwitter(), 
because your computer may need to download some or all of 
these packages. you may notice the warning message above, 
for example,, about objects being "masked." generally speak-
ing, this message refers to a variable or function that has be-
come invisible because another variable or function with the 
same name has been loaded. usually this is    ne: the newer 
thing works the same as the older thing with the same name. 
take a look at the four panes in r-studio, each of which con-
tains something of interest. the upper left pane is the code/
script window, where we should have the code for our two 
new functions.  the lower left pane shows our r console with 
the results of the most recently run commands. the upper 
right pane contains our workspace and history of prior com-
mands, with the tab currently set to workspace. as a re-
minder, in r parlance, workspace represents all of the cur-
rently available data objects include functions. our two new 
functions which we have de   ned should be listed there, indi-
cating that they have each run at least once and r is now 
aware of them. in the lower right pane, we have    les, plots, 
packages, and help, with the tab currently set to packages. 

this window is scrolled to the bottom to show that rcurl, 
rjsonio, and twitter are all loaded and "libraryed" meaning 
that they are ready to use from the command line or from 
functions.
getting new ssl tokens on windows
for windows users, depending upon which version of operat-
ing system software you are using as well as your upgrade 
history, it may be necessary to provide new ssl certi   cates. 
certi   cates help to maintain secure communications across 
the internet, and most computers keep an up-to-date copy on 
   le, but not all of them do. if you encounter any problems us-
ing r to access the internet, you may need new tokens.
download.file(url="http://curl.haxx.se/ca/cacert.pem",+
destfile="cacert.pem")
this statement needs to be run before the r tries to contact 
twitter for authentication. this is because twitter uses rcurl 
which in turn employs ssl security whenever    https    ap-
pears in a url. the command above downloads new certi   -
cates and saves them within the current working directory 
for r. you may need to use cacert.pem for many or most of 
the function calls to twitter by adding the argument 
cainfo="cacert.pem".
using your oauth tokens
remember at the beginning of the chapter that we went 
through some rigamarole to get a consumer key and a con-
sumer secret from twitter. before we can get started in retriev-

81

ing data from twitter we need to put those long strings of 
numbers and letters to use. 
begin this process by getting a credential from roauth. re-
member that in the command below where i have put "letter-
sandnumbers" you have to substitute in your consumerkey 
and your consumersecret that you got from twitter. the con-
sumerkey is a string of upper and lowercase letters and dig-
its about 22 characters long. the consumersecret is also let-
ters and digits and it is about twice as long as the con-
sumerkey. make sure to keep these private, especially the 
consumersecret, and don   t share them with others. here   s 
the command:
> credential <- 
oauthfactory$new(consumerkey="lettersandnumbers",   
+cons
umersecret="lettersandnumbers",   
+requesturl="https://api.twitter.com/oauth/request_t
oken",   
+accessurl="https://api.twitter.com/oauth/access_tok
en",   
+authurl="https://api.twitter.com/oauth/authorize")
this looks messy but is really very simple. if you now type:
> credential
you will    nd that the credential data object is just a conglom-
eration of the various    elds that you speci   ed in the argu-
ments to the oauthfactory$new method. we have to put that 
data structure to work now  with the following function call:
> credential$handshake()

or, on windows machines, if you have downloaded new cer-
ti   cates:
> credential$handshake(cainfo="cacert.pem")
you will get a response back that looks like this:
when complete, record the pin given to you and provide it 
here:
to enable the connection, please direct your web browser to: 
https://api.twitter.com/oauth/authorize?oauth_token=...
when complete, record the pin given to you and provide it here:
this will be followed by a long string of numbers. weirdly, 
you have to go to a web browser and type in exactly what 
you see in the r-studio output window (the url and the 
long string of numbers). while typing the url to be redi-
rected to twitter, be sure that you type http:// instead of 
https:// otherwise twitter will not entertain the request be-
cause the twitter server invokes ssl security itself. if you 
type the url correctly, twitter will respond in your browser 
window with a big button that says "authorize app." go 
ahead and click on that and you will receive a new screen 
with a pin on it (my pin had seven digits). take those seven 
digits and type them into the r-studio console window (the 
credential$handshake() function will be waiting for them). 
type the digits in front of    when complete, record the pin 
given to you and provide it here:    hit enter and, assuming 
you get no errors, you are fully authorized! hooray! what a 
crazy process! thankfully, you should not have to do any of 

82

this again as long as you save the credential data object and 
restore it into future sessions. the credential object, and all of 
the other active data, will be stored in the default workspace 
when you exit r or r-studio. make sure you know which 
workspace it was saved in so you can get it back later.
ready, set, go!
now let   s get some data from twitter. first, tell the twitter 
package that you want to use your shiny new credentials:
> registertwitteroauth(credential)
[1] true
the return value of true shows that the credential is work-
ing and ready to help you get data from twitter. subsequent 
commands using the twitter package will pass through the 
authorized application interface.
the twitter package provides a function called searchtwit-
ter() that allows us to retrieve some recent tweets based on a 
search term. twitter users have invented a scheme for organ-
izing their tweets based on subject matter. this system is 
called "hashtags" and is based on the use of the hashmark 
character (#) followed by a brief text tag. for example, fans of 
oprah winfrey use the tag #oprah to identify their tweets 
about her. we will use the searchtwitter() function to search 
for hashtags about global climate change. the website 
hashtags.org lists a variety of hashtags covering a range of 
contemporary topics. you can pick any hashtag you like, as 
long as there are a reasonable number of tweets that can be 
retrieved. the searchtwitter() function also requires specify-

ing the maximum number of tweets that the call will return. 
for now we will use 500, although you may    nd that your re-
quest does not return that many. here   s the command:
tweetlist <- searchtwitter("#climate", n=500)
as above, if you are on windows, and you had to get new cer-
ti   cates, you may have to use this command:
tweetlist <- searchtwitter("#climate", n=500, 
cainfo="cacert.pem")
depending upon the speed of your internet connection and 
the amount of traf   c on twitter   s servers, this command may 
take a short while for r to process. now we have a new data 
object, tweetlist, that presumably contains the tweets we re-
quested. but what is this data object? let   s use our r diagnos-
tics to explore what we have gotten:
> mode(tweetlist)
[1] "list"
id48, this is a type of object that we have not encountered 
before. in r, a list is an object that contains other data objects, 
and those objects may be a variety of different modes/types. 
contrast this de   nition with a vector: a vector is also a kind 
of list, but with the requirement that all of the elements in the 
vector must be in the same mode/type. actually, if you dig 
deeply into the de   nitions of r data objects, you may realize 
that we have already encountered one type of list: the data-
frame. remember that the dataframe is a list of vectors, 
where each vector is exactly the same length. so a dataframe 
is a particular kind of list, but in general lists do not have 

83

those two restrictions that dataframes have (i.e., that each ele-
ment is a vector and that each vector is the same length).
so we know that tweetlist is a list, but what does that list con-
tain? let   s try using the str() function to uncover the structure 
of the list:
str(tweetlist)
whoa! that output scrolled right off the screen. a quick 
glance shows that it is pretty repetitive, with each 20 line 
block being quite similar. so let   s use the head() function to 
just examine the    rst element of the list. the head() function 
allows you to just look at the    rst few elements of a data ob-
ject. in this case we will look just at the    rst list element of the 
tweetlist list. the command, also shown on the screen shot 
below is:
str(head(tweetlist,1))
looks pretty messy, but is simpler than it may    rst appear. 
following the line "list of 1," there is a line that begins "$ :ref-
erence class" and then the word    status    in single quotes. in 
twitter terminology a "status" is a single tweet posting (it sup-
posedly tells us the "status" of the person who posted it). so 
the author of the r twitter package has created a new kind of 
data object, called a    status    that itself contains 10    elds. the 
   elds are then listed out. for each line that begins with "..$" 
there is a    eld name and then a mode or data type and then a 
taste of the data that that    eld contains. 
so, for example, the    rst    eld, called "text" is of type "chr" 
(which means character/text data) and the    eld contains the 

string that starts with, "get the real facts on gas prices." you 
can look through the other    elds and see if you can make 
sense of them. there are two other data types in there: "logi" 
stands for logical and that is the same as true/false; "po-
sixct" is a format for storing the calendar date and time. (if 
you   re curious, posix is an old unix style operating system, 
where the current date and time were stored as the number of 
seconds elapsed since 12 midnight on january 1, 1970.) you 
can see in the "created"    eld that this particular tweet was cre-
ated on april 5, 2012 one second after 2:10 pm. it does not 

84

show what time zone, but a little detective work shows that 
all twitter postings are coded with "coordinated universal 
time" or what is usually abbreviated with utc. 
one last thing to peek at in this data structure is about seven 
lines from the end, where it says, "and 33 methods..." in com-
puter science lingo a "method" is an operation/activity/
procedure that works on a particular data object. the idea of 
a method is at the heart of so called "object oriented program-
ming." one way to think of it is that the data object is the 
noun, and the methods are all of the verbs that work with 
that noun. for example you can see the method "getcreated" 
in the list: if you use the method getcreated on an reference 
object of class    status   , the method will return the creation 
time of the tweet. 
if you try running the command: 
str(head(tweetlist,2))
you will    nd that the second item in the tweetlist list is struc-
tured exactly like the    rst time, with the only difference being 
the speci   c contents of the    elds. you can also run:
length(tweetlist)
to    nd out how many items are in your list. the list obtained 
for this exercise was a full 500 items long. se we have 500 
complex items in our list, but every item had exactly the same 
structure, with 10    elds in it and a bunch of other stuff too. 
that raises a thought: tweetlist could be thought of as a 500 
row structure with 10 columns! that means that we could 
treat it as a dataframe if we wanted to (and we do, because 

this makes handling these data much more convenient as you 
found in the "rows and columns" chapter). 
happily, we can get some help from r in converting this list 
into a dataframe. here we will introduce four powerful new 
r functions: as(), lapply(), rbind(), and do.call(). the    rst of 
these, as(), performs a type coercion: in other words it 
changes one type to another type. the second of these, lap-
ply(), applies a function onto all of the elements of a list. in 
the command below, lapply(tweetlist, as.data.frame), applies 
the as.data.frame() coercion to each element in tweetlist. 
next, the rbind() function "binds" together the elements that 
are supplied to it into a row-by-row structure. finally, the 
do.call() function executes a function call, but unlike just run-
ning the function from the console, allows for a variable num-
ber of arguments to be supplied to the function. the whole 
command we will use looks like this:
tweetdf <- do.call("rbind", lapply(tweetlist, 
+   
                    as.data.frame))
you might wonder a few things about this command. one 
thing that looks weird is "rbind" in double quotes. this is the 
required method of supplying the name of the function to 
do.call(). you might also wonder why we needed do.call() at 
all. couldn   t we have just called rbind() directly from the com-
mand line? you can try it if you want, and you will    nd that 
it does provide a result, but not the one you want. the differ-
ence is in how the arguments to rbind() are supplied to it: if 
you call it directly, lapply() is evaluated    rst, and it forms a 
single list that is then supplied to rbind(). in contrast, by us-

85

ing do.call(), all 500 of the results of lapply() are supplied to 
rbind() as individual arguments, and this allows rbind() to 
create the nice rectangular dataset that we will need. the ad-
vantage of do.call() is that it will set up a function call with a 
variable number of arguments in cases where we don   t know 
how many arguments will be supplied at the time when we 
write the code. 
if you run the command above, you should see in the upper 
right hand pane of r-studio a new entry in the workspace un-
der the heading of "data." for the example we are running 
here, the entry says, "500 obs. of 10 variables." this is just 
what we wanted, a nice rectangular data set, ready to ana-
lyze. later on, we may need more than one of these data sets, 
so let   s create a function to accomplish the commands we just 
ran:
# tweetframe() - return a dataframe based on 
a   
#                search of twitter
tweetframe<-function(searchterm, maxtweets)
{
  twtlist<-
searchtwitter(searchterm,n=maxtweets)  
  return(do.call("rbind",+   
                
lapply(twtlist,as.data.frame)))
}
there are three good things about putting this code in a func-
tion. first, because we put a comment at the top of the func-

tion, we will remember in the future what this code does. sec-
ond, if you test this function you will    nd out that the vari-
able twtlist that is created in the code above does not stick 
around after the function is    nished running. this is the re-
sult of what computer scientists call "variable scoping." the 
variable twtlist only exists while the tweetframe() function 
is running. once the function is done, twtlist evaporates as if 
it never existed. this helps us to keep our workspace clean 
and avoid collecting lots of intermediate variables  that are 
not reused.
the last and best thing about this function is that we no 
longer have to remember the details of the method for using 
do.call(), rbind(), lapply(), and as.data.frame() because we 
will not have to retype these commands again: we can just 
call the function whenever we need it. and we can always go 
back and look at the code later. in fact, this would be a good 
reason to put in a comment just above the return() function. 
something like this:
# as.data.frame() coerces each list element into a row   
# lapply() applies this to all of the elements in twtlist   
# rbind() takes all of the rows and puts them together   
# do.call() gives rbind() all the rows as individual elements
now, whenever we want to create a new data set of tweets, 
we can just call tweetframe from the r console command 
line like this:
lgdata <- tweetframe("#ladygaga", 250)

86

starts at about 4:20 am and goes until about 10:10 am, a span 
of roughly six hours.  there are 22 different bars so each bar 
represents about 16 minutes - for casual purposes we   ll call it 
a quarter of an hour. it looks like there are something like 20 
tweets per bar, so we are looking at roughly 80 tweets per 
hour with the hashtag "#climate." this is obviously a pretty 
popular topic. this distribution does not really have a dis-
cernible shape, although it seems like there might be a bit of a 
growth trend as time goes on, particularly starting at about 
7:40 am.

this command would give us a new dataframe "lgdata" all 
ready to analyze, based on the supplied search term and maxi-
mum number of tweets.
let   s start to play with the tweetdf dataset that we created 
before. first, as a matter of convenience, let   s learn the at-
tach() function. the attach() function saves us some typing by 
giving one particular dataframe priority over any others that 
have the same variable names. normally, if we wanted to ac-
cess the variables in our dataframe, we would have to use the 
$ notation, like this:
tweetdf$created
but if we run attach(tweetdf)    rst, we can then refer to cre-
ated directly, without having to type the tweetdf$ before it:
> attach(tweetdf)
> head(created,4)
[1] "2012-04-05 14:10:01 utc" "2012-04-05 14:09:21 
utc"
[3] "2012-04-05 14:08:15 utc" "2012-04-05 14:07:12 
utc"
let   s visualize the creation time of the 500 tweets in our data-
set. when working with time codes, the hist() function re-
quires us to specify the approximate number of categories we 
want to see in the histogram:
hist(created, breaks=15, freq=true)
this command yields the histogram that appears below. if we 
look along the x-axis (the horizontal), this string of tweets 

87

take note of something very important about these data: it 
doesn   t make much sense to work with a measure of central 
tendency. remember a couple of chapters ago when we were 
looking at the number of people who resided in different u.s. 
states? in that case it made sense to say that if state a had one 
million people and state b had three million people, then the 
average of these two states was two million people. when 
you   re working with time stamps, it doesn   t make a whole lot 
of sense to say that one tweet arrived at 7 am and another ar-
rived at 9 am so the average is 8 am. fortunately, there   s a 
whole area of statistics concerned with "arrival" times and 
similar phenomena, dating back to a famous study by ladis-
laus von bortkiewicz of horsemen who died after being 
kicked by their horses. von bortkiewicz studied each of 14 
cavalry corps over a period of 20 years, noting when horse-
men died each year. the distribution of the "arrival" of kick-
deaths turns out to have many similarities to other arrival 
time data, such as the arrival of buses or subway cars at a sta-
tion, the arrival of customers at a cash register, or the occur-
rence of telephone calls at a particular exchange. all of these 
kinds of events    t what is known as a "poisson distribution" 
(named after simeon denis poisson, who published it about 
half a century before von bortkiewicz found a use for it). 
let   s    nd out if the arrival times of tweets comprise a poisson 
distribution. 
right now we have the actual times when the tweets were 
posted, coded as a posix date and time variable. another 
way to think about these data is to think of each new tweet as 
arriving a certain amount of time after the previous tweet. to 
   gure that out, we   re going to have to "look back" a row in or-

der to subtract the creation time of the previous tweet from 
the creation time of the current tweet. in order to be able to 
make this calculation, we have to make sure that our data are 
sorted in ascending order of arrival - in other words the earli-
est one    rst and the latest one last. to accomplish this, we 
will use the order() function together with r   s built-in square 
bracket notation. 
as mentioned brie   y in the previous chapter, in r, square 
brackets allow "indexing" into a list, vector, or data frame. for 
example, mylist[3] would give us the third element of myl-
ist. keeping in mind that a a dataframe is a rectangular struc-
ture, really a two dimensional structure, we can address any 
element of a dataframe with both a row and column designa-
tor: myframe[4,1] would give the fourth row and the    rst col-
umn. a shorthand for taking the whole column of a data-
frame is to leave the row index empty: myframe[ , 6] would 
give every row in the sixth column. likewise, a shorthand for 
taking a whole row of a dataframe is to leave the column in-
dex empty: myframe[10, ] would give every column in the 
tenth row. we can also supply a list of rows instead of just 
one row, like this: myframe[ c(1,3,5), ] would return rows 1, 
3, 5 (including the data for all columns, because we left the 
column index blank). we can use this feature to reorder the 
rows, using the order() function. we tell order() which vari-
able we want to sort on, and it will give back a list of row indi-
ces in the order we requested. putting it all together yields 
this command:
 tweetdf[order(as.integer(created)), ]

88

technically, what we have with our created variable now is a 
time series, and because statisticians like to have convenient 
methods for dealing with time series, r has a built-in func-
tion, called diff(), that allows us to easily calculate the differ-
ence in seconds between each pair of neighboring values. try 
it:
diff(created)
you should get a list of time differences, in seconds, between 
neighboring tweets. the list will show quite a wide range of 
intervals, perhaps as long as several minutes, but with many 
intervals near or at zero. you  might notice that there are only 
499 values and not 500: this is because you cannot calculate a 
time difference for the very    rst tweet, because we have no 
data on the prior tweet. let   s visualize these data and see 
what we   ve got:
hist(as.integer(diff(created)))

working our way from the inside to the outside of the expres-
sion above, we want to sort in the order that the tweets were 
created. we    rst coerce the variable "created" to integer - it 
will then truly be expressed in the number of seconds since 
1970 - just in case there are operating system differences in 
how posix dates are sorted. we wrap this inside the order() 
function. the order() function will provide a list of row indi-
ces that re   ects the time ordering we want. we use the square 
brackets notation to address the rows in tweetdf, taking all of 
the columns by leaving the index after the comma empty. 
we have a choice of what to do with the dataframe that is re-
turned from this command. we could assign it back to 
tweetdf, which would overwrite our original dataframe with 
the sorted version. or we could create a new sorted data-
frame and leave the original data alone, like so:
sortweetdf<-tweetdf[order(as.integer(created)), ]
if you choose this method, make sure to detach() tweetdf 
and attach() sortweetdf so that later commands will work 
smoothly with the sorted dataframe:
> detach(tweetdf)
> attach(sortweetdf)
another option, which seems better than creating a new data-
frame, would be to build the sorting into the tweetframe() 
function that we developed at the beginning of the chapter. 
let   s leave that to the chapter challenge. for now, we can 
keep working with sortweetdf.

89

[1] 41.12826
we have to be careful though, in using measures of central 
tendency on this positively skewed distribution, that the 
value we get from the mean() is a sensible representation of 
central tendency. remembering back to the previous chapter, 
and our discussion of the statistical mode (the most fre-
quently occurring value), we learn that the mean and the 
mode are very different:
> library("modeest")
> mfv(as.integer(diff(created)))
[1] 0
we use the library() function to make sure that the add on 
package with the mfv() function is ready to use. the results of 
the mfv() function show that the most commonly occurring 
time interval between neighboring tweets is zero!
likewise the median shows that half of the tweets have arri-
val times of under half a minute:
> median(as.integer(diff(created)))
[1] 28
in the next chapter we will delve more deeply into what it 
means when a set of data are shaped like a poisson distribu-
tion and what that implies about making use of the mean.
one last way of looking at these data before we close this 
chapter. if we choose a time interval, such as 10 seconds, or 
30 seconds, or 60 seconds, we can ask the question of how 

90

as with earlier commands, we use as.integer() to coerce the 
time differences into plain numbers, otherwise hist() does not 
know how to handle the time differences. this histogram 
shows that the majority of tweets in this group come within 
50 seconds or less of the previous tweets. a much smaller 
number of tweets arrive within somewhere between 50 and 
100 seconds, and so on down the line. this is typical of a pois-
son arrival time distribution. unlike the raw arrival time 
data, we could calculate a mean on the time differences:
> mean(as.integer(diff(created)))

many of our tweet arrivals occurred within that time interval. 
here   s code that counts the number of arrivals that occur 
within certain time intervals:
> sum((as.integer(diff(created)))<60)
[1] 375
> sum((as.integer(diff(created)))<30)
[1] 257
> sum((as.integer(diff(created)))<10)
[1] 145
you could also think of these as ratios, for example 145/500 = 
0.29. and where we have a ratio, we often can think about it 
as a id203: there is a 29% id203 that the next tweet 
will arrive in 10 seconds or less. you could make a function to 
create a whole list of these probabilities. some sample code 
for such a function appears at the end of the chapter. some 
new scripting skills that we have not yet covered (for exam-
ple, the "for loop") appear in this function, but try making 
sense out of it to stretch your brain. output from this function 
created the plot that appears below.

this is a classic poisson distribution of arrival probabilities. 
the x-axis contains 10 second intervals (so by the time you 
see the number 5 on the x-axis, we are already up to 50 sec-
onds). this is called a cumulative id203 plot and you 
read it by talking about the id203 that the next tweet 
will arrive in the amount of time indicated on the x-axis or 
less. for example, the number    ve on the x-axis corresponds 
to about a 60% id203 on the y-axis, so there is a 60% 
id203 that the next tweet will arrive in 50 seconds or 
less. remember that this estimate applies only to the data in 
this sample!

91

sources
http://cran.r-project.org/web/packages/twitter/twitter.pdf
http://cran.r-project.org/web/packages/twitter/vignettes/twitte
r.pdf 
http://en.wikipedia.org/wiki/ladislaus_bortkiewicz 
http://en.wikipedia.org/wiki/poisson_distribution 
http://hashtags.org/ 
http://www.inside-r.org/packages/cran/twitter/docs/example
oauth 
http://www.khanacademy.org/math/id203/v/poisson-proc
ess-1
http://www.khanacademy.org/math/id203/v/poisson-proc
ess-2
https://support.twitter.com/articles/49309 (hashtags explained)
http://www.rdatamining.com/examples/text-mining 

in the next chapter we will reexamine sampling in the context 
of poisson and learn how to compare two poisson distribu-
tions to    nd out which hashtag is more popular.
let   s recap what we learned from this chapter. first, we have 
begun to use the project features of r-studio to establish a 
clean environment for each r project that we build. second, 
we used the source code window of r-studio to build two or 
three very useful functions, ones that we will reuse in future 
chapters. third, we practiced the skill of installing packages 
to extend the capabilities of r. speci   cally, we loaded jeff 
gentry   s twitter package and the other three packages it de-
pends upon. fourth, we put the twitter package to work to 
obtain our own fresh data right from the web.  fifth, we 
started to condition that data, for example by creating a 
sorted list of tweet arrival times. and    nally, we started to 
analyze and visualize those data, by conjecturing that this 
sample of arrival times    tted the classic poisson distribution.
chapter challenge
modify the tweetframe() function created at the beginning of 
this chapter to sort the dataframe based on the creation time 
of the tweets. this will require taking the line of code from a 
few pages ago that has the order() function in it and adding 
this to the tweetframe() function with a few minor modi   ca-
tions. here   s a hint: create a temporary dataframe inside the 
function and don   t attach it while you   re working with it. 
you   ll need to use the $ notation to access the variable you 
want to use to order the rows.

92

r script - create vector of probabilities from arrival times
# arrivalid203 - given a list of arrival times
# calculates the delays between them using lagged differences
# then computes a list of cumulative probabilities of arrival
# for the sequential list of time increments
# times - a sorted, ascending list of arrival times in posixct
# increment - the time increment for each new slot, e.g. 10 sec
# max - the highest time increment, e.g., 240 sec
#
# returns - an ordered list of probabilities in a numeric vector
# suitable for plotting with plot()
arrivalid203<-function(times, increment, max)
{
  # initialize an empty vector
  plist <- null
  
  # id203 is defined over the size of this sample
  # of arrival times
  timelen <- length(times)
  
  # may not be necessary, but checks for input mistake
  if (increment>max) {return(null)}
  
  for (i in seq(increment, max, by=increment))
  {
    # diff() requires a sorted list of times

    # diff() calculates the delays between neighboring times
    # the logical test <i provides a list of trues and falses
    # of length = timelen, then sum() counts the trues.
    # divide by timelen to calculate a proportion
    plist<-c(plist,(sum(as.integer(diff(times))<i))/timelen)
  }
  return(plist)
}

r functions used in this chapter
attach() - makes the variables of a dataset available without $ 
as.integer() - coerces data into integers
detach() - undoes an attach function
diff() - calculates differences between neighboring rows
do.call() - calls a function with a variable number of arguments
function() - de   nes a function for later use
hist() - plots a histogram from a list of data
install.packages() - downloads and prepares a package for use
lapply() - applies a function to a list
library() - loads a package for use; like require()
mean() - calculates the arithmetic mean of a vector
median() - finds the statistical center point of a list of numbers 

93

mfv() - most frequent value; part of the modeest() package
mode() - shows the basic data type of an object
order() - returns a sorted list of index numbers 
rbind() - binds rows into a dataframe object
require() - tests if a package is loaded and loads it if needed
searchtwitter() - part of the twitter package
str() - describes the structure of a data object
sum() - adds up a list of numbers

94

chapter 11

popularity contest

in the previous chapter we found that arrival times of tweets on a given topic seem to    t a poisson 
distribution. armed with that knowledge we can now develop a test to compare two different twitter 
topics to see which one is more popular (or at least which one has a higher posting rate). we will use 
our knowledge of sampling distributions to understand the logic of the test.

95

which topic on twitter is more popular, lady gaga or oprah win-
frey? this may not seem like an important question, depending 
upon your view of popular culture, but if we can make the com-
parison for these two topics, we can make it for any two topics. cer-
tainly in the case of presidential elections, or a corruption scandal 
in the local news, or an international crisis, it could be a worth-
while goal to be able to analyze social media in a systematic way.  
and on the surface, the answer to the question seems trivial: just 
add up who has more tweets. surprisingly, in order to answer the 
question in an accurate and reliable way, this won   t work, at least 
not very well. instead, one must consider many of the vexing ques-
tions that made inferential statistics necessary. 
let   s say we retrieved one hour   s worth of lady gaga tweets and a 
similar amount of oprah winfrey tweets and just counted them 
up. what if it just happened to be a slow news day for oprah? it 
really wouldn   t be a fair comparison. what if most of lady gaga   s 
tweets happen at midnight or on saturdays? we could expand our 
sampling time, maybe to a day or a week. this could certainly 
help: generally speaking, the bigger the sample, the more represen-
tative it is of the whole population, assuming it is not collected in a 
biased way. this approach de   nes popularity as the number of 
tweets over a    xed period of time. its success depends upon the 
choice of a suf   ciently large period of time, that the tweets are col-
lected for the two topics at the same time, and that the span of time 
chosen happens to be equally favorable for both two topics.
another approach to the popularity comparison would build upon 
what we learned in the previous chapter about how arrival times 
(and the delays between them)    t into the poisson distribution. in 
this alternative de   nition of the popularity of a topic, we could sug-

gest that if the arrival curve is "steeper" for the    rst topic in con-
trast to the second topic, then the    rst topic is more active and 
therefore more popular. another way of saying the same thing is 
that for the more popular topic, the likely delay until the arrival of 
the next tweet is shorter than for the less popular topic. you could 
also say that for a given interval of time, say ten minutes, the num-
ber of arrivals for the    rst topic would be higher than for the sec-
ond topic. assuming that the arrival delays    t a poisson distribu-
tion, these are all equivalent ways of capturing the comparison be-
tween the two topics. 
just as we did in the chapter entitled, "sample in a jar," we can use 
a random number generator in r to illustrate these kinds of differ-
ences more concretely. the relevant function for the poisson distri-
bution is rpois(), "random poisson." the rpois() function will gener-
ate a stream of random numbers that roughly    t the poisson distri-
bution. the    t gets better as you ask for a larger and larger sample.  
the    rst argument to rpois() is how many random numbers you 
want to generate and the second number is the average delay be-
tween arrivals that you want the random number generator to try 
to come close to. we can look at a few of these numbers and then 
use a histogram function to visualize the results: 
> rpois(10,3)
 [1] 5 4 4 2 0 3 6 2 3 3
> mean(rpois(100,3))
[1] 2.99
> var(rpois(100,3))
[1] 3.028182

96

> hist(rpois(1000,3))

in the    rst command above, we generate a small sample of n=10 
arrival delays, with a hoped for mean of 3 seconds of delay, just to 
see what kind of numbers we get. you can see that all of the num-
bers are small integers, ranging from 0 to 6. in the second com-
mand we double check these results with a slightly larger sample 
of n=100 to see if rpois() will hit the mean we asked for. in that run 
it came out to 2.99, which was pretty darned close. if you run this 
command yourself you will    nd that your result will vary a bit 
each time: it will sometimes be slightly larger than three and occa-
sionally a little less than three (or whatever mean you specify). 

97

this is normal, because of the random number generator. in the 
third command we run yet another sample of 100 random data 
points, this time analyzing them with the var() function (which cal-
culates the variance; see the chapter entitled "beer, farms, and 
peas"). it is a curious fact of poission distributions that the mean 
and the variance of the "ideal" (i.e., the theoretical) distribution are 
the same. in practice, for a small sample, they may be different.
in the    nal command, we ask for a histogram of an even larger 
sample of n=1000. the histogram shows the most common value 
hanging right around three seconds of delay with a nice tail that 
points rightwards and out to about 10 seconds of delay. you can 
think of this as one possible example of what you might observe of 
the average delay time between tweets was about three seconds. 
note how similar the shape of this histogram is to what we ob-
served with real tweets in the last chapter.
compare the histogram on the previous page to the one on the 
next page that was generated with this command:
hist(rpois(1000,10))
it is pretty easy to see the different shape and position of this histo-
gram, which has a mean arrival delay of about ten seconds. first of 
all, there are not nearly as many zero length delays. secondly, the 
most frequent value is now about 10 (as opposed to two in the pre-
vious histogram). finally, the longest delay is now over 20 seconds 
(instead of 10 for the previous histogram).  one other thing to try is 
this:
> sum(rpois(1000,10)<=10)
[1] 597

this command generated 1000 new random numbers, following 
the poisson distribution and also with a hoped-for mean of 10, just 
like in the histogram on the next page. using the "<=" inequality 
test and the sum() function, we then counted up how many events 
were less than or equal to 12, and this turned out to be 597 events. 
as a fraction of the total of n=1000 data points that rpois() gener-
ated, that is 0.597, or 59.7%.  

review 11.1 popularity contest (mid-chapter review)

question 1 of  4
the poisson distribution has a characteristic shape that 
would be described as:

a. negatively (left) skewed
b. positively (right) skewed
c. symmetric (not skewed)
d. none of the above

98

check answerwe can look at the same kind of data in terms of the id203 of 
arrival within a certain amount of time. because rpois() generates 
delay times directly (rather than us having to calculate them from 
neighboring arrival times), we will need a slightly different func-
tion than the arrivalprobabilities() that we wrote and used in the 
previous chapter. we   ll call this function "delayid203" (the 
code is at the end of this chapter):
> delayid203(rpois(100,10),1,20)
 [1] 0.00 0.00 0.00 0.03 0.06 0.09 0.21 0.33 0.48 
0.61 0.73 0.82 0.92
[14] 0.96 0.97 0.98 0.99 1.00 1.00 1.00
at the heart of that command is the rpois() function, requesting 
100 points with a mean of 10. the other two parameters are the in-
crement, in this case one second, and the maximum delay time, in 
this case 20 seconds. the output from this function is a sorted list 
of cumulative probabilities for the times ranging from 1 second to 
20 seconds. of course, what we would really like to do is compare 
these probabilities to those we would get if the average delay was 
three seconds instead of ten seconds. we   re going to use two cool 
tricks for creating this next plot. first, we will use the points() com-
mand to add points to an existing plot. second, we will use the 
col= parameter to specify two different colors for the points that 
we plot. here   s the code that creates a plot and then adds more 
points to it:
> plot(delayid203(rpois(100,10),1,20), col=2)
> points(delayid203(rpois(100,3),1,20), col=3)

again, the heart of each of these lines of code is the rpois() function 
that is generating random poisson arrival delays for us. our pa-
rameters for increment (1 second) and maximum (20 seconds) are 
the same for both lines. the    rst line uses col=2, which gives us red 
points, and the second gives us col=3, which yields green points:

 

this plot clearly shows that the green points have a "steeper" pro-
   le. we are more likely to have earlier arrivals for the 3-second de-
lay data than we are for the 10-second data. if these were real 
tweets, the green tweets would be piling in much faster than the 
red tweets. here   s a reminder on how to read this plot: look at a 
value on the x-axis, for example "5." then look where the dot is 

99

and trace leftward to the y-axis. for the red dot, the id203 
value at time (x) equal 4 is about 0.10. so for the red data there is 
about a 10% chance that the next event will occur within    ve time 
units (we   ve been calling them seconds, but they could really be 
anything, as long as you use the units consistently throughout the 
whole example). for the green data there is about a 85% chance 
that the next event will occur within four time units. the fact that 
the green curve rises more steeply than the red curve means that 
for these two samples only the green stuff is arriving much more often 
than the red stuff. 
these reason we emphasized the point "for these samples only" is 
that we know from prior chapters that every sample of data you 
collect varies by at least a little bit and sometimes by quite a lot. a 
sample is just a snapshot, after all, and things can and do change 
from sample to sample. we can illustrate this by running and plot-
ting multiple samples, much as we did in the earlier chapter:
> plot(delayid203(rpois(100,10),1,20))
> for (i in 1:15) {points(delayid203(r-
pois(100,10),1,20))}
this is the    rst time we have used the "for loop" in r, so let   s walk 
through it. a "for loop" is one of the basic constructions that com-
puter scientists use to "iterate" or repeatedly run a chunk of code. 
in r, a for loop runs the code that is between the curly braces a cer-
tain number of times. the number of times r runs the code de-
pends on the expression inside the parentheses that immediately 
follow the "for." 
in the example above, the expression "i in 1:15" creates a new data 
object, called i, and then puts the number 1 in it. then, the for loop 

keeps adding one to the value of i, until i reaches 15. each time that 
it does this, it runs the code between the curly braces. the expres-
sion "in 1:15" tells r to start with one and count up to 15. the data 
object i, which is just a plain old integer, could also have been used 
within the curly braces if we had needed it, but it doesn   t have to 
be used within the curly braces if it is not needed. in this case we 
didn   t need it. the code inside the curly braces just runs a new ran-
dom sample of 100 poisson points with a hoped for mean of 10.

when you consider the two command lines on the previous page 
together you can see that we initiate a plot() on the    rst line of 

100

code, using similar parameters to before (random poisson numbers 
with a mean of 10, fed into our id203 calculator, which goes 
in increments of 1 second up to 20 seconds). in the second line we 
add more points to the same plot, by running exactly 15 additional 
copies of the same code. using rpois() ensures that we have new 
random numbers each time:
now instead of just one smooth curve we have a bunch of curves, 
and that these curves vary quite a lot. in fact, if we take the exam-
ple of 10 seconds (on the x-axis), we can see that in one case the 
id203 of a new event in 10 seconds could be as low as 0.50, 
while in another case the id203 is as high as about 0.70. 
this shows why we can   t just rely on one sample for making our 
judgments. we need to know something about the uncertainty that 
surrounds a given sample. fortunately, r gives us additional tools 
to help us    gure this situation out. first of all, even though we had 
loads of fun programming the delayid203() function, there is 
a quicker way to get information about what we ideally expect 
from a poisson distribution. the function ppois() gives us the theo-
retical id203 of observing a certain delay time, given a particu-
lar mean. for example:
> ppois(3, lambda=10)
[1] 0.01033605
so you can read this as: there is a 1% chance of observing a delay 
of 3 or less in a poisson distribution with mean equal to 10. note 
that in statistical terminology, "lambda" is the term used for the 
mean of a poisson distribution. we   ve provided the named parame-
ter "lambda=10" in the example above just to make sure that r 
does not get confused about what parameter we are controlling 

when we say "10." the ppois() function does have other parame-
ters that we have not used here. now, using a for loop, we could 
get a list of several of these theoretical probabilities:
> plot(1,20,xlim=c(0,20),ylim=c(0,1))
> for (i in 1:20) {points(i,ppois(i,lambda=10)) }
we are using a little code trick in the    rst command line above by 
creating a nearly empty set of axes with the plot() function, and 
then    lling in the points in the second line using the points() func-
tion. this gives the following plot:

you may notice that this plot looks a lot like the ones earlier in this 

101

chapter as well as somewhat similar to the id203 plot in the 
previous chapter. when we say the "theoretical distribution" we 
are talking about the ideal poisson distribution that would be gen-
erated by the complex equation that mr. poisson invented a couple 
of centuries ago. another way to think about it is this: instead of 
just having a small sample of points, which we know has a lot of 
randomness in it, what if we had a truly humongous sample with 
zillions of data points? the curve in the plot above is just about 
what we would observe for a truly humongous sample (where 
most of the biases up or down cancel themselves out because the 
large number of points). 
so this is the ideal, based on the mathematical theory of the pois-
son distribution, or what we would be likely to observe if we cre-
ated a really large sample. we know that real samples, of reason-
able amounts of data, like 100 points or 1000 points or even 10,000 
points, will not hit the ideal exactly, because some samples will 
come out a little higher and others a little lower. 
we also know, from the histograms and output earlier in the chap-
ter, that we can look at the mean of a sample, or the count of events 
less than or equal to the mean, or the arrival probabilities in the 
graph on this page, and in each case we are looking at different versions 
of the same information. check out these    ve commands:
> mean(rpois(100000,10))
[1] 10.01009
> var(rpois(100000,10))
[1] 10.02214
> sum(rpois(100000,10)<=10)/100000

[1] 0.58638
> ppois(10,lambda=10)
[1] 0.58303
> qpois(0.58303,lambda=10)
[1] 10
in the    rst command, we con   rm that for a very large random sam-
ple of n=100,000 with a desired mean of 10, the actual mean of the 
random sample is almost exactly 10. likewise, for another large 
random sample with a desired mean of 10, the variance is 10. in 
the next command, we use the inequality test and the sum() func-
tion again to learn that the id203 of observing a value of 10 or 
less in a very large sample is about 0.59 (note that the sum() func-
tion yielded 58,638 and we divided by 100,000 to get the reported 
value of 0.58638). likewise, when we ask for the theoretical distri-
bution with ppois() of observing 10 or less in a sample with a mean 
of 10, we get a id203 of 0.58303, which is darned close to the 
empirical result from the previous command. finally, if we ask 
qpois() what is the threshold value for a id203 of 0.58303 is in a 
poisson sample with mean of 10, we get back the answer: 10. you 
may see that qpois() does the reverse of what ppois() does. for fun, 
try this formula on the r command line:    
!
here   s one last point to cap off this thinking. even with a sample of 
100,000 there is some variation in samples. that   s why the 0.58638 
from the sum() function above does not exactly match the theoreti-
cal 0.58303 from the ppois() function above. we can ask r to tell us 
how much variation there is around one of these probabilities us-
ing the poisson.test() function like this:

qpois(ppois(10, lambda=10), lambda=10)

102

> poisson.test(58638, 100000)
95 percent confidence interval:
 0.5816434 0.5911456 
we   ve truncated a little of the output in the interests of space: what 
you have left is the upper and lower bounds on a 95% con   dence 
interval. here   s what a con   dence interval is: for 95% of the sam-
ples that we could generate using rpois(), using a sample size of 
100,000, and a desired mean of 10, we will get a result that lies be-
tween 0.5816434 and 0.5911456 (remember that this resulting pro-
portion is calculated as the total number of events whose delay 
time is 10 or less). so we know what would happen for 95% of the 
rpois() samples, but the assumption that statisticians also make is 
that if a natural phenomenon, like the arrival time of tweets, also 
   ts the poisson distribution, that this same con   dence interval 
would be operative. so while we know that we got 0.58638  in one 
sample on the previous page, it is likely that future samples will 
vary by a little bit (about 1%). just to get a feel for what happens to 
the con   dence interval with smaller samples, look at these:
> poisson.test(5863, 10000)
95 percent confidence interval:
 0.5713874 0.6015033
> poisson.test(586, 1000)
95 percent confidence interval:
 0.5395084 0.6354261
> poisson.test(58, 100)

95 percent confidence interval:
 0.4404183 0.7497845 
we   ve bolded the parameters that changed in each of the three com-
mands above, just to emphasize that in each case we   ve reduced 
the sample size by a factor of 10. by the time we get to the bottom 
look how wide the con   dence interval gets. with a sample of 100 
events, of which 58 had delays of 10 seconds or less, the con   dence 
interval around the proportion of 0.58 ranges from a low of 0.44 to 
a high of 0.75! that   s huge! the con   dence interval gets wider and 
wider as we get less and less con   dent about the accuracy of our esti-
mate. in the case of a small sample of 100 events, the con   dence in-
terval is very wide, showing that we have a lot of uncertainty 
about our estimate that 58 events out of 100 will have arrival de-
lays of 10 or less. note that you can    lter out the rest of the stuff 
that poisson.test() generates by asking speci   cally for the "conf.int" 
in the output that is returned:
> poisson.test(58, 100)$conf.int
[1] 0.4404183 0.7497845
attr(,"conf.level")
[1] 0.95
the bolded part of the command line above shows how we used 
the $ notation to get a report of just the bit of output that we 
wanted from poisson.test(). this output reports the exact same con-
   dence interval that we saw on the previous page, along with a re-
minder in the    nal two lines that we are looking at a 95% con   -
dence interval.

103

at this point we have all of the knowledge and tools we need to 
compare two sets of arrival rates. let   s grab a couple of sets of 
tweets and extract the information we need. first, we will use the 
function we created in the last chapter to grab the    rst set of 
tweets:
tweetdf <- tweetframe("#ladygaga",500)
next, we need to sort the tweets by arrival time, that is, of course, 
unless you accepted the chapter challenge in the previous chapter 
and built the sorting into your tweetframe() function.
sortweetdf<-tweetdf[order(as.integer( +    
                             tweetdf$created)), ]
now, we   ll extract a vector of the time differences. in the previous 
chapter the use of the diff() function occurred within the arrival-
id203() function that we developed. here we will use it di-
rectly and save the result in a vector:
eventdelays<- +    
             as.integer(diff(sortweetdf$created))
now we can calculate a few of the things we need in order to get a 
picture of the arrival delays for lady gaga   s tweets:
> mean(eventdelays)
[1] 30.53707
> sum(eventdelays<=31)
[1] 333
so, for lady gaga tweets, the mean arrival delay for the next tweet 
is just short of 31 seconds. another way of looking at that same sta-

tistic is that 333 out of 500 tweets (0.666, about two thirds) arrived 
within 31 seconds of the previous tweet. we can also ask 
poisson.test() to show us the con   dence interval around that value:
> poisson.test(333,500)$conf.int
[1] 0.5963808 0.7415144
attr(,"conf.level")
[1] 0.95
so, this result suggests that for 95% of the lady gaga samples of 
tweets that we might pull from the twitter system, the proportion 
arriving in 31 seconds or less would fall in this con   dence band. in 
other words, we   re not very likely to see a sample with a propor-
tion under 59.6% or over 74.1%. that   s a pretty wide band, so we 
do not have a lot of exactitude here. 
now let   s get the same data for oprah:
> tweetdf <- tweetframe("#oprah",500)
> sortweetdf<-tweetdf[order( +    
                  as.integer(tweetdf$created)), ]
> eventdelays<- +   
             as.integer(diff(sortweetdf$created))
> mean(eventdelays)
[1] 423.01
id48, i guess we know who is boss here! now let   s    nish the job:
> sum(eventdelays<=31)
[1] 73

104

> poisson.test(73,500)$conf.int
[1] 0.1144407 0.1835731
attr(,"conf.level")
[1] 0.95
the sum() function, above, calculates that only 73 out of oprah   s 
sample of 500 tweets arrive in an interval of 31 or less. we use 31, 
the mean of the lady gaga sample, because we need to have a com-
mon basis of comparison. so for oprah, the proportion of events 
that occur in the 31 second timeframe is, 73/500 = 0.146, or about 
14.6%. that   s a lot lower than the 66.6% of lady gaga tweets, for 
sure, but we need to look at the con   dence interval around that 
value. so the poisson.test() function just above for oprah reports 
that the 95% con   dence interval runs from about 11.4% to 18.4%. 
note that this con   dence interval does not overlap at all with the 
con   dence interval for lady gaga, so we have a very strong sense 
that these two rates are statistically quite distinctive - in other 
words, this is a difference that was not caused by the random in   u-
ences that sampling always creates. we can make a bar graph to 
summarize these differences. we   ll use the barplot2() function, 
which is in a package called gplots(). if you created the ensurepack-
age() function a couple of chapters ago, you can use that. other-
wise make sure to load gplots manually:
> ensurepackage("gplots")
> barplot2(c(0.666,0.146), +    
	
	

ci.l=c(0.596,0.114), +    
ci.u=c(0.742,0.184), +   

	
	

	
	

plot.ci=true, +   
names.arg=c("gaga","oprah"))

	
	
this is not a particularly ef   cient way to use the barplots() func-
tion, because we are supplying our data by typing it in, using the 
c() function to create short vectors of values on the command line. 
on the    rst line,, we supply a list of the means from the two sam-
ples, expressed as proportions. on the next two lines we    rst pro-
vide the lower limits of the con   dence intervals and then the up-
per limits. the plot.ci=true parameter asks barplot2() to put con   -
dence interval whiskers on each bar. the    nal line provides labels 
to put underneath the bars. here   s what we get:

105

comparison of poisson rates

> poisson.test(c(333,73),c(500,500))
	
data:  c(333, 73) time base: c(500, 500) 
count1 = 333, expected count1 = 203, p-value < 
2.2e-16
alternative hypothesis: true rate ratio is not 
equal to 1 
95 percent confidence interval:
 3.531401 5.960511 
sample estimates:
rate ratio 
  4.561644 
let   s walk through this output line by line. right after the com-
mand, we get a brief con   rmation from the function that we   re 
comparing two event rates in this test rather than just evaluating a 
single rate: "comparison of poisson rates." the next line con   rms 
the data we provided. the next line, that begins with "count1 = 
333" con   rms the basis of of the comparison and then shows a 
"pooled" count that is the weighted average of 333 and 73. the p-
value on that same line represents the position of a id203 tail 
for "false positives." together with the information on the next line, 
"alternative hypothesis," this constitutes what statisticians call a 
"null hypothesis signi   cance test." although this is widely used in 
academic research, it contains less useful information than con   -
dence intervals and we will ignore it for now.

this is not an especially attractive bar plot, but it does represent 
the information we wanted to display accurately. and with the as-
sistance of this plot, it is easy to see both the substantial difference 
between the two bars and the fact that the con   dence intervals do 
not overlap. 
for one    nal con   rmation of our results, we can ask the 
poisson.text() function to evaluate our two samples together. this 
code provides the same information to poisson.test() as before, but 
now provides the event counts as short lists describing the two 
samples, with 333 events (under 31 seconds) for lady gaga and 73 
events for oprah, in both cases out of 500 events:

106

the next line, "95% con   dence interval," is a label for the most im-
portant information, which is on the line that follows. the values 
of 3.53 and 5.96 represent the upper and lower limits of the 95% 
con   dence interval around the observed rate ratio of 4.56 (reported on 
the    nal line). so, for 95% of samples that we might draw from twit-
ter, the ratio of the gaga/oprah rates might be as low as 3.53 and 
as high as 5.96. so we can be pretty sure (95% con   dence) that 
lady gaga gets tweets at least 3.5 times as fast as oprah. because 
the con   dence interval does not include 1, which would be the 
same thing as saying that the two rates are identical, we can be 
pretty certain that the observed rate ratio of 4.56 is not a statistical 
   uke. 
for this comparison, we chose two topics that had very distinctive 
event rates. as the bar chart on the previous page attests, there was 
a substantial difference between the two samples in the rates of arri-
val of new tweets. the statistical test con   rmed this for us, and al-
though the ability to calculate and visualize the con   dence inter-
vals was helpful, we probably could have guessed that such a large 
difference over a total of 1000 tweets was not a result due to sam-
pling error. 
with other topics and other comparisons, however, the results will 
not be as clear cut. after completing the chapter challenge on the 
next page, we compared the "#obama" hashtag to the "#romney" 
hashtag. over samples of 250 tweets each, obama had 159 events 
at or under the mean, while romney had only 128, for a ratio of 
1.24 in obama   s favor. the con   dence interval told a different 
story, however: the lower bound of the con   dence interval was 
0.978, very close to, but slightly below one. this signi   es that we 
can   t rule out the possibility that the two rates are, in fact, equal 

and that the slightly higher rate (1.24 to 1) that we observed for 
obama in this one sample might have come about due to sampling 
error. when a con   dence interval overlaps the point where we con-
sider something to be a "null result" (in this case a ratio of 1:1) we 
have to take seriously the possibility that peculiarities of the sam-
ple(s) we drew created the observed difference, and that a new set 
of samples might show the opposite of what we found this time.
chapter challenge
write a function that takes two search strings as arguments and 
that returns the results of a poisson rate ratio test on the arrival 
rates of tweets on the two topics. your function should    rst run the 
necessary twitter searches, then sort the tweets by ascending time 
of arrival and calculate the two vectors of time differentials. use 
the mean of one of these vectors as the basis for comparison and 
for each vector, count how many events are at or below the mean. 
use this information and the numbers of tweets requested to run 
the poisson.test() rate comparison. 

sources
barplots
http://addictedtor.free.fr/graphiques/rgraphgallery.php?graph
=54 
http://biostat.mc.vanderbilt.edu/twiki/pub/main/statgraphco
urse/graphscourse.pdf 
http://rgm2.lab.nig.ac.jp/rgm2/func.php?rd_id=gplots:barplot2 

107

poisson distribution
http://books.google.com/books?id=zkswvkqhygyc&printsec=fr
ontcover 
http://www.khanacademy.org/math/id203/v/poisson-proc
ess-1
http://www.khanacademy.org/math/id203/v/poisson-proc
ess-2 
http://stat.ethz.ch/r-manual/r-patched/library/stats/html/poi
sson.html
http://stat.ethz.ch/r-manual/r-patched/library/stats/html/poi
sson.test.html 

http://stats.stackexchange.com/questions/10926/how-to-calculat
e-con   dence-interval-for-count-data-in-r 
 
http://www.computing.dcu.ie/~mbezbradica/teaching/ca266/
ca266_13_poisson_distribution.pdf 

diff() - calculates time difference on neighboring cases
ensurepackage() - custom function, install() and require() package
for() - creates a loop, repeating execution of code
hist() - creates a frequency histogram
mean() - calculates the arithmetic mean
order() - provides a list of indices re   ecting a new sort order
plot() - begins an x-y plot
points() - adds points to a plot started with plot()
poisson.test() - con   dence intervals for poisson events or ratios
ppois() - returns a cumulative id203 for particular threshold
qpois() - does the inverse of ppois(): id203 into threshold
rpois() - generates random numbers    tting a poisson distribution
sum() - adds together a list of numbers
tweetframe() - custom procedure yielding a dataset of tweets
var() - calculates variance of a list of numbers

r functions used in this chapter
as.integer() - coerces another data type to integer if possible
barplot2() - creates a bar graph
c() - concatenates items to make a list

108

r script - create vector of probabilities from delay times
# like arrivalid203, but works with unsorted list
# of delay times
delayid203<-function(delays, increment, max)
{
  # initialize an empty vector
  plist <- null
  
  # id203 is defined over the size of this sample
  # of arrival times
  delaylen <- length(delays)
  
  # may not be necessary, but checks for input mistake
  if (increment>max) {return(null)}
  
  for (i in seq(increment, max, by=increment))
  {
    # logical test <=i provides list of trues and falses
    # of length = timelen, then sum() counts the trues
    plist<-c(plist,(sum(delays<=i)/delaylen))
  }
  return(plist)
}

109

chapter 12

string theory

prior chapters focused on statistical analysis of tweet arrival times and built on earlier knowledge of 
samples and distributions. this chapter switches gears to focus on manipulating so-called 
"unstructured" data, which in most cases means natural language texts. tweets are again a useful 
source of data for this because tweets are mainly a short (140 characters or less) character strings.

110

yoiks, that last chapter was very challenging! lots of numbers, lots 
of statistical concepts, lots of graphs. let   s take a break from all 
that (temporarily) and focus on a different kind of data for a while. 
if you think about the internet, and speci   cally about the world 
wide web for a while, you will realize: 1) that there are zillions of 
web pages; and 2) that most of the information on those web 
pages is "unstructured," in the sense that it does not consist of nice 
rows and columns of numeric data with measurements of time or 
other attributes. instead, most of the data spread out across the 
internet is text, digital photographs, or digital videos. these last 
two categories are interesting, but we will have to postpone consid-
eration of them while we consider the question of text.
text is, of course, one of the most common forms of human commu-
nication, hence the label that researchers use sometimes: natural 
language. when we say natural language text we mean words cre-
ated by humans and for humans. with our cool computer technol-
ogy, we have collectively built lots of ways of dealing with natural 
language text. at the most basic level, we have a great system for 
representing individual characters of text inside of computers 
called "unicode." among other things unicode provides for a bi-
nary representation of characters in most of the world   s written lan-
guages, over 110,000 characters in all. unicode supersedes ascii 
(the american standard code for information interchange), which 
was one of the most popular standards (especially in the u.s.) for 
representing characters from the dawn of the computer age.
with the help of unicode, most computer operating systems, and 
most application programs that handle text have a core strategy for 
representing text as lists of binary codes. such lists are commonly 
referred to as "character strings" or in most cases just "strings." one 

of the most striking things about strings from a computer program-
ming perspective is that they seem to be changing their length all 
the time. you can   t perform the usual mathematical operations on 
character strings the way you can with numbers - no multiplica-
tion or division - but it is very common to "split" strings into 
smaller strings, and to "add" strings together to form longer 
strings. so while we may start out with, "the quick brown fox," we 
may end up with "the quick brown" in one string and "fox" in an-
other, or we may end up with something longer like, "the quick 
brown fox jumped over the lazy dog."
fortunately, r, like most other data handling applications, has a 
wide range of functions for manipulating, keeping track of, search-
ing, and even analyzing string data. in this chapter, we will use our 
budding skills working with tweet data to learn the essentials of 
working with unstructured text data. the learning goal here is sim-
ply to become comfortable with examining and manipulating text 
data. we need these basic skills before we can tackle a more inter-
esting problem.
let   s begin by loading a new package, called "stringr". although r 
has quite a few string functions in its core, they tend to be a bit dis-
organized. so hadley wickham, a professor of statistics at rice uni-
versity, created this "stringr" package to make a set of string ma-
nipulation functions a bit easier to use and more comprehensive. 
you can install() and library() this package using the point and 
click features of r-studio (look in the lower right hand pane under 
the packages tab), or if you created the ensurepackage() function 
from a couple of chapters back, you can use that:
ensurepackage("stringr")

111

now we can grab a new set of tweets with our custom function 
tweetframe() from a couple of chapters ago (if you need the code, 
look in the chapter entitled "tweet, tweet"; we   ve also pasted the 
enhanced function, that sorts the tweets into arrival order, into the 
end of this chapter):
tweetdf <- tweetframe("#solar",100)
this command should return a data frame containing about 100 
tweets, mainly having to do with solar energy. you can choose any 
topic you like - all of the string techniques we examine in this chap-

ter are widely applicable to any text strings. we should get ori-
ented by taking a look at what we retrieved. the head() function 
can return the    rst entries in any vector or list:
we provide a screen shot from r-studio here just to preserve the 
formatting of this output. in the left hand margin, the number 97 
represents r   s indexing of the original order in which the tweet 

112

was received. the tweets were re-sorted into arrival order by our 
enhanced tweetframe() function (see the end of the chapter for 
code). so this is the    rst element in our dataframe, but internally r 
has numbered it as 97 out of the 100 tweets we obtained. on the 
   rst line of the output, r has place the label "text" and this is the 
   eld name of the column in the dataframe that contains the texts of 
the tweets. other dataframe    elds that we will not be using in this 
chapter include: "favorited," "replytosn," and "truncated." you 
may also recognize the    eld name "created" which contains the po-
six format time and date stamp that we used in previous chapters. 

generally speaking, r has placed the example data 
(from tweet 97) that goes with the    eld name just under-
neath it, but the text justi   cation can be confusing, and 
it makes this display very hard to read. for example, 
there is a really long number that starts with "1908" that 
is the unique numeric identi   er (a kind of serial num-
ber) for this tweet. the    eld name "id" appears just 
above it, but is right justi   ed (probably because the 
   eld is a number). the most important fact for us to 
note is that if we want to work with the text string that 
is the tweet itself, we need to use the    eld name "text." 
let   s see if we can get a somewhat better view if we use 
the head() function just on the text    eld. this command 
should provide just the    rst 2 entries in the "text" col-

umn of the dataframe:
head(tweetdf$text,2)
 [1] "if your energy needs increase after you in-
stall a #solar system can you upgrade? our ex-
perts have the answer! http://t.co/ims8gdww"                           

 [2] "#green new solar farms in west tennessee 
signal growth: two new solar energy farms produc-
ing electricity ... http://t.co/37pkaf3n #solar"                     

a couple of things which will probably seem obvious, but are none-
theless important to point out: the [1] and [2] are not part of the 
tweet, but are the typical line numbers that r uses in its output. 
the actual tweet text is between the double quotes. you can see the 
hashtag "#solar" appears in both tweets, which makes sense be-
cause this was our search term. there is also a second hashtag in 
the    rst tweet "#green" so we will have to be on the lookout for 
multiple hashtags. there is also a "shortened" url in each of these 
tweets. if a twitter user pastes in the url of a website to which 
they want to refer people, the twitter software automatically short-
ens the url to something that begins with "http://t.co/" in order 
to save space in the tweet. 
an even better way to look at these data, including the text and the 
other    elds is to use the data browser that is built into r-studio. if 
you look in the upper right hand pane of r-studio, and make sure 

that the workspace tab is clicked, you should see a list of available 

113

dataframes, under the heading "data." one of these should be 
"tweetdf." if you click on tweetdf, the data browser will open in 
the upper left hand pane of r-studio and you should be able to see 
the    rst    eld or two of the    rst dozen rows. here   s a screen shot:
this screen shot con   rms what we observed in the command line 
output, but gives us a much more appealing and convenient way 
of looking through our data. before we start to manipulate our 
strings, let   s attach() tweetdf so that we don   t have to keep using 
the $ notation to access the text    eld. and before that, let   s check 
what is already attached with the search() function:
> search()
 [1] ".globalenv"         "sortweetdf"         "package:gplots"    
 [4] "package:kernsmooth" "package:grid"       "package:catools"
we   ve truncated this list to save space, but you can see on the    rst 
line "sortweetdf" left over from our work in a previous chapter. 
the other entries are all function packages that we want to keep ac-
tive. so let   s detach() sortweetdf and attach tweetdf:
> detach(sortweetdf)

> attach(tweetdf)
these commands should yield no additional output. if 
you get any messages about "the following object(s) 
are masked from..." you should run search() again and 
look for other dataframes that should be detached be-
fore proceeding. once you can run attach("tweetdf") 
without any warnings, you can be sure that the    elds 
in this dataframe are ready to use without interference.

the    rst and most basic thing to do with strings is to see how long 
they are. the stringr package gives us the str_length() function to 
accomplish this task:
> str_length(text)
  [1] 130 136 136 128  98  75 131 139  85 157 107  49  75 139 136 136
 [17] 136  72  73 136 157 123 160 142 142 122 122 122 122 134  82  87
 [33]  89 118  94  74 103  91 136 136 151 136 139 135  70 122 122 136
 [49] 123 111  83 136 137  85 154 114 117  98 125 138 107  92 140 119
 [65]  92 125  84  81 107 107  73  73 138  63 137 139 131 136 120 124
 [81] 124 114  78 118 138 138 116 112 101  94 153  79  79 125 125 102
 [97] 102 139 138 153
these are the string lengths of the texts as reported to the com-
mand line. it is interesting to    nd that there are a few of them (like 
the very last one) that are longer than 140 characters:
> tail(text,1)
[1] "rt @solarfred: hey, #solar & wind people. 
tell @speakerboehner and @reuters that you have a 
green job and proud to be providing energy inde-
pendence to us"
as you can see, the tail() command works like the head() com-
mand except from the bottom up rather than the top down. so we 
have learned that under certain circumstances twitter apparently 
does allow tweets longer than 140 characters. perhaps the initial 
phrase "rt @solarfred" does not count against the total. by the 
way "rt" stands for "retweet" and it indicates when the receiver of 
a tweet has passed along the same message to his or her followers.

we can glue the string lengths onto the respective rows in the data-
frame by creating a new    eld/column:
tweetdf$textlen <- str_length(text)
after running this line of text, you should use the data browser in 
r-studio to con   rm that the tweetdf now has a new column of 
data labeled "textlen". you will    nd the new column all the way on 
the rightmost side of the dataframe structure. one peculiarity of 
the way r treats attached data is that you will not be able to access 
the new    eld without the $ notation unless you detach() and then 
again attach() the data frame. one advantage of grafting this new 
   eld onto our existing dataframe is that we can use it to probe the 
dataframe structure:
> detach(tweetdf)
> attach(tweetdf)
> tweetdf[textlen>140, "text"]
[1] "rt @andyschonberger: exciting (and tempting) 
to see #evs all over the #gls12 show. combine evs 
w #solar generation and we have a winner! 
http://t.co/nvsfq4g3"
we   ve truncated the output to save space, but in the data we are us-
ing here, there were nine tweets with lengths greater than 140. not 
all of them had "rt" in them, though, so the mystery remains. an 
important word about the    nal command line above, though: 
we   re using the square brackets notation to access the elements of 
tweetdf. in the    rst entry, "textlen>140", we   re using a conditional 
expression to control which rows are reported. only those rows 
where our new    eld "textlen" contains a quantity larger than 140 

114

will be reported to the output. in the second entry within square 
brackets, "text" controls which columns are reported onto the out-
put. the square bracket notation is extremely powerful and some-
times a little unpredictable and confusing, so it is worth experi-
menting with. for example, how would you change that last com-
mand above to report all of the columns/   elds for the matching 
rows? or how would you request the "screenname" column in-
stead of the "text" column? what would happen if you substituted 
the number 1 in place of "text" on that command?
the next common task in working with strings is to count the num-
ber of words as well as the number of other interesting elements 
within the text. counting the words can be accomplished in several 
ways. one of the simplest ways is to count the separators between 
the words - these are generally spaces. we need to be careful not to 
over count, if someone has mistakenly typed two spaces between a 
word, so let   s make sure to take out doubles. the str_replace_all() 
function from stringr can be used to accomplish this:
> tweetdf$modtext <- str_replace_all(text,"  "," ")
> tweetdf$textlen2 <- str_length(tweetdf$modtext)
> detach(tweetdf)
> attach(tweetdf)
> tweetdf[textlen != textlen2,]
the    rst line above uses the str_replace_all() function to substitute 
the one string in place of another as many times as the matching 
string appears in the input. three arguments appear on the func-
tion above: the    rst is the input string, and that is tweetdf$text (al-
though we   ve referred to it just as "text because the dataframe is at-

tached). the second argument is the string to look for and the third 
argument is the string to substitute in place of the    rst. note that 
here we are asking to substitute one space any time that two in a 
row are found. almost all computer languages have a function 
similar to this, although many of them only supply a function that 
replaces the    rst instance of the matching string. 
in the second command we have calculated a new string length 
variable based on the length of the strings where the substitutions 
have occurred. we preserved this in a new variable/   eld/column 
so that we can compare it to the original string length in the    nal 
command. note the use of the bracket notation in r to address a 
certain subset of rows based on where the inequality is true. so 
here we are looking for a report back of all of the strings whose 
lengths changed. in the tweet data we are using here, the output 
indicated that there were seven strings that had their length re-
duced by the elimination of duplicate spaces.
now we are ready to count the number of words in each tweet us-
ing the str_count() function. if you give it some thought, it should 
be clear that generally there is one more word than there are 
spaces. for instance, in the sentence, "go for it," there are two 
spaces but three words. so if we want to have an accurate count, 
we should add one to the total that we obtain from the str_count() 
function:
> tweetdf$wordcount<-(str_count(modtext," ") + 1)
> detach(tweetdf)
> attach(tweetdf)
> mean(wordcount)

115

[1] 14.24
in this last command, we   ve asked r to report the mean value of 
the vector of word counts, and we learn that on average a tweet in 
our dataset has about 14 words in it. 
next, let   s do a bit of what computer scientists (and others) call 
"parsing." parsing is the process of dividing a larger unit, like a sen-
tence, into smaller units, like words, based on some kind of rule. in 
many cases, parsing requires careful use of pattern matching. most 
computer languages accomplish pattern matching through the use 
of a strategy called "id157." a regular expression is a 
set of symbols used to match patterns. for example, [a-z] is used to 
match any lowercase letter and the asterisk is used to represent a 
sequence of zero or more characters. so the regular expression "[a-
z]*" means, "match a sequence of zero or more lowercase charac-
ters.
if we wanted to parse the retweet sequence that appears at the be-
ginning of some of the tweets, we might use a regular expression 
like this: "rt @[a-z,a-z]*: ". each character up to the square 
bracket is a "literal" that has to match exactly. then the "[a-z,a-z]*" 
lets us match any sequence of uppercase and lowercase characters. 
finally, the ": " is another literal that matches the end of the se-
quence. you can experiment with it freely before you commit to us-
ing a particular expression, by asking r to echo the results to the 
command line, using the function str_match() like this:
str_match(modtext,"rt @[a-z,a-z]*: ")
once you are satis   ed that this expression matches the retweet 
phrases properly, you can commit the results to a new column/
   eld/variable in the dataframe:

> tweetdf$rt <- str_match(modtext,"rt @[a-z,a-z]*: ")
> detach(tweetdf)
> attach(tweetdf)
now you can review what you found by echoing the new variable 
"rt" to the command line or by examining it in r-studio   s data 
browser:
> head(rt, 10)
      [,1]                   
 [1,] na                     
 [2,] na                     
 [3,] na                     
 [4,] na                     
 [5,] na                     
 [6,] na                     
 [7,] na                     
 [8,] "rt @seia: "           
 [9,] na                     
[10,] "rt @andyschonberger: "
this may be the    rst time we have seen the value "na." in r, na 
means that there is no value available, in effect that the location is 
empty. statisticians also call this missing data. these nas appear 
in cases where there was no match to the regular expression that 
we provided to the function str_match(). so there is nothing wrong 

116

here, this is an expected outcome of the fact that not all tweets 
were retweets. if you look carefully, though, you will see some-
thing else that is interesting.
r is trying to tell us something with the bracket notation. at the 
top of the list there is a notation of [,1] which signi   es that r is 
showing us the    rst column of something. then, each of the entries 
looks like [#,] with a row number in place of # and an empty col-
umn designator, suggesting that r is showing us the contents of a 
row, possibly across multiple columns. this seems a bit mysteri-
ous, but a check of the documentation for str_match() reveals that 
it returns a matrix as its result. this means that tweetdf$rt could 
potentially contain its own rectangular data object: in effect, the 
variable rt could itself contain more than one column!
in our case, our regular expression is very simple and it contains 
just one chunk to match, so there is only one column of new data 
in tweetdf$rt that was generated form using str_match(). yet the 
full capability of id157 allows for matching a whole 
sequence of chunks, not just one, and so str_match() has set up the 
data that it returns to prepare for the eventuality that each row of 
tweetdf$rt might actually have a whole list of results. 
if, for some reason, we wanted to simplify the structure of 
tweetdf$rt so that each element was simply a single string, we 
could use this command:
tweetdf$rt <- tweetdf$rt[ ,1]
this assigns to each element of tweetdf$rt the contents of the    rst 
column of the matrix. if you run that command and reexamine 

tweetdf$rt with head() you will    nd the simpli   ed structure: no 
more column designator.
for us to be able to make some use of the retweet string we just iso-
lated, we probably should extract just the "screenname" of the indi-
vidual whose tweet got retweeted. a screenname in twitter is like 
a username, it provides a unique identi   er for each person who 
wants to post tweets. an individual who is frequently retweeted 
by others may be more in   uential because their postings reach a 
wider audience, so it could be useful for us to have a listing of all 
of the screennames without the extraneous stuff. this is easy to do 
with str_replace(). note that we used str_replace_all() earlier in the 
chapter, but we don   t need it here, because we know that we are 
going to replace just one instance of each string:
tweetdf$rt<-str_replace(rt, "rt @","")
tweetdf$rt<-str_replace(rt,": ","")
> tail(rt, 1)
       [,1]       
[100,] "solarfred"
tweetdf$rt <- tweetdf$rt[ ,1]
in the    rst command, we substitute the empty string in place of the 
four character pre   x "rt @", while in the second command we sub-
stitute the empty string in place of the two character suf   x ": ". in 
each case we assign the resulting string back to tweetdf$rt. you 
may be wondering why sometimes we create a new column or 
   eld when we calculate some new data while other times we do 
not. the golden rule with data columns is never to mess with the 

117

original data that was supplied. when you are working ona "de-
rived" column, i.e., one that is calculated from other data, it may 
require several intermediate steps to get the data looking the way 
you want. in this case, rt is a derived column that we extracted 
from the text    eld of the tweet and our goal was to reduce it to the 
bare screenname of the individual whose post was retweeted. so 
these commands, which successfully overwrite rt with closer and 
closer versions of what we wanted, were fair game for modi   ca-
tion. 
you may also have noticed 
the very last command. it 
seems that one of our steps, 
probably the use of 
str_match() must have 
"matrix-ized" our data 
again, so we use the column 
trick that appeared earlier in 
this chapter to    atten the ma-
trix back to a single column 
of string data. 
this would be a good point to visualize what we have obtained. 
here we introduce two new functions, one which should seem fa-
miliar and one that is quite new:
table(as.factor(rt))
the as.factor() function is a type/mode coercion and just a new 
one in a family we have seen before. in previous chapters we used 
as.integer() and as.character() to perform other conversions. in r a 
factor is a collection of descriptive labels and corresponding 

unique identifying numbers. the identifying numbers are not usu-
ally visible in outputs. factors are often used for dividing up a data-
set into categories. in a survey, for instance, if you had a variable 
containing the gender of a participant, the variable would fre-
quently be in the form of a factor with (at least) two distinct catego-
ries (or what statisticians call levels), male and female. inside r, 
each of these categories would be represented as a number, but the 
corresponding label would usually be the only thing you would 
see as output. as an experiment, try running this command:

str(as.factor(rt))
this will reveal the 
"structure" of the data 
object after coercion. 
returning to the earlier 
table(as.factor(rt)) com-
mand, the table() func-
tion takes as input one 
or more factors and re-
turns a so called contin-
gency table. this is easy to understand for use with just one factor: 
the function returns a unique list of factor "levels" (unique: mean-
ing no duplicates) along with a count of how many rows/instances 
there were of each level in the dataset as a whole. 
the screen shot on this page shows the command and the output. 
there are about 15 unique screennames of twitter users who were 
retweeted. the highest number of times that a screenname ap-
peared was three, in the case of seia. the table() function is used 
more commonly to create two-way (two dimensional) contingency 

118

tables. we could demonstrate that here if we had two factors, so 
let   s create another factor.
remember earlier in the chapter we noticed some tweets had texts 
that were longer than 140 characters. we can make a new variable, 
we   ll call it longtext, that will be true if the original tweet was 
longer than 140 characters and false if it was not:
> tweetdf$longtext <- (textlen>140)
> detach(tweetdf)
> attach(tweetdf)
the    rst command above has an inequality expression on the right 
hand side. this is tested for each row and the result, either true 
or false, is assigned to the new variable longtext. computer scien-
tists sometimes call this a "   ag" variable because it    ags whether or 
not a certain attribute is present in the data. now we can run the 
table() function on the two factors:
> table(as.factor(rt),as.factor(longtext))
                 
                  false true
  earthtechling       0    1
  feedthegrid         2    0
  firstsolar          1    0
  greenergynews       1    0
  raygil              0    1

  seia                3    0
  solarfred           0    2
  solarindustry       1    0
  solarnovus          1    0
  andyschonberger     0    2
  deepgreendesign     0    1
  gerdvdlogt          2    0
  seia                2    0
  solarfred           1    0
  thesolsolution      1    0
for a two-way contingency table, the    rst argument you supply to 
table() is used to build up the rows and the second argument is 
used to create the columns. the command and output above give 
us a nice compact display of which retweets are longer than 140 
characters (the true column) and which are not (the false col-
umn). it is easy to see at a glance that there are many in each cate-
gory. so, while doing a retweet may contribute to having an extra 
long tweet, there are also many retweets that are 140 characters or 
less. it seems a little cumbersome to look at the long list of retweet 
screennames, so we will create another    ag variable that indicates 
whether a tweet text contains a retweet. this will just provide a 
more compact way of reviewing which tweets have retweets and 
which do not:
> tweetdf$hasrt <- !(is.na(rt))
> detach(tweetdf)

119

> attach(tweetdf)
> view(tweetdf)
the    rst command above uses a function we have not encountered 
before: is.na(). a whole family of functions that start with "is" exists 
in r (as well as in other programming languages) and these func-
tions provide a convenient way of testing the status or contents of 
a data object or of a particular element of a data object. the is.na() 
function tests whether an element of the input variable has the 
value na, which we know from earlier in the chapter is r   s way of 
showing a missing value (when a particular data element is 
empty). so the expression, is.na(rt) will return true if a particular 
cell of tweetdf$rt contains the empty value na, and false if it con-
tains some real data. if you look at the name of our new variable, 
however, which we have called "hasrt" you may see that we want 
to reverse the sense of the true and false that is.na() returns. to 
do that job we use the "!" character, which computers scientists 
may either call "bang" or more accurately, "not." using "not" is 
more accurate because the "!" character provides the boolean not 
function, which changes a true to a false and vice versa. one 
last little thing is that the view() command causes r-studio to 
freshen the display of the dataframe in its upper left hand pane. 
let   s look again at retweets and long tweet texts:
> table(hasrt,longtext)
       longtext
hasrt   false true
  false    76    2
  true     15    7

there are more than twice as many extra long texts (7) when a 
tweet contains a retweet than when it does not.
let   s now follow the same general procedure for extracting the 
urls from the tweet texts. as before the goal is to create a new 
string variable/column on the original dataframe that will contain 
the urls for all of those tweets that have them. additionally, we 
will create a    ag variable that signi   es whether or not each tweet 
contains a url. here, as before, we follow a key principle: don   t 
mess with your original data. we will need to develop a new regu-
lar expression in order to locate an extract the url string from in-
side of the tweet text. actually, if you examine your tweet data in 
the r-studio data browser, you may note that some of the tweets 
have more than one url in them. so we will have to choose our 
function call carefully and be equally careful looking at the results 
to make sure that we have obtained what we need.
at the time when this was written, twitter had imposed an excel-
lent degree of consistency on urls, such that they all seem to start 
with the string "http://t.co/". additionally, it seems that the com-
pacted urls all contain exactly 8 characters after that literal, com-
posed of upper and lower case letters and digits. we can use 
str_match_all() to extract these urls using the following code:
str_match_all(text,"http://t.co/[a-z,a-z,0-9]{8}")
we feed the tweetdf$text    eld as input into this function call (we 
don   t need to provide the tweetdf$ part because this dataframe is 
attached). the regular expression begins with the 12 literal charac-
ters ending with a forward slash. then we have a regular expres-
sion pattern to match. the material within the square brackets 
matches any upper or lowercase letter and any digit. the numeral 

120

8 between the curly braces at the end say to match the previous pat-
tern exactly eight times. this yields output that looks like this:
[[6]]
     [,1]                  
[1,] "http://t.co/w74x9jci"

[[7]]
     [,1]                  
[1,] "http://t.co/dzbuoz5l"
[2,] "http://t.co/gmtedcqi"
this is just an excerpt of the output, but there are a couple of impor-
tant things to note. first, note that the    rst element is preceded by 
the notation [[6]]. in the past when r has listed out multiple items 
on the output, we have seen them with index numbers like [1] and 
[2]. in this case, however, that could be confusing because each ele-
ment in the output could have multiple rows (as item [[7]] above 
clearly shows). so r is using double bracket notation to indicate 
the ordinal number of each chunk of data in the list, where a given 
chunk may itself contain multiple elements. 
confusing? let   s go at it from a different angle. look at the output 
under the [[7]] above. as we noted a few paragraphs ago, some of 
those tweets have multiple urls in them. the str_match_all() func-
tion handles this by creating, for every single row in the tweet data, a 
data object that itself contains exactly one column but one or possi-
bly more than one row - one row for each url that appears in the 

tweet. so, just as we saw earlier in the chapter, we are getting back 
from a string function a complex matrix-like data object that re-
quires careful handling if we are to make proper use of it. 
the only other bit of complexity is this: what if a tweet contained 
no urls at all? your output from running the str_match_all() func-
tion probably contains a few elements that look like this:
[[30]]
character(0)

[[31]]
character(0)
so elements [[30]] and [[31]] of the data returned from 
str_match_all() each contain a zero length string. no rows, no col-
umns, just character(0), the so-called null character, which in many 
computer programming languages is used to "terminate" a string. 
let   s go ahead and store the output from str_match_all() into a 
new vector on tweetdf and then see what we can do to tally up 
the urls we have found:
> tweetdf$urlist<-str_match_all(text,+   
                 "http://t.co/[a-z,a-z,0-9]{8}")
> detach(tweetdf)
> attach(tweetdf)
> head(tweetdf$urlist,2)
[[1]]

121

     [,1]                  
[1,] "http://t.co/ims8gdww"

[[2]]
     [,1]                  
[1,] "http://t.co/37pkaf3n"
now we are ready to wrestle with the problem of how to tally up 
the results of our url parsing. unlike the situation with retweets, 
where there either was or was not a single retweet indication in the 
text, we have the possibility of zero, one or more urls within the 
text of each tweet. our new object "urlist" is a multi-dimensional 
object that contains a single null character, one row/column of 
character data, or one column with more than one row of character 
data. the key to summarizing this is the length() function, which 
will happily count up the number of elements in an object that you 
supply to it:
> length(urlist[[1]])
[1] 1
> length(urlist[[5]])
[1] 0
> length(urlist[[7]])
[1] 2
here you see that double bracket notation again, used as an index 
into each "chunk" of data, where the chunk itself may have some 

internal complexity. in the case of element [[1]] above, there is one 
row, and therefore one url. for element [[5]] above, we see a zero, 
which means that length() is telling us that this element has no 
rows in it at all. finally, for element [[7]] we see 2, meaning that 
this element contains two rows, and therefore two urls.
in previous work with r, we   ve gotten used to leaving the inside 
of the square brackets empty when we want to work with a whole 
list of items, but that won   t work with the double brackets:
> length(urlist[[]])
error in urlist[[]] : invalid subscript type 'symbol'
the double brackets notation is designed to reference just a single 
element or component in a list, so empty double brackets does not 
work as a shorthand for every element in a list. so what we must 
do if we want to apply the length() function to each element in url-
ist is to loop. we could accomplish this with a for loop, as we did 
in the last chapter, using an index quantity such as "i" and substitut-
ing i into each expression like this: urlist[[i]]. but let   s take this op-
portunity to learn a new function in r, one that is generally more 
ef   cient for looping. the rapply() function is part of the "apply" 
family of functions,  and it stands for "recursive apply." recursive 
in this case means that the function will dive down into the com-
plex, nested structure of urlist and repetitively run a function for 
us, in this case the length() function:
> tweetdf$numurls<-rapply(urlist,length)
> detach(tweetdf)
> attach(tweetdf)
> head(numurls,10)

122

 [1] 1 1 1 1 0 1 2 1 1 1
excellent! we now have a new    eld on tweetdf that counts up the 
number of urls. as a last step in examining our tweet data, let   s 
look at a contingency table that looks at the number of urls to-
gether with the    ag indicating an extra long tweet. earlier in the 
chapter, we mentioned that the table() function takes factors as its 
input. in the command below we have supplied the numurls    eld 
to the table() function without coercing it to a factor. fortunately, 
the table() function has some built in intelligence that will coerce a 
numeric variable into a factor. in this case because numurls only 
takes on the values of 0, 1, or 2, it makes good sense to allow ta-
ble() to perform this coercion:
> table(numurls,longtext)
       longtext
numurls false true
      0    16    3
      1    72    6
      2     3    0
this table might be even more informative if we looked at it as pro-
portions, so here is a trick to view proportions instead of counts:
> prop.table(table(numurls,longtext))
       longtext
numurls false true
      0  0.16 0.03

      1  0.72 0.06
      2  0.03 0.00
that looks familiar! now, of course, we remember that we had ex-
actly 100 tweets, so each of the counts could be considered a per-
centage with no further calculation. still, prop.table() is a useful 
function to have when you would rather view your contingency 
tables as percentages rather than counts. we can see from these re-
sults that six percent of the tweets have one url, but only three 
percent have no urls.
so, before we close out this chapter, let   s look at a three way contin-
gency table by putting together our two    ag variables and the num-
ber of urls:
> table(numurls,hasrt,longtext)
, , longtext = false
       hasrt
numurls false true
      0    15    1
      1    58   14
      2     3    0

, , longtext = true
       hasrt
numurls false true
      0     0    3

123

      1     2    4
      2     0    0
not sure this entirely solves the mystery, but if we look at the sec-
ond two-way table above, where longtext = true, it seems that ex-
tra long tweets either have a retweet (3 cases), or a single url (2 
cases) or both (4 cases). 
when we said we would give statistics a little rest in this chapter, 
we lied just a tiny bit. check out these results:
> mean(textlen[hasrt&longtext])
[1] 155
> mean(textlen[!hasrt&longtext])
[1] 142
in both commands we have requested the mean of the variable 
textlen, which contains the length of the original tweet (the one 
without the space stripped out). in each command we have also 
used the bracket notation to choose a particular subset of the cases. 
inside the brackets we have a logical expression. the only cases 
that will be included in the calculation of the mean are those where 
the expression inside the brackets evaluates to true. in the    rst 
command we ask for the mean tweet length for those tweets that 
have a retweet and are extra long (the ampersand is the boolean 
and operator). in the second command we use the logical not 
(the "!" character) to look at only those cases that have extra long 
text but do not have a retweet. the results are instructive. the 
really long tweets, with a mean length of 155 characters, are those 
that have retweets. it seems that twitter does not penalize an indi-
vidual who retweets by counting the number of characters in the 

"rt @screenname:" string. if you have tried the web interface 
for twitter you will see why this makes sense: retweeting is accom-
plished with a click, and the original tweet - which after all may al-
ready be 140 characters - appears underneath the screenname of 
the originator of the tweet. the "rt @" string does not even appear 
in the text of the tweet at that point.
looking back over this chapter, we took a close look at some of the 
string manipulation functions provided by the package "stringr". 
these included some of the most commonly used actions such as 
   nding the length of a string,    nding matching text within a string, 
and doing search and replace operations on a string. we also be-
came aware of some additional complexity in nested data struc-
tures. although statisticians like to work with nice, well-ordered 
rectangular datasets, computer scientists often deal with much 
more complex data structures - although these are built up out of 
parts that we are familiar with such as lists, vectors, and matrices.
twitter is an excellent source of string data, and although we have 
not yet done much in analyzing the contents of tweets or their 
meanings, we have looked at some of the basic features and regu-
larities of the text portion of a tweet. in the next chapter we will be-
come familiar with a few additional text tools and then be in a posi-
tion to manipulate and analyze text data
chapter challenges
create a function that takes as input a dataframe of tweets and re-
turns as output a list of all of the retweet screennames. as an extra 
challenge, see if you can reduce that list of screennames to a 
unique set (i.e., no duplicates) while also generating a count of the 
number of times that each retweet screenname appeared.

124

once you have written that function, it should be a simple matter 
to copy and modify it to create a new function that extracts a 
unique list of hashtags from a dataframe of tweets. recall that 
hashtags begin with the "#" character and may contain any combi-
nation of upper and lowercase characters as well as digits. there is 
no length limit on hashtags, so you will have to assume that a hash-
tag ends when there is a space or a punctuation mark such as a 
comma, semicolon, or period.
sources
http://cran.r-project.org/web/packages/stringr/index.html 
http://en.wikipedia.org/wiki/ascii 
http://en.wikipedia.org/wiki/regular_expression 
http://en.wikipedia.org/wiki/unicode 
http://had.co.nz/ (hadley wickham)
http://mashable.com/2010/08/14/twitter-140-bug/ 
http://stat.ethz.ch/r-manual/r-devel/library/base/html/search
.html 
http://stat.ethz.ch/r-manual/r-devel/library/base/html/table.h
tml

r code for tweetframe() function
# tweetframe() - return a dataframe based on a search of twit-
ter
tweetframe<-function(searchterm, maxtweets)
{
  tweetlist <- searchtwitter(searchterm, n=maxtweets)
  
  # as.data.frame() coerces each list element into a row
  # lapply() applies this to all of the elements in twtlist
  # rbind() takes all of the rows and puts them together
  # do.call() gives rbind() all rows as individual elements
  tweetdf<- do.call("rbind", lapply(tweetlist,as.data.frame))
  
  # this last step sorts the tweets in arrival order
  return(tweetdf[order(as.integer(tweetdf$created)), ])
}

125

chapter 13

word perfect

in the previous chapter we mastered some of the most basic and important functions for examining 
and manipulating text. now we are in a position to analyze the actual words that appear in text 
documents. some of the most basic functions of the internet, such as keyword search, are 
accomplished by analyzing the "content" i.e., the words in a body of text.

126

the picture at the start of this chapter is a so called "word cloud" 
that was generated by examining all of the words returned from a 
twitter search of the term "data science" (using a web application 
at http://www.jasondavies.com)  these colorful word clouds are 
fun to look at, but they also do contain some useful information. 
the geometric arrangement of words on the    gure is partly ran-
dom and partly designed and organized to please the eye. same 
with the colors. the font size of each word, however, conveys some 
measure of its importance in the "corpus" of words that was pre-
sented to the word cloud graphics program. corpus, from the 
latin word meaning "body," is a word that text analysts use to refer 
to a body of text material, often consisting of one or more docu-
ments. when thinking about a corpus of textual data, a set of docu-
ments could really be anything: web pages, word processing docu-
ments on your computer, a set of tweets, or government reports. in 
most cases, text analysts think of a collection of documents, each of 
which contains some natural language text, as a corpus if they plan 
to analyze all the documents together.
the word cloud on the previous page shows that "data" and "sci-
ence" are certainly important terms that came from the search of 
twitter, but there are dozens and dozens of less important, but per-
haps equally interesting, words that the search results contained. 
we see words like algorithms, molecules, structures, and research, 
all of which could make sense in the context of data science. we 
also see other terms, like #christian, facilitating, and coordinator, 
that don   t seem to have the same obvious connection to our origi-
nal search term "data science." this small example shows one of 
the fundamental challenges of natural language processing and the 
closely related area of search: ensuring that the analysis of text pro-
duces results that are relevant to the task that the user has in mind.

in this chapter we will use some new r packages to extend our 
abilities to work with text and to build our own word cloud from 
data retrieved from twitter. if you have not worked on the chapter 
"string theory" that precedes this chapter, you should probably do 
so before continuing, as we build on the skills developed there. 
depending upon where you left off after the previous chapter, you 
will need to retrieve and pre-process a set of tweets, using some of 
the code you already developed, as well as some new code. at the 
end of the previous chapter, we have provided sample code for the 
tweetframe() function, that takes a search term and a maximum 
tweet limit and returns a time-sorted dataframe containing tweets. 
although there are a number of comments in that code, there are 
really only three lines of functional code thanks to the power of the 
twitter package to retrieve data from twitter for us.  for the activi-
ties below, we are still working with the dataframe that we re-
trieved in the previous chapter using this command:
tweetdf <- tweetframe("#solar",100)
this yields a dataframe, tweetdf, that contains 100 tweets with the 
hashtag #solar, presumably mostly about solar energy and related 
"green" topics. before beginning our work with the two new r 
packages, we can improve the quality of our display by taking out 
a lot of the junk that won   t make sense to show in the word cloud. 
to accomplish this, we have authored another function that strips 
out extra spaces, gets rid of all url strings, takes out the retweet 
header if one exists in the tweet, removes hashtags, and eliminates 
references to other people   s tweet handles. for all of these transfor-
mations, we have used string replacement functions from the 
stringr package that was introduced in the previous chapter. as an 
example of one of these transformations, consider this command, 

127

which appears as the second to last line of the cleantweet() func-
tion:
tweets <- str_replace_all(tweets,"@[a-z,a-z]*","")
you should feel pretty comfortable reading this line of code, but if 
not, here   s a little more practice. the left hand side is easy: we use 
the assignment arrow to assign the results of the right hand side 
expression to a data object called "tweets." note that when this 
statement is used inside the function as shown at the end of the 
chapter, "tweets" is a temporary data object, that is used just within 
cleantweets() after which it disappears automatically. 
the right hand side of the expression uses the str_replace_all() func-
tion from the stringr package. we use the "all" function rather than 
str_replace() because we are expecting multiple matches within 
each individual tweet. there are three arguments to the str_re-
place_all() function. the    rst is the input, which is a vector of char-
acter strings (we are using the temporary data object "tweets" as 
the source of the text data as well as its destination), the second is 
the regular expression to match, and the third is the string to use to 
replace the matches, in this case the empty string as signi   ed by 
two double quotes with nothing between them. the regular expres-
sion in this case is the at sign, "@", followed by zero or more upper 
and lowercase letters. the asterisk, "*", after the stuff in the square 
brackets is what indicates the zero or more. that regular expres-
sion will match any screenname referral that appears within a 
tweet.
if you look at a few tweets you will    nd that people refer to each 
other quite frequently by their screennames within a tweet, so @so-
larfred might occur from time to time within the text of a tweet. 

here   s something you could  investigate on your own: can screen-
names contain digits as well as letters? if so, how would you have 
to change the regular expression in order to also match the digits 
zero through nine as part of the screenname? on a related note, 
why did we choose to strip these screen names out of our tweets? 
what would the word cloud look like if you left these screennames 
in the text data?
whether you typed in the function at the end of this chapter or you 
plan to enter each of the cleaning commands individually, let   s be-
gin by obtaining a separate vector of texts that is outside the origi-
nal dataframe:
> cleantext<-tweetdf$text
> head(cleantext, 10)
there   s no critical reason for doing this except that it will simplify 
the rest of the presentation. you could easily copy the tweetdf$text 
data into another column in the same dataframe if you wanted to. 
we   ll keep it separate for this exercise so that we don   t have to 
worry about messing around with the rest of the dataframe. the 
head() command above will give you a preview of what you are 
starting with. now let   s run our custom cleaning function:
> cleantext<-cleantweets(cleantext)
> head(cleantext, 10)
note that we used our "cleantext" data object in the    rst command 
above as both the source and the destination. this is an old com-
puter science trick for cutting down on the number of temporary 
variables that need to be used. in this case it will do exactly what 
we want,    rst evaluating the right hand side of the expression by 

128

running our cleantweets() function with the cleantext object as in-
put and then taking the result that is returned by cleantweets() 
and assigning it back into cleantext, thus overwriting the data that 
was in there originally. remember that we have license to do what-
ever we want to cleantext because it is a copy of our original data, 
and we have left the original data intact (i.e., the text column inside 
the tweetdf dataframe).
the head() command should now show a short list of tweets with 
much of the extraneous junk    ltered out. if you have followed 
these steps, cleantext is now a vector of character strings (in this 
example exactly 100 strings) ready for use in the rest of our work 
below. we will now use the "tm" package to process our texts. the 
"tm" in this case refers to "id111," and is a popular choice 
among the many text analysis packages available in r. by the way, 
id111 refers to the practice of extracting useful analytic infor-
mation from corpora of text (corpora is the plural of corpus). al-
though some people use id111 and natural language process-
ing interchangeably, there are probably a couple subtle differences 
worth considering. first, the "mining" part of id111 refers to 
an area of practice that looks for unexpected patterns in large data 
sets, or what some people refer to as knowledge discovery in data-
bases. in contrast, natural language processing re   ects a more gen-
eral interest in understanding how machines can be programmed 
(or learn on their own) how to digest and make sense of human lan-
guage. in a similar vein, id111 often focuses on statistical ap-
proaches to analyzing text data, using strategies such as counting 
word frequencies in a corpus. in natural language processing, one 
is more likely to hear consideration given to linguistics, and there-
fore to the processes of breaking text into its component grammati-
cal pieces such as nouns and verbs. in the case of the "tm" add on 

package for r, we are de   nitely in the statistical camp, where the 
main process is to break down a corpus into sequences of words 
and then to tally up the different words and sequences we have 
found. 
to begin, make sure that the tm package is installed and "library-
ed" in your copy of r and r-studio. you can use the graphic inter-
face in r-studio for this purpose or the ensurepackage() function 
that we wrote in a previous chapter.  once the tm package is ready 
to use, you should be able to run these commands:
> tweetcorpus<-corpus(vectorsource(cleantext))
> tweetcorpus
a corpus with 100 text documents
> tweetcorpus<-tm_map(tweetcorpus, tolower)
> tweetcorpus<-tm_map(tweetcorpus, removepunctuation)
> tweetcorpus<-tm_map(tweetcorpus,removewords,+   
                                stopwords('english'))
in the    rst step above , we "coerce" our cleantext vector into a cus-
tom "class" provided by the tm package and called a "corpus," 
storing the result in a new data object called "tweetcorpus." this is 
the    rst time we have directly encountered a "class." the term 
"class" comes from an area of computer science called "object ori-
ented programming." although r is different in many ways from 
object-oriented languages such as java, it does contain many of the 
most fundamental features that de   ne an object oriented language. 
for our purposes here, there are just a few things to know about a 
class. first, a class is nothing more or less than a de   nition for the 
structure of a data object. second, classes use basic data types, such 

129

as numbers, to build up more complex data structures. for exam-
ple, if we made up a new "dashboard" class, it could contain one 
number for "miles per hour," another number for "rpm," and per-
haps a third one indicating the remaining "fuel level." that brings 
up another point about classes: users of r can build their own. in 
this case, the author of the tm package, ingo feinerer, created a 
new class, called corpus, as the central data structure for text min-
ing functions. (feinerer is a computer science professor who works 
at the vienna university of technology in the database and arti   -
cial intelligence group.) last, and most important for this discus-
sion, a class not only contains de   nitions about the structure of 
data, it also contains references to functions that can work on that 
class. in other words, a class is a data object that carries with it in-
structions on how to do operations on it, from simple things like 
add and subtract all the way up to complicated operations such as 
graphing.
in the case of the tm package, the corpus class de   nes the most 
fundamental object that text miners care about, a corpus contain-
ing a collection of documents. once we have our texts stored in a 
corpus, the many functions that the tm package provides to us are 
available. the last three commands in the group above show the 
use of the tm_map() function, which is one of the powerful capabili-
ties provided by tm. in each case where we call the tm_map() func-
tion, we are providing tweetcorpus as the input data, and then we 
are providing a command that undertakes a transformation on the 
corpus. we have done three transformations here,    rst making all 
of the letters lowercase, then removing the punctuation, and    nally 
taking out the so called "stop" words.

the stop words deserve a little explanation. researchers who devel-
oped the early search engines for electronic databases found that 
certain words interfered with how well their search algorithms 
worked. words such as "the," "a," and "at" appeared so commonly 
in so many different parts of the text that they were useless for dif-
ferentiating between documents. the unique and unusual nouns, 
verbs, and adjectives that appeared in a document did a much bet-
ter job of setting a document apart from other documents in a cor-
pus, such that researchers decided that they should    lter out all of 
the short, commonly used words. the term "stop words" seems to 
have originated in the 1960s to signify words that a computer proc-
essing system would throw out or "stop using" because they had 
little meaning in a data processing task. to simplify the removal of 
stop words, the tm package contains lists of such words for differ-
ent languages. in the last command on the previous page we re-
quested the removal of all of the common stop words.
at this point we have processed our corpus into a nice uniform 
"bag of words" that contains no capital letters, punctuation, or stop 
words. we are now ready to conduct a kind of statistical analysis of 
the corpus by creating what is known as a "term-document ma-
trix." the following command from the tm package creates the ma-
trix:
> tweettdm<-termdocumentmatrix(tweetcorpus)
> tweettdm
a term-document matrix (375 terms, 100 documents)
non-/sparse entries: 610/36890
sparsity           : 98%

130

maximal term length: 21 
weighting          : term frequency (tf)
a term-document matrix, also sometimes called a document-term 
matrix, is a rectangular data structure with terms as the rows and 
documents as the columns (in other uses you may also make the 
terms as columns and documents as rows). a term may be a single 
word, for example, "biology," or it could also be a compound word, 
such as "data analysis." the process of determining whether words 
go together in a compound word can be accomplished statistically 
by seeing which words commonly go together, or it can be done 
with a dictionary. the tm package supports the dictionary ap-
proach, but we have not used a dictionary in this example. so if a 
term like "data" appears once in the    rst document, twice in the sec-
ond document, and not at all in the third document, then the col-
umn for the term data will contain 1, 2, 0.
the statistics reported when we ask for tweettdm on the com-
mand line give us an overview of the results. the termdocument-
matrix() function extracted 375 different terms from the 100 tweets. 
the resulting matrix mainly consists of zeros: out of 37,500 cells in 
the matrix, only 610 contain non-zero entries, while 36,890 contain 
zeros. a zero in a cell means that that particular term did not ap-
pear in that particular document. the maximal term length was 21 
words, which an inspection of the input tweets indicates is also the 
maximum word length of the input tweets. finally, the last line, 
starting with "weighting" indicates what kind of statistic was 
stored in the term-document matrix. in this case we used the de-
fault, and simplest, option which simply records the count of the 
number of times a term appears across all of the documents in the 

corpus. you can peek at what the term-document matrix contains 
by using the inspect function:
inspect(tweettdm)
be prepared for a large amount of output. remember the term 
"sparse" in the summary of the matrix? sparse refers to the over-
whelming number of cells that contain zero - indicating that the 
particular term does not appear in a given document. most term 
document matrices are quite sparse. this one is 98% sparse be-
cause 36890/37500 = 0.98. in most cases we will need to cull or    l-
ter the term-document matrix for purposes of presenting or visual-
izing it. the tm package provides several methods for    ltering out 
sparsely used terms, but in this example we are going to leave the 
heavy lifting to the word cloud package.
as a    rst step we need to install and library() the "wordcloud" 
package. as with other packages, either use the package interface 
in r-studio or the ensurepackage() function that we wrote a few 
chapters ago. the wordcloud package was written by freelance stat-
istician ian fellows, who also developed the "deducer" user inter-
face for r. deducer provides a graphical interface that allows users 
who are more familiar with spss or sas menu systems to be able 
to use r without resorting to the command line.
once the wordcloud package is loaded, we need to do a little 
preparation to get our data ready to submit to the word cloud gen-
erator function. that function expects two vectors as input argu-
ments, the    rst a list of the terms, and the second a list of the fre-
quencies of occurrence of the terms. the list of terms and frequen-
cies must be sorted with the most frequent terms appearing    rst. 
to accomplish this we    rst have to coerce our tweet data back into 

131

a plain data matrix so that we can sort it by frequency. the    rst 
command below accomplishes this:
> tdmatrix <- as.matrix(tweettdm)
> sortedmatrix<-sort(rowsums(tdmatrix),+   
                              decreasing=true)
> cloudframe<-data.frame( +   
   word=names(sortedmatrix),freq=sortedmatrix)
> wordcloud(cloudframe$word,cloudframe$freq)
in the next command above, we are accomplishing two things in 
one command: we are calculating the sums across each row, which 
gives us the total frequency of a term across all of the different 
tweets/documents. we are also sorting the resulting values with 
the highest frequencies    rst. the result is a named list: each item of 
the list has a frequency and the name of each item is the term to 
which that frequency applies.
in the second to last command above, we are extracting the names 
from the named list in the previous command and binding them 
together into a dataframe with the frequencies. this dataframe, 
"cloudframe", contains exactly the same information as the named 
list. "sortedmatrix," but cloudframe has the names in a separate 
column of data. this makes it easier to do the    nal command 
above, which is the call to the wordcloud() function. the word-
cloud() function has lots of optional parameters for making the 
word cloud more colorful, controlling its shape, and controlling 
how frequent an item must be before it appears in the cloud, but 
we have used the default settings for all of these parameters for the 
sake of simplicity. we pass to the wordcloud() function the term 

list and frequency list that we bound into the dataframe and word-
cloud() produces the nice graphic that you see below.  

if you recall the twitter search that we used to retrieve those 
tweets (#solar) it makes perfect sense that "solar" is the most fre-
quent term (even though we    ltered out all of the hashtags. the 
next most popular term is "energy" and after that there are a vari-
ety of related words such as "independence," "green," "wind," and 
"metering."

132

# tweet texts
cleantweets<-function(tweets)
{
  # remove redundant spaces
  tweets <- str_replace_all(tweets,"  "," ")
  # get rid of urls
  tweets <- str_replace_all(tweets, +    
               "http://t.co/[a-z,a-z,0-9]{8}","")
  # take out retweet header, there is only one
  tweets <- str_replace(tweets,"rt @[a-z,a-z]*: ","")
  # get rid of hashtags
  tweets <- str_replace_all(tweets,"#[a-z,a-z]*","")
  # get rid of references to other screennames
  tweets <- str_replace_all(tweets,"@[a-z,a-z]*","")
  return(tweets)
}

chapter challenge
develop a function that builds upon previous functions we have 
developed, such as tweetframe() and cleantweets(), to take a 
search term, conduct a twitter search, clean up the resulting texts, 
formulate a term-document matrix, and submit resulting term fre-
quencies to the wordcloud() function. basically this would be a 
"turnkey" package that would take a twitter search term and pro-
duce a word cloud from it, much like the jason davies site de-
scribed at the beginning of this chapter. 
sources used in this chapter
http://cran.r-project.org/web/packages/wordcloud/wordcloud.
pdf 
http://www.dbai.tuwien.ac.at/staff/feinerer/ 
http://en.wikipedia.org/wiki/document-term_matrix
http://en.wikipedia.org/wiki/stop_words 
http://en.wikipedia.org/wiki/text_mining 
http://stat.ethz.ch/r-manual/r-devel/library/base/html/colsu
ms.html 
http://www.jasondavies.com/wordcloud/ 

r code for cleantweets() function
# cleantweets() - takes the junk out of a vector of

133

chapter 14

storage wars

before now we have only used small amount of data that we typed in ourselves, or somewhat larger 
amounts that we extracted from twitter. the world is full of other sources of data, however, and we 
need to examine how to get them into r, or at least how to make them accessible for manipulation in 
r. in this chapter, we examine various ways that data are stored, and how to access them.

134

most people who have watched the evolution of technology over 
recent decades remember a time when storage was expensive and 
it had to be hoarded like gold. over the last few years, however, 
the accelerating trend of moore   s law has made data storage al-
most "too cheap to meter" (as they used to predict about nuclear 
power). although this opens many opportunities, it also means 
that people keep data around for a long time, since it doesn   t make 
sense to delete anything, and they may keep data around in many 
different formats. as a result, the world is full of different data for-
mats, some of which are proprietary - designed and owned by a 
single company such as sas - and some of which are open, such as 
the lowly but in   nitely useful "comma separated variable," or csv 
format.
in fact, one of the basic dividing lines in data formats is whether 
data are human readable or not. formats that are not human read-
able, often called binary formats, are very ef   cient in terms of how 
much data they can pack in per kilobyte, but are also squirrelly in 
the sense that it is hard to see what is going on inside of the format. 
as you might expect, human readable formats are inef   cient from 
a storage standpoint, but easy to diagnose when something goes 
wrong. for high volume applications, such as credit card process-
ing, the data that is exchanged between systems is almost univer-
sally in binary formats. when a data set is archived for later reuse, 
for example in the case of government data sets available to the 
public, they are usually available in multiple formats, at least one 
of which is a human readable format.
another dividing line, as mentioned above is between proprietary 
and open formats. one of the most common ways of storing and 
sharing small datasets is as microsoft excel spreadsheets. although 

this is a proprietary format, owned by microsoft, it has also be-
come a kind of informal and ubiquitous standard. dozens of differ-
ent software applications can read excel formats (there are several 
different formats that match different versions of excel). in con-
trast, the opendocument format is an open format, managed by a 
standards consortium, that anyone can use without worrying what 
the owner might do. opendocument format is based on xml, 
which stands for extensible markup language. xml is a whole 
topic in and of itself, but brie   y it is a data exchange format de-
signed speci   cally to work on the internet and is both human and 
machine readable. xml is managed by the w3c consortium, 
which is responsible for developing and maintaining the many 
standards and protocols that support the web.
as an open source program with many contributors, r offers a 
wide variety of methods of connecting with external data sources. 
this is both a blessing and a curse. there is a solution to almost 
any data access problem you can imagine with r, but there is also 
a dizzying array of options available such that it is not always obvi-
ous what to choose. we   ll tackle this problem in two different 
ways. in the    rst half of this chapter we will look at methods for 
importing existing datasets. these may exist on a local computer 
or on the internet but the characteristic they share in common is 
that they are contained (usually) within one single    le. the main 
trick here is to choose the right command to import that data into 
r. in the second half of the chapter, we will consider a different 
strategy, namely linking to a "source" of data that is not a    le. 
many data sources, particularly databases, exist not as a single dis-
crete    le, but rather as a system. the system provides methods or 
calls to "query" data from the system, but from the perspective of 
the user (and of r) the data never really take the form of a    le.

135

the    rst and easiest strategy for getting data into r is to use the 
data import dialog in r-studio. in the upper right hand pane of r-
studio, the "workspace" tab gives views of currently available data 
objects, but also has a set of buttons at the top for managing the 
work space. one of the choices there is the "import dataset" but-
ton: this enables a drop down menu where one choice is to import, 
"from text file..." if you click this option and choose an appropri-
ate    le you will get a screen like this:

the most important stuff is on the left side. heading controls 
whether or not the    rst line of the text    le is treated as containing 
variable names. the separator drop down gives a choice of differ-
ent characters that separate the    elds/columns in the data. r-
studio tries to guess the appropriate choice here based on a scan of 

136

the data. in this case it guessed right by choosing "tab-delimited." 
as mentioned above, tab-delimited and comma-delimited are the 
two most common formats used for interchange between data 
programs.the next drop down is for "decimal" and this option ac-
counts for the fact the a dot is used in the u.s. while a comma may 
be used for the decimal point in europe and elsewhere. finally, the 
"quote" drop down controls which character is used to contain 
quoted string/text data. the most common method is double 
quotes. 
of course, we skipped ahead a bit here because we assumed that 
an appropriate    le of data was available. it might be useful to see 
some examples of human readable data:
name, age, gender   
"fred",22,"m"   
"ginger",21,"f"
the above is a very simple example of a comma delimited    le 
where the    rst row contains a "header," i.e. the information about 
the names of variables. the second and subsequent rows contain 
actual data. each    eld is separated by a comma, and the text 
strings are enclosed in double quotes. the same    le tab-delimited 
might look like this:
age! gender   
name!
22!
"fred"!
"ginger"! 21!
of course you can   t see the tab characters on the screen, but there is 
one tab character in between each pair of values. in each case, for 
both comma- and tab-delimited, one line equals one row. the end 

"m"   
"f"

of a line is marked, invisibly, with a so called "newline" character. 
on occasion you may run into differences between different operat-
ing systems on how this end of line designation is encoded. 
files containing comma or tab delimited data are ubiquitous across 
the internet, but sometimes we would like to gain direct access to 
binary    les in other formats. there are a variety of packages that 
one might use to access binary data. a comprehensive access list 
appears here:
http://cran.r-project.org/doc/manuals/r-data.html
this page shows a range of methods for obtaining data from a 
wide variety of programs and formats. because excel is such a 
widely used program for small, informal data sets, we will use it as 
an example here to illustrate both the power and the pitfalls of ac-
cessing binary data with r. down near the bottom of the page men-
tioned just above there are several paragraphs of discussion of how 
to access excel    les with r. in fact, the    rst sentence mentions that 
one of the most commonly asked data questions about r is how to 
access excel data. 
interestingly, this is one area where mac and linux users are at a 
disadvantage relative to windows users. this is perhaps because 
excel is a microsoft product, originally written to be native to win-
dows, and as a result it is easier to create tools that work with win-
dows. one example noted here is the package called rodbc. the 
acronym odbc stands for open database connection, and this is  
a windows facility for exchanging data among windows pro-
grams. although there is a proprietary odbc driver available for 
the mac, most mac users will want to try a different method for get-
ting access to excel data. 

another windows-only package for r is called xlsreadwrite. this 
package provides convenient one-command calls for importing 
data directly from excel spreadsheets or exporting it directly to 
spreadsheets. there are also more detailed commands that allow 
manipulating individual cells.
two additional packages, xlsx and xlconnect, supposedly will 
work with the mac, but at the time of this writing both of these 
packages had version incompatibilities that made it impossible to 
install the packages directly into r. note that the vast majority of 
packages provide the raw "source code" and so it is theoretically 
possible, but generally highly time consuming, to "compile" your 
own copies of these packages to create your own installation. 
fortunately, a general purpose data manipulation package called 
gdata provides essential facilities for importing spreadsheet    les. 
in the example that follows, we will use a function from gdata to 
read excel data directly from a website. the gdata package is a 
kind of "swiss army knife" package containing many different 
functions for accessing and manipulating data. for example, you 
may recall that r uses the value "na" to represent missing data. 
frequently, however, it is the case that data sets contain other val-
ues, such as 999, to represent missing data. the gdata package has 
several functions that    nd and transform these values to be consis-
tent with r   s strategy for handling missing data.
begin by using install.package() and library() functions to prepare 
the gdata package for use:
> install.packages("gdata")
# ... lots of output here

137

> library("gdata")
gdata: read.xls support for 'xls' (excel 97-2004) 
files
gdata: enabled.
gdata: read.xls support for 'xlsx' (excel 2007+) 
files enabled.
of course, you could also use the ensurepackage() function that 
we developed in an earlier chapter, but it was important here to 
see the output from the library() function. note that the gdata pack-
age reported some diagnostics about the different versions of excel 
data that it supports. note that this is one of the major drawbacks 
of binary data formats, particularly proprietary ones: you have to 
make sure that you have the right software to access the different 
versions of data that you might encounter. in this case it looks like 
we are covered for the early versions of excel (97-2004) as well as 
later versions of excel (2007+). we must always be on the lookout, 
however, for data that is stored in even newer versions of excel 
that may not be supported by gdata or other packages.
now that gdata is installed, we can use the read.xls() function that 
it provides. the documentation for the gdata package and the 
read.xls() function is located here:
http://cran.r-project.org/web/packages/gdata/gdata.pdf
a review of the documentation reveals that the only required argu-
ment to this function is the location of the xls    le, and that this lo-
cation can be a pathname, a web location with http, or an internet 
location with ftp (   le transfer protocol, a way of sending and re-
ceiving    les without using a web browser). if you hearken back to 

138

a very early chapter in this book, you may remember that we ac-
cessed some census data that had population counts for all the dif-
ferent u.s. states. for this example, we are going to read the excel 
   le containing that data directly into a dataframe using the 
read.xls() function:
> testframe<-read.xls( +   
"http://www.census.gov/popest/data/state/totals/2011/
tables/nst-est2011-01.xls")
trying url 
'http://www.census.gov/popest/data/state/totals/2011/
tables/nst-est2011-01.xls'
content type 'application/vnd.ms-excel' length 31232 
bytes (30 kb)
opened url
==================================================
downloaded 30 kb
the command in the    rst three lines above provides the url of the 
excel    le to the read.xls() function. the subsequent lines of output 
show the function attempting to open the url, succeeding, and 
downloading 30 kilobytes of data.
next, let   s take a look at what we got back. in r-studio we can 
click on the name of the dataframe in the upper right hand data 
pane or we can use this command:
> view(testframe)

either method will show the contents of the dataframe in the up-
per left hand window of r-studio. alternatively, we could use the 
str() function to create a summary of the structure of testframe:
> str(testframe)
'data.frame':	 65 obs. of  10 variables:
 $ 
table.with.row.headers.in.column.a.and.column.hea
ders.in.rows.3.through.4...leading.dots.indicate.
sub.parts.: factor w/ 65 levels "",".alabama",..: 
62 53 1 64 55 54 60 65 2 3 ...
 $ x                                                                                                           
: factor w/ 60 levels "","1,052,567",..: 1 59 60 
27 38 47 10 49 32 50 ...
 $ x.1                                                                                                         
: factor w/ 59 levels "","1,052,567",..: 1 1 59 
27 38 47 10 49 32 50 ...
 $ x.2                                                                                                         
: factor w/ 60 levels "","1,052,528",..: 1 60 21 
28 39 48 10 51 33 50 ...
 $ x.3                                                                                                         
: factor w/ 59 levels "","1,051,302",..: 1 1 21 
28 38 48 10 50 33 51 ...
 $ x.4                                                                                                         
: logi  na na na na na na ...
 $ x.5                                                                                                         
: logi  na na na na na na ...

 $ x.6                                                                                                         
: logi  na na na na na na ...
 $ x.7                                                                                                         
: logi  na na na na na na ...
 $ x.8                                                                                                         
: logi  na na na na na na ...
the last few lines are reminiscent of that late 60s song entitled, 
""na na hey hey kiss him goodbye." setting aside all the na na 
na nas, however, the overall structure is 65 observations of 10 
variables, signifying that the spreadsheet contained 65 rows and 10 
columns of data. the variable names that follow are pretty bizarre. 
the    rst variable name is:
 
"table.with.row.headers.in.column.a.and.column.headers.in.rows.
3.through.4...leading.dots.indicate.sub.parts." 
what a mess! it is clear that read.xls() treated the upper leftmost 
cell as a variable label, but was    ummoxed by the fact that this was  
really just a note to human users of the spreadsheet (the variable 
labels, such as they are, came on lower rows of the spreadsheet). 
subsequent variable names include x, x.1, and x.2: clearly the 
read.xls() function did not have an easy time getting the variable 
names out of this    le.
the other worrisome    nding from str() is that all of our data are 
"factors." this indicates that r did not see the incoming data as 
numbers, but rather as character strings that it interpreted as factor 
data. again, this is a side effect of the fact that some of the    rst 
cells that read.xls() encountered were text rather than numeric. the 

139

numbers came much later in the sheet. this also underscores the 
idea that it is much better to export a data set in a more regular, 
structured format such as csv rather than in the original spread-
sheet format. clearly we have some work to do if we are to make 
use of these data as numeric population values.  
first, we will use an easy trick to get rid of stuff we don   t need. the 
census bureau put in three header rows that we can eliminate like 
this:
> testframe<-testframe[-1:-3,]
the minus sign used inside the square brackets refers to the index 
of rows that should be eliminated from the dataframe. so the nota-
tion -1:-3 gets rid of the    rst three rows. we also leave the column 
designator empty so that we can keep all columns for now. so the 
interpretation of all of the notation within the square brackets is 
that rows 1 through 3 should be dropped, all other rows should be 
included, and all columns should be included. we assign the result 
back to the same data object thereby replacing the original with 
our new, smaller, cleaner version. 
next, we know that of the ten variables we got from read.xls(), 
only the    rst    ve are useful to us (the last    ve seem to be blank). so 
this command keeps the    rst    ve columns of the dataframe:
> testframe<-testframe[,1:5]
in the same vein, the tail() function shows us that the last few rows 
just contained some census bureau notes:
> tail(testframe,5)
so we can safely eliminate those like this:

> testframe<-testframe[-58:-62,]
if you   re alert you will notice that we could have combined some 
of these commands, but for the sake of clarity we have done each 
operation individually. the result (which you can check in the up-
per right hand pane of r-studio) is a dataframe with 57 rows and 
   ve observations. now we are ready to perform a couple of data 
transformations. before we start these, let   s give our    rst column a 
more reasonable name:
> testframe$region <- testframe[,1]
we   ve used a little hack here to avoid typing out the ridiculously 
long name of that    rst variable/column. we   ve used the column 
notation in the square brackets on the right hand side of the expres-
sion to refer to the    rst column (the one with the ridiculous name) 
and simply copied the data into a new column entitled "region." 
let   s also remove the offending column with the stupid name so 
that it does not cause us problems later on:
> testframe<-testframe[,-1]
next, we can change formats and data types as needed. we can re-
move the dots from in front of the state names very easily with 
str_replace():
> testframe$region <- str_replace( +   
                       testframe$region,"\\.","")
don   t forget that str_replace() is part of the stringr package, and 
you will have to use install.packages() and library() to load it if it is 
not already in place. the two backslashes in the string expression 
above are called "escape characters" and they force the dot that fol-
lows to be treated as a literal dot rather than as a wildcard charac-

140

ter. the dot on its own is a wildcard that matches one instance of 
any character.
next, we can use str_replace_all() and as.numeric() to convert the 
data contained in the population columns to usable numbers. re-
member that those columns are now represented as r "factors" and 
what we are doing is taking apart the factor labels (which are basi-
cally character strings that look like this: "308,745,538") and making 
them into numbers. this is suf   ciently repetitive that we could 
probably bene   t by created our own function call to do it:
# numberize() - gets rid of commas and other junk and 
# converts to numbers
# assumes that the inputvector is a list of data that 
# can be treated as character strings
numberize <- function(inputvector)
{
  # get rid of commas
  inputvector<-str_replace_all(inputvector,",","")
  # get rid of spaces
  inputvector<-str_replace_all(inputvector," ","")
  
  return(as.numeric(inputvector))
}
this function is    exible in that it will deal with both unwanted 
commas and spaces, and will convert strings into numbers 

whether they are integers or not (i.e., possibly with digits after the 
decimal point). so we can now run this a few times to create new 
vectors on the dataframe that contain the numeric values we 
wanted:
testframe$april10census <-numberize(testframe$x)
testframe$april10base <-numberize(testframe$x.1)
testframe$july10pop <-numberize(testframe$x.2)
testframe$july11pop <-numberize(testframe$x.3)
by the way, the choice of variable names for the new columns in 
the dataframe was based on an examination of the original data set 
that was imported by read.xls(). you can (and should) con   rm that 
the new columns on the dataframe are numeric. you can use str() 
to accomplish this. 
we   ve spent half a chapter so far just looking at one method of im-
porting data from an external    le (either on the web or local stor-
age). a lot of our time was spent conditioning the data we got in 
order to make it usable for later analysis. herein lies a very impor-
tant lesson (or perhaps two). an important, and sometimes time 
consuming aspect of what data scientists do is to make sure that 
data are "   t for the purpose" to which they are going to be put. we 
had the convenience of importing a nice data set directly from the 
web with one simple command, and yet getting those data actually 
ready to analyze took several additional steps.
a related lesson is that it is important and valuable to try to auto-
mate as many of these steps as possible. so when we saw that num-
bers had gotten stored as factor labels, we moved immediately to 
create a general function that would convert these to numbers. not 

141

only does this save a lot of future typing, it prevents mistakes from 
creeping into our processes.
now we are ready to consider the other strategy for getting access 
to data: querying it from external databases. depending upon your 
familiarity with computer programming and databases, you may 
notice that the abstraction is quite a bit different here. earlier in the 
chapter we had a    le (a rather messy one) that contained a com-
plete copy of the data that we wanted, and we read that    le into r 
and stored it in our local computer   s memory (and possibly later 
on the hard disk for safekeeping). this is a good and reasonable 
strategy for small to medium sized datasets, let   s say just for the 
sake of argument anything up to 100 megabytes.
but what if the data you want to work with is really large - too 
large to represent in your computer   s memory all at once and too 
large to store on your own hard drive. this situation could occur 
even with smaller datasets if the data owner did not want people 
making complete copies of their data, but rather wanted everyone 
who was using it to work from one "of   cial" version of the data. 
similarly, if data do need to be shared among multiple users, it is 
much better to have them in a database that was designed for this 
purpose: for the most part r is a poor choice for maintaining data 
that must be used simultaneously by more than one user. for these 
reasons, it becomes necessary to do one or both of the following 
things:
1. allow r to send messages to the large, remote database asking 

for summaries, subsets, or samples of the data.

2. allow r to send computation requests to a distributed data proc-
essing system asking for the results of calculations performed on 
the large remote database.

like most contemporary programming languages, r provides sev-
eral methods for performing these two tasks. the strategy is the 
same across most of these methods: a package for r provides a "cli-
ent" that can connect up to the database server. the r client sup-
ports sending commands - mostly in sql, structured query lan-
guage - to the database server. the database server returns a result 
to the r client, which places it in an r data object (typically a data 
frame) for use in further processing or visualization.
the r community has developed a range of client software to en-
able r to connect up with other databases. here are the major data-
bases for which r has client software:
    rmysql - connects to mysql, perhaps the most popular open 
source database in the world. mysql is the m in "lamp" which 
is the acronym for linux, apache, mysql, and php. together, 
these four elements provide a complete solution for data driven 
web applications.

    roracle - connects with the widely used oracle commercial da-
tabase package. oracle is probably the most widely used com-
mercial database package. ironically, oracle acquired sun micro-
systems a few years ago and sun developers predominate in de-
velopment and control of the open source mysql system,

    rpostgresql - connects with the well-developed, full featured 
postgresql (sometimes just called postgres) database system. 
postgresql is a much more venerable system than mysql and 

142

has a much larger developer community. unlike mysql, which 
is effectively now controlled by oracle, postgresql has a devel-
oper community that is independent of any company and a li-
censing scheme that allows anybody to modify and reuse the 
code.

    rsqlite - connects with sqlite, another open source, independ-
ently developed database system. as the name suggests, sqlite 
has a very light "code footprint" meaning that it is fast and com-
pact.

    rmongo - connects with the mongodb system, which is the 

only system here that does not use sql. instead, mongodb uses 
javascript to access data. as such it is well suited for web devel-
opment applications.

    rodbc - connects with odbc compliant databases, which in-

clude microsoft   s sqlserver, microsoft access, and microsoft ex-
cel, among others. note that these applications are native to win-
dows and windows server, and as such the support for linux 
and mac os is limited.

for demonstration purposes, we will use rmysql. this requires 
installing a copy of mysql on your computer. use your web 
browser to go to this page:
http://dev.mysql.com/downloads/ 
then look for the "mysql community server." the term commu-
nity in this context refers to the free, open source developer com-
munity version of mysql. note that there are also commercial ver-
sions of sql developed and marketed by various companies in-
cluding oracle. download the version of mysql community 

server that is most appropriate for your computer   s operating sys-
tem and install it. note that unlike user applications, such as a 
word processor, there is no real user interface to server software 
like the mysql community server. instead, this software runs in 
the "background" providing services that other programs can use. 
this is the essence of the client-server idea. in many cases the 
server is on some remote computer to which we do not have physi-
cal access. in this case, we will run the server on our local com-
puter so that we can complete the demonstration. 
on the mac installation used in preparation of this chapter, after 
installing the mysql server software, it was also important to in-
stall the "mysql preference pane," in order to provide a simple 
graphical interface for turning the server on and off. because we 
are just doing a demonstration here, and we want to avoid future 
security problems, it is probably sensible to turn mysql server off 
when we are done with the demonstration. in windows, you can 
use mysql workbench to control the server settings on your local 
computer.
returning to r, use install.packages() and library() to prepare the 
rmysql package for use. if everything is working the way it 
should, you should be able to run the following command from 
the command line:
> con <- dbconnect(dbdriver("mysql"), dbname = "test")
the dbconnect() function establishes a linkage or "connection" be-
tween r and the database we want to use. this underscores the 
point that we are connecting to an "external" resource and we must 
therefore manage the connection. if there were security controls in-
volved, this is where we would provide the necessary information 

143

to establish that we were authorized users of the database. in this 
case, because we are on a "local server" of mysql, we don   t need 
to provide these. the dbdriver() function provided as an argument 
to dbconnect speci   es that we want to use a mysql client. the da-
tabase name - speci   ed as dbname="test" - is just a placeholder at 
this point. we can use the dblisttables() function to see what ta-
bles are accessible to us (for our purposes, a table is just like a data-
frame, but it is stored inside the database system):
> dblisttables(con)
character(0)
the response "character(0)" means that there is an empty list, so no 
tables are available to us. this is not surprising, because we just in-
stalled mysql and have not used it for anything yet. unless you 
have another database available to import into mysql, we can just 
use the census data we obtained earlier in the chapter to create a 
table in mysql:
> dbwritetable(con, "census", testframe, +   
               overwrite = true)
[1] true
take note of the arguments supplied to the dbwritetable() func-
tion. the    rst argument provides the database connection that we 
established with the dbconnect() function. the "census" argument 
gives our new table in mysql a name. we use testframe as the 
source of data - as noted above a dataframe and a relational data-
base table are very similar in structure. finally, we provide the ar-
gument overwrite=true, which was not really needed in this case 
- because we know that there were no existing tables - but could be 

important in other operations where we need to make sure to re-
place any old table that may have been left around from previous 
work. the function returns the logical value true to signal that it 
was able to    nish the request that we made. this is important in 
programming new functions because we can use the signal of suc-
cess or failure to guide subsequent steps and provide error or suc-
cess messages.
now if we run dblisttables() we should see our new table:
> dblisttables(con)
[1] "census"
now we can run an sql query on our table:
> dbgetquery(con, "select region, july11pop from      
census where july11pop<1000000")
                region july11pop
1               alaska    722718
2             delaware    907135
3 district of columbia    617996
4              montana    998199
5         north dakota    683932
6         south dakota    824082
7              vermont    626431
8              wyoming    568158
note that the dbgetquery() call shown above breaks onto two 
lines, but the string starting with select has to be typed all on 

144

one line. the capitalized words in that string are the sql com-
mands. it is beyond the scope of this chapter to give an sql tuto-
rial, but, brie   y, select chooses a subset of the table and the 
   elds named after select are the ones that will appear in the result. 
the from command choose the table(s) where the data should 
come from. the where command speci   ed a condition, in this 
case that we only wanted rows where the july 2011 population was 
less than one million. sql is a powerful and    exible language and 
this just scratches the surface.
in this case we did not assign the results of dbgetquery() to an-
other data object, so the results were just echoed to the r console. 
but it would be easy to assign the results to a dataframe and then 
use that dataframe for subsequent calculations or visualizations.
to emphasize a point made above, the normal motivation for ac-
cessing data through mysql or another database system is that a 
large database exists on a remote server. rather than having our 
own complete copy of those data, we can use dbconnect(), 
dbgetquery() and other database functions to access the remote 
data through sql. we can also use sql to specify subsets of the 
data, to preprocess the data with sorts and other operations, and to 
create summaries of the data. sql is also particularly well suited 
to "joining" data from multiple tables to make new combinations. 
in the present example, we only used one table, it was a very small 
table, and we had created it ourselves in r from an excel source, so 
none of these were very good motivations for storing our data in 
mysql, but this was only a demonstration.
the next step beyond remote databases is toward distributed com-
puting across a "cluster" of computers. this combines the remote 

access to data that we just demonstrated with additional computa-
tional capabilities. at this writing, one of the most popular systems 
for large scale distributed storage and computing is "hadoop" 
(named after the toy elephant of the young son of the developer). 
hadoop is not a single thing, but is rather a combination of pieces 
of software called a library. hadoop is developed and maintained 
by the same people who maintain the apache open source web 
server. there are about a dozen different parts of the hadoop 
framework, but the hadoop distributed files system (hdfs) and 
hadoop mapreduce framework are two of the most important 
frameworks.
hdfs is easy to explain. imagine your computer and several other 
computers at your home or workplace. if we could get them all to 
work together, we could call them a "cluster" and we could theoreti-
cally get more use out of them by taking advantage of all of the 
storage and computing power they have as a group. running 
hdfs, we can treat this cluster of computers as one big hard drive. 
if we have a really large    le - too big to    t on any one of the com-
puters - hdfs can divide up the    le and store its different parts in 
different storage areas without us having to worry about the de-
tails. with a proper con   guration of computer hardware, such as 
an it department could supply, hdfs can provide an enormous 
amount of "throughput" (i.e., a very fast capability for reading and 
writing data) as well as redundancy and failure tolerance. 
mapreduce is a bit more complicated, but it follows the same logic 
of trying to divide up work across multiple computers. the term 
mapreduce is used because there are two big processes involved: 
map and reduce. for the map operation, a big job is broken up into 

145

lots of separate parts. for example, if we wanted to create a search 
index for all of the    les on a company   s intranet servers, we could 
break up the whole indexing task into a bunch of separate jobs. 
each job might take care of indexing the    les on one server.
in the end, though, we don   t want dozens or hundreds of different 
search indices. we want one big one that covers all of the    les our 
company owns. this is where the reduce operation comes in. as all 
of the individual indexing jobs    nish up, a reduce operation com-
bines them into one big job. this combining process works on the 
basis of a so called "key." in the search indexing example, some of 
the small jobs might have found    les that contained the word 
"   sh." as each small job    nishes, it mentioned whether or not    sh 
appeared in a document and perhaps how many times    sh ap-
peared. the reduce operation uses    sh as a key to match up the re-
sults from all of the different jobs, thus creating an aggregated sum-
mary listing all of the documents that contained    sh. later, if any-
one searched on the word    sh, this list could be used to direct 
them to documents that contained the word.
in short, "map" takes a process that the user speci   es and an indica-
tion of which data it applies to, and divides the processing into as 
many separate chunks as possible. as the results of each chunk be-
come available, "reduce" combines them and eventually creates 
and returns one aggregated result. 
recently, an organization called revolutionanalytics has devel-
oped an r interface or "wrapper" for hadoop that they call rha-
doop. this package is still a work in progress in the sense that it 
does not appear in the standard cran package archive, not be-
cause there is anything wrong with it, but rather because revolu-

tionanalytics wants to continue to develop it without having to 
provide stable versions for the r community. there is a nice tuto-
rial here:
https://github.com/revolutionanalytics/rhadoop/wiki/tutoria
l
we will break open the    rst example presented in the tutorial just 
to provide further illustration of the use of mapreduce. as with 
our mysql example, this is a rather trivial activity that would not 
normally require the use of a large cluster of computers, but it does 
show how mapreduce can be put to use.
the tutorial example    rst demonstrates how a repetitive operation 
is accomplished in r without the use of mapreduce. in prior chap-
ters we have used several variations of the apply() function. the 
lapply() or list-apply is one of the simplest. you provide an input 
vector of values and a function to apply to each element, and the 
lapply() function does the heavy lifting. the example in the rha-
doop tutorial squares each integer from one to 10. this    rst com-
mand    lls a vector with the input data:
> small.ints <- 1:10
> small.ints
 [1]  1  2  3  4  5  6  7  8  9 10
next we can apply the "squaring function" (basically just using the 
^ operator) to each element of the list:
> out <- lapply(small.ints, function(x) x^2)
> out

146

[[1]]
[1] 1

[[2]]
[1] 4
... (shows all of the values up to [[10]] [1] 100)
in the    rst command above, we have used lapply() to perform a 
function on the input vector small.ints. we have de   ned the func-
tion as taking the value x and returning the value x^2. the result is 
a list of ten vectors (each with just one element) containing the 
squares of the input values. because this is such a small problem, r 
was able to accomplish it in a tiny fraction of a second.
after installing both hadoop and rhadoop - which, again, is not 
an of   cial package, and therefore has to be installed manually - we 
can perform this same operation with two commands:
> small.ints <- to.dfs(1:10)
> out <- mapreduce(input = small.ints, +   
              map = function(k,v) keyval(v, v^2))
in the    rst command, we again create a list of integers from one to 
ten. but rather than simply storing them in a vector, we are using 
the "distributed    le system" or dfs class that is provided by rha-
doop. note that in most cases we would not need to create this our-
selves because our large dataset would already exist on the hdfs 
(hadoop distributed file system). we would have connected to 

hdfs and selected the necessary data much as we did earlier in 
this chapter with dbconnect(). 
in the second command, we are doing essentially the same thing as 
we did with lapply(). we provide the input data structure (which, 
again is a dfs class data object, a kind of pointer to the data stored 
by hadoop in the cluster). we also provide a "map function" which 
is the process that we want to apply to each element in our data 
set. notice that the function takes two arguments, k and v. the k 
refers to the "key" that we mentioned earlier in the chapter. we ac-
tually don   t need the key in this example because we are not sup-
plying a reduce function. there is in fact, no aggregation or combin-
ing activity that needs to occur because our input list (the integers) 
and the output list (the squares of those integers) are lists of the 
same size. if we had needed to aggregate the results of the map 
function, say by creating a mean or a sum, we would have had to 
provide a "reduce function" that would do the job.
the keyval() function, for which there is no documentation at this 
writing, is characterized as a "helper" function in the tutorial. in 
this case it is clear that the    rst argument to keyval, "v" is the intger 
to which the process must be applied, and the second argument, 
"v^2" is the squaring function that is applied to each argument. 
the data returned by mapreduce() is functionally equivalent to 
that returned by lapply(), i.e., a list of the squares of the integers 
from 1 to 10.
obviously there is no point in harnessing the power of a cluster of 
computers to calculate something that could be done with a pencil 
and a paper in a few seconds. if, however, the operation was more 
complex and the list of input data had millions of elements, the use 

147

of lapply() would be impractical as it would take your computer 
quite a long time to    nish the job. on the other hand, the second 
strategy of using mapreduce() could run the job in a fraction of a 
second, given a suf   cient supply of computers and storage.
on a related note, amazon, the giant online retailer, provides vir-
tual computer clusters that can be used for exactly this kind of 
work. amazon   s product is called the elastic compute cloud or 
ec2, and at this writing it is possible to create a small cluster of 
linux computers for as little as eight cents per hour.
to summarize this chapter, although there are many analytical 
problems that require only a small amount of data, the wide avail-
ability of larger data sets has added new challenges to data science. 
as a single user program running on a local computer, r is well 
suited for work by a single analyst on a data set that is small 
enough to    t into the computer   s memory. we can retrieve these 
small datasets from individual    les stored in human readable 9e.g., 
csv) or binary (e.g., xls) formats.
to be able to tackle the larger data sets, however, we need to be 
able to connect r with either remote databases or remote computa-
tional resources or both. a variety of packages is available to con-
nect r to mature database technologies such as mysql. in fact, we 
demonstrated the use of mysql by installing it on a local machine 
and then using the rmysql package to create a table and query it. 
the more cutting edge technology of hadoop is just becoming 
available for r users. this technology, which provides the potential 
for both massive storage and parallel computational power, prom-
ises to make very large datasets available for processing and analy-
sis in r.

chapter challenge
hadoop is a software framework designed for use with apache, 
which is    rst and foremost a linux server application. yet there are 
development versions of hadoop available for windows and mac 
as well. these are what are called single node instances, that is 
they use a single computer to simulate the existence of a large clus-
ter of computers. see if you can install the appropriate version of 
hadoop for your computer   s operating system. 
as a bonus activity, if you are successful in installing hadoop, then 
get a copy of the rhadoop package from revolutionanalytics and 
install that. if you are successful with both, you should be able to 
run the mapreduce code presented in this chapter.
sources 
http://cran.r-project.org/doc/manuals/r-data.html
http://cran.r-project.org/doc/manuals/r-data.pdf 
http://cran.r-project.org/web/packages/gdata/gdata.pdf
http://dev.mysql.com/downloads/ 
http://en.wikipedia.org/wiki/comparison_of_relational_databas
e_management_systems 
http://en.wikipedia.org/wiki/mapreduce
https://github.com/revolutionanalytics/rhadoop/wiki/tutoria
l 
r functions used in this chapter

148

 

as.numeric() - convert another data type to a number
dbconnect() - connect to an sql database
dbgetquery() - run an sql query and return the results
dblisttables() - show the tables available in a connection
dbwritetable() - send a data table to an sql systems
install.packages() - get the code for an r package
lapply() - apply a function to elements of a list
library() - make an r package available for use
numberize() - a custom function created in this chapter
read.xls() - import data from a binary r    le; part of the gdata pack-
age
return() - used in functions to designate the data returned by the 
function
str_replace() - replace a character string with another
str_replace_all() - replace multiple instances of a character string 
with another

149

chapter 15

map mashup

much of what we have accomplished so far has focused on the standard rectangular dataset: one neat 
table with rows and columns well de   ned. yet much of the power of data science comes from 
bringing together difference sources of data in complementary ways. in this chapter we combine 
different sources of data to make a unique product that transcends any one source.

150

mashup is a term that originated in the music business decades 
ago related to the practice of overlaying one music recording on 
top of another one. the term has entered general usage to mean 
anything that brings together disparate in   uences or elements. in 
the application development area, mashup often refers to bringing 
together various sources of data to create a new product with 
unique value. there   s even a non-pro   t group called the open 
mashup alliance that develops standards and methods for creating 
new mashups.
one example of a mashup is http://www.housingmaps.com/ , a 
web application that grabs apartment rental listings from the classi-
   ed advertising service craigslist and plots them on an interactive 
map that shows the location of each listing. if you have ever used 
craigslist you know that it provides a very basic text-based inter-
face and that the capability to    nd listings on a map would be wel-
come.
in this chapter we tackle a similar problem. using some address 
data from government records, we call the google geocoding api 
over the web to    nd the latitude and longitude of each address. 
then we plot these latitudes and longitudes on a map of the u.s. 
this activities reuses skills we learned in the previous chapter for 
reading in data    les, adds some new skills related to calling web 
apis, and introduces us to a new type of data, namely the shape-
   les that provide the basis for electronic maps. 
let   s look at the new stuff    rst. the internet is full of shape   les 
that contain mapping data. shape   les are a partially proprietary, 
partially open format supported by a california software company 
called esri. shape   le is actually an umbrella term that covers sev-

eral different    le types. because the r community has provided 
some packages to help deal with shape   les, we don   t need to much 
information about the details. the most important thing to know is 
that shape   les contain points, polygons, and "polylines." everyone 
knows what a point and a polygon are, but a polyline is a term 
used by computer scientist to refer to a multi-segment line. in 
many graphics applications it is much easier to approximate a 
curved line with many tiny connected straight lines than it is to 
draw a truly curved line. if you think of a road or a river on a map, 
you will have a good idea of a polyline.
the u.s. census bureau publishes shape   les at various levels of de-
tail for every part of the country. search for the term "shape   le" at 
"site:census.gov" and you will    nd several pages with listings of 
different shape   les. for this exercise, we are using a relatively low 
detail map of the whole u.s. we downloaded a "zip"    le. unzip 
this (usually just by double-clicking on it) and you will    nd several 
   les inside it. the    le ending in "shp" is the main shape   le. an-
other    le that will be useful to us ends in "dbf" - this contains labels 
and other information.
to get started, we will need two new r packages called pbsmap-
ping and maptools. pbsmapping refers not to public broadcasting, 
but rather to the paci   c biology station, whose researchers and 
technologists kindly bundled up a wide range of the r processing 
tools that they use to manage map data.  the maptools package 
was developed by nicholas j. lewin-koh (university of singapore) 
and others to provide additional tools and some "wrappers" to 
make pbsmapping functions easier to use. in this chapter we only 
scratch the surface of the available tools: there could be a whole 
book just dedicated to r mapping functions alone. 

151

before we read in the data we grabbed from the census bureau, 
let   s set the working directory in r-studio so that we don   t have to 
type it out on the command line. click on the tools menu and then 
choose "set working directory." use the dialog box to designate 
the folder where you have unzipped the shape data. after that, 
these commands will load the shape data into r and show us what 
we have:
> usshape <- importshapefile( +   
           "gz_2010_us_040_00_500k",readdbf=true)
> summary(usshape)
polyset
records         : 90696
  contours      : 574
    holes       : 0
  events        : na
    on boundary : na
ranges
  x             : [-179.14734, 179.77847]
  y             : [17.884813, 71.3525606439998]
attributes
  projection    : ll
  zone          : null

extra columns   : 
> plotpolys(usshape)
this last command gives us a simple plot of the 90,696 shapes that 
our shape   le contains. here is the plot:

this is funny looking! the ranges output from the summary() com-
mand gives us a hint as to why. the longitude of the elements in 
our map range from -179 to nearly 180: this covers pretty much the 
whole of the planet. the reason is that the map contains shapes for 
hawaii and alaska. both states have far western longitudes, but 
the aleutian peninsula in alaska extends so far that it crosses over 
the longitude line where -180 and 180 meet in the paci   c ocean. as 

152

a result, the continental u.s. is super squashed. we can specify a 
more limited area of the map to consider by using the xlim and 
ylim parameters. the following command:
> plotpolys(usshape,+   
                  xlim=c(-130,-60),ylim=c(20,50))
...gives a plot that shows the continental u.s. more in its typical 
proportions.

> x <- -100
> y <- 30
> eid <- 1
> pointdata <- data.frame(eid,x,y)
> eventdata <- as.eventdata( +   
                         pointdata,projection=na)
> addpoints(eventdata,col="red",cex=.5)

so now we have some map data stored away and ready to use. the 
pbsmapping package gives us the capability of adding points to an 
existing map. for now, let   s demonstrate this with a made up point 
somewhere in texas:

you have to look carefully, but in southern texas there is now a lit-
tle red dot. we began by manually creating a single point - speci-
   ed by x (longitude), y (latitude), and eid (an identi   cation num-

153

ber) - and sticking it into a dataframe. then we converted the data 
in that dataframe into an eventdata object. this is a custom class 
of object speci   ed by the pbsmapping package. the    nal com-
mand above adds the eventdata to the plot. 
the idea of eventdata is a little confusing, but if you remember 
that this package was developed by biologists at the paci   c biol-
ogy station to map sightings of    sh and other natural phenomena 
it makes more sense. in their lingo, an event was some observation 
of interest that occurred at a particular day, time, and location. the 
"event id" or eid <- 1 that we stuck in the data frame was just say-
ing that this was the    rst point in our list that we wanted to plot. 
for us it is not an event so much as a location of something we 
wanted to see on the map.
also note that the "projection=na" parameter in the 
as.eventdata() coercion is just letting the mapping software know 
that we don   t want our point to be transformed according to a map-
ping projection. if you remember from your geography class, a pro-
jection is a mapping technique to make a curved object like the 
earth seem sensible on a    at map. in this example, we   ve already 
   attened out the u.s., so there is no need to transform the points.
next, we need a source of points to add to our map. this could be 
anything that we   re interested in: the locations of restaurants, 
crime scenes, colleges, etc. in google a search for    letype:xls or    le-
type;csv with appropriate additional search terms can provide in-
teresting data sources. you may also have mailing lists of custom-
ers or clients. the most important thing is that we will need street 
address, city, and state in order to geocode the addresses. for this 
example, we searched for "housing street address list    letype:csv" 

and this turned up a data set of small businesses that have con-
tracts with the u.s. department of health and human services. 
let   s read this in using read.csv():
> dhhsaddrs <- read.csv("dhhs_contracts.csv")
> str(dhhsaddrs)
'data.frame':	 599 obs. of  10 variables:
 $ contract.number           : factor w/ 285 lev-
els "26301d0054","500000049",..: 125 125 125 279 
164 247 19 242 275 70 ...
 $ contractor.name           : factor w/ 245 lev-
els "2020 company limited liability company",..: 
1 1 1 2 2 3 4 6 5 7 ...
 $ contractor.address        : factor w/ 244 lev-
els "1 chase square 10th flr, rochester, ny ",..: 
116 116 116 117 117 136 230 194 64 164 ...
 $ description.of.requirement: factor w/ 468 lev-
els "9th sow - defintize the letter contract",..: 
55 55 55 292 172 354 308 157 221 340 ...
 $ dollars.obligated         : factor w/ 586 lev-
els " $1,000,000.00 ",..: 342 561 335 314 294 2 
250 275 421 21 ...
 $ naics.code                : factor w/ 55 lev-
els "323119","334310",..: 26 26 26 25 10 38 33 29 
27 35 ...
 $ ultimate.completion.date  : factor w/ 206 lev-
els "1-aug-2011","1-feb-2013",..: 149 149 149 10 
175 161 124 37 150 91 ...

154

 $ contract.specialist       : factor w/ 95 lev-
els "alan  fredericks",..: 14 14 60 54 16 90 55 
25 58 57 ...
 $ contract.specialist.email : factor w/ 102 lev-
els "410-786-8622",..: 16 16 64 59 40 98 60 29 62 
62 ...
 $ x               : logi  na na na na na na ...
there   s that crazy 60s song again! anyhow, we read in a comma 
separated data set with 599 rows and 10 variables. the most impor-
tant    eld we have there is contractor.address. this contains the 
street addresses that we need to geocode. we note, however, that 
the data type for these is factor rather than character string. so we 
need to convert that:
> dhhsaddrs$straddr <- +   
       as.character(dhhsaddrs$contractor.address)
> mode(dhhsaddrs$straddr)
[1] "character"
> tail(dhhsaddrs$straddr,4)
[1] "1717 w broadway, madison, wi "   
[2] "1717 w broadway, madison, wi "  
[3] "1717 w broadway, madison, wi "   
[4] "789 howard ave, new haven, ct, "
this looks pretty good: our new column, called dhhsaddrs, is 
character string data, converted from the factor labels of the origi-
nal contractor.address column. now we need to geocode these. 

we will use the google geocoding application programming inter-
face (api) which is pretty easy to use, does not require an account 
or application id, and allows about 2500 address conversions per 
day. the api can be accessed over the web, using what is called an 
http get request. note that the terms of service for the google 
geocoding api are very speci   c about how the interface can be 
used - most notably on the point that the geocodes must be used 
on google maps. make sure you read the terms of service before 
you create any software applications that use the geocoding serv-
ice. see the link in the bibliography at the end of the chapter. the 
bibliography has a link to an article with dozens of other geocod-
ing apis if you disagree with google   s terms of service.
these acronyms probably look familiar. http is the hyper text 
transfer protocol, and it is the standard method for requesting and 
receiving web page data. a get request consists of information 
that is included in the url string to specify some details about the 
information we are hoping to get back from the request. here is an 
example get request to the google geocoding api:
http://maps.googleapis.com/maps/api/geocode/json?address=1
600+pennsylvania+avenue,+washington,+dc&sensor=false
the    rst part of this should look familiar: the 
http://maps.googleapis.com part of the url speci   es the domain 
name just like a regular web page. the next part of the url,    
"/maps/api/geocode" tells google which api we want to use. 
then the "json" indicates that we would like to receive our result in 
"java script object notation" (json) which is a structured, but hu-
man readable way of sending back some data. the address appears 
next, and we are apparently looking for the white house at 1600 

155

pennsylvania avenue in washington, dc. finally, sensor=false is a 
required parameter indicating that we are not sending our request 
from a mobile phone. you can type that whole url into the ad-
dress    eld of any web browser and you should get a sensible result 
back. the json notation is not beautiful, but you will see that it 
makes sense and provides the names of individual data items 
along with their values. here   s a small excerpt that shows the key 
parts of the data object that we are trying to get our hands on:
{
   "results" : [
      {
         "address_components" : [
         "geometry" : {
            "location" : {
               "lat" : 38.89788009999999,
               "lng" : -77.03664780
            },
   "status" : "ok"
}
there   s tons more data in the json object that google returned, 
and fortunately there is an r package, called jsonio, that will ex-
tract the data we need from the structure without having to parse 
it ourselves. 

in order to get r to send the http get requests to google, we will 
also need to use the rcurl package. this will give us a single com-
mand to send the request and receive the result back - essentially 
doing all of the quirky steps that a web browser takes care of auto-
matically for us. to get started, install.packages() and library() the 
two packages we will need - rcurl and jsonio. if you are work-
ing on a windows machine, you may need to jump through a hoop 
or two to get rcurl, but it is available for windows even if it is not 
in the standard cran repository. search for "rcurl windows" if 
you run into trouble.
next, we will create a new helper function to take the address    eld 
and turn it into the url that we need:
# format an url for the google geocode api
makegeourl <- function(address) 
{
  root <- "http://maps.google.com/maps/api/geocode/" 
  url <- paste(root, "json?address=", +   
              address, "&sensor=false", sep = "")
  return(urlencode(url))
}
there are three simple steps here. the    rst line initializes the begin-
ning part of the url into a string called root. then we use paste() 
to glue together the separate parts of the strong (note the sep="" so 
we don   t get spaces between the parts). this creates a string that 
looks almost like the one in the white house example two pages 

156

ago. the    nal step converts the string to a legal url using a utility 
function called urlencode() that rcurl provides. let   s try it:
> makegeourl( +   
"1600 pennsylvania avenue, washington, dc")
[1] 
"http://maps.google.com/maps/api/geocode/json?add
ress=1600%20pennsylvania%20avenue,%20washington,%
20dc&sensor=false"
looks good! just slightly different than the original example (%20 
instead of the plus character) but hopefully that won   t make a dif-
ference. remember that you can type this function at the command 
line or you can create it in the script editing window in the upper 
left hand pane of r-studio. the latter is the better way to go and if 
you click the "source on save" checkmark, r-studio will make sure 
to update r   s stored version of your function every time you save 
the script    le.
now we are ready to use our new function, makegeourl(), in an-
other function that will actually request the data from the google 
api:
addr2latlng <- function(address) 
{
  url <- makegeourl(address)
  apiresult <- geturl(url)
  geostruct <- fromjson(apiresult, +    
                               simplify = false)
  lat <- na

  lng <- na  
  try(lat <- +   
    geostruct$results[[1]]$geometry$location$lat)
  try(lng <- +   
    geostruct$results[[1]]$geometry$location$lng)
  return(c(lat, lng))
}
we have de   ned this function to receive an address string as its 
only argument. the    rst thing it does is to pass the url string to 
makegeourl() to develop the formatted url. then the function 
passes the url to geturl(), which actually does the work of send-
ing the request out onto the internet. the geturl() function is part 
of the rcurl package. this step is just like typing a url into the 
address box of your browser.
we capture the result in an object called "apiresult". if we were to 
stop and look inside this, we would    nd the json structure that 
appeared a couple of pages ago. we can pass this structure to the 
function fromjson() - we put the result in an object called 
geostruct. this is a regular r data frame such that we can access 
any individual element using regular $ notation and the array in-
dex [[1]]. in other instances, a json object may contain a whole list 
of data structures, but in this case there is just one. if you compare 
the variable names "geometry", "location", "lat" and "lng" to the 
json example a few pages ago you will    nd that they match per-
fectly. the fromjson() function  in the jsonio package has done 
all the heavy lifting of breaking the json structure into its compo-
nent pieces.

157

note that this is the    rst time we have encountered the try() func-
tion. when programmers expect the possibility of an error, they 
will frequently use methods that are tolerant of errors or that catch 
errors before they disrupt the code. if our call to geturl() returns 
something unusual that we aren   t expecting, then the json struc-
ture may not contain the    elds that we want. by surrounding our 
command to assign the lat and lng variables with a try() function, 
we can avoid stopping the    ow of the code if there is an error. be-
cause we initialized lat and lng to na above, this function will re-
turn a two item list with both items being na if an error occurs in 
accessing the json structure. there are more elegant ways to ac-
complish this same goal. for example, the google api puts an er-
ror code in the json structure and we could choose to interpret 
that instead. we will leave that to the chapter challenge!
in the last step, our new addr2latlng() function returns a two item 
list containing the latitude and longitude. we can test it out right 
now:
> testdata <- addr2latlng( +   
      "1600 pennsylvania avenue, washington, dc")
> str(testdata)
 num [1:2] 38.9 -77
perfect! we called our new function addr2latlng() with the address 
of the white house and got back a list with two numeric items con-
taining the latitude and longitude associate with that address. with 
just a few lines of r code we have harnessed the power of google   s 
extensive geocoding capability to convert a brief text street address 
into mapping coordinates. 

at this point there isn   t too much left to do. we have to create a 
looping mechanism so that we can process the whole list of ad-
dresses in our dhhs data set. we have some small design choices 
here. it would be possible to attach new    elds to the existing data-
frame. instead, the following code keeps everything pretty simple 
by receiving a list of addresses and returning a new data frame 
with x, y, and eid ready to feed into our mapping software:
# process a whole list of addresses
processaddrlist <- function(addrlist)
{
  resultdf <- data.frame(atext=character(), +   
              x=numeric(),y=numeric(),eid=numeric())
  i <- 1
  for (addr in addrlist)
  {
    latlng = addr2latlng(addr)
    resultdf <- rbind(resultdf,+   
                 data.frame(atext=addr, +   
                 x=latlng[[2]],y=latlng[[1]], eid=i))
    i <- i + 1
  }
  return(resultdf)
}
this new function takes one argument, the list of addresses, which 
should be character strings. in the    rst step we make an empty da-

158

> dhhspoints <- processaddrlist(dhhsaddrs$straddr)
> dhhspoints <- dhhspoints[!is.na(dhhspoints$x),]
> eventdata <- as.eventdata(dhhspoints,projection=na)
> addpoints(eventdata,col="red",cex=.5)
the second command above is the only one of the four that may 
seem unfamiliar. the as.eventdata() coercion is picky and will not 
process the dataframe if there are any    elds that are na. to get rid 
of those rows that do not have complete latitude and longitude 
data, we use is.na() to test whether the x value on a given row is 
na. we use the ! (not) operator to reverse the sense of this test. so 
the only rows that will survive this step are those where x is not 
na. the plot below shows the results.

taframe for use in the loop. in the second step we initialize a scalar 
variable called i to the number one. we will increment this in the 
loop and use it as our eid. 
then we have the for loop. we are using a neat construct here 
called "in". the expression "addr in addrlist" creates a new vari-
able called addr. every time that r goes through the loop it assigns 
to addr the next item in addrlist. when addrlist runs out of items, 
the for loop ends. very handy!
inside the for loop the    rst thing we do is to call the function that 
we developed earlier: addr2latlng(). this performs one conversion 
of an address to a geocode (latitude and longitude) as described 
earlier. we pass addr to it as add contains the text address for this 
time around the loop. we put the result in a new variable called 
latlng. remember that this is a two item list.
the next statement, starting with "resultdf <- rbind" is the heart of 
this function. recall that rbind() sticks a new row on the end of a 
dataframe. so in the arguments to rbind() we supply our earlier 
version of resultdf (which starts empty and grows from there) 
plus a new row of data. the new row of data includes the text ad-
dress (not strictly necessary but handy for diagnostic purposes), 
the "x" that our mapping software expects (this is the longitude), 
the "y" that the mapping software expects, and the event id, eid, 
that the mapping software expects. 
at the end of the for loop, we increment i so that we will have the 
next number next time around for eid. once the for loop is done 
we simply return the dataframe object, resultdf. piece of cake!
now let   s try it and plot our points:

159

the dollars.obligated    eld. this would require running add-
points() in a loop and setting the col= or cex= parameters for each 
new point.
chapter challenge(s)
improve the addr2latlng() function so that it checks for errors re-
turned from the google api. this will require going to the google 
api site, looking more closely at the json structure, and reviewing 
the error codes that google mentions in the documentation. you 
may also learn enough that you can repair some of the addresses 
that failed.
if you get that far and are still itching for another challenge, try im-
proving the map. one idea for this was mentioned above: you 
could change the size or color of the dots based on the size of the 
contract received. an even better (and much harder) idea would be 
to sum the total dollar amount being given within each state and 
then color each state according to how much dhhs money it re-
ceives. this would require delving into the shape   le data quite sub-
stantially so that you could understand how to    nd the outline of a 
state and    ll it in with a color.
r functions used in this chapter
addr2latlng() -  a custom function built for this chapter
addpoints() - place more points on an existing plot
as.character() - coerces data into a character string
as.eventdata() - coerce a regular dataframe into an eventdata ob-
ject for use with pbsmapping routines.

if you like conspiracy theories, there is some support in this graph: 
the vast majority of the companies that have contracts with dhhs 
are in the washington, dc area, with a little trickle of additional 
companies heading up the eastern seaboard as far as boston and 
southern new hampshire. elsewhere in the country there are a 
few companies here and there, particularly near the large cities of 
the east and south east. 
using some of the parameters on plotpolys() you could adjust the 
zoom level and look at different parts of the country in more detail. 
if you remember that the original dhhs data also had the mone-
tary size of the contract in it, it would also be interesting to change 
the size or color of the dots depending on the amount of money in 

160

sources
http://blog.programmableweb.com/2012/06/21/7-free-geocodin
g-apis-google-bing-yahoo-and-mapquest/ 
https://developers.google.com/maps/terms
http://en.wikipedia.org/wiki/open_mashup_alliance 
http://en.wikipedia.org/wiki/shape   le 
http://www.housingmaps.com/ 
http://www.census.gov/geo/www/cob/cbf_state.html 

data.frame() - takes individual variables and ties them together
for() - runs a loop, iterating a certain number of times depending 
upon the expression provided in parentheses
fromjson() - takes json data as input and provides a regular r 
dataframe as output
function() - creates a new function
importshapefile() - gets shape   le data from a set of esri compati-
ble polygon data    les
makegeourl() - custom helper function built for this chapter
paste() - glues strings together
plotpolys() - plots a map from shape data
rbind() - binds new rows on a dataframe
return() - speci   es the object that should be returned from a func-
tion
urlencode() - formats a character string so it can be used in an 
http request

161

chapter 16

line up, please

sheepdog demonstration, lone pine sanctuary, brisbane, qld. photo credit: jeff stanton

data users are often interested in questions about relationships and prediction. for example, those 
interested in athletics might want to know how the size of the fan base of a team is connected with 
attendance on game day. in this chapter, our australian colleague, robert de graaf, introduces the 
techniques of id75, a very important data science tool.

162

using r to find relationships between sets of data via multiple 
regression, by robert w. de graaf
finding relationships between sets of data is one of the key aims of 
data science. the question of    does x in   uence y    is of prime con-
cern for data analysts     are house prices in   uenced by incomes, is 
the growth rate of crops improved by fertilizer, do taller sprinters 
run faster?
the work horse method used by statisticians to interpret data is lin-
ear modeling, which is a term covering a wide variety of methods, 
from the relatively simple to very sophisticated. you can get an 
idea of how many different methods there are by looking at the re-
gression analysis page in wikipedia and checking out the number 
of entries listed under    models    on the right hand sidebar (and, by 
the way, the list is not exhaustive). 
the basis of all these methods is the idea that is possible to    t a line 
to a set of data points which represents the effect an "independent" 
variable is having on a "dependent" variable. it is easy to visualize 
how this works with one variable changing in step with another 
variable. figure one shows a line    tted to a series of points, using 
the so called "least squares" method (a relatively simple mathemati-
cal method of    nding a best    tting line). note that although the 
line    ts the points fairly well, with an even split (as even as it can 
be for    ve points!) of points on either side of the line, none of the 
points are precisely on the line     the data do not    t the line pre-
cisely. as we discuss the concepts in regression analysis further, we 
will see that understanding these discrepancies is just as important 
as understanding the line itself.

figure 1: a line    tted to some points

the graph in    gure 1 above shows how the relationship between 
an input variable     on the horizontal x axis     relates to the output 
values on the y axis. 
the original ideas behind id75 were developed by 
some of the usual suspects behind many of the ideas we   ve seen al-
ready, such as laplace, gauss, galton, and pearson. the biggest in-

163

dividual contribution was probably by gauss, who used the proce-
dure to predict movements of the other planets in the solar system 
when they were hidden from view, and hence correctly predict 
when and where they would appear in view again. 
the mathematical idea that allows us to    t lines of best    t to a set 
of data points like this is that we can    nd a position for the line 
that will minimize the distance the line is from all the points. while 
the mathematics behind these techniques can be handled by some-
one with college freshman mathematics the reality is that with 
even only a few data points, the process of    tting with manual cal-
culations becomes very tedious, very quickly. for this reason, we 
will not discuss the speci   cs of how these calculations are done, 
but move quickly to how it can be done for us, using r. 
football or rugby?
we can use an example to show how to use id75 on 
real data. the example concerns attendances at australian rules 
football matches. for all of you in the u.s., australian rules foot-
ball is closer to what you think of as rugby and not so much like 
american football. the data in this example concerns matches/
games at the largest stadium for the sport, the melbourne cricket 
ground (note: australian rules football is a winter sport, cricket is 
a summer sport). the melbourne cricket ground or mcg is also 
considered the most prestigious ground to play a football match, 
due to its long association with australian rules football.
australian rules football is the most popular sport to have been 
developed in australia. the object is to kick the ball through the 
larger goal posts, scoring six points. if the ball is touched before it 
crosses the line under the large posts, or instead passes through the 

smaller posts on either side of the large goal posts, a single point is 
scored. the rules ensure that possession of the ball is always chang-
ing. there is full body tackling and possession is turned over fre-
quently. this leads to continuous and exciting on-   eld action.
the main stronghold of australian rules football is in the state of 
victoria (south eastern australia), where the original league, the 
vfl, became the afl after its league of mostly melbourne subur-
ban teams added teams to represent areas outside victoria, such as 
west coast (perth, the capital of the state of western australia) and 
adelaide (capital of the state of south australia). note that mel-
bourne is the capital city of victoria. much of the popularity of the 
vfl was based on the rivalries between neighboring suburbs, and 
teams with long histories, like the collingwood football club - 
based in one of melbourne   s most working class suburbs     have 
large and loyal fan bases.
while it isn   t necessary to know anything about how the sport is 
played to understand the example, it is useful to know that the 
australian football league, the premiere organization playing aus-
tralian rules football, was formerly the victorian football league, 
and although teams from outside the australian state of victoria 
have joined, more than half the teams in the league are victorian, 
even though victoria is only one of six australian states.
getting the data
the data are available from ozdasl, a website which provides pub-
lic  domain data sets for analysis, and the mcg attendance data 
has its own page at http://www.statsci.org/data/oz/a   .html.

164

the variable of interest is mcg attendance, and named    mcg    in 
the dataset. most statisticians would refer to this variable as the de-
pendent variable, because it is the variable that we are most inter-
ested in predicting: it is the "outcome" of the situation we are try-
ing to understand. potential explanatory, or independent, variables 
include club membership, weather on match day, date of match, 
etc. there is a detailed description of each of the variables available 
on the website. you can use the data set to test your own theories 
of what makes football fans decide whether or not to go to a game, 
but to learn some of the skills we will test a couple of those factors 
together.
before we can start, we need r to be able to    nd the data. make 
sure that you download the data to the spot on your computer that 
r considers the "working" directory. use this command to    nd out 
what the current working directory is:
> getwd()
after downloading the data set from ozdasl into your r working 
directory, read the data set into r as follows:
> attend<-read.delim("afl.txt", header=true)
> attach(attend)
we include the optional    header = true    to designate the    rst row 
as the column names, and the    attach    commands turns each of the 
named columns into a single column vector. 
once we   ve read the data into r, we can examine some plots of the 
data. with many techniques in data science, it can be quite valu-
able to visualize the data before undertaking a more detailed analy-
sis. one of the variables we might consider is the combined mem-

165

bership of the two teams playing, a proxy for the popularity of the 
teams playing. 
> plot(mcg ~ members, xlab = "combined membership 
of teams playing", ylab = "mcg match day atten-
dance" )

0
8

0
6

0
4

0
2

e
c
n
a
d
n
e

t
t

 

a
 
y
a
d
h
c
t
a
m
g
c
m

 

20

30

40

50

combined membership of teams playing

figure 2: scatterplot of membership versus attendance

we see evidence of a trend in the points on the left hand side of the 
graph, and a small group of points representing games with very 
high combined membership but that don   t seem to    t the trend ap-
plying to the rest of data. if it wasn   t for the four "outliers" on the 
right hand side of the plot, we would be left with a plot showing a 
very strong relationship.
as a next step we can use r to create a linear model for mcg atten-
dance using the combined membership as the single explanatory 
variable using the following r code:
> model1 <- lm(mcg ~ members-1)
> summary(model1)
there are two steps here because in the    rst step the lm() com-
mand creates a nice big data structure full of output, and we want 
to hang onto that in the variable called "model1." in the second 
command we request an overview of the contents of model1.
it is important to note that in this model, and in the others that fol-
low, we have added a    -1    term to the speci   cation, which forces 
the line of best    t to pass through zero on the y axis at zero on the x 
axis (more technically speaking, the y-intercept is forced to be at 
the origin). in the present model that is essentially saying that if 
the two teams are so unpopular they don   t have any members, no 
one will go to see their matches, and vice versa. this technique is 
appropriate in this particular example, because both of the meas-
ures have sensible zero points, and we can logically reason that 
zero on x implies zero on y. in most other models, particularly for 
survey data that may not have a sensible zero point (think about 
rating scales ranging from 1 to 5), it would not be appropriate to 
force the best    tting line through the origin.

the second command above, summary(model1), provides the fol-
lowing output:
call:
lm(formula = mcg ~ members - 1)

residuals:
    min      1q  median      3q     max 
-44.961  -6.550   2.954   9.931  29.252 

coefficients:
        estimate std. error t value pr(>|t|)    
members  1.53610    0.08768   17.52   <2e-16 ***
---
signif. codes:  0    ***    0.001    **    0.01    *    0.05 
   .    0.1         1 

residual standard error: 15.65 on 40 degrees of 
freedom
multiple r-squared: 0.8847,     
adjusted r-squared: 0.8818 
f-statistic: 306.9 on 1 and 40 df,  p-value: < 
2.2e-16  

166

wow! that   s  a lot information that r has provided for us, and we 
need to use this information to decide whether we are happy with 
this model. being    happy    with the model can involve many fac-
tors, and there is no simple way of deciding. to start with, we will 
look at the r-squared value, also known as the coef   cient of deter-
mination.
the r squared value     the coef   cient of determination     represents 
the proportion of the variation which is accounted for in the de-
pendent variable by the whole set of independent variables (in this 
case just one independent variable). an r-squared value of 1.0 
would mean that the x variable(s), the independent variable(s), per-
fectly predicted the y, or dependent variable. an r-squared value of 
zero would indicated that the x variable(s) did not predict the y 
variable at all. r-squared cannot be negative. the r-squared of 
.8847 in this example means that the combined members variable 
account for 88.47% of the mcg attendance variable, an excellent 
result. note that there is no absolute rule for what makes an r-
squared good. much depends on the purpose of the analysis. in the 
analysis of human behavior, which is notoriously unpredictable, an 
r-squared of .20 or .30 may be very good.
in    gure 3, below, we have added a line of best    t based on the 
model to the x-y plot of mcg attendance against total team mem-
bership with this command:
> abline(model1)
while the line of best    t seems to    t the points in the middle, the 
points on the lower right hand side and also some points towards 
the top of the graph, appear to be a long way from the line of best 
   t.

adding another independent variable
we discussed at the beginning of this chapter the origin of austra-
lian rules football in victoria, where the mcg is located. while 
most of the teams in the afl are also victoria teams, and therefore 
have a supporter base which can easily access the mcg, a number 
of the teams are from other states, and their supporters would 

167

need to make a signi   cant interstate journey to see their team play 
at the mcg. for example, the journey from sydney to melbourne 
is around eight hours by car or two by plane, whereas from perth, 
where most west coast   s supporter base is located, is close to    ve 
hours by air     and two time zones away. australia is a really huge 
country.
the dataset doesn   t have a variable for interstate teams but fortu-
nately there are only four teams that are interstate: brisbane, syd-
ney, adelaide, and west coast, abbreviated respectively as "bris", 
"syd", "adel", and "wc". we can make a binary coded variable to 
indicate these interstate teams with a simple command:
> away.inter <-    
ifelse(away=="wc" |   
       away=="adel"|   
       away=="syd"|    
       away=="bris",1,0)
the code above checks the values in the column labeled    away   , 
and if it    nds an exact match with one of the names of an interstate 
team, it stores a value of 1. otherwise it stores a value of 0. note 
that we use a double equals sign for the exact comparison in r, and 
the vertical bar is used to represent the logical    or    operator. these 
symbols are similar, although not precisely the same, as symbols 
used to represent logical operators in programming languages 
such as c and java. having created the new    away team is inter-
state    variable, we can use this variable to create a new linear re-
gression model that includes two independent variables.
> model2<-lm(mcg~members+away.inter-1)

> summary(model2)

call:
lm(formula = mcg ~ members + away.inter - 1)

residuals:
     min       1q   median       3q      max 
-30.2003  -8.5061   0.0862   8.5411  23.5687 
coefficients:
          estimate std. error t value pr(>|t|)    
members   1.69255    0.07962  21.257  < 2e-16 ***
away.inter -22.84122 5.02583  -4.545  5.2e-05 ***
---
signif. codes:  0    ***    0.001    **    0.01    *    0.05 
   .    0.1         1 

residual standard error: 12.82 on 39 degrees of 
freedom
multiple r-squared: 0.9246,        
adjusted r-squared: 0.9208 
f-statistic: 239.2 on 2 and 39 df,  p-value: < 
2.2e-16

168

note that the r-squared value is now 0.9246, which is quite a bit 
higher than the 0.8847 that we observed in the previous model. in 
this new model, the two independent variables working together 
account for 92.46% of the dependent variable. so together, the total 
fan base and the status as an away team are doing a really great job 
of predicting attendance. this result is also intuitive     we would 
expect that football fans, regardless of how devoted they are to 
their team, are more likely to come to games if they   re a moderate 
car ride away, compared to a plane journey.
because we have two independent variables now, we have to look 
beyond the r-squared value to understand the situation better. in 
particular, about one third of the way into the output for the lm() 
command there is a heading that says "estimate." right below that 
are slope values for members and for away.inter. notice that the 
slope (sometimes called a "b-weight") on members is positive: this 
makes sense because the more fans the team has the higher the at-
tendance. the slope on away.inter is negative because when this 
variable is 1 (in the case of interstate teams) the attendance is 
lower) whereas when this variable is 0 (for local teams), attendance 
is higher. 
how can you tell if these slopes or b-weights are actually impor-
tant contributors to the prediction? you can divide the unstandard-
ized b-weight by its standard error to create a "t value". the lm() 
command has done this for you and it is reported in the output 
above. this "t" is the student   s t-test, described in a previous chap-
ter. as a rule of thumb, if this t value has an absolute value (i.e., ig-
noring the minus sign if there is one) that is larger than about 2, 
you can be assured that the independent/predictor variable we are 
talking about is contributing to the prediction of the dependent 

variable. in this example we can see that members has a humon-
gous t value of 21.257, showing that it is very important in the pre-
diction. the away.inter variable has a somewhat more modest, but 
still important value of -4.545 (again, don   t worry about the minus 
sign when judging the magnitude of the t value). 
we can keep on adding variables that we think make a difference. 
how many variables we end up using depends, apart from our 
ability to think of new variables to measure, somewhat on what we 
want to use the model for.
 the model we have developed now has two explanatory variables 
    one which can be any positive number, and one which is two lev-
els. we now have what could be considered a very respectable r-
squared value, so we could easily leave well enough alone. that is 
not to say our model is perfect, however     the graphs we have pre-
pared suggest that the    members    effect is actually different if the 
away team is from interstate rather than from victoria     the crowd 
does not increase with additional combined membership as 
quickly with an away team, which is in line with what we might 
expect intuitively. 
one thing we didn   t mention was the actual prediction equation 
that one might construct from the output of lm(). it is actually very 
simple and just uses the estimates/b-weights from the output:
mcg =  (21.257 * members) - ( 4.545 * away.inter)
this equation would let us predict the attendance of any game 
with a good degree of accuracy, assuming that we knew the com-
bined fan base and whether the team was interstate. interestingly, 
statisticians are rarely interested in using prediction equations like 

169

the one above: they are generally more interested in just knowing 
that a predictor is important or unimportant. also, one must be 
careful with using the slopes/b-weights obtained from a linear re-
gression of a single sample, because they are likely to change if an-
other sample is analyzed - just because of the forces of random-
ness.
conclusion
the material we have covered is really only a taste of multiple re-
gression and linear modeling. on the one hand, there are a number 
of additional factors that may be considered before deciding on a 
   nal model. on the other hand, there are a great number of tech-
niques that may be used in specialized circumstances. for exam-
ple, in trying to model attendance at the mcg, we have seen that 
the standard model    ts the data some of the time but not others, 
depending on the selection of the explanatory variables.
in general, a simple model is a good model, and will keep us from 
thinking that we are better than we really are. however, there are 
times when we will want to    nd as many dependent variables as 
possible. contrast the needs of a manager trying to forecast sales to 
set inventory with an engineer or scientist trying to select parame-
ters for further experimentation. in the    rst case, the manager 
needs to avoid a falsely precise estimate which could lead her to be 
overcon   dent in the forecast, and either order too much stock or 
too little. the manager wants to be conservative about deciding 
that particular variables make a difference to prediction variable. 
on the other hand the experimenter wants to    nd as many vari-
ables as possible for future research, so is prepared to be optimistic 
about whether different parameters affect the variables of interest.

chapter challenge
we intentionally ignored some of the output of these regression 
models, for the sake of simplicity. it would be quite valuable for 
you to understand those missing parts, however. in particular, we 
ignored the "p-values" associated with the t-tests on the slope/b-
weight estimates and we also ignored the overall f-statistic re-
ported at the very bottom of the output. there are tons of great re-
courses on the web for explaining what these are.
for a super bonus, you could also investigate the meaning of the 
"adjusted" r-squared that appears in the output.
sources
http://en.wikipedia.org/wiki/australian_rules_football
http://stat.ethz.ch/r-manual/r-patched/library/stats/html/lm.
html
http://www.ddiez.com/teac/r/linear_models.php
r functions used in this chapter
abline - plots a best    tting line on top of a scatterplot
attach - makes a data structure the "focus" of attention
getwd - show the current working directory for r
ifelse - a conditional test that provides one of two possible outputs
lm - "linear models" and for this chapter, multiple regression
plot - general purpose graphing function, many uses in r

170

summary - produces an overview of an output structure

171

chapter 17

hi ho, hi ho - data mining we go

data mining is an area of research and practice that is focused on discovering novel patterns in data. 
as usual, r has lots of possibilities for data mining. in this chapter we will begin experimentation 
with essential data mining techniques by trying out one of the easiest methods to understand: 
association rules mining. more beer and diapers please!

172

data mining is a term that refers to the use of algorithms and com-
puters to discover novel and interesting patterns within data. one 
famous example that gets mentioned quite frequently is the super-
market that analyzed patterns of purchasing behavior and found 
that diapers and beer were often purchased together. the super-
market manager decided to put a beer display close to the diaper 
aisle and supposedly sold more of both products as a result. an-
other familiar example comes from online merchant sites that say 
things like, "people who bought that book were also interested in 
this book." by using an algorithm to look at purchasing patterns, 
vendors are able to create automatic systems that make these kinds 
of recommendations.
over recent decades, statisticians and computer scientists have de-
veloped many different algorithms that can search for patterns in 
different kinds of data. as computers get faster and the researchers 
do additional work on making these algorithms more ef   cient it 
becomes possible to look through larger and larger data sets look-
ing for promising patterns. today we have software that can search 
through massive data haystacks looking for lots of interesting and 
usable needles.
some people refer to this area of research as machine learning. ma-
chine learning focuses on creating computer algorithms that can 
use pre-existing inputs to re   ne and improve their own capabilities 
for dealing with future inputs. machine learning is very different 
from human learning. when we think of human learning, like 
learning the alphabet or learning a foreign language, humans can 
develop    exible and adaptable skills and knowledge that are appli-
cable to a range of different contexts and problems. machine learn-
ing is more about    guring out patterns of incoming information 

that correspond to a speci   c result. for example, given lots of exam-
ples like this - input: 3, 5, 10; output: 150 - a machine learning algo-
rithm could    gure out on its own that multiplying the input values 
together produces the output value. 
machine learning is not exactly the same thing as data mining and 
vice versa. not all data mining techniques rely on what researchers 
would consider machine learning. likewise, machine learning is 
used in areas like robotics that we don   t commonly think of when 
we are thinking of data mining as such. 
data mining typically consists of four processes: 1) data prepara-
tion, 2) exploratory data analysis, 3) model development, and 4) in-
terpretation of results. although this sounds like a neat, linear set 
of steps, there is often a lot of back and forth through these proc-
esses, and especially among the    rst three. the other point that is 
interesting about these four steps is that steps 3 and 4 seem like the 
most fun, but step 1 usually takes the most amount of time.  step 1 
involves making sure that the data are organized in the right way, 
that missing data    elds are    lled in, that inaccurate data are lo-
cated and repaired or deleted, and that data are "recoded" as neces-
sary to make them amenable to the kind of analysis we have in 
mind. 
step 2 is very similar to activities we have done in prior chapters of 
this book: getting to know the data using histograms and other 
visualization tools, and looking for preliminary hints that will 
guide our model choice. the exploration process also involves    g-
uring out the right values for key parameters. we will see some of 
that activity in this chapter.

173

step 3 - choosing and developing a model - is by far the most com-
plex and most interesting of the activities of a data miner. it is here 
where you test out a selection of the most appropriate data mining 
techniques. depending upon the structure of a dataset, there may 
be dozens of options, and choosing the most promising one has as 
much art in it as science. 
for the current chapter we are going to focus on just one data min-
ing technique, albeit one that is quite powerful and applicable to a 
range of very practical problems. so we will not really have to do 
step 3, because we will not have two or more different mining tech-
niques to compare. the technique we will use in this chapter is 
called "association rules mining" and it is the strategy that was 
used to    nd the diapers and beer association described earlier.  
step 4 - the interpretation of results - focuses on making sense out 
of what the data mining algorithm has produced. this is the most 
important step from the perspective of the data user, because this 
is where an actionable conclusion is formed. when we discussed 
the example of beer and diapers, the interpretation of the associa-
tion rules that were derived from the grocery purchasing data is 
what led to the discover of the beer-diapers rule and the use of that 
rule in recon   guring the displays in the store.
let   s begin by talking a little bit about association rules. take a 
look at the    gure below with all of the boxes and arrows:

customers

carts

store+inventory

customer+1

customer+2

customer+3

item+1
item+2
item+3
item+4
item+5

item+1
item+2
item+3

item+1
item+2
item+3
item+4
item+5
item+6
item+7

baby+wipes
beer
bread
cheddar
chips
corn+flakes
diapers
lettuce
mayonnaise
milk
peanut+butter
salami
shampoo
sponges
tomatoes
toothpaste

from the    gure you can see that each supermarket customer has a 
grocery cart that contains several items from the larger set of items 
that the grocery store stocks. the association rules algorithm (also 
sometimes called af   nity analysis) tries out many different proposi-
tions, such as "if diapers are purchased, then beer is also pur-
chased." the algorithm uses a dataset of transactions (in the exam-
ple above, these are the individual carts) to evaluate a long list of 
these rules for a value called "support." support is the proportion 
of times that a particular pairing occurs across all shopping carts. 
the algorithm also evaluates another quantity called "con   dence," 

174

which is how frequently a particular pair occurs among all the 
times when the    rst item is present. if you look back at the    gure 
again, we had support of 0.67 (the diapers-beer association oc-
curred in two out of the three carts) and con   dence of 1.0 ("beer" 
occurred 100% of the time with "diapers"). in practice, both sup-
port and con   dence are generally much lower than in this exam-
ple, but even a rule with low support and smallish con   dence 
might reveal purchasing patterns that grocery store managers 
could use to guide pricing, coupon offers, or advertising strategies. 
we can get started with association rules mining very easily using 
the r package known as "arules." in r-studio, you can get the 
arules package ready using the following commands:
> install.packages("arules")
> library("arules")
we will begin our exploration of association rules mining using a 
dataset that is built into the arules package. for the sake of familiar-
ity, we will use the groceries dataset. note that by using the grocer-
ies data set, we have relieved ourselves of the burden of data prepa-
ration, as the authors of the arules package have generously made 
sure that groceries is ready to be analyzed. so we are skipping 
right to step 2 in our four step process - exploratory data analysis. 
you can make the groceries data set ready with this command:
> data(groceries)
next, lets run the summary() function on groceries so that we can 
see what is in there:
> summary(groceries)

transactions as itemmatrix in sparse format with
 9835 rows (elements/itemsets/transactions) and
 169 columns (items) and a density of 0.02609146 
most frequent items:
      whole milk other vegetables       rolls/buns             
soda 
            2513             1903             1809             
1715 
          yogurt          (other) 
            1372            34055 

element (itemset/transaction) length distribution:
sizes
   1    2    3    4    5    6    7    8    9   10   
11   12   13   14   15   16 
2159 1643 1299 1005  855  645  545  438  350  246  
182  117   78   77   55   46 
  17   18   19   20   21   22   23   24   26   27   
28   29   32 
  29   14   14    9   11    4    6    1    1    1    
1    3    1 

   min. 1st qu.  median    mean 3rd qu.    max. 
  1.000   2.000   3.000   4.409   6.000  32.000 

175

includes extended item information - examples:
       labels  level2           level1
1 frankfurter sausage meet and sausage
2     sausage sausage meet and sausage
3  liver loaf sausage meet and sausage
right after the summary command line we see that groceries is an 
itemmatrix object in sparse format. so what we have is a nice, rec-
tangular data structure with 9835 rows and 169 columns, where 
each row is a list of items that might appear in a grocery cart. the 
word "matrix" in this case, is just referring to this rectangular data 
structure. the columns are the individual items. a little later in the 
output we see that there are 169 columns, which means that there 
are 169 items. the reason the matrix is called "sparse" is that very 
few of these items exist in any given grocery basket. by the way, 
when an item appears in a basket, its cell contains a one, while if 
an item is not in a basket, its cell contains a zero. so in any given 
row, most of the cells are zero and very few are one and this is 
what is meant by sparse. we can see from the min, median, mean, 
and max output that every cart has at least one item, half the carts 
have more than three items, the average number of items in a cart 
is 4.4 and the maximum number of items in a cart is 32.
the output also shows us which items occur in grocery baskets 
most frequently. if you like working with spreadsheets, you could 
imagine going to the very bottom of the column that is marked 
"whole milk" and putting in a formula to sum up all of the ones in 
that column. you would come up with 2513, indicating that there 

176

are 2513 grocery baskets that contain whole milk. remember that 
every row/basket that has a one in the whole milk column has 
whole milk in that basket, whereas a zero would appear if the bas-
ket did not contain whole milk. you might wonder what the data 
   eld would look like if a grocery cart contained two gallons of 
whole milk. for the present data mining exercise we can ignore 
that problem by assuming that any non-zero amount of whole milk  
is represented by a one. other data mining techniques could take 
advantage of knowing the exact amount of a product, but associa-
tion rules does not need to know that amount - just whether the 
product is present or absent. 
another way of inspecting our sparse matrix is with the itemfre-
quencyplot() function. this produces a bar graph that is similar in 
concept to a histogram: it shows the relative frequency of occur-
rence of different items in the matrix. when using the itemfrequen-
cyplot() function, you must specify the minimum level of "sup-
port" needed to include an item in the plot. remember the mention 
of support earlier in the chapter - in this case it simply refers to the 
relative frequency of occurrence of something. we can make a 
guess as to what level of support to choose based on the results of 
the summary() function we ran earlier in the chapter. for example, 
the item "yogurt" appeared in 1372 out of 9835 rows or about 14% 
of cases. so we can set the support parameter to somewhere 
around 10%-15% in order to get a manageable number of items:
> itemfrequencyplot(groceries,support=0.1)
this command produces the following plot:

this will keep the labels from overlapping at the expense of mak-
ing the font size much smaller. here   s an example:
> itemfrequencyplot(groceries,support=0.05,cex.names=0.5)
this command yields about 25 items on the x-axis. without worry-
ing too much about the labels, you can also experiment with lower 
values of support, just to get a feel for how many items appear at 
the lower frequencies. we need to guess at a minimum level of sup-
port that will give us quite a substantial number of items that can 
potentially be part of a rule. nonetheless, it should also be obvious 
that an item that occurs only very rarely in the grocery baskets is 
unlikely to be of much use to us in terms of creating meaningful 
rules. let   s pretend, for example, that the item "venezuelan beaver 
cheese" only occurred once in the whole set of 9835 carts. even if 
we did end up with a rule about this item, it won   t apply very of-
ten, and is therefore unlikely to be useful to store managers or oth-
ers. so we want to focus our attention on items that occur with 
some meaningful frequency in the dataset. whether this is one per-
cent or half a percent, or something somewhat larger or smaller 
will depend on the size of the data set and the intended application 
of the rules.
now we can prepare to generate some rules with the apriori() com-
mand. the term "apriori" refers to the speci   c algorithm that r will 
use to scan the data set for appropriate rules. apriori is a very com-
monly used alrgorithm and it is quite ef   cient at    nding rules in 
transaction data. rules are in the form of "if lhs then rhs." the 
acronym lhs means "left hand side" and, naturally, rhs means 
"right hand side." so each rule states that when the thing or things 
on the left hand side of the equation occur(s), the thing on the right 
hand side occurs a certain percentage of the time. to reiterate a de   -

177

we can see that yogurt is right around 14% as expected and we 
also see a few other items not mentioned in the summary such as 
bottled water and tropical fruit. 
you should experiment with using different levels of support, just 
so that you can get a sense of the other common items in the data 
set. if you show more than about ten items, you will    nd that the 
labels on the x-axis start to overlap and obscure one another. use 
the "cex.names" parameter to turn down the font size on the labels. 

nition provided earlier in the chapter, support for a rule refers to 
the frequency of co-occurrence of both members of the pair, i.e., 
lhs and rhs together. the con   dence of a rule refers to the pro-
portion of the time that lhs and rhs occur together versus the to-
tal number of appearances of lhs. for example, if milk and butter 
occur together in 10% of the grocery carts (that is "support"), and 
milk (by itself, ignoring butter) occurs in 25% of the carts, then the 
con   dence of the milk/butter rule is 0.10/0.25 = 0.40.
there are a couple of other measures that can help us zero in on 
good association rules - such as "lift"and "conviction" - but we will 
put off discussing these until a little later.
one last note before we start using apriori(): for most of the work 
the data miners do with association rules, the rhs part of the equa-
tion contains just one item, like "butter." on the other hand, lhs 
can and will contain multiple items. a simple rule might just have 
milk in lhs and butter in rhs, but a more complex rule might 
have milk and bread together in lhs with butter in rhs.  
in the spirit of experimentation, we can try out some different pa-
rameter values for using the apriori() command, just to see what 
we will get:
> apriori(groceries,parameter=list(support=0.005,+   
confidence=0.5))

parameter specification:
 confidence minval smax arem  aval 
        0.5    0.1    1 none false
originalsupport support minlen maxlen target

          true   0.005      1     10  rules
   ext
 false

algorithmic control:
 filter tree heap memopt load sort verbose
    0.1 true true  false true    2    true

apriori - find association rules with the apriori algo-
rithm
version 4.21 (2004.05.09)        (c) 1996-2004   chris-
tian borgelt
set item appearances ...[0 item(s)] done [0.00s].
set transactions ...[169 item(s), 9835 transaction(s)] 
done [0.00s].
sorting and recoding items ... [120 item(s)] done 
[0.00s].
creating transaction tree ... done [0.01s].
checking subsets of size 1 2 3 4 done [0.01s].
writing ... [120 rule(s)] done [0.00s].
creating s4 object  ... done [0.00s].
set of 120 rules 
we set up the apriori() command to use a support of 0.005 (half a 
percent) and con   dence of 0.5 (50%) as the minimums. these val-

178

ues are con   rmed in the    rst few lines of output. some other con   r-
mations, such as the value of "minval" and "smax" are not relevant 
to us right now - they have sensible defaults provided by the apri-
ori() implementation. the "minlen" and "maxlen" parameters also 
have sensible defaults: these refer to the minimum and maximum 
length of item set that will be considered in generating rules. obvi-
ously you can   t generate a rule unless you have at least one item in 
an item set, and setting maxlen to 10 ensures that we will not have 
any rules that contain more than 10 items. if you recall from earlier 
in the chapter, the average cart only has 4.4 items, so we are not 
likely to produce rules involving more than 10 items.
in fact, a little later in the apriori() output above, we see that the 
apriori() algorithm only had to examine "subsets of size" one, two 
three, and four. apparently no rule in this output contains more 
than four items. at the very end of the output we see that 120 rules 
were generated. later on we will examine ways of making sense 
out of a large number of rules, but for now let   s agree that 120 is 
too many rules to examine. let   s move our support to one percent 
and rerun apriori(). this time we will store the resulting rules in a 
data structure called ruleset:
> ruleset <- apriori(groceries,+   
parameter=list(support=0.01,confidence=0.5))
if you examine the output from this command, you should    nd 
that we have slimmed down to 15 rules, quite a manageable num-
ber to examine one by one. we can get a preliminary look at the 
rules using the summary function, like this:
> summary(ruleset)
set of 15 rules

rule length distribution (lhs + rhs):sizes
 3 
15 

   min. 1st qu.  median    mean 3rd qu.    max. 
      3       3       3       3       3       3 

summary of quality measures:
    support          confidence          lift      
 min.   :0.01007   min.   :0.5000   min.   :1.984  
 1st qu.:0.01174   1st qu.:0.5151   1st qu.:2.036  
 median :0.01230   median :0.5245   median :2.203  
 mean   :0.01316   mean   :0.5411   mean   :2.299  
 3rd qu.:0.01403   3rd qu.:0.5718   3rd qu.:2.432  
 max.   :0.02227   max.   :0.5862   max.   :3.030  

mining info:
      data ntransactions support confidence
 groceries          9835    0.01        0.5
looking through this output, we can see that there are 15 rules in 
total. under "rule length distribution" it shows that all 15 of the 
rules have exactly three elements (counting both the lhs and the 

179

rhs). then, under "summary of quality measures," we have an 
overview of the distributions of support, con   dence, and a new pa-
rameter called "lift." 
researchers have done a lot of work trying to come up with ways 
of measuring how "interesting" a rule is. a more interesting rule 
may be a more useful rule because it is more novel or unexpected. 
lift is one such measure. without getting into the math, lift takes 
into account the support for a rule, but also gives more weight to 
rules where the lhs and/or the rhs occur less frequently. in other 
words, lift favors situations where lhs and rhs are not abundant 
but where the relatively few occurrences always happen together. 
the larger the value of lift, the more "interesting" the rule may be.
now we are ready to take a closer look at the rules we generated. 
the inspect() command gives us the detailed contents of the dta ob-
ject generated by apriori():
> inspect(ruleset)

   lhs                     rhs                   support confidence     lift

1  {curd,                                                                   

    yogurt}             => {whole milk}       0.01006609  0.5823529 2.279125

2  {other vegetables,                                                       

    butter}             => {whole milk}       0.01148958  0.5736041 2.244885

3  {other vegetables,                                                       

    domestic eggs}      => {whole milk}       0.01230300  0.5525114 2.162336

4  {yogurt,                                                                 

    whipped/sour cream} => {whole milk}       0.01087951  0.5245098 2.052747

5  {other vegetables,                                                       

    whipped/sour cream} => {whole milk}       0.01464159  0.5070423 1.984385

6  {pip fruit,                                                              

    other vegetables}   => {whole milk}       0.01352313  0.5175097 2.025351

7  {citrus fruit,                                                           

    root vegetables}    => {other vegetables} 0.01037112  0.5862069 3.029608

8  {tropical fruit,                                                         

    root vegetables}    => {other vegetables} 0.01230300  0.5845411 3.020999

9  {tropical fruit,                                                         

    root vegetables}    => {whole milk}       0.01199797  0.5700483 2.230969

10 {tropical fruit,                                                         

    yogurt}             => {whole milk}       0.01514997  0.5173611 2.024770

11 {root vegetables,                                                        

    yogurt}             => {other vegetables} 0.01291307  0.5000000 2.584078

12 {root vegetables,                                                        

    yogurt}             => {whole milk}       0.01453991  0.5629921 2.203354

13 {root vegetables,                                                        

    rolls/buns}         => {other vegetables} 0.01220132  0.5020921 2.594890

14 {root vegetables,                                                        

    rolls/buns}         => {whole milk}       0.01270971  0.5230126 2.046888

15 {other vegetables,                                                       

    yogurt}             => {whole milk}       0.02226741  0.5128806 2.007235
with apologies for the tiny font size, you can see that each of the 15 
rules shows the lhs, the rhs, the support, the con   dence, and the 
lift. rules 7 and 8 have the highest level of lift: the fruits and vegeta-
bles involved in these two rules have a relatively low frequency of 
occurrence, but their support and con   dence are both relatively 
high. contrast these two rules with rule 1, which also has high con-
   dence, but which has low support. the reason for this is that milk 
is a frequently occurring item, so there is not much novelty to that 
rule. on the other hand, the combination of fruits, root vegetables, 

180

mand. the answer to this conundrum is that arulesviz has put 
some plumbing into place so that when plot runs across a data ob-
ject of type "rules" (as generated by apriori) it will use some of the 
code that is built into arulesviz to do the work. so by installing 
arulesviz we have put some custom visualization code in place 
that can be used by the generic plot() command. the command is 
very simple:
> plot(ruleset)
the    gure below contains the result:

and other vegetables suggest a need to    nd out more about custom-
ers whose carts may contain only vegetarian or vegan items. 
now it is possible that we have set our parameters for con   dence 
and support too stringently and as a result have missed some truly 
novel combinations that might lead us to better insights. we can 
use a data visualization package to help explore this possibility. 
the r package called arulesviz has methods of visualizing the rule 
sets generated by apriori() that can help us examine a larger set of 
rules. first, install and library the arulesviz package:
> install.packages("arulesviz")
> library(arulesviz)
these commands will give the usual raft of status and progress 
messages. when you run the second command you may    nd that 
three or four data objects are "masked." as before, these warnings 
generally will not compromise the operation of the package.
now lets return to our apriori() command, but we will be much 
more lenient this time in our minimum support and con   dence pa-
rameters:
> ruleset <- apriori(groceries,+   
parameter=list(support=0.005,confidence=0.35))
we brought support back to half of one percent and con   dence 
down to 35%. when you run this command you should    nd that 
you now generate 357 rules. that is way too many rules to exam-
ine manually, so let   s use the arulesviz package to see what we 
have. we will use the plot() command that we have also used ear-
lier in the book. you may ask yourself why we needed to library 
the arulesviz package if we are simply going to use an old com-

181

even though we see a two dimensional plot, we actually have 
three variables represented here. support is on the x-axis and con   -
dence is on the y-axis. all else being equal we would like to have 
rules that have high support and high con   dence. we know, how-
ever, that lift serves as a measure of interestingness, and we are 
also interested in the rules with the highest lift. on this plot, the lift 
is shown by the darkness of a dot that appears on the plot. the 
darker the dot, the close the lift of that rule is to 4.0, which appears 
to be the highest lift value among these 357 rules.
the other thing we can see from this plot is that while the support 
of rules ranges from somewhere below 1% all the way up above 
7%, all of the rules with high lift seem to have support below 1%. 
on the other hand, there are rules with high lift and high con   -
dence, which sounds quite positive.  
based on this evidence, lets focus on a smaller set of rules that only 
have the very highest levels of lift. the following command makes 
a subset of the larger set of rules by choosing only those rules that 
have lift higher than 3.5:
> goodrules <- ruleset[quality(ruleset)$lift > 3.5]
note that the use of the square braces with our data structure rule-
set allows us to index only those elements of the data object that 
meet our criteria. in this case, we use the expression quality(rule-
set)$lift to tap into the lift parameter for each rule. the inequality 
test > 3.5 gives us just those rules with the highest lift. when you 
run this line of code you should    nd that goodrules contins just 
nine rules. let   s inspect those nine rules:
> inspect(goodrules)

  lhs                   rhs                   support confidence     lift
1 {herbs}            => {root vegetables} 0.007015760  0.4312500 3.956477
2 {onions,                                                               
   other vegetables} => {root vegetables} 0.005693950  0.4000000 3.669776
3 {beef,                                                                 
   other vegetables} => {root vegetables} 0.007930859  0.4020619 3.688692
4 {tropical fruit,                                                       
   curd}             => {yogurt}          0.005287239  0.5148515 3.690645
5 {citrus fruit,                                                         
   pip fruit}        => {tropical fruit}  0.005592272  0.4044118 3.854060
6 {pip fruit,                                                            
   other vegetables,                                                     
   whole milk}       => {root vegetables} 0.005490595  0.4060150 3.724961
7 {citrus fruit,                                                         
   other vegetables,                                                     
   whole milk}       => {root vegetables} 0.005795628  0.4453125 4.085493
8 {root vegetables,                                                      
   whole milk,                                                           
   yogurt}           => {tropical fruit}  0.005693950  0.3916084 3.732043
9 {tropical fruit,                                                       
   other vegetables,                                                     
   whole milk}       => {root vegetables} 0.007015760  0.4107143 3.768074
there we go again with the microscopic font size. when you look 
over these rules, it seems evidence that shoppers are purchasing 
particular combinations of items that go together in recipes. the 
   rst three rules really seem like soup! rules four and    ve seem like 

182

a fruit platter with dip. the other four rules may also connect to a 
recipe, although it is not quite so obvious what.
the key takeaway point here is that using a good visualization tool 
to examine the results of a data mining activity can enhance the 
process of sorting through the evidence and making sense of it. if 
we were to present these results to a store manager (and we would 
certainly do a little more digging before formulating our    nal con-
clusions) we might recommend that recipes could be published 
along with coupons and popular recipes, such as for homemade 
soup, might want to have all of the ingredients group together in 
the store along with signs saying, "mmmm, homemade soup!"
chapter challenge
the arules package contains other data sets, such as the epub data-
set with 3975 transactions from the electronic publication platform 
of the vienna university of economics. load up that data set, gen-
erate some rules, visualize the rules, and choose some interesting 
ones for further discussion.
data mining with rattle
a company called togaware has created a graphical user interface 
(gui) for r called rattle. at this writing (working with r version 
3.0.0), one of rattle   s components has gotten out of date and will 
not work with the latest version of r, particularly on the mac. it is 
likely, however, that those involved with the rattle project will 
soon update it to be compatible again. using rattle simpli   es 
many of the processes described earlier in the chapter. try going to 
the togaware site and following the instructions there for installing 
rattle for your particular operating system. 

sources
http://en.wikipedia.org/wiki/association_rule_learning 
http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11
a.pdf 
http://journal.r-project.org/archive/2009-2/rjournal_2009-2_will
iams.pdf 
http://www.r-bloggers.com/examples-and-resources-on-associati
on-rule-mining-with-r/ 
http://rattle.togaware.com 
http://www.statsoft.com/textbook/association-rules/ 
reference
michael hahsler, kurt hornik, and thomas reutterer (2006) impli-
cations of probabilistic data modeling for mining association rules. 
in m. spiliopoulou, r. kruse, c. borgelt, a. nuernberger, and w. 
gaul, editors, from data and information analysis to knowledge 
engineering, studies in classi   cation, data analysis, and knowl-
edge organization, pages 598   605. springer-verlag.
r functions used in this chapter
apriori() - uses the algorithm of the same name to analyze a trans-
action data set and generate rules.
itemfrequencyplot() - shows the relative frequency of commonly 
occurring items in the spare occurrence matrix. 

183

inspect() - shows the contents of the data object generated by apri-
ori() that generates the association rules
install.packages() - loads package from the cran respository
summary() - provides an overview of the contents of a data struc-
ture.

184

chapter 18

what   s your vector, victor?

in the previous chapter, we explored an unsupervised learning technique known as association rules 
mining. in this chapter, we will examine a set of supervised learning approaches known as support 
vector machines (id166s). id166s are    exible algorithms that excel at addressing classi   cation 
problems.

185

from the previous chapter you may remember that data mining 
techniques fall into two large categories: supervised learning tech-
niques and unsupervised learning techniques. the association 
rules mining examined in the previous chapter was an unsuper-
vised technique. this means that there was no particular criterion 
that we were trying to predict, rather we were just looking for pat-
terns that would emerge from the data naturally.
in the present chapter we will examine a supervised learning tech-
nique called "support vector machines." why the technique is 
called this we will examine shortly. the reason this is considered a 
supervised learning technique is that we "train" the algorithm on 
an initial set of data (the "supervised" phase) and then we test it 
out on a brand new set of data. if the training we accomplished 
worked well, then the algorithm should be able to predict the right 
outcome most of the time in the test data.
take the weather as a simple example. some days are cloudy, some 
are sunny. the barometer rises some days and fall others. the 
wind may be strong or weak and it may come from various direc-
tions. if we collect data on a bunch of days and use those data to 
train a machine learning algorithm, the algorithm may    nd that 
cloudy days with a falling barometer and the wind from the east 
may signal that it is likely to rain. next, we can collect more data 
on some other days and see how well our algorithm does at pre-
dicting rain on those days. the algorithm will make mistakes. the 
percentage of mistakes is the error rate, and we would like the er-
ror rate to be as low as possible. 
this is the basic strategy of supervised machine learning: have a 
substantial number of training cases that the algorithm can use to 

discover and mimic the underlying pattern and then use the re-
sults of that process on a test data set in order to    nd out how well 
the algorithm and parameters perform in a "cross validation." 
cross validation, in this instance, refers to the process of verifying 
that the trained algorithm can carry out is prediction or classi   ca-
tion task accurately on novel data.
in this chapter, we will develop a "support vector machine" (id166) 
to classify emails into spam or not spam. an id166 maps a low di-
mensional problem into a higher dimensional space with the goal 
of being able to describe geometric boundaries between different 
regions. the input data (the independent variables) from a given 
case are processed through a "mapping" algorithm called a kernel 
(the kernel is simply a formula that is run on each case   s vector of 
input data), and the resulting kernel output determines the posi-
tion of that case in multidimensional space. 
a simple 2d-3d mapping example illustrates how this works: 
imagine looking at a photograph of a snow-capped mountain pho-
tographed from high above the earth such that the mountain looks 
like a small, white circle completely surrounded by a region of 
green trees. using a pair of scissors, there is no way of cutting the 
photo on a straight line so that all of the white snow is on one side 
of the cut and all of the green trees are on the other. in other words 
there is no simple linear separation function that could correctly 
separate or classify the white and green points given their 2d posi-
tion on the photograph. 
next, instead of a piece of paper, think about a realistic three-
dimensional clay model of the mountain. now all the white points 
occupy a cone at the peak of the mountain and all of the green 

186

points lie at the base of the mountain. imagine inserting a sheet of 
cardboard through the clay model in a way that divides the snow-
capped peak from the green-tree-covered base. it is much easier to 
do now because the white points are sticking up into the high alti-
tude and the green points are all on the base of the mountain.
the position of that piece of cardboard is the planar separation 
function that divides white points from green points. a support 
vector machine analysis of this scenario would take the original 
two dimensional point data and search for a projection into three 
dimensions that would maximize the spacing between green 
points and white points. the result of the analysis would be a 
mathematical description of the position and orientation of the 
cardboard plane. given inputs describing a novel data point, the 
id166 could then map the data into the higher dimensional space 
and then report whether the point was above the cardboard (a 
white point) or below the cardboard (a green point). the so called 
support vectors contain the coef   cients that map the input data for 
each case into the high dimensional space.
to get started with support vector machines, we can load one of 
the r packages that supports this technique. we will use the "kern-
lab" package. use the commands below:
> install.packages("kernlab")
> library(kernlab)
i found that it was important to use the double quotes in the    rst 
command, but not in the second command. the data set that we 
want to use is built into this package. the data comes from a study 
of spam emails received by employees at the hewlett-packard com-
pany. load the data with the following command:

4601 obs. of  58 variables:

> data(spam)
this command does not produce any output. we can now inspect 
the "spam" dataset with the str() command:
> str(spam)
'data.frame':	
 $ make             : num  0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...
 $ address          : num  0.64 0.28 0 0 0 0 0 0 0 0.12 ...
 $ all              : num  0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...
 $ num3d            : num  0 0 0 0 0 0 0 0 0 0 ...
.
.
.
 $ chardollar       : num  0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...
 $ charhash         : num  0 0.048 0.01 0 0 0 0 0 0.022 0 ...
 $ capitalave       : num  3.76 5.11 9.82 3.54 3.54 ...
 $ capitallong      : num  61 101 485 40 40 15 4 11 445 43 ...
 $ capitaltotal     : num  278 1028 2259 191 191 ...
 $ type             : factor w/ 2 levels "nonspam","spam": 2 2 2 2 2 2 2 2 
2 2 ...
some of the lines of output have been elided from the material 
above. you can also use the dim() function to get a quick overview 
of the data structure:
> dim(spam)
[1] 4601   58
the dim() function shows the "dimensions" of the data structure. 
the output of this dim() function shows that the spam data struc-

187

ture has 4601 rows and 58 columns. if you inspect a few of the col-
umn names that emerged from the str() command, you may see 
that each email is coded with respect to its contents. there is lots of 
information available about the data set here:
http://archive.ics.uci.edu/ml/datasets/spambase 
for example, just before "type" at the end of the output of the str() 
command on the previous page, we see a variable called "capitalto-
tal." this is the total number of capital letters in the whole email. 
right after that is the criterion variable, "type," that indicates 
whether an email was classi   ed as spam by human experts. let   s 
explore this variable a bit more:
> table(spam$type)
nonspam    spam 
   2788    1813 
we use the table function because type is a factor rather than a nu-
meric variable. the output shows us that there are 2788 messages 
that were classi   ed by human experts as not spam, and 1813 mes-
sages that were classi   ed as spam. what a great dataset!
to make the analysis work we need to divide the dataset into a 
training set and a test set. there is no universal way to do this, but 
as a rule of thumb, you can use two thirds of the data set to train 
and the remainder to test. let   s    rst generate a randomized index 
that will let us choose cases for our training and test sets. in the fol-
lowing command, we create a new list/vector variable that sam-
ples at random from a list of numbers ranging from 1 to the    nal 
element index of the spam data (4601).

> randindex <- sample(1:dim(spam)[1])
> summary(randindex)
   min. 1st qu.  median    mean 3rd qu.    max. 
      1    1151    2301    2301    3451    4601 
> length(randindex)
[1] 4601
the output of the summary() and length() commands above show 
that we have successfully created a list of indices ranging from 1 to 
4601 and that the total length of our index list is the same as the 
number of rows in the spam dataset: 4601. we can con   rm that the 
indices are randomized by looking at the    rst few cases:
> head(randindex)
[1] 2073  769 4565  955 3541 3357
it is important to randomize your selection of cases for the training 
and test sets in order to ensure that there is no systematic bias in 
the selection of cases. we have no way of knowing how the origi-
nal dataset was sorted (if at all) - in case it was sorted on some vari-
able of interest we do not just want to take the    rst 2/3rds of the 
cases as the training set.
next, let   s calculate the "cut point" that would divide the spam da-
taset into a two thirds training set and a one third test set:
> cutpoint2_3 <- floor(2 * dim(spam)[1]/3)
> cutpoint2_3
[1] 3067

188

the    rst command in this group calculates the two-thirds cut point 
based on the number of rows in spam (the expression    
dim(spam)[1] gives the number of rows in the spam dataset). the 
second command reveals that that cut point is 3067 rows into the 
data set, which seems very sensible given that there are 4601 rows 
in total. note that the    oor() function chops off any decimal part of 
the calculation. we want to get rid of any decimal because an index 
variable needs to be an integer.
now we are ready to generate our test and training sets from the 
original spam dataset. first we will build our training set from the 
   rst 3067 rows:
> traindata <- spam[randindex[1:cutpoint2_3],]
we make the new data set, called traindata, using the randomized 
set of indices that we created in the randindex list, but only using 
the    rst 3067 elements of randindex (the inner expression in 
square brackets, 1:cutpoint2_3, does the job of selecting the    rst 
3067 elements. from here you should be able to imagine the com-
mand for creating the test set:
> testdata <- 
spam[randindex[(cutpoint2_3+1):dim(spam)[1]],]
the inner expression now selects the rows from 3068 all the way 
up to 4601 for a total of 1534 rows. so now we have two separate 
data sets, representing a two-thirds training and one third test 
breakdown of the original data. we are now in good shape to train 
our support vector model. the following command generates a 
model based on the training data set:

> id166output <- kid166(type ~ ., data=traindata,    
kernel="rbfdot",kpar="automatic",c=5,cross=3,   
prob.model=true)
using automatic sigma estimation (sigest) for rbf 
or laplace kernel
let   s examine this command in some detail. the    rst argument, 
"type ~ .", speci   es the model we want to test. using the word 
"type" in this expression means that we want to have the "type" 
variable (i.e., whether the message is spam or non-spam) as the out-
come variable that our model predicts. the tilde character ("~") in 
an r expression simply separates the left hand side of the expres-
sion from the right hand side. finally, the dot character (".") is a 
shorthand that tell r to us all of the other variables in the data-
frame to try to predict "type." 
the "data" parameter let   s us specify which dataframe to use in the 
analysis, in this case, we have speci   ed that the procedure should 
use the traindata training set that we developed. 
the next parameter is an interesting one: kernel="rbfdot".  you will 
remember from the earlier discussion that the kernel is the custom-
izable part of the id166 algorithm that lets us project the low dimen-
sional problem into higher dimensional space. in this case, the 
rbfdot designation refers to the "radial basis function." one simple 
way of thinking about the radial basis function is that if you think 
of a point on a regular x,y coordinate system the distance from the 
origin to the point is like a radius of a circle. the "dot" in the name 
refers to the mathematical idea of a "dot product," which is a way 
of multiplying vectors together to come up with a single number 
such as a distance value. in simpli   ed terms, the radial basis func-

189

tion kernel takes the set of inputs from each row in a dataset and 
calculates a distance value based on the combination of the many 
variables in the row. the weighting of the different variables in the 
row is adjusted by the algorithm in order to get the maximum sepa-
ration of distance between the spam cases and the non-spam cases.
the "kpar" argument refers to a variety of parameters that can be 
used to control the operation of the radial basis function kernel. in 
this case we are depending on the good will of the designers of this 
algorithm by specifying "automatic." the designers came up with 
some "heuristics" (guesses) to establish the appropriate parameters 
without user intervention. 
the c argument refers to the so called "cost of constraints." remem-
ber back to our example of the the white top on the green moun-
tain? when we put the piece of cardboard (the planar separation 
function) through the mountain, what if we happen to get one 
green point on the white side or one white point on the green side? 
this is a kind of mistake that in   uences how the algorithm places 
the piece of cardboard. we can force these mistakes to be more or 
less "costly," and thus to have more in   uence on the position of our 
piece of cardboard and the margin of separation that it de   nes. we 
can get a large margin of separation - but possibly with a few mis-
takes - by specifying a small value of c. if we specify a large value 
of c we may possibly get fewer mistakes, but on at the cost of hav-
ing the cardboard cut a very close margin between the green and 
white points - the cardboard might get stuck into the mountain at a 
very weird angle just to make sure that all of the green points and 
white points are separated. on the other hand if we have a low 
value of c we will get a generalizable model, but one that makes 
more classi   cation mistakes.

in the next argument, we have speci   ed "cross=3." cross refers to 
the cross validation model that the algorithm uses. in this case, our 
choice of the    nal parameter, "prob.model=true," dictates that we 
use a so called three-fold cross validation in order to generate the 
probabilities associate with whether a message is or isn   t a spam 
message. cross validation is important for avoiding the problem of 
over   tting. in theory, many of the algorithms used in data mining 
can be pushed to the point where they essentially memorize the in-
put data and can perfectly replicate the outcome data in the train-
ing set. the only problem with this is that the model base don the 
memorization of the training data will almost never generalize to 
other data sets. in effect, if we push the algorithm too hard, it will 
become too specialized to the training data and we won   t be able to 
use it anywhere else. by using k-fold (in this case three fold) cross-
validation, we can rein in the    tting function so that it does not 
work so hard and so that it does creat a model that is more likely to 
generalize to other data.
let   s have a look at what our output structure contains:
> id166output
support vector machine object of class "kid166" 
sv type: c-svc  (classification) 
 parameter : cost c = 5 
gaussian radial basis id81. 
 hyperparameter : sigma =  0.0287825580201687 
number of support vectors : 953 

190

objective function value : -1750.51 
training error : 0.027388 
cross validation error : 0.076296 
id203 model included. 
most of this is technical detail that will not necessarily affect how 
we use the id166 output, but a few things are worth pointing out. 
first, the sigma parameter mentioned was estimated for us by the 
algorithm because we used the "automatic" option. thank good-
ness for that as it would have taken a lot of experimentation to 
choose a reasonable value without the help of the algorithm. next, 
note the large number of support vectors. these are the lists of 
weights that help to project the variables in each row into a higher 
dimensional space. the "training error" at about 2.7% is quite low. 
naturally, the cross-validation error is higher, as a set of parame-
ters never perform as well on subsequent data sets as they do with 
the original training set. even so, a 7.6% cross validation error rate 
is pretty good for a variety of prediction purposes. 
we can take a closer look at these support vectors with the follow-
ing command:
> hist(alpha(id166output)[[1]])
the alpha() accessor reveals the values of the support vectors. note 
that these are stored in a nested list, hence the need for the [[1]] ex-
pression to access the    rst list in the list of lists. because the particu-
lar dataset we are using only has two classes (spam or not spam), 
we only need one set of support vectors. if the "type" criterion vari-
able had more than two levels (e.g., spam, not sure, and not spam), 
we would need additional support vectors to be able to classify the 

cases into more than two groups. the histogram output reveals the 
range of the support vectors from 0 to 5:

the maximum value of the support vector is equal to the cost pa-
rameter that we discussed earlier. we can see that about half of the 
support vectors are at this maximum value while the rest cluster 
around zero. those support vectors at the maximum represent the 
most dif   cult cases to classify. with respect to our mountain meta-
phor, these are the white points that are below the piece of card-
board and the green points that are above it.

191

if we increase the cost parameter we can get fewer of these prob-
lem points, but at only at the cost of increasing our cross validation 
error:
> id166output <- kid166(type ~ ., data=traindata, 
kernel="rbfdot",kpar="automatic",c=50,cross=3,pro
b.model=true)
> id166output
support vector machine object of class "kid166" 

sv type: c-svc  (classification) 
 parameter : cost c = 50 

gaussian radial basis id81. 
 hyperparameter : sigma =  0.0299992970259353 

number of support vectors : 850 

objective function value : -6894.635 
training error : 0.008803 
cross validation error : 0.085424 
id203 model included. 

in the    rst command above, the c=50 is bolded to show what we 
changed from the earlier command. the output here shows that 
our training error went way down, to 0.88%, but that our cross-
validation error went up from 7.6% in our earlier model to 8.5% in 
this model. we can again get a histogram of the support vectors to 
show what has happened:

now there are only about 100 cases where the support vector is at 
the maxed out value (in this case 50, because we set c=50 in the 
id166 command). again, these are the hard cases that the model 

192

could not get to be on the right side of the cardboard (or that were 
right on the cardboard). meanwhile, the many cases with the sup-
port vector value near zero represent the combinations of parame-
ters that make a case lie very far from the piece of cardboard. these 
cases were so easy to classify that they really made no contribution 
to "positioning" the hyperplane that separates the spam cases from 
the non-spam cases.  
we can poke out way into this a little more deeply by looking at a 
couple of instructive cases. first, let   s    nd the index numbers of a 
few of the support vectors that were near zero:
 > alphaindex(id166output)[[1]][alpha(id166output)[[1]]<0.05]
 [1]   90   98  289  497  634  745 1055 1479 1530 1544 
1646 1830 1948 2520 2754
this monster of a command is not as bad as it looks. we are tap-
ping into a new part of the id166output object, this time using the 
alphaindex() accessor function. remember that we have 850 sup-
port vectors in this output. now imagine two lists of 850 right next 
to each other: the    rst is the list of support vectors themselves, we 
get at that list with the alpha() accessor function. the second list, 
lined right up next to the    rst list, is a set of indices into the origi-
nal training dataset, traindata. the left hand part of the expression 
in the command above let   s us access these indices. the right hand 
part of the expression, where it says alpha(id166output)[[1]]<0.05, 
is a conditional expression that let   s us pick from the index list just 
those cases with a very small support vector value. you can see the 
output above, just underneat the command: about 15 indices were 
returned. just pick off the    rst one, 90, and take a look at the indi-
vidual case it refers to:

 > traindata[90,]
make address all num3d our over remove
  0     0   0     0   0    0      0       
internet order mail receive will
    0     0    0       0    0
.
.
.
charexclamation chardollar charhash capitalave 
     1.123          0        0        2.6          
capitallong capitaltotal
   16           26
type
nonspam
the command requested row 90 from the traindata training set. a 
few of the lines of the output were left off for ease of reading and 
almost all of the variables thus left out were zero. note the very 
last two lines of the output, where this record is identi   ed as a 
non-spam email. so this was a very easy case to classify because it 
has virtually none of the markers that a spam email usually has 
(for example, as shown above, no mention of internet, order, or 
mail). you can contrast this case with one of the hard cases by run-
ning this command:
> alphaindex(id166output)[[1]][alpha(id166output)[[1]]==50]

193

you will get a list of the 92 indices of cases where the support vec-
tor was "maxed out" to the level of the cost function (remember 
c=50 from the latest run of the id166() command). pick any of those 
cases and display it, like this:
> traindata[11,]
this particular record did not have many suspicious keywords, but 
it did have long strings of capital letters that made it hard to clas-
sify (it was a non-spam case, by the way). you can check out a few 
of them to see if you can spot why each case may have been dif   -
cult for the classi   er to place. 
the real acid test for our support vector model, however, will be to 
use the support vectors we generated through this training process 
to predict the outcomes in a novel data set. fortunately, because 
we prepared carefully, we have the testdata training set ready to 
go. the following commands with give us some output known as a 
"confusion matrix:"
> id166pred <- predict(id166output, testdata, 
type="votes")
> comptable <- data.frame(testdata[,58],id166pred[1,])
> table(comptable)
              id166pred.1...
testdata...58.   0   1
       nonspam  38 854
       spam    574  68

the    rst command in the block above uses our model output from 
before, namely id166output, as the parameters for prediction. it 
uses the "testdata," which the support vectors have never seen be-
fore, to generate predictions, and it requests "votes" from the pre-
diction process. we could also look at probabilities and other types 
of model output, but for a simple analysis of whether the id166 is 
generating good predictions, votes will make our lives easier. 
the output from the predict() command is a two dimensional list. 
you should use the str() command to examine its structure. basi-
cally there are two lists of "vote" values side by side. both lists are 
1534 elements long, corresponding to the 1534 cases in our testdata 
object. the lefthand list has one for a non-spam vote and zero for a 
spam vote. because this is a two-class problem, the other list has 
just the opposite. we can use either one because they are mirror im-
ages of each other.
in the second command above, we make a little dataframe, called 
comptable, with two variables in it: the    rst variable is the 58th 
column in the test data, which is the last column containing the 
"type" variable (a factor indicating spam or non-spam). remember 
that this type variable is the human judgments from the original 
dataset , so it is our ground truth. the second variable is the    rst 
column in our votes data structure (id166pred), so it contains ones 
for non-spam predictions and zeros for spam predictions. 
finally, applying the table() command to our new dataframe 
(comptable) gives us the confusion matrix as output. along the 
main diagonal we see the erroneous classi   cations - 38 cases that 
were not spam, but were classi   ed as spam by the support vector 
matrix and 68 cases that were spam, but were classi   ed as non-

194

sources
http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/id166basic/sv
mbasic_notes.pdf 
http://cran.r-project.org/web/packages/kernlab/kernlab.pdf 
http://en.wikipedia.org/wiki/confusion_matrix 
http://stackover   ow.com/questions/9480605/what-is-the-relatio
n-between-the-number-of-support-vectors-and-training-data-and 
http://www.louisaslett.com/courses/data_mining/st4003-lab7
-introduction_to_support_vector_machines.pdf 
http://www.jstatsoft.org/v11/i09/paper 

spam by the support vector matrix. on the counter-diagonal, we 
see 854 cases that were correctly classi   ed as non-spam and 574 
cases that were correctly classi   ed as spam. 
overall, it looks like we did a pretty good job. there are a bunch of 
different ways of calculating the accuracy of the prediction, de-
pending upon what you are most interested in. the simplest way 
is to sum the 68 + 38 = 106 error cases and divided by the 1534 total 
cases for an total error rate of about 6.9%. interestingly, that is a tad 
better than the 8.5% error rate we got from the k-fold cross-
validation in the run of id166() that created the model we are test-
ing. keep in mind, though, that we may be more interested in cer-
tain kinds of error than other kinds. for example, consider which is 
worse, an email that gets mistakenly quarantined because it is not 
really spam, or a spam email that gets through to someone   s inbox? 
it really depend on the situation, but you can see that you might 
want to give more consideration to either the 68 misclassi   cation 
errors or the other set of 38 misclassi   cation errors. 
chapter challenge
look up the term "confusion matrix" an then follow-up on some 
other terms such as type i error, type ii error, sensitivity, and speci-
   city. think about how the support vector machine model could be 
modi   ed to do better at either sensitivity or speci   city.
for a super challenge, try using another dataset with the kernlab 
id166 technology. there is a dataset called promotergene that is built 
into the kernlab package. you could also load up your own data set 
and try creating an id166 model from that. 

195

