the decision tree tutorial

by avi kak

id90: how to construct

them and how to use them for

classifying new data

avinash kak

purdue university

august 28, 2017

8:59am

an rvl tutorial presentation

(first presented in fall 2010; minor updates in august 2017)

c(cid:13)2017 avinash kak, purdue university

1

the decision tree tutorial

by avi kak

contents

1

2

3

4

5

6

7

8

9

introduction

id178

conditional id178

average id178

using class id178 to discover the best feature
for discriminating between the classes

constructing a decision tree

incorporating numeric features

the python module

decisiontree-3.4.3

the perl module

algorithm::decisiontree-3.43

10 bulk classi   cation of test data in csv files

11 dealing with large dynamic-range and

heavy-tailed features

12 testing the quality of the training data

13 decision tree introspection

14

15

incorporating id112

incorporating boosting

page

3

10

15

17

19

25

38

50

57

64

67

70

76

84

92

16 working with randomized id90

102

17

speeding up dt based classi   cation with hash tables

113

18 constructing regression trees

19 historical antecedents of decision tree

classi   cation in purdue rvl

120

125

2

the decision tree tutorial

by avi kak

1.

introduction

    let   s say your problem involves making a
decision based on n pieces of information.
let   s further say that you can organize the
n pieces of information and the correspond-
ing decision as follows:

f_1

f_2

f_3

......

f_n

=>

decision

---------------------------------------------------------------------------

val_2
val_2
val_2
val_2
val_2

val_3
val_3
val_3
val_3
val_3

.....
.....
.....
.....
.....

val_n
val_n
val_n
val_n
val_n

=>
=>
=>
=>
=>

d1
d2
d1
d1
d3

val_1
val_1
val_1
val_1
val_1
....
....

for convenience, we refer to each column
of the table as representing a feature f i
whose value goes into your decision making
process. each row of the table represents
a set of values for all the features and the
corresponding decision.

3

the decision tree tutorial

by avi kak

    as to what speci   cally the features f i shown

on the previous slide would be, that would
obviously depend on your application.
[in a
medical context, each feature f i could represent a laboratory

test on a patient, the value val i the result of the test, and

the decision d i the diagnosis. in drug discovery, each feature

f i could represent the name of an ingredient in the drug, the

value val i the proportion of that ingredient, and the decision

d i the e   ectiveness of the drug in a drug trial.

in a wall

street sort of an application, each feature could represent a

criterion (such as the price-to-earnings ratio) for making a
buy/sell investment decision, and so on.]

    if the di   erent rows of the training data,
arranged in the form of a table shown on
the previous slide, capture adequately the
statistical variability of the feature values
as they occur in the real world, you may be
able to use a decision tree for automating
the decision making process on any new
data.
[as to what i mean by    capturing adequately the
statistical variability of feature values   , see section 12 of this
tutorial.]

4

the decision tree tutorial

by avi kak

    let   s say that your new data record for
which you need to make a decision looks
like:

new_val_1

new_val_2

new_val_2

....

new_val_n

the decision tree will spit out the best pos-
sible decision to make for this new data
record given the statistical distribution of
the feature values for all the decisions in
the training data supplied through the ta-
ble on slide 3. the    quality    of this deci-
sion would obviously depend on the quality
of the training data, as explained in section
12.

    this tutorial will demonstrate how the no-
tion of id178 can be used to construct a
decision tree in which the feature tests for
making a decision on a new data record are
organized optimally in the form of a tree of
decision nodes.

5

the decision tree tutorial

by avi kak

    in the decision tree that is constructed from
your training data, the feature test that is
selected for the root node causes maximal
disambiguation of the di   erent possible de-
[in terms of
cisions for a new data record.

information content as measured by id178, the feature test

at the root would cause maximum reduction in the decision

id178 in going from all the training data taken together to
the data as partitioned by the feature test.]

    one then drops from the root node a set
of child nodes, one for each value of the
feature tested at the root node for the case
of symbolic features. for the case when a
numeric feature is tested at the root node,
one drops from the root node two child
nodes, one for the case when the value of
the feature tested at the root is less than
the decision threshold chosen at the root
and the other for the opposite case.

6

the decision tree tutorial

by avi kak

    subsequently, at each child node, you pose
the same question you posed at the root
node when you selected the best feature
to test at that node: which feature test at
the child node in question would maximally
disambiguate the decisions for the train-
ing data associated with the child node in
question?

    in the rest of this introduction, let   s see
how a decision-tree based classi   er can be
used by a id161 system to au-
tomatically    gure out which features work
the best in order to distinguish between a
set of objects. we assume that the vision
system has been supplied with a very large
number of elementary features (we could
refer to these as the vocabulary of a com-
puter vision system) and how to extract
them from images. but the vision system
has not been told in advance as to which
of these elementary features are relevant
to the objects.

7

the decision tree tutorial

by avi kak

    here is how we could create such a self-

learning id161 system:

    we show a number of di   erent objects
to a sensor system consisting of cam-
eras, 3d vision sensors (such as the mi-
crosoft kinect sensor), and so on. let   s
say these objects belong to m di   erent
classes.

    for each object shown, all that we tell
the computer is its class label. we do
not tell the computer how to discrim-
inate between the objects belonging to
the di   erent classes.

    we supply a large vocabulary of features
to the computer and also provide the
computer with tools to extract these
features from the sensory information
collected from each object.

8

the decision tree tutorial

by avi kak

    for image data, these features could
be color and texture attributes and the
presence or absence of shape primitives.
[for depth data, the features could be di   erent types of
curvatures of the object surfaces and junctions formed by
the joins between the surfaces, etc.]

    the job given to the computer: from
the data thus collected, it must    gure
out on its own how to best discrimi-
nate between the objects belonging to
the di   erent classes. [that is, the computer
must learn on its own what features to use for discrimi-
nating between the classes and what features to ignore.]

    what we have described above constitutes
an exercise in a self-learning computer vi-
sion system.

    as mentioned in section 19 of this tu-
torial, such a id161 system was
successfully constructed and tested in my
laboratory at purdue as a part of a ph.d
thesis.

9

the decision tree tutorial

by avi kak

2. id178

    id178 is a powerful tool that can be used
by a computer to determine on its own
own as to what features to use and how
to carve up the feature space for achieving
the best possible discrimination between
the classes.
[you can think of each decision of a certain

type in the last column of the table on slide 3 as de   ning a

class. if, in the context of id161, all the entries in

the last column boil down to one of    apple,       orange,    and
   pear,    then your training data has a total of three classes.]

    what is id178?

    if a random variable x can take n di   er-
ent values, the ith value xi with id203
p(xi), we can associate the following en-
tropy with x:

h(x) =    

n

xi=1

p(xi) log2 p(xi)

10

the decision tree tutorial

by avi kak

    to gain some insight into what h mea-
sures, consider the case when the normal-
ized histogram of the values taken by the
random variable x looks like

1/8

____ ____ ____ ____ ____ ____ ____ ____

^
|

|
|
|
(normalized) |

hist(x)

|
|
|
|

|
|
|
|

|
|
|
|

|
|
|
|

|
|
|
|

|
|
|
|

|
|
|
|

|
|
|
|

-------------------------------------------->

1

2

3

4

5

6

7

8

    in this case, x takes one of 8 possible val-
ues, each with a id203 of p(xi) = 1/8.
for a such a random variable, the id178
is given by

h(x)

=

   

=

   

8

xi=1
xi=1

8

1
8

1
8

log2

1
8

log2 2   3

=

3 bits

11

the decision tree tutorial

by avi kak

    now consider the following example in which
the uniformly distributed random variable
x takes one of 64 possible values:

1/64 ____ ____ ____ ____

|
histogram
(normalized) |

|
|

|
|

|
|

|
|

........

___

____

|
|

|
|

|
|

------------------------------------------------->

1

2

3

4

......

63

64

    in this case,

h(x)

=

   

=

   

64

xi=1
xi=1

8

1
64

log2

1
64

1
8

log2 2   6

=

6 bits

    so we see that the id178, measured in
bits because of 2 being the base of the
logarithm, has increased because now we
have greater uncertainty or    chaos    in the
values of x.
it can now take one of 64
values with equal id203.

12

the decision tree tutorial

by avi kak

    let   s now consider an example at the other
end of the    chaos   : we will consider an x
that is always known to take on a particular
value:

1.0

___

histogram
(normalized)

|
|
|
|
|
|

|
|
|
|
|
|

| 0

|
| 0
------------------------------------------------->

........

| 0 | 0

| 0

| 0

1

2

3

..

k

........

63

64

    in this case, we obviously have

p(xi)

=
=

1
0

xi = k
otherwise

    the id178 for such an x would be given

by:

h(x)

=

=

   

n

xi=1

p(xi) log2 p(xi)

    [p1 log2 p1 + ...pk log2 pk + ... + pn log2 pn ]

13

the decision tree tutorial

by avi kak

=

    1    log2 1 bits

= 0 bits

p     0+,
where we use the fact that as
p log p     0 in all of the terms of the sum-
mation except when i = k.

    so we see that the id178 becomes zero

when x has zero chaos.

    in general, the more nonuniform the
id203 distribution for an entity, the
smaller the id178 associated with the
entity.

14

the decision tree tutorial

by avi kak

3. conditional id178

    given two interdependent random variables
x and y , the conditional id178 h(y |x)
measures how much id178 (chaos) re-
mains in y if we already know the value of
the random variable x.

    in general,

h(y |x) = h(x, y )     h(x)

the id178 contained in both variables
when taken together is h(x, y ). the above
de   nition says that, if x and y are interde-
pendent, and if we know x, we can reduce
our measure of chaos in y by the chaos
that is attributable to x.
[for independent x
and y , one can easily show that h(x, y ) = h(x) + h(y ).]

15

the decision tree tutorial

by avi kak

    but what do we mean by knowing x in the
context of x and y being interdependent?
before we answer this question, let   s    rst
look at the formula for the joint id178
h(x, y ), which is given by

h(x, y ) =    xi,j

p(cid:0)xi, yj(cid:1) log2 p(cid:0)xi, yj(cid:1)

    when we say we know x, in general what
we mean is that we know that the variable
x has taken on a particular value. let   s
say that x has taken on a speci   c value a.
the id178 associated with y would now
be given by:

h(cid:0)y(cid:12)(cid:12)x = a(cid:1) =    xi

p(cid:0)yi(cid:12)(cid:12)x = a(cid:1)    log2 p(cid:0)yi(cid:12)(cid:12)x = a(cid:1)

    the formula shown for h(y |x) on the pre-

vious page is the average of h(cid:16)y (cid:12)(cid:12)(cid:12)

over all possible instantiations a for x.

x = a(cid:17)

16

the decision tree tutorial

by avi kak

4. average entropies

    given n independent random variables x1,
x2, . . . xn , we can associate an average
id178 with all n variables by

hav =

n

x1

h(xi)    p(xi)

    for another kind of an average, the condi-
tional id178 h(y |x) is also an average,
in the sense that the right hand side shown
below is an average with respect to all of
the di   erent ways the conditioning variable
can be instantiated:

h(cid:0)y(cid:12)(cid:12)x(cid:1) = xa

h(cid:0)y(cid:12)(cid:12)x = a(cid:1)    p(cid:0)x = a)

where h(cid:0)y(cid:12)(cid:12)x = a(cid:1) is given by the formula at

the bottom of the previous slide.

17

the decision tree tutorial

by avi kak

    to establish the claim made in the previous

bullet, note that

x = a(cid:17)o p(x = a)

x(cid:17) = xa

h(cid:16)y(cid:12)(cid:12)(cid:12)

h(cid:16)y(cid:12)(cid:12)(cid:12)
x = a(cid:17)    p(x = a)
p(cid:16)yj(cid:12)(cid:12)(cid:12)
x = a(cid:17) log2 p(cid:16)yj(cid:12)(cid:12)(cid:12)
=    xa nxj
p(cid:16)yj(cid:12)(cid:12)(cid:12)
xi(cid:17) log2 p(cid:16)yj(cid:12)(cid:12)(cid:12)
xi(cid:17) p(xi)
=    xi xj
=    xi xj
=    xi xj
= h(x, y ) + xi xj
= h(x, y ) + xi

p(xi, yj) log2 p(xi)

p(xi) log2 p(xi)

p(xi, yj)

p(xi, yj)

p(xi)

p(xi)

p(xi)

log2

p(xi, yj)h log2 p(xi, yj)     log2 p(xi)i

= h(x, y )     h(x)

the 3rd expression is a rewrite of the 2nd
with a more compact notation.
[in the 7th, we

note that we get a marginal id203 when a joint id203
is summed with respect to its free variable.]

18

the decision tree tutorial

by avi kak

5. using class id178 to discover the
best feature for discriminating between

the classes

    consider the following question: let us say
that we are given the measurement data as
described on slides 3 and 4. let the ex-
haustive set of features known to the com-
puter be {f1, f2, ....., fk}.

    now the computer wants to know as to
which of these features is best in the sense
of being the most class discriminative.

    how does the computer do that?

19

the decision tree tutorial

by avi kak

    to discover the best feature, all that the
computer has to do is to compute the class
id178 as conditioned on each speci   c fea-
ture f separately as follows:

h(c|f ) = xa

v(f ) = a(cid:17)    p(cid:16)v(f ) = a(cid:17)

h(cid:16)c(cid:12)(cid:12)(cid:12)

where the notation v(f ) = a means that
the value of feature f is some speci   c value
a. the computer selects that feature f for
[in
which h(c|f ) is the smallest value.
the formula shown above, the averaging carried out over the

values of the feature f is the same type of averaging as shown
at the bottom of slide 17.] notation: note that

c is a random variable over the class labels. if your training

data mentions the following three classes:    apple,       orange,   

and    pear,    then c as a random variable takes one of these

labels as its value.

    let   s now focus on the calculation of the
right hand side in the equation shown above.

20

the decision tree tutorial

by avi kak

    the id178 in each term on the right hand
side in the equation shown on the previous
slide can be calculated by

v(f ) = a(cid:17) =    xm

h(cid:16)c(cid:12)(cid:12)(cid:12)

p(cid:16)cm(cid:12)(cid:12)(cid:12)

v(f ) = a(cid:17)    log2 p(cid:16)cm(cid:12)(cid:12)(cid:12)

v(f ) = a(cid:17)

where cm is the name of the mth class and
the summation is over all the classes.

    but how do we    gure out p(cid:18)cm(cid:12)(cid:12)(cid:12)(cid:12)

that is needed on the right hand side?

v(f ) = a(cid:19)

    we will next present two di   erent ways for

v(f ) = a(cid:19). the    rst ap-

calculating p(cid:18)cm(cid:12)(cid:12)(cid:12)(cid:12)

proach works if we can assume that the ob-
jects shown to the sensor system are drawn
uniformly from the di   erent classes. if that
is not the case, one must use the second
approach.

21

the decision tree tutorial

by avi kak

    our    rst approach for calculating p(cm(cid:12)(cid:12)v(f ) =

a) is count-based: given m classes of objects
that we show to a sensor system, we pick objects

randomly from the population of all objects be-

longing to all classes. say the sensor system is

allowed to measure k di   erent kinds of features:
f1, f2, ...., fk. for each feature fk, the sensor sys-
tem keeps a count of the number of objects that
gave rise to the v(fk) = a value. now we estimate

p(cm(cid:12)(cid:12)v(f ) = a) for any choice of f = fk simply by

counting o    the number of objects from class cm
that exhibited the v(fk) = a measurement.

    our second approach for estimating p(cid:0)cm(cid:12)(cid:12)v(f ) =
a(cid:1) uses the bayes    theorem:
p(cid:16)v(f ) = a(cid:12)(cid:12)(cid:12)

cm(cid:17)    p(cm)

v(f ) = a(cid:17) =

p(cid:16)v(f ) = a(cid:17)

p(cid:16)cm(cid:12)(cid:12)(cid:12)

this formula also allows us to carry out
separate measurement experiments for ob-
jects belonging to di   erent classes.

22

the decision tree tutorial

by avi kak

    another advantage of the formula shown
at the bottom of the previous slide is that
it is no longer a problem if only a small
number of objects are available for some
of the classes     such non-uniformities in
object populations are taken care of by the
p(cm) term.

    the denominator in the formula at the bot-
tom of the previous slide can be taken care
of by the required id172:

xm

p(cid:16)cm(cid:12)(cid:12)(cid:12)

v(f ) = a(cid:17) = 1

    what   s interesting is that if we do obtain

p(cid:16)v(f ) = a(cid:17) through the id172 men-

tioned above, we can also use it in the for-
mula for calculating h(c|f ) as shown at

the top in slide 20. otherwise, p(cid:16)v(f ) = a(cid:17)

would need to be estimated directly from
the raw experimental data.

23

the decision tree tutorial

by avi kak

    so now we have all the information that is
needed to estimate the class id178 h(c|f )
for any given feature f by using the formula
shown at the top in slide 20.

    it follows from the nature of id178 (see
slides 10 through 14) that the smaller the
value for h(c|f ), especially in relation to
the value of h(c), the greater the class
discriminatory power of f .

    should it happen that h(c|f ) = 0 for some
feature f , that implies that feature f can
be used to identify objects belonging to
at least one of the m classes with 100%
accuracy.

24

the decision tree tutorial

by avi kak

6. constructing a decision tree

    now that you know how to use the class
id178 to    nd the best feature that will
discriminate between the classes, we will
now extend this idea and show how you can
construct a decision tree. subsequently
the tree may be used to classify future sam-
ples of data.

    but what is a decision tree?

    for those not familiar with decision tree
ideas, the traditional way to classify multi-
dimensional data is to start with a feature
space whose dimensionality is the same as
that of the data.

25

the decision tree tutorial

by avi kak

    in the traditional approach, each feature
in the space would correspond to the at-
tribute that each dimension of the data
measures. you then use the training data
to carve up the feature space into di   erent
regions, each corresponding to a di   erent
class. subsequently, when you are trying to
classify a new data sample, you locate it in
the feature space and    nd the class label
of the region to which it belongs. one can
also give the data point the same class la-
bel as that of the nearest training sample.
(this is referred to as the nearest neighbor
classi   cation.)

    a decision tree classi   er works di   erently.

    when you construct a decision tree, you
select for the root node a feature test that
can be expected to maximally disambiguate
the class labels that could be associated
with the data you are trying to classify.

26

the decision tree tutorial

by avi kak

    you then attach to the root node a set of
child nodes, one for each value of the fea-
ture you chose at the root node.
[this statement

is not entirely accurate. as you will see later, for the case of symbolic features, you create child nodes for only those

feature values (for the feature chosen at the root node) that reduce the class id178 in relation to the value of the class

id178 at the root.] now at each child node you pose
the same question that you posed when
you found the best feature to use at the
root node: what feature at the child node
in question would maximally disambiguate
the class labels to be associated with a
given data vector assuming that the data
vector passed the root node on the branch
that corresponds to the child node in ques-
tion. the feature that is best at each node
is the one that causes the maximal reduc-
tion in class id178 at that node.

    based on the discussion in the previous sec-
tion, you already know how to    nd the best
feature at the root node of a decision tree.
now the question is: how we do construct
the rest of the decision tree?

27

the decision tree tutorial

by avi kak

    what we obviously need is a child node for
every possible value of the feature test that
was selected at the root node of the tree.

    assume that the feature selected at the
root node is fj and that we are now at one
of the child nodes hanging from the root.
so the question now is how do we select
the best feature to use at the child node.

    the root node feature was selected as that

f which minimized h(c|f ). with this choice,
we ended up with the feature fj at the
root. the feature to use at the child on the
branch v(fj) = aj will be selected as that
v(fj) = aj, f(cid:17).
[reminder: whereas v(fj) stands for
the    value of feature fj,    the notation aj
stands for a speci   c value taken by that
feature.]

f 6= fj which minimizes h(cid:16)c(cid:12)(cid:12)(cid:12)

28

the decision tree tutorial

by avi kak

    that is, for any feature f not previously
used at the root, we    nd the conditional
id178 (with respect to our choice for f) when we
are on the v(fj) = aj branch:

h(cid:16)c(cid:12)(cid:12)(cid:12)

f, v(fj) = aj(cid:17) =
h(cid:16)c(cid:12)(cid:12)(cid:12)
xb

v(f ) = b, v(fj) = aj(cid:17)    p(cid:16)v(f ) = b, v(fj) = aj(cid:17)

whichever feature f yields the smallest value
for the id178 mentioned on the left hand
side of the above equation will become the
feature test of choice at the branch in ques-
tion.

    strictly speaking, the id178 formula shown

above for the calculation of average en-
tropy is not correct since it does not re   ect
the fact that the probabilistic averaging on
the right hand side is only with respect to
the values taken on by the feature f .

29

the decision tree tutorial

by avi kak

    in the equation on the previous slide, for
the summation shown on the right to yield
a true average with respect to di   erent
possible values for the feature f , the for-
mula would need to be expressed as   

h(cid:16)c(cid:12)(cid:12)(cid:12)
xb

f, v(fj) = aj(cid:17) =
h(cid:16)c(cid:12)(cid:12)(cid:12)

v(f ) = b, v(fj) = aj(cid:17)   

p(cid:16)v(f ) = b, v(fj) = aj(cid:17)
p(cid:16)v(f ) = b, v(fj) = aj(cid:17)
paj

    the component entropies in the above sum-

mation on the right would be given by

h(cid:16)c(cid:12)(cid:12)(cid:12)
   xm

v(f ) = b, v(fj) = aj(cid:17) =
p(cid:16)cm(cid:12)(cid:12)(cid:12)

v(f ) = b, v(fj) = aj(cid:17)    log2 p(cid:16)cm(cid:12)(cid:12)(cid:12)

for any given feature f 6= fj.

v(f ) = b, v(fj) = aj(cid:17)

   in my lab at purdue, we refer to such id172s in the calcu-
lation of average id178 as    jz id172       after padmini
jaikumar and josh zapf.

30

the decision tree tutorial

by avi kak

    the id155 needed in the
previous formula is estimated using bayes
theorem:

p(cid:16)cm(cid:12)(cid:12)(cid:12)

v(f ) = b, v(fj) = aj(cid:17) =
p(cid:16)v(f ) = b, v(fj) = aj(cid:12)(cid:12)(cid:12)

=

cm(cid:17)    p(cid:16)cm(cid:17)

p(cid:16)v(f ) = b, v(fj) = aj(cid:17)
cm(cid:17)    p(cid:16)v(fj) = aj(cid:12)(cid:12)(cid:12)
p(cid:16)v(f ) = b(cid:17)    p(cid:16)v(fj) = aj(cid:17)

cm(cid:17)    p(cid:16)cm(cid:17)

=

p(cid:16)v(f ) = b(cid:12)(cid:12)(cid:12)

where the second equality is based on the
assumption that the features are statisti-
cally independent.

31

the decision tree tutorial

by avi kak

feature tested at root

f j

f   =  2
j

f   =  1
j

feature tested at this node

f k

feature tested at this node

f

l

    you will add other child nodes to the root
in the same manner, with one child node
for each value that can be taken by the
feature fj.

    this process can be continued to extend
the tree further to result in a structure that
will look like what is shown in the    gure
above.

32

the decision tree tutorial

by avi kak

    now we will address the very important is-
sue of the stopping rule for growing the
tree. that is, when does a node get a fea-
ture test so that it can be split further and
when does it not?

    a node is assigned the id178 that re-
sulted in its creation. for example, the
root gets the id178 h(c) computed from
the class priors.

    the children of the root are assigned the
fj) that resulted in their cre-

id178 h(c(cid:12)(cid:12)(cid:12)

ation.

    a child node of the root that is on the
branch v(fj) = aj gets its own feature test
(and is split further) if and only if we can

   nd a feature fk such that h(cid:16)c(cid:12)(cid:12)fk, v(fj) = aj(cid:17)
is less than the id178 h(c(cid:12)(cid:12)(cid:12)

by the child from the root.

fj) inherited

33

the decision tree tutorial

by avi kak

f, v(fj) = aj(cid:17) < h(cid:16)c(cid:12)(cid:12)(cid:12)

    if the condition h(cid:16)c(cid:12)(cid:12)(cid:12)

fj)(cid:17)
cannot be satis   ed at the child node on the
branch v(fj) = aj of the root for any fea-
ture f 6= fj, the child node remains without
a feature test and becomes a leaf node of
the decision tree.

    another reason for a node to become a
leaf node is that we have used up all the
features along that branch up to that node.

    that brings us to the last important is-
sue related to the construction of a de-
cision tree: associating class probabilities
with each node of the tree.

    as to why we need to associate class prob-
abilities with the nodes in the decision tree,
let us say we are given for classi   cation a
new data vector consisting of features and
their corresponding values.

34

the decision tree tutorial

by avi kak

    for the classi   cation of the new data vec-
tor mentioned above, we will    rst subject
this data vector to the feature test at the
root. we will then take the branch that
corresponds to the value in the data vec-
tor for the root feature.

    next, we will subject the data vector to
the feature test at the child node on that
branch. we will continue this process until
we have used up all the feature values in
the data vector. that should put us at one
of the nodes, possibly a leaf node.

    now we wish to know what the residual
class probabilities are at that node. these
class probabilities will represent our classi-
   cation of the new data vector.

35

the decision tree tutorial

by avi kak

    if the feature tests along a path to a node
in the tree are v(fj) = aj, v(fk) = bk, . . ., we
will associate the following class id203
with the node:

v(fj) = aj, v(fk) = bk,

. . .(cid:19)

p(cid:18)cm(cid:12)(cid:12)(cid:12)(cid:12)

for m = 1, 2, . . . , m where m is the number
of classes.

    the above id203 may be estimated

with id47:

v(fj) = aj, v(fk) = bk,

p(cid:16)cm(cid:12)(cid:12)(cid:12)
p(cid:16)v(fj) = aj, v(fk) = bk, . . .(cid:12)(cid:12)(cid:12)
p(cid:16)v(fj) = aj, v(fk) = bk,

. . .(cid:17) =
cm(cid:17)    p(cid:16)cm(cid:17)

. . .(cid:17)

36

the decision tree tutorial

by avi kak

    if we again use the notion of statistical
independence between the features both
when they are considered on their own and
when considered conditioned on a given
class, we can write:

p(cid:16)v(fj) = aj, v(fk) = bk,
. . .(cid:12)(cid:12)(cid:12)

p(cid:16)v(fj) = aj, v(fk) = bk,

. . .(cid:17) = yf on branch

cm(cid:17) = yf on branch

p(cid:16)v(f ) = value(cid:17)
p(cid:16)v(f ) = value(cid:12)(cid:12)(cid:12)

cm(cid:17)

37

the decision tree tutorial

by avi kak

7.

incorporating numeric features

    a feature is numeric if it can take any
   oating-point value from a continuum of
values. the sort of reasoning we have de-
scribed so far for choosing the best feature
at a node and constructing a decision tree
cannot be applied directly to the case of
numeric features.

    however, numeric features lend themselves
to recursive partitioning that eventually re-
sults in the same sort of a decision tree you
have seen so far.

    when we talked about symbolic features in
section 5, we calculated the class id178
with respect to a feature by constructing a
probabilistic average of the class entropies
with respect to knowing each value sepa-
rately for the feature.

38

the decision tree tutorial

by avi kak

    let   s say f is a numeric feature. for a
numeric feature, a better approach consists
of calculating the class id178 vis-a-vis a
decision threshold on the feature values:

h(cid:16)c(cid:12)(cid:12)(cid:12)

vth(f ) =   (cid:17) = h(cid:16)c(cid:12)(cid:12)(cid:12)
v(f )       (cid:17)    p(cid:16)v(f )       (cid:17) +
h(cid:16)c(cid:12)(cid:12)(cid:12)
v(f ) >   (cid:17)    p(cid:16)v(f ) >   (cid:17)
where vth(f ) =    means that we have set
the decision threshold for the values of the
feature f at    for the purpose of parti-
tioning the data into parts, one for which
v(f )        and the other for which v(f ) >   .

    the left side in the equation shown above
is the average id178 for the two parts
considered separately on the right hand side.
the threshold for which this average en-
tropy is the minimum is the best threshold
to use for the numeric feature f .

39

the decision tree tutorial

by avi kak

    to illustrate the usefulness of minimizing
this average id178 for discovering the best
threshold, consider the case when we have
only two classes, one for which all values
of f are less than    and the other for which
all values of f are greater than   . for this
case, the left hand side above would be
zero.

    the components entropies on the right hand

side in the previous equation can be calcu-
lated by

h(cid:16)c(cid:12)(cid:12)(cid:12)

and

h(cid:16)c(cid:12)(cid:12)(cid:12)

v(f )       (cid:17) =    xm

p(cid:16)cm(cid:12)(cid:12)(cid:12)

v(f )       (cid:17)    log2 p(cid:16)cm(cid:12)(cid:12)(cid:12)

v(f )       (cid:17)

v(f ) >   (cid:17) =    xm

p(cid:16)cm(cid:12)(cid:12)(cid:12)

v(f ) >   (cid:17)    log2 p(cid:16)cm(cid:12)(cid:12)(cid:12)

v(f ) >   (cid:17)

40

the decision tree tutorial

by avi kak

    we can estimate p(cid:0)cm(cid:12)(cid:12)v(f )       (cid:1) and p(cid:0)cm(cid:12)(cid:12)v(f ) >
  (cid:1) by using the bayes    theorem:

v(f )       (cid:17) =

p(cid:16)cm(cid:12)(cid:12)(cid:12)

and

v(f ) >   (cid:17) =

p(cid:16)cm(cid:12)(cid:12)(cid:12)

cm(cid:17)    p(cm)

p(cid:16)v(f )       (cid:12)(cid:12)(cid:12)

p(cid:16)v(f )       (cid:17)

cm(cid:17)    p(cm)

p(cid:16)v(f ) >   (cid:12)(cid:12)(cid:12)

p(cid:16)v(f ) >   (cid:17)

the various terms on the right sides in the
two equations shown above can be esti-
mated directly from the training data.

    however, in practice, you are better o    us-
ing the id172 shown on the next
page for estimating the denominator in the
equations shown above.

41

the decision tree tutorial

by avi kak

    although the denominator in the equations
on the previous slide can be estimated di-
rectly from the training data, you are likely
to achieve superior results if you calculate
this denominator directly from (or, at least,
adjust its calculated value with) the follow-
ing id172 constraint on the proba-
bilities on the left:

xm

p(cid:16)cm (cid:12)(cid:12)(cid:12)

v(f )       (cid:17) = 1

and

xm

p(cid:16)cm (cid:12)(cid:12)(cid:12)

v(f ) >   (cid:17) = 1

    now we are all set to use this partitioning
logic to choose the best feature for the
root node of our decision tree. we proceed
as explained on the next slide.

42

the decision tree tutorial

by avi kak

    given a set of numeric features and a train-
ing data    le, we seek that numeric feature
for which the average id178 over the two
parts created by the thresholding partition
is the least.

    for each numeric feature, we scan through
all possible partitioning points (these would
obviously be the sampling points over the
interval corresponding to the values taken
by that feature), and we choose that par-
titioning point which minimizes the aver-
age id178 of the two parts. we consider
this partitioning point as the best decision
threshold to use vis-a-vis that feature.

    given a set of numeric features, their as-
sociated best decision thresholds, and the
corresponding average entropies over the
partitions obtained, we select for our best
feature that feature that has the least av-
erage id178 associated with it at its best
decision threshold.

43

the decision tree tutorial

by avi kak

    after    nding the best feature for the root
node in the manner described above, we
can drop two branches from it, one for the
training samples for which v(f )        and the
other for the samples for which v(f ) >    as
shown in the    gure below:

feature tested at root

f

j

<=   

v( 

f )
j

>   

v( 

f )
j

    the argument stated above can obviously
be extended to a mixture of numeric and
symbolic features as explained on the next
slide.

44

the decision tree tutorial

by avi kak

    given a mixture or symbolic and numeric
features, we associate with each symbolic
feature the best possible id178 calculated
in the manner explained in section 5. and,
we associate with each numeric feature the
best id178 that corresponds to the best
threshold choice for that feature. given all
the features and their associated best class
entropies, we choose that feature for the
root node of our decision tree for which
the class id178 is the minimum.

    now that you know how to construct the
root node for the case when you have just
numeric features or a mixture of numeric
and symbolic features, the next question is
how to branch out from the root node.

    if the best feature selected for the root
node is symbolic, we proceed in the same
way as described in section 5 in order to
grow the tree to the next level.

45

the decision tree tutorial

by avi kak

    on the other hand, if the best feature f
selected at the root node is numeric and
the best decision threshold for the feature
is   , we must obviously construct two child
nodes at the root, one for which v(f )       
and the other for which v(f ) >   .

    to extend the tree further, we now select
the best features to use at the child nodes
of the root. let   s assume for a moment
that the best feature chosen for a child
node also turns out to be numeric.

    let   s say we used the numeric feature fj,
along with its decision threshold   j, at the
root and that the choice of the best feature
to use at the left child turns out to be fk
and that its best decision threshold is   k.

46

the decision tree tutorial

by avi kak

    the choice (fk,   k) at the left child of the
root must be the best possible among all
possible features and all possible thresholds
for those features so that following average
id178 is minimized:

v(fj)       j, v(fk)       k(cid:17)    p(cid:16)v(fj)       j, v(fk)       k(cid:17)

v(fj)       j, v(fk) >   k(cid:17)    p(cid:16)v(fj)       k, v(fk) >   k(cid:17)

+

h(cid:16)c (cid:12)(cid:12)(cid:12)
h(cid:16)c (cid:12)(cid:12)(cid:12)

    for the purpose of our explanations, we
assume that the left child at a node with a
numeric test feature always corresponds to
the    less than or equal to the threshold   
case and the right child to the    greater
than the threshold    case.

47

the decision tree tutorial

by avi kak

    at this point, our decision tree will

look

like what is shown below:

feature tested at root

f

j

)

v( f
j

  
<  
j

  
v( f ) >=  
j

j

feature tested

f

k

feature tested

f

l

v( f
k

)

  
<  
k

  
v( f ) >=  
k

k

    as we continue growing the decision tree in
this manner, an interesting point of di   er-
ence arises between the previous case when
we had purely symbolic features and when
we also have numeric features. when we
consider the features for the feature tests
to use at the children of the node where
we just used the fj feature for our feature
test, we throw the parent node   s feature fk
back into contention.

48

the decision tree tutorial

by avi kak

    in general, this di   erence between the deci-
sion trees for the purely symbolic case and
the id90 needed when you must
also deal with numeric features is more il-
lusory than real. that is because when
considering the root node feature fl at the
third-level nodes in the tree, the values of
fl will be limited to the interval [vmin(fl),   l)
in the left children of the root and to the
interval [  l, vmax(fl)) in the right children
of the root. testing for whether the value
of the feature fl
is in, say, the interval
[v(fmin(fl),   l) is not the same feature test
as testing for whether this value is in the
interval [vmin(fl), vmax(fl)).

    once we arrive at a child node, we carry
out at the child node the same reasoning
that we carried out at the root for the se-
lection of the best feature at the child node
and to then grow the tree accordingly.

49

the decision tree tutorial

by avi kak

8. the python module

decisiontree-3.4.3

note: versions prior to 2.0 could only handle symbolic train-

ing data. versions 2.0 and higher can handle both symbolic and

numeric training data.

    version 2.0 was a major re-write of the
module for incorporating numeric features.

    version 2.1 was a cleaned up version of v.
2.0. version 2.2 introduced the functional-
ity to evaluate the quality of training data.
the latest version is 3.4.3. to download
version 3.4.3:

https://engineering.purdue.edu/kak/distdt/decisiontree-3.4.3.html

click on the active link shown above to navi-

gate directly to the api of this software pack-

age.

50

the decision tree tutorial

by avi kak

    the module makes the following two as-
sumptions about the training data in a    .csv   
   le:
that the    rst column (meaning the
column with index 0) contains a unique in-
teger identi   er for each data record, and
that the    rst row contains the names to
be used for the features.

    shown below is a typical call to the con-

structor of the module:

training_datafile = "stage3cancer.csv"

dt = decisiontree.decisiontree(

training_datafile = training_datafile,
csv_class_column_index = 2,
csv_columns_for_features = [3,4,5,6,7,8],
id178_threshold = 0.01,
max_depth_desired = 3,
symbolic_to_numeric_cardinality_threshold = 10,
csv_cleanup_needed = 1,

)

in this call to the decisiontree constructor,
the option csv class column index is used to tell
the module that the class label is in the col-
umn indexed 2 (meaning the third column)
of the    .csv    training data    le.

51

the decision tree tutorial

by avi kak

    the constructor option csv columns for features

is used to tell the module that the columns
indexed 3 through 8 are to be used as fea-
tures.

    to explain the role of the constructor op-
tion symbolic to numeric cardinality threshold in
the call shown on the previous slide, note
that the module can treat those numeric
looking features symbolically if the di   er-
ent numerical values taken by the feature
in the call shown, if
are small in number.
a numeric feature takes 10 or fewer unique
values, it will be treated like a symbolic
feature.

    if the module can treat certain numeric
features symbolically, you might ask as to
what happens if the value for such a feature
in a test sample is not exactly the same as
one of the values in the training data.

52

the decision tree tutorial

by avi kak

    when a numeric feature is treated symbol-
ically, a value in a test sample is    snapped   
to the closest value in the training data.

    no matter whether you construct a de-
cision tree from purely symbolic data, or
purely numeric data, or a mixture of the
two, the two constructor parameters that
determine the number of nodes in the deci-
sion are id178 threshold and max depth desired.
more on these on the next slide.

    as for the role of id178 threshold, recall
that a child node is created only if the dif-
ference between the id178 at the current
node and the child node exceeds a thresh-
old. this parameter sets that threshold.

    regarding the parameter max depth desired, note

that the tree is grown in a depth-   rst man-
ner to the maximum depth set by this pa-
rameter.

53

the decision tree tutorial

by avi kak

    the option csv cleanup needed is important for
extracting data from    messy    csv    les.
that is, csv    les that use double-quoted
strings for either the    eld names or the    eld
values and that allow for commas to be
used inside the double-quoted strings.

    after the call to the constructor, the fol-
lowing three methods must be called to
initialize the probabilities:

dt.get_training_data()
dt.calculate_first_order_probabilities_for_numeric_features()
dt.calculate_class_priors()

    the tree itself is constructed and, if so de-

sired, displayed by the following calls:

root_node = dt.construct_decision_tree_classifier()
root_node.display_decision_tree("

")

where the    white-space    string supplied as
the argument to the display method is used
to o   set the display of the child nodes in
relation to the display of the parent nodes.

54

the decision tree tutorial

by avi kak

    after you have constructed a decision tree,

it is time classify a test sample.

    here is an example of the syntax used for a
test sample and the call you need to make
to classify it:

test_sample = [   g2 = 4.2   ,

   grade = 2.3   ,
   gleason = 4   ,
   eet = 1.7   ,
   age = 55.0   ,
   ploidy = diploid   ]

classification = dt.classify(root_node, test_sample)

    the classification returned by the call to
classify() as shown on the previous slide
is a dictionary whose keys are the class
names and whose values are the classi   ca-
tion probabilities associated with the classes.

    for further information, see the various ex-
ample scripts in the examples subdirectory
of the module.

55

the decision tree tutorial

by avi kak

    the module also allows you to generate
your own synthetic symbolic and numeric
data    les for experimenting with decision
trees.

    for large test datasets, see the next section
for the demonstration scripts that show
you how you can classify all your data records
in a csv    le in one go.

    see the web page at

https://engineering.purdue.edu/kak/distdt/decisiontree-3.4.3.html

for a full description of the api of this
python module.

56

the decision tree tutorial

by avi kak

9. the perl module

algorithm::decisiontree-3.43

note: versions 2.0 and higher of this module can handle simulta-

neously the numeric and the symbolic features. even for the purely

symbolic case, you are likely to get superior results with the latest

version of the module than with the older versions.

    the goal of this section is to introduce
the reader to some of the more impor-
tant functions in my perl module algo-
rithm::decisiontree that can be down-
loaded by clicking at the link shown below:

http://search.cpan.org/ avikak/algorithm-decisiontree-3.43/lib/algorithm/decisiontree.pm

please read the documentation at the cpan
site for the api of this software package. [if

clicking on the link shown above does not work you, you can

also just do a google search on    algorithm::decisiontree   

and go to version 3.43 when you get to the cpan page for
the module.]

57

the decision tree tutorial

by avi kak

    to use the perl module, you    rst need to

construct an instance of the algorithm::decisiontree
class as shown below:

my $training_datafile = "stage3cancer.csv";

my $dt = algorithm::decisiontree->new(

training_datafile => $training_datafile,
csv_class_column_index => 2,
csv_columns_for_features => [3,4,5,6,7,8],
id178_threshold => 0.01,
max_depth_desired => 8,
symbolic_to_numeric_cardinality_threshold => 10,
csv_cleanup_needed => 1,

);

    the constructor option csv class column index
informs the module as to which column of
your csv    le contains the class labels for
the data records. the column index-
ing is zero based. the constructor
option csv columns for features speci   es which
columns are to be used for feature values.
the    rst row of the csv    le must specify
the names of the features. see examples
of csv    les in the examples subdirectory of
the module.

58

the decision tree tutorial

by avi kak

    the option symbolic to numeric cardinality threshold

in the constructor is also important. for
the example shown above, if an ostensibly
numeric feature takes on only 10 or fewer
di   erent values in your training data    le,
it will be treated like a symbolic features.
the option id178 threshold determines the
granularity with which the entropies are
sampled for the purpose of calculating en-
tropy gain with a particular choice of de-
cision threshold for a numeric feature or a
feature value for a symbolic feature.

    the option csv cleanup needed is important for
extracting data from    messy    csv    les.
that is, csv    les that use double-quoted
strings for either the    eld names or the    eld
values and that allow for commas to be
used inside the double-quoted strings.

59

the decision tree tutorial

by avi kak

    after you have constructed an instance of
the decisiontree module, you read in the
training data    le and initialize the proba-
bility cache by calling:

$dt->get_training_data();
$dt->calculate_first_order_probabilities();
$dt->calculate_class_priors();

    now you are ready to construct a decision

tree for your training data by calling:

$root_node = $dt->construct_decision_tree_classifier();

where $root node is an instance of the dtnode
class that is also de   ned in the module    le.

    with that, you are ready to start classifying
new data samples     as i show on the next
slide.

60

the decision tree tutorial

by avi kak

    let   s say that your data record looks like:

my @test_sample

= qw / g2=4.2

grade=2.3
gleason=4
eet=1.7
age=55.0
ploidy=diploid /;

you can classify it by calling:

my $classification = $dt->classify($root_node, \@test_sample);

    the call to classify() returns a reference
to a hash whose keys are the class names
and the values the associated classi   cation
probabilities. this hash also includes an-
other key-value pair for the solution path
from the root node to the leaf node at
which the    nal classi   cation was carried
out.

61

the decision tree tutorial

by avi kak

    the module also allows you to generate
your own training datasets for experiment-
ing with id90 classi   ers. for that,
the module    le contains the following classes:
(1) trainingdatageneratornumeric, and
(2) trainingdatageneratorsymbolic

    the class trainingdatageneratornumeric outputs
a csv training data    le for experimenting
with numeric features.

    the numeric values are generated using
a multivariate gaussian distribution whose
mean and covariance are speci   ed in a pa-
rameter    le. see the    le param numeric.txt
in the examples directory for an example of
such a parameter    le. note that the di-
mensionality of the data is inferred from
the information you place in the parameter
   le.

62

the decision tree tutorial

by avi kak

    the class trainingdatageneratorsymbolic gener-
ates synthetic training data for the purely
symbolic case.
it also places its output
in a    .csv       le. the relative frequencies
of the di   erent possible values for the fea-
tures is controlled by the biasing informa-
tion you place in a parameter    le. see
param symbolic.txt for an example of such a
   le.

    see the web page at

http://search.cpan.org/~avikak/algorithm-decisiontree-3.43/

for a full description of the api of this perl
module.

    additionally, for large test datasets, see
section 10 of this tutorial for the demon-
stration scripts in the perl module that show
you how you can classify all your data records
in a csv    le in one go.

63

the decision tree tutorial

by avi kak

10. bulk classi   cation of all test data

records in a csv file

    for large test datasets, you would obvi-
ously want to process an entire    le of test
data records in one go.

    the examples directory of both the perl and
the python versions of the module include
demonstration scripts that show you how
you can classify all your data records in one
fell swoop if the records are in a csv    le.

    for the case of perl, see the following scripts

in the examples directory of the module for
bulk classi   cation of data records:

classify test data in a file.pl

64

the decision tree tutorial

by avi kak

    and for the case of python, check out the
following script in the examples directory for
doing the same things:

classify test data in a file.py

    all scripts mentioned on this section re-
quire three command-line arguments, the
   rst argument names the training data   le,
the second the test data   le, and the third
the    le in which the classi   cation results
will be deposited.

    the other examples directories, examplesid112,

examplesboosting, and examplesrandomizedtrees,
also contain scripts that illustrate how to
carry out bulk classi   cation of data records
when you wish to take advantage of bag-
ging, boosting, or tree randomization.
in
their respective directories, these scripts
are named:

65

the decision tree tutorial

by avi kak

id112_for_bulk_classification.pl
boosting_for_bulk_classification.pl
classify_database_records.pl

id112_for_bulk_classification.py
boosting_for_bulk_classification.py
classify_database_records.py

66

the decision tree tutorial

by avi kak

11. dealing with large dynamic-range

and heavy-tailed features

    for the purpose of estimating the probabil-
ities, it is necessary to sample the range of
values taken on by a numerical feature. for
features with    nice    statistical properties,
this sampling interval is set to the median
of the di   erences between the successive
feature values in the training data. (obvi-
ously, as you would expect, you    rst sort all
the values for a feature before computing
the successive di   erences.) this logic will
not work for the sort of a feature described
below.

    consider a feature whose values are heavy-
tailed, and, at the same time, the values
span a million to one range.

67

the decision tree tutorial

by avi kak

    what i mean by heavy-tailed is that rare
values can occur with signi   cant probabili-
ties. it could happen that most of the val-
ues for such a feature are clustered at one
of the two ends of the range. at the same
time, there may exist a signi   cant number
of values near the end of the range that is
less populated.

    typically, features related to human eco-
nomic activities     such as wealth, incomes,
etc.     are of this type.

    with the median-based method of setting
the sampling interval as described on the
previous slide, you could end up with a
sampling interval that is much too small.
that could potentially result in millions of
sampling points for the feature if you are
not careful.

68

the decision tree tutorial

by avi kak

    beginning with version 2.22 of the perl
module and version 2.2.4 of the python
module, you have two options for dealing
with such features. you can choose to go
with the default behavior of the module,
which is to sample the value range for such
a feature over a maximum of 500 points.

    or, you can supply an additional option
to the constructor that sets a user-de   ned
value for the number of points to use. the
name of the option is number of histogram bins.
the following script

construct_dt_for_heavytailed.pl

in the    examples    directory shows an ex-
ample of how to call the constructor of the
module with the number of histogram bins
option.

69

the decision tree tutorial

by avi kak

12. testing the quality of the training

data

    even if you have a great algorithm for con-
structing a decision tree, its ability to cor-
rectly classify a new data sample would de-
pend ultimately on the quality of the train-
ing data.

    here are the four most important reasons
for why a given training data    le may be
of poor quality:
(1) insu   cient number
of data samples to adequately capture the
statistical distributions of the feature val-
ues as they occur in the real world; (2) the
distributions of the feature values in the
training    le not re   ecting the distribution
as it occurs in the real world; (3) the num-
ber of the training samples for the di   erent
classes not being in proportion to the real-
world prior probabilities of the classes; and
(4) the features not being statistically in-
dependent.

70

the decision tree tutorial

by avi kak

    a quick way to evaluate the quality of your
training data is to run an n -fold cross-
validation test on the data. this test di-
vides all of the training data into n parts,
with n     1 parts used for training a deci-
sion tree and one part used for testing the
ability of the tree to classify correctly. this
selection of n    1 parts for training and one
part for testing is carried out in all of the n
di   erent possible ways. typically, n = 10.

    you can run a 10-fold cross-validation test
on your training data with version 2.2 or
higher of the python decision tree mod-
ule and version 2.1 or higher of the perl
version of the same.

    the next slide presents a word of caution
in using the output of a cross-validation to
either trust or not trust your training data
   le.

71

the decision tree tutorial

by avi kak

    strictly speaking, a cross-validation test
is statistically meaningful only if the
training data does not su   er from
any of the four shortcomings i men-
tioned at the beginning of this section.
[the real purpose of a cross-validation test is to
estimate the bayes classi   cation error     meaning
the classi   cation error that can be attributed to the
overlap between the class id203 distributions in
the feature space.]

    therefore, one must bear in mind the fol-
lowing when interpreting the results of a
cross-validation test: if the cross-validation
test says that your training data is of poor
quality, then there is no point in using a de-
cision tree constructed with this data for
classifying future data samples. on the
other hand, if the test says that your data
is of good quality, your tree may still be a
poor classi   er of the future data samples
on account of the four data shortcomings
mentioned at the beginning of this section.

72

the decision tree tutorial

by avi kak

    both the perl and the python decision-
tree modules contain a special subclass
evaltrainingdata that is derived from the
main decisiontree class. the purpose of
this subclass is to run a 10-fold cross-validation
test on the training data    le you specify.

    the code fragment shown below illustrates
how you invoke the testing function of the
evaltrainingdata class in the python ver-
sion of the module:

training_datafile = "training3.csv"
eval_data = decisiontree.evaltrainingdata(

training_datafile = training_datafile,
csv_class_column_index = 1,
csv_columns_for_features = [2,3],
id178_threshold = 0.01,
max_depth_desired = 3,
symbolic_to_numeric_cardinality_threshold = 10,
csv_cleanup_needed = 1,

)

eval_data.get_training_data()
eval_data.evaluate_training_data()

in this case, we obviously want to evaluate
the quality of the training data in the    le
training3.csv.

73

the decision tree tutorial

by avi kak

    the last statement in the code shown on
the previous slide prints out a confusion
matrix and the value of training data qual-
ity index on a scale of 0 to 100, with 100
designating perfect training data. the con-
fusion matrix shows how the di   erent classes
were misidenti   ed in the 10-fold cross-validation
test.

    the syntax for invoking the data testing

functionality in perl is the same:

my $training_datafile = "training3.csv";

my $eval_data = evaltrainingdata->new(

training_datafile => $training_datafile,
csv_class_column_index => 1,
csv_columns_for_features => [2,3],
id178_threshold => 0.01,
max_depth_desired => 3,
symbolic_to_numeric_cardinality_threshold => 10,
csv_cleanup_needed => 1,

);

$eval_data->get_training_data();
$eval_data->evaluate_training_data()

74

the decision tree tutorial

by avi kak

    this testing functionality can also be used
to    nd the best values one should use for
the constructor parameters id178 threshold,
max depth desired, and symbolic to numeric
cardinality threshold.

    the following two scripts in the examples
directory of the python version of the mod-
ule:

evaluate_training_data1.py
evaluate_training_data2.py

and the following two in the examples di-
rectory of the perl version

evaluate_training_data1.pl
evaluate_training_data2.pl

illustrate the use of the evaltrainingdata
class for testing the quality of your data.

75

the decision tree tutorial

by avi kak

13. decision tree introspection

    starting with version 2.3.1 of the python
module and with version 2.30 of the perl
module, you can ask the dtintrospection
class of the modules to explain the clas-
si   cation decisions made at the di   erent
nodes of the decision tree.

    perhaps the most important bit of infor-
mation you are likely to seek through dt
introspection is the list of the training sam-
ples that fall directly in the portion of the
feature space that is assigned to a node.

    however, note that, when training samples
are non-uniformly distributed in the under-
lying feature space, it is possible for a node

76

the decision tree tutorial

by avi kak

to exist even when there are no training
samples in the portion of the feature space
assigned to the node. [that is because the de-
cision tree is constructed from the id203 den-

sities estimated from the training data. when the

training samples are non-uniformly distributed, it is

entirely possible for the estimated id203 densi-

ties to be non-zero in a small region around a point

even when there are no training samples speci   cally

in that region. (after you have created a statisti-

cal model for, say, the height distribution of people

in a community, the model may return a non-zero

id203 for the height values in a small

inter-

val even if the community does not include a single
individual whose height falls in that interval.)]

    that a decision-tree node can exist even
where there are no training samples in that
portion of the feature space that belongs
to the node is an important indicator of
the generalization abilities of a decision-
tree-based classi   er.

77

the decision tree tutorial

by avi kak

    in light of the explanation provided above,
before the dtintrospection class supplies
any answers at all, it asks you to accept
the fact that features can take on non-
zero probabilities at a point in the feature
space even though there are zero training
samples at that point (or in a small re-
gion around that point). if you do not ac-
cept this rudimentary fact, the introspec-
tion class will not yield any answers (since
you are not going to believe the answers
anyway).

    the point made above implies that the
path leading to a node in the decision tree
may test a feature for a certain value or
threshold despite the fact that the portion
of the feature space assigned to that node
is devoid of any training data.

78

the decision tree tutorial

by avi kak

    see the following three scripts in the examples

directory of version 2.3.2 or higher of the
python module for how to carry out dt
introspection:

introspection_in_a_loop_interactive.py
introspection_show_training_samples_at_all_nodes_direct_influence.py
introspection_show_training_samples_to_nodes_influence_propagation.py

and the following three scripts in the examples
directory of version 2.31 or higher of the
perl module

introspection_in_a_loop_interactive.pl
introspection_show_training_samples_at_all_nodes_direct_influence.pl
introspection_show_training_samples_to_nodes_influence_propagation.pl

    in both cases, the    rst script places you
in an interactive session in which you will
   rst be asked for the node number you are
interested in.

79

the decision tree tutorial

by avi kak

    subsequently, you will be asked for whether
or not you are interested in speci   c ques-
tions that the introspection can provide an-
swers for.

    the second of the three scripts listed on
the previous slide descends down the deci-
sion tree and shows for each node the train-
ing samples that fall directly in the portion
of the feature space assigned to that node.

    the last of the three script listed on the
previous slide shows for each training sam-
ple how it a   ects the decision-tree nodes
either directly or indirectly through the gen-
eralization achieved by the probabilistic mod-
eling of the data.

80

the decision tree tutorial

by avi kak

    the output of the script introspection show

training samples at all nodes direct influence looks
like:

node 0: the samples are: none
node 1: the samples are: [   sample_46   ,    sample_58   ]
node 2: the samples are: [   sample_1   ,    sample_4   ,    sample_7   , .....]
node 3: the samples are: []
node 4: the samples are: []
...
...

    the nodes for which no samples are listed
come into existence through the general-
ization achieved by the probabilistic mod-
eling of the data.

    the output produced by the script introspection

show training samples to nodes influence propagation
looks like what is shown on the next slide.

81

the decision tree tutorial

by avi kak

sample_1:

nodes affected directly: [2, 5, 19, 23]
nodes affected through probabilistic generalization:

2=> [3, 4, 25]

25=> [26]

5=> [6]

6=> [7, 13]

7=> [8, 11]

8=> [9, 10]
11=> [12]
13=> [14, 18]

14=> [15, 16]
16=> [17]

19=> [20]

20=> [21, 22]

23=> [24]

sample_4:

nodes affected directly: [2, 5, 6, 7, 11]
nodes affected through probabilistic generalization:

2=> [3, 4, 25]

25=> [26]

5=> [19]

19=> [20, 23]

20=> [21, 22]
23=> [24]

6=> [13]

13=> [14, 18]

14=> [15, 16]
16=> [17]

7=> [8]

8=> [9, 10]

11=> [12]

82

...
...
...

the decision tree tutorial

by avi kak

    for each training sample, the display on
the previous slide    rst presents the list of
nodes that are directly a   ected by the sam-
ple. a node is a   ected directly by a sam-
ple if the latter falls in the portion of the
feature space that belongs to the former.
subsequently, for each training sample, the
display shows a subtree of the nodes that
are a   ected indirectly by the sample through
the generalization achieved by the proba-
bilistic modeling of the data.
in general,
a node is a   ected indirectly by a sample if
it is a descendant of another node that is
a   ected directly.

    in the on-line documentation associated with

the perl and the python modules, the sec-
tion titled    the introspection api    lists
the methods you can invoke in your own
code for carrying out dt introspection.

83

the decision tree tutorial

by avi kak

14.

incorporating id112

    starting with version 3.0 of the python
decisiontree module and version 3.0 of the
perl version of the same you can now carry
out decision-tree based classi   cation with
id112.

    id112 means randomly extracting smaller
datasets (we refer to them as bags of data)
from the main training dataset and con-
structing a separate decision tree for each
bag. subsequently, given a test sample,
you can classify it with each decision tree
and base your    nal classi   cation on, say,
the majority vote from all the id90.

84

the decision tree tutorial

by avi kak

    (1) if your original training dataset is su   -
ciently large; (2) you have done a good job
of catching in it all of the signi   cant sta-
tistical variations for the di   erent classes,
and (3) assuming that no single feature is
too dominant with regard to inter-class dis-
criminations, id112 has the potential to
reduce classi   cation noise and bias.

    in both the python and the perl versions of
the decisiontree module, id112 is imple-
mented through the decisiontreewithid112
class.

    when you construct an instance of this

class, you specify the number of bags through
the constructor parameter how many bags and
the extent of overlap in the data in the bags
through the parameter bag overlap fraction,
as shown on the next slide.

85

the decision tree tutorial

by avi kak

    here is an example of how you   d call decision

treewithid112 class   s constructor in python:

import decisiontreewithid112
dtbag = decisiontreewithid112.decisiontreewithid112(

training_datafile = training_datafile,
csv_class_column_index = 2,
csv_columns_for_features = [3,4,5,6,7,8],
id178_threshold = 0.01,
max_depth_desired = 8,
symbolic_to_numeric_cardinality_threshold=10,
how_many_bags = 4,
bag_overlap_fraction = 0.20,
csv_cleanup_needed = 1,

)

    and here is how you would do it in perl:

use algorithm::decisiontreewithid112;
my $training_datafile = "stage3cancer.csv";
my $dtbag = algorithm::decisiontreewithid112->new(

training_datafile => $training_datafile,
csv_class_column_index => 2,
csv_columns_for_features => [3,4,5,6,7,8],
id178_threshold => 0.01,
max_depth_desired => 8,
symbolic_to_numeric_cardinality_threshold=>10,
how_many_bags => 4,
bag_overlap_fraction => 0.2,
csv_cleanup_needed => 1,

);

86

the decision tree tutorial

by avi kak

    as mentioned previously, the constructor

parameters how many bags and bag overlap fraction
determine how id112 is carried vis-a-vis
your training dataset.

    as implied by the name of the parameter,
the number of bags is set by how many bags.
initially, the entire training dataset is ran-
domized and divided into how many bags non-
overlapping partitions. subsequently, we
expand each such partition by a fraction
equal to bag overlap fraction by drawing
samples randomly from the other bags. for
example, if how many bags is set to 4 and
bag overlap fraction set to 0.2, we    rst di-
vide the training dataset (after it is ran-
domized) into 4 non-overlapping partitions
and then add additional samples drawn from
the other partitions to each partition.

87

the decision tree tutorial

by avi kak

    to illustrate, let   s say that the initial non-
overlapping partitioning of the training data
yields 100 training samples in each bag.
with bag overlap fraction set to 0.2, we
next add to each bag 20 additional train-
ing samples that are drawn randomly from
the other three bags.

    after you have constructed an instance of
the decisiontreewithid112 class, you can
call the following methods of this class for
the id112 based decision-tree classi   ca-
tion:
get training data for id112(): this method reads

your training data   le, randomizes it, and then
partitions it into the speci   ed number of bags.
subsequently, if the constructor parameter bag
overlap fraction is some positive fraction, it adds
to each bag a number of additional samples
drawn at random from the other bags. as to
how many additional samples are added to each
bag, suppose the parameter bag overlap fraction
is set to 0.2, the size of each bag will grow by
20% with the samples drawn from the other
bags.

88

the decision tree tutorial

by avi kak

show training data in bags(): shows for each bag

the name-tags of the training data samples in
that bag.

calculate    rst order probabilities(): calls on the
appropriate methods of the main decisiontree
class to estimate the    rst-order probabilities from
the samples in each bag.

calculate class priors(): calls on the appropriate
method of the main decisiontree class to esti-
mate the class priors for the data classes found
in each bag.

construct id90 for bags(): calls on the
appropriate method of the main decisiontree
class to construct a decision tree from the train-
ing data in each bag.

display id90 for bags(): display separately

the decision tree for each bag.

classify with id112( test sample ): calls on the

appropriate methods of the main decisiontree
class to classify the argument test sample.

display classi   cation results for each bag(): displays

separately the classi   cation decision made by
each the decision tree constructed for each bag.

89

the decision tree tutorial

by avi kak

get majority vote classi   cation(): using major-
ity voting, this method aggregates the classi   -
cation decisions made by the individual decision
trees into a single decision.

see the example scripts in the directory exam-
plesid112 for how to call these methods for
classifying individual samples and for bulk clas-
si   cation when you place all your test samples
in a single    le.

    the examplesid112 subdirectory in the main

installation directory of the modules con-
tains the following scripts that illustrate
how you can incorporate id112 in your
decision tree based classi   cation:

id112_for_classifying_one_test_sample.py

id112_for_bulk_classification.py

the same subdirectory in the perl version
of the module contains the following scripts:

id112_for_classifying_one_test_sample.pl

id112_for_bulk_classification.pl

90

the decision tree tutorial

by avi kak

    as the name of the script implies, the    rst
perl or python script named on the pre-
vious slide shows how to call the di   erent
methods of the decisiontreewithid112 class
for classifying a single test sample.

    when you are classifying a single test sam-
ple, as in the    rst of the two scripts named
on the previous slide, you can also see how
each bag is classifying the test sample. you
can, for example, display the training data
used in each bag, the decision tree con-
structed for each bag, etc.

    the second script named on the previous
slide is for the case when you place all
of the test samples in a single    le. the
demonstration script displays for each test
sample a single aggregate classi   cation de-
cision that is obtained through majority vot-
ing by all the id90.

91

the decision tree tutorial

by avi kak

15.

incorporating boosting

    starting with version 3.2.0 of the python
decisiontree module and version 3.20 of
the perl version of the same, you can now
use boosting for decision-tree based classi-
   cation.

    in both cases, the module includes a new
class called boosteddecisiontree that makes
it easy to incorporate boosting in a decision-
tree classi   er.
[note: boosting does not
always result in superior classi   cation per-
formance. ordinarily, the theoretical guar-
antees provided by boosting apply only to
the case of binary classi   cation. addition-
ally, your training dataset must capture all
of the signi   cant statistical variations in
the classes represented therein.]

92

the decision tree tutorial

by avi kak

    if you are not familiar with boosting, you
may want to    rst browse through my tu-
torial    adaboost for learning binary and
multiclass discriminations    that is avail-
able at:

https://engineering.purdue.edu/kak/tutorials/adaboost.pdf

boosting for designing classi   ers owes its
origins to the now celebrated paper    a decision-
theoretic generalization of on-line learn-
ing and an application to boosting    by
yoav freund and robert schapire that ap-
peared in 1995 in the proceedings of the
2nd european conf.
on computational
learning theory.

    a boosted decision-tree classi   er consists
of a cascade of id90 in which each
decision tree is constructed with samples
that are mostly those that are misclassi   ed
by the previous decision tree.

93

the decision tree tutorial

by avi kak

    you specify a id203 distribution over
the training dataset for selecting samples
for training each decision tree in the cas-
cade. at the beginning, the distribution is
uniform over all of the samples.

    subsequently, this id203 distribution
changes according to the misclassi   cations
by each tree in the cascade:
if a sample
is misclassi   ed by a given tree in the cas-
cade, the id203 of its being selected
for training the next tree increases signi   -
cantly.

    you also associate a trust factor with each
decision tree depending on its power to
classify correctly all of the training data
samples.

94

the decision tree tutorial

by avi kak

    after a cascade of id90 is con-
structed in this manner, you construct a
   nal classi   er that calculates the class label
for a test data sample by taking into ac-
count the classi   cation decisions made by
each individual tree in the cascade, the de-
cisions being weighted by the trust factors
associated with the individual classi   ers.

    here is an example of how you   d call the
constructor of the boosteddecisiontree class
in python:

import boosteddecisiontree
training_datafile = "training6.csv"

boosted = boosteddecisiontree.boosteddecisiontree(

training_datafile = training_datafile,
csv_class_column_index = 1,
csv_columns_for_features = [2,3],
id178_threshold = 0.01,
max_depth_desired = 8,
symbolic_to_numeric_cardinality_threshold = 10,
how_many_stages = 10,
csv_cleanup_needed = 1,

)

95

the decision tree tutorial

by avi kak

    and here is an example of how you   d call
the constructor of the boosteddecisiontree
class in perl:

use algorithm::boosteddecisiontree;
my $training_datafile = "training6.csv";
my $boosted = algorithm::boosteddecisiontree->new(
training_datafile => $training_datafile,
csv_class_column_index => 1,
csv_columns_for_features => [2,3],
id178_threshold => 0.01,
max_depth_desired => 8,
symbolic_to_numeric_cardinality_threshold=>10,
how_many_stages => 4,
csv_cleanup_needed => 1,

);

    in both constructor calls shown above, note
the parameter how many stages. this pa-
rameter controls how many stages will be
used in the boosted decision tree classi   er.
as mentioned earlier, a separate decision
tree is constructed for each stage of boost-
ing using a set of training samples drawn
randomly through a id203 distribution
maintained over the entire training dataset.

96

the decision tree tutorial

by avi kak

    after you have constructed an instance of
the boosteddecisiontree class, you can call
the following methods of this class for con-
structing the full cascade of id90
and for boosted decision-tree classi   cation
of your test data:

    get training data for base tree(): in this method
name, the string base tree refers to the    rst
tree of the cascade. this is the tree for which
the training samples are drawn assuming a uni-
form distribution over the entire dataset. this
method reads your training data   le, creates the
data structures from the data ingested for con-
structing the base decision tree.

show training data for base tree(): shows the train-

ing data samples and some relevant properties
of the features used in the training dataset.

calculate    rst order probabilities and class priors():

this method calls on the appropriate methods
of the main decisiontree class to estimate the
   rst-order probabilities and the class priors.

97

the decision tree tutorial

by avi kak

construct base decision tree(): this method calls

on the appropriate method of the main decisiontree
class to construct the base decision tree.

display base decision tree(): as you would guess,

this method displays the base decision tree.

construct cascade of trees(): uses the adaboost

algorithm (described in the adaboost tutorial
mentioned at the beginning of this section) to
construct a cascade of id90. as men-
tioned earlier, the training samples for each tree
in the cascade are drawn using a id203 dis-
tribution over the entire training dataset. this
id203 distribution for any given tree in the
cascade is heavily in   uenced by which training
samples are misclassi   ed by the previous tree.

display id90 for di   erent stages(): this

method displays separately the decision tree con-
structed for each stage of the cascade.

classify with boosting( test sample ): this method

calls on each decision tree in the cascade to
classify the argument test sample.

98

the decision tree tutorial

by avi kak

display classi   cation results for each stage() this

method shows you the classi   cation decisions
made by each decision tree in the cascade. the
method also prints out the trust factor associ-
ated with each decision tree. it is important to
look simultaneously at the classi   cation decision
and the trust factor for each tree     since a clas-
si   cation decision made by a speci   c tree may
appear bizarre for a given test sample. this
method is useful primarily for debugging pur-
poses.

show class labels for misclassi   ed samples in stage(stage index):

as for the previous method, this method is use-
ful mostly for debugging. it returns class labels
for the samples misclassi   ed by the stage whose
integer index is supplied as an argument to the
method. say you have 10 stages in your cas-
cade. the value of the argument stage index
would run from 0 to 9, with 0 corresponding to
the base tree.

trust weighted majority vote classi   er(): uses the

      nal classi   er    formula of the adaboost algo-
rithm to pool together the classi   cation deci-
sions made by the individual trees while taking
into account the trust factors associated with
the trees. as mentioned earlier, we associate
with each tree of the cascade a trust factor that
depends on the overall misclassi   cation rate as-
sociated with that tree.

99

the decision tree tutorial

by avi kak

    the examplesboosting subdirectory in the
main installation directory contains the fol-
lowing three scripts:

boosting_for_classifying_one_test_sample_1.py
boosting_for_classifying_one_test_sample_2.py
boosting_for_bulk_classification.py

that illustrate how you can use boosting
with the help of the boosteddecisiontree
class. the perl version of the module con-
tains the following similarly named scripts
in its examplesboosting subdirectory:

boosting_for_classifying_one_test_sample_1.pl
boosting_for_classifying_one_test_sample_2.pl
boosting_for_bulk_classification.pl

    as implied by the names of the    rst two
scripts, these show how to call the di   erent
methods of the boosteddecisiontree class

100

the decision tree tutorial

by avi kak

for classifying a single test sample. when
you are classifying a single test sample, you
can see how each stage of the cascade of
id90 is classifying the test sam-
ple. you can also view each decision tree
separately and also see the trust factor as-
sociated with the tree.

    the third script listed on the previous slide
is for the case when you place all of the
test samples in a single    le. the demon-
stration script displays for each test sam-
ple a single aggregate classi   cation deci-
sion that is obtained through trust-factor
weighted majority voting by all the decision
trees.

101

the decision tree tutorial

by avi kak

16. working with randomized

id90

    consider the following two situations that
call for using randomized id90,
meaning multiple id90 that are
trained using data extracted randomly from
a large database of training samples:

    consider a two-class problem for which
the training database is grossly imbal-
anced in how many majority-class sam-
ples it contains vis-a-vis the number of
minority class samples. let   s assume
for a moment that the ratio of majority
class samples to minority class samples
is 1000 to 1. let   s also assume that you
have a test dataset that is drawn ran-
domly from the same population mix-
ture from which the training database

102

the decision tree tutorial

by avi kak

was created. now consider a stupid
data classi   cation program that classi-
   es everything as belonging to the ma-
jority class.
if you measure the clas-
si   cation accuracy rate as the ratio of
the number of samples correctly classi-
   ed to the total number of test samples
selected randomly from the population,
this classi   er would work with an accu-
racy of 99.99%.

    let   s now consider another situation in
which we are faced with a huge train-
ing database but in which every class is
equally well represented. feeding all the
data into a single decision tree would
be akin to polling all of the population
of the united states for measuring the
coke-versus-pepsi preference in the coun-
try. you are likely to get better results
if you construct multiple id90,

103

the decision tree tutorial

by avi kak

each trained with a collection of training
samples drawn randomly from the train-
ing database. after you have created all
the id90, your    nal classi   ca-
tion decision could then be based on,
say, majority voting by the trees.

    both the data classi   cation scenarios men-

tioned above can be tackled with ease through
the programming interface provided by the
new randomizedtreesforbigdata class that
comes starting with version 3.3.0 of the
python version and version 3.42 of the perl
version of the decisiontree module.

    if you want to use randomizedtreesforbigdata

for classifying data that is overwhelmingly
dominated by one class, you would call the
constructor of this class in the following
fashion for the python version of the mod-
ule:

104

the decision tree tutorial

by avi kak

import randomizedtreesforbigdata
training_datafile = "mylargedatabase.csv"
rt = randomizedtreesforbigdata.randomizedtreesforbigdata(

training_datafile = training_datafile,
csv_class_column_index = 48,
csv_columns_for_features = [39,40,41,42],
id178_threshold = 0.01,
max_depth_desired = 8,
symbolic_to_numeric_cardinality_threshold = 10,
looking_for_needles_in_haystack = 1,
how_many_trees = 5,
csv_cleanup_needed = 1,

)

except for obvious changes, the syntax
is very similar for the perl case also.

    note in particular the constructor parame-

ters:

looking_for_needles_in_haystack

how_many_trees

the parameter looking for needles in haystack
invokes the logic for constructing an en-
semble of id90, each based on a

105

the decision tree tutorial

by avi kak

training dataset that uses all of the mi-
nority class samples, and a random draw-
ing from the majority class samples. the
second parameter, how many trees, tells the
system how many trees it should construct.

    with regard to the second data classi   ca-
tion scenario presented at the beginning of
this section, shown at the top of the next
slide is how you would invoke the construc-
tor of the randomizedtreesforbigdata class
for constructing an ensemble of decision
trees, with each tree trained with randomly
drawn samples from a large database of
training data (with no consideration given
to any population imbalances between the
di   erent classes):

106

the decision tree tutorial

by avi kak

import randomizedtreesforbigdata
training_datafile = "mylargedatabase.csv"
rt = randomizedtreesforbigdata.randomizedtreesforbigdata(

training_datafile = training_datafile,
csv_class_column_index = 48,
csv_columns_for_features = [39,40,41,42],
id178_threshold = 0.01,
max_depth_desired = 8,
symbolic_to_numeric_cardinality_threshold = 10,
how_many_training_samples_per_tree = 50,
how_many_trees = 17,
csv_cleanup_needed = 1,

)

again, except for obvious changes, the
syntax is very similar for the perl ver-
sion of the module.

    note in particular the constructor parame-

ters in this case:

how_many_training_samples_per_tree
how_many_trees

the    rst parameter will set the number of
samples that will be drawn randomly from
the training database and the second the

107

the decision tree tutorial

by avi kak

number of id90 that will be con-
structed.
important: when you set
the how many training samples per tree pa-
rameter, you are not allowed to also set the
looking for needles in haystack parameter,
and vice versa.

    after you have constructed an instance of

the randomizedtreesforbigdata class, you can
call the following methods of this class for
constructing an ensemble of id90
and for data classi   cation with the ensem-
ble:

get training data for n trees(): what this method

does depends on which of the two constructor
parameters, looking for needles in haystack or
how many training samples per tree, is set. when
the former is set, it creates a collection of train-
ing datasets for how many trees number of deci-
sion trees, with each dataset being a mixture of
the minority class and sample drawn randomly

108

the decision tree tutorial

by avi kak

from the majority class. however, when
the latter option is set, all the datasets
are drawn randomly from the training
database with no particular attention given
to the relative populations of the two
classes.

show training data for all trees(): as the

name implies, this method shows the
training data being used for all the deci-
sion trees. this method is useful for de-
bugging purposes using small datasets.

calculate class priors(): calls on the ap-

propriate method of the main decisiontree
class to estimate the class priors for the
training dataset to be used for each de-
cision tree.

construct all id90(): calls on the

appropriate method of the main decisiontree
class to construct the id90.

109

the decision tree tutorial

by avi kak

display all id90(): displays all the

id90 in your terminal window.
(the textual form of the id90
is written out to the standard output.)

classify with all trees(): a test sample is
sent to each decision tree for classi   ca-
tion.

display classi   cation results for all trees():

the classi   cation decisions returned by
the individual id90 are written
out to the standard output.

get majority vote classi   cation(): this method

aggregates the classi   cation results re-
turned by the individual id90
and returns the majority decision.

110

the decision tree tutorial

by avi kak

    the examplesrandomizedtrees subdirectory
in the main installation directory of the
module shows example scripts that you can
use to become more familiar with the
randomizedtreesforbigdata class for solving
needle-in-a-haystack and big-data data clas-
si   cation problems. these scripts are:

randomized_trees_for_classifying_one_test_sample_1.py

randomized_trees_for_classifying_one_test_sample_2.py

classify_database_records.py

    the    rst of the scripts listed above shows
the constructor options to use for solving
a needle-in-a-haystack problem     that is,
a problem in which a vast majority of the
training data belongs to just one class.

111

the decision tree tutorial

by avi kak

    the second script shows the constructor
options for using randomized id90
for the case when you have access to a
very large database of training samples and
you   d like to construct an ensemble of de-
cision trees using training samples pulled
randomly from the training database.

    the third script listed on the previous page
illustrates how you can evaluate the classi-
   cation power of an ensemble of decision
trees as constructed by randomizedtreesforbigdata
by classifying a large number of test sam-
ples extracted randomly from the training
database.

112

the decision tree tutorial

by avi kak

17. speeding up decision tree based

classi   cation with hash tables

    once you have constructed a decision tree
for classi   cation, it can be converted into
a hash table for fast classi   cation.

    in this section, we will assume that we only
have numeric features. as you now know,
each interior node for such a decision tree
has only two children, unless the node is a
leaf node, in which case it has no children.

    for the purpose of explanation and pic-
torial depiction, let   s assume that we are
dealing with the case of just two numeric
features. we will denote these features by
f1 and f2.

113

the decision tree tutorial

by avi kak

    with just the two features f1 and f2, let   s
say that our decision tree looks like what
is shown in the    gure below:

feature tested at root

1

f 1

v( f
1

)

<= 

  
1

2

feature tested

f 2

v( f )
1

>

  
1

5

feature tested

f2

v( f ) <= 

2

  
2

v( f ) >

2

  
2

3

4

feature tested

f 1

feature tested

f 1

    the numbers in red circles in the deci-
sion tree shown above indicate the order
in which the nodes were visited.

114

the decision tree tutorial

by avi kak

    note that when we create two child nodes
at any node in the tree, we are dividing up
a portion of the underlying feature space,
the portion that can be considered to be
allocated to the node in question.

1

5

umax

f   values
2

4

2

3

umin

vmin

vmax

f   values
1

115

the decision tree tutorial

by avi kak

    each node being in charge of a portion
of the feature space and how it gets par-
titioned when we create two child nodes
there is illustrated by the    gure on the
previous page.
in this    gure, the circled
numbers next to the partitioning lines cor-
respond to the numbers attached to the
nodes in the decision tree on page 65.

    it is good for mental imagery to associate
the entropies we talked about earlier with
the di   erent portions of the feature space.
for example, the id178 h(c | f1<) ob-
viously corresponds to the portion of the
feature space to the left of the vertical di-
viding line that has the number 1 in the    g-
ure. similarly, the id178 h(c | f1<, f2<)
corresponds to the portion that is to the
left of the vertical dividing line numbered 1
and below the horizontal dividing line num-
bered 2.

116

the decision tree tutorial

by avi kak

    as we grow the decision tree, our goal must
be to reach the nodes that are pure or until
there is no further reduction in the id178
in the sense we talked about earlier. a
node is pure if it has zero id178. obvi-
ously, the classi   cation made at that node
will be unambiguous.

    after we have    nished growing up the tree,
we are ready to convert it into a hash table.

    we    rst create a su   ciently    ne quantiza-
tion of the underlying feature space so that
the partitions created by the decision tree
are to the maximum extent feasible on the
quantization boundaries.

    we are allowed to use di   erent quantiza-
tion intervals along the di   erent features
to ensure the ful   llment of this condition.

117

the decision tree tutorial

by avi kak

    the resulting divisions in the feature space
will look like what is shown in the    gure on
the next slide.

1

5

umax

f   values
2

4

2

3

umin

vmin

f   values
1

vmax

    the tabular structure shown above can now
be linearized into a 1-d array of cells, with
each cell pointing to the unique class label
that corresponds to that point in the fea-
ture space (assuming that portion of the
feature space is owned by a pure node).

118

the decision tree tutorial

by avi kak

    however, should it be the case that the
portion of the feature space from which
the cell is drawn is impure, the cell in our
linearized structure can point to all of the
applicable class labels and the associated
probabilities.

    the resulting one-dimensional array of cells
lends itself straightforwardly to being stored
as an associative list in the form of a hash
table. for example, if you are using perl,
you would use the built-in hash data struc-
ture for creating such a hash table. you
can do the same in python with a dictio-
nary.

119

the decision tree tutorial

by avi kak

18. constructing regression trees

    so far we have focused exclusively on de-
cision trees. as you should know by this
time, decision tree based modeling requires
that the class labels be distinct. that is,
the training dataset must contain a rela-
tively small number of discrete class labels
for all of your data records if you want to
model the data with one or more decision
trees.

    however, when one is trying to understand
all of the associational relationships that
exist in a large database, one often runs
into situations where, instead of discrete
class labels, you have a continuously val-
ued variable as a dependent variable whose
values are predicated on a set of feature
values.

120

the decision tree tutorial

by avi kak

    it is for such situations that you will    nd
useful the new class regressiontree that is
now a part of the decisiontree module. if in-
terested in regression, look for the regressiontree
class in version 3.4.3 of the python module
and in version 3.43 of the perl module.

    for both the perl and the python cases,
the regressiontree class has been programmed
as a subclass of the main decisiontree class.
the regressiontree calls on the decisiontree
class for several record keeping and some
key low-level data processing steps.

    you can think of regression with a re-
gression tree as a powerful generaliza-
tion of the very commonly used linear
regression algorithms.

121

the decision tree tutorial

by avi kak

    although you can certainly carry out poly-
nomial regression with run-of-the-mill lin-
ear regression algorithms for modeling non-
linearities between the predictor variables
and the dependent variable, specifying the
degree of a polynomial is often tricky. ad-
ditionally, a polynomial can inject continuities be-

tween the predictor and the predicted variables that

may not actually exist in the real data.

    regression trees, on the other hand, give
you a piecewise linear relationship between
the predictor and the predicted variables
that is freed from the constraints of super-
imposed continuities at the joins between
the di   erent segments.

    see the following tutorial for further in-
formation regarding the standard linear re-
gression approach and the regression that
can be achieved with the regressiontree class:

https://engineering.purdue.edu/kak/tutorials/regressiontree.pdf

122

the decision tree tutorial

by avi kak

    while id75 has su   ced for many
applications, there are many others where
it fails to perform adequately. just to il-
lustrate this point with a simple example,
shown below is some noisy data for which
the id75 yields the line shown
in red. the blue line is the output of the
tree regression algorithm as implemented
in the regressiontree class:

123

the decision tree tutorial

by avi kak

    you will    nd the regressiontree class easy to
use in your own scripts. see my regression
tree tutorial at:

https://engineering.purdue.edu/kak/tutorials/regressiontree.pdf

for how to call the constructor of this class
and how to invoke the functionality incor-
porated in it.

    you will also    nd example scripts in the
examplesregression subdirectories of the main
installation directory that you can use to
become more familiar with tree regression.

124

the decision tree tutorial

by avi kak

19. historical antecedents of decision

tree classi   cation in purdue rvl

    during her ph.d dissertation in the robot
vision lab at purdue, lynne grewe cre-
ated a full-blown implementation of a decision-
tree/hashtable based classi   er for recog-
nizing 3d objects in a robotic workcell. it
was a pretty amazing dissertation. she
not only implemented the underlying the-
ory, but also put together a sensor suite for
collecting the data so that she could give
actual demonstrations on a working robot.

    the learning phase in lynne   s demonstra-
tions consisted of merely showing 3d ob-
jects to the sensor suite. for each object
shown, the human would tell the computer
what its identity and pose was.

125

the decision tree tutorial

by avi kak

    from the human supplied class labels and
pose information, the computer constructed
a decision tree in the manner described in
the previous sections of this tutorial. sub-
sequently, the decision tree was converted
into a hash table for fast classi   cation.

    the testing phase consisted of the robot
using the hash table constructed during the
learning phase to recognize the objects and
to estimate their poses. the fact that
the robot successfully manipulated the ob-
jects established for us the viability of using
decision-tree based learning in the context
of robot vision.

    the details of this system are published in

lynne grewe and avinash kak, "interactive learning
of a multi-attribute hash table classifier for
fast object recognition," id161 and
image understanding, pp. 387-416, vol. 61,
no. 3, 1995.

126

the decision tree tutorial

by avi kak

20. acknowledgments

in one form or another, id90 have been around
for over    fty years. from a statistical perspective, they
are closely related to classi   cation and regression by
recursive partitioning of multidimensional data. early
work that demonstrated the usefulness of such parti-
tioning of data for classi   cation and regression can be
traced, in the statistics community, to the work done by
terry therneau in the early 1980   s, and, in the machine
learning community, to the work of ross quinlan in the
mid 1990   s.

i have enjoyed several animated conversations with josh
zapf and padmini jaikumar on the topic of decision
tree induction. as a matter of fact, this tutorial was
prompted by conversations with josh regarding lynne
grewe   s implementation of decision-tree induction for
id161 applications. (as i mentioned in sec-
tion 19, lynne grewe was a former ph.d. student of
mine in purdue robot vision lab.) we are still in some
disagreement regarding the computation of average en-
tropies at the nodes of a decision tree. but then life
would be very dull if people always agreed with one an-
other all the time.

127

