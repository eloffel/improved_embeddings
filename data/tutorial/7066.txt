deep generative models

ulrich paquet

deepmind

version: 7 september 2017

2-minute exercise 

talk to your friend next to you, and tell him or her everything you can about this 
data set:

data 

2

3

4

2

1

4

2

2

1

3

data manifold

we can capture most of the variability in the data through one number

for each image n, even though each image is 16 dimensional

how?

how?

1. take 
2. draw bar in column 2 of image
3. et voila! you have 

some bar-drawing process

how?

1. take 
2. draw bar in column 2 of image
3. et voila! you have 

maybe some neural 

network, that takes z as 

input, and outputs a 

16-dimensional vector x   ?

3-minute exercise

write or draw a function (like a multi-layer 
id88) that takes               and 
produces

is your input one-dimensional?

is your output 16-dimensional?

identify all the    tunable    parameters     of 
your function

3-minute exercise

scratch space

write or draw a function (like a multi-layer 
id88) that takes               and 
produces

is your input one-dimensional?

is your output 16-dimensional?

identify all the    tunable    parameters     of 
your function

data manifold

the 16-dimensional images live on a 1-dimensional manifold, plus some    noise   

   -0.1   

   1   

   0   

   2   

   0.2   

   3   

   0.3   

   4   

...and noise 

the 16-dimensional images live on a 1-dimensional manifold, plus some    noise   

   -0.1   

   1   

   0   

   2   

   0.2   

   3   

   0.3   

   4   

3-minute exercise

change your multi-layer id88 to 
take     and produce a distribution over 

3-minute exercise

scratch space

change your multi-layer id88 to 
take     and produce a distribution over 

decoder

def generative_network (z, ...):

...
return bernoulli_logits    # for binary pixels

id136

?

inversing our world

two big problems to solve:

id136

you wrote down                       and can compute it.

say i give you      . keeping     fixed, what was     ? or                  ?

learning

is there a better (best)     to generate the observed      from     ?

id136

you wrote down                       and can compute it.

say i give you      . keeping     fixed, what was     ? or                  ?

1      2      3      4      5    ...

-0.1     0.0      0.1     0.2      0.3    ...

100001   100002   100003 ...

id136

you wrote down                       and can compute it.

say i give you      . keeping     fixed, what was     ? or                  ?

1      2      3      4      5    ...

-0.1     0.0      0.1     0.2      0.3    ...

100001   100002   100003 ...

to really answer that question, we need some notion of where we might 
have started! no id136 without prior assumptions :)

prior assumptions

area = 1

   unobserved random variables   

   observed random variables   

id136

i give you      . keeping     fixed, what was     ?

id136

i give you      . keeping     fixed, what was     ?

3-minute exercise

assuming the largest value of                is 1, 
draw

as a function of      on the same axis as above

joint density (with x observed)

assuming the largest value of                is 1, 
draw

as a function of      on the same axis as above

joint density (with x observed)

area = 1

area = ?

1-minute exercise: 
what is the area?

marginal likelihood (evidence)

area = 1

area = ?

relationship to posterior

area = 1

area = 1

dividing by the marginal 
likelihood (evidence) scales the 
area back to 1...

evidence, for all data points

area for data 

point n

evidence, for all data points

area for data 

point n

evidence, for all data points

the product of the areas 

underneath the green curves

maximizing the evidence

the product of the areas 

underneath the green curves

by changing       we can make the 
evidence for these data points bigger...

these        s don   t generate images like the ones in the data set   

(with this    , the prior doesn   t capture the data manifold well) 

maximizing the evidence

the product of the areas 

underneath the green curves

that   s better   !

for the sharp-sighted

the product of the areas 

underneath the green curves

roughly...

20%

40% 20% 20%

learning

we want to maximize

area for data 

point n

except that we cannot write down an analytically tractable expression for the area.

strategies: stochastic (monte carlo samples + gradients) or deterministic (approximate id136). we   ll follow the    deterministic    path next...

approximate id136

we want to use this quantity for    learning   , but cannot compute it in an analytically 
tractable way:

   variational lower bound   

unnormalized

strategy: we choose some 
other                   so that we can 
compute the area underneath 
the blue curve (e.g. gaussian)

unnormalized
we cannot (tractably) compute 
the area underneath the green 
curve

can   t compute

can compute

encoder

def id136_network (x, latent_dim=1):

...
return mu, sigma

encoder decoder

sample

strategy

change      to inflate the area under the blue curve. we can do that!

change    to change the green curve, so that we can inflate the area under the 
blue curve even more

...and so, hopefully, the area under the green curve also gets bigger

can   t compute

can compute

whhaaaatttt?

strategy

change      to inflate the area under the blue curve. we can do that!

change    to change the green curve, so that we can inflate the area under the 
blue curve even more

...and so, hopefully, the area under the green curve also gets bigger

3-minute exercise

create and draw                                                                     as a function.

it could be a multi-layer id88 (mlp) that takes 16-dimensional      , and 
produces two 1-dimensional quantities,

scratch space

what are your parameters     ?

objective function discussion 
maximize (for all data points)...

kl     0

evidence lower bound (elbo) for one data point

elbo

evidence lower bound (elbo) for one data point

can   t compute

evidence lower bound (elbo) for one data point

concave function

expectation

concave function

3-minute exercise

jensen   s inequality

draw log(...) as a function, and convince yourself that

is always true for any (nonnegative) setting of z1 and z2.

logarithm (concave)

evidence lower bound (elbo) for one data point

kullback-leibler 

divergence between 

two gaussian 

distributions (here). 

can compute in closed 

form

reconstruction

expected log likelihood.

cannot compute in 
closed form, and will 
have to get a monte 

carlo estimate

(with sgd)

elbo

expected log likelihood

we can estimate the expected log likelihood with a monte carlo estimate:

draw l samples                                                               ...

expected log likelihood

we can estimate the expected log likelihood with a monte carlo estimate:

draw l samples                                                               and use them to estimate 
the average:

expected log likelihood

we can estimate the expected log likelihood with a monte carlo estimate:

draw l samples                                                               and use them to estimate 
the average:

using samples in this way removes      from part of the objective function, and 
even though we can evaluate it, we can   t take derivatives / get the gradients!

naive sampling

forward computation

derivatives

expected log likelihood: reparameterization trick

we can estimate the expected log likelihood with a monte carlo estimate:

draw l samples                                         and transform them!

1

0

expected log likelihood

we can estimate the expected log likelihood with a monte carlo estimate:

draw l samples                                         and use them to estimate the average:

expected log likelihood

we can estimate the expected log likelihood with a monte carlo estimate:

draw l samples                                         and use them to estimate the average:

the noise is introduced    from outside    the computation graph, and we can 
evaluate the objective function and take derivatives / get the gradients!

reparameterization trick

forward computation

derivatives

elbo for full data set

you now have all the tools to estimate the elbo for a whole data set,

take mini-batch subsamples, and use stochastic gradient ascent to maximize it.

practical

the end

evidence lower bound (elbo) for one data point

elbo: can estimate

