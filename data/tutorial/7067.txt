gated recurrent 

models

stephan gouws & richard klein

outline

part 1: intuition, id136 and training

    building intuitions: from feedforward to recurrent models
    id136 in id56s: fprop
    training in id56s: id26-through-time (bptt)

short break

outline

part 2: gated models & applications

    long short-term memory (lstms)
    id149 (grus)
    applications:

    image captioning
    sequence classification (practical 4: mnist)
    id38
    sequence-labeling (lots of nlp tasks, e.g. id52, ner,    )
    sequence-to-sequence learning (machine translation, dialogue modeling,    )

recurrent models

part 1: intuition, id136 and training

introduction

?

introduction

?

x2

introduction

?

x3

we need to be able to remember information 

from previous time steps

recurrent neural networks: intuition

long-term dependencies: why do they matter?

michel c. was born in paris, france. he is married and has three children. he received a m.s. 
in neurosciences from the university pierre & marie curie and the ecole normale sup  rieure in 1987, 
and  and  then  spent  most  of  his  career  in  switzerland,  at  the  ecole  polytechnique  de  lausanne.  he 
specialized in child and adolescent psychiatry and his first field of research was severe mood disorders 
in adolescent, topic of his phd in neurosciences (2002). his mother tongue is   ? ? ? ? ?

michel

c.

was

born

in

   

short context

hn-1

    

hn

english,
german,
russian,
french     

french

long context       problems    

types of sequence models

ffns

image 
captioning

mnist 
predictor

id195

sequence labeling 
(e.g. ner)

types of sequence models

ffns

image 
captioning

mnist 
predictor

id195

sequence labeling 
(e.g. ner)

types of sequence models

?

we   ll talk about this a little later. we   ll 
also implement this in today   s practical!

ffns

image 
captioning

mnist 
predictor

id195

sequence labeling 
(e.g. ner)

types of sequence models

ffns

image 
captioning

mnist 
predictor

id195

sequence labeling 
(e.g. ner)

types of sequence models

ffns

image 
captioning

mnist 
predictor

id195

sequence labeling 
(e.g. ner)

types of sequence models

ffns

image 
captioning

mnist 
predictor

id195

sequence labeling 
(e.g. ner)

ffns vs id56s
classify following examples:

,

,

,    

ffns vs id56s

h

ffn

x

predict: 1

y: outputs

softmax

tanh

x: inputs

,

,

,    

y
ffns vs id56s

h

ffn

x

predict: 9

y: outputs

softmax

tanh

x: inputs

,

,

,    

y
ffns vs id56s

h

ffn

x

predict: 2

y: outputs

softmax

tanh

x: inputs

,

,

,    

y
ffns vs id56s
but what if these were not digits, but longer numbers?

,

,

problem? variable length inputs.

ffns vs id56s

id56 cell

h

xt

y: outputs

     1?

softmax

tanh

x: inputs at step t

,

,

,    

y
t
ffns vs id56s

id56 cell

h

xt

h: internal
  state

y: outputs

     19?

softmax

tanh

        1   

h at t-1
,

,

x: inputs at step t

,    

y
t
ffns vs id56s

maintains a state (memory) that carries information between inputs!

h: internal
  state

y: outputs

    192   ?

softmax

tanh

id56 cell

h

xt

     19   
h at t-1
,

,

x: inputs at step t

,    

y
t
the id56 api

prev_state

next_state

recurrent_fn()

x

outputs

the id56 computation graph

yt

xt

t-1

   feedback loop    / state / memory / stack

(previous time-step)

   unrolling    the id56 computation graph

y1

h1

h0

x1

unrolling the id56 computation graph

y1

h1

y2

h2

h0

x1

x2

unrolling the id56 computation graph

y1

h1

y2

h2

h0

yn

hn

...

x1

x2

x3

   unrolled    over n time-steps.

unrolling the id56 computation graph

nb: we reuse the same 
weights at every time-step!

y1

h0

h1

y2

h2

yn

hn

...

x1

the same 

x2

x3

   unrolled    over n time-steps.

unrolling the id56 computation graph

nb: we reuse the same 
weights at every time-step!

y1

y2

h0

we can therefore think of an id56 as a 
composition of identical feedforward neural 
networks (with replicated/tied weights), one 
for each moment or step in time.

h1

h2

yn

hn

...

x1

the same 

x2

x3

   unrolled    over n time-steps.

the ffn api

class feedforwardmodel():

  

  # ...

 

  def forward(self, x):

    # compute activations on the hidden layer.

    hidden_layer = self.act_fn(np.dot(self.w_xh, x) + b)

      

    # compute the (linear) output layer activations.     

    y = np.dot(self.w_hy, hidden_layer)

    

    return y

y

x

the ffn api

class feedforwardmodel():

  

  # ...

 

  def forward(self, x):

    # compute activations on the hidden layer.

    hidden_layer = self.act_fn(np.dot(self.w_xh, x) + b)

      

    # compute the (linear) output layer activations.     

    y = np.dot(self.w_hy, hidden_layer)

    

    return y

y

x

the ffn api

class feedforwardmodel():

  

  # ...

 

  def forward(self, x):

    # compute activations on the hidden layer.

    hidden_layer = self.act_fn(np.dot(self.w_xh, x) + b)

      

    # compute the (linear) output layer activations.     

    y = np.dot(self.w_hy, hidden_layer)

    

    return y

y

x

the ffn api

class feedforwardmodel():

  

  # ...

 

  def forward(self, x):

    # compute activations on the hidden layer.

    hidden_layer = self.act_fn(np.dot(self.w_xh, x) + b)

      

    # compute the (linear) output layer activations.     

    y = np.dot(self.w_hy, hidden_layer)

    

    return y

y

x

the ffn api

class feedforwardmodel():

  

  # ...

 

  def forward(self, x):

    # compute activations on the hidden layer.

    hidden_layer = self.act_fn(np.dot(self.w_xh, x) + b)

      

    # compute the (linear) output layer activations.     

    y = np.dot(self.w_hy, hidden_layer)

    

    return y

y

x

the id56 api

class recurrentmodel():

  

  # ...

  def recurrent_fn(self, x, prev_state):

    # compute the new state based on the previous state and current input.  

    new_state = self.act_fn(np.dot(self.w_xh, x) + np.dot(self.w_hh, prev_state) + b)

    # compute the output vector.

    y = np.dot(self.w_hy, new_state)

    return new_state, y

yt

xt

t-1

the id56 api

class recurrentmodel():

  

  # ...

  def recurrent_fn(self, x, prev_state):

    # compute the new state based on the previous state and current input.  

    new_state = self.act_fn(np.dot(self.w_xh, x) + np.dot(self.w_hh, prev_state) + b)

    # compute the output vector.

    y = np.dot(self.w_hy, new_state)

    return new_state, y

yt

xt

t-1

the id56 api

class recurrentmodel():

  

  # ...

new 
state

recurrent 
function

input at 
current 
time-step

previous 

state

  def recurrent_fn(self, x, prev_state):

    # compute the new state based on the previous state and current input.  

    new_state = self.act_fn(np.dot(self.w_xh, x) + np.dot(self.w_hh, prev_state))

    # compute the output vector.

    y = np.dot(self.w_hy, new_state)

    return new_state, y

yt

xt

t-1

the id56 api

class recurrentmodel():

  

  # ...

new 
state

recurrent 
function

input at 
current 
time-step

previous 

state

  def recurrent_fn(self, x, prev_state):

    # compute the new state based on the previous state and current input.  

    new_state = self.act_fn(np.dot(self.w_xh, x) + np.dot(self.w_hh, prev_state))

    # compute the output vector.

    y = np.dot(self.w_hy, new_state)

    return new_state, y

yt

xt

t-1

the id56 api

  def forward(self, data_sequence, initial_state):

    state = initial_state

    all_states, all_ys = [state], []

    cache = []

    for x, y in data_sequence:

      new_state, y_pred = recurrent_fn(x, state)

      loss += cross_id178(y_pred, y)

      

cache.append((new_state, y_pred))

      state = new_state

    return loss, cache

the id56 api

  def forward(self, data_sequence, initial_state):

    state = initial_state

    all_states, all_ys = [state], []

    cache = []

    for x, y in data_sequence:

      new_state, y_pred = recurrent_fn(x, state)

      loss += cross_id178(y_pred, y)

      

cache.append((new_state, y_pred))

      state = new_state

    return loss, cache

the id56 api

  def forward(self, data_sequence, initial_state):

    state = initial_state

    all_states, all_ys = [state], []

    cache = []

    for x, y in data_sequence:

      new_state, y_pred = recurrent_fn(x, state)

      loss += cross_id178(y_pred, y)

      

cache.append((new_state, y_pred))

      state = new_state

    return loss, cache

math: ffns v id56s

notation: wxh is a matrix that 
maps a vector x into a vector h.

y

x

math: ffns v id56s

notation: wxh is a matrix that 
maps a vector x into a vector h.

input

y

x

math: ffns v id56s

notation: wxh is a matrix that 
maps a vector x into a vector h.

activation 
function

input

y

x

math: ffns v id56s

hidden layer

activation 
function

input

notation: wxh is a matrix that 
maps a vector x into a vector h.

y

x

math: ffns v id56s

hidden layer

activation 
function

input

...

new state

recurrent 
function

input at 
current 
time-step

notation: wxh is a matrix that 
maps a vector x into a vector h.

yt

xt

t-1

math: ffns v id56s

hidden layer

activation 
function

input

new state

recurrent 
function

input at 
current 
time-step

notation: wxh is a matrix that 
maps a vector x into a vector h.

yt

xt

t-1

math: ffns v id56s

hidden layer

notation: wxh is a matrix that 
maps a vector x into a vector h.

activation 
function

input

   recurrent    

weights

new state

recurrent 
function

input at 
current 
time-step

previous 

state

yt

xt

t-1

id136 & training
    how do we make predictions using id56s?

forward propagation:    fprop   
essentially a composition of functions: a2 = f2(f1(x)).

   
   
    we    unroll    the computational graph over time-steps.

    how do we train id56s?

backward propagation:    backprop-through time   

   
    we need to consider predictions over several time-steps!
    credit assignment over time.
    we work backwards in time from the last state to the first.

training: ways to train id56s
    echo state networks: initialize wxh, whh, who, carefully, then only train  who!
    id26 through time (bptt): propagate errors backwards 

through the unrolled graph. 

    there are other options.

training: esns
    simple solution: don   t train the recurrent weights (whh & wxh)!
    initialization very important.
    super simple. however, with recent improvements in initialization etc, bptt 

does better!

[scholarpedia]

id136 & training
    how do we make predictions using id56s?

forward propagation:    fprop   
essentially a composition of functions: a2 = f2(f1(x)).

   
   
    we    unroll    the computational graph over time-steps.

    how do we train id56s?

error

propagate errors backwards through unrolled graph:    backprop-through time    (bptt).

   
    we need to consider predictions over several time-steps!
    credit assignment over time.
    we work backwards in time from the last state to the first.

training: bptt intuition

training: truncated bptt

training: truncated bptt

training: truncated bptt

unrolling the id56 computation graph

y1

h1

y2

h2

h0

yt

ht

...

x1

x2

x3

   unrolled    over n time-steps.

unrolling the id56 computation graph

y1

h1

y2

h2

h0

yt

ht

...

x1

x2

x3

   unrolled    over n time-steps.

unrolling the id56 computation graph

step 1: compute all errors.

y1

e1

y2

e2

yt

et

h0

h1

h2

...

ht

x1

x2

x3

   unrolled    over n time-steps.

unrolling the id56 computation graph

step 1: compute all errors.
step 2: pass error back for each 
time-step from n back to 1.

y1

e1

y2

e2

yt

et

h0

h1

h2

...

ht

x1

x2

x3

   unrolled    over n time-steps.

unrolling the id56 computation graph

step 1: compute all errors.
step 2: pass error back for each 
time-step from n back to 1.
step 3: update weights.

y1

e1

y2

e2

yt

et

h0

h1

h2

...

ht

x1

x2

x3

   unrolled    over n time-steps.

unrolling the id56 computation graph
yt-2

yt-1

yt

ht-2

ht-1

ht

xt-1

xt

unrolling the id56 computation graph
et
yt-2

yt-1

yt

ht-2

ht-1

ht

unrolling the id56 computation graph
et

yt

ht-2

ht-1

ht

unrolling the id56 computation graph
et

yt

ht-2

ht-1

ht

unrolling the id56 computation graph
et

yt

ht-2

ht-1

ht

unrolling the id56 computation graph
et

yt

ht-2

ht-1

ht

unrolling the id56 computation graph
et

yt

ht-2

ht-1

ht

unrolling the id56 computation graph
et

yt

ht-2

ht-1

ht

unrolling the id56 computation graph
et

yt

ht-2

ht-1

ht

where

unrolling the id56 computation graph
et
yt-2

et-1

et-2

yt-1

yt

ht-2

ht-1

ht

where

total error = e1 + e2 +     + et
total gradient = sum of all det/d     s

training: truncated bptt code

def bptt(model, x_train, y_train, initial_state):  

  # forward

  loss, caches = forward(x_train, y_train, model, initial_state)

  avg_loss /= y_train.shape[0]

  # backward

  dh_next = np.zeros((1, last_state.shape[0]))

  grads = {k: np.zeros_like(v) for k, v in model.items()}

  for t in reversed(range(len(x_train))):

    grad, dh_next = cell_fn_backward(ys[t], y_train[t], dh_next, caches[t])

    for k in grads.keys():

        grads[k] += grad[k]

  return grads, avg_loss

error

   lego block   !

training: truncated bptt code

def bptt(model, x_train, y_train, initial_state):  

  # forward

  loss, caches = forward(x_train, y_train, model, initial_state)

  avg_loss /= y_train.shape[0]

  # backward

  dh_next = np.zeros((1, last_state.shape[0]))

  grads = {k: np.zeros_like(v) for k, v in model.items()}

  for t in reversed(range(len(x_train))):

    grad, dh_next = cell_fn_backward(ys[t], y_train[t], dh_next, caches[t])

    for k in grads.keys():

        grads[k] += grad[k]

  return grads, avg_loss

error

   lego block   !

total gradient = sum of these 
lego-gradients over time!

vanilla id56 gradient flow

vanilla id56 gradient flow

vanilla id56 gradient flow

vanilla id56 gradient flow

vanilla id56 gradient flow

vanilla id56 gradient flow

part 2!

gated recurrent 

models

part ii: gated architectures & applications

recap: the id56 api

new state recurrent 
function

input at 
current 
time-step

previous 

state

prev_state

next_state

recurrent_fn()

x

outputs

the gated id56 api

recurrent_fn()

it is the same! just a different 
way of computing the outputs.

prev_state

x

memory 

cell

gates

controller

next_state

outputs

implementing a memory cell in a neural network

propagating through a memory cell

backpropagating through a memory cell?

lstm

xt

ht-1

ct-1

  

  

tanh

  

  

+

  

  
tanh

lstm = long short term memory

concatenate :

forget gate :
update gate :
result gate :
input :
new c :
new h :

ht

ct

vector sizes
p+n

n

x = xt | ht-1
f =   (x.wf + bf)
u =   (x.wu + bu)
r =   (x.wr + br)
x    = tanh(x.wc + bc)
n
ct = f * ct-1 + u * x   
n
ht = r * tanh(ct)
n

n

n

yt

output :

yt = softmax(ht.w + b)

m

lstm

xt

ht-1

  

  

tanh

  

ct-1

  

  

+

  
  

tanh neural net. layers
tanh

element-wise operations

ht

ct

  

tanh

yt

lstm

concatenation

xt

ht-1

  

  

tanh

  

ct-1

  

  

+

  
  

tanh neural net. layers
tanh

element-wise operations

ht

ct

  

tanh

yt

lstm

xt

what to forget?

ht-1

  

  

tanh

  

ct-1

  

  

+

  
  

tanh neural net. layers
tanh

element-wise operations

ht

ct

  

tanh

yt

lstm

xt

ht-1

actually forget

  

  

tanh

  

ct-1

  

  

+

  
  

tanh neural net. layers
tanh

element-wise operations

ht

ct

  

tanh

yt

lstm

xt

what to update?

what   s the new value?

ht-1

  

  

tanh

  

ct-1

  

  

+

  
  

tanh neural net. layers
tanh

element-wise operations

ht

ct

  

tanh

yt

lstm

xt

ht-1

  

  

tanh

  

actually update

ct-1

  

  

+

  
  

tanh neural net. layers
tanh

element-wise operations

ht

ct

  

tanh

yt

lstm

xt

ht-1

  

  

tanh

  

ct-1

  

  

+

  
  

tanh neural net. layers
tanh

element-wise operations

updated!

  

tanh

yt

ht

ct

lstm

xt

ht-1

  

  

tanh

  

ct-1

  

  

+

  
  

tanh neural net. layers
tanh

element-wise operations

result gate!

  

tanh

yt

ht

calculate result

ct

lstm

xt

ht-1

remember the result for next time step

  

  

tanh

  

ct-1

  

  

tanh

  

+

  
  

tanh neural net. layers
tanh

element-wise operations

ht

ct

yt

output

lstm

lstm = long short term memory

xt

concatenation

ht-1

  

  

ct-1

  

  

  
tanh

tanh
  
+

ht

ct

yt

  
  

tanh neural net. layers
tanh

element-wise operations

x = xt     ht-1
f =   (x.wf + bf)
u =   (x.wu + bu)
r =   (x.wr + br)
x    = tanh(x.wc + bc)
ct = f * ct-1 + u * x   
ht = r * tanh(ct)

yt = softmax(ht.w + b)

lstm

lstm = long short term memory

xt

concatenation

ht-1

  

  

ct-1

  

  

  
tanh

tanh
  
+

ht

ct

yt

  
  

tanh neural net. layers
tanh

element-wise operations

x = xt     ht-1
f =   (x.wf + bf)
u =   (x.wu + bu)
r =   (x.wr + br)
x    = tanh(x.wc + bc)
ct = f * ct-1 + u * x   
ht = r * tanh(ct)

yt = softmax(ht.w + b)

gru !

id149 (grus)
gru = gated
recurrent unit

2 gates instead 
of 3 => cheaper

ht-1

xt

gru
ht

yt

ht

vector sizes
p+n

x = xt | ht-1
z =   (x.wz + bz)
r =   (x.wr + br)
x    = xt | r * ht-1
x    = tanh(x   .wc + bc)
ht = (1-z) * ht-1 + z * x   

n

n

p+n

n

n

m

yt = softmax(ht.w + b)

id149 (grus)
gru = gated
recurrent unit

2 gates instead 
of 3 => cheaper

ht-1

xt

gru
ht

yt

ht

vector sizes
p+n

x = xt | ht-1
z =   (x.wz + bz)
r =   (x.wr + br)
x    = xt | r * ht-1
x    = tanh(x   .wc + bc)
ht = (1-z) * ht-1 + z * x   

n

n

p+n

n

n

m

yt = softmax(ht.w + b)

long short-term memory (lstm): gradient flow

long short-term memory (lstm): gradient flow

applications/tasks
    image captioning
    sequence classification (practical 4: mnist)
    id38
    sequence-labeling (lots of nlp tasks, e.g. id52, ner,    )
    sequence-to-sequence learning (machine translation, summarization,    )

one-to-many: image captioning

goal: given image, generate a sentence to describe its content.

one-to-many: image captioning

goal: given image, generate a sentence to describe its content.

many-to-one: sequence classifier (prac 4)

goal: given a sequence of inputs, predict the label for the whole sequence.

examples: 

    given a sentence, say if it is {negative, neutral, positive}
    given the words in an email, predict if it is a spam message.
    given    pieces    of an image, predict what number is in the image.

many-to-1: polarity/sentiment classifer

we feed all the words into the model one at a time, 
and make one prediction at the end:

pos/neg?

h0

h1

h2

h3

cats

are

awesome

many-to-1: spam classifer

we feed all the words into the model one at a time, 
and make one prediction at the end:

spam/ham?

h0

h1

h2

h3

viagra

for

cheap

many-to-1: image classifier (prac 4)

we chop up the image and feed all the pieces through 
the model, and then make one prediction at the end:

h0

h1

h2

yimag
e

h3

next-token prediction: id38

1. computing p(next word | previous words)

2.

p(the)

p(cat|the)

p(sits|the cat)

p(xm | ...)

h0

h1

h2

...

h3

the

cat

next-token prediction: id38

x1

h0

x2

h1

x3

h2

xn

hn

...

x1

x2

x3

many-to-many: sequence labeling
    mapping each input x1, x2,    , xn to its own label y1, y2,    , yn 
    (notice: same length m; each input has an output.)
    a lot of nlp tasks fall in this category, e.g.:

part-of-speech tagging: map words to their parts-of-speech (noun, verb, etc).

   
    named-entity recognition: identify mentions of people, places, etc in text
   

id14: find the main actions, and who performs them on whom/what

many-to-many: sequence labeling
    part-of-speech tagging

many-to-many: sequence labeling
    part-of-speech tagging

many-to-many: sequence labeling

heat water in a large vessel.

verb

noun

h0

h1

h2

...

heat

water

in

...

hn

many-to-many: sequence-to-sequence modeling
    mapping input sequences {x1, x2,     , xm} to output sequences {y1, y2,    , yn}.
    note: arbitrary m and n.
    many applications, most notably machine translation.
    also: id52, parsing, summarization, dialog

many-to-many: sequence-to-sequence modeling

machine 
translation

encoder id56

decoder id56

la

maison

id7

<eos>

the

blue

house

<s>

la

maison

id7

many-to-many: sequence-to-sequence modeling

dialogue 
modeling

prompt: you can   t eat pasta every day. you can   t live only on italian food.

system: i don   t eat italian food, but i do eat a lot of italian food.

many-to-many: sequence-to-sequence modeling

prompt: what do you have planned tonight?

baseline: i have no idea what i want to do with my life. i have no idea what i want 
to (bad, mediocre, bad)

ours: i was planning on doing a few of these, but i   m not sure what to choose. i 
need to find a place to camp, and i don   t have a lot of time to make a plan. 
(excellent, good, mediocre)

many-to-many: sequence-to-sequence modeling

prompt: have you seen suicide squad?

baseline: i have not, but i will check it out. (good, good, good)

ours: i saw it in a theater once and it was great. it was very... i don   t know, a little 
dark. i like the psychological effects and the way it portrays the characters in the 
movie. (excellent, excellent, excellent)

key take-aways
    id56s have memory/state that evolve over time.
    we unroll the graph over time to do forward propagation.
    backprop-through-time (bptt): 

   

   

perform chain rule over the unrolled graph efficiently by saving and reusing previous 
computations.
de/dw is sum over all time-steps (b/c of tied weights)

    vanishing/exploding gradients
    gated architectures

state is selectively overwritten per time-step

   
    uninterrupted gradient flow through time: no vanishing/exploding gradients!

    lots of cool applications!

slide credits

thank-you to the following resources, from which some of these slides were 
drawn and adapted.

    stanford cs231n
    tensorflow without a phd

the end.

