learning	
   paraphras-c	
   representa-ons	
   

of	
   natural	
   language	
   sentences	
   

kevin	
   gimpel	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

john	
   wie-ng,	
   mohit	
   bansal,	
   karen	
   livescu	
   

1	
   

word	
   embeddings	
   

turian	
   et	
   al.	
   (2010)	
   

2	
   

n       pretrained	
   word	
   embeddings	
   are	
   really	
   useful!	
   

n       what	
   about	
   pretrained	
   embeddings	
   for	
   

phrases	
   and	
   sentences?	
   

	
   

3	
   

recursive	
   neural	
   net	
   autoencoders	
   

n       composi-on	
   based	
   on	
   syntac-c	
   parse	
   

socher,	
   huang,	
   pennington,	
   ng,	
   manning	
   (2011)	
   

4	
   

paragraph	
   vectors	
   

n       represent	
   sentence	
   (or	
   paragraph)	
   by	
   

predic-ng	
   its	
   own	
   words	
   or	
   context	
   words	
   

le	
   &	
   mikolov	
   (2014)	
   

5	
   

neural	
   machine	
   transla-on	
   

n       encode	
   source	
   sentence,	
   decode	
   transla-on	
   

sutskever,	
   vinyals,	
   le	
   (2014)	
   
cho,	
   van	
   merrienboer,	
   gulcehre,	
   bahdanau,	
   bougares,	
   schwenk,	
   bengio	
   (2014)	
   

6	
   

encoder	
   as	
   a	
   sentence	
   embedding	
   model?	
   

sutskever,	
   vinyals,	
   le	
   (2014)	
   

7	
   

skip-     thoughts	
   

n       encode	
   sentence,	
   decode	
   neighboring	
   sentences	
   

   i	
   got	
   back	
   home	
   	
   	
   i	
   could	
   see	
   the	
   cat	
   on	
   the	
   steps	
   	
   	
   this	
   was	
   strange	
      	
   

kiros,	
   zhu,	
   salakhutdinov,	
   zemel,	
   torralba,	
   urtasun,	
   fidler	
   (2015)	
   

8	
   

skip-     thoughts	
   

query	
   sentence:	
   	
   
im	
   sure	
   youll	
   have	
   a	
   glamorous	
   evening	
   ,	
   she	
   said	
   ,	
   
giving	
   an	
   exaggerated	
   wink	
   .	
   
	
   
nearest	
   neighbor:	
   
im	
   really	
   glad	
   you	
   came	
   to	
   the	
   party	
   tonight	
   ,	
   he	
   said	
   ,	
   
turning	
   to	
   her	
   .	
   

kiros,	
   zhu,	
   salakhutdinov,	
   zemel,	
   torralba,	
   urtasun,	
   fidler	
   (2015)	
   

9	
   

lstm	
   autoencoders	
   

n       encode	
   sentence,	
   decode	
   sentence	
   

li,	
   luong,	
   jurafsky	
   (2015)	
   

10	
   

lstm	
   denoising	
   autoencoders	
   
n       encode	
      corrupted   	
   sentence,	
   decode	
   sentence	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   

11	
   

learning	
   paraphras4c	
   representa-ons	
   

of	
   natural	
   language	
   sentences	
   

12	
   

n       how	
   are	
   paraphras-c	
   sentence	
   embeddings	
   

useful?	
   
        mul--     document	
   summariza-on	
   
        automa-c	
   essay	
   grading	
   
        evalua-on	
   of	
   text	
   genera-on	
   systems	
   
        machine	
   transla-on	
   
        entailment/id136	
   

13	
   

evalua-on:	
   seman-c	
   textual	
   similarity	
   (sts)	
   
other	
   ways	
   are	
   needed.	
   

	
   	
   

we	
   must	
      nd	
   other	
   ways.	
   	
   

4.4	
   

i	
   absolutely	
   do	
   believe	
   there	
   was	
   an	
   iceberg	
   in	
   
those	
   waters.	
   
	
   
i	
   don't	
   believe	
   there	
   was	
   any	
   iceberg	
   at	
   all	
   
anywhere	
   near	
   the	
   titanic.	
   

	
   	
   

1.2	
   

we	
   evaluate	
   on	
   22	
   datasets	
   from	
   many	
   domains:	
   
web	
   forum	
   posts,	
   tweets,	
   machine	
   transla-on	
   output,	
   news,	
   headlines,	
   
de   ni-on	
   glosses,	
   image	
   and	
   video	
   cap-ons,	
   etc.	
   

14	
   

evalua-on	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   

sts	
   pearson	
   x	
   100	
   

44	
   
42	
   
31	
   
43	
   
38	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   
wie-ng,	
   bansal,	
   g,	
   livescu	
   (2016)	
   

15	
   

evalua-on	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
fastsent	
   (bag	
   of	
   words)	
   
avg.	
   pretrained	
   word	
   embeddings	
   

sts	
   pearson	
   x	
   100	
   

44	
   
42	
   
31	
   
43	
   
38	
   
64	
   
65	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   
wie-ng,	
   bansal,	
   g,	
   livescu	
   (2016)	
   

16	
   

paraphrase	
   database	
   (ppdb)	
   

(ganitkevitch,	
   van	
   durme,	
   and	
   callison-     burch,	
   2013)	
   

credit:	
   chris	
   callison-     burch	
   

17	
   

good	
   

be	
   given	
   the	
   opportunity	
   to	
   

training	
   data:	
   phrase	
   pairs	
   from	
   ppdb	
   
	
   
	
   
	
   
	
   
	
   
	
   

i	
   can	
   hardly	
   hear	
   you	
   .	
   
and	
   the	
   establishment	
   
laying	
   the	
   founda-ons	
   
making	
   every	
   e   ort	
   

great	
   

   	
   

   	
   

pave	
   the	
   way	
   
to	
   do	
   its	
   utmost	
   

have	
   the	
   possibility	
   of	
   
you	
   're	
   breaking	
   up	
   .	
   

as	
   well	
   as	
   the	
   development	
   

	
   

	
   tens	
   of	
   millions	
   more!	
   

	
   
	
   
	
   
	
   
	
   
	
   

learning	
   

goal:	
   learn	
   sentence	
   embedding	
   func-on	
   	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   for	
   now,	
   it   s	
   just	
   word	
   averaging:	
   

19	
   

learning	
   

goal:	
   learn	
   sentence	
   embedding	
   func-on	
   	
   

20	
   

learning	
   

sum	
   over	
   

paraphrase	
   pairs	
   

21	
   

learning	
   

sum	
   over	
   

paraphrase	
   pairs	
   

nega-ve	
   
example	
   

22	
   

learning	
   

nega-ve	
   
example	
   

23	
   

learning	
   

nega-ve	
   
example	
   

24	
   

only	
   do	
   argmax	
   over	
   
current	
   mini-     batch	
   

(for	
   e   ciency)	
   

learning	
   

we	
   regularize	
   by	
   penalizing	
   squared	
   l2	
   distance	
   to	
   

ini-al	
   (pretrained	
   glove)	
   embeddings	
   

25	
   

evalua-on	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
fastsent	
   (bag	
   of	
   words)	
   
avg.	
   pretrained	
   word	
   embeddings	
   

sts	
   pearson	
   x	
   100	
   

44	
   
42	
   
31	
   
43	
   
38	
   
64	
   
65	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   
wie-ng,	
   bansal,	
   g,	
   livescu	
   (2016)	
   

26	
   

evalua-on	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
fastsent	
   (bag	
   of	
   words)	
   
avg.	
   pretrained	
   word	
   embeddings	
   
ours	
   (avg.	
   trained	
   on	
   ppdb)	
   	
   

sts	
   pearson	
   x	
   100	
   

44	
   
42	
   
31	
   
43	
   
38	
   
64	
   
65	
   
71	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   
wie-ng,	
   bansal,	
   g,	
   livescu	
   (2016)	
   

27	
   

word	
   averaging	
   throws	
   away	
   word	
   order!	
   

how	
   about	
   an	
   lstm?	
   

28	
   

evalua-on	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
fastsent	
   (bag	
   of	
   words)	
   
avg.	
   pretrained	
   word	
   embeddings	
   
ours	
   (avg.	
   trained	
   on	
   ppdb)	
   	
   

sts	
   pearson	
   x	
   100	
   

44	
   
42	
   
31	
   
43	
   
38	
   
64	
   
65	
   
71	
   

29	
   

evalua-on	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
fastsent	
   (bag	
   of	
   words)	
   
avg.	
   pretrained	
   word	
   embeddings	
   
ours	
   (avg.	
   trained	
   on	
   ppdb)	
   	
   
ours	
   (lstm	
   trained	
   on	
   ppdb)	
   

sts	
   pearson	
   x	
   100	
   

44	
   
42	
   
31	
   
43	
   
38	
   
64	
   
65	
   
71	
   
52	
   

30	
   

evalua-on	
   

sts	
   pearson	
   x	
   100	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
fastsent	
   (bag	
   of	
   words)	
   
avg.	
   pretrained	
   word	
   embeddings	
   
ours	
   (avg.	
   trained	
   on	
   ppdb)	
   	
   
ours	
   (lstm	
   trained	
   on	
   ppdb)	
   

44	
   
42	
   
31	
   
43	
   
38	
   
what   s	
   going	
   on	
   here?	
   
64	
   
65	
   
71	
   
52	
   

31	
   

in-     domain	
   evalua-on:	
   	
   

held-     out,	
   annotated	
   ppdb	
   pairs	
   

can	
   not	
   be	
   separated	
   from	
   

is	
   inseparable	
   from	
   

hoped	
   to	
   be	
   able	
   to	
   

looked	
   forward	
   to	
   

come	
   on	
   ,	
   think	
   about	
   it	
   

people	
   ,	
   please	
   

how	
   do	
   you	
   mean	
   that	
   

what	
   worst	
   feelings	
   

similarity	
   
annota-on	
   

5.0	
   

3.4	
   

2.2	
   

1.6	
   

32	
   

	
   

l

n
o
-
a
e
r
r
o
c

70	
   

66	
   

62	
   

58	
   

54	
   

50	
   

held-     out,	
   annotated	
   

sec-on	
   of	
   ppdb:	
   

semeval	
   sentence	
   

similarity	
   tasks	
   

(avg.	
   of	
   22	
   datasets):	
   

word	
   averaging	
   
lstm	
   

65.7	
   

61.3	
   

60	
   

55.7	
   

33	
   

	
   

l

n
o
-
a
e
r
r
o
c

70	
   

66	
   

62	
   

58	
   

54	
   

50	
   

held-     out,	
   annotated	
   

sec-on	
   of	
   ppdb:	
   

semeval	
   sentence	
   

similarity	
   tasks	
   

(avg.	
   of	
   22	
   datasets):	
   

word	
   averaging	
   
lstm	
   

65.7	
   

61.3	
   

60	
   

55.7	
   

34	
   

	
   	
   

l

n
o
-
a
e
r
r
o
c

80	
   
75	
   
70	
   
65	
   
60	
   
55	
   
50	
   

sentence	
   length	
   comparison	
   

<=4	
   

5	
   

7	
   
6	
   
word	
   averaging	
   

8	
   
lstm	
   

9	
   

>=	
   10	
   

word	
   averaging	
   is	
   beder	
   at	
   all	
   
sentence	
   lengths	
   in	
   test	
   data	
   

35	
   

n       this	
   is	
   troubling	
   

n       why	
   does	
   the	
   lstm	
   struggle	
   on	
   out-     of-     

domain	
   data?	
   

36	
   

maybe	
   the	
   problem	
   is	
   the	
   training	
   data   	
   
n       new	
   data:	
   sentence	
   pairs	
   automa-cally	
   
extracted	
   by	
   coster	
   and	
   kauchak	
   (2011)	
   

n       developed	
   for	
   text	
   simpli   ca-on	
   applica-ons;	
   

we	
   use	
   it	
   as	
   a	
   paraphrase	
   training	
   set!	
   

37	
   

new	
   data:	
   examples	
   

this	
   was	
   also	
   true	
   for	
   pompeii	
   ,	
   where	
   the	
   temple	
   of	
   jupiter	
   that	
   
was	
   already	
   there	
   was	
   enlarged	
   and	
   made	
   more	
   roman	
   when	
   
the	
   romans	
   took	
   over	
   .	
   
	
   
this	
   held	
   true	
   for	
   pompeii	
   ,	
   where	
   the	
   previously	
   exis-ng	
   temple	
   
of	
   jupiter	
   was	
   enlarged	
   and	
   romanized	
   upon	
   conquest	
   .	
   
	
   

38	
   

new	
   data:	
   examples	
   

this	
   was	
   also	
   true	
   for	
   pompeii	
   ,	
   where	
   the	
   temple	
   of	
   jupiter	
   that	
   
was	
   already	
   there	
   was	
   enlarged	
   and	
   made	
   more	
   roman	
   when	
   
the	
   romans	
   took	
   over	
   .	
   
	
   
this	
   held	
   true	
   for	
   pompeii	
   ,	
   where	
   the	
   previously	
   exis-ng	
   temple	
   
of	
   jupiter	
   was	
   enlarged	
   and	
   romanized	
   upon	
   conquest	
   .	
   
	
   

39	
   

new	
   data:	
   examples	
   

	
   
two	
   days	
   later	
   leo	
   crowned	
   charlemagne	
   at	
   st.	
   peter	
   's	
   tomb	
   .	
   

	
   

two	
   days	
   later	
   ,	
   on	
   christmas	
   day	
   800	
   ,	
   leo	
   crowned	
   charlemagne	
   
as	
   roman	
   emperor	
   .	
   

40	
   

l

n	
   
o
-
a
e
r
r
o
c

80	
   

70	
   

60	
   

50	
   

40	
   

30	
   

20	
   

data	
   source	
   comparison	
   

67.7	
   

54.2	
   

avg.	
   

lstm	
   

train	
   on	
   ppdb	
   

train	
   on	
   simple-     

standard	
   wikipedia	
   

l

n	
   
o
-
a
e
r
r
o
c

80	
   

70	
   

60	
   

50	
   

40	
   

30	
   

20	
   

data	
   source	
   comparison	
   

67.7	
   

68.4	
   

54.2	
   

59.3	
   

avg.	
   

lstm	
   

train	
   on	
   ppdb	
   

train	
   on	
   simple-     

standard	
   wikipedia	
   

l

n	
   
o
-
a
e
r
r
o
c

80	
   

70	
   

60	
   

50	
   

40	
   

30	
   

20	
   

data	
   source	
   comparison	
   

67.7	
   

68.4	
   

54.2	
   

59.3	
   

avg.	
   

lstm	
   

both	
   improve,	
   but	
   lstm	
   improves	
   more!	
   

train	
   on	
   ppdb	
   

train	
   on	
   simple-     

standard	
   wikipedia	
   

maybe	
   the	
   lstm	
   is	
   just	
   memorizing	
   the	
   

training	
   sequences   	
   

44	
   

scrambling	
   

n       with	
   some	
   id203,	
   scramble	
   both	
   sentences:	
   
	
   
originally	
   ,	
   the	
   college	
   was	
   just	
   for	
   boys	
   from	
   eton	
   college	
   .	
   
originally	
   ,	
   the	
   college	
   was	
   to	
   be	
   speci   cally	
   for	
   boys	
   from	
   eton	
   college	
   .	
   
	
   
	
   
just	
   was	
   boys	
   originally	
   from	
   ,	
   .	
   for	
   eton	
   college	
   college	
   the	
   
the	
   college	
   eton	
   .	
   to	
   speci   cally	
   boys	
   was	
   ,	
   from	
   be	
   originally	
   for	
   college	
   

n       scrambling	
   rate	
   tuned	
   over	
   {0.25,	
   0.5,	
   0.75}	
   

45	
   

regulariza-on	
   

68.4	
   

59.3	
   

avg.	
   

lstm	
   

l

n	
   
o
-
a
e
r
r
o
c

80	
   

70	
   

60	
   

50	
   

40	
   

regulariza-on	
   

68.4	
   

65.1	
   

59.3	
   

avg.	
   
lstm	
   
lstm+scrambling	
   

l

n	
   
o
-
a
e
r
r
o
c

80	
   

70	
   

60	
   

50	
   

40	
   

regulariza-on	
   

68.4	
   

68.4	
   

65.1	
   

59.3	
   

avg.	
   
lstm	
   
lstm+scrambling	
   
lstm+scrambling+dropout	
   

l

n	
   
o
-
a
e
r
r
o
c

80	
   

70	
   

60	
   

50	
   

40	
   

regulariza-on	
   

68.4	
   

68.4	
   

68.6	
   

65.1	
   

59.3	
   

avg.	
   

lstm	
   

lstm+scrambling	
   

lstm+scrambling+dropout	
   

lstm	
   avg.+scrambling
+dropout	
   

l

n	
   
o
-
a
e
r
r
o
c

80	
   

70	
   

60	
   

50	
   

40	
   

lstm	
   is	
   bexer	
   than	
   averaging:	
   

	
   

sentence	
   1	
   

sentence	
   2	
   

bloomberg	
   chips	
   in	
   a	
   
billion	
   
in	
   other	
   regions,	
   the	
   
sharia	
   is	
   imposed.	
   

bloomberg	
   gives	
   $1.1	
   b	
   to	
   
university	
   
in	
   other	
   areas,	
   sharia	
   law	
   is	
   being	
   
introduced	
   by	
   force.	
   

lstm	
   
sim.	
   

3.99	
   

avg.	
   
sim.	
   

3.04	
   

gold	
   
sim.	
   

4.0	
   

4.44	
   

3.72	
   

4.75	
   

word	
   averaging	
   underes4mates	
   similarity	
   when	
   

there	
   are	
   mul4word	
   paraphrases:	
   
   chips	
   in   	
   =	
      gives   	
   
   a	
   billion   	
   =	
      $1.1	
   b   	
   
   the	
   sharia   	
   =	
      sharia	
   law   	
   
   imposed   	
   =	
      being	
   introduced	
   by	
   force   	
   

50	
   

lstm	
   overes-mates	
   similarity:	
   

	
   

sentence	
   1	
   

sentence	
   2	
   

three	
   men	
   in	
   suits	
   
siyng	
   at	
   a	
   table.	
   
we	
   never	
   got	
   out	
   of	
   it	
   
in	
   the	
      rst	
   place!	
   
two	
   birds	
   interac-ng	
   
in	
   the	
   grass.	
   

two	
   women	
   in	
   the	
   kitchen	
   
looking	
   at	
   a	
   object.	
   
where	
   does	
   the	
   money	
   come	
   
from	
   in	
   the	
      rst	
   place?	
   
two	
   dogs	
   play	
   with	
   each	
   other	
   
outdoors.	
   

lstm	
   
sim.	
   

3.33	
   

avg.	
   
sim.	
   

2.79	
   

gold	
   
sim.	
   

0.0	
   

4.00	
   

3.33	
   

0.8	
   

3.44	
   

2.81	
   

0.2	
   

lstm	
   overes4mates	
   similarity	
   with	
   

similar	
   sequences	
   of	
   syntac4c	
   categories,	
   	
   

but	
   di   erent	
   meanings	
   

51	
   

gated	
   recurrent	
   averaging	
   network	
   (gran)	
   
n       inspired	
   by	
   the	
   success	
   of	
   averaging	
   and	
   the	
   

lstm,	
   we	
   propose	
   a	
   new	
   model:	
   

embedding	
   of	
   

word	
   at	
   posi-on	
   t	
   

lstm	
   hidden	
   vector	
   

at	
   posi-on	
   t	
   

52	
   

gran	
   

68.4	
   

68.4	
   

68.6	
   

68.9	
   

65.1	
   

59.3	
   

avg.	
   

lstm	
   

lstm+scrambling	
   

lstm+scrambling+dropout	
   

lstm	
   avg.+scramb.+dropout	
   

gran+scramb.+dropout	
   

l

n	
   
o
-
a
e
r
r
o
c

75	
   

65	
   

55	
   

analyzing	
   gran	
   gates	
   

54	
   

supervised	
   learning	
   +	
   regularize	
   to	
   unsupervised	
   representa-ons	
   

85.6	
   

86	
   

85.2	
   

85.1	
   

84	
   

avg.	
   
avg.+prior	
   
lstm	
   
lstm+prior	
   
gran	
   
gran+prior	
   

90	
   

l

n	
   
o
-
a
e
r
r
o
c

85	
   

84.5	
   

80	
   

supervised	
   learning	
   +	
   regularize	
   to	
   unsupervised	
   representa-ons	
   

85.6	
   

86	
   

85.2	
   

85.1	
   

84	
   

avg.	
   
avg.+prior	
   
lstm	
   
lstm+prior	
   
gran	
   
gran+prior	
   

90	
   

l

n	
   
o
-
a
e
r
r
o
c

85	
   

84.5	
   

80	
   

all	
   models	
   bene   t	
   from	
   regularizing	
   
toward	
   unsupervised	
   representa4ons	
   

ongoing	
   work	
   

n       new	
   data:	
   
automa-cally-     translated	
   bilingual	
   sentence	
   pairs	
   

the	
   room	
   was	
   very	
   pleasant	
   and	
   the	
   hotel	
   's	
   locadon	
   next	
   to	
   the	
   
park	
   and	
   teh	
   maridme	
   museum	
   was	
   suberb	
   .	
   
	
   
excellent	
   locadon	
   -     	
   right	
   next	
   door	
   to	
   the	
   maridme	
   museum	
   and	
   
greenwich	
   park	
   with	
   the	
   observatory	
   and	
   dme	
   museum	
   .	
   

	
   

57	
   

thank	
   you!	
   

58	
   

