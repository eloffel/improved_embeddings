graph-based text representations: 

boosting id111, nlp and information retrieval with graphs

fragkiskos d. malliaros!
michalis vazirgiannis!

uc san diego!
  cole polytechnique!

updated slides at: http://fragkiskosm.github.io/projects/graph_text_tutorial 

 

conference on empirical methods in natural language processing (emnlp)!

copenhagen, denmark!
september 7   11, 2017!

!
id111     terminology

       id111 on a collection of documents:!

       the collection is the data set!
       the documents are the data points!

       since text is unstructured, a document is usually converted in a 

common representation!

world	

barks	

the	

dog	

paris	

bag-of-words!

2!

example: text categorization

textual 
data!

feature extraction!
term weighting!

model 
learning!

text 

categorization!

evaluation!

applications:!
       opinion mining (id31)!
       email spam classi   cation!
       web-pages classi   cation!
          !

3!

!
bag-of-words (bow) - issues

example text!
information retrieval is the activity of obtaining!

information resources relevant to an information need!

from a collection of information resources!

bag of words: [(activity,1), (collection,1) !

                  (information,4), (relevant,1), !

             

     (resources, 2), (retrieval, 1),    ]!

       term independence assumption !
       term frequency weighting!

assumptions made by 

the bow model 

4!

!
!
!
graph-based id194

       challenge the term independence and term frequency weighting 

assumptions taking into account word dependence, order and 
distance!

       employ a graph-based id194 capturing the 

above!

       graphs have been successfully used in ir to encompass relations 

and propose meaningful weights (e.g., id95)!

5!

graph-based id194 - example

information  retrieval  is  the  activity  of  obtaining 

information resources relevant to an information need 

from a collection of information resources

idea: replace term frequency with 
node centrality!

captures: frequency, order and distance!

6!

goal of the tutorial and outline

goal: offer a comprehensive presentation of recent methods 
that rely on graph-based text representations to deal with 

various tasks in nlp and ir!

       part i.  graph-theoretic concepts and graph-based text 

representation!

       part ii. information retrieval!
       part iii. keyword extraction and text summarization!
       part iv. text categorization!
       part v. final remarks and future research directions!

7!

tutorial outline

       part i.  graph-theoretic concepts and graph-based text 

representation!

       part ii. information retrieval!
       part iii. keyword extraction and text summarization!
       part iv. text categorization!
       part v. final remarks and future research directions!

8!

basic graph-theoretic concepts 

and de   nitions

9	

graphs and networks

graphs: modeling dependencies!

nodes (or vertices)!

(objects/entities)!

g = (v, e) 
(network or graph)!

n = |v|    is the number of nodes!
m =|e|    is the number of edges!
	

edges (or links)!
(interconnections)!

10!

undirected vs. directed networks

undirected!
       links: undirected 
(symmetrical, reciprocal)!

directed!
       links: directed !
(arcs)!

l

f

g

a

b

d

c

m

i

h

b

a

c

g

d

e

f

       examples!

       collaborations!
       friendship on facebook!

       examples!

       phone calls!
       following on twitter!

11!

node degree

d
e
t
c
e
r
i
d
n
u

d
e

t

c
e
r
i

d

a

c

b

a

g

d

e

f

ka = 4

node degree ki : the number !
of edges adjacent to node i!
average degree:!
  k = hki = 1
in directed networks we de   ne an in-
degree and out-degree!
the (total) degree of a node is the sum of in- 
and out-degrees!

npn

i=1 ki = 2|e|n

kin
c = 2

kout
c = 1

kc = 3

source: node with kin = 0!
sink: node with kout = 0!

average:!

  kin =   kout

12!

!
!
more types of graphs

unweighted!
(undirected)!

4

1

weighted !
(undirected)!

4

1

3

2

aij =

1
0
1
1

3
0
    
1
    
    
1
    
0
    
    
aii = 0
1
n
2

2

0
1
    
1
1
    
    
0
0
    
0
0
    
    
aij = aji
2 | e |
n

    

aij

| e |=
examples: friendship, hyperlink!

    k =
i, j=1

aij =

0.5
1
0
0

2
0
1
4

0
    
2
    
    
0.5
    
0
    
    
aii = 0
n
nonzero(aij)
   
i, j=1

0
4
0
0

    
    
    
    
    
    
aij = aji
2 | e |
k =
n

1
2

| e |=
examples: collaboration, internet, roads!
13!

    

subgraphs

       let g = (v, e) be a graph and let                be any subset of its vertices!

6 

4 

3 

s     v
5 

1 

2 

       de   nition: the induced subgraph g[s] = (s, e   ) is the graph whose vertex 

set is s and its edge set consists of all of the edges in e that have both 
endpoints in s!

5 

2 

1 

s={1, 2, 3, 5}!

3 

14!

!
representation matters!

choice of the proper network!

representation of a given!
system determines our!
ability to use networks!

successfully!

15!

centrality criteria

16	

centrality in networks (1/2)

       determine the relative importance of a node in the network!
       applications in social network analysis, the internet, epidemiology, 

urban informatics,    !

       what do we mean by centrality?!

       a central node is more important or powerful    !
       or, more in   uential ...!
       or, is more critical due to its location in the graph!

       also, very closely related to the problem of ranking in the 

context of web search!
       each webpage can be considered as a    user   !
       each hyperlink is an endorsement relationship!
       centrality measures provide a query independent link-based score of 

importance of a web page!

17!

centrality in networks (2/2)

source: https://www.macalester.edu/~abeverid/thrones.html!

18!

types of centrality

       starting point: the central node of a star is the most important!
       why?!

       the node with the highest degree!
       the node that is closest to the rest nodes (e.g., has the smallest 

average distance to other nodes)!

       the node through which all shortest paths pass!
       the node that maximizes the dominant eigenvector (the one that 

corresponds to the largest eigenvalue) of the adjacency matrix!

       the node with highest id203 in the stationary distribution of a 

random walk on the graph!

various competing views of centrality!

19!

measures of centrality

       this observation leads to the following classes of indices of 

centrality:!
       measures based on distances (e.g., degree, closeness)!
       measures based on paths (e.g., betweenness, katz   s index)!
       spectral measures (eigenvector, id95, hits, salsa, random 

walks with restarts)!

       measure based on groups of nodes (e.g., cliques, plexes, cores)!

       related to the    id91    structure!
       more on that in another lecture!

20!

a first example

in each of the following networks, x has higher centrality than y according to!
a particular measure!

x 

x 

y 

y 

y 

x 

x 

y 

in-degree!

out-degree!

betweenness!

closeness!

21!

degree centrality (1/2)

      

idea: a central node is one with many connections!

       cd (i) = k(i), where k(i) is the degree of node i!

22!

degree centrality (2/2)

      

idea: a central node is one with many connections!

       normalized degree centrality: divide by the max possible degree 

(n-1) !

23!

closeness centrality

       motivation: it measures the ability to quickly access or pass 

information through the graph!

ccl(i) =

n   1
pj,i d(i, j)

values in the 
range [0,1] 

mean distance 
from a node to 
other nodes!

    d(i, j) is the length of the shortest path between i and j (geodesic distance)!

       the closeness of a node is de   ned as the inverse of the sum of the shortest 

path (sp) distances between the node and all other nodes in the graph!

be close to everybody else 
(e.g., influence on other nodes) 

why inverse the distance?!
       nodes with low mean distance 

should get high score!

[mateos,    17]!

24!

betweenness centrality

       motivation: a node is important if it lies in many shortest paths!

cbt(i) = xs,i,t2v

 (s, t|i)
 (s, t)

         (s, t) is the total number of shortest paths from s to t!
         (s, t|v) is the number of shortest paths from s to t that pass through i!

oftentimes it is normalized:		

cbt(i)

 n 1
2  

essential nodes in passing 
information through the 
network 

25!

the hits algorithm

(hubs and authorities)

26	

hubs and authorities (1/3)

interesting pages fall into two classes:!

1.    authorities are pages containing !

useful information!
       newspaper home pages!
       course home pages!
       home pages of auto manufacturers!

2.    hubs are pages that link to authorities!

       list of newspapers!
       course bulletin!
       list of u.s. auto manufacturers!

27!

!
hubs and authorities (2/3)

       pages have double identity!

       hub identity!
       authority identity!

       good hubs point to good 

authorities!

       good authorities are pointed by 

good hubs!

hubs!

authorities!

28!

hubs and authorities (3/3)

       two kind of weights:!

       hub weight!
       authority weight!

       the hub weight is the sum of the authority weights of the 

authorities pointed to by the hub!

       the authority weight is the sum of the hub weights that point to 

this authority!

       represented as vectors h and   , where the i th element is the 

hub/authority score of the i th node!

29!

hits algorithm

      

initialize: !

j = 1/pn,
   0

j = 1/pn
h0

       repeat until convergence !

       authority:!

       hub:!

       normalize:

[kleinberg    98]!

   (t+1)
i

h(t+1)
i

xi

=xj!i
=xi!j
   2
      (t+1)

i

h(t)
j , 8i

   (t)
j , 8i
and

= 1

xj    h(t+1)

j

   2

 !
= 1

30!

hits and eigenvectors

       hits in vector notation!

          = [  1,   2, ...,   n]t   and h = [h1, h2,    , hn]t!

       we can rewrite   i and hi based on the adjacency matrix!

                              as!

   j

hi =xi!j

hi =xj

aij       j

       thus, h = a       and       = at h!

         (t+1) = at h(t)  and h(t+1) = a   (t)!
         (t+1) = at a   (t) and h(t+1) = aat h(t)!

repeated iterations 
will converge to the 

eigenvectors!

       authority weight vector    : eigenvector of ata!
       hub weight vector h : eigenvector of aat!

svd 
the vectors    and h are 
the singular vectors of a!

31!

!
!
id95

       good authorities should be pointed by good authorities!

       the value of a node comes from the value of the nodes that point to it!

       how do we implement that?!

       assume that we have a unit of authority to distribute to all nodes!

       initially, each node gets 1/n amount of authority!

       each node distributes its authority value to its neighbors!
       the authority value of each node is the sum of the authority fractions 

that they collect from their neighbors!

   v = x8(u,v)2e

1

kout(u)    u

  v: the id95 value of node v!
       recursive de   nition!

32!

a simple example

  !

  !

  !

   +    +    = 1 !

   =     +    !
   =      !
   =      !

       solving the system of equations we get the authority values 

for the nodes!
          =        =        =    !

33!

a more complex example

  1 = 1/3   4 + 1/2   5!
  2 = 1/2   1 +   3 + 1/3   4!
  3 = 1/2   1 + 1/3   4!
  4 = 1/2   5!
  5 =   2 !

   v = x8(u,v)2e

1

kout(u)    u

v2!

v1!

v5!

v3!

v4!

34!

computing id95 weights

       a simple way to compute the weights is by iteratively 

updating the weights!

initialize all id95 weights to 1/n !

repeat:!

   v = x8(u,v)2e

1

kout(u)    u

until the weights do not change!

this process converges!

35!

!
!
!
!
!
!
core decomposition in networks

36	

core decomposition

       tool to analyze the structure of real networks!

       quantify community and id91 structure!

       hierarchical representation of a graph into nested subgraphs 

of increased connectivity and coherence properties!

       basic idea:!

       set a threshold on the node degree, say k!
       nodes that do not satisfy the threshold are removed from the graph!

       extensions to other node properties (e.g., triangles)!
       plethora of applications!

       dense subgraph discovery and community detection!
       evaluation of collaboration in social networks!
      
       text analytics!

identi   cation of in   uential spreaders in social networks!

37!

k-core decomposition

       degeneracy for an undirected graph g!

       also known as the k-core number!
       the k-core of g is the largest subgraph in which every vertex has 

degree at least k within the subgraph!

example:!

3-core

2-core

1-core

core number ci = 1

core number ci = 2

core number ci = 3

graph degeneracy     (g) = 3

g0 = g
g1 = 1-core of g
g2 = 2-core of g
g3 = 3-core of g

g0     g1     g2     g3

important property:!
      
      
       scalable to large scale 

fast and easy to compute!
linear to the size of the graph!

graphs!

note:!
the degeneracy and the size of 

the k-core provide a good 

indication of the cohesiveness of 

the graph!

also known as graph degeneracy!

38!

another example

                !

core0(g)	

core1(g)	

core2(g)	

core3(g)	

core4(g)	

39!

algorithm for k-core decomposition

algorithm k-core(g, k)!
input: an undirected graph g and  positive integer k!
output: k-core(g)!
1. let f := g!
2. while there is a node x in f such that  degf(x)<k!

delete node x from f!

3. return f!

       many  ef   cient algorithms have been proposed for the computation!

       time complexity: o(m)!

[batagelj and zaversnik,    03]!

40!

k-truss decomposition (triangles)

       k-truss decomposition [cohen    08], [wang and cheng    12]!
       triangle-based extension of the k-core decomposition!
       each edge of the k-truss subgraph participates in at least k-2 triangles!

       informally, the    core    of the maximal k-core subgraph!
       subgraph of higher coherence compared to the k-core!

6

truss set t!

set c
set t

graph-based text representations

42	

graph semantics

       let g = (v, e) be the graph that corresponds to a document d!
       the nodes can correspond to:!

       paragraphs!
       sentences!
       phrases!
       words [main focus of the tutorial]!
       syllables!

       the edges of the graph can capture various types of relationships 

between two nodes:!
       co-occurrence within a window over the text [main focus of the tutorial]!
       syntactic relationship!
       semantic relationship!

43!

graph-of-words (gow) model

       each document d   d is represented by a graph  gd = (vd, ed), where the 

nodes correspond to the terms t of the document and the edges capture co-
occurrence relationships between terms within a    xed-size sliding window of 
size w!

   

       directed vs. undirected graph!

       directed graphs are able to preserve the actual    ow of a text!
      

in undirected graphs, an edge captures co-occurrence of two terms whatever the 
respective order between them is!

       the higher the number of co-occurrences of two terms in the document, the higher 

       weighted vs. unweighted graph!

the weight of the corresponding edge!

       size w of the sliding window!

       add edges between the terms of the document that co-occur within a sliding 

window of size w!

       larger window sizes produce graphs that are relatively dense!

[mihalcea and tarau, emnlp    04], [blanco and lioma, inf. retr.    12], 
[rousseau and vazirgiannis, cikm    13]!

example of unweighted gow

data science is the extraction of knowledge from large volumes of data
that are structured or unstructured which is a continuation of the    eld of
data mining and predictive analytics, also known as knowledge discovery
and data mining.

predict

scienc

   eld

knowledg

mine

extract

structur

data

discoveri

analyt

larg

unstructur

volum

known

continu

w = 3!
unweighted, undirected graph!

45!

example of weighed undirected gow

and 

aspects 

of 
mathematical 
computer-aided  share  trading. 
consider  problems  of 
we 
statistical  analysis  of 
share 
propose 
prices 
to 
probabilistic  characteristics 
describe  the  price  series.  we 
of 
discuss 
three  methods 
of 
mathematical  modelling 
given 
price 
probabilistic characteristics.

series 

with 

edge weights

mathemat
   

price
   

1
2
3
4
5

   

probabilist

   
seri

computer   aid

   

aspect
   

problem
   

statist
   

share
   

characterist

   

   

trade

   
analysi

   
model

method

   

46!

tutorial outline

       part i.  graph-theoretic concepts and graph-based text 

representation!

       part ii. information retrieval!
       part iii. keyword extraction and text summarization!
       part iv. text categorization!
       part v. final remarks and future research directions!

47!

in-degree based tw

       the weight of a term in a document is its in-degree  in the graph-of-words!
      
it represents the number of distinct contexts of occurrence!
       we store the document as a vector of weights in the direct!

      

index and similarly in the inverted index!
for example:!
information
retrieval
is
the
activity
of
obtaining
resources
relevant
to
an
need
from
a
collection

5!
1!
2!
2!
2!
3!
2!
3!
2!
2!
2!
2!
2!
2!
2!

bag of words: !
((activity,1), (collection,1),  !
 (information,4), (relevant,1), !
(resources, 2), (retrieval, 1)..)!

48!

tf-idf and bm25!

       term  t,  document  d,  collection  of  size  n,  term  frequency  tf(t,  d), 
document  frequency  df(t),  document  length  |d|,  average  document 
length avdl, asymptotical marginal gain k1 (1.2), slope parameter b!

       tf-idf [singhal et al., trec-7]!
           tf-idf(t, d) = tfpol-idf(t, d) = tfpotfl(t, d) x idf(t) =!

       bm25 [lv and zhai, cikm    11]!

#
%
%
%
$

1+log 1+log tf (t,d)
)

(

(

1   b +b   

| d |
avdl

         bm25(t, d) =!

#
%
%
%
%
$

(k1 +1)  tf (t,d)
&
(+tf (t,d)
'

| d |
avdl

k1    1   b +b   

#
%
$

  log n +1
df (t)

#
%
$

&
(
'

&
(
(
(
(
'

  log n +1
df (t)

#
%
$

&
(
'

)

&
(
(
(
'

49!

!
!
!
tw-idf

       term t, document d, collection of size n, term weight tw(t, d), document 

frequency df(t), document length |d|, average document length avdl, 
asymptotical marginal gain k1 (1.2), slope parameter b!

             tw-idf(t, d) = !

#
%
%
%
$

tw(t,d)
1   b +b   

| d |
avdl

&
(
(
(
'

  log n +1
df (t)

#
%
$

&
(
'

      

      

in the bag-of-word representation, tw is usually de   ned as the term 
frequency or sometimes just the presence/absence of a term (binary tf)!
in the graph-of-word representation, tw is the in-degree of the vertex 
representing the term in the graph !

[rousseau and vazirgiannis, cikm    13]!

50!

experimental evaluation

       datasets!
       platforms!
       evaluation!
       results!

51!

datasets (1/2)

       disks 1 & 2 (trec)!

741,856 news articles from wall street journal (1987-1992), federal 
register (1988-1989), associated press (1988-1989 and information from 
the computer select disks (1989-1990) !

       disks 4 & 5 (trec, minus the congressional record)!
528,155 news releases from federal register (1994), financial times 
(1991-1994), foreign broadcast information service (1996) and los 
angeles times (1989-1990) !

       wt10g (trec)!

      

1,692,096 crawled pages from a snapshot of the web in 1997 !
.gov2 (trec)!
25,205,179 crawled web pages from .gov sites in early 2004!

52!

datasets (2/2)

dataset!

statistic!
# of documents!
# of unique terms!
average # of terms 
(avdl)!
average # of vertices!
average # of edges!

disks 1 & 2! disks 4 & 5!

wt10g!

.gov2!

741,856!
535,001!

528,155!
520,423!

1,692,096!
3,135,780!

25,205,179!
15,324,292!

237!

125!
608!

272!

157!
734!

398!

165!
901!

645!

185!
1,185!

table: statistics on the four trec datasets used; disks 4&5 excludes the congressional 

record. the average values are computed per document. !

53!

evaluation

       mean average precision (map) and precision at 10 (p@10)!

       considering only the top-ranked 1000 documents for each run!

       statistical signi   cance of improvement was assessed using 

the student   s paired t-test!
       r implementation (t.test {stats} package), trec_eval output as input!
       two-sided p-values less than 0.05 and 0.01 to reject the null hypothesis !

       likelihood of relevance vs. likelihood of retrieval [singhal et al., sigir    96]!
       4 baseline models: tf-idf, bm25, piv+ and bm25+!

       tuned slope parameter b for pivoted document length id172 (2-fold cross-

validation, odd vs. even topic ids, map maximization)!

       default (1.0) lower-bounding gap [lv and zhai, cikm    11]!

54!

graph-based ad hoc ir

       evaluation in terms of:!

       mean average precision!
       precision@10!
       id203 of relevance!
vs. id203 of retrieval!

55!

likelihood of relevance vs. likelihood of retrieval

id203 of relevance/retrieval vs. document length on wt10g

relevance
tf   idf
piv+
bm25
bm25+
tw   idf
tw   idf [b = 0]

l

a
v
e
i
r
t
e
r
/
e
c
n
a
v
e
e
r
 
f

l

o
 
y
t
i
l
i

b
a
b
o
r
p

0.04

0.03

0.02

0.01

0.00

100

1000

document length

10000

id203 of retrieval 
(tw-idf, b = 0)!

id203 of retrieval 
(tw-idf)!

id203 of relevance 
(ground truth)!

56!

tutorial outline

       part i.  graph-theoretic concepts and graph-based text 

representation!

       part ii. information retrieval!
       part iii. keyword extraction and text summarization!
       part iv. text categorization!
       part v. final remarks and future research directions!

57!

single document keyword extraction

keywords are used everywhere!
       looking up information on the web (e.g., via a search engine bar)!
       finding similar posts on a blog (e.g., tag cloud)!
       for ads matching (e.g., adwords    keyword planner)!
       for research paper indexing and retrieval (e.g., springerlink)!
       for research paper reviewer assignment  !

applications are numerous!
       summarization (to get a gist of the content of a document) !
      
      
       id183 (using additional keywords from top results) !

information    ltering (to select speci   c documents of interest) !
indexing (to answer keyword-based queries)!

58!

!
graph-based keyword extraction (1/2) 

existing graph-based keyword 
extractors:  !
-   

assign a centrality based score to a 
node !
top ranked ones will correspond to the 
most representative!

-   

-   
-   
-   

textrank (id95) [mihalcea and tarau, 
emnlp    04] !
hits [litvak and last, mmies    08]!
node centrality (degree, betweenness, 
eigenvector) [boudin, ijnlp    13]!

k-core decomposition of the graph !

idea: retain the k-core subgraph of the graph to extract the 

nodes based on their centrality and cohesiveness!

59!

!
graph-based keyword extraction (2/2) 

       single-document keyword 

extraction  !
       select the most cohesive sets of 
words in the graph as keywords!

       use k-core decomposition to 
extract the main core of the 
graph!

       weighted edges!

[rousseau and vazirgiannis, ecir    15]!

60!

id95 vs. k-core

61!

keywords are not unigrams

       500 abstracts from the inspec database used in our 

experiments, !

       4,913 keywords manually assigned by human annotators !
       only 662  are unigrams (13%). !
       bigrams (2,587     52%)      7-grams (5). !
    keywords are bigrams, if not higher order id165s. !
    the interactions within keywords need to be captured in the 
   rst place     i.e. in the graph. !
    we can consider a k-core to form a    long-distance (k+1)-
gram    [bassiou and kotropoulos, 2010] !

62!

how many keywords?

       most techniques in keyword extraction assign a score to each 

feature and then take the top ones!

       but how many? !

       absolute number (top x) or relative number (top x%)? !

       besides, at    xed document length, humans may assign more 

keywords for a document than for another one !

x is decided at document level (size of the k-core 

subgraph)!

k-cores are adaptive 

63!

!
!
datasets

       hulth2003     500 abstracts from the inspec database [hulth, 

2003]!

       krapi2009     2,304 acm full papers in computer science 

(references and captions excluded) [krapivin et al., 2009] !

all approaches are unsupervised and single-document!

64!

!
!
!
!
models and baseline methods

graph-of-words: !
       undirected edges !
       forward edges!

a sliding window!
       backward edges  !

       natural    ow of the text!
       an edge term1     term2 meaning that term1 precedes term2 in 

keyword extractors:!
       id95!
       hits (authority scores only) !
       k-core!
       weighted k-core !

top 33% or top 15% keywords!

main core!

65!

!
id74  

       each document has a set of golden keywords assigned 

by humans !

       precision, recall and f1-score per document !

       macro-average each metric at the collection level!

66!

!
!
performance evaluation

precision!
recall!
f1-score!
precision/recall!

67!

example     ecir   15 paper

       stemmed unigrams of the main core of the graph- of-words of 

the paper document: {keyword, extract, graph, represent, 
text, weight, graph-of-word, k-core, degeneraci, edg, vertic, 
number, document}!

       using id95,    work    appears in the top 5,    term    and 
   id95    in the top 10, and    case    and    order    in the top 
15. central words but not in cohesion with the rest and 
probably not relevant!

68!

a di   erent point of view

graph degeneracy:!
      

in social networks, nodes part of the 
highest levels of the hierarchy are 
better spreaders than nodes high on 
id95!

       nodes with high truss numbers are 

even more in   uential than nodes with 
high core numbers!

       spreading in   uence may be a better 
   keywordness    metric than prestige 
(captured by id95)!

69!

drawbacks of graph degeneracy (1/4)
retaining the top level like in may be an appealing initial idea!

70!

drawbacks of graph degeneracy (2/4)
but many keywords live below the top level -> good precision, poor recall!

71!

drawbacks of graph degeneracy (3/4)

how to automatically select the best level in the 

hierarchy?!

72!

drawbacks of graph degeneracy (4/4)

how to automatically select the best level in the 

hierarchy?!

in order to improve recall while not losing too 

much in precision?!

	

73!

graph degeneracy for keyword extraction

heuristics:!
       dens: go down the hierarchy until a drop in k-core (or truss) density is 
observed, i.e., as long as the desirable cohesiveness properties are 
kept!
inf: go down the hierarchy as long as the shells increase in size (starting 
at the main     1 level)!

      

problem: both methods work at the subgraph level -> lack    exibility for!
     large graphs (adding an entire group of nodes or not)!

[tixier et al., emnlp    16]!

74!

graph degeneracy for keyword extraction

how to work at the node level while still retaining 
heuristics:!
the valuable cohesiveness information captured 
       dens: go down the hierarchy until a drop in k-core (or truss) density is 
observed, i.e., as long as the desirable cohesiveness properties are 
kept!
inf: go down the hierarchy as long as the shells increase in size (starting 
at the main     1 level)!

by degeneracy?!

      

problem: both methods work at the subgraph level -> lack    exibility for!
     large graphs (adding an entire group of nodes or not)!

[tixier et al., emnlp    16]!

75!

corerank

corerank (cr):!
       assign to each node the sum of the core (or truss) numbers of its neighbors!
       granularity is much    ner and allows for much    exible selection!
       comparable to applying id95 to the graph-of-words (aka textrank) but 

taking into account cohesiveness concerns rather than individual prestige 
only!

heuristics: nodes can be selected based on the elbow or top p% method!

[tixier et al., emnlp    16]!

76!

!
corerank     experimental evaluation (1/2)

datasets!
       hulth2003: 500 abstracts from the inspec physics & engineering 
database!
       marujo2012: 450 web news stories covering 10 different topics!
       semeval: 100 scienti   c papers from the acm!

77!

corerank     experimental evaluation (2/2)

       for small documents (i.e., small graphs), the subgraph-level heuristics 
signi   cantly outperform main core retention (main) and textrank (trp, 
tre)!

       recall is drastically improved, precision is maintained (especially with inf)!
       for long documents (semeval), the node-level heuristics are better!
       corerank with top p% retention (crp) reaches best performance!

78!

extractive summarization

79	

extension to extractive document summarization

same as before!

80!

extension to extractive document summarization

same as before!

how to use keywords (and their scores) to select the 

best sentences in a document? !

81!

extractive document summarization (1/4)

       generating a summary in an extractive way is akin to 

selecting the best sentences in the document under a budget 
constraint (max number of words allowed)!

       combinatorial optimization task:!

       s is a given summary (a subset of the set of sentences v)!
       f is the objective function to maximize (measuring summary 

quality)!

       cv is the cost of sentence v (number of words it contains)!
       b is the budget (in words)!

[lin and bilmes, naacl    10]!

82!

extractive document summarization (2/4)

      
      

solving this task is np-complete!
it has been shown that if f is non-decreasing and submodular, a 
greedy !
algorithm can approach the best solution with factor (e     1)/e!

       at each step, the algorithm selects the sentence v that maximizes:!

objective function gain 

scaled cost 
r is a tuning parameter!

      

83!

!
!
!
!
!
!
!
!
!
!
!
!
!
extractive document summarization (3/4)

       the choice of f, the summary quality objective function, is what matters!
       a good summary should cover all the important topics in the document,  

while not repeating itself!
       maximize coverage!
       penalize redundancy (reward diversity to ensure monotonicity) !

weighted sum of the keywords 

contained in the summary!

proportion of unique keywords 

contained!

[lin and bilmes, naacl    10], [meladianos et al., eacl    17]!

84!

extractive document summarization (4/4)
tested for multiparty virtual meetings summarization:!

ami	corpus	

icsi	corpus	

85!

gowvis visualization tool

86	

gowvis visualization tool

[tixier et al., acl    16]!

https://safetyapp.shinyapps.io/gowvis/ !
87!

gowvis

       builds a graph-of-words and displays an interactive 

representation of any text pasted by the user!

       allows the user to tune many parameters:!
       text pre-processing (stopword removal,    )!
       graph building (window size,    )!
       graph mining (node ranking and community detection 

algorithms,    )!

       extracts keyphrases and generates a summary of the input 

text!

       built in r shiny with the visnetwork library!

https://safetyapp.shinyapps.io/gowvis/ !

88!

!
multi-sentence compression 

in word graphs

89	

multi sentence compression/fusion

       setting:  we  are  given  a  group  of  similar  sentences  (e.g.,     rst 
sentence  of  each  article  on  a  google  news  cluster).  each 
sentence  contains  important  bits  of  information.  collectively,  the 
sentences cover everything, but none single sentence 'gets it all   !

       goal:  fuse  the  sentences  into  a  single,  compact  one,  that 
contains as much information as possible while being    uent and 
grammatical!

       method: many approaches can be used. however, it is possible 
to produce excellent results in a fully unsupervised way, with only 
a list of stopwords and a part-of-speech tagger!

[filippova et al., acl    10]!

90!

word-graph sentence compression

1) lonesome george, the world   s last pinta island giant tortoise, has passed away!
2) the giant tortoise known as lonesome george died sunday at the galapagos 
national park in ecuador!
3) he was only about a hundred years old, but the last known giant pinta tortoise, 
lonesome george, has passed away!
4) lonesome george, a giant tortoise believed to be the last of his kind, has died!

91!

!
word-graph construction

       build  a  directed  graph  from  the     rst  sentence,  with  'start'  and  'end'  nodes. 

then, consider each word in the remaining sentences.!

i.   

if the word is not a stopword, and if there is already a node in the graph for 
it (with same lowercased spelling and pos tag), and assuming that no word 
from the same sentence has already been mapped onto the node => map 
word to the node!

       otherwise:!
ii.   

if the word is not a stopword, but there are more than one candidate in the 
graph or multiple occurrences of the word in the sentence!
if the word is a stopword!

iii.   
=>  select  the  candidate  which  has  larger  overlap  in  context  (preceding  and 
following words in sentence and neighbors in the graph), or the node which has 
more words mapped onto it!

92!

word-graph construction

edge weights (the smaller the better):!

where:!

(1)!

(2)!

      
freq(i) is the number of words that have been mapped to node i!
       diff(s,i,j) is the distance between word i and word j in sentence s !

      

intuition for (2): !
       edges between strongly associated words are given more importance, taking into 
account the overall freq. of the nodes (edge freq. of 3 should count more if the edge connects 2 
nodes with freq. 3 rather than with freq. >>3)!

       connections between nodes between which there are multiple paths are also given 

more importance, proportionally to the lengths of the paths!

93!

word-graph construction

edge weights (the smaller the better):!

where:!

(1)!

(2)!

      
freq(i) is the number of words that have been mapped to node i!
       diff(s,i,j) is the distance between word i and word j in sentence s !

      

intuition for (1):!
       eq. (2) is a measure of cohesion between 2 words, but disregards the individual 
importance  of  the  words  =>  we  need  to  take  saliency  into  account.  edges 
connecting two important words are thus favored.!

94!

path ranking and selection

       a k-shortest paths algorithm is applied on the graph to    nd the 50 

paths with smallest edge weights!

       all the paths which are shorter than eight words and do not contain a 

verb are    ltered out!

       the survivors are re-ranked by normalizing the total path weight over 

its length!

       the path which has the lightest average edge weight is    nally 

considered as the best compression!

95!

examples

1) lonesome george, the world   s last pinta island giant tortoise, has passed away!
2) the giant tortoise known as lonesome george died sunday at the galapagos 
national park in ecuador!
3) he was only about a hundred years old, but the last known giant pinta tortoise, 
lonesome george, has passed away!
4) lonesome george, a giant tortoise believed to be the last of his kind, has died!

96!

examples

1) the wife of a former u.s. president bill clinton hillary clinton visited china last monday!
2) hillary clinton wanted to visit china last month but postponed her plans till monday last 
week!
3) hillary clinton paid a visit to the people republic of china on monday!
4) last week the secretary of state ms. clinton visited chinese of   cials!

97!

lessons learned

       syntactic parsers, language models, and/or handcrafted rules are 

not the only way of controlling the grammaticality of the output!

       redundancy  provides  a  reliable  way  of  generating  grammatical 

sentences!

	

98!

!
id37 in text streams

99	

sub-id37 in twitter streams

1.   large volume of documents in social media!
2.   events are not covered by traditional media!
3.   news appear fast in twitter!
4.   is tweet rate suited for sub-id37?!

true events!

falsely estimated events!

contribution:!
a novel real-time detection 
mechanism that accurately 
detects sub-events in an 
unsupervised and out-of-

the-box manner !

[meladianos et al., icwsm    15]!

tweet rate histogram of a football match!

100!

sub-id37 in twitter streams

real time event summarization!

1.    feature extraction: extracts the 

terms that best describe the 
current state of the event!

2.    sub-id37: decides 

whether a sub-event has occurred!

3.    tweet selection: ranks all the 
tweets and selects the    rst one!

system architecture!

      
      

steps are repeated every 60 seconds!
the summary of the whole event is 
constructed by aggregating the individual 
sub-event descriptions!

101!

!
!
!
graph-based representation of tweets

       represents all the input tweets !
       nodes: unique terms !
       edges: #co-occurrences within a 

tweet!

example graph!
1.    good goal by neymar!
2.    goal! neymar scores for brazil!
3.    goal!! neymar scores again!
4.    watching the game tonight!

dataset: tweets from the 2014 fifa 
world cup in brazil!

the graph that was built from 4 tweets!

102!

!
k-core decomposition for feature extraction

       each term is given a score 
corresponding to its core 
number!

       extract the k-core subgraph!
       detect sub-events by 

considering how the sum of 
the core numbers extracted 
from the graph at time t has 
changed from a previous time 
point t-1!

k-core decomposition of the graph-of-words!

103!

!
sub-id37

sub-id37 steps:!
(every 60 seconds)!

core number of term at time slot !
number of terms selected!
decision threshold!
number of previous time slots!

1.    extract the top     terms with highest 

weights!

2.    sum the term weights!
3.    if it exceeds the threshold a sub-

event is detected!

104!

!
!
germany   s goal - 2014 world cup

snapshot of the four highest cores of the graph generated after 

germany   s goal in the 2014 fifa world cup    nal!

105!

tweet selection as sub-event summarization

       activated only if a sub-event has been detected!

       tweets are scored based on the sum of their term weights!

       selects the most informative tweet of the sub-event!

       the tweet with the highest score is chosen!

106!

expreimental setup

sub-id37!

sub-event summarization!

tweet rate!

 term weights!

frequency of terms! core number of terms!

baselines-approaches!
(detection -term weight)!
      rate   freq: the common baseline!
      rate   core!
      weight   core: our approach!
      weight   freq!

107!

dataset

#tweets!

#sub-events!

match!
germany - argentina ! 8!
7!
argentina - belgium !
6!
france - germany !
honduras-switzerland! 7!
10!
greece - ivory coast !
11!
croatia - mexico !
11!
cameroon - brazil !
7!
netherlands - chile !
9!
australia - spain !
8!
germany - ghana !
australia - netherlands ! 11!
all matches!
95!
fifa 2014 world cup dataset!

1,907,999!
1,355,472!
1,321,781!
168,519!
251,420!
600,776!
532,756!
301,067!
252,086!
718,709!
126,971!
7,537,556!

108!

evaluation (1/2)

)

%

(
 
y
t
i
l
i

b
a
b
o
r
p
n
o

 

i
t

t

 

c
e
e
d
d
e
s
s
m

i

)!
r
e

t
t

e
b
 
s
i
 
r
e
w
o
l
(

average det curves over 11 matches 

for the 4 considered approaches!

false alarm id203 (%)!

(lower is better)!

109!

!
evaluation (2/2)

method!

weight-core !

rate-core !

weight-freq!

rate-freq !

micro!
f1-score!
0.68!

macro!
f1-score!
0.72!

0.61!

0.61!

0.54!

0.63!

0.64!

0.60!

average micro and macro f1-score 

over 11 matches for the 4 
considered approaches!

event type!

#actual!
events!
goal          !
32!
penalty    !
2!
red card!
1!
yellow card ! 27!
match start ! 11!
match end ! 11!
11!
half time !

#detected!
events!
30!
2!
0!
14!
8!
11!
10!

number of sub-events detected!

110!

tweet summarization performance

time!

summary!

espn fc!

8   !

45   +2   !

52   !

75   !

90+5' !

goal!!!!argentina!! after eight 
minutes argentina lead belgium by 
1-0 scored by higuain!

goal! argentina 1, belgium 0. gonzalo higuain 
(argentina) right footed shot from the centre of 
the box to the bottom left corner.!

ht: argentina 1-0 belgium. fantastic 
goal by higuain gives argentina the 
slight lead over the red devils.!

first half ends, argentina 1, belgium 0.!

52m - belgium's eden hazard with 
the    rst yellow card of the game!

eden hazard (belgium) is shown the yellow 
card for a bad foul.!

argentina 1 - 0 belgium | biglia 
booked a yellow card. meanwhile, 
chadli on for eden hazard.!

well at least that goal makes them 
advance to the semi    nals. argentina 
gets the ticket to advance and 
belgium goes home.!

lucas biglia (argentina) is shown the yellow 
card for a bad foul. !

match ends, argentina 1, belgium 0.!

summary of the argentina vs. belgium match generated automatically 

using weight   core and manually by espn!

111!

summary

       sub-id37 approach based on the k-core decomposition on 

graph-of-words!

       the algorithm exploits the fact that the vocabulary of tweets gets more 

speci   c when a sub-event occurs!

       the detection mechanism is able to accurately detect important moments 

as they occur!

       the tweets selected by our system give an overview of the event!

112!

tutorial outline

       part i.  graph-theoretic concepts and graph-based text 

representation!

       part ii. information retrieval!
       part iii. keyword extraction and text summarization!
       part iv. text categorization!
       part v. final remarks and future research directions!

113!

text categorization (tc) pipeline

textual 
data!

feature extraction!
term weighting!

model 
learning!

text 

categorization!

evaluation!

main focus 

114!

applications of tc

       applications of text classi   cation are numerous:!

       news    ltering!
       document organization!
       spam detection!
       opinion mining !

       text documents classi   cation compared to other domains:!

       high number of features!
       sparse feature vectors!
       multi-class scenario !
       skewed class distribution !

115!

tc as a graph classi   cation problem

116	

tc as a graph classi   cation problem

       single-label multi-class text categorization !
       graph-of-words representation of textual documents!
       mining of frequent subgraphs as features for 

classi   cation!

       main core retention to reduce the graph   s sizes !
       long-distance id165s more discriminative than 

standard id165s !

[rousseau et al., acl    15]!

117!

background (1/2)

       text categorization!

       standard baseline: unsupervised id165 feature mining + 

supervised linear id166 learning  !

       common approach for spam detection: same with naive bayes !

       id165s to take into account some word order and some 

word dependence as opposed to unigrams !

       word inversion? subset matching? !

[sebastiani, csur    02], [aggarwal and zhai, mining text data    12] !

118!

background (2/2)


       graph classi   cation!

       subgraphs as features!
       graph kernels [vishwanathan et al., jmlr    10] !

       frequent subgraph feature mining !

       gspan [yan and han, icdm    02]!
       fid122 [huan et al., icdm    03]!
       gaston [nijssen and kok, elect. notes tcs    04] !

       expensive to mine all subgraphs, especially for    large    

collections of    large    graphs !

[covered next] 

       unsupervised discriminative feature selection? !

119!

subgraph-of-words 

       a subgraph of size n corresponds to a long-distance id165!

       takes into account word inversion and subset matching !

       for instance, on the r8 dataset, {bank, base, rate} was a 

discriminative (top 5% id166 features) long-distance 3-gram for 
the category    interest   !
          barclays bank cut its base lending rate   !
          midland bank matches its base rate   !
          base rate of natwest bank dropped    !

patterns hard to capture with traditional id165 bag-of-words !

120!

!
!
graph of words classi   cation

unsupervised feature mining and support selection !
       gspan mines the most frequent    subgraph-of-words    in the 

collection of graph-of-words !

       subgraph frequency == long-distance id165 document frequency !
       minimum document frequency controlled via a support parameter !
       the lower the support, the more features but the longer the mining, 

the feature vector generation and the learning !
       unsupervised support selection using the elbow method (inspired from 

selecting the number of clusters in id116) !

[rousseau et al., acl    15]!

121!

multiclass scenario

       text categorization == !

multiple classes + skewed class distribution + single overall 
support value (local frequency)!

       100k features for majority classes vs. 100 features for minority 

ones !

       mining per class with same relative support value !

122!

main core mining and id165 feature selection 

       complexity to extract all features!!

        reduce the size of the graphs!

       maintain word dependence and subset matching     keep the 

densest subgraphs !

       retain the main core of each graph-of-words use gspan to 

mine frequent subgraphs in main cores!

       extract id165 features on remaining text (terms in main 

cores) !

123!

experimental evaluation

       webkb: 4 most frequent categories among labeled webpages from 
various cs departments     split into 2,803 for training and 1,396 for 
test [cardoso-cachopo,    07]!

       r8: 8 most frequent categories of reuters-21578, a set of labeled 
news articles from the 1987 reuters newswire     split into 5,485 for 
training and 2,189 for test [debole and sebastiani,    05]!

       lingspam: 2,893 emails classi   ed as spam or legitimate messages 

    split into 10 sets for 10-fold cross validation [androutsopoulos et al., 
   00]!

       amazon: 8,000 product reviews over four different sub-collections 
(books, dvds, electronics and kitchen appliances) classi   ed as 
posi- tive or negative     split into 1,600 for training and 400 for test 
each [blitzer et al.,    07] !

124!

models

       3 baseline models (id165 features)!

       knn (k=5)!
       multinomial naive bayes (similar results with bernoulli)!
       linear id166 !

       3 proposed approaches!

       gspan + id166 (long-distance id165 features)!
       mc + gspan + id166 (long-distance id165 features) !
       mc + id166 (id165 features) !

125!

id74 

       micro-averaged f1-score (accuracy, overall effectiveness) !
       macro-averaged f1-score (weight each class uniformly) !
       statistical signi   cance of improvement in accuracy over the 

id165 id166 baseline assessed using the micro sign test (p < 
0.05) !

       for the amazon dataset, we report the average of each 

metric over the four sub-collections !

126!

e   ectiveness results (1/2) 

[rousseau et al., acl    15]!

127!

e   ectiveness results (2/2) 

[rousseau et al., acl    15]!

128!

dimension reduction     main core

129!

unsupervised support selection

130!

distribution of mined id165s

figure: distribution of id165s 

(standard and long-distance ones) 

among all!

the features on webkb dataset!

figure: distribution of id165s (standard 

and long-distance ones) among the!
top 5% most discriminative features for 

id166 on webkb dataset!

131!

summary

       explored a graph-based approach, to challenge the 

traditional bag-of-words for text classi   cation!

       first trained a classi   er using frequent subgraphs as features 

for increased effectiveness!

       reduced each graph-of-words to its main core before mining 

the features for increased ef   ciency!

       reduced the total number of id165 features considered in 
the baselines for little to no loss in prediction performances!

132!

id173 for text categorization

133	

id173 for text categorization

       why id173?!
       address over   tting: high training score, low test score!
       better accuracy!

       we want our model to generalize in new unseen test instances!

       harvest the full potential hidden in the rich textual data!

       feed meaningful group of words to group lasso for 

id173!

134!

objective function + loss

       text categorization as a loss minimization problem:!

       id28 with binary predictions (y={-1,1}), h  ,b(x) =     x+b and 
       only minimizing the empirical risk can lead to over   tting.!

l(x,  ,y) = e-yh(x) (log loss)!

       l1, l2 id173 aka lasso [tibshirani,    96] and ridge [hoerl, kennard,    70]!

135!

!
learning

       a constrained optimization problem is formed that can be solved as 

an augmented lagrangian problem:!

       the problem becomes the iterative update of   , v and u:!

      

[yogatama and smith    14] proved that admm for sparse overlapping 
group lasso converges. a good approximate solution is reached in 
a few tens of iterations!

136!

structured id173

      
in l1 and l2 id173, features are considered as independent!
       group lasso: [bakin,    99], [yuan and lin,    06] introduced group sparsity 

in the model:!

137!

!
structured id173 in nlp

statistical regularizers:!
       sentence regularizer (state-of-the-art) (overlapping) !
       large numebr of groups but small group sizes. !

semantic regularizers:!
       lda and lsi regularizers!

words in each group!

       groups are considered the lda/lsi results     keep top10 

graph-of-words regularizers!
       graph-of-words regularizer: community detection on collection 
       id97 regularizer: id116 id91 in id97 space 

graph!

(overlapping)!

trying to extract groups of words that talk about similar topics.  !

[skianis et al., emnlp    16]!

138!

!
!
!
results (1/2)

139!

results (2/2)

140!

summary

       find and extract semantic and syntactic structures that lead to 

sparser feature spaces     faster learning times!

       linguistic prior knowledge in the data can be used to improve 

categorization performance for baseline bag-of-words models, by 
mining inherent structures!

       no signi   cant change in results with different id168s as the 

proposed regularizers are not log loss speci   c!

interesting questions 
       how can we create and cluster graphs, i.e., covering weighted and/or 
       find better clusters in id97? (+overlapping with gmm)!
       explore alternative id173 algorithms diverging from group-lasso?!

signed cases?!

141!

graph representation learning 

with applications in nlp

(text categorization and word analogy)

142	

feature extraction from graphs

       the    rst step of any ml algorithm for graphs is to extract 

graph features!
       node features (e.g., degree)!
       pairs of nodes (e.g., number of common neighbors)!
       groups of nodes (e.g., community assignments)!

|v| x |v|!

feature 

engineering 

input graph!

adjacency matrix!

       link prediction!
       node classi   cation!
       id91!
       anomaly detection!
       attribute prediction!
          !

ml tasks 

143!

graph representation

       create features by transforming the graph into a lower 

dimensional latent representation!

|v| x |v|!

d << |v|!

[f1,    , fd]!

rd

|v|!

input graph!

adjacency matrix!

latent dimensions!

how to learn a latent 
representation of a 

graph?!

       link prediction!
       node classi   cation!
       id91!
       anomaly detection!
       attribute prediction!
          !

ml tasks 

144!

example: community detection

18
18

1
1

22
22

8
8

4
4

12
12

13
13

17
17

6
6

7
7

11
11

5
5

21
21

15
15

31
31

20
20

2
2

16
16

23
23

19
19

30
30

27
27

14
14

3
3

9
9

32
32

33
33

34
34

24
24

10
10

29
29

28
28

26
26

25
25

input graph!

learn latent representation!

other applications: classi   cation, link prediction,    !

[perozzi et al., kdd    14]!

145!

problem statement

       how to embed large networks into low-dimensional spaces?!

       requirements!

       globality/locality: it is desirable to preserve both local and 

global network structure when seeking for node representations!

       scalability: when considering network with millions of nodes 

and billions of edges: traditional methods (nonlinear 
id84) suffer from lack of scalability!

[tang et al.    15]!

146!

intuition     local and global structures

       local structure: observed edges in the network!

       first order proximity!
       most traditional embedding methods (e.g., isomap) capture    rst order 

       global structure: nodes with shared neighbors are likely to be similar 

proximity!

(homophily)!

       nodes 6 and 7:    rst-order proximity!

       should be represented closely in the 
 !

embedded space!

       nodes 5 and 6: second-order proximity!

       same for those nodes!

line algorithm: form an objective function that 
optimizes both local and global network structure!

[tang et al., www    15]!

147!

line with first-order proximity (1/2) 

line with first-order proximity (1/2) 
model the id203 of an edge (i, j) between vi  and vj as!

embeddings space 
1

p1(vi, vj) =

i

   ~uj)

1 + exp( ~ut
~ui 2 rd low dimensional vector 
representation of node vi 
joint id203 between vi and vj 

original (graph) space 

  p1(i, j) =

edge!

wij

p(i,j)2e wij

empirical distribution over 

the space v  v 

logistic function!

find vectors               to make those 
distributions to be as close as possible 

~ui 2 rd

148!

line with first-order proximity (2/2) 

how to preserve    rst-order proximity?!

o1 = d(   p1(  ,  ), p1(  ,  ))

minimize the distance between 

two distributions!

kl(p||q) =xi

p(i) log

p(i)
q(i)

kl-divergence 
o1 = kl(   p1(  ,  ), p1(  ,  ))

o1 =   x(i,j)2e

wij log p1(vi, vj)

by    nding those                  that minimize o1, we can 
represent every node in the d-dimensional space!

{~ui}i=1..|v|

149!

line with second-order proximity (1/3)

      

it assumes that nodes sharing many connections to other nodes are 
similar to each other!

       each node plays two roles: !

       the node itself!
       a speci   c    context    of other nodes!

       for each node vi, we model the conditional distribution p2(!|vi) over 

all the    contexts    (all the nodes in the network)!

       assumption of second-order proximity: nodes with similar 

distributions p2(!|vi)  over the    contexts    are similar!

150!

!
line with second-order proximity (2/3) 

line with first-order proximity (1/2) 
for directed edge (i, j), model the id203 of context vj  generated 
by node vi (i.e., id203 of an edge from vi to vj)!

embeddings space 
exp(~u0t
j

   ~ui)
p2(vj|vi) =
p|v|k=1 exp(~u0t
~ui 2 rd low dimensional vector 
representation of node vi 
conditional distribution p2(!|vi) over 
the contexts  

   ~ui)

k

original (graph) space 

wij
di

out-degree of 

  p2(vj|vi) =
di = xk2n(i)

node i!
empirical distribution over 

wik

the space v  v 

make those distributions to be as 

close as possible 

151!

line with second-order proximity (3/3) 

to preserve second-order proximity, minimize the distance between 
true and empirical distributions!

 id(   p2(  |vi), p2(  |vi))

minimize the distance between 

two distributions!

o2 =xi2v

         i : represents the prestige 
       set   i = di!

of node i in the graph!

kl-divergence 

o2 =   x(i,j)2e

wij log p2(vj|vi)

by    nding those                  and                  that minimize o2, 
we can represent every node i with d-dimensional space       !
~ui

{~u0i}i=1..|v|

{~ui}i=1..|v|

152!

combining both models

       goal: embed the networks by preserving both the    rst-order and 

second-order proximity!

1.    train the line model for    rst-order proximity!
2.    train the line model for second-order proximity!

train separately 

       then, concatenate the embeddings trained by the two methods for 

each node!

153!

experiments - datasets

word co-occurrence 

network!

[tang et al., www    15]!

154!

experiments     word analogy

       language network: word analogy!

   paris   !

       find solution to (   china   ,    beijing          france,    ?   )!
       given id27s,    nd word d* whose embedding ud is 

closest to vector ubeijing     uchina + ufrance!

proximity in terms of 

cosine similarity!

line (2nd) outperforms other embedding methods in the word 

analogy task!

155!

!
experiments     document classi   cation

       classi   cation of wikipedia articles!
       choose articles from 7 categories!

       how to obtain the document vectors for classi   cation?!

       average of the corresponding word vector representations!

156!

!
tutorial outline

       part i.  graph-theoretic concepts and graph-based text 

representation!

       part ii. information retrieval!
       part iii. keyword extraction and text summarization!
       part iv. text categorization!
       part v. final remarks and future research directions!

157!

summary

       graphs have been widely used as modeling tools in !

       nlp !
       id111!
       information retrieval!

       goal of the tutorial!

       presentation of recent methods that rely on graph-based text 

representations to deal with various tasks in nlp and ir!

       focus on the graph-of-words model!
       borrow ideas from the graph mining and network analysis    eld!

158!

thank you! - questions? 

       fragkiskos d. malliaros!

 university of california san diego!
 fmalliaros@ucsd.edu!
 http://fragkiskos.me!

       michalis vazirgiannis!

   cole polytechnique, france!
      mvazirg@lix.polytechnique.fr!

http://www.lix.polytechnique.fr/~mvazirg!

tutorial material: http://fragkiskosm.github.io/projects/graph_text_tutorial 

 

159!

!
references (1/4)

      

      

      

      

      

      

      

      

      

      

boudin, f., and morin, e. 2013. keyphrase extraction for n-best reranking in multi-sentence compression. in 
north american chapter of the association for computational linguistics (naacl).!
mehdad, y., carenini, g., tompa, f. w., and ng, r. t. 2013. abstractive meeting summarization with entailment 
and fusion. in proc. of the 14th european workshop on id86 (pp. 136-146).!
manu aery and sharma chakravarthy. 2005. infosift: adapting graph mining techniques for text classi   cation. in 
flairs, pages 277   282.!
ricardo a. baeza-yates and berthier ribeiro-neto.1999. modern information retrieval. addison-wesley 
longman publishing co., inc., boston, ma, usa.!
roi blanco and christina lioma. 2012. graph-based term weighting for information retrieval. inf. retr., 15(1):54   
92.!
florian boudin. 2013. a comparison of centrality measures for graph-based keyphrase extraction (ijcnlp    13). in 
sixth international joint conference on natural language processing, pages 834   838.!
adrien bougouin, florian boudin, and b  eatrice daille. 2013. topicrank: graph-based topic ranking for 
keyphrase extraction. in sixth international joint conference on natural language processing (ijcnlp    13), 
pages 543   551.!
adrien bougouin, florian boudin, and b  eatrice daille. 2016. keyphrase annotation with graph co-ranking. in 
26th international conference on computational linguistics (coling    16).!
gunes erkan and dragomir r radev. 2004. lexrank: graph-based lexical centrality as salience in text 
summarization. journal of arti   cial intelligence research, pages 457   479.!
filippova, k. 2010. multi-sentence compression: finding shortest paths in word graphs. in proceedings of the 
23rd international conference on computational linguistics (pp. 322-330). association for computational 
linguistics.!

160!

references (2/4)

      

      

      

      

      

      

      

      

maria grineva, maxim grinev, and dmitry lizorkin. 2009. extracting key terms from noisy and multitheme 
documents. in proceedings of the 18th international conference on world wide web (www    09) , pages 661   
670. acm.!
samer hassan, rada mihalcea, and carmen banea. 2007. random-walk term weighting for improved text 
classi   cation. in icsc, pages 242   249.!
chuntao jiang, frans coenen, robert sanderson, and michele zito. 2010. text classi   cation using graph mining-
based feature extraction. knowl.- based syst., 23(4):302   308.!
marina litvak and mark last. 2008. graphbased keyword extraction for single-document summarization. in 
proceedings of the workshop on multi-source multilingual information extraction and summarization (mmies 
   08), pages 17   24. association for computational linguistics.!
marina litvak, mark last, hen aizenman, inbal gobits, and abraham kandel. 2011. degext     a language-
independent graph-based keyphrase extractor. in elena mugellini, piotr s. szczepaniak, maria chiara pettenati, 
and maria sokhn, editors, proceedings of the 7th atlantic web intelligence conference (awic    08), pages 121   
130.!
fragkiskos d. malliaros and konstantinos skianis. 2015. graph-based term weighting for text categorization. in 
asonam, pages 1473   1479. acm.!
alex markov, mark last, and abraham kandel. 2007. fast categorization of web documents represented by 
graphs. in advances in web mining and web usage analysis, volume 4811, pages 56   71.!
polykarpos meladianos, giannis nikolentzos, franc  ois rousseau, yannis stavrakas, and michalis vazirgiannis. 
2015. degeneracy-based real-time subid37 in twitter stream. in ninth international aaai conference 
on web and social media (icwsm    15).!

161!

references (3/4)

      

      

      

      

      

      
      

      

      

rada mihalcea and paul tarau. 2004. textrank: bringing order into texts. in proceedings of the 2004 
conference on empirical methods in natural language processing (emnlp    04). association for computational 
linguistics.!
francois rousseau and michalis vazirgiannis. 2013. graph-of-word and tw-idf: new approach to ad hoc ir. in 
proceedings of the 22nd acm international conference on conference on information & knowledge management 
(cikm    13), pages 59   68. acm.!
francois rousseau and michalis vazirgiannis. 2015. main core retention on graph-of-words for singledocument 
keyword extraction. in european conference on information retrieval (ecir    15), pages 382   393. springer.!
francois rousseau, emmanouil kiagias, and michalis vazirgiannis. 2015. text categorization as a graph 
classi   cation problem. in acl (1), pages 1702    1712. the association for computer linguistics.!
francois rousseau. 2015. graph-of-words: mining and retrieving text with networks of features. ph.d. thesis, 
ecole polytechnique.!
stephen b. seidman. 1983. network structure and minimum degree. social networks, 5:269   287.!
konstantinos skianis, francois rousseau, and michalis vazirgiannis. 2016. regularizing text categorization with 
clusters of words. in emnlp, pages 1827    1837. the association for computational linguistics.!
shashank srivastava, dirk hovy, and eduard h. hovy. 2013. a walk-based semantically enriched tree kernel over 
distributed word representations. in proceedings of the 2013 conference on empirical methods in natural 
language processing (emnlp    13), pages 1411   1416.!
jian tang, meng qu, mingzhe wang, ming zhang, jun yan, qiaozhu mei. 2015. line: large-scale information 
network embedding. in proceedings of the 24th international conference on world wide web (www    15), pages 
1067-1077!

162!

references (4/4)

      

      

      

      

antoine j.-p. tixier, fragkiskos d. malliaros, and michalis vazirgiannis. 2016a. a graph degeneracybased 
approach to keyword extraction. in proceedings of the 2016 conference on empirical methods in natural 
language processing (emnlp    16), pages 1860   1870. the association for computational linguistics.!
antoine j.-p. tixier, konstantinos skianis, and michalis vazirgiannis. 2016b. gowvis: a web application for graph-
of-words-based text visualization and summarization. in acl. the association for computational linguistics.!
s. v. n. vishwanathan, nicol n. schraudolph, risi kondor, and karsten m. borgwardt. 2010. graph kernels. j. 
mach. learn. res., 11:1201   1242.!
jia wang and james cheng. 2012. truss decomposition in massive networks. proc. vldb endow., 5(9):812   
823.!

       wei wang, diepbich do, and xuemin lin. 2005. term graph model for text classi   cation. in advanced data 

mining and applications, volume 3584, pages 19   30.!
rui wang, wei liu, and chris mcdonald. 2015. corpus-independent generic keyphrase extraction using word 
embedding vectors. in workshop on deep learning for web search and data mining (dl-wsdm    15).!
xifeng yan and jiawei han. 2002. gspan: graphbased substructure pattern mining. in proceedings of the 2002 
ieee international conference on data mining (icdm    02).!

      

      

163!

