gsl_stats march 24, 2009

modeling with data

gsl_stats march 24, 2009

gsl_stats march 24, 2009

modeling with data

tools and techniques for scienti   c computing

ben klemens

princeton university press

princeton and oxford

gsl_stats march 24, 2009

copyright    2009 by princeton university press

published by princeton university press
41 william street, princeton, new jersey 08540

in the united kingdom: princeton university press
6 oxford street, woodstock, oxfordshire, ox20 1tw

all rights reserved

klemens, ben.

modeling with data : tools and techniques for scienti   c computing / ben klemens.

p. cm.

includes bibliographical references and index.
isbn 978-0-691-13314-0 (hardcover : alk. paper)
1. mathematical statistics. 2. mathematical models. i. title.
qa276.k546 2009
519.5   dc22

2008028341

british library cataloging-in-publication data is available

this book has been composed in latex

the publisher would like to acknowledge the author of this volume for providing the camera-ready
copy from which this book was printed.

printed on acid-free paper.    

press.princeton.edu

printed in the united states of america

10 9 8 7 6 5 4 3 2 1

gsl_stats march 24, 2009

we believe that no one should be deprived of books for any reason.

   russell wattenberg, founder of the book thing

the author pledges to donate 25% of his royalties to the book thing

of baltimore, a non-pro   t that gives books to schools, students,

libraries, and other readers of all kinds.

gsl_stats march 24, 2009

gsl_stats march 24, 2009

preface

chapter 1. statistics in the modern day

part i computing

chapter 2. c

2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9

3.1
3.2
3.3
3.4
3.5

lines
variables and their declarations
functions
the debugger
compiling and running
pointers
arrays and other pointer tricks
strings

  errors
  doing more with queries

basic queries

joins and subqueries
on database design
folding queries into c code

chapter 3. databases

contents

xi

1

15

17

18

28

34

43

48

53

59

65

69

74

76

80

87

94

98

gsl_stats march 24, 2009

viii

3.6
3.7

models

chapter 5. graphics

4.1
4.2
4.3
4.4
4.5
4.6
4.7

5.1
5.2
5.3
5.4
5.5
5.6
5.7
5.8

the gsl   s matrices and vectors

chapter 4. matrices and models

maddening details
some examples

shunting data
id202
numbers

from arrays to plots
a sampling of special plots
animation
on producing good plots

a   _da a
 g  _ a  ix andg  _ve
    internals
    
  some common settings
  graphs   nodes and    owcharts
  printing and latex
chapter 6.  more coding tools
  syntactic sugar
  principal component analysis

moments
sample distributions
using the sample distributions
non-parametric description

function pointers
data structures
parameters

chapter 7. distributions for description

chapter 8. linear projections

6.1
6.2
6.3
6.4
6.5

part ii statistics

7.1
7.2
7.3
7.4

more tools

8.1
8.2
8.3
8.4

ols and friends
discrete variables
multilevel modeling

contents

103

108

113

114

120

123

129

135

140

143

157

160

163

166

171

177

180

182

185

189

190

193

203

210

214

217

219

219

235

252

261

264

265

270

280

288

gsl_stats march 24, 2009

contents

chapter 9. hypothesis testing with the clt

9.1
9.2
9.3
9.4
9.5
9.6

the central limit theorem
meet the gaussian family
testing a hypothesis
anova
regression
goodness of    t

chapter 10. id113

10.1
10.2
10.3
10.4

log likelihood and friends
description: maximum likelihood estimators
missing data
testing with likelihoods

chapter 11. monte carlo

11.1
11.2
11.3
11.4
11.5

random number generation
description: finding statistics for a distribution
id136: finding statistics for a parameter
drawing a distribution
non-parametric testing

appendix a: environments and make   les

a.1
a.2
a.3

environment variables
paths
make

appendix b: text processing

b.1
b.2
b.3
b.4
b.5

shell scripts
some tools for scripting
id157
adding and deleting
more examples

appendix c: glossary

bibliography

index

ix

295

297

301

307

312

315

319

325

326

337

345

348

356

357

364

367

371

375

381

381

385

387

392

393

398

403

413

415

419

435

443

gsl_stats march 24, 2009

gsl_stats march 24, 2009

preface

mathematics provides a framework for dealing precisely with notions of    what is.   
computation provides a framework for dealing precisely with notions of    how to.   

   alan j perlis, in abelson et al. (1985, p xvi)

should you use this book? this book is intended to be a complement to the

standard stats textbook, in three ways.

first, descriptive and inferential statistics are kept separate beginning with the    rst
sentence of the    rst chapter. i believe that the fusing of the two is the number one
cause of confusion among statistics students.

once descriptive modeling is given its own space, and models do not necessarily
have to be just preparation for a test, the options blossom. there are myriad ways
to convert a subjective understanding of the world into a mathematical model,
including simulations, models like the bernoulli/poisson distributions from tradi-
tional id203 theory, ordinary least squares, and who knows what else.

if those options aren   t enough, simple models can be combined to form multi-
level models to describe situations of arbitrary complexity. that is, the basic linear
model or the bernoulli/poisson models may seem too simple for many situations,
but they are building blocks that let us produce more descriptive models. the over-
all approach concludes with multilevel models as in, e.g., eliason (1993), pawitan
(2001) or gelman & hill (2007).

gsl_stats march 24, 2009

xii

preface

second, many stats texts aim to be as complete as possible, because completeness
and a thick spine give the impression of value-for-money: you get a textbook and
a reference book, so everything you need is guaranteed to be in there somewhere.

but it   s hard to learn from a reference book. so i have made a solid effort to provide
a narrative to the important points about statistics, even though that directly implies
that this book is incomplete relative to the more encyclop  dic texts. for example,
moment generating functions are an interesting narrative on their own, but they are
tangential to the story here, so i do not mention them.

computation

the third manner in which this book complements the traditional
stats textbook is that it acknowledges that if you are working with
data full time, then you are working on a computer full time. the better you un-
derstand computing, the more you will be able to do with your data, and the faster
you will be able to do it.

the politics of software

all of the software in this book is free software,
meaning that it may be freely downloaded and dis-
tributed. this is because the book focuses on porta-
bility and replicability, and if you need to purchase a
license every time you switch computers, then the code
is not portable.

if you redistribute a functioning program that you
wrote based on the gsl or apophenia, then you need
to redistribute both the compiled    nal program and the
source code you used to write the program. if you are
publishing an academic work, you should be doing this
anyway. if you are in a situation where you will dis-
tribute only the output of an analysis, there are no obli-
gations at all.

people like to characterize comput-
ing as fast-paced and ever-changing,
but much of that is just churn on the
syntactic surface. the fundamental
concepts, conceived by mathemati-
cians with an eye toward the sim-
plicity and elegance of pencil-and-
paper math, have been around for
as long as anybody can remember.
time spent learning those funda-
mentals will pay off no matter what
exciting new language everybody
happens to be using this month.

this book is also reliant on posix-compliant sys-
tems, because such systems were built from the ground
up for writing and running replicable and portable
projects. this does not exclude any current operating
system (os): current members of the microsoft win-
dows family of oses claim posix compliance, as do
all oses ending in x (mac os x, linux, unix, . . . ).

i spent much of my life ignor-
ing the fundamentals of computing
and just hacking together projects
using the package or language of
the month: c++, mathematica, oc-
tave, perl, python, java, scheme, s-
plus, stata, r, and probably a few
others that i   ve forgotten. albee (1960, p 30) explains that    sometimes it   s neces-
sary to go a long distance out of the way in order to come back a short distance
correctly;    this is the distance i   ve gone to arrive at writing a book on data-oriented
computing using a general and basic computing language. for the purpose of mod-
eling with data, i have found c to be an easier and more pleasant language than the
purpose-built alternatives   especially after i worked out that i could ignore much
of the advice from books written in the 1980s and apply the techniques i learned
from the scripting languages.

gsl_stats march 24, 2009

preface

xiii

what is the level of this book? the short answer is that this is intended
for the graduate student or independent re-
searcher, either as a supplement to a standard    rst-year stats text or for later study.
here are a few more ways to answer that question:

    ease of use versus ease of initial use: the majority of statistics students are just
trying to slog through their department   s stats requirement so they can never look
at another data set again. if that is you, then your sole concern is ease of initial use,
and you want a stats package and a textbook that focus less on full pro   ciency and
more on immediate intuition.1
conversely, this book is not really about solving today   s problem as quickly as
physically possible, but about getting a better understanding of data handling, com-
puting, and statistics. ease of long-term use will follow therefrom.

    level of computing abstraction: this book takes the fundamentals of computing
seriously, but it is not about reinventing the wheels of statistical computing. for
example, numerical recipes in c (press et al., 1988) is a classic text describing the
algorithms for seeking optima, ef   ciently calculating determinants, and making
random draws from a normal distribution. being such a classic, there are many
packages that implement algorithms on its level, and this book will build upon
those packages rather than replicate their effort.

    computing experience: you may have never taken a computer science course, but
do have some experience in both the basics of dealing with a computer and in
writing scripts in either a stats package or a scripting language like perl or python.
    computational detail: this book includes about 80 working sample programs.
code clari   es everything: english text may have a few ambiguities, but all the
details have to be in place for a program to execute correctly. also, code rewards
the curious, because readers can explore the data,    nd out to what changes a pro-
cedure is robust, and otherwise productively break the code.
that means that this book is not computing-system-agnostic. so if you are a devo-
tee of a stats package not used here, then why look at this book? although i do
not shy away from c-speci   c details of syntax, most of the book focuses on the
conceptual issues common to all computing environments. if you never look at c
code again after you    nish this book, you will still have a better grounding for
effectively working in your preferred programming language.

    id202: you are reasonably familiar with id202, such that an ex-
pression like x   1 is not foreign to you. there are a countably in   nite number of
id202 tutorials in books, stats text appendices, and online, so this book
does not include yet another.

    statistical topics: the book   s statistical topics are not particularly advanced or
trendy: ols, maximum likelihood, or id64 are all staples of    rst-year
grad-level stats. but by creatively combining building blocks such as these, you
will be able to model data and situations of arbitrary complexity.

1i myself learned a few things from the excellently written narrative in gonick & smith (1994).

gsl_stats march 24, 2009

gsl_stats march 24, 2009

modeling with data

gsl_stats march 24, 2009

gsl_stats march 24, 2009

1

statistics in the modern day

retake the falling snow: each drifting    ake
shapeless and slow, unsteady and opaque,
a dull dark white against the day   s pale white
and abstract larches in the neutral light.

   nabokov (1962, lines 13   16)

statistical analysis has two goals, which directly con   ict. the    rst is to    nd pat-
terns in static: given the in   nite number of variables that one could observe, how
can one discover the relations and patterns that make human sense? the second
goal is a    ght against apophenia, the human tendency to invent patterns in random
static. given that someone has found a pattern regarding a handful of variables,
how can one verify that it is not just the product of a lucky draw or an overactive
imagination?

or, consider the complementary dichotomy of objective versus subjective. the
objective side is often called id203; e.g., given the assumptions of the central
limit theorem, its conclusion is true with mathematical certainty. the subjective
side is often called statistics; e.g., our claim that observed quantity a is a linear
function of observed quantity b may be very useful, but nature has no interest in
it.

this book is about writing down subjective models based on our human under-
standing of how the world works, but which are heavily advised by objective in-
formation, including both mathematical theorems and observed data.1

1of course, human-gathered data is never perfectly objective, but we all try our best to make it so.

gsl_stats march 24, 2009

2

chapter 1

the typical scheme begins by proposing a model of the world, then estimating the
parameters of the model using the observed data, and then evaluating the    t of the
model. this scheme includes both a descriptive step (describing a pattern) and an
inferential step (testing whether there are indications that the pattern is valid). it
begins with a subjective model, but is heavily advised by objective data.

figure 1.1 shows a model in    owchart form. first, the descriptive step: data and
parameters are fed into a function   which may be as simple as a is correlated
to b, or may be a complex set of interrelations   and the function spits out some
output. then comes the testing step: evaluating the output based on some criterion,
typically regarding how well it matches some portion of the data. our goal is to
   nd those parameters that produce output that best meets our evaluation criterion.

data

function

output

evaluation

figure 1.1 a    owchart for distribution    tting, id75, maximum likelihood methods,
multilevel modeling, simulation (including agent-based modeling), data mining, non-
parametric modeling, and various other methods. [online source for the diagram:

the ordinary least squares (ols) model is a popular and familiar example, pic-
tured in figure 1.2. [if it is not familiar to you, we will cover it in chapter 8.] let
x indicate the independent data,    the parameters, and y the dependent data. then
the function box consists of the simple equation yout = x  , and the evaluation
step seeks to minimize squared error, (y     yout)2.

x

  

y

x  

yout

(y     yout)2

figure 1.2 the ols model: a special case of figure 1.1.

for a simulation, the function box may be a complex    owchart in which variables
are combined non-linearly with parameters, then feed back upon each other in
unpredictable ways. the    nal step would evaluate how well the simulation output
corresponds to the real-world phenomenon to be explained.

the key computational problem of statistical modeling is to    nd the parameters at

data

parameters

  de  .d  .]

gsl_stats march 24, 2009

statistics in the modern day

3

the beginning of the    owchart that will output the best evaluation at the end. that
is, for a given function and evaluation in figure 1.1, we seek a routine to take in
data and produce the optimal parameters, as in figure 1.3. in the ols model above,
there is a simple, one-equation solution to the problem:   best = (x   x)   1x   y.
but for more complex models, such as simulations or many multilevel models, we
must strategically try different sets of parameters to hunt for the best ones.

data

estimation

parameters

x, y

(x   x)   1x   y

  best

figure 1.3 top: the parameters which are the input for the model in figure 1.1 are the output for the

estimation routine.
bottom: the estimation of the ols model is a simple equation.

and that   s the whole book: develop models whose parameters and tests may dis-
cover and verify interesting patterns in the data. but the setup is incredibly versa-
tile, and with different function speci   cations, the setup takes many forms. among
a few minor asides, this book will cover the following topics, all of which are vari-
ants of figure 1.1:

    id203: how well-known distributions can be used to model data

    projections: summarizing many-dimensional data in two or three dimensions

    estimating linear models such as ols

    classical hypothesis testing: using the central limit theorem (clt) to ferret out

apophenia

    designing multilevel models, where one model   s output is the input to a parent

model

    id113

    hypothesis testing using likelihood ratio tests

    monte carlo methods for describing parameters

       nonparametric    modeling (which comfortably    ts into the parametric form here),

such as smoothing data distributions

    id64 to describe parameters and test hypotheses

gsl_stats march 24, 2009

4

chapter 1

the snowflake problem, or a brief
history of statistical computing

the simplest models in the above list
have only one or two parameters, like
a binomial(n, p) distribution which is
built from n identical draws, each of which is a success with id203 p [see
chapter 7]. but draws in the real world are rarely identical   no two snow   akes
are exactly alike. it would be nice if an outcome variable, like annual income, were
determined entirely by one variable (like education), but we know that a few dozen
more enter into the picture (like age, race, marital status, geographical location, et
cetera).

the problem is to design a model that accommodates that sort of complexity, in
a manner that allows us to actually compute results. before computers were com-
mon, the best we could do was analysis of variance methods (anova), which
ascribed variation to a few potential causes [see sections 7.1.3 and 9.4].

the    rst computational milestone, circa the early 1970s, arrived when civilian
computers had the power to easily invert matrices, a process that is necessary for
most linear models. the linear models such as ordinary least squares then became
dominant [see chapter 8].

the second milestone, circa the mid 1990s, arrived when desktop computing power
was suf   cient to easily gather enough local information to pin down the global op-
timum of a complex function   perhaps thousands or millions of evaluations of the
function. the functions that these methods can handle are much more general than
the linear models: you can now write and optimize models with millions of inter-
acting agents or functions consisting of the sum of a thousand sub-distributions
[see chapter 10].

the ironic result of such computational power is that it allows us to return to the
simple models like the binomial distribution. but instead of specifying a    xed n
and p for the entire population, every observation could take on a value of n that is
a function of the individual   s age, race, et cetera, and a value of p that is a different
function of age, race, et cetera [see section 8.4].

the models in part ii are listed more-or-less in order of complexity. the in   nitely
quotable albert einstein advised,    make everything as simple as possible, but not
simpler.    the central limit theorem tells us that errors often are normally dis-
tributed, and it is often the case that the dependent variable is basically a linear or
log-linear function of several variables. if such descriptions do no violence to the
reality from which the data were culled, then ols is the method to use, and using
more general techniques will not be any more persuasive. but if these assumptions
do not apply, we no longer need to assume linearity to overcome the snow   ake
problem.

gsl_stats march 24, 2009

statistics in the modern day

5

the pipeline a statistical analysis is a guided series of transformations of the data
from its raw form as originally written down to a simple summary

regarding a question of interest.

the    ow above, in the statistics textbook tradition, picked up halfway through the
analysis: it assumes a data set that is in the correct form. but the full pipeline goes
from the original messy data set to a    nal estimation of a statistical model. it is
built from functions that each incrementally transform the data in some manner,
like removing missing data, selecting a subset of the data, or summarizing it into a
single statistic like a mean or variance.

thus, you can think of this book as a catalog of pipe sections and    lters, plus
a discussion of how to    t elements together to form a stream from raw data to
   nal publishable output. as well as the pipe sections listed above, such as the
ordinary least squares or maximum likelihood procedures, the book also covers
several techniques for directly transforming data, computing statistics, and welding
all these sections into a full program:

    structuring programs using modular functions and the stack of frames
    programming tools like the debugger and pro   ler

    methods for reliability testing functions and making them more robust

    databases, and how to get them to produce data in the format you need

model from data.

tions of your data

    talking to external programs, like graphics packages that will generate visualiza-

    optimization routines: how they work and how to use them

    finding and using pre-existing functions to quickly estimate the parameters of a

is available from the book   s web site, linked fromh   ://  e  .  i 
e   .
ed	/ i  e /8706.h   . this means that you can learn by running and modify-

to make things still more concrete, almost all of the sample code in this book

    monte carlo methods: getting a picture of a model via millions of random draws

ing the examples, or you can cut, paste, and modify the sample code to get your
own analyses running more quickly. the programs are listed and given a complete
discussion on the pages of this book, so you can read it on the bus or at the beach,
but you are very much encouraged to read through this book while sitting at your
computer, where you can run the sample code, see what happens given different
settings, and otherwise explore.

figure 1.4 gives a typical pipeline from raw data to    nal paper. it works at a number
of different layers of abstraction: some segments involve manipulating individual
numbers, some segments take low-level numerical manipulation as given and op-

gsl_stats march 24, 2009

6

input
data

chapter 1

appendix b

figure 1.4 filtering from input data to outputs. [online source:da afi  e i g.d  ]

database

c matrix

part ii

ch 5

ch 3

sql

plots

output

parameters

and graphs

erate on database tables or matrices, and some segments take matrix operations as
given and run higher-level hypothesis tests.

the lowest level

chapter 2 presents a tutorial on the c programming language it-
self. the work here is at the lowest level of abstraction, covering
nothing more dif   cult than adding columns of numbers. the chapter also discusses
how c facilitates the development and use of libraries: sets of functions written by
past programmers that provide the tools to do work at higher and higher levels of
abstraction (and thus ignore details at lower levels).2

for a number of reasons to be discussed below, the book relies on the c program-
ming language for most of the pipe-   tting, but if there is a certain section that
you    nd useful (the appendices and the chapter on databases comes to mind) then
there is nothing keeping you from welding that pipe section to others using another
programming language or system.

dealing with large data sets

computers today are able to crunch numbers a hun-
dred times faster they did a decade ago   but the data
sets they have to crunch are a thousand times larger. geneticists routinely pull
550,000 genetic markers each from a hundred or a thousand patients. the us
census bureau   s 1% sample covers almost 3 million people. thus, the next layer
of abstraction provides specialized tools for dealing with data sets: databases and
a query language for organizing data. chapter 3 presents a new syntax for talking
to a database, structured query language (sql). you will    nd that many types
of data manipulation and    ltering that are dif   cult in traditional languages or stats
packages are trivial   even pleasant   via sql.

of c here introduces tools like package managers, the debugger, and the ake utility as early as possible, so you

2why does the book omit a id202 tutorial but include an extensive c tutorial? primarily because the
use of id202 has not changed much this century, while the use of c has evolved as more libraries have
become available. if you were writing c code in the early 1980s, you were using only the standard library and
thus writing at a very low level. in the present day, the process of writing code is more about joining together
libraries than writing from scratch. i felt that existing c tutorials and books focused too heavily on the process of
writing from scratch, perpetuating the myth that c is appropriate only for low-level bit shifting. the discussion

can start calling existing libraries as quickly and easily as possible.

gsl_stats march 24, 2009

statistics in the modern day

7

as huber (2000, p 619) explains:    large real-life problems always require a com-
bination of database management and data analysis. . . . neither database manage-
ment systems nor traditional statistical packages are up to the task.    the solution is
to build a pipeline, as per figure 1.4, that includes both database management and
statistical analysis sections. much of graceful data handling is in knowing where
along the pipeline to place a    ltering operation. the database is the appropriate
place to    lter out bad data, join together data from multiple sources, and aggregate
data into group means and sums. c matrices are appropriate for    ltering operations
like those from earlier that took in data, applied a function like (x   x)   1x   y, and
then measured (yout     y)2.
because your data probably did not come pre-loaded into a database, appendix
b discusses text manipulation techniques, so when the database expects your data
set to use commas but your data is separated by erratic tabs, you will be able to
quickly surmount the problem and move on to analysis.

computation

the gnu scienti   c library works at the numerical computation
layer of abstraction. it includes tools for all of the procedures com-
monly used in statistics, such as id202 operations, looking up the value
of f , t,   2 distributions, and    nding maxima of likelihood functions. chapter 4
presents some basics for data-oriented use of the gsl.

the apophenia library, primarily covered in chapter 4, builds upon these other
layers of abstraction to provide functions at the level of data analysis, model    tting,
and hypothesis testing.

pretty pictures good pictures can be essential to good research. they often reveal
patterns in data that look like mere static when that data is pre-
sented as a table of numbers, and are an effective means of communicating with
peers and persuading grantmakers. consistent with the rest of this book, chapter 5
will cover the use of gnuplot and graphviz, two packages that are freely available
for the computer you are using right now. both are entirely automatable, so once
you have a graph or plot you like, you can have your c programs autogenerate
it or manipulate it in amusing ways, or can send your program to your colleague
in madras and he will have no problem reproducing and modifying your plots.3
once you have the basics down, animation and real-time graphics for simulations
are easy.

3following a suggestion by thomson (2001), i have chosen the gender of representative agents in this book

by    ipping a coin.

gsl_stats march 24, 2009

8

chapter 1

why c? you may be surprised to see a book about modern statistical computing
based on a language composed in 1972. why use c instead of a special-
ized language or package like sas, stata, spss, s-plus, sage, siena, su-
daan, systat, sst, shazam, j, k, gauss, gams, glim, genstat,
gretl, eviews, egret, eqs, pcgive, matlab, minitab, mupad, maple, mplus,
maxima, mln, mathematica, winbugs, tsp, hlm, r, rats, lisrel, lisp-
stat, limdep, bmdp, octave, orange, oxmetrics, weka, or yorick? this may
be the only book to advocate statistical computing with a general computing lan-
guage, so i will take some time to give you a better idea of why modern numerical
analysis is best done in an old language.

one of the side effects of a programming language being stable for so long is that a
mythology builds around it. sometimes the mythology is outdated or false: i have
seen professional computer programmers and writers claim that simple structures
like linked lists always need to be written from scratch in c (see section 6.2 for
proof otherwise), that it takes ten to a hundred times as long to write a program in
c than in a more recently-written language like r, or that because people have used
c to write device drivers or other low-level work, it can not be used for high-level
work.4 this section is partly intended to dispel such myths.

is c a hard language?

c was a hard language. with nothing but a basic 80s-era
compiler, you could easily make many hard-to-catch mis-
takes. but programmers have had a few decades to identify those pitfalls and build
tools to catch them. modern compilers warn you of these issues, and debuggers let
you interact with your program as it runs to catch more quirks. c   s reputation as a
hard language means the tools around it have evolved to make it an easy language.

computational speed   really using a stats package sure beats inverting matri-
ces by hand, but as computation goes, many stats
packages are still relatively slow, and that slowness can make otherwise useful
statistical methods infeasible.

r and apophenia use the same c code for doing the fisher exact test, so it makes
a good basis for a timing test.5 listings 1.5 and 1.6 show programs in c and r
(respectively) that will run a fisher exact test    ve million times on the same data
set. you can see that the c program is a bit more verbose: the steps taken in lines
3   8 of the c code and lines 1   6 of the r code are identical, but those lines are

5that is, if you download the source code for r   sfi he . e   function, you will    nd a set of procedures
written in c. save for a few minor modi   cations, the code underlying thea   _ e  _fi he _exa
  function

4out of courtesy, citations are omitted. this section makes frequent comparisons to r partly because it is a
salient and common stats package, and partly because i know it well, having used it on a daily basis for several
years.

is line-for-line identical.

gsl_stats march 24, 2009

statistics in the modern day

9

1
2
3
4
5
6
7
8
9

1
2
3
4
5
6
7

listing 1.5 c code to time a fisher exact test. it runs the same test    ve million times. online source:

#include <apop.h>
int main(){

24, 38 };

apop_test_   sher_exact(testdata);

int i, test_ct = 5e6;
double data[] = { 30, 86,

apop_data *testdata = apop_line_to_data(data,0,2,2);
for (i = 0; i< test_ct; i++)

 i efi he .
.
listing 1.6 r code to do the same test as listing 1.5. online source:r i efi he .

   sher.test(testdata)

testdata<    matrix(data, nrow=2)
for (i in 1:test_ct){

test_ct <    5e6
data <    c( 30, 86,
24, 38 )

}

}

longer in c, and the c program has some preliminary code that the r script does
not have.

on my laptop, listing 1.5 runs in under three minutes, while listing 1.6 does the
same work in 89 minutes   about thirty times as long. so the investment of a little
more verbosity and a few extra stars and semicolons returns a thirty-fold speed
gain.6 nor is this an isolated test case: i can   t count how many times people have
told me stories about an analysis or simulation that took days or weeks in a stats
package but ran in minutes after they rewrote it in c.

even for moderately-sized data sets, real computing speed opens up new possibili-
ties, because we can drop the (typically false) assumptions needed for closed-form
solutions in favor of maximum likelihood or monte carlo methods. the monte
carlo examples in section 11.2 were produced using over a billion draws from t
distributions; if your stats package can   t produce a few hundred thousand draws
per second (some can   t), such work will be unfeasibly slow.7

6these timings are actually based on a modi   ed version offi he . e   that omits some additional r-side
calculations. if you had to put a fisher test in af   loop without    rst editing r   s code, the r-to-c speed ratio
7if you can produce random draws from t distributions as a batch (d aw <	   5e6 df ), then r takes a
mere 3.5 times as long as comparable c code. but if you need to produce them individually (f   ii 1:5e6 
{d aw<	   1 df }), then r takes about    fteen times as long as comparable c code. on my laptop, r in

would be between    fty and a hundred.

gsl_stats march 24, 2009

10

chapter 1

simplicity

c is a super-simple language. its syntax has no special tricks for poly-
morphic operators, abstract classes, virtual inheritance, lexical scoping,
lambda expressions, or other such arcana, meaning that you have less to learn.
those features are certainly helpful in their place, but without them c has already
proven to be suf   cient for writing some impressive programs, like the mac and
linux operating systems and most of the stats packages listed above.

simplicity affords stability   c is among the oldest programming languages in
common use today8   and stability brings its own bene   ts. first, you are reason-
ably assured that you will be able to verify and modify your work    ve or even
ten years from now. since c was written in 1972, countless stats packages have
come and gone, while others are still around but have made so many changes in
syntax that they are effectively new languages. either way, those who try to follow
the trends have on their hard drives dozens of scripts that they can   t run anymore.
meanwhile, correctly written c programs from the 1970s will compile and run on
new pcs.

second, people have had a few decades to write good libraries, and libraries that
build upon those libraries. it is not the syntax of a language that allows you to easily
handle complex structures and tasks, but the vocabulary, which in the case of c is
continually being expanded by new function libraries. with a statistics library on
hand, the c code in listing 1.5 and the r code in listing 1.6 work at the same high
level of abstraction.

alternatively, if you need more precision, you can use c   s low-level bit-twiddling
to shunt individual elements of data. there is nothing more embarrassing than a
presenter who answers a question about an anomaly in the data or analysis with
   stata didn   t have a function to correct that.    [yes, i have heard this in a real live
presentation by a real live researcher.] but since c   s higher-level and lower-level
libraries are equally accessible, you can work at the level of laziness or precision
called for in any given situation.

interacting with c scripts many of the stats packages listed above provide a pleas-
ing interface that let you run regressions with just a few
mouse-clicks. such systems are certainly useful for certain settings, such as ask-
ing a few quick questions of a new data set. but an un-replicable analysis based
on clicking an arbitrary sequence of on-screen buttons is as useful as no analysis
at all. in the context of building a repeatable script that takes the data as far as
possible along the pipeline from raw format to    nal published output, developing

batch mode produced draws at a rate     424, 000/sec, while c produced draws at a rate     1, 470, 000/sec.
8however, it is not the oldest, an honor that goes to fortran. this is noteworthy because some claim that
c is in common use today merely because of inertia, path dependency, et cetera. but c displaced a number of
other languages such as algol and pl/i which had more inertia behind them, by making clear improvements
over the incumbents.

11

but c is ugly!

statistics in the modern day

gsl_stats march 24, 2009

about equivalent   especially since compilation on a modern computer takes on
the order of 0.0 seconds.

c is by no means the best language for all possible purposes. dif-
ferent systems have specialized syntaxes for communicating with
other programs, handling text, building web pages, or producing certain graphics.
but for data analysis, c is very effective. it has its syntactic    aws: you will for-

with a debugger, the distance is even smaller, because you can jump around your
c code, change intermediate values, and otherwise interact with your program the
way you would with a stats package. graphical interfaces for stats packages and
for c debuggers tend to have a similar design.

 
 i  .d  for an interpreter and developing   g a .
 for a compiler become
get to append semicolons to every line, and will be frustrated that3/2==1 while
3/2.==1.5. but then, perl also requires semicolons after every line, and3/2 is one
the type of your variable ( , , or#) with every use, and r will guess the type you
like{14} is really just an integer. c   s  i  f statements look terribly confusing
friendly syntax possible, chose to use c   s  i  f syntax over many alternatives
  seeing the forest for the trees on the one hand, a good textbook should be a
sions and side-paths. sections marked with a  cover details that may be skipped

in short, c does not do very well when measured by initial ease-of-use. but there is
a logic to its mess of stars and braces, and over the course of decades, c has proven
to be very well suited for designing pipelines for data analysis, linking together
libraries from disparate sources, and describing detailed or computation-intensive
models.

in perl, python, and ruby too. type declarations are one more detail to remember,
but the alternatives have their own warts: perl basically requires that you declare

narrative that plots a de   nite course through a
   eld. on the other hand, most    elds have countless interesting and useful digres-

at    rst, but the authors of ruby and python, striving for the most programmer-

meant to use, but will often guess wrong, such as thinking that a one-element list

typography here are some notes on the typographic conventions used by this

that are easier on the eyes but harder to use.

book.

on a    rst reading. they are not necessarily advanced in the sense of being some-
how more dif   cult than unmarked text, but they may be distractions to the main
narrative.

gsl_stats march 24, 2009

chapter 1

questions and exercises are marked like this paragraph. the exercises are
not thought experiments. it happens to all of us that we think we understand
something until we sit down to actually do it, when a host of hairy details
turn up. especially at the outset, the exercises are relatively simple tasks
that let you face the hairy details before your own real-world complications
enter the situation. exercises in the later chapters are more involved and
require writing or modifying longer segments of code.

12

q1.1

notation

x: boldface, capital letters are matrices. with few exceptions, data matrices in this
book are organized so that the rows are each a single observation, and each column
is a variable.

x: lowercase boldface indicates a vector. vectors are generally a column of num-
bers, and their transpose, x   , is a row. y is typically a vector of dependent variables
(the exception being when we just need two generic data vectors, in which case
one will be x and one y).

x: a lowercase variable, not bold, is a scalar, i.e., a single real number.

x    is the transpose of the matrix x. some authors notate this as xt.

x is the data matrix x with the mean of each column subtracted, meaning that each
column of x has mean zero. if x has a column of ones (as per most regression
techniques), then the constant column is left unmodi   ed in x.

n: the number of observations in the data set under discussion, which is typically
the number of rows in x. when there is ambiguity, n will be subscripted.

i: the identity matrix. a square matrix with ones along its diagonal and zeros
everywhere else.

  : greek letters indicate parameters to be estimated; if boldface, they are a vector
of parameters. the most common letter is   , but others may slip in, such as. . .

  ,   : the standard deviation and the mean. the variance is   2.

    ,     : a carat over a parameter indicates an empirical estimate of the parameter
derived from data. typically read as, e.g., sigma hat, beta hat.

       n (0, 1): read this as epsilon is distributed as a normal distribution with
parameters 0 and 1.

gsl_stats march 24, 2009

statistics in the modern day

13

ex(f (x,   )): read as the expectation over x of the given function, which will take

p (  ): a id203 density function.
ll(  ): the log likelihood function, ln(p (  )).
s(  ): the score, which is the vector of derivatives of ll(  ).
i(  ): the information matrix, which is the matrix of second derivatives of ll(  ).
e(  ): the expected value, aka the mean, of the input.
p (x|  ): the id203 of x given that    is true.
p (x,   )|x: the id203 density function, holding x    xed. mathematically, this
is simply p (x,   ), but in the given situation it should be thought of as a function
only of   .9

 e e y e y efa
e indicates text that can be typed directly into a text    le and

a  a   e_fi e: slanted teletype text indicates a placeholder for text you will
code here as,    let a   e_fi e be the name of a    le on your hard drive. then
type
a  a   e_fi e at the command prompt   .
2.3e6: engineers often write scienti   c notation using so-called exponential or e

a form liker   x f (x,   )p (x)dx. because the integral is over all x, ex(f (x,   )) is

a     b: read as    a is equivalent to b    or    a is de   ned as b   .
a     b: read as    a is proportional to b   .

insert   a variable name rather than text to be read literally. you could read the

understood as a valid shell script, c commands, sql queries, et cetera.

not itself a function of x.

notation, such as 2.3    106     2.3e6. many computing languages (including c,
sql, and gnuplot) recognize e-notated numbers.

9others use a different notation. for example, efron & hinkley (1978, p 458):    the log likelihood function
l  (x). . . is the log of the density function, thought of as a function of   .    see page 329 for more on the philosoph-
ical considerations underlying the choice of notation.

gsl_stats march 24, 2009

14

chapter 1

every section ends with a summary of the main points, set like this para-
graph. there is much to be said for the strategy of    ipping ahead to the
summary at the end of the section before reading the section itself.
the summary for the introduction:

   this book will discuss methods of estimating and testing the param-

eters of a model with data.

   it will also cover the means of writing for a computer, including tech-
niques to manage data, plot data sets, manipulate matrices, estimate
statistical models, and test claims about their parameters.

credits

thanks to the following people, who added higher quality and richness to
the book:

    anjeanette agro for graphic design suggestions.

    amber baum for extensive testing and critique.

    the brookings institution   s center on social and economic dynamics, includ-
ing rob axtell, josh epstein, carol graham, emily groves, ross hammond, jon
parker, matthew raifman, and peyton young.

    dorothy gambrel, author of cat and girl, for the lonely planet data.
    rob goodspeed and the national center for smart growth research and educa-

tion at the university of maryland, for the washington metro data.

    derrick higgins for comments, critique, and the perl commands on page 414.

    lucy day hobor and vickie kearn for editorial assistance and making working

with princeton university press a pleasant experience.

    guy klemens, for a wide range of support on all fronts.

    anne laumann for the tattoo data set (laumann & derick, 2006).

    abigail rudman for her deft librarianship.

gsl_stats march 24, 2009

computing

i

gsl_stats march 24, 2009

gsl_stats march 24, 2009

2

c

this chapter introduces c and some of the general concepts behind good program-
ming that script writers often overlook. the function-based approach, stacks of
frames, debugging, test functions, and overall good style are immediately applica-
ble to virtually every programming language in use today. thus, this chapter on c
may help you to become a better programmer with any programming language.

as for the syntax of c, this chapter will cover only a subset. c has 32 keywords
and this book will only use 18 of them.1 some of the other keywords are basi-
cally archaic, designed for the days when compilers needed help from the user to
optimize code. other elements, like bit-shifting operators, are useful only if you
are writing an operating system or a hardware device driver. with all the parts of
c that directly manipulate hexadecimal memory addresses omitted, you will    nd
that c is a rather simple language that is well suited for simulations and handling
large data sets.

an outline

this chapter divides into three main parts. sections 2.1 and 2.2 start
small, covering the syntax of individual lines of code to make assign-
ments, do arithmetic, and declare variables. sections 2.3 through 2.5 introduce
functions, describing how c is built on the idea of modular functions that are each
independently built and evaluated. sections 2.6 through 2.8 cover pointers, a some-
what c-speci   c means of handling computer memory that complements c   s means
of handling functions and large data structures. the remainder of the chapter offers
some tips on writing bug-free code.

1for comparison, c++ has 62 keywords as of this writing, and java has an even 50.

gsl_stats march 24, 2009

18

chapter 2

cess of putting together a complete c development environment and using the tools
for gathering the requisite libraries.2

tools you will need a number of tools before you can work, including a c compiler,
a debugger, the make facility, and a few libraries of functions. some systems
have them all pre-installed, especially if you have a benevolent system adminis-
trator taking care of things. if you are not so fortunate, you will need to gather

the tools yourself. the online appendix to this book, at the site linked fromh   :
//  e  .  i 
e   .ed	/ i  e /8706.h   , will guide you through the pro-
    download the sample code for this book from the link ath   ://
  e  .  i 
e   .ed	/ i  e /8706.h   .
    decompress the.zi     le, go into the directory thus created, and com-
pile the program with the commandg

he   _w   d.
. if you are
eithera. 	  orhe   _w   d. from the command line, you can ex-
ecute it using./a. 	  or./he   _w   d.
    you may also want to try the akefi e, which you will also    nd in

check your c environment by compiling and running    hello, world,    a clas-
sic    rst program adapted from kernighan & ritchie (1988).

    if all went well, you will now have a program in the directory named

using an ide, see your manual for compilation instructions.

q2.1

the code directory. see the instructions at the head of that    le.

if you need troubleshooting help, see the online appendix, ask your local
computing guru, or copy and paste your error messages into your favorite
search engine.

2.1 lines

the story begins at the smallest level: a single line of code. most
of the work on this level will be familiar to anyone who has written
programs in any language, including instructions like assignments, basic arith-
metic, if-then conditions, loops, and comments. for such common programming
elements, learning c is simply a question of the details of syntax. also, c is a
typed language, meaning that you will need to specify whether every variable and
function is an integer, a real, a vector, or whatever. thus, many of the lines will be
simple type declarations, whose syntax will be covered in the next section.

the focus is ong

, because that is what i expect most readers will be using. the command-line switches
for theg

 command are obviously speci   c to that compiler, and users of other compilers will need to check
compliant compiler. finally, theg

 switch most relevant to this footnote is	  d=g 	99, which basically puts

2a pedantic note on standards: this book makes an effort to comply with the iso c99 standard and the ieee
posix standard. the c99 standard includes some features that do not appear in the great majority of c textbooks
(like designated initializers), but if your compiler does not support the features of c99 used here, then get a new
compiler   it   s been a long while since 1999. the posix standard de   nes features that are common to almost
every modern operating system, the most notable of which is the pipe; see appendix b for details.

the compiler manual for corresponding switches. however, all c code should compile for any c99- and posix-

the compiler in c99 + posix mode.

gsl_stats march 24, 2009

19

c

   

ratio = a / b;

example,

an assignment, not an assertion about equality; on paper, computer scientists often

assignment most of the work you will be doing will be simple assignments. for

semicolon at the end of everything but the few exceptions below.3 you can use all

11 divided by 3?    the common answer is that 11/3 =
3.66, but some say that it is three with a remainder of two. many programming lan-
guages, including c, take the second approach. dividing an integer by an integer

will    nd the value ofa divided byb and put the value in a i . the= indicates
write this as a i 
a/b, which nicely gets across an image of a i  taking
on the value ofa/b. there is a semicolon at the end of the line; you will need a
of the usual operations: ,	,/, and . as per basic algebraic custom,  and/ are
evaluated before  and	, so4 6/2 is seven, and 4 6 /2 is    ve.
  two types of division there are two ways to answer the question,    what is
gives the answer with the fractional part thrown out, while the modulo operator, ,
   nds the remainder. so11/3 is3 and11 3 is2.
is k an even number? if it is, thenk 2 is zero.4
real numbers like 3.66. thus, the machine   s evaluation of 11.0/3.0  3.0 may
be ever-so-slightly different from11.0. but with the special handling of division
for integers, you are guaranteed that for any integersa andb (whereb is not zero),
 a/b  b a b is exactlya.
ger value, by adding a decimal point.11/3 is3, as above, but11./3 is3.66... as
4in practice, you can check evenness withgs _ s_eve  orgs _ s_ dd:
#i 
 	de <g  /g  _ a h.h>
...if gs _ s_eve  k  
d _   e hi g  ;

desired. get into the habit of adding decimal points now, because integer division
is a famous source of hard-to-debug errors. page 33 covers the situation in slightly
more detail, and in the meantime we can move on to the more convenient parts of
the language.

but in most cases, you just want 11/3 = 3.66. the solution is to say when you
mean an integer and when you mean a real number that happens to take on an inte-

splitting the process into two parts provides a touch of additional precision, be-
cause the machine can write down integers precisely, but can only approximate

3the number one cause of compiler complaints like    line 41: syntax error    is a missing semicolon on line 40.

gsl_stats march 24, 2009

a=a b;   so common that c has a special syntax for it:

chapter 2

incrementing it is incredibly common to have an operation of the form

20

a += b;

this is slightly less readable, but involves less redundancy. all of the above arith-
metic operators can take this form, so each of the following lines show two equiv-
alent expressions:

a    = b;
a *= b;
a /= b;
a %= b;

/* is equivalent
/* is equivalent
/* is equivalent
/* is equivalent

to */
to */
to */
to */

a = a     b;
a = a * b;
a = a / b;
a = a % b;

the most common operation among these is incrementing or decrementing by one,
and so c offers the following syntax for still less typing:5

conditions c has no need forfa se andtrue keywords for boolean operations:

/* is equivalent
/* is equivalent

a = a + 1;
a = a     1;

a++;
a      ;

to */
to */

if an expression is zero, then it is false, and otherwise it is true. the
standard operations for comparison and boolean algebra all appear in somewhat
familiar form:

( a > b)
( a < b)
( a >= b)
( a <= b)
( a == b)
( a != b)
( a && b)
( a
|| b)
(! a)

// a is greater than b
// a is less than b
// a is greater than or equal to b
// a is less than or equal to b
// a equals b
// a is not equal to b
// a and b
// a or b
// not a

5there is also the pre-increment form,  a and		a. pre- and post-incrementing differ only when they are

    all of these evaluate to either a one or a zero, depending on whether the expression

in parens is true or false.

being used in situations that are bad style and should be avoided. leave these operations on a separate line and
stick to whichever form looks nicer to you.

c

21

gsl_stats march 24, 2009

    the comparison for equality involves two equals signs in a row. one equals sign

intended. your compiler will warn you in most of the cases where you are probably
using the wrong one, and you should heed its warnings.

 a=b  will assign the value ofb to the variablea, which is not what you had
    the   and|| operators have a convenient feature: ifa is suf   cient to determine
whether the entire expression is true, then it won   t bother withb at all. for example,
   will never take the square root of a negative number. ifa is less than zero, then
stops. ifa>=0, then the    rst part of this expression is not suf   cient to evaluate the
one, both  a>b || 
>d   and a> b||
 >d  make sense to c. you probably
won   t be sure which of the two c thinks you mean by a>b||
>d .6

the evaluation of the expression is done after the    rst half (it is true), and evaluation
whole expression, so the second part is evaluated to determine whether    a < 3.

why all the parentheses? first, parentheses indicate the order of operations, as
they do in pencil-and-paper math. since all comparisons evaluate to a zero or a

meant the    rst, but unless you have the order-of-operations table memorized, you

this code fragment   

((a < 0) || (sqrt(a) < 3))

second, the primary use of these conditionals is in    ow control: causing the pro-
gram to repeat some lines while a condition is true, or execute some lines only if a
condition is false. in all of the cases below, you will need parentheses around the
conditions, and if you forget, you will get a confusing compiler error.

else

1
2
3
4

if (a > 0)

{ b = 0; }

{ b = sqrt(a); }

if-else statements here is a fragment of code (which will not compile by itself)

showing the syntax for conditional evaluations:

ifa is positive, thenb will be given the value ofa   s square root; ifa is zero or
negative, thenb is given the value zero.
    the condition to be evaluated is always in parentheses following theif statement,
 a   e a    from the command prompt]. most people remember only the basics like how multiplication and

and there should be curly braces around the part that will be evaluated when the

6the order-of-operations table is available online, but you are encouraged to not look it up. [if you must, try

division come before addition and subtraction; if you rely on the order-of-operations table for any other ordering,
then you will merely be sending future readers (perhaps yourself) to check the table.

gsl_stats march 24, 2009

22

chapter 2

is common, and much less likely to cause trouble).

condition is true, and around the part that will be evaluated when the condition is
false.

    you can exclude the curly braces on lines two and four if they surround exactly
one line, but this will at some point confuse you and cause you to regret leaving
them out.

    you can exclude thee  e part on lines three and four if you don   t need it (which
    theif statement and the line following it are smaller parts of one larger ex-
pression, so there is no semicolon between theif ...  clause and what hap-
pens should it be true; similarly with thee  e clause. if you do put a semicolon
after anif statement   if a >0 ;   then yourif statement will execute the
null statement   / d    hi g /;   whena>0. your compiler will warn you
modifyhe   _w   d.
 to print
 1||0  0  is true, and print a different message of your choos-
ing if it is false. did c think you meant  1||0   0  (which evaluates
to 0) or 1|| 0  0   (which evaluates to 1)?

its greeting if

expression

of this.

the

q2.2

loops listing 2.1 shows three types of loop, which are slightly redundant.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

#include <stdio.h>
int main(){

int i = 0;
while (i < 5){

printf("hello.\n");
i ++;

}

for (i=0; i < 5; i++){

printf("hi.\n");

}

i = 0;
do {

printf("hello.\n");
i ++;

listing 2.1 c provides three types of loop: thewhi e loop, thef   loop, and thed 	whi e loop.
online source:f  w.
.

} while (i < 5);

return 0;

}

c

23

gsl_stats march 24, 2009

loops based on a counter (i = 0, i = 1, i = 2, . . . ) are so common that they get

the expression in parentheses on line four is true (mustn   t forget the parentheses),
execute the instructions in brackets, lines    ve and six.

the simplest is awhi e loop. the interpretation is rather straightforward: while
their own syntax, thef   loop. thef   loop in lines 9   11 is exactly equivalent to
thewhi e loop in lines 3   7, but gathers all the instructions about incrementing the
you can compare thef   andwhi e loop to see when the three subelements in the
parentheses are evaluated: the    rst part (i=0) is evaluated before the loop runs; the
second part (i<5) is tested at the beginning of each iteration of the loop; the third
part (i  ) is evaluated at the end of each loop. after the section on arrays, you
will be very used to thef   i=0;i< i i ;i    form, and will recognize it
ad 	whi e loop (with a semicolon at the end of thewhi e line to conclude the
thought). thed 	whi e loop in listing 2.1 is equivalent to thewhi e andf  

to mean step through the array. there may even be a way to get your text editor to
produce this form with one or two keystrokes.

finally, if you want to guarantee that the loop will run at least once, you can use

counter onto a single line.

loops. but say that you want to iteratively evaluate a function until it converges to
within 1    10   3. naturally, you would want to run the function at least once. the
form would be something like:

do {

error = evaluate_function();

} while (error > 1e   3);

example: the birthday paradox

the birthday paradox is a staple of undergraduate
statistics classes.7 the professor writes down the
birth date of every student in the class, and    nds that even though there is a 1 in
365 chance that any given pair of students have the same birthday, the odds are
good that there is a match in the class overall.

  w and     are not included in the standard c library. they are in the separate

listing 2.2 shows code to    nd the likelihood that another student shares the    rst
person   s birthday, and the likelihood that any two students share a birthday.

    most of the world   s programs never need to take a square root, so functions like

math library, which you must refer to on the command line. thus, compile the
program with

7it even mysti   es tv talk show hosts, according to paulos (1988, p 36).

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

gsl_stats march 24, 2009

24

chapter 2

#include <math.h>
#include <stdio.h>

int main(){

double no_match = 1;
double matches_me;
int ct;

}

}
return 0;

gcc birthday.c    lm    o birthday

listing 2.2 print the odds that other students share my birthday, and that any two students in the

printf("people\t matches me\t any match\n");
for (ct=2; ct<=40; ct ++){

matches_me = 1    pow(364/365., ct   1);
no_match *= (1     (ct   1)/365.);
printf("%i\t %.3f\t\t %.3f\n", ct, matches_me, (1   no_match));

room share a birthday. online source:bi  hday.
.
where	   indicates the math library and	  indicates that the output program will
be namedbi  hday (rather than the defaulta. 	 ). more on linking and libraries
#i 
 	de-ing a few external    les, and a list of the dramatis person  : variables
named  _ a 
h, a 
he _ e, and
 .
    line 8 prints a header line labeling the columns of numbers thef   loop will be
producing; it is easy to read once you know that \  means print a tab and \ 
    line 9 tells us that the counter
  will start at two, and count up until it reaches 40.
birthday is (364/365)2, et cetera.8 thus, the odds that among
 	1 additional peo-

    as for the math itself, it is easier to calculate the complement   the odds that no-
body shares a birthday. the odds that one person does not share the    rst person   s
birthday is 364/365; the odds that two people both do not share the    rst person   s

    lines 1   7 are introductory material, to be discussed below, including a preface

will follow below.

means newline.

see this calculation on line ten.

    as above, the odds that the second person does not share the    rst person   s birthday

ple, none have the same birthday as the    rst person is 1    (364/365)ct   1. you can
365(cid:1). the odds that an additional person shares no birthday with the    rst two
is(cid:0) 364
given that the    rst two do not share a birthday is(cid:0) 363
365(cid:1), so the odds that the    rst

8we assume away leap years, and the fact that the odds of being born on any given day are not exactly

1/365   more children are born in the summer.

gsl_stats march 24, 2009

c

25

three do not share a birthday is

this expression is best produced incrementally. in the introductory material,  _	
 a 
h was initialized at 1, and on line 11, another element of the sequence headed
by expression 2.1.1 gets multiplied in to  _ a 
h at each step of thef   loop.
    line 12 prints the results. the    rst input to the  i  f function will be discussed
 a 
he _ e, and1	   _ a 
h .
modify thef   loop to verify that the program prints the correct values for

in detail below, but the next inputs indicate what is to be printed: the counter,

(cid:18) 364
365(cid:19)(cid:18) 363
365(cid:19) .

(2.1.1)

q2.3

a class of one student.

comments

put a long block of comments at the head of a    le and at the head of
each function to describe what the    le or function does, using complete
sentences. describe what the function expects to come in, and what the function
will put out. the common wisdom indicates that these comments should focus
on why your code is doing what it is doing, rather than how, which will be self-
explanatory in clearly-written code.9

the primary audience of your comment should be you, six months from now.
when you are shopping for black boxes to plug in to your next project, or re-
auditing your data after the referee    nally got the paper back to you, a note to self
at the head of each function will pay immense dividends.

/* long comments begin with a slash   star,

continue as long as you want, and end
at the    rst star   slash.
*/

but don   t want to delete them entirely, simply put a/  and a / around the code,

the stars and slashes are also useful for commenting out code. if you would like
to temporarily remove a few lines from your program to see what would happen,

and the compiler will think it is a comment and ignore it.

however, there is a slight problem with this approach: what if there is a comment
in what you had just commented out? you would have a sequence like this in your
code:

9the sample code for this book attempts to be an example of good code in most respects, but it has much less

documentation than real-world code should have, because this book is the documentation.

gsl_stats march 24, 2009

26

chapter 2

/* line a;

/* line b */
line c;

will ignore everything from the    rst/  until it sees the    rst /. that means line

we had hoped that all three lines would be commented out now, but the compiler

*/

a and line b will be ignored, but

line c;

*/

will be read as code   and malformed code at that.10

you will always need to watch out for this when commenting out large blocks of
code. but for small blocks, there is another syntax for commenting individual lines
of code that deserve a note.

this_is_code; //everything on a line

//after two slashes
//will be ignored.

because the preprocessor will skip everything between the#if statement which
evaluates to zero and the#e dif:

later, we will meet the preprocessor, which modi   es the program   s text before
compilation. it provides another solution for commenting out large blocks that
may have comments embedded. the compiler will see none of the following code,

#if 0
/*this function does nothing. */
void do_nothing(){ }
#endif

printing c prints to anything   the screen, a string of text in memory, a    le   
using the same syntax. the formatting works much like the mad libs
party game (price & stern, 1988). first, there is a format speci   er, showing what
the output will be, but with blanks to be    lled in:

10q: if c didn   t have this quirk, and allowed comments inside comments, what different quirk would you have

to watch out for instead?

gsl_stats march 24, 2009

c

27

then, the user provides a speci   c instance of the noun and adjective to be    lled in
(which in this case is left as an exercise for the reader). since c is a programming
language and not a party game, the syntax is a little more terse. instead of

is very

.

adjective

i  

is number

in line.

noun

my

   i g
  i  f uses:

%s is number %i in line.

here is a complete example:

1
2
3
4
5
6
7
8

}

#include <stdio.h>

int main(){

int position = 3;
char name[] = "steven";
printf("%s is number %i in line\n", name, position);
return 0;

the  i  f function is not actually de   ned by default   its de   nition in the stan-
dard input/output header must be#i 
 	ded, which is what line one does. lines
finally, line six is the actual print statement, which will inserts eve  into the    rst
placeholder (  ), and insert3 into the second placeholder ( i). it will thus print
s eve i  	 be 3i  i e (plus an invisible newline).

four and    ve are variable declarations, de   ning the types of the variables; these
lines foreshadow the next section.

here are the odd characters you will need for almost all of your work.

insert an integer here
insert a real number in general format here
insert a string of text here
a plain percent sign
begin a new line
tab
a quote that won   t end the text string
continue the text string on the next line

\
\
\
\(newline)

 i
 g
  
  
 
 
"

gsl_stats march 24, 2009

28

chapter 2

your command line.

at this point, you may want to    ip through this book to    nd a few examples of

there are many more format speci   ers, which will give you a great deal of control;
you may want them when printing tables, for example, and can refer to any of a

number of detailed references when you need these, such as a 3  i  f from
  i  f and verify that they will indeed print what they promise to.
   assignment uses a single equals sign:a  ig ee=va 	e;.
   the usual arithmetic works: e =2 3 8/2;.
   conditions such as a>b , a<=b , and a==b  (two equals
   conditional    ow uses the form:if 
  di i   {d _if_  	e;}
e  e{d _if_fa  e;}.
   the basic loop is awhi e loop:whi e  hi _i _  	e {d _	
 hi ;}.
   when iterating through an array, af   loop makes the iteration
f   j=0;j< i i ;j   {  i  f "   
e  i g
i e  i\ " j ;}.

signs) can be used to control    ow.

clearer:

   write comments for your future self.

2.2 variables and their

declarations

having covered the verbs that a line of code will
execute, we move on to the nouns   variables.

you would never use x or z in a paper without    rst declaring, say,    let x     r2
and z     c   . you could leave the reader to guess at what you mean by x by its
   rst use, but some readers would misunderstand, and your referee would wonder
why you did not just come out and declare x. c is a strict referee, and requires that
you declare the type of every variable before using it. the declaration consists of
listing the type of the variable and then the variable name, e.g.

int a_variable, counter=0;
double stuff;

    we could initialize
 	  e  to zero as it is declared.

    this snippet declared three variables in two lines.

i  
d 	b e

ha 

gsl_stats march 24, 2009

    the other variables (such asa_va iab e) have unknown values right now. as-

29

c

sume nothing about what is contained in a declared but uninitialized value.11

    since the    rst step in using a variable is typically an assignment, and you can
declare and initialize on the same line, the burden of declaring types basically
means putting a type at the head of the    rst line where the variable is used, as on
lines four and    ve of the sample code on page 27.

here is a comprehensive list of the useful basic types for c.

signi   cantly more than common estimates of the number of atoms in the universe

that involves    ve billion agents or other such uses for counting into the trillions, in

there are ways to extend or shrink the size of the numbers, which are basically

size.13 section 4.5 offers detailed notes about how numbers are represented.

an integer:    1, 0, 3
a real number: 2.4,   1.3e8, 27
a character:    a   ,    b   ,    c   

ani   can only count to about 232     4.3 billion; you may have a simulation
which case you can use the   gi   type.12
not worth caring about. ad 	b e counts up to about   1e308, which is already
(circa 1e80), but there is a   gd 	b e type in case you need more precision or
global and static variables are automatically initialized to zero (or u  ), while local variables are not. but you
12thei   type on most 64-bit systems is still 32 bits. though this norm will no doubt change in the future,
the safe bet is to write code under the assumption that ani   counts to 232.
13thed 	b e name is short for    double-precision    oating-point number,    and thef  a  thus has half the
precision and range of ad 	b e. why    oating point? the computer represents a real using a form comparable
thef  a  type is especially not worth bothering with because the gsl   s matrices and vectors default to
holdingd 	b es, and c   s    oating-point functions internally operate on doubles. for example, thea  f function
(ascii text to    oating-point number) actually returns ad 	b e.
14the exception are indices for counters andf   loops, which are almost alwaysi,j, ork.

finally, notice that the variable names used throughout are words, not letters.14
using english variable names is the number one best thing you could do to make
your code readable. imagine how much of your life you have spent    ipping back
through journal articles trying to remember what   , m , and m stood for. why
impose that on yourself?

to scienti   c notation: n    10k, where n represents the number with the decimal point in a    xed location, and k
represents the location of the decimal point. multiplying by ten doesn   t change the number n, it just causes the
decimal point to    oat to a different position.

will suffer fewer painful debugging sessions if you ignore this fact and get into the habit of explicitly initializing
everything that needs initialization.

11i am reluctant to mention this, but later you will see the distinction between global, static, and local variables.

gsl_stats march 24, 2009

30

chapter 2

int a_list[100];

arrays much of the art of describing the real world consists of building aggregates
of these few basic types into larger structures. the simplest such aggregate
is an array, which is simply a numbered list of items of the same type. to declare
a list of a hundred integers, you would use:

reasoning behind this system will become evident in the section on pointers.

the index is an offset from the    rst element. the    rst element is zero items away

then, to refer to the items of the list, you would use the same square brack-
ets. for example, to assign the value seven to the last element of the array,

you would use:a_ i  [99   =7;. why is 99 the last element of the list? because
from itself, so it isa_ i  [0   , nota_ i  [1    (which is the second element). the
2-d arrays simply require more indices   i  a_2d_ i  [100   [100   . but there
chapter 4 will introduce theg  _ a  ix, which provides many advantages over
with the squares (so he_a  ay[7    will hold 49). then, print a message

just as you can initialize a scalar with a value at declaration, you can do the same
with an array, e.g.:

you do not have to bother counting how many elements the array has; c is smart
enough to do this for you.

are details in implementation that make 2-d arrays dif   cult to use in practice;

write a program to create an array of 100 integers, and then    ll the array

double data[ ] = {2,4,8,16,32,64};

the raw 2-d array.

like    7 squared is 49.    for each element of the array. use the hello world
program as a template from which to start.

q2.4

q2.5

the    rst element of the fibonacci sequence is de   ned to be 0, the second
is de   ned to be 1, and then element n is de   ned to be the sum of elements
n     1 and n     2. thus, the third element is 0+1=1, the fourth is 1+1=2, the
   fth is 1+2=3, then 2+3=5, then 3+5=8, et cetera.
the ratio of the nth element over the (n    1)st element converges to a value
known as the golden ratio.
demonstrate this convergence by producing a table of the    rst 20 elements
of the sequence and the ratio of the nth over the (n     1)st element for each
n.

c

31

} complex;

complex a, b;

gsl_stats march 24, 2009

typedef double triplet[3];
triplet tri1, tri2;

typedef struct {
double real;
double imaginary;

declaring types you can de   ne your own types. for example, these lines will

this is primarily useful for designing complex data types that are collections of

   rst declare a new type,  i  e , and then declare two such
triplets,  i1 and  i2:
many subelements, in conjunction with the   	
  keyword. for example:
you now have two variables of type
    ex and can now usea. ea  orb.i a	
gi a y to refer to the appropriate constituents of these complex numbers.
   	
 .
    those lines only de   ned a type; line 11 declares a variable,day , which will be
bday_   	
 s.
    in line 12, the   e_ a 
h element ofday [1    is given a value. lines 14 and 15
assign values to the elements ofday [2    throughday [40   . having calculated
the array syntax above, is to remember the order of the   	
    s elements and

    lines 4   7 de   ne the structure: it will hold one variable indicating the probabil-
ity of somebody matching the    rst person   s birthday, and one variable giving the
id203 that no two people share a birthday.

the values and stored them in an organized manner, it is easy for lines 18   20 to
print the values.

of this type. since there is a number in brackets after the name, this is an array of

listing 2.3 repeats the birthday example, but stores each class size   s data in a

initializing as with an array, you can initialize most or all of the elements of a
struct to a value on the declaration line. the    rst option, comparable to

make the assignments in that order. for example,

complex one = {1, 0};
complex pioverfour = {1, 1};

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

gsl_stats march 24, 2009

32

chapter 2

#include <math.h>
#include <stdio.h>

typedef struct {

double one_match;
double none_match;

} bday_struct;

int main(){

int ct, upto = 40;
bday_struct days[upto+1];
days[1].none_match = 1;
for (ct=2; ct<=upto; ct ++){

}

}
return 0;

printf("%i\t %.3f\t\t %.3f\n", ct, days[ct].one_match, 1   days[ct].none_match);

}
printf("people\t matches me\t any match\n");
for (ct=2; ct<=upto; ct ++){

days[ct].one_match = 1    pow(364/365., ct   1);
days[ct].none_match = days[ct   1].none_match * (1     (ct   1)/365.);

listing 2.3 the birthday example (listing 2.2) rewritten using a   	
  to hold each day   s data.
online source:bday   	
 .
.
would initialize  e to 1 + 0i and i ve f 	  to 1 + 1i. this is probably the
best way to initialize a   	
  where there are few elements and they have a well-
initializers will prove to be invaluable when dealing with structures like thea   _	
  de , which has a large number of elements in no particular order.

the other option is to use designated initializers, which are best de   ned by exam-
ple. the above two initializations are equivalent to:

in the    rst case, the imaginary part is not given, so it is initialized to zero. in
the second case, the elements are out of order, which is not a problem. designated

complex one = {.real = 1};
complex pioverfour = {.imaginary = 1, .real = 1};

known order.

two    nal notes on designated initializers. they can also be used for arrays, and
they can be interspersed with unlabeled elements. the line

int isprime[] = {[1]=1, 1, 1, [5]=1, [7]=1, [11]=1};

gsl_stats march 24, 2009

c

33

one type to a variable of a different type. when you assign a

to an integer. if you want to accept the truncation and assign a    oating-point real

if you are con   dent that you want to assign a variable of one type to a variable of
another, then you can do so by putting the type to re-cast into in parentheses before

necessarily match, so you may get unpredictable results with large numbers even
when there is nothing after the decimal point.

structs are syntactically simple, so there is little to say about them, but much of
good programming goes in to designing structs that make sense and are a good
re   ection of reality. this book will be    lled with them, including both purpose-

initializes an array from zero to eleven (the length is determined by the last ini-
tialized element), setting the elements whose index is a prime number to one. the
two ones with no label will go into the 2 and 3 slot, because their index will follow
sequentially after the last index given.

built structures like thebday_   	
  and structures de   ned by libraries like the
gsl, such as theg  _ a  ix mentioned above.
  type casting there are a few minor complications when assigning a value of
d 	b e value to an integer, such asi  i=3.2, everything after the decimal
point would be dropped. also, the range off  a s,d 	b es, andi  s do not
the variable name. for example, if a i  is ad 	b e, i    a i  will cast it
to an integer, sayi   , then explicitly tell the compiler that you meant to do this
by making the cast yourself; e.g., = i    a i ;.
ter. if 	  andde  are bothi  s, then a i = d 	b e  	 /de  does the
there are two other ways of getting the same effect:  	  0.0  is ani   plus
ad 	b e, which is ad 	b e. then  	  0.0 /de  is division of a real by an
if one of the numbers is a constant, then just add a decimal point, because2 is an
i  , while2. is a    oating-point real number.
finally, note that when casting fromd 	b e toi  , numbers are truncated, not
15in the real world, use i   (in a h.h) to round to integer:  	 ded_va = i   	   	 ded_ 	 be  .

rounded. as a lead-in to the discussion of functions, here is a function that uses
type casting to correctly round off numbers:15

type casting solves the division-by-integers problem from the head of this chap-

integer, which again works as expected (but don   t forget the parens). and as above,

division of a real by an integer, which will produce a real number as expected.

gsl_stats march 24, 2009

34

chapter 2

int round(double unrounded){
/* input a real number and output the number

rounded to the nearest integer. */

if (unrounded > 0)

return (int) (unrounded + 0.5);

else

}

return (int) (unrounded     0.5);

   all variables must be declared before the    rst use.

   until a variable is given a value, you know nothing about its value.

   you can assign an initial value to the variable on the declaration line,

   arrays are simply declared by including a size in brackets after the

such asi  i=3;.
variable name:i  a  ay[100   ;. refer to the elements using an
offset, so the    rst element isa  ay[0    and the last isa  ay[99   .
simpler types: y edef   	
 {d 	b e e g h wai  ;i  
 eg_
 ;} a   ;.
   after declaring a variable as a structure, say a   
	  ff ;, refer
to structure elements using a dot:
	  ff . eg_
 =1;.
   an integer divided by an integer is an integer:9/4==2. by putting
sion works as expected:9./4==2.25.

   you can declare new types, including structures that amalgamate

a decimal after a whole number, it becomes a real number, and divi-

2.3 functions

the instruction take the inverse of the matrix is six words long,
but refers to a sequence of steps that typically require several

pages to fully describe.

like many    elds, mathematics progresses through the development of new vocab-
ulary like the phrase take the inverse. we can comprehensibly express a complex
statement like the variance is   2(x   x)   1 because we didn   t need to write out
exactly how to do a squaring, a transposition (x   ) and an inverse.

similarly, most of the process of writing code is not about describing the pro-
cedures involved, but building a specialized vocabulary to make describing the

gsl_stats march 24, 2009

above using both basic nouns and   	
 s that aggregate them to larger concepts.

procedures trivial. adding new nouns to the vocabulary is a simple task, discussed

35

c

this section covers functions, which are single verbs that encapsulate a larger pro-
cedure.

#include <math.h>
#include <stdio.h>

typedef struct {

double one_match;
double none_match;

} bday_struct;

int upto = 40;
void calculate_days(bday_struct days[]);
void print_days(bday_struct days[]);

int main(){

bday_struct days[upto+1];
calculate_days(days);
print_days(days);
return 0;

}

void calculate_days(bday_struct days[]){

int ct;

days[1].none_match = 1;
for (ct=2; ct<=upto; ct ++){

days[ct].one_match = 1    pow(364/365., ct   1);
days[ct].none_match = days[ct   1].none_match * (1     (ct   1)/365.);

}

}

}

}

int ct;

void print_days(bday_struct days[]){

printf("%i\t %.3f\t\t %.3f\n", ct, days[ct].one_match, 1   days[ct].none_match);

printf("people\t matches me\t any match\n");
for (ct=2; ct<=upto; ct ++){

listing 2.4 the birthday example broken into logical functions. online source:bdayf  .
.
tiplef   loops. listing 2.4 re-presents the program using one function to do the
math and one to print the output. the ai  function (lines 13   18) now describes
the procedure with great clarity: declare an array ofbday_   	
 s, calculate val-
ues for the days, print the values, and exit. the functions to which ai  refers   on

the second birthday example, listing 2.3 can be hard to read, with its mess of mul-

lines 20   27 and 29   35   are short, and so are easier to read than the long string of

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35

gsl_stats march 24, 2009

36

chapter 2

code in listing 2.3. simply put, the functions provide structure to what had been a
relatively unstructured mess.

several arguments, in which case the argument declarations are a comma-separated
list.

functional form have a look at the function headers   the    rst line of each func-
tion, on lines 13, 20 and 29. in parens are the inputs to the func-
tion (aka the arguments), and they look like the familiar declarations from before.

structure takes up space   you can see that this listing is more lines of code than
the unstructured version. but consider the format of the book you are reading right
now: it uses such stylistic features as paragraphs, chapter headings, and indenta-
tion, even though they take up space. brevity is a good thing, which means that
it is typically worth the effort to minimize redundancy and search for simple and
brief algorithms. but brevity should never come at the cost of clarity. by eliminat-
ing intermediate variables and not using subfunctions, you can sometimes reduce
an entire program into a single line of code, but that one-liner may be virtually
impossible to debug, modify, or simply understand. no trees have to be killed to
add a few lines of white space or a few function headers to your on-screen code,
and the additional structure will save you time when dealing with your code later
on.

the ai  function takes no arguments, while you will see that many functions take
or consider the function declaration for the  	 d function above:
if we ignore the argument list in parens,i    	 d looks like a declaration as
output to a variable, viai  eigh =  	 d 8.3 .
 ai  function thus has an idea of what to expect when it comes across these func-
functions, because it can compile ai  knowing only what the other functions take
  thev id type
if a function returns nothing, declare it as typev id. such func-
of the inputs (like
a 
	 a e_day ) or printing data to the screen or an external

declaring a function you can declare the existence of a function separately from
the function itself, as per lines 10 and 11 of listing 2.3. the

well   and it is. it indicates that this function will return an integer value, that can
be used anywhere we need an integer. for example, you could assign the function

tions on lines 15 and 16, even though the functions themselves appear later. you
will see below that the compiler gets immense mileage out of the declaration of

tions will be useful for side effects such as changing the values

in and return, leaving the inner workings as a black box.

int round (double unrounded)

c

37

gsl_stats march 24, 2009

of the following are valid declarations for functions:

void do_something(double a);
double do_something_else(void);
double do_something_else();

   le (like  i  _day ). you can also have functions which take no inputs, so any
write a function with headerv id  i  _a  ay i  i _a  ay[    
i  a  ay_ ize  that takes in an integer array and the size of the array,
for example, in the birthday example, you could begin by writing the ai  func-

the last two are equivalent, but you can   t forget the parentheses entirely   then the
compiler would think you are declaring a variable instead of a function.

and prints the array to the screen. modify your square-printing program
from earlier to use this function for output.

how to write a program given a blank screen and a program to write, how should
you begin? write an outline, based on function headers.

q2.6

tion, which describes the broad outline of calculating probabilities and then print-
ing to the screen. in writing the outline, you will need to write down the inputs,
outputs, and intent of a number of functions. then you can begin    lling in each
function. when writing a function   s body, you can put the rest of the program out
of your mind and focus on making sure that the black box you are working on
does exactly what it should to produce the right output. when all of the functions
correctly do their job, and the main outline is fully    eshed out, you will have a
working program.

you want your black boxes to be entirely predictable and error-free, and the best
way to do this is to keep them small and autonomous. flip through this book and
have a look at the structure of the longer sample programs. you will notice that
few functions run for more than about    fteen lines, especially after discounting
the introductory material about declaring variables and checking inputs.

   le in which the function is de   ned, including those in    les that were#i 
 	ded

frames the manner in which the computer evaluates functions also abides by the
principle of encapsulating functions, focusing on the context of one func-
tion at a time. when a function is called, the computer creates a frame for that
function. into that frame are placed any variables that are declared at the top of the

(see below); and copies of variables that are passed as arguments.

the function is then run, using the variables it has in that frame, blithely ignorant
of the rest of the program. it does its math, making a note of the return value it

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

gsl_stats march 24, 2009

38

chapter 2

calculates (if any), and then destroys itself entirely, erasing all of the variables
created in the frame and copies of variables that had been put in to the frame.
variables that were not passed as an argument but were put in the frame anyway
(global variables; see below) come out unscathed, as does the return value, which
is sent back to the function which had called the frame into existence.

#include <stdio.h> //printf
double globe=1; //a global variable.

double factorial (int a_c){

while (a_c){

globe *= a_c;
a_c       ;

}
return globe;

}

}

one way to think about this is in terms of a stack of frames. the base of the stack

int main(void){
int a = 10;
printf("%i factorial is %f.\n", a, factorial(a));
printf("a= %i\n", a);
printf("globe= %f\n", globe);
return 0;

listing 2.5 a program to calculate factorials. online source:
a  byva .
.
is always the function named ai . for example, in the program in listing 2.5, the
computer at    rst ignores the functionfa
   ia , instead starting its work at line
twelve, where it    nds the ai  function. it creates a ai  frame and then starts
working, reading the declaration ofa, and creating that variable in the ai  frame.
the global variable declared on line two is also put into the ai  frame.
then, on line 14, it is told to print the value offa
   ia  a , which means that
tionfa
   ia . so the system freezes the ai  frame, generates a frame for the
fa
   ia  function, and jumps to line four. think of the new frame as being put
ofa=10 will be copied intoa_
, the global variableg  be will be put into the
frame, and the function does its math and returns the calculated value ofg  be.16
16why isg  be ad 	b e, when the factorial is always an integer? because thed 	b e uses exponential
notation when necessary, so its range is much larger than that of thei  , which does not. you could also try
using a   gi   (and replacing printf   si   placeholder i with the   gi   placeholder  i), but even that
fails after abouta=31.

it will have to evaluate that expression. this is a function call, which commands
the program to halt whatever it is doing and start working on evaluating the func-

on top of the    rst, leaving only the topmost frame visible and active. the value

c

39

gsl_stats march 24, 2009

ment list are put in the frame.

   nishes its work, and its frame is destroyed, leaving an empty stack and a    nished
program.

call-by-value a common error is to forget that global variables are put in all func-
tion frames, but only copies of the variables in the function   s argu-

having returned a value, thefa
   ia  frame and its contents are discarded.
since a copy of the valuea=10 was sent into the frame,a still has the value
10 when the function returns, even though the copya_
 was decremented to zero.
the ai  frame is now at the top of the stack, so it can pick up where it had left
off, printing the value ofa and 10! to the screen, using the calls to the  i  f
function   which each create their own frames in turn. finally, the ai  function
when thefa
   ia  function is called, the system puts a copy ofa intoa_
, and
then the function modi   es the copy,a_
. meanwhile,g  be is not a copy of itself,
this is why the output you got when you ran the program showeda=10, nota=0.
on the one hand, thefa
   ia  function could manglea_
 without affecting the
better alternative (and explain why thebday_   	
  examples worked).
  static variables
destroyed with their frame: you can de   ne a  a i
 variable.
  a i
 before them. you can also put an initialization on the declaration line,
calling function knows the length of 	 vey_da a and does bounds-checking ac-

when the function   s frame is destroyed, the program makes a note of the value of
the static variable, and when the function is called again, the static variable will
start off with the same value as before. this provides continuity within a function,
but only that function knows about the variable.

which will be taken into consideration only the    rst time the function is called.
here is a sample function to enter data points into an array. it assumes that the

original; on the other hand, we sometimes want functions to change their inputs.
this may make global variables tempting to you, but resist. section 2.6 will give a

but the real thing, so when it is changed inside the function, it changes globally.

static variable declarations look just like other declarations but with the word

there is one exception to the rule that all local variables are

cordingly.

void add_a_point(double number, double survey_data[]){

static int count_so_far = 0;
survey_data[count_so_far] = number;
count_so_far++;

}

40

gsl_stats march 24, 2009

the    rst time this function is called,
 	  _  _fa  will be initialized at zero, the
number passed in will be put in 	 vey_da a[0   , and
 	  _  _fa  will be in-
ber that
 	  _  _fa  is one, and will thus put the second value in 	 vey_	
da a[1   , where we would want it to be.
  the ai  function all programs must have one and only one function named
 ai , which is where the program will begin executing   the
system that called the program, which will expect ai  to be declared in one of

base of the stack of frames. the consistency checks are now with the operating

cremented to one. the second time the function is called, the program will remem-

chapter 2

two forms:

int main(void);
int main(int argc, char **argv);

#include <stdlib.h>
#include <stdio.h>

int main(int argc, char **argv){

}

if (argc==1){

printf("give me a command to run.\n");
return 1;

listing 2.6 a shell is a program that is primarily intended for the running of other programs; this is

the second form will be discussed on page 206, but for now, listing 2.6 provides

}
int return_value = system(argv[1]);
printf("the program returned %i.\n", return_value);
return return_value;

a very rudimentary one. online source: i   e he  .
.
a quick example of the use of inputs to ai . it uses c   s y  e  function to call a
and calling the ai  function of a foreign program. in this case, the y  e  func-
tion would call  , effectively putting the   program   s ai  function on top of
boot, the system starts a c program namedi i , and every other program is a
child ofi i  (or a child of a child of init, or a child of a child of a child, et cetera).

the current stack. more generally, you can think of your computer   s entire func-
tioning, from boot to shutdown, as the evaluation of a set of stacks of frames. at

conceptually, there is little difference between calling a function that you wrote

program. the usage of this program would be something like

./simpleshell "ls /a_directory"

c

41

gsl_stats march 24, 2009

with the parent, which is how a system runs several programs at once.

no indication is given otherwise, which is how most of the programs in this book

integer indicates a type of error.17 because so many people are not concerned with

scope when one function is running, only the variables in that frame are visible: all
of the variables in the rest of the program are dormant and inaccessible. this
is a good thing, since you don   t want to have to always bear in mind the current
state of all the variables in your program, the gnu scienti   c library, the standard
library, and who knows what else.

there is also af  k function that generates a second stack that runs concurrently
the y  e  function will pass back the return value of the subprogram   s ai .
the general custom is that if ai  returns 0 then all went well, while a positive
the return value of ai , the current c standard assumes that ai  returns zero if
get away with not having a e 	   statement in their ai  function.
   whi e ... {i  i;...} works as you expect,
declaringi only once.
   whi e ... {i  i=1;...} will re-seti to one
   f   i  i=0;i< ax;i   {...} works, but
g

 may complain about it unless you specify the
	  d=
99 or	  d=g 	99    ags for the compiler.
the header for a pair of curly braces, like af   loop)
any function in a    le that#i 
 	des
thef   i  i;...  form   but bear in mind that
the function (possibly as a  a i
 variable).
    if a variable is used by only a few functions, then declare the variable in the ai 
in ?, soe
h  ? will print its value.

a variable   s scope is the set of func-
tions that can see the variable. a
variable declared inside a function
is visible only inside that function.
if a variable is declared at the top of
a    le, then that variable is global to
the    le, and any function in that    le
can see that variable. if declared in a
header    le (see below, including an
important caveat on page 50), then

    if a variable is used throughout a single    le and is infrequently changed, then let it
be globally available throughout the    le, by putting it at the top of the    le, outside
all the function bodies.

is destroyed at the end of the bracketed code. this is
known as block scope. function-level scope could be
thought of a special case of block scope.
block scope is occasionally convenient   especially

the strategy behind deciding on the
scope of a variable is to keep it as
small as possible.

    if only one function uses a variable, then by all means declare the variable inside

you won   t be able to refer to a block-internal variable
after the loop ends.

17in the bash shell (the default on many posix systems), the return value from the last program run is stored

function and pass it as an argument to the functions that use it.

    a variable declared inside a bracketed block (or at

that header can see the variable.

you can declare variables inside a loop.

for every iteration of the loop.

block scope

gsl_stats march 24, 2009

42

chapter 2

    finally, if a variable is used throughout all parts of a program consisting of multiple
   les, then declare it in a header    le, so that it will be globally available in every

there is often the temptation to declare every variable as global, and just not worry
about scope issues. this makes maintaining and writing the code dif   cult: are you

that nothing in the rest of the program affects it, so what could have been a question
of just cutting and pasting a black box from one    le to another has now become an
involved analysis of the original program.

   le which#i 
 	des that header    le (see page 50).18
sure a tweak you made to the black box namedf	 
 i  _a won   t change the
workings inside the black box namedf	 
 i  _b? next month, when you want
to usef	 
 i  _a in a new program you have just written, you will have to verify
the formf	 
 i  _ y e
f	 
 i  _ a e  1_ y e 1_ a e  2_ y e 2_ a e ... .
named ai ; therefore a complete program must have one and only
object-oriented scope. in this system, functions are bound to objects, where an object is effectively a   	
 

18this is the appropriate time to answer a common intro-to-c question: what is the difference between c
and c++? there is much confusion due to the almost-compatible syntax and similar name   when explaining
the name c-double-plus, the language   s author references the newspeak language used in george orwell   s 1984
(orwell, 1949; stroustrup, 1986, p 4).

   the computer evaluates each function as an independent entity. it
maintains a stack of frames, and all activity is only in the current
top frame.

   global variables are passed into a new frame, but only copies of pa-
rameters are passed in. if a variable is not in the frame, it is out of
scope and can not be accessed.

   when a program starts, it will    rst build a frame for the function

   good coding form involves breaking problems down into functions

the key difference is that c++ adds a second scope paradigm on top of c   s    le- and function-based scope:

and writing each function as an independent entity.

   the header of a function is of

one such function.

holding several variables and functions. variables that are private to the object are in scope only for functions
bound to the object, while those that are public are in scope whenever the object itself is in scope.

in c, think of one    le as an object: all variables declared inside the    le are private, and all those declared in a
header    le are public. only those functions that have a declaration in the header    le can be called outside of the
   le.

but the real difference between c and c++ is in philosophy: c++ is intended to allow for the mixing of
various styles of programming, of which object-oriented coding is one. c++ therefore includes a number of other
features, such as yet another type of scope called namespaces, templates and other tools for representing more
abstract structures, and a large standard library of templates. thus, c represents a philosophy of keeping the
language as simple and unchanging as possible, even if it means passing up on useful additions; c++ represents
an all-inclusive philosophy, choosing additional features and conveniences over parsimony.

the joy of segfaults

2.4 the debugger

gsl_stats march 24, 2009

c

43

for the computer to do but halt.

there are a few ways in which your program can break. for

memory that is being used for the operating system or your dissertation. in this

happens to fall on a space that has something that can be interpreted as an integer,
and the computer processes whatever junk is at that location as if nothing were

the debugger is somewhat mis-named. a better name
would perhaps be the interrogator, because it lets you
interact with and ask questions of your program: you can look at every line as it
is being executed, pause to check the value of variables, back up or jump ahead in
the program, or insert an extra command or two. the main use of these powers is
to    nd and    x bugs, but even when you are not actively debugging, you may still
want to run your program from inside the debugger.

example, if you attempt to calculate1/0, there is not much
or, say that you have declared an array,i  da a[100    and you attempt to read
toda a[1000   . this is a location somewhere in memory, 901i  s    distance past
the end of the space allocated for the array. one possibility is thatda a[1000   
wrong. or,da a[1000    could point to an area of protected memory, such as the
case, referring toda a[1000    will halt the program with the greatest of haste,
compile when you refer to an undeclared variable. if you declared e
ei    and
da a[100   , then setting e
ie    orda a[999    to a value is probably an error. a
19in fact, you will    nd that the worst thing that can happen with an error like the above read ofda a[1000   

before it destroys something valuable. this is a segmentation fault (segfault for
short), since you attempted to refer to memory outside of the segment that had
been allocated for your program. below, in the section on pointers, you will en-
counter the null pointer, which by de   nition points to nothing. mistakenly trying
to read the data a null pointer is pointing to halts the program with the complaint
attempting to dereference a null pointer.

language that saves you the trouble of making declarations and refuses to segfault
will just produce a new variable, expand the array, and thus insert errors into the
output that you may or may catch.

a segfault is by far the clearest way for the computer to tell you that you mis-
coded something and need to    re up the debugger.19 it is much like refusing to

would be for the program to not segfault, but to continue with bad data and then break a hundred lines later. this
is rare, but when such an event becomes evident, you will need to use a memory debugger to    nd the error; see
page 214.

the debugging process

44

chapter 2

gdb prompt.20

gsl_stats march 24, 2009

possible, you should use it every time you compile.

plains that it can   t    nd any debugging symbols, then that means that you forgot

you need to tell the compiler to include the names of the variables and functions

the    rst thing you will want to know is where you are in the program. you can

when it does, you will be returned to gdb   s prompt, so you can interrogate the
program.

to debug the program 	 _ e under the debugger, type
gdb 	 _ e at the command line. you will be given the
in the compiled    le, by adding the	g    ag on the compiler command line. for
example, instead ofg

he   .
, useg

	ghe   .
. if the debugger com-
the	g switch. because	g does not slow down the program but makes debugging
if you know the program will segfault or otherwise halt, then just startgdb as
above, run the program by typing 	  atgdb   s prompt, and wait for it to break.
do this with theba
k  a
e command, which you can abbreviate to eitherb  or
whe e. it will show you the stack of function calls that were pending when the
program stopped. the    rst frame is always ai , where the program started. if
 ai  called another function, then that will be the next frame, et cetera. often,
not write, such as in a call to a     . ignore those. you did not    nd a bug in
 a     . find the topmost frame that is in the code that you wrote.
in the stack; to change to frame number three, give the commandf a e3 (orf3
for short). you can also traverse the stack via the	  andd w  commands, where
	  goes to a parent function andd w  goes to the child function.
can get a list of the local variables usingi f   
a  , or information about the
arguments to the function usingi f a g  (though the argument information is
may be in the frame using  i  va _ a e, or more brie   y, va _ a e. you
some are stand-alone programs likeddd and others are integrated into ides. they will not be discussed here

at this point, the best thing to do is look at a listing of your code in another window
and look at the line the debugger pointed out. often, simply knowing which line
failed is enough to make the error painfully obvious.

if the error is still not evident, then go back to the debugger and look at the vari-
ables. you need to be aware of which frame you are working in, so you know
which set of variables you have at your disposal. you will default to the last frame

your program will break somewhere in the internals of a piece of code you did

already in the frame description). or, you can print any variable that you think

once you are in the frame you want, get information about the variables. you

20asking your favorite search engine for gdb gui will turn up a number of graphical shells built around gdb.

because they work exactly like gdb, except that they involve using the mouse more.

also, gdb itself offers many conveniences not described here. see stallman et al. (2002) for the full story.

breaking and stepping

c

45

gsl_stats march 24, 2009

just before line 35 is evaluated.

variables and functions are available to the scope in which you are working. or,

the result. generally, you can execute any line of c code that makes sense in the

gdb has a special syntax for viewing several elements of an array at once. if you

if your program is doing things wrong but is not kind
enough to segfault, then you will need to    nd places to

can print the value of any expression in scope:      va   or a   _ h w_	
 a  ix    will display the square root ofva  and the matrix , provided the
   ddev=     va   will set the variable  ddev to the given value and print
given context via the  i   command.
would like to see the    rst    ve elements of the arrayi e  , then use:  i e   5.
halt the program yourself. do this with theb eak command. for a program with
only one    le of code, simply give a line number:b eak35 will stop the program
    for programs based on many    les, you may need to specify a    le name:b eak
fi e2.
:35.
after the function   s header. e.g,b eak
a 
	 a e_day .
when an iterator reaches 10,000. i.e.,b eak35if
 	  e >10000.21
    all breakpoints are given a number, which you can list withi f b eak. you can
delete break point number three with the commandde 3.
once you have set the breakpoints, 	  (or just ) will run the program until it
     will step to the next line to be evaluated, which could mean backing up in the
    ex  or  will step through the function (which may involve backtracking) but will
   	  i  or	 will keep going until you get to the next line in the function, so the
   
 will continue along until the next break point or the end of the program.22
21you can also set watchpoints, which tell gdb to watch a variable and halt if that variable changes, e.g.,wa 
h
 yva . watchpoints are not as commonly used as breakpoints, and sometimes suffer from scope issues.
22a mnemonic device for remembering which is which:  is the slowest means of stepping,  slightly faster,
	 still faster, and
 the fastest. in this order, they spell  	
, which is almost an english word with implications

debugger will run through subfunctions and loops until forward progress is made
in the current function.

run without stopping in any subframes which may be created (i.e., if subfunctions
are called).

reaches a break point, and then you can apply the interrogation techniques above.
you may want to carefully step through from there:

    you may also want the program to break only under certain conditions, such as

    or, you can specify a function name, and the debugger will stop at the    rst line

current function or going to a subfunction.

of stepping slowly.

46

chapter 2

gsl_stats march 24, 2009

some variables tweaked, or skip over a few lines. odd things will happen if you
jump out of the frame in which you are working, so use this only to jump around
a single function.

   j	   i e   will jump to the given line number, so you can repeat a line with
    e 	   will exit the given frame and resume in the parent frame. you can give a
return value, like e 	  va .
hitting  to step through many lines.
breakbdayf  .
 (from listing 2.4) and debug it.
    modify line 13 fromday [1    today [	1   .
    recompile. be sure to include the	g    ag.
    start the debugger. if the program segfaulted, just type 	  and wait
for failure; otherwise, insert a breakpoint,b eak
a 
	 a e_day ,
and then 	 .

    just hitting <enter> will repeat the last command, so you won   t have to keep

    run the program and observe the output (if any).

q2.7

    check the backtrace to see where you are on the stack. what evidence

can you    nd that things are not right?

a note on debugging strategy

especially for numeric programs, the strategy in
debugging is to    nd the    rst point in the chain of
logic where things look askew. below, you will see that your code can include
assertions that check that things have not gone astray, and the debugger   s break-
and-inspect system provides another means of searching for the earliest misstep.
but if there are no intermediate steps to be inspected, debugging becomes very
dif   cult.
say you are writing out the roots of the quadratic equation, x =    b     b2   4ac
erroneously code the    rst root as:

there are basically no intermediate steps: you put ina,b, and
, and the system
spits out a bad value forfi       . now say that you instead wrote:

   rstroot =    b + sqrt(b*b     4 *a *c)/2*a; //this is wrong.

, and

2a

1
2
3

   rstroot =    b;
   rstroot += sqrt(b*b     4*a*c);
   rstroot =    rstroot/2*a; //still wrong.

gsl_stats march 24, 2009

about the error. say thata=2. as you step through, you    nd that the value of
fi        does not change after line three runs. from there, the error is obvious:
the line should have been eitherfi       =fi       / 2 a  orfi       
/=2 a. such a chain of logic would be impossible with the one-line version of

if you know that the output is wrong, you can interrogate this sequence for clues

47

c

the routine.23

however, for the typical reader, the second version is unattractively over-verbose.
a    rst draft of code should err on the side of inelegant verbosity and easy de-
bugability. you can then incrementally tighten the code as it earns your trust by
repeatedly producing correct results.

q2.8

    use

that any natural number can be expressed as the sum of at most three trian-
gular numbers. for example, 13 = 10+3 and 19=15+3+1. demonstrate this
via a program that    nds up to three triangular numbers for every number
from 1 to 100.

a triangular number is a number like 1 (    ), 1+2=3 (         ), 1+2+3=6(cid:16)                (cid:17),
           (cid:19), et cetera. fermat   s polygonal number theorem states
1+2+3+4=10(cid:18)                

    write a functioni    ia g	 a  i  i  that takes in an index and
returns the ith triangular number. e.g.,  ia g	 a  5  would return
1+2+3+4+5=15. write a ai  to test it.
functioni  fi d_ ex _	
  ia g	 a  i  i   that
triangular number larger than its input. modify ai  to test it.
    write a functionv idfi d_  i  e  i  i  i   	 [     that
 	 . you can usefi d_ ex _  ia g	 a  to    nd the largest triangu-
lar number to try, and then write three nestedf   loops to search the
in-a-loop    nds three numbers that sum toi , then the function can
 e 	  , thus cutting out of the loops.
    finally, write a ai  function that    rst declares an array of threei  s
that will be    lled byfi d_  i  e , and then runs af   loop that
callsfi d_  i  e  for each integer from 1 to 100 and prints the

range from zero to the maximum you found. if the loop-in-a-loop-

takes in a number and puts three triangular numbers that sum to it in

returns the index of

function to write

the smallest

that

a

result.

23the one-line version also has a second error; spotting it is left as a quick exercise for the reader.

48

gsl_stats march 24, 2009

   the debugger will allow you to view intermediate results at any point

   you can either wait for the program to segfault by itself, or useb eak
   you can execute and print any expression or variable using 
va iab e.
   once the program has stopped, use , , and	 to step through the

along a program   s execution.

to insert breakpoints.

chapter 2

program at various speeds.

what to type

2.5 compiling and running

compiler to

to this point, you have been using a minimal command line to com-
pile programs, but you can specify much more. say that we want the

the process of compiling program text into
machine-executable instructions relies hea-
vily on the system of frames. if function a calls function b, the compiler can
write down the instructions for creating and executing function a without knowing
anything about function b beyond its declaration. it will simply create a frame with
a series of instructions, one of which is a call to function b. since the two frames
are always separate, the compiler can focus on creating one at a time, and then link
them later on.

    include symbols for debugging (	g),
    warn us of all potential coding errors (	wa  ),
    use the c99 and posix standards (	  d=g 	99),
    compile using two source    les,fi e1.
 andfi e2.
, plus
    the sqlite3 and standard math library (	    i e3	  ), and    nally
    output the resulting program to a    le named 	 _ e (	  	 _ e).
this is a lot to type, so there is a separate program, ake, which is designed to
be able to simply type ake instead of the mess above. you may bene   t from
reading appendix a at this point. or, if you decide against using ake, you could

facilitate compiling. after setting up a make   le to describe your project, you will

you could specify all of this on one command line:

gcc    g    wall    std=gnu99    le1.c    le2.c    lsqlite3    lm    o run_me

gsl_stats march 24, 2009

c

49

write yourself an alias in your shell, write a batch    le, or use an ide   s compilation
features.

multiple windows also come in handy here: put your code in one window and
compile in another, so you can see the inevitable compilation errors and the source
code at the same time. some text editors and ides even have features to compile
from within the program and then step you through the errors returned.

the components

even though we refer to the process above as compilation, it ac-
tually embodies three separate programs: a preprocessor, a com-

piler, and a linker.24

#include <gsl/gsl_matrix.h>
#include "a_   le.h"

the three sub-programs embody the steps in developing a set of frames: the pre-
processor inserts header    les declaring functions to be used, the compilation step
uses the declarations to convert c code into machine instructions about how to
build and execute a standalone frame, and the linking step locates all the disparate
frames, so the system knows where to look when a function call is made.

the preprocessing step the preprocessor does nothing but take text you wrote
and convert it into more text. there are a dozen types
of text substitutions the preprocessor can do, but its number one use is dumping the
contents of header    les into your source    les. when the preprocessor is processing

the    le ai .
 and sees the lines
it    nds theg  _ a  ix.h and thea_fi e.h header    les, and puts their entire
you rung

 with the	e    ag); the preprocessor just passes the expanded code
to the compiler. for example, theg  _ a  ix.h header    le declares theg  _	
 a  ix type and a few dozen functions that act on it, and the preprocessor inserts
the angle-bracket form,#i 
 	de<g  /g  _ a  ix.h> indicates that the pre-
the headers of library    les, and see appendix a for details. the#i 
 	de"a_	
fi e.h" form searches the current directory for the header; use this for header
25the#i 
 	de"a_fi e.h" form searches the include path as well, so you could actually use it for both
home-grown and system#i 
 	des. in practice, the two forms serve as an indication of where one can    nd the

those declarations into your program, so you can use the structure and its functions
as if you   d written them yourself.

contents verbatim at that point in the    le. you will never see the expansion (unless

processor should look at a pre-speci   ed include path for the header; use this for

24as a technical detail which you can generally ignore in practice, the preprocessor and compiler are typically

one program, and the linker is typically a separate program.

   les you wrote yourself.25

given header    le, so most authors use the <> form even though it is redundant.

gsl_stats march 24, 2009

50

chapter 2

header aggregation

this means that you could ignore the headers at the
top of all of the code snippets in this chapter.

header    les allow you to simulate
truly global scope, but with    ner
control if you want it. if some vari-

of course, you will still need to include any headers
you have written, and if the compiler complains about
an undeclared function, then its header is evidently not

the apophenia library provides a convenience
header that aggregates almost every header you will
likely be using. by placing

at the top of your    le, you should not need to in-
clude any of the other standard headers that one would
normally include in a program for numerical analysis

many programming languages have
a way to declare variables as hav-
ing global scope, meaning that ev-
ery function everywhere can make
use of the variable. technically, c
has no such mechanism. instead, the
best you can do is what i will call
   le-global scope, meaning that ev-
ery function in a single    le can see
any variable declared above it in that
   le.

#i 
 	de <a   .h>
(  di .h,  d ib.h, a h.h,g  _a y hi g.h).
included ina   .h.
ables should be global to your entire program, then create a    le namedg  ba  .h,
and put all declarations in that    le (but see below for details). by putting#i 
 	de
"g  ba  .h" at the top of every    le in your project, all variables declared therein
are now project-global. if the variables of   
e  .
 are used in only one or two
other code    les, then project-global scope is overkill:#i 
 	de"   
e  .h"
ray of numbers and their squares (page 30), and another function,  i  _	
a  ay, to print those values (page 37).
    move  i  _a  ay to a new text    le,	 i i y_f  .
.
    write the corresponding one-line    le	 i i y_f  .h with  i  _	
a  ay   s header.
   #i 
 	de"	 i i y_f  .h" in the main square-printing program.
    modify the square-calculating code to call  i  _a  ay.
    compile both    les at once, e.g.,g

y 	 _ ai .
	 i i y_	
f  .
.
  variables in headers
the system of putting    le-scope variables in the base.
   le and global-scope variables in the.h    le has one ugly

in prior exercises, you wrote a program with one function to create an ar-

    run the compiled program and verify that it does what it should.

only in those few code    les that need it.

q2.9

detail. a function declaration is merely advice to the compiler that your function

gsl_stats march 24, 2009

c

51

later in your code?

the variable is allocated only once.

that follows is not for memory allocation, but is purely informative. simply put it in

has certain inputs and outputs, so when multiple    les reread the declaration, the
program suffers only harid113ss redundancy. but a variable declaration is a com-

mand to the compiler to allocate space as listed. ifhead.h includes a declaration
i  x;, andfi e1.
 includeshead.h, it will set aside ani     s worth of mem-
ory and name itx; iffi e2.
 includes the same header, it will also set aside some
memory namedx. so which bit of memory are you referring to when you usex
c   s solution is theex e   keyword, which tells the compiler that the declaration
front of the normal declaration:ex e  i  x;ex e     gd 	b ey; will
both work. then, in one and only one.
    le, declare the variables themselves, e.g.,
i  x;   gd 	b ey=7.0. thus, all    les that#i 
 	de the header know
what to make of the variablex, sox   s scope is all    les with the given header, but
to summarize: function declarations and y edefs can go into a header    le that
will be included in multiple.
    les. variables need to be declared as usual in one
and only one.
    le, and if you want other.
    les to see them, re-declare them in
a header    le with theex e   keyword.
the compilation step the compilation stage consists of taking each.
    le
fi e1.
 will result infi e1. , andfi e2.
 will compile tofi e2. . these
frame forg  _ a  ix_add with these input variables, but executing that instruc-
tion does not require any knowledge of whatg  _ a  ix_add looks like   that is
standalone frames. some are in.     les that the compiler just
the computer to go evaluateg  _ a  ix_add, the computer will have no problem

object    les are self-encapsulated    les that include a table of all of the symbols de-
clared in that    le (functions, variables, and types), and the actual machine code that
tells the computer how to allocate memory when it sees a variable and how to set
up and run a frame when it sees a function call. the preprocessor inserted declara-
tions for all external functions and variables, so the compiler can run consistency
checks as it goes.

created, and some are in libraries elsewhere on the system. the linker collects all
of these elements into a single executable, so when one function   s instructions tell

the instructions for a function may include an instruction like at this point, create a

the linking step after the compilation step, you will have on hand a number of

a separate frame in which the current frame has no business meddling.

in turn and writing a machine-readable object    le, so

locating and loading that function. your primary interaction with the linker will be

52

chapter 2

on your hard drive.

gsl_stats march 24, 2009

to    nd libraries that will do your work for you, both online and

note well that a library   s header    le and its object    le are separate entities   
meaning that there are two distinct ways in which a call to a library function can

in telling it where to    nd libraries, via	  commands on the compiler command
line (	 g  	 g  
b a 	  , et cetera).
go wrong. to include a function from the standard math library like    , you will
need to (1) tell the preprocessor to include the header    le via#i 
 	de< a h.h>
in the code, and (2) tell the linker to link to the math library via a	     ag on the
command line, in this case	  . appendix a has more detail on how to debug your
#i 
 	de statements and	     ags.
  finding libraries an important part of the art of c programming is knowing how
hard drive (tryi f g ib
), you can easily    nd it online. it is worth giving the
    the gnu/unesco website (g 	.  g) holds a hefty array of libraries, all of
     	 
ef  ge. e  hosts on the order of 100,000 projects (of varying quality). to
of your functions relating to   i
 into a    le named   i
.
, and put the useful
declarations into a separate header    le named   i
.h. you already have a start
pansions like replacing#i 
 	de<heade .h> with the entire con-
tents ofheade .h.
   therefore, put public variable declarations (with theex e   key-
   the next step consists of compilation, in which each source (.
)    le
is converted to an object (. )    le.

    the    rst library to know is the standard library. being standard, this was installed
on your computer along with the compiler. if the documentation is not on your

    finally, you can start writing your own library, since next month   s project will
probably have some overlap with the one you are working on now. simply put all

be hosted on sourceforge, a project must agree to make its code public, so you
may fold anything you    nd there into your own work.

   compilation is a three-step process. the    rst step consists of text ex-

documentation a skim so you know which wheels to not reinvent.

on creating a utility library from the exercise on page 50.

word) and function declarations in header    les.

which are free for download.

   

gsl_stats march 24, 2009

c

   

53

   therefore, each source    le should consist of a set of standalone func-
tions that depend only on the    le   s contents and any declarations in-
cluded via the headers.

   the    nal step is linking, in which references to functions in other

libraries or object    les are reconciled.

   therefore, you can    nd and use libraries of functions for any set of

tasks you can imagine.

2.6 pointers

pointers embody the concept of the location of data   a concept with which we

pointers will change your life. if you have never dealt with
them before, you will spend some quantity of time puzzling over
them, wondering why anybody would need to bother with them. and then, when
you are    nally comfortable with the difference between data and the location of
data, you will wonder how you ever wrote code without them.

deal all the time. i know the locationh   :// y i e .
  , and expect that if i go
returning to the computer for a moment, when you declarei  k, then the com-
puter is going to putk somewhere in memory. perhaps with a microscope, you

to that location, i will get information about today   s events. i gave my colleagues
an email address years ago, and when they have new information, they send it
to that location. when so inclined, i can then check that same location for new
information. some libraries are very regimented about where books are located, so
if you need a book on id203 (library of congress classi   cation qa273) the
librarian will tell you to go upstairs to the third bookshelf on the left. the librarian
did not have to know any information about id203, just the location of such
information.

could even    nd it: there on the third chip, two hundred transistors from the bot-
tom. you could point to it.

lacking a    nger with which to point, the computer will use an illegible hexadec-
imal location, but you will never have to deal with the hexadecimal directly, and
lose nothing by ignoring the implementation and thinking of pointers as just a very
precise    nger, or a book   s call number.

the confusing part is that the location of data is itself data. after all, you could
write    qa273    on as slip of paper as easily as    p (a     b) = p (a|b)p (b).   

gsl_stats march 24, 2009

54

chapter 2

further, the location of information may itself have a location. before computers
took over, there was a card catalog somewhere in the library, so you would have
to go to the card catalog   the place where location data is stored   and then look
up the location of your book. it sometimes happens that you arrive at the qa273
shelf and    nd a wood block with a message taped to it saying    oversized books are
at the end of the aisle.   

variable likek could be integer data or the location of integer data. but c uses the

in these situations, we have no problem distinguishing between information that
is just the location of data and data itself. but in the computing context, there
is less to guide us. is 8,049,588 just a large integer (data), or a memory address
(the location of data)?26 c   s syntax will do little to clear up the confusion, since a

location of data to solve a number of problems, key among them being function
calls that allow inputs to be modi   ed and the implementation of arrays.

call-by-address v call-by-value

a common function could take a noticeable amount of time. also, you will often
want your function to change the variables that get sent to it.

this setup, known as call-by-value since only values are passed to the function,
allows for a more stable implementation of the paradigm of standalone frames.

first, a quick review of how functions are called:
when you call a function, the computer sets up
a separate frame for the function, and puts into that frame copies of all of the
variables that have been passed to the function. the function then does its thing
and produces a return value. then, the entire frame is destroyed, including all of
the copies of variables therein. a copy of the return value gets sent back to the
main program, and that is all that remains of the defunct frame.

but ifk is an array of a milliond 	b es, then making a copy every time you call
picture, a function is called with the pointer as an argument, via a form likef _	

a     i  e  . there are now two    ngers, original and copy, pointing to the
26this number is actually an address lifted from my debugger, where it is listed as0x08049588. the0x pre   x

same spot, but the function knows only about the copy. given its copy of a    nger,
it is easy for the function to change the value pointed to to seven. when the function
returns, in the after picture, the copy of a    nger is destroyed but the changes are
not undone. the original    nger (which hasn   t changed and is pointing to the same
place it was always pointing to) will now be pointing to a modi   ed value.

pointers    x these problems. the trick is that instead of sending the function a copy
of the variable, we send in a copy of the location of the variable: we copy the
book   s call number onto a slip of paper and hand that to the function. in figure
2.7, the before picture shows the situation before the function call, in the main
program: there is a pointer to a location holding the number six. then, in the during

indicates that the number is represented in hexadecimal.

gsl_stats march 24, 2009

c

55

figure 2.7 before, during, and after a function call that modi   es a pointed-to value

returning to c syntax, here are the rules for using pointers:

    to declare a pointer to an integer, usei   k.
    outside the declarations, to refer to the integer being pointed to, use k.
    outside the declarations, to refer to the pointer itself, usek.
the declarationi     i means that  will be a pointer to an integer andi is
an integer, but in a non-declaration line likei=  ,   refers to the integer value
that  points to. there is actually a logical justi   cation for the syntax, which i will
to give another example, let us say that we are declaring a new pointer 2 that
will be initialized to point to the same address as . then the declaration would be
i    2= , because 2 is being declared as a pointer, and  is being used as a
the spaces around our stars do not matter, so use whichever ofi   k,i   k, or
i   k you like best. general custom prefers the    rst form, because it minimizes
the chance that you will writei   k b (allocate a pointer namedk and ani  
namedb) when you meanti   k  b (allocate two pointers). the star also still

not present here because it tends to confuse more often than it clari   es. instead,
just bear in mind that the star effectively means something different in declarations
than in non-declaration use.

pointer.

means multiply. there is never ambiguity, but if this bothers you, use parentheses.

listing 2.8 shows a sample program that uses c   s pointer syntax to implement the
call-by-address trick from figure 2.7.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

gsl_stats march 24, 2009

56

chapter 2

#include <stdio.h> //printf
#include <malloc.h> //malloc

int globe=1; //a global variable.

int factorial (int *a_c){

while (*a_c){

globe *= *a_c;
(*a_c)       ;

}
return globe;

}

int main(void){

}

int *a = malloc(sizeof(int));

*a = 10;
printf("%i factorial ...", *a);
printf(" is %i.\n", factorial(a));
printf("*a= %i\n", *a);
printf("globe= %i\n", globe);
free(a);
return 0;

listing 2.8 a version of the factorial program using call-by-address. online source:
a  byadd.
.
    in the ai  function,a is a pointer   the address of an integer   as indicated by the
star in its declaration on line 15; the a   
 part is discussed below.
    to print the integer being pointed to, as on line 19, we use a.
    the header for a function is a list of declarations, so on line 6,fa
   ia  i  
 a_
  tells us that the function takes a pointer to an integer, which will be named
a_
.
    thus, in non-declaration use like lines eight and nine, a_
 is an integer.
now for the call-by-address trick, as per figure 2.7. when the call tofa
   ia  is
made on line 18, the pointera gets passed in. the computer builds itself a frame,
using a copy ofa   that is, a copy of the location of an integer. botha (in the ai 
frame) anda_
 (in thefa
   ia  frame) now point to the same piece of data.
line 9,  a_
 		, tells the computer to go to the addressa_
 and decrement the
value it    nds there. when the frame is destroyed (anda_
 goes with it), this will
 a   the integera points to   has changed as a side effect to calling thefa
   ia 
function, you saw that line 19 printed a=0 when you ran the program.

not be undone: that slot of memory will still hold the decremented value. because

gsl_stats march 24, 2009

dealing with memory

c

57

the ampersand

int *a = malloc(sizeof(int));

low-level work of    nding a free slot of memory, claiming it so nothing else on the

finally, we must contend with the pointer initialization on
line 15:

there are actually three character-
istics to a given pointer:
the lo-
cation (where the    nger is point-
ing),
the
amount of memory which
has been reserved for the pointer

malloc, a function declared in  d ib.h, is short for memory allocate. by the
library metaphor, a   
 builds bookshelves that will later hold data. just as we
have no idea what is in ani   variable before it is given a value, we have no idea
what addressa points to until we initialize it. the function a   
   will do the
computer uses it, and returning that address. the input to a   
 is the quantity
of memory we need, which in this case is the size of one integer: ize f i   .
the type (here,i  ), and
dress: if
 	   is an integer, then 
 	   is a pointer
   
 	   ==
 	  , which may imply that they
( ize f i    bytes   enough for
mind the type and size of your pointer. if you treat the data pointed to by ani  
pointer as if it is pointing to ad 	b e, then the computer will read good data as
by the way,i   k=7 will fail   the initialization on the declaration line is for
one convenience that will help with allocating pointers is
a   
, which you can
read as clear and allocate: it will run a   
 and return the appropriate address,
and will also set everything in that space to zero, running k=0 for you. sample

are symmetric, but the star will appear much more of-
ten in your code than the ampersand, and an ampersand
will never appear in a declaration or a function header.
as a mnemonic, ampersand, and sign, and address of
all begin with the letter a.

garbage, and if you read twenty variables from a space allocated for    fteen, then
the program will either read garbage or segfault.

the pointer, not the value the pointer holds. given that k is a pointer to an integer,
all of these lines are correct:

one integer). the location is up
to the computer   you should never
have to look at hexadecimal ad-
dresses. but you need to bear in

every variable has an address, whether you declared
it as a pointer or not. the ampersand    nds that ad-

int *k = malloc(sizeof(int));
*k = 7;
k = malloc(sizeof(int));

to an integer. the ampersand and star are inverses:

usage:

int *k = calloc(1, sizeof(int));

58

chapter 2

exchanges their values.

gsl_stats march 24, 2009

computer to do the freeing.

turns. check that it compiles.

the syntax requires that we explicitly state that we want one space, the size of

finally, both allocation and de-allocation are now your responsibility. the de-

an integer. you need to give more information than a   
 because the process
of putting a zero in ad 	b e pointer may be different from putting a zero in
ai   pointer. thus
a   
 requires two arguments:
a   
 e e e  _
 	   
 ize f e e e  _ y e  .
allocation comes simply by callingf ee k  when you are done with the pointer
k. when the program ends, the operating system will free all memory; some peo-
ple free all pointers at the end of ai  as a point of good form, and some leave the
write a function named wa  that takes two pointers toi   variables and
    first, write a ai  function that simply declares twoi  s (not point-
ers)fi    and e
  d, gives them values, prints the values and re-
    then, write a wa  function that accepts two pointers, but does noth-
ing. that is, write out the header but let the body be{}.
    call your empty function from ai . do you need to use fi   
(as per the box on page 57), fi   , or justfi   ? check that the
i   e  .) add a  i  f to the end of ai  to make sure that your
modify your swap program so that the two variables in ai  are now point-
ers toi  s.
    add allocations via a   
, either in the declaration itself or on an-
    which of fi   , fi   , orfi    will you need to send to wa 
now? do you need to modify the wa  function itself?
modify
a  byadd.
 so that the declaration on line 15 is for an integer, not
a pointer. that is, replace the currenti   a= a   
 ... ; withi  
a;. make the necessary modi   cations to ai  to get the program running.
do not modify thefa
   ia  function.

    finally, write the swap function itself. (hint: include a local variable

program still compiles.

function worked.

other line.

q2.10

q2.11

q2.12

gsl_stats march 24, 2009

c

59

   a variable can hold the address of a location in memory.

   by passing that address to a function, the function can modify that
location in memory, even though only copies of variables are passed
into functions.

   a star in a declaration means the variable is a pointer, e.g.,i   k.
which the pointer points, e.g., w _ i e = k  k.
i   i  ege _add e  =
 a   
,
 a   
  ize f i    ;.
f ee i  ege _add e   .

   when you are certain a pointer will not be used again, free it, e.g.,

a star in a non-declaration line indicates the data at the location to

to which a pointer
e.g.,

is pointing needs

prepared

   the

to be

space

using

tricks

2.7 arrays and other pointer

you can use a pointer as an array: in-
stead of pointing to a single integer, for
example, you can point to the    rst of a
sequence of integers. listing 2.9 shows some sample code to declare an array and
   ll it with square numbers:

int main(){

squares[i] = i * i;

for (int i=0; i < array_length; i++)

#include <stdlib.h>
#include <stdio.h>

int array_length=1000;
int *squares = malloc (array_length * sizeof(int));

listing 2.9 allocate an array and    ll it with squares. online source:  	a e .
.
pointer, except we needed to allocate a block of size1000  ize f i    in-
stead of just a single ize f i   . referring to an element of  	a e  uses

the syntax for declaring the array exactly matches that of allocating a single

}

identical syntax to the automatically-declared arrays at the beginning of this chap-
ter. internally, both types of array are just a sequence of blocks of memory holding
a certain data type.

60

chapter 2

q2.14

q2.13

gsl_stats march 24, 2009

but despite their many similarities, arrays and pointers are not identical: one is
automatically allocated memory and the other is manually allocated. given the
declarations

the listing in  	a e .
 is not very exciting, since it has no output. add
a secondf   loop to print a table of squares to the screen, by printing the
indexi and the value in the  	a e  array at positioni.
after thef   loop,  	a e [7    holds a plain integer (49). thus, you can
refer to that integer   s address by putting a  before it. extend your version
of  	a e .
 to use your swap function to swap the values of  	a e [7   
and  	a e [8   .
the    rst declares an automatically allocated array, just asi  i is automatically al-
a_ h 	 a d_d 	b e  until you decide to free it.
but be careful: if your functionf ees an automatically allocated array passed in
from the parent, or assigns it a new value with a   
, then you are stepping on
arrays of structs before, when we used the   	
  for complex numbers,
we referred to its elements using a dot, such asa. ea  or
b.i agi a y. for a pointer to a structure, use	> instead of a dot. here are some
examples using the de   nition of the
    ex structure from page 31.

located, and therefore the allocation and de-allocation of the variable is the respon-
sibility of the compiler. the second allocates memory that will be at the location

double a_thousand_doubles[1000];
// and
double *a_thousand_more_doubles = malloc(1000 * sizeof(double));

int a_function(double *our_array);
//and
int a_function(double our_array[]);

in function arguments, you can interchange their syntaxes. these are equivalent:

c   s turf, and will get a segfault.

complex *ptr_to_cplx = malloc (sizeof(complex));
ptr_to_cplx   >real = 2;
ptr_to_cplx   >imaginary =    2;
complex *array_of_cplxes = malloc (30 * sizeof(complex));
array_of_cplexes[15]   >real = 3;

reallocating if you know how many items you will have in your array, then you

c

61

gsl_stats march 24, 2009

that the machine hadn   t allocated for you   a segfault.

the computer. but if you try to put 301 elements in the list (which, you will recall,

if you get an error like request for member    real    in something not a structure or

that feedback to understand what you misunderstood, then switch to the other and
try again.

if you are not sure about the size of your array, then you will need to expand
the array as you go. listing 2.10 is a program to    nd prime numbers, with a few
amusing tricks thrown in. since we don   t know how many primes we will    nd, we

union then you are using a dot where you should be using	> or vice versa. use
probably won   t bother with pointers, and will instead use thei  
fixed_ i  [300    declaration, so you can leave the memory allocation issues to
means putting something infixed_ i  [300   ), then you will be using memory
need to use ea   
. the program runs until you hit <ctrl-c>, and then dumps
    line 13:s g  t is the signal that hitting <ctrl-c> sends to your program. by
one-line function on line 9 when it receives this signal. thus, thewhi e loop be-
    lines 16   17: check whether e   e is evenly divisible by  i e [i   . the sec-
ond element of thef   loop, the run-while condition, includes several conditions
shows how it is done. the newline \  is actually two steps: a carriage return (go to
beginning of line) plus a line feed (go to next line). the \  character is a carriage
return with no line feed, so the current line will be rewritten at the next  i  f.
theff 	 h function tells the system to make sure that everything has been written
to ea   
 is the pointer whose space needs resizing, and the second argument is
the new size. the    rst part of this new block of memory will be the  i e  array

    lines 20   24: having found a prime number, we add it to the list, which is a three-
step process: reallocate the list to be one item larger, put the element in the last
space, and add one to the counter holding the size of the array. the    rst argument

    line 19: computers in tv and movies always have fast-moving counters on the
screen giving the impression that something important is happening, and line 19

ginning at line 14 will keep running until you hit <ctrl-c>; then the program will
continue to line 26.

default, it halts your program immediately, but line 13 tells the system to call the

out the complete list to that point.

to the screen.

at once.

so far, and the end will be an allocated but garbage-   lled space ready for us to    ll
with data.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30

gsl_stats march 24, 2009

62

chapter 2

#include <math.h>
#include <stdio.h>
#include <signal.h>
#include <malloc.h>

int ct =0, keepgoing = 1;
int *primes = null;

void breakhere(){ keepgoing = 0; }

int main(){

int i, testme = 2, isprime;

signal(sigint, breakhere);
while(keepgoing){

isprime = 1;
for (i=0; isprime && i< sqrt(testme) && i<ct; i++)

isprime = testme % primes[i];

if (isprime){

printf("%i \r", testme); f   ush(null);
primes = realloc(primes, sizeof(int)*(ct+1));
primes[ct] = testme;
ct ++;

}

printf("\n");

}
testme ++;

printf("%i\t", primes[i]);

}
printf("\n");
for (i=0; i< ct; i++)

listing 2.10 find prime numbers and put them in an array. online source:  i e .
.
copy of a pointer is destroyed, so there is no way to refer to the a   
ed space in

figure 2.11 shows two errors in pointer handling. the
   rst step picks up from the second step of figure 2.7:
a function has been called with a pointer as an argument. then, the function frees
what the copy of a pointer is pointing to   and thus frees what the original    nger
was pointing to. next, it allocates new space, moving the copy of a    nger to point
to a new location. but when the function    nishes, depicted in the    nal step, the

the main program. we now have a pointer in the main frame with no space and a
space with no pointer.

returning to the library metaphor for a moment, the calling function wrote down
a call number on a slip of paper, handed it to the function, and the function then
went into the shelves and moved things around. but since the function has no
mechanism of telling the caller where it put things, the shelves are now a mess.

listing 2.12 is a repeat of the prime-   nding code, with the cute tricks removed and

some common faux pas

gsl_stats march 24, 2009

c

63

figure 2.11 how to mess up your pointers

#include <math.h>
#include <stdio.h>
#include <malloc.h>

void add_a_prime(int addme, int *ct, int **primes){
*primes = realloc(*primes, sizeof(int)*(*ct+1));
(*primes)[*ct] = addme;
(*ct)++;

}

int main(){

int ct =0, i, j, testme = 2, isprime, max = 1000;
int *primes = null;

for (j=0; j< max; j++){

isprime = 1;
for (i=0; isprime && i< sqrt(testme) && i<ct; i++)

testme ++;

if (isprime)

isprime = testme % primes[i];

add_a_prime(testme, &ct, &primes);

}
for (i=0;i< ct; i++)

listing 2.12 find prime numbers and put them in an array. online source:  i e 2.
.

printf("%i\t", primes[i]);

printf("\n");

}

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

the process of adding a prime relegated to a separate function (as it should be). the
wrong way to implement the function in lines 5   9 would be

void add_a_prime_incorrectly(int addme, int *ct, int *primes){

primes = realloc(primes, sizeof(int)*(*ct+1));
primes[*ct] = addme;
(*ct)++;

}

64

chapter 2

be an invalid location.

gsl_stats march 24, 2009

allocating new data at the new location   but whatever called that function has no

pointer   the location of the location of data. the calling function sends in the
location of the card catalog, and the called function can then revise the locations
listed in the card catalog when it makes changes on the shelves.

this commits the above faux pas of changing the value of the copy of  i e  and
idea about these changes in  i e , and will keep on pointing to what may now
the correct method is shown in listing 2.12. it passes a pointer to the  i e 
the syntax may seem confusing, but compare it to how
  is treated. because the
function will modify it, line 19 sends in its location, 
 , and the function header
has an extra star: instead ofi  
 , it refers to the addressi   
 . similarly,
the function call sends in the array   s address,   i e , and the function adds a
star to the header,i      i e .
kernighan & pike (1999) point out that ea   
 can be slow, so you are
that every time an array of length n is ea   
ed, its size be doubled to 2n.
thus, if an array will eventually have ten elements, it will be ea   
ed
rewrite the code in    gure  i e 2 to implement this method of selective
   arrays are internally represented as pointers. thei   a  ay[100   
thei   ha  ay=
 a   
  ize f i    100 ; form creates a manually-allocated
notation, such asha  ay[14   .
refer to the elements in a pointer-to-struct using	>.
   you can expand a manually-allocated array using ea   
.

when adding the    rst, second, fourth, and eighth data points, for a total of
four reallocations instead of ten.

form creates an automatically-allocated array, where the com-
puter

better off not calling it every time an array is extended. instead, they suggest

   arrays of structs can be declared just as with arrays of basic variables.

   refer to array elements in both cases using the same square-brackets

array that is yours to allocate and deallocate.

and destroys

reallocation.

creates

array;

the

q2.15

   just as copies of normal variables are passed to functions, copies of
pointers are sent in. therefore, be careful when modifying a pointer
in a subfunction.

gsl_stats march 24, 2009

c

65

2.8 strings

strings of characters.

followed by an invisible null character, written as \

there are three approaches to dealing with c   s awkwardness regarding text. you
can (and should) skim the standard library documentation, to get to know what
functions are always available for the most common string operations. you can

c   s handling of text strings is simple, elegant, unpleasant and
awkward. although this book is oriented toward programs about
manipulating numbers rather than building web pages or other such text, words
and phrases are inevitable in even the mathiest of programs. so this section will
cover the basics of how a system deals with variable-length text.

(see chapter 6 on glib). or, you can leave c entirely and do text manipulation
via a number of command-line tools or a text-focused language like ruby or perl
(see appendix b). nonetheless, all of these methods are based on c   s raw string-
handling at their core, and the raw c methods are always at hand, so it is worth
getting to know c   s string-handling even if you prefer higher-level methods.

use a higher-level library-provided data type for text, such as glib   sgs  i g type
c implements lines of text such as"he   " as arrays of individual characters,
0. that is, you may think of the
above single word as a shorthand for the array{ h   e              
 \0 }. this means that you have to think in terms of arrays when dealing with
static form here or via a   
.
    but it is a common error to think that line three will copy the text" i." into
he   , but as with pointers to integers, the actual function of third line is a pointer
operation: instead of copying the data pointed to byhe   2 to the location pointed
to byhe   , it simply copies the location ofhe   2. when you change the text
27line four will compile, because the string" i he e." is held in memory somewhere, so the pointer
he    can point to it. however, a pointer to literal text is a
     pointer, meaning that the    rst time you try to
changehe   , the program will crash.

at one pointer later on, the other (now pointing to the same location) will change
as well. along a similar vein, line four also does not behave as you may expect.27

the    rst implication of the elegant use of arrays to represent text is that your
expectations about assignment won   t work. here are some examples:

char hello[30];
char hello2[] = "hi.";
hello = hello2; //this is probably not what you meant.
hello = "hi there"; //nor is this.

    line two shows that, as with arrays of integers, we can specify a list to put in to

    line one shows that strings are declared with array-style declarations, either of the

the array when we initialize the array, but not later.

1
2
3
4

gsl_stats march 24, 2009

66

chapter 2

#include <string.h>
strncpy(hello, "hi there.", 30);
strncpy(hello, hello2, 30);

continuing the above example, this is the right way to copy data into

there are two lengths associated with a string pointer: the    rst is the space

instead, there are a series of functions that take in strings to copy and otherwise
handle strings.

    e 
 a   
ed for the pointer, and the second is the number of characters until
the string-terminating \0  character appears. size of free memory is your re-
sponsibility, but    e  y 	 _   i g  will return the number of text characters
iny 	 _   i g.
    
 y
he   :
the third argument is the total space a   
ed tohe   , not    e  he    .
    
a 
for example,    
a  he    he   2 30  will leave" i he e. e   ."
inhe   .
being changed, then ea   
 the string to the appropriate size, then    nally
write a functiona    
 y 
ha   ba e 
ha  
  y e , that will
copy
  y e to ba e. internally, it will use    e , ea   
, and
    
 y to execute the three steps of string extension. all of the above dis-
once this function is working, write a functiona    
a  that executes the

cussion regarding re-pointing pointers inside a function applies here, which
is why the function needs to take in a pointer-to-pointer as its    rst argument.

the key problem with pointers-as-strings is that editing a string often be-
comes a three-step problem: measure the length the string will have after

rather than overwriting one string with another, you can also append (i.e.,
concatenate) one string to another, using

make the change to the string.

strncat(base_string, addme, freespace);

q2.16

same procedure for string concatenation. after you   ve tested both functions,
add them to your library of utilities.

c

67

gsl_stats march 24, 2009

wasted memory on a modern computer, but this method is also error-prone: what

prints its output to a string instead of the screen.28 the    ll-in-the-blanks

blanks are    lled in. one way to make sure there is enough room for adding more
text would be to simply allocate an absurd amount of space for each string, like

#include <string.h>
...
int string_length = 1000;
char write_to_me[string_length];
char name[] = "steven";
int position = 3;
snprintf(write_to_me, string_length, "person %s is number %i in line\n", name, position);

    i  f
the    i  f function works just like the  i  f statements above, but
syntax for  i  f works in exactly the same manner with strings.
however, the three-step process of measuring the new string, calling ea   
,
and then    nally modifying the string is especially painful for    i  f, because
it is hard to know how long the  i  f-style format speci   er will be after all its
just under a megabyte of memory:
ha he   [1000000   . you won   t notice the
if you have a brilliant idea about af   loop that will add a little text for each of a
a   i  f
phenia, you could usea   i  f, which allocates a string that is just big
enough to handle the inputs, and then runs    i  f. here is a simple example,
with no memory allocation in sight, to print to i e.
once again, becausea   i  f will probably move i e in memory, we need to
you can comfortably puta   i  f into af   loop without worrying about over-
28the other nice feature of    i  f is that it is more secure: other common functions like   
 y and
   i  f do not check the length of the input, and so make it easy for you to inadvertently overwrite important
29becausea   i  f does not free the current space taken up by   i g before reallocating the new version,
thisf   loop is a memory leak. in many situations it isn   t enough of a leak to matter, but if it is, you will need to
use:f   i  i=0;i<100;i   {

ha     =   i g;
a   i  f     i g "   i"    i g i ;
f ee     ;
}

bits of memory with the input string, including the location of the next instruction to be executed. unsafe string-
handling functions are thus a common security risk, allowing the execution of malicious code.

char *line;
asprintf(&line, "%s is number %i in line.", "steven", 7);

   ow. here is a snippet to write a string that counts to a hundred:29

if you are using the gnu c library, bsd   s standard c library, or apo-

send the location of the pointer, not the pointer itself.

million data points into the string?

gsl_stats march 24, 2009

}

68

chapter 2

asprintf(&string, "%s %i", string, i);

procedure directly, e.g.:30

char *string = null;
char int_as_string[10000];
for (int i =0; i< 100; i++){

int newlen = strlen(string) + 10000;
string = realloc(string, newlen);
snprintf(int_as_string, 10000, " %i", i);
strncat(string, int_as_string, newlen);

char *string = null;
asprintf(&string, ""); //initialize to empty, non   null string.
for (int i =0; i< 100; i++)

if you don   t havea   i  f on hand, you can hack your way through by guess-
ing the    nal length of the    lled-in string and running the measure/ ea   
/write
    the c standard de   nes ize f 
ha  ==1, so it is safe to write ew e  in the
place of ize f 
ha    ew e .31
modify  i e 2.
 to write to a string named  i e _  _fa  rather than
printing to the screen. as the last step of the program,  i  f   i e _	
  _fa  .
is seven. (hint: if you have written a number to    i g, you can compare
the last element in    i g   s array of characters to the single character
 7 .)
because strings are arrays, the formif "  e"==" a e"  does not make
   
  
sense. instead, the   
   function goes through the two arrays of charac-
you are encouraged to not use   
  . if 1 and 2 are identical strings, then
   
    1  2 ==0; if they are different, then   
    1  2 !=0. there is
30it would be nice if we could use    i  f    i g  ew e  "   i"    i g i , but using
    i  f to insert   i g into   i g behaves erratically. this is one more reason to    nd a system with
a   i  f.

see listing 3.7, page 112, for another example of extending a string inside a loop.

ters you input, and determines character-by-character whether they are equal.

modify the program in the last exercise to print only primes whose last digit

q2.18

q2.17

31iso c standard, committee draft, iso/iec 9899:tc2,   6.5.3.4, par 3.

q2.19

c

69

gsl_stats march 24, 2009

enced programmers the world over regularly get this wrong.

other, so when they are equal, then the difference is zero; when they are not equal,

a rationale for this: the   
   function effectively subtracts one string from the
the difference is nonzero. but the great majority of humans readif    
    1 
 2   to mean    if 1 and 2 are the same, do the following.    to translate that en-
glish sentiment into c, you would need to useif !   
    1  2  . experi-
strings are not identical. feel free to use   
   internally.
   strings are actually arrays of
ha s, so you must think in pointer
2.9   errors

   there are a number of functions that facilitate copying, adding to, or
printing to strings, but before you can use them, you need to know
how long the string will be after the edit.

add a function to your library of convenience functions to compare two
strings and return a nonzero value if the strings are identical, and zero if the

the compiler will warn you of syntax errors, and you have seen
how the debugger will help you    nd runtime errors, but the best
approach to errors is to make sure they never happen. this section presents a few
methods to make your code more robust and reliable. they dovetail with the notes
above about writing one function at a time and making sure that function does its
task well.

terms when dealing with them.

testing the inputs here is a simple function to take the mean of an input ar-

ray.32

mean += in[i];

return mean/length;

double mean = in[0];

for (int i=1; i < length, i++)

double    nd_means(double *in, int length){

what happens if the user calls the function with a u   pointer? it crashes. what
happens if e g h==0? it crashes. you would have an easy enough time pulling
32normally, we   d just assignd 	b e ea =0 at    rst and loop beginning withi=0; i used this slightly odd

}

initialization for the sake of the example. how would the two approaches differ for a zero-length array?

gsl_stats march 24, 2009

70

below is a version offi d_ ea   that will save you trips to the debugger. it
introduces a new member of the  i  f family:f  i  f, which prints to    les
note that writing to the  de   stream withf  i  f is the appropriate means of

out the debugger and drilling down to the point where you sent in bad values, but
it would be easier if the program told you when there was an error.

and streams. streams are discussed further in appendix b; for now it suf   ces to

chapter 2

displaying errors.

double    nd_means(double *in, int length){

if (in==null){

fprintf(stderr, "you sent a null pointer to    nd_means.\n");
return nan;

}
if (length<=0){

fprintf(stderr, "you sent an invalid length to    nd_means.\n");
return nan;

}
double mean = in[0];
for (int i=1; i < length, i++)

mean += in[i];

return mean/length;

}

the   and|| are perfect for inserting quick tests, because the left-hand side can

this took more typing, and does not display the brevity that mathematicians ad-
mire, but it gains in clarity and usability. listing conditions on the inputs provides
a touch of additional documentation on what the function expects and thus what it
will do. if you misuse the function, you will know the error in a heartbeat.

test for validity and the right-hand side will execute only if the validity test passes.
for example, let us say that the user gives us a list of element indexes, and we will
add them to a counter only if the chosen array elements are even. the quick way
to do this is to simply use the % operator:

if (!(array[i] % 2))

evens += array[i];

but if the array index is invalid, this will break. so, we can add tests before testing
for evenness:

if (i > 0 && i < array_len && !(array[i]%2))

evens += array[i];

c

gsl_stats march 24, 2009

ifi is out of bounds, then the program just throwsi out and moves on. when
list:a  e  .
a  e   thea  e   macro makes a claim, and if the claim is false, the program
housekeeping like checking for u   pointers. here is the above input-checking
function rewritten usinga  e  :

failing silently is ok, these types of tests are perfect; when the system should
complain loudly when it encounters a failure, then move on to the next tool in our

halts at that point. this can be used for both mathematical assertions and for

71

#include <assert.h>

double    nd_means(double *in, int length){

assert (in!=null);
assert (length>0);
double mean = in[0];
for (int i=1; i < length, i++)

mean += in[i];

return mean/length;

}

assert: your_program.c:4:    nd_means: assertion    length > 0    failed.
aborted

if your assertion fails, then the program will halt, and a notice of the failure will
print to the screen. on a gcc-based system, the error message would look some-
thing like

some people comment out the assertions when they feel the program is adequately
debugged, but this typically saves no time, and defeats the purpose of having the
assertions to begin with   are you sure you   ll never    nd another bug? if you   d like

to compare timing with and without assertions, the -d debug    ag to the compiler
(just add it to the command line) will compile the program with all thea  e  
array of a million elements, ea  will grow to a million times the average
assertions about the inputs. for a solution, see the gslg  _ve
   _ ea 
function, or the code ina   _db_   i e.
.

value before being divided down to its natural scale.
rewrite the function so that it calculates an incremental mean as a function
of the mean to date and the next element. given the sequence x1, x2, x3, . . . ,
the    rst mean would be   1 = x1, the second would be   2 =   1
2 , the
third would be   3 = 2  2
3 , et cetera. be sure to make the appropriate

the method above for taking a mean runs risks of over   ow errors: for an

statements skipped over.

3 + x3

2 + x2

q2.20

gsl_stats march 24, 2009

72

chapter 2

main function.

test functions the best way to know whether a function is working correctly is
to test it, via a separate function whose sole purpose is to test the

likefi d_ ea   a  ay4 	3  will fail appropriately. here is a function to run
fi d_ ea   through its paces:

a good test function tries to cover both the obvious and strange possibilities: what
if the vector is only one element, or has unexpected values? do the corner cases,
such as when the input counter is zero or already at the maximum, cause the
function to fail? it may also be worth checking that the absolutely wrong inputs,

void test_   nd_means(){

double array1[] = {1,2,3,4};
int length = 4;

assert(   nd_means(array1, length) == 2.5);

double array2[] = {infinity,2,3,4};

assert(   nd_means(array2, length) == infinity);

double array3[] = {   9,2,3,4};

assert(   nd_means(array3, length) == 0);

double array4[] = {2.26};

assert(   nd_means(array4, 1) == 2.26);

}

writing test functions for numerical computing can be signi   cantly harder than
writing them for general computing, but this is no excuse for skipping the testing
stage. say you had to write a function to invert a ten-by-ten matrix. it would take
a tall heap of scrap paper to manually check the answer for the typical matrix. but
you do know the inverse of the identity matrix (itself), and the inverse of the zero
matrix (nan). you know that x  x   1 = 1 for any x where x   1 is de   ned. errors
may still slip through tests that only look at broad properties and special cases, but
that may be the best you can do with an especially ornery computation, and such
simple diagnostics can still    nd a surprising number of errors.

some programmers actually write the test functions    rst. this is one more manner
of writing an outline before    lling in the details. write a comment block explaining
what the function will do, then write a test program that gives examples of what
the comment block described in prose. finally, write the actual function. when the
function passes the tests, you are done.

 e  _fi d_ ea   function, but as you write more functions and their tests, they

once you have a few test functions, you can run them all at once, via a supple-
mentary test program. right now, it would be a short program that just calls the

can be added to the program as appropriate. then, when you add another test, you
will re-run all your old tests at the same time. peace of mind will ensue. for ulti-
mate peace of mind, you can call your test functions at the beginning of your main

gsl_stats march 24, 2009

c

73

analysis. they should take only a microsecond to run, and if one ever fails, it will
be much easier to debug than if the function failed over the course of the main
routine.

q2.21

write a test function for the incremental mean program you   d written above.
did your function pass on the    rst try?
some programmers (donald knuth is the most famous example) keep a bug
log listing errors they have committed. if your function didn   t pass its test
the    rst time, you now have your    rst entry for your bug log.

time, soa  e   your expectations to ensure that they are met.

   before you have even written a function, you will have expectations
about how it will behave; express those in a set of tests that the func-
tion will have to pass.

   you also have expectations about your function   s behavior at run

q2.22

this chapter stuck to the standard library, which is installed by default with
any c compiler. the remainder of the book will rely on a number of li-
braries that are commonly available but are not part of the posix standard,
and must therefore be installed separately, including apophenia, the gnu
scienti   c library, and sqlite. if you are writing simulations, you will need
the glib library for the data structures presented in chapter 6.
now that you have compiled a number of programs and c source is not so
foreign, this is a good time to install these auxiliary libraries. most will be
available via your package manager, and some may have to be installed from
c source code. see the online appendix (linked from the book   s web site,

h   ://  e  .  i 
e   .ed	/ i  e /8706.h   ) for notes on    nd-

ing and installing these packages, and appendix a for notes on preparing
your environment.

gsl_stats march 24, 2009

3

databases

there is a way between voice and presence
where information    ows.

   rumi (2004, p 32)

structured query language (sql1) is a specialized language that deals only with
the    ow of information. some things, like joining together multiple data sets, are
a pain using traditional techniques of matrix manipulation, but are an easy query
in a database language. meanwhile, operations like id127 or inver-
sion just can not be done via sql queries. with both database tables and c-side
matrices, your data analysis technique will be unstoppable.

as a broad rule, try to do data manipulation, like pulling subsets from the data
or merging together multiple data tables, using sql. then, as a last step, pull the
perfectly formatted data into an in-memory matrix and do the statistical analysis.

is not nearly as complex as c. here is some valid sql: e e
 age ge de  
yea f    	 vey. that   s almost proper english. it goes downhill from there in

because sql is a specialized language that deals only with information    ows, it

terms of properness, but at its worst, it is still not dif   cult to look at an sql query
and have some idea of what the rows and columns of the output table will look
like.

1some people pronounce sql as sequel and some as ess queue ell. the of   cial iso/iec standard has no

comment on which is correct.

gsl_stats march 24, 2009

databases

like c, sql is merely a language, and it is left to the programmers of the world
to write code that can parse sql and return data from sql queries. just as this

book leans towardg

 to interpret c code, it recommends the sqlite library, by
program,   i e3, but there are many other alternatives that are more reminiscent

d richard hipp, to interpret code written in sql. sqlite provides a library of
functions that parse sql queries and uses those instructions to read and write a
speci   c format of    le [see binary trees in chapter 6]. any program that uses the
sqlite function library is reading and writing the same    le format, so sqlite    les
can be traded among dozens of programs.

as with c utilities, the only problem is selecting which sqlite database viewer
to use among the many options. the sqlite library comes with a command-line

75

of the table view in the standard stats package or spreadsheet; ask your search
engine for sqlite browser or sqlite gui. these programs will give you immediate
feedback about any queries you input, and will let you verify that the tables you
are creating via c code are as you had expected.

why is sqlite lite? because most sql-oriented databases are designed to be used
by multiple users, such as a    rm   s customers and employees. with multiple users
come issues of simultaneous access and security, that add complications on top of
the basic process of querying data. sqlite is designed to be used by one user at
a time, which is exactly right for the typical data analysis project. if you hope to
use another database system, you will need to learn the (typically vendor-speci   c)
commands for locking and permissions.

check that both the sqlite executable and development libraries are cor-
rectly installed. in the online code supplement, you will    nd an sqlite-

this chapter will primarily consist of an overview of sql, with which you can
follow along using any of the above tools. section 3.5 will describe the apophenia
library functions that facilitate using an sql database such as sqlite or mysql
from within a c program.

formatted database namedda a	wb.db listing the 2005 gdp and popula-
using one of the above tools (e.g.,   i e3da a	wb.db from the com-
 e e
  f      ;.
of the queries in this chapter are also in the 	e ie     le in the online code

once you have a working query interpreter, you can follow along with the
discussion in this chapter. for your cutting and pasting convenience, most

mand prompt), and that you can execute and view the results of the query

tion for the countries of the world. verify that you can open the database

q3.1

supplement.

gsl_stats march 24, 2009

76

chapter 3

data format a database holds one or more tables. each column in a table repre-
sents a distinct variable. for example, a health survey would include
columns such as subject   s age, weight, and height. expect the units to be different
from column to column.

naming a row, although it is common enough to have a plain column named  w_	
 a e, or another identi   er such as  
ia _ e
	 i y_   that serves this purpose.

each row in a table typically represents one observation. for example, in a survey,
each row would be data about a single person. there is no mechanism in sql for

the asymmetry between columns and rows will be very evident in the syntax for
sql below. you will select columns using the column name, and there is no real
mechanism for selecting an arbitrary subset of columns; you will select rows by
their characteristics, and there is no real mechanism to select rows by name.2

your c-side matrices will generally be expected to have a similar format; see page
147 for further notes.

3.1 basic queries

thusly:

sql   s greatest strength is selecting subsets of a data set.
if you need all of the data for those countries in the world

most of the world   s data sets are already in this format. if your data set is not,
your best bet is to convert it rather than    ghting sql   s design; see the notes on
crosstabs, page 101, for tips on converting from the most common alternative data
format.

bank data set (da a	wb.db) with populations under 50 million, you can ask for it
you can read this like english (once you know that  means    all columns   ): it will
   nd all of the rows in a table named    where   	 a i   in that row is less
2if there is a  w_ a e variable, then you could select rowswhe e  w_ a e=   e , but that is simply
selecting rows with the characteristic of having a  w_ a e variable whose value is   e . that is, column

than or equal to 50, and return all the columns for those rows.

select *
from pop
where population <= 50;

names are bona    de names; row names are just data.

77

the

select

databases

rating two columns.

table will have;

commas and semicolons

gsl_stats march 24, 2009

commas are separators, meaning that the last ele-
ment in a comma-separated list must not have a comma

generally,
gives a list of columns that
output
clause declares where the source

in sql, semicolons are terminators for a given com-
mand. you can send two sql commands at once,
each ending with a semicolon. many sqlite-based
programs will forgive you for omitting the    nal semi-
colon.

clause lists restrictions on the rows
to be output. and that   s it. every
query you run will have these three
parts in this order: column speci   -
cation, data source, row speci   ca-
tion.3 this simple means of specify-
ing rows, columns, and source data
allows for a huge range of possibili-
ties.

the e e
  statement
thef   
data comes from; and thewhe e
after it. for example, if you write a query like e e
 

 	   y     f      	 a i   then you will
get an error like    syntax error nearf       which is re-
ferring to the comma just beforef    that is not sepa-
the e e
  clause will specify the columns of the table that will be output.
the easiest list is , which means    all the columns   . other options:
 e e
 
 	   y    	 a i  
 e e
    .   	 a i   gd .
 	   y
 e e
    .
 	   ya 
 	   y gd a gd _i _ i  i   _	 d
if you do not alias   .
 	   ya 
 	   y, then you will need to use the name
   \.
 	   y in future queries, which is a bit annoying.
 e e
 
 	   y gd  0.506a gd _i _gb 
thea gd _i _gb  subclause is again more-or-less essential if you hope to refer
from thef    clause speci   es the tables from which you will be pulling data. the
simplest case is a single table:f   da a_ ab, but you can specify as many
tables as necessary:f   da a_ ab1 da a_ ab2.
you can alias the tables, for easier reference. the clausef   da a_ ab1d1 

this is unnecessary now, but will become essential when dealing with multiple
tables below.

    generate your own new columns. for example, to convert gdp in dollars to gdp

    explicitly mention the table(s) from which you are pulling data:

in british pounds using the conversion rate as of this writing:

    rename the output columns:

    explicitly list the columns:

to this column in the future.

3you may have no row restrictions, in which case your query will just have the    rst two parts and a null third

part.

78

rather

chapter 3

tirely independent tables.

gsl_stats march 24, 2009

borrowing c   s annoyances

sql accepts c-style block comments of the form

notice, by the way, that when we

comments as c (see p 25). with one-line comments,

return the real number it should. the add-zero trick

produces an integer, not
mans expect. thus,

another option is to take data from subqueries; see below.

also following c   s lead, dividing two integers
the real number we hu-
than calculating, say,

aliasing is generally optional but
convenient, but one case where it
is necessary arises when you are
joining a table to itself. for now,

da a_ ab2d2 gives short names to both tables, which can be used for lines like
 e e
 d1.age d2.heigh .
/ ... /. it has the same trouble with nested block
simply note the syntax:f   da a
everything after two dashes,		, is ignored, compara-
 1 da a 2 will let you refer to
ble to the two slashes,//, in c. [mysql users will
theda a table as if it were two en-
need two dashes and a space:		 .]

 	  1/
 	  2, cast one of the columns to a real
aliased something in the e e
 
number by adding0.0: 
 	  1 0.0 /
 	  2 will
the form was e e
 
   g_
  _de 
 i  i  a  
d,
also works to turn the string"1990" into the number
while in thef    section there
1990. [sql has a
a   keyword, but it is much easier
to just use the trick of adding0.0.]
is no as:f      g_fi e_ a e
 f .4
thewhe e clause is your chance to pick out only those rows that interest
you. with nowhe e clause, the query will return one line for every line
the e e
  clause). for example, try e e
 1f   gd  using theda a	wb.db
you can use the boolean operators you know and love as usual:whe e  d1.age
>13    d2.heigh >=175  a d d1.weigh =70 . sql does not re-
ally do assignments to variables, so the clause d1.weigh =70  is a test for
 d1.weigh ==70 ; other sql parsers (like mysql) are less forgiving and con-
    you can select based on text the same way you select on a number, such aswhe e

 	   y= u i eds a e  . any string that is not an sql keyword or the
4thea  is actually optional in the e e
  clause, but it improves readability.
bene   cial that sql uses single-ticks while c uses double-ticks, because    i  f   100 " e e
  
whe e
 	   y= 	a a  "  requires no unsightly backslashes, while double-tick quotation marks do:
    i  f   100 " e e
  whe e
 	   y= \"	a a \"" .

equality, not an assignment. sqlite is easygoing, and will also accept the c-format

in your original table (and the columns returned will match those you speci   ed in

name of a table or column must be in    single-tick    quotation marks.5

5again, sqlite is forgiving, and will also accept c-style    double-tick    quotation marks. however, it is

sider the double-equals to be an error.

database.

section,

where

q3.2

79

databases

gsl_stats march 24, 2009

generalizing from equality and inequalities, you may want a group of elements or

tion of your home country. once you know this amount, select all of the
countries that are more populous than your country.

    case matters: u i eds a e  != 	 i ed  a e  . however, there is an
out should you need to be case-insensitive: the ike keyword. the clausewhe e

 	   y ike 	 i ed  a e   will match the fully-capitalized country name
as well as the lower case version. the ike keyword will even accept two wild
cards:_ will match any single character, and  will match any set of characters.
both
 	   y ike 	 i  a e   and
 	   y ike 	 i ed_  a e   will
match u i eds a e  .
    thewhe e clause refers to the root data, not the output, meaning that you can
readily refer to columns that you do not mention in the e e
  clause.
use awhe e clause and the   	 a i   table to    nd the current popula-
a range. for this, there are thei  andbe wee  keywords. say that we want only
thei  keyword typically makes sense for text data; for numeric data you probably
write a query using<= and>= to replicate the above query that used
be wee .
   the columns are speci   ed in the e e
  statement. you can pull all
the columns from the data using e e
  , or you can specify indi-
vidual columns like e e
 a b  a 0.0 /ba  a i .    

the united states and china in our output. then we would ask only for columns
where the country name is in that short list:

select *
from gdp
where country in ("united states", "china")

select *
from gdp
where gdp between 10000 and 20000

want a range. here are the countries with gdp between $10 and $20 billion:

   a query consists of three parts: the columns to be output, the data

source, and the rows to be output.

q3.3

gsl_stats march 24, 2009

80

   

tables.

chapter 3

ditions that all rows must meet. it can be missing (and so all possible

clude several auxiliary clauses to re   ne the output further. here is the complete

   the data source is in thef    clause, which is typically a list of
   the row speci   cation, generally in thewhe e clause, is a list of con-
rows are returned) or it can include a series of conditions, likewhe e
 a=b a d b<=
 .
3.2   doing more with queries
beyond the basic e e
 	f   	
whe e format, a e e
  query can in-
format of a e e
  query, which this section will explore clause by clause.
pruning rows withdi  i 
  theda a	 e   .db    le includes a listing of all
which the station lies. the query e e
  i ef    i e  produces massive
thedi  i 
  keyword will tell the sql engine that if several rows would be
thedi  i 
  word prunes the rows, but is placed in the e e
  portion of the
 e e
  statement speci   es the columns and thewhe e statement speci   es the

redundancy, because there are a few dozen stations on every line, so each color
appears a few dozen times in the table.

select [distinct] columns
from tables
where conditions
group by columns
having group_conditions
order by columns
limit n offset n

program. this reads more like english, but it breaks the story above that the

exact duplicates, to return only one copy of that row. in this case, try

stations and the color of the subway line(s) on

select distinct line
from lines

rows.

81

databases

q3.4

gsl_stats march 24, 2009

select count(*) as row_ct
from gdp;

tion for the answer will appear in the section on joins, below.

this produces a table with one column and one row, listing the total number of

you probably want more re   nement than that; if you would like to know how much

aggregation here is how to get the number of rows in thegd  table ofda a	wb.db:
rows in theda a table.
how many rows does e e
  f       gd  produce? the explana-
data you have in each region, then use theg  	 by clause to say so:
after
 	  , the two most common aggregation commands are 	    andavg  .
these take an existing row as an argument. for example, theda a	 a    .db
feel free to specify multipleg  	 by clauses. for example, you could modify
the above query to sort by race and age by changingg  	 by a
e tog  	 
by a
e  a     . yea  fbi  h . when you want to analyze the output,
you will be very interested in thea   _db_  _
     ab function; see page 101.
in the  e
i  table of theda a	
 i a e.db database, theyea     h col-
umn encodes dates in forms like199608 to mean august, 1996. fortunately,
the sql-standard  	 d   function can be used to produce a plain year:
  	 d 199608./100. ==1996.0. use  	 d,g  	 by, andavg to
   nd the average precipitation ( 
 ) in each year.
you can use
 	   with thedi  i 
  keyword to    nd out how many of each row

database has a single table representing a telephone survey regarding tattoos. to
get the average number of tattoos per person broken down by race, you could use
this query:

select class, count(*) as countries_per_class
from classes
group by class;

select race, avg(tattoos.   ct tattoos ever had   )
from tattoos
group by race;

q3.5

you have in a table. this is useful for producing weights for each observation type,

gsl_stats march 24, 2009

82

function

abs, avg, count, max, min, round,a sum

standard
sql

   

mysql

   

chapter 3

sqlite via
apophenia

   

   

   

   

rand, sin, sqrt, stddevs,

ran, vars, skews, kurtosiss, kurts

around is not part of the sql standard, which instead

acos, asin, atan, cos, exp, ln, log10,
pow,
tan,
variancep, stdp, stddev_popp, stddev_-
samps, var_samps, var_popp

as in this query to produce a tabulation of respondents to the tattoo survey by race
and birth year:

table 3.1 standard sql offers very few mathematical functions, so different systems offer different
extensions. the p and s subscripts indicate functions for populations or for samples (see
box on page 222.

providesf     and
ei .
with ag  	 by command, you have two levels of elements, items and groups,
with awhe e clause. similarly, you can exclude some groups from your query
using thehavi g keyword. for example, the above query produced a lot of low-
weighted groups. what groups have a
 	     >4? we can   t answer this using
whe eweigh >4, because there is noweigh  column in the data table, only in
the post-aggregation table. this is where thehavi g keyword comes in:
  sql extensions

select distinct race, tattoos.   year of birth    as birthyear, count(*) as weight
from tattoos
group by race, birthyear
having weight > 4

select distinct race, tattoos.   year of birth    as birthyear, count(*) as weight
from tattoos
group by race, birthyear

and you may want subsets of each. as above, you can get a subset of the items

that   s all the aggregators you get in standard sql. so imple-
menters of the sql standard typically add additional functions
beyond the standard; see table 3.1 for a list, including both aggregation functions

83

databases

gsl_stats march 24, 2009

select *
from pop
order by country

to get the desired statistics on the matrix side.

the list of country populations in alphabetical order, use

and the standard and mysql both include several functions for manipulation of
text, dates, and other sundry types of data; see the online references for details.

bear portability in mind when using these functions, and be careful to stick to
the sql standard if you ever hope to use your queries in another context. if you

likeva  and r     r functions like  g. the table focuses on numeric functions,
want to stay standard, call your data into a c-side vector or matrix and usea   _	
ve
   _  g,a   _ve
   _ex ,a   _ve
   _ kew,a   _ve
   _va , . . . ,
sorting to order the output table, add an  de by clause. for example, to view
    you may have multiple elements in the clause, such as  de by
 	   y    .
    the keywordde 
, short for descending, will reverse the order of the variable   s
sorting. sample usage:  de by
 	   yde 
    .
a e e
  clause. the output may be a million lines long, but twenty should be
enough to give you the gist of it, so use a i i  clause. for example, the follow-
ing query will return only the    rst twenty rows of the    table:
you may want later rows, and so you can add the ff e  keyword. for example,
will see rows 4   8. beyond making interactive querying easier, i i 	 ff e 

getting less especially when interactively interrogating a database, you may
not want to see the whole of the table you have constructed with

will return the    rst    ve rows, after discarding the    rst three rows. thus, you

if there are ties in the    rst variable, they are broken by the second.

select *
from pop
limit 5 offset 3

select *
from pop
limit 20

84

table.

chapter 3

gsl_stats march 24, 2009

clauses can also be used to break tables that are somehow giving you problems

which may not be representative. if this is a problem, you can
take a random draw of some subset of your data. ideally, you could provide a query

above.6 for every call to the function (and thus, for every row), it draws a uniform
random number between zero and one.7

into more manageable pieces, probably via a c-sidef   loop.
    you get one i i / ff e  per query, which must be the last thing in the query.
    if you are using	 i   and family to combine e e
  statements (see below), your
 i i  clause should be at the end of all of them, and applies only to the aggregate
  random subsets
the i i  clause gives you a sequential subset of your data,
like e e
  f   da awhe e a d  <0.14 to draw 14% of your data.
sqlite-via-apophenia and mysql provide a a d function that works exactly as
creating tables there are two ways to create a table. one is via a
 ea e state-
ment and then ani  e   statement for every single row of
data. the
 ea e statement requires a list of column names;8 thei  e   state-
thebegi -
   i  wrapper, by the way, means that everything will happen in
6standard sql   s a d   function is absolutely painful. sqlite   s version currently produces a number be-
never need; read1<<x as 2x. that said, e e
  f   da awhe e  a d    / 	 1<<63 	1.0  1 /2
<0.14 will pull approximately 14% of the data set.
usea   _db_  g_i i  7 . if you do not call this function, the database rng auto-allocates at    rst use with
table declarations that look a little like c functions, e.g.,
 ea e ab e ew ab  a eva 
ha [30    age
i   ; see your database engine documentation for details.

tween   9,223,372,036,854,775,807, which the reader will recognize as   (263    1). so we need to pull a random
number, divide by 263     1, shift it to the familiar [0, 1] range, and then compare it to a limit. standard sql does
not even provide exponentiation, so doing this requires the bit-shifting operator which i had promised you would

begin;
create table newtab(name, age);
insert into newtab values("joe", 12);
insert into newtab values("jill", 14);
insert into newtab values("bob", 14);
commit;

7after you read section 11.1, you will wonder about the stream of random numbers produced in the database.
there is one stream for the database, which apophenia maintains internally. to initialize it with a seed of seven,

memory until the    nal commit. the program may run faster, but if the program

8sqlite has the pleasant property that its columns are basically type-less. other database engines insist on

ment requires a list of one data element for each column.

seed zero.

gsl_stats march 24, 2009

85

save:

databases

create table tourist_traps as

select country
from lonely_planet
where (0.0+pp) > 600

the other method of creating a table is by saving the results of a query. simply

crashes in the middle, then you will have lost everything. the optimal speed/secu-
rity trade-off is left as an exercise for the reader.

the command-line program with the same name. the form above is mostly useful
in situations where you are creating the table in mid-program, as in the example
on page 108.

if you have hundreds or thousands ofi  e  s, you are almost certainly better off
putting the data in a text    le and using either the c functiona   _ ex _  _db or
put
 ea e ab e ew ab_ a ea  at the head of the query you would like to
the ide   table of theda a	 e   .db database includes the average
its opening. create a ide  _ e _yea  table with one column for the year
see alsoa   _ ab e_exi    on the c-side (p 108), which can also delete tables
named  wid. it is a simple integer counting from one up to the number of rows,
and does not appear when you query e e
  f    ab e. but if you query
 e e
   wid  f    ab e, then the hidden row numbers will appear in the
9mysql users will need to explicitly ask for such a column when creating the table. a statement like
 ea e
 ab e ew ab id_
  	  i  a	  _i 
 e e   i f 1
ha  30  i f 2d 	b e ...  will cre-
ate the table with the typical columns that you will    ll, plus anid_
  	   that the system will    ll. after
i  e  i    ew abva 	e  "  e" 23 ;i  e  i    ew abva 	e  " a e"21.8 ;, the ta-
ble will have one row for joe whereid_
  	  ==1 and one for jane whereid_
  	  ==2.

sometimes, you need a unique identi   er for each output row. this would
be dif   cult to create from scratch, but sqlite always inserts such a row,

and one column for total average boardings across the system for the given
year.

boardings in each station of the washington metro system, every year since

drop table newtab;

if desired.

output.9

q3.6

rowid

dropping a table the converse of table creation is table dropping:

86

chapter 3

page 106.

q3.7

gsl_stats march 24, 2009

among countries in the world bank database.

in practical terms, this table is primarily good for getting the lay of an unfamil-

metadata what tables are in the database? what are their column names? stan-
dard sql provides no easy way to answer these questions, so every
database engine has its own speci   c means. sqlite gives each database a table

using  de by and  wid,    nd the rank of your home country   s gdp
named   i e_ a  e  that provides such information. it includes the type of ob-
ject (either index or table, in the y e column), the name (in the a e column),
and the query that generated the object (in the    column). mysql users, see
iar database   a quick e e
  f      i e_ a  e ; when you    rst open the
database never hurts. if you are using the sqlite command line, there is a. ab e
command that does exactly what this program does. thus, the command   i e3
 ydb.db. ab e just lists available tables, and the. 
he a command gives all
of the information from   i e_ a  e .
de e e unliked   , which acts on an entire table,de e e acts on individual rows
make a copy, e.g., via
 ea e ab egd 2a  e e
  f   gd ]:
i  e  
sawi  e   used above in the context of creating a table and then inserting
elements item-by-item. you can also insert via a query, via the formi  e  i   
exi  i g_ ab e e e
  f   ....

sql is primarily oriented toward the    ltering style of pro-
gram design: e.g., have one query to    lter a data table to pro-
duce a new table with bad data removed, then have another query to    lter the
resulting table to produce an aggregate table, then select some elements from the
aggregate table to produce a new table, et cetera.

but you will often want to modify a table in place, rather than sending it through a
   lter to produce a new table (especially if the table is several million entries long).
sql provides three operations that will modify a table in place.

of a database. for example, to remove the columns with missing gdp data,
you could use this query [   but before you destroy data in the sample databases,

the obvious complement to deleting lines is inserting them. you already

delete from gdp
where gdp=   ..   

modifying tables

87

databases

gsl_stats march 24, 2009

update pop
set population=26783
where country=   iraq   

gives you a sequential snippet, or via random draws.

   the sql standard includes a few simple aggregation commands:

vide a few more nonstandard aggregators for queries called using its
functions.

example, the world bank refrained from estimating iraq   s 2006 population,
but the us central intelligence agency   s world factbook for 2006 estimates it at

	 da e
the	 da e query will replace the data in a column with new data. for
26,783,383. here is how to change iraq   s population (in the    table) from.. to
26783:
   you can limit your queries to fewer rows using a i i  clause, which
avg  , 	   , and
 	    , and most sql implementations pro-
   when aggregating, you can add ag  	 by clause to indicate how
   sort your output using an  de by clause.
   you can create tables using the
 ea e andi  e   commands, but
d    to delete a table.
   sqlite gives every row a  wid, though it is hidden unless you ask
data from disparate sources. the joining process is not based on aj i  keyword,
but simply specifying multiple data sources in thef    section of your query and
describing how they mesh together in thewhe e section.
if you specify two tables in yourf    line, then, lacking any restrictions, the
      ; then e e
 

so far, we have been cutting one table down, ei-
ther by selecting a subset of rows or by group-
ing rows. sql   s other great strength is in building up tables by joining together

       and table 2 have one column with data      

column with data      

database will return one joined line for every pair of lines. let table 1 have one

you are probably better off just reading the table from a text    le. use

the aggregation should be grouped.

joins and subqueries

for it explicitly.

a
b
c

1
2
3

3.3

88

chapter 3

gsl_stats march 24, 2009

3    3 = 9 rows:

 f    ab e1  ab e2 will produce an output table with every combination,
1a1b1
2a2b2
3a3b3
.
how joining the 208 countries in the world bank data   s    table with the same
208 countries in thegd  table produces a few hundred pages of rows.
thus, thewhe e clause becomes essential. its most typical use for a join arises
whe e   .
 	   y=gd .
 	   y, and so the query makes sense only when

when one column in each table represents identical information. out of the 43,264
rows from the above join, including those that matched qatar with ghana and
cameroon with zimbabwe, we are interested only in those that match qatar with
qatar, cameroon with cameroon, and so on. that is, we want only those rows

such a product quickly gets overwhelming: in the exercise on page 81, you saw

that restriction is added in:

select pop.country, pop.population, gdp.gdp

from pop, gdp
where pop.country = gdp.country

you can see that using the table-dot-column format for the column names is now

essential. in the e e
  clause specifying the output columns, you can use either
   .
 	   y orgd .
 	   y, since the two will be by de   nition identical, or if
add a calculation to the e e
  portion of the above query to    nd the gdp
likegd _ e _
a  so you can  de bygd _ e _
a .

you are unconcerned with the country names and just want the numeric data you
can omit names entirely.

per capita of each country. be sure to give the calculated column a name,

q3.8

gsl_stats march 24, 2009

q3.9

example: a time lag

89

databases

region, and divide by total population in the region.

the world bank data includes a classi   cation for each country. countries
receiving world bank assistance (what the wb calls client countries) are
classed by region (e.g., middle east and north africa), while other countries
are binned into a generic class like    lower-middle-income economies.   
find the total gdp per capita for each world bank grouping. here, you will

join using the country columns in thegd  and
 a  e  table, and by the
country columns in the    and
 a  e  table. add up total gdp in the
theda a	
 i a e.db database includes a table of the deviation from the century-
for methods, caveats, and discussion). a quick e e
  f    e   will show
the fact that there are separateyea  and    h columns. one solution would be
to deal only withyea      h/12., which moves through time in smooth in-
values is not reliable:1900 1./12.	1./12. could wind up as something
like1900.00001, and a test whether this value exactly equals1900 will fail. as a

the form above, where two columns match, is by far the
most common type of join, but there are other creative uses
of joins. for example, it is common in time series analysis to include the value of
a variable at time t     1 as data that in   uenced the value at time t.

that there is an upward trend in the data: the    rst few years are all below zero; the
last few years hover around 0.5.10

12 . this creates its own problem, because comparing    oating-point

long norm for aggregate worldwide temperatures (see smith & reynolds (2005)

what does the month-to-month change look like? the    rst step is dealing with

crements of 1

variant that solves this problem, instead of dividing months by 12, multiply years
by 12, so that we are comparing only integers:

select r.year+r.month/12., r.temp     l.temp
from temp l, temp r
where r.year*12 +r.month = l.year*12 +l.month +1;

the salient feature of this data set is that not much happens. the long-term shift is
the result of a large number of very small month-to-month changes.

10chapter 5 will cover graphing, but for now, trya   _    _ 	e yda a	
 i a e.db" e e
  e  
f    e  " from your command line to get a visual indication of the trend.

90

q3.10

gsl_stats march 24, 2009

chapter 3

ginning to show a pattern?

perhaps we would see a larger change via a larger time span. calculate the
year-to-year differences.

    create ana  	a ized table with two columns: the year and average
 e   over all months for the year.
yea  is a string, then it will treatyea  0.0 as a number.
    create ade
ade  table with the average for each decade. (hint:
g  	 by  	 d yea /10 .)

    join that table with itself lagged by one year. you won   t have to worry
about unreliable    oat comparisons, but recall that if sqlite thinks

having looked at year-long differences, try decades.

    join the table with itself lagged by ten years. are the differences be-

select l.temp     r.temp
from temp l, temp r
where r.rowid+0.0=l.rowid   1;

given that the data is sorted, we could also have done the matching of rows using

the  wid:
then joining them using a clause likewhe ea=b requires 1e6    1e6 = 1e12 (a
11say that you mean to join a million subjects via id number, via e e
  1.   2. f    1  2
whe e 1.id= 2.id, but you forget to include thewhe e clause. then you just asked the system to cre-

trillion) comparisons. this is impossibly slow, so there are a number of tricks to
avoid making all those 1e12 comparisons.11

create index pop_index on population(country)
create index gdp_index on gdp(country)

to use in a join later. the commands:

ate a trillion-entry table, which will take from several hours to weeks. thus, the    rst step in speeding up an
inordinately slow query is not to try the tricks in this section, but to make sure that you actually wrote the query
you had intended to write.

speeding it up now that you have seen how to join tables, we now cover how to
avoid joining tables. if two tables have a million elements each,

indices you can ask the sql engine to create an index for a table that you intend

91

databases

gsl_stats march 24, 2009

you should prepare by creating another index that puts that column in the    rst (or
the only) position.

sounds nice to you. once you have created this index, a join using any of the
indexed columns goes much faster, because the system no longer has to do 1e12

is 17   and then check the right table   s index for the list of elements whose value
is 17. that is, instead of one million comparisons to join the    rst element, it only
has to do one index lookup. the lookup and the process of building the tree took
time as well, but these processes are on the order of millions of operations, not
millions squared. the tree is internally structured as a binary tree; see chapter 6
for discussion of b-trees.

would index the    andgd  tables on the
 	   y column. the name of the
index, such as   _i dex, is basically irrelevant and can be any gibberish that
comparisons. basically, it can look at the    rst value ofva  in the left table   say it
there is standard sql syntax for indexing multiple columns, e.g.,
 ea ei dex
   _i dex2      
 	   y    	 a i   , which goes by lexicographic or-
der. this is just an index on the    rst item (
 	   y) with the second column
(   	 a i  ) as a backup ordering; if you want to join by the second column,
grouping? answering this question is a two-step process: get a
 	      for each
 e e
  statement directly into the query where it is used:
the query inside thef    clause will return a table, and even though that table
output needs a name, you can alias the result as usual:f     e e
 ...  1
will allow you to refer to the query   s output as 1 elsewhere in the query.

category, and then get an average of that. you could run a query to produce a table
of counts, save the table, and then run a query on that table to    nd the averages.

has no name, it can be used as a data source like any other table. if the query

but rather than generating a temporary table, sql allows you to simply insert the

create table temptab as
select count(*) as ct

from classes
group by class)

from classes
group by class;

from (select count(*) as ct

from temptab

select avg(ct)

select avg(ct)

subqueries among sql   s nicest tricks is that it allows for the input tables to be
queries themselves. for example: how large is the average world bank

gsl_stats march 24, 2009

92

chapter 3

q3.12

q3.11

subsetting via a foreign table

join syntax from the head of this section.

write a query to pull only the gdp of countries where the population is

but the full join (as per the exercise) is not necessary: we are not particularly
concerned with the population per se, but are just using it to eliminate rows. it

if you look at the world bank data, you will see a
large number of countries that are small islands of
a few million people. say that we are unconcerned with these countries, and want

on page 79, you    rst found your home country   s population, then the coun-
tries with populations greater than this. use a subquery to do this in one
query. (hint: you can replace a number with a query that returns one ele-
ment.)

only the gdp of countries where   	 a i  >270.
greater than 270 million, using the standardwhe e ef 
  = igh 
  
would thus be logical to    t the query into thewhe e clause, since that is the clause
directly into awhe e...i  clause:
the boost in ef   ciency implies some slight restrictions: because thef    clause
  joining via af   loop

this is typically much faster than a full join operation, because there was no need
to make (left table row count)    (right table row count) comparisons.

the subquery will return a list of country names, and the main query can then use
those as if you had directly typed them in.

does not list the table used in the subquery, you can not refer to any of the sub-
query   s columns in the output.

select *
from gdp
where country in (select country from pop where population > 270)

that is typically used to select a subset of the rows. indeed, we can put a query

the time it takes to do an especially large join is not
linear in the number of rows, primarily for real-world
reasons of hardware and software engineering. if your computer can not store all
the data points needed for a query in fast memory, it will need to do plenty of
swapping back and forth between different physical locations in the computer. but
your computer may be able to store a hundredth or a thousandth of the data set
in fast memory, and so you can perhaps get a painfully slow query to run in    nite

gsl_stats march 24, 2009

93

databases

time by breaking it down into a series of shorter queries.

tens of millions of values. even after creating the appropriate indices, the straight
join   

here is an example from my own work (baum et al., 2008). we had gathered
550,000 genetic markers (snps) from a number of pools of subjects, and wanted

the mean for each pool. omitting a few details, the database included a     
table with the subjectid and the    id of its pool, with only about a hundred
elements; and a table of individualids, thes   labels, and their values, which had
our solution was to use a c-sidef   loop, plus subsetting via a foreign table,
create a blank table to be    lled, get a list of    ids, and then usei  e  i   
... e e
 ... to add each    id   s data to the main table. the details of the

select pools.poolid as poolid, snp, avg(val) as val, var(val) as var
from genes, pools
where genes.id=pools.id
group by pools.poolid, snp

to avoid the join that was taking so long. there are three steps to the process:

   was taking hours.

functions will be discussed below, but these three steps should be evident in this
code snippet.

apop_query("create table t (poolname, snp, val, var);");
apop_data *names = apop_query_to_text("select distinct poolid from pools");
for (int i=0; i< names   >textsize[0]; i++)

apop_query("insert into t \n\

", names[i][0], names[i][0]);

select    %s   , snp, avg(val), var(val) \n\
from genes \n\
where id in (select id from pools where poolid =    %s   ) \n\
group by snp; \n\

f   loop using the i i ... ff e  form can also break a too-long table into

this allowed the full aggregation process to run in only a few minutes. the next
week we bought better hardware.

as noted above, if there is no natural grouping like the pools in this example, a

smaller pieces.

stacking tables you can think of joining two tables as setting one table to the
right of another table. but now and then, you need to stack one

on top of the other. there are four keywords to do this.

94

chapter 3

gsl_stats march 24, 2009

select id, age, zip
from data_set_1
union
select id, age, zip
from data_set_2

will produce the results of the    rst query stacked directly on top of the second
query. be careful that both tables have the same number of columns.

   u i  : sandwiching	 i   between two complete queries, such as
   u i  a  : if a row is duplicated in both tables, then the	 i   operation throws
out one copy of the duplicate lines, much like e e
 di  i 
  includes only
one of the duplicates. replacing	 i   with	 i  a   will retain the duplicates.
      e  e
 : as you can guess, puttingi  e  e
  between two e e
  state-
   ex
e  : this does subtraction, returning only elements from the    rst table that do
   you can put the output of a query into thef    clause of a parent
   you can join tables by listing multiple tables in thef    clause. when
you do, you will need to specify awhe e clause, and possibly the
di  i 
  keyword, to prevent having an unreasonably long output
   if the join still takes too long, you can sidestep it via the e e
 ...
whe e
  i   e e
 ...  form, or via a c-sidef   loop.
   tables can be stacked using	 i  ,	 i  a  ,i  e  e
 , and
ex
e  .

not appear in the second. notice the asymmetry: nothing in the second table will
appear.

ments returns a single copy of only those lines that appear in both tables.

   if you intend to join elements, you can speed up the join immensely

by creating an index    rst.

query.

table.

3.4 on database design

say that you are not reading in existing data, but
are gathering your own, either from a simulation
or data collected from the real world. here are some considerations and sugges-
tions for how you could design your database, summarizing the common wisdom
about the best way to think about database tables.

gsl_stats march 24, 2009

databases

95

the basic premise is that each type of object should have a single table, and each
object should have a single row in that table.

figure 3.2 shows a table of observations for a generic study involving several sub-
jects and treatments, whose information was measured at several times. the simple
one-table design is how the typical spreadsheet is designed. this version has one
row per subject, so each row has two observations, and information about subjects,
treatments, observations, and pools are mixed together.

figure 3.3 shows a structure better suited for databases. for most statistical studies,
the key object is the observation, and that gets its own table; we now see that there
were twenty observations. the other objects in the study   subjects, pools, and
treatments   all get their own tables as well. by giving each element of each table
an id number, each table can easily cross-reference others. this setup has many
advantages.

minimize redundancy

this is rule number one in database design, and many a
book and article has been written about how one goes about
reducing data to the redundancy-minimized normal form (codd, 1970). if a human
had to enter all of the redundant data, this creates more chances for error, and the
same opportunities for failure come up when the data needs to be modi   ed when
somebody notices that there were actually nine subjects in the pool from 6/2/02. in
the single-table form, information about the pool was repeated for every member of
the pool, while having a separate table for pools means that each pool   s information
is listed exactly once.

ask non-observation questions

there are reasons to ask questions based on treat-
ments or pools, but a setup with only an obser-
vation-based table does not facilitate this. from the multiple tables, it is easy to
ask questions that focus on data, treatments, or pools, via join operations on the
observation, pool, subject, or treatment ids.

gelman & hill (2007, p 239) point out that separating subjects and groups facil-
itates multilevel models, where each group has parameters for its own submodel
estimated, and then those parameters are used to estimate an overall model. this
sort of modeling will be covered in later chapters.

use the power of row subsets

figure 3.2 includes multiple observations on one
line, for the morning and evening measurements.
but what if we went from two observations to hourly observations for 24 hours?
remember, there is no way to arbitrarily select a subset of columns, so columns

gsl_stats march 24, 2009

96

chapter 3

subjid
1
2
3
4
5
6
7
8
9
10

value_morn
23.28
14.07
20.98
12.12
30.28
22.15
19.78
21.53
27.42
18.57

value_eve
nan
nan
nan
nan
28.11
14.05
12.54
9.01
23.20
12.29

poolcount
12
12
12
12
11
11
8
8
19
19

pooldate
2/2/02
2/2/02
2/2/02
2/2/02
4/2/02
4/2/02
4/2/02
4/2/02
6/2/02
6/2/02

t_dosage

t_type
control nan
control nan
control nan
control nan
case
case
case
case
case
case

0.2
0.2
0.4
0.4
0.2
0.2

figure 3.2 spreadsheet style: one monolithic table, with much redundancy.

obsid
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

subjid
1
2
3
4
5
6
7
8
9
10
1
2
3
4
5
6
7
8
9
10

value
time
23.28 morn
14.07 morn
20.98 morn
12.12 morn
30.28 morn
22.15 morn
19.78 morn
21.53 morn
27.42 morn
18.57 morn
nan
nan
nan
nan
28.11
14.05
12.54
9.01
23.20
12.29

eve
eve
eve
eve
eve
eve
eve
eve
eve
eve

subjid
1
2
3
4
5
6
7
8
9
10

poolid
1
2
3
4

poolid
1
1
1
1
2
2
3
3
4
4

treatmentid
1
1
1
1
2
2
3
3
2
2

poolcount
12
11
8
19

pooldate
2/2/02
4/2/02
4/2/02
6/2/02

treatmentid
1
2
3
4

t_dosage

t_type
control nan
case
case
case

0.2
0.4
0.6

figure 3.3 database style: one table for each object type, one row for each object.

97

databases

simply use:

select avg(value)

gsl_stats march 24, 2009

from observations
where time like    %am   

if there is any chance that two observations will somehow be compared or aggre-
gated, then they should probably be recorded in different rows of the same column.
for 24 hours and ten subjects, the table would be 240 rows, which is not nearly as
pleasing or human-digestible as a 10    24 spreadsheet. but you will rarely need
to look at all the data at once, and can easily construct the crosstab if need be via

named1a ,2a , . . . , would be dif   cult to use. if we needed the mean of all morn-
ing observations, we   d need to do something like e e
  12a  1a  2a 
 3a  ... /12, but if the table in figure 3.3 had an hour column, we could
(orwhe e i e<12, depending on the format we choose for the time).
a   _db_  _
     ab.
it is easy to e e
  f   a  da awhe e e a e_v  ei     	   if
 e e
  f   a  da awhe e   	 a i  >  e e
    	 a i  f   
a  da awhe e  a e= dc   won   t work: it will return only 49 out of 50 states, because the population of

even worse than having two data points of the same type in separate columns is
having two data points of the same type in separate tables, such as a cases table
and a controls table. or, say that a political scientist wants to do a study of county-
level data throughout the united states, including variables such as correlations
between tax rates, votes by senators, and educational outcomes. because dc has
no county subdivisions and its residents have no congressional representation, the
dc data does not    t the form of the data for the states and commonwealths of the
united states. but the correct approach is nonetheless to put dc data in the same
table as the counties of the    fty states, rather than creating a table for dc and a
table for all other states   or still worse, a separate table for every state.

   bear in mind the tools you have when designing your table layouts.
it is easy to join tables,    nd subsets of tables, and create spreadsheet-
like crosstabs from data tables.

   databases are not spreadsheets. they are typically designed for many

   each type of object (observations, treatments, groups) should have a

single table, and each object should have a single row in that table.

dc   s lack of representation will affect the analysis.12

tables, which may have millions of rows if necessary.

12by

way,

the

dc (zero senators, zero representatives) is 572,000, while wyoming (two senators, one representative) has a
population of 494,000. [2000 census data]

gsl_stats march 24, 2009

98

chapter 3

command-line utilities

3.5 folding queries into c code

when the program exits. apophenia uses only one database at a time, but see the

will live on your hard drive. this is slower than memory, but will exist after you
stop and restart the program, and so other programs will be able to use the    le, you
have more information for debugging, and you can re-run the program without

this section covers the functions in
the apophenia library that will cre-
ate and query a database. all of these functions are wrappers of functions in the
sqlite or mysql libraries that do the dirty work, but they are suf   ciently com-
plete that you should never need to use the functions in the sqlite/mysql c
libraries directly. the details of the main discussion will apply to sqlite; mysql
users, see page 106 for the list of differences.

importing the    rst command you will need isa   _  e _db. if you give it the
name of a    le, likea   _  e _db "  	dy.db" , then the database
re-reading in the data. conversely, if you give a null argument   a   _  e _	
db  u      then the database is kept in memory, and will run faster but disappear
a   _ e ge_db  and sqlite   sa  a
h functions below.
   le. thea   _ ex _  _db func-
need to write a full-blown c program.a   _ ex _	
  _db reads a text    le into a database table,a   _	
 e ge_db  will send tables from one database to an-
other,a   _    _ 	e y will send query output di-
rectly to gnuplot, anda   _db_  _
     ab will
ties, you can use the	h parameter to get detailed in-
structions (e.g.,a   _    _ 	e y	h).
queries, runa   _
   e_db to close the database. if you send the function a
one   a   _
   e_db 1    then sqlite will take a minute to clean up the da-
the simplest function isa   _ 	e y, which takes a single text argu-
is appropriate for
 ea e ori  e   queries:

tion will do this for you, or you can
try it on the command line (see box).
the    rst line of the text    le can be
column names, and the remaining
rows are the data. if your data    le is
not quite in the right format (and it
rarely is), see appendix b for some
text massaging techniques.

tabase before exiting, leaving you with a smaller    le on disk; sending in a zero
doesn   t bother with this step. of course, if your database is in memory, it   s all
moot and you can forget to close the database without consequence.

take a table from the sqlite database and produce a
crosstab. all of these are simply wrappers for the cor-
responding apophenia functions. for all of the utili-

unless your program is generat-
ing its own data, you will probably
   rst be importing data from a text

apophenia includes a handful of command-line util-
ities for handling sqlite databases where there is no

ment: the query. this line runs the query and returns nothing, which

when you are done with all of your

the queries

gsl_stats march 24, 2009

databases

int page_limit = 600;
apop_query(

99

"create table tourist_traps as \
select country \
from lonely_planet \
where (pp + 0.0) > %i ", page_limit);

    as the example shows, all of apophenia   s query functions accept the  i  f-

    a string is easiest for you as a human to read if it is broken up over several lines;
to do this, end every line with a backslash, until you reach the end of the string.
the next example will use another alternative.

style arguments from page 26, so you can easily write queries based on c-side
calculations.

int page_limit = 600;
apop_data *tourist_traps = apop_query_to_text(

"select country "
"from lonely_planet "
"where (0.0+pp) > %i ", page_limit);

there are also a series of functions to query the database and put the result in a c-
side variable. this function will run the given query and return the resulting table
for your analysis:

    c merges consecutive strings, so" e e
 
 	   y""f   " will be merged
into" e e
 
 	   yf   ". we can use this to split a string over several lines.
but be careful to include whitespace:" e e
 
 	   y""f   " merges into
" e e
 
 	   yf   ".
after this snippet,  	 i  _  a   is allocated,    lled with data, and ready to use   
unless the query returned no data, in which case it is u  . it is worth checking for
 u   output after any query that could return nothing. there area   _ 	e y_...
functions for all of the types you will meet in the next chapter, includinga   _	
 	e y_  _ a  ix to pull a query to ag  _ a  ix,a   _ 	e y_  _ ex  to
pull a query into the text part of ana   _da a set,a   _ 	e y_  _da a to pull
data into the matrix part, anda   _ 	e y_  _ve
    anda   _ 	e y_  _	
f  a  to pull the    rst column or    rst number of the returned table into ag  _	
ve
    or ad 	b e.
for immediate feedback, you can usea   _da a_ h w to dump your data to
screen ora   _da a_  i   to print to a    le (or even back to the database). if

you want a quick on-screen picture of a table, try

apop_data_show(apop_query_to_data("select * from table"));

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

gsl_stats march 24, 2009

100

4, so the application of the e 
a  function may mystify those reading this book
sequentially. but the ai  function should make sense: it opens the database, sets
thea   _    .db_ a e_
  	   to an appropriate value, and then usesa   _	
 	e y_  _da a to pull out a data set. its last two steps do the math and show the

listing 3.4 gives an idea of how quickly data can be brought from a database-side
table to a c-side matrix. the use of these structures is handled in detail in chapter

chapter 3

results on screen.

#include <apop.h>

void percap(gsl_vector *in){

double gdp_per_cap = gsl_vector_get(in, 1)/gsl_vector_get(in, 0);
gsl_vector_set(in, 2, gdp_per_cap); //column 2 is gdp_per_cap.

}

int main(){

}

apop_opts.verbose ++;
apop_db_open("data   wb.db");
strcpy(apop_opts.db_name_column, "country");
apop_data *d = apop_query_to_data("select pop.country as country, \

    line 11: as above, sql tables have no special means of handling row names,

pop.population as pop, gdp.gdp as gdp, 1 as gdp_per_cap\
from pop, gdp \
where pop.country == gdp.country");
apop_matrix_apply(d   >matrix, percap);
apop_data_show(d);
apop_opts.output_type =    d   ;
apop_data_print(d, "wbtodata_output");

listing 3.4 query populations and gdp to ana   _da a structure, and then calculate the gdp per
capita using c routines. online source:wb  da a.
.
whilea   _da a sets can have both row and column labels. you can seta   _	
    .db_ a e_
  	   to a column name that will be specially treated as holding
row names for the sake of importing to ana   _da a set.
planning ahead: it is dif   cult to resizeg  _ a  ixes anda   _da a sets, so we
old print functions likea   _da a_  i   anda   _ a  ix_  i  .
lines 18   19 of listing 3.4 will write the data table to a table namedwb  da a_	
 	  	 . say that tomorrow you decide you would prefer to have the data dumped
to a    le; then just change the d  to an f  and away you go.

query a table of the appropriate size, and then    ll the column of dummy data with
correct values in the c-side matrix.

    lines 12   15: the    nal table will have three columns (pop, gdp, gdp/cap), so the
query asks for three columns, one of which is    lled with ones. this is known as

to go from c-side matrices to database-side tables, there are the plain

data to db

gsl_stats march 24, 2009

crosstabs

101

databases

conversely, the most convenient form for this data in a database is three columns:

in the spreadsheet world, we often get tables in a form where both the x-
and y-dimensions are labeled, such as the case where the x-dimension
is the year, the y-dimension is the location, and the (x, y) point is a measurement
taken that year at that location.

year, location, statistic. after all, how would you write a query such as e e
 
  a i  i
f    abwhe eyea <1990 if there were a separate column for
provides functions to do conversions back and forth,a   _db_  _
     ab and
a   _
     ab_  _db.
imagine a data table with two columns,heigh  andwid h, whereheigh  may
take on values like	 , idd e, ord w , andwid h takes on values like ef  and
 igh . then the query

each year? converting between the two forms is an annoyance, and so apophenia

create table anovatab as

select height, width, count(*) as ct
group by height, width

will produce a table looking something like

height width

up
up

middle
middle
down
down

left
right
left
right
left
right

ct
12
18
10
7
6
18

then, the command

will put intoa  va_ ab data of the form

apop_data *anova_tab = apop_db_to_crosstab("anovatab", "height", "width", "ct");

left right
12
10
6

18
7
18

up

middle
down

q3.13

multiple databases

102

chapter 3

gsl_stats march 24, 2009

online reference for details.

for both sql and c, the dot means subelement. just as a

you can print this table as a summary, or use it to run anova tests, as in sec-

the typical database system (including mysql and sqlite) begins with one da-

ble of temperatures, where each row is a year and each column a month.
import the output into your favorite spreadsheet program.

tion 9.4. thea   _
     ab_  _db function goes in the other direction; see the
use the command-line programa   _db_  _
     ab (or the corre-
sponding c function) and theda a	
 i a e.db database to produce a ta-
c   	
  named e     might have a subelement named
 e    .heigh , the full name of a column isdb a e. ab e a e.
   a e.
tabase open, which always has the alias ai , but allows you to attach additional
databases. for sqlite, the syntax is simplya  a
hda aba e" ewdb.db"a 
dba ia ; after this you can refer to tables via thedba ia . ab e a e form.
for mysql, you don   t even need thea  a
h command, and can refer to tables in
other mysql databases using thedb a e. ab e a e form at any time.
aliases again help to retain brevity. instead of using the fulldb. ab e.
   format
for a column, this query assigns aliases for thedb. ab e parts in thef    clause,
then uses those alises in the e e
  clause:
given two attached databases, say ai  and ew, you could easily copy tables
apophenia also provides two convenience functions,a   _db_ e ge anda   _	
db_ e ge_ ab e, which facilitate such copying.

attach database newdb as n;
select t1.c1, t2.c2
from main.   rsttab t1, n.othertab t2

as select * from main.origial

create table new.tablecopy

between them via

in-memory databases are faster, but at the close of the program, you may want
the database on the hard drive. to get the best of both worlds, use an in-memory
database for the bulk of the work, and then write the database to disk at the end of
the program, e.g.:

gsl_stats march 24, 2009

databases

103

}

int main(void){

    removing the    le before merging prevented the duplication of data (because du-

apop_db_open(null); //open a db in memory.
do_hard_math(...);
remove("on_disk.db");
apop_db_merge("on_disk.db");

    e  ve is the standard c library function to delete a    le.
   open an sqlite database in memory usinga   _db_  e   u   ,
and on the hard drive usinga   _db_  e  "fi e a e" .
   import data usinga   _ ex _  _db.
   if you don   t need output, usea   _ 	e y to send queries to the da-
   usea   _ 	e y_  _ da a| a  ix|ve
   | ex |f  a  

plicate tables are appended to, not overwritten).

tabase engine.

to

write a query result to various formats.

spaces in column names

3.6 maddening details

data are never as clean as it seems in the text-
books, and our faster computers have done noth-
ing to help the fact that everybody has different rules regarding how data should
be written down. here are a few tips on dealing with some common frustrations of
data importation and use; appendix b offers a few more tools.

 e 
e   f a e  ea  e  1
a e  h wi g   y ig   f a	 ea,
give a brief name like a e_ 1_  de a e, and then create a documentation table
the query e e
   e 
e   f a e   ea  e  1 f   da a will pro-
duce a table with the literal string e 
e   f a e   ea  e  1 repeated
notation to specify a table: e e
 da a.  e 
e   f a e   ea  e  1 

column names should be short and have no punctu-
ation but underscores. instead of a column name like

for each row, which is far from what you meant. the solution is to use the dot

not everybody follows this advice, however, which creates a small frustration.

that describes exactly what that abbreviation means.

text and numbers

104

chapter 3

gsl_stats march 24, 2009

alias that is much easier to use.

in some cases, you need both text and numeric data in the same

structure includes slots for both text and numbers, so you only need to specify

elements. this provides maximal    exibility, but requires knowing exactly what
the query will output.13

a  a e _ 1f   da a will correctly return the data column, and give it an
data set. as you will see in the next chapter, thea   _da a
which column goes where. the    rst argument to thea   _ 	e y_  _ ixed_	
da a function is a speci   er consisting of the letters ,v, , , indicating whether
each column should be read in to the outputa   _da a   s name, vector, a matrix
column, or a text column. for example,a   _ 	e y_  _ ixed_da a "   " 
" e e
 a b  i 
f   da a" 
 	  e   would use columna as the row
names,b 
 	  e  as the    rst column of the matrix, and
 as a column of text
now that you have text in ana   _da a set, what can you do with it? in most
everybody represents missing data differently. sqlite uses u   to
can take a  values, whose use is facilitated by the gsl   sgs _ a  macro. the
typical input data set indicates a missing value with a text marker like a ,..,	,
	1, a, or some other arbitrary indicator.
when reading in text, you can seta   _    .db_ a  to a regular expression that
at the    rst row of data and use that to cast the entire column, but if the    rst element in a column is a , then

//apophenia   s default nan string, matching nan, nan, or nan:
strcpy(apop_opts.db_nan, "nan");
//literal text:
strcpy(apop_opts.db_nan, "missing");
//matches two periods. periods are special in regexes, so they need backslashes.
strcpy(apop_opts.db_nan, "\\.\\.");

cases, the data will be unordered discrete data, and the only thing you can do with
it is to turn it into a series of dummy variables. see page 123 for an example.

matches the missing data marker. if you are unfamiliar with id157,
see appendix b for a tutorial. for now, here are some examples:

13why doesn   t apophenia automatically detect the type of each column? because it stresses replicability, and
it is impossible to replicably guess column types. one common approach used by some stats packages is to look

indicate missing data; section 4.5 will show that real numbers in c

numeric data may wind up as text or vice versa, depending on arbitrary rules. the system could search the entire
column for text and presume that some count of text elements means the entire column is text, but this too is
error-prone. next month, when the new data set comes in, columns that used to be auto-typed as text may now
be auto-typed as numbers, so scripts written around the    rst data set break. explicitly specifying types may take
work, but outguessing the system   s attempts at cleaning real-world data frequently takes more work.

missing data

105

databases

gsl_stats march 24, 2009

the searched-for text must be the entire string, plus or minus surrounding quota-

mathematically, any operation on unknown data produces an unknown result, so
you will need to do something to ensure that your data set is complete before mak-
ing estimations based on the data. the na  ve approach is to simply delete every ob-
servation that is not complete. allison (2002) points out that this na  ve approach,
known in the jargon as listwise deletion, is a somewhat reasonable approach, espe-
cially if there is no reason to suspect that the pattern of missing data is correlated
to the dependent variable in your study.14 missing data will be covered in detail on
page 345.

tion marks or white space. none of these will match a cy or i  i g e     .
once the database has a u   in the right place, apophenia   s functions to read
between databases on one side andg  _ a  ixes,a   _da a, and other c struc-
tures on the other will translate between database u  s and    oating-pointgs _	
 a s.
implementing listwise deletion in sql is simple: givenda a
  1 andda a
  2,
add awhe eda a
  1i     	  a dda a
  2i     	   clause to
whe e da a
  1 da a
  2 i     	  .
using the above notes and theda a	 a    .db    le, query to ana   _	
da a set the number of tattoos, number of piercings, and the political af-
   liation of each subject. make sure that all a s are converted to zeros at
some point along the chain. print the table to screen (viaa   _da a_ h w)
litical parties in the data set. (hint: e e
 di  i 
 .) write af   loop
outlier, via awhe e clause restricting the tattoo count to under 30?
table. the world bank database includes a   e y_  a e  table list-

to run through the list,    nding the mean number of tattoos and piercings for
democrats, republicans, . . . . would you keep the last person in the survey
(who has far more tattoos than anybody else) or eliminate the person as an

to make sure that all is correctly in place. then, query out a list of the po-

your query. if both are numeric data, then you can even summarize this to

ing the number of pages in the given country   s lonely planet tourist guidebook.
antarctica has a 328-page guidebook, but no gdp and a negligible population, so
the query

14systematic relationships between missingness and the independent variables is much less of a concern.

outer join another possibility is that a row of data is entirely missing from one

q3.14

gsl_stats march 24, 2009

106

chapter 3

select pp, gdp

from lonely_planet lp, gdp
where lp.country=gdp.country

gd  table. the solution is the 	 e j i , which includes all data in the    rst
   .
 	   y=gd .
 	   y) now appears in a different location from the norm,

table, plus data from the second table or a blank if necessary. here is a join that
will include antarctica in its output. the condition for joining the two tables (join

will not return an antarctica line, because there is no corresponding line in the

because the entire left outer join clause describes a single table to be used as a data
source.

select pp, gdp

from lonely_planet lp left outer join gdp

q3.15

on l.country=gdp.country

where l.country like    a%   

names from both the lonely planet and gdp tables. then use a few left
outer joins beginning with the reference table to produce a complete data
set.

the query above is a left outer join, which includes all data from the left
table, but may exclude data from the right table. as of this writing, this
is all that sqlite supports, but other systems also support the right outer
join (include all entries in the right table) and the full outer join (include all
entries from both tables).

using the	 i   keyword, generate a reference table with all of the country
  mysql as well as sqlite, apophenia supports mysql. mysql is somewhat
either set your shell   sa   _db_e g  e environment variable to y   ,15 or in
your code, seta   _    .db_e gi e=   . you can thus switch back and forth
between sqlite and mysql; if the variable is    then any database operations
15as discussed in appendix a, you will probably want to addex    a   _db_e g  e= y    to your
.ba h 
 on systems using ys	 .

better for massive data sets, but will work only if you already have a
mysql server running, have permission to access it, and have a database in place.
your package manager will make installing the mysql server, client, and develop-
ment libraries easy, and mysql   s maintainers have placed online a comprehensive
manual with tutorial.

once mysql is set up on your system, you will need to make one of two changes:

will go to the mysql engine and if it is not, then database operations will be sent

gsl_stats march 24, 2009

databases

107

to the sqlite engine. this could be useful for transferring data between the two.
for example:

apop_opts.db_engine =    m   ;
apop_db_open("mysqldb");
apop_data *d = apop_query_to_data("select * from get_me");
apop_opts.db_engine =    l   ;
apop_db_open("sqlitedb");
apop_opts.output_type =    d   ; //print to database.
apop_data_print(d, "put_me");

as noted above, every sql system has its own rules for metatadata. from the

mysql digresses from the sql standard in different manners from sqlite   s
means of digressing from the standard:

sqlite   s concept of a database is a single    le on the hard drive, or a database
in memory. conversely mysql has a server that stores all databases in a central
repository (whose location is of no concern to end-users). it has no concept of an
in-memory database.

 y    prompt, you can query the mysql server for a complete list of databases
with h wda aba e , and then attach to one using	 edb a e; (or type y   
db a e at the command prompt to attach todb a e from the outset). you can
use h w ab e ; to get the list of tables in the current database (like the sqlite
prompt   s. ab e  command), or use h w ab e f   y 	 _db; to see the
tables iny 	 _db without    rst attaching to it. given a table, you can use h w

  	   f   y 	 _ ab e to see the column names ofy 	 _ ab e.16
    sqlite is somewhat forgiving about details of punctuation, such as taking== and
= as equivalent, and    double-ticks    and    single-ticks    as equivalent. mysql de-
mands a single= and    single-ticks   .
    after every e e
 ,
 ea e, and so on, mysql   s results need to be internally
when sending a string holding multiple semicolon-separated queries to thea   _	
 	e y... functions. similarly, you may have trouble usingbegi /
   i  wrap-
of additional utilities. for example, there is a  ad command that will read in a
text    le much more quickly thana   _ ex _  _db.
16or, use the command-line program y    h w to do all of these things in a slightly more pleasant format.

processed, lest you get an error about commands executed out of order. apophe-
nia   s functions handle the processing for you, but you may still see odd effects

pers to bundle queries, though mysql   s internal cache management may make
such wrappers unnecessary.

    mysql includes many more functions beyond the sql standard, and has a number

108

gsl_stats march 24, 2009

   sql represents missing data via a u   marker, so queries may in-
clude conditions likewhe e
  i     	  .
a   _    .db_ a  to a regular expression appropriate for your data.
to joint tables by name, use the 	 e j i  to ensure that all names

   data    les use whatever came to mind to mark missing data, so set

   if a name appears in one table but not another, and you would like

chapter 3

appear.

3.7 some examples

here are a few examples of how c code and sql calls
can neatly interact.

taking simulation notes

say that you are running a simulation and would like
to take notes on its state each period. the following
code will open a    le on the hard drive, create a table, and add an entry each period.
the begin-commit wrapper puts data in chunks of 10,000 elements, so if you get
tired of waiting, you can halt the program and walk away with your data to that
point.17

double sim_output;
apop_db_open("sim.db");
apop_table_exists("results", 1); //see below.
apop_query("create table results (period, output); begin;");
for (int i=0; i< max_periods; i++){

apop_query("commit; begin;");

}
apop_query("commit;");
apop_db_close(0);

sim_output = run_sim(i);
apop_query("insert into results values(%i, %g);", i, sim_output);
if (!(i%1e4))

    thea   _ ab e_exi    command checks for whether a table already exists. if
leaves the table intact if it is there. it is especially useful inif statements.
17sometimes such behavior will leave the database in an unclean state. if so, try the sqlite commandva
		 .

the second argument is one, as in the example above, then the table is deleted
so that it can be created anew subsequently; if the second argument is zero, then
the function simply returns the answer to the question    does the table exist?    but

    every 1e4 entries, the system commits what has been entered so far and begins
a new batch. with some sqlite systems, this can add signi   cant speed. mysql

gsl_stats march 24, 2009

does its own batch management, so thebegi s and
   i s should be omitted for

databases

109

mysql databases.

easy t-tests

people on the east and west coasts of the united states sometimes
joke that they can   t tell the difference between all those states in
the middle. this is a perfect chance for a t test: are incomes in north dakota
signi   cantly different from incomes in south dakota? first, we will go through
the test algorithm in english, and then see how it is done in code.

let the    rst data set be the income of counties in north dakota, and let the second
be the income of counties in south dakota. if     ,     2, and n are the estimated mean,
variance, and actual count of elements of the north and south data sets,

stat =

    n         s
n /nn +     2

s/ns

q    2

    tnn +ns   2.

(3.7.1)

double con   dence = (1     2* gsl_cdf_tdist_q(|stat|, nn + ns    2));

[that is, the given ratio has a t distribution with nn + ns     2 degrees of freedom.]
the    nal step is to look up this statistic in the standard t tables as found in the
back of any standard statistics textbook. of course, looking up data is the job of a
computer, so we instead ask the gsl for the two-tailed con   dence level (see page
305 for details):

if
  fide 
e is large, say > 95%, then we can reject the null hypothesis that
    lines 4   8 comprise two queries, that are read into ag  _ve
   . both ask for
the same data, but one has awhe e clause restricting the query to pull only north
dakotan counties, and the other has awhe e clause restricting the query to south

north and south dakotan incomes (by county) are different. otherwise, there isn   t
enough information to say much with con   dence.

listing 3.5 translates the process into c.

dakota.

    lines 10   15 get the vital statistics from the vectors: count, mean, and variance.

    given this, line 17 is the translation of equation 3.7.1.

    finally, line 18 is the con   dence calculation from above, which line 19 prints as a

percentage.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

gsl_stats march 24, 2009

chapter 3

110

#include <apop.h>

int main(){

apop_db_open("data   census.db");
gsl_vector *n = apop_query_to_vector("select in_per_capita from income "

"where state= (select state from geography where name =   north dakota   )");

gsl_vector *s = apop_query_to_vector("select in_per_capita from income "

"where state= (select state from geography where name =   south dakota   )");

double n_count = n   >size,

n_mean = apop_vector_mean(n),
n_var = apop_vector_var(n),
s_count = s   >size,
s_mean = apop_vector_mean(s),
s_var = apop_vector_var(s);

}

no, easier

listing 3.5 are north dakota incomes different from south dakota incomes? answering the long

but this is not quite as easy as it could be, because apophenia provides
a high-level function to do the math for you, as per listing 3.6. the

double stat = fabs(n_mean     s_mean)/ sqrt(n_var/ (n_count   1) + s_var/(s_count   1));
double con   dence = 1     (2 * gsl_cdf_tdist_q(stat, n_count + s_count    2));
printf("reject the null with %g%% con   dence\n", con   dence*100);

way. online source:  e  .   g.
.
code is identical until line eight, but then line nine calls thea   _ _ e   function,
which takes the two vectors as input, and returns ana   _da a structure as output,
dummy variables the
a e command is the if-then-else of sql. say that you
into a one-zero variable would be via thea   _da a_  _d	  ie  function on
intuition. then we can put a
a e statement in with the other column de   nitions
to produce a column that is one whenbi a y  is af   rmative and zero otherwise:

listing the relevant statistics. line ten prints the entire output structure, and line
eleven selects the single con   dence statistic regarding the two-tailed hypothesis
that incomend 6= incomesd.

the matrix side. this works partly because of our luck that y > n and t > f in
english, so y and t will map to one and n and f will map to zero. but say that our
survey used af   rmative and negative, so the mapping would be backward from our

have data that are true/false or yes/no. one way to turn this

select id,

case binaryq when "af   rmative" then 1 else 0 end,
other_vars
from datatable;

1
2
3
4
5
6
7
8
9
10
11
12

gsl_stats march 24, 2009

111

databases

#include <apop.h>

int main(){

}

series of dummy variables using this technique.

"where state= (select state from geography where name =   south dakota   )");

"where state= (select state from geography where name =   north dakota   )");

gsl_vector *s = apop_query_to_vector("select in_per_capita from income "

listing 3.6 are north dakota incomes different from south dakota incomes? online source:

apop_db_open("data   census.db");
gsl_vector *n = apop_query_to_vector("select in_per_capita from income "

to take this to the extreme, we can turn a variable that is discrete but not or-
dered (such as district numbers in the following example) into a series of dummy

apop_data *t = apop_t_test(n,s);
apop_data_show(t); //show the whole output set...
printf ("\n con   dence: %g\n", apop_data_get_ti(t, "conf.*2 tail",    1)); //...or just one value.

  e  .
.
variables. it requires writing down a separate
a e statement for each value the
variable could take, but that   s whatf   loops are for. [again, this is demonstration
code. usea   _da a_  _d	  ie  to do this in practice.] listing 3.7 creates a
    on lines 5   6, theb	i d_a_ 	e y function queries out the list of districts.
    then the query writes a select statement with a line
a es a ewhe   a e_	
 a e he 1e  e0 for every  a e_ a e.
not at the end of the e e
  clause.
    you can seta   _    .ve b  e=1 at the head of ai  to have the function dis-
note well that thef   loop starting on line eight goes fromi=1, noti=0. when
vent x from being singular; excludingi=0 means alabama will be the baseline.
q: rewrite thef   loop to use another state as a baseline. or, set thef   loop to

    line 18 pulls the data from this massive query, and line 19 runs an ols regression

    lines 20   21 show the parameter estimates, but suppress the gigantic variance   

    line 11 uses the obfuscatory if (page 211) to print a comma between items, but

including dummy variables, you always have to exclude one baseline value to pre-

play the full query as it executes.

on the returned data.

covariance matrix.

run the full range from zero to the end of the array, and watch disaster befall the
analysis.

gsl_stats march 24, 2009

112

chapter 3

#include <apop.h>

char *build_a_query(){

char *q = null;

apop_data *state = apop_query_to_text("select name as state, state as id \

from geography where suid113vel+0.0 = 40");

asprintf(&q, "select in_per_capita as income, ");
for (int i=1; i< state   >textsize[0]; i++)

asprintf(&q, "%s (case state when    %s    then 1 else 0 end)    %s    %c \n",

q, state   >text[i][1], state   >text[i][0],
(i< state   >textsize[0]   1) ?    ,   :       );

asprintf(&q,"%s from income\n", q);
return q;

}

int main(){

apop_db_open("data   census.db");
apop_data *d = apop_query_to_data(build_a_query());
apop_model *e = apop_estimate(d, apop_ols);
e   >covariance = null; //don   t show it.
apop_model_show(e);

listing 3.7 a sample of af   loop that creates sql that creates dummy variables. online source:
  a ed	  ie .
.
   there is no standard forf   loops, assigning variables, or matrix-

}

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

style manipulation within sql, so you need to do these things on the
c-side of your analysis.

   functions exist to transfer data between databases and matrices, so

you can incorporate database-side queries directly into c code.

gsl_stats march 24, 2009

4

matrices and models

my freedom thus consists in moving about within the narrow frame that i have as-
signed myself for each one of my undertakings. . . . whatever diminishes constraint
diminishes strength. the more constraints one imposes, the more one frees one   s self
of the chains that shackle the spirit.

   stravinsky (1942, p 65)

recall that the c language provides only the most basic of basics, such as addition
and division, and everything else is provided by a library. so before you can do
data-oriented mathematics, you will need a library to handle matrices and vectors.

there are many available; this book uses the gnu scienti   c library (gsl). the
gsl is recommended because it is actively supported and will work on about as
many platforms as c itself. beyond functions useful for statistics, it also includes a
few hundred functions useful in engineering and physics, which this book will not
mention. the full reference documentation is readily available online or in book
form (gough, 2003). also, this book co-evolved with the apophenia library, which
builds upon the gsl for more statistics-oriented work.

this chapter goes over the basics of dealing with the gsl   s matrices and vectors.
although insisting that matrices and vectors take on a speci   c, rigid form can be a
constraint, it is the constraint that makes productive work possible. the predictable
form of the various structures makes it is easy to write functions that allocate and
   ll them, multiply and invert them, and convert between them.

gsl_stats march 24, 2009

114

chapter 4

4.1 the gsl   s matrices and vectors

quick   what   s 14 times 17?
thanks to calculators, we are
all a bit rusty on our multiplication, so listing 4.1 produces a multiplication table.

#include <apop.h>

int main(){

gsl_matrix *m = gsl_matrix_alloc(20,15);

gsl_matrix_set_all(m, 1);
for (int i=0; i< m   >size1; i++){

apop_matrix_row(m, i, one_row);
gsl_vector_scale(one_row, i+1);

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

}

apop_matrix_col(m, i, one_col);
gsl_vector_scale(one_col, i+1);

}
for (int i=0; i< m   >size2; i++){

}
apop_matrix_show(m);
gsl_matrix_free(m);

listing 4.1 allocate a matrix, then multiply each row and each column by a different value to

    the matrix is allocated in the introductory section, on line four. it is no surprise

produce a multiplication table. online source: 	  i  i
a i   ab e.
.
that it hasa   
 in the name, giving indication that memory is being allocated
six to nine, begins with thea   _ a  ix_  w macro to pull a single row, which
it puts into a vector named  e_  w.
    given the vector  e_  w, line eight multiplies every element byi 1. when this
memory anyway; others consider freeing at the end of ai  to be a waste of time.
1due to magic discussed below, vectors allocated bya   _ a  ix_  w and_
   do not really exist and do

    line 14 displays the constructed matrix to the screen.
    line 15 frees the matrix.1 the system automatically frees all matrices at the end
of the program. some consider it good style to free matrices and other allocated

for the matrix. in this case, the matrix has 20 rows and 15 columns. row always
comes    rst, then column, just like the order in roman catholic, randy choirboy,
or rc cola.

    the rest of the    le works one row or column at a time. the    rst loop, from lines

    line    ve is the    rst matrix-level operation: set every element in the matrix to one.

happens again by columns on line 12, we have a multiplication table.

not need to be freed.

naming conventions

matrices and models

gsl_stats march 24, 2009

every function in the gsl library will begin withg  _, and
to be acted upon. most gsl functions that affect a matrix will begin withg  _	
 a  ix_ and most that operate on vectors begin withg  _ve
   _. the other
functions begin witha   _ and a great majority of them begin with a data type
such asa   _da a_ ora   _  de _, and glib   s functions all begin withg_	
 bje
 :g_  ee_,g_ i  _, et cetera.2
the same program. if two libraries both have a function namedda a_a   
, then

this custom is important because c is a general-purpose language, and the design-
ers of any one library have no idea what other libraries authors may be calling in

libraries used in this book stick to such a standard as well: 100% of apophenia   s

the    rst argument of all of these functions will be the object

115

one will break.

c   s library-loaded matrix and vector operations are clearly more verbose and re-
dundant than comparable operations in languages that are purpose-built for matrix
manipulation. but c   s syntax does provide a few advantages   notably that it is
verbose and redundant. as per the discussion of debugging strategy on page 46,
spacing out the operations can make debugging numerical algorithms less painful.
when there is a type name in the function name, there is one more clue in the
function call itself whether you are using the function correctly.

the authors of the mathematica package chose not to use abbreviations; here is
their answer to the question of why, which applies here as well:

the answer. . . is consistency. there is a general convention . . . that all
function names are spelled out as full english words, unless there is a
standard mathematical abbreviation for them. the great advantage of
this scheme is that it is predictable. once you know what a function
does, you will usually be able to guess exactly what its name is. if the
names were abbreviated, you would always have to remember which
shortening of the standard english words was used. (wolfram, 2003,
p 35)

vectors alphabetized underg  _ve
   _..., and the index of this book gives a
2there is one awkward detail to the naming scheme: some functions in the apophenia library act ong  _	
 a  ixes andg  _ve
   s. those have names beginning witha   _ a  ix anda   _ve
   , compromis-

the naming convention also makes indices very helpful. for example, the index
of the gsl   s online reference gives a complete list of functions that operate on

partial list of the most useful functions.

ing between the library name and the name of the main input.

q4.1

116

chapter 4

sections as well.

gsl_stats march 24, 2009

void mset(gsl_matrix *m, int row, int col, double data){

if you    nd the naming scheme to be too verbose, you can write your own wrap-

sections of the index to this book or the gsl   s online reference and skim
over the sort of operations you can do. the apophenia package has a num-
ber of higher-level operations that are also worth getting to know, so have

don   t delay   have a look at theg  _ve
   _... andg  _ a  ix_...
a look at thea   _ve
   _...,a   _ a  ix_..., anda   _da a_...
per functions that require less typing. for example, you could write a    le y_	

  ve ie 
e_f  .
, which could include:
you would also need a header    le, y_
  ve ie 
e_f  .h:
after throwing an#i 
 	de" y_
  ve ie 
e_f  .h" at the top of your pro-
gram, you will be able to use your abbreviated syntax such asvge  v 3 . it   s up

#include <gsl/gsl_matrix.h>
#include <gsl/gsl_vector.h>
void mset(gsl_matrix *m, int row, int col, double data);
void vset(gsl_vector *v, int row, double data);

// for simple functions, you can rename them via #de   ne; see page 212:
#de   ne vget(v, row) gsl_vector_get(v, row)
#de   ne mget(m, row, col) gsl_matrix_get(m, row, col)

#de   ne vector_alloc(vname, length) gsl_vector *vname = gsl_vector_alloc(length);

void vset(gsl_vector *v, int row, double data){

gsl_matrix_set(m, row, col, data);

gsl_vector_set(v, row, data);

}

}

to your   sthetic as to whether your code will be more or less legible after you
make these changes. but the option is always there: if you    nd a function   s name
or form annoying, just write a more pleasant wrapper function for your personal
library that hides the annoying parts.

basic matrix and vector operations the simplest operations on matrices
and vectors are element-by-element
operations such as adding the elements of one matrix to those of another. the
gsl provides the functions you would expect to do such things. each modi   es its
   rst argument.

gsl_stats march 24, 2009

matrices and models

117

gsl_matrix_add (a,b);
gsl_matrix_sub (a,b);
gsl_matrix_mul_elements (a,b);
gsl_matrix_div_elements (a,b);
gsl_matrix_scale (a,x);
gsl_matrix_add_constant (a,x);

// aij     aij + bij,     i, j
// aij     aij     bij,     i, j
// aij     aij    bij ,     i, j
// aij     aij /bij ,     i, j
// aij     aij    x,     i, j     n, x     r
// aij     aij + x,     i, j     n, x     r

gsl_vector_add (a,b);
gsl_vector_sub (a,b);
gsl_vector_mul (a,b);
gsl_vector_div (a,b);
gsl_vector_scale (a,x);
gsl_vector_add_constant (a,x);
apop_vector_log(a);
apop_vector_log10(a);
apop_vector_exp(a);

// ai     ai + bi,     i
// ai     ai     bij ,     i
// ai     ai    bi,     i
// ai     ai/bi,     i
// ai     ai    x,     i     n, x     r
// ai     ai + x,     i     n, x     r
// ai     ln(ai),     i
// ai     log10(ai),     i
// ai     eai ,     i

q4.2

rewrite the structured birthday paradox program from page 35 using a

the functions to multiply and divide matrix elements are given slightly lengthier
names to minimize the potential that they will be confused with the process of mul-
tiplying a matrix with another matrix, ab, or its inverse, ab   1. those operations
require functions with more computational    repower, introduced below.

g  _ a  ix instead of the   	
  that it currently uses.
   a   
 or
a   
 the matrix in ai ; pass it to both functions.
    replace the#i 
 	de directives to call ina   .h.
    replace everything after the title-printing line in  i  _day  with
a   _ a  ix_ h w da a_ a  ix .
threeg  _ a  ix_ e  commands
in thef   loop of

a 
	 a e_day  to set the number of people, likelihood of match-
that likelihood, as inbdayf  .
).
tion in listing 4.2 will take in ad 	b e indicating taxable income and will return

beyond the simple operations above, you will no doubt want to
transform your data in more creative ways. for example, the func-

ing the    rst, and likelihood of any match (as opposed to one minus

    put

us income taxes owed, assuming a head of household with two dependents tak-
ing the standard deduction (as of 2006; see internal revenue service (2007)). this
function can be applied to a vector of incomes to produce a vector of taxes owed.

apply and map

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

gsl_stats march 24, 2009

118

chapter 4

#include <apop.h>

double calc_taxes(double income){

double cutoffs[] = {0, 11200, 42650, 110100, 178350, 349700, infinity};
double rates[] = {0, 0.10, .15, .25, .28, .33, .35};
double tax = 0;
int bracket = 1;

income    = 7850; //head of household standard deduction
income    = 3400*3; //exemption: self plus two dependents.
while (income > 0){

tax += rates[bracket] * gsl_min(income, cutoffs[bracket]   cutoffs[bracket   1]);
income    = cutoffs[bracket];
bracket ++;

}
return tax;

}

int main(){

}

from income where suid113vel =    040   \
order by household_median_in desc");

listing 4.2 read in the median income for each us state and    nd the taxes a family at the median

apop_db_open("data   census.db");
strncpy(apop_opts.db_name_column, "geo_name", 100);
apop_data *d = apop_query_to_data("select geo_name, household_median_in as income\

apop_col_t(d, "income", income_vector);
d   >vector = apop_vector_map(income_vector, calc_taxes);
apop_name_add(d   >names, "tax owed",    v   );
apop_data_show(d);

would owe. online source: axe .
.
    lines 24   27 of listing 4.2 demonstrate the use of thea   _da a structure, and
ag  _ve
    namedi 
  e_ve
   , holding the median household income for
    the bulk of the program is the speci   cation of the tax rates in the
a 
_ axe 
four and    ve. the
	  ff  array has a    nal value that guarantees that thewhi e
value like u   or a 3 to the end of a list and rewrite yourf   loop   s header to
f   i  i=0;da a[i   != a ;i   . this means you have to remember to
3 a  is read as not-a-number, and will be introduced introduced on page 135.

put the sentinel value at the end of the list, but do not need to remember to    x a
counter every time you    x the array.

    the program does not bother to    nd out the length of the arrays declared in lines

loop on lines 10   14 will exit at some point. similarly, you can always add a    nal

will be explained in detail below. for now, it suf   ces to know that line 24 produces

function. in the exercise on page 192, you will plot this function.

each state.

119

output.

threading

matrices and models

gsl_stats march 24, 2009

can split their work among multiple processors. set

ment of the income vector in turn.

will do this for you. let c() be

turns c(i), which is then assigned to
the vector element of the data set,

even low-end laptops ship with processors that are
capable of simultaneously operating on two or more

threads (probably the number of processor cores in
your system), and these functions apportion work to
processors appropriately.

when threading, be careful writing to global vari-
ables: if a thousand threads could be modifying a
global variable in any order, the outcome is likely un-
de   ned. when writing functions for threading, your
best bet is to take all variables that were not passed
in explicitly as read-only.

beginning: apophenia provides a
small family of functions to map and apply a function to a data set. the full index
of functions is relegated to the manual pages, but here is a list of examples to give
you an idea.

you could write af   loop to apply
the
a 
_ ax function to each ele-
but thea   _ve
   _ a  function
stacks of frames, so the a  anda   y functions
the
a 
_ axe  function, and i be
a   _    . h ead_
 	   to the desired number of
thei 
  e_ve
   ; then the call
toa   _ve
   _ a  on line 25 re-
d	>ve
   . line 27 displays the
buta   _ve
   _ a  is just the
    you saw thata   _ve
   _ a  i 
  e_ve
    
a 
_ axe   will take in
ag  _ve
    and returns another vector. or,a   _ve
   _a   y i 
  e_	
ve
    
a 
_ axe   would replace every element ofi 
  e_ve
    with

a 
_ axe  e e e   .
    one often sees functions with a header liked 	b e  g_ ike ih  d g  _	
ve
   i da a , which takes in a data vector and returns a log likelihood. then if
every row ofda a e  is a vector representing a separate observation, thena   _	
 a  ix_ a  da a e    g_ ike ih  d  would return the vector of log likeli-
    functions with..._ a _..._ 	 , likea   _ a  ix_ a _a  _ 	 , will return
the sum of f (item) for every item in the matrix or vector. for example,a   _	
 a  ix_ a _a  _ 	    g  _i  a   will return the total number of elements
of  that are a . continuing the log likelihood example from above,a   _	
 a  ix_ a _ 	  da a e    g_ ike ih  d  would be the total log likeli-
a   _ a  ix_a   y to generate a vector of gdp per capita from a matrix with
   you can express matrices and vectors viag  _ a  ix andg  _	
ve
    structures.
   refer to elements usingg  _ a  ix_ e  andg  _ a  ix_ge 
(and similarly fora   _da a sets andg  _ve
    ).

    another example from the family appeared earlier: listing 3.4 (page 100) used

hoods of each observation.

gdp and population.

hood of all rows.

   

gsl_stats march 24, 2009

120

chapter 4

   v   

   c0   

   c1   

   c2   

   t0   

   t1   

   t2   

4.2

   r0   
   r1   
   r2   

(0,    1)
(1,    1)
(2,    1)

(0, 0)
(1, 0)
(2, 0)

(0, 1)
(1, 1)
(2, 1)

(0, 2)
(1, 2)
(2, 2)

(0, 0)
(1, 0)
(2, 0)

(0, 1)
(1, 1)
(2, 2)

(0, 2)
(1, 2)
(2, 2)

|

|

   

row names are shared by all three elements.

send every row of a vector/matrix to a function in turn.

figure 4.3 the vector is column    1 of the matrix, while the text gets its own numbering system.

   once your data set is in these forms, you can operate on the matrix

or vector as a whole using functions likeg  _ a  ix_add a b  or
g  _ve
   _ 
a e a x .
ve
   )_( a 
   use thea   _( a  ix
a   y) family of functions to
thea   _da a structure is the joining-together of four data
a   _da a
types: theg  _ve
   ,g  _ a  ix, a table of strings, and an
a   _ a e structure.
    there are various means of creating ana   _da a set, includinga   _ 	e y_	
  _da a,a   _ a  ix_  _da a,a   _ve
   _  _da a, or creating a blank
slate witha   _da a_a   
; see below.
    for example, listing 4.2 useda   _ 	e y_  _da a to read the table into an
a   _da a set, and by setting thea   _    .db_ a e_
  	   option to a col-
umn name on line 20, the query set row names for the data. line 25 sets theve
   
element of the data set, and line 26 adds an element to theve
    slot of the a e 
    you can easily operate on the subelements of the structure. if your a  ix_ a 	
i 	 a e function requires ag  _ a  ix, buty 	 _da a is ana   _da a struc-
ture, then you can call a  ix_ a i 	 a e y 	 _da a	> a  ix . similarly,

the conceptual layout is given in figure 4.3. the vector, columns of the matrix,
and columns of text are all named. also, all rows are named, but there is only one
set of row names, because the presumption is that each row of the structure holds
information about a single observation.

    think of the vector as the    1st element of the matrix, and the text elements as
having their own addresses.

element of the set.

}

121

printf("\n");

matrices and models

printf("%s\t", set   >text[r][c]);

gsl_stats march 24, 2009

for (int c=0; c< set   >textsize[1]; c++)

apop_data *set = apop_query_to_text(...);
for (int r=0; r< set   >textsize[0]; r++){

you can manipulate the names and the table of text data directly. the size of the

text data is stored in the ex  ize element. sample usage:
ve
   	> ize, or a  ix	> ize1 are equal. if you want to put a vector of    fteen
assigned values at all. ifve
   _ ize is zero, then
will initialize most elements of ewda a to u  , but produce a _  w 
 _
   
matrix with an empty set of names. alternatively, if _  w ==0 butve
   _	
 ize is positive, then the vector element is initialized and the matrix set to u  .4
get, set, and point you can use any of the gsl tools above to dissect theg  _	
 a  ix element of thea   _da a struct, and similarly for the
ve
    element. in addition, there is a suite of functions for setting and getting
an element from ana   _da a set using the names. let t be a title and i be a

elements and a 10    10 matrix in the same structure, and name only the    rst two
columns, you are free to do so. in fact, the typical case is that not all elements are

    there is no consistency-checking to make sure the number of row names, the

apop_data *newdata_m = apop_data_alloc(vector_size, n_rows, n_cols);

  

numeric index; then you may refer to the row   column coordinate using the (i, i),
(t, i), (i, t), or (t, t) form:

apop_data_get(your_data, i, j);
apop_data_get_ti(your_data, "rowname", j);
apop_data_get_it(your_data, i, "colname");
apop_data_get_tt(your_data, "rowname", "colname");
apop_data_set(your_data, i, j, new_value);
apop_data_set_ti(your_data, "rowname", j, new_value);
...
apop_data_ptr(your_data, i, j);
apop_data_ptr_ti(your_data, "rowname", j);
...

4seasoned c programmers will recognize such usage as similar to a	 i   between ag  _ve
   , ag  _	
 a  ix, and a
ha  array, though thea   _da a set can hold both simultaneously. c++ programmers will
a   _da a as input, but operates on one or both of ag  _ve
    or ag  _ a  ix, depending on which is not
 u   in the input.

observe that the structure allows a form of polymorphism, because you can write one function that takes an

122

chapter 4

gsl_stats march 24, 2009

row, including both vector and matrix.

structures.
    as above, you can think about the vector as the    1st element of the matrix, so for

    these functions use case-insensitive regular-expression matching to    nd the right
name, so you can even be imprecise in your column request. appendix b dis-
cusses id157 in greater detail; for now it suf   ces to know that you

    thea   _da a_   ... form returns a pointer to the given data point, which
you may read from, write to, increment, et cetera. it mimics theg  _ a  ix_   
andg  _ve
   _    functions, which do the same thing for their respective data
example,a   _da a_ e _ i y 	 _da a "  w a e" 	1  will operates on
thea   _da a structure   s vector rather than the matrix. this facilitates forms like
f   i  i=	1;i<da a	> a  ix	> ize2;i   , that runs across an entire
can be approximate about the name:" .va . " will match va 	e, 	va  and
 .va 	e .
for an example,    ip back to  e  .
, listed on page 111. line ten showed the full
used the set   s rownames andve
    elements. line eleven pulled a single named
  forming partitioned matrices you can copy the entire data set, stack two data
tables are in the database instead ofa   _da a sets the vertical and horizontal

matrices one on top of the other (stack rows),
stack two data matrices one to the right of the other (stack columns), or stack
two data vectors:

apop_data *newcopy = apop_data_copy(oldset);
apop_data *newcopy_tall = apop_data_stack(oldset_one, oldset_two,    r   );
apop_data *newcopy_wide = apop_data_stack(oldset_one, oldset_two,    c   );
apop_data *newcopy_vector = apop_data_stack(oldset_one, oldset_two,    v   );

output of the t test, which was a list of named elements, meaning that the output

again, you are generally better off doing data manipulation in the database. if the

element from the vector.

stacking commands above are equivalent to

select * from oldset_one
union
select * from oldset_two

/* and */

select t1.*, t2.*

from oldset_one t1, oldset_two t2

q4.3

gsl_stats march 24, 2009

123

matrices and models

trix with a column for all but the    rst category.

    stack that matrix to the right of the original table.

the output for the categorical variables indicates the effect relative to
the omitted category.

    encapsulate the routine in a function: using the code you just wrote,
put together a function that takes in data and a text column name or
number and returns an augmented data set with dummy variables.

the output of the exercise on page 105 is a table with tattoos, piercings, and
political af   liation. run a probit regression to determine whether political
af   liation affects the count of piercings.

    the functiona   _da a_  _d	  ie  will produce a new data ma-
    send the augmented table to thea   _   bi .e  i a e function.
   thea   _da a structure combines a vector, matrix, text array, and
put) usinga   _da a_ge _ i and family.
like theg  _ a  ix org  _ve
   , but coding is much easier when there is the
example, you can go from ad 	b e[    to ana   _da a set viad 	b e[   
g  _ a  ix
a   _da a. as will be proven below, it is only two steps from

table 4.4 provides the key to the method most appropriate for each given con-
version. from/to pairs marked with a dot are left as an exercise for the reader;
none are particularly dif   cult, but may require going through another format; for
   

   exibility of easily switching among the various constrained forms. to that end,
this section presents suggestions for converting among the various data formats
used in this book. it is not an exciting read (to say the least); you may prefer to
take this section as a reference for use as necessary.

igor stravinsky, who advocated constraints at the head
of this chapter, also points out that    rigidity that slightly
yields, like justice swayed by mercy, is all the beauty of earth.   5 none of function
to this point would make any sense if they did not operate on a speci   c structure

   you can pull named items from a data set (such as an estimation out-

names for all of these elements.

any format to any other.

   

4.3 shunting data

5stravinsky (1942), p 54, citing gk chesterton,    the furrows,    in alarms and discursions.

gsl_stats march 24, 2009

  

p

to

124

m
o
r
f

chapter 4

c
  
  

text   le

text    le
db table

table 4.4 a key to methods of conversion.

  
f
q   
  
c
f
p
p
f
f
p

  
f
q q q
  
f
f
f
c
f
v c
f
c
s
s

the computer can very quickly copy blocks without bother-
ing to comprehend what that data contains; the function to

d btabled 	b e[   g  _ve
   g  _ a  ixa   _da a
d 	b e[   
g  _ve
    p
g  _ a  ix p
a   _da a
  copying structures
do this is e   ve, which is a safe variant of e 
 y. for example, borrowing the

    ex structure from chapter 2:
the computer will go to the location offi    and blindly copy what it    nds to the
location of e
  d, up to the size of one
    ex struct. sincefi    and e
  d
but there is one small caveat: if one element of the   	
  is a pointer, then it is
example, theg  _ve
    includes ada a pointer, so using e   ve would result
as per method v below; if you want a copy, then you need to e   ve both the
baseg  _ve
    and theda a array. this sets the stage for the series of functions
below with e 
 y in the name that are modeled on c   s basic e   ve/ e 
 y
theg  _..._ e 
 y functions assume that the destination
right: in c,de  =  	 
e; in r,de  <	  	 
e; in pseudocode,de  
  	 
e. similarly, most copying
functions have the data    ow from end of line to beginning: e   ve de     	 
e .

to which you are copying has already been allocated; this al-
lows you to reuse the same space and otherwise carefully oversee memory. the

complex    rst = {.real = 3, .imaginary =    1};
complex second;
memmove(&second, &   rst, sizeof(complex));

now have identical data, their constituent parts are guaranteed to also be identical.6

the pointer that is copied, not the data itself (which is elsewhere in memory). for

in two identical structs that both point to the same data. if you want this, use a view,

6how to remember the order of arguments: computer scientists think in terms of data    owing from left to

functions but handle internal pointers correctly.

method c: copying

   

125

//text    le     text    le

matrices and models

gsl_stats march 24, 2009

apop_system("cp %s %s", from_   le_name, to_   le_name);

copy on the same line, and more easily embed a copy into a    ltering operation.

gsl_vector *copy = gsl_vector_alloc(original   >size);
gsl_vector_memcpy(copy, original);
gsl_vector *copy2 = apop_vector_copy(original);

gsl_matrix *copy = gsl_matrix_alloc(original   >size1, original   >size2);
gsl_matrix_memcpy(copy, original);
gsl_matrix *copy2 = apop_matrix_copy(original);

double *copy1 = malloc(sizeof(double) * original_size);
memmove(copy1, original, sizeof(double) * original_size);
double copy2[original_size];
memmove(&copy2, original, sizeof(original));

a   _..._
  y functions allocate and copy in one step, so you can declare and
//just use the system   s    le copy command. thea   _ y  e  function acts like
//the standard c y  e  command, but accepts printf-style arguments:
//g  _ve
       g  _ve
   
//d 	b e[       d 	b e[   
//let  igi a _ ize be the length of the original array.7
//g  _ a  ix    g  _ a  ix
//a   _da a    a   _da a
there are two ways to express a matrix ofd 	b es. the analog to using a pointer is
7the ize f function is not
 ize f. if  igi a  is an array of 100d 	b es, then ize f   igi a  =100  ize f d 	b e , while
 ize f    igi a  = ize f d 	b e , and so you could use ize f   igi a   as the third argument for
 e   ve. however, this is incredibly error prone, because this is one of the few places in c where you could send

to declare a list of pointers-to-pointers, and the analog to an automatically allocated
array is to use double-subscripts:

the    rst method is rather inconvenient. the second method seems convenient,
because it lets you allocate the matrix at once. but due to minuti   that will not be

these are functions designed to convert one format to
another.

double **method_one = malloc(sizeof(double*)*size_1);
for (int i=0; i< size_1; i++)

apop_data_memcpy(copy1, original);
apop_data *copy2 = apop_data_copy(original);

apop_data *copy1 = apop_data_alloc(original   >vector   >size, original   >matrix   >size1,

method_one[i] = malloc(sizeof(double) * size_2);

double method_two[size_1][size_2] = {{2,3,4},{5,6,7}};

types: you can also send an array or other element

original   >matrix   >size2);

just

for

to

method f: function call

either an object or a pointer to an object to the same function without a warning or error. in cases with modest
complexity, the difference between an array and its    rst element can be easy to confuse and hard to debug.

gsl_stats march 24, 2009

126

chapter 4

apop_text_to_db("original.txt", "tablename", 0 , 1, null);

apop_data *copyd = apop_text_to_data("original.txt", 0 , 1);

tions to convert to a matrix. for another example, see page 9.

instead, declare your data as a single line, listing the entire    rst row, then the sec-

//text     db table
//the    rst number states whether the    le has row names; the second
//whether it has column names. finally, if no colnames are present,

discussed here (see kernighan & ritchie (1988, p 113)), that method is too much
of a hassle to be worth anything.

double original[] = {2,3,4,5,6,7};
int orig_vsize = 0, orig_size1 = 2, orig_size2 = 3;
gsl_matrix *copym = apop_line_to_matrix(original, orig_size1, orig_size2);

double original[] = {{2,3,4}, {5,6,7}};
gsl_vector *copv = apop_array_to_vector(original, original_size);
gsl_matrix *copm = apop_array_to_matrix(original, original_size1, original_size2);

ond, et cetera, with no intervening brackets. then, use thea   _ i e... func-
//you can provide them in the last argument as a
ha   
//text    a   _da a
//d 	b e[   [       g  _ve
   ,g  _ a  ix
//d 	b e[       g  _ a  ix
//d 	b e[       a   _da a
//g  _ve
       d 	b e[   
//g  _ve
        n    1g  _ a  ix
//g  _ve
   ,g  _ a  ix    a   _da a
the four choices for thea   _    . 	  	 _ y e variable are
apop_opts.output_type =    p   ; //write to the pipe ina   _    . 	  	 _ i e.

method p: printing apophenia   s printing functions are actually four-in-one func-
tions: you can dump your data to either the screen, a    le, a
database, or a system pipe [see appendix b for an overview of pipes]. early in
putting together an analysis, you will want to print all of your results to screen,
and then later, you will want to save temporary results to the database, and then
next month, a colleague will ask for a text    le of the output; you can make all of
these major changes in output by changing one character in your code.

apop_opts.output_type =    s   ; //default: print to screen.
apop_opts.output_type =    f   ; //print to    le.
apop_opts.output_type =    d   ; //store in a database table.

apop_data *copydv = apop_vector_to_data(original_vec);
apop_data *copydm = apop_matrix_to_data(original_matrix);

apop_data *copyd = apop_line_to_data(original, orig_vsize, orig_size1, orig_size2);

gsl_matrix *copym = apop_vector_to_matrix(original_vec);

double *copyd = apop_vector_to_array(original_vec);

gsl_stats march 24, 2009

matrices and models

127

method q: querying

the only way to get data out of a database is to query it out.

apop_opts.output_type =    d   
apop_vector_print(original_vector, "db_copy");
apop_matrix_print(original_matrix, "db_copy");
apop_data_print(original_data, "db_copy");

apop_opts.output_type =    t   
apop_vector_print(original_vector, "text_   le_copy");
apop_matrix_print(original_matrix, "text_   le_copy");
apop_data_print(original_data, "text_   le_copy");

    the second argument to the output functions is a string. output to screen or pipe
ignores this; if outputting to    le, this is the    le name; if writing to the database,
then this will be the table name.8

    the screen output will generally be human-readable, meaning different column
sizes and other notes and conveniences for you at the terminal to understand what
is going on. the    le output will generally be oriented toward allowing a machine
to read the output, meaning stricter formatting.

//g  _ve
   ,g  _ a  ix,a   _da a     text    le
//g  _ve
   ,g  _ a  ix,a   _da a     db table
//db table    d 	b e,g  _ve
   ,g  _ a  ix, ora   _da a
notice, by the way, that theda a subelement of ag  _ve
    can not necessarily
be copied directly to ad 	b e[      the stride may be wrong; see section 4.6 for
//a   _da a    g  _ a  ix,g  _ve
   
database, the    le name thus has its dots stripped: 	 . 	 .
 v becomes the table name 	 _ 	 .

double d = apop_query_to_   oat("select value from original");
gsl_vector *v = apop_query_to_vector("select * from original");
gsl_matrix *m = apop_query_to_matrix("select * from original");
apop_data *d = apop_query_to_data("select * from original");

sometimes, even a function is just overkill; you can just
pull a subelement from the main data item.

8file names tend to have periods in them, but periods in table names produce dif   culties. when printing to a

details. instead, use the copying functions from method f above.

apop_query("create table copy as \
select * from original");

my_data_set    > matrix
my_data_set    > vector

//db table     db table

method s: subelements

method v: views

gsl_stats march 24, 2009

128

chapter 4

however, it is not quite as easy as just    nding the second row and pointing to it,

the number of rows and columns. thus, there are a few macros to help you pull

apop_matrix_row(m, 3, row_v);
apop_matrix_col(m, 5, col_v);
apop_submatrix(m, 2, 4, 6, 8, submatrix);

pointers make it reasonably easy and natural to look at subsets
of a matrix. do you want a matrix that represents x with the    rst

row. since the new matrix is pointing to the same data as the original, any changes
will affect both matrices, which is often what you want; if not, then you can copy
the submatrix   s data to a new location.

row lopped off? then just set up a matrix whoseda a pointer points to the second
since ag  _ a  ix includes information about your data (i.e., metadata), such as
a row, column, or submatrix from a larger matrix. for example, say that  is a
g  _ a  ix , then
will produce ag  _ve
     named  w_v holding the third row, another named

  _v holding the    fth column, and a 6  8g  _ a  ix  named 	b a  ix
for ana   _da a set, we have the names at our disposal, and so you could
use eithera   _  w   3   w_v  anda   _
     5 
  _v  to pull the
given vectors from the matrix element of ana   _da a structure using row/col-
umn number, ora   _  w_    "f 	  h  w"   w_v  anda   _
  _    
" ix h
  	  " 
  _v  to pull these rows and columns by their titles.
g  _ a  ix orve
    with the requisite metadata, and then declare a pointer
the methods from prior pages (e.g.,g  _ve
     e  a e  _
  y=a   _	
ve
   _
  y  e  _view ;).
g  _ve
   v=g  _ a  ix_
   a_ a  ix 4 .ve
   ;
a   _ve
   _ h w  v ;

with the name you selected, that can be used like any other pointer to a matrix or
vector. however, because these macros used only automatically allocated memory,
you do not need to free the matrix or vector generated by the macro. thus, they
provide a quick, disposable view of a portion of the matrix.9 if you need a more
permanent record, them copy the view to a regular vector or matrix using any of

the macros work a bit of magic: they internally declare an automatically-allocated

9these macros are based on gsl functions that are slightly less convenient. for example:

whose (0, 0)th element is at (2, 4) in the original.

if the macro seems to be misbehaving, as macros sometimes do, you can fall back on this form.

gsl_stats march 24, 2009

matrices and models

129

4.4 id202

say that we have a transition matrix, showing whether
the system can go from a row state to a column state.
for example, figure 4.4 was such a transition matrix, showing which formats can
be converted to which other formats.

omitting the labels and marking each transition with a one and each dot in figure
4.4 with a zero, we get the following transition matrix:

}

#include <apop.h>
int main(){

1 1 0 0 0 1
0 1 0 1 1 1
0 0 1 1 1 0
1 1 1 1 1 1
1 1 1 1 1 1
1 1 1 1 1 1

listing 4.5 shows a brief program to read the data set from a text    le, take the dot

apop_data *t = apop_text_to_data("data   markov", 0, 0);
apop_data *out = apop_dot(t, t, 0, 0);
apop_data_show(out);

product of  with itself, and display the result.
listing 4.5 two transitions along a transition matrix. online source: a k v.
.
before discussing the syntax ofa   _d   in detail, here is the program   s output:
thea   _d   function takes up to four arguments: twoa   _da a structures, and
one    ag for each matrix indicating what to do with it (   =transpose the matrix,
 v =use the vector element,0=use the matrix as-is). for example, ifx is a matrix,

this tells us, for example, that there are three ways to transition from the    rst state
to the second in two steps (you can verify that they are: 1     1     2, 1     2     2,
and 1     6     2).

2 3 1 2 2 3
3 4 3 4 4 4
2 2 3 3 3 2
4 5 4 5 5 5
4 5 4 5 5 5
4 5 4 5 5 5

then

gsl_stats march 24, 2009

130

chapter 4

apop_dot(x, x,    t   , 0);

written (any other character).10

version is transposed and the second is not.

    if both elements are vectors, then you are probably better off just using

    there should be exactly as many transposition    ags as matrices. if the    rst
element is a vector, it is always taken to be a row; if the second element
is a vector, it is always a column. in both cases, if the other element is a

will    nd x   x: the function takes the dot product ofx with itself, and the    rst
    if a data set has a a  ix component, then it will be used for the dot product,
and if the a  ix element is u   then theve
    component is used.
matrix, you will need one    ag to indicate whether to use thea   _da a set   s
ve
    element ( v ), use the transposed matrix (   ), or use the matrix as
g  _b a _dd  , below, but if you usea   _d  , the output will be an
a   _da a set that has a vector element of length one.
write a function with the headera   _da a  	ad a i
_f    a   _	
da a x a   _da a y ; that takes twog  _ a  ixes and returns the
quadratic form as above. be sure to check thaty is square and has the same
dimension asx	> ize1.
vector    vector given two vectors x and y,g  _b a _dd   returns x1y1 + x2y2 +
return value, it takes the location of ad 	b e, and places the output there. e.g., if
x andy areg  _ve
    s, use
10why do you have to tell a computer whether to transpose or not? some feel that if you send a1 to indicate
transposition when you meant0 (or vice versa), the system should be able to determine this. say that you have a
the third is 10    10. you write a simplef   loop:
f   i  i=0;i<3;i   
 	 [i   =a   _d   da a[i    v 1 ;
ati=0, a    smart    system realizes that you committed a faux pas: an 8    10 matrix dot a 10    1 column vector
works without transposition. so it corrects you without telling you, and does the same withda a[1   . with
da a[2   , the transposition works, since there are both ten rows and ten columns. so 	 [0    and 	 [1    are
correct and 	 [2    is not. good luck catching and debugging that.

      +xnyn. rather than outputting the value of x  y as the function   s

1    10 vector that you will multiply against three data sets, where the    rst is 8    10, the second is 15    10, and

the quadratic form x   yx appears very frequently in statistical work.

double dotproduct;
gsl_blas_ddot (x, y, &dotproduct);

q4.4

gsl_stats march 24, 2009

matrices and models

131

write a table displaying the sum of squares 12 + 22 + 32 +        + n2 for
n = 1 through 10.

    allocates ag  _ve
     of size n,
    calculates and returns v    v usingg  _b a _dd  ,
    and    nally freesv.

       lls the vector with 1, . . . , n,

    write a function that takes in n and

q4.5

    write a loop for n = 1 through 10 that calls the above function and

then prints n and the returned value.

    verify your work, by printing n(n + 1)(2n + 1)/6 alongside your

calculation of the sum of squares up to n.

an example: cook   s distance

cook   s distance is an estimate of how much each
data point affects a regression (cook, 1977). the

formula is

ci = pj(  yr

j )2
j       yri
p    m se

,

(4.4.1)

where p is the number of parameters, m se is mean squared error for the overall
regression,   yr
j is the jth element of the predicted value of   y based on the overall
regression, and   yri
j is the jth element of the predicted value of   y based on a regres-
sion excluding data point i. that is, to    nd cook   s distance for 3,000 data points,
we would need to do a separate regression on 3,000 data sets, each excluding a
different data point. the formula simply quanti   es whether the predictions made
by the main regression change signi   cantly when excluding a given data point.

the procedure provides us a good opportunity to do some matrix-shunting and
id202, since we will need functions to produce the subsets, functions to
calculate   y = x    , and to    nd the squared differences and mse.

the    rst function is in listing 4.6. it produces the series of data sets, each with
one row missing. the function is named after the jackknife procedure, which uses
the same delete-one loop for calculating covariances or correcting bias.11

11the jackknife is not discussed in this book; see the online documentation for apophenia   sa   _	
ja
kk ife_
 v.

    lines 9   10 use a submatrix to produce a view of the main matrix starting at the

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

gsl_stats march 24, 2009

132

#include <apop.h>

typedef double (*math_fn)(apop_data *);

chapter 4

gsl_vector *jack_iteration(gsl_matrix *m, math_fn do_math){

int height = m   >size1;
gsl_vector *out = gsl_vector_alloc(height);
apop_data *reduced = apop_data_alloc(0, height     1, m   >size2);

apop_submatrix(m, 1, 0, height     1, m   >size2, mv);
gsl_matrix_memcpy(reduced   >matrix, mv);
for (int i=0; i< height; i++){

}

}

}
return out;

that to a new matrix.

gsl_vector_set(out, i, do_math(reduced));
if (i < height     1){

apop_matrix_row(m, i, onerow);
gsl_matrix_set_row(reduced   >matrix, i, onerow);

listing 4.6 iteratively producei 	> ize1 submatrices, each with one omitted row of data. online
source:ja
ki e a i  .
.
position (1,0), and with size ( 	> ize1	1  	> ize2)   that is, the original
matrix with the    rst row missing   and then usesg  _ a  ix_ e 
 y to copy
    thef   loop then repeats the view-and-copy procedure row by row. it begins with
the y edef on line 3.
    the 	 _  	a ed_diff function calculatespi(li     ri)2. the    rst line    nds
l     r, and the second line applies the functiong  _  w_2 to each element of
    the   je
  function is taking the dot product yest = x  . by giving this single
12the gsl provides ef   cient power calculators fromg  _  w_2 up tog  _  w_9, and the catch-all function
g  _  w_i   va 	e ex   e   , that will raiseva 	e to any integer exponent in a more ef   cient manner
than the general-purpose  w.

row zero, which was omitted before, and overwrites row zero in the copy, aka row
one of the original. it then copies over original row one, overwriting the copy of
original row two, and so on to the end of the matrix.

    line 12 calls the function which was sent in as an argument. see page 190 for
notes on writing functions that take functions as inputs, including the meaning of

now that we have the matrix-shunting out of the way, listing 4.7 provides addi-
tional functions to do the linear alegbra.

l     r (that is, it squares each element) and returns the post-squaring sum.12

gsl_stats march 24, 2009

matrices and models

133

#include <apop.h>

typedef double (*math_fn)(apop_data *);
gsl_vector *jack_iteration(gsl_matrix *, math_fn);
apop_data *ols_data;
gsl_vector * predicted;
double p_dot_mse;

double sum_squared_diff(gsl_vector *left, gsl_vector *right){

gsl_vector_sub(left, right); //destroys the left vector
return apop_vector_map_sum(left, gsl_pow_2);

}

gsl_vector *project(apop_data *d, apop_model *m){

return apop_dot(d, m   >parameters, 0,    v   )   >vector;

}

double cook_math(apop_data *reduced){

apop_model *r = apop_estimate(reduced, apop_ols);
double out =sum_squared_diff(project(ols_data, r), predicted)/p_dot_mse;
apop_model_free(r);
return out;

}

gsl_vector *cooks_distance(apop_model *in){
apop_data *c = apop_data_copy(in   >data);
apop_ols.prep(in   >data, in);
ols_data = in   >data;
predicted = project(in   >data, in);
p_dot_mse = c   >matrix   >size2 * sum_squared_diff(in   >data   >vector, predicted);
return jack_iteration(c   >matrix, cook_math);

}

}

int main(){

apop_data *dataset = apop_text_to_data("data   regressme", 0, 1);
apop_model *est = apop_estimate(dataset, apop_ols);
printf("plot          \n");
strcpy(apop_opts.output_delimiter, "\n");
apop_vector_show(cooks_distance(est));

ja
ki e a i  .
. online source:
  k .
.
    the
  k_ a h function calculates equation 4.4.1 for each value ofi. it is not
called directly, but is passed toja
k_i e a i  , which will apply the function to

line a function of its own, we hide some of the details and get a self-documenting
indication of the code   s intent.

listing 4.7 calcluate the cook   s distance, by running 3,200 regressions. compile with

q4.6

134

chapter 4

gsl_stats march 24, 2009

each of its 3,200 submatrices.

ulate outliers or erroneous data. does the cook   s distance of the bad data
stand out as especially large?

a list of the cook   s distance for every point in the set, which we can use to search
for outliers.

    the
  k _di  a 
e function produces two copies of the data set: an untainted
copy
, and a regression-style version with the dependent variable in theve
   
element of the data set, and the    rst column of the a  ix all ones.
    after ai  calls
  k _di  a 
e, which calls the various id202 proce-
dures andja
k_i e a i  , which calls
  k_ a h for each submatrix, we have
    the ai  function produces gnuplot-ready output, so run this using, e.g.,./
  k
|g 	    	 e  i  . some researchers prefer to sort the data points before plot-
ting; i.e., try sending the output vector tog  _ve
   _     before plotting.
add some bad data points to theda a	 eg e   e    le, like1|1|1, to sim-
and blas   s triangular decomposition functions), nameda   _ a  ix_i ve  e,
a   _ a  ix_de e  i a  , and for both at once,a   _de _a d_i v. exam-
solving (x   x)   = x   y, which involves no inversion. ifx x is the matrix x   x
andx y is the vector x   y, theng  _ i a g_  _   ve x x x y be av 
  a   _da a
a   _da a:a   _d  .
   vector    vector:g  _b a _dd  .
   inversion:a   _ a  ix_i ve  e,a   _ a  ix_de e  i a  , or
a   _de _a d_i v.

inverting a matrix requires signi   cantly more computa-
tion than the element-by-element operations above. but
here in the modern day, it is not such a big deal: my old
laptop will invert a 1, 000    1, 000 matrix in about ten seconds, and does the in-
version step for the typical ols regression, around a 10    10 matrix at the most,
in well under the blink of an eye.

sometimes, you do not have to bother with inversion. for example, we often write
the ols parameters as    = (x   x)   1(x   y), but you could implement this as

ples for using this function are located throughout the book; e.g., see the calcula-
tion of ols coef   cients on page 280.

apophenia provides functions to    nd determinants and inverses (via the gsl

will return the vector   .

  

matrix inversion and
equation solving

gsl_stats march 24, 2009

important of which are  f   ty,	  f   ty, and a .13 data-
oriented readers will mostly be interested in a  (read: not a number), which is an
cabulary. all fourif statements will be true, and print their associated statements.

appropriate way to represent missing data. listing 4.8 shows you the necessary vo-

floating-point numbers can take several special values, the most

matrices and models

135

4.5 numbers

#include <math.h> //nan handlers
#include <stdio.h> //printf

int main(){

double missing_data = nan;
double big_number = infinity;
double negative_big_number =    infinity;

}

if (isnan(missing_data))

if (is   nite(big_number)== 0)

if (is   nite(missing_data)== 0)

printf("big_number is not    nite.\n");

if (isinf(negative_big_number)==    1)

printf("missing_data isn   t    nite either.\n");

printf("missing_data is missing a data point.\n");

printf("negative_big_number is negative in   nity.\n");

because    oating-point numbers can take these values, division by zero won   t crash

listing 4.8 some functions to test for in   nity or nans. online source:   a 	 be .
.
your program. assigningd 	b ed=1.0/0.0 will result ind==  f   ty,
andd=0.0/0.0 will result ind being set to a . however, integers have none
of these luxuries: tryi  i=1/0 and you will get something in the way of
a i h e i
ex
e  i   
  ed	  ed .
comparison to an a  value always fails:
respectively, they may be taken as given: to the best of my knowledge, all current hardware supports  f   ty
and a . recall thatg

 requires(cid:21)  d=g 	99 for c99 features; otherwise, the gsl providesgs _  s  f,
gs _ eg  f, andgs _ a  that work in non-c99 and non-ieee 754 systems.

13pedantic note on standards: these values are de   ned in the c99 standard (  7.12) only on machines that
support the ieee 754/iec 60559    oating-point standards, but since those standards are from 1985 and 1989,

// this evaluates to false.
// this evaluates to false. (!)
// returns 1: the correct way to check for an nan value.

double blank = nan;
blank == nan;
blank == blank;
isnan ( blank) ;

136

ln   

chapter 4

gsl constant

gsl constant

gsl_stats march 24, 2009

table 4.9 the gsl de   nes a number of useful constants.

there are a number of useful constants that are de   ned

p1/2 = 1/   2

approx.
3.14159
1.57080
0.78540
0.31831
0.63662
1.77245
1.12838
1.14473

approx.
2.71828   
1.44270   /2
0.43429   /4
1/  
0.69315
2/  
2.30259
1.41421      
0.70711
1.73205
0.57722

e
log2 e
log10 e
ln 2
ln 10
   2
   3
euler constant (  )

 _e
 _  
 _  g2e
 _  _2
 _  g10e
 _  _4
 _  2
 _1_  
 _  10
 _2_  
 _s	rt  
 _s	rt2
 _s	rt1_2
2/       _2_s	rt  
 _s	rt3
 _    
 _eu er
  prede   ned constants
by the gsl (via preprocessor#defi es); they are listed
forgotten this notation,    is written as 3.14159    100, or3.14159e0, and 100   as
3.14159    102, or3.14159e2. numbers always have exactly one digit before the
your computer works in binary, so    oating-point numbers (of typef  a  and
d 	b e) are of the form d    2n, where d is a string of ones and zeros and n is an
gsl is not available. the exceptions are _s	rt3 and _s	rt  , which are gsl-speci   c.

the scale of a number is its overall magnitude, and is expressed by the exponent n.
the    oating-point system can express a wide range of scales with equal ease: it is
as easy to express three picometers (3e   12) as three million kilometers (3e9). the
precision is how many signi   cant digits of information are held in d: 3.14e   12
and 3.14e9 both have three signi   cant decimal digits of information.

in table 4.9.14 it is generally better form to use these constants, because they are
more descriptive than raw decimal values, and they are de   ned to greater precision
than you will want to type in yourself.

precision as with any    nite means of writing real numbers, there is a roundoff er-
ror to the computer   s internal representation of numbers. the computer
basically stores non-integer numbers using scienti   c notation. for those who have

there is a    xed space for d, and when that space is exceeded, n is adjusted to suit,
but that change probably means a loss in precision for d. to give a small base-ten

decimal point, and the exponent is chosen to ensure that this is the case.

14the gsl gets most of these constants from bsd and unix, so you may be able to    nd them even when the

exponent.15

15this oversimpli   es some details that are basically irrelevant for users. for example, the    rst digit of d is

always one, so the computer normally doesn   t bother storing it.

gsl_stats march 24, 2009

matrices and models

137

i
100
200
300
400
500
600
700
800
900
1000
1100
1200

2i
1.26765e+30
1.60694e+60
2.03704e+90
2.58225e+120
3.27339e+150
4.14952e+180
5.26014e+210
6.66801e+240
8.45271e+270
1.07151e+301
inf
inf

2   i
7.88861e   31
6.22302e   61
4.90909e   91
3.87259e   121
3.05494e   151
2.40992e   181
1.90109e   211
1.4997e   241
1.18305e   271
9.33264e   302
0
0

table 4.10 multiplying together large columns of numbers will eventually fail.

precision can easily be lost in this manner, and once lost can never be regained.
one general rule of thumb implied by this is that if you are writing a precision-

example, say that the space for d is only three digits; then 5.89e0  892e0 = 525e1,
though 5.89    892 = 5, 254. the    nal four was truncated to zero to    t d into its
given space.

sensitive function to act on af  a , used 	b e variables internally, and if you
are taking ind 	b es, use   gd 	b e internally.
d 	b e can represent. table 4.10 shows a table of powers of two as represented
by ad 	b e. for i > 1, 000   a modest number of data points   ad 	b e throws
produce this table, and also repeats the experiment using a   gd 	b e, which

the loss of precision becomes especially acute when multiplying together a long
list of numbers. this will be discussed further in the chapter on maximum likeli-
hood estimation (page 330), because the likelihood function involves exactly such
multiplication. say that we have a column of a thousand values each around a half.
then the product of the thousand elements is about 2   1000, which strains what a

in the towel and calls 2i =     and 2   i = 0. these are referred to as an over   ow
error and under   ow error, respectively.

for those who would like to try this at home, listing 4.11 shows the code used to

doesn   t give up until over 16,000 doublings and halvings.

gsl_stats march 24, 2009

138

#include <math.h>
#include <stdio.h>

chapter 4

}

int main(){

for large i. online source:

printf("%i\t %g \t %g\n", i, ldexp(1,i), ldexp(1,   i));

printf("%i\t %lg \t %lg\n", i, ldexpl(1,i), ldexpl(1,   i));

listing 4.11 find the computer   s representation of 2i and 2   i

printf("powers of two held in a double:\n");
for(int i=0; i< 1400; i+=100)

printf("powers of two held in a long double:\n");
for(int i=0; i< 18000; i+=1000)

  we   f w .
.
    the program uses the dex  family of functions, which manipulate the    oating-
    the  i  f format speci   er for the   gd 	b e type is  g.16
 i   / i    form for as long as possible. either system will need to provide its

if you need to calculate    to a million decimal points, you will need to    nd a
library that can work with numbers to arbitrary precision. such libraries typi-
cally work by representing all numbers as a data structure listing the ones place,
tens place, hundreds place, . . . , and then extending the list in either direction as
necessary. another alternative is rational arithmetic, which leaves all numbers in

the solution to the problem of    nding the product of a large number of elements
is to calculate the log of the product rather than the product itself; see page 330.

point representation of a number directly (and are thus probably bad form).

own add/subtract/multiply/divide routines to act on its data structures, rather than
using c   s built-in operators. unfortunately, the added layer of complexity means
that the arithmetic operations that had been fast procedures (often implemented via
special-purpose registers on the processor hardware itself) are now a long series of
library calls.

so to do math ef   ciently on large matrices, we are stuck with    nite precision, and
therefore must not rely too heavily on numbers after around maybe four signi   cant
digits. for the purposes of estimating and testing the parameters of a model using
real-world data, this is ok. if two numbers differ only after eight signi   cant dig-
its (say, 3.14159265 versus 3.14159268), there is rarely any reason to take these
numbers as signi   cantly different. even if the hypothesis test indicates that they
are different, it will be dif   cult to convince a referee of this.

16the g and  g format speci   ers round off large values, so change them to to f and  f to see the precise

value of the calculations without exponential notation.

gsl_stats march 24, 2009

conditioning most matrix routines do badly when the determinant is near zero,
or when eigenvalues are different orders of magnitude. one way
to cause such problems with your own data is to have one column that is of the
order of 1    1010 and another that is on the order of 1    10   10. in    nite-precision
arithmetic on two numbers of such wide range, the smaller number is often simply

139

matrices and models

swallowed:3.14e10 5.92e	10=3.14e10.
or you could modify it in theg  _ a  ix:

select population/1000., nail_thickness*1000.
from health_data;

thus, try to ensure that each column of the data is approximately of the same
order of magnitude before doing calculations. say that you have a theory that mean
   ngernail thickness is in   uenced by a location   s population. you could modify the
scale when pulling data from the database,

apop_col(data, 0, pop)
gsl_vector_scale(pop, 1/1000.);
apop_col(data, 1, nails)
gsl_vector_scale(nails, 1000.);

these notes about conditioning are not c-speci   c. any mathematics package that
hopes to work ef   ciently with large matrices must use    nite-precision arithmetic,
and will therefore have the same problems with ill-conditioned data matrices. for
much more about precision issues and the standard machine representation of num-
bers, see goldberg (1991).

comparison

every number to be a little bit off from where it should be.

floating-point numbers are exact representations of a real number
with id203 zero. simply put, there is a bit of fuzz, so expect

it is normally not a problem that 4 + 1e   20 6= 4, and such fuzz can be safely
ignored, but polhill et al. (2005) point out that the fuzziness of numbers can be
a problem for comparisons, and those problems can create odd effects in simu-
lations or agent-based models. after a long series of    oating-point operations, a

comparison of the form  ==0  will probably not work. for example, listing
there are labor-intensive solutions to the problem, like always using   gi  s
17this example is from a web site af   liated with the authors of the above paper, ath   ://www. a
a	 ay.
a
.	k/fea  	 /f  a i g	  i  /.

4.12 calculates 1.2    3   0.4 using standard ieee arithmetic, and    nds that it is less
than zero.17

gsl_stats march 24, 2009

140

chapter 4

#include <stdio.h>

}

int main(){

double t = 1.2;

t    = 0.4;
t    = 0.4;
t    = 0.4;
if (t<0)

printf ("by the ieee    oating   point standard, 1.2     3*.04 < 0.\n");

for everything,18 but the most sensible solution is to just bear in mind that no
comparison is precise so, for example, agents should not die when their wealth is
zero, but when it is less than maybe 1e   6. otherwise, the model should be robust
to agents who have an iota of negative wealth.

listing 4.12 the ieee standard really does imply that 1.2     3    0.4 < 0. online source:f	zz.
.
   floating-point numbers can take on values of	  f   ty,  f   ty,
and a .
f==0, butfab  f <1e	6 (or so).
4.6  g  _ a  ix andg  _ve
   
g  _ a  ix_ge  andg  _ve
   _ e . doing so is bad form, inviting errors and
18recall that a/b  b a b is exactlya for   gi  a b,b
19if you are really concerned about the overhead from these functions, then#defi egs _ra ge_c ec _	
 ff, either via that preprocessor directive or adding -dgsl_range_check_off to your compilation com-

   multiplying together a column of a thousand numbers will break, so
get the log of the product by summing the logs of the column   s ele-
ments.

first, a warning: the intent of this sec-
tion is not to show you how to circum-
vent the gsl   s access functions such as

making code more dif   cult to read.19 instead, these notes will be useful to you for

   try to keep the scale of your variables within a factor of about a

   reporting results based on the    fth signi   cant digit (or so) is spuri-

   exact comparisons of    oating-point numbers can fail, so do not test

thousand of each other.

internals

6= 0.

ous.

gsl_stats march 24, 2009

matrices and models

here is the relevant section of the declaration of theg  _ a  ix structure:

understanding why the data structures are the way they are, and giving you a han-
dle on what operations are easy and what operations are dif   cult.

141

typedef struct {

size_t size1;
size_t size2;
size_t tda;
double * data;
[...]
int owner;

} gsl_matrix;

(0,0) (0, 1) . . . (0, 9) (1,0) (1, 1) . . . (1, 9) (2,0) (2, 1) . . . (2, 9) . . . (9,0) (9, 1) . . . (9, 9)

   

(0,0)
      (1,0)
      (2,0)
      (3,0)
      (4,0)
      (5,0)
      (6,0)
      (7,0)
      (8,0)
      (9,0)

(0,1)
(1,1)
(2,1)
(3,1)
(4,1)
(5,1)
(6,1)
(7,1)
(8,1)
(9,1)

(0,2)
(1,2)
(2,2)
(3,2)
(4,2)
(5,2)
(6,2)
(7,2)
(8,2)
(9,2)

(0,3)
(1,3)
(2,3)
(3,3)
(4,3)
(5,3)
(6,3)
(7,3)
(8,3)
(9,3)

(0,4)
(1,4)
(2,4)
(3,4)
(4,4)
(5,4)
(6,4)
(7,4)
(8,4)
(9,4)

(0,5)
(1,5)
(2,5)
(3,5)
(4,5)
(5,5)
(6,5)
(7,5)
(8,5)
(9,5)

(0,6)
(1,6)
(2,6)
(3,6)
(4,6)
(5,6)
(6,6)
(7,6)
(8,6)
(9,6)

(0,7)
(1,7)
(2,7)
(3,7)
(4,7)
(5,7)
(6,7)
(7,7)
(8,7)
(9,7)

(0,8)
(1,8)
(2,8)
(3,8)
(4,8)
(5,8)
(6,8)
(7,8)
(8,8)
(9,8)

(0,9)
(1,9)
(2,9)
(3,9)
(4,9)
(5,9)
(6,9)
(7,9)
(8,9)
(9,9)

steps from the (2,4) element.

figure 4.13 within a submatrix, the (3,4) element is still one step from the (3,3) element, and ten

as you know, ize1 and ize2 are simply the count of rows and columns. the
da a pointer is a single pointer to a stream of numbers. since memory addresses
stepping along the row means simply stepping along by ize f d 	b e  units,
and stepping down a matrix column means stepping by ize f d 	b e   ize2
jump ize f d 	b e  35 steps from the base element.

are linear, the top of figure 4.13 is closer to what is actually in memory: the    rst
row of data, followed immediately afterward by the second row, then the third row,
and so on, forming one long row of data. by adding line breaks, we humans can
think of this one long row of data as actually being a grid, like the second half of
figure 4.13.

steps from the current element. for example, to reach the (3,5) element of a ten by
ten matrix, the processor must skip three rows and then skip    ve items, so it would

modern computers are proactive about data gathering. when they read data from
slower types of memory, they also check the neighbors as well. if the code is rel-
atively predictable, the system can gather the next bit of data at the same time as

mand line. between this and the compiler   s optimization routines, the function call will reduce to the appropriate
array operation.

142

chapter 4

gsl_stats march 24, 2009

size1 = 100;
size2 = 10;
tda = 10;
data = [location of (0,0)];
owner =1;

now say that we have a 100    10 matrix, which would have the following infor-
mation:

if we wanted to pull out the 4   6 submatrix that begins at (3, 2), then the resulting
submatrix data would look like this:

derfully with such a system, because steps are predictable and of    xed size, so the
processor has a good chance of correctly guessing what data to put into its faster
caches.

it is crunching the current data element. theg  _ a  ix structure works won-
with da equal to ize2, jumping down a column would require a jump of ize	
 f d 	b e   da.20
elements in the    rst row, we would step ize f d 	b e  2 forward from the
base element pointed to byda a, and to get to the beginning of the next column,
we would jump ize f d 	b e   da. thus, the process of pulling a subset of
for ize1 and ize2. no actual data was copied. this is howg  _ a  ix_  w,
a   _r w,a   _c  ,a   _sub atr x, and other such routines work.
the w e  variable now becomes important, because there could be multiple sub-
theg  _ve
    has a similar structure, including a starting point and a stride to
20the abbreviation da stands for trailing dimension of array. for ag  _ve
   , the analogous element is

size1 = 4;
size2 = 6;
tda = 10;
data = [location of (3,2) in the original matrix];
owner =0;

matrices all pointing to the same data. since the submatrix is not the owner of its
data, the gsl will not allow it to free the data set to which it points.

indicate how far to jump to the next element. thus, taking a row or column subset
of a matrix also merely requires writing down the correct coordinates and lengths.

the gsl   s structures are good for fast access, because the next element is always

we can use this matrix exactly as with the full matrix: for example, to get the third

the data merely required    nding the    rst point and writing down arbitrary limits

the stride.

gsl_stats march 24, 2009

matrices and models

143

other designers with different goals have used different means of representing a

very well for the goal of allowing the hardware to process rows and columns of
homogeneous data with maximal ef   ciency.

since space is always allocated for every element, there is no way to ef   ciently
represent sparse matrices.

systems that deal well with variable-sized jumps have the pros and cons reversed
from the above. for example, one solution is to let the overall table be a list of
column vectors, where each column has its own type. but traversing along a row
could involve jumping all over memory, so common operations like    nding the
sum for each row becomes a signi   cantly slower operation.

a    xed jump relative to the current element, whether you are traversing by rows or
columns. they are exceptionally good for describing submatrices and subvectors,
because doing so merely requires writing down new coordinates. they handle a
1    10, 000 matrix just as easily as a 10, 000    1 matrix or a 100    100 matrix.
they are not good for non-contiguous subsets like the    rst, second, and    fth
columns of a data set, since the relative jumps from one column to the next are
not identical. similarly, they are not good for holding various types of data, where

some jumps could be ize f i    and others could be ize f d 	b e . also,
data matrix, and theg  _ a  ix is by no means the best for all needs. but it does
   there is no simple way to take non-contiguous subsets ofg  _	
 a  i
e  org  _ve
    . you will either need to copy the data
manually (i.e., using af   loop), or do the manipulations in the da-
tabase before your data is ing  _ a  ix form.
this level of abstraction, in the form of thea   _  de  anda   _da a structures

recall the one-sentence summary of statistical analysis from the
   rst page of the introduction: estimate the parameters of a model
using data. the apophenia library provides functions and data structures at exactly

   it is very easy to take contiguous subvectors or submatrices of these
structures. doing so requires copying only a few bits of metadata, but
not the data itself.

   the gsl   s matrix and vector structures are very good for ef   cient
computation, because each element has a    xed size and is a    xed
distance from the neighboring elements.

4.7 models

and the functions that operate on them.

gsl_stats march 24, 2009

you have already met thea   _da a structure, which lent a hand to operations
thea   _  de  structure, which provides similar forms of strength through con-

on the matrix algebra layer of abstraction; the remainder of the chapter introduces

chapter 4

144

straint: it encapsulates model information in a uniform manner, allows models to
be used interchangeably in functions that can take any model as an input, and al-
lows sensible defaults to be    lled in as necessary.

a great deal of statistical work consists of converting or combining existing models
to form new ones. that is, models can be    ltered to produce models just as data
can be    ltered to provide new information. we can read estimation as    ltering an
un-parameterized model into a parameterized one. bayesian updating (discussed
more thoroughly on page 258) takes in a prior model, a likelihood function, and
data, and outputs a new model   which can then be used as the input to another
round of    ltering when new data comes in.

another example discussed below, is the imposition of a constraint: begin by esti-
mating a general model, then generate a new model with a constraint imposed on
some of the parameters, and re-estimate. the difference in log likelihoods of the
constrained and unconstrained models can then be used for hypothesis testing.

the structure of the model struct

in the usage of this book, a model intermedi-
ates between data and parameters. from there,

the model can go in three directions:

i) x       : given data, estimate parameters.
ii)        x: given parameters, generate arti   cial data (e.g., make random draws

from the model, or    nd the expected value).

id203.

iii) (x,   )     p: given both data and parameters, estimate their likelihood or

for many common models, there area   _  de s already written, including dis-

to give a few examples, form (i) is the descriptive problem, such as estimating a
covariance or ols parameters. monte carlo methods use form (ii): producing a
few million draws from the model given    xed parameters. bayesian estimation is
based on form (iii), describing a posterior id203 given both data and param-
eters, as are the likelihoods in a id113.

tributions like the normal, multivariate normal, gamma, zipf, et cetera, and gen-
eralized linear models like ols, wls, probit, and logit. because they are in a
standardized form, they can be sent to model-handling functions, and be applied
to data in sequence. for example, you can    t the data to a gamma, a lognormal,
and an exponential distribution and compare the outcomes (as in the exercise on
page 257).

gsl_stats march 24, 2009

matrices and models

145

every model can be estimated via a form such as

apop_model *est = apop_estimate(data, apop_normal);

examples of this form appear throughout the book   have a look at the code later
in this section, or on pages 133, 289, 352, or 361, for example.

changing the defaults a complete model includes both the model   s functions and
the environment in which those functions are evaluated

discussion of the other directions   making random draws of data given parame-
ters and    nding likelihoods given data and parameters   will be delayed until the
chapters on monte carlo methods and id113, respec-
tively.

(gentleman & ihaka, 2000). thea   _  de  thus includes both the outputs, the
but a generic   	
  indended to hold settings for all models faces the complica-
apohenia standarda   _  de  struct thus has an open space for attaching differ-

functions, and everything one would need to replicate one from the other. for the
purposes of apophenia   s model estimations, an n (0, 1) is a separate model from
an n (1, 2), and a maximum likelihood model whose optimization step is done via
a conjugate gradient method is separate from an otherwise identical model esti-
mated via a simplex algorithm.

tion that different methods of estimation require different settings. the choice of
conjugate gradient or simplex algorithm is meaningless for an instrumental vari-
able regression, while a list of instrumental variables makes no sense to a maxi-
mum likelihood search.

ent groups of settings as needed. if the model   s defaults need tweaking, then you
can    rst add an id113, ols, histogram, or other settings group, and then change
whatever details need changing within that group. again, examples of the usage
and syntax of this two-step processs abound, both in online documentation and
throughout the book, such as on pages 153, 339, or 352.

writing your own

it would be nice if we could specify a model in a single form
and leave the computer to work out the best way to implement
all three of the directions at the head of this section, but we are rather far from
such computational nirvana. recall the example of ols from the    rst pages of
chapter 1. the    rst form of the model      nd the value of    such that (y     x  )2
is minimized   gave no hint that the correct form in the other direction would be
(x   x)   1x   y. other models, such as the probit model elaborated in chapter 10,
begin with similar x  -type forms, but have no closed form solution.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

gsl_stats march 24, 2009

cedure for one, two, or all three of the above directions, such as ane  i a e
function, a  g_ ike ih  d and  (id203) function, and ad aw function to

thus, writing down a model for a computable form requires writing down a pro-

chapter 4

146

make random draws. you can    ll in those that you can solve in closed form, and
can leave apophenia to    ll in computationally-intensive default procedures for the
rest.

#include <apop.h>

apop_model new_ols;

static apop_model *new_ols_estimate(apop_data *d, apop_model *params){

apop_col(d, 0, v);
apop_data *ydata = apop_data_alloc(d   >matrix   >size1, 0, 0);
gsl_vector_memcpy(ydata   >vector, v);
gsl_vector_set_all(v, 1); //af   ne:    rst column is ones.
apop_data *xpx = apop_dot(d, d,    t   , 0);
apop_data *inv = apop_matrix_to_data(apop_matrix_inverse(xpx   >matrix));
apop_model *out = apop_model_copy(new_ols);
out   >data = d;
out   >parameters = apop_dot(inv, apop_dot(d, ydata, 1), 0);
return out;

}

}

int main(){

.estimate = new_ols_estimate};

apop_model new_ols = {.name ="a simple ols implementation",

apop_data *dataset = apop_text_to_data("data   regressme", 0, 1);
apop_model *est = apop_estimate(dataset, new_ols);
apop_model_show(est);

listing 4.14 a new implementation of the ols model. online source: ew   .
.
    in this case, only thee  i a e function is speci   ed.
    lines 12   14 allocate ana   _  de  and set its a a e e  element to the correct

for example, listing 4.14 shows a new implementation of the ols model. the
math behind ols is covered in detail on page 274.

    the procedure itself is simply a matter of pulling out the    rst column of data and

replacing it with ones, and calculating (x   x)   1x   y.

value. line 13 keeps a pointer to the original data set, which is not used in this
short program, but often comes in handy.

    the allocation of the output on line 12 needs the model, but we have not yet de-
clared it. the solution to such circularity is to simply give a declaration of the

gsl_stats march 24, 2009

147

matrices and models

model on line 3.

on line 23 will do internally).

simple thanks to designated initializers (see p 32).

    given the function, line 18 initializes the model itself, a process which is rather

    lines 1   3 provide the complete header an external    le would need to use the new

only be known to this    le, so you can name it what you wish without worrying
about cluttering up the global name space. but with is a pointer to the function

    although the ai  function is at the bottom of this    le, the typical model deserves
its own    le. by using the  a i
 keyword in line    ve, the function name will
in the model object itself, a routine in another    le could use thea   _ ew_	
  s.e  i a e ...  form to call this function (which is whata   _e  i a e
model, since the structure of thea   _  de  is provided ina   .h.
compare the ew_  s model with thea   _    model.
    modify listing 4.14 to declare an array ofa   _  de s. declare
the    rst element to bea   _    and the second to be ew_  s.
[the reverse won   t work, because ew_  s destroys the input data.]
    write af   loop to    ll a second array of pointers-to-models with the
e  i a e[0   	> a a e e  	>ve
    and
e  i a e[1   	> a a e e  	>ve
   .
  an example: network data the default, for models such asa   _    or
a   _   bi , is that each row of the data is

    calculate and display the difference between

estimate from the two models.

q4.7

assumed to be one observation, the    rst column of the data is the dependent vari-
able, and the remaining columns are the independent variable.

for the models that are merely a distribution, the rule that one row equals one
observation is not necessary, so the data matrix can have any form: 1    10,000, or
10,000    1, or 100    100. this provides maximum    exibility in how you produce
the data.

but for data describing ranks (score of    rst place, second place, . . . ) things get
more interesting, because such data often appears in multiple forms. for example,
say that we have a classroom where every student wrote down the id number his
or her best friend, and we tallied this list of student numbers:
1 1 2 2 2 2 3 4 4 4 6 7 7 7.
first, we would need to count how often each student appeared:

gsl_stats march 24, 2009

chapter 4

148

id_no count
2
4
7
1
3
6

4
3
3
2
1
1.

in sql:

select id_no, count(*) as ct

from surveys
group by id_no
order by ct desc

if we were talking about city sizes (another favorite for rank-type analysis), we
would list the size of the largest city, the second largest, et cetera. the labels are
not relevant to the analysis; you would simply send the row of counts for most
popular, second most popular, et cetera:
4 3 3 2 1 1.
each row of the data set would be one classroom like the above, and the column
number represents the ranking being tallied.

as mentioned above, you can add groups of settings to a model to tweak its be-
havior. in the case of the models commonly used for rank analysis, you can signal
to the model that it will be getting rank-ordered data. for example:

apop_model *rank_version = apop_model_copy(apop_zipf);
apop_settings_add_group (rank_version, apop_rank, null);
apop_model_show(apop_estimate(ranked_draws, rank_version));

group of settings; e.g.,a   _e  i a e  a ked_d aw  a   _ga  a .

alternatively, some data sets are provided with one entry listing the rank for each
observation. there would be four 1   s, three 2   s, three 3   s, et cetera:
1 1 1 1 2 2 2 3 3 3 4 4 5 5.
in the city-size example, imagine drawing people uniformly at random from all
cities, and then writing down whether each person drawn is from the largest city,
the second largest, et cetera. here, order of the written-down data does not matter.
you can pass this data directly to the various estimation routines without adding a

149

q4.8

one above.

matrices and models

gsl_stats march 24, 2009

    read the data into a database.

on which one would run a network density analysis.

estimate. how well does the zipf model    t the data?

a row, with the nth-ranked in the nth column. this data set includes
only one classroom, so you will have only one row of data.

the   i ee column in the    leda a	
 a       is exactly the sort of data
    query the vector of ranks to ana   _da a set, using a query like the
    transpose the matrix (hint:g  _ a  ix_  a     e_ e 
 y), be-
causea   _zi f   se  i a e function requires each classroom to be
    calla   _e  i a e y 	 da a a   _zi f ; show the resulting
  id113 models to give some examples from a different style of model, here are
here,be a holds the parameters to be maximized andd is the    xed parameters   
model with. = ew_ .
using the same format asa   _da a_a   
: size of vector, rows in ma-
trix, then columns in matrix. if any of these is	1, then the	1 will be re-
placed with the number of columns in the input data set   s matrix (i.e.,y 	 _	

the data. this function will return the value of the log likelihood function at the
given parameters and data.
in some cases, it is more natural to express probabilities in log form, and some-
times in terms of a plain id203; use the one that works best, and most func-
tions will calculate the other as needed.

    if you are using a id203 instead of a log likelihood, hook it into your

    the three numbers after the name are the size of the parameter structure,

some notes on writing models based on a maximum likelihood

static double apop_new_log_likelihood(gsl_vector *beta, apop_data *d)

    write a likelihood function. its header will look like this:

apop_model new_model = {"the me distribution", 2, 0, 0,

.log_likelihood = new_log_likelihood };

    declare the model itself:

estimation.

150

gsl_stats march 24, 2009

da a	> a  ix	> ize2).21 this is what you would use for an ols regres-
with this little, a call likea   _e  i a e y 	 _da a  ew_  de   will work,
becausea   _e  i a e defaults to doing a maximum likelihood search if there
is no explicitly-speci   ed ew_  de .e  i a e function.
requiring just a few derivatives. see listing 4.17 (or any existinga   _  de ) for

    for better estimations, write a gradient for the log likelihood function. if you do
not provide a closed-form gradient function, then the system will    ll in the blank
by numerically estimating gradients, which is slower and has less precision. cal-
culating the closed-form gradient is usually not all that hard anyway, typically

sion, for example, where there is one parameter per independent variable.

chapter 4

an example showing the details of syntax.

setting constraints a constraint could either be imposed because the author
of the model declared an arbitrary cutoff (   we can   t spend
more than $1,000.   ) or because evaluating the likelihood function fails (ln(   1)).
thus, the system needs to search near the border, without ever going past it, and it
needs to be able to arbitrarily impose a constraint on an otherwise unconstrained
function.

straint function that will ensure that both parameters of a two-dimensional input
are greater than given values.

apophenia   s solution is to add a constraint function that gets checked before the
actual function is evaluated. it does two things if the constraint is violated: it
nudges the point to be evaluated into the valid area, and it imposes a penalty to
be subtracted from the    nal likelihood, so the system will know it is not yet at an
optimum. the unconstrained maximization routines will then have a continuous
function to search but will never    nd an optimum beyond the parameter limits.22

to give a concrete example, listing 4.15 adds to thea   _    a  model a con-
21if your model has a more exotic parameter count that needs to be determined at run-time, use the  e 
method of thea   _  de  to do the allocation.
the best you could expect would be a series of estimations with decreasing penalties;a   _e  i a e_ e  a  
23if you need to calculate the distance to a point in your own constraint functions, see eithera   _ve
   _	
di  a 
e ora   _ve
   _g id_di  a 
e.

observe how the constraint function manages all of the requisite steps. first, it
checks the constraints and quickly returns zero if none of them binds. then, if
they do bind, it sets the return vector to just inside the constrained region. finally,
it returns the distance (on the manhattan metric) between the input point and the
point returned.23 the unconstrained evaluation system should repeatedly try points
closer and closer to the zero-penalty point, and the penalty will continuously de-
cline as we approach that point.

22this is akin to the common penalty function methods of turning a constrained problem into an unconstrained
one, as in avriel (2003), but the formal technique as commonly explained involves a series of optimizations where
the penalty approaches zero as the series progresses. it is hard to get a computer to    nd the limit of a sequence;

can help with the process.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

30

gsl_stats march 24, 2009

matrices and models

151

#include <apop.h>

double linear_constraint(apop_data * d, apop_model *m){

double limit0 = 2.5,

limit1 = 0,
tolerance = 1e   3; // or try gsl_epsilon_double

double beta0 = apop_data_get(m   >parameters, 0,    1),

beta1 = apop_data_get(m   >parameters, 1,    1);

if (beta0 > limit0 && beta1 > limit1)

return 0;

//else create a valid return vector and return a penalty.
apop_data_set(m   >parameters, 0,    1,gsl_max(limit0 + tolerance, beta0));
apop_data_set(m   >parameters, 1,    1, gsl_max(limit1 + tolerance, beta1));
return gsl_max(limit0 + tolerance     beta0, 0)

+ gsl_max(limit1 + tolerance     beta1, 0);

}

int main(){

apop_model *constrained = apop_model_copy(apop_normal);
constrained   >estimate = null;
constrained   >constraint = linear_constraint;

}

gsl_cdf_chisq_p(test_stat, 1)*100);

listing 4.15 an optimization, a constrained optimization, and a likelihood ratio test comparing the

apop_db_open("data   climate.db");
apop_data *dataset = apop_query_to_data("select pcp from precip");
apop_model *free = apop_estimate(dataset, apop_normal);
apop_model *constr = apop_estimate(dataset, *constrained);
apop_model_show(free);
apop_model_show(constr);
double test_stat = 2 * (free   >llikelihood     constr   >llikelihood);
printf("reject the null (constraint has no effect) with %g%% con   dence\n",

two. online source:    a   .
.
in the real world, set linear constraints using thea   _ i ea _
     ai   func-
the ai  portion of the program does a likelihood ratio test comparing constrained
the copy. thee  i a e routine for the normal doesn   t use any constraint, so it is
    lacking an explicite  i a e routine in the model, line 25 resorts to maximum

tion, which takes in a set of contrasts (as in the form of the f test) and does all
the requisite work from there. for an example of its use, have a look at the budget
constraint in listing 4.17.

    lines 19   21 copy off the basic normal model, and add the constraint function to

and unconstrained versions of the normal distribution.

invalid in the unconstrained case, so line 20 erases it.

likelihood estimation (id113). the id113 routine takes the model   s constraint into

gsl_stats march 24, 2009

152

account.

chapter 4

    lines 28   29 are the hypothesis test. basically, twice the difference in log likeli-

hoods has a   2 distribution; page 350 covers the details.

an example: utility maximization the process of maximizing a function
subject to constraints is used extensively
outside of statistical applications, such as for economic agents maximizing their
welfare or physical systems maximizing their id178. with little abuse of the opti-
mization routines, you could use them to solve any model involving maximization
subject to constraints. this section gives an extended example that numerically
solves an econ 101-style utility maximization problem.

the data (i.e., thea   _da a set of    xed elements) will have a one-element vector

the consumer   s utility from a consumption pair (x1, x2) is u = x  
2 . given
prices p1 and p2 and b dollars in cash, she has a budget constraint that requires
p1x1 + p2x2     b. her goal is to maximize utility subject to her budget constraint.

1 x  

and a 2    2 matrix, structured like this:(cid:20) budget

price1   
price2

   (cid:21).

once the models are written down, the estimation is one function call, and calcu-
lating the marginal values is one more. overall, the program is overkill for a prob-
lem that can be solved via two derivatives, but the same framework can be used
for problems with no analytic solutions (such as for consumers with a stochastic
utility function or dynamic optimizations with no graceful closed form).

because the estimation    nds the slopes at the optimum, it gives us comparative
statics, answering questions about the change in the    nal decision given a marginal
rise in price p1 or p2 (or both).

    thee
  101_e  i a e routine just sets some optimization settings and calls
a   _ axi 	 _ ike ih  d.
    the budget constraint, in turn, is a shell fora   _ i ea _
     ai  . that

listing 4.16 shows the model via numerical optimization, and because the model
is so simple, listing 4.17 shows the analytic version of the model.

function requires a constraint matrix, which will look much like the matrix of
equations sent in to the f tests on page 310. in this case, the equations are

      

   budget <    p1  1     p2  2

0
0

  1

<
<

       .

  2

all inequalities are in less-than form, meaning that the    rst   the cost of goods is

gsl_stats march 24, 2009

matrices and models

153

#include <apop.h>
apop_model econ_101;

static apop_model * econ101_estimate(apop_data *choice, apop_model *p){

apop_settings_add_group(p, apop_id113, p);
apop_settings_add(p, apop_id113, tolerance, 1e   4);
apop_settings_add(p, apop_id113, step_size, 1e   2);
return apop_maximum_likelihood(choice, *p);

}

static double budget(apop_data *beta, apop_model* m){

double price0 = apop_data_get(m   >data, 0, 0),
price1 = apop_data_get(m   >data, 1, 0),
cash = apop_data_get(m   >data, 0,    1);
apop_data *constraint = apop_data_alloc(3, 3, 2);

apop_data_   ll(constraint,

   cash,    price0,    price1,
0., 1., 0.,
0., 0., 1.);

return apop_linear_constraint(m   >parameters   >vector, constraint, 0);

}

static double econ101_p(apop_data *d, apop_model *m){

}

double alpha = apop_data_get(d, 0, 1),

return pow(qty0, alpha) * pow(qty1, beta);

.estimate = econ101_estimate, .p = econ101_p, .constraint= budget};

apop_model econ_101 = {"max cobb   douglass subject to a budget constraint", 2, 0, 0,

beta = apop_data_get(d, 1, 1),
qty0 = apop_data_get(m   >parameters, 0,    1),
qty1 = apop_data_get(m   >parameters, 1,    1);

listing 4.16 an agent maximizes its utility. online source:e
  101.
.
that
     ai   is only allocated once by declaring it as  a i
 and initially
setting it to u  , then allocating it only if it is not u  .

less than the budget   had to be negated. however, the next two statemets,   1 is
positive and   2 is positive, are natural and easy to express in these terms. convert-
ing this system of inequalities into the familiar vector/matrix pair gives

    the budget consraint as listed has a memory leak that you won   t notice at this
scale: it re-speci   es the constraint every time. for larger projects, you can ensure

   p1    p2
0
1
1
0

       .

   budget

      

0
0

    the analytic version of the model in listing 4.17 is a straightforward translation

gsl_stats march 24, 2009

154

chapter 4

#include <apop.h>
apop_model econ_101_analytic;
apop_model econ_101;

#de   ne fget(r, c) apop_data_get(   xed_params, (r), (c))

static apop_model * econ101_analytic_est(apop_data *    xed_params, apop_model *pin){

apop_model *est = apop_model_copy(econ_101_analytic);
double budget = fget(0,    1), p1 = fget(0, 0), p2 = fget(1, 0),

alpha = fget(0, 1), beta = fget(1, 1);

double x2 = budget/(alpha/beta + 1)/p2,

x1 = (budget     p2*x2)/p1;

est   >data =    xed_params;
est   >parameters = apop_data_alloc(2,0,0);
apop_data_   ll(est   >parameters, x1, x2);
est   >llikelihood = log(econ_101.p(   xed_params, est));
return est;

}

static void econ101_analytic_score(apop_data *   xed_params, gsl_vector *gradient,

apop_model *m){

}

apop_model econ_101_analytic = {"analytically solve cobb   douglass maximization subject

double x1 = apop_data_get(m   >parameters, 0,    1);
double x2 = apop_data_get(m   >parameters, 1,    1);
double alpha = fget(0, 1), beta = fget(1, 1);

gsl_vector_set(gradient, 0, alpha*pow(x1,alpha   1)*pow(x2,beta));
gsl_vector_set(gradient, 1, beta*pow(x2,beta   1)*pow(x1,alpha));

to a budget constraint",
.vbase =2, .estimate=econ101_analytic_est, .score = econ101_analytic_score};

listing 4.17 the analytic version. online source:e
  101.a a y i
.
.
the long function call to pull parameters tofge . section 6.4 (page 211) gives the

of the solution to the constrained optimization. if you are familiar with lagrange
multipliers you should have little dif   culty in verifying the equations expressed
by the routines. the routines are in the natural slots for estimating parameters and
estimating the vector of parameter derivatives.

    the term score is de   ned in chapter 10, at which point you will notice that its
use here is something of an abuse of notation, because the score is de   ned as the
derivative of the log-utility function, while the function here is the derivative of the
utility function.

    the preprocessor can sometimes provide quick conveniences; here it abbreviates

details of the preprocessor   s use and many caveats.

    the process of wrapping library functions in standardized model routines, and oth-

gsl_stats march 24, 2009

matrices and models

155

#include <apop.h>
apop_model econ_101, econ_101_analytic;

void est_and_score(apop_model m, apop_data *params){

gsl_vector *marginals = gsl_vector_alloc(2);
apop_model *e = apop_estimate(params, m);

apop_model_show(e);
printf("\nthe marginal values:\n");
apop_score(params, marginals, e);
apop_vector_show(marginals);
printf("\nthe maximized utility: %g\n", exp(e   >llikelihood));

}

}

int main(){

listing 4.18 given the models,

0, 3, 0.6}; //0 is just a dummy.

double param_array[] = {8.4, 1, 0.4,

apop_data *params = apop_line_to_data(param_array, 2,2,2);

sprintf(apop_opts.output_delimiter, "\n");
est_and_score(econ_101, params);
est_and_score(econ_101_analytic, params);

the ai  program is but a series of calls. online source:
e
  101. ai .
.
erwise putting everything in its place, pays off in the ai  function in listing 4.18.
notably, thee  _a d_ 
  e function can run without knowing anything about
metaphor that likelihood=utility   thee
  101 model   s maximization returned the
log utility in the  ike ih  d element of the output model.
    thee
  101.a a y i
 model calculates the parameters without calculating util-
utility calculation from thee
  101 model. thanks to such re-calling of other
    the only public parts ofe
  101.
 ande
  101.a a y i
.
 are the models,
models themselves at the top ofe
  101. ai .
.
   le as per appendix a, with an b ects line listing all three.     les.

model internals, and can thus run on both the closed-form and numeric-search ver-
sions of the model. it also displays the maximized utility, because   continuing the

    the model    les will be compiled separately, and all linked together, using a make-

ity, but there is no need to write a separate calculation to    ll it in   just call the

so we don   t have to bother with a header    le, and can instead simply declare the

models    functions, it is easy to produce variants of existing models.

q4.9

use one of the models to produce a plot of marginal change in x0 and x1 as
   expands.

156

chapter 4

gsl_stats march 24, 2009

data, drawing data given parameters, and estimating likelihoods given
both.

   thea   _  de  aggregates methods of estimating parameters from
   mosta   _  de s take in a data structure with an observation on
model by putting your data into an appropriatea   _da a struc-
ture, and then usinga   _e  i a e da a y 	 _  de  . this
will produce ana   _  de  that you can interrogate or display on
functions that takea   _  de s as input make their best effort to

   if closed-form calculations for any of the model elements are avail-
able, then by all means write them in to the model. but the various

each row and a variable on each column. if the model includes a de-
pendent variable, it should be the    rst column.

   given a prepackaged model, you can estimate the parameters of the

screen.

   ll in the missing methods. for example, the score function is not
mandatory to use gradient-based optimization methods.

gsl_stats march 24, 2009

5

graphics

graphs are friendly.

   tukey (1977, p 157)

graphics is one of the places where the computing world has not yet agreed on a
standard, and so instead there are a dozen standards, including jpg, png, pdf,
gif, and many other tlas. you may    nd yourself in front of a computer that
readily handles everything, including quick displays to the screen, or you may    nd
yourself logging in remotely to a command-line at the university   s servers, that
support only svg graphics. some journals insist on all graphics being in eps
format, and others require jpgs and gifs.

the solution to the graphics portability problem is to use a handful of external
programs, easily available via your package manager, that take in plain text and
output an image in any of the panoply of graphics formats. the text-to-graphics
programs here are as open and freely available as gcc, so you can be con   dent that
your code will be portable to new computers.1

but this chapter is not just about a few plotting programs: it is about how you can
control any text-driven program from within c. if you prefer to create graphics or
do other portions of your analytic pipeline using a separate package (like one of
the stats packages listed in the introduction), then you can use the techniques here
to do so.

1there is some politics about how this is not strictly true: the maintainers of gnuplot will not allow you to
modify their code and then distribute the modi   ed package independently (i.e., to fork the code base). the project
is entirely unrelated to the gnu project, and the name is simply a compromise between the names that the two
main authors preferred: nplot and llamaplot.

gsl_stats march 24, 2009

158

chapter 5

actual work.

the plot-producing program with which this chapter will primarily be concerned

unset key
set title "us national debt"
set term postscript color
set out    debt.eps   
plot    data   debt    with lines

appendix b takes a different approach to converting data to an executable script,
via command-line tools to modify text. if your data is not going through any com-
putations or transformations, you may be better off using a shell script as per ap-
pendix b instead of writing a c program as per this chapter.

is gnuplot. its language basically has only two verbs    e  and       plus a few
variants (	  e , e    , et cetera). to give you a feel for the language, here is a
typical gnuplot script; you can see that it consists of a string of e  commands to
eliminate the legend, set the title, et cetera, and a    nal     command to do the
in the code supplement, you will    nd the         le, which provides many of the
plots in this chapter in cut-and-paste-able form. you will also    nd theda a	deb 
for gnuplot   s commands (g 	    ), so you can interactively try
different e  commands to see what options look best. after you have    nished
script (e.g., a    le named     e).
    from the shell command line, rung 	    	 e  i  <     e. this runs the
script and exits, but the plot persists on the screen. without the	 e  i   option,
looking at the plot on screen, then the	 e  i   option is unnecessary.
    rung 	     with no options, and from its prompt, type  ad      e . this
    the hybrid:g 	         e	 from the command line. this executes the in-
structions in     e, but leaves you at the gnuplot prompt to play with different

shopping for the best settings, you can put them into a script that will always
produce the perfect plot, or better still, write a c program to autogenerate a script
that will always produce the perfect plot.

   le plotted in this example. but    rst, here are a few words on the various means of
sending these plot samples to gnuplot.

the plot will disappear after a split second. if you are writing to a    le rather than

there are various ways by which you can get gnuplot to read commands from a

leaves you at the gnuplot prompt to experiment with settings.

preliminaries as with sqlite or mysql, there is a command-line interpreter

settings.

159

graphics

latex docs

scalable vector graphics

gsl_stats march 24, 2009

table 5.1 some of the terminal types that gnuplot supports.

output goal
display on screen
display on screen
display on screen
browser
browser, word processor
browser, word processor

 e  e  ... meaning
x11 window system; most posix oses
wi d w  window system; windows oses
a 	a window system; mac os x
  g portable network graphics
gif graphics interchange format
 vg
   postscript or encapsulated postscript pdf
 a ex latex graphics sub-language
if you are at a gnuplot prompt, you can exit via either theexi  command or <ctrl-
check your gnuplot installation. write a one-line text    le named     e
whose text reads:     i  x . execute the script using one of the above
 e  e   and e  	  gnuplot defaults to putting plots on the screen, which is
the e  e  i a  command dictates the language with which to write the out-
scribes the deatils regarding each of them; e.g.,he   e  e   a ex orhe  
 e  e       
 i  . the default is typically the on-screen format appropriate
the e  	  command provides a    le name to write to. for example, if a gnuplot

useful for looking at data, but is not necessarily useful for
communicating with peers. still worse, some systems are not even capable of
screen output. there are many potential solutions to the problem.

methods. once that works, try the national debt example above.
if your system is unable to display plots to the screen, read on for alternative
output formats.

d>. on many systems, you can also interact with the plot, spinning 3-d plots with
the mouse or zooming in to selected subregions of 2-d plots.

put. table 5.1 presents the more common options, and gnuplot   s help system de-

for your system.2

q5.1

   le has

set term postscript color
set out    a_plot.eps   

2the popular jpg format is not listed because its compression is designed to work well with photographic

images, and makes lines and text look fuzzy. use it for plots and graphs only as a last resort.

gsl_stats march 24, 2009

160

chapter 5

comments

from a web browser. or, you could

can open via your familiar pdf viewer.

then these lines are ignored, and gnuplot will display
plots on the screen as usual. if you later decide to print
the plot, you can delete the #s and the script will write

gnuplot follows the commenting standards of many
scripting languages: everything on a line after a # is
ignored. for example, if a script begins with

at its head, then the script will not display to the screen and will instead write the
designated    le, in postscript format.

now that you know how to run a gnuplot script and view its output, we can move
on to what to put in the script.

you can rest assured that no mat-
ter where you are, there is some
way to view graphics, but it may
take some experimentation to    nd
out how. for example, if you are
dialing in to a remote server, you
may be able to copy the graphics to

# e  e  i a      
 i  
    
# e  	    i   e.e   
a 	b i
_h    directory, make the
   le publicly readable (
h  d644
to  i   e.e  .
    .  g) and then view the plot
produce postscript output, and then run  2 df to produe a pdf    le, which you
    
the     command will set the basic shape of the plot.
da a	deb     le in the online code supplement, simply use     da a	deb  .
for example, the    rst portion of theda a	deb     le includes three columns: the
use     da afi e 	 i g1:3. this produces a scatterplot with x values
say that you just want to see a single data series, maybe column three; then    
 da afi e 	 i g3. with only one column given, the plot will assume that the
 e     you will often want multiple data sets on the same plot, and gnuplot does
this easily using the e     command. just change every use of     after
the    rst to e    . page 170 presents a few more notes on using this function.

from column one and y values from column 3. notice that gnuplot uses index
numbering instead of offset numbering: the    rst column is one, not zero.

to plot a basic scatterplot of the    rst two columns of data in a    le, such as the

year, the debt, and the de   cit. to plot only the    rst and third columns of data,

you will often dump more columns of data to your data   le than are necessary.

x values are the ordinal series 1, 2, 3, . . . and your data are the y values.

5.1

gsl_stats march 24, 2009

graphics

161

set xrange [   4:6]
plot sin(x)
replot cos(x)
replot log(x) + 2*x     0.5*x**2

math-package notation for x2.

of the above applies directly, but with three dimensions. if your data set has

    gnuplot knows all of the functions in the standard c math library. for example, the

    gnuplot always understands the variable x to refer to the    rst axis and y to the
second. if you are doing a parametric plot (see the example in figure 11.4, page
360), then you will be using the variables t, u, and v.

above sequence will produce a set of pleasing curves. notice thatx  2 is common
      the     command prints    at, 2-d plots. to plot 3-d surfaces, use     . all
three columns, then you can plot it with      da afi e . if your data set has
more than three columns, then specify the three you want with a form like     
 da afi e 	 i g1:5:4.
at the northeast corner. to plot such data, use      da afi e  a  ix.
    surface plotting goes hand-in-hand with the  3d (palette-mapped 3-d) option,
in a slightly-modi   ed form inage  g id.g 	     in the code supplement. the

that produces a pleasing color-gradient surface. for example, here is a simple ex-
ample that produces the sort of plot used in advertisements for math programs.
again, if you run this from the gnuplot prompt and a system that supports it, you
should be able to use the mouse to spin the plot.

    there is also the crosstab-like case where the data   s row and column dimensions
represent the x and y axes directly: the (1, 1)st element in the data    le is the
height at the southwest corner of the plot, and the (n, n)th element is the height

here is a more extended script, used to produce figure 5.2. [this example appears

set pm3d
splot sin(x) * cos(y) with pm3d

simulation that produced it is available upon request.]

1
2
3
4
5

set term postscript color;
set out    plot.eps   ;
set pm3d; #for the contour map, use set pm3d map;
unset colorbox
set xlabel    percent acting   ; set ylabel    value of emulation (n)   ;

gsl_stats march 24, 2009

chapter 5

162

 120

 100

 80

 60

 40

 20

 0

1.2

1

0.8

0.6

0.4

0.2

)
n
(
 

n
o

i
t

l

a
u
m
e

 
f

o

 

e
u
a
v

l

0

0

0.25

0.5

0.75

1

percent acting

0

0.25

0.5

percent acting

0.75

1

0

1.2

1
n  ( n )

0.8

u l a ti o

0.6

e   o f  e

m

0.4
a l u

v

0.2

figure 5.2 two views of the same density plot from a few thousand simulations of a group of agents.
agents act iff ti + nki > c, where ti is a normally distributed private preference,
ki is the percentage of other people acting, n is the agent   s preference for emulating
others, and c is a uniformly-distributed cutoff. the vertical axis shows the density of
simulations with the given percent acting and value of n. when the value of emulation is
low, outcomes are unimodally distributed, but outcomes become bimodally distributed as
emulation becomes more valuable.

6
7
8
9

    line three tells gnuplot to prepare for a palette-mapped surface, and is necessary

    lines one and two send the output to a postscript    le instead of the screen. given

set palette gray;
set xtics (   0    0,   0.25    250,   0.5    500,   0.75    750,    1    999);
set ytics (   0    0,    0.2    1,    0.4    2,    0.6    3,    0.8    4,    1    5,    1.2    6)
splot    data   le    matrix with pm3d

line six, the
     modi   er is optional in this case.
before the      command on line nine.
black and white. there are many palettes to be had; the default, omitting the e 
 a e  e line entirely, works for most purposes.
    changing line three to e   3d a  produces an overhead view of the same

    lines seven and eight    x the axis labels, because gnuplot defaults to using the
index of the column or row as the label. the format requires a text label, followed
by the index at which the label will be placed; see below for further notes.

    line    ve sets the labels, and demonstrates that gnuplot commands may put each
either on a separate line or separated with a semicolon. as you can see from this
script, ending a line with a semicolon is optional.

    line six sets the color scheme to something appropriate for a book printed in

    line four deletes the legend, which in the pm3d case is known as the colorbox.

surface, sometimes known as a contour plot, as in the second plot of figure 5.2.

gsl_stats march 24, 2009

graphics

163

the short version

the sample lines above were all relatively brief, but you can put
a huge amount of information onto one line. for example,

set style data bars
set style function lines
set linetype 3
set xrange [   10:10]
set yrange [0:2]
plot    data    using 1:3 title    data   
replot sin(x) title    sine   

# can be rewritten as:
plot    data    using 1:3 [   10:10][0:2] with bars title    data   , sin(x) with lines linetype 3 title    sine   

tradeoff, and you are encouraged to stick with the clear version at    rst.

once you have everything on one line, you can abbreviate almost anything, such

# or as
plot    data    using 1:3 [   10:10][0:2] w bars title    data   , sin(x) w l lt 3 title    sine   

all of these settings will be discussed below. the purpose of this example is to
show that style information can be put above the plot command, or it can be mixed

in on the line de   ning the    . the e     command is also technically optional,
because you can add additional steps on one     line using a comma. finally,
as replacingwi h i e  withw . this is yet another minimalism-versus-clarity
5.2   some common settings
will need to put a few e  commands before the    nal    .
which can be accessed from inside the gnuplot command-line program viahe  ,
optionally followed by any of the headers below (e.g.,he   e   y e,he  
 e   i   y e).
a e     command (one word with no options) before the settings take effect.

at this point, you can produce a basic plot
of data or a function (or both at once). but
you may have in mind a different look from gnuplot   s default, which means you

finally, if you are interactively experimenting with settings   which you are en-
couraged to do while reading this section   bear in mind that you may have to give

this section catalogs the most common settings. for more information on the set-
tings here and many more, see the very comprehensive gnuplot documentation,

164

chapter 5

gsl_stats march 24, 2009

es with error bars, or many other possibilities. gnuplot keeps track of two

functions and data, you can easily plot data overlaid by a function in a different
style.

 e   y e the basic style of the plot may be a simple line or points, a bar plot, box-
types of style: that for function plotting ( e   y ef	 
 i  ) and for data plot-
ting ( e   y eda a). for example, to plot something that looks like a bar chart,
try e   y eda ab xe , followed on the next line with    y 	 da a.
as above, this is equivalent to the slightly shorter form    y 	 da awi h
b xe , but it is often useful to separate the style-setting from the plot content.
other favorite data styles include i e ,d   ,i  	  e  (lines from the x-axis to
the data level),  e   (a continuous line that takes no diagonals), i e   i    (a
line with the actual data point marked), ande    ba   (to be discussed below).
if you are plotting a function, like     i  x , then use e   y ef	 
 i  
 i e  (ord    ori  	  e , et cetera). because there are separate styles for
 e   i   y e  e  i e y e you can set the width and colors of your lines,
the  i   y e and i e y e commands, among a handful of other commands,
the e   command. e.g.:
 e  i  e  e x abe   e y abe  these simple commands label the x and
3as of this writing, gnuplot   s default for the    rst two plotted lines is to use i e y e1=red and i e y e
2=green. seven percent of males and 0.4% of females are red   green colorblind and therefore won   t be able to dis-
tinguish one line from the other. try, e.g.,     i  x ; e    
   x  i e y e3, to bypass i e y e
2=green, thus producing a red/blue plot.

y axes and the plot itself. if the plot is
going to be a    gure in a paper with a paper-side caption, then the title may be
optional, but there is rarely an excuse for omitting axis labels. sample usage:

may differ from on-screen to postscript to png to other formats, depending upon
what is easy in the different formats. you can see what each terminal can do via

among other things, the test page displays a numbered catalog of points and lines
available for the given terminal.

set terminal postscript
set out    testpage.ps   
test

and whether your points will display as balls, tri-

angles, boxes, stars, et cetera.3

gsl_stats march 24, 2009

165

graphics

positions (or just experiment).

on moving the box or changing its orientation.

sonably intelligent about it, but sometimes the legend gets in the way. your

set xlabel    time, days   
set ylabel    observed density, picograms/liter   
set title    density over time   

 e key gnuplot puts a legend on your plot by default. most of the time, it is rea-
   rst option in this case is to just turn off the key entirely, via	  e key.
the more moderate option is to move the key, using some combination of ef  
 igh , or 	  ide to set its horizontal position and    b     , orbe  w to
set its vertical; askhe   e key for details on the precise meaning of these
    the key also sometimes bene   ts from a border, via e keyb x.
    for surface plots with  3d, the key is a thermometer displaying the range of colors
and their values. to turn this off, use	  e 
    b x. seehe   e 
    b x
 e x a ge, e y a ge gnuplot generally does a good job of selecting a default
using e  a ge[ i : ax   .
   x the other end, in which case you can use a  to indicate the automatically-set
bound. for example, e y a ge[ :10       xes the top of the plot at ten, but lets
1 is best; then e y a ge[ :1    eve  e will put    rst place at the top of the
 e x i
  and e y i
  gnuplot has reasonably sensible defaults for how the
code from c, here is a simple routine to write a e y i
  line:

axes will be labeled, but you may also set the tick marks
directly. to do so, provide a full list in parens for the text and position of every last
label, such as on lines seven and eight of the code on page 161.

plot, and autoset the bottom of the plot to just past the largest ranking in the data
set.

    you may want the axes to go backward. say that your data represents rankings, so

    sometimes, you will want to leave one end of the range to be set by gnuplot, but

producing this by hand is annoying, but as a    rst indication of producing gnuplot

range for the plot, but you can manually override this

the lower end of the plot fall where it may.

gsl_stats march 24, 2009

166

chapter 5

static void deal_with_y_tics(file *f, double min, double max, double step){

int j = 0;

fprintf(f, "set ytics (");
for (double i=n_min; i< n_max; i+=n_step){

fprintf(f, "   %g    %i", i, j++);
if (i+n_step <n_max   1)

fprintf(f, ", ");

}
fprintf(f, ")\n");

}

assorted here are a few more settings that you may    nd handy.

5.3 from arrays to plots

unset border
unset grid
set size square
set format y "%.3g"
set format y ""
set zero 1e   20

#delete the border of the plot.
#make the plot even more minimalist.
#set all axes to have equal length on screen or paper.
#you can use printf strings to format axis labels.
#or just turn off printing on the y axis entirely.
#set limit at which a point is rounded to zero (default: 1e   8).

to read the data (     da a	deb  ). alter-
natively,     	  tells gnuplot to plot data to be placed immediately after the
     command. with the 	  trick, the process of turning a matrix into a basic
letda a be ana   _da a set whose    rst and    fth columns we
put a     	  command in the    rst line (perhaps preceded by a series of static
 e  commands), and then    ll the remainder with the data to be plotted. below

plot is trivial. in fact, the principle is so simple that there are several ways of im-
plementing it.

would like to plot against each other. then we need to create a    le,

the scripts above gave a    le name from which

is the basic code to create a gnuplot    le. since virtually anything you do with
gnuplot will be a variant of this code, it will be dissected in detail.

1 file *f = fopen("plot_me", "w");
2
3
4
5
6

if (!f) exit(0);
fprintf(f, "set key off; set ylabel    picograms/liter   \n set xrange [   10:10]\n");
fprintf(f, "plot           using 1:5 title    columns one and    ve   \n");
fclose(f);
apop_matrix_print(data   >matrix, "plot_me");

write to a    le

167

graphics

gsl_stats march 24, 2009

you will need to investigate.

    you can separate gnuplot commands with a semicolon or newline, and can put

    the    rst argument tof  e  is the    le to be written to, and the second option
should be either"a" for append or"w" for write anew; in this case, we want to
start with a clean    le so we use"w".
    thef  e  function can easily fail, for reasons including a mistyped directory
name, no permissions, or a full disk. thus, you are encouraged to check thatf  e 
worked after every use. if the    le handle is u  , then something went wrong and
    as you can see from lines three and four, the syntax forf  i  f is similar to
that of the rest of the  i  f family: the    rst argument indicates to what you
are writing, and the rest is a standard  i  f line, which may include the usual
insertions via g, i, and so on.
them in onef  i  f statement or many, as is convenient. however, you will need
to end each line of data (and the     line itself) with a newline.
    since gnuplot lets us select columns via the	 i g1:5 clause, there is no need
    the i  e clause shows that gnuplot accepts both    double quotes    and    single
   le. it prints the matrix instead of the fulla   _da a structure so that no names
or labels are written. sincea   _ a  ix_  i   defaults to appending, the matrix
appears after the     header that lines two and three wrote to the    le. you would
need to seta   _    . 	  	 _a  e d=0 to overwrite.
    at the end of this,    _ e will be executable by gnuplot, using the forms like
g 	    	 e  i  <    _ e, as above.
the command   e  does two things: it runs the speci   ed program, and it pro-

the above method involved writing your commands and data
to a    le and then running gnuplot, but you may want to pro-
duce plots as your program runs. this is often useful for simulations, to give you a
hint that all is ok while the program runs, and to impress your friends and funders.
this is easy to do using a pipe, so named because of unix   s running data-as-water
metaphor; see appendix b for a full exposition.

quotes    around text such as    le names or labels. single quotes are nothing special
to c, so this makes it much easier to enter such text.

to pare down the data set in memory. notice again that the    rst column in gnuplot
is 1, not 0.

    line    ve closes the    le, so there is no confusion when line six writes the data to the

instant grati   cation

duces a data pipe that sends a stream of data produced by your program to the
now-running child program. any commands you write to the pipe are read by the
child as if someone had typed those commands into the program directly.

listing 5.3 presents a sample function to open and write to a pipe.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

gsl_stats march 24, 2009

168

#include <apop.h>

void plot_matrix_now(gsl_matrix *data){

static file *gp = null;

if (!gp)

gp = popen("gnuplot    persist", "w");

if (!gp){

printf("couldn   t open gnuplot.\n");
return;

chapter 5

}
fprintf(gp,"reset; plot           \n");
apop_opts.output_type =    p   ;
apop_opts.output_pipe = gp;
apop_matrix_print(data, null);
f   ush(gp);

}

}

int main(){

indicate that you will be writing to gnuplot rather than reading from it. most sys-

apop_db_open("data   climate.db");
plot_matrix_now(apop_query_to_matrix("select (year*12+month)/12., temp from temp"));

listing 5.3 a function to open a pipe to gnuplot and plot a vector. online source: i e    .
.
    the   e  function takes in the location of the gnuplot executable, and aw to
tems will accept the simple program name,g 	    , and will search the program
path for its location (see appendix a on paths). ifg 	     is not on the path,
then you will need to give an explicit location like/	  /  
a /bi /g 	    . in
whi
hg 	    . the   e  function then returns af  e , here assigned tog .
    sinceg  was declared to be a static variable, and   e  is called only when
g == u  , it will persist through multiple calls of this function, and you can re-
ifg  is u   after the call to   e , then something went wrong. this is worth
familiar process of writing     	  and a matrix to a    le. the e e  command
    lines 12 and 13 set the output type to    and the output pipe tog ;a   _	
 a  ix_  i   uses these global variables to know that it should write to that pipe
instead of a    le or  d 	 .

to gnuplot (line 11) ensures that next time you call the function, the new plot will
not have any strange interactions with the last plot.

    but if the pipe was created properly, then the function continues with the now-

this case, you can    nd where gnuplot lives on your machine using the command

peatedly call the function to produce new plots in the same window.

checking for every time a pipe is created.

    notice the resemblance between the form here and the form used to write to a    le

graphics

gsl_stats march 24, 2009

the program writes to either in exactly the same manner. they are even both closed

above. af  e pointer can point to either a program expecting input or a    le, and
usingf
   e. the only difference is that a    le opens viaf  e  and a program
opens with   e . thus, although the option is nameda   _    . 	  	 _	
 i e, it could just as easily point to a    le; e.g.,f  e f=f  e       e "w" ;
a   _    . 	  	 _ i e=f;.
the buffer worth writing. the function on line 15,ff 	 h, tells the system to send
all elements of theg  buffer down the pipeline. the function also works when you
are expecting standard output to the screen, by the way, viaff 	 h   d 	  .4

    one    nal detail: piped output is often kept in a buffer, a space in memory that the
system promises to eventually write to the    le or the other end of the pipe. this
improves performance, but you want your plot now, not when the system deems

169

the main drawback to producing real-time plots is that they can take over your
computer, as another plot pops up and grabs focus every half second, and can
signi   cantly slow down the program. thus, you may want to settle for occasional
redisplays, such as every    fty periods of your simulation, via a form like

}

plot_vector_now(output);

now would be a good time to plot some data series.

for (int period =0; period< 1000; period++){
gsl_vector *output = run_simulation();
if (!(period % 50))

year in the    rst column and the mean number of tattoos for respon-
dents in that birth year in the second.

    query a two-column table fromda a	 a    .db giving the birth
    dump the data to a gnuplottable    le using the abovef  i  f tech-
    plot the    le, then add e  commands that youf  i  f to    le to
4alternatively,ff 	 h  u    will    ush all buffers at once. for programs like the ones in this book, where
there are only a handful of streams open, it doesn   t hurt to just useff 	 h  u    in all cases.

    modify your program to display the plot immediately using pipes.
how could you have written the program initially to minimize the
effort required to switch between writing to a    le and a pipe?

    how does the graph look when you restrict your query to include only

produce a nicer-looking plot. e.g., try boxes and impulses.

those with a nonzero number of tattoos?

niques.

q5.2

170

gsl_stats march 24, 2009

 self-executing    les
script to plot sin(x). you could even use y  e  "./     e"  to have the script

those experienced with posix systems know that a script can be made executable
by itself. the    rst line of the script must begin with the special marker #! fol-
lowed by the interpreter that will read the    le, and the script must be given execute
permissions. listing 5.4 shows a program that produces a self-executing gnuplot

this chapter is about the many possible paths from a data
set to a script that can be executed by an external program;

here   s one more.

chapter 5

execute at the end of this program.

#include <apop.h>
#include <sys/stat.h> //chmod

int main(){

plot sin(x)");

char    lename[] = "plot_me";
file *f = fopen(   lename, "w");
fprintf(f, "#!/usr/bin/gnuplot    persist\n\

./    _ e from the command line. online source: e fexe
	 e.
.
 e     and the     	  mechanism are incompatible, be-

  e     revisited

listing 5.4 write to a    le that becomes a self-executing gnuplot script. run the script using

fclose(f);
chmod(   lename, 0755);

}

cause gnuplot needs to re-read the data upon replotting, but
can not back up in the stream to reread it. instead, when replotting data sets, write
the data to a separate    le and then refer to that    le in the    le to be read by gnuplot.
to clarify, here is a c snippet that writes to a    le and then asks gnuplot to read the
   le:

apop_data_print(data, "data   le");
file *f = fopen("gnuplot", "w");
fprintf(f, "plot    data   le    using 1 title    data column 1   ;\n \

replot    data   le    using 5 title    data column 5   ;\n");

fclose(f);

also, when outputting to a paper device, replotting tends to make a mess. set the
output terminal just before the    nal replot:

plot    data   le    using 1
replot    data   le    using 2
replot    data   le    using 3
set term postscript color
set out    four_lines.eps   
replot    data   le    using 4

5.4 a sampling of special plots

171

graphics

gsl_stats march 24, 2009

command    le immediately after the command. you can use this and

   a gnuplot command    le basically consists of a group of e  com-
mands to specify how the plot will look, and then a single     (2-d
version) or      (3-d version) command.
   you can run external text-driven programs from within c usingf  e 
and   e .
   using the     	  form, you can put the data to be plotted in the
a   _da a_  i   to produce plot    les of your data.
a e  and a     command, gnuplot
use thea   _    _ a  i
e function.
 a  i
e.
.
listing 5.5 produces a    le that (viag 	    	 e  i  < 	 ) produces the plot

is surprisingly versatile. here are some specialized visualizations that go well be-
yond the basic 2-d plot.

perhaps plotting two pairs of columns at a time is not suf   cient   you
want bulk, displaying every variable plotted against every other. for this,

listing 5.5 the code to produce figure 5.6. link with the code in listing 8.2, p 267. online source:

for a system that basically only has

apop_plot_lattice(query_data(), "out");

#include "eigenbox.h"

int main(){

}

in figure 5.6. yes, it is one line of code, but you will need to link it with the data-
querying code in listing 8.2, p 267. each variable is plotted against every other;
e.g., the upper-middle plot shows males per 100 females versus state population,
and the middle-left plot shows a mirror image (along the diagonal line) of the same
plot.

what is a lattice plot good for? some call it getting a lay of the land, while others
call it data snooping. given ten perfectly random variables, there is a good chance
that at least one pair of lattice plots will look to you as if it demonstrates a nice
correlation. a formal regression on the chosen pair of variables will likely verify
your initial visual impression. for more on this con   ict, see page 316.

lattices

gsl_stats march 24, 2009

172

chapter 5

m_per_100_f

population

median_age

figure 5.6 the census data, as queried on page 267.

error bars the typical error bar has three parts: a center, a top limit, and a bot-

tom limit. gnuplot supports this type of data directly, via e   y e
da ae    ba  . you can provide the necessary information in a variety of for-
   nal histogram to gnuplot. fortunately,a   _    _hi   g a  does the binning

mats; the most common are (x, y center, y top, y bottom) and (x, y center, y range),
where the range is typically a standard deviation. listing 5.7, which produces fig-
ure 5.8, takes the second approach, querying out a month, the mean temperature
for the month, and the standard deviation of temperature for the month. plotting
the data shows both the typical annual cycle of temperatures and the regular    uc-
tuation of variances of temperature.

histograms a histogram is a set of x- and y-values like any other, so plotting it
requires no special tools. however, gnuplot will not take a list of data
and form a histogram for you   you have to do this on the c-side and then send the

for you. have a look at listing 11.2, page 359, for an example of turning a list of
data items into a histogram (shown in figure 11.3).

gsl_stats march 24, 2009

graphics

#include <apop.h>

173

int main(){

apop_data *d = apop_query_to_data("select \

apop_db_open("data   climate.db");

printf("set xrange[0:13]; plot           with errorbars\n");
apop_matrix_show(d   >matrix);

(yearmonth/100.     round(yearmonth/100.))*100 as month, \
avg(tmp), stddev(tmp) \
from precip group by month");

to stdout, so pipe the output through gnuplot:./e    ba  |g 	    	 e  i  .
online source:e    ba  .
.

listing 5.7 query out the month, average, and variance, and plot the data using errorbars. prints

}

f
   

,
p
m
e
t

80

75

70

65

60

55

50

45

40

35

30

25

monthly temp     

+

+ +

+

+

+

+

+

+

+

+

+

+

0

1

2

3

4

5

6

7

8

9

10

11

12

13

month

figure 5.8 monthly temperature,     .

check this against a data set of your choosing, such as the   	 a i   col-
umn in the world bank data, or thet  a _a ea column from the us cen-

the leading digit of a number is simply its most signi   cant digit: the lead-
ing digit of 3247.8 is 3, and the leading digit of 0.098 is 9. benford   s law
(benford, 1938) states that the digit d     {0, 1, . . . , 9} will be a leading digit
with frequency
(5.4.1)

fd     ln ((d + 1)/d) .

sus data. the formal test is the exercise on page 322; since this is the chapter
on plotting, just produce a histogram of the chosen data set and verify that it
slopes sharply downward. (hint: if d = 3.2e7, then 10(int)log10(d) = 1e7.)

q5.3

gsl_stats march 24, 2009

174

r
a
e
y

h
t
r
i

b

1990

1985

1980

1975

1970

1965

1960

1955

1950

0

b
b
b

b
b

b

b

b
b
b
b

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b

b

b
b
b
b
b
b
b
b
b
b

b
b
b
b
b

1

b
b

b
b

b
b
b
b
b
b

b

b
b

b

b

b

b

b
b
b

2

b

b
b

b
b

b

b
b
b

b

b
b

b

b

3

chapter 5

b
b

b

b

b

b

b

b
b
b

b

b
b
b

b

b

b

b

b

4

5

6

7

8

9

tattoos

log plots you can plot your data on a log scale by either transforming it before it

figure 5.9 each point represents a person.

gets to gnuplot or by transforming the plot.

log plots work best on a log-base-10 scale, rather than the typical natural loga-
rithm, because readers can immediately convert a 2 on the scale to 1e2, a    4 to

1e   4, et cetera. from c, you can use the  g10 x  function to calculate log10x,
and if your data is in ag  _ve
   , you can usea   _ve
   _  g10 to trans-
in gnuplot, simply e   g 
a ey to use a log-scaled y axis, e   g 
a e
x to use a log-scaled x axis, or e   g 
a exy for both.
redo theda a	deb  plot from the beginning of this chapter using a log

form the entire vector at once.

q5.4

scale.

pruning and jittering plotting the entire data set may be detrimental for a few
reasons. one is the range problem: there is always that
one data point at y = 1e20 throws off the whole darn plot. if you are using an
interactive on-screen plot, you can select a smaller region, but it would be better to
just not plot that point to begin with.

the second reason for pruning is that the data set may be too large for a single

gsl_stats march 24, 2009

175

graphics

page. the black blob you get from plotting ten million data points on a single
piece of paper is not very informative. in this case, you want to use only a random
subset of the data.5

sql. a simple e e
  f        ewhe eva 	e<1e7 will eliminate val-
in gnuplot, you can add theeve y keyword to a plot, such as     da a 
eve y5 to plot every    fth data point. this is quick and easy, but take care that

ues greater than a million, and page 84 showed how to select a random subset of
the data.

both of these are problems of selecting data, and so they are easy to handle via

there are no every-   ve patterns in the data.

now consider graphing the number of tattoos a person has against her year of
birth. because both of these are discrete values, we can expect that many people
will share the same year of birth and the same tattoo count, meaning that the plot of
those people would be exactly one point. a point at (1965, 1 tattoo) could represent
one person or    fty.

#include <apop.h>
gsl_rng *r;

void jitter(double *in){

*in += (gsl_rng_uniform(r)     0.5)/10;

}

int main(){

apop_db_open("data   tattoo.db");
gsl_matrix *m = apop_query_to_matrix("select \

tattoos.   ct tattoos ever had    ct, \
tattoos.   year of birth   +1900 yr \
from tattoos \
where yr < 1997 and ct+0.0 < 10");

}

apop_matrix_show(m);

set ylabel    birth year   ; \n\
plot           pointtype 6\n");

r = apop_rng_alloc(0);
apop_matrix_apply_all(m, jitter);
printf(set key off; set xlabel    tattoos   ; \n\

ji  e .
.
for the typical terminal, do this with    ...  i   y e0.

listing 5.10 by adding a bit of noise to each data point, the plot reveals more data. online source:

5another strategy for getting less ink on the page is to change the point type from the default cross to a dot.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

gsl_stats march 24, 2009

176

chapter 5

further, the process of jittering is rather simple. listing 5.10 shows the code used
to produce the plot. lines 10   14 are a simple query; lines 17   20 produce a gnuplot

element of the matrix. that function is on lines 4   6, and simply adds a random
number     [   0.05, 0.05] to its input. comment out line 16 to see the plot without
jitter.

one solution is to add a small amount of noise to every observation, so that two
points will fall exactly on top of each other with id203 near zero. figure 5.9
shows such a plot. without jittering, there would be exactly one point on the one-
tattoo column for every year from about 1955 to 1985; with jittering, it is evident
that there are generally more people with one tattoo born from 1965   1975 than in
earlier or later years, and that more than half of the sample with two tattoos was
born after 1975.

header and    le that get printed to  d 	  (so run the program as./ji  e |
g 	    ). in between these blocks, line 16 applies theji  e  function to every
sql query instead of in theg  _ a  ix.
against every other, viaa   _    _ a  i
e.
   with gnuplot   s e   y ee    ba   command, you can plot a
data in c via  g10, plot the log in gnuplot via e   g 
a ey,
or trim the data in sql via, e.g., e e
 
   f    ab ewhe e
va <1e20.

   you need to aggregate data into histograms outside of gnuplot, but
once that is done, plotting them is as easy as with any other data set.

modify the code in listing 5.10 to do the jittering of the data points in the

   if the data set has an exceptional range, you can take the log of the

   apophenia will construct a grid of plots, plotting every variable

range or a one-   spread for each data point.

q5.5

   if data falls on a grid (e.g., integer-valued rows and columns), then

you can add jitter to the plot to reveal the density at each point.

5.5 animation

gsl_stats march 24, 2009

graphics

trices one after the next and call     in between. however, many media (such
plot reads ane alone on a line to indicate the end of a data set. second, gnuplot
allows you to de   ne constants via a simple equals sign; e.g., the command =
0.6 creates the variable  and sets it to 0.6. third, gnuplot has a a	 e  com-

as paper) do not yet support animation, meaning that your output will generally
be dependent on your display method. the gif format provides animation and is
supported by all major web browsers, so you can also put your movies online.6

perhaps three dimensions is not quite enough, and you need
one more. gnuplot easily supports animation: just stack ma-

there are two details that will help you with plotting multiple data sets. first, gnu-

177

mand that will wait p seconds before drawing the next plot.

tying it all together, we want a gnuplottable    le that looks something like this:

set pm3d;

p = 1;

splot           with pm3d
{data[0] here}
e

pause p;
splot           with pm3d
{data[1] here}
e

pause p;
splot           with pm3d
{data[2] here}
e

you can run the resulting output    le through the gnuplot command line as usual.
if a one-second pause is too long or too short, you only need to change the single

value of  at the head of the    le to change the delay throughout.
viewer, or perhaps print them into a    ipbook. when writing a    le, set =0 in the
6for gifs, you will need to request animation in the e  e   line, e.g., e  e  gifa i a ede ay
100. the number at the end is the pause between frames in hundredths of a second.

you can write the above plots to a paper-oriented output format, in which case each
plot will be on a separate page. you could then page through them with a screen

in addition to the example below, listings 7.12 (page 253) and 9.1 (page 298) also
demonstrate the production of animations.

above, since there is no use delaying between outputs.

gsl_stats march 24, 2009

178

chapter 5

figure 5.11 the game of life: at left is the original colony; at right is the colony after 150 periods.
the vaguely v-shaped    gures (such as the groups farthest left, right, and top) are known
as gliders, because they travel one step forward every four periods.

an example: the game of life

these simple rules produce complex and interesting patterns. at left in figure 5.11
is the so-called r pentomino, a simple con   guration of    ve blobs. at right is the
outcome after 150 periods of life. listing 5.12 presents the code to run the game.

there is a tradition of agent-based modeling built
around plotting agents on a grid, pioneered by ep-
stein & axtell (1996). a simple predecessor is conway   s game of life, a cellular
automaton discussed at length in gardner (1983). the game is played on a grid,
where each point on the grid can host a single blob. these blobs can not move, and
are somewhat delicate: if the blob has only zero or one neighbors, it dies of loneli-
ness, and if it has four or more neighbors, it dies of overcrowding. if an empty cell
is surrounded by exactly three blobs, then a new blob is born on that cell.

because the program prints gnuplot commands to  d 	 , run it using./ ife|
g 	    .
    the game uses two grids: the completed grid from the last period, nameda
 ive
next period, namedi a
 ive.
    the main work each period is preparing the inactive grid, which is what the
a 
_	
g id function does. it sets everything to zero, and then the two loops (i-indexed
for rows andj-indexed for columns) checks every point in the grid except the
    thea ea_    function calculates the population in a 3  3 space; line 16 needs

    line 32 is the gnuplot header, and line 34 tells gnuplot that data points are coming.

    both grids are initialized to zero (lines 27   28), and then lines 29   31 de   ne the r

in the code, and the incomplete grid that will soon represent the state of life in the

pentomino.

borders.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41

gsl_stats march 24, 2009

graphics

179

#include <apop.h>
int area_pop(gsl_matrix *a, int row, int col){

int i, j, out = 0;

for (i=row   1; i<= row+1; i++)

for (j=col   1; j<= col+1; j++)

out += gsl_matrix_get(a, i, j);

return out;

}

void calc_grid(gsl_matrix* active, gsl_matrix* inactive, int size){

int i, j, s, live;

gsl_matrix_set_all(inactive, 0);
for(i=1; i< size   1; i++)

for(j=1; j< size   1; j++){

live = gsl_matrix_get(active, i, j);
s = area_pop(active, i, j)     live;
if ((live && (s == 2 || s == 3))

|| (!live && s == 3)){

gsl_matrix_set(inactive, i, j, 1);
printf("%i %i\n", i, j);

}

}

}

int main(){

int i, gridsize=100, periods = 550;
gsl_matrix *t, *active = gsl_matrix_calloc(gridsize,gridsize);
gsl_matrix *inactive = gsl_matrix_calloc(gridsize,gridsize);

gsl_matrix_set(active, 50, 50, 1); gsl_matrix_set(active, 49, 51, 1);
gsl_matrix_set(active, 49, 50, 1); gsl_matrix_set(active, 51, 50, 1);
gsl_matrix_set(active, 50, 49, 1);
printf("set xrange [1:%i]\n set yrange [1:%i]\n", gridsize, gridsize);
for (i=0; i < periods; i++){

}

}

printf("plot           with points pointtype 6\n");
calc_grid(active, inactive, gridsize);
t = inactive;
inactive = active;
active = t;
printf("e\n pause .02\n");

listing 5.12 conway   s game of life. online source: ife.
.

    notice that the loops in botha ea_    and
a 
_g id never consider the edges
of the grids. that means thata ea_    does not need to concern itself with edge

to subtract the population (if any) at the central point to get the population in the
point   s neighborhood.

gsl_stats march 24, 2009

conditions likeif i!=0 ....
    the rules of the game of life are summarized in theif statement in lines 17   18.
    if theif statement    nds that there will be a blob in this space next period, it prints

there will be a blob at this point if either there is currently a blob and it has two
or three neighbors, or there is no living blob there but there are three neighbors.

chapter 5

180

a point to gnuplot, and marks it in the currently inactive grid.

    lines 36   38 is the classic swap of the active and inactive grids, using a temp
location to help make the exchange. since we are only shunting the addresses of
data, the operation takes zero time.

q5.6

other rules for life or death also produce interesting results. for example,
rewrite the rules so that a living blob stays alive only if there are 2, 3, 4, or
5 neighbors, and an empty space has a birth only if there is one neighbor.
or try staying-alive rules of 2 and 6 with birth rules of 1 and 3.

5.6 on producing good plots

cleveland & mcgill (1985) offer some
suggestions for producing plots for the
purpose of perceiving the patterns among the static. their experiments were aimed
at how well people could compare data presented in various formats, and arrived
at an ordering of graphical elements from those that were most likely to allow
accurate perception to layouts that inhibited accurate perception:

i) position along a common scale (e.g., the height of the means in the temper-

ature plot on page 173)

ii) position on identical but nonaligned scales (such as comparing points on

two separate graphs)

iii) length (e.g., the height of the error bars in the temperature plot on page 173)
iv) angle
v) slope (when not too close to vertical or horizontal)
vi) area
vii) volume, density, color saturation (e.g., a continuous scale from light blue

to dark blue)

viii) color hue (e.g., a continuous scale from red to blue)

data should be presented using techniques as high up on the scale as possible.
pie charts (representing data via angle and area) are a bad idea because there are
many better ways to present the same data. gnuplot does provide a means of set-
ting data-dependent color, but given that color is at the bottom of cleveland and

gsl_stats march 24, 2009

graphics

181

mcgill   s list, it should be used as a last resort. they did not run experiments with
animation, but our eyes have cells exclusively dedicated to sensing motion, so it
seems sensible that if movement were included on the list, it would rank highly.

as for angles and slopes, consider
these plots of the us national debt
since 1995. the top plot has a y
axis starting at $0, while the sec-
ond has a y axis starting at $4.5
trillion. is the rate of change from
1995   1996 larger or smaller than
the rate of change from 2005   2006?
was growth constant or decelerat-
ing from 1995   2000? it
is dif   -
cult to answer these questions from
the top graph, because everything is
somewhat    at   the change in an-
gles is too small to be perceived,
and we are bad at discerning slopes.
differences in slope on the second
scale are more visible. the lesson is
that plots show their patterns most
clearly when the axes are set such
that the slope is around 45   .

us national debt

t
b
e
d

l
a
n
o
i
t
a
n

t
b
e
d

l
a
n
o
i
t
a
n

9e+12
8e+12
7e+12
6e+12
5e+12
4e+12
3e+12
2e+12
1e+12
0

4
9
9
1

9e+12
8.5e+12
8e+12
7.5e+12
7e+12
6.5e+12
6e+12
5.5e+12
5e+12
4.5e+12

4
9
9
1

6
9
9
1

8
9
9
1

0
0
0
2

2
0
0
2

4
0
0
2

6
0
0
2

6
9
9
1

8
9
9
1

0
0
0
2

2
0
0
2

4
0
0
2

6
0
0
2

6e+11

5e+11

4e+11

3e+11

t
i
c
   
e
d

the bottom plot is the us national
de   cit. this is the amount the gov-
ernment spends above its income,
and is thus the rate of change of the
debt. the rate of change in the    rst
two graphs (a slope in those graphs)
is the height of this plot for each
year, and position along a common
scale is number one on cleveland
and mcgill   s list. the answers to
the above questions are now obvi-
ous: the rate of change for the debt slowed until 2000 and then quickly rose to
about double 1995 rates.

[online source:deb .g 	    .]

0
0
0
2

2
0
0
2

4
0
0
2

4
9
9
1

6
9
9
1

8
9
9
1

l
a
u
n
n
a

1e+11

2e+11

0

6
0
0
2

darrell huff, in the classic how to lie with statistics (huff & geis, 1954, chapter
5), has a different goal and so makes different recommendations. he points out that
the top two graphs tell a different narrative. the second graph tells a story that the
national debt is rapidly increasing, because the height of the point at 2006 is about
eight times the height at 1995. the top graph shows that the debt rose, but not at a

gsl_stats march 24, 2009

182

chapter 5

and flowcharts

aim only to group nodes via the links connecting them.

fast-multiplying rate. huff concludes that the full story is best told when the zero
of the y axis always in the picture, even if this means blank space on the page   
advice that directly contradicts the advice above. so the choice for any given plot
depends on the context and intent, although some rules   like avoiding pie charts
and continuous color scales   are valid for almost all situations.

in common conversation, we typically mean the word
graph to be a plot of data like every diagram to this
point in the chapter. the mathematician   s de   nition
of graph, however, is a set of nodes connected by edges, as in figure 5.13. gnuplot
can only plot; if you have network data that you would like to visualize, graphviz
is the package to use. like all of the tools in this book, graphviz installs itself
gracefully via your package manager.

5.7   graphs   nodes
the package includes various executables, the most notable of which ared   and
 ea  . both take the same input    les, butd   produces a    owchart where there
is a de   nite beginning and end, while ea   produces more amorphous plots that
graph in the    gure using the following input tod  :
the lines with= in them set parameters, stating that the graph should read left-
instead of the default ellipses. the line with the	>s de   nes how the nodes should

the programs are similar to gnuplot in that they take in a plain text description of
the nodes and edges, and produce an output    le in any of a plethora of graphics
formats. the syntax for the input    les is entirely different from gnuplot   s, but the
concept is familiar: there are elements to describe settings interspersed with data
elements.

rankdir = lr;
node [shape=box];
"data"    > "estimation"    > "parameters";

to-right instead of the top-to-bottom default, and that the nodes should be boxes

for example,    ip back to page 3 and have a look at figure 1.3. produce the    rst

digraph {

}

link, and already looks like a text version of figure 1.3. the command line

dot    tpng <graphdata.dot > output.png

gsl_stats march 24, 2009

graphics

183

figure 5.13 the social network of a junior high classroom.

void produce_network_graph(apop_data *link_data, char *out   le){

produces a graph much like that in figure 1.3, in the png format.7

at this point, you have all the tools you need to autogenerate a graph. for example,
say that you have an n    n grid where a one in position (i, j) indicates a link

between agents i and j and a zero indicates no link. then a simplef   loop would
convert this data into a ea  -plottable    le, with a series of rows with a form like,
e.g.,  de32	>  de12.
in the code supplement, you will    nd a    le namedda a	
 a      , which lists
7the second half of figure 1.3 was produced using exactly the same graph, plus the  f ag package to replace

the survey results from a junior high classroom in la, in which students listed

fprintf(g, "digraph{\n");
for (int i=0; i< link_data   >matrix   >size1; i++)

for (int j=i+1; j< link_data   >matrix   >size2; j++)

fprintf(g, "node%i    > node%i;\n", i,j);

file *g = fopen(out   le, "w");

fprintf(g, "}\n");
fclose(g);

if (apop_data_get(link_data, i,j))

}

text like data with the ols-speci   c math shown in the    gure.

gsl_stats march 24, 2009

figure 5.13 graphs the classroom, using ea   and the following options for the

their    ve best friends. the ego column is the student writing down the name of his
or her best friend, and the nominee column gives the number of the best friend.

chapter 5

184

graph:

digraph{

node [label="",shape=circle,height=0.12,width=0.12];
edge [arrowhead=open,arrowsize=.4];

...

}

write a program to replicate figure 5.13.

a few patterns are immediately evident in the graph: at the right is a group of four
students who form a complete clique, but do not seem very interested in the rest
of the class. the student above the clique was absent the day of the survey and
thus is nominated as a friend but has no nominations him or herself; similarly for
one student at the lower left. most of the graph is made from two large clumps of
students who are closely linked, at the left and in the center, probably representing
the boys and the girls. there are two students who nominated themselves as best
friends (one at top right and one at the bottom), and those two students are not very
popular.

    read theda a	
 a          le into ana   _da a set, using either
a   _ ex _  _db and a query, ora   _ ex _  _da a.
    write a singlef   loop to write one line to the output    le for each
running the program will produce a ea  -formatted    le.
 ea  	t  < y_ 	  	 >
 	 .e  . the output graph should look just like figure 5.13.
    in thed   manual ( a d   from the command line), you will see
graph. what does the classroom look like viad  ,
i 
 , and w  i?

line in the data set. base your printing function on the one from page
183.

that there are many variant programs to produce different types of

    open your output    le, and copy in the header from above.

    close the    le.

    from the

command

line,

run

q5.7

the exercise on page 414 will show you another way to produce the
graphviz-readable output    le.

gsl_stats march 24, 2009

graphics

185

internal use

if you have been sticking to the philosophy of coding via small, simple
functions that call each other, your code will look like a set of elements
linked via function calls   exactly the sort of network for which graphviz was
written. if you feel that your code    les are getting a bit too complex, you can use
graphviz to get the big picture.

likeba e_ ab	>
hi d_ ab. then,d   can sort all those individual links into
there is a program to do this for you. ask your package manager ford xyge ,

for example, say that your database is growing involved, with queries that merge
tables into new tables, other queries to split the tables back into still more tables, et
cetera. for each query that creates a table, it is easy to write down a line (or lines)

you could also graph the calling relationships among the functions in your c
code   but before you start manually scanning your code, you should know that

a relatively coherent    ow from raw data to    nal output.8

which generates documentation via specially-formatted comments in the source
   le. if con   gured correctly, it will use graphviz to include call graphs in the doc-
umentation.

the online code supplement includes a few more examples of graphviz at work,
including the code used to create figures 1.1, 6.5, and 6.7.

   the graphviz package produces graphs from a list of nodes and

tions in your code or tables in your database.

edges. such lists are easy to autogenerate from c.

   you can also use graphviz to keep track of relationships among func-

5.8   printing and latex this book focuses on tools to write replicable,
thea   _da a structure   s i  e element). but you can set up a metadata table, with a column for the table
you need only add ani  e  i    e ada a... query above any query that generates a table. q: write a
were produced via e  e   a ex in gnuplot, to minimize complications with sending postscript to the press.

portable analyses, where every step is described
in a handful of human-legible text    les that are sent to programs that behave in the
same manner on almost any computer. the tex document preparation system (and
the set of macros built on top of it, latex) extend the pipeline to the    nal writeup.
for example, you can easily write a script to run an analysis and then regenerate
the    nal document using updated tables and plots.9

function to take in such a table and output a    owchart demonstrating the    ow of data through the database tables.
9to answer some questions you are probably wondering: yes, this book is a latex document. most of the plots

8sql does not have the sort of metadata other systems have for describing a table   s contents in detail (e.g.,

name, its description, and the tables that generated that table. such a table is reasonably easy to maintain, because

save for the pointer-and-box diagrams in the c chapter, student   s hand-drawn menagerie, and the snow   ake at

gsl_stats march 24, 2009

186

chapter 5

a complete tutorial on latex would be an entire book   which has already been
written dozens of times. but this chapter   s discussion of the pipeline from raw
data to output graphs is incomplete without mention of a few unpleasant details
regarding plots in latex.

you have two options when putting a plot in a texed paper: native latex and
postscript.

native format

just as you can set the output device to a screen or a postscript
printer, you can also send it to a    le written using latex   s graphics
sub-language. one the plus side, the fonts will be identical to those in your docu-
ment, and the resolution is that of tex itself (100 times    ner than the wavelength
of visible light). on the minus side, some features, such as color, are currently not
available.

producing a plot in latex format requires setting the same e  / 	  settings as
with any other type of output: e  e   a ex; e  	      . ex .10
just as you can dump one c    le into another via#i 
 	de, you can include the

i  	  command:

gnuplot output via the \

\documentclass{article}
\usepackage{latexsym}
\begin{document}
...
\begin{   gure}
\input out   le.tex
\caption{this    gure was autogenerated by a c program.}
\end{   gure}
...
\end{document}

another common complaint: the y -axis label isn   t rotated properly. the solution
provides a good example of how you can insert arbitrary latex code into your gnu-
plot code. first, in the gnuplot    le, you can set the label to any set of instructions
that latex can understand. let    be an arbitrary label; then the following command
will write the label and tell latex to rotate it appropriately:

10by the way, for a single plot, the e  	  command is optional, since you could also use a pipe:g 	    
<     e> 	 fi e. ex.

the snow   ake was generated by covering a triangle with a uniform-by-area distribution of dots, each with a
randomly selected color and size, and then rotating the triangle to form the    gure. therefore, any patterns you
see beyond the six-sided rotational symmetry are purely apophenia.

and in my experience helping others build data-to-publication pipelines, the detail discussed in this section

the head of every chapter, i made a point of producing the entire book using only the tools it discusses.

about rotating the y -axis label really is a common complaint.

gsl_stats march 24, 2009

187

graphics

set ylabel    \rotatebox{90}{your $\lambda$ here.}   

   a eb x is in theg a hi
x package,
native dvi format, the rotation won   t appear. use either df a ex ordvi   to

two    nal notes to complete the example:\
so it needs to be called in the document preamble:

second, many dvi viewers do not support rotation, so if you are viewing via tex   s

\usepackage{latexsym, graphicx}

view the output as it will print.

the postscript route which brings us to the second option for including a graphic:

postscript.

use the graphicx package to incorporate the plot. e.g.:

\documentclass{article}
\usepackage{graphicx}
\begin{document}
...
\begin{   gure}
\rotatebox{90}{\scalebox{.35}{\includegraphics{out   le.eps}}}}
\caption{this    gure was autogenerated by a c program.}
\end{   gure}
...
\end{document}

the    rst option for generating pdfs is to usee     df. first, convert all of your
e      les to df    les on the command line. inba h, try

notice that you will frequently need to rotate the plot 90    and scale the    gure down
to a reasonable size.

for i in *.eps; do
epstopdf $i;

done

then, in your latex header, add

\usepackage[pdftex]{eps   g}

188

chapter 5

gsl_stats march 24, 2009

out incident; the drawback is that you now have two versions of every    gure clut-
tering up your directory, and must regenerate the pdf version of the graphic every
time you regenerate the postscript version.

the bene   t to this method is that you can now run df a ex y_d 
	 e   with-
 ake program, which is discussed in detail in appendix a. listing 5.14 is a sample
just typing ake at the command line. the creative reader could readily combine
report every time the analysis or data are updated. (hint: add age _a   target that

make   le for producing a pdf document via the postscript route. as with your c
programs, once the make   le is in place, you can generate    nal pdf documents by

latex a_report
dvips < a_report.dvi > a_report.ps
ps2pdf a_report.ps

this make   le with the sample c make   le from page 387 to regenerate the    nal

either method is a lot of typing, but there is a way to automate the process: the

the alternative is to go through postscript in generating the document:

depends on the other targets. that target   s actions may be blank.)

docname = a_report

pdf: $(docname).pdf

$(docname).dvi: $(docname).tex

latex $(docname); latex $(docname)

clean:

$(docname).ps: $(docname).dvi

$(docname).pdf: $(docname).ps

ps2pdf $(docname).ps $(docname).pdf

dvips    f < $(docname).dvi > $(docname).ps

rm    f $(docname).blg $(docname).log $(docname).ps

listing 5.14 a make   le for producing pdfs from latex documents. online source: akefi e. ex.
using the e  e   and e  	  commands.
   for printing, you will probably want to use e  e       
 i  .
for online presentation, use e  e    g or e  e  gif. for
inserting into a latex document, you can either use postscript or e 
 e   a ex.

   once a plot looks good on screen, you can send it to an output    le

gsl_stats march 24, 2009

6

  more coding tools

if you have a good handle on chapter 2, then you already have what you need to
write some very advanced programs. but c is a world unto itself, with hundreds
of utilities to facilitate better coding and many features for the programmer who
wishes to delve further.

this chapter covers some additional programming topics, and some details of c
and its environment. as with earlier chapters, the syntax here is c-speci   c, but it
is the norm for programming languages to have the sort of features and structures
discussed here, so much of this chapter will be useful regardless of language.

the statistician reader can likely get by with just a skim over this chapter (with a
focus on section 6.1), but readers working on simulations or agent-based models
will almost certainly need to use the structures and techniques described here.

the chapter roughly divides into three parts. after section 6.1 covers functions
that operate on other functions, section 6.2 will use such functions to build struc-
tures that can hold millions of items, as one would    nd in an agent-based model.
section 6.3 shows the many manners in which your programs can take in parame-
ters from the outside world, including parameter    les, enivornment variables, and
the command line. sections 6.4 and 6.5 cover additional resources that make life
in front of a computer easier, including both syntactic tricks in c and additional
programs useful to programmers.

6.1 function pointers

types before we can start writing functions to act on functions, we need to take the

190

chapter 6

gsl_stats march 24, 2009

compiler needs to know this, so it can block attempts to send the function a string
or array.

what is a pointer to a function good for? it lets us write functions that will ac-
cept any function pointer and then use the pointed-to function. [functions calling
functions is already confusing enough, so i will capitalize function to indicate a
parent function that takes a lower-case function as an input.] for example, a func-
tion could search for the largest value of an input function over a given range, or a
bootstrap function could take in a statistic-calculating function and a data set and
then return the variance of the statistic.

a data pointd is stored somewhere in memory, so
we can refer to its address, d. similarly, a func-
tionf is stored somewhere in memory, so we can refer to its address as well.
type of input function into consideration. if a function expectsi  s, then the
function. say that we want to write a function that will take in an array ofd 	b es
an array ofi  s. then the input function has to have a form like
but with another star, likei   i; the same goes for declaring function pointers,
d 	b e and returns a pointer toi  ; you can see it is identical to line a, but for
by the way, if the function returned ani    instead of a plaini  , the declaration
the type declarations do nothing by themselves, just as the wordi   does nothing
to an array ofd 	b es would have a header like this:

by itself. but now that you know how to de   ne a function type, you can put the
declaration of the function into your header line. a function that applies a function

plus a function, and will apply the function to every element of the array, returning

but there are also extra parens. here is a type for a function pointer that takes in a

the syntax for declaring a function pointer is based on the syntax for declaring a

recall that a pointer declaration is just like a declaration for the pointed-to type

the addition of a star and parens:

int *(*double_to_int) (double *x)

int (*double_to_int) (double x)

int double_to_int (double x);

would be:

(a)

(b)

int* apply (double *v, int (*instance_of_function) (double x));

(c)

more coding tools

gsl_stats march 24, 2009

putting y edef to work are you confused yet? each component basically makes
a way out: y edef. by putting that word before line b   
   we have created an new type namedd 	b e_  _i   that we can use like any

sense, but together it is cluttered and confusing. there is

typedef int (*double_to_int) (double x);

191

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

other type. now, line c simpli   es to

int* apply (double *v, double_to_int instance_of_function);

#include <apop.h>

typedef double (*dfn) (double);

double sample_function (double in){

return log(in)+ sin(in);

}

void plot_a_fn(double min, double max, dfn plotme){

double val;
file *f = popen("gnuplot    persist", "w");

if (!f)

printf("couldn   t    nd gnuplot.\n");

fprintf(f, "set key off\n plot           with lines\n");
for (double i=min; i<max; i+= (max   min)/100.0){

val = plotme(i);
fprintf(f, "%g\t%g\n", i, val);

}

}

int main(){

plot_a_fn(0, 15, sample_function);

}
fprintf(f, "e\n");

source:    af	 
 i  .
.
gnuplot program described in chapter 5. with a y edef in place, the syntax

listing 6.1 shows a program to plot any function of the form r     r, using the
is easy. you don   t need extra stars or ampersands in either the declaration of the
function-of-a-function or in the call to that function, and you can call the pointed-
to function like any other.

listing 6.1 a demonstration of a function that takes in any function r     r and plots it. online

192

chapter 6

gsl_stats march 24, 2009

    line 16: using the passed-in function is as simple as using any other function: this

    line 3: to make life easier, thedf  type is declared at the top of the    le.
    line 5: the header for the a   e_f	 
 i   matches the format of thedf  func-
tion type (i.e.,d 	b e in,d 	b e out).
    line 9: the    _a_f  function speci   es that it takes in a function of typedf .
line gives no indication that     e is in any way special.
    line 23: finally, in ai , you can see how    _a_f  is called. the a   e_	
f	 
 i   is passed in with just its name.
for another example, have a look atja
ki e a i  .
 on page 132.
turn    af	 
 i  .
 into a library function callable by other programs.
    comment out the ai  function.
    write a header    af	 
 i  .h with the necesary type and func-
    write a test program that#i 
 	des    af	 
 i  .h and plots
the
a 
_ axe  function from axe .
 (p 118).
linking bothy 	 _
 de.  and    af	 
 i  . .
de   ne a typedf  as in line three of listing 6.1. then write a function
with headerv ida   y df f  d 	b e a  ay i  a  ay_ e  
changes each elementa  ay[i    tof  a  ay[i    . [apophenia provides

that takes as arguments a function, an array, and the length of the array, and

    modify the make   le to produce the    nal program by creating and

tion de   nitions.

q6.1

q6.2

comparable functions; see page 117 for details.]
test your function by creating an array of the natural numbers 1, 2, 3, . . . 20
and transforming it to a list of squares.

   you can pass functions as function arguments, just as you would pass

arrays or numbers.

   the syntax for declaring a function pointer is just like the syntax for
declaring a function, but the name is in parens and is preceded by a
star.
   

gsl_stats march 24, 2009

more coding tools

   

requires putting y edef in front of the function pointer declaration
   once you have a y edef in place, you can declare functions that
tion as you would expect. given the y edef, you need neither stars

   de   ning a new type to describe the function helps immensely. this

take functions, use the passed-in functions, and call the parent func-

in the last summary point.

193

nor ampersands for these operations.

6.2 data structures

say that you have a few million observations to store
on your computer. you want to    nd any given item
quickly, add or delete elements easily, and not worry too much about a compli-
cated organization system. there are several options for balancing these goals,
and choosing among them is not trivial. this section will consider three: the array,
the linked list, and the binary tree.

they will be implemented here via glib, a library of general-use functions that ev-
ery c programmer seems to re-implement. it includes a few features for string han-
dling and other such conveniences, and modules to handle the data structures de-
scribed here.1 the extended example below provides documentation-by-example
of initializing, adding to, removing from, and    nding elements within the various
structures, but your package manager will be happy to install the complete docu-
mentation, as well as glib itself.

an example this game consists of a series of meetings between pairs of birds,
who compete over r utils of resource.2 if two doves meet, they split
the resource evenly between them. if a dove and a hawk meet, the dove backs
down and the hawk gets the resource. if two hawks meet, then the hawks    ght,
destroying c utils in resources before    nally splitting what is left. table 6.2 shows
a payoff table summarizing the outcomes. for each pairing, the row player gets the
   rst payoff, and the column player gets the second.

1glib also provides a common data structure known as a hash table, which is another technique for easy data
retrieval. it converts a piece of data, such as a string, into a number that can then be used to jump to the string   s
data in a table very quickly. binary trees tend to work better in the context of agent-based modeling, so i have
omitted hashes from this chapter. see kernighan & pike (1999), chapter 3, for an extended example of hash
tables that produce nonsense text. it was intended as an amusement (compare with the exquisite corpse-type
game played by pierce (1980, p 262)), but is now commonly used to produce spam email. q: try implementing
kernighan and pike   s nonsense generator using glib   s hash tables, string hash functions, and list structures.

2the util is the unit of measurement for the quantity of utility an agent gets from an action.

gsl_stats march 24, 2009

194

chapter 6

dove
( r
2 , r
2 )
(r, 0)

hawk
(0, r)
2 , r   c
2 )

( r   c

dove

hawk

table 6.2 the payoff matrix for the hawk/dove game. if c < r, then this is a prisoner   s dilemma.

with c < r, the game is commonly known as a prisoner   s dilemma, due to a
rather contrived story about two separated prisoners who must choose between
providing evidence about the other prisoner and remaining silent. its key feature
is that being a dove (cooperating) always makes the agent worse off than being a
hawk (not cooperating, which the literature calls defection). the only equilibrium
to the p.d. game is when nobody cooperates, destroying resources every period,
but the societal optimum is when everyone cooperates, producing r utils of utility
total every time.

on top of this we can add an evolutionary twist: say that a bird that is very success-
ful will spawn chicks. in any one interaction, a bird gets an equal or better payoff
as a hawk than as a dove, so it seems that over time, the hawks would approach
100% of the population. in the simulation below, a bird   s odds of reproducing are
proportional to the percentage of total    ock wealth the bird holds, and its odds of
dying are inversely proportional to the same.

bi d /bi d .h in the online code supplement. it begins by describing the basic

to simulate the game, we will need a    ock of birds. have a look at the header    le

structure that the rest of the functions depend upon, describing a single bird:

typedef struct {

char type;
int wealth;
int id;

} bird;

the header then lists two types of function. the    rst are functions for each relevant
action in the simulation: startup, births, deaths, and actual plays of the hawk/dove
game. the second group are functions for    ock management, such as counting the
   ock or iterating over every member of the    ock.

individually, should make sense:  ay_hd_ga e takes in two birds and modi   es
their payoff according to the game rules above;bi d_  ay  takes in a single bird,

the    rst set of functions are implemented in listing 6.3. each function, taken

   nds an opponent, and then has them play against each other; et cetera.

gsl_stats march 24, 2009

more coding tools

195

#include "birds.h"
#include <time.h>

gsl_rng *r;
int periods = 400;
int initial_pop = 1000;
int id_count = 0;

void play_hd_game(bird *row, bird *col){

double resource = 2,

cost = 2.01;

if (row   >type ==    d    && col   >type ==    h   )
else if (row   >type ==    h    && col   >type ==    d   )
else if (row   >type ==    d    && col   >type ==    d   ){

col   >wealth += resource;
row   >wealth += resource;
col   >wealth += resource/2;
row   >wealth += resource/2;
col   >wealth += (resource   cost)/2;
row   >wealth += (resource   cost)/2;

} else { // hawk v hawk

} }

void bird_plays(void *in, void *dummy_param){

bird *other;

while(!(other =    nd_opponent(gsl_rng_uniform_int(r,id_count))) && (in != other))

;//do nothing.

play_hd_game(in, other); }

bird *new_chick(bird *parent){

bird *out = malloc(sizeof(bird));

if (parent)

else{

out   >type = parent   >type;
if (gsl_rng_uniform(r) > 0.5)

else

out   >type =    d   ;
out   >type =    h   ;

}
out   >wealth = 5* gsl_rng_uniform(r);
out   >id = id_count;
id_count ++;
return out; }

void birth_or_death(void *in, void *t){

bird *b = in; //cast void to bird;
int *total_wealth = t;

add_to_   ock(new_chick(b));

if (b   >wealth*20./ *total_wealth >= gsl_rng_uniform(r))
if (b   >wealth*800./ *total_wealth <= gsl_rng_uniform(r))

free_bird(b); }

void startup(int initial_   ock_size){

   ock_init();
r = apop_rng_alloc(time(null));
printf("period\thawks\tdoves\n");
for(int i=0; i< initial_   ock_size; i++)

add_to_   ock(new_chick(null)); }

gsl_stats march 24, 2009

196

int main(){

startup(initial_pop);
for (int i=0; i< periods; i++){

   ock_plays();
count(i);

} }

listing 6.3 the birds. online source:bi d /bi d .

chapter 6

now for the    ock management routines, which will be implemented three times:
as an array, as a list, and as a binary tree.

their data in arrays of this type.

arrays an array is as simple as data representation can get: just write each element
right after the other. the matrices and vectors throughout this book keep

   nd that there is more free space for the array to grow. if you are not lucky, then
the array will have to be moved in its entirety to a new, more spacious home.

the system can retrieve an item from an array faster than from any other data
structure, since the process consists of simply going to a    xed location and reading
the data there. on the other hand, adding and deleting elements from an array is

dif   cult: the simulation has to call ea   
 every time the list expands. if you are
lucky, ea   
 will not move the array from its current location, but will simply
call to e   ve to execute thousands or millions of copy operations.
listing 6.4 marks dead birds by setting theirid to	1. this means that as the
as for    nding an element,add_  _f  
k takes pains to ensure that theid and

an array can not have a hole in the middle, so elements can not be deleted by
freeing the memory. there are a few solutions, none of which are very pleasant.
the last element of the list could be moved in to the space, requiring a copy, a
shrinking of the array, and a loss of order in the elements. if order is important,
every element could be shifted down a notch, so if item 50 is deleted, item 51 is
put in slot 50, item 52 is put in slot 51, et cetera. thus, every death could mean a

program runs, more and more memory is used by dead elements, and the rest of
the system must check for the marker at every use.

array index will always match one-to-one, so    nding a bird given its id number
is trivial. as with the code above, the code consists of a large number of short
functions, meaning that it is reasonably easy to understand, read, and write each
function by itself.

gsl_stats march 24, 2009

more coding tools

197

#include "birds.h"

bird *   ock;
int size_of_   ock, hawks, doves;

void    ock_plays(){

for (int i=0; i< size_of_   ock; i++)

if (   ock[i].id >= 0)

bird_plays(&(   ock[i]), null); }

void add_to_   ock(bird* b){
size_of_   ock = b   >id;
   ock = realloc(   ock, sizeof(bird)*(size_of_   ock+1));
memcpy(&(   ock[b   >id]), b, sizeof(bird));
free(b); }

void free_bird(bird* b){ b   >id =    1; }
bird *    nd_opponent(int n){

if (   ock[n].id >= 0)

return &(   ock[n]);

else return null; }

int    ock_size(){ return size_of_   ock; }

int    ock_wealth(){

int i, total =0;

for (i=0; i< size_of_   ock; i++)

if (   ock[i].id >= 0)

total +=    ock[i].wealth;

return total; }

double count(int period){

int i, tw =    ock_wealth();

hawks = doves = 0;
for(i=0; i< size_of_   ock; i++)

if (   ock[i].id>=0)

birth_or_death(&(   ock[i]), &tw);

for(i=0; i< size_of_   ock; i++)

if (   ock[i].id>=0){

if (   ock[i].type ==    h   )

hawks ++;
else doves ++;

}

void    ock_init(){
   ock = null;
size_of_   ock = 0; }

printf("%i\t%i\t%i\n", period, hawks, doves);
return (doves+0.0)/hawks;}

dead birds will eventually pile up. online source:bi d /a  ayf  
k.

listing 6.4 the birds, array version. the fatal    aw is that birds are copied in, but never eliminated.

198

chapter 6

q6.4

q6.3

chapter to produce a plot.

gsl_stats march 24, 2009

directory of the code supplement.]

matches the array index. keep a counter of current allocated size (which

value of c and returns the proportion of doves at the end of the simulation.

lose 0.005 utils when they    ght, so it is marginally better to be a dove when
meeting a hawk, and the game is not quite a prisoner   s dilemma. after run-
ning the simulation a few times to get a feel for the equilibrium number
of birds, change c to 2.0 and see how the equilibrium proportion of doves

because the  ay_hd_ga e function sets r == 2 and c == 2.01, hawks
changes. [for your convenience, a sample make   le is included in thebi d 
finally, rename ai  to  e_ 	  and wrap it in a function that takes in a
send the function to the    _a_f	 
 i   function from earlier in the
rewritea  ayf  
k.
 to delete birds instead of just mark them as dead.
use e   ve to close holes in the array, then renumber birds so theirid
may be greater than the number of birds) so you can ea   
 the array
linked lists a linked list is a set of   	
 s connected by pointers. the    rst
   	
  includes a ex  pointer that points to the next element,
whose ex  pointer points to the next element, et cetera; see figure 6.5.
figure 6.5 the archetypal linked list. online source: i  .d  .
the u   pointer at the end of the list with a pointer to the new node. deleting a
node is also simple: to delete bird 2, simply reroute the ex  pointer from bird
in an array,    nding the ten thousandth element is easy:f  
k[9999   . you can see
in the code of listing 6.6 that glib provides ag_ i  _  h_da a function to return
function can    nd the ten thousandth member of thef  
k is to start at the head of
the list and take 9,999	> ex  steps. in fact, if you compile and run this program,

1     2 so that it points from bird 1     3, and then free the memory holding bird 2.
but the real failing of the linked list is the trouble of    nding an arbitrary element.

the linked list is a favorite for agent-based simulation, because birth and death is
easy to handle. to add an element to a linked list, just create a new node and replace

the nth element of the list, which makes it look simple, but the only way that that

only when necessary.

null

bird 3

bird 4

bird 2

bird 1

next

next

next

gsl_stats march 24, 2009

more coding tools

199

you will see that it runs much more slowly than the array and tree versions.

#include "birds.h"
#include <glib.h>

glist *   ock;
int hawks, doves;

void    ock_plays(){ g_list_foreach(   ock, bird_plays, null); }

void add_to_   ock(bird* b){    ock = g_list_prepend(   ock, b); }

void free_bird(bird* b){

   ock = g_list_remove(   ock, b);
free(b); }

bird *    nd_opponent(int n){ return g_list_nth_data(   ock, n); }

void wealth_foreach(void *in, void *total){
*((int*)total) += ((bird*)in)   >wealth; }

int    ock_wealth(){

int total = 0;

g_list_foreach(   ock, wealth_foreach, &total);
return total; }

int    ock_size(){ return g_list_length(   ock); }

void bird_count(void *in, void *v){

bird *b = in;

if (b   >type ==    h   )
hawks ++;
else doves ++;

}

double count(int period){

int total_wealth =   ock_wealth();

void    ock_init(){    ock = null; }

listing 6.6 the birds, linked list version. the fatal    aw is that    nding a given bird requires traversing

hawks = doves = 0;
g_list_foreach(   ock, birth_or_death, &total_wealth);
g_list_foreach(   ock, bird_count, null);
printf("%i\t%i\t%i\n", period, hawks, doves);
return (doves+0.0)/hawks;}

the entire list every time. online source:bi d / i  f  
k.
    theg_ i  _f  ea
h function implements exactly the sort of apply-function-to-
bi d structure, so how could they write a linked list that would hold it? the so-
lution isv id pointers   that is, a pointer with no type associated, which could

list setup implemented in section 6.1. it takes in a list and a function, and internally
applies the function to each element.

    the folks who wrote the glib library could not have known anything about the

therefore point to a location holding data of any type whatsoever. for example,

200

chapter 6

gsl_stats march 24, 2009

guaranteed, use the form here to reassign the list to a new value for every add/de-

    the    rst step in using a void pointer is casting it to the correct type. for example,

    as for adding and removing, the glib implementation of the list takes in a pointer

and the second being any sort of user-speci   ed data (which in this case is just
ignored).

bi d_
 	   takes in two void pointers, the    rst being the element held in the list,
the    rst line inbi d_
 	     bi d b=i ;   pointsb to the same address as
i , but sinceb has a type associated, it can be used as normal.
to ag i   and a pointer to the data to be added, and returns a new pointer to
ag i  . the input and output pointers could be identical, but since this is not
lete. for example, the    ock starts inf  
k_i i  as u  , and is given its    rst
non- u   value on the    rst call toadd_  _f  
k.
bi d5, start at the head (bi d1), then go left, then go right. but with eight data
knows whether to go left or right at each step. in this case, theid for each bird
provides a natural means of ordering. for text data,   
   would provide a simi-
a function for comparing keys. in the code below,g_  ee_ ew initializes a tree
using the
   a e_bi d  function.

points in a linked list, you would need up to seven steps to get to any element, and
on average 3.5 steps. in a tree, the longest walk is three steps, and the average is
1.625 steps. generally, the linked list will require on the order of n steps to    nd
an item, and a b-tree will require on the order of ln(n) steps (knuth, 1997, pp
400   401).3

eariler, you saw an interesting implementation of a set of binary trees: a data-
base. since databases require fast access to every element, it is natural that they
would internally structure data in a binary tree, and this is exactly how sqlite and
mysql operate internally: each new index is its own tree.

lar ordering. more generally, there must be a key value given to each element, and
the tree structure must have a function for comparing keys.

binary trees the binary tree takes the linked list a step further by giving each
node two outgoing pointers instead of one. as per figure 6.7, think
of these pointers as the left pointer and the right pointer. the branching allows for
a tree structure. the directions to an element are now less than trivial   to get to

the tree arrangement needs some sort of order to the elements, so the system

this adds some complication, because you now need to associate with each tree

3for those who follow the reference, notice that knuth presents the equation for the sum of path lengths, which
he calls the internal path length. he    nds that it is of order n ln(n) +o(n) for complete binary trees; the average
path length is thus ln(n) + o(1).

gsl_stats march 24, 2009

more coding tools

201

bird 5

left

right

bird 3

bird 7

left

right

left

right

left

left

left

left

left

right

right

right

right

right

bird 1

bird 8

bird 6

bird 4

bird 2

what if two birds have the same

figure 6.7 the archetypal binary tree. online source:b  ee.d  .
the
     keyword
id? then there is no way to order
the
     modi   ers in the header for
   a e_	
key  indicate that the data to which the pointers point
til page 201, the
     keyword is mostly optional,
 ea   
ing, although there is often
the subfunction so that it too takes
     inputs (and

though it is good form and provides one more check
that your functions don   t do what you hadn   t intended.
however, when conforming with function speci   ca-
tions elsewhere, like glib   s function header for key-
comparison functions, you may need to use it. if you
then get an error like    subfunction    discards quali   ers
from pointer target type, then you will need to rewrite

the added complication of a tree
solves many of the problems above.
as with the list, inserting and delet-
ing elements does not require major

them uniquely, and therefore there
is no way to reliably store and re-
trieve them. thus, the key for each
element must be unique.4

will not be changed over the course of the function.
as you can see by the fact that it has not appeared un-

minor internal reshuf   ing to keep
the branches of the tree at about
even length. with the key and short
chains,    nding an element is much faster.

does not modify them).

#include "birds.h"
#include <glib.h>

gtree *   ock = null;
int hawks, doves;

static gint compare_keys(const void *l, const void *r){

const int *lb = l;
const int *rb = r;

return *lb     *rb;

}

4there exist tree implementations that do not require unique keys, but it is a requirement for glib. similarly,

some databases are very strict about requiring that each table have a    eld representing a key, and some are not.

gsl_stats march 24, 2009

202

chapter 6

static gboolean tree_bird_plays(void *key, void *in, void *v){

bird_plays(in, null);
return 0;

}

void    ock_plays(){ g_tree_foreach(   ock, tree_bird_plays, null); }

void add_to_   ock(bird* b){ g_tree_insert(   ock, &(b   >id), b); }
bird *    nd_opponent(int n){return g_tree_lookup(   ock, &n);}

int    ock_size(){ return g_tree_nnodes(   ock); }

static gboolean wealth_foreach(void *key, void *in, void *t){

int *total = t;

*total += ((bird*)in)   >wealth;
return 0; }

int    ock_wealth(){

int total = 0;

g_tree_foreach(   ock, wealth_foreach, &total);
return total; }

static gboolean tree_bird_count(void *key, void *in, void *v){

if (((bird *)in)   >type ==    h   )
hawks ++;
else doves ++;
return 0; }

glist *dying_birds;

void free_bird(bird* b){dying_birds = g_list_prepend(dying_birds, b);}

static gboolean tree_birth_or_death(void *key, void *in, void *t){

birth_or_death(in, t);
return 0; }

static void cull_foreach(void *b, void *v){

bird* a_bird = b;

g_tree_remove(   ock, &(a_bird   >id));
free(a_bird); }

double count(int period){

int total_wealth =   ock_wealth();

hawks = doves = 0;
dying_birds = null;
g_tree_foreach(   ock, tree_birth_or_death, &total_wealth);
g_list_foreach(dying_birds, cull_foreach, null);
g_list_free(dying_birds);
g_tree_foreach(   ock, tree_bird_count, null);
printf("%i\t%i\t%i\n", period, hawks, doves);
return (doves+0.0)/hawks;}

for every bird. online source:bi d /  eef  
k.

void    ock_init(){    ock = g_tree_new(compare_keys); }

listing 6.8 the birds, binary tree version. the fatal    aw is the complication in maintaining the key

gsl_stats march 24, 2009

more coding tools

a tree. in the implementation of listing 6.8, thef ee_bi d function actually freed
the bird; here it just adds dying birds to ag i  , and then another post-traversal
step goest through theg i   and cull marked birds from the tree.

    culling the    ock is especially dif   cult because a tree can internally re-sort when
an element is added/deleted, so it is impossible to delete elements while traversing

203

   there are various means of organizing large data sets, such as collec-

tions of agents in an agent-based model.

   arrays are simply sequential blocks of structs. pros: easy to imple-
ment; you can get to the 10, 000th element in one step. cons: no easy
way to add, delete, or reorganize elements.

   a linked list is a sequence of structs, where each includes a pointer to
the next element in the list. pro: adding/deleting/resorting elements is
trivial. con: getting to the 10, 000th element takes 9,999 steps.

   a binary tree is like a linked list, but each struct has a left and right
successor. pros: adding and deleting is only marginally more dif   -
cult than with a linked list; getting to the 10, 000th element takes at
most 13 steps. con: each element must be accessed via a unique key,
adding complication.

6.3 parameters

this section will cover a cavalcade of means of setting parameters and speci   ca-
tions, in increasing order of ease of use and dif   culty in implementation.

your simulations and analyses will require tweaking. you
will want to try more agents, or you may want your program
to load a data set from a text    le to a database for one run and then use the data in
the database for later runs.

the    rst option   a default of sorts   is to set variables at the top of your.
    le
 
a f andfge  . listing 6.9 shows a program that asks data of the
user and then returns manipulated data. the 
a f function basically works like
  i  f in reverse, reading text with the given format into pointers to variables.
comma or period can entirely throw off the format string. thefge   function will

or a header    le. this is trivial to implement, but you will need to recompile every
time you change parameters.

unfortunately, the system tends to be rather fragile in the real world, as a stray

the second option is to interactively get parameters from the user, via

interactive

gsl_stats march 24, 2009

204

chapter 6

read an entire line into a string, but has its own quirks. in short, the interactive
input features are good for some quick interrogations or a bit of fun, but are not to
be heavily relied upon.

#include <stdio.h>
#include <string.h> //strlen

int main(){

   oat indata;
char s[100];

printf("give me a number: ");
scanf("%g", &indata);
printf("your number squared: %g\n", indata*indata);
printf("ok, now give me a string (max length, 100):\n");
fgets(s, 99, stdin); //eat a newline.
fgets(s, 99, stdin);
printf("here it is backward:\n");
for (int i=strlen(s)   2; i>=0; i      )

listing 6.9 reading inputs from the command line. online source:ge    i g.
.

printf("%c", s[i]);

printf("\n");

}

environment variables

these are variables passed from the shell (aka the com-
mand prompt) to the program. they are relatively easy to
set, but are generally used for variables that are infrequently changing, like the
username. environment variables are discussed at length in appendix a.

parameter    les

there are many libraries that read parameter    les; consistent with
the rest of this chapter, listing 6.10 shows a    le in glib   s key    le
format, which will be read by the program in listing 6.11. the con   guration    le
can be in a human language like english, you can modify it as much as you want
without recompiling the code itself, it provides a permanent record of parameters
for each run, and you can quickly switch among sets of variables.

    the payoff for listing 6.11 is on line 22: printing the name of a distribution, a
parameter, and the mean of that distribution given that parameter. the program to
that point    nds these three items.

the exponential section. below, you will see that setting the
  fig variable on
    line 10 reads the entireg ib.
  fig    le into thekey  structure. if something

    line seven indicates which section of listing 6.10 the following code will read.
by commenting out line seven and uncommenting line eight, the code would read

the command line is not dif   cult.

gsl_stats march 24, 2009

more coding tools

205

#gkeys.c reads this    le

[chi squared con   guration]
distribution name = chi squared
parameter = 3

listing 6.10 a con   guration in the style of glib   s key    les. online source:g ib.
  fig.

[exponential con   guration]
distribution name = exponential
parameter = 2.2

#include <glib.h>
#include <apop.h>

int main(){

gkeyfile *keys = g_key_   le_new();
gerror *e = null;
char *con   g = "chi squared con   guration";
// char *con   g = "exponential con   guration";

double (*distribution)(double, double);

if (!g_key_   le_load_from_   le(keys, "glib.con   g", 0, &e))

fprintf(stderr, e   >message);

double param = g_key_   le_get_double(keys, con   g, "parameter", &e);
if (e) fprintf(stderr, e   >message);
char* name = g_key_   le_get_string(keys, con   g, "distribution name", &e);
if (e) fprintf(stderr, e   >message);

}

param, distribution(0.5, param));

if (!strcmp(name, "chi squared"))

distribution = gsl_cdf_exponential_pinv;

printf("mean of a %s distribution with parameter %g: %g\n", name,

distribution = gsl_cdf_chisq_pinv;
else if (!strcmp(name, "exponential"))

listing 6.11 a program that reads listing 6.10. online source:gkey .
.
goes wrong, then line 11 prints the error message stored ine. properly, the program
should exit at this point; for the sake of brevity the e 	  0 lines have been
    now thatkey  holds all the values in the con   g    le, lines 12 and 14 can get indi-
vidual values. the twog_key_fi e_ge ... functions take in a    lled key struc-

omitted.

ture, a section name, a variable name, and a place to put errors. they return the
requested value (or an error).

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

q6.5

    unfortunately, there is no way to specify functions in a text    le but by name, so

206

data.

chapter 6

function.

the con   g    le.

gsl_stats march 24, 2009

    write a text    le with three columns: con   guration, parameters, and

rewrite the code in listing 6.11 to set parameters via database, rather than
via the command line.

lines 17   20 set the function pointerdi   ib	 i   according to the name from
    read in the    le usinga   _ ex _  _db at the beginning of ai .
    write a function with headerd 	b ege _ a a  
ha  
  fig 

ha     that queries the database for the parameter named  in the
con   guration group
  fig and returns its value. then modify the
program to get the distribution and its parameter using thege _ a a 
the ai  function takes inputs and produces an output like any other. the output
is an integer e 	  ed at the end of ai , which is typically zero for success or a
the number of command-line elements, and a
ha      an array of strings   listing
arbitrary. however, the universal custom is to name thema g
 (argument count)
anda gv (argument values).5 this is an ingrained custom, and you can expect to
listing 6.12 shows the rudimentary use ofa g
 anda gv. here is a sample usage
5they are in alphabetical order in the parameters to ai , which provides an easy way to remember that the
6perl usesa gv, python uses y .a gv, and ruby usesargv. all three structures automatically track array
lengths, so none of these languages uses ana g
 variable.

reading parameters from the command line can take the most ef-
fort to implement among the parameter-setting options here, but it
is the most dynamic, allowing you to change parameters every time you run the
program. you can even write batch    les in perl, python, or a shell-type language to
run the program with different parameter variants (and thus keep a record of those
variants).

the command-line elements themselves. like any function speci   cation, the types
are non-negotiable but the internal name you choose to give these arguments is

positive integer indicating a type of failure. the inputs are always an integer, giving

see those names everywhere.6

from my command line:

count comes    rst.

command line

gsl_stats march 24, 2009

207

more coding tools

}

#include <stdio.h>

for (int i=0; i< argc; i++)

int main(int argc, char **argv){

printf("command line argument %i: %s\n", i, argv[i]);

listing 6.12 this program will simply print out the command line arguments given to it. online

>>> /home/klemens/argv one 2       three fo\ ur "cinco        \"   ve\""
command line argument 0: /home/klemens/argv
command line argument 1: one
command line argument 2: 2
command line argument 3:       three
command line argument 4: fo ur
command line argument 5: cinco        "   ve"

source:a gv.
.
    argument zero (a gv[0   ) is always the name of the command itself. some cre-
norm is to just skip overa gv[0   .
    after that, the elements ofa gv are the command line broken at the spaces, and
    as you can see from the parsing off \	 , a space preceded by a backslash is
actions with a ai  like the following:

for some purposes, this is all you will need to set program options from your
command line. for example you could have one program to run three different

    argument    ve shows that everything between a pair of quotation marks is a single

ative programs run differently if they are referred to by different names, but the

argument, and a backslash once again turns the quotation mark into plain text.

taken to be a character like any other, rather than an argument separator.

could be dashes, numbers, or any other sort of text.

int main(int argc, int **argv){

if (argc == 1){

printf("i need a command line argument.\n")
return 1;

}
if (!strcmp(argv[1], "read"))

read_data();

else if (!strcmp(argv[1], "analysis_1"))

run_analysis_1();

else if (!strcmp(argv[1], "analysis_2"))

run_analysis_2();

}

q6.6

getopt

208

chapter 6

portable.7

gsl_stats march 24, 2009

function converts a text string to the integer it represents; for example,

listing 6.13 shows a program that will display a series of exponents. as explained

after the switches. sample usage (which also demonstrates that spaces between
the switch and the data are optional):

the
a  byva .
 program (listing 2.5, page 38) calculated the factorial
of a number which was hard-coded into ai . rewrite it to take the number
from the command line, sofa
   ia 15 will    nd 15!. (hint: thea  i
a  i "15" ==15.)
for more complex situations, usege    , which parses command lines for
switches of the form	x.... it is part of the standard c library, so it is fully
by the message on lines 9   14, you can set the minimum of the series via	 , the
maximum via	 , and the increment via	i. specify the base of the exponents
   #i 
 	de<	 i  d.h>.
info (here, every switch but	h), indicate this with a colon after the letter.
    write awhi e loop to callge     (line 27), and then act based upon the
value of the
ha  thatge     returned.
   a gv is text, but you will often want to specify numbers. the functionsa  i,a   ,
anda  f convert ascii text to ani  ,   gi  , ord 	b e, respectively.8
7the gnu version of the standard c library provides a sublibrary nameda g  that provides many more
8getopt readily handles negative numbers that are arguments to a    ag (	 	3), but a negative number after the
options will look like just another    ag, e.g.,./ge    	 	3	 4	2 looks as if there is a    ag named2. the
special    ag		 indicates thatge     should stop parsing    ags, so./ge    	 	3	 4			2 will work.
this is also useful elsewhere, such as handling    les that begin with a dash; e.g., given a    le named	a_ i  ake,
you can delete it with  			a_ i  ake.

functions and does more automatically, but is correspondingly more complex and less portable. glib also has a
subsection devoted to command-line arguments, which also provides many more features so long as the library is
installed.

    specify a set of letters indicating valid single-letter switches in a crunched-
together string like line 15 of listing 6.13. if the switch takes in additional

>>> ./getopt    m 3    m4    i 0.3 2
2^3: 8
2^3.3: 9.84916
2^3.6: 12.1257
2^3.9: 14.9285

there are three steps to the process:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41

gsl_stats march 24, 2009

more coding tools

209

#include <stdio.h> //printf
#include <unistd.h> //getopt
#include <stdlib.h> //atof
#include <math.h> //powf

double min = 0., max = 10.;
double incr = 1., base = 2.;

void show_powers(){

for (double i=min; i<=max; i+= incr)

printf("%g^%g: %g\n", base, i, powf(base, i));

}

int main(int argc, char ** argv){

char c, opts[]= "m:m:i:h";
char help[]= "a program to take powers of a function. usage:\n"

"\t\tgetopt [options] [a number]\n"
"   h\t this help\n"
"   m\t the minimum exponent at which to start.\n"
"   m\t the maximum exponent at which to    nish.\n"
"   i\t increment by this.\n";

if (argc==1) {

printf(help);
return 1;

}
while ( (c=getopt(argc, argv, opts)) !=    1)

if (c==   h   ){

printf(help);
return 0;

} else if (c==   m   ){

min = atof(optarg);

}

}

show_powers();

if (optind < argc)

} else if (c==   i   ){

incr = atof(optarg);

} else if (c==   m   ){

max = atof(optarg);

base = atof(argv[optind]);

listing 6.13 command-line parsing withge    . online source:ge    .
.
      a g also sets the variable   i d to indicate the position ina gv that it last
ge       s help.

visited. thus, line 38 was able to check whether there are any non-switch argu-
ments remaining, and line 39 could parse the remaining argument (if any) without

q6.7

210

chapter 6

to recompile the program.

gsl_stats march 24, 2009

   there are many ways to change a program   s settings without having

listing 6.1 (page 191) is hard-coded to plot a range from x = 0 to x = 15.

off all switches entirely, then the program prints a help message and exits. every
variable that the user could forget to set via the command line has a default value.

    the program provides human assistance. if the user gives the	h switch or leaves
modify it to usege     to get a minimum and maximum from the user,
with zero and    fteen as defaults. provide help if the user uses the	h    ag.
   the ai  function takes in arguments listed on the command line, and
some c functions (likege    ) will help you parse those arguments
6.4   syntactic sugar
as the single expressiona =b=i  >j;. the seventh element of the arrayk can be
calledk[6   ,  k 6 , or   for the truly perverse   6[k   . that is, this book over-

returning to c syntax, there are several ways to do
almost everything in chapter 2. for example, you

   environment variables, covered in appendix a, are a lightweight

   there are many libraries for parsing parameter    les, or you could use

means of setting variables in the shell that the program can use.

sql to pull settings from a database.

could rewrite the three lines

b = (i > j);
a += b;
i++;

into program settings.

looks a great deal of c syntax, which is sometimes useful and even graceful, but
confuses as easily as it clari   es.

this section goes over a few more details of c syntax which are also not strictly
necessary, but have decent odds of being occasionally useful. in fact, they are
demonstrated a handful of times in the remainder of the book. if your interest is
piqued and you would like to learn more about how c works and about the many
alternatives that not mentioned here, see the authoritative and surprisingly readable
reference for the language, kernighan & ritchie (1988).

gsl_stats march 24, 2009

there is another way to write anif statement:

more coding tools

the obfuscatory if

211

if (a < b)

   rst_val;

else

second_val;

/* is equivalent to */

a < b ?    rst_val : second_val;

else

out   >type =    h   ;

out   >type =    d   ;

if (gsl_rng_uniform(r) > 0.5)

you saw the following snippet:

the condensed form is primarily useful because you can put it on the right side

both have all three components:    rst the condition, then the    what to do if the
condition is true    part, and then the    what to do if the condition is false    part.
however, the    rst is more-or-less legible to anybody who knows basic english,
and the second takes the reader a second to parse every time he or she sees it. on
the other hand, the second version is much more compact.

of an assignment. for example, in the ew_
hi
k function of listing 6.3 (p 195),
macros as well as#i 
 	de-ing    les, the preprocessor can also do text substitu-
9global and static variables are initialized before the program calls ai , meaning that they have to be al-

text substitution can do a few things c can   t do entirely by itself. for example, you
may have encountered the detail of c that all global variables must have constant
size (due to how the compiler sets them up).9 thus, if you attempt to compile the
following program:

using the obfuscatory if, these four lines can be reduced to one:

tion, where it simply replaces one set of symbols with another.

out   >type = gsl_rng_uniform(r) > 0.5 ?    d    :    h   ;

located without evaluating any non-constant expressions elsewhere in the code. local variables are allocated as
needed during runtime, so they can be based on evaluated expressions as usual.

gsl_stats march 24, 2009

212

chapter 6

int main(){ }

int main(){ }

int array_size = 10;
int a[array_size];

#de   ne array_size 10
int a[array_size];

following program will compile properly, because the preprocessor will substitute

the easy alternative is to simply leave the declaration at the top of the    le but move

you will get an error likeva iab e	 ize y ede
 a ed 	  ide fa y
f	 
 i  .
the initialization into ai , but you can also    x the problem with#defi e. the
the number 10 forarray_s ze before the compiler touches the code:
do not use an equals sign or a semicolon with#defi es.
you can also#defi e function-type text substitutions. for example, here is the
code for thegs _    macro from the<g  /g  _ a h.h> header    le:
it would expand every instance in the code ofgs _    a b  to the if-then-else
expression in parens.gs _ ax is similarly de   ned.
ates its arguments and then calls the function, sof 2 3  is guaranteed to evaluate
exactly asf 5  does. a macro works by substituting one block of text for another,
then wi
e 2 3  expands to2 2 3, which is not equal to wi
e 5 =2 5.

this is a macro, which is evaluated differently from a function. a function evalu-

without regard to what that text means. if the macro is

#de   ne gsl_min(a,b) ((a) < (b) ? (a) : (b))

#de   ne twice(x) 2*x

we thus arrive at the    rst rule of macro writing, which is that everything in the
macro de   nition should be in parentheses, to prevent unforseen interactions be-
tween the text to be inserted and the rest of the macro.

213

more coding tools

gsl_stats march 24, 2009

var_array = realloc(var_array, new_length * sizeof(double))

this can be rewritten with a macro to create a simpler form:

repeated evaluation is another common problem to look out for. for example,

may be incremented twice, not once as it would with a function. again, the    rst
solution is to not use macros except as a last resort, and the second is to make sure
calls to macros are as simple as possible.

the one thing that a macro can do better than a function is take a type as an argu-
ment, because the preprocessor just shunts text around without regard to whether
that text represents a type, a variable, or whatever else. for example, recall the

gs _    a   b  expands to  a   < b ? a   : b  , meaning thata
form for reallocating a pointer to an array ofd 	b es:
#i 
 	ded with every    le), but may make the code more readable. this macro
a call likerea   c     8 1 d 	b e  would allocate 8 + sizeof(double)
    if you need to debug a macro, the	e    ag tog

 will run only the preprocessor, so
you can see what expands to what. you probably want to rung

	e  efi e.
| e  .
   shortif statements can be summarized to one line via the
  di	
 i  ?  	e_va 	e:fa  e_va 	e form.
   you can use the preprocessor to#defi e constants and short func-

    the custom is to put macro names in capitals. you can rely on this in code you see
from others, and are encouraged to stick to this standard when writing your own,
as a reminder that macros are relatively fragile and tricky. apophenia   s macros can
be written using either all-caps or, if that looks too much like yelling to you, using
only an initial capital.

#de   ne realloc(ptr, length, type) ptr = realloc((ptr), (length) * sizeof(type))
//which is used like this:
realloc(var_array, new_length, double);

bytes of memory instead of 9    sizeof(double) bytes.

also gives yet another demonstration of the importance of parens: without parens,

it gives you one more moving part that could break (and which now needs to be

tions.

gsl_stats march 24, 2009

214

chapter 6

6.5 more tools

packages designed to handle just this problem.

if you are using the gnu standard library (which you probably are if you are using

memory debugger the setup is this: you make a mistake in memory handling
early in the program, but it is not fatal, so the program con-
tinues along using bad data. later on in the program, you do something innocuous

since c is so widely used, there is an ecosystem of tools
built around helping you easily write good code. beyond the
debugger, here are a few more programs that will make your life as a programmer
easier.

with your bad data and get a segfault. this is a pain to trace usinggdb, so there are
g

), then you can use the shell command
to set the a   c_c ec _ enviornment variable; see appendix a for more on
 a   
. when it is set to one, the library uses a version of a   
 that checks
  de  . when the variable is set to two, the system halts on the    rst error, which
is exactly what you want when running viagdb.
available via your package manager. it also provides a different version of a   
recompile the program using the efence library, by either adding	 efe 
e to the
compilation command or the    f ags line in your make   le (see appendix a).

environment variables. when it is not set or is set to zero, the library uses the usual

that crashes on any mis-allocations and mis-reads. to use it, you would simply

for common errors like double-freeing and off-by-one errors, and reports them on

another common (and entirely portable) alternative is electric fence, a library

export malloc_check_=2

revision control the idea behind the revision control system (rcs) is that
your project lives in a sort of database known as a reposi-
tory. when you want to work, you check out a copy of the project, and when you
are done making changes, you check the project back in to the repository and can
delete the copy. the repository makes a note of every change you made, so you
can check out a copy of your program as it looked three weeks ago as easily as you
could check out a current copy.

this has pleasant psychological bene   ts. don   t worry about experimenting with
your code: it is just a copy, and if you break it you can always check out a fresh
copy from the repository. also, nothing matches the con   dence you get from mak-
ing major changes to the code and    nding that the results precisely match the
results from last month.

gsl_stats march 24, 2009

more coding tools

215

finally, revision control packages facilitate collaboration with coauthors. if your
changes are suf   ciently far apart (e.g., you are working on one function and your
coauthor on another), then the rcs will merge all changes to a single working
copy. if it is unable to work out how to do so, then it will give you a clearly
demarcated list of changes for you to accept or reject.

this method also works for any other text    les you have in your life, such as papers
written in latex, html, or any other text-based format. for example, this book is
under revision control.

the profiler

directory, with the machine-readable timings that the pro   ler will use.10 unlike

there is no universal standard revision control software, but the subversion pack-
age is readily available via your package manager. for usage, see subversion   s
own detailed manual describing set-up and operation from the command line, or
ask your search engine for the various guis written around subversion.

if you feel that your program is running too slowly, then the    rst
step in    xing it is measurement. the pro   ler times how long every
function takes to execute, so you know upon which functions to focus your clean-
up efforts.

first, you need to add a    ag to the compilation to include pro   ler symbols,	 g.
then, execute your program, which will produce a    le namedg   . 	  in the
the debugger   s	g option, the	 g option may slow down the program signi   cantly
as it writes tog   . 	 , so use	g always and	 g only when necessary.
finally, callg   f./ y_exe
	 ab e to produce a human-readable table from
g   . 	 .11 see the manual ( a g   f) for further details about reading the
10if the program is too fast for the pro   ler, then rename ai  toi  e  a _ ai  and write a new ai 
function with af   loop to calli  e  a _ ai  ten thousand times.
11g   f outputs to  d 	 ; use the usual shell tricks to manipulate the output, such as piping output through
a pager   g   f./ y_exe
	 ab e| e     or dumping it to a text    le   g   f./ y_exe
	 ab e>
 	 fi e   that you can view in your text editor.

if you are just trying to get your programs to run, optimizing for speed may seem
far from your mind. but it can nonetheless be an interesting exercise to run a mod-
estly complex program through the pro   ler because, like the debugger   s backtrace,
its output provides another useful view of how functions call each other.

as with the debugger, once the pro   ler points out where the most time is being
taken by your program, what you need to do to alleviate the bottleneck often be-
comes very obvious.

output.

optimization

216

chapter 6

gsl_stats march 24, 2009

long as you are optimizing, why not go all out?]

it run faster. for example, it may change the order in which lines of

theg

 compiler can do a number of things to your code to make
code are executed, or if you assignx=y z, it may replace every instance ofx
withy z. to turn on optimization, use the	 3    ag when compiling withg

.
[that   s an    o    as in optimization, not a zero. there are also	 1 and	 2, but as
ofx has been replaced with something else, then you can not check its value. it
you may just have to re-scour your code to    nd the problem. thus, the	 3    ag
add the	 g switch to the make   le in the birds directory and check the tim-
ing of the three different versions. it may help to comment out the  i  f
function and run the simulation for more periods. how does the	 3    ag

also sometimes happens that you did not do your memory allocation duties quite
right, and things went ok without optimization, but suddenly the program crashes
when you have optimization on. a memory debugger may provide some clues, but

the problem with optimization, however, is that it makes debugging dif   cult. the
program jumps around, making stepping through an odd trip, and if every instance

is a    nal step, to be used only after you are reasonably con   dent that your code is
debugged.

q6.8

change the timings?

gsl_stats march 24, 2009

statistics

ii

gsl_stats march 24, 2009

gsl_stats march 24, 2009

7

distributions for description

this chapter covers some methods of describing a data set, via a number of strate-
gies of increasing complexity. the    rst approach, in section 7.1, consists of simply
looking at summary statistics for a series of observations about a single variable,
like its mean and variance. it imposes no structure on the data of any sort. the next
level of structure is to assume that the data is drawn from a distribution; instead of
   nding the mean or variance, we would instead use the data to estimate the param-
eters that describe the distribution. the simple statistics and distributions in this
chapter are already suf   cient to describe rather complex models of the real world,
because we can chain together multiple distributions to form a larger model. chap-
ter 8 will take a slightly different approach to modeling a multidimensional data
set, by projecting it onto a subspace of few dimensions.

7.1 moments

  estimator vocabulary a statistic is the output of a function that takes in

the    rst step in analyzing a data set is always to get a quick lay
of the land: where do the variables generally lie? how far do

they wander? as variable a goes up, does variable b follow?

data, typically of the form f : rn     r. that is, a
statistic takes in data and summarizes it to a single dimension. common statistics
include the mean of x, its variance, max(x), the covariance of x and y, or the
regression parameter   2 from a regression of x on y (which could be written in
the form   2(x, y)).

gsl_stats march 24, 2009

220

chapter 7

thus, many of the means of describing a data set, such as writing down its mean,
could be described as the generation of statistics. one goal of writing down a
statistic is dimension reduction: simply summarizing the data via a few human-
comprehensible summary statistics, such as the data set   s mean and variance.

another goal, which is more often the case, is to use the statistics of the data, x, to
estimate the same statistic of the population. let p signify the population. when
the us census bureau said in an august 2006 press release1 that 46.6 million
people in the united states have no health insurance, they meant that the count
of people in the current population survey that did not have health insurance (a
sample statistic) indicated that the count of people in the united states without
health insurance (a population statistic) was 46.6 million. is the estimate of the

statistic based on the sample data,          f (x), a valid estimate of the population
statistic,        f (p)? there are several ways to make this a precise question. for
example, as x grows larger, does            ? do there exist estimates of    that have
smaller variance than     ? after discussing some desirable qualities in an estimator,
we will begin with some simple statistics.

  evaluating an estimator

from a given population, one could take many possi-
ble samples, say x1, x2, . . . , which means that there

could be many possible calculated statistics,     1 = f (x1),     2 = f (x2), . . . .

there are many means of describing whether the collection of statistics     i (for
i = 1, i = 2, . . . ) is a precise and accurate estimate of the true value of   . you
will see in the sections to follow that intuitive means of estimating a population
statistic sometimes work on all of these scales at once, and sometimes fail.

    unbiasedness: the expected value of      (discussed in great detail below) equals the

true population value: e(     i) =   .

    variance: the variance is the expected value of the squared distance to the expected

value: e(cid:16)(     i     e(     ))2(cid:17). the variance is also discussed in detail below.
    mean squared error: mse of          e(            )2. below we will see that the mse
equals the variance plus the square of bias. so if      is an unbiased estimator of   ,
meaning that e(     ) =   , then the mse is equivalent to the variance, but as the bias
increases, the difference between mse and variance grows.

    ef   ciency: an estimator      is ef   cient if, for any other estimator     , m se(     )    
m se(     ). if      and      are unbiased estimators of the same   , then this reduces to
var(     )     var(     ), so some authors describe an ef   cient estimator as the unbiased

136, 29 august 2006,h   ://www.
e  	 .g v/  e  	re ea e/www/ e ea e /a 
hive /i 
  e_
wea  h/007419.h   .

1us census bureau,    income climbs, poverty stabilizes, uninsured rate increases,    press release #cb06-

gsl_stats march 24, 2009

distributions for description

221

estimator with minimum variance among all unbiased estimators.
we test this using inequality 10.1.7 (page 333), that the variance must be greater
than or equal to the cram  r   rao lower bound. if var(     ) equals the crlb, we
know we have a minimum variance.

    blue:      is the best linear unbiased estimator if var(     )     var(     ) for all linear
unbiased estimators     , and      is itself a linear function and unbiased.
    aymptotic unbiasedness: de   ne     n = f (x1, . . . , xn). for example,     1 = x1,
    2 = (x1 + x2)/2,     3 = (x1 + x2 + x3)/3, . . . . then limn       e(     n) =   .
clearly, unbiasedness implies asymptotic unbiasedness.
    consistency: plim(     n) =   , i.e., for a    xed small   , limn       p (|     n     | >   ) = 0.
equivalently, limn       p ((     n       )2 >   2) = 0.
one can verify consistency using chebychev   s inequality; see, e.g., casella &
berger (1990, p 184).
in a sense, consistency is the asymptotic analog to the low mse condition. if mse
goes to zero, then consistency follows (but not necessarily vice versa).
however, a biased estimator or a high-variance estimator may have a few things
going for it, but an inconsistent estimator is just a waste. you could get yourself
two near-in   nite samples and    nd that      is different for each of them   and then
what are you supposed to pick?

    asymptotic ef   ciency: var(     )     the cram  r   rao lower bound. this makes sense
only if        s asymptotic distribution has a    nite mean and variance and      is consis-
tent.

expected value

say that any given value of x has id203 p(x). then if f (x)
is an arbitrary function,

e (f (x)) =z   x

f (x)p(x)dx.

the p(x)dx part of the integral is what statisticians call a measure and everyone
else calls a weighting. if p(x) is constant for all x, then every value of x gets equal
weighting, as does every value of f (x). if p(x1) is twice as large as p(x2), meaning
that we are twice as likely to observe x1, then f (x1) gets double weighting in the
integral.

if we have a vector of data points, x, consisting of n elements xi, then we take
each single observation to be equally likely: p(xi) = 1
n ,    i. the expected value
for a sample then becomes the familiar calculation

e(x) = pi xi

n

,

and (given no further information about the population) is the best unbiased esti-
mator of the true mean   .2

2the term expected value implies that the mean is what we humans actually expect will occur. but if i have

gsl_stats march 24, 2009

222

chapter 7

variance and its dissections the variance for discrete data is the familiar
formula of the mean of the squared distance to
the average. let x indicate the mean of the data vector x; then the best unbiased
estimate of the variance of the sample is

var(x) =

1

nxi

(xi     x)2 .

(7.1.1)

degrees of freedom

rather than calculating the variance of a sample, say
that we seek the variance of a population, and have
only a sample from which to estimate the variance.
the best unbiased estimate of the variance of the pop-
ulation is

cvar(x) =

1

n     1 x

i

(xi     x)2 .

(7.1.2)

we can think of the sum being divided by n     1 in-
stead of n (as in equation 7.1.1) because there are only
n     1 random elements in the sum: given the mean x
and n     1 elements, the nth element is deterministi-
cally solved. that is, there are only n     1 degrees of
freedom. an online appendix to this book provides a
more rigorous proof that equation 7.1.2 is an unbiased
estimator of the population variance.

as n        , 1/n     1/(n     1), so both the esti-
mate of variance based on 1/n and on 1/(n     1) are
asymptotically unbiased estimators of the population
variance.

the number of degrees of freedom (df ) will appear in
other contexts throughout the book. the df indicates
the number of dimensions in which the data could
possibly vary. with no additional information, this is
just the number of independently drawn variables, but
there may be more restrictions on the data. imagine
three variables, which would normally have three di-
mensions, with the added restriction that x1 + 2x2 =
x3. then this de   nes a plane (which happens to be
orthogonal to the vector (1, 2,    1) and goes through
the origin). that is, by adding the restriction, the data
points have been reduced to a two-dimensional sur-
face. for the sample variance, the restriction is that the
mean of the sample is     .

the square root of the variance is
called the standard deviation. it is
useful because the normal distribu-
tion is usually described in terms
of the standard deviation (  ) rather
than the variance (  2). outside of
the context of the normal, the vari-
ance is far more common.

the variance is useful in its own
right as a familiar measure of dis-
persion. but it can also be decom-
posed various ways, depending on
the situation, to provide still more
information, such as how much of
the variance is due to bias, or how
much variation is explained by a
id75. since information
is extracted from the decomposi-
tion of variance time and time again
throughout classical statistics, it is
worth going over these various dis-
sections.

recall from basic algebra that the
form (x + y)2 expands to x2 + y2 +
2xy. in some special cases the un-
sightly 2xy term can be eliminated
or merged with another term, leav-
ing the pleasing result that (x +
y)2 = x2 + y2.

a one in a million chance of winning a two million dollar lottery, there are no states of the world where i am
exactly two dollars wealthier. further, research pioneered by kahneman and tversky (e.g., kahneman et al.
(1982)) found that humans tend to focus on other features of a id203 distribution. they will consider events
with small id203 to either have zero id203 or a more manageable value (e.g., reading p = 1e   7 as
p = 1e   3). or, they may assume the most likely state of the world occurs with certainty. bear in mind that
human readers of your papers may be interested in many de   nitions of expectation beyond the mean.

gsl_stats march 24, 2009

distributions for description

223

throughout the discussion below   x = e[x]; that is   x is constant for a given data
set. the expectation of a constant is the constant itself, so e[  x] is simply   x; and
e[y2   x] would expand to 1

i=1 y2

i =   x    e[y2].

i=1(cid:2)y2
npn

i   x(cid:3) =   x    1

npn

the    rst breakdown of variance is the equation as above:

var(x) = e(cid:2)(x       x)2(cid:3)

= e(cid:2)x2     2x  x +   x2(cid:3)
= e[x2]     e[2x  x] + e[  x2]
= e[x2]     2e[x]2 + e[x]2
= e[x2]     e[x]2.

read this as: var(x) is the expectation of the squared values minus the square of
the expected value. this form simpli   es many transformations and basic results of
the type that frequently appear in id203 homework questions.

q7.1

write a function to display var(x), e[x2], e[x]2, and e[x2]   e[x]2 for any
input data, then use it to verify that the    rst and last expressions are equal
for a few columns of data selected from any source on hand.

mean squared error

the next breakdown appears with the mean squared error.
say that we have a biased estimate of the mean,   x; if you had
the true mean   x, then you could de   ne the bias as (  x      x). it turns out that the mse
is a simple function of the true variance and the bias. the value can be derived by
inserting      x +   x = 0 and expanding the square:

m se     e(cid:2)(x       x)2(cid:3)

= eh((x       x) + (  x       x))2i
= e [(x       x)]2 + 2e [(x       x)(  x       x)] + eh(  x       x)2i
= var(x)     2    bias(  x)e (x       x) + bias(  x)2
= var(x) + bias(  x)2

in this case the middle term drops out because e(x      x) = 0, and the mse breaks
down to simply being the variance of x plus the bias squared.

gsl_stats march 24, 2009

224

chapter 7

within group/among group variance

the next breakdown of variance, common
in anova estimations (where anova is
short for analysis of variance), arises when the data is divided into a set of groups.
then the total variance over the entire data set could be expressed as among group
variance and within group variance. above, x consisted of a homogeneous se-
quence of elements xi, i = {1, . . . , n}, but now break it down into subgroups xij,
where j indicates the group and i indicates the elements within the group. there
is thus a mean   xj for each group j, which is the simple mean for the nj elements
in that group. the unsubscripted   x continues to represent the mean of the entire
sample. with that notation in hand, a similar breakdown to those given above can
be applied to the groups:

1

1

=

=

var(x) = e(cid:2)(x       x)2(cid:3)
nxj " njxi=1
nxj " njxi=1
nxj " njxi=1
nxj

=

=

1

1

(xij       xj +   xj       x)2#
(xij       xj)2 +

njxi=1
(  xj       x)2 + 2
(  xj       x)2#
njxi=1
nxj (cid:2)nj(  xj       x)2(cid:3)

1

(xij       xj)2 +

[nj var(xj)] +

(xij       xj)(  xj       x)#
njxi=1

(7.1.3)

(7.1.4)

for a given j, andpnj

the transition from equation 7.1.3 to 7.1.4 works because (  xj       x) is constant
i=1(xij       xj) =   xj       xj = 0. once again, the unsightly
middle term cancels out, and we are left with an easily interpretable    nal equation.
in this case, the    rst element is the weighted mean of within-group variances, and
the second is the weighted among-group variance, where each group is taken to be
one unit at   xj, and then the variance is taken over this set of group means.

theda a	 e   .db set gives average weekday passenger boardings at every sta-
    line 20 is the query to join the ide   and i e  tables. the parens mean that it

tion in the washington metro subway system, from the founding of the system in
november 1977 to 2007.3 the system has    ve lines (blue, green, orange, red,
yellow), and listing 7.1 breaks down the variance of ridership on the washington
metro into within-line and among-line variances.

3as a washington-relevant detail, all post-   77 measurements were made in may, outside the peak tourist

season.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

gsl_stats march 24, 2009

distributions for description

225

#include <apop.h>

void variance_breakdown(char *table, char *data, char *grouping){

apop_data* aggregates = apop_query_to_mixed_data("mmw",

"select var_pop(%s) var, avg(%s) avg, count(*) from %s group by %s",
data, data, table, grouping);
apop_col_t(aggregates, "var", vars);
apop_col_t(aggregates, "avg", means);
double total= apop_query_to_   oat("select var_pop(%s) from %s", data, table);
double mean_of_vars = apop_vector_weighted_mean(vars, aggregates   >weights);
double var_of_means = apop_vector_weighted_var(means, aggregates   >weights);
printf("total variance: %g\n", total);
printf("within group variance: %g\n", mean_of_vars);
printf("among group variance: %g\n", var_of_means);
printf("sum within+among: %g\n", mean_of_vars + var_of_means);

}

}

    the query on line 9 pulls the total variance   the total sum of squares   and the

    lines 10 and 11 take the weighted mean of the variances and the weighted variance

int main(){

where lines.station =riders.station)";

variance_breakdown(joinedtab, "riders", "line");

query on lines 4   6 gets the within-group variances and means.

listing 7.1 decomposing variance between among-group and within-group. online source:

apop_db_open("data   metro.db");
char joinedtab[] = "(select riders/100 as riders, line from riders, lines \

a   gwi hi .
.
can comfortably be inserted into af    clause, as in the query on line four.4
rewrite the program to use theda a	wb.db data set (including the
 a  e 
is clearly inef   cient. therefore, when using functions likea   _a  va in the wild,    rst run a
 ea e ab e
... query to join the data, perhaps index the table, and then send that table to thea   _a  va function.

within-group and among-group variance is interesting by itself. to give one ex-
ample, glaeser et al. (1996, equation 8) break down the variance in crime into

table) to break down the variance in gdp per capita into within-class and
among-class variance.

4using a subquery like this may force the sql interpreter to re-generate the subtable for every query, which

indeed add up to the total variance.

of the means.

    lines 14   17 print the data to screen, showing that these two sub-calculations do

q7.2

gsl_stats march 24, 2009

226

chapter 7

within-city and among-city variances, and    nd that among-city variance is orders
of magnitude larger than within-city variance.

returning to the metro data, we could group data by year, and look for within- and
among-group variation in that form, or we could group data by line and ask about
within- and among-group variation there.

#include <apop.h>

}

int main(){

apop_data_show(apop_anova(joinedtab, "riders", "line", "year"));

from riders, lines \
where riders.station = lines.station)";

apop_db_open("data   metro.db");
char joinedtab[] = "(select year, riders, line \

listing 7.2 produce a two-way anova table breaking variance in per-station passenger boardings
into by-year effects, by-line effects, an interaction term, and the residual. online source:

 e   a  va.
.
how much variation would remain? withg  	 i g1 andg  	 i g2, there are
three ways to group the data:g  	 byg  	 i g1 [(green line), (red line), . . . ],
g  	 byg  	 i g2 [(1977), (1978), . . . , (2007)], and the interaction:g  	 
byg  	 i g1 g  	 i g2 [(green line, 1977), (red line, 1977), . . . (green
break down the total sum of squares into (weighted sum of squares,g  	 i g1) +
(weighted sum of squares,g  	 i g2) + (weighted sum of squares,g  	 i g1,
g  	 i g2) + (weighted sum of squares, residual).

listing 7.2 produces an anova table, which is a spreadsheet-like table giving the
within-group and among-group variances. the form of the table dates back to the
mid-1900s   anova is basically the most complex thing that one can do without
a matrix-inverting computer, and the tabular form facilitates doing the calculation
with paper, pencil, and a desk calculator. but it still conveys meaning even for
those of us who have entirely forgotten how to use a pencil.

the    rst three rows of the output present the between-group sum of squares. that
is, if we were to aggregate all the data points for a given group into the mean,

line, 2007), (red line, 2007)]. using algebra much like that done above, we can

we can compare the weighted grouped sums to the residual sum, which is listed as
the f statistic in the anova table. as will be discussed in the chapter on testing
(see page 309), an f statistic over about two is taken to indicate that the grouping
explains more variation than would be explained via a comparable random group-
ing of the data. the output of this example shows that the grouping by year is very
signi   cant, as is the more re   ned interaction grouping by line and year, but the

gsl_stats march 24, 2009

distributions for description

227

grouping by line is not signi   cant, meaning that later studies may be justi   ed in
not focusing on how the subway stations are broken down into lines.

q7.3

most stations are on multiple lines, so a station like metro center is included
in the blue, orange, and red groups. in fact, the yellow line has only two
stations that it doesn   t share with other lines. [you can easily    nd an online
map of the washington metro to verify this.] this probably causes us to un-
derestimate the importance of the per-line grouping. how would you design
a grouping that puts all stations in only one group? it may help in implemen-
tation to produce an intermediate table that presents your desired grouping.
does the anova using your new grouping table show more signi   cance to
the line grouping?

by changing the second group in the code listing from"yea " to u  , we would
(weighted sum of squares,g  	 i g1) + (weighted sum of squares, residual).
  regression variance next, consider the ols model, which will be detailed in

the residual sum of squares is therefore larger, the df of the residual is also larger,
and in this case the overall change in the f statistic is not great.

get a one-way anova, which breaks down the total sum of squares into just

served value to the estimated value plus the error: y = yest +   .

section 8.2.1. in this case, we will break down the ob-

var(y) = e(cid:2)(yest +          y)2(cid:3)

= e(cid:2)(yest       y)2(cid:3) + e[  2] + 2e [(yest       y)  ]

it will be shown below that yest   =   olsx   = 0 (because x   = 0), and      = 0,
so e[  y  ] =   ye[  ] = 0. thus, the 2e[. . . ] term is once again zero, and we are left
with

var(y) = e(cid:2)(yest       y)2(cid:3) + e[  2]

(7.1.5)

gsl_stats march 24, 2009

228

chapter 7

make the following de   nitions:

sst     total sum of squares

= var(y)

= e(cid:2)(y       y)2(cid:3)
= e(cid:2)(yest       y)2(cid:3)

= e[  2]

ssr     regression sum of squares

sse     sum of squared errors

then the expansion of var(y) in equation 7.1.5 could be written as

sst = ssr + sse.

this is a popular breakdown of the variance, because it is relatively easy to cal-
culate and has a reasonable interpretation: total variance is variance explained by
the regression plus the unexplained, error variance. as such, these elements will
appear on page 311 with regard to the f test, and are used for the common coef   -
cient of determination, which is an indicator of how well a regression    ts the data.
it is de   ned as:

r2    

ssr
sst
= 1    

sse
sst

.

you will notice that the terminology about the sum of squared components and the
use of the f test matches that used in the anova breakdowns, which is not just
a coincidence: in both cases, there is a portion of the data   s variation explained by
the model (grouping or regression), and a portion that is unexplained by the model
(residual). in both cases, we can use this breakdown to gauge whether the model
explains more variation than would be explained by a random grouping. the exact
details of the f test will be delayed until the chapter on hypothesis testing.

covariance the population covariance is   2

xy = 1

expansion above to prove this.] the variance is a special case where x = y.

is equivalent to e[xy]     e[x]e[y]. [q: re-apply the    rst variance

npi(xi     x)(yi     y), which

as with the variance, the unbiased estimate of the sample covariance is s2
n   1 , i.e.,pi(x     x)(y     y) divided by n     1 instead of n.
xy    n
  2
given a vector of variables x1, x2, . . . xn, we typically want the covariance of
every combination. this can neatly be expressed as a matrix

xy =

gsl_stats march 24, 2009

distributions for description

229

  2
12
  2
2

  2
1
  2
21
...
n1   2
  2
n2

               

. . .   2
1n
. . .   2
2n
. . .
. . .   2
n

,

               

where the diagonal elements are the variances (i.e., the covariance of xi with itself
for all i), and the off-diagonal terms are symmetric in the sense that   2
ji for
all i and j.

ij =   2

correlation and cauchy   schwarz:

the correlation coef   cient (sometimes called
the pearson correlation coef   cient) is

  xy    

  xy
  x  y

.

by itself, the statistic is useful for looking at the relations among columns of data,
and can be summarized into a matrix like the covariance matrix above. the cor-
relation matrix is also symmetric, and has ones all along the diagonal, since any
variable is perfectly correlated with itself.

the cauchy   schwarz inequality, 0       2     1, puts bounds on the correlation
coef   cient. that is,    is in the range [   1, 1], where    =    1 indicates that one
variable always moves in the opposite direction of the other, and    = 1 indicates
that they move in perfect sync.5

tive information about a data set; produce it viaa   _da a_
   e a i  . the

the matrix of correlations is another popular favorite for getting basic descrip-

correlation matrix will be the basis of the cram  r   rao lower bound on page 333.

more moments given a continuous id203 distribution from which the data
was taken, you could write out the expectation in the variance

equation as an integral,

var(f (x)) = e(cid:18)(cid:16)f (x)     f (x)(cid:17)2(cid:19)
=z   x(cid:16)f (x)     f (x)(cid:17)2

p(x)dx.

5it would be a digression to prove the cauchy   schwarz inequality here, but see h  lder   s inequality in any

id203 text, such as feller (1966, volume ii, p 155).

gsl_stats march 24, 2009

230

chapter 7

similarly for higher powers as well:

skew (f (x))   z   x(cid:16)f (x)     f (x)(cid:17)3
kurtosis (f (x))   z   x(cid:16)f (x)     f (x)(cid:17)4

p(x)dx

p(x)dx.

these three integrals are central moments of f (x). they are central because we
subtracted the mean from the function before taking the second, third, or fourth
power.6

1 = k/(  2)2, k   

transformed moments

let s and k be the third and fourth central moments
as given here. some use a standardized moment for
kurtosis, which may equal k   
2 =
k/(  2)2     3, or whatever else the author felt would
be convenient. similarly, some call s     = s/(  2)3/2
the skew. these adjustments are intended to ease com-
parisons to the standard normal and to acommodate
differences in scale.

what information can we get from
the higher moments? section 9.1
will discuss the powerful central
limit theorem, which says that if a
variable represents the mean of a set
of independent and identical draws,
then it will have an n (  ,   ) distri-
bution, where    and    are unknowns
that can be estimated from the data.
these two parameters completely
de   ne the distribution: the skew of
a normal is always zero, and the
kurtosis is always 3  4. if the kurto-
sis is larger, then this often means
that the assumption of independent
draws is false   the observations are
interconnected. one often sees this among social networks, stock markets, or other
systems where independent agents observe and imitate each other.

the only way to know what a given source means
when it refers to skew and kurtosis is to look up the
de   nitions. the gsl uses k   
2 (because engineers are
probably comparing their data to a standard normal);
apophenia uses k (because the corrections can add
complication in situations outside the normal distri-
bution, and is easy to make when needed).

positive skew indicates that a distribution is upward leaning, and a negative skew
indicates a downward lean. kurtosis is typically put in plain english as fat tails:
how much density is in the tails of the distribution? for example, the kurtosis of
an n (0, 1) is three, while the kurtosis of a student   s t distribution with n degrees
of freedom is greater than three, and decreases as n        , converging to three
(see page 365 for a full analysis). an un-normalized kurtosis > 3  4 is known as
leptokurtic and < 3  4 is known as platykurtic; see figure 7.3 for a mnemonic.

the caveats about unbiased estimates of the sample versus population variance
(see box, page 222) also hold for skew and kurtosis: calculating the mean as done in
the de   nitions above leads to a biased estimate of the population skew or kurtosis,
but there are simple corrections that can produce an unbiased estimate. an online

6the central    rst moment is always zero; the non-central second, third, . . . , moments are dif   cult to interpret
and basically ignored. since there is no ambiguity, some authors refer to the useful moments as the nth moment,
n     {1, 2, 3, 4}, and leave it as understood when the moment is central or non-central.

gsl_stats march 24, 2009

distributions for description

231

figure 7.3 leptokurtic, mesokurtic and platykurtic, illustration by gosset in biometrika (student,

1927, p 160). in the notation of the time,   2     kurtosis/(variance squared).

appendix to this book offers a few more facts about central moments, and derives
the correction factors.

but in all cases, the population vs sample detail is relevant only for small n. efron
& tibshirani (1993, p 43) state that estimating variance via n is    just as good   
as estimating it via n     1, so there is highly-esteemed precedent for ignoring this
detail. for the higher moments, the sample and population estimates converge even
more quickly.

coding it given a vector, apophenia provides functions to calculate most of the

above, e.g.:

apop_data *set = gather_your_data_here();
apop_data *corr = apop_data_correlation(set);
apop_col(set, 0, v1);
apop_col(set, 1, v2);
double mean1 = apop_vector_mean(v1);
double var1 = apop_vector_var(v1);
double skew1 = apop_vector_skew(v1);
double kurt1 = apop_vector_kurtosis(v1);
double cov = apop_vector_cov(v1, v2);
double cor = apop_vector_correlation(v1, v2);
apop_data_show(apop_data_summarize(set));

the last item in the code,a   _ a  ix_ 	  a ize, produces a table of some

summary statistics for every column of the data set.

gsl_stats march 24, 2009

232

chapter 7

query will

make use of it.

    write a query that pulls the number of males per 100 females and

this is not the place to go into details about statistically sound means of weighting

your data may be aggregated so that one line of data represents multiple observa-
tions. for example, sampling ef   ciency can be improved by sampling subpopula-
tions differently depending upon their expected variance (s  rndal et al., 1992). for
this and other reasons, data from statistical agencies often includes weightings.

data, but if you have a separate vector with weights, you can usea   _ve
   _	
weigh ed_ ea ,a   _ve
   _weigh ed_va , et cetera, to use those weights.
or, if youra   _da a set   sweigh   vector is    lled,a   _da a_ 	  a ize will
the population density from the census data (da a	
e  	 .db). the
    join together thege g a hy andde    tables by county num-
    write a functionv id 	  a ize_ ai ed_da a 
ha     that
    write a ai    that sends your query to the above function, and run
    add a line to ai    to send that query to your summarizing function

    write another query that pulls the ratio of (median income for full-
time workers, female)/(median income for full-time workers, male)
and the population density.

takes in a query that produces a two-column table, and outputs some
of the above summary information about both columns, including the
mean, variance, and correlation coef   cients.

the program. is population density positively or negatively correlated
to males per female?

    exclude states and the national total, and

    return a two-column table.

ber, and

q7.4

as well. how is the new pair of variables correlated?

quantiles

the mean and variance can be misleading for skewed data. the    rst
option for describing a data set whose distribution is likely ab-normal is

to plot a histogram of the data, as per page 172.

a numeric option is to print the quartiles, quintiles, or deciles of the data. for
quartiles, sort the data, and then display the values of the data points 0%, 25%,
50%, 75%, and 100% of the way through the set. the 0% value is the minimum of

gsl_stats march 24, 2009

233

distributions for description

apop_data_sort(my_data, 2,    d   );
gsl_vector_sort(my_vector);

the data set, the 100% value is the maximum, and the 50% value is probably the
median (see below). for quintiles, print the values of data points 0%, 20%, 40%,
60%, 80%, and 100% of the way through the set, and for deciles, print the values
every ten percent.

sorting your data is simple. if you have ana   _da a set and ag  _ve
   , then
would sort y_da a in place so that column 2 is in    d   escending order, and sort the
vector in place to ascending order, sog  _ve
   _ge   y_ve
    0  is the
minimum of the data,g  _ve
   _ge   y_ve
     y_ve
   	> ize  is the
maximum, andg  _ve
   _ge   y_ve
     y_ve
   	> ize/2  is about
alternatively, the functiona   _ve
   _ e 
e  i e  takes in ag  _ve
   
character describing the rounding method    	  for rounding up, d  for rounding
down, and a  for averaging. since the number of elements in a data set is rarely
through the set, so which is the tenth percentile? if you specify 	 , then it is the
eleventh data point; if you specify d  then it is the tenth data point, and if you
specify a , then it is the simple average of the two.

divisible by a hundred and one, the position of most percentile points likely falls
between two data points. for example, if the data set has 107 points, then the tenth
data point is 9.47% through the data set, and the eleventh data point is 10.38%

and returns the percentiles   the value of the data point 0%, 1%, . . . , 99%, 100%
of the way through the data. it takes in two arguments: the data vector, and a

the median.

the standard de   nition of the median is that it is the middle value of the data point
if the data set has an odd number of elements, and it is the average of the two data
points closest to the middle if the data set has an even number of elements. thus,
here is a function to    nd the median of a data set. it    nds the percentiles using the
averaging rule for interpolation, marks down the 50th percentile, then cleans up
and returns that value.

double    nd_median(gsl_vector *v){

double *pctiles = apop_vector_percentiles(v,    a   );
double out = pctiles[50];
free(pctiles);
return out;

}

chapter 7

gsl_stats march 24, 2009

write a function with headerd 	b e h w_ 	a  i e  g  _ve
   
 v 
ha   	 di g_ e h d i  divi i     that passesv and
  	 di g_ e h d toa   _ve
   _ e 
e  i e , and then prints a ta-
ble of selected quantiles to the screen. for example, ifdivi i   ==4,
print quartiles, ifdivi i   ==5, print quintiles, ifdivi i   ==10, print
use your function to print the deciles for country incomes fromda a	wb.
write a function that takes in a vector of data points, appliesa   _	
ve
   _ e 
e  i e  internally, and returns the trimean. how does the

the trimean is 1
4 the sum of the    rst quartile, third quartile, and two times
the median (tukey, 1977, p 46). it uses more information about the distri-
bution than the median alone, but is still robust to extreme values (unlike
the mean).

deciles, et cetera.
on page 88 you tabulated gdp per capita for the countries of the world.

234

q7.5

q7.6

trimean of gdp per capita compare to the mean and median, and why?

see also page 319, which compares percentiles of the data to percentiles of an
assumed distribution to test whether the data were drawn from the distribution.

   the most basic means of describing data is via its moments. the basic
moments should be produced and skimmed for any data set; in simple
cases, there is no need to go further.

   the variance can often be decomposed into smaller parts, thus reveal-

   the mean and variance are well known, but there is also information

   it is also important to know how variables interrelate, which can be

summarized using the correlation matrix.

in higher moments   the skew and kurtosis.

ing more information about how a data set   s variation arose.

formation. notably,a   _da a_ 	  a ize produces a summary of
tribution using quartiles or quintiles; to do so, usea   _ve
   _	
 e 
e  i e .

each column of a data set.

   you can get a more detailed numerical description of a data set   s dis-

   there is a one-line function to produce each of these pieces of in-

gsl_stats march 24, 2009

distributions for description

235

7.2 sample distributions

here are some distributions that an observed
variable may take on. they are not just here so
you can memorize them before the next statistics test. each has a story attached,
which is directly useful for modeling. for example, if you think that a variable is
the outcome of n independent, binary events, then the variable should be modeled
as a binomial distribution, and once you estimate the parameter to the distribution,
you will have a full model of that variable, that you can even test if so inclined.
table 7.4 presents a table of what story each distribution is telling. after the catalog
of models, i will give a few examples of such modeling.

the distribution

the story

bernoulli

binomial

hypergeometric

normal/gaussian

lognormal

multinomial

multivariate

normal

a single success/failure draw,    xed p.
what are the odds of getting x successes from n
bernoulli draws with    xed p?
what are the odds of getting x successes from n
bernoulli draws, where p is initially    xed, but draw-
ing is without replacement?

i=1 xij/n, then   j    

normal.

binomial as n        ; if   j    pn
if   j    qn
. . . , pm,pm

multinomial as n        .

i=1 pi = 1.

i=1 xij, then   j     lognormal.

n draws from m possibilities with probabilities p1,

negative binomial how many bernoulli draws until n successes?

poisson

gamma

exponential

beta

uniform

given    events per period, how many events in t pe-
riods?

the    negative poisson   : given a poisson setup, how
long until n events?
a proportion    of the remaining stock leaves each
period; how much is left at time t?
a versatile way to describe the odds that p takes on
any value     (0, 1).
no information but the upper and lower bounds.

table 7.4 every id203 distribution tells a story.

common distributions of statistical parameters (as opposed to natural populations)
are discussed in section 9.2.

gsl_stats march 24, 2009

236

chapter 7

bernoulli and poisson events

the core of the system is an event, which some-
times happens and sometimes does not. some peo-
ple have a disease, some do not; some days it rains, some days it does not. events
add up to more-or-less continuous quantities: some cities see a 22% chance of rain
on a given day, and some see a 23.2% chance; some populations have high rates of
disease and some do not.

from there, there are variants: instead of asking how many successes we will see
in n trials (the binomial and poisson distributions), we can ask how many trials we
can expect to make to get n successes (the negative binomial and gamma distri-
butions). we can look at sampling without replacement (hypergeometric). we can
look at the case where the number of trials goes to in   nity (normal) or aggregate
trials and take their product (lognormal). in short, a surprisingly wide range of
situations can be described from the simple concept of binary events aggregated in
various ways.

the snow   ake problem for a controlled study (typical of physical sciences), the
claim of a    xed id203 (p or   ) is often plausible, but
for most social science experiments, where each observation is an individual or a
very different geographic region, the assumption that all observations are identical
is often unacceptable   every snow   ake is unique.

it may seem like the    xed-p and    xed-   models below are too simple to be appli-
cable to many situations   and frankly, they often are. however, they can be used
as building blocks to produce more descriptive models. section 8.2.1 presents a
set of id75 models, which handle the snow   ake problem well, but
throw out the id203 framework presented in this chapter. but we can solve
the snow   ake problem and still make use of simple id203 models by em-
bedding a linear model inside a distribution: let pi differ among each element i,
and estimate pi using a linear combination of element i   s characteristics. page 288
covers examples of the many possibilities provided by models-within-models.

statistics and their estimators

    the catalog in this section includes the three most typical items one would want
from a distribution: a random number generator (rng) that would produce data
with the given distribution, a id203 density function (pdf), and a cumulative
density function (cdf).7

7this is for the case when the independent variable is continuous. when it is discrete, the pdf is named a
id203 mass function (pmf) and the cdf a cumulative mass function (cmf). for every result or statement
about pdfs, there is an analogous result about pmfs; for every result or statement about cdfs, there is an
analogous result about cmfs.

gsl_stats march 24, 2009

distributions for description

237

    the catalog also includes the expected value and variance of these distributions,
which are distinct from the means and variances to this point in a key manner:
given a data set, the mean is a statistic   a function of data of the form f (x). mean-
while, given a model, the mean and variance of a draw are functions of the param-
eters, e.g., given a binomial distribution with parameters n and p, e(x|n, p) = np.
of course, we rarely know all the parameters, so we are left with estimating them
from data, but our estimate of p,   p is once again a function of the data set on hand.
we will return to this back-and-forth between estimates from data and estimates
from the model after the catalog of models.

g  _  g, which will be named .

    the full details of rng use will be discussed in chapter 11, but the rng functions
are included in the catalog here for easy reference; each requires a pointer to a

the bernoulli family the    rst set of distributions are built around a narrative
of drawing from a pool of events with a    xed probabil-
ity. the most commonly-used example is    ipping a coin, which is a single event
that comes out heads with id203 p = 1/2. but other examples abound: draw
one recipient from a sales pitch out of a list of such people and check whether he
purchased or did not purchase, or pull a single citizen from a population and see if
she was the victim of a crime. for one event, a draw can take values of only zero
or one; this is known as a bernoulli draw.

bernoulli

the bernoulli distribution represents the result of one bernoulli draw,
meaning that p (x = 1|p) = p, p (x = 0|p) = 1    p, and p (x = anything

else |p) = 0. notice that e(x|p) = p, even though x can be only zero or one.

p (x, p) = px(1     p)(1   x), x     {0, 1}

= gsl_ran_bernoulli_pdf(x, p)

e(x|p) = p
var(x|p) = p(1     p)

rn g : gsl_ran_bernoulli(r, p)

binomial now take n bernoulli draws, so we can observe between 0 and n events.
the output is now a count: how many people dunned by a telemarketer
agree to purchase the product, or how many crime victims there are among a    xed
population. the id203 of observing exactly k events is p(k)     binomial(n, p).
counting x successes out of n trials is less than trivial. for example, there are six
ways by which two successes could occur over four trials: (0, 0, 1, 1), (0, 1, 0, 1),

gsl_stats march 24, 2009

p = 0.1
p = 0.3
p = 0.6
p = 0.9

0.3

0.25

0.2

0.15

0.1

0.05

)
n

,
p
,

x
(
p

chapter 7

n = 10
n = 30
n = 60
n = 100

0 10 20 30 40 50 60 70 80 90 100

x

0

0

10

20

40

50

60

30
x

238

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

)
n

,
p
,

x
(
p

figure 7.5 left: the binomial distribution with n = 100 and various values of p. right: the bi-
nomial distribution with p = 0.4 and various values of n. as n grows, the distribution

approaches an n (np,pnp(1     p)).

(0, 1, 1, 0), (1, 1, 0, 0), (1, 0, 0, 1), or (1, 0, 1, 0), and the model underlying the bi-
nomial model treats them all equally.

x elements that can be pulled from n objects. the equation is

the form of the distribution therefore borrows a counting technique from combi-

natorics. the notation(cid:0) n

and the function isg  _ f_
h   e   x  (in the gsl   s special functions sec-

x(cid:1) indicates n choose x, the number of unordered sets of

x(cid:19) =
(cid:18)n

x!(n     x)!

n!

,

tion). for example, we could get exactly thirty successful trials out of a hundred

in(cid:0) 100
30(cid:1) ways (    2.94e25).

combinatorics also dictates the shape of the curve. there is only one way each to
list four zeros or four ones   (0, 0, 0, 0) and (1, 1, 1, 1)   and there are four ways
to list one one   (0, 0, 0, 1), (0, 0, 1, 0), . . .    and symmetrically for one zero. in
order, the counts for zero through four ones are 1, 4, 6, 4, and 1. this simple
counting scheme already produces something of a bell curve. returning to coin-
   ipping, if p = 1/2 and the coin is    ipped 100 times (n = 100), p(50 heads) is
relatively high, while p(0 heads) or p(100 heads) is almost nil.

in fact, as n        , the id203 distribution approaches the familiar normal
distribution with mean np and variance np(1     p), as in figure 7.5. assuming
that every telemarketer   s id203 of selling is equal, we expect that a plot of
many months    telemarketer results will look like a normal distribution, with many
telemarketers successfully selling to np victims, and others doing exceptionally
well or poorly. the assumption that every telemarketer is equally effective can
even be tested, by checking for digression from the normal distribution.

gsl_stats march 24, 2009

distributions for description

239

p (x, p, n) =(cid:18)n

x(cid:19) px(1     p)(n   x)

= gsl_ran_binomial_pdf(x, p, n)

e(x|n, p) = np
var(x|n, p) = np(1     p)

rn g : gsl_ran_binomial(r, p, n)

(7.2.1)

(7.2.2)

binomial(n, p).

    if x     bernoulli(p), then for the sum of n independent draws, pn
    as n        , binomial(n, p)     poisson(np) or n (np,pnp(1     p)).

i=1 xi    

since n is known and e(x) and var(x) can be calculated from the data, equations
7.2.1 and 7.2.2 are an oversolved system of two variables for one unknown, p.
thus, you can test for excess variance, which indicates that there are interactions
that falsify that the observations were iid (independent and identically distributed)
bernoulli events.

a variant: one p from n draws

the statistic of interest often differs from that cal-
culated in this catalog, but it is easy to transform
the information here. say that we multiply the elements of a set x by k. then the

k  x. the variance goes from being   2

mean goes from being   x    pi xi/n to being   kx    pi(kxi)/n = kpi xi/n =
kx     pi(kxi    
k  x)2/n = k2pi(xi       x)2/n = k2  2

x     pi(xi       x)2/n to   2

for example, we are often interested in estimating   p from data with a binomial-
type story. since e(x) = np under a binomial model, one could estimate   p as
e(x/n). as for the variance, let k in the last paragraph be 1/n; then the variance
of x/n is the original variance (var(x) = n  p(1       p)) times 1/n2, which gives
var(  p) =   p(1       p)/n.

k.

hypergeometric

say that we have a pool of n elements, initially consisting of
s successes and f     n     s failures. so n = s + f , and the
bernoulli id203 for the entire pool is p = s/n . what are the odds that we
get x successes from n draws without replacement? in this case, the id203 of
a success changes as the draws are made. the counting is more dif   cult, resulting
in a somewhat more involved equation.

gsl_stats march 24, 2009

240

chapter 7

p (x, s, f, n) =(cid:0)s

n   x(cid:1)
x(cid:1)(cid:0) f
(cid:0)n
n(cid:1)

e(x|n, s, f ) =
var(x|n, s, f ) =

ns
n
n( s

= gsl_ran_hypergeometric_pdf(x, s, f, n)

n )(1     s

n )(n     n)

(n     1)

rn g : gsl_ran_hypergeometric(r, s, f, n)

    as n        , drawing with and without replacement become equivalent, so the
hypergeometric distribution approaches the binomial.

multinomial

the binomial distribution was based on having a series of events that
could take on only two states: success/failure, sick/well, heads/tails,
et cetera. but what if there are several possible events, like left/right/center, or
africa/eurasia/australia/americas? the multinomial distribution extends the bi-
nomial distribution for such cases.

the binomial case could be expressed with one parameter, p, which indicated
success with id203 p and failure with id203 1     p. the multinomial

i=1 pi = 1.

case requires k variables, p1, . . . , pk, such thatpk
px1
1        pxk

p (x, p, n) =

n!

k

x1!       xk!

= gsl_ran_multinomial_pdf(k, p, n)

x1
x2
...
xn

p1
p2
...
pn
p1(1     p1)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
e               
= n               
               
n, p               
               
               
var(x|n, p) = n               
               
    there are two changes from the norm for the gsl   s functions. first,  and  are
arrays ofd 	b es of size k. ifpk

. . .
. . .
. . .
. . . pk(1     pk)
rn g : gsl_ran_multinomial(r, draws, k, p, out)

i=1 p[i] 6= 1, then the system normalizes the

   p1p2
p2(1     p2)

   p1pk
   p2pk

   pkp1

   pkp2

   p1p2

...

...

gsl_stats march 24, 2009

but this one draws k elements at a time, which will be put into the bins of the 	 

probabilities to make this the case. also, most rngs draw one number at a time,

distributions for description

241

array.

    you can verify that when k = 2, this is the binomial distribution.

normal you know and love the bell curve, aka the gaussian distribution. it is pic-

tured for a few values of   2 in figure 7.6.

as section 9.1 will explain in detail, any set of means generated via iid draws will
have a normal distribution. that is,

    draw k items from the population (which can have any nondegenerate distribu-

tion), x1, x2, . . . , xk. the normal approximation works best when k is large.

    write down the mean of those items,   xi.
    repeat the    rst two steps n times, producing a set x = {  x1,   x2, . . . ,   xn}.

then x has a normal distribution.

alternatively, the binomial distribution already produced something of a bell curve
with n = 4 above; as n        , the binomial distribution approaches a normal
distribution.

p (x,   ,   ) =

1

     2  

exp    

1

2(cid:20) (x       )

  

(cid:21)2!

= gsl_ran_gaussian_pdf(x, sigma) + mu

e(x|  ,   ) =   
var(x|  ,   ) =   2
n (y|  ,   )dy = gsl_cdf_gaussian_p(x     mu, sigma)

z x
      
z    
x n (y|  ,   )dy = gsl_cdf_gaussian_q(x     mu, sigma)

rn g : gsl_ran_gaussian(r, sigma) + mu

    if x     n (  1,   1) and y     n (  2,   2) then x + y     n (  1 +   2,p  2
    because the normal is symmetric, x     y     n (  1       2,p  2

    section 9.1 (p 297) discusses the central limit theorem in greater detail.

1 +   2

2).

1 +   2

2).

gsl_stats march 24, 2009

chapter 7

   = 0.75
   = 1
   = 1.75
   = 2.5

242

)
  

,

  

,

x
(
p

0.6

0.5

0.4

0.3

0.2

0.1

0
   3

   2

   1

0
x

1

2

3

figure 7.6 the normal distribution, with    = 0.

multivariate normal

just as the normal distribution is the extension of the bino-
mial, the multivariate normal is the extension of the multi-
nomial. say that we have a data set x that includes a thousand observations and
seven variables (so x is a 1000  7 matrix). let its mean be    (a vector of length
seven) and the covariance among the variables be    (a seven by seven matrix).
then the multivariate normal distribution that you could    t to this data is

exp(cid:0)    1

2 (x       )        1(x       )(cid:1)
p(2  )n det(  )

p (x,   ,   ) =

e(x|  ,   ) =   
var(x|  ,   ) =   

    when x has only one column and    = [  2], this reduces to the univariate normal

distribution.

lognormal

the normal distribution is apropos when the items in a sample are the
mean of a set of draws from a population, xi = (s1 + s2 +       + sk)/k.
but what if a data point is the product of a series of iid samples,   xi = s1  s2          sk?
then the log of   xi is ln(  xi) = ln(s1) + ln(s2) +        + ln(sk), so the log is a sum

gsl_stats march 24, 2009

distributions for description

243

)
  

,

  

,

x
(
p

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0

   = 0,    = 1
   =    0.5,    = 1
   = 1,    = 1
   = 0,    = 2

1

2

3

4

5

x

figure 7.7 the lognormal distribution.

of independent elements (i.e., n times a mean). very broadly, when a point in the
data set is produced by summing iid draws, it will be normally distributed; when
a point in the data set is produced by taking the product of iid draws, its log will be
normally distributed   i.e., it will have a lognormal distribution. the next section
will present an example. figure 7.7 shows some lognormal distributions.

a notational warning: in the typical means of expressing the lognormal distribu-
tion,    and    refer to the mean of the normal distribution that you would get if
you replaced every element x in your data set with ex, thus producing a standard
normal distribution. be careful not to confuse this with the mean and variance of
the data you actually have.

p(x,   ,   ) =

exp(cid:0)   (ln x       )2/(2  2)(cid:1)

x     2  

= ran_lognormal_pdf(x, mu, sigma)

e(x|  ,   ) = e     +   2
2    
    1)e(2  +  2)
var(x|  ,   ) = (e  2

rn g : ran_lognormal(rng, mu, sigma)

gsl_stats march 24, 2009

244

chapter 7

negative binomial

say that we have a sequence of bernoulli draws. how many
failures will we see before we see n successes? if p percent of
cars are illegally parked, and a meter reader hopes to write n parking tickets, the
negative binomial tells her the odds that she will be able to stop with n + x cars.

the form is based on the gamma function,

  (z) =z    

0

xz   1e   xdx

= gsl_sf_gamma(z).

you can easily verify that   (z + 1) = z  (z). also,   (1) = 1,   (2) = 1,   (3) = 2,
  (4) = 6, and generally,   (z) = (z     1)! for positive integers. thus, if n and
x are integers, formulas based on the gamma function reduce to more familiar
factorial-based counting formulas.

p (x, n, p) =

  (n + x)

  (x + 1)  (n)

pn(1     p)x

= gsl_ran_negative_binomial_pdf(x, p, n)

e(x|n, p) =

var(x|n, p) =

n(1     p)

p

n(1     p)

p2

rn g : gsl_ran_negative_binomial(rng, p, n)

rates a poisson process is very much like a bernoulli draw, but the unit of mea-
surement is continuous   typically a measure of time or space. it makes sense
to have half of an hour, but not half of a coin    ip, so the stories above based on
bernoulli draws are modi   ed slightly to allow for a rate of    events per hour to be
applied to half an hour or a week.

baltimore, maryland, sees about 110 days of precipitation per year, somewhat
consistently spaced among the months. but for how many days will it rain or snow
in a single week? the poisson distribution answers this question. we can also
do a count of weeks: how often does it rain once in a week, twice in a week, et
cetera? the exponential distribution answers this question. turning it around, if
we want a week with three rainfalls, how long would we have to wait? the gamma
distribution answers this question.

gsl_stats march 24, 2009

distributions for description

245

   = 1
   = 3
   = 10
   = 20

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

)
  

,

x
(
p

0

0

5

10

15
x

20

25

30

figure 7.8 the poisson distribution.

poisson

say that independent events (rainy day, landmine, bad data) occur at the
mean rate of    events per span (of time, space, et cetera). what is the

id203 that there will be x events in a single span?

we are assuming that events occur at a suf   ciently even rate that the same rate
applies to different time periods: if the rate per day is   1, then the rate per week is
7  1, and the rate per hour is   1/24. see figure 7.8.

p (x,   ) =

e       x

x!

= gsl_ran_poisson_pdf(x, lambda)

e(x|  ) =   
var(x|  ) =   

rn g : gsl_ran_poisson(r, lambda)

    as n        , binomial(n, p)    poisson(np).
    if x     poisson(  1), y     poisson(  2), and x and y are independent, then
(x + y )     poisson(  1 +   2).
    as           , poisson(  )     n (  ,     ).

gsl_stats march 24, 2009

246

q7.7

chapter 7

    calculate the binomial-distributed id203 of three rainfalls in
seven days, given the id203 of rain in one day of p = (110/365).

    calculate the poisson-distributed id203 of three rainfalls in

seven days, given a one-day    = (110/365).

gamma distribution

the gamma distribution is so-named because it relies heavily
on the gamma function,    rst introduced on page 244. along
with the beta distribution below, this naming scheme is one of the great notational
tragedies of mathematics.

a better name in the statistical context would be    negative poisson,    because it
relates to the poisson distribution in the same way the negative binomial relates to
the binomial. if the timing of events follows a poisson distribution, meaning that
events come by at the rate of    per period, then this distribution tells us how long
we would have to wait until the nth event occurs.

the form of the gamma distribution, shown for some parameter values in fig-
ure 7.9, is typically expressed in terms of a shape parameter        1/  , where   
is the poisson parameter. here is the summary for the function in terms of both
parameters:

p (x, n,   ) =

1

  (n)  n xn   1e   x/  , x     [0,   )

= gsl_ran_gamma_pdf(x, n, theta)

xn   1e     x, x     [0,   ) (7.2.3)

1

p (x, n,   ) =

  (n)( 1

   )n
e(x|n,    or   ) = n   = n/  
var(x|n,    or   ) = k  2 = k/  2
z x
z    

      

x

g(y|n,   )dy = gsl_cdf_gamma_p(x, theta)

g(y|n,   )dy = gsl_cdf_gamma_q(n, theta)
rn g : gsl_ran_gamma(r, n, theta)

gsl_stats march 24, 2009

distributions for description

247

n = 1,    = 1
n = 2,    = 1
n = 5,    = 1
n = 2,    = 2
n = 2,    = 0.5

1

0.8

0.6

0.4

0.2

)
  
,

n

,

x
(
p

0

0

1

2

3

4
x

5

6

7

8

figure 7.9 the gamma distribution.

    with n = df /2 and    = 2, the gamma distribution becomes a   2

df distribution

(introduced on page 301).

    with n = 1, the gamma distribution becomes an exponential(  ) distribution.

exponential distribution

the gamma distribution found the time until n events
occur, but consider the time until the    rst event occurs.
  (1)     1, 1   = 1 for all positive   , and x0 = 1 for all positive x, so at n =
1, equation 7.2.3 de   ning the pdf of the gamma distribution reduces to simply
e     x.

if we had a population of items,r t

0 e     xdx percent would have had a    rst event
between time zero and time t. if the event causes the item to leave the population,
then one minus this percent are still in the population at time t. the form ex is very
easy to integrate, and doing so gives that the percent left at time t = e     t/  .

so we now have a story of a population where members leave via a poisson pro-
cess. common examples include the stock of unemployed workers as some    nd
a job every period, radioactive particles emanating from a block, or drug dosage
remaining in a person   s blood stream. figure 7.10 shows a few examples of the
exponential distribution.

gsl_stats march 24, 2009

chapter 7

   = 1
   = 3
   = 10

248

)
  

,

x
(
p

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

2

3
x

4

5

6

figure 7.10 the exponential distribution.

since the exponent is      , this is sometimes called the negative exponential distri-
bution.

p (x,   ) =

   x

  

e

1
  

= gsl_ran_exponential_pdf(x, lambda)

e(x|  ) =   
var(x|  ) =   2
exp(  )dy = gsl_cdf_exponential_p(x, lambda)

exp(  )dy = gsl_cdf_exponential_q(x, lambda)

rn g : gsl_ran_exponential(r, lambda)

      

z x
z    

x

gsl_stats march 24, 2009

distributions for description

249

   = 2,    = 2
   = 0.5,    = 0.5
   = 5,    = 1
   = 1,    = 5

5

4

3

2

1

)
  

,

  

,

x
(
p

0

0

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
x

figure 7.11 the beta distribution.

description here are a few more distributions that are frequently used in model-

ing to describe the shape of a random variable.

beta distribution

just as the gamma distribution is named for the gamma function,
the beta distribution is named after the beta function   whose
parameters are typically notated as    and   . this book will spell out beta(  ) for
the beta function and use b(  ,  ) for the beta distribution.
the beta function can be described via the following forms:

beta(  ,   ) =z 1

0
  (  )  (  )
  (   +   )

=

x(     1)(1     x)(     1)dx

= gsl_sf_beta(alpha, beta).

the beta distribution is a    exible way to describe data inside the range [0, 1].
figure 7.11 shows how different parameterizations could lead to a left-leaning,
right-leaning, concave, or convex curve; see page 358 for more.

gsl_stats march 24, 2009

250

chapter 7

p (x,   ,   ) = beta(  ,   )x     1(1     x)     1

= gsl_ran_beta_pdf(x, alpha, beta)

e(x|  ,   ) =

  

   +   

    

var(x|  ,   ) =

(   +   )2(   +    + 1)

b(y|  ,   )dy = gsl_cdf_beta_p(x, alpha, beta)

z x
      
z    
x b(y|  ,   )dy = gsl_cdf_beta_q(x, alpha, beta)

rn g : gsl_ran_beta(r, alpha, beta)

    if    < 1 and    < 1, then the distribution is bimodal, with peaks at zero and one.
    if    > 1 and    > 1, then the distribution is unimodal.
    as    rises, the distribution leans toward one; as    rises, the distribution leans

  the beta distribution and order statistics

toward zero; if    =   , then the distribution is symmetric.
    if    =    = 1, then this is the uniform[0, 1] distribution.

assume that the    +        1 elements of x are drawn from a uniform[0, 1] distribu-
tion. then the   th order statistic has a b(  ,   ) distribution.

the    rst order statistic of a set of
numbers x is the smallest number
in the set; the second is the next-to-smallest, up to the largest order statistic, which
is max(x).

    write a function that takes in ag  _  g and two integersa andb,
produces a list ofa b	1 random numbers in [0, 1], sorts them, and
the returned data (usinga   _    _hi   g a ). it helps to precede
the plot output to gnuplot with e x a ge[0:1    to keep the range
    write a ai  that produces an animation of the pdfs of the    rst

    write a function to call that function 10,000 times and plot the pdf of

returns the ath order statistic.

consistent.

q7.8

through 100th order statistic for a set of 100 numbers.

    replace the call to the draw-and-sort function with a draw from the
b(a, b) distribution, and re-animate the results.

gsl_stats march 24, 2009

distributions for description

251

uniform distribution what discussion of distributions would be complete without
mention of the uniform? it represents a belief that any value

within [  ,   ] is equally possible.

p (x,   ,   ) =( 1

       
0

x     [  ,   ]
x <   , x >   

= gsl_ran_flat_pdf(x, alpha, beta)

e(x|  ,   ) =
var(x|  ,   ) =

2

         
(         )2

      

12
0
x <   
x     
        x     [  ,   ]
x >   
1

u(y|  ,   )dy =               

z x
z    
x u(y|  ,   )dy = gsl_cdf_flat_q(x, alpha, beta)
rn g, general : gsl_ran_flat(r, alpha, beta)

= gsl_cdf_flat_p(x, alpha, beta)

rn g,    = 0,    = 1 : gsl_rng_uniform(r)

   id203 theorists through the ages have developed models that in-
dicate that if a process follows certain guidelines, the data will have a
predictable form.

   a single draw from a binary event with    xed id203 has a
bernoulli distribution; from this, a wealth of other distributions can
be derived.

   an event which occurs with frequency    per period (or    per volume,
et cetera) is known as a poisson process; a wealth of distributions can
be derived for such a process.

   if x is the mean of a set of independent, identically distributed draws
from any nondegenerate distribution, then the distribution of x ap-
proaches a normal distribution. this is the central limit theorem.

   the beta distribution is useful for modeling a variety of variables
that are restricted to [0, 1]. it can be unimodal, bimodal, lean in either
direction, or can simply match the uniform distribution.

gsl_stats march 24, 2009

252

7.3 using the sample distributions

described above to practical bene   t.

chapter 7

here are some examples of how
you could use the distributions

looking up figures

successes?

if i have    fty draws from a bernoulli event with id203
.25, what is the likelihood that i will have more than twenty

statistics textbooks used to include an appendix listing tables of common distribu-
tions, but those tables are effectively obsolete, and more modern textbooks refer
the reader to the appropriate function in a stats package. for those who long for

the days of grand tables, the code supplement includes    a  ab e.
, code for
alternatively, apophenia   s command-line programa   _   k	  will look up a

producing a neatly formatted table of cdfs for a set of normal distributions (the
p-value often reported with hypothesis tests is one minus the listed value).

the code is not printed here because it is entirely boring, but the tables it produces
provide another nice way to get a feel for the distributions.

quick number for you.

generating data from a distribution each distribution neatly summarizes
an oft-modeled story, and so each
can be used as a capsule simulation of a process, either by itself or as a build-
ing block for a larger simulation.

listing 7.12 gives a quick initial example. it is based on work originated by gibrat
(1931) and extended by many others, including axtell (2006), regarding zipf   s
law, that the distribution of the sizes of cities,    rms, or other such agglomera-
tions tends toward an exponential-type form. in the model here, this comes about
because agents    growth rates are assumed to be the mean of a set of iid random
shocks, and so are normally distributed.

g  _ve
   . thei i ia ize function draws agent sizes from a uniform[0, 100]
distribution. to do this, it requires ag  _  g, which ai  allocates usinga   _	
  g_a   
 and passes toi i ia ize. see chapter 11 for more on using random
    each period, the    rms grow by a normally distributed rate (via theg  w function).
that is, theg  w function randomly draws g from ag  _  g_ga	  ia , and then

    first, the program produces a set of agents with one characteristic: size, stored in a

number generators.

reassigns the    rm size to size     size     exp(g). the most likely growth rate is
therefore exp(0) = 1. when g < 0, exp(g) < 1; and when g > 0, exp(g) > 1.

gsl_stats march 24, 2009

distributions for description

253

#include <apop.h>

int agentct = 5000;
int periods = 50;
int binct = 30;
double pauselength = 0.6;
gsl_rng *r;

void initialize(double *setme){

*setme = gsl_rng_uniform(r)*100;

}

void grow(double *val){

*val *= exp(gsl_ran_gaussian(r,0.1));

}

double estimate(gsl_vector *agentlist){

return apop_vector_mean(agentlist);

}

int main(){

gsl_vector *agentlist = gsl_vector_alloc(agentct);

}

}
fprintf(stderr, "the mean: %g\n", estimate(agentlist));

r = apop_rng_alloc(39);
apop_vector_apply(agentlist, initialize);
for (int i=0; i< periods; i++){

apop_plot_histogram(agentlist, binct, null);
printf("pause %g\n", pauselength);
apop_vector_apply(agentlist, grow);

listing 7.12 a model of normally distributed growth. online source:    a g  w h.
.
    the output is a set of gnuplot commands, so use./    a g  w h|g 	    .
with a a	 e between each histogram, the output becomes an animation, showing
size of the so-ranked    rm. converting to this form is left as an exercise to the reader. (hint: useg  _ve
   _	
    .)

also, exp(g)     exp(   g) = 1, and by the symmetry of the normal distribution, g
and    g have equal likelihood, so it is easy for an agent to    nd good luck in period
one countered by comparable bad luck in period two, leaving it near where it had
started.

a quick transition from a uniform distribution to a steep lognormal distribution,
where most agents are fast approaching zero size, but a handful have size approach-
ing 1,000.8

8here, the x-axis is the    rm size, and the y-axis is the number of    rms. typically, zipf-type distributions are
displayed somewhat differently: the x-axis is the rank of the    rm, 1st, 2nd, 3rd, et cetera, and the y-axis is the

gsl_stats march 24, 2009

output is printed to  de  , aka the screen, so that the pipe to gnuplot is not

    the last step is a model estimation, to which we will return in a few pages. its

chapter 7

254

disturbed.

simulation

fein et al. (1988) found that their depressive patients responded well
to a combination of lithium and a monoamine oxidase inhibitor (maoi).

but both types of drug require careful monitoring: lithium overdoses are common
and potentially damaging, while the combination of maois and chocolate can be
fatal.

#include <apop.h>

double    nd_lambda(double half_life){
double lambda =    half_life/log(1/2.);
return gsl_cdf_exponential_q(1, lambda);

}

int main(){

double li = 0, maoi = 0;
int days = 10;
gsl_matrix *d = gsl_matrix_alloc(days*24,4);
double hourly_decay1 =    nd_lambda(20.); //hours; lithium carbonate
double hourly_decay2 =    nd_lambda(11.); //hours; phenelzine

for (size_t i=0; i < days*24; i ++){

li *= hourly_decay1;
maoi *= hourly_decay2;
if (i % 24 == 0)

li+= 600;

if ((i+12) % 24 == 0)

maoi+= 45;

}
printf("plot    maoi.out    using 1:2 with lines title    li/10   , \

apop_matrix_row(d, i, onehour);
apop_vector_   ll(onehour, i/24., li/10., maoi, maoi/li*100.);

listing 7.13 a simulation of the blood stream of a person taking two drugs. online source: a i.
.

   maoi.out    using 1:3 with lines title    maoi   , \
   maoi.out    using 1:4 with lines title    maoi/li, pct\n");

remove("maoi.out");
apop_matrix_print(d, "maoi.out");

}

listing 7.13 simulates a patient   s blood stream as she follows a regime of lithium
carbonate (average half life: about 20 hours, with high variance) and an maoi
named phenelzine (average half life: 11 hours). as per the story on page 247,
when the drug leaves the blood stream via a poisson process, the amount of a drug
remaining in the blood is described by an exponential distribution.

gsl_stats march 24, 2009

distributions for description

255

li/10
maoi
maoi/li, pct

120

100

80

60

40

20

i

l
/
i

o
a
m

t
n
e
c
r
e
p

;
i

o
a
m
d
n
a

i

l

t
n
u
o
m
a

9

8

7

6

4

3

2

1

0

0

10

5
day

figure 7.14 the typical sawtooth pattern of decay and renewal.

percentage of a given initial level is remaining after one hour.

    the    rst step is to convert from the half life commonly used by pharmacists to the

drug in the blood stream by the amount calculated above, then checks whether it
is time for our patient to take one of her meds, and then records the various levels
on her chart.

   parameter in the exponential distribution. thefi d_ a bda function does this.
    given   ,g  _
df_ex   e  ia _	 1  a bda  answers the question of what
    the main simulation is a simple hourlyf   loop, that decrements the amount of
    the ungainly  i  f statement at the end plots the result. gnuplot does not save
derive or verify that thefi d_ a bda function correctly converts between

figure 7.14 shows the density in blood as our subject takes 600 mg of lithium at
midnight every day, and 45 mg of maoi at noon every day. for convenience in
scaling of the plot, the amount of lithium in the blood stream is divided by ten. in
the later days, after the subject   s system has reached its dynamic equilibrium, she
takes in 600 mg of li per day, and loses about 600 mg per day; similarly for the
45 mg of maoi. the ratio of maoi/li jumps constantly over the range of 187%
to 824%.

data, so it needs to reread the data    le three times to plot the three columns.

half life and the exponential distribution   s    parameter.

q7.9

gsl_stats march 24, 2009

256

q7.10

chapter 7

let there be two stocks: employed and unemployed. let the half-life of
employment (i.e., transition to unemployment) be 365 days, and the half-
life of unemployment (i.e.,    nding a job) be 3 weeks (21 days).
modify the lithium/maoi program to model the situation. for each period,
calculate the loss from both the employment and the unemployment stocks,
and then transfer the appropriate number of people to the other stock. what
is the equilibrium unemployment rate?

fitting existing data the common goal throughout the book is to estimate the
parameters of the model with data, so given a data set,

how can we    nd the parameters for the various distributions above?

you can see above that almost every parameter can be solved   sometimes over-
solved   using the mean and variance. for example, equations 7.2.1 and 7.2.2 (de-
scribing the parameters of a binomial distribution) are a system of two equations
in two unknowns:

   = np
  2 = np(1     p)

it is easy to calculate estimates of    and    from data,      and     , and we could plug
those estimates into the above system of equations to    nd the parameters of the
distribution. you can verify that for these two equations we would have

    2

  n =

             2
    2
    

.

  p = 1    

this is method of moments estimation (see, e.g., greene (1990, pp 117ff)). to
summarize the method, we write down the parameter estimates as functions of
the mean, variance, skew, and kurtosis, then we    nd estimates of those parame-
ters from the data, and use those parameter estimates to solve for the parameter
estimates of the model itself.

but problems easily crop up. for example, we can just count observations to    nd
the value of n for our data set, so given n,     , and     2, our system of equations is now
two equations with only one unknown (p). the poisson distribution had a similar
but simpler story, because its single parameter equals two different moments:

   =   
  2 =   .

so if our data set shows      = 1.2 and     2 = 1.4, which do we use for     ? apophenia
doesn   t fret much about this issue and just uses     , because this is also the maximum

gsl_stats march 24, 2009

distributions for description

257

can quickly estimate the above parameters:

likelihood estimator (id113) of    (where id113 will be discussed fully in chapter
10).

for the uniform, the method of moments doesn   t work either: the expression (      
  ) is oversolved with the two equations, but there is no way to solve for    or   
alone. however, a few moments    thought will show that the most likely value for
(  ,   ) given data x is simply (min(x), max(x)).

most of the above distributions have ana   _  de  associated (a   _    a ,
a   _ga  a,a   _	 if   , et cetera), and if you have a data set on hand, you
estimation in thee  i a e function to see how well it    ts.
the lognormal, zipf, exponential, and gamma. write af   loop to esti-

apop_data *d = your_data;
apop_model *norm = apop_estimate(d, apop_normal);
apop_model *beta = apop_estimate(d, apop_beta);
apop_model_show(norm);
apop_model_show(beta);
apop_model_show(apop_estimate(d, apop_gamma));

listing 7.12 produces a data set that should be zipf distributed. add an

better still, run a tournament. first, declare an array of several models, say

mate each model with the data, and    ll an array of con   dence levels based
on log-likelihood tests. [is such a tournament valid? see the notes on the
multiple testing problem on 316.]

q7.11

the method of moments provides something of a preview of the working of the
various model-based estimations in the remainder of the book. it took in data, and
produced an estimate of the model parameters, or an estimate of a statistic using
the estimate of the model parameters that were produced using data.

as the reader may have noticed, all these interactions between data, model param-
eters, and statistics create many opportunities for confusion. here are some notes
to bear in mind:

    the expected value, variance, and other such measures of a data set, when no

model is imposed, is a function of the data. [e.g., e(x).]

    the expected value, variance, and other such measures of a model are functions of
the ideal parameters, not any one data set. [e.g., e(x|  ) is only a function of   .]
    our estimate of model parameters given a data set is a function of the given data
set (and perhaps any known parameters, if there are any on hand). for example,
the normal parameter    is a part of the model speci   cation, but the estimate of   ,

gsl_stats march 24, 2009

258

chapter 7

which we write as     , is a function of the data. any variable with a hat, like   p, could
be notated as a function of the data,   p(x).

    we will often have a statistic like e(x) that is a function of the data   in fact,
we de   ne a statistic to be a function of data. but models often have data-free
analogues to these statistics. given a id203 distribution p (x,   ), the expected

value e(f (x)|  ) = r   x f (x)p (x,   )dx, meaning that we integrate over all x,

and so e(f (x)|  ) is a function of only   . the model in which    lives is almost
always taken as understood by context, and many authors take the parameters as
understood by context as well, leaving the expected value to be written as e(f (x)),
even though this expression is a function of   , not x.

bayesian updating the de   nition of a id155 is based on the
statement p (a     b) = p (a|b)p (b); in english, the like-
lihood of a and b occurring at the same time equals the likelihood of a occurring
given that b did, times the likelihood that b occurs. the same could be said re-
versing a and b: p (a     b) = p (b|a)p (a). equating the two complementary
forms and shunting over p (b) gives us the common form of bayes   s rule:

p (a|b) =

p (b|a)p (a)

p (b)

.

now to apply it. say that we have a prior belief regarding a parameter, such as
that the distribution of the mean of a data set is     n (0, 1); let this be p ri(  ). we
gather a data set x, and can express the likelihood that we would have gathered
this data set given any haphazard value of   , p (x|  ). let b be the entire range of
values that    could take on. we can then use bayes   s rule to produce a posterior
distribution:

p ost(  |x) =

p (x|  )p ri(  )

p (x)

so on the right-hand side, we had a prior belief about      s value expressed as a
distribution, and a likelihood function p (x|  ) expressing the odds of observing
the data we observed given any one parameter. on the left-hand side, we have a
new distribution for   , which takes into account the fact that we have observed
the data x. in short, this equation used the data to update our beliefs about the
distribution of    from p ri(  ) to p ost(  ).

the numerator is relatively clear, and requires only local information, but we can
write p (x) in its full form   

p ost(  |x) =

p (x|  )p ri(  )

r   b   b p (x|b)p ri(b)db

gsl_stats march 24, 2009

distributions for description

259

   to reveal that the denominator is actually global information, because calculating
it requires covering the entire range that    could take on. local information is easy
and global information is hard (see pages 325 ff), so bayesian updating is often
described via a form that just ignores the global part:

p ost(  |x)     p (x|  )p ri(  ).

computationally, there are two possibilities for moving forward given the problem
of determining the global scale of the distribution. first, there are a number of
conjugate distribution pairs that can be shown to produce an output model that

that is, the posterior equals the amount on the right-hand side times a    xed amount
(the denominator above) that does not depend on any given value of   . this is
already enough to compare ratios like p ost(  1|x)/p ost(  2|x), and given the
right conditions, such a ratio is already enough for running likelihood ratio tests
(as discussed in chapter 10).

matches the prior in form but has updated parameters. in this case, thea   _	
	 da e function simply returns the given model and its new parameters; see the
likelihood). but for now we can takea   _	 da e as a black box that takes in

chapter 11 will present a computationally-intensive method of producing a pos-
terior distribution when the analytic route is closed (i.e., monte carlo maximum

example below.

two models and outputs an updated conjugate form where possible, and an empiri-
cal distribution otherwise. we could then make draws from the output distribution,
plot it, use it as the prior to a new updating procedure when a new data set comes
in, et cetera.

an example: beta     binomial

for now, assume that the likelihood that someone
has a tattoo is constant for all individuals, regard-
less of age, gender, . . . (we will drop this clearly false assumption in the section
on multilevel modeling, page 288). we would like to know the value of that over-
all likelihood. that is, the statistic of interest is p     (count of people who have
tattoos)/(total number of people in the sample).

because we have weak knowledge of p, we should describe our beliefs about its
value using a distribution: p has small odds of being near zero or one, a reasonable
chance of being about 10%, and so on. the beta distribution is a good way to de-
scribe the distribution, because it is positive only for inputs between zero and one.
let b indicate the beta distribution; then b(1, 1) is a uniform(0, 1) distribution,
which is a reasonably safe way to express a neutral prior belief about p. alterna-
tively, setting p ri(p) to be b(2, 2) will put more weight around p = 1/2 and less
at the extremes, and raising the second parameter a little more will bring the mode
of our beliefs about p below 1/2 [see figure 7.11].

gsl_stats march 24, 2009

260

chapter 7

given p, the distribution of the expected count of tattooed individuals is binomial.
for each individual, there is a p chance of having a tattoo   a simple bernoulli
draw. the overall study makes n = 500 such draws, and thus    ts the model un-
derlying the binomial distribution perfectly. but we do not yet know p, so this
paragraph had to begin by taking p as given. that is, the binomial distribution
describes p (data|p).
it so happens that the beta and binomial distributions are conjugate. this means
that, given that p ri(p) is a beta distribution and p (data|p) is a binomial distribu-
tion, the posterior p ost(p|data) is a beta distribution, just like the prior. tables of
other such conjugate pairs are readily available online.

however, the parameters are updated to accommodate the new information. let x
be the number of tattoos observed out of n subjects, and the prior distribution be
b(  ,   ). then the posterior is a b(   + x,    + n     x) distribution. the discussion
of the prior offered possibilities like    =    = 1 or    =    = 2. but the survey has
500 subjects; the count of tattooed individuals alone dwarfs    = 2. therefore, we
can approximate the posterior as simply b(x, n     x).
the catalog above listed the expected value of a beta distribution as   
  +   . with
   = x and    = n     x, this reduces simply to x/n. that is, the expected posterior
value of p is the percentage of people in our sample who have tattoos (  p). bayesian
updating gave us a result exactly as we would expect.

the variance of a beta distribution is

    

(   +   )2(   +    + 1)

.

again, with n around 500, the 1 in the denominator basically disappears. filling
in    = x and    = n     x, we get

  p(1       p)

n

.

again, this is what we would get from the binomial distribution.

we call a b(  ,   ) distribution with small    and    a weak prior, by which we mean
that a moderately-sized data set entirely dwarfs the beliefs we expressed in the
prior. so what is the point of the updating process? first, we could use a stronger
prior, like    = 200,    = 300, which would still have some effect on the posterior
distribution even after updating with the data set.

second, the system provides a consistent mechanism for combining multiple data
sets. the posterior distribution that you have after accounting for a data set will
have a form appropriate for use as a prior distribution to be updated by the next
data set. thus, bayesian updating provides a natural mechanism for running metas-
tudies.

distributions for description

gsl_stats march 24, 2009

verify thata   _	 da e using a beta prior, a binomial likelihood function,
the simple x/n estimate. gather the data from the column a     . eg 
ha  a      , which is coded 1=yes, 2=no, and calculate      andb  2 using

the formul   in the above few paragraphs.
what results do you get when you assume a stronger prior, like b(200, 800)
or b(800, 200)?

and the tattoo data does indeed produce the estimated mean and variance as

261

q7.12

7.4 non-parametric

description

say that we have a data set and would like to know
the distribution from which the data was drawn. to
this point, we assumed the form of the distribution
(normal, binomial, poisson, et cetera) and then had only to estimate the parame-
ters of the distribution from data. but without assuming a simple parametric form,
how else could we describe the distribution from which the data was drawn?

the simplest answer would be a plain old histogram of the drawn data. this is
often suf   cient. but especially for small data sets, the histogram has dissatisfactory
features. if we make four draws, and three have value 20 and one has value 22, does
this mean that 21 has id203 zero, or we just didn   t have the luck of drawing a
21 this time?

thus, a great deal of nonparametric modeling consists of    nding ways to smooth
a histogram based on the claim that the actual distribution is not as lumpy as the
data.

the histogram the histogram is the most assumption-free way to describe the like-
lihood distribution from which the data was drawn. simply lay
down a row of bins, pile each data point into the appropriate bin, normalize the
bins to sum to one if so desired, and plot. because the most common use of a his-
togram (after just plotting it) is using it to make random draws, the full discussion
of histogram production will appear in the chapter on random draws, on page 361.

the key free variable in a histogram is the bandwidth   the range over the x-axis
that goes into each data point. if the bandwidth is too small, then the histogram
will have many slices and generally be as spiky as the data itself. a too-large
bandwidth oversmooths   at an in   nite bandwidth, the histogram would have only
one bar, which is not particularly informative. formally, there is a bias-variance
trade-off between the two extremes, but most of us just try a few bandwidths until
we get something that looks nice. see givens & hoeting (2005, ch 11) for an
extensive discussion of the question in the context of data smoothing.

gsl_stats march 24, 2009

moving average

262

series. for example,  vi gavg.
 in the online code supplement plots the tem-
perature deviances as shown inda a	
 i a e.db, and a moving average that
on thea   _ve
   _  vi g_ave age function.

the simplest means of smoothing data is a moving average, re-
placing each data point with the mean of the adjacent b data points
(where b is the bandwidth). you could use this for histograms or for any other

replaces each data point with the mean deviation over a two-year window, based

chapter 7

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

line 1

3.5

3

2.5

2

1.5

1

0.5

line 1

0
-2000

0

2000

4000

6000

8000

10000

12000

0
-2000

0

2000

4000

6000

8000

10000

12000

8

7

6

5

4

3

2

1

line 1

12

10

8

6

4

2

line 1

0
-2000

0

2000

4000

6000

8000

10000

12000

0
-2000

0

2000

4000

6000

8000

10000

12000

figure 7.15 a series of density plots. as h rises, the kernel density smooths out and has fewer peaks.

kernel smoothing

the kernel density estimate is based on this function:

  f (t, x, h) = pn

i=1 n ((t     xi)/h)

,

n    h

where x1, x2, . . . xn     r are the n data points observed, n (y) is a normal(0, 1)
density function evaluated at y, and h     r+ is the bandwidth. thus, the overall
curve is the sum of a set of subcurves, each centered over a different data point.
figure 7.15 shows the effect of raising h on the shape of a set of    tted curves.9
when h is very small, the normal distributions around each data point are sharp
spikes, so there is a mode at every data point. as h grows, the spikes spread out
and merge, until the sum of subdistributions produces a single bell curve. see page

9the data is the male viewership for 86 tv specials, from chwe (2001).

gsl_stats march 24, 2009

distributions for description

263

376 for more on how these plots were generated; see also silverman (1985).

as usual, there is a simple form for code to produce a default kernel density from
a data set, and a more extensive form that allows more control. try listing 7.16,
which plots the histogram of precipitation    gures and the kernel-density smoothed
version based on a n (0, 0.1) kernel. also try    = 0.001, 0.01, and 0.2 to see the
progression from the data   s spikes to a smooth bell curve.

#include <apop.h>

int main(){

apop_db_open("data   climate.db");
apop_data *data = apop_query_to_data("select pcp from precip");
apop_model *h = apop_estimate(data, apop_histogram);
apop_histogram_normalize(h);
remove("out.h"); remove("out.k");
apop_histogram_print(h, "out.h");
apop_model *kernel = apop_model_set_parameters(apop_normal, 0., 0.1);
apop_model *k = apop_model_copy(apop_kernel_density);
apop_settings_add_group(k, apop_kernel_density, null, h, kernel, null);
apop_histogram_print(k, "out.k");
printf("plot    out.h    with lines title    data   ,    out.k    with lines title    smoothed   \n");

listing 7.16 a histogram before and after smoothing via kernel densities. run via     hi g|
g 	    . online source:     hi g.
.
plot theda a	 v set using:

}

    a histogram, using 40 and 100 bins,

    a smoothed version of the 40-bin histogram, via a moving average of

q7.13

bandwidth four,

    the 40-bin histogram smoothed via a normal(x, 100.0) kernel density,

    the 40-bin histogram smoothed via a uniform(x     500.0, x + 500.0)
kernel density.

gsl_stats march 24, 2009

8

linear projections

our weakness forbids our considering the entire universe and makes us cut it up into
slices.

   poincar   (1913, p 1386)

this chapter covers models that make sense of data with more dimensions than we
humans can visualize. the    rst approach, taken in section 8.1 and known as prin-
cipal component analysis (pca), is to    nd a two- or three-dimensional subspace
that best describes the    fty-dimensional data, and    atten the data down to those
few dimensions.

the second approach, in section 8.2, provides still more structure. the model la-
bels one variable as the dependent variable, and claims that it is a linear combina-
tion of the other, independent, variables. this is the ordinary least squares (ols)
model, which has endless variants. the remainder of the chapter looks at how
ols can be applied by itself and in combination with the distributions in the prior
chapter.

one way to characterize the two projection approaches is that both aim to project
n -dimensional data onto the best subspace of signi   cantly fewer than n dimen-
sions, but they have different de   nitions of best. the standard ols regression
consists of    nding the one-dimensional line that minimizes the sum of squared
distances between the data and that line, while pca consists of    nding the few
dimensions where the variance of the projection of the data onto those dimensions
is maximized.

gsl_stats march 24, 2009

8.1   principal component analysis

linear projections

265

pca is closely related to factor
analysis, and in some    elds is
known as spectral decomposition. the    rst phase (calculating the eigenvalues)
is sometimes called the singular value decomposition. it is a purely descriptive
method. the idea is that we want a few dimensions that will capture the most
variance possible   usually two, because we can plot two dimensions on paper.

after plotting the data, perhaps with markers for certain observations, we may    nd
intuitive descriptions for the dimensions on which we had just plotted the data. for
example, poole & rosenthal (1985) projected the votes cast by all congressmen
in all us congresses, and found that 90% of the variance in vote patterns could
be explained by two dimensions.1 one of these dimensions could be broadly de-
scribed as       scal issues    and the other as    social issues.    this method stands out
because poole & rosenthal did not have to look at bills and place them on either
scale   the data placed itself, and they just had to name the scales.

shepard & cooper (1992) asked their sighted subjects questions regarding color
words (red, orange, . . . ), and did a principal component analysis on the data to
place the words in two dimensions, where they formed a familiar color wheel.
they did the same with blind subjects, and found that the projection collapsed to a
single dimension, with violet, purple, and blue on one end of the scale, and yellow
and gold on the other. thus, the data indicates that the blind subjects think of colors
on a univariate scale ranging from dark colors to bright colors.

it can be shown that the best n axes, in the sense above, are the n eigenvectors of
the data   s covariance matrix with the n largest associated eigenvalues.

the programs discussed below query three variables from the us census data: the
population, median age, and males per 100 females for each us state and common-
wealth, the district of columbia and puerto rico. they do a factor analysis and
then project the original data onto the space that maximizes variance, producing
the plot in figure 8.1.

the programs also display the eigenvectors on the screen. they    nd that the    rst
eigenvector is approximately (0.06,   1, 0.03). among the three variables given,
the second term   population   by itself describes the most variance in the data.
the x-axis in the plot follows population2

the eigenvalues for the y -axis are (0.96, 0.05,   0.29), and are thus a less one-
sided combination of the    rst variable (males per female) and the last (median age).
that said, how can we interpret the y axis? those familiar with us geography will
observe that the states primarily covered by cities (at the extreme, dc) are high on

1they actually did the analysis using an intriguing maximum likelihood method, rather than the eigenvector

method here. nonetheless, the end result and its interpretation are the same.

2as of the 2000 census, california    33.8 million, texas    20.8, new york    19.0, florida    16.0, et cetera.

gsl_stats march 24, 2009

pa

nj

oh

ny

fl

chapter 8

wa

az

co

tx

ca

266

-74

-76

-78

-80

-82

-84

-86

-88

-90

-92

r
o
t
c
e
v
n
e
g
i
e

d
n
o
c
e
s

ma

wv
me
de
vt

nh
nm
ne

md
mo
tn

al
pr
la
sc
ky

ct
ms
ar
ia
ok

ks

or

wi
mn

va

nc

in

mi

il

ga

dc

ri

sd
mt
nd
hiid
wy

ut

nv

ak

-94

-10

-5

0

5

10

15

20

25

30

first eigenvector

figure 8.1 states decomposed into population on the x axis, and a combination of median age and

gender balance (urban-ness?) on the y axis.

but involves a number of details. this section will show you the com-
putation of a principal component analysis on two levels. the    rst goes through
the steps of calculating eigenvectors yourself; the second is a single function call.

the y -axis, while states that are far more rural than urban (at the extreme, alaska)
are toward the bottom. thus, the principal component analysis indicates that we
could plausibly interpret the variation in median age and male/female balance by a
single variable representing a state   s urban   rural balance. because interpreting the
meaning of an arti   cial axis is a subjective exercise, other stories are also possible;
for example, one could also argue that this second axis instead represents a state   s
east-to-west location.

  coding it as with many algorithms, the process of coding is straightforward,
included in the online code supplement aseige b x.h.
    the 	e y_da a function is self-explanatory. with the clause e e
 ge _	
 a e a   w_ a e , apophenia will use the state names as row names rather
    these programs are intended to be run via pipe to gnuplot, likeeige ea y|
g 	    	 e  i  . so if we want additional information that gnuplot will not

    the input and output functions are identical for both programs, so the redundancy-
minimizing method of implementing these functions is via a separate    le, listing
8.2, and a corresponding header, which is too simple to be repeated here but is

than plain data.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

gsl_stats march 24, 2009

linear projections

267

#include "eigenbox.h"

apop_data *query_data(){

apop_db_open("data   census.db");
return apop_query_to_data(" select postcode as row_names, \n\

m_per_100_f, population/1e6 as population, median_age \n\
from geography, income,demos,postcodes \n\
where income.suid113vel=    040    \n\
and geography.geo_id = demos.geo_id \n\
and income.geo_name = postcodes.state \n\
and geography.geo_id = income.geo_id ");

}

void show_projection(gsl_matrix *pc_space, apop_data *data){

}

listing 8.2 the tools used below, including the query and a method for displaying labeled points.

understand, we need to send it to a different location. thus, lines 15   18 send out-

apop_opts.output_type =    p   ;
apop_opts.output_pipe = stderr;
fprintf(stderr,"the eigenvectors:\n");
apop_matrix_print(pc_space, null);
apop_data *projected = apop_dot(data, apop_matrix_to_data(pc_space), 0, 0);
printf("plot           using 2:3:1 with labels\n");
apop_data_show(projected);

online source:eige b x.
.
put to  de  , so the eigenvectors are printed to screen instead of sent down the
  d 	  pipeline to gnuplot.
    the     command on line 20 iswi h abe  , meaning that instead of points,
g  _eige _ y   functions to calculate the eigenvectors of a symmetric matrix.
thefi d_eige   function shows how one would use that function. the gsl is
allocated workspaces when it needs such things. thus, thefi deige   function
future tests in the way ofif ! 	bje
  ... work, the last line sets the pointer
to u  .
3there is one cheat: thea   _ v_de
     i i   function would use thea   _    a ize_f  _	
 vd x x	> a  ix  function to ensure that for each row, x   x = 1. you can erase line 28 ofeige ha d.
,
input this svd-speci   c id172 function after thea   _d   step, and look for subtle shifts.

allocates the workspace, calls the eigenfunction, then frees the workspace. it frees
the matrix whose eigenvalues are being calculated at the end because the matrix
is destroyed in the calculations, and should not be referred to again. to make sure

    as for the actual math, listing 8.3 shows every step in the process.3 the only hard
part is    nding the eigenvalues of x   x; the gsl saw us coming, and gives us the

too polite to allocate large vectors behind our backs, so it asks that we pass in pre-

we get the two-letter postal codes, as seen in figure 8.1.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33

gsl_stats march 24, 2009

268

chapter 8

#include "eigenbox.h"

void    nd_eigens(gsl_matrix **subject, gsl_vector *eigenvals, gsl_matrix *eigenvecs){

gsl_eigen_symmv_workspace * w = gsl_eigen_symmv_alloc((*subject)   >size1);
gsl_eigen_symmv(*subject, eigenvals, eigenvecs, w);
gsl_eigen_symmv_free (w);
gsl_matrix_free(*subject); *subject = null;

}

gsl_matrix *pull_best_dims(int ds, int dims, gsl_vector *evals, gsl_matrix *evecs){

size_t indexes[dims], i;
gsl_matrix *pc_space = gsl_matrix_alloc(ds,dims);

gsl_sort_vector_largest_index(indexes, dims, evals);
for (i=0; i<dims; i++){

apop_matrix_col(evecs, indexes[i], temp_vector);
gsl_matrix_set_col(pc_space, i, temp_vector);

}
return pc_space;

}

int main(){

}

int dims = 2;
apop_data *x = query_data();
apop_data *cp = apop_data_copy(x);
int ds = x   >matrix   >size2;
gsl_vector *eigenvals = gsl_vector_alloc(ds);
gsl_matrix *eigenvecs = gsl_matrix_alloc(ds, ds);

apop_matrix_normalize(x   >matrix,    c   ,    m   );
apop_data *xpx = apop_dot(x, x, 1, 0);
   nd_eigens(&(xpx   >matrix), eigenvals, eigenvecs);
gsl_matrix *pc_space = pull_best_dims(ds, dims, eigenvals, eigenvecs);
show_projection(pc_space, cp);

listing 8.3 the detailed version. online source:eige ha d.
.
three-dimensional data. the 	  _be  _di e  i    function allocates a new
matrix that will have onlydi   dimensions, and the best eigenvectors are in-
cluded therein. again, the gsl saw us coming, and provides theg  _    _	
ve
   _ a ge  _i dex function, which returns an array of the indices of the
largest elements of theeva   vector. thus,i dexe [0    is the index of the largest
eigenvalue,i dexe [1    is the point in the vector of values with the second largest
eigenvector, et cetera. given this information, it is an easyf   loop (lines 14   17)
to copy columns from the set of all eigenvectors to the 
_  a
e matrix.
    finally, after the principal component vectors have been calculated, h w_   je
	
 i   produces a graphical representation of the result. it projects the data onto the

    if the space of the data has full rank, then there will be three eigenvectors for

gsl_stats march 24, 2009

space via the simple dot productda a

linear projections

  
output as per the tricks discussed above.

 
_  a
e, and then produces gnuplottable

269

    given all these functions, the main routine is just declarations and function calls
to implement the above procedure: pull the data, calculate x   x, send the result to
the eigencalculator, project the data, and show the results.

#include "eigenbox.h"

}

int main(){

fprintf(stderr, "total explained: %lf\n", apop_sum(pc_space   >vector));
show_projection(pc_space   >matrix, cp);

int dims = 2;
apop_data *x = query_data();
apop_data *cp = apop_data_copy(x);
apop_data *pc_space = apop_matrix_pca(x   >matrix, dims);

leta   _ v_de
     i i   do the work. online source:
eige ea y.
.
listing 8.4 presents the same program using thea   _ a  ix_ 
a function to do
does the id172 and bookkeeping necessary to use theg  _ i a g_sv_	
de
    function. all that is left for the ai  function in listing 8.4 is to query the

the singular value decomposition in one quick function call. that function simply

listing 8.4 the easy way:

just

input data to a matrix, call the svd function, and display the output results.

q8.1

a matrix is positive de   nite (pd) iff all of its eigenvalues are greater than
zero, and positive semide   nite (psd) iff all of its eigenvalues are greater
than or equal to zero. similarly for negative de   nite (nd) and negative
semide   nite (nsd).
these are often used as multidimensional analogues to positive or negative.
for example, just as x2     0,   x     r, x    x is psd for any x with real
elements. an extremum of f (x) is a maximum when the second derivative
f      (x) is negative, and a minimum when f      (x) is positive; f (x) is a maxi-
mum when the matrix of second partial derivatives is nsd, and a minimum
when the matrix of second partials is psd (otherwise it   s a saddle point rep-
resenting a maximum along one direction and a minimum along another,
and is thus a false extremum).
   

270

q8.1

   

chapter 8

gsl_stats march 24, 2009

write a function a  ix_i _defi i e that takes in a matrix and outputs a
single character, say   ,   ,   , or   , to indicate whether the matrix
is one of the above types (and another character, say x , if it is none of
  a   _ a  ix_ 
a runs the entire process for the ef   ciently lazy

   given the data matrix x, the process involves    nding the eigenvalues
of the matrix x   x associated with the largest eigenvalues, and then
projecting the data onto the space de   ned by those eigenvectors.

them). write a test function that takes in any data matrix x (your favorite
data set or a randomly-generated matrix) and checks that x    x is psd.

   principal component analysis projects data of several dimensions

onto the dimensions that display the most variance.

user.

8.2 ols and friends

assume that our variable of interest, y, is described
by a linear combination of the explanatory variables,
the columns of x, plus maybe a normally-distributed error term,   . in short, y =
x   +   , where    is the vector of parameters to be estimated. this is known as
the ordinary least squares (ols) model, for reasons discussed in the introduction
to this chapter

to a great extent, the ols model is the null prior of models: it is the default that
researchers use when they have little information about how variables interrelate.
like a good null prior, it is simple, it is easy for the computer to work with, it    ex-
ibly adapts to minor digressions from the norm, it handles nonlinear subelements
(despite often being called id75), it is exceptionally well-studied and
understood, and it is sometimes the case that y really is a linear combination of the
columns of x.

ols is frequently used for solving the snow   ake problem from the last chapter:
we had a series of very clean univariate models that stated that the outcome of
interest is the result of a series of identical draws with equal id203, but in
most real-world situations, each draw is slightly different   in the terminology of
ols, we need to control for the other characteristics of each observation. the term
control for is an analogy to controlled experiments where nuisance variables are
   xed, but the metaphor is not quite appropriate, because adding a column to x
representing the control leads to a new projection onto a new space (see below),

gsl_stats march 24, 2009

linear projections

271

and may completely change the estimate of the original ols parameters.4 later
sections will present other options for surmounting the snow   ake problem when
using the distributions from the last chapter.

because linear models are so well-studied and documented, this book will only
brie   y skim over them.5 this chapter puts ols in the context from the    rst page
of this book: a model that claims a relationship between x and y and whose pa-
rameters can be estimated with data, whose computational tools provide several
conveniences. the next chapter brie   y covers ols for hypothesis testing.

unlike the models to this point, ols implicitly makes a causal claim: the variables
listed in x cause y to take the values they do. however, there is no true concept of
causality in statistics. the question of when statistical evidence of causality is valid
is a tricky one that will be left to the volumes that cover this question in detail.6
for the purposes here, the reader should merely note the shift in descriptive goal,
from    tting distributions to telling a causal story.

a brief derivation

minimizes the squared error,        , where y = x     +   .

part of ols   s charm is that it is easy (and instructive) to de-
rive      for the ols case. we seek the parameter estimate      that

  proof: after adding    to     , the new equation would be y = x     +x  +(     x  ),

that x      = 0. if x and    = y     x     were not orthogonal, then we could always
reduce the size of    by twiddling      by an iota to either      +    or            .

this is smallest when the error term    is orthogonal to the space of x, meaning

meaning that the new error term is now   n            x  , and

     n  n = (       x  )   (       x  )

=             2     x      +      x   x  .

the last term,      x   x  , is always a non-negative scalar, just as x    x is non-negative
for any real value of x. so the only way that      n  n can be smaller than         is if
2     x      > 0. but if x      = 0, then this is impossible.

the last step of the proof is to show that if      x      is not zero, then there is always
a way to pick    such that      n  n <        . q: prove this. (hint: if      x      6= 0, then the

4that is, regression results can be unstable, so never trust a single regression.
5economists are especially dependent on id75, because it is dif   cult to do controlled studies on
the open economy. thus, econometrics texts such as maddala (1977), kmenta (1986), or greene (1990) are a
good place to start when exploring linear models.

6see perl (2000) or any book on structural equation modeling. the typical full causal model is a directed
acyclic graph representing the causal relationships among nodes, and the data can reject or fail to reject it like any
other model.

gsl_stats march 24, 2009

272

same is true for   d = 2   and   h =   /2.)

chapter 8

(cid:7)

projection

so we seek      that will lead to an error such that x      = 0. expanding   
to y     x    , we can solve for     :

x   [y     x    ] = 0

x   y = x   x    

(x   x)   1x   y =     

this is the familiar form from page 3.

now consider the projection matrix, which is de   ned as

xp     x(x   x)   1x   .7

it is so named because, as will be demonstrated below, xp v projects the vector v
onto the space of x.

start with xp x. writing this out, it reduces instantly: x(x   x)   1x   x = x. so
the projection matrix projects x onto itself.

the expression xp    also simpli   es nicely:

xp    = x(x   x)   1x   [y     x    ]

= x(x   x)   1x   [y     x(x   x)   1x   y]
= x(x   x)   1x   y     x(x   x)   1x   x(x   x)   1x   y
= x(x   x)   1x   y     x(x   x)   1x   y
= 0.

the projection matrix projects    onto the space of x, but    and x are orthogonal,
so the projected vector is just 0.

what does the projection matrix have to do with ols? since      = (x   x)   1x   y,
x     = x(x   x)   1x   y = xp y. thus, the ols estimate of y,   y     x    , is the
projection of y onto the space of x:   y = xp y.

and that, in a nutshell, is what ols is about: the model claims that y can be
projected onto the space of x, and    nds the parameters that achieve that with least
squared error.

7this is sometimes called the hat matrix, because (as will be shown), it links y and   y.

gsl_stats march 24, 2009

linear projections

273

a sample projection at this point in the narrative, most statistics textbooks would
include a picture of a cloud of points and a plane onto which
they are projected. but if you have a gnuplot setup that lets you move plots, you
can take the data and projected points in your hands, and get a feel for how they
move.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

#include "eigenbox.h"
gsl_vector *do_ols(apop_data *set);

gsl_vector *project(apop_data *d, apop_model *m){

apop_data *d2 = apop_data_copy(d);
apop_col(d2, 0, ones);
gsl_vector_set_all(ones, 1);
return apop_dot(d2, m   >parameters, 0,    v   )   >vector;

}

int main(){

}

splot    projected    using 1:3:4 with points,    projected    using 2:3:4\n");

apop_data *d = query_data();
apop_model *m = apop_estimate(d, apop_ols);
d   >vector = project(d, m);
//d   >vector = do_ols(d);
d   >names   >rowct = 0;
d   >names   >colct = 0;
apop_data_print(d, "projected");
file *cmd = fopen("command.gnuplot", "w");
fprintf(cmd, "set view 20, 90\n\

listing 8.5 code to project data onto a plane via ols. compile witheige b x.
 from earlier.
online source:   je
 i  .
.
    line 13 does the ols regression. thea   _e  i a e function estimates the pa-
a   _  de  structure with the parameters set.
    the   je
  function makes a copy of the data set and replaces the dependent

listing 8.5 queries the same data as was plotted in figure 8.1, does an ols projec-
tion, and produces a gnuplot command    le to plot both the original and projected
data.

    ignore lines 2 and 15 for now; they will allow us to do the projection manually

rameters of a model from data. it takes in a data set and a model, and returns an

later on.

variable with a column of ones, thus producing the sort of data on the right-hand
side of an ols regression. line seven calculates x  .

    when the data set is printed on line 18, the    rst column will be the x   vector just
calculated, the second will be the original y data, and the third and fourth columns

gsl_stats march 24, 2009

274

chapter 8

will be the non-constant variables in x.

    gnuplot has some awkwardness with regards to replotting data (see page 170).
lines 16   18 write the data to a    le (with the row and column names masked out),
then lines 19   20 write a two-line command    le. you can run it from the command

line viag 	    
   a d.g 	    	, and should then have on your screen a
170. [q: add thewi h abe  commands to put labels on the plot.] spinning the
graph a bit, to e view83 2, shows that all of the red points are indeed on a
single plane, while in another position, at e view90 90, you can verify that

the view set on line 20 is basically a head-on view of the plane onto which the
data has been projected, and you can see how the data (probably in green) shifts to
a projected value (red). it is a somewhat different picture from the pca in listing

plot that you can move about.

the points do indeed match on two axes.8

the catalog because ols is so heavily associated with hypothesis testing, this
section will plan ahead and present both the estimates of    produced
by each model, its expected value, and its variance. this gives us all that we need
to test hypotheses regarding elements of our estimate of   ,     .

ols the model:

    y = x   +   
    n = the number of observations, so y and    are n    1 matrices.
    k = the number of parameters to be estimated. x is n    k;    is k by 1.
    many results that will appear later assume that the    rst column of    is a column of
ones. if it isn   t, then you need to replace every non-constant column xi below with
xi     xi, the equivalence of which is left as an exercise for the reader.9 see below

for the actual format to use when constructing youra   _da a set.
command    le to print three separate static graphs in three    les via the e view commands here, or you can try
a command like e view83 2; e     at the gnuplot command prompt.
9if you would like to take the route of normalizing each column to have mean zero, trya   _ a  ix_    	
a ize da a e      . this will normalize a 1 column to 0, so after calling the normalize function, you may
need to do something likea   _c   y 	 _da a 0   e
   ;g  _ve
   _ e _a     e
   1 .

    e(  ) = 0.
    var(  i) =   2, a constant,     i.

8if your gnuplot setup won   t let you spin the plot with a pointing device, then you can modify the gnuplot

assumptions:

gsl_stats march 24, 2009

linear projections

275

    cov(  i,   j) = 0,     i 6= j. along with the above assumption, this means that the
n    n covariance matrix for the observations    errors is          2i.
    the columns of x are not collinear (i.e., det(x   x) 6= 0, so (x   x)   1 exists).10
    n > k.

notice that we do not assume that    has a normal distribution, but that assumption
will be imposed in the next chapter, when we apply t tests to     . when all of that
holds, then

    ols = (x   x)   1(x   y)

e(    ols) =   
var(    ols) =   2(x   x)   1

almost anything can have a variance, which often creates confusion. a column of
data has a variance, the column of errors    has a variance, the estimate     ols has a
covariance matrix, and (if so inclined) you could even estimate the variance of the
variance estimate     2. the variance listed above is the k    k covariance matrix
for the estimate     ols, and will be used for testing hypotheses regarding    in later
chapters. it is a combination of the other variances: the variance of the error term   
is   2, and the various columns of the data set have covariance x   x, so the variance
of     ols is the    rst times the inverse of the second.

instrumental variables the proofs above gave us a guarantee that we will
calculate a value of      such that x will be uncorre-

lated to      = y     x    .
our hope is to estimate a    true    model, claiming that y = x   +   , where x is
uncorrelated to   , and so on. but if it is the case that a column of x really is
correlated to the error in this model, then there is no way that      (which guarantees
that the estimated error and the columns of x are not correlated) could match   
(which is part of a model where error and a column of x are correlated). this
creates major problems.

for example, say that one column of the data, xi, is measured with error, so we are
actually observing xi +   i. this means that the error term in our equation is now

10if this assumption is barely met, so det(x   x) is not zero but is very small, then the resulting estimates
will be unstable, meaning that very small changes in x would lead to large changes in (x   x)   1 and thus in the
parameter estimates. this is known as the problem of multicollinearity. the easiest solution is to simply exclude
some of the collinear variables from the regression, or do a principal component analysis to    nd a smaller set of
variables and then regress on those. see, e.g., maddala (1977, pp 190   194) for further suggestions.

gsl_stats march 24, 2009

276

chapter 8

the true error joined with an offset to the measurement error,      =         i. if the true
xi and the true    have no correlation, then xi +   i and          i almost certainly do.
as above, the ols estimate of    is     ols = (x   x)   1x   y, or taking a step back
in the derivation, (x   x)    ols = x   y. also, the true    is de   ned to satisfy y =
x   +   . with a few simple manipulations, we can    nd the distance between    and
    :

y = x   +   

x   y = x   x   + x     
(x   x)    ols = x   x   + x     

(x   x)(    ols       ) = x     

    ols        = (x   x)   1x     

(8.2.1)

if x      = 0, then the distance between     ols and    is zero       ols is a consis-
tent estimator of   . but if x and    are correlated, then     ols does not correctly
estimate   . further, unless we have a precise measure of the right-hand side of
equation 8.2.1, then we don   t know if our estimate is off by a positive, negative,
large or small amount. still further, every column of x affects every element of
(x   x)   1, so mismeasurement of one column can throw off the estimate of the
ols coef   cient for every other column as well.

the solution is to replace the erroneously-measured column of data with an in-
strument, a variable that is not correlated to    but is correlated to the column of
data that it will replace. let xi be the column that is measured with error (or is
otherwise correlated to   ), let z be a column of alternate data (the instrument), and
let z be the original data set x with the column xi replaced by z. if cov(z,   ) = 0,
then the following holds:

    iv = (z   x)   1(z   y)

e(    iv) =   
var(    iv) =   2(z   x)   1z   z(x   z)   1

whe det(z   x) is small, (z   x)   1   and thus the variance   is large; this happens
when cov(z, xi)     0. we want the variance of our estimator to be as small as
possible, which brings us to the usual rule for searching for an instrument:    nd
a variable that can be measured without signi   cant error, but which is as well-
correlated as possible to the original variable of interest.

gsl_stats march 24, 2009

linear projections

277

gls generalized least squares generalizes ols by allowing         to be a known
matrix   , with no additional restrictions. note how neatly plugging   2i in to
the estimator of    and its variance here reduces the equations to the ols versions
above.

    gls = (x        1x)   1(x        1y)

e(    gls) =   
var(    gls) = (x        1x)   1

tens of millions ofd 	b es in memory at once, so simply writing down such a

but there is a computational problem with gls as written here. for a data set of
a million elements,    is 106    106 = 1012 (a trillion) elements, and it is often the
case that all but a million of them will be zero. a typical computer can only hold

matrix is dif   cult   let alone inverting it. thus, although this form is wonderfully
general, we can use it only when there is a special form of    that can be exploited
to make computation tractable.

weighted least squares

for example, let    be a diagonal matrix. that is, er-
rors among different observations are uncorrelated,
but the error for each observation itself is different. this is heteroskedastic data.
the classic example in econometrics is due to differences in income: we expect
that our errors in measurement regarding the consumption habits of somebody
earning $10,000/year will be about a thousandth as large as our measurement er-
rors regarding consumption by somebody earning $10 million/year.

it can be shown (e.g., kmenta (1986)) that the optimum for this situation, where
  i is known for each observation i, is to use the gls equations above, with    set
to zero everywhere but the diagonal, where the ith element is 1
  2
i

.

the gls equations about    now apply directly to produce weighted least squares
estimates   and there is a trick that lets us do computation using just a vector of
diagonal elements of   , instead of the full matrix. let       be a vector where the
ith element is the square root of the ith diagonal element of   . for wls, the ith
element of       is thus 1
. now let y   be a vector whose ith element is the ith
element of y times the ith element of      , and x   be the column-wise product of
x and      . that is,

  i

gsl_stats march 24, 2009

278

chapter 8

void columnwise_product(gsl_matrix *data, gsl_vector *sqrt_sigma){

for (size_t i=0; i< data   >size2; i++){

apop_matrix_col(data, i, v);
gsl_vector_mul(v, sqrt_sigma);

}

}

then you can verify that x     x   = x     x, x     y   = x     y, and so

fitting it

e(    wls) =   

estimate an ols model in one line:

thus, we can solve for the weighted least squares elements without ever writing

(which is what you would use in practice, rather than calculating x   yourself).

var(    wls) =   2(cid:0)x     x  (cid:1)   1

    wls =(cid:0)x     x  (cid:1)   1 (x     y  )

down the full    matrix in all its excess. this is the method used bya   _w  
if you already have a data matrix ina   _da a  e , then you can
if your data set has a non- u  weigh   vector, then you could replacea   _   
in the above witha   _w  .
the detailed list of options. thea   _ v model requires the settings setting treat-
ment, because that model requires that the settings   i    	 e    element is set.
it toa   _e  i a e y 	 _da a a   _     to check the coef   cients

in the exercise on page 232, you found a relationship between males per
female (the dependent variable) and both density and male   female wage
ratios (the independent variables). produce an appropriate data set and send

if you would like more control over the details of the estimation routine, see page
145 on the format for changing estimation settings, and the online references for

apop_estimate_show(apop_estimate(set, apop_ols));

when both independent variables are included.

q8.2

gsl_stats march 24, 2009

279

linear projections

if the assumptions of ols do not    t, then you will need to be ready to modify the
innards of the ols estimation to suit, so the remainder of this section goes back
to the id202 layer of abstraction, to go over the steps one would take to
estimate   .

(by adding   je
 i   w .  to the b ects line in the make   le), and re-run to

listing 8.6 extends the example in listing 8.5 (page 273) by doing every step
in the id202 of ols. uncomment line 15 in that code, link it with this

produce the new projection, which should be identical to the old.

#include <apop.h>

gsl_vector *do_ols(apop_data *set){

apop_data *d2 = apop_data_copy(set);
apop_col(d2, 0,    rstcol);
apop_data *y_copy = apop_vector_to_data(apop_vector_copy(   rstcol));
gsl_vector_set_all(   rstcol, 1);

apop_data *xpx = apop_dot(d2,d2,   t   ,0);
gsl_matrix *xpxinv = apop_matrix_inverse(xpx   >matrix); //(x   x)   1
apop_data *second_part = apop_dot(apop_matrix_to_data(xpxinv), d2,0,   t   );

}

apop_data *projection_matrix = apop_dot(d2, second_part,0,0); //x(x   x)   1x   
return apop_dot(projection_matrix, y_copy, 0,0)   >vector;

apop_data *beta = apop_dot(second_part, y_copy, 0, 0); //(x   x)   1x   y
strcpy(beta   >names   >title, "the ols parameters");
apop_data_show(beta);

   je
 i   w .
.
onea   _da a set namedy_
  y which copies off the    rst column of the input
matrix, and a second set namedd2, which is a copy of the input matrix with the

    typically, the y values are the    rst column of the data matrix, so the    rst step
is to extract the data into a separate vector. lines 4   7 do this, and end up with

listing 8.6 the ols procedure and its use for projection, spelled out in great detail. online source:

same    rst column set to 1.

    lines 9   11    nd (x   x)   1x   . if you like, you can use the debugger to check the

value of any of the intermediate elements all along the calculation.

    now that we have (x   x)   1x   , lines 13   15 do the single additional dot product

to    nd    = (x   x)   1x   y, and display the result.

    line 17 produces the projection matrix x(x   x)   1x   , and given the projection

matrix, another dot product projects y onto it.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

280

q8.3

gsl_stats march 24, 2009

chapter 8

to the equation:

gsl_linalg_hh_solve (xpx   >matrix, xpy   >vector, *beta);

the gsl has a function to solve equations of the type a   = c using
householder transformations, and this happens to be exactly the form we

in practice, it is almost necessary to take the inverse to solve for ols pa-
rameters, because the covariance matrix of      is   2(x   x)   1. but the house-
holder method manages to avoid explicitly    nding the inverse of x   x, and
may thus be quicker in situations like the cook   s distance code (page 133)

have here   (x   x)   = (x   y). givena   _da a sets for x   x, x   y, and a
g  _ve
    allocated forbe a, this line would    llbe a with the solution
where af   loop solves for thousands of ols parameter sets.
rewrite   je
 i   w .
 to use the householder transformation to    nd
be a. check that the results using this alternate method match thebe a
   in most cases, you can usea   _e  i a e. but if need be, coding

   the ordinary least squares model assumes that the dependent vari-
able is an af   ne projection of the others. given this and other as-
sumptions, the likelihood-maximizing parameter vector is   ols =
(x   x)   1(x   y).

   if    6= i, then   gls = (x     x)   1(x     y). depending on the value

of   , one can design a number of models.

found via inversion.

these processes is a simple question of stringing together lines of lin-
ear algebra operations from chapter 4.

8.3 discrete variables

to this point, the regression methods have been
assuming that both y and the elements of x are
all continuous variables     r. if they are discrete variables     {0, 1, 2, . . . }, then
we need to make modi   cations.

there are a number of distinctions to be made. first, the approaches to handling
columns of discrete variables in x are different, and simpler, than approaches to
handling discrete values of the outcome variable y. if y only takes on integer
values, or worse, is just zero or one, then there can   t possibly be a    and a normally
distributed    that satisfy y = x   +   . there are convolutions like stipulating that
for observations where x   < 0 we force    such that x   +    = 0 and where
x   > 1 we force x   +    = 1, but then    is non-normal, which makes a mess of

gsl_stats march 24, 2009

linear projections

281

the hypothesis tests we   ll run in later chapters. more generally, such an approach
lacks grace, and requires a cascade of little    xes (greene, 1990, p 637).

how we proceed in either case depends on the type of discrete variable at hand:

    a discrete binary variable takes on exactly two values, such as male/female or

case/control, and can be rewritten as simply either zero or one.

    ordered, discrete data are typically a count, such as the number of children or cars

a person owns.

    most qualitative categorizations lead to multi-valued and unordered data. for ex-
ample, a metro station could be on the red, blue, green, orange, and yellow line,
or a voter could align himself with a red, blue, green, or other party.

dummy variables

for a column of zero   one data in the independent data x, we
don   t really have to modify ols at all, but we change the in-
terpretation slightly: taking zero as observations in the baseline and one as the
observations in the treatment group, the ols coef   cient on this zero   one dummy
variable can indicate how effective the treatment is in changing the outcome.
tests for the signi   cance of the dummy variable can be shown to be equivalent
to anova tests of a category   s signi   cance.

as an extension, if the categories are ordered but discrete, like the number of chil-
dren, then we again don   t have to do anything: if a jump from x = 0 to x = 1
induces a shift of size    in the outcome, then a jump from x = 1 to x = 2 would
do the same under a linear model, and a jump from x = 0 to x = 2 would produce
a jump of size 2  . if it is natural to presume this, then the model does no violence
to reality.11

in the binary case. here, the functiona   _da a_  _d	  ie  saves the day: it

but if the categories are discrete and unordered, then we can   t use one variable with
the values {0, 1, 2, . . . }, because the implied linear relationship makes no sense.
with 0=green, 1=blue, 2=red, does it make sense that a shift from green to blue
will have exactly the same effect as a shift from blue to red, and that a shift from
green to red would have exactly twice that effect? in this case, the solution is to
have a separate variable for all but one of the choices, and thus to produce n     1
coef   cients for n options. the excluded option is the baseline, like the zero option

takes the ith column from the data set and outputs a table with n   1 binary dummy
variables; see the example below for usage.

given multiple categories, you could even produce interaction terms to repre-
sent membership in multiple categories: e.g., control    male, control    female,

11if the perfectly linear form is implausible, it may be sensible to transform the input variable, to the square

of x or    x.

gsl_stats march 24, 2009

282

query e e
   ef  ||  igh   produces the string ef  igh ; in mysql,

  
a    ef     igh    does the same. then break the column down into

treatment    male, treatment    female. the easiest way to do this is to sim-
ply create a column with two other columns mashed together: in sqlite, the

chapter 8

n     1 dummy variables as above.
the underlying claim with zero   one dummy variables is y = x   + k, where
k indicates a constant value that gets added to the controls but not the cases, for
example. but say that we expect the slopes to differ from cases to controls; for
cases the equation is y = x1  1+x2  2 and for controls it is y = x1(  1+k)+x2  2.
the way to get a standard regression to produce k in this case would be to produce
a data set which has the appropriate value x1 for each control, but zero for each
case. then the regression would be equivalent to y = x1  1 + x1k + x2  2 for
controls and y = x1  1 + x2  2 for cases, as desired.

#include <apop.h>

apop_model * dummies(int slope_dummies){

apop_data *d = apop_query_to_mixed_data("mmt", "select riders, year   1977, line \

from riders, lines \
where riders.station=lines.station");

apop_data *dummi   ed = apop_data_to_dummies(d, 0,    t   , 0);
if (slope_dummies){

apop_col(d, 1, yeardata);
for(int i=0; i < dummi   ed   >matrix   >size2; i ++){

apop_col(dummi   ed, i, c);
gsl_vector_mul(c, yeardata);

}

}
apop_data *regressme = apop_data_stack(d, dummi   ed,    c   );
apop_model *out = apop_estimate(regressme, apop_ols);
apop_model_show(out);
return out;

}

#ifndef testing
int main(){

listing 8.7 two regressions with dummy variables. online source:d	  ie .
.

apop_db_open("data   metro.db");
printf("with constant dummies:\n"); dummies(0);
printf("with slope dummies:\n"); dummies(1);

}
#endif

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

listing 8.7 runs a simple regression of ridership at each station in the washington
metro on a constant, the year, and a set of dummy variables for green, orange,

gsl_stats march 24, 2009

283

text column.

linear projections

red, and yellow lines (meaning the blue line is the baseline).

    the query on line four pulls two numeric columns (ridership and year) and one

    line 7 converts text column zero into ana   _da a set consisting of dummy
variables. if you ask the debugger to display thed	  ified data set, you will see
#if def wrapper on lines 21 and 27 will let us read this    le into the testing pro-
    the    e_d	  ie  switch determines whether the dummies will be constant
so as above,a   _da a_  _d	  ie  gives us what we need. for slope dummies,

    line 15 stacks the dummies to the right of the other numeric data, at which point
the data set is in the right form to be regressed, as on line 16. again, it is worth
asking the debugger to show you the data set in its    nal, regressable form.

    we will test the claim that these two tests are equally likely on page 354; the

dummies or slope dummies. for constant dummies, we need only zeros or ones,

that every row has a 1 in at most a single column.

gram. here, the wrapper is innocuous.

we need to replace every 1 with the current year, which is what the column-by-
column vector multiplication achieves over lines 9   13.

probit and logit now we move on to the question of discrete outcome vari-
ables. for example, a person either buys a house or does not,
which makes house purchasing a binary variable. say that the value of a house is a
linear sum of the value of its components: an extra bedroom adds so many dollars,
a nearby highly-rated school adds so many more, each square meter is worth a
few thousand dollars, et cetera. that is, one could write down a linear model that
total value u =   1  cost +   2  bedrooms +   3  school quality +   4  square meters
+       +   , where   1 is typically normalized to    1 and    is the usual error term.12
to phrase the model brie   y: u = x   +   . but rather than leaving the model in the
standard linear form, we add the rule that a person buys iff u > 0. thus, the input
is the same set of characteristics x (of consumers or of houses) and weightings   ,
and the output is a binary variable: the person acted or did not; the buyer consumed
or did not; the geyser erupted or did not, et cetera.

from here, the details of the decision model depend on whether the outcome is
binary, multivalued but unordered, or ordered.

    logit with two choices: the    rst alternative, the logit model or logistic model, as-
sumes that        a gumbel distribution. the gumbel is also known as the type i

12real estate economists call this the hedonic pricing model, whose implementation typically differs in some

respects from the logit model discussed here. see cropper et al. (1993) for a comparison.

gsl_stats march 24, 2009

284

chapter 8

extreme value distribution, meaning that it expresses the distribution of the statis-
tic max(x) under appropriate conditions. mcfadden (1973) won a nobel prize
partly by showing that the above assumptions, including the rather eccentric error
term, reduce to the following simple form:

p (0|x) =

p (1|x) =

1

1 + exp(x  1)

exp(x  1)

1 + exp(x  1)

there is no   0 because is it normalized to 0, so exp(x  0) = 1.
    probit: the probit model has a similar setup to the logit, but        n (0, 1).
the model can be rephrased slightly. say that u (x) is deterministic, but the prob-
ability of consuming given some utility is random. to describe such a setup, let
u =    x   rather than    x   +    (the introduction of a minus sign will be discussed
in detail below), and let the id203 that the agent acts be

p (x|  ) =z u(   x  )

      

n (y|0, 1)dy,

where n (y|0, 1) indicates the normal pdf at y with    = 0 and    = 1, so the
integral is the cdf up to u (   x  ).
as you can see, this is a much more ornery form, especially since the cdf of the
normal (what engineers call the error function, or more affectionately, erf) does
not simplify further and is not particularly easy to calculate. this partly explains
the prevalence of the logit form in existing journal articles, but is not much of an
issue now that id113 is cheap, even when erf is involved.
    with k different options, the logit model generalizes neatly for the case of multiple

values, to the multinomial logit:

p (0|b) =

p (k|b) =

1

i=1 exp(x  i)

exp(x  x)

i=1 exp(x  i)

1 +pk
1 +pk

and

, k 6= 0

(8.3.1)

where b is the set of choices from which x is chosen, and   k is a different vector
of coef   cients for each option k 6= 0. you can see that   0 is once again normalized
to 0, and given only options zero and one, the form here reduces to the binomial
form above.13

models: thea   _  gi  model just counts the options and acts accordingly.

    ordered multinomial probit: presume that there is an underlying continuous vari-
able y     =    x   such that the true yi = 0 if y    i     0; yi = 1 if 0 < y    i     a1;
yi = 2 if a1 < y    i     a2; and so on. that is,

13the generalization is so clean that apophenia doesn   t even include separate binary and multinomial logit

gsl_stats march 24, 2009

linear projections

285

n (y|0, 1)dy

p (yi = 0) =z a1   x  
p (yi = 1) =z a2   x  
p (yi = 2) =z    

      
a1   x   n (y|0, 1)dy
a2   x   n (y|0, 1)dy

several cutoffs, between choosing zero, one, two, . . . items; estimatinga   _	
 	  i   ia _   bi  will therefore return the usual vector of parameters   , plus

rather than    nding a single cutoff between acting and not acting, we estimate

a vector of a1, a2, . . . , ak   1 for k options.

parameter overload very reasonable models can produce an overwhelming num-
ber of parameters. for the multinomial logit model above,
with k options and n elements in each observation x, there are n   (k     1) param-
eters to be estimated. your goals will dictate upon which parameters you should
focus, and how you get meaning from them.

if the sole goal is to prove that a causes b, then the univariate test that the coef   -
cient on a is signi   cant is all we will look at from one of the above tests.

just the average probabilities viaa   _da a_ 	  a ize  	   de .ex e
 ed .

if we want to predict how comparable people will act in the future, then we need
the model   s estimate for the odds that each individual will select any one choice,
and the predicted most likely selection. perhaps you will want the full catalog, or

or, you may want to know the dynamic shifts: what happens to b when a rises
by 1%? economists call this the marginal change, though virtually every    eld has
some interest in such questions. this is closely related to the question of whether
the parameter on a is statistically signi   cant: you will see in the chapter on max-
imum likelihood estimates that their variance (used to measure con   dence for a
hypothesis tests) is literally the inverse of the second derivative (used to measure
change in outcome variable given change in income variable).

for a linear model, the marginal change is trivial to calculate: we proposed that
y = x  , meaning that    y/   xi =   i. that is, a 1% change in xi leads to a   %
change in y.

interpreting the marginal changes in the above models can be more dif   cult. for
the probit, the cutoff between draws that induce a choice of option zero and those
that lead to option one was    x  . as +x   grows (either because of a marginal
expansion in an element of x or of   ), the cutoff falls slightly, and the set of draws

gsl_stats march 24, 2009

286

chapter 8

that lead to option one grows. this    ts our intiution, because we characterized the
elements of the linear sum x   as increasing the proclivity to choose option one.

for the ordered probit, there are multiple cutoffs. a marginal change in    will per-
haps raise the cutoff between options two and three, lowering the odds of choosing
three, but at the same time raise the cutoff between options three and four, raising
the odds of choosing three. see greene (1990, p 672) or your estimation program   s
documentation for more on making productive use of the estimated model.

iia

return to the multinomial logit formula, equation 8.3.1, and consider the ratio
of the id203 of two events, e1 and e2.

p (e1|x, b)
p (e2|x, b)

=

=

exp(x  1)

py   b exp(x   y)

exp(x  2)

py   b exp(x   y)
exp(x  1)
exp(x  2)

the key point in this simple result is that this ratio does not depend on what else
is in the set of options. the term for this property is independence of irrelevant
alternatives, or iia. the term derives from the claim that the choice between one
option and another in no way depends upon the other options elsewhere.

the standard example is commuters choosing among a red bus, a blue bus, and a
train, where the red bus and blue bus are identical except for their paint job. we
expect that the ratio of p(red)/p(blue) = 1, and let p(red)/p(train) = p(blue)/p(train)
= 1
x . if the train were out of service, then p(red)/p(blue) would still be 1, because
as many former train riders would now pick the blue bus as would pick the red, but
if the blue bus were knocked out of commission, then everybody who was riding
it would likely just switch to the red bus, so without the blue bus, p(red)/p(train)
= 2
x . in short, the train option is irrelevant to the choice of red or blue bus, but the
blue bus option is not irrelevant in the choice between train and red bus.

in the code supplement, you will    ndda a	e e
 i  .db, which includes a small

selection of survey data regarding the united states   s 2000 election, from national
election studies (2000). the survey asked respondents their opinion of various
candidates (including the two candidates for vice president) on a scale from 0 to
100; in the database, you will    nd the person that the respondent most preferred.14

14duverger   s law tells us that a    rst-past-the-post system such as that of the united states tends to lead to
a stable two-party system, due to the various strategic considerations that go into a vote. we see this in the
data: question 793 of the survey (not included in the online supplement) asked the respondent for whom he or
she expects to vote, and the totals were: (gore, 704), (bush, 604), (everybody else, 116). the favorite from the
thermometer score avoids the strategic issues involved in vote choice, and produces a much broader range of
preferences: (gore, 711), (bush, 524), (nader, 157), (cheney, 68), et cetera. q: write a query to get the complete
count for each candidate from the survey data.

gsl_stats march 24, 2009

linear projections

287

the iia condition dictates that the logit estimates of the relative odds of choosing
candidates won   t change as other candidates enter and exit, though intuitively, we
would expect some sort of shift. the examples also suggest a two-level model,
wherein people    rst choose a category (bus/train, republican/democrat/green),
and then having chosen a category, select a single item (such as red bus/blue bus,
buchanan/mccain). such a model weakens iia, because a choice   s entry or exit
may affect the choice of category. these hierarchies of class and item are a type
of multilevel model; the next section will give an overview of multilevel modeling
possibilities, including some further discussion of the nested logit.

the theoretical iia property is irrefutable   there is not much room for error in the
two lines of algebra above. but many theoretical models can be pushed to produce
mathematically clean results that are just not relevant in the application of the
model to real data. does iia have a strong in   uence on real-world logit estimates?

we would test iia by running one unconstrained logit, and another logit estima-
tion restricted so that one option is missing; this    ts the likelihood ratio (lr) form
which will appear on page 351, and a few variants on this test exist in the literature.
cheng & long (2007) built a number of arti   cial data sets, some designed around
agents who make choices conforming to iia, and some designed to not have the
iia property. they found that the lr tests were ineffective: the unconstrained and
constrained odds ratios sometimes did not differ (demonstrating iia) and some-
times did (against iia), but there was no suf   ciently reliable relationship between
when the underlying data had the iia property and when the parameter estimates
demonstrated iia. fry & harris (1996) used a less broad method that had slightly
more optimistic results, but still encountered glitches, such as problems with power
and results that were sensitive to which option was removed.

the probit model matches the logit save for the type of bell curve that describes
the error term, so one expects the parameters and predicted odds to be similar.
in fact, amemiya (1981) points out that logit parameters typically match probit
parameters, but that they are scaled by a factor of about 1.6. that is, for each
option i,   l
i . yet, the logit parameters have the iia property, while the
probit parameters do not. this is hard to reconcile, unless we accept that the iia
property of the logit is theoretically absolute but empirically weak.

i     1.6  p

   for discrete explanatory variables, you can use the standard family of

ols models, by adding dummy variables.

   the standard linear form x   can be plugged in to a parent function,
like the probit   s comparison of x   to a normal distribution or the
logit   s comparison to a gumbel distribution, to generate models of
discrete choice.
   

gsl_stats march 24, 2009

288

   

chapter 8

   the logit model has the property that the ratio of the odds of selecting
two choices, e.g., p(a)/p(b), does not depend on what other choices
exist.

   the computational pleasantries that ols demonstrates are no longer
applicable, and we usually need to do a maximum likelihood search
to    nd the best parameters.

8.4 multilevel modeling

retake the snow   ake problem from page 236:
the models to this point all assumed that each
observation is independently and identically distributed relative to the others, but
this is frequently not the case.

one way in which this may break is if observations fall into clusters, such as fami-
lies, classrooms, or geographical region. a regression that simply includes a fami-
ly/classroom/region dummy variable asserts that each observation is iid relative to
the others, but its outcome rises or falls by a    xed amount depending on its group
membership. but the distribution of errors for one family may be very different
from that of another family, the unobserved characteristics of one classroom of
students is likely to be different from the unobserved characteristics of another,
and so on.

a better alternative may be to do a model estimation for each group separately. at
the level of the subgroup, unmodeled conditions are more likely to be constant for
all group members. then, once each group has been estimated, the parameters can
be used to    t a model where the observational unit is the group.

the other type of multilevel model is that where there are no distinct subclusters,
but we would like to give more detail about the derivation of the parameters. for
example, say that we feel that the propensity to consume a good is normally dis-
tributed, but we know that the likelihood of consumption is also based on a number
of factors. we just saw this model   it is the probit, which asserted that the likeli-
hood of consuming is simply the cdf of a normal distribution up to a parameter,
and that parameter has the form x  . the logit asserted that the id203 of
consuming was exp(  )/(1 + exp(  )), where    is of the form x  .

these are examples of multilevel models. there is a parent model, which is typ-
ically the primary interest, but instead of estimating it with data, we estimate it
using the results from subsidiary models. to this point, we have broadly seen two
types of model: simple distributions, like the normal(  ,   ) or the poisson(  ), and
models asserting a more extensive causal structure, like the ols family. either
form could be the parent in a multilevel model, and either form could be the child.

gsl_stats march 24, 2009

linear projections

289

id91 a classroom model may assume a distribution of outcomes for each
classroom and estimate   i and   i for each class, and then assert a linear
form that the outcome variable is   1  i+  2  i. unemployment models are typically
modeled as poisson processes, so one could estimate   i for each region i, but then
link those estimates together to assert that the      s have a normal distribution.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32

#include <apop.h>

void with_means(){

apop_data *d2 = apop_query_to_data("select avg(riders), year   1977 \

from riders, lines \
where riders.station=lines.station\
group by lines.line, year");

apop_model_show(apop_estimate(d2, apop_ols));

}

void by_lines(){

apop_data *lines = apop_query_to_text("select distinct line from lines");
int linecount = lines   >textsize[0];
apop_data *parameters = apop_data_alloc(0, linecount, 2);

for(int i=0; i < linecount; i ++){

char *color = lines   >text[i][0];
apop_data *d = apop_query_to_data("select riders, year   1977 from riders, lines\n\

where riders.station = lines.station and lines.line =    %s   ", color);

apop_model *m = apop_estimate(d, apop_ols);
apop_row(parameters, i, r);
gsl_vector_memcpy(r, m   >parameters   >vector);

}
apop_data_show(parameters);
apop_data *s = apop_data_summarize(parameters);
apop_data_show(s);

int main(){

}

}

source: ide  hi .
.

apop_db_open("data   metro.db");
printf("regression parent, normal child:\n\n"); with_means();
printf("normal parent, regression child:\n\n"); by_lines();

listing 8.8 two model estimations: with a regression parent and with a normal parent. online

listing 8.8 estimates two models of the change in ridersip of the washington metro
over time. you already saw two models of this data set, in the form of two dummy-
variable regressions, on page 282. here, we take a multilevel approach:    rst with
a regression parent and distribution child, and then the other way around.

gsl_stats march 24, 2009

290

chapter 8

having sql on hand pays off immensely when it comes to id91, because
it is so easy to select data where its group label has a certain value, or calculate

just under 2,000 data points, but because sql tables can be indexed, grouping and
aggregation can be much faster than sifting through a table line by line.

aggregates using ag  	 by clause. it will not be apparent in this example using
then lines 15   22 are af   loop that runs a separate regression for each color, and
writes the results in the a a e e   matrix. then, line 24    nds      and      for the

the    rst model is a regression parent, distribution child: we estimate a normal
model for each metro line for each year, and then regress on the set of        s thus
estimated. of course,    nding      is trivial   it   s the mean, which even standard sql
can calculate, so the output from the query on line 4 is already a set of statistics
from a set of normal distribution estimations. then, the regression estimates the
parameters of a regression on those statistics.

the second model is a distribution parent, regression child: we run a separate re-
gression for every metro line and then    nd the mean and variance of the ols
parameters. line 12 queries a list of the    ve colors (blue line, red line, . . . ), and

set of parameters.

the estimations tell different stories, and produce different estimates for the slope
of ridership with time. notably, the green line added some relatively unused sta-
tions in the 1990s, which means that the slope of the green line   s ridership with
time is very different from that of the other lines. this is very clear in the second
case where we run a separate regression for every line, but is drowned out when
the data set includes every line.

notice also that the size of the parent model   s data set changes with different spec-
i   cations: in the    rst model, it was 150; in the second, it was 5. thus, the    rst
model had greater con   dence that the slope was different from zero.15 gelman
& hill (2007) point out that we test parameters only on the parent model, which
means that if n is very small for some of the clusters, then this should have no
effect on the parent   even n = 1 for some clusters is not a problem. clusters with
small n should show large natural variation in child parameter estimates, and that
would be re   ected in the quality of the estimation of the parent parameters. but
since we neither make nor test claims about the child parameters, there is no need
to concern ourselves directly with the    quality    of the child parameters.

15in the second speci   cation, with    ve data points, one has a negative slope and four a positive slope. nonethe-

less, the mean is still about three times     /   n, giving us about 99% con   dence that the mean is signi   cant.

gsl_stats march 24, 2009

linear projections

291

to get a better handle on what differs among the lines and within the overall
regression, draw some graphs:

    total ridership per year

q8.4

    average ridership per year

    total ridership for each line.

    average ridership for each line.

the plots for each line are best written via af   loop based on the one be-

the difference between the total and average ridership is based on the fact
that the number of stations is not constant over time   produce a crosstab of
the data to see the progression of new station additions.

ginning on line    fteen of listing 8.8. how do the graphs advise the statistics
calculated by the two dummy and two multilevel models?

an example: multi-level logit we can break down the process of choosing a pres-
idential candidate into two steps:    rst, the voter de-
cides whether to choose a democrat, republican, or green candidate, and then
chooses among the offered candidates in the chosen party. the id203 of se-
lecting a candidate is thus p (candidate) = p (candidate|party)p (party). we would
thus need to do a few logit estimations: one for the democratic candidates, one for
the republican candidates, and one for the party choice.16 the iia property would
continue to hold within one party, but among parties, the change in candidates
could shift p (party), so the proportional odds of choosing a given democrat ver-
sus choosing a given republican may shift. listing 8.9 does some of these logit
estmations along the way to the multilevel model.

in the  gi  function, but the outcome variable will change. thus, the function
    the tails of the queries, in ai , are straightforward: get all candidates, get all
    out of the overwhelming amount of data, the  gi  function displays on screen

    once again, sql saves the day in doing these multilevel estimations. in this case,
the explanatory variables won   t change from model to model, so they are    xed

takes in the varying tail of the query and appends it to the    xed list of explanatory
variables to pull out consistent data.

democrats, get all republicans, get only party names.

only the mean odds of selecting each option.

16there was only one green candidate, ralph nader, so p (nader) = p (green).

gsl_stats march 24, 2009

292

#include <apop.h>

chapter 8

apop_model * logit(char *querytail){

apop_data *d = apop_query_to_mixed_data("mmmmmt", "select 1, age, gender, \

illegal_immigrants, aid_to_blacks, %s", querytail);

apop_text_to_factors(d, 0, 0);
apop_model *m = apop_estimate(d, apop_logit);
apop_data *ev = apop_expected_value(d, m);
apop_data_show(apop_data_summarize(ev));
return m;

apop_db_open("data   nes.db");
logit("favorite from choices");
logit("favorite from choices c, party p where p.person = c.favorite and p.party =    r   ");
logit("favorite from choices c, party p where p.person = c.favorite and p.party =    d   ");
logit("p.party from choices c, party p where p.person = c.favorite");

listing 8.9 logit models with and without grouping of candidates by party. online source:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

}

}

int main(){


a dida e .
.

q8.5

the example is not quite complete: we have the id203 of choosing a
candidate given a party and the id203 of choosing a party, but not the
product of the two. fill in the remainder of the code by    nding the predicted
odds for each candidate for each observation.
you can count how often a person chose the candidate that the model says
the person is most likely to pick. did the multilevel logit do a better job of
picking candidates than the standard one-level logit?

the simple concept of nesting together models is akin to mcfadden   s nested logit
(mcfadden, 1978). in fact, if these aren   t enough possibilities for you, see gelman
& hill (2007), who offer several more.

describing snow   akes now consider the multilevel model as used to model pa-
rameters that are used as inputs to other models. this is
the typical form for when we want to use one of the stories from chapter 7, like
the one about the negative binomial model or the one about the poisson, but we
want the parameter to vary according to per-observation conditions. as above, the
probit model from the last section    ts this description, as each agent has a cutoff
based on a linear model and that agent   s characteristics (i.e., the cutoff for person
i is xi  ), and that cutoff is then fed into a parent normal distribution.

gsl_stats march 24, 2009

293

linear projections

    find the linear estimates of the parameters for each observation, probably using

because many of apophenia   s model-handling functions can work with a model
that has only a log likelihood speci   ed, the process of writing a log likelihood for
such a model is supremely simple:

a   _d   on the input matrix and input parameters.
    use the stock  g_ ike ih  d function to    nd the log likelihood of the outcome
is simply the two steps above:    nd x  , then run af   loop to estimate a separate
parameter. the ai  program declares the model, pulls the data, and then runs the

listing 8.10 implements such a process. the    rst step is building the model, which
basically consists of reusing existing building blocks. the log likelihood function

poisson model for each row, based on the ith outcome variable and the ith poisson

data given the parameter estimates from the last step.

data through both the new model and the standard ols model.

#include <apop.h>

double pplc_ll(apop_data *d, apop_model *child_params){

apop_data *lambdas = apop_dot(d, child_params   >parameters, 0);
apop_data *smallset = apop_data_alloc(0, 1, 1);
double ll = 0;
for(size_t i=0; i < d   >vector   >size; i ++){

double lambda = apop_data_get(lambdas, i,    1);
apop_model *pp = apop_model_set_parameters(apop_poisson, lambda);
apop_data_set(smallset, 0,0, apop_data_get(d, i,    1));
ll += pp   >log_likelihood(smallset, pp);

}
return ll;

}

int main(){

apop_model pplc = {"poisson parent, linear child",    1,

.log_likelihood= pplc_ll, .prep=apop_probit.prep};

apop_db_open("data   tattoo.db");
apop_data *d = apop_query_to_data("select \

tattoos.   ct tattoos ever had    ct, tattoos.   year of birth    yr, \
tattoos.   number of days drank alcohol in last 30 days    booze \
from tattoos \
where yr+0.0 < 97 and ct+0.0 < 10 and booze notnull");

listing 8.10 a multilevel model. online source:   bi  eve  .
.

apop_data *d2 = apop_data_copy(d);
apop_model_show(apop_estimate(d, pplc));
apop_model_show(apop_estimate(d2, apop_ols));

}

gsl_stats march 24, 2009

294

chapter 8

in this case, you can interpret the parameters in a similar manner to the discussion
of the probit and logit cases above. the parent parameter    is calculated as x  , so
a 1% shift in xi leads to a   i% shift in   . thus, after checking whether the param-
eters are signi   cantly different from zero, you can directly compare the relative
magnitudes of the parameters to see the relative effect of a 1% shift in the various
inputs.

statistics is a subjective    eld at this point, you have seen quite a range of mod-
els: you saw the distribution models from chapter
7, the linear models from earlier in this chapter, and here you can see that you can
embed any one model into any other to form an in   nite number of richer models.
plus, there are the simulation and agent-based models, standing alone or nested
with a different type of model. the metro data has already been modeled at least
four different ways (depending on how you count), and the male-to-female ratio
among counties in still more ways. which model to choose?

a model is a subjective description of a situation, and many situations afford mul-
tiple perspectives. this is rare advice from a statistics textbook, but be creative.
we   re not living in the 1970s anymore, and we have the tools to tailor the model to
our perceptions of the world, instead of forcing our beliefs to    t a computationally
simple model. try as many models as seem reasonable. having many different
perspectives on the same situation only raises the odds that you will come away
with a true and correct understanding of the situation.

we also have some more objective tools at our disposal for selecting a model:
chapter 10 will demonstrate a means of testing which of two disparate models has
a better    t to the data. for example, running the code in listing 8.10 here reveals
that the likelihood of the alternative model is higher than the log likelihood of the
ols model. you can use the test on page 353 to test whether the difference in
likelihood is truly signi   cant.

   we can create models where the data used to estimate the parameters

in one model is generated by estimation in another model.

   one common use is id91: develop a model for individual groups
(states, classrooms, families) and then discuss patterns among a set of
groups (the country, school, or community).

   we can also use multiple levels to solve the snow   ake problem, spec-
ifying a different id203 of an event for every observation, but
still retaining a simple basic model of events. for example, the probit
is based on plugging the output from a standard linear form into a
normal cdf.

gsl_stats march 24, 2009

9

hypothesis testing with the clt

i   m looking for the patterns in static: they start to make sense
the longer i   m at it.

   gibbard (2003)

the purpose of descriptive statistics is to say something about the data you have.
the purpose of hypothesis testing is to say something about the data you don   t
have.

say that you took a few samples from a population, maybe the height of several
individuals, and the mean of your sample measurements is      = 175 cm. if you
did your sums right, then this is an indisputable, certain fact. but what is the mean
height of the population from which you drew your data set? to guess at the answer
to this question, you need to make some assumptions about how your data set
relates to the population from which it was drawn.

statisticians have followed a number of threads to say something about data they
don   t have. each starts with a data set and some assumptions about the environ-
ment and data generation method, and concludes with an output distribution that
can be compared to the data. here is a list of some common assumptions. it is
impossible for it to be comprehensive, and many of the categories overlap, but it
offers a reasonable lay of the discussion in the following chapters.

    classical methods: claim that the data was produced via a process that allows

application of the central limit theorem.

gsl_stats march 24, 2009

296

chapter 9

    id113: write down a likelihood function for any given

data/parameter pairing, and    nd the most likely parameter given the data.

    bayesian analysis: claim a distribution expressing prior beliefs about the parame-
ters and a likelihood function for the data on hand, then combine them to produce
a posterior distribution for the parameters.

    resampling methods: claim that random draws from the data are comparable to
random draws from the population (the bootstrap principle), then generate a dis-
tribution via random draws from the data.

    kernel/smoothing methods: claim that the histogram of the existing data is a lumpy
version of the true distribution; smooth the data to produce the output distribution.

all of these approaches will be discussed over the remainder of this book. this
chapter will focus on the    rst: making id136s about the population via use
of the central limit theorem (clt). the clt describes the distribution of the
sample mean, x, and works regardless of the form of the underlying data. that is,
no matter the true distribution of the data, the distribution of the sample mean has
a very speci   c form   as long as n is large enough. for relatively small n, another
of the above methods of id136, such as the monte carlo methods discussed in
chapter 11, may be preferable.

metadata

metadata is data about data. any statistic is a function
of data, so it is by de   nition metadata. be careful not
to confuse the characteristics of the data and metadata;
for example, the variance of the mean is almost always
smaller than the variance of the base data. like many
hypothesis tests, the central limit theorem is primar-
ily concerned not with the distribution of the base data
set, but the distribution of the mean of the data set.

the clt gives us a basis for the
normal distribution; we can then
produce variants based on the nor-
mal. the square of a normally dis-
tributed variable x will have a chi
squared distribution (which is writ-
ten as x2       2
1, and read as:
the statistic is distributed as a chi
squared with one degree of free-
dom). dividing a normal distribu-
tion by a transformed   2 distribution produces another distribution (the t distri-
bution), and the ratio of two   2   s produces an f distribution. because all of this is
rooted in the clt, the statements are true regardless of the vagaries of the under-
lying population that the statistics describe.

having found a means of describing the distribution of the unobservable popula-
tion parameter   , section 9.3 will then look a number of simple tests regarding
  . they are direct applications of the above distributions, and so are often given
names like the t test,   2 test, and f test.

the remainder of the chapter applies these building blocks in more complex struc-
tures to test hypotheses about more elaborate statistics. for example, if two inde-
pendent statistics   1 and   2 are       2
2. so if the squared distance

1, then   1+  2       2

gsl_stats march 24, 2009

hypothesis testing with the clt

297

between a histogram segment and a hypothesized distribution is       2
1, then the to-
tal distance between a thousand such segments and the hypothesized distribution
is       2
1000, and that total distance could thus be used to test the aggregate claim
that the data histogram is close to the distribution.

9.1 the central limit theorem the clt is the key piece of magic
for this chapter. make a series of n
independent, identically distributed draws, x1, x2, . . . xn, from a    xed underlying
population. the underlying population may have any nondegenerate distribution.1
let the mean of this sequence of draws be x, and the true mean of the overall
population be   . then as n        ,
   n

(x       )

(9.1.1)

  

    n (0, 1).

that is, no matter the underlying population, the distribution of a mean of a series
of draws will approach a normal distribution.

put another way, if we repeated the procedure and drew k independent data sets
from the population and plotted a histogram of x1, . . . , xk, we would eventually
see a familiar bell curve.

    on line four (and the    rst panel of figure 9.2), you can see the data from which the
program makes draws. it is nowhere near a bell curve: everything is either     11 or
    90.

because it is about the distribution of x, the clt embodies a two-stage procedure:
we    rst produce the means of a series of k sets of draws   metadata from the base
distribution   and then seek the distribution of the metadata (the id116), not the
data itself. listing 9.1 demonstrates exactly how this two-stage procedure works,
and is worth understanding in detail.

    the inner loop of the ake_d aw  function (thej-indexed loop) takes
  draws
from the cdf, and adds them to   a . when   a  is divided by
  in the
line after the loop, it becomes the mean of
  draws from the distribution. the
outer loop (thei-indexed loop) recordsd aw
  such means. line 23 plots the
data sets below. say that we haved aw
  data points in our data set, and we are

distribution of this set of several means.
this double-loop is the base of the clt, and is re   ected in the assumptions about

1by    nondegenerate    i mean that more than one outcome has positive id203. if your data is    fty items that
all have value 10, the mean of your samples will be 10 no matter how you slice them. however, if your sample
takes as few as two distinct values, the clt will eventually provide you with a bell curve. you can verify this by
modifying the code in listing 9.1. there are also theoretical distributions with in   nite variance, which also cause
problems for the clt, but this is of course not an issue for    nite data sets.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

gsl_stats march 24, 2009

298

#include <apop.h>

chapter 9

int drawct = 10000;
double data[] = {1, 2, 3, 10, 11, 10, 11, 90, 91, 90, 91};

gsl_vector *make_draws(int ct, gsl_rng *r){

double total;
gsl_vector *out = gsl_vector_alloc(drawct);

for(int i=0; i< drawct; i++){

total = 0;
for(int j=0; j< ct; j++)

total += data[gsl_rng_uniform_int(r, sizeof(data)/sizeof(data[0]))];

gsl_vector_set(out, i, total/ct);

}
return out;

}

int main(){

gsl_rng *r = apop_rng_alloc(23);

}

}

for (int ct=1; ct<= 1018; ct+=3){

listing 9.1 take the mean of an increasing number of draws. the distribution of the means

printf("set title    mean of %i draws   \n", ct);
gsl_vector *o =make_draws(ct, r);
apop_plot_histogram(o, 200, null);
gsl_vector_free(o);
printf("pause 0.6\n");

approaches a normal distribution. online source:
  de  .
.
    the ai  function is intended to show that the clt works best when each data
calls ake_d aw . at the    rst call,
 ==1, so ake_d aw  makes 10,000 draws
draws. the program dumps plots of the histograms tostd ut, so run the program
via./
  de  |g 	    .

claiming that they are normally distributed. we presume that they are normally
distributed (and not just constant) because a multitude of events have affected each
data point in some sort of haphazard way. that is, each individual data point went
through a process like the inner loop in lines 11   12, absorbing a large number of
random shocks. after all the little shocks, we gathered a single data point, as in
line 13.

from the distribution itself. the next call produces 10,000 data points where each
is the mean of four draws, and so on up to each data point being the mean of 1018

point is the mean of several draws from the base distribution. line 22 repeatedly

gsl_stats march 24, 2009

hypothesis testing with the clt

299

figure 9.2 shows a few frames of output from the program. the    rst frame of the
animation is simply a set of spikes representing the base data. the second frame,
where each data point is the mean of four draws, has a series of humps, because
some draws have all large numbers, some have three large numbers and one small,
some have two of each, and so on. in the third frame, there are more combinations
possible, so there are more humps. as the frames progress, the humps merge to-
gether to form a familiar bell curve. this is a re-telling of the counting story on
page 237, which explained why the binomial distribution approaches a bell curve
as well.

means of 1 draw

means of 4 draws

y
c
n
e
u
q
e
r
f

y
c
n
e
u
q
e
r
f

2000
1800
1600
1400
1200
1000
800
600
400
200
0

0

20

40

60

80

100

x

y
c
n
e
u
q
e
r
f

1000
900
800
700
600
500
400
300
200
100
0

0

20

40

60

80

100

x

means of 22 draws

means of 1018 draws

300

250

200

150

100

50

0

0

10

20

30

50

60

70

40
x

y
c
n
e
u
q
e
r
f

180

160

140

120

100

80

60

40

20

0

30

32

34

36

40

42

44

38
x

figure 9.2 sample outputs from the clt demo in listing 9.1

gsl_stats march 24, 2009

300

finally, notice the x-axes of the snapshots: the original data was plotted from 0   
100, but the scale in the fourth frame only goes from 30 to 45.2 so not only does
the distribution of x approach a bell curve, but it approaches a rather narrow bell
curve.

draw. [thanks to the creative use of ize f on line 12, you don   t need

modify line 4 to try different base distributions from which the system will

chapter 9

q9.1

to specify the size of the array. but see the footnote on page 125 on why
this could be bad form.] deleting the elements {1, 2, 3} produces some es-
pecially interesting patterns. what sort of data sets lead quickly to a bell
curve, and what data sets require averaging together thousands of elements
before achieving a decent approximation to the normal?

true mean   , and variance   2, then as n        , (  x       )(cid:14)      n

equation 9.1.1 put the story more formally: if we have a data set x with n elements,
approaches a n (0, 1)
distribution. from that regularity of nature, we can derive all of the distributions to
follow.

variants on variance

there is often confusion about when to use   2,   , or   /   n,
so it is worth a quick review.

    for any data set or distribution, the variance is notated as   2. the formula for the
i=1(xi       )2/n, so it makes sense that its symbol would

variance of a data set ispn

have a square included.

    for the normal distribution, the square root of the variance is known as the stan-
dard deviation,   , and is used to describe the    width    of the distribution. for exam-
ple, just over 95% of a normal distribution is within plus or minus two standard
deviations of the mean. outside of the normal distribution,    is rarely used.

    let us say that we have a data set x whose variance is   2. then the variance of the
mean of x, var(x), is   2/n, and the standard deviation of x is   /   n. once again,
it is important to bear in mind whether you are dealing with data or metadata.

   the central limit theorem states that if each observation xi is the
mean of some draws from an iid distribution, then as n        , the
distribution of xi follows equation 9.1.1.

   that is, if    is the overall mean and    is the square root of the vari-

ance of the set of xi   s, then (xi       )/ (  /   n) approaches a n (0, 1)

  i  f " e x a ge[0:100   \ " ; at the top of ai .

distribution.

2if the shift in x-axis bothers you, you could ask gnuplot to hold a constant scale by adding a line like

gsl_stats march 24, 2009

hypothesis testing with the clt

301

9.2 meet the gaussian family with the exception of the normal, the
distributions below are distinct from the
distributions of section 7.2. the distributions there are typically used to describe
data that we have observed in the real world. the distributions here are aimed at
describing metadata, such as the means and variances of model parameters.

normal the normal distribution, presented on page 241, will also be used to de-
scribe some of the parameters derived below. the big problem with the
normal distribution is that it depends on   , an unknown. it also depends on   ,
but we are frequently testing a claim that    has some    xed value, so we assume
rather than derive it. thus, much of the trickery in this section involves combining
distributions in ways such that the unknown      s cancel out.

  2 distribution the square of a variable with distribution n (0, 1) has a   2 distri-
bution with one degree of freedom, and the sum of n independent
  2-distributed variables is also       2, with n degrees of freedom. figure 9.3 shows
the distribution for a few different degrees of freedom.

    if x     n (0, 1), then x 2       2
1.
    if xi     n (0, 1) for i = 1, . . . , n, then x 2
    if xi       2

n, then e(xi) = n.

1 +        + x 2

n       2
n.

the summation property is immensely useful, because we often have sums of vari-
ables to contend with. the most common case is the sample variance, which is a
sum of squares. being a sum of squares of normally-distributed elements, it is
easy to show that (snedecor & cochran, 1976, p 74)

(cid:2)pn(x     x)2(cid:3)

  2

      2

n   1.

(9.2.1)

the numerator is the estimate of the sample variance times n     1, so we can use
this to test that the sample variance equals a given   2, or establish a con   dence
interval around an estimate of the variance. but we will see that it is useful for
much more than just describing variance estimates.

the sample variance is       2
n, because given the    rst n     1 data points
and the mean, the last one can actually be calculated from that data, meaning that
we effectively have the sum of n     1 variables       2
1, plus a no longer stochastic
constant. for more on degrees of freedom, see the sidebar on page 222.

n   1, not   2

it is worth mentioning the origin of the   2 distribution as a common form. pear-
son (1900) did a taylor expansion of errors from what we now call a multinomial

gsl_stats march 24, 2009

chapter 9

1 df
3 df
10 df

302

)
f
d
,

x
(
p

0.25

0.2

0.15

0.1

0.05

0

0

2

4

6

10

12

14

16

8
x

figure 9.3 the   2 distribution for 3, 5, and 10 degrees of freedom. being a sum of squares, it is
always greater than zero. as more elements are added in to the sum, the distribution
becomes less lopsided, and approaches a normal distribution.

distribution, of the form k1   + k2  2 + k3  3 +        , where the ki   s are appropriate
constants. he found that one can get satisfactory precision using only the   2 term
of the series. the anova family of tests is based on this approximation, because
those tests claim that the data are random draws that    t the story of the multino-
mial distribution (as on page 240), so a sum of such distributions leads to a   2
distribution as well.

student   s t distribution let x be a vector of data (such as the error terms in

a regression). then

x       
    /   n     tn   1,

where      = x   x/n (a scalar). it might look as though this is just an approximation
of the normal, with      replacing   , but it is not. to see where the form of the t
distribution came from, consider dividing the clt equation (equation 9.1.1),

x       
     n     n (0, 1),

gsl_stats march 24, 2009

hypothesis testing with the clt

303

)
f
d
,

x
(
p

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

1 df
3 df
10 df

-4

-2

0
x

2

4

figure 9.4 the t distribution for 1, 3, and 10 degrees of freedom. for one degree of freedom, the
distribution has especially heavy tails   the variance is in   nite   but as df grows, the
distribution approaches a n (0, 1).

by

then

  2    s   2
r     2
  x       
  2! =
     n ,r     2

x       
    /   n

.

.

n   1
n     1

the key stumbling block, the unknown value of   , cancels out from the numera-
tor and denominator. this is a work of genius by mr. student, because he could
calculate the exact shape of the distribution through straightforward manipulation
of the normal and   2 tables.3 some t distributions for various degrees of freedom
are pictured in figure 9.4.

    the t1 distribution (i.e., n = 2) is called a cauchy distribution.
    as n        , the tn distribution approaches the n (0, 1) distribution.

3student is actually mr. william sealy gosset, who published the t distribution in 1908 based on his work as

an employee of the guinness brewery.

gsl_stats march 24, 2009

chapter 9

10, 10 df
2, 2 df
10, 1 df
1, 10 df

304

)
2
f
d
,
1
f
d
,

x
(
p

1.2

1

0.8

0.6

0.4

0.2

0

0

0.5

1

1.5

2.5

3

3.5

4

2
x

figure 9.5 the f distribution for various pairs of numerator/denominator degrees of freedom.

f distribution

instead of a ratio of an n and ap  2, you could also take the ra-

tio of two   2-distributed variables. the      s in both denominators
would again cancel out, leaving a distribution that could be calculated from the   2
tables. this is the derivation and de   nition of the f distribution:

[  2

m/m]/[  2

n/n]     f (m, n).

see figure 9.5 for some sample f distributions.

also, consider the square of a t distribution. the numerator of a tn distribution is
a normal distribution, so its square is a   2
1; the denominator is the square root of
a   2
n. thus, the square of a tn-distributed
variable has an f1,n distribution as well.

n distributed variable, so its square is a   2

the f distribution allows us to construct tests comparing one   2-distributed vari-
able in the numerator to another   2-distributed variable in the denominator, and
either of these   2 variables could be the sum of an arbitrary number of elements.
we can thus use the f distribution to construct comparisons among relatively com-
plex statistics.

command-line programa   _   k	 . for getting the the value of the pdfs at a

lookup tables there are three things that cover most of what you will be doing
with a distribution:    nding values of its pdf, values of its cdf,
and occasionally values of its inverse cdf. for a single quick number, see the

gsl_stats march 24, 2009

hypothesis testing with the clt

305

ation itself will be delayed to chapter 11.

the next most common distribution calculation found in tables in the back of statis-

    the mean for the normal function is    xed at zero, so modify x accordingly, e.g., if

given point from inside a program, here are the headers for the gsl   s pdf lookup
functions:

double gsl_ran_gaussian_pdf (double x, double sigma);
double gsl_ran_tdist_pdf (double x, double df);
double gsl_ran_chisq_pdf (double x, double df);
double gsl_ran_fdist_pdf (double x, double df1, double df2);

calculate the cdf below a point, i.e.r x
the cdf above a point, i.e.r    x f (y)dy. these sum to one, so you can express any

    the pre   xg  _ a  indicates that these functions are from the random number
generation module (#i 
 	de<g  /g  _ a di  .h>). random number gener-
x is drawn from a n (7, 1), then ask the gsl forg  _ a _ga	  ia _ df x	7 
1 .
tics texts is calculating the cdf above or below a point. the 	functions below
f (y)dy, while the		functions calculate
. . . plus all of these with the  replaced by a	.
of the letter p can easily lead to confusion. theg  _
df_ga	  ia _  function
than zero, and theg  _
df_ga	  ia _	 function gives the infrequently-used q-
1	g  _
df_ga	  ia _  2.2 1 ==g  _
df_ga	  ia _	 2.2 1 .

these will be used to test hypotheses, which in this context are claims like    > 0.
if you are shaky with hypothesis tests, see the next section. but if you are well-
versed with the traditional notation for hypothesis tests, notice that the overuse

value for the same hypothesis. put another way, if we    nd that the mean of our
normally distributed data is 2.2 standard deviations below zero, then we reject the
one-tailed hypothesis that the mean is less than or equal to zero with con   dence

double gsl_cdf_gaussian_p (double x, double sigma);
double gsl_cdf_tdist_p (double x, double df);
double gsl_cdf_chisq_p (double x, double df);
double gsl_cdf_fdist_p (double x, double df1, double df2);

gives what is known as the p-value for the one-tailed test that the mean is less

area in terms of whichever function is clearest.

here is the list of functions:

      

for a hypothesis that    > 0, everything is reversed. here is a table that strives to
clarify which function goes with the con   dence with which the null is rejected and

gsl_stats march 24, 2009

306

chapter 9

which goes with the p-value, and when:

g  _..._ 
g  _..._	

g  _..._	
g  _..._ 

h0 :    > 0 h0 :    < 0

con   dence

p-value

for a centered two-tailed test, the p-value takes the form

2 * gsl_min(gsl_ran_gaussian_p(mu, sigma), gsl_ran_gaussian_q(mu, sigma))
// or equivalently,
2 * gsl_ran_gaussian_q(fabs(mu), sigma)

the con   dence with which we fail to reject the two-tailed null is one minus this.

in the other direction, we may want to know where we will need to be to reject a
hypothesis with 95% certainty. for example, a value-at-risk oriented regulator will
want to know the worst one-day loss a bank can expect over a month. to formalize
the question, what is the value of the 1-in-20, or 5%, point on the cdf? assuming
a normal(  ,   ) distribution of pro   t and loss,4 the bank will report a value at risk

ofg  _
df_ga	  ia _ i v 0.05         . here are the requisite function
. . . plus all of these with the i vs replaced by	i vs.

double gsl_cdf_gaussian_pinv (double p, double sigma);
double gsl_cdf_tdist_pinv (double p, double df);
double gsl_cdf_chisq_pinv (double p, double df);
double gsl_cdf_fdist_pinv (double p, double df1, double df2);

declarations:

q9.2

the power of a test is the likelihood of successfully rejecting the null hy-
pothesis if there really is an effect in the data and the null should be rejected
(see page 335 for more). when designing an experiment, you will need to
estimate the power of a given design so you can decide whether to gather
ten samples or a million.
i expect the mean of my normally-distributed data to be 0.5, and      to be 1.1.
given these assumptions, what must n be to reject the null hypothesis    = 0
with 99.9% con   dence? what if the normal approximation assumption is
deemed inapplicable, so the data is taken to be t-distributed?

4this assumption is false. securities typically have leptokurtic (fat-tailed) returns; see page 230.

gsl_stats march 24, 2009

hypothesis testing with the clt

307

   the square of a normal distribution is a   2 distribution.

   both of these distributions rely on an unknown variance   2. we can
guess at   2, but then our con   dence intervals are mere approxima-
tions as well.

   the ratio of a normal over the square root of a transformed   2 dis-
tribution is a t distribution. by taking the ratio of the form of the
two distributions, the unknown      s cancel out, so a valid con   dence
region can be constructed from a    nite amount of data.

   the ratio of two   2 distributions is an f distribution. again, the un-

known      s cancel out.

9.3 testing a hypothesis

the chapter to this point has discussed how
certain manners of gathering data and aggre-
gating it into a statistic, such as taking its mean or taking the sum of squares, lead
to certain known distributions. thus, if we have a statistic produced in such a man-
ner, we can evaluate the con   dence with which a claim about that statistic is true.
for example, the mean of a data set can be transformed to something having a t
distribution (assuming the clt holds). similarly for the difference between the
means for two data sets, so a precise id203 can be placed on claims about the
difference in two means.

data is equal to   h . the procedure to the test:

claiming a fixed mean this test is sometimes called a z-test, but see the foot-
note below. the claim is that the mean of a column of

    find the mean of the data     .
    given the variance of the data set     2
via     m =     d/   n.
    for a one-tailed test,    nd the percentage of the tn   1 distribution that is over

|  h         |/(    /   n), i.e.,g  _
df_ di  _	 fab    h          /    m  	1 . report
50% poverty rate? use the county-level info from the  ve  y_ 
 _a  
column from thei 
  e table ofda a	
e  	 .db as your data set. given

can we reject the claim h0: the typical puerto rican county has over a

    for a two-tailed test, report twice the calculated number as the p-value.

d, estimate the standard deviation of the mean

this as the p-value.

q9.3

that the us census bureau de   nes poverty by the cost of living in the main
part of the united states, how would you interpret your result?

gsl_stats march 24, 2009

308

chapter 9

consider the parameters      from the ols regression, which you will recall takes
the form      = (x   x)   1x   y. this is a more complex expression than a simple
mean, but each element of the      vector,     0,     1, . . . , can still be shown to have a
simple t distribution. thus, the standard test that a regression parameter   i is sig-
ni   cantly different from zero is a variant of the test here regarding a t-distributed
scalar. the full details will be given in the section on regression tests below.

comparing the mean
of two sets

among the most common and simplest questions is: are
two sets of observations from the same process? chapter
3 (page 109) already showed how to do a t test to test
this claim.5 you are encouraged to reread that section with an eye toward the test
procedure.

reporting con   dence

there is some tradition of reporting only whether the
p value of a test is greater than or less than some ar-
ti   cial threshold, such as p > 0.05 or p < 0.05. but
gigerenzer (2004) cites fisher (1956) as stating:

. . . no scienti   c worker has a    xed level
of signi   cance at which from year to
year, and in all circumstances, he re-
jects hypotheses; he rather gives his
mind to each particular case in the light
of his evidence and his ideas.

based on this observation, it would be better form
to list the actual con   dence calculated, and allow the
reader to decide for herself whether the given value
provides small or great con   dence in the results. the
error bars from chapter 5 provide a convenient way to
do this.

the paired t test is a common vari-
ant to the standard t test. say that the
data are paired in the sense that for
each element in the    rst set, there is
an element in the second set that is
related; put another way, this means
that for each element ai, there is a
corresponding element bi such that
the difference ai     bi makes real-
world sense. for example, we could
look at student scores on a test be-
fore a set of lessons and scores by
the same students after the lessons.
then, rather than looking at the t
distribution for the before data and
comparing it to the t distribution for

5there is no standardized naming scheme for tests. a test basically consists of three components:

1. the context,
2. the statistic to be calculated, and
3. the distribution that the statistic is compared to.
thus, there are tests with names like the paired data test (using component 1), sum of squares test (component 2),
or f test (component 3).

there is no correct way to name a procedure, but you are encouraged to avoid approach #3 where possible.
first, there are really only about four distributions (normal,   2, t, f ) that are used in most real-world applica-
tions, which means that approach #3 gives us only four names for myriad tests. two people could both be talking
about running a chi-squared test and    nd that they are talking about entirely different contexts and statistics.

there is an odd anomaly regarding naming customs for the normal distribution: rather than calling the statistic
to be compared to the normal a normal statistic or gaussian statistic, it is typically called a z statistic. there is a
z distribution, but it has nothing to do with the z test: it is one half the log of an f distribution, and is no longer
commonly used because the f is slightly more convenient.

finally, which distribution to use depends on the context and data: if a statistic has a t distribution for small
n, then it approaches normal as n        , so we could easily    nd ourselves in a situation where we are looking
up the statistic for what is called a t test on the normal distribution tables, or looking up a z statistic in the t
distribution tables.

gsl_stats march 24, 2009

hypothesis testing with the clt

thea   _ ai ed_ _ e   function to run this test where appropriate.

the after data, we could look at the vector of differences ai     bi and    nd the con-
   dence with which zero falls in the appropriate t distribution. this is generally a
more powerful test, meaning that we are more likely to reject the null hypothesis of
no difference between the two vectors, and therefore the paired t test is generally
preferable over the unpaired version (when it is applicable). apophenia provides

309

  2-based tests one quick application of the   2 distribution is for testing whether
a variance takes on a certain value. we posit that the denominator
of equation 9.2.1 is a    xed number, and then check the   2 tables for the given
degrees of freedom. this is a relatively rare use of the distribution.

a more common use is to take advantage of the summation property to combine
individual statistics into more elaborate tests. any time we have statistics of the
form (observed     expected)2/expected, where (observed     expected) should be
normally distributed, we can use pearson   s taylor series approximation to piece
together the statistics to form a   2 test. there are examples of this form in the
section on anova and goodness-of-   t testing below.

f -based tests because of all the squaring that goes into a   2 distributed statistic,
x and    x are indistinguishable, and so it becomes dif   cult to test
one-tailed claims of the form a > b. we could use the t test for a one-tailed claim
about a single variable, or an f test for a combination of multiple variables.

1
0
0

  1
  2
  3

let h0 be the claim that q      = c. this is a surprisingly versatile hypothesis. for

example, say that    is a vector with three elements,      
then h0 is   1 = 7. or, q =       
we want to test h0 :   2 = 2  3. then let q =       

      , and c = [7].
      , q =      
       and c = (cid:2)0(cid:3) gives h0 :   1 =   2. or, say
       and c = 0. in anova

terminology, a hypothesis about a linear combination of coef   cients is known as a
contrast.

0
1
   2

1
   1
0

to test all three hypotheses at once, simply stack them, one hypothesis to a row:

q    =      

1
0
1    1
0

0
0
1    2

       c =      

7
0
0

      .

(9.3.1)

gsl_stats march 24, 2009

310

chapter 9

any linear (or af   ne) hypothesis having to do with a parameter    can be    t into
this form.

de   ne q to be the number of constraints (rows of q   ), n the sample size, and
k the number of parameters to be estimated (  ). as before, let x represent x
normalized so that each column but the    rst has mean zero, and the    rst column
is the constant vector 1. now, if h0 is true and    was estimated using ols, then
q          n (c,   2q   (x   x)   1q),6 and we can construct a   2-distributed linear com-
bination of the square of q standard normals via

(q             c)   [q   (x   x)   1q]   1(q             c)

  2

      2
q.

(9.3.2)

alternatively, say that we are testing the value of the variance of the regression
error, and        n ; then

       
  2       2

n   k.

(9.3.3)

n     k

(q             c)   [q   (x   x)   1q]   1(q             c)

as above, we can divide scaled versions of equation 9.3.2 by equation 9.3.3 to
give us a statistic with an f distribution and no unknown   2 element:

to do this, you will need to feed the function an estimate of    and ana   _	
da a set indicating the set of contrasts you wish to test, whose vector element is

if you have read this far, you know how to code all of the operations in equation
9.3.4. but fortunately, apophenia includes a function that will calculate it for you.

    fq,n   k.

(9.3.4)

       

q

c and matrix element is q   . as in equation 9.3.1, each row of the input matrix
represents a hypothesis, so to test all three equality constraints at once, you could
use a vector/matrix pair like this:

double line = {

7, 1, 0, 0
0, 1,    1, 0
0, 0, 1,    2};

apop_data *constr = apop_line_to_data(line, 3, 3, 3);

6for any other method, the form of the variance is q   (the variance from section 8.2)q. see, e.g., amemiya

(1994).

gsl_stats march 24, 2009

hypothesis testing with the clt

311

together with that code.

listing 9.6 presents a full example.

    the constraint is only one condition: that   3 = 0.

as commonly used is so closely married to ols-type regressions.

    it runs a regression on the males-per-female data from page 267, so link this code

the    nal [vector|matrix] form for the constraint matches the form used for con-
straints on pp 152   153, but in this case the constraints are all equalities.

    thea   _f_ e   function takes in a set of regression results, because the f test
listing 9.6 run an f -test on an already-run regression. online source:f e  .
.

double line[] = {0, 0, 0, 1};
apop_data *constr = apop_line_to_data(line, 1, 1, 3);
apop_data *d = query_data();
apop_model *est = apop_estimate(d, apop_ols);
apop_model_show(est);
apop_data_show(apop_f_test(est, constr));

#include "eigenbox.h"

int main(){

}

here is a useful simpli   cation. let r2 be the coef   cient of determination (de   ned
further below), n be the number of data points, k be the number of parameters
(including the parameter for the constant term), and    be the f -statistic based on
q = i and c = 0. then it can be shown that
(n     k)r2
k(1     r2)

(9.3.5)

=   .

verify the identity of equation 9.3.5 using equation 9.3.4 and these de   ni-
tions (from page 228):

q9.4

yest     x   (the estimated value of y),

ssr    x (yest     y)2,
sse            , and
r2    
.

ssr
sse

gsl_stats march 24, 2009

312

chapter 9

statistical custom is based on the availability of computational shortcuts, so the f
statistic of equation 9.3.5 often appears in the default output of many regression
packages.7 it is up to you to decide whether this particular test statistic is relevant
for the situation you are dealing with, but because it is a custom to report it, apo-
phenia facilitates this hypothesis test by assuming it as the default when you send

in u   variables, as ina   _f_ e   e  i a e  u   .
data set you produced for the exercise on page 278, then passing thea   _	
  de  thus produced toa   _f_ e   to    nd the f statistic and apohenia   s

verify the identity of equation 9.3.5 by running a id75 on the

q9.5

r2-   nding function to    nd the sse and ssr.

   the simplest hypothesis test regarding the parameters of a model is
the t test. it claims that the mean of a data set is different from a given
value of   . a special case is the claim that the mean of two data sets
differ.

   the   2 test allows the comparison of linear combinations of allegedly
normal parameters. but since everything is squared to get the   2 pa-
rameter, it can not test asymmetric one-tailed hypotheses.

   the f test provides full generality, and can test both one-tailed and
two-tailed hypotheses, and claims that several contrasts are simulta-
neously true.

9.4 anova anova is a contraction for analysis of variance, and is a catch-
all term for a variety of methods that aim to decompose a data
set   s variance into subcategories. given a few variables and a few groups, is there
more variation between groups or within groups? can the variation in a dependent
variable be attributed primarily to some independent variables more than others?

you may want to re-run e   a  va.
, which    rst appeared as listing 7.2 on

the descriptive portion of anova techniques was covered back on pages 224   
227. this section covers the hypothesis testing part of anova.

page 226. it produces an anova table that includes several sums of squared er-
rors, and the ratio between them. at this point, you will recognize a sum of squared
errors as having a   2 distribution (assuming the errors are normally distributed),
and the df -weighted ratio of two sums of squared errors as being f -distributed.

7scheff   (1959) parodies the singular focus on this form by calling it    the    f test throughout the book.

gsl_stats march 24, 2009

hypothesis testing with the clt

313

thus, the traditional anova table includes an f test testing the claim that the
among-group variation is larger than the within-group variation, meaning that the
grouping explains a more-than-random amount of variation in the data.

independence

the crosstab represents another form of grouping, where the rows
divide observations into the categories of one grouping, and the
columns divide the observations into categories of another. are the two groupings
independent?

to give a concrete example, say that we have a two-by-two array of events, wherein
178 people chose between up/down and left/right:

left right
30
24
54

86
38
124

  
116
62
178

up

down
  

is the incidence of up/down correlated to the incidence of left/right, or are the
two independent? draws from the four boxes should follow a multinomial dis-
tribution: if up/down were a bernoulli draw with probabilities pu and pd, and
left/right were a separate, independent bernoulli draw with probabilities pl and
pr, then the expected value of up/left would be eu l = npu pl, and similarly
for edl, eu r, and edr. notating the actual incidence of up/left as ou l = 30,
we can use the fact (from page 301) that the   2 is a reasonable approximation of
errors from a multinomial distribution to say that the observed variance over the
expected value (ou l     eu l)2/eu l       2. similarly for the other three cells, so
the sum
(ou l     eu l)2

listing 9.7 calculates this, once the long way and twice the short way. the
a 
_	

hi_  	a ed function calculates equation 9.4.1, using the  e_
hi_   function
to calculate each individual term. finally, ai  gathers the data and calls the above
functions. after all that, it also callsa   _ e  _a  va_i de e de 
e, which

this expression has one degree of freedom because the horizontal set has two
elements and one mean     one df; similarly for the vertical set; and 1 df    1 df =
1 df. if there were three rows and six columns, there would be 2    5 = 10 df.

(odr     edr)2

(ou r     eu r)2

(odl     edl)2

      2
1.
(9.4.1)

edr

eu r

edl

eu l

+

+

+

does all this work for you on one line.

the distribution of means of a series of binomial draws will approach a normal as
n        , but for many situations, n is closer to around ten. for such a case, fisher

chapter 9

gsl_stats march 24, 2009

314

#include <apop.h>

double one_chi_sq(apop_data *d, int row, int col, int n){

apop_row(d, row, vr);
apop_col(d, col, vc);
double rowexp = apop_vector_sum(vr)/n;
double colexp = apop_vector_sum(vc)/n;
double observed = apop_data_get(d, row, col);
double expected = n * rowexp * colexp;
return gsl_pow_2(observed     expected)/expected;

}

double calc_chi_squared(apop_data *d){

double total = 0;
int n = apop_matrix_sum(d   >matrix);

for (int row=0; row <d   >matrix   >size1; row++)

for (int col=0; col <d   >matrix   >size2; col++)

total += one_chi_sq(d, row, col, n);

return total;

}

int main(){

double dataline[] = { 30,86,

}

24,38 };

apop_data *data = apop_line_to_data(dataline, 0, 2, 2);
double stat, chisq;

stat = calc_chi_squared(data);
chisq = gsl_cdf_chisq_q(stat, (data   >matrix   >size1     1)* (data   >matrix   >size2     1));
printf("chi squared statistic: %g; p, chi   squared: %g\n", stat,chisq);
apop_data_show(apop_test_anova_independence(data));
apop_data_show(apop_test_   sher_exact(data));

listing 9.7 pearson   s   2 test and fisher   s exact test. online source:fi he .
.
and its calculation is trivial: just calla   _ e  _fi he _exa
 , as in the last
  scaling how would the calculation be affected if we replicated every count in

(1922) calculated the id203 of a given table using direct combinatorial com-
putation. the equations for the fisher exact test are a mess, but the story is the same
as above   its null hypothesis is that up/down and left/right are independent   

line of listing 9.7.

the data set into k counts, so o   u l = kou l and e   u l = keu l? then
(o   u l     e   u l)2/e   u l = k(ou l     eu l)2/eu l. that is, scaling the data set by k
scales the test statistic by k as well. for almost any data set, there exists a k for
which the null hypothesis will be rejected.

gsl_stats march 24, 2009

hypothesis testing with the clt

315

across data sets, the scale can easily be different, and statistical signi   cance will
be easier to achieve in the set with the larger scale. generally, it is tenuous to as-
sert that a data set with a test statistic in the 96th percentile of the   2 distribution
diverges from independence less than a data set whose test statistic is in the 99.9th
percentile. use the test to establish whether the data rejects the null hypothesis,
then use other methods (a simple covariance will often do) to establish the magni-
tude of the difference.

for comparison, prior tests involving the mean are not as sensitive to scale. no-
tably, consider the ratio upon which the central limit theorem is based, after
every element of the data vector x is scaled by k:

mean

pvar /n

= p(kx     kx)/n
pp(kx     kx)2/n2
= p(x     x)
pp(x     x)2

.

all else equal, the ratio of the mean top    2/n (often written     /   n) is not affected

by the scale of x, or even the number of elements in the data set, the way the   2
statistic above was affected by rescaling.

9.5 regression

in the prior chapter, we used the id75 model for
purely descriptive purposes, to    nd the best way to project y
onto x. if we add the assumption that    is normally distributed, then we can also
test hypotheses regarding the parameter estimates. given this assumption, it can
be shown that the coef   cients on the independent variables (the    vector) have a t
distribution, and therefore the con   dence with which an element of    differs from
any constant can be calculated (see, e.g., greene (1990, p 158)).

the covariance matrix of   ols is    =   2(x   x)   1, where   2 is the variance of
the regression   s error term: if    is the vector of errors, and there are n data points
and k regressors (including any constant column 1), then        /(n     k) provides
an unbiased estimate of   2.8 the estimated variance of   1 is the    rst diagonal
element,   11; the estimated variance of   2 is the second diagonal element,   22;
and so on for all   i.

as is typical for a test of a statistic of the data, the count of degrees of freedom
is data points minus constraints; speci   cally, for n data points and k regression
parameters (including the one attached to the constant column), df = n     k.

8the form of the variance of the error term analogizes directly with the basic one-variable unbiased estimate
i=1(xi     x)2/(n     1). first, the setup of ols guarantees that    = 0, so   i        =   i, and
   matches the numerator in the basic formula. the denominator in all cases is the degrees of freedom; for

of variance, pn
thus   
example, with k regressors and n data points, there are n     k degrees of freedom.

   

gsl_stats march 24, 2009

316

chapter 9

given the estimated variance     2 for   i and any constant c, we could write down a
test statistic |  i     c|/    , and then check that statistic against the tn   k distribution
to test the claim that   i = c. this test bears a close resemblance to the test for
the mean of a data set (also a t-distributed scalar statistic) presented on page 307.
if you have a joint hypotheses about contrasts among the elements of   , you can
directly apply the above discussion of f tests: just use the estimated mean of   ,
its covariance matrix   , and n     k degrees of freedom.

comparison with anova

if a regression consists of nothing but dummy vari-
ables, then it can be shown that the process is equiv-

alent to the anova-style categorization methods above.

q9.6

q9.7

alaska is famously low on females, due to its many features that distin-
guish it from the lower 48 states. create a dummy variable where 1=alaska,
0=other state, and regress males per female against both the alaska dummy
and population density (and a constant 1, of course). are one or both of the
independent variables signi   cant?

run an independence test on the two-by-two table whose row categories
are alaska and not-alaska, and whose column categories are males and
females. (hint: you will need to combine the population and males per fe-
males columns to produce a count for each region, then sum over all re-
gions.)
how does the test differ if you compare the percent male/female or the
total count of males and females in each region? what changes in the story
underlying the test, and which version better represents the hypothesis?

ols (along with its friends) has two real advantages over testing via crosstab
approaches like anova. first, it readily handles continuous variables, which
anova can handle only via approximation by rough categories.

second, it allows the comparison of a vast number of variables. anovas typically
top out at comparing two independent variables against one dependent, but an ols
regression could project the dependent variable into a space of literally hundreds
of independent variables. in fact, if you run such a regression, you are basically
guaranteed that some number of those variables will be signi   cant.

the multiple
testing problem

freedman (1983) showed the dangers of data snooping by ran-
domly generating a set of 51 columns of 100 random numbers
each.9 he set one column to be the dependent variable to be ex-

9data snooping used to also be called data mining, but that term has lost this meaning, and is now used to

refer to categorization techniques such as classi   cation trees.

gsl_stats march 24, 2009

hypothesis testing with the clt

317

plained, and the other    fty to be potential regressors. using a simple exploratory
technique, he culled the    fty potential explanatory variables down to 15 variables.
he then ran a 15-variable regression, and found that 14 variables were signi   cant
with a p-value better than 25%, and six were signi   cant with p better than 5%.
other tests of the regression also indicated a good    t. but the data was pure noise
by construction.

recall from the    rst paragraph of this book that there are two goals of statistical
analysis, and they directly con   ict. if a researcher spends too much time looking
for descriptive statistics about the data, then he is committing informal data snoop-
ing, akin to freedman   s initial exploratory regression, and thus biases the chances
of rejecting a null in her favor. but it would be folly for the researcher to not check
the data for outliers or other quirks before running the regression, or to embark
upon producing an entirely new data set for every regression.

what is the correct balance? statistics has no answer to this, though most statisti-
cians do. those in the descriptive-oriented camp are very serious about the impor-
tance of good graphical displays and viewing the data every way possible, while
those in the testing-oriented camp believe that so much pre-test searching is simply
asking for apophenia.

here is another spin on the issue: people who are testing exactly one hypothesis
tend to develop an affection for it, and become reluctant to reject their pet hypoth-
esis. thus, research as conducted by humans may improve if there are multiple
hypotheses simultaneously competing. chamberlin (1890, p 93) explains:

love was long since represented as blind, and what is true in the per-
sonal realm is measurably true in the intellectual realm. . . . the mo-
ment one has offered an original explanation for a phenomenon which
seems satisfactory, that moment affection for his intellectual child
springs into existence; and as the explanation grows into a de   nite
theory, his parental affections cluster about his intellectual offspring,
and it grows more and more dear to him, so that, while he holds it
seemingly tentative, it is still lovingly tentative. . . . the mind lingers
with pleasure upon the facts that fall happily into the embrace of the
theory, and feels a natural coldness toward those that seem refrac-
tory. . . . there springs up, also, an unconscious . . . pressing of the facts
to make them    t the theory. . . . the search for facts, the observation of
phenomena and their interpretation, are all dominated by affection for
the favored theory until it appears to its author or its advocate to have
been overwhelmingly established. the theory then rapidly rises to the
ruling position, and investigation, observation, and interpretation are
controlled and directed by it. from an unduly favored child, it readily
becomes master, and leads its author whithersoever it will.

his solution, as above, is to test multiple hypotheses simultaneously.    the inves-

gsl_stats march 24, 2009

318

chapter 9

tigator thus becomes the parent of a family of hypotheses; and, by his parental
relation to all, he is forbidden to fasten his affections unduly upon any one.    he
also points out that maintaining multiple hypotheses allows for complex explana-
tions about how an outcome was partly caused by one factor, partly by another,
partly by another. after all, nature is not compelled to conform to exactly one
hypothesis.

apophenia   s model-as-object makes it very easy to test or mix diverse hypotheses,
as per chamberlin   s suggestion, and you will see more methods of comparing
models in later chapters. but as the number of models grows, the odds of failing to
reject at least one model purely by chance grows as well. there is no hard-and fast
rule for determining the    correct    number of models to test; just bear in mind that
there is a tension among multiple goals and a balance to be struck between them.

correcting for multiple testing moving on from informally poking at the data,
consider the case when the experiment   s basic de-
sign involves a systematic,    xed series of tests, like running a separate test for
every genetic marker among a list of a million. this is known as the multiple test-
ing problem, and there is a simple means of correcting for it.

say that a number is drawn from [0, 1], and the draw is less than p with id203
p. then the id203 that a draw is greater than p is 1     p, and the id203
that n independent draws are all greater than p is (1     p)n, which can be reversed
to say that the id203 that at least one of n independent draws is less than p is
1     (1     p)n.
thus, the id203 that, out of n tests with a    xed p-value, at least one will
indicate a positive result is    = 1    (1    p)n. for example, with p = 0.05 and n =
100, the likelihood of rejecting the null at least once is 1    (1    0.05)100     99.4%.
we can instead    x    at a value like 0.05 or 0.01, and reverse the above equation to
   nd the p-value for the individual tests that would lead to rejection of all nulls with
5% or 1% likelihood. a line or two of algebra will show that p = 1     (1       )1/n.
for n = 100 and    = 0.05, you would need to set the p-value for the individual
tests to 0.0005128. in the example of testing n = 1, 000, 000 genetic markers, if
the desired overall    = 0.05, then the p-value for the individual tests would be
5.129e   8.
there is a wonderfully simple approximation for the above expression: just let
p =   /n. for the    rst example above, this approximation is 0.05/100 = 0.0005;
for the second it is 0.05/1, 000, 000 = 5e   8. both of these approximations are
within about   2.5% of the true value.

gsl_stats march 24, 2009

hypothesis testing with the clt

319

thus, we have a simple rule, known as the bonferroni correction, for multiple tests:
just divide the desired overall p-value by the number of tests to get the appropriate
individual p-values. the correction is standard in biometrics but virtually unknown
in the social sciences. when reading papers with pages of regressions and no cor-
rections for multiple testing, you can easily apply this equation in your head, by
multiplying the reported individual p-value by the number of tests and comparing
that larger    gure to the usual signi   cance levels of 0.05 or 0.01.

   because we know their expected mean and covariances, the regres-
sion parameters for ols, iv, wls, and other such models can be
tested individually using the standard t test, or tested as a group via
an f test.

   when running a battery of several tests (based on a regression or oth-
erwise), use the bonferroni correction to create a more stringent sig-
ni   cance level. the common form of calculating the more stringent
p-value is to simply divide the one-test p-value by the number of tests.

9.6 goodness of fit

this section will present various ways to test claims of
the form the empirical distribution of the data matches
a certain theoretical distribution. for example, we often want to check that the
errors from a regression are reasonably close to a normal distribution.

the visually appealing way to compare two distributions is the q   q plot, which
stands for quantile   quantile plot. the    rst (x, y) coordinate plotted is x1 = the
   rst percentile of your data and y1 = the    rst percentile of the distribution you
are testing, the second is x2 = the second percentile of your data and y2 = the
second percentile of the ideal distribution, et cetera. to the extent that the data    ts
the ideal distribution, the points will draw out the x = y line, while digressions
from the line will stand out.

ther precipitation is normally distributed. it gathers the data in the usuala   _	
 	e y_  _da a manner, estimates the closest-   tting normal distribution, and

the    rst half of listing 9.9 presents an example, displaying a plot to check whe-

plots the percentiles of the data against the percentiles of the just-estimated dis-
tribution. as figure 9.8 shows, the data set closely    ts the normal distribution
(though the extremes of the bottom tail is a bit more elongated and the extreme of
the top tail a bit less so).

q9.8

modify listing 9.9 to test whether temperature or log of temperature is
normally distributed. would any other distribution    t better?

gsl_stats march 24, 2009

320

chapter 9

s
e
l
i
t
n
e
c
r
e
p

a
t
a
d

4.5

4

3.5

3

2.5

2

1.5

1

0.5

+

+

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

+

+

0.5

1

1.5

2

2.5

3

3.5

4

4.5

normal percentiles

figure 9.8 percentiles of precipitation on the y axis plotted against percentiles of the normal distri-

bution along the x axis.

#include <apop.h>

int main(){

apop_db_open("data   climate.db");
apop_data *precip = apop_query_to_data("select pcp from precip");
apop_model *est = apop_estimate(precip, apop_normal);
apop_col_t(precip, "pcp", v);
apop_plot_qq(v, *est, "out   le.gnuplot");

double var = apop_vector_var(v);
double skew = apop_vector_skew(v)/pow(var, 3/2);
double kurt = apop_vector_kurtosis(v)/gsl_pow_2(var)     3;
double statistic = v   >size * (gsl_pow_2(skew)/6. + gsl_pow_2(kurt)/24.);
printf("the skew is %g, the normalized kurosis is %g, "

distribution. the output is presented in figure 9.8. online source:      .
.

"and we reject the null that your data is normal with %g con   dence.\n",

skew, kurt, gsl_cdf_chisq_p(statistic, 2));

}

listing 9.9 pull data; estimate the normal that best    ts the data; plot the data against the ideal

higher moments a slightly more rigorous alternative means of testing for nor-
mality is to check the higher moments of the normal distri-
bution (bowman & shenton, 1975; kmenta, 1986, pp 266   267). there is a more
general chi-squared goodness-of-   t test for any distribution below.

gsl_stats march 24, 2009

hypothesis testing with the clt

321

a normal distribution has only two parameters, the mean and the standard de-
viation, and everything else about the normal distribution is de   ned therefrom.
notably, the third moment is zero, and the fourth moment is 3  4.

we already have everything we need to calculate the distribution of these statistics.
the skew and kurtosis are both the mean of an iid process (recall their de   nitions
on page 230: a sum divided by n), so their square is       2. let s be the third
moment of the data divided by   3 and let    be the fourth moment divided by   4.
then

has a   2 distribution with one degree of freedom, as does

some prefer to test both simultaneously using

ls = n(cid:20) s2
6(cid:21)
(cid:21) .
lk = n(cid:20)(       3)2
lsk = n(cid:20) s2
(       3)2

24

24

+

6

(cid:21) ,

which has a   2 distribution with two degrees of freedom.

the second half of listing 9.9 translates this formula into code. given the q   q
plot, it is no surprise that the test soundly fails to reject the null hypothesis that the
data is normally distributed.

chi-squared goodness-of-fit test

another alternative, keeping with the theme of this book, would be to bootstrap
the variance of the kurtosis, which would let you    nd a con   dence interval around
3  4 and state with some percentage of certainty that the kurtosis is or is not where
it should be; this suggestion is put into practice on page 365.

mathematically, it is simple. we have k bins, and two histograms:h0 holds the
histogram from which the draws were allegedly made, andh1 holds the data.10

say that we have a histogram and a vec-
tor of points that we claim was drawn
from that histogram. it would be nice to test the con   dence with which our claim
holds; this is a goodness-of-   t test.

then

10recall from page 314 that scaling matters for a   2 test: the histograms representing two pdfs will each sum
to one (by de   nition), while a histogram representing the density of a population of size n will have bins summing
to n (by de   nition). that means that the   2 statistics for a test of the pdfs and a test of the distribution of counts
will be different, with the null more likely to be rejected for the distribution of counts. so when investigating a
histogram, be careful that you are testing the right hypothesis; claims about the distribution of a population are
typically best represented by a test of the pdfs (   = 1) rather than the counts (   = n).

gsl_stats march 24, 2009

322

kxi=0

(h0   > bins[i]     h1   > bins[i])2

h0   > bins[i]

      2

k   1.

chapter 9

(9.6.1)

you will recognize this form as matching the (observed - expected)2/expected form
from the anova tests earlier in this chapter.

q9.9

on page 173, you plotted the leading digit of an arbitrary data set, and saw
that it sloped sharply down. now use a chi-squared goodness of    t test to
formally check that your data set    ts equation 5.4.1.

    write a function to produce a vector of nine elements, with the count
of elements in each slot equal to the number of data points with the
given leading digit. don   t forget that vectors count from zero but you
want the    rst slot to represent leading digit one, and to rescale your
   nal vector so that it is a pmf (i.e., its elements sum to one).

    equation 5.4.1 isn   t quite a pmf: the sum of its values from one to
nine isn   t one. thus, you will need to get the total mass, and rescale
your calculations from benford   s equation accordingly.

    once you have two nine-element vectors of equal mass, you can di-
rectly apply expression 9.6.1 to    nd the   2 statistic and run the   2
test.

#include <apop.h>

1
2
3
4
5
6
7
8
9
10
11

}

int main(){

apop_db_open("data   climate.db");
apop_data *precip = apop_query_to_data("select pcp from precip");
apop_model *est = apop_estimate(precip, apop_normal);
gsl_rng *r = apop_rng_alloc(0);
apop_model *datahist = apop_estimate(precip, apop_histogram);
apop_model *modelhist = apop_histogram_model_reset(datahist, est, 1e6, r);
apop_data_show(apop_histograms_test_goodness_of_   t(datahist, modelhist));

listing 9.10 the same precipitation data, another test. online source:g  dfi .
.
    lines 1   6 are a repeat of the query and estimation from      .
 (page 320).

listing 9.10 tests whether the precipitation data is normally distributed using the
  2 goodness-of-   t test.

323

expressing a model.

hypothesis testing with the clt

gsl_stats march 24, 2009

is covered in full on page 357.

    the new histogram gets    lled via random draws from the model, which means that

    by line ten, we have two histograms representing the data and the model, and they

    you can   t do a goodness-of-   t test on just any two histograms: the bins have to
match, in the sense that the range of each bin in the    rst histogram exactly matches
the range in the corresponding bin of the second. the easiest way to ensure that two
histograms match is to generate the second histogram using the    rst as a template,

    line eight turns the input data into a histogram. notice that it uses the samea   _	
e  i a e form as other models, because a histogram is just another means of
which is whata   _hi   g a _  de _ e e  does. if we wanted to compare
two vectors via this test, this line would usea   _hi   g a _ve
   _ e e .
we need to givea   _hi   g a _  de _ e e  the number of draws to make
(here, 1e6), and ag  _  g to provide random numbers. the use of theg  _  g
are in sync. thus, it is a simple matter to send the histograms toa   _hi   	
g a  _ e  _g  d e  _ f_fi  to calculate the statistic in expression 9.6.1.
produced using one of the above-mentioned methods,a   _ e  _k    g   v
    pull the log of gdp per capita data from theda a	wb.db data set.
    create a histogram (i.e., estimate ana   _hi   g a  model).
usinga   _hi   g a _  de _ e e .
    send the two histograms toa   _ e  _k    g   v.
    how do the test results compare with those produced bya   _	
hi   g a  _ e  _g  d e  _ f_fi ?

   nds the maximum difference in the cmfs and    nd the id203 that such a
cmf would occur if both histograms were from the same base data set. because
this test uses the ordering of the slices of the pmf, while the chi-squared test does
not, this test generally has higher power.

kolmogorov   s test serves as yet another test for normality, because it can compare
a data set   s cdf to that of a normal distribution.

kolmogorov (1933) suggested considering the steps in a histogram
to be a poisson process, and developed a test based upon this
parametrization [see also conover (1980)]. given two histograms

    fit a normal distribution and use it to create a matching histogram

is gdp per capita log-normally distributed?

kolmogorov   s
method

q9.10

gsl_stats march 24, 2009

324

q9.11

how about precipitation? figure 9.8 gave the strong suggestion that the data
is normally distributed; modify the code from the last example to formally
test the hypothesis that the data set is drawn from a normal distribution
using the kolmogorov   smirnov method.

chapter 9

   the q   q (quantile-to-quantile) plot gives a quick visual impression
of whether the distribution of the data matches that of a theoretical
distribution.

   we can test the claim that a data set is normally distributed using
the fact that the skew and kurtosis of a normal distribution are    xed
(given the mean and variance).

   more generally, we can compare any two distributions by dividing
them into bins and comparing the square of the deviation of one dis-
tribution from another via a   2 test.

   the kolmogorov   smirnov test offers another method for compar-
ing two distributions, which typically has more power than the chi-
squared method.

gsl_stats march 24, 2009

10

id113

since the fabric of the universe is most perfect and the work of a most wise creator,
nothing at all takes place in the universe in which some rule of maximum or minimum
does not appear.

itsa   _ axi 	 _ ike ih  d function.

whether by divine design or human preference, problems involving the search for
optima are everywhere. to this point, most models have had closed-form solutions
for the optimal parameters, but if there is not a nice computational shortcut to
   nding them, you will have to hunt for them directly. there are a variety of routines
to    nd optima, and apophenia provides a consistent front-end to many of them via

   leonhard euler1

given a distribution p(  ), the value at one input, p(x), is local information: we
need to evaluate the function at only one point to write down the value. however,
the optimum of a function is global information, meaning that you need to know
the value of the distribution at every possible input from x =        up to x =     in
order to know where the optimum is located.

this chapter will begin with the simple mathematical theory behind maximum
likelihood estimation (id113), and then confront the engineering question of how
we can    nd the global optimum of a function by gathering local information about
a small number of points. once the prerequisites are in place, the remainder of the
chapter covers id113s in practice, both for description and testing.

1letter to pierre-louis moreau de maupertuis, c. 1740   1744, cited in kline (1980, p 66).

gsl_stats march 24, 2009

326

chapter 10

10.1 log likelihood

and friends

to this point, we have met many id203 distri-
butions, whose pdfs were listed in sections 7.2 and
9.2. this section takes such a id203 distribution
p (x,   ) as given, and from that produces a few variant and derivative functions
that will prove to be easier to work with. also, a reminder: there is a list of notation
for the book on page 12.

let x1 and x2 be two independent, identical draws (iid) from a distribution. the
independence assumption means that the joint id203 is the product of the
individual probabilities; that is, p ({x1 and x2},   ) = p (x1,   )    p (x2,   ). the
assumption of identical distributions (i.e., that both are draws from the same dis-
tribution p (  ,   )) allows us to write this more neatly, as

p ({x1 and x2},   ) = yi={1,2}

p (xi,   ).

a id203 function gives the id203 that we   d see the data that we have
given some parameters; a likelihood function is the id203 that we   d see the
speci   ed set of parameters given some observed data. the philosophical implica-
tions of this distinction will be discussed further below.

there are three basic transformations of the likelihood function that will appear
repeatedly, and are worth getting to know.

de   ne the log likelihood function as ll     ln p (x,   )|x, the score as its derivative
with respect to   :

s             

    ln p
     1

...

    ln p
     n

          =         

   ll
     1

...

   ll
     n

          ,

and the information matrix as the negation of derivative of the score with respect
to   .

i =   

   s
     

=               

   2ll
     2
1

...

   2ll
     1  n

      
. . .
      

   2ll
     n  1

...

   2ll
     2
n

             .

gsl_stats march 24, 2009

id113

327

an example: bernoulli

say that we have nine draws from a bernoulli distribution,
which you will recall from page 237 means that each draw
is one with id203 p and is zero with id203 1    p, and say that in our case
   ve draws were ones and four were zeros. the likelihood of drawing    ve ones is
p5; the likelihood of drawing four zeros is (1     p)4; putting them together via the
independence assumption, the likelihood of an arbitrary value of p given this data
set x is

p (x, p)|x = p5    (1     p)4.

the log likelihood is thus

ll(x, p) = 5 ln(p) + 4 ln(1     p).

the score, in this case a one-dimensional vector, is

s(x, p) =

5
p    
and the information value (a 1    1 matrix) is
5
p2 +

i(x, p) =

4

(1     p)

4

(1     p)2 .

,

(10.1.1)

both intuitively and for a number of reasons discussed below, it makes sense to
focus on the most likely value of p   that is, the value that maximizes p (x, p) given
our observed data x. since the log is a monotonic transformation, p maximizes
p (x, p) if and only if it maximizes ll(x, p). recall from basic calculus that a
necessary condition for a smooth function f (x) to be at a maximum is that df
dx =
0   in the case of ll(x, p), that s(x, p) = 0. setting equation 10.1.1 to zero and
solving for p gives p = 5
9 .

but a zero derivative can indicate either a maximum or a minimum; to tell which,
look at the second derivative. if the second derivative is negative when the    rst
derivative is zero, then the    rst derivative is about to become negative   the func-
tion is beginning to dip downward. at this point, the function must be at a maxi-
mum and not a minimum.

since the information matrix is de   ned as the negation of the score   s derivative,
we can check that we are at a maximum and not a minimum by verifying that the
information value is positive   and indeed it is: for p = 5/9, i     4.05. in more
dimensions, the analog is that the information matrix must be positive de   nite; see
the exercise on page 269.

to summarize, given p = 5
9 , p     0.0020649, ll        6.182, s = 0, and i     4.05.
it is easy to check other values of p to verify that they produce lower probabilities
of observing the given data.

gsl_stats march 24, 2009

328

q10.1

verify the above statements.

chapter 10

zeros (in any order). put the data set in a global variable.

    generate a data set whose matrix element includes    ve ones and four

    produce an output model viaa   _e  i a e y 	 _da a a   _	
be   	  i .
    display the output model viaa   _  de _ h w; check that the prob-
    write a function that takes in ana   _da a struct holding a vector
the global data, usinga   _  g_ ike ih  d.
    send that function to your    af	 
 i   routine from page 191

with one item, and returns the log likelihood of that parameter given

ability is as it should be (i.e., 5/9 = .555).

(with the range [0, 1]) and verify that the log likelihood is at its highest
where the parameter is 5/9.

an example: probit

recall the probit model from page 283. it speci   ed that an
agent would act iff its utility from doing so is greater than a

normally-distributed error term. that is, act with id203

p (x,   ) =z x  

      

n (y|0, 1)dy,

where n (y|0, 1) indicates the standard normal pdf at y (and so the integral is the
cdf up to x  ).

reversing this, let xa be the set of x   s that led to action and xn be the set that led
to non-action. then the likelihood of    given the data set is

p (x,   )|x =yi   a

so

p (xa

i ,   )   yi   n(cid:0)1     p (xn

i ,   )(cid:1) ,

ll(x,   )|x =xi   a

ln p (xa

i ,   ) +xi   n

ln(cid:0)1     p (xn

i ,   )(cid:1) .

by the way, the logit (equation 8.3.1, page 284) tells the same story, but it simpli-
   es signi   cantly, to

gsl_stats march 24, 2009

id113

329

ll(x,   )|x =xi   a

xa

i       x   i

ln(cid:16)1 + exi  (cid:17) ,

where the    rst term counts only those who act, while the second includes every-
body. [q: verify the log likelihood using equation 8.3.1.]

unlike the binomial example above, we can not    nd the optimum of the log like-
lihood function for either the probit or logit using just a derivative and some quick
algebra. we will instead need to use a search method such as those discussed later
in this chapter.

  a digression: the

philosophy of notation

the id203 function has a frequentist interpre-
tation: if you give me a    xed distribution, the story
behind it, and a    xed parameter   , then after a few
million draws, x will occur p (x,   )|      100 percent of the time. the likelihood
function has no such interpretation, because we assume that the data was produced
using one model that had a    xed   , that we happen to be ignorant of. there is no
mysterious pool of      s from which ours was drawn with some id203.

thus, the id203 of x given    (and a model) is in many cases an objectively
veri   able fact; the likelihood of    given x (and a model) is a subjective construct
that facilitates various types of comparison among      s. the integral over all x is

always one (i.e., for any    xed   ,r   x p (x,   )dx = 1). the integral over all    of

the likelihood function, however, could be anything.

ronald aylmer fisher, the famed eugenicist and statistician whose techniques ap-
pear throughout this book, was vehement about keeping a clear distinction:    . . . [i]n
1922, i proposed the term    likelihood,    in view of the fact that, with respect to [the
parameter], it is not a id203, and does not obey the laws of id203, while
at the same time it bears to the problem of rational choice among the possible
values of [the parameter] a relation similar to that which id203 bears to the
problem of predicting events in games of chance. . . . whereas, however, in relation
to psychological judgment, likelihood has some resemblance to id203, the
two concepts are wholly distinct. . . .    (fisher, 1934, p 287) see pawitan (2001) for
more on the interpretation of likelihood functions.

the computer, there is no point writing down separate functions  x be a  and
 ike ih  d be a x    a single function will serve both purposes. just    x x to

but as a simple practical matter, the id203 of x given    xed parameter    is
p (x,   ), and the likelihood of    given    xed data x is the very same p (x,   ). at

produce a likelihood function over   , and    x    to get a id203 distribution of
values of x.

gsl_stats march 24, 2009

330

chapter 10

we have two choices for notation, both of which can lead to confusion. the    rst
is to use two distinct function names for id203 and likelihood   p (x) and
l(x) are typical   which clari   es the philosophical differences and leaves it to the
reader to recognize that they are numerically identical, and that both are functions
of x and   . the second option is to use a single function for both, which clari   es
the computational aspects, but leaves the reader to ponder the philosophical im-
plications of a single function that produces an objective id203 distribution
when viewed one way and a subjective likelihood function when viewed the other
way.2 because this book is centrally focused on computation, it takes the second
approach of listing both id203 and likelihood using the same p (x,   ) form.

more on ll, s, and i the log of the likelihood function has a number of di-
vine properties, which add up to making the log likelihood
preferable to the plain likelihood in most cases   and wait until you see what the
score can do.

first, due to all that exponentiation in the distributions of sections 7.2 and 9.2,
ln p is often much easier to deal with, yet is equivalent to p (  ) for most of our
purposes   notably, if we have found a maximum for one, the we have found a
maximum for the other.

also, consider calculating an iid likelihood function given a thousand data points.
the id203 of observing the data set will have the formq1000
i=1 p (xi). since
each p (xi)     (0, 1], this product is typically on the order of 1    10   1000. as
discussed on page 137, such a number is too delicate for a computer to readily
deal with. taking logs, each value of pi is now a negative number (e.g., ln(0.5)    
   0.69 and ln(0.1)        2.3), and the product above is now a sum:

ln"1000yi=1

p (xi)# =

1000xi=1

ln (p (xi)) .

thus, the log likelihood of our typical thousand-point data set is on the order of
   1000 instead of 1    10   1000   much more robust and manageable. you saw an
example of these different scales with the nine-point sample in the bernoulli ex-
ample, which had p     0.002 but ll        6.
analytically, the maximum of the log likelihood function is useful for two reasons
with four names: the cram  r   rao lower bound and the neyman   pearson lemma.
it all begins with this useful property of the score:3

2there are consistent means of describing subjective id203 that accommodate both ways of slicing
p (x,   ). the subjectivist approach (closely related to the bayesian approach) takes all views of p (x,   ) as
existing only in our minds   no matter how you slice it, there need not be a physical interpretation. the axiomatic
approach, led by ramsey, savage, and de finetti, posits a few rules that lead to    consistent    subjective beliefs
when followed, but places no further constraints on either id203 or likelihood. again, once both id203
and likelihood are accepted as subjective beliefs, there is less reason to distinguish them notationally.

3all proofs here will be in the case where    is a scalar. proofs for the multidimensional case are analogous but

gsl_stats march 24, 2009

id113

331

theorem 10.1.1. if p (x,   ) satis   es certain regularity conditions as described
in the footnote,4 then for any statistic f (x),

ex(s    f ) =

   ex(f )

     

.

that is, the score is a sort of derivative machine: the expected value of the score
times a statistic is equivalent to the derivative of the expected value of the statistic.
finding an optimum requires    nding the point where the derivative is zero and the
second derivative is negative, and this theorem gives us an easy trick for    nding
those derivatives. the next few pages will show how this trick is used.

 proof: the expected value of the score times the statistic is

when reading this theorem, it is worth recalling the sleight-of-notation from page
257: f (x) is a function only of the data, but ex(f (x)) (where x is produced using
a certain model and parameters) is a function only of the parameters.

f (x)p (x,   )dx

s(  )f (x)p (x,   )dx

    ln p (x,   )

     
   p (x,  )

ex(s    f ) =z   x
=z   x
=z   x
=z   x
   (cid:0)r   x f (x)p (x,   )dx(cid:1)

   p (x,   )

p (x,   )

f (x)

     

     

dx

=

     

f (x)p (  , x)dx

   ex(f (x))

     

(10.1.2)

(10.1.3)

(10.1.4)

(10.1.5)

=

require more involved notation.

and derivative like this, none of this applies.

4equation 10.1.4 of the proof uses the claim that r f    dp

d   r f    p dx. if we can   t reverse the integral
the common explanation for when the switch is valid is in the case of any exponential family; the de   nition
of an exponential family will not be covered in this book, but rest assured that it applies to the normal, gamma,
beta, binomial, poisson, et cetera   just about every distribution but the uniform.

d   dx = d

but it also applies more generally: we need only uniform convergence of the pdf as its parameters go to any
given limit (casella & berger, 1990, section 2.4). roughly, this is satis   ed for any pdf whose value and deriva-
tive are always    nite. for those who prefer the exponential family story, note that any pdf can be approximated
arbitrarily closely by a sum of exponential-family distributions (barron & sheu, 1991), so for any distribution
that fails, there is an arbitrarily close distribution that works. for example, the uniform[  1,   2] distribution fails
because of the in   nite slope at either end, but a distribution with a steep slope up between   1     1e   10 and   1
and a steep slope down between   2 and   2 + 1e   10 works    ne.

gsl_stats march 24, 2009

332

chapter 10

the sequence of events consisted of substituting in the de   nition of the score
(equation 10.1.2), then substituting the familiar form for the derivative of the log
(equation 10.1.3), and canceling out a pair of p (x,   )   s. at this point, the simple
weighting p (x) (   rst introduced on page 221) has been replaced with the weight-
ing dp (x). before, if x1 was twice as likely as x2 (i.e., p (x1) = 2p (x2)), then
f (x1) would get double weighting. now, if the slope of p (x) at x1 is twice as
steep as the slope at x2, then f (x1) gets double weighting.

the    nal steps state that, under the right conditions, the integral of f (x) using a
measure based on dp (x) is equivalent to the derivative of the integral of f (x) using
a measure based on p (x). equation 10.1.4 switched the integral and derivative,
using the assumptions in the theorem   s footnote, and equation 10.1.5 recognized
the integral as an expectation under the given id203 density.
(cid:7)

corollary 10.1.2.

e(s) = 0.

proof: let the statistic f (x) be the trivial function f (x) = 1. then theorem 10.1.1
tells us that e(s    1) =    e(1)/      = 0.

q10.2

verify that the expected value of the score is zero for a few of the distribu-
tions given in chapter 7, such as the exponential on page 248. (hint: you
will need to calculate the integral of the expected value of the score over the
range from zero to in   nity; integration by parts will help.)

 proof: the    rst half comes from the fact that var(s) = e(s    s)    e(s)   e(s),

var(s) = e(s    s) = e(i).

lemma 10.1.3. the information equality

but we just saw that e(s) = 0.

for the second half, write out e(i), using the expansion of s =(cid:18)    p (x,  )

p (x,  )(cid:19) and the

usual rules for taking the derivative of a ratio.

     

gsl_stats march 24, 2009

id113

333

e(cid:20)    2 ln p (x,   )

     

(cid:21) = e            
= e         
= e      

     

     

(p (x,   ))2

p (x,   )    2p (x,  )

   (cid:18)    p (x,  )
p (x,  )(cid:19)

            
     2    (cid:16)    p (x,  )
      (cid:17)2
p (x,   )     s    s      
p (x,  ) (cid:21) = 0. (hint: use the lessons from

         

   2p (x,  )

     2

=   e [s    s]

q: prove the    nal step, showing that e(cid:20)    2p (x,  )

     2

the proof of theorem 10.1.1 to write the expectation as an integral and switch the
integral and one of the derivatives.)
(cid:7)

the information equality will be computationally convenient because we can re-
place a variance, which can be hard to directly compute, with the square of a
derivative that we probably had to calculate anyway.

for the culmination of the sequence, we need the cauchy   schwarz inequality,
which    rst appeared on page 229. it said that the correlation between any two
variables ranges between    1 and 1. that is,

   1       (f, g) =

cov(f, g)

pvar(g) var(f )     1

cov(f, g)2
var(g) var(f )     1

cov(f, g)2
var(g)     var(f ).

(10.1.6)

lemma 10.1.4. the cram  r   rao lower bound

let f (x,   ) be any statistic, and assume a distribution p (x,   ) that meets the cri-
teria from the prior results. then

(   ex (f (x,   )) /     )2

e(i)

   

    var(f (x,   )).

(10.1.7)

gsl_stats march 24, 2009

334

chapter 10

the proof consists of simply transforming the cauchy   schwarz inequality using
the above lemmas. let g in equation 10.1.6 be the score; then the equation expands
to

(ex(f (x)    s)     ex(f (x))ex(s))2

var(s)

    var(f (x))

(10.1.8)

the left-hand side has three components, each of which can be simpli   ed using
one of the above results:

    ex(f (x,   )    s) =    f (x,   )/     , by theorem 10.1.1.
    corollary 10.1.2 said e(s) is zero, so the second half of the numerator disappears.
    the information equality states that that var(s) = e(i).

applying all these at once gives the cram  r   rao lower bound.

further, id113s have a number of properties that let us further tailor the crlb to
say still more.5 let the statistic f (x) be the maximum likelihood estimate of the
parameter, m le(x,   ).

    id113s can be biased for    nite data sets, but can be shown to be asymptotically un-
biased. this means that for n suf   ciently large, e(m le(x,   )) =   . therefore,
   m le(x,   )/      = 1, so the numerator on the left-hand side of equation 10.1.7
is one.

    it can be proven that maximum likelihood estimators actually achieve the crlb,

meaning that in this case the inequality in equation 10.1.7 is an equality.

    the information matrix is additive: if one data set gives us i1 and another gives us
i2, then the two together produce itotal = i1 + i2. for a set of iid draws, each draw
has the same amount of information (i.e., the expected information matrix, which
is a property of the distribution, not any one draw of data), so the total information
from n data points is ni.

the end result is the following form, which we can use to easily calculate the
covariance matrix of the id113 parameter estimates.

var(m le(x,   )) =

1

nex(i)

.

(10.1.9)

equation 10.1.9 makes id113s the cornerstone of statistics that they are. given that
id113s achieve the crlb for large n, they are asymptotically ef   cient, and if we
want to test the parameter estimates we    nd via a t or f test, there is a relatively

5see casella & berger (1990, pp 310   311) for formal proofs of the statements in this section.

gsl_stats march 24, 2009

id113

335

easy computation for    nding the variances we would need to run such a test.6 for
many models (simulations especially), we want to know whether the outcome is
sensitive to the exact value of a parameter, and the information matrix gives us a
sensitivity measure for each parameter.

how to evaluate a test a hypothesis test can be fooled two ways: the hypoth-
esis could be true but the test rejects it, or the hypoth-

esis could be false but the test fails to reject it.

there is a balance to be struck be-
tween the two errors: as one rises,
the other falls. but not all tests are
born equal. if a hypothesis has a 50   
50 chance of being true, then the
coin-   ip test,    heads, accept; tails,
reject    gives us a 50% chance of
a type i error and a 50% chance
of a type ii error, but in most sit-
uations there are tests where both
errors are signi   cantly lower than
50%. by any measure we would
call those better tests than the coin-
   ipping test.

evaluation vocab

here are some vocabulary terms; if you are in a stats

class right now, you will be tested on this:
likelihood of a type i error       : rejecting the null
when it is true.
likelihood of a type ii error       : accepting the null
when it is false.
power     1       : the likelihood of rejecting a false
null.
unbiased: (1       )        for all values of the parame-
ter. i.e., you are less likely to accept the null when it is
false than when it is true.
consistent: the power     1 as n        .

a big help in distinguishing type i from type ii error is that one minus the type
ii error rate has the surprisingly descriptive name of power. to a high-power tele-
scope, every star is slightly different   some things that seem like stars are even
galaxies or planets. but to a low-power lens like the human eye, everything just
looks like a little dot. similarly, a high-power test can detect distinctions where a
low-power test fails to reject the null hypothesis of no difference. or, for the more
cynical: since most journals have limited interest in publishing null results, a high-
power test increases the odds that you will be able to publish results. as you can
imagine, researchers are very concerned with maximizing power.

the neyman   pearson lemma the neyman   pearson lemma (neyman & pear-
son, 1928a,b) states that a likelihood ratio test
will have the minimum possible type ii error   the maximum power   of any test
with a given level of   . after establishing this fact, we can select a type i error
level and be con   dent that we did the best we could with the type ii errors by
using a likelihood ratio test.

6of course, if we run one of the tests from chapter 9 derived from the clt, then we need to make sure the
clt holds for the maximum likelihood estimate of the statistic in question. this could be a problem for small
data sets; for large data sets or simulations based on millions of runs, it is less of a concern.

gsl_stats march 24, 2009

336

chapter 10

likelihood ratios

say the cost of a test that correctly accepts or rejects the hy-
pothesis is zero, the cost to a type i error is ci , and the cost
to a type ii error is cii. then it is sensible to reject h0 iff the expected cost
of rejecting is less than the expected cost of not rejecting. that is, reject h0 iff
ci p (h0|x) < cii p (h1|x). we can translate this cost-minimization rule into the
ratio of two likelihoods.

recall bayes   s rule from page 258:

p (a|b) =

p (b|a)p (a)

p (b)

.

to apply bayes   s rule to the rejection test, set a = h0 and b = x, so p (a|b) =
p (h0|x) and p (b|a) = p (x|h0) (and similarly for h1). then:

ci p (h0|x) < cii p (h1|x)

ci

p (x|h0)p (h0)

p (x)

< cii

p (x|h1)p (h1)

p (x)

c <

p (x|h1)
p (x|h0)

(10.1.10)

(10.1.11)

(10.1.12)

inequality 10.1.10 is the rejection rule from above; inequality 10.1.11 uses bayes   s
rule to insert the likelihood functions; inequality 10.1.12 does some cross-division,
canceling out the p (x)   s and de   ning the critical value c     ci p (h1)/cii p (h0),
i.e., everything that doesn   t depend on x. if you tell me the shape of p (  |h1) and
p (  |h0) and some number        (0, 1), then i can give you a value of c such that
inequality 10.1.12 is true with id203   .7 the test will then be: gather the
data, calculate the likelihood ratio on the right-hand side of inequality 10.1.12,
and reject h0 iff the inequality is true.

the neyman   pearson lemma states that this test is the    best    in the sense that
for a type i error    xed at   , the lr test minimizes the id203 of a type ii
error.8 so we can design any test we like by just    xing    at a value with which we
are comfortable (custom says to use 95 or 99%) and calculating a few likelihood
functions, and we are assured that we did the best we could regarding type ii
errors. most standard tests can be expressed in a likelihood ratio form, and so
type ii errors pretty much never get mentioned, since they   re considered taken
care of.9

7alternatively, you could give me a ratio of costs ci /cii and i could again give you a value of c. thus, one

could draw a relation between the relative costs and the choice of   .

8for a proof, see e.g. amemiya (1994, pp 189   191).
9every test has a type i and type ii error, but thanks to the neyman   pearson lemma, we just describe a test
using the type i level, with phrases like a test with 5% p-value. the introductory chapter of hunter & schmidt
(2004) is an excellent essay on how such description can be severely misleading. the extreme-case test always
fail to reject the null has a 0% type i error rate, but if the null hypothesis is false, then it is wrong 100% of the
time.

gsl_stats march 24, 2009

id113

337

   for any suf   ciently well-speci   ed model, you can    nd the id203

that a given data set was produced via the model.

   if the data set consists of independent and identically distributed el-
ements, then the likelihood is a product with one term for each data
point. for both computational and analytic reasons, the log likelihood
is easier to work with; the product then becomes a sum.

   the parameters that maximize the likelihood function for a model (or
identically, maximize the log likelihood) will have the minimum vari-
ance among all unbiased estimators. the variance is a known quan-
tity, given by the cram  r   rao lower bound.

   type i and type ii errors are complementary: as one goes up, the
other generally goes down. however, given a type i error level, differ-
ent tests will have different type ii error levels. the neyman   pearson
lemma guarantees that a likelihood ratio test has the minimum prob-
ability of type ii error for a    xed type i error level.

model   s optimum,a   _ axi 	 _ ike	
 ih  d   but what a function it is. it pro-
the variance of a most-likely parameter,a   _ axi 	 _ ike ih  d will return
a parametrizeda   _  de  with the variances, along with other useful informa-

vides a standardized interface to several types of optimization routines that take
very different approaches toward    nding optima. you will have to provide a log
likelihood function, but if you are unable to provide the derivatives, the maximiza-
tion routines will    nd them for you. since the cram  r   rao lower bound tells us

apophenia provides one function to    nd a

10.2 description: maximum
likelihood estimators

tion. this section gives an overview of some standard optimization methods, and
how to choose among them to raise the odds that they will    nd an optimum for
your functions.

you may be wondering why you need to know these details. isn   t    nding the opti-
mum of a likelihood function a solved problem?

the answer is decidedly no. most standard optimization algorithms are built to
work well with a smooth, closed-form, globally concave function, such as    nding
the value of x that maximizes f (x) =    x2. if your function more-or-less meets
such conditions, the odds are good that the default optimization routine in any stats
package of your choosing will work    ne. but anything that produces a likelihood
value could be a model: a simulation could be a model, where the likelihood is a
function of how well the model matches a real-world data set. a dynamic program-

gsl_stats march 24, 2009

338

chapter 10

ming problem could be a model. the consumer choosing among goods at the end
of chapter 4.7 was a model. if your model has a stochastic element, has multiple
equilibria, or otherwise fails to ful   ll the expectation of being a simple globally
concave function, then you will need to tailor a method and settings around the
problem at hand.10

#include <apop.h>

double sin_square(apop_data *data, apop_model *m){

double x = apop_data_get(m   >parameters, 0,    1);

return    sin(x)*gsl_pow_2(x);

}

listing 10.1 a model to be optimized. online source: i   .
.

apop_model sin_sq_model ={"   sin(x) times x^2",1, .p = sin_square};

)
x
(
n
i
s

2
x

4

3

2

1

0
   4

   3

   2
x

   1

0

0
   4    3    2    1

x

0

1

)
x
(
n
i
s

2
x

5e     25
0
   5e     25
   1e     24
   1e     24
   2e     24
   2e     24
   3e     24
   3e     24

   4    3    2    1
x

1

0

and root search. [online source:  
a  ax_  i  .
]

2

figure 10.2 top row: the simplex method and conjugate gradients; bottom row: simulated annealing

10the problem of    nding an optimum is so broad that there are large sections of optimization research that
this book does not mention. for example, there is the broad and active    eld of combinatorial optimization, which
covers questions like the optimal ordering of n elements given an optimization function (which is a problem with
n! options). see, e.g., papadimitriou & steiglitz (1998) for more on combinatorial optimization methods.

   
2

e

   

2

4

6

8

1

1

1

2

e

e

e

e

e

e

e

e

0

   

   

   

   

   

   

   

   

0

9

0

0

0

0

0

0

0

0

9

9

9

9

8

8

8

8

x

1
2
3
4
5
6
7
8

4

3

2

1

4
3
2
1
0

   4   3   2   1

)
x
(
n
i
s

2
x

)
x
(
n
i
s

2
x

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

gsl_stats march 24, 2009

id113

339

#include <apop.h>

apop_model sin_sq_model;

void do_search(int m, char *name){

double p[] = {0};
double result;

apop_settings_add(&sin_sq_model, apop_id113, starting_pt, p);
apop_settings_add(&sin_sq_model, apop_id113, method, m);
apop_model *out = apop_maximum_likelihood(null, sin_sq_model);
result = gsl_vector_get(out   >parameters   >vector, 0);
printf("the %s algorithm found %g.\n", name, result);

}

}

int main(){

apop_opts.verbose ++;
apop_settings_add_group(&sin_sq_model, apop_id113, &sin_sq_model);
do_search(apop_simplex_nm, "n   m simplex");
do_search(apop_cg_fr, "f   r conjugate gradient");
do_search(apop_siman, "simulated annealing");
do_search(apop_rf_newton, "root      nding");

listing 10.1 presents a simple model, consisting of the equation    x2 sin(x). this
is a very simple equation, but it has an in   nite number of local modes. as x        ,
the value of the function at these modes rises in proportion to x2, so there is no
global maximum. figure 10.2 shows various attempts to search the function, one
of which gives a very good picture of the shape of the curve.

listing 10.3 usinga   _ axi 	 _ ike ih  d and four types of method to solve for a maximum.
compile with i   .
. online source:  
a  ax.
.
analysis, the function takes in a data set and ana   _  de  holding the parame-
    thea   _  de  struct can hold an array of groups of settings. line 17 adds to

    line eight declares a new model with some of the information the id113 function
will need, like the name, the number of parameters (one) and the id203 func-
tion.

    on lines 3   6, the function is de   ned. because the system is oriented toward data

ters. in this case, the data set is simply ignored.

listing 10.3 does the optimization four ways.

the model a group of id113-appropriate settings. that group includes things like the
starting point, method, tolerance, and many other details that you will see below.

340

chapter 10

gsl_stats march 24, 2009

two lines interrogate the output.

    all the real work is done by line ten, which calls the maximization. the subsequent

    lines eight and nine change the  a  i g_   and e h d elements of the model   s
id113 settings group to  and , respectively.
    the ai  routine does the optimization with four different methods.
by insertinga   _ e  i g _add   i _  _  de  a   _  e   a
e_ a h 
" 	 fi e"  somewhere around line eight or nine, the system writes down the
via random walk, but in the meantime, simulated annealing and  a
e_ a h pro-

points it tests during its search for an optimum; you can then produce plots like
those in figure 10.2. you can already see disparate styles among the four meth-
ods: the simplex and conjugate gradient methods are somewhat similar in this case,
but simulated annealing is much more exhaustive   you can clearly see the shape
of the curve   while the root search barely leaves zero before deciding that it is
close enough.

chapter 11 will demonstrate another method of producing a picture of a function

vide a pretty effective way to get a graph of a complex and unfamiliar function.

when you run the program, you will see that asking four different methods gives
you three different answers, which leads to an important lesson: never trust a sin-
gle optimization search. by redoing the optimization at different points with dif-
ferent methods, you can get a better idea of whether the optimum found is just a
local peak or the optimum for the entire function. see below for tips on restarting
optimizations.

methods of finding
optima

here is the problem: you want to    nd the maximum of
f (x), but only have time to evaluate the function and its
derivative at a limited number of points. for example, f (x)
could be a complex simulation that takes in a few parameters and runs for an hour
before spitting out a value, and you would like to have an answer this week.

here are the methods that are currently supported by apophenia. they are basi-
cally a standardization of the various routines provided by the gsl. in turn, the
gsl   s choice of optimization routines bears a close resemblance to those recom-
mended by press et al. (1988), so see that reference for a very thorough discussion
of how these algorithms work, or see below for some broad advice on picking
an algorithm. also, avriel (2003) provides a thorough, mathematician-oriented
overview of optimization, and gill et al. (1981) provides a more practical, modeler-
oriented overview of the same topics.

341

id113

gsl_stats march 24, 2009

simplex method   nelder   mead

for a d dimensional search, this method draws
a polygon with d + 1 corners and, at each step,
shifts the corner of the polygon with the small-
est function value to a new location, which may move the polygon or may contract
it to a smaller size. eventually, the polygon should move to and shrink around the
maximum. when the average distance from the polygon midpoint to the d + 1

[a   _s    ex_     
corners is less than   e a 
e, the algorithm returns the midpoint.
polak   ribiere[a   _cg_ r   
fletcher   reeves[a   _cg_fr   
broyden   fletcher   goldfarb   shanno[a   _cg_bfgs   

this method doesn   t require derivatives at all.

conjugate gradient

including:

begin by picking a starting point and a direction, then    nd the minimum along that
single dimension, using a relatively simple one-dimensional minimization proce-
dure like newton   s method. now you have a new point from which to start, and
the conjugate gradient method picks a new single line along which to search.

given a direction vector d1, vector d2 is orthogonal iff d   1d2 = 0. colloquially,
two orthogonal vectors are at right angles to each other. after doing an optimiza-
tion search along d1, it makes intuitive sense to do the next one-dimensional search
along an orthogonal vector. however, there are many situations where this search
strategy does not do very well   the optimal second direction is typically not at
right angles to the    rst.

instead, a conjugate gradient satis   es d   1ad2 = 0, for some matrix a. orthog-
onality is the special case where a = 1. for quadratic functions of the form
f (x) = 1
2 x   ax     bx, a search along conjugate gradients will    nd the optimum
in as many steps as there are dimensions in a. however, your function probably
only approximates a quadratic form   and a different quadratic form at every point,
meaning that for approximations at points one and two, a1 6= a2. it is not neces-
sary to actually calculate a at any point, but the quality of the search depends on
how close to a quadratic form your function is; the further from quadratic, the less
effective the method.

polak   ribiere and fletcher   reeves differ only in their choice of method to build
the next gradient from the prior; press et al. (1988) recommend polak   ribiere.

the bfgs method is slightly different, in that it maintains a running best guess
about the hessian, and uses that for updating the gradients.11 however, the same

11formally, one could argue that this means that it is not a conjugate gradient method, but i class it with the

gsl_stats march 24, 2009

342

else

chapter 10

settings that can be tuned before calling the routine, as on lines 8 and 9 of listing

pick a new candidate point, pc = pi + gi    step_size/|gi|.
if pc > pi

start at p0 = starting_pt, and gradient vector g0.
while (pi    gi < tolerance|pi||gi|)

general rules about its underlying assumptions hold: the closer your function is to
a    xed function with smooth derivatives, the better bfgs will do.

pi+1     pc
pi+1     the minimum point on the line between pi and pc (to

here is the pseudocode for the three algorithms. the variables in e e y e are
10.3. for all routines, setve b  e to1 to see the progress of the search on screen.
within   e a 
e12).
point. if your model has nod  g_ ike ih  d function, then the system will use
newton   s method[a   _rf_ ewt     
hybrid method[a   _rf_ ybr d   
hybrid method; no internal scaling[a   _rf_ ybr d_  sca e   

we are guaranteed that pi+1 6= pi because we followed the gradient uphill. if the
function continues uphill in that direction, then pc will be the next point, but if the
function along that path goes uphill and then downhill, the next point will be in
between pi and pc.

the step to calculate gi+1 is the only step that differs between the three methods.
in all three cases, it requires knowing derivatives of the function at the current

find gi+1 using a method-speci   c equation.

a numerical approximation.

including:

root    nding

a root search    nds the optimum of the function by searching for the point where
the    rst derivative of the function to be maximized is zero. notice that this can   t
distinguish between maxima and minima, though for most likelihood functions,
the minima are at    =      .13

fr and pr methods because all three routines bear a very close resemblance, and all share the same pseudocode.
12do not confuse the tolerance in these algorithms with a promise (for example, the promise in a taylor
expansion) that if the tolerance is    and the true value of the parameter is   , then the id113 estimate will be such
that |     id113       | <    . there is no way to guarantee such a thing. instead, the tolerance indicates when the
internal measure of change (for most algorithms,    f (  )) is small enough to indicate convergence.
13in fact, it is worth making sure that your models do not have a divergent likelihood, where the likelihood

increases as            or       . divergent likelihoods are probably a sign of a misspeci   ed model.

gsl_stats march 24, 2009

id113

343

let f (x) be the function whose root we seek (e.g., the score), and let    f be the
matrix of its derivatives (e.g., the information matrix). then one step in newton   s
method follows

xi+1     xi     (   f (xi))   1 f (xi).

you may be familiar with the one-dimensional version, which begins at a point x1,
and follows the tangent at coordinate (x1, f (x1)) down to the x   axis; the equation
for this is x2 = x1     f (x1)/f   (x1), which generalizes to the multidimensional
version above.

the algorithm repeats until the function whose zero is sought (e.g., the score) has

the hybrid edition of the algorithm imposes a region around the current point be-
yond which the algorithm does not venture. if the tangent sends the search toward
in   nity, the algorithm follows the tangent only as far as the edge of the trust region.
the basic hybrid algorithm uses a trust region with a different scale for each di-
mension, which takes some extra gradient-calculating evaluations and could make
the trust region useless for ill-de   ned functions; the no-scaling version simply uses
the standard euclidian distance between the current and proposed point.

methods, the system tries a new point, and
if it is better, switches. initially, the system is allowed to make large jumps, and
then with each iteration, the jumps get smaller, eventually converging. also, there
is some decreasing id203 that if the new point is less likely, it will still be
chosen. one reason for allowing jumps to less likely parameters is for situations
where there may be multiple local optima. early in the random walk, the system
can readily jump from the neighborhood of one optimum to another; later it will
   ne-tune its way toward the optimum. other motivations for the transition rules
will be elucidated in the chapter on monte carlo methods.

a value less than   e a 
e.
simulated annealing[a   _s  a     a controlled random walk. as with the other
start with temp = _i i ia 
  a  i g_  i  
 _ i 
repeat the followingi e  _fixed_t times:
draw a new point xt, at most  e _ ize units away from xt   1.
cool: let temp     temp/ 	_ 

draw a random number r     [0, 1].
if f (xt) > f (xt   1)
else if r < exp(   (xt   1     xt)/(k    temp))
else remain at the old point: xt+1     xt   1.

jump to the new point: xt+1     xt.
jump to the new point: xt+1     xt.

here is the algorithm in greater detail, with setting names in appropriate places:

let x0    
while temp    

gsl_stats march 24, 2009

 _ i  in 4,000 steps, then that is exactly how many steps it will take. if you know

unlike with the other methods, the number of points tested in a simulated anneal-
ing run is not dependent on the function: if you give it a speci   cation that reaches

chapter 10

344

your model is globally convex (as are most standard id203 functions), then
this method is overkill; if your model is a complex interaction, simulated annealing
may be your only bet. it does not use derivatives, so if the derivatives do not exist
or are ill-behaved, this is appropriate, but if they are available either analytically or
via computation, the methods that use derivative information will converge faster.

if your model is stochastic, then methods that build up a picture of the space (no-
tably conjugate gradient methods) could be fooled by a few early bad draws. sim-
ulated annealing is memoryless, in the sense that the only inputs to the next deci-
sion to jump are the current point and the candidate. a few unrepresentative draws
could send the search in the wrong direction for a while, but with enough draws
the search could eventually meander back to the direction in which it should have
gone.

global v local optima as you saw in the case of    x2 sin(x), none of the methods
guarantee that the optimum found is the global optimum,
since there is no way for a computer to have global knowledge of a function f (x)
for all x     (      ,   ). one option is to restart the search from a variety of starting
points, in the hope that if there are multiple peaks, then different starting points
will rise to different peaks.

the simulated annealing algorithm deals well with multiple peaks, because its
search can easily jump from the neighborhood of one peak to that of another. in
fact, as the number of steps in the simulated annealing algorithm        , the algo-
rithm can be shown to converge to the global optimum with id203 one. how-
ever, calculating an in   nite number of steps tends to take an unreasonable amount
of time, so you will need to select a time-to-con   dence trade-off appropriate to
your situation.

line switches usingge     so you can do this exercise from a batch    le.]
thee
  101 models from chapter 4 provide the relatively rare situation

lines 13 and 14 of listing 10.3 set the key parameters of the method and the
starting point. try various values of both. which do a better job of jumping
out toward the larger modes? [bonus: rewrite the program to take command-

where we have an optimization and the analytic values. [hopefully your
own simulations have at least one special case where this is also true.] this
is therefore a    ne opportunity to try various methods, values of delta, step
size, tolerance, method, et cetera do extreme prices or preferences create
problems, and under which optimization settings?

q10.3

q10.4

gsl_stats march 24, 2009

id113

345

restarting to reiterate a recommendation from above: never trust a single opti-
mization search. but there are a number of ways by which you can

order your multiple searches.

    you could start with a large step size and wide tolerance, so the search jumps
around the space quickly, then restart with smaller step size and tolerance to hone
in on a result.

mizations; see the online reference for details.

points   either different extremes of the space, or randomly-generated points.

    if you suspect there could be multiple local optima, then try different starting

    along a similar vein, different search methods have different stopping criteria, so
switching between a simplex algorithm and a conjugate gradient algorithm, for
example, may lead to a better optimum.

    if you are running a constrained optimization, and one of the constraints binds,
then there may be odd interactions between the penalty and the function being
optimized. try a series of optimizations with the penalty declining toward zero, to
see if the optimum gets closer to the boundary.

    thea   _e  i a e_ e  a   function will help you to run sequences of opti-
   given an appropriatea   _da a set anda   _  de , thea   _	
 axi 	 _ ike ih  d function will apply any of a number of
   you can try various methods in sequence usinga   _e  i a e_	
 e  a  . you can also use the restarting technique to do a coarse

   no computational method can guarantee a global optimum, because
the computer can only gather local information about the function.
restarting the search in different locations may help to establish a
unique optimum or    nd multiple optima.

maximum-searching techniques to    nd the optimal parameters.

search for the neighborhood of an optimum, and then a    ner search
beginning where the coarse search ended.

10.3 missing data

say that your data set is mostly complete, but has an nan
in observation    fty, column three. when you run a re-
gression, the nan   s propagate, and you wind up with nan   s all over the parameter
estimates. how can you    ll in the missing data?

we could turn this into an id113 problem: we seek the most likely values to    ll in
for the nan   s, based on some model of how the data was generated.

gsl_stats march 24, 2009

346

chapter 10

infant mortality

income

missingness

to be missing. [online source: a .d  ]

missingness

infant mortality

figure 10.4 two different    ows of causation. at left is non-ignorable missingness: the value of the
infant mortality statistic determines whether infant mortality data will be missing. at
right is mar: low income causes high infant mortality, and causes infant mortality data

but    rst, we need to distinguish among three types of missingness. data are miss-
ing completely at random (mcar) when the incidence of missing data is uncor-
related to every variable in the data set. this is truly haphazard error: somebody
tripped over the meter   s power cord, or one of the surveyors was drunk. the cause
of the missing data is nowhere in the data set.

data are missing at random (mar) when the incidence of missing data in col-
umn i is uncorrelated to the existing data in column i once we condition on the
observed data in all other columns. for example, poor countries tend to have bad
demographic data, so the incidence of a missing infant mortality rate is correlated
to low gnp per capita. once we have conditioned on gnp per capita, there is no
reason to expect that missingness is correlated to infant mortality. the cause of the
missing values in column i is something in the data set, but not column i. as in
the right-hand diagram in figure 10.4, there is no    ow of causation from infant
mortality to missingness.

conversely, say that governments are embarrassed by high infant mortality rates.
statistics bureaux are under orders to measure the rate, but release the measure
only if it falls below a certain threshold. in this case, the incidence of missing data
is directly related to the value of the missing data. the cause of the missing data
is the value of the data. this is known as missing not at random (mnar) or non-
ignorable missingness, and is a serious problem because it implies bias almost by
de   nition.

there are many methods for dealing with censored or otherwise non-ignorable
missingness discussed in many sources, such as greene (1990). for a full discus-
sion of the many types of missing data, see allison (2002).

gsl_stats march 24, 2009

id113

347

listwise deletion one option for dealing with nan   s that are mcar is listwise
deletion. the idea here is supremely simple: if a row is missing
data for any variable, then throw out the entire row. this is conceptually simple,
does not impose any additional structure or model on the data, and can be executed
in one line of code:

apop_data *nan_free_data = apop_data_listwise_delete(dirty_data);

alternatively, see page 105 for the syntax to do listwise deletion on the sql side.

but listwise deletion isn   t always appropriate. as a worst-case situation, say that a
survey has a hundred questions, and everybody    lled out exactly 99 of them. by
listwise deletion, we would throw out the entire data set.

but with listwise deletion, the data set is going to be shorter, meaning that we lose
information, and if data is mar (not mcar), then throwing out observations with
missing data means biasing the information among other variables. in the example
above, if we throw out countries with missing infant mortality data, we would
mostly be throwing out countries with low gdp per capita.

ml imputation

this is where id113 comes in (dempster
et al., 1977). let the missing data be   , and the existing data (with
holes) be x, as usual. then our goal is to    nd the most likely value of   . the    rst
step is to specify a model from which the data was allegedly generated, so that we
can evaluate the likelihood of any given   . the norm is that the completed data
set has a multivariate normal distribution, meaning that the n columns of the data
are distributed as an n-dimensional bell curve with mean vector    and covariance
matrix   . however, it may make sense for your data to take on any of a number of
other forms. but given a parametrized distribution, one could search for the data
points that are most likely to have occurred.

in theda a	
   	  i  .db database, you will    nd transparency international   s

corruption perceptions index from 2003   2006. because the index depends on
about a dozen hard-to-gather measures, there are many missing data points. list-
ing 10.5 goes through the entire process of    lling in those data points, by pulling
the data from the database, reducing it to an estimable subset via listwise deletion,
   tting a multivariate normal to the subset, and then    lling in the nan   s in the full
data set via maximum likelihood. it may run slowly:    lling in about eighty nan   s
means a search in an 80-dimensional space. for more missing data than this, you
are probably better off    nding a means of dividing the data set or otherwise incre-
mentally    lling in the blanks.

you are encouraged to look at the results and decide whether they seem plausible.

gsl_stats march 24, 2009

348

chapter 10

#include <apop.h>

int main(){

apop_db_open("data   corruption.db");
apop_data *corrupt = apop_db_to_crosstab("cpi", "country", "year", "score");
apop_data *clean = apop_data_listwise_delete(corrupt);
apop_model *mlv = apop_estimate(clean, apop_multivariate_normal);
apop_ml_imputation(corrupt, mlv);
apop_crosstab_to_db(corrupt, "cpi_clean", "country", "year", "score");

listing 10.5 filling in nan   s via a multivariate normal. online source:
   	  .
.

}

for example, would you use the data for yugoslavia? is a multivariate normal the
most appropriate model of how the data was formed?

on lengthy surveys, few if any people successfully    ll out the entire form. in the
worst case, we may have 100 questions, and all subjects answered 99 of them.
listwise deletion would throw out every subject. in this case, the best bet is pair-
wise deletion: calculate the mean of each vector by just ignoring nan   s, and the
covariance of each pair of columns by removing only those observations that have
an nan for one of those two variables. pairwise deletion can introduce odd biases
in the covariance matrix, so it should be used as a last resort.

10.4 testing with likelihoods

in order to test a hypothesis regarding
a statistic, we need to have a means of
describing its theoretical distribution. when the statistic is an id113, the standard
means of doing this is via two interlocking approximations: a taylor expansion and
a normal approximation. this is convenient because the normal approximation
proves to be innocuous in many settings, and the taylor expansion involves the
same cast of characters with which we have been dealing to this point   ll, s,
and i.

using the information matrix the cram  r   rao lower bound gives us a value
for the variance of the parameters: the inverse
of the expected information matrix. given a variance on each parameter, we can
do the same t and f tests as before.

it is even easier if we make one more approximation. the expected information
matrix is an expectation over all possible parameters, which means that it is a
property of the model, not of any one set of parameters. conversely, the estimated

gsl_stats march 24, 2009

id113

349

information matrix is the derivative of the score around the most likely values of
the parameters. we can expect that it is different for different parameter estimates.

efron & hinkley (1978) found that for most of the applications they consider, the
inverse of the estimated information matrix is preferable as an estimator of the vari-
ance as the expected information matrix. for exponential family distributions, the
two are identical at the optimum. from a computational perspective, it is certainly
preferable to use the estimated information, because it is a local property of the
optimum, not a global property of the entire parameter space. simulated annealing
does a decent job of sampling the entire space, but the other methods go out of
their way to not do so, meaning that we would need to execute a second round
of data-gathering to get variances. apophenia   s id113
returns a covariance matrix constructed via the estimated information matrix.

the covariance matrix provides an estimate of the stability of each individual pa-
rameter, and allows us to test hypotheses about individual parameters (rather than
tests about the model as a whole, as done by the likelihood ratio methods discussed
below).14 however, there are a number of approximations that had to be made to
get to this point. basically, by applying a t test, we are assuming that a few million
draws of a parameter   s id113 (generated via a few million draws of new data) would
be asymptotically normally distributed. we already encountered this assumption
earlier: when testing parameters of a id75 we assume that the errors
are normally distributed. so the same caveats apply, and if you have a means of
generating several data sets, you could test for normality; if you do not, you could
use the methods that will be discussed in chapter 11 to bootstrap a distribution;
or if you are working at a consulting    rm, you could just assume that normality
always holds.

there is no sample code for this section because you already know how to run a t
test given a statistic   s mean and its estimated variance.

using likelihood ratios we can use likelihood ratio (lr) tests to compare
models. for example, we could claim that one model
is just like another, but with the constraint that   12 = 0, and then test whether the
constraint is actually binding via the ratio of the likelihood with the constraint and
the likelihood without. or, say that we can   t decide between using an ols model
or a probit model; then the ratio of the likelihood of the two models can tell us the
con   dence with which one is more likely than the other.

14in klemens (2007), i discuss at length the utility of the variance of the id113 as a gauge of which of a

simulation   s parameters have a volatile effect on the outcome and which have little effect.

gsl_stats march 24, 2009

350

chapter 10

a loose derivation as intimated by the neyman   pearson lemma, the ratio of two
likelihoods is a good way to test a hypothesis. given the ratio

of two likelihoods p1/p2, the log is the difference ln(p1/p2) = ll1     ll2.
now consider the taylor expansion of a log likelihood function around     . the
taylor expansion is a common means of approximating a function f (x) via a series
of derivatives evaluated at a certain point. for example, the second-degree taylor
expansion around seven would be f (x)     f (7)+(x   7)f   (7)+(x   7)2f      (7)/2+  ,
where    is an error term. the approximation is exactly correct at x = 7, and
decreasingly precise (meaning    gets larger) for values further from seven. in the
case of the log likelihood expanded around     , the taylor expansion is

ll(  ) = ll(     ) + (           )ll   (     ) +

(           )2

2

ll      (     ) +   .

as per the de   nitions from the beginning of the chapter, the derivative in the sec-

ond term is the score, and the second derivative in the third term is    i. when     
is the optimum, the score is zero. also, as is the norm with taylor expansions, we
will assume    = 0. then the expansion simpli   es to
(           )2

(10.4.1)

i(     ).

ll(  ) = ll(     )    

2

typically, the likelihood ratio test involves the ratio of an unrestricted model and
the same model with a constraint imposed. let llc be the constrained log likeli-
hood; then we can repeat equation 10.4.1 with the constrained log likelihood:

llc(  ) = llc(     )    

(           )2

2

ic(     ).

now the hypothesis: the constraint is not binding, and therefore both constrained
and unconstrained optimizations    nd the same value of     . then

   2(ll(  )     llc(  )) = 2llc(     )     2ll(     ) + (           )2i(     )     (           )2ic(     )

(10.4.2)

= (           )2(cid:16)i(     )     ic(     )(cid:17)

the second equation follows from the    rst because having the same value for     
for constrained and unconstrained optimizations means that ll(     ) = llc(     ).

but we still haven   t said anything about the distribution of    2(ll(  )     llc(  )).
consider the case of the normal distribution with    xed    (so the only free param-

gsl_stats march 24, 2009

id113

eter is the mean   ); there, the score is

s(x,   ) =xi

(xi       )/  2.

351

(10.4.3)

q: verify this by    nding    ll(x,   )/      using the id203 distribution on page
241.

the right-hand side of equation 10.4.3 takes the familiar mean-like form upon
which the clt is based, and so is is normally distributed. since e(i) = e(s   
s), and a normally-distributed statistic squared has a   2 distribution, expression
10.4.2 has a   2 distribution.

and in fact, this holds for much more than a normal likelihood function (pawi-
tan, 2001, p 29). say that there exists a transformation function t(x,   ) such that
p (x,   )    t(x,   ) is normally distributed. then
=

.

p (x,   )    t(x,   )
pc(x,   )    t(x,   )

p (x,   )
pc(x,   )

instead of canceling out the transformation here, we could also cancel it out in the
log likelihood step:

ll(x,   ) + t(x,   )     llc(x,   )     t(x,   ) = ll(x,   )     llc(x,   ).

either way, expression 10.4.2 is the same with or without the transformation   
which means the untransformed version is also       2. so provided the likelihood
function is suf   ciently well-behaved that t(x,   ) could exist, we don   t have to
worry about deriving it. this is a speci   c case of the invariance principle of like-
lihood functions, that broadly says that transformations of the likelihood function
do not change the information embedded within the function.

this is what we can use to do the likelihood ratio tests that the neyman   pearson
lemma recommended. we    nd the log likelihood of the model in its unconstrained
and constrained forms, take two times the difference, and look up the result in the
  2 tables.

the lr test, constraint case as above, the typical likelihood ratio test involves
the ratio of an unrestricted and a restricted model,
and a null hypothesis that the constraint is not binding. let p be the (not-log,
plain) likelihood of the overall model, and pc be the likelihood of a model with k
restrictions, such as k parameters    xed as zero.

earlier, such as thea   _   bi ,a   _    a , or evena   _   , because the

in modeling terms, the unrestricted model could be any of the models discussed

=    2[ln p     ln pc]       2
k.

in this context, the above discussion becomes

   2 ln

(10.4.4)

p
pc

gsl_stats march 24, 2009

352

chapter 10

ols parameters (  ols = (x   x)   1xy) can be shown to be identical to the max-
imum likelihood estimate of   .

#include "eigenbox.h"

double linear_constraint(apop_data * d, apop_model *m){

apop_data *constr = apop_line_to_data((double[]) {0, 0, 0, 1}, 1, 1, 3);
return apop_linear_constraint(m   >parameters   >vector, constr, 0);

}

void show_chi_squared_test(apop_model *unconstrained, apop_model *constrained, int

constraints){
double statistic = 2 * (unconstrained   >llikelihood     constrained   >llikelihood);
double con   dence = gsl_cdf_chisq_p(statistic, constraints);
printf("the chi squared statistic is: %g, so reject the null of non   binding constraint "

"with %g%% con   dence.\n", statistic, con   dence*100);

}

int main(){

apop_data *d = query_data();
apop_model *unconstr = apop_estimate(d, apop_ols);
apop_model_show(unconstr);

apop_settings_add_group(&apop_ols, apop_id113, &apop_ols);
apop_settings_add(&apop_ols, apop_id113, starting_pt, unconstr   >parameters   >vector   >

data);

apop_settings_add(&apop_ols, apop_id113, use_score, 0);
apop_settings_add(&apop_ols, apop_id113, step_size, 1e   3);
apop_ols.estimate = null;
apop_ols.constraint = linear_constraint;
apop_model *constr = apop_estimate(d, apop_ols);
printf("new parameters:\n");
apop_vector_show(constr   >parameters   >vector);
show_chi_squared_test(unconstr, constr, 1);

model with constraint, and a logit model. online source:   e  .
.

}

listing 10.6 comparing three different models using likelihood ratios: an ols model, an ols

1
2
3
4
5
6
7
8

9
10
11
12
13
14
15
16
17
18
19
20
21

22
23
24
25
26
27
28
29
30

listing 10.6 presents an unconstrained and a constrained optimization. it uses the
query from page 267 that produces a data set whose outcome variable is males
per 100 females, and whose independent variables are population and median age.
the question is the coef   cient on median age signi   cant? can be rephrased to: if
we constrain the median age coef   cient to zero, does that have a signi   cant effect
on the log likelihood?

    the unconstrained optimization, on line 17, is the ordinary least squares model

(which, as above,    nds the id113).

id113

gsl_stats march 24, 2009

    lines 20   24 mangle the base ols model into a constrained model estimated via

maximum likelihood. by setting thee  i a e element to u   the estimation on
    the constraint function is on line 3, and it uses thea   _ i ea _
     ai  
function to test that an input vector satis   es a constraint expressed as ana   _	
da a set of the type    rst introduced on page 152. in this case, the constraint is
form, d 	b e[    , looks like a type cast, and it basically acts that way, declaring
that the data in braces is a nameless array ofd 	b es. the line is thus equivalent
to two lines of code, as at the top of the ai  routine in thef e  .
 program on

0 <   3; since the unconstrained ols estimation    nds that   3 < 0, this is a binding
constraint.

    line four uses a new syntactic trick: anonymous structures. the type-in-parens

line 25 uses the default method, which is id113.

353

page 311:

double tempvar[] = {0, 0, 0, 1};
apop_line_to_data(tempvar, 1, 1, 3);

but we can get away with packing it all onto one line and not bothering with the
temp variable. when used in conjunction with designated initializers, anonymous
structs can either convey a lot of information onto one line or make the code an
unreadable mess, depending on your   sthetic preferences.

    by commenting out the constraint-setting on line 24, you will have an uncon-
strained model estimated via maximum likelihood, and can thus verify that the
ols parameters and the id113 parameters are identical.

    you will recognize the function on line nine as a direct translation of expression
10.4.4. it is thus a test of the claim that the constraint is not binding, and it rejects
the null with about the same con   dence with which the t test associated with the
id75 rejected the null that the third parameter is zero.
    the statistic on line nine, ll     llc, is always positive, because whatever opti-
mum the constrained optimization found could also be used by the unconstrained
model, and the unconstrained model could potentially    nd something with even
higher likelihood. if this term is negative, then it is a sign that the unconstrained
optimization is still far from the true optimum, so restart it with a new method,
new starting point, tighter tolerances, or other tweaks.

be sure to compare the results of the test here with the results of the f test on page
311.

the lr test, non-nested case

the above form is a test of two nested models,
where one is a restricted form of the other, so un-
der the hypothesis of the nonbinding constraint, both can    nd the same estimate     
and so both can conceivably arrive at the same log likelihood. if this is not the case,

gsl_stats march 24, 2009

354

chapter 10

then the cancellation of the    rst part of the taylor expansion in equation 10.4.2
does not happen.

in this case (cox, 1962; vuong, 1989), the statistic and its distribution is

ln p1

p2(cid:17)
p2     e(cid:16)ln p1

   n

    n (0, 1).

(10.4.5)

the denominator is simply the square root of the sample size. the    rst part of the
numerator is just ll1     ll2, with which we are very familiar at this point. the
expected value is more problematic, because it is a global value of the log likeli-
hoods, which we would conceivably arrive at by a id203-weighted integral of
ll1(  )     ll2(  ) over the entire space of   s.
alternatively, we could just assume that it is zero. that is, the easiest test to run
with expression 10.4.5 is the null hypothesis of no difference between the expected
value of the two logs.

#de   ne testing
#include "dummies.c"

void show_normal_test(apop_model *unconstrained, apop_model *constrained, int n){
double statistic = (unconstrained   >llikelihood     constrained   >llikelihood)/sqrt(n);
double con   dence = gsl_cdf_gaussian_p(fabs(statistic), 1); //one   tailed.
printf("the normal statistic is: %g, so reject the null of no difference between models "

}

}

int main(){

"with %g%% con   dence.\n", statistic, con   dence*100);

apop_db_open("data   metro.db");
apop_model *m0 = dummies(0);
apop_model *m1 = dummies(1);
show_normal_test(m0, m1, m0   >data   >matrix   >size1);

listing 10.7 compare the two metro ridership models from page 282 online source:      e  .
.
    if you    ip back to thed	  ie .
    le, you will see that the ai  function is
wrapped by a preprocessor if-then statement:#if deftest  g. becausetest  g
is de   ned here, the ai  function in that    le will be passed over.
    therefore, the next line can read ind	  ie .
 directly, without ending up with
two ai s.

listing 10.7 reads in the code for the two ols estimations of washington metro
ridersiop from page 282, one with a zero-one dummy and one with a dummy for
the year   s slope.

gsl_stats march 24, 2009

    the ai  function here simply estimates two models, and then calls the h w_	
    a _ e   function, which is a translation of expression 10.4.5 under the null

id113

355

hypothesis that e(llols     lllogit) = 0.

remember, a number of approximations underly both the nested and non-nested
lr tests. in the nested case, they are generally considered to be innocuous and are
rarely veri   ed or even mentioned. for the non-nested probit and logit models, their
log likelihoods behave in a somewhat similar manner (as n        ), so it is reason-
able to apply the non-nested statistic above. but for two radically different models,
like an ols model versus an agent-based model, the approximations may start to
strain. you can directly compare the two log-likelihoods, and the test statistic will
give you a sense of the scale of the difference, but from there it is up to you to
decide what these statistics tell you about the two disparate models.

gsl_stats march 24, 2009

11

monte carlo

monte carlo (italian and spanish for mount carl) is a city in monaco famous
for its casinos, and has more glamorous associations with its name than reno or
atlantic city.

monte carlo methods are thus about randomization: taking existing data and mak-
ing random transformations to learn more about it. but although the process in-
volves randomness, its outcome is not just the mere whim of the fates. at the
roulette table, a single player may come out ahead, but with millions of suckers
testing their luck, casinos    nd that even a 49   51 bet in their favor is a reliable
method of making money. similarly, a single random transformation of the data
will no doubt produce a somehow distorted impression, but reapplying it thousands
or millions of times will present an increasingly accurate picture of the underlying
data.

this chapter will    rst look at the basics of random number generation. it will then
discuss the general process of describing parameters of a distribution, parameters
of a data set, or a distribution as a whole via monte carlo techniques. as a special
case, id64 is a method for getting a variance out of data that, by all rights,
should not be able to give you a variance. nonparametric methods also make a
return in this chapter, because shuf   ing and redrawing from the data can give you
a feel for the odds that some hypothesized event would have occurred; that is, we
can write hypothesis tests based on resampling from the data.

gsl_stats march 24, 2009

monte carlo

357

gsl_rng *apop_rng_alloc(int seed){

static int    rst_use = 1;
if (   rst_use){

   rst_use       ;
gsl_rng_env_setup();

}
gsl_rng *setme = gsl_rng_alloc(gsl_rng_taus2);
gsl_rng_set(setme, seed);
return setme;

}

listing 11.1 allocating and initializing a random number generator.

11.1 random number generation we need a stream of random num-
bers, but to get any programming

done, we need a replicable stream of random numbers.1

there are two places where you will need replication. the    rst is with debugging,
since you don   t want the segfault you are trying to track down to appear and dis-
appear every other run. the second is in reporting your results, because when a
colleague asks to see how you arrived at your numbers, you should be able to
reproduce them exactly.

of course, using the same stream of numbers every time creates the possibility
of getting a lucky draw, where lucky can mean any of a number of things. the
compromise is to use a collection of deterministic streams of numbers that have no
apparent pattern, where each stream of numbers is indexed by its    rst value, the
seed.2 the gsl implements such a process.

listing 11.1 shows the innards of thea   _  g_a   
 function from the apo-
phenia library to initialize ag  _  g. in all cases, the function takes in an integer,
the gsl documentation for setting up ag  _  g with alternative algorithms. on
the    rst call, the function calls theg  _  g_e v_ e 	  function to work some

and then sets up the random number generation (rng) environment to produce
new numbers via the tausworth routine. fans of other rng methods can check

internal magic in the gsl. listings 11.6 and 11.7 below show an example using
this function.

1is it valid to call a replicable stream of seemingly random numbers random? because such rngs are arguably
not random, some prefer the term pseudorandom number generator (prng) to describe them. this question is
rooted in a philosophical question into which this book will not delve: what is the difference between perceived
randomness given some level of information and true randomness? see, e.g., good (1972, pp 127   8).

2formally, the rng produces only one stream, that eventually cycles around to the beginning. the seed simply
speci   es where in the cycle to begin. but because the cycle is so long, there is little loss in thinking about each
seed producing a separate stream.

if you initialize one rng stream with the value of another, then they are both at the same point in the cycle,
and they will follow in lock-step from then on. this is to be avoided; if you need a sequence of streams, you are
better off just using a simple list of seeds like 0, 1, 2, . . . .

gsl_stats march 24, 2009

358

chapter 11

random number distributions now that you have a random number gener-
ator, here are some functions that use it to
draw from all of your favorite distributions. input an rng as allocated above plus
the appropriate parameters, and the gsl will transform the rng as necessary.

    the gaussian draw assumes a mean of zero, so if you intend to draw from, e.g., a

    the    at distribution is a uniform[a,b) distribution. the uniform[0,1) distribution

tions to make random draws, and allows standardization with more exotic models
like the histogram below; see the example in listing 11.5, page 361.

double gsl_ran_bernoulli (gsl_rng *r, double p);
double gsl_ran_beta (gsl_rng *r, double a, double b);
double gsl_ran_binomial (gsl_rng *r, double p, int n);
double gsl_ran_chisq (gsl_rng *r, double df);
double gsl_ran_fdist (gsl_rng *r, double df1, double df2);
double gsl_ran_gaussian (gsl_rng *r, double sigma);
double gsl_ran_tdist (gsl_rng *r, double df);
double gsl_ran_   at (gsl_rng *r, double a, double b);
double gsl_rng_uniform (gsl_rng *r);

gets its own no-options function,g  _  g_	 if      .
n (7, 2), then useg  _ a _ga	  ia    2  7.
    thea   _  de  struct includes ad aw method that works like the above func-
a convenience function,a   _be a_f   _ ea _va , that takes in    and   2 and
that has positive density iff x     [0, 1] must be     (0, 1). if you senda   _be a_	
f   _ ea _va  values of    and   2 that are outside of these bounds, the function
will returngs _ a .

the beta distribution is wonderful for all sorts
of modeling, because it can describe such a
wide range of id203 functions for a variable     [0, 1]. for example, you saw
it used as a prior on page 259. but its    and    parameters may be dif   cult to
interpret; we are more used to the mean and variance. thus, apophenia provides

as you know, the variance of a uniform[0, 1] is exactly 1
the beta distribution will never have a variance greater than 1
perverse things may happen computationally for    6    1

12 , which means that
12 (and close to 1
12 ,
2 ).3 the mean of a function

returns an appropriate beta distribution, with the corresponding values of    and   .

an example: the beta distribution

what does a beta distribution with, say,    = 3
24 = .04166 look like?
listing 11.2 sets up an rng, makes a million draws from a beta distribution, and
plots the result.

8 ,   2 = 1

3more trivia: the uniform[0, 1] is symmetric, so its skew is zero. its kurtosis is 1

80 .

gsl_stats march 24, 2009

359

monte carlo

#include <apop.h>

int main(){

int draws = 1e7;
int bins = 100;
double mu = 0.492; //also try 3./8.
double sigmasq = 0.093; //also try 1./24.
gsl_rng *r = apop_rng_alloc(0);
apop_data *d = apop_data_alloc(0, draws, 1);

apop_draw(apop_data_ptr(d, i, 0), r, m);

apop_model *m = apop_beta_from_mean_var(mu, sigmasq);
for (int i =0; i < draws; i++)

listing 11.2 building a picture of a distribution via random draws. online source:d awbe a.
.

apop_settings_add_group(&apop_histogram, apop_histogram, d, bins)
apop_histogram_normalize(&apop_histogram);
apop_histogram_plot(&apop_histogram, null);

}

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

0.02

0.018

0.016

0.018
0.016
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

x

0.01

0.014

0.012

0.008

)
x
(
p

)
x
(
p

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

figure 11.3 the    exible beta distribution. run viad awbe a|g 	    .
wherea   _be a_f   _ ea _va  takes in    and   2 and returns ana   _  de 
    in line 14, the data setd is    lled with random draws from the model. thea   _	
d aw function takes in a pointer-to-d 	b e as the    rst argument, and puts a value
ments. thus, you will need to usea   _da a_   ,g  _ve
   _   , org  _	
 a  ix_    witha   _d aw to get a pointer to the right location. the slightly
    the    nal few lines of code take the generica   _hi   g a  model, set it to use

awkward pointer syntax means that no copying or reallocation is necessary, so
there is less to slow down drawing millions of numbers.

    most of the code simply names constants; the    rst real action occurs on line 11,

in that location based on the rng and model sent as the second and third argu-

representing a beta distribution with the appropriate parameters.

x

gsl_stats march 24, 2009

the datad and the given number of bins, normalize the histogram thus produced to

chapter 11

360

have a total density of one, and plot the result.

the output of this example (using 1e7 draws) is at left in figure 11.3; by contrast,
the case where    = 0.492,   2 = 0.093 is pictured at right.

a(   ) =   r2 =   
a(2) = (2r)2 = 4

r

is   /4. online source:  	a e
i 
 e.g 	    .

figure 11.4 a circle inscribed in a square. the ratio of the area of the circle to the area of the square

q11.1

as per figure 11.4, when a circle is inscribed inside a square, the ratio of
the area of the circle to the square is   /4. thus, if we randomly draw 100
points from the square, we expect 100  /4 to fall within the circle.
estimate    via random draws from a square. for i in zero to about 1e8:

from a uniform[   1, 1] distribution; draw yi

    draw xi
uniform[   1, 1] distribution.
    determine whether (xi, yi) falls within the unit circle, meaning that

clearer to displayfab   _  	 i_e  i a e  instead of the esti-

    every 10,000 draws, display the proportion of draws inside the circle
times four. how close to    does the estimate come? (hint: it may be

i     1 (which is equivalent to x2

qx2

i     1).

i + y2

i + y2

from a

mate itself.)

gsl_stats march 24, 2009

361

monte carlo

approximates, would be to draw from your actual data.

if your data are in a vector, then just draw a random index and return the value

drawing from your own data another possibility, beyond drawing from fa-
mous distributions that your data theoretically

at that index. let  be an appropriately initializedg  _  g, and lety 	 _da a
be ag  _ve
    from which you would like to make draws. then the following
  drawing from histograms
7, for goodness of    t tests in chapter 9, and as the output from thea   _	 da e

there are a few reasons for your data to be in a his-
togram form   a rough id203 mass function   
like the ones used for plotting data in chapter 5, for describing data in chapter

gsl_vector_get(your_data, gsl_rng_uniform_int(r, your_data   >size));

one-liner would make a single draw:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

function. here, we will draw from them to produce arti   cial data sets.

#include <apop.h>
gsl_rng *r;

void plot_draws(apop_model *m, char *out   le){

int draws = 2e3;
apop_data *d = apop_data_alloc(0, draws,1);

for(size_t i=0; i < draws; i++)

apop_draw(apop_data_ptr(d, i, 0), r, m);

apop_model *h3 = apop_estimate(d, apop_histogram);
apop_histogram_print(h3, out   le);

}

int main(){

r = apop_rng_alloc(1);
apop_db_open("data   wb.db");
apop_data *pops = apop_query_to_data("select population+0.0 p from pop where p>500");
apop_model *h = apop_estimate(pops, apop_histogram);
apop_histogram_print(h, "out.hist");
plot_draws(apop_estimate(pops, apop_lognormal), "lognormal_   t");
printf("set xrange [0:2e5]; set yrange [0:0.12]; \n \

distribution that most closely    ts. online source:d awf      .
.

plot    out.hist    with boxes,    lognormal_   t    with lines\n");

listing 11.5 draw from the actual histogram of country populations, and from the exponential

}

for example, say that we would like to generate communities whose populations
are distributed the way countries of the world are distributed. listing 11.5 does

gsl_stats march 24, 2009

362

chapter 11

this two ways. the simplest approach is to simply generate a histogram of world
populations and make draws from that histogram.

distribution, and then plots those. you can see that the result is smoother, without
the zero-entry bins that the real-world data has.

in some cases, there are simply not enough data for the job. the world bank data
set lists 208 countries; if your simulation produces millions of communities, the
repetition of 208 numbers could produce odd effects. the solution presented here
is to estimate a lognormal distribution, and then draw from that ideal distribution.

    line 17 creates a    lled histogram by    lling the un-parametrized basea   _	
hi   g a  model with a list of populations.
line 21 does the model    t and then sends the output of thea   _e  i a e func-
tion to the    _d aw  function, which makes a multitude of draws from the ideal
    lines 7   8    ll column zero ofd with data, then line 9 turns that data into a his-
  seeding with the time
run your program. the easiest solution is to seed using the i e function. the
standard library function i e  u    will return the number of seconds that have
composed. as i write this, i e returns 1,147,182,523   not a very good way to
listing 11.6, i e.
, shows a sample program that produces ten draws from an
case, try compiling i e.
, and run it continuously from the shell. in a bourne-

tell the time. there are a number of functions that will turn this into hours, minutes,
or months; see any reference on c   s standard library (such as the gnu c library
documentation) for details. but this illegible form provides the perfect seed for an
rng, and you get a new one every second.

    the easiest way to put two data sets on one plot is to write both of them to separate
   les (lines 18 and 10), and then call those    les from a gnuplot script (lines 20   21).

rng seeded with the time. this is not industrial-strength random number gener-
ation, because patterns could conceivably slip in. for an example of the extreme

there are situations where a    xed seed is really not what
you want   you want different numbers every time you

elapsed since the beginning of 1970, which is roughly when unix and c were    rst

togram.

family shell (what you are probably using on a posix system), try

while true; do ./time; done

you should see the same numbers a few dozen times until the clock ticks over, and
then another stream repeated several times. [you can get your command prompt

gsl_stats march 24, 2009

monte carlo

363

}

int main(){

printf("\n");

printf("%.3g\t", gsl_rng_uniform(r));

#include <time.h>
#include <apop.h>

//i assume the database is already open and has a one-column

gsl_rng *r = apop_rng_alloc(time(null));
for (int i =0; i< 10; i++)

long int right_now = time(null);
apop_query("insert into runs (%li);", right_now);
gsl_rng *r = apop_rng_alloc(right_now);

back using <ctrl-c>.] if you have multiple processors and run one simulation on
each, then runs that start in the same second will be replicating each other. finally,
if you ever hope to debug this program, then you will need to write down the time
started so that you can replicate the runs that break:

listing 11.6 seeding an rng using the time. online source: i e.
.
//table named 	  . the time is a long integer
//in the gnu standard library, so its printf tag is  i.
  the standard c rng if the gsl is not available, the standard c library in-
cludes a a d function to make random draws and an
  a d function to set its seed. e.g.:
the gsl   s rngs are preferable for a few reasons. first,g  _ a _ ax    is typ-
ically greater thanra d_ ax, giving you greater variation and precision. second,
the c language standard speci   es that there must be a a d function, but not how it
finally, a d gives your entire program exactly one stream of numbers, while you
can initialize manyg  _  gs that will be independent of each other. for example,

caveats aside, if you just want to see some variety every time the program runs,
then seeding with the time works    ne.

works, meaning two machines may give you different streams of random numbers
for the same seed.

#include <stdlib.h>
srand(27);
printf("one draw from a u[0,1]: %g", rand()/(rand_max +0.0));

if you give every agent in a simulation its own rng, you can re-run the simulation

gsl_stats march 24, 2009

364

chapter 11

with one agent added or removed (probably at a breakpoint in gdb) and are guar-
anteed that the variation is due to the agent, not rng shifts. here is some sample
code to clarify how such a setup would be initialized:

typedef struct agent{

long int agent_number;
gsl_rng *r;
...

} agent;

void init_agent(agent *initme, int agent_no){

initme   >agent_number = agent_no;
initme   >r = apop_rng_init(agent_no);
...

}

   random number generators produce a deterministic sequence of val-
ues. this is a good thing, because we could never debug a program or
replicate results without it. change the stream by initializing it with a
different seed.

   given a random number generator, you can use it to draw from any

common distribution, from a histogram, or from a data set.

11.2 description: finding

statistics for

a distribution

for many statistic-distribution pairs, there ex-
ists a closed-form solution for the statistic:
the kurtosis of a n (  ,   ) is 3  4, the vari-
ance of a binomial distribution is np(1     p),
et cetera. you can also take recourse in the slutsky theorem, that says that given an
estimate r for some statistic    and a continuous function f (  ), then f (r) is a valid
estimate of f (  ). thus, sums or products of means, variances, and so on are easy
to calculate as well.

however, we often    nd situations where we need a global value like a mean or
variance, but have no closed-form means of calculating that value. even when
there is a closed-form theorem that begins in the limit as n        , it holds that. . . ,
there is often evidence of the theorem falling    at for the few dozen data points
before us.

one way to calculate the expected value of a statistic f (  ) given id203 distri-
bution p(  ) would be a numeric integral over the entire domain of the distribution.
for a resolution of 100,000 slices, write a loop to sum

gsl_stats march 24, 2009

monte carlo

e[f (  )|p(  )] =

f (   500.00)    p(   500.00)

100, 000

+

+        +

f (499.99)    p(499.99)

100, 000

+

f (   499.99)    p(   499.99)

100, 000
f (500.00)    p(500.00)

100, 000

365

.

this can be effective, but there are some details to be hammered out: if your
distribution has a domain over (      ,   ), should you integrate over [   3, 3] or
[   30, 30]? you must decide up-front how    ne the resolution will be, because (bar-
ring some tricks) each resolution is a new calculation rather than a modi   cation of
prior calculations. if you would like to take this approach, the gsl includes a set
of numerical integration functions.

another approach is to evaluate f (  ) at values randomly drawn from the distribu-
tion. just as listing 11.2 produced a nice picture of the beta distribution by taking
enough random draws, a decent number of random draws can produce a good es-
timate of any desired statistic of the overall distribution. values will, by de   nition,
appear in proportion to their likelihood, so the p(  ) part takes care of itself. there
is no cutoff such that the tails of the distribution are assumed away. you can incre-
mentally monitor e[f (  )] at 1,000 random draws, at 10,000, and so on, to see how
much more you are getting with the extra time.

kurtosis of a vector is easy to calculate   just calla   _ve
   _k	    i . by
    the ai  function just sets up the header of the output table (see table 11.8) and
calls  e_df for each df .
    thef   loop on lines 6   7 does the draws, storing them in the vectorv.

an example: the kurtosis of a t distribution you probably know that a t distribu-
tion is much like a normal distribu-
tion but with fatter tails, but probably not how much fatter those tails are. the

taking a million or so draws from a t distribution, we can produce a vector whose
values cover the distribution rather well, and then    nd the kurtosis of that vector.

listing 11.7 shows a program to execute this procedure.

    once the vector is    lled, line eight calculates the partially normalized kurtosis.
that is, it calculates raw kurtosis over variance squared; see the box on page 230
on the endless debate over how best to express kurtosis.

the closed-form formula for the partially-normalized kurtosis of a t distribution
with df > 4 degrees of freedom is (3df     6)/(df     4). for df     4, the kurtosis
is unde   ned, just as the variance is unde   ned for a cauchy distribution (i.e., a t
distribution with df = 1). at df = 5, it is    nite, and it monotonically decreases as
df continues upwards.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

gsl_stats march 24, 2009

366

#include <apop.h>

void one_df(int df, gsl_rng *r){

long int i, runct = 1e6;
gsl_vector *v = gsl_vector_alloc(runct);

for (i=0; i< runct; i++)

gsl_vector_set(v, i, gsl_ran_tdist(r, df));

chapter 11

printf("%i\t %g", df, apop_vector_kurtosis(v)/gsl_pow_2(apop_vector_var(v)));
if (df > 4)

printf("\t%g", (3.*df     6.)/(df   4.));

printf("\n");
gsl_vector_free(v);

listing 11.7 monte carlo calculation of kurtoses for the t distribution family. online source:

}

}

int main(){

one_df(df, r);

printf("df\t k (est)\t k (actual)\n");
for (df=1; df< df_max; df++)

int df, df_max = 31;
gsl_rng *r = apop_rng_alloc(0);

 di  k	    i .
.
line 17 of the code with i e  u   ).

table 11.8 shows an excerpt from the simulation output, along with the true kur-
tosis.

the exact format for the variance of the estimate of kurtosis will not be given here
(q: use the methods here to    nd it), but it falls with df : with df < 4, we may
as well take the variance of the kurtosis estimate as in   nite, and it shrinks as df
grows. correspondingly, the bootstrap estimates of the kurtosis are unreliable for
df = 5 or 6, but are more consistent for df over a few dozen. you can check this
by re-running the program with different seeds (e.g., replacing the zero seed on

   by making random draws from a model, we can make statements
about global properties of the model that improve in accuracy as the
number of draws        .

   the variance of the monte carlo estimation of a parameter tends
to mirror the variance of the underlying parameter. the maximum
likelihood estimator for a parameter achieves the cram  r   rao lower
bound, so the variance of the monte carlo estimate will be larger
(perhaps signi   cantly so).

gsl_stats march 24, 2009

monte carlo

367

df
1
2
3
4
5
6
7
8
9
10
15
20
25
30

k (est)
183640
5426.32
62.8055
19.2416
8.5952
6.00039
4.92161
4.52638
4.17413
4.00678
3.55957
3.37705
3.281
3.23014

k (analytic)

9
6
5
4.5
4.2
4
3.54545
3.375
3.28571
3.23077

table 11.8 the fatness of the tails of t distributions at various df .

11.3

id136: finding
statistics for a

the t distribution example made random draws
from a closed-form distribution, in order to pro-
duce an estimate of a function of the distribu-
tion parameters. conversely, say that we want
to estimate a function of the data, such as the variance of the mean of a data set,

parameter

cvar(    (x)). we have only one data set before us, but we can make random draws

from x to produce several values of the statistic     (xr), where xr represents a ran-
dom draw from x, and then estimate the variance of those draws. this is known
as the bootstrap method of estimating a variance.

id64 the standard error the core of the bootstrap is a simple

algorithm:

repeat the following m times:

let   x be n elements randomly drawn from the data, with replacement.
write down the statistic(s)   (   x).

find the standard error of the m values of   (   x).

this algorithm bears some resemblance to the steps demonstrated by the clt
demo in listing 9.1 (page 298): draw m iid samples,    nd a statistic like the mean
of each, and then look at the distribution of the several statistics (rather than the
underlying data itself). so if   (   x) is a mean-like statistic (involving a sum over

1
2
3
4
5
6
7

8

9
10
11
12
13
14
15
16
17
18
19

gsl_stats march 24, 2009

368

chapter 11

n), then the clt applies directly, and the arti   cial statistic approaches a normal
distribution. thus, it makes sense to apply the usual normal distribution-based test
to test hypotheses about the true value of   .

#include "oneboot.h"

int main(){

int rep_ct = 10000;
gsl_rng *r = apop_rng_alloc(0);

apop_db_open("data   census.db");
gsl_vector *base_data = apop_query_to_vector("select in_per_capita from income where

suid113vel+0.0 =40");

double ri = apop_query_to_   oat("select in_per_capita from income where suid113vel+0.0

=40 and geo_ +0.0=44");

listing 11.9 id64 the standard error of the variance in state incomes per capita. online

mean, stderror, (ri   mean)/stderror, 2*gsl_cdf_gaussian_q(fabs(ri   mean), stderror));

gsl_vector *boot_sample = gsl_vector_alloc(base_data   >size);
gsl_vector *replications = gsl_vector_alloc(rep_ct);
for (int i=0; i< rep_ct; i++){

one_boot(base_data, r, boot_sample);
gsl_vector_set(replications, i, apop_mean(boot_sample));

}
double stderror = sqrt(apop_var(replications));
double mean = apop_mean(replications);
printf("mean: %g; standard error: %g; (ri   mean)/stderr: %g; p value: %g\n",

source:da ab   .
.
listing 11.10 a function to produce a bootstrap draw. online source:  eb   .
.

gsl_vector_get(base_data, gsl_rng_uniform_int(r, base_data   >size)));

for (int i =0; i< boot_sample   >size; i++)

gsl_vector_set(boot_sample, i,

void one_boot(gsl_vector *base_data, gsl_rng *r, gsl_vector* boot_sample){

#include "oneboot.h"

}

}

listings 11.10 and 11.9 shows how this algorithm is executed in code. it tests the
hypothesis that rhode island   s income per capita is different from the mean.

    lines 4   8 of listing 11.9 are introductory material and the queries to pull the req-
uisite data. for rhode island, this is just a scalar, used in the test below, but for
the rest of the country, this is a vector of 52 numbers (one for each state, common-
wealth, district, and territory in the data).

gsl_stats march 24, 2009

369

monte carlo

makes the draws and then    nds the mean of the draws.

    listing 11.10 is a function to make a single bootstrap draw, which will be used in

    lines 11   14 show the main loop repeated m times in the pseudocode above, which

a few other scripts below. the  e_b    function draws with replacement, which
simply requires repeatedly callingg  _  g_	 if   _i   to get a random index
rewriteda ab   .
 to calculate the standard error ofba e_da a directly

    recall the discussion on page 300 about the standard deviation of a data set, which
is the square root of its variance,   ; and the standard deviation of the mean of a
data set, which is   /   n. in this case, we are interested in the distribution of   (   x)
itself, not the distribution of e(  (   x))s, so we use    instead of   /   n.

    lines 15 and 16 of listing 11.10    nd the mean and standard error of the returned
data, and then lines 17   18 run the standard hypothesis test comparing the mean of
a normal distribution to a scalar.

and then writing down the value at that index in the data vector.

q11.2

(without id64), and test the same hypothesis using that standard
error estimate rather than the bootstrapped version. do you need to test
with your calculated value of      or with     /   n?

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

#include <apop.h>

int main(){

int draws = 5000, boots = 1000;
double mu = 1., sigma = 3.;
gsl_rng *r = apop_rng_alloc(2);
gsl_vector *d = gsl_vector_alloc(draws);
apop_model *m = apop_model_set_parameters(apop_normal, mu, sigma);
apop_data *boot_stats = apop_data_alloc(0, boots, 2);
apop_name_add(boot_stats   >names, "mu",    c   );
apop_name_add(boot_stats   >names, "sigma",    c   );

for (int i =0; i< boots; i++){

for (int j =0; j< draws; j++)

apop_draw(gsl_vector_ptr(d, j), r, m);

apop_data_set(boot_stats, i, 0, apop_vector_mean(d));
apop_data_set(boot_stats, i, 1, sqrt(apop_vector_var(d)));

}
apop_data_show(apop_data_covariance(boot_stats));
printf("actual:\n var(mu) %g\n", gsl_pow_2(sigma)/draws);
printf("var(sigma): %g\n", gsl_pow_2(sigma)/(2*draws));

    a b   .
.

}

listing 11.11 estimate the covariance matrix for

the normal distribution online source:

gsl_stats march 24, 2009

370

chapter 11

a normal example

say that we want to know the covariance matrix for the esti-
mates of the normal distribution parameters, (    ,     ). we could

look it up and    nd that it is

"   2

n
0

2n# ,

0
  2

but that would require effort and looking in books.4 so instead we can write a
program like listing 11.11 to generate the covariance for us. running the program
will show that the computational method comes reasonably close to the analytic
value.

    the introductory material up to line 11 allocates a vector for the bootstrap sam-
ples and a data set for holding the mean and standard deviation of each bootstrap
sample.

matrix rather than just a scalar variance.

    in this case, we are producing two statistics,    and   , so line 18    nds the covariance

    instead of drawing permutations from a    xed data set, this version of the program
produces each arti   cial data vector via draws from the normal distribution itself,
on lines 13   14, and then lines 15   16 write down statistics for each data set.

a normal distribution equals 3  4 via a bootstrap. modify    a b   .
 to

on page 321, i mentioned that you could test the claim that the kurtosis of

q11.3

test this hypothesis. where the program currently    nds means and variances
of the samples,    nd each sample   s kurtosis. then run a t test on the claim
that the mean of the vector of kurtoses is 3  4.

   to test a hypothesis about a model parameter, we need to have an

estimate of the parameter   s variance.

   if there is no analytic way to    nd this variance, we can make multi-
ple draws from the data itself, calculate the parameter, and then    nd
the variance of that arti   cial set of parameters. the central limit
theorem tells us that the arti   cial parameter set will approach a well-
behaved normal distribution.

4e.g., kmenta (1986, p 182). that text provides the covariance matrix for the parameters (    ,     2). the variance

of      is the same, but the variance of     2 = 2  4
n .

gsl_stats march 24, 2009

monte carlo

371

11.4 drawing a distribution

to this point, we have been searching for
global parameters of either a distribution
or a data set, but the output has been a single number or a small matrix. what if
we want to    nd the entire distribution?

we have at our disposal standard rngs to draw from normal or uniform distribu-
tions, and this section will present are a few techniques to transform those draws
into draws from the distribution at hand. for the same reasons for which random
draws were preferable to brute-force numeric integration, it can be much more ef-
   cient to produce a picture of a distribution via random draws than via grid search:
the draws focus on the most likely points, the tails are not cut off at an arbitrary
limit, and a desired level of precision can be reached with less computation.

(currently hard-coded into the model   sg  w h function). given a run, we could

and remember, anything that produces a nonnegative univariate measure can be
read as a subjective likelihood function. for example, say that we have real-world
data about the distribution of    rm sizes. the model on page 253 also gives us an
arti   cial distribution of    rm sizes given input parameters such as the standard error

   nd the distance d(  ) between the actual distribution and the arti   cial. notice that
it is a function of   , because every    produces a new output distribution and there-
fore a new distance to the actual. the most likely value of    is that which produces
the smallest distance, so we could write down l(  ) = 1/d(  ), for example.5
then we could use the methods in this chapter and the last to    nd the most likely
parameters, draw a picture of the likelihood function given the input parameters,
calculate the variance given the subjective likelihood function, or test hypotheses
given the subjective likelihood.

importance
sampling

there are many means of making draws from one distribution to in-
form draws from another; train (2003, chapter 9) catalogs many of
them.

for example, importance sampling is a means of producing draws from a new
function using a well-known function for reference. let f (  ) be the function of
interest, and let g(  ) be a well-known distribution like the normal or uniform. we
want to use the stock normal or uniform distribution rngs to produce an rng
for an arbitrary function.

for i = 1 to a few million:

draw a new point, xi, from the reference distribution g(  ).
give the point weighting f (xi)/g(xi).

bin the data points into a histogram (weighting each point appropriately).

5recall the invariance principle from page 351, that basic transformations of the likelihood function don   t
change the data therein, so we don   t have to fret over the exact form of the subjective likelihood function, and
would get the same results using 1/d as using 1/d2 or 1/(d + 0.1), for example.

gsl_stats march 24, 2009

372

chapter 11

at the end of this, we have a histogram that is a valid representation of f (  ), which
can be used for making draws, graphing the function, or calculating global infor-
mation.

what reference distribution should you use? theoretically, any will do, but engi-
neering considerations advise that you pick a function that is reasonably close to
the one you are trying to draw from   you want the high-likelihood parts of g(  )
to match the high-likelihood parts of f (  ). so if f (  ) is generally a bell curve, use
a normal distribution; if it is focused near zero, use a lognormal or exponential;
and if you truly have no information, fall back to the uniform (perhaps for a rough
exploratory search that will let you make a better choice).

id115 id115 is an iterative al-
gorithm that starts at one state and has a rule
de   ning the likelihood of transitioning to any other given state from that initial
state   a markov chain. in the context here, the state is simply a possible value for
the parameter(s) being estimated. by jumping to a parameter value in proportion
to its likelihood, we can get a view of the id203 density of the parameters.
the exact id203 of jumping to another point is given a speci   c form to be
presented shortly.

this is primarily used for bayesian updating, so let us review the setup of that
method. the analysis begins with a prior distribution expressing current beliefs
about the parameters, pprior(  ), and a likelihood function pl(x|  ) expressing
the likelihood of an observation given a value of the parameters. the goal is to
combine these two to form a posterior; as per page 258, bayes   s rule tells us that
this is

ppost(  |x) =

pl(x|  )pprior(  )

r   b   b pl(x|b)pprior(  )db

the numerator is easy to calculate, but the denominator is a global value, meaning
that we can not know it with certainty without evaluating the numerator at an
in   nite number of points. the metropolis   hastings algorithm, a type of mcmc,
offers a solution.

the gist is that we start at an arbitrary point, and draw a new candidate point from
the prior distribution. if the candidate point is more likely than the current point
(according to pl), jump to it, and if the candidate point is less likely, perhaps jump
to it anyway. after a burn-in period, record the values. the histogram of recorded
values will be the histogram of the posterior distribution. to be more precise, here
is a pseudocode description of the algorithm:

gsl_stats march 24, 2009

monte carlo

373

begin by setting   0 to an arbitrary starting value.
for i = 1 to a few million:

draw a new proposed point,   p, from pprior.
if pl(x|  p) > pl(x|  i   1)
else

  i       p
draw a value u     u[0, 1]
if u < pl(x|  p)/pl(x|  i   1)
else

  i       p
  i       i   1

if i > 1, 000 or so, record   i.

report the histogram of   is as the posterior distribution of   .

as should be evident from the description of the algorithm, it is primarily used
to go from a prior to a posterior distribution. to jump to a new point   new, it
must    rst be chosen by the prior (which happens with id203 pprior(  new))
and then will be selected with a likelihood roughly proportional to pl(x|  new),
so the recorded draw will be proportional to pprior(  new)pl(x|  new). this rough
intuitive argument can be made rigorous to prove that the points recorded are a
valid representation of the posterior; see gelman et al. (1995).

why not just draw from the prior and multiply by the likelihood directly, rather
than going through this business of conditionally jumping? it is a matter of ef   -
ciency. the more likely elements of the posterior are more likely to contribute a
large amount to the integral in the denominator of the updating equation above,
so we can estimate that integral more ef   ciently by biasing draws toward the most
likely posterior values, and the jumping scheme achieves this with fewer draws
than the na  ve draw-from-the-prior scheme does.

you have already seen one concrete example of mcmc: the simulated annealing
algorithm from chapter 10. it also jumps from point to point using a decision
rule like the one above. the proposal distribution is basically an improper uniform
distribution   any point is as likely as any other   and the likelihood function is the
id203 distribution whose optimum the simulated annealing is seeking. the
difference is that the simulated annealing algorithm makes smaller and smaller
jumps; combined with the fact that mcmc is designed to tend toward more likely
points, it will eventually settle on a maximum value, at which point we throw away
all the prior values. for the bayesian updating algorithm here, jumps are from a
   xed prior, and tend toward the most likely values of the posterior, but the 1, 000th
jump is as likely to be a long one as the    rst, and we use every point found to
produce the output distribution.

at the computer, the function to execute this algorithm isa   _	 da e. this func-

tion takes in a parametrized prior model, an unparametrized likelihood and data,
and outputs a parametrized posterior model.

gsl_stats march 24, 2009

374

chapter 11

form gamma distribution with appropriately updated parameters. but if the tables
of closed form conjugates offer nothing applicable, the system will fall back on

the output model will be one of two types. as with the beta/binomial example on
page 259, there are many well-known conjugate distributions, where the posterior
will be of the same form as the prior distribution, but with updated parameters.

for example, if the prior is named gamma distribution (which thea   _ga  a
model is named) and the likelihood is named exponential distribution (anda   _	
ex   e  ia  is so named), then the output ofa   _	 da e will be a closed-
mcmc and output ana   _hi   g a  model with the results of the jumps.
randomly-generated data, and   prior toa   _	 da e.
    make a copy of thea   _ex   e  ia  model and change its name
to something else (so thata   _	 da e won   t    nd it in the conjugate
distribution table). re-send everything toa   _	 da e.

    pick an arbitrary parameter for the exponential distribution,   i . gen-

check how close the mcmc algorithm comes to the closed-form model.

    pick a few arbitrary values for the gamma distribution parameters,

    estimate the posterior, by sending the two distributions,

erate ten or twenty thousand random data points.

  prior.

the

q11.4

    use a goodness-of-   t test to    nd how well the two output distributions

match.

the output to the updating process is just another distribution, so it can be
used as the prior for a new updating step. in theory, distributions repeatedly
updated with new data will approach putting all id203 mass on the true
parameter value. continuing from the last exercise:

distribution using the same   i .

    send the new data, the closed-form posterior from the prior exercise,

    regenerate a new set of random data points from the exponential

af   loop to generate an animation beginning with the prior and

and the same exponential model to the updating routine.

    once you have the generate/update/plot routine working, call it from

    plot the output.

continuing with about a dozen updates.

    repeat with the mcmc-estimated posteriors.

q11.5

gsl_stats march 24, 2009

monte carlo

375

11.5 non-parametric testing

section 11.3 presented a procedure for
testing a claim about a data set that con-
sisted of drawing a bootstrap data set, regenerating a statistic, and then using the
many draws of the statistic and the clt to say something about the data. but we
can test some hypotheses by simply making draws from the data and checking
their characteristics, without bothering to produce a statistic and use the clt.

to give a concrete example of testing without parametric assumptions, consider
the permutation test. say that you draw ten red cards and twenty black cards from
what may be a crooked deck. the hypothesis is that the mean value of the red
cards you drew equals the mean of the black cards (counting face cards however
you like). you could put the red cards in one pile to your left, the black cards in a
pile to your right, calculate xred,     red, xblack, and     black, assume xblack     xred     t
distribution, and run a traditional t test to compare the two means.

but if   red really equals   black, then the fact that some of the cards you drew are
black and some are red is irrelevant to the value of xleft     xright. that is, if you
shuf   ed together the stacks of black and red cards, and dealt out another ten cards
to your left and twenty to your right, then xleft     xright should not be appreciably
different. if you deal out a few thousand such shuf   ed pairs of piles, then you can
draw the distribution of values for xleft     xright. if xred     xblack looks as if it is
very far from the center of that distribution, then we can reject the claim that the
color of the cards is irrelevant.

what is the bene   t of all this shuf   ing and redealing when we could have just run
a t test to compare the two groups? on the theoretical level, this method relies on
the assumption of the bootstrap principle rather than the assumptions underlying
the clt. instead of assuming a theoretical distribution, you can shuf   e and redraw
to produce the distribution of outcome values that matches the true sample data
(under the assumption that   red =   black), and then rely on the bootstrap principle
to say that what you learned from the sample is representative of the population
from which the sample was drawn.

on a practical level, the closer match to the data provides real bene   ts. lehmann
& stein (1949) showed that the permutation test is more powerful   it is less likely
to fail to reject the equality hypothesis when the two means are actually different.

in pseudocode, here is the test procedure:6

6the test is attributed to chung & fraser (1958). from p 733:    the computation took one day for programming

and two minutes of machine time.   

gsl_stats march 24, 2009

376

chapter 11

       |xred     xblack|
d     the joined test and control vectors.
allocate a million-element vector v.
for i = 1 to a million:

draw (without replacement) from d a vector the same size as xred, l.
put the other elements of d into a second vector, r.
vi     |l     r|.

       the percentage of vi less than   .
reject the claim that    = 0 with con   dence   .
fail to reject the claim that    = 0 with con   dence 1       .

baum et al. (2008) sampled genetic material from a large number of cases
with bipolar disorder and controls who were con   rmed to not have bipo-
lar disorder. to save taxpayer money, they pooled the samples to form the

groups listed in the    leda a	ge e  in the code supplement. each line of
difference in marker frequency for a given snp, and    nally write a ai 

that    le lists the percent of the pool where the given snp (single nucleotide
polymorphism) has a given marker.
for which of the snps can we reject the hypothesis of no difference between
the cases and controls? write a program to read the data into a database,
then use the above algorithm to test whether we reject the hypothesis of no

q11.6

that runs the test for each snps in the database. the data here is cut from
550,000 snps, so base the bonferroni correction on that many tests.

other nonparametric tests tend to follow a similar theme: given a null hypothesis
that some bins are all equiprobable, we can develop the odds that the observed
data occurred. see conover (1980) for a book-length list of such tests, including
kolmogorov   s method from page 323 of this book.

testing for bimodality as the    nale to the book, listing 11.12 shows the use
of kernel densities to test for multimodality. this in-
volves generating a series of kernel density estimates of the data,    rst with the
original data, and then with a few thousand bootstrap estimates, and then doing a
nonparametric test of the hypothesis that the distribution has fewer than n modes,
for each value of n.

recall the kernel density estimate from page 262, which was based on the form

  f (t, x, h) = pn

i=1 n ((t     xi)/h)

,

n    h

where x is the vector of n data points observed, n (y) is a normal(0, 1) pdf
evaluated at y, and h     r+ is the bandwidth. let k be the number of modes.

gsl_stats march 24, 2009

monte carlo

377

figure 7.15 (p 262) showed that as h rises, the spike around each point spreads
out and merges with other spikes, so k falls, until eventually the entire data set is
subsumed under one single-peaked curve, so k = 1. thus, there is a monotonic
relationship between h and the number of modes in the density function induced
by h.

silverman (1981) offers a bootstrap method of testing a null hypothesis of the form
the distribution has more than k modes. let h0 be the smallest value of h such that
  f (t, x, h0) has more than k modes; then the null hypothesis is that h0 is consistent
with the data.

listing 11.12 shows the code used to produce these    gures and test a data set of
television program ratings for bimodality. since it draws a few hundred bootstrap

generates a histogram: at each point on the x-axis, it piles up the value at that point
of a normal distribution centered at every data point. the result will be a histogram
like those pictured in figure 7.15, page 262, or those that are produced in the

we then draw a few hundred bootstrap samples from the data, x   1, . . . , x   200; write
down   f (t, x   1, h0), . . . ,   f (t, x   200, h0); and count the modes of each of these func-
tions. silverman shows that, thanks to the monotonic relationship between h and
the number of modes, the percentage of bootstrap distributions with more than k
modes matches the likelihood that   f (t, x, h0) is consistent with the data. if we re-
ject this hypothesis, then we would need a lower value of h, and therefore more
modes, to explain the data.

samples, link it with  eb   .
 from page 368.
    the
 	    de  function makes two scans across the range of the data. the    rst
gnuplot animation the program will write to theke  e         le. the second
    h     k: for each indexi,k ab[i    is intended to be the smallest value ofh that
produces fewer thani modes. thefi  _k a  function calls
 	    de  to    nd
the number of modes produced by a range of values ofh. the function runs from
the largesth to the smallest, so the last value ofh written to any slot will be the
the actual data,b    does the id64:  e_b    (p 368) uses a straight-
forwardf   loop to produce a sample, then the same
 	    de  function used
7also, thef   loop in this function demonstrates a pleasant trick for scanning a wide range: instead of stepping

smallest value that produces that mode count.7
    k     p: now that we have the smallest bandwidth h that produces a given k using

above is applied to the arti   cial sample. if the mode count is larger than the num-
ber of modes in the original, it is a success for the hypothesis. the function then

pass checks for modes, by simply asking each point whether it is higher than the
points to its left and right.

by a    xed increment, it steps by percentages, so it quickly scans the hundreds but looks at the lower end of the
range in more detail.

gsl_stats march 24, 2009

378

chapter 11

returns the percent successes, which we can report as a p value for the hypothesis
that the distribution has more than k modes.

    in some parts of the output, such as for k = 12, the battery of tests    nds low
con   dence that there are more than k modes and high con   dence that there are
more than k + 1 modes. if both tests were run with the same value of h, this would
be inconsistent, but we are using the smallest level of smoothing possible in each
case, so the tests for k and k + 1 parameters have little to do with each other. for
small k, the output is monotonic, and shows that we can be very con   dent that the
data set has more than three modes, and reasonably con   dent that it has more than
four.

#include "oneboot.h"

double modect_scale, modect_min, modect_max,

h_min=25, h_max=500, max_k = 20,
boot_iterations = 1000,
pauselength = 0.1;

char out   le[] = "kernelplot";

void plot(apop_data *d, file *f){

fprintf(f, "plot           with lines\n");
apop_data_print(d, null);
fprintf(f, "e\npause %g\n", pauselength);

}

int countmodes(gsl_vector *data, double h, file *plothere){

int len =(modect_max   modect_min)/modect_scale;
apop_data *ddd = apop_data_calloc(0, len, 2);
double sum, i=modect_min;

for (size_t j=0; j < ddd   >matrix   >size1; j ++){

sum = 0;
for (size_t k = 0; k< data   >size; k++)

sum += gsl_ran_gaussian_pdf((i   gsl_vector_get(data,k))/h,1)/(data   >size*h);

apop_data_set(ddd, j, 0, i);
apop_data_set(ddd, j, 1, sum);
i+=modect_scale;

}
int modect =0;
for (i = 1; i< len   1; i++)

if(apop_data_get(ddd,i,1)>=apop_data_get(ddd,i   1,1)

&& apop_data_get(ddd,i,1)>apop_data_get(ddd,i+1,1))

modect++;

if (plothere) plot(ddd, plothere);
apop_data_free(ddd);
return modect;

}

void    ll_kmap(gsl_vector *data, file *f, double *ktab){

for (double h = h_max; h> h_min; h*=0.99){

gsl_stats march 24, 2009

monte carlo

379

int val = countmodes(data, h, f);
if (val <max_k)

ktab[val     1] = h;

}

}

double boot(gsl_vector *data, double h0, int modect_target, gsl_rng *r){

double over_ct = 0;
gsl_vector *boots = gsl_vector_alloc(data   >size);

for (int i=0; i < boot_iterations; i++){

one_boot(data, r, boots);
if (countmodes(boots, h0, null) > modect_target)

over_ct++;

}
gsl_vector_free(boots);
return over_ct/boot_iterations;

}

apop_data *produce_p_table(gsl_vector *data, double *ktab, gsl_rng *r){

apop_data *ptab = apop_data_alloc(0, max_k, 2);

apop_name_add(ptab   >names, "mode",    c   );
apop_name_add(ptab   >names, "likelihood of more",    c   );
for (int i=0; i< max_k; i++){

apop_data_set(ptab, i, 0, i);
apop_data_set(ptab, i, 1, boot(data, ktab[i], i, r));

}
return ptab;

}

void setvars(gsl_vector *data){ //rescale based on the data.

double m1 = gsl_vector_max(data);
double m2 = gsl_vector_min(data);
modect_scale = (m1   m2)/200;
modect_min = m2   (m1   m2)/100;
modect_max = m1+(m1   m2)/100;

}

int main(){

apop_col(apop_text_to_data("data   tv", 0,0), 0, data);
setvars(data);
file *f = fopen(out   le, "w");
apop_opts.output_type =    p   ;
apop_opts.output_pipe = f;
double *ktab = calloc(max_k, sizeof(double));
   ll_kmap(data, f, ktab);
fclose(f);
gsl_rng *r = apop_rng_alloc(3);
apop_data_show(produce_p_table(data, ktab, r));

listing 11.12 silverman   s kernel density test for bimodality. online source:bi  da i y.

}

380

gsl_stats march 24, 2009

another way to speed the process inbi  da i y.
 is to clump the data before
given weight (you can use theweigh  element of thea   _da a struc-
ture to record it). rewrite the
 	    de  function to use the clumped and

summing the normal distributions. if there are three points at    .1, 0, and .1, then
this will require three calculations of the normal pdf for every point along the real
line. if they are clumped to 0, then we can calculate the normal pdf for    = 0
times three, which will run three times as fast.

write a function to group nearby data points into a single point with a

chapter 11

q11.7

weighted data set. how much clumping do you need to do before the results
degrade signi   cantly?

   via resampling, you can test certain hypotheses without assuming

parametric forms like the t distribution.

   the typical kernel density estimate consists of specifying a distribu-
tion for every point in the data set, and then combining them to form
a global distribution. the resulting distribution is in many ways much
more faithful to the data.

   as the bandwidth of the sub-distributions grows, the overall distribu-

tion becomes smoother.

   one can test hypotheses about multimodality using kernel densities,
by    nding the smallest level of smoothing necessary to achieve n-
modality, and id64 to see the odds that that level of smooth-
ing would produce n-modality.

gsl_stats march 24, 2009

a: environments and makefiles

since c is a standard, not a product from a single corporation or foundation, differ-
ent c development environments have minor, potentially maddening differences.

the problem of whether a decent compiler and useful libraries are available on
a given system has basically been surmounted: if something is missing, just ask
the package manager to install it from the internet. but then the next question is:
where did the package manager put everything? some systems like to put libraries

in/	  /  
a , some like to put them in/   /, and neither of these locations
if you typee v at the command prompt,

the solution is to set environment variables that will specify how your compiler
will    nd the various elements it needs to produce your program. these variables
are maintained by the operating system, as speci   ed by the posix standard.1

even makes sense for the ever-eccentric windows operating system.

a.1 environment variables

ables set on your system. here is a sampling from my own prompt:

you will get a list of the environment vari-

shell=/bin/bash
user=klemens

1later members of the windows family of operating systems claim posix compliance, meaning that most
of this works from the windows command prompt, although this may require downloading microsoft   s interix
package. however, the cygwin system has its own prompt which basically keeps its own set of environment
variables, and i will assume you are using cygwin   s shell rather than that of windows.

gsl_stats march 24, 2009

382

appendix a

setting now that you know which shell you are using, the syntax for setting envi-

export user=stephen

ronment variables differs slightly from shell to shell.

ld_library_path=/usr/local/lib:
path=/home/klemens/tech/bin:/usr/local/bin:/usr/bin:/bin:/usr/games:/sbin
home=/home/klemens

every time a program spawns a new subprogram, the environment variables are
duplicated and passed to the child program. notably, when you start a program
from the shell prompt, all of the environment variables you saw when you typed

e v are passed on to the program.
thee v command works well withg e  (see page 404). for example, to    nd out
which shell you are using, trye v|g e s e  .
you are probably usingba h or another variant of the bourne shell.2 in these
systems, set environment variables using the shell   sex     command:
this form clari   es that the process of setting an environment veriable inba h con-
to the c programming language. in
 h, use e e v:
2ba h=bourne-again shell, since it is an overhaul of the shell stephen bourne wrote for unix in 1977.

sists of    rst setting a local variable (which is not passed on to child programs) and
then moving it from the set of local variables to the set of environment variables.
if you do not do much shell programming, there is no loss in setting all variables
in the environment.

some shells are picky about spacing, and will complain if there is a space before or
after the equals sign. others do not even accept this syntax, and require a two-line
version:

the other family of shells is the c shell, which bears a vague, passing resemblance

user=stephen
export user

setenv user stephen

gsl_stats march 24, 2009

getting

setting for good

383

echo $user

example,

environments and makefiles

export path=$path:$home/bin

path=/home/klemens/tech/bin:/usr/local/bin:/usr/bin:/bin:/usr/games:/sbin:/home/klemens/bin

in all shells, and many shell-like programs, you can get the value of a vari-
able by putting a dollar sign before the variable name. for example,

directory at the end. for my own already lengthy path, listed above, this command
would result in the following new path:

will print the current value of theuser variable to the screen. to give a more useful
will extend the at , replacing that variable with a copy of itself plus another
   les; see your shell   s manual page ( a ba h, a 
 h, . . . ) for
with a dot and ends in 
, such as.ba h 
 or.
 h 
.
the usual directory listing such as   will not show these    les. however, if you
explicitly ask for hidden    les, via  	a,  	d. , or your gui    le browser   s
show hidden option, you will certainly    nd a number of. 
    les in your home
the shell   s. 
    les are plain text, and generally look like the sort of thing you
ex     or e e v command in this    le, then those commands will be read ev-
ther exit and re-enter the shell, or use  	 
e.ba h 
 or  	 
e.
 h 
 to

ery time your shell starts, and the variables will thus be set appropriately for all of
your work.

details. but most all of them read a    le in your home directory whose name begins

would type on a command line   because that is what they are. if you put an

it is a posix custom that if a    le begins with a dot, then it is hidden, meaning that

every time your shell starts, it reads a number of con   guration

directory.3

explicitly ask the shell to read the    le.

3by the way, when making backups of your home directory, it is worth verifying that hidden    les are also

backed up.

    the    le does not auto-execute after you edit. to see the effects of your edit, ei-

384

appendix a

gsl_stats march 24, 2009

ables in a posix system because it is easy to

should be overwritten. there are a few other useful environment-handling func-
tions; see your standard c library reference for details.

  getting and setting from c it is easy to read and write environment vari-
read and write them in c. listing a.1 shows that thege e v function will return
a string holding the value of the given environment variable, and e e v will set
the given environment variable (on line 6,user) to the given value (   stephen   ).
the third argument to e e v speci   es whether an existing environment variable
listing a.1 getting and setting environment variables. online source:e v.
.
make e e v useless: if your program starts other programs via a function like
 y  e  or   e , then they will inherit the current set of environment variables.

why is it safe to run the program in listing a.1, which will overwrite an important
environment variable? because a child process (your program) can only change its
own copy of the environment variables. it can not overwrite the variables in the
environment of a parent process (the shell). overriding this basic security precau-
tion requires a great deal of cooperation from the parent process. but this does not

printf("you are: %s\n", getenv("user"));
setenv("user", "stephen", 1);
printf("but now you are: %s\n", getenv("user"));

#include <stdlib.h> //environment handling
#include <stdio.h>

1
2
3
4
5
6
7
8

int main(){

}

when the shell opened your program, it passed in its environment in exactly this
manner.

   the operating system maintains a set of variables describing the cur-
rent overall environment, such as the user   s name or the current work-
ing directory.

   these environment variables are passed to all child programs.

   they can be set or read on the command line or from within a pro-

gram.

a.2 paths

385

gsl_stats march 24, 2009

environments and makefiles

on to the problem of getting your c compiler to    nd your libraries.

hard drive and loads it into memory. but given that an executable could be any-

tories separated by colons (except on non-cygwin windows systems, where they
are separated by semicolons). you saw a number of examples of such a variable

when you type a command, the shell checks the    rst directory on the path for the
program you requested. if it    nds an executable with the right name, then the shell
runs that program; else it moves on to the next element of the path and tries again.
if it makes its way through the entire path without    nding the program, then it
gives up and sends back the familiar command not found error message.

when you type a command, such asg

, the shell locates that program on your
where (maybe/bi ,/   , or
:\   g a fi e ), how does the shell know
where to look? it uses an environment variable named at  that lists a set of direc-
above, and can rune v|g e  at  to check your own path.
the current directory,./, is typically not in the path, so programs you can see
with   will not be found by searching the path.4 the solution is to either give an
explicit path, like./ 	 _ e, or extend the current directory to your path:
recall that by adding this line to your.ba h 
 or.
 h 
, the path will be ex-
path: when you#i 
 	de a    le (e.g., lines 1 and 2 of listing a.1), the prepro-
user could put a script named   in a common directory, where the script contains the command  	 f    e.
protection, but the malicious script could instead be named after a common typo, such as
 or  . but given that
many of today   s posix computers are used by one person or a few close associates, adding./ to the path is not
5java users will notice a close resemblance between these paths and thec ass at  environment variable,

cessor steps through each directory listed in the include path checking for the re-
quested header    le. it uses the    rst    le that matches the given name, or gives up
with a header not found error. when the linker needs to get a function or structure
from a library, it searches the library path in the same manner.

but unlike the executable path, there are several ways for a directory to appear on
the include or library path.

there are two paths that are relevant to compilation of a c program: the include
path and the library path.5 the procedure is the same as with the shell   s executable

4many consider putting the current directory in the path to be a security risk (e.g., frisch (1995, p 226)); in
security lingo, it allows a current directory attack. if the current directory is at the head of the path, a malicious

when you switch to that location and try to get a directory listing, the shell instead    nds the malicious script,
and your home directory is erased. putting the current directory at the end of the path provides slightly more

tended every time you log in.

export path=$path:./

a real risk in most situations.

which is effectively an include and a library path in one.

386

appendix a

gsl_stats march 24, 2009

would add that directory to the library path.

would add that directory to the include path, and the    ag

    you can add to the paths on the compiler   s command line. the    ag

    there is a default include path and a default library path, built in to the compiler

also, some libraries are shared, which in this context means that they are linked
not during compilation, but when actually called in execution. thus, when you
execute your newly-complied program, you could still get a missing library error;
to    x this you would need to add the directory to your libpath environment variable.

and linker. this typically includes/	  /i 
 	de and/	  /  
a /i 
 	de in
the include path, and/	  / ib and/	  /  
a / ib in the libpath.
    the environment variables  c ude at  and  b at  are part of the path. these
names are sometimes different, depending on the linker used:g

 uses d, so it
looks for d_  brary_ at    except on mac systems, where it looks fordy d_	
  brary_ at . on hp-ux systems, trys   b_ at . or more generally, try a 
 d or a 

 to    nd the right environment variable for your system.
	 /	  /  
a /i 
 	de
	 /	  /  
a / ib
so, now that you know the syntax of adding a directory to your at s,
the header for the tla library, which will be a    le with a name like  a.h. the
commandfi ddi 	 a e  a.h will searchdi  and every subdirectory ofdi 
directory, viafi d/	 a e  a.h, typically takes a human-noticeable amount
of time, so try some of the typical candidates fordi     rst:/	  ,/   ,/  
a , or
for mac os x,/ w.
search for all    les beginning with a given string, e.g.,fi ddi 	 a e   a.  .
list the non-standard directories to search, using the capital	     ag, then you will
need another lower-case	     ag for each library to be called in.
companion cblas. if your program already has an object    le named  	 
e. 

order matters when specifying libraries. begin with the most-dependent library or
object    le, and continue to the least dependent. the apophenia library depends on
sqlite3 and the gsl library   which depends on a blas library like the gsl   s

there is nothing in your source code
that tells the system which libraries are
to be linked in, so the libraries must be listed on the command line. first, you will

for a    le with the given name. searching the entire hierarchy beginning at the root

compiled libraries have different names on different systems, so your best bet is to

which directory should you add? say that you know you need to add

assembling the compiler command line

searching

gsl_stats march 24, 2009

and you want the output program to be named 	 _ e, then you will need to

environments and makefiles

387

specify something like

gcc source.o    lapophenia    lgsl    lcblas    lsqlite3
or
gcc source.o    lapophenia    lsqlite3    lgsl    lcblas

the sqlite3 library and gsl/cblas are mutually independent, so either may
come    rst. the rest of the ordering must be as above for the compilation to work.6
you may also need to specify the location of some of these libraries; the path ex-
tension will have to come    rst, so the command now looks like

gcc    l/usr/local/lib source.o    lapophenia    lgsl    lcblas    lsqlite3

this is an immense amount of typing, but fortunately, the next section offers a
program designed to save you from such labor.

   many systems, including the shell, the preprocessor, and the linker,

search a speci   ed directory path for requested items.

   the default path for the preprocessor and linker can be extended via
both environment variables and instructions on the compiler   s com-
mand line.

the ake program provides a convenient system for you to specify
your system, and then ake assembles the elaborate command line required to
invoke the compiler. many programs, such asgdb,vi, ande acs, will even let
you run ake without leaving the program. because assembling the command line

   ags for warnings and debugging symbols, include paths, libraries
to link to, and who knows what other details. you write a make   le that describes
what source    les are needed for compilation and the    ags the compiler needs on

a.3 make

by hand is so time consuming and error-prone, you are strongly encouraged to
have a make   le for each program.

fortunately, once you have worked out the    ags and paths to compile one program,
the make   le will probably work for every program on your system. so the effort
of writing a make   le pays off as you reuse copies for every new program.

the discussion below is based on listing a.2, a make   le for a program named

6with some linkers, order does not matter. if this is the case on your system, consider yourself lucky, but try

to preserve the ordering anyway to ease any potential transition to another system.

gsl_stats march 24, 2009

 	 _ e, which has two.
    les and one header    le. it is not very far removed

appendix a

388

from the sample make   le in the online code supplement.

gcc $(cflags) $(objects) $(linkflags)    o$(progname)

progname = run_me

executable: $(objects)

1 objects =    le1.o    le2.o
2
3 cflags =    g    wall    werror    std=gnu99
4 linkflags =    l/usr/local/lib    lgsl    lgslcblas    lsqlite
5 compile = gcc $(cflags)    c $<    o $@
6
7
8
9
10
11
12
13
14
15
16
17

./$(progname) $(pargs)

   le1.o:    le1.c my_headers.h

$(compile)

$(compile)

run: executable

   le2.o:    le2.c my_headers.h

following dependencies.

listing a.2 a sample make   le for a program with two source    les.

variable expansion variable names in a make   le look much like environment vari-
ables, and behave much like them as well. they are slightly

the ake program does two types of operation: expanding variable names and
easier to set   as on lines 1   5, just usevar=va 	e. when referencing them,
use a dollar sign and parens, so line    ve will replace  cf ags  with	g	wa  
	we    	  d=g 	99. all of the environment variables are also available this
the   and < variables on line    ve are special variables that indicate the target

as with environment variables, variables in make   les are customarily in all caps,
and as with constants in any program, it is good form to group them at the top of
the    le for ease of reference and adjustment.

way, so your make   le can include de   nitions like:

includepath = $(includepath):$(home)/include

and the    le used to build it, respectively, which brings us to the discussion of
dependency handling.

gsl_stats march 24, 2009

environments and makefiles

389

dependencies

fail.7

that span several lines.

having described the format of the make   le, let us go over the process that the

an important annoyance: the lines describing the building process are indented by

the remainder of the    le is a series of target/dependency pairs and
actions. a target is something to be produced. here the targets are
object    les and an executable, and on page 188, there is a make   le whose targets
are pdfs and graphics. the dependency is the source containing the data used
to produce the target. an object    le depends on the c source code de   ning the
functions and structures, an executable depends on the object    les being linked
together, and a pdf document depends on the underlying text. in most of these
cases, the dependency is data that you yourself wrote, while the target is always
computer generated.

after each target line, there are instructions for building the target from the de-
pendencies. for a simple c program, the instructions are one line (typically a call

the lines with colons, such as line ten, are target/dependency descriptions: the
single item before the colon is the target name, and the one or several items after
the colon are the    les or other targets upon which the target depends.

tog

). the make   le on page 188 shows some more involved target build scripts
tabs, not spaces. if your text editor replaces tabs with a set of spaces, ake will
system will go through when you type ake on the command line. the    rst target
in a make   le is the default target, and is where ake begins if you do not specify a
target on the command line. so ake looks at the target speci   cation on line seven
and, using its powers of variable expansion, sees that 	 _ e depends onfi e1. 
andfi e2. . let us assume that this is the    rst time the program is being built,
so neither object    le exists. then ake will search for a rule to build them, and
  c     e  on lines 11 and 14. having created the subsidiary targets, it then
let us say that you then modifyfi e1.
 (but none of the other    les) and then
call ake again. the program again starts at the top of the tree of targets, and sees
that it needs to check onfi e1.  andfi e2. . checking the dependencies for
fi e1. , it sees that the    le is not up-to-date, because its dependency has a newer
time stamp.8 sofi e1.  is recompiled, and 	 _ e then recreated. the system
knows thatfi e2.  does not need to be recompiled, and so it does not bother to
seconds or even minutes, ake will thus save you time as well as typing.
7the odds are good that<
   	v> will let you insert an odd character into your text. e.g.,<
   	v>< ab>
time zone or was otherwise mis-stamped? the  	
h command (e.g.,  	
h .
) will update the time stamp on
a    le to the current time. if you want make to recompilefi e1. , you can do so with  	
hfi e1.
; ake.

returns to the original target, and runs the command to link together the two object
   les.

thus digresses to the later targets. there, it executes the command speci   ed by

do so. in a larger project, where recompilation of the whole project can take several

8what should you do if the time stamp on a    le is broken, because it was copied from a computer in a different

will insert a tab without converting it to spaces.

gsl_stats march 24, 2009

390

appendix a

compile && run almost without fail, the next step after a successful compile is to
run the program. the make   le above gives you the option of do-

program. it considers a compilation with warnings but no errors to be successful,

dependency on line 7. you can specify targets on the command line; if you simply

ing this via the 	  dependency on lines 16   17, which depends on theexe
	 ab e
type ake, then the system assumes the    rst target in the    le (line 7), and if you
type the command ake 	 , then it will use the instructions under the 	  target.
 ake halts on the    rst error, so if the program fails to compile, the system stops
and lets you make changes; if it compiles correctly, then ake goes on to run the
but you should heed all warnings. the solution is to add	we     to thecf afs
 ake   s command line to that of your program. the make   le here uses the hack of
de   ning an environment variable args. on the command line, you canex    
 args= 	b|g 	    	 e  i   , and ake will run your program with the	b
   ag and pipe the output throughg 	    .
if you    nd the args hack to be too hackish, you can just run the program from
the command line as usual. recall that in c syntax, theb in a  b  is eval-
uated only ifa is true. similarly on the command line: the command ake  
./ 	 _ e will    rst run ake, then either halt if there are errors or continue on to
./ 	 _ e if ake was successful. again, you have one command line that does
command line (try the up-arrow or!!), you have a very fast means of recompiling

both compilation and execution; coupled with most shells    ability to repeat the last

there is no easy way to pass switches and other command-line information from

command line, to tell the compiler to treat warnings as errors.

and executing.

environments and makefiles

gsl_stats march 24, 2009

391

take one of the sample programs you wrote in chapter 2 and create a direc-
tory for it.

    create a new directory, and copy over the.
    le.
    copy over the sample akefi e from the online sample code.
    modify the r g a e and b ects line to correspond to your
you will need to modify the  c ude at  line. if you get errors
the    f ags line.
    type ake and verify that the commands it prints are what they
changing the r g a e or b ects).

now that you have a make   le that works for your machine, you can copy
it for use for all your programs with only minimal changes (probably just

should be, and that you get the object and executable    les you ex-
pected.

    if you get errors that the system can not    nd certain headers, then

from the linker about symbols not found, you will need to change

project.

qa.1

   the make   le summarizes the method of compiling a program on your

system.

   at the head of the make   le, place variables to describe the    ags

needed by your compiler and linker.

   the make   le also speci   es what    les depend on what others, so that

only the    les that need updating are updated.

gsl_stats march 24, 2009

b: text processing

if you are lucky, then your data set is in exactly the format it needs to be in to be
read by your stats package, apophenia, graphviz, et cetera.

the book so far has included a great deal on writing new text    les via the  i  f
ing10 with100 requires rewriting the entire    le from that point on.

by de   nition, you will rarely get lucky, so this appendix will explain how to mod-
ify a text    le to conform to an input standard.

family, but not much on the process of modifying existing    les. that is because
modifying    les in place is, simply put, frustrating. something as simple as replac-

thus, this appendix will look at some means of modifying text via existing pro-
grams that go through the tedium of modifying    les for you. its goal, narrowly
de   ned by the rest of the text, is to show you how to convert a data    le either into
the input format required by a stats package or into a command    le in the language
of gnuplot, a graphviz-family tool, or sql.

as with the rest of the analysis, you should be able to execute these procedures
in a fully automated manner. this is because you will no doubt receive a revised
data set next week with either corrections or more data, and because writing down
a script produces an audit trail that you or your colleagues can use to check your
work.

the primary technique covered in this appendix is the parsing of regular expres-
sions, wherein you specify a certain regularity, such as numbers followed by letters,

gsl_stats march 24, 2009

text processing

and a program searches for that regular expression in text and modi   es it as per
your instructions.

location to another, burbling out of data    les and piping through    lters like ed
get large, you will desire tools that    nd every use of, say,b	ggy_va iab e in your

(the stream editor). eventually, the stream of data reaches its    nal destination: a
graphic, a database, or another    le holding clean,    ltered data.

along the way, you will see how to effectively search your    les. as your projects

a secondary theme is the posix water metaphor. data    ows in a stream from one

393

program or every reference to an author in your documents. regular expression
tools will help you with this.

after the discussion of how one assembles a pipeline from small parts, this ap-
pendix will cover simple searching, as an introduction to id157. the
   nal section applies these tools to produce several scripts to reformat text    les into
a format appropriate for your data-handling systems.

   le, and    nally call gnuplot to display a graph.

as with gnuplot or sqlite   s command-line utility, you
can operate the shell (what many just call the command
line) by typing in commands or by writing a script and having the shell read the
script. thus, you can try the commands below on the command line, and when
they work as desired, cut and paste them into a script as a permanent record.

your script can do more than just reformat text    les: it can begin with some e  
or ed commands to reformat the data    le, then call your c program to process the
the quickest way to make a script executable is to use the  	 
e command.
given a list of commands in the text    le y 
 i  , you can execute them using
  	 
e y 
 i  .1
write a short script to compile and executehe   _w   d.
. on the    rst
page 18), and on the second line, execute./a. 	 . then run your script
is not the place for a full posix tutorial, but
h  d755 y 
 i   or
h  d x y 
 i   will do the trick.
having made the script executable, you can call a script in the current directory (aka./) from the shell using
./ y 
 i  . [why do you need the./? see the discussion of paths in appendix a.]

line of the script, give the compilation command (as per the exercise on

1if you are going to run the script more than a few times, you may want to make it directly executable. this

from the command line.

b.1 shell scripts

qb.1

gsl_stats march 24, 2009

394

(use

appendix b

the reader

exercise for

c equivalent

send output to a    le

read output from a    le

append output to a    le

shell symbol meaning

table b.1 redirection via the command prompt and c

take input from an in-
line script

redirect output to an-
other program

f  e  "fi e a e" "w" ;
>
f  i  f...;
>>
f  e  "fi e a e" "a" ;
f  i  f...;
<
f  e  "fi e a e" " " ;
fge  ...;
   e  "   g a e" "w" ;
|
f  i  f...;
   e  "   g a e" " " ;
fge  ...;
<<
fge  )
stream, named  di  and  d 	 , plus a third stream typically used
for error messages,  de  . this section will cover the various means of bending
by default  di  and  d 	  are the keyboard and screen, respectively. for ex-
ample, ed reads from  di  and writes to  d 	 , so the command
simply rints to the screen whatever is typed in. if you try it, you will see that this
means simply that ed repeats whatever you type.
clause of the form>fi e a e tells the system to write whatever it would have

and redirecting these streams, and the next section will cover a few tools you join
together to    lter streams of text.

shell prompt> sed    n p
hello.
hello.
how are you?
how are you?
stop imitating me.
stop imitating me.
<ctrl   d>

now for the shifting of the streams, via the shell symbols listed in table b.1. a

sed    n p

redirection each program has a standard input stream and a standard output

qb.2

395

the screen.

text processing

shell expansion

gsl_stats march 24, 2009

your    nal redirection option is the

on the command line, then nothing will print to the screen, but whatever you type

put on the screen to a    le. thus, if you put the command ed	  > 	 fi e
(until you hit<
   	d> to exit) will be written to 	 fi e.
as a variant,>> 	 fi e will append to a    le, rather than overwriting it as>
 	 fi e would.
a< tells the system that it should take input not from the keyboard but from a
   le. thus, ed	  <i  	 _fi e will dump the contents ofi  	 _fi e to
use ed	   and redirection of both input and output to copy a    le.
pipe. the pipe connects  d 	  for
one program to  di  for another.
ferently. the single-tick form does not expand var
to the value of thevar environment variable, and does
not treat \ or! as special. conversely, the    double-
   nd a program,ge    , that prints
exponents to  d 	 . below, you
will see thatg e  will search its
is basically impossible to put a! inside double-ticks;
you will need a form like"  a     i g(cid:21)	 a	 e
ample,g e  6  < yfi e will
   i g"\!"(cid:21)	a d
   i 	e."
search yfi e for lines ending in a
six. thus, we could redirect the output forge     to a    le, then input that    le to
g e .
the pipe,|, streamlines this by directly linking  d 	  from the    rst program and
  di  to the second, bypassing the need for a    le in between the two commands.
this line prints the    ltered output ofge     to the screen, at which point the
now that you have seen>,>>, and<, you may be wondering about<<. appending

tick    form makes all of these changes before the
called program sees the string. single-ticks tend to
be easier to use for id157, because of
the preponderance of special characters. notably, it

question: what exponents of four
end in a six? on page 209 (and
in the code supplement), you will

posix-compliant shells use both    single-ticks    and
   double-ticks    to delimit strings, but they behave dif-

pattern in the exponents is eminently clear.

./getopt 4 > powers_of_four
grep    6$    < powers_of_four

input for a certain pattern; for ex-

./getopt 4 | grep    6$   

to input does not quite make sense; instead, this form allows you to put small

gsl_stats march 24, 2009

396

appendix b

intended to print error messages or diagonstics to the screen, even when

gnuplot << end   of   script
set term postscript
set out "redirect_test.ps"
plot sin(x)
end   of   script

scripts that would normally be in a separate    le directly on the command line.
here is a sample usage:

such as when your program is producing so many errors that they scroll faster than
you can read them.

cates when the script will end. everything from the    rst line until the concluding
string (on its own line) is sent to gnuplot as if it were typed in directly.

thee d	 f	 
 i   marker can be any string (e f is a popular choice), and indi-
   de  
there is one more stream after  di  and  d 	 :  de  , which is
  d 	  is dumping data to a    le. there are reasons to redirect this stream as well,
    in
 h, use>  anywhere you would use> to redirect both  d 	  and  de   to
the given destination, e.g., ake> e     . x .
    inba h, write  de   to a    le is to use2> as you would> above. for example,
 ake2>e     . x  will dump all compilation errors to thee     . x     le.
  streams and c c and unix co-evolved, so all of the above methods of redi-
as you saw on pages 166ff, you can usef  i  f to send data to a    le or another
if this code were in a program namedwa e , then thef  e  andf  i  f instruc-
tions are analogous towa e >w i e_  _ e. the   e  andf  i  f pair are
analogous towa e |g 	    .

file *f = fopen("write_to_me", "w");
file *g = popen("gnuplot", "w");
fprintf(f, "the waves roll out.\n");
fprintf(g, "set label    the waves roll out.   ;set yrange [   1:1]; plot    1");
fclose(f);
pclose(g);

program. you may have noticed the close similarity between opening/writing to a
   le and opening/writing to a pipe:

recting inputs and outputs have an easy c implementation.

qb.3

397

text processing

gsl_stats march 24, 2009

this book has spent little time on the subject of reading data from  di  (look
upf 
a f in your favorite c reference), but by changing the"w"s to" "s in the
above code, the    ow of data is from thef  e  to the program itself, analogous to
wa e <w i e_  _ e andg 	    |wa e .
modifyge    .
 to    lter its output throughg e .
    popeng e  6   for writing.
    modify the  i  f statement to anf  i  f statement.
you would any other   e ed stream. that is,f  i  f   d 	  "da af   h	

  i g:\ "  andf  i  f   de   "da ge !\ "  will write to  d 	  and
  de   as expected. what is the difference betweenf  i  f   d 	  "da a" 
and  i  f "da a" ? there is none; use whichever form is most convenient for
finally, there is the y  e  function, which simply runs its argument as if it were
namedf   	 e, that prints fortune-cookie type sayings. most of us can   t eat just
mates the process via af   loop. the program does not use  i  f and does not
#i 
 	de<  di .h>, because the c program itself is not printing anything   the

the three standard pipes are de   ned by c   s standard library, and can be used as

one, but consume a dozen or so at a time. listing b.2 shows a program that auto-

run from the command line. most posix machines come with a small program

your situation.

child programs are.2

}

}

int main(){

#include <stdlib.h>

for (int i=0; i< 12; i++){

system("fortune");
system("echo");
system("sleep 6");

listing b.2 print fortunes. online source:f   	 a e.
.

there are two differences between a y  e  command and a   e  command.
 y  e    s input and output, while   e  allows input or output from within the
2this program is longer than it needs to be, because all three y  e  lines could be summarized to one:
 y  e  "f   	 e;e
h ;  ee 6" ;. notice also that thea   _ y  e  convenience function allows you
to give a command with printf-style placeholders; e.g.,a   _ y  e  "
     " f   _fi e   _fi e .

first, there is the fact that there is no way for the rest of the program to access

398

append.

appendix b

the keyboard and screen.

gsl_stats march 24, 2009

   all programs have standard inputs and outputs, which are by default

and if the command is something that should run in the background or should wait

program. but also, y  e  will stop your program and wait for the child program
to    nish, while   e  will open the pipe and then move on to the next step in your
program. so if the command has to    nish before you can continue, use y  e ,
for data to become available, use   e .
   both  di  and  d 	  can be redirected.  di  can read from a
   le using the<i fi e form;  d 	  can write to a    le using the
> 	 fi e form. use> 	 fi e to overwrite and>> 	 fi e to
   programs can send their output to other programs using the pipe,|.
   you can do redirection from inside c, usingf  e  to effect the
command-line   s<i fi e and> 	 fi e, and   e  to open a
are summarized in table b.3. all but a few (
  	  ,eg e , e  ) are posix-
be portable. the basic    le handling functions ( kdi ,  ,
 , . . . ) are listed for
the book: you could use
	  to pull one column of data, or you could read the data
into a database and e e
  the column; you could usew
 to count data points, or
use e e
 
 	      to do the same.3 generally, command-line work is good
3 there is even aj i  command, which will do database-style joins of two    les. however, it is hard to use
for any but clean numeric codes. for example, try usingj i  to mergeda a	wb	    andda a	wb	gd .
4a   _ 	e y_  _d 	b e " e e
 
 	     f   da a" 	a   _ 	e y_  _d 	b e " e e
 

 	     f     e e
 di  i 
  f   da a " 

depending on your system, you can type somewhere
between a few hundred and a few thousand commands
at the command prompt. this section points out a few
that are especially useful for the purposes of dealing with data in text format; they

for pure text manipulation and quick questions (e.g., do i have the thousands of
data points i was expecting?), while you will need to write a program or query for
any sort of numeric analysis or to answer more detailed questions about the data
(e.g., how many rows are duplicates?).4

reference, but are not discussed here. if you are not familiar with them, you are
encouraged to refer to any of an abundance of basic unix and posix tutorials
online or in print (e.g., peek et al. (2002)).

most of these commands are in some ways redundant with methods elsewhere in

standard, and you can expect that scripts based on the standard commands will

scripting

pipe.

b.2 some tools for

gsl_stats march 24, 2009

text processing

399

change to a new current directory
list directory contents
copy or move/rename a    le
remove a    le

basic    le handling

 kdi /  di  make or remove a directory

d
  

 / v
  

a 
head/ ai 
 e  /   e

  	  
    

	 / a  e
  
	 i 
 a 
w
diff
 ed
g e /eg e 
 e  

reading
list a    le, or concatenate multiple    les
list only the    rst/last few lines of a    le
interactively browse through a    le
display a    le with its columns aligned

writing
sort
modify    les by columns rather than lines
put a line number at the head of each line
delete duplicate lines

information
manual pages for commands
count words, lines, or characters
find differences between two    les

id157
stream editor: add/delete lines, search/replace
search a    le for a string
all of the above; used here for search/replace

table b.3 some commands useful for handling text    les.

400

appendix b

gsl_stats march 24, 2009

of a program and its various switches. they are not a very good way to learn to use
a posix system, but are the de   nitive reference for command-line details.

 a  what does the755 in
h  d755 do? ask a 
h  d. what did thee
h  com-
mand in listing b.2 do? try a e
h . the manual pages list the basic function

a  the simplest thing you could do with a text    le is print it, and this is what

a  does. instead of opening a    le in your text editor, you can simply type
a 
fi e_  _ ead on the command prompt for a quick look.
the other use of
a , for which it was named, is concatenation. given two    lesa
andb,
a ab>
 writesa and, immediately thereafter,b, into
.
 e  /   e these are the paging programs, useful for viewing longer    les. the posix
standard,   e, is so named because for    les of more than a screenful of
   e. the successor, e  , provides many more features (such as paging up as
well as down). these programs can also read from  di , so you can try combi-
nations like    da a_fi e| e  .
head/ ai 
head yfi e will print the    rst ten lines of yfi e, while ai  yfi e
in a large    le. also, ai 	f yfi e will follow yfi e,    rst showing the last
 ed the stream editor will be discussed in much more detail below, but for now it
suf   ces to know that it can be used to replicatehead or ai . the command ed
	  1 3  <a_fi e will print the    rst through third lines ofa_fi e. the last
line is indicated by , so ed	  2    <a_fi e will print the second through
     this is as self-descriptive as a command can get. theda a	wb	       le (in the
code supplement) is sorted by population;    da a	wb	    would output
blanks, by the second (or nth) column of data, in reverse order, et cetera. see a 
     for details.

ten lines, and then updating as new lines are added. this can provide reassurance
that a slow-running program is still working.

text, it displays a page of text with a banner at the bottom of the screen reading

the    le in alphabetical order. sort has many useful switches, that will sort ignoring

will print the last ten. these are good for getting a quick idea of what is

last lines of the    le.

401

qb.4

text processing

gsl_stats march 24, 2009

use the command string from the last exercise to sort the population

sortingda a	wb	    like this sorts the two header lines in with all the
countries. write a command that    rst calls ed to delete the headers, then
pipes the output to    .

	 / a  e almost all of the commands here operate on rows;
	  and a  e operate
on columns. specify the delimiter with the	d    ag, and then the    eld(s)
with the	f    ag (where the    rst column is column one, not zero). to get a list of just
countries from the world bank data, try
	 	d"|"	f1da a	wb	   ; if you
would like to see just the population numbers, use
	 	d"|"	f2da a	wb	   .
 a  e puts its second input to the right of its    rst input   it is a vertical version of

a .
and gdp data into two new    les.
	  the second    le down to include
only the column of numbers. then use a  e    ed_       ed_gd >

  bi ed_da a to produce one    le with both types of data. verify that the
w
 this program (short for word count) will count words, lines, and characters. the
default is to print all three, butw
	w,w
	 , andw
	
 will give just one of the
when writing a text document, you could use it for a word count as usual:w
	w
 e    . ex. if you have a data    le, the number of data points is probablyw
	 
da a (if there is a header, you will need to subtract those lines).
this program is especially useful in tandem withg e  (also discussed at length
below). how many lines of theda a	wb	       le have missing data (which the
wb indicates by..)?g e  \.\. da a	wb	    will output those lines where
the string.. appears;w
	  will count the lines of the input; piping them together,
g e  \.\. da a	wb	   |w
	  will count lines with..s.
   this command puts line numbers at the beginning of every line. remember that
sqlite puts a  wid on every row of a table, which is invisible but appears if
explicitly e e
 ed. but if you want to quickly put a counter column in your data

lines of data are aligned correctly (i.e., that the albania column does not list
the gdp of algeria).

three counts.

qb.5

set, this is the way to do it.

402

appendix b

gsl_stats march 24, 2009

name<tab>age
rumpelstiltskin<tab>78
dopey<tab>35

this will read into a spreadsheet perfectly, but when displayed on the screen, the
output will be messy, since rumpelstiltskin   s tab will not be aligned with dopey   s.

[this command is not posix-standard, but is common for gnu- or bsd-
based systems.] text data will be easy to read for either a computer or a
human, but rarely both. say that a data    le is tab-delimited, so each    eld has ex-
actly one tab between columns:


  	  
the
  	   command addresses exactly this problem, by splitting columns and
inserting spaces for on-screen human consumption. for example,
  	  	 "|"
	 da a	wb	    will format theda a	wb	       le into columns, splitting at the
| delimiters. the    rst column will include the header lines, but you already know
how to get ed to pipe headerless data into
  	  .
diff
difff1f2 will print only the lines that differ between two    les. this can
marginally different, anddiff can help to sort them out. by the end of this chap-
ter, that will be the case, so various opportunities to usediff will appear below.
e acs,vi , and some ides have modes that rundiff for you and simultaneously
gram, writing the output to 	 1. then clean up the code as you see    t, and run
the cleaned code, writing to 	 2. if you did no damage, thendiff 	 1 	 2
	 i  this program removes many duplicate lines from a    le.	 i di  y>
 ea 

to give another use, say that you have a running program, but want to clean up
the code a little. save your original to a backup, and then run the backup pro-

quickly be overwhelming for    les that are signi   cantly different, but after a
long day of modifying    les, you may have a few versions of a    le that are only

display the differences between versions of a    le.

should return nothing.

will write a version of the input    le with any successive duplicate lines deleted.

for example, given the input

1
2
3
4
5
6
7

a;
b;

b;
c;
c;
a;

403

text processing

gsl_stats march 24, 2009

b.3 id157

this one seemingly small subject, the most comprehensive being friedl (2002).

the output will omit line    ve but leave the remainder of the    le intact. if order

the remainder of this appendix will cover pro-
grams to parse id157. regular
expressions comprise a language of their own, which provides a compact means
of summarizing a text pattern. knowledge of regexes comes in handy in a wide

does not matter, then    y 	 _fi e|	 i  will ensure that identical lines
are sequential, so	 i  will easily be able to clean them all. alternatively, read the
   le to a database and use e e
 di  i 
  f   y 	 _ ab e.
variety of contexts, including standard viewers like e  , a fair number of online
search tools, and text editors likevi ande acs. entire books have been written on
  standard syntax, lack thereof unfortunately, there are many variants on the
ed, a text editor from back when the only output available to computers was the
line printer. most posix utilities, such as ed org e  orawk, use this basic regex
syntax, available viaeg e  and a switch to many other utilities. new features like
the special meaning of characters like , , , or| evolved after a host of bre
actual , , , or| in the text, while they can be used as special operators when
preceded by a backslash (e.g., \ , \|, . . . ). eres used these characters to indicate
special operations from the start, so  is a special operator, while \  indicates a

this chapter will use grep, sed, and perl because it is trivial to use them for regular
expression parsing and substitution from the command line. other systems, such
as c or python, require a regex compilation step before usage, which is nothing
but an extra step in a script or program, but makes one-line commands dif   cult.

regex syntax, since so many programs process
them, and the author of each program felt the need to make some sort of tweak
to the standard. broadly, the basic regular expression (bre) syntax derives from

there is a barely-modi   ed variant known as extended regular expression (ere)

programs were already in place, so in the bre syntax, these characters match an

syntax.

plain plus sign (and so on for most other characters).

the scripting language perl introduced a somewhat expanded syntax for regular
expressions with signi   cantly more extensions and modi   cations, and many post-
perl programs adopted perl-compatible id157 (pcres).

this chapter covers perl and gnu grep and sed as they exist as of this writing, and
you are encouraged to check the manual pages or online references for the subtle
shifts in backslashes among other regular expression systems.

gsl_stats march 24, 2009

404

appendix b

qb.6

the match; repeat your search with two context lines before and after each

the basic search the simplest search is for a literal string of text. say that you
have a c program, and are searching for every use of a vari-

this is already enough to do a great deal. in olden times, programmers would
keep their phone book in a simple text    le, one name/number per line. when

able,va . you can use the command-line programg e  to search forva  among
your    les.5 the syntax is simple:g e va  .
 will search all    les in the current
directory ending in.
 for the stringva , and print the result on the command line.
they needed jones   s number, they could just rung e    e  y_ h  e_b  k
to    nd it. if you need to know venezuela   s gdp,g e  will    nd it in no time:g e 
ve ez	e ada a	wb	gd .
useg e  to search all of your .c    les for uses of  i  f.
the	c  option (e.g.,g e 	c2) outputs n context lines before and after
  i  f.
for example, the expression[f     will match eitherf or . thus, the expression
[f      i  f will match bothf  i  f or   i  f (but not  i  f), sog e 
[f      i  f .
 will    nd all such uses in your c    les. more bits of bracket ex-
    you can use ranges:[a	z    searches for english capital letters,[a	za	z    searches
for english capital or lowercase letters, and[0	9    matches any digit.
item in the bracketed list. thus,[  f     matches one single character that is notf
or .6
ties. for example,[[:digi :       will search for all numbers,[0	9   . notice that
[:digi :    is the character class for digits, so[[:digi :       is a bracket expres-
are locale-dependent. for example,  is not in[a	z   , but if it is a common letter
in the language your computer speaks, then it will be an element of[:a  ha:   .
5the name comes from an olded command (we   ll meeted later) for global regular expression printing:
g/ e/ . many just take it as an acronym for general regular expression parser.
literal dash that does not indicate a range, put it    rst. to    nd all lines with carats or dashes, tryg e "[	     "
 yfi e.

bracket expressions now we start to add special characters from the regex lan-
guage. an expression in square brackets always represents
a single character, but that single character could be one of a number of choices.

    you can match any character except those in a given list by putting    as the    rst

    there are a few common sets of characters that are named for the posix utili-

sion matching any single digit. see table b.4 for a partial list. these character sets

6if you are searching for the    character itself, just don   t put it    rst in the bracket. conversely, if you need a

pression syntax:

text processing

405

\w

posix

perl
\d

gsl_stats march 24, 2009

english
numeric characters.
alphabetic characters.
uppercase alphabetic characters.
lowercase alphabetic characters.
alphanumeric characters.
space and tab characters.

[:digi :   
[:a  ha:   
[:	  e :   
[:  we :   
[:a  	 :   
[:b a k:   
[:  a
e:   
any white space, including space, tab (\ ),
newline (\ ).
[: 	 
 :   
[:  i  :   
[:g a h:   
as[::   , a named group
[    bracket expression
. any character
a range, e.g.,0	9
	 a plain dash

punctuation characters
printable characters, including white space.
printable characters, excluding white space.

don   t match following chars
plain period

\d, \w, \s not numeric chars, not alphanumeric chars,

chars main regex meaning meaning in brackets

table b.4 some useful regular expression character sets.

   beginning of line

not white space

\s

table b.5 some characters have different meanings inside and outside brackets.

all of these can be combined into one expression. for example,

will match any digit, a minus or plus, or the lettere. or, if you named a variable
    butg e     .
 spits out too many comment lines about   i   , tryg e 
   [  i    .
.

[   +ee[:digit:]]

brackets can be a bit disorienting because there is nothing in the syntax to visually
offset a group from the other elements in the bracket, and no matter how long the
bracketed expression may be, the result still represents exactly one character in the
text. also, as demonstrated by table b.5, the syntax inside brackets has nothing to
do with the syntax outside brackets.

alternatives

406

appendix b

gsl_stats march 24, 2009

recall that apophenia includes various printing functions, such as

id157 are vehemently case-sensitive.g e  e   ea 
h_ e will
not turn up any instances of e  . the    rst option is to use the	i command-line
switch to search without case:g e 	i e   ea 
h_ e. the second option, oc-
casionally useful for more control, is to use a bracket:g e [     e   ea 
h_ e.
a   _da a_  i   anda   _ a  ix_  i  . say that you would
sic regex notation as a|b . notice how this analogizes nicely with c   s a||b 
start to creep in. plaing e  uses bres, and so needs backslashes before the parens
and pipe: \ a\|b\ . when run asg e 	e,g e  uses eres. most systems have
aneg e  command that is equivalent tog e 	e.7
with \.  instead of"\\.".
all that said, here are a few commands that will search for botha   _da a_  i  
anda   _ a  ix_  i  :

also, recall the difference between    single-ticks    and    double-ticks    on the com-
mand line, as discussed in the box on page 395: the single-tick form does not treat
backslashes in a special manner, so for example, you can    nd a single literal period

like to see all such examples. alternatives of the form a or b are written in ba-

this is where the various    avors of regular expression diverge, and the backslashes

form.

grep "apop_\\(data\\|matrix\\)_print" *.c
grep    apop_\(data\|matrix\)_print    *.c
grep    e "apop_(data|matrix)_print" *.c
egrep "apop_(data|matrix)_print" *.c

grep    apop.*_print    *.c

a star can repeat the prior atom. therefore

a few special symbols you sometimes won   t care at all about a certain segment.
a dot matches any character, and you will see below that

will    nd any line that hasa   , followed by anything, eventually followed by
_  i  .
    once again, the symbols. and  mean different things in different contexts. in a
7of   cially,eg e  is obsolete and no longer part of the posix standard.

regex, the dot represents any character, and the star represents repetition; on the

gsl_stats march 24, 2009

    a single space or tab are both characters, meaning that a dot will match them. in

    the caret and dollar sign (   and $) indicate the beginning and end of the line,

407

would be

text processing

of the line.8

grep    ^\w*int    *.c

anything that is not white space. since there are frequently an unknown bunch of

command line (where wildcard matching is referred to as globbing), the dot is just
a dot, and the star represents any character.

respectively.g e "  i  " .
 will only match lines wherei   is right at the
beginning of the line, andg e "{ " .
 will only match open-braces at the end
grep, the atom \w will match any single space or tab, and the atom \w will match
tabs and spaces at the head of code    les, a better way to search fori   declarations
   the quickest way to search on the command line is viag e . the
syntax isg e   eg	 a ex  e  i   fi e _  _ ea 
h.
   without special characters,g e  simply searches for every line that
matches the given text. to    nd every instance offix e in a    le:g e 
 fix e fi e a e.
    the set can include single characters:[     e   matches both
 e   and e   (though you maybe want a case-insensitive
search withg e 	i).
    the set can include ranges:[a	za	z    will match any standard
    the set can exclude elements:[   	s    will    nd any character
   express alternatives using the form a|b .
ter. the ere/pcre form for alternation used byeg e  and e   is
 a|b ; the bre form used byg e  is \ a\|b\ .
   a single dot (.) represents any character including a single space or
tab, and grep understands \w to mean any white space character and
\w to mean any non-white space character.
8it is dif   cult to search across lines. perl has the  option ( //). a more reliable and universal option is to
form like   \   | <i fi e|g e ..., where   translates elements of the    rst set (in this case, the

   different systems have different rules about what is a special charac-

simply remove newlines, turning your input stream into a single long line. the easiest way to do this is via a

   a bracketed set indicates a single character.

except p, q, r, and s.

english letter.

newline) into the elements of the second (the pipe).

408

appendix b

perl and sed syntax

gsl_stats march 24, 2009

you want to change what you    nd, you will need to use a more com-

both programs generally expect that you will use a command    le, but you can also

many modern system utilities use it. if it is not installed, you can install it via your
package manager.

regular expression parsing syntax. if you    nd yourself using regexes frequently,
you may want to get to know how they are handled in a full perl-compatible regex
scripting system like python, ruby, or perl itself. these programs complement c
nicely, because they provide built-in facilities for string handling, which is decid-
edly not c   s strongest suit.

replacing g e  is wonderful for searching, but is a purely read-only program. if
plex program, such as e   or ed (the stream editor). ed will easily handle 95%
of data-   ltering needs, but e   adds many very useful features to the traditional
 ed is certainly installed on your system, since it is part of
the posix standard; e   is almost certainly installed, since
give a command directly on the command line via the	e command.
will run the perl command  i  " e    w   d.\ ".
 ed always operates on an input    le, so we might as well start by emulating grep:9
sed likes its regexes between slashes, and the  is short for print, so the command
/ egex/  means    print every line that matches egex.    sed   s default is to print
every input line, and then the print command will repeat lines that match egex,
which leads for massive redundancy. the	  command sets ed to print only when
the command ( ) should operate only on those lines that match/ egex/.
9as you saw in the ed examples in the section on redirection, the	e    ag is optional using this form, but it

generally, a sed command has up to three parts: the lines to work on, a single letter
indicating the command, and some optional information for the command.

above, the speci   cation of lines to work on was a regular expression, meaning that

asked to, so only matching lines print.

sed    n    e    /regex/p    <    le_to_search

perl    e    print "hello, world.\n"   

for example,

never hurts.

search and
replace syntax

409

insert before.

ten lines.

text processing

sed commands

print.
substitute.

but the    rst ten lines.

gsl_stats march 24, 2009

the box presents the    ve single-letter sed commands dis-

for most relevant work. the third part of the command,
following the single-letter command, will vary depending
on the command. as in the examples above, the third part

    you can replace this regex with a    xed set of lines, such as1 10 to mean the    rst
    in this context,  means the last line of the    le, so11    would print everything
    you can print everything but a given set of lines using an! after the range, so
1 10!  would also print everything but the    rst ten lines.10
cussed here; ed has a few more but this is already enough
 
 
d delete.
is blank for the  command.
i
a append after.
the basic syntax for a search and replace is / e  a
e e/wi h
 e/g. the slashes distinguish the command ( ), the text to search
( e  a
e e), the text for replacement (wi h e), and the modi-
   ers (theg; see box).11
perl   s	  switch is the opposite of
why the/g?
sed   s	  switch. as opposed to sed,
that if you do not give it the	 
the second instance of a match with /.../.../2,
does not match the regex. with	 ,
the third instance with /.../.../3, and all instances
with /.../.../g (global). this is still valid syntax
   le, you will almost always want the/g option.
in this case, ed is doing the right
thing by default, so you do not need the	  switch.
10the! is used by the bash shell for various command history purposes, and as a result has absolutely perverse
behavior in    double-ticks   , so using    single-ticks    is essential here. to print all the lines not beginning with a#,
for example, use ed	  /#/!  <i fi e.
parts of a search and replace. for example, | e  a
e e|wi h e|g works equally well. this is primarily
to escape slashes in the text    /\/	  \/  
a /\/	  /g   while using a different delimiter means slashes are
no longer special, so |/	  /  
a |/	  |g would work    ne. the norm is slashes, so that is what the text will

back when regexes were used for real-time editing
of    les, it made sense to    x only one instance of an
expression on a line. thus, the default was to modify
only the    rst instance, and to specify replacing only

non-matching lines appear as-is and
lines that match have the appropri-
ate substitutions made, as we want.

perl    p    e "s/replace me/with me/g" <   le_to_modify >modi   ed_   le
sed    e "s/replace me/with me/g" <   le_to_modify >modi   ed_   le

perl   s default is to never print unless
the pattern matches, which means

if you have only one    le to scan, you can use these programs as    lters:

11a convenient trick that merits a foontote is that you can use any delimiter, not just a slash, to separate the

useful if the search or replace text has many slashes, because using a slash as a delimiter means you will need

that is occasionally useful, but when    ltering a data

switch, it will pass over any line that

use.

qb.7

qb.8

410

appendix b

gsl_stats march 24, 2009

the pipes in the data, leaving the pipe in the comments unchanged.

since you probably want to search and replace on every line in the    le, there

perl    p    i.bak    e    s/replace me/with me/g       les_to_modify
sed    i.bak    e    s/replace me/with me/g       les_to_modify

say that your database manager does not accept pipe-delimited text,
but wants commas instead. write a script to replace every pipe in the

is nothing preceding the single-letter command . but if you want to replace
 e  a
e e only on lines beginning with#, you can specify the sed command
/#/ / e  a
e e/wi h e/g. in this case, sed is taking in two regular expres-
sions: it    rst searches vertically through the    le for lines that match#. when it
   nds such a line, it searches horizontally through the line for e  a
e e.
da a	
 a          le with a comma. then modify the script to replace only
another alternative is to replace-in-place, using the	i switch:12
with the	i option, perl and sed do not write the substitutions to the screen or the
>    le, but make them directly on the original    le. the.bak extension tells perl and
 ed to make a backup before modi   cation that will have a.bak extension. you
may use any extension that seems nice: e.g.,	i  will produce a backup of a    le
named e   that would be named e   . if you specify no suf   x at all (	i), then
create a    le namedab 	 _ e with one line, reading a a ea   . use
either e   or ed to transform from a teapot to something else, such as a
kettle or a toaster. verify your change usingdiff.
you are welcome to include multiple commands on the line, by the way. in e  ,
separate them with a semicolon, as in c. in ed or e  , you may simply specify
additional	e commands, that will be executed in order   or you can just use a
12the	i switch is not standard in ed, but works on gnu ed, which you probably have. bear this in mind
13if editing a    le in place is so dif   cult, how does ed do it? by writing a new    le and then switching it for

perl    pi.bak    e    s/replace me/with me/g; s/then replace this/with this/g       les_to_modify
perl    pi.bak    e    s/replace me/with me/g       e   s/then replace this/with this/g       les_to_modify
sed    i.bak    e    s/replace me/with me/g       e   s/then replace this/with this/g       les_to_modify

no backup copy will be made before the edits are done. it is up to you to decide
when (if ever) you are suf   ciently con   dent to not make backups.13

if you are concerned about a script   s portability.

pipe.

the original when it is    nished. so if you are    ltering a 1gb    le    in place    with no backup and you do not have
1gb of free space on your hard drive, you will get a disk full error.

gsl_stats march 24, 2009

text processing

411

replacing with modi   cations

modi   ed_version

>modi   ed_version

sed    s/replace me/with me/g    <modify_me | sed    s/then replace this/with this/g    >

lines for replacing an unknown number of monkeys are

perl    p    e    s/replace me/with me/g    < modify_me | perl    p    e   s/then replace this/with this/g   

your copyeditor feels that the sentence as written is imprecise. you forgot the

however, you can put a segment of the search term in parentheses, and then use

parentheses, used above to delimit conditional seg-
ments, can also be used to store sections for later

sed    i~    e    s/\([0   9]*\) monkeys/\1 callimico goeldii/g    monkey_   le
sed    r    i~    e    s/([0   9]*) monkeys/\1 callimico goeldii/g    monkey_   le
perl    p    i~    e    s/([0   9]*) monkeys/$1 callimico goeldii/g    monkey_   le

use. for example, say that your    le readsthe ea e4   key a   ay, but
number of monkeys, so you are tempted to use /[0	9      key /ca  i i
 
g e dii/g   but this will lose the number of monkeys.
\1 in ed or 1 in perl to refer to it in the replacement. thus, the correct command
    the \1 or 1 will be replaced by whatever was found in the parentheses [0	9     .
    ed normally uses bres, but the gnu version of ed uses eres given the	 
say that we would like to formalize the sentence he7   key a efigh i g
 he4 ab w e  . do this using multiple parens, such as
with" a ". the search /[0	9   /" a "/g won   t work, because45
will be replaced by" a "" a ", since both4 and5 count as separate matches. or
say that values over ten are suspect. the search /[1	9   [0	9   /" a "/g won   t
work, because100 would be replaced by" a "0. we thus need a means of speci-

if there are multiple substitutions to be made, you will need higher numbers.

sed    i~    e    s/\([0   9]*\) monkeys are    ghting the \([0   9]*\) lab owners/\1 callimico goeldii

say that values under ten in the data are suspect, and should be replaced

are    ghting the \2 homo sapiens/g    monkey_   le

   ag.

fying repititions more precisely.

here are some symbols to match repetitions or to make an element optional.

repetition

412

appendix b

gsl_stats march 24, 2009

to replace all of the integers less than 100 in a    le:

perl    pi~    e    s/([^0   9]|^)([0   9][0   9]?)([^0   9]|\$)/$1nan$3/g    search_me
sed    i~    s/\([^0   9]\|^\)\([0   9][0   9]\?\)\([^0   9]\|$\)/\1nan\3/g    search_me
sed    r    i~    s/([^0   9]|^)([0   9][0   9]?)([^0   9]|$)/\1nan\3/g    search_me

the last atom appears zero or more times
the last atom appears one or more times (gnu grep/sed: \
the last atom appears zero or one times (gnu grep/sed: \

 
 
 )
?
?)
    the    rst part, [  0	9   |    will match either a non-numeric character or the be-
    the second part, [0	9   [0	9   ? , matches either one or two digits (but never
    the third part, [  0	9   |   will match either a non-numeric character or the end
replaces the second with a , as desired.

thus, we have precisely communicated that we want something, then an integer
under 100, then something else. since the search string used three sets of parens,
the output string can refer to all three. it repeats the    rst and last verbatim, and

this is a mess, but a very precise mess (which will be made more legible below).

ginning of the line.

of the line.

three).

structured regexes

chapter 2 presented a number of techniques for writing code
that is legible and easy to debug:

    debug the subfunctions incrementally.

    break large chunks of code into subfunctions.

    write tests to verify that components act as they should.

pressions. this is the version forba h or
 h.14
14you can also replace variables in the shell using a form like {digi  }. this is useful when the vari-
 {digi  }e digi  .

all of this applies to id157. you can break expresssions down using
variables, which your shell can then substitute into the expression. for example
the illegible regexes above can be somewhat clari   ed using a few named subex-

able name may merge with the following text, such as a search for exponential numbers of a certain form:

gsl_stats march 24, 2009

b.4 adding and deleting

413

text processing

it makes one-line tests relatively easy. for example:

echo 123 | sed $replacecmd
echo 12 | sed $replacecmd
echo 00 | sed $replacecmd

modify the search to include an optional decimal of arbitrary length. write
a test    le and test that your modi   cation works correctly.

notdigit=   \([^0   9]\|^\|$\)   
digits=   \([0   9][0   9]\?\)   
replacecmd="s/$notdigit$digits$notdigit/\\1nan\\3/g"
sed    i~ $replacecmd search_me

thee
h  command is useful for testing purposes. it dumps its text to  d 	 , so
a line from page 408:/fi d_ e/ . this con-
that matchfi d_ e, but you can also explicitly specify a line number or a range
of lines, such as7  to print line seven,   to print the last line of the input    le, or
1    to print the entire    le.
you can use the same line-addressing schemes withd to delete a line. for example,
 ed" d"<i fi e will print all ofi fi e but the last line.
used to produce a version ofda a	
 a       with the comments re-
you can also usei to insert above the given line(s), anda to append after the given
by the way, if you really want to get ed to print    hello, world    you can do it by

#add a pause after a gnuplot data block
sed    i~    e "/^e$/ a pause pauselength" plot   le
#put the text plot           at the head of a    le.
sed    i~    e "1 i plot          " plot   le
#pretend missing data does not exist
sed    i~    e "/nan/ d" plot   le

sists of a location and a command. with the slashes, the location is: those lines

recall the format for the sed command to print

line. a few examples:

moved.

inserting at line one and ignoring the rest of the    le:

sed    n    e "1 i hello, world." < any_random_   le

qb.9

qb.10

414

appendix b

script to

    delete the header line.

gsl_stats march 24, 2009

    add a header (as on page 184).

    add the end-brace on the last line.

refer to the exercise on page 184, which read in a text    le and produced

perl can do all of these things easily from inside a perl script, but inserting and

deleting lines from the command line is not as pleasant as using ed.15
a graphviz-readable output    le. that exercise read the text to ana   _	
da a set and then wrote the output, which is sensible when pulling many
classes from a database. but given the text    leda a	
 a       in the code
supplement, you can modify it directly into graphviz   s format. write a ed
    replace each pipe with a	>.
    replace each number n with  de .
pipe your output through ea   to check that your processing produced a
correctly ea  -readable    le and the right graph.
turn theda a	
 a          le into an sql script to create a table.
    add a header
 ea e ab e
 a   eg     i ee .
    replace each pair of numbers |  with the sql commandi  e  
i   
 a       ;.
    for added speed, put abegi ;   
   i ; wrapper around the entire
pipe the output to   i e3 to verify that your edits correctly created and
 e  	 	e   i  ;  i  " a	 e a	 e e g h"if/  e / 
 e  	 	e   i  "     	 \ "	  e   a; a=1 
 e  	 	e   i  	  e  / a / 

[bonus
points: write the search-and-replace that converts the existing header
into this form.]

15 a reader recommends the following for inserting a line:

    delete the header line.

populated the table.

for adding a line at the top of a    le:

for deleting a line:

   le.

qb.11

qb.12

b.5 more examples

415

. . . .

text processing

gsl_stats march 24, 2009

   if you have a set of parens in the search portion, you can refer to it in

expression zero or more, one or more, and zero or one times, respec-
tively.

   your best bet for command-line search-and-replace is e   or ed.
syntax: e  	 i
" / e  a
e e/wi h e/g"da a_fi e or
 ed	i
" / e  a
e e/wi h e/g"da a_fi e.
perl   s replace portion by 1, 2, . . . ; and in sed   s replace via \1, \2,
   the , , and? let you repeat the previous character or bracketed
looking for expressions in quotes, such as"a y hi g". it may seem that this
translates to the regular expression". ", meaning any character, repeated zero or
more times, enclosed by quotes. but consider this line:"fi   bi " " e
  d
bi ". you meant to have two matches, but instead will get only one:fi   bi " 
" e
  dbi , since this is everything between the two most extreme quotes. what
you meant to say was that you want anything that is not a" between two quotes.
that is, use"[  "    ", or"[  "    " if a blank like"" is acceptable.
ing data by commas; for example,da a	wb	gd  reports

say that the program that produced your data put it all in quotes, but you are
reading it in to a program that does not like having quotes. then this will    x the
problem:

now that you have the syntax for regexes, the applica-
tions come fast and easy.

are the ed command-line versions of the process:

that the usa   s gdp for 2005 was 12,455,068 millions of dollars. unfortunately,
if your program reads commas as    eld delimiters, this human-readability conve-
nience ruins computer readability. but commas in text are entirely valid, so we
want to remove only commas between two numbers. we can do this by searching
for a number-comma-number pattern, and replacing it with only the numbers. here

some data sources improve human readability by separat-

perl    pi~    e    s/"([^"]*)"/$1/g    data_   le

quoting and unquoting although the dot (to match any character) seems conve-
nient, it is almost never what you want. say that you are

getting rid of commas

gsl_stats march 24, 2009

416

appendix b

sed    i~    s/\([0   9]\),\([0   9]\)/\1\2/g       xme.txt
sed    r    i~    s/([0   9]),([0   9])/\1\2/g       xme.txt

names like   (cid:3)    (cid:3) i  . it is a hard problem to diagnose, but an easy problem to
   x. since[:  i  :    matches all printing characters,[  [:  i  :       matches all

some sources like to put odd non-printing characters in
their data. since they don   t print, they are hard to spot,
but they produce odd effects like columns in tables with

suspicious non-printing
characters

non-printing characters, and the following command replaces all such characters
with nothing:

sed    i~    s/[^[:print:]]//g       xme.txt

blocks of delimiters

sed    i~    s/[[:blank:]]\+/ /g    data   le

you could use the same strategy for reducing any other block
of delimiters, where it would be appropriate to do so, such as

some programs output multiple spaces to approximate tabs,
but programs that expect input with whitespace as a delimiter
will read multiple spaces as a long series of blank    elds.16 but it is easy to merge
whitespace, by just    nding every instance of several blanks (i.e., spaces or tabs) in
a row, and replacing them with a single space.

 /  / /.
mark missing data: if a data    le is pipe-delimited, then7||3 may be the data-
producer   s way of saying7| a |3. if your input program has trouble with this,
you will need to insert a    s yourself.
this may seem easy enough: /||/| a |/g. but there is a catch: you will need
lap their matches. we expect the input||| to invoke two replacements to form
| a | a |, but the regular expression parser will match the    rst two pipes, leav-
ing only a single| for later matches; in the end, you will have| a || in the output.

to run your substitution twice, because regular expression parsers will not over-

alternatively, a sequence of repeated delimiters may not need merging, but may

by running the replacement twice, you guarantee that every pipe    nds its pair.

sed    i~ data   le    e    s/||/|nan|/g       e    s/||/|nan|/g   

16the solution at the root of the problem is to avoid using whitespace as a delimiter. i recommend the pipe, |,

as a delimiter, since it is virtually never used in human text or other data.

417

text processing

text to database

gsl_stats march 24, 2009

sed    /#/ d    data   classroom | apop_text_to_db           friends classes.db

for many posix programs that typically take    le input, the traditional way to

the    le name. for example, after you did all that work in the exercise above to

put it at the end of any of the above streams to directly dump data to an sqlite
database.

thea   _ ex _  _db command line program (and its corre-
sponding c function) can take input from  di . thus, you can
indicate that you are sending data from  di  instead of a text    le is to use	 as
convert data to sql commands, here is one way to do it usinga   _ ex _  _	
db:17
g 	    
database to plot apophenia includes the command-line programa   _    _	
 	e y, which takes in a query and outputs a gnuplottable    le. it
provides some extra power: the	  option will bin the data into a histogram before
plotting, and you can use functions such asva  that sqlite does not support. but
     	  above the data. if there is a query in the    le 	e yfi e, then the se-
the	 e a a   "" clause is necessary because gnuplot does not like pipes as
could just add	e" /|//" to the sed command.
ature anomalies (theyea  and e   columns from the e   table of the
da a	
 i a e.db database).
17even this is unnecessary, because the program knows to read lines matching   # as comments. but the
example would be a little too boring as justa   _ ex _  _dbda a	
 a      f ie d 
 a  e .db.

sqlite will read a query    le either on the command line or from a pipe, and gnu-
plot will read in a formatted    le via a pipe. as you saw in chapter 5, turning a
column of numbers (or a matrix) into a gnuplottable    le simply requires putting

a delimiter. of course, if you did not have that option available via sqlite3, you

write a single command line to plot the yearly index of surface temper-

sqlite3    separator " " data.db < query   le | sed "1 i set key off\nplot          " | gnuplot    persist

for many instances, this is unnecessary.

quence is:

qb.13

unix versus windows: the end of the line

gsl_stats march 24, 2009

418

appendix b

the designers of at&t unix decided that it is suf   cient to end a line with just

if your    le is all one long line with
no breaks but a few funny characters

so none of the lines end.18 as further frustration, some programs auto-correct the
line endings while others do not, meaning that the    le may look ok in your text
editor but fall apart in your stats package.

cross   re of a long-standing war over how lines should end. in the style of manual
typewriters, starting a new line actually consists of two operations: moving hori-
zontally to the beginning of the line (a carriage return, cr), and moving vertically
down a line (a line feed, lf). the ascii character for cr is <ctrl-m>, which

interspersed, or has       s all over the place, then you have just found yourself in the
often appears on-screen as the single character    ; the ascii character for lf is
   .
a lf,    , while the designers of microsoft dos decided that a line should end
with a cr/lf pair,       . when you open a dos    le on a posix system, it will
recognize     as the end-of-line, but consider the     to be garbage, which it leaves
in the    le. when you open a posix    le in windows, it can   t    nd any        pairs,
recall from page 61 that \  is a cr and \  is a lf. going from dos to unix
adding a cr to each line, and both of these are simple ed commands:
some systems haved  2	 ix and	 ix2d   commands that do this for you,19 but
line of ed.
18typing    and then  will not produce a cr.     is a single special character confusingly represented on-
the sequence <ctrl-v> <ctrl-m> will insert the single cr character which appears on the screen as    ; and
19perhaps ask your package manager for thed  	 i   package.

means removing a single cr from each line, going from unix to dos means

they are often missing, and you can see that these commands basically run a single

screen with two characters. in most shells, <ctrl-v> means    insert the next character exactly as i type it,    so

#convert a unix    le to dos:
sed    i~    s/$/\r/    unix_   le

#convert a dos    le to unix:
sed    i~    s/\r$//    dos_   le

<ctrl-v> <ctrl-l> will similarly produce a lf.

gsl_stats march 24, 2009

c: glossary

see also the list of notation and symbols on page 12.

acronyms

ansi: american national standards
institute

ascii: american standard code for
information interchange

anova: analysis of variance [p 312]

blas: basic id202 system

blue: best linear unbiased estimator
[p 221]

bre: basic regular expression [p 403]

cdf: cumulative density function [p
236]

cmf: cumulative mass function [p 236]

clt: central limit theorem [p 296]

df: degree(s) of freedom

ere: extended regular expression [p
403]

erf: error function [p 284]

gcc: gnu compiler collection [p 48]

gdp: gross domestic product

gls: generalized least squares [p 277]

gnu: gnu   s not unix

gsl: gnu scienti   c library [p 113]

gui: graphical user interface

ide: integrated development environ-
ment

international electrotechnical

iec:
commission

ieee: institute of electrical and elec-
tronics engineers

iia: independence of irrelevant alterna-
tives [p 286]

gsl_stats march 24, 2009

420

appendix c

independent and identically dis-

iid:
tributed [p 326]

pdf: id203 density function [p
236]

iso: international standards organiza-
tion

iv: instrumental variable [p 275]

pdf: portable document format

prng: pseudorandom number genera-
tor [p 357]

lr: likelihood ratio [p 351]

pmf: id203 mass function [p 236]

mar: missing at random [p 345]

mcar: missing completely at random
[p 345]

mcmc: id115 [p
372]

ml: maximum likelihood

id113: maximum likelihood estima-
tion/estimate [p 325]

mnar: missing not at random [p 345]

rng: random number generator [p
357]

sse: sum of squared errors [p 227]

ssr: sum of squared residuals [p 227]

sst: total sum of squares [p 227]

sql: structured query language [p
74]

svd: singular value decomposition [p
265]

mse: mean squared error [p 223]

tla: three-letter acronym

ols: ordinary least squares [p 270]

pca: principal component analysis [p
265]

pcre: perl-compatible regular expres-
sion [p 403]

unix: not an acronym; see main glos-
sary

wls: weighted least squares [p 277]

terms

af   ne projection: a linear projection can always be expressed as a matrix t such
that x transformed is xt. but any such projection maps 0 to 0. an af   ne projection
adds a constant, transforming x to xt+k, so 0 now transforms to a nonzero value.
[p 280]

anova:    the analysis of variance is a body of statistical methods of analyzing
measurements assumed to be of the structure [yi = x1i  1+x2i  2+      +xpi  p+ei,
i = 1, 2, . . . , n], where the coef   cients {xji} are integers usually 0 or 1    (scheff  ,
1959) [p 312]

apophenia: the human tendency to see patterns in static. [p 1]

array: a sequence of elements, all of the same type. an array of text characters is
called a string. [p 30]

gsl_stats march 24, 2009

glossary

421

arguments: the inputs to a function. [p 36]

assertion: a statement in a program used to test for errors. the statement does
nothing, but should always evaluate to being true; if it does not, the program halts
with a message about the failed assertion. [p 71]

bandwidth: most distribution-smoothing techniques, including some kernel den-
sity estimates, gather data points from a    xed range around the point being evalu-
ated. the span over which data points are gathered is the bandwidth. for cases like
the normal kernel density estimator, whose tails always span (      ,   ), the term
is still used to indicate that as bandwidth gets larger, more far-reaching data will
have more of an effect. [p 261]

bernoulli draw: a single draw from a    xed process that produces a one with
id203 p and a zero with id203 1     p. [p 237]
bias: the distance between the expected value of an estimator of    and      s true
value, |e(     )       |. see unbiased statistic. [p 220]
binary tree: a set of structures, similar to a linked list, where each structure con-
sists of data and two pointers, one to a next-left structure and one to a next-right
structure. you can typically go from the head of the tree to an arbitrary element
much more quickly than if the same data were organized as a linked list. [p 200]

blue: the estimator      is a linear function, unbiased, and best in the sense that
var(     )     var(     ) for all linear unbiased estimators     . [p 221]

bootstrap: repeated sampling with replacement from a population produces a
sequence of arti   cial samples, which can be used to produce a sequence of iid
statistics. the central limit theorem then applies, and you can    nd the expected
value and variance of the statistic for the entire data set using the set of iid draws
of the statistic. the name implies that using samples from the data to learn about
the data is a bit like pulling oneself up by the bootstraps. see also jackknife and the
bootstrap principle. [p 367]

bootstrap principle: the claim that samples from your data sample will have
properties matching samples from the population. [p 296]

call-by-address: when calling a function, sending to the function a copy of an
input variable   s location (as opposed to its value). [p 54]

call-by-value: when calling a function, sending to the function a copy of an input
variable   s value. [p 39]

gsl_stats march 24, 2009

422

appendix c

xy     1. [p 229]

npx   x(cid:16)f (x)     f (x)(cid:17)k

cauchy   schwarz inequality: given the correlation coef   cient between any two
vectors x and y,   xy, it holds that 0       2
central limit theorem: given a set of means, each being the mean of a set of n
iid draws from a data set, the set of means will approach a normal distribution as
n        . [p 297]
central moments: given a data vector x and mean x, the kth central moment
of f (  ) is 1
p(x), then the kth central moment of f (x) is r    

p(x)dx. in
both cases, the    rst central moment is always zero (but see noncentral moment).
the second is known as the variance, the third as skew, and the fourth as kurtosis.
[p 230]

compiler: a non-interactive program (e.g.,g

) to translate code from a human-

closed-form expression: an expression, say x2 + y3, that can be written down us-
ing only a line or two of notation and can be manipulated using the usual algebraic
rules. this is in contrast to a function algorithm or an empirical distribution that
can be described only via a long code listing, a histogram, or a data set.

      (cid:16)f (x)     f (x)(cid:17)k

. in the continuous case, if x has distribution

readable source    le to a computer-readable object    le. the compiler is often closely
intertwined with the preprocessor and linker, to the point that the preprocessor/-
compiler/linker amalgam is usually just called the compiler. compare with inter-
preter. [p 18]

conjugate distributions: a prior/likelihood pair such that if the prior is updated
using the likelihood, the posterior has the same form as the prior (but updated
parameters). for example, given a beta distribution prior and a binomial likelihood
function, the posterior will be a beta distribution. unrelated to conjugate gradient
methods. [p 374]

consistent estimator: an estimator     (x) is consistent if, for some constant c,
limn       p (|     (x)     c| >   ) = 0, for any    > 0. that is, as the sample size
grows, the value of     (x) converges in id203 to the single value c. [p 221]

consistent test: a test is consistent if the power     1 as n        . [p 335]
contrast: a hypothesis about a linear combination of coef   cients, like 3  1   2  2 =
0. [p 309]

correlation coef   cient: given the square roots of the covariance and variances,
  xy,   x, and   y, the correlation coef   cient   xy       xy

. [p 229]

  x  x

gsl_stats march 24, 2009

glossary

423

covariance: for two data vectors x and y,   2

xy     1

npi(xi     x)(yi     y). [p 228]

cram  r   rao lower bound: the elements of the covariance matrix of the estimate
of a parameter vector must be equal to or greater than a limit that is constant for a
given pdf, as in equation 10.1.7 (page 333). for an id113, the crlb reduces to
1/(ni), where i is the information matrix. [p 333]

crosstab: a two-dimensional table, where each row represents values of one vari-
able (y), each column represents values of another variable (x), and each (row,
column) entry provides some summary statistic of the subset of data where y has
the given row value and x has the given column value. see page 101 for an exam-
ple. [p 101]

cumulative density function: the integral of a pdf. its value at any given point
indicates the likelihood that a draw from the distribution will be equal to or less
than the given point. since the pdf is always non-negative, the cdf is monotoni-
cally nondecreasing. at       , the cdf is always zero, and at     the cdf is always
one. [p 236]

cumulative mass function: the integral of a pmf. that is, a cdf when the dis-
tribution is over discrete values. [p 236]

data mining: formerly a synonym for data snooping, but in current usage, meth-
ods of categorizing observations into a small set of (typically nested) bins, such as
generating trees or separating hyperplanes.

data snooping: before formally testing a hypothesis, trying a series of preliminary
tests to select the form of the    nal test. such behavior can taint inferential statistics
because the statistic parameter from one test has a very different distribution from
the statistic most favorable parameter from    fty tests. [p 316]

debugger: a standalone program that runs a program, allowing the user to halt
the program at any point, view the stack of frames, and query the program for the
value of a variable at that point in the program   s execution. [p 43]

declaration: a line of code that indicates the type of a variable or function. [p 28]

degrees of freedom: the number of dimensions along which a data set varies. if
all n data points are independent, then df = n, but if there are restrictions that
reduce the data   s dimensionality, df < n. you can often think of the df as the
number of independent pieces of information. [p 222]

dependency: a statement in a make   le indicating that one    le depends on another,
such as an object    le that depends on a source    le. when the depended-on    le
changes, the dependent    le will need to be re-produced. [p 388]

gsl_stats march 24, 2009

424

appendix c

descriptive statistics: the half of id203 and statistics aimed at    ltering useful
patterns out of a world of overwhelming information. the other half is inferential
statistics. [p 1]

dummy variable: a variable that takes on discrete values (usually just zero or
one) to indicate the category in which an observation lies. [p 281]

ef   ciency: a parameter estimate that comes as close as possible to achieving the
cram  r   rao lower bound, and thus has as small a variance as possible, is dubbed
an ef   cient estimate. [p 220]

to all child programs. they are typically set from the shell   sex     or e e v

environment variable: a set of variables maintained by the system and passed

error function: the cdf of the normal(0, 1) distribution. [p 284]

command. [p 381]

expected value: the    rst noncentral moment, aka the mean or average. [p 221]

frame: a collection of a function and all of the variables that are in scope for the
function. [p 37]

gcc: the gnu compiler collection, which reads source    les in a variety of
languages and produces object    les accordingly. this book uses only its ability to
read and compile c code. [p 48]

generalized least squares: the ordinary least squares model assumes that the
covariance matrix among the observations is    =   2i (i.e., a scalar times the
identity matrix). a gls model is any model that otherwise conforms to the ols
assumptions, but allows    to take on a different form. [p 277]

panding .
 to the full list of    le names ending in.
. uses an entirely different

globbing: the limited regular expression parsing provided by a shell, such as ex-

syntax from standard regular expression parsers. [p 407]

graph: a set of nodes, connected by edges. the edges may be directional, thus
forming a directional graph. not to be confused with a plot. [p 182]

grid search: divide the space of inputs to a function into a grid, and write down the
value of the function at every point on the grid. such an exhaustive walk through
the space can be used to get a picture of the function (this is what most graphing
packages do), or to    nd the optimum of the function. however, it is a last resort
for most purposes; the search and random draw methods of chapters 10 and 11 are
much more ef   cient and precise. [p 371]

gsl_stats march 24, 2009

glossary

#i 
 	de-ing it in multiple c    les, the variables, functions, and types declared in

header    le: a c    le consisting entirely of declarations and type de   nitions. by

hat matrix: please see projection matrix. [p 272]

425

the header    le can be de   ned in one    le and used in many. [p 49]

hessian: the matrix of second derivatives of a function evaluated at a given point.
given a log likelihood function ll(  ), the negation of its hessian is the informa-
tion matrix. [p 341]

heteroskedasticity: when the errors associated with different observations have
different variances, such as observations on the consumption rates of the poor and
the wealthy. this violates an assumption of ols, and can therefore produce inef-
   cient estimates; weighted least squares solves this problem. [p 277]

identically distributed: a situation where the process used to produce all of the
elements of a data set is considered to be identical. for example, all data points
may be drawn from a poisson(0.4) distribution, or may be individuals randomly
sampled from one population. [p 326]

identity matrix: a square matrix where every non-diagonal element is zero, and
every diagonal element is one. its size is typically determined by context, and it is
typically notated as i. there are really an in   nite number of identity matrices (a
1    1 matrix, a 2    2 matrix, a 3    3 matrix, . . . ), but the custom is to refer to any
one of them as the identity matrix.

iff: if and only if. the following statements are equivalent: a        b; a iff b;
a     b; a is de   ned to be b; b is de   ned to be a.
iid: independent and identically distributed. these are the conditions for the cen-
tral limit theorem. see independent draws and identically distributed. [p 326]

importance sampling: a means of making draws from an easy-to-draw-from dis-
tribution to make draws from a more dif   cult distribution. [p 371]

independence of irrelevant alternatives: the ratio of (likelihood of choice a be-
ing selected)/(likelihood of choice b being selected) does not depend on what
other options are available   adding or deleting choices c, d, and e will not
change the ratio. [p 286]

independent draws: two events x1 and x2 (such as draws from a data set) are
independent if p (x1     x2)   that is, the id203 of (x1 and x2)   is equal to
p (x1)    p (x2). [p 326]

gsl_stats march 24, 2009

426

appendix c

inferential statistics: the half of id203 and statistics aimed at    ghting against
apophenia. the other half is descriptive statistics. [p 1]

information matrix: the negation of the derivative of the score. put differently,
given a log likelihood function ll(  ), the information matrix is the negation of
its hessian matrix. see also the cram  r   rao lower bound. [p 326]

instrumental variable: if a variable is measured with error, then the ols pa-
rameter estimate based on that variable will be biased. an instrumental variable
is a replacement variable that is highly correlated with the measured-with-error
variable. a variant of ols using the instrumental variable will produce unbiased
parameter estimates. [p 275]

interaction: an element of a model that contends that it is not x1 or x2 that causes
an outcome, but the combination of both x1 and x2 simultaneously (or x1 and not
x2, or not x1 but x2). this is typically represented in ols regressions by simply
multiplying the two together to form a new variable x3     x1    x2. [p 281]
interpreter: a program to translate code from a human-readable language to a
computer   s object code or some other binary format. the user inputs individual
commands, typically one by one, and then the interpreter produces and executes
the appropriate machine code for each line. gnuplot and the sqlite3 command-line
program are interpreters. compare with compiler.

jackknife: a relative of the bootstrap. a subsample is formed by removing the    rst
element, then estimating     j1; the next subsample is formed by replacing the    rst
element and removing the second, then re-estimating     j2, et cetera. the multitude
of     jn   s thus formed can be used to estimate the variance of the overall parameter
estimate     . [p 131]

join: combining two database tables to form a third, typically including some
columns from the    rst and some from the second. there is usually a column on
which the join is made; e.g., a    rst table of names and heights and a second table
of names and weights would be joined by matching the names in both tables. [p
87]

kernel density estimate: a method of smoothing a data set by centering a standard
pdf (like the normal) around every point. summing together all the sub-pdfs
produces a smooth overall pdf. [p 262]

kurtosis: the fourth central moment. [p 230]

lexicographic order: words in the dictionary are    rst sorted using only the    rst
letter, completely ignoring all the others. then, words beginning with a are sorted

gsl_stats march 24, 2009

glossary

427

by their second letter. those beginning with the same    rst two letters (aandblom,
aard-wolf, aasvogel, . . . ) are sorted using their third letter. thus, a lexicographic
ordering sorts using only the    rst characteristic, then breaks ties with a second
characteristic, then breaks further ties with a third, and so on. [p 91]

library: a set of functions and variables that perform tasks related to some speci   c
task, such as numeric methods or linked list handling. the library is basically an
object    le in a slightly different format, and is typically kept somewhere on the
library path. [p 52]

likelihood function: the likelihood p (x,   )|x is the id203 that we   d have
the parameters    given some observed data x. this is in contrast to the id203
of a data set given    xed parameters, p (x,   )|  . see page 329 for discussion. [p
326]

likelihood ratio test: a test based on a statistic of the form p1/p2. this is some-
times logged to ll1     ll2. many tests that on their surface seem to not    t this
form can be shown to be equivalent to an lr test. [p 335]

linked list: a set of structures, where each structure holds data and a pointer to the
next structure in the list. one could traverse the list by following the pointer from
the head element to the next element, then following that element   s pointer to the
next element, et cetera. [p 198]

linker: a program that takes in a set of libraries and object    les and outputs an
executable program. [p 51]

x + d2

distance viaqd2

manhattan metric: given distances in several dimensions, say dx = |x1   x2| and
dy = |y1     y2|, the standard euclidian metric combines them to    nd a straight-line
y. the manhattan metric simply adds the distance on each
dimension, dx + dy. this is the distance one would travel by    rst going only along
east   west streets, then only along north   south streets. [p 150]

make: a program that keeps track of dependencies, and runs commands (speci   ed
in a make   le) as needed to keep all    les up-to-date as their dependencies change.
usually used to produce executables when their source    les change. [p 387]

preprocessor may replace every occurrence ofgs _    a b  with  a < b 
? a : b  . [p 212]

macro: a rule to transform strings of text with a    xed pattern. for example, a

metadata: data about data. for example, a pointer is data about the location of
base data, and a statistic is data summarizing or transforming base data. [p 128]

gsl_stats march 24, 2009

428

appendix c

mean squared error: given an estimate of    named     , the mse is e(            )2.
this can be shown to be equivalent to var(     ) + bias2(     ). [p 220]

memory leak: if you lose the address of space that you had allocated, then the
space remains reserved even though it is impossible for you to use. thus, the sys-
tem   s usable memory is effectively smaller. [p 62]

missing at random: data for variable i is mar if the incidence of missing data
points is unrelated to the existing data for variable i, given the other variables.
generally, this means that there is an external cause (not caused by the value of i)
that causes values of i to be missing. [p 346]

missing completely at random: data for variable i are mcar if there is no cor-
relation between the incidence of missing data and anything else in the data set.
that is, the cause of missingness is entirely external and haphazard. [p 346]

missing not at random: data for variable i is mnar if there is a correlation
between the incidence of missing data and the missing data   s value. that is, the
missingness is caused by the data   s value. [p 346]

monte carlo method: generating information about a distribution, such as pa-
rameter estimates, by repeatedly making random draws from the distribution. [p
356]

multicollinearity: given a data set x consisting of columns x1, x2, . . . , if two
columns xi and xj are highly correlated, then the determinant of x   x will be
near zero and the value of the inverse (x   x)   1 unstable. as a result, ols-family
estimates will not be reliable. [p 275]

noncentral moment: given a data vector x and mean x, the kth noncentral mo-
ment is 1

npx   x xk. in the continuous case, if x has distribution p(x), then the kth

f (x)kp(x)dx. the only noncentral moment

noncentral moment of f (x) is r    

anybody cares about is the    rst   aka, the mean. [p 230]

      

non-ignorable missingness: see missing not at random. [p 346]

non-parametric: a test or model is non-parametric if it does not rely on a claim
that the statistics/parameters in question have a textbook distribution (t,   2, nor-
mal, bernoulli, et cetera). however, all non-parametric models have parameters
to tune, and all non-parametric tests are based on a statistic whose characteristics
must be determined.

null pointer: a special pointer that is de   ned to not point to anything. [p 43]

429

glossary

gsl_stats march 24, 2009

bears no relation to objects or object-oriented programming. [p 51]

object    le: a computer-readable    le listing the variables, functions, and types

order statistic: the value at a given position in the sorted data, such as the largest
number in a set, the second largest number, the median, the smallest number, et
cetera.

object: a structure, typically implemented via a   	
 , plus any supporting func-
tions that facilitate use of that structure, such as theg  _ve
    plus theg  _	
ve
   _add,g  _ve
   _dd  , . . . , functions.
de   ned in a.
    le. object    les are not executables until they go through linking.
things may occur. for example, on some systems, ax_  t 1==	 ax_  t.
the ieee standard speci   es that if af  a  ord 	b e variable over   ows, it be set
shells have a at  environment variable along which they search for executable
programs. similarly, the preprocessor searches for header    les (e.g.,#i 
 	de
<  d ib.h>) along the directories in the  c ude at  environment variable,
which can be extended using the	     ag on the compiler command line. the linker
	  compiler    ag. [p 385]
c, it is formed using the   e  function. [p 395]

ordinary least squares: a model, fully speci   ed on page 274, that contends
that a dependent variable is the linear combination of a number of independent
variables, plus an error term.

pipe: a connection that directly redirects the output from one program to the input
of another. in the shell, a pipe is formed by putting a | between two programs; in

to a special pattern indicating in   nity. see also under   ow error. [p 137]

over   ow error: when the value of a variable is too large for its type, unpredictable

path: a list of directories along which the computer will search for    les. most

searches for libraries to include using a libpath and its extensions speci   ed via the

pivot table: see crosstab. [p 101]

plot: a graphic with two or three axes and function values marked relative to those
axes. not to be confused with a graph. [p 158]

pointer: a variable holding the location of a piece of data. [p 53]

posix: the portable operating system interface standard. by the mid-1980s, a
multitude of variants on the unix operating system appeared; the institute of elec-
trical and electronics engineers convened a panel to write this standard so that pro-
grams written on one    avor of unix could be more easily ported to another    avor.
santa cruz operation   s unix, international business machines    aix, hewlett-

gsl_stats march 24, 2009

430

appendix c

packard   s hp-ux, linux, sun   s solaris, some members of microsoft   s windows
family, and others all more or less comply with this standard.

power: the likelihood of rejecting a false null. that is, if there is a signi   cant
effect, what are the odds that the test will detect it? this is one minus the likelihood
of a type ii error. [p 335]

prime numbers: prime numbers are what is left when you have taken all the pat-
terns away. (haddon, 2003, p 12) [p 61]

principal component analysis: a projection of data x onto a basis space con-
sisting of n eigenvalues of x   x, which has a number of desirable properties. [p
265]

id203 density function: the total area under a pdf for any given range is
equal to the id203 that a draw from the distribution will fall in that range. the
pdf is always nonnegative. e.g., the familiar bell curve of a normal distribution.
compare with cumulative density function. [p 236]

id203 mass function: the distribution of probabilities that a given discrete
value will be drawn. i.e., a pdf when the distribution is over discrete values. [p
236]

projection matrix: xp     x(x   x)   1x   . xp v equals the projection of v onto
the column space of x. [p 272]

pro   ler: a program that executes other programs, and determines how much time
is spent in each of the program   s various functions. it can thus be used to    nd the
bottlenecks in a slow-running program. [p 215]

query: any command to a database. typically, the command uses the e e
 

pseudorandom number generator: a function that produces a deterministic se-
quence of numbers that seem to have no pattern. initializing the prng with a
different seed produces a different streams of numbers. [p 357]

keyword to request data from the database, but a query may also be a non-question
command, such as a command to create a new table, drop an index, et cetera. [p
74]

random number generator: see pseudorandom number generator. [p 357]

id157: a string used to describe patterns of text, such as    two num-
bers followed by a letter   . [p 403]

gsl_stats march 24, 2009

glossary

431

scope: the section of code that is able to refer to a variable. for variables declared
outside of a function, the scope is everything in a    le after the declaration; for vari-
ables declared inside a block, the scope is everything after the declaration inside
the block. [p 41]

score: given a log likelihood function ln p (  ), its score is the vector of its deriva-
tives: s = (    ln p/     ). [p 326]

seed: the value with which a pseudorandom number generator is initialized. [p
357]

segfault: an affectionate abbreviation for segmentation fault. [p 43]

segmentation fault: an error wherein the program attempts to access a part of
the computer   s memory that was not allocated to the program. if reading from
unauthorized memory, this is a security hole; if writing to unauthorized memory,
this could destroy data or create system instability. therefore, the system catches
segfaults and halts the program immediately when they occur. [p 43]

shell: a program whose primary purpose is to facilitate running other programs.
when you log in to most text-driven systems, you are immediately put at the shell   s
input prompt. most shells include facilities for setting variables and writing scripts.
[p 393]

singular value decomposition: given an m    n data matrix x (where typically
m >> n), one can    nd the n eigenvectors associated with the n largest eigenval-
ues.20 this may be done as the    rst step in a principal component analysis. svd
as currently practiced also includes a number of further techniques to transform
the eigenvectors as appropriate. [p 265]

skew: the third central moment, used as an indication of whether a distribution
leans to the left or right of the mean. [p 230]

establishing a ai  frame, and then if ai  calls another function, that function   s
frame is thought of as being laid on top of the ai  frame. similarly for subsequent

source code: the human-readable version of a program. it will be converted into
object code for the computer to execute.

stack: each function runs in its own frame. when a program starts, it begins by

functions, so pending frames pile up to form a stack of frames. when the stack is
empty, the program terminates. [p 38]

20this is assuming that x   x has full rank.

gsl_stats march 24, 2009

432

appendix c

standard deviation: the square root of the variance of a variable, often notated as
  . if the variable is normally distributed, we usually compare a point   s distance to
the mean against 1  , 2  , . . . . for distributions that are not normal (or at least bell-
shaped),    is of limited descriptive utility. see also standard error and variance.
[p 222]

standard error: an abbreviation for the standard deviation of the error. [p 367]

statistic: a function that takes data as an input, such as the mean of x; the vari-
ance of the error term of a regression of x on y, or the ols parameter      =
(x   x)   1x   y. [p 219]

string: an array of characters. because the string is an array, it is handled using
pointer-type operations, but there are also functions to print the string like the plain
text it represents. [p 65]

structure: a set of variables that are intended to collectively represent one object,
such as a person (comprising, e.g., a name, height, and weight) or a bird (compris-
ing, e.g., a type and pointers to offspring). [p 31]

structured query language: a standard language for writing database queries.
[p 74]

switches: as with physical machinery, switches are options to affect a program   s
operation. they are usually set on the command line, and are usually marked by a

dash, like	x. [p 208]

trimean: (   rst quartile + two times the median + third quartile)/4. (tukey, 1977, p
46) [p 234]

threading: on many modern computers, the processor(s) can execute multiple
chains of commands at once. for example, the data regarding two independent
events could be simultaneously processed by two processors. in such a case, the
single thread of program instructions can be split into multiple threads, which must
be gathered together before the program completes. [p 119]

type: the class of data a variable is intended to represent, such as an integer,
character, or structure (which is an amalgamation of subtypes). [p 27]

type casting: converting a variable from one type to another. [p 33]

type i error: rejecting the null when it is true. [p 335]

type ii error: accepting the null when it is false. see also power. [p 335]

gsl_stats march 24, 2009

glossary

433

unbiased statistic: the expected value of the statistic      equals the true population
value: e(     ) =   . [p 220]

unbiased estimator: let    be a test   s type i error, and let    be its type ii error.
a test is unbiased if (1       )        for all values of the parameter. i.e., you are less
likely to accept the null when it is false than when it is true. [p 335]

under   ow error: occurs when the value of a variable is smaller than the smallest
number the system can represent. for example, on any current system with    nite-
precision arithmetic, 2   10,000 is simply zero. see also over   ow error. [p 137]

unix: an operating system developed at bell labs. many call any unix-like op-
erating system by this name (often by the plural, unices), but unix properly refers
only to the code written by bell labs, which has evolved into code owned by santa
cruz operation. others are correctly called posix-compliant. the name does not
stand for anything, but is a pun on a predecessor operating system, multics.

variance: the second central moment, usually notated as   2. [p 222]

weighted least squares: a type of gls method wherein different observations
are given different weights. the weights can be for any reason, such as producing
a representative survey sample, but the method is often used for heteroskedastic
data. [p 277]

gsl_stats march 24, 2009

gsl_stats march 24, 2009

bibliography

abelson, harold, sussman, gerald jay, & sussman, julie. 1985. structure and

interpretation of computer programs. mit press.

albee, edward. 1960. the american dream and zoo story. signet.

allison, paul d. 2002. missing data. quantitative applications in the social

sciences. sage publications.

amemiya, takeshi. 1981. qualitative response models: a survey. journal of

economic literature, 19(4), 1483   1536.

amemiya, takeshi. 1994. introduction to statistics and econometrics. harvard

university press.

avriel, mordecai. 2003. nonid135: analysis and methods. dover

press.

axtell, robert. 2006. firm sizes: facts, formulae, fables and fantasies. center

on social and economic dynamics working papers, 44(feb.).

barron, andrew r, & sheu, chyong-hwa. 1991. approximation of density func-
tions by sequences of exponential families. the annals of statistics, 19(3),
1347   1369.

baum, ae, akula, n, cabanero, m, cardona, i, corona, w, klemens, b, schulze,
tg, cichon, s, rietschel, m, nathen, mm, georgi, a, schumacher, j, schwarz,
m, jamra, r abou, hofels, s, propping, p, satagopan, j, consortium, nimh ge-
netics initiative bipolar disorder, detera-wadleigh, sd, hardy, j, & mcmahon,

gsl_stats march 24, 2009

436

bibliography

fj. 2008. a genome-wide association study implicates diacylglycerol kinase eta
(dgkh) and several other genes in the etiology of bipolar disorder. molecular
psychiatry, 13(2), 197   207.

benford, frank. 1938. the law of anomalous numbers. proceedings of the

american philosophical society, 78(4), 551   572.

bowman, k o, & shenton, l r. 1975. omnibus test contours for departures

from normality based on    b1 and b2. biometrika, 62(2), 243   250.

casella, george, & berger, roger l. 1990. statistical id136. duxbury press.

chamberlin, thomas chrowder. 1890. the method of multiple working hypothe-

ses. science, 15(366), 10   11.

cheng, simon, & long, j scott. 2007. testing for iia in the multinomial logit

model. sociological methods research, 35(4), 583   600.

chung, j h, & fraser, d a s. 1958. randomization tests for a multivariate two-
sample problem. journal of the american statistical association, 53(283), 729   
735.

chwe, michael suk-young. 2001. rational ritual: culture, coordination, and

common knowledge. princeton university press.

cleveland, william s, & mcgill, robert. 1985. graphical perception and graphi-

cal methods for analyzing scienti   c data. science, 229(4716), 828   833.

codd, edgar f. 1970. a relational model of data for large shared data banks.

communications of the acm, 13(6), 377   387.

conover, w j. 1980. practical nonparametric statistics. 2nd edn. wiley.

cook, r dennis. 1977. detection of in   uential observations in id75.

technometrics, 19(1), 15   18.

cox, d r. 1962. further results on tests of separate families of hypotheses.
journal of the royal statistical society. series b (methodological), 24(2), 406   
424.

cropper, maureen l, deck, leland, kishor, nalin, & mcconnell, kenneth e.
1993. valuing product attributes using single market data: a comparison of
hedonic and discrete choice approaches. review of economics and statistics,
75(2), 225   232.

dempster, a p, laird, n m, & rubin, d b. 1977. maximum likelihood from
incomplete data via the em algorithm. journal of the royal statistical society.
series b (methodological), 39(1), 1   38.

gsl_stats march 24, 2009

bibliography

437

efron, bradley, & hinkley, david v. 1978. assessing the accuracy of the max-
imum likelihood estimator: observed versus expected fisher information.
biometrika, 65(3), 457   482.

efron, bradley, & tibshirani, robert j. 1993. an introduction to the bootstrap.

monographs on statistics and id203, no. 57. chapman and hall.

eliason, scott r. 1993. id113: logic and practice.

quantitative applications in the social sciences. sage publications.

epstein, joshua m, & axtell, robert. 1996. growing arti   cial societies: social

science from the bottom up. brookings institution press and mit press.

fein, sidney, paz, victoria, rao, nyapati, & lagrassa, joseph. 1988. the combi-
nation of lithium carbonate and an maoi in refractory depressions. american
journal of psychiatry, 145(2), 249   250.

feller, william. 1966. an introduction to id203 theory and its applications.

wiley.

fisher, r a. 1934. two new properties of mathematical likelihood. proceedings
of the royal society of london. series a, containing papers of a mathematical
and physical character, 144(852), 285   307.

fisher, ronald aylmer. 1922. on the interpretation of   2 from contingency ta-
bles, and the calculation of p . journal of the royal statistical society, 85(1),
87   94.

fisher, ronald aylmer. 1956. statistical methods and scienti   c id136. oliver

& boyd.

freedman, david a. 1983. a note on screening regression equations. the amer-

ican statistician, 37(2), 152   155.

friedl, jeffrey e f. 2002. mastering id157. 2nd edn. o   reilly

media.

frisch,   leen. 1995. essential system administration. o   reilly & associates.

fry, tim r l, & harris, mark n. 1996. a monte carlo study of tests for the
independence of irrelevant alternatives property. transportation research part
b: methodological, 30(1), 19   30.

gardner, martin. 1983. wheels, life, and other mathematical amusements. w h

freeman.

gelman, andrew, & hill, jennifer. 2007. data analysis using regression and

multilevel/id187. cambridge university press.

gsl_stats march 24, 2009

438

bibliography

gelman, andrew, carlin, john b, stern, hal s, & rubin, donald b. 1995. bayesian
data analysis. 2nd edn. chapman & hall texts in statistical science. chapman
& hall/crc.

gentle, james e. 2002. elements of computational statistics. statistics and com-

puting. springer.

gentleman, robert, & ihaka, ross. 2000. lexical scope and statistical comput-

ing. journal of computational and graphical statistics, 9(3), 491   508.

gibbard, ben. 2003. lightness. barsuk records. in death cab for cutie, transat-

lanticism.

gibrat, robert. 1931. les in  galit  s   conomiques; applications: aux in  galit  s
des richesses, a la concentration des entreprises, aux populations des villes,
aux statistiques des familles, etc., d   une loi nouvelle, la loi de l   effet propor-
tionnel. librarie du recueil sirey.

gigerenzer, gerd. 2004. mindless statistics. the journal of socio-economics, 33,

587   606.

gill, philip e, murray, waler, & wright, margaret h. 1981. practical optimiza-

tion. academic press.

givens, geof h, & hoeting, jennifer a. 2005. computational statistics. wiley

series in id203 and statistics. wiley.

glaeser, edward l, sacerdote, bruce, & scheinkman, jose a. 1996. crime and

social interactions. the quarterly journal of economics, 111(2), 507   48.

goldberg, david. 1991. what every computer scientist should know about

floating-point arithmetic. acm computing surveys, 23(1), 5   48.

gonick, larry, & smith, woollcott. 1994. cartoon guide to statistics. collins.

good, irving john. 1972. random thoughts about randomness. psa: proceed-
ings of the biennial meeting of the philosophy of science association, 1972,
117   135.

gough, brian (ed). 2003. gnu scienti   c library reference manual. 2nd edn.

id177, ltd.

greene, william h. 1990. econometric analysis. 2nd edn. prentice hall.

haddon, mark. 2003. the curious incident of the dog in the night-time. vintage.

huber, peter j. 2000. languages for statistics and data analysis. joural of com-

putational and graphical statistics, 9(3), 600   620.

huff, darrell, & geis, irving. 1954. how to lie with statistics. w. w. norton &

company.

gsl_stats march 24, 2009

bibliography

439

hunter, john e, & schmidt, frank l. 2004. methods of meta-analysis: correcting

error and bias in research findings. 2nd edn. sage publications.

internal revenue service. 2007. 2007 federal tax rate schedules. department of

the treasury.

kahneman, daniel, slovic, paul, & tversky, amos. 1982. judgement under un-

certainty: heuristics and biases. cambridge university press.

karlquist, a (ed). 1978. spatial interaction theory and residential location.

north holland.

kernighan, brian w, & pike, rob. 1999. the practice of programming. addison-

wesley professional.

kernighan, brian w, & ritchie, dennis m. 1988. the c programming language.

2nd edn. prentice hall ptr.

klemens, ben. 2007. finding optimal agent-based models. brookings center on

social and economic dynamics working paper #49.

kline, morris. 1980. mathematics: the loss of certainty. oxford university

press.

kmenta, jan. 1986. elements of econometrics. 2nd edn. macmillan publishing

company.

knuth, donald ervin. 1997. the art of computer programming. 3rd edn. vol. 1:

fundamental algorithms. addison-wesley.

kolmogorov, andrey nikolaevich. 1933. sulla determinazione empirica di una

legge di distributione. giornale dell    istituto italiano degli attuari, 4, 83   91.

laumann, anne e, & derick, amy j. 2006. tattoos and body piercings in the
united states: a national data set. journal of the american academy of der-
matologists, 55(3), 413   21.

lehmann, e l, & stein, c. 1949. on the theory of some non-parametric hy-

potheses. the annals of mathematical statistics, 20(1), 28   45.

maddala, g s. 1977. econometrics. mcgraw-hill.

mcfadden, daniel. 1973. conditional logit analysis of qualitative choice be-

havior. in:zarembka (1973). chap. 4, pages 105   142.

mcfadden, daniel. 1978. modelling the choice of residential location.

in:karlquist (1978). pages 75   96.

nabokov, vladimir. 1962. pale fire. g p putnams   s sons.

gsl_stats march 24, 2009

440

bibliography

national election studies. 2000. the 2000 national election study [dataset].

university of michigan, center for political studies.

newman, james r (ed). 1956. the world of mathematics. simon and schuster.

neyman, j, & pearson, e s. 1928a. on the use and interpretation of certain test
criteria for purposes of statistical id136: part i. biometrika, 20a(1/2), 175   
240.

neyman, j, & pearson, e s. 1928b. on the use and interpretation of certain
test criteria for purposes of statistical id136: part ii. biometrika, 20a(3/4),
263   294.

orwell, george. 1949. 1984. secker and warburg.

papadimitriou, christos h, & steiglitz, kenneth. 1998. combinatorial optimiza-

tion: algorithms and complexity. dover.

paulos, john allen. 1988.
quences. hill and wang.

innumeracy: mathematical illiteracy and its conse-

pawitan, yudi. 2001. in all likelihood: statistical modeling and id136 using

likelihood. oxford university press.

pearson, karl. 1900. on the criterion that a given system of deviations from
the probable in the case of a correlated system of variables is such that it
can be reasonably supposed to have arisen from random sampling. london,
edinburgh and dublin philosophical magazine and journal of science, july,
157   175. reprinted in pearson (1948, pp 339   357).

pearson, karl. 1948. karl pearson   s early statistical papers. cambridge.

peek, jerry, todino-gonguet, grace, & strang, john. 2002. learning the unix

operating system. 5th edn. o   reilly & associates.

perl, judea. 2000. causality. cambridge university press.

pierce, john r. 1980. an introduction to id205: symbols, signals,

and noise. dover.

poincar  , henri. 1913. chance. in newman (1956), translated by george bruce

halsted. pages 1380   1394.

polhill, j gary, izquierdo, luis r, & gotts, nicholas m. 2005. the ghost in the
model (and other effects of floating point arithmetic). journal of arti   cial
societies and social simulation, 8(1).

poole, keith t, & rosenthal, howard. 1985. a spatial model for legislative roll

call analysis. american journal of political science, 29(2), 357   384.

gsl_stats march 24, 2009

bibliography

441

press, william h, flannery, brian p, teukolsky, saul a, & vetterling, william t.
1988. numerical recipes in c: the art of scienti   c computing. cambridge
university press.

price, roger, & stern, leonard. 1988. mad libs. price stern sloan.

rumi, jelaluddin. 2004. the essential rumi. penguin. translated by coleman

barks.

s  rndal, carl-erik, swensson, bengt, & wretman, jan. 1992. model assisted sur-

vey sampling. springer series in statistics. springer-verlag.

scheff  , henry. 1959. the analysis of variance. wiley.

shepard, roger n, & cooper, lynn a. 1992. representation of colors in the blind,

color-blind, and normally sighted. psychological science, 3(2), 97   104.

silverman, b w. 1985. some aspects of the spline smoothing approach to non-
parametric regression curve fitting. journal of the royal statistical society.
series b (methodological), 47(1), 1   52.

silverman, bernard w. 1981. using kernel density estimates to investigate mul-
timodality. journal of the royal statistical society, series b (methodological),
43, 97   99.

smith, thomas m, & reynolds, richard w. 2005. a global merged land air
and sea surface temperature reconstruction based on historical observations
(1880    1997). journal of climate, 18, 2021   2036.

snedecor, george w, & cochran, willian g. 1976. statistical methods. 6th edn.

iowa state university press.

stallman, richard m, pesch, roland h, & shebs, stan. 2002. debugging with

gdb: the gnu source-level debugger. free software foundation.

stravinsky, igor. 1942. poetics of music in the form of six lessons: the charles

eliot norton lectures. harvard university press.

stroustrup, bjarne. 1986. the c++ programming language. addison-wesley.

student. 1927. errors of routine analysis. biometrika, 19(1/2), 151   164.

thomson, william. 2001. a guide for the young economist: writing and speaking

effectively about economics. mit press.

train, kenneth e. 2003. discrete choice methods with simulation. cambridge

university press.

tukey, john w. 1977. exploratory data analysis. addison-wesley.

gsl_stats march 24, 2009

442

bibliography

vuong, quang h. 1989. likelihood ratio tests for model selection and non-

nested hypotheses. econometrica, 57(2), 307   333.

wolfram, stephen. 2003. the mathematica book. 5th edn. wolfram media.

zarembka, p (ed). 1973. frontiers in econometrics. academic press.

gsl_stats march 24, 2009

assigning rngs, 363

agent-based modeling, 178
agents

animation, 177
anonymous structures, 353
anova, 224, 226, 312,

albee (1960), xii
allison (2002), 105, 346
amemiya (1981), 287
amemiya (1994), 310, 336
analysis of variance, 312

and, see  
a   _...
f_ e  , 311, 312
 v, 278
a  va, 226
a  ay_  _ a  ix, 125
a  ay_  _ve
   , 125
be a_f   _ ea _va ,

     ab_  _db,

comparison to ols, 316
for description, 224   227
for testing, 312   315

316, 419, 420

ansi, 419

358

229, 231

index

da a_a   
, 120
da a_
  y, 122
da a_
   e a i  ,
da a_fi  , 310
da a_ge ..., 110, 121
da a_ i  wi e_	
de e e,
da a_ e 
 y, 125
da a_  i  , 99, 100,
da a_   ..., 121
da a_   , 359
da a_ e ..., 121
da a_ h w, 99, 231
da a_    , 233
da a_  a
k, 282
da a_ 	  a ize, 232
da a_  _d	  ie , 110,
da a, 104, 120   130, 232
db_ e ge_ ab e, 102
db_ e ge, 102
db_  g_i i , 84
db_  _
     ab, 81,

111, 123, 281, 283

347

126

100   102

100   102

  , 136

! (c), 20
	> (c), 60
<, 395
< (c), 20
= (c), 19
== (c), 20
> (c), 20
>fi e a e, 394
>>, 395
#defi e, 116
#if def, seeif def
#i 
 	de, seei 
 	de
  (c), 19
   (c), 20
 , 11
|| (c), 20
0x, 54

out of regex brackets
(head of line), 407

in regex brackets
(negation), 404

   (caret)

&, 57

abelson et al. (1985), xi
af   ne projection, 280, 420

444

323

323

323

273, 323

150, 345

de _a d_i v, 134
d  , 129, 267
d aw, 359
e  i a e_ e  a  ,
e  i a e, 150, 257,
ex   e  ia , 374
ga  a, 374
hi   g a  _ e  _	
g  d e  _ f_fi ,
hi   g a _  de _	
 e e ,
hi   g a _ve
   _	
 e e ,
hi   g a , 323
ja
kk ife_
 v, 131
 i e_  _da a, 125, 353
 i e_  _ a  ix, 125
 i ea _
     ai  ,
 i ea _
     ai  ,
  gi , 284
   k	 , 252, 304
 a  ix_..._a  , 119
 a  ix_..., 119
 a  ix_de e  i a  ,
 a  ix_i ve  e, 134
 a  ix_ a _a  _ 	 ,
 a  ix_ a , 100
 a  ix_    a ize, 274
 a  ix_ 
a, 269
 a  ix_  i  , 100,
 a  ix_ 	  a ize, 231
 a  ix_  _da a, 120,
 axi 	 _ ike ih  d,
 e ge_db , 98
  de , 32, 143 ff, 273,

325, 337   340

126, 167, 168

152, 353

151

134

119

125

337, 339, 359

gsl_stats march 24, 2009

127

267

285

120, 127

speci   c options

 	  i   ia _   bi ,
    a ize_f  _ vd,
   , 279
  e _db, 98
    , seea   _     for
 ai ed_ _ e  , 309
    _hi   g a , 358
    _hi   g a , 172
    _  , 320
    _ 	e y, 98, 417
 	e y_  _da a, 99,
 	e y_  _f  a , 99
 	e y_  _ a  ix, 99,
 	e y_  _ ixed_da a,
 	e y_  _ ex , 93, 99,
 	e y_  _ve
   , 99,
 	e y, 98, 127
  g_a   
, 252, 357
 y  e , 125, 397
 _ e  , 110
 ab e_exi   , 85, 108
 e  _a  va_	
i de e de 
e,
 e  _fi he _exa
 ,
 e  _k    g   v, 323
 ex _  _da a, 125
 ex _  _db, 85, 98,
	 da e, 259, 361, 373,
ve
   _..., 119
ve
   _a   y, 119
ve
   _
   e a i  ,
ve
   _di  a 
e, 150
ve
   _ex , 83
ve
   _g id_	
di  a 
e,

107, 125, 206, 417

104

121

127

313

314

374

231

231

125

231

233

262

365

150

index

ve
   _k	    i , 230,
ve
   _  g10, 117, 174
ve
   _  g, 83, 117
ve
   _ a , 119
ve
   _ ea , 231
ve
   _  vi g_	
ave age,
ve
   _ e 
e  i e ,
ve
   _  i  , 126
ve
   _ kew, 83, 230,
ve
   _  _da a, 120,
ve
   _  _ a  ix, 125
ve
   _va , 83, 230,
ve
   _weigh ed_	
 ea ,
ve
   _weigh ed_va ,
w  , 278
zi f, 149
a   _c  , 142
a   _db_e g  e, 106
a   _ a  ix_  w, 114
a   _    ....
db_ a e_
  	  , 100,
db_ a , 108
 	  	 _a  e d, 167
 	  	 _ i e, 126, 169
 	  	 _ y e, 126
 h ead_
 	  , 119
a   _r w, 142
a   _sub atr x, 142
a g
, 206
a g , 208
a gv, 206
a i h e i
ex
e  i  
 
  ed	  ed , 135

apophenia, 1, 420
arbitrary precision, 138

arguments, 36, 420

232

232

120

gsl_stats march 24, 2009

bfgs algorithm, see

broyden-fletcher-
goldfarb-shanno
conjugate gradient
algorithm
bias, 220, 421
binary tree, 200, 421
binomial distribution, 237,

g  _ a _bi   ia  _	
 df ,

260, 331, 358

238

birthday paradox, 23
blas, 419
block scope, 41
blue, 221, 419, 421
bonferroni correction, 319
boolean expressions, 20
bootstrap, 367, 421
bootstrap principle, 296,

421

bourne shell, 382
bowman & shenton

(1975), 320

bracket expressions

in regexes, 404

bre, 403, 419
breaking via <ctrl-c>, 61,

goldfarb   shanno
conjugate gradient
algorithm, 341

broyden   fletcher   

buffer, 169

c keywords

363

!, 20	>, 60
<, 20==, 20
=, 19>, 20 , 19  , 20

ha , 29

    , 65
d 	b e, 29, 135
d , 23

445

357

e  e, 21
ex e  , 51
f  a , 29, 135
f  , 23, 118
f ee, 58
if def, 354
if, 21, 211
i 
 	de, 49   50, 385
i  , 29
   g, 29
  a i
, 39, 147, 153,
   	
 , 31, 60
 y edef, 31, 191
	 i  , 121
v id, 36
whi e, 23
||, 20

a   
, 57

a e (sql), 110

a   (sql), 78

a  (posix), 399, 400

d (posix), 399

ha  (c), 29

cellular automaton, 178
central limit theorem,

cumulative density
function

inequality, 229, 333,
421

c shell, 382
c++, 42
call-by-address, 54, 421
call-by-value, 39, 54, 421

casting, see type casting

cauchy distribution, 303,

casella & berger (1990),

carriage return, 61, 418

cdf, 236, 419, see

cauchy   schwarz

295, 297, 422

221, 331, 334

causality, 271

365

central moments, 230, 422
chamberlin (1890), 317

chebychev   s inequality,

221

221

421

index

assertion, 421

avriel (2003), 150, 340

asymptotic ef   ciency, 221

bandwidth, 261, 262, 376,

axtell (2006), 252
aymptotic unbiasedness,

arithmetic in c, 19
array, 30, 59, 420
arrays, 125
ascii, 419

a   i  f, 67, 111
a  e  , 71
assignment, see=
a  f, 29, 208
a  i, 208
a   , 208
awk (posix), 403
ba h (posix), 187, 382
begi  (sql), 84
g  _ a _	
be   	  i _ df ,
g  _
df_be a_   	 ,
g  _ a _be a _ df ,
be wee  (sql), 79

benford   s law, 173
benford (1938), 173
bernoulli, 327
bernoulli distribution, 237,

baum et al. (2008), 93, 376
bayes   s rule, 258, 336
bayesian, 330
bayesian updating, 144,

bar chart, 164
barron & sheu (1991), 331
bash, 41, 382

beta distribution, 249, 259,

bernoulli draw, 237, 260,

beta function, 249

258, 372

331, 358

421

358

237

249

249

446

cheng & long (2007), 287
  2 distribution, 301, 358

g  _
df_
hi  _ , 305
g  _
df_
hi  _ i v,

h  d (posix), 160
chooseg  _ f_
h   e, 238

goodness-of-   t, 321
scaling, 314

  2 test, 309

306

chung & fraser (1958),

375

chwe (2001), 262
cleveland & mcgill

(1985), 180

closed-form expression,

422

color, 180

determination, 228

codd (1970), 95
coef   cient of

command-line utilities, 98
combinatorial optimization,

clt, 296, 419
id91, 289   291
cmf, 236, 419, see
cumulative mass
function


  	   (posix), 399, 402

   i  (sql), 84

in c, 25   26
in gnuplot, 160
in sql, 78

commenting out, 25
comments

see posix commands

command-line programs,

arguments on, 203

command line

338

comparative statics, 152
compilation, 48, 51
compiler, 18, 422
id155, 258
conditionals, 20
con   guration    les, 383

gsl_stats march 24, 2009

151

422

422

374, 422

conjugate distributions,

constrained optimization,

consistent test, 335, 422

conjugate gradient, 341
conover (1980), 323, 376
consistent estimator, 221,

contour plot, 162
contrast, 309, 422
conway, john, 178
cook   s distance, 131
cook (1977), 131
correlation coef   cient, 229,


     (c), 65

  (posix), 399

 ea e (sql), 84

 h
redirecting  de  , 396

 h (posix), 382

	  (posix), 399, 401

counting words, 401
covariance, 228, 422
cox (1962), 354

cropper et al. (1993), 283
crosstab, 101, 423

cram  r   rao lower bound,

cumulative mass function,

cram  r   rao inequality,

current directory attack,

221, 229, 333, 423

function, 236, 423

cumulative density

236, 423

335

385

cvs, see subversion

data

conditioning, 139
format, 75, 147

data mining, 316, 423
data snooping, 316, 423
data structures, 193
de finetti, 330
debugger, 43, 423

index

423

descriptive statistics, 1, 423
designated initializers, 32,

dempster et al. (1977), 347
dependency, 423
dereferencing, 43

of functions, 36

353
df, 419

degrees of freedom, 222,

debugging, 43   47
decile, see quantile
declaration, 423

of pointers, 57
of types, 31
of variables, 28   33

ofg  _ a  ix, 114
ofg  _ve
   , 114
de e e (sql), 86
de 
 (sql), 83
diff (posix), 399, 402
di  i 
 , 80, 81
d  (c), 23
d  2	 ix (posix), 418
d 	b e (c), 29, 135
d xyge  (posix), 185
d    (sql), 85, 86
ed (posix), 403, 404
eg e  (posix), 403, 406

ef   ciency, 220, 334, 424
efron & hinkley (1978),

discards quali   ers from

discrete data, 123

dot    les, 383
dot product, 129
dot, graphing program, 182

dummy variable, 110   111,
123, 281   283, 316, 424

efron & tibshirani (1993),

pointer target type, 201

13, 349

e, 136

231

eigenvectors, 267
einstein, albert, 4
electric fence, 214
eliason (1993), xi

247

178

403

424

index

excess variance, 239

epstein & axtell (1996),

environment variable, 381,

expected value, 221, 424
exponential distribution,

ere, 403, 419
erf, 284, 419
error function, 284, 424
euler   s constant, 136

e  e (c), 21
e acs (posix), 387, 402,
e v (posix), 381, 382
eve y (gnuplot), 175
ex
e   (sql), 94
exi , 166
g  _
df_	
ex   e  ia _   	 ,
g  _ a _	
ex   e  ia  _ df ,
ex     (posix), 382, 424
ex e   (c), 51
g  _
df_fdi  _ , 305
a   _f_ e  , 310
f
   e, 169
ff 	 h, 61, 169
fge  , 203, 394
fi d (posix), 386

fein et al. (1988), 254
feller (1966), 229
fermat   s polygonal number

fibonacci sequence, 30
   les

extreme value distribution,

f distribution, 304, 358

exponential family, 349

factor analysis, 265

theorem, 47

hidden, 383

f test, 309

247

247

284

gsl_stats march 24, 2009

fisher, ra, 308

fortran, 10

on likelihood, 329

uniform distribution

frame, 37   39, 54, 424

   at distribution, 358
fletcher   reeves conjugate
gradient algorithm, 341

fisher (1922), 313
fisher (1934), 329
fisher (1956), 308
flat distribution, see

freedman (1983), 316
frequentist, 329
friedl (2002), 403
frisch (1995), 385

f  a  (c), 29, 135
f  e , 167, 394
f   (c), 23, 118
f   	 e (posix), 397
f  i  f, 70, 166, 396
f ee (c), 58
f    (sql), 77
g_key_fi e_ge ..., 205
g  _
df_ga  a_   	 ,
g  _ a _ga  a _ df ,
g

, 18, 216
g

 (posix), 41, 214
gdb, see debugging
gdb (posix), 44, 387

gamma function, 244, 246
gardner (1983), 178
gaussian distribution, 358,
see normal distribution

fry & harris (1996), 287
full outer join, 106
function pointers, 190

game of life, 178
gamma distribution, 144,

gcc, 48, 419, 424

246, 331

246

246

gdp, 419
gelman & hill (2007), xi,

95, 290, 292

fisher exact test, 8, 314

gelman et al. (1995), 373

ge e v, 384
ge  , 394

277, 424

145

447

generalized least squares,

gentleman & ihaka (2000),

gibbard (2003), 295
gibrat (1931), 252
gif, 177
gigerenzer (2004), 308
gill et al. (1981), 340
givens & hoeting (2005),

261

glaeser et al. (1996), 225
glib, 65, 193
global information, 325
global variables

initializing, 29, 211

generalized least
squares

gnu, 419
gnu scienti   c library, 7,

comments, 160

gnuplot keywords

113
gnuplot

157   180, 417
comments, 160

globbing, 407, 424
gls, 277, 419, see

g 	     (posix),
eve y, 175
    , 160
 e    , 160, 170
 e e , 168
     , 161
key, 165
 	 , 159
  3d, 161, 165
 e  , 159
 i  e, 164
x abe , 164
x i
 , 165
y abe , 164
y i
 , 165

gnuplot settings

goldberg (1991), 139
golden ratio, 30

gsl_stats march 24, 2009

140

149

117

 a  ix_ 	 _e e e   ,
 a  ix_   , 122, 359
 a  ix_  w, 142
 a  ix_ 
a e, 117
 a  ix_ e _a  , 114
 a  ix_ e _
  , 114
 a  ix_ e _  w, 114
 a  ix_ e , 116
 a  ix_ 	b, 117
 a  ix_  a     e_	
 e 
 y,
 a  ix, 99, 114, 120,
  w_2, 132
  w_i  , 132
 a _..., 358
 a _be   	  i _ df ,
 a _be a _ df , 249
 a _bi   ia  _ df ,
 a _ex   e  ia  _	
 df ,
 a _f a  _ df , 251
 a _ga  a _ df , 246
 a _ga	  ia  _ df ,
 a _	hy e ge  e  i
 _	
 df ,
 a _  g    a  _ df ,
 a _ ax, 363
 a _ 	  i   ia  _	
 df ,
 a _ ega ive_	
bi   ia  _ df ,
 a _ ega ive_	
bi   ia ,
 a _  i     _ df ,

237

238

247

241

239

243

240

244

244

448

gonick & smith (1994),

xiii

graphviz

403, 404   408

scienti   c library

281, 286, 315, 346

grid search, 371, 424

gsl, 113, 419, see gnu

   owcharts, 182
nodes, 182

graphviz, 182   185
greene (1990), 256, 271,

good (1972), 357
goodness of    t, 319
gough (2003), 113
graph, 182, 424
graphing, see gnuplot, see

g e  (posix), 382, 395,
g e /eg e  (posix), 399
g  	 by, 81, 91
g  _...
b a _dd  , 130

df_..., 305

df_be a_   	 , 249

df_ex   e  ia _	
   	 ,

df_f a _   	 , 251

df_ga  a_   	 , 246

df_ga	  ia _   	 ,

df_  g    a _   	 ,

df_ ega ive_	
bi   ia _   	 ,
 i a g_  _   ve, 134
 i a g_sv_de
   , 269
 a  ix_add_
    a  ,
 a  ix_add, 117
 a  ix_a   
, 114
 a  ix_div_e e e   ,
 a  ix_f ee, 114
 a  ix_ge , 116
 a  ix_ e 
 y, 125,

247

241

243

244

117

117

132

244

index

117

268

361, 369

  g_e v_ e 	 , 357
  g_	 if   _i  , 297,
  g_	 if   , 251, 358
  g, 357
 f_be a, 249
 f_
h   e, 238
 f_ga  a, 244
    _ve
   _	
 a ge  _i dex,
  a  _va ia 
e, 230
ve
   _add_
    a  ,
ve
   _add, 117
ve
   _div, 117
ve
   _f ee, 114
ve
   _ge , 116
ve
   _ e 
 y, 125
ve
   _ 	 , 117
ve
   _   , 122, 359
ve
   _ 
a e, 117
ve
   _ e , 116
ve
   _    , 233
ve
   _ 	b, 117
ve
   , 119, 120, 140,
gs _ s_eve , 19
gs _ s_ dd, 19
gs _ ax, 212
gs _   , 212
gs _ a , 135
gs _ eg  f, 135
gs _  s  f, 135
havi g (sql), 82
head (posix), 399, 400

gui, 419
guinness brewery, 303
gumbel distribution, 283

haddon (2003), 430
half life, 255
halting via <ctrl-c>, 61,

hash table, 193
hat matrix, 272, 424

142

363

header    le, 49, 425

aggregation, 50

index

switches, see a 

with command-line

variables in, 50

hedonic pricing model, 283
help

within gnuplot, 163

hessian, 341, 425
heteroskedasticity, 277, 425
hidden    les, 383
hierarchical model, see

multilevel model
hipp, d richard, 75
histograms

drawing from, 361   362
plotting, 172
testing with, 321   324

householder
solver, 134
transformations, 280

239

336

181

distribution, 239

how to lie with statistics,

hybrid method, 342
hypergeometric

huber (2000), 7
huff & geis (1954), 181
hunter & schmidt (2004),

g  _ a _hy e ge 	
 e  i
 _ df ,
if (c), 21, 211
if def (c), 354

ide, 18, 419
identical draws, 326
identically distributed, 425
identity matrix, 425
iec, 419

ieee 754    oating-point

standard, 135

standard, 135

iec 60559    oating-point

ieee, 419

iff, 425

iia, 286, 419
iid, 326, 419, 425

gsl_stats march 24, 2009

425

425

indices, 90

likelihood, 347

importance sampling, 371,

imputation, maximum

internal representation,

include path, 49
incrementing, 20
independence of irrelevant
alternatives, 286, 425
independent draws, 326,

i  (sql), 79
i 
 	de (c), 49   50, 385
i dex (sql), 90
  f   ty, 135
i  e   (sql), 84, 86
i   (c), 29
i  e  e
  (sql), 94
i fi i e, 135
i i f, 135
i  a , 135

in   nity, 135
information equality, 332
information matrix, 326,

interaction, 281, 426
internal revenue service

(2007), 117
interpreter, 426

initializers, designated, 32

invariance principle, 351

instrumental variable, 275,

inferential statistics, 1, 425

426

426

200

iso, 420
iv, 275, 420, see

instrumental variables

jackknife, 131, 426
jittering, 175
join, 426

449

kahneman et al. (1982),

kernel density estimate,

222

262, 426

key (gnuplot), 165

64, 193

kernighan & ritchie

(1988), 18, 126, 210

kernighan & pike (1999),

key    les, 204
key value, 200
klemens (2007), 349
kline (1980), 325
kmenta (1986), 271, 277,

320, 370

14

see ordinary least

layers of abstraction, 5

latex, 185
lattices, 171
laumann & derick (2006),

leading digit, 173
least squares

knuth, donald, 73
knuth (1997), 200
kolmogorov (1933), 323
kurtosis, 230, 365, 426

 dex , 137
 e   (posix), 399, 400,
 ike, 79

left outer join, 106
legend, plot, 165
lehmann & stein (1949),

libraries, 6
library, 52, 427
life, game of, 178

leptokurtic, 230, 231, 306

lexicographic order, 91,

squares 227

426

403

375

likelihood function, 326,

427

command-line program,

philosophical

398

database, 87   91

implications, 329
likelihood ratio, 336 ff

450

logit model

335, 351, 427

nested, 286, 291

lognormal distribution

logit, 144, 283   292, 328

likelihood ratio test, 151,

logical expressions, 20
logistic model, 283, see

local information, 325
log likelihood function, 326
log plots, 174

line feed, 61, 418
line numbers, 401
linked list, 198, 427
linker, 51, 427
listwise deletion, 105, 347

 i i  (sql), 83
  ad (sql), 107
  g10, 173, 174
g  _ a _	
  g    a  _ df ,
   g (c), 29
   gd 	b e
  i  f format speci   er,
   (posix), 399
 ai , 40
 ake (posix), 48, 188,
 a   
, 57 ff, 214
 a   c_c ec _, 214
 a  (posix), 399, 400
 a h.h, 52

manhattan metric, 150, 427
mar, 420
marginal change, 285
markov chain monte

macro, 212, 427
maddala (1977), 271, 275

lognormal distribution, 242

blindness of, 317

math library, 52

lr, 351, 420

make, 427

carlo, 372

387   391

love

243

138

gsl_stats march 24, 2009

matrices

223, 427

median, 233   234

tracing the path, 340

maximum likelihood

maximum likelihood
estimation, 325 ff

determinants, 134
dot product, 129
inversion, 134
views, 128

memory debugger, 214
memory leak, 62, 428
mesokurtic, 231
metadata, 128, 427

mcar, 420
mcfadden (1973), 284
mcfadden (1978), 292
mcmc, 420
mean squared error, 220,

max, seegs _ ax
 e 
 y, 124
 e   ve, 124, 198
min, seegs _   
 kdi  (posix), 399
modulo, see 
   e (posix), 399, 400

missing completely at
random, 346, 428

ml, 420
id113, 325, 420, see

maximum likelihood
estimation

missing at random, 346,

metastudies, 260
method of moments, 256
metropolis   hastings, 372

moments, 229
monte carlo method, 356,

moment generating

in the database, 86

mnar, 420

functions, xii

428

428

428

missing data, 104, 105, 345
missing not at random, 346,

index

240

240

257, 316   319

multinomial distribution

multinomial logit, 284
multiple testing problem,

multivariate normal
distribution, 347
multivariate normal

mse, 420
multicollinearity, 275, 428
multilevel model, 288 ff
multilevel models, 288
multinomial distribution,

g  _ a _	
 	  i   ia  _ df ,
 v (posix), 399
 y    h w, 107
 a , 104, 135, see not a
g  _
df_ ega ive_	
bi   ia _   	 ,
g  _ a _ ega ive_	
bi   ia  _ df ,

nabokov (1962), 1
naming functions, 115

national election studies

distribution, 144, 242

negative binomial

mysql, 75, 106

distribution, 244

(2000), 286

number

244

244

negative de   nite, 269
negative exponential
distribution, 248

negative semide   nite, 269
nelder   mead simplex

algorithm, 341

nested logit, 291
network analysis, 147
networks

graphing, 183

newton   s method, 342
neyman & pearson

(1928a), 335

gsl_stats march 24, 2009

351

outliers

(1998), 338

spotting, 134

over   ow error, 137, 429

parameter    les, 204
partitioned matrices, 122

pairwise deletion, 348
papadimitriou & steiglitz

path, 385, 429
paulos (1988), 23
pawitan (2001), xi, 329,

 	 e j i  (sql), 106
 a  e (posix), 399, 401
 
   e, 396
 e   (posix), 399, 408,
     (gnuplot), 160
  3d (gnuplot), 161, 165

perl (2000), 271
permutation test, 375
  , 136
pierce (1980), 193
pipe, 167, 395, 429
pivot table, 101, 429
platykurtic, 230, 231
plot, 429

pearson correlation
coef   cient, 229
pearson (1900), 301
peek et al. (2002), 398
penalty function, 150
percentile, see quantile

pcre, 403, 420
pdf, 236, 420, see

id203 density
function

plotting, see gnuplot

pca, 265, 420

408   418

pmf, 236, 420, see

id203 mass
function

poincar   (1913), 264
pointer, 53 ff, 429
declaration, 57
function, 190
null, 43

305

241

241

428

index

335, 350

346, 428

non-ignorable missingness,

(1928b), 335

241, 301, 331

variance of, 370

neyman & pearson

normal distribution, 144,

neyman   pearson lemma,

non-parametric, 428
noncentral moment, 230,

   (posix), 399, 401
g  _
df_ga	  ia _	
   	 ,
g  _ a _ga	  ia  _	
 df ,
g  _
df_ga	  ia _ ,
 	   (sql), 97, 105
 ff e , 83
or, see||
  de by (sql), 83
 	  (gnuplot), 159

order statistic, 250, 429
order statistics, 318
ordinary least squares, 2,

object, 428
object    le, 51, 429
object-oriented

not, see !
not a number, 104, 135

144, 264, 274   275,
315, 429

optimization, 216
constrained, 150

null pointer, 43, 428

tests for, 319, 323, 370

programming, 42, 121

ols, 270, 420, see

orwell (1949), 42

normality

ordinary least squares

227

decomposing its variance,

451

gradient algorithm, 341

265

244

245, 246, 331

polak   ribiere conjugate

poisson distribution, 144,

polhill et al. (2005), 139
poole & rosenthal (1985),

positive de   nite, 269
positive semide   nite, 269
posix, 381, 429
posix commands

g  _ a _  i     _	
 df ,
   e , 167, 394, 396, 429
e acs, 387, 402, 403
awk, 403
ba h, 187, 382

a , 399, 400

d, 399

h  d, 160

  	  , 399, 402

 , 399

 h, 382

	 , 399, 401
diff, 399, 402
d  2	 ix, 418
d xyge , 185
ed, 403, 404
eg e , 403, 406
e v, 381, 382
ex    , 382, 424
fi d, 386
f   	 e, 397
g

, 41, 214
gdb, 44, 387
g 	    , 157   180, 417
g e /eg e , 399
g e , 382, 395, 403,
head, 399, 400
 e  , 399, 400, 403
  , 399
 ake, 48, 188, 387   391
 a , 399, 400
 kdi , 399
   e, 399, 400
 v, 399

404   408

452

408   418

  , 399, 401
 a  e, 399, 401
 e  , 399, 408, 408   418
  2 df, 160
  di , 399
  , 399
 ed, 394, 399, 400, 403,
 e e v, 424
    , 399, 400
 ai , 399, 400
  	
h, 389
  , 407
	 i , 399, 402
	 ix2d  , 418
vi , 402
vi, 387, 403
w
, 399, 401
  w, 23, 132
  i  f, 28, 137, 138
printing, see  i  f
  2 df (posix), 160

power, 306, 335, 430
precision, numerical, 136
preprocessor, 49   50, 213
press et al. (1988), xiii,

price & stern (1988), 26
prime numbers, 61, 430
principal component

prisoner   s dilemma, 194
prng, 357, 420
id203 density

probit model, 284
pro   ler, 215, 430
programs, see posix

posterior distribution, 258
postscript, 159, 185,

projection matrix, 272, 430

analysis, 265, 275, 430

id203 mass function,

probit, 144, 283   292,

function, 236, 430

commands

236, 430

187   188

328   329

340, 341

gsl_stats march 24, 2009

pseudorandom number

430

430

python, 11, 403

generator, 430

random numbers, 357   364

random number generator,

regression, 315
id157, 403 ff,

bracket expressions, 404
case sensitivity, 405
white space, 407

r2, 311
ramsey, 330

from sql, 84
ranks, 147, 253

q   q plot, 319
quantile, 232, 319
query, 74, 430

 a d, 84, 363
 a d   (sql), 84
 ea   
, 196
 e     (gnuplot), 160,
 e e  (gnuplot), 168
 i  , 33
   (posix), 399
  di  (posix), 399
  	 d, 81
  wid, 401
  wid (sql), 85
 akefi e. ex, 188
r i efi he , 9
age  g id.g 	    ,
a   gwi hi .
, 225

ruby, 11
rumi (2004), 74

s  rndal et al. (1992), 232
sample code

revision control, see

right outer join, 106

rng, 357, 420

rounding, 33

subversion

170

161

index

154

a gv.
, 207
a  ayf  
k.
, 197
bdayf  .
, 35
bday   	
 .
, 32
bi  da i y, 378
bi d .
, 195
bi  hday.
, 24

a  byadd.
, 56

a  byva .
, 38

a dida e .
, 292

  de  .
, 298

  k .
, 133

   	  .
, 348
da ab   .
, 368
d awbe a.
, 359
d awf      .
, 361
d	  ie .
, 282
e
  101.a a y i
.
,
e
  101.
, 153
e
  101. ai .
, 155
eige b x.
, 267
eige ea y.
, 269
eige ha d.
, 268
e v.
, 384
e    ba  .
, 173
fi he .
, 314
f  w.
, 22
f   	 a e.
, 397
f e  .
, 311
f	zz.
, 140
ge    .
, 209
ge    i g.
, 204
gkey .
, 205
g ib.
  fig, 205
g  dfi .
, 322
ja
ki e a i  .
, 132
ji  e .
, 175
 a  i
e.
, 171
 ife.
, 179
 i  f  
k.
, 199
  
a  ax.
, 339
      e  .
, 354
   e  .
, 352
 a i.
, 254
 a k v.
, 129
 e   a  va.
, 226

114

index

 	  i  i
a i   ab e.
,
 ew   .
, 146
    a b   .
, 369
    a g  w h.
, 253
    a   .
, 151
    a  ab e.
, 252
   a 	 be .
, 135
  eb   .
, 368
 i e    .
, 168
    af	 
 i  .
, 191
  we   f w .
, 138
  i e .
, 62
  i e 2.
, 63
   bi  eve  .
, 293
   je
 i  .
, 273
   je
 i   w .
, 279
      .
, 320
 ide  hi .
, 289
 e fexe
	 e.
, 170
 i   e he  .
, 40
 i   .
, 338
     hi g.
, 263
  	a e .
, 59
  a ed	  ie .
, 112
 axe .
, 118
 di  k	    i .
, 366
 i e.
, 363
 i efi he .
, 9
  eef  
k.
, 201
  e  .
, 111
  e  .   g.
, 110
wb  da a.
, 100
 
a f, 203
 ed (posix), 394, 399,

scheff   (1959), 312, 420
scienti   c notation, 136
scope, 41   42, 430

sample distributions, 235
sampling

from an arti   cial
population, 361

score, 326, 431

savage, 330

global, 50

400, 403, 408   418

seed, 357, 431

gsl_stats march 24, 2009

segmentation fault, 43, 214,

373

265

431

settings

singular value

segmentation fault

segfault, 43, 431, see

shell, 393, 431
shepard & cooper (1992),

silverman (1981), 377
silverman (1985), 263
simulated annealing, 343,

decomposition, 265,
431

 e e
  (sql), 77 ff
 e e v, 384
 e e v (posix), 424
fora   _  de  , 339
s g  t, 61
 ize f, 125, 300
    i  f, 67
     (posix), 399, 400
sorting    _ve
   _	
 a ge  _i dex,
ofg  _ve
   s and
a   _da as, 233
      (gnuplot), 161
   i  f, 67
  ad, 107
begi , 84

sql, 6, 74, 420
comments, 78
sql keywords

source code, 431
spectral decomposition,

skew, 230, 431
slutsky theorem, 364
smith & reynolds (2005),

snedecor & cochran

database output, 83

(1976), 301

288

268

265

89

snow   ake problem, 4, 270,

453

be wee , 79

a e, 110

a  , 78

   i , 84

 ea e, 84
de e e, 86
de 
, 83
d   , 85, 86
ex
e  , 94
f   , 77
havi g, 82
i dex, 90
i  e  , 84, 86
i  e  e
 , 94
i , 79
 i i , 83
 	  , 97, 105
  de by, 83
 	 e j i , 106
 a d  , 84
  wid, 85
 e e
 , 77 ff
	 i  a  , 94
	 i  , 94
	 da e, 87
whe e, 78
   i e_ a  e , 86
    , 23, 52
  a d, 363
  a i
 (c), 39, 147, 153,
  de  , 70, 267, 394, 396
  di , 394
  d 	 , 215, 267, 394

sse, 227, 311, 420
ssr, 227, 311, 420
sst, 227, 420
stack, 38, 44, 431
stallman et al. (2002), 44
standard deviation, 222,

statistic, 219, 432
statistics packages

standard error, 367, 432

rants regarding, 8   11

initializing, 29, 211

static variables, 39

sqlite, 75

431

357

363

454

anonymous, 353

stroustrup (1986), 42

stravinsky (1942), 113, 123

stopping via <ctrl-c>, 61,

structural equation
modeling, 271

stride, 142
string, 432
strings, 65 ff

   
  , 68
   
 y, 67
g ib library, 193
    e , 66
    
a , 66
    
 y, 66
   	
  (c), 31, 60
structure, 432, see   	
 
 y  e , 125, 397
g  _
df_ di  _ , 305
g  _
df_ di  _ i v,
a   _ ai ed_ _ e  ,
a   _ _ e  , 308
 ai  (posix), 399, 400

student, 231, 303
student   s t distribution, 230
student (1927), 231
subjectivist, 330
subqueries, 91
subversion, 214   215
surface plots, 161
svd, 265, 420
switches, 208, 432
syntax error, 19

t distribution, 365
t distribution, 302, 358, 365

t test, 109   110, 308   309

language, 74, 432

structured query

306

308

gsl_stats march 24, 2009

tla, 420

taylor expansion, 350

train (2003), 371
transition matrix, 129
transposition, see

test functions, 72
tex, 185
thomson (2001), 7
threading, 119, 432

 e   (gnuplot), 159
 i e, 362
 i  e (gnuplot), 164
  	
h (posix), 389
   (posix), 407
g  _ a  ix_	
  a     e_ e 
 y
 y edef (c), 31, 191
g  _
df_f a _   	 ,
g  _ a _f a  _ df ,
	 i   (c), 121
	 i   (sql), 94
	 i  a   (sql), 94
	 i  (posix), 399, 402

tree, see binary tree
trimean, 234, 432
tukey (1977), 157, 234,

type, 27, 432
type casting, 33   34, 432
type i error, 335, 335, 432
type ii error, 335, 335, 432

unbiased statistic, 220, 432
under   ow error, 137, 433
uniform distribution, 250,

unbiased estimator, 335,

in dot products, 129

251, 358

433

432

251

251

united states of america

index

181

views, 128

unix, 420, 433

national debt and de   cit,

utility maximization, 152
utils, 193

value-at-risk, 306
variance, 222, 228, 433

	 ix2d   (posix), 418
	 da e (sql), 87
va
		 , 108
vi (posix), 387, 403
vi  (posix), 402
v id (c), 36
w
 (posix), 399, 401
whe e (sql), 78
whi
h, 168
whi e (c), 23
x abe  (gnuplot), 164
x i
  (gnuplot), 165
y abe  (gnuplot), 164
y i
  (gnuplot), 165

void pointers, 199
vuong (1989), 354

william s gosset, 231, 303
windows, 381
wls, 277, 420
wolfram (2003), 115
word count, 401

weighted least squares,

144, 277, 433

z distribution, 308
z test, 307
zipf distribution, 144
zipf   s law, 252

