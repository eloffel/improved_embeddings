learning at scale:

deep, distributed and multi-dimensional

anima anandkumar

..

amazon ai & caltech

deep learning

significantly improve many applications on multiple domains

image understanding

id103

natural language 

processing

autonomy

   

   deep learning    trend in the past 10 years

image classification

multilevel feature extractions from raw pixels 

to semantic meanings

layer 1 layer 2 output

explore spatial information with convolution layers

image classification

state-of-the-art networks have tens to hundreds layers

   hard to define the network

   the definition of the inception network has >1k lines of codes in caffe

   a single image requires billions floating-point operations

  

  

intel i7 ~500 gflops 
nvidia titan x: ~5 tflops

   memory consumption is linear with number of layers

outline

1

introduction

2 distributed deep learning using mxnet

3 learning in multiple dimensions

4 conclusion

3. mxnet

    imperative and declarative programming
    language support
    backend and automatic parallelization

image credit - wikipedia

ca   e

resnet-101-deploy.prototxt

    protobuf as the interface 
    portable 
    ca   e binary + protobuf model 
    reading and writing protobuf are 

not straightforward

   .  

(4k lines of codes)

3

tensor   ow

implement adam 

> 300 lines of codes

    a rich set of operators (~2000) 
    the codes are not very easy to 

read, e.g. not python-like

4

keras

    simple and easy to use  
    di   cult to implement 

sophisticated algorithms

5

pytorch & chainer

    flexible 
    complicate programs might 

be slow to run

6

mxnet

implement resnet

implement adam

    symbolic on network de   nition 
    imperative on tensor computation 
    huh.., not good enough

7

imperative
symbolic

mxnet

gluon

before

2012

2013

2014

2015

2016

2017

8

gluon at a glance

net.hybridize()    

converts from 

imperative to symbolic 

execution

9

front-end

back-end

back-end system

import mxnet as mx 
a = mx.nd.zeros((100, 50)) 
b = mx.nd.ones((100, 50)) 
c = a * b 
c += 1

import mxnet as mx
net = mx.symbol.variable('data') 
net = mx.symbol.fullyconnected(
         data=net, num_hidden=128)
net = mx.symbol.softmaxoutput(data=net)
texec = mx.module.module(net)
texec.forward(data=c)
texec.backward()

a

   

b

+

1

c

fullc

softmax

weight

bias

    optimization 
    memory optimization 
    operator fusion 
    scheduling 
    auto-parallelization

11

memory optimization

traverse the computation graph to reduce the memory footprint    
with time complexity linear in graph size

aliveness analysis

a

d

c

b
now a is  
deletable

shared space between 

variables

a

b

c

share a and b

12

results for deep id98s
winner neural networks

training
baseline

mxnet

1.8x

2.6x

1.8x

alexnet inception

vgg

)
b
g

(
 
y
r
o
m
e
m

9
6.75
4.5
2.25
0

)

b
g

(
 
y
r
o
m
e
m

9
6.75
4.5
2.25
0

prediction

baseline

mxnet

3.2x

4.4x

4x

alexnet inception

vgg

13

trade computation for memory

forward

backword

forward

backward

backward

segment 1

segment 2

only the 
segment 
head node 
results are 
stored  

re-

compute 
results 

re-

compute 
results 

    needs an extra forward pass 
    reduces the memory complexity from           to               ,    
o(n) o(pn)
where n is the number of layers

14

results on resnet

no optimization

with optimization

    batch size = 32 
    increase 30% 

computation cost 
when optimization is 
applied

)
b
g

(
 
y
r
o
m
e
m

100

10

1
100

157.4 gb

4.1 gb

1000

# of layers

15

operator fusion and runtime compilation

add

mul

x2

x0

x1

fusion

codegen

fused 
op

x0

x1

x2

fuse adam into a single operator

20% performance 

improvement on resnet

16

writing parallel programs is painful 
dependency graph for 2-layer neural 

networks with 2 gpus

each forward-backward-update 
involves o(num_layer), which is 

often 100   1,000, tensor 

computations and communications

data[gpu0].copyfrom(data[0:50])

data = next_batch()

data[gpu0].copyfrom(data[51:100])

fc1[gpu0] = fullcforward(data[gpu0], 

fc1_weight[gpu0])

fc2_wgrad[cpu]  = 

fc2_wgrad[gpu0] + fc2_wgrad[gpu1]

fc2[gpu0] = fullcforward(fc1[gpu0], 

fc2_weight[gpu0])

fc2_ograd[gpu0] = lossgrad(fc2[gpu0], 

label[0:50])

fc1_ograd[gpu0], fc2_wgrad[gpu0] = 

fullcbackward(fc2_ograd[gpu0] , 

fc2_weight[gpu0])

_, fc1_wgrad[gpu0] = 

fullcbackward(fc1_ograd[gpu0] , 

fc1_weight[gpu0])

fc2_weight[cpu] -= 
lr*fc12_wgrad[gpu0] 

fc2_weight[cpu].copyto(
fc2_weight[gpu0] , 
fc2_weight[gpu1])

fc1_wgrad[cpu]  = 

fc1_wgrad[gpu0] + fc1_wgrad[gpu1]

fc1_weight[cpu] -= lr *  

fc1_wgrad[gpu0] 

fc1_weight[cpu].copyto(
fc1_weight[gpu0] , 
fc1_weight[gpu1])

fc1[gpu1] = fullcforward(data[gpu1], 

fc1_weight[gpu1])

fc2[gpu1] = fullcforward(fc1[gpu1], 

fc2_weight[gpu1])

fc2_ograd[gpu1] = 

lossgrad(fc2[gpu1], label[51:100])

fc1_ograd[gpu1], fc2_wgrad[gpu1] = 

fullcbackward(fc2_ograd[gpu1] , 

fc2_weight[gpu1])

_, fc1_wgrad[gpu1] = 

fullcbackward(fc1_ograd[gpu1] , 

fc1_weight[gpu1])

auto parallelization

write serial programs
>>> import mxnet as mx 
>>> a = mx.nd.ones((2,2)) *2 
>>> c = a + 2 
>>> b = a + 1  
>>> d = b * c
>>> d.wait_to_read()

run in parallel

a = 2

c = a + 2

b = a + 1

d = b     c

18

data parallelism

key-value store

1. read a data partition 
2. pull the parameters 
3. compute the gradient 
4. push the gradient 
5. update the parameters

examples

19

distributed computing

a user does not need 
to change the codes 
when using multiple 

machines

key-value store

multiple  

server machines

push and pull    
over network

multiple  

worker machines

read over network

examples
examples

store data in  

a distributed    lesystem

20

scale to multiple gpu machines

hierarchical parameter server

1.25 gb/s  
10 gbit ethernet

15.75 gb/s  
pcie 3.0 16x

63 gb/s  
4 pcie 3.0 16x

network switch

cpu

pcie switch

g
p
u

g
p
u

g
p
u

g
p
u

cpus

gpus

level-2 servers

level-1 servers

workers

21

experiment setup

      
    1.2 million images with 1000 classes 
    resnet 152-layer model 
    ec2 p2.16xlarge

    minibatch sgd 
    synchronized updating 

cpu

pcie switches

gpu 0-15

22

scalability over multiple machines

1

0.75

0.5

0.25

h
t
a
b
 
/
 
)
c
e
s
(
 
e
m

i
t

0

0

115x

comm cost
batch size/gpu=2
batch size/gpu=4
batch size/gpu=8
batch size/gpu=16

32

64

96

128

# of gpus

23

    increase learning 

rate by 5x 

    increase learning 

rate by 10x, decrease 
it at epoch 50, 80

convergence

0.8

0.625

0.45

0.275

 

y
c
a
r
u
c
c
a
n
o
i
t
a
d
i
l
a
v
 
1
-
p
o
t

0.1

0

24

batch size=256
batch size=2,560
batch size=5,120

30

60

90

120

# of epochs

time to achieve 22.5% top-1 accuracy

8 gpus

80 gpus

160 gpus

9.6x

18.8x

~ 1 week

~ 1 day

~ half day

0

50 100 150 200 250

hour

25

    symbolic 
    e   cient & portable 
    but hard to use

in summary
    gluon 
    imperative  for developing 
    symbolic for deploying

    imperative 
       exible  
    may be slow

    tesla

10

deep learning any way you want on aws

amazon machine image for
deep learning
http://bit.ly/deepami

getting started with deep learning

    for data scientists and developers
    setting up a dl system takes (install) time & skill

    keep packages up to date and compile
    install all dependencies (licenses, compilers, drivers, etc.)
    nvidia drivers for g2 and p2 servers
    intel mkl linked for everything else (c5 coming soon)

http://bit.ly/deepami

outline

1

introduction

2 distributed deep learning using mxnet

3 learning in multiple dimensions

4 conclusion

tensors: beyond 2d world

modern data is inherently multi-dimensional

tensors: beyond 2d world

modern data is inherently multi-dimensional

inputhidden 1hidden 2outputtensor contraction

extends the notion of matrix product

matrix product
m v = x
vjmj

j

=

+

tensor contraction

t (u, v,   ) = x

uivj ti,j,:

i,j

=

+

+

+

tensor decompositions

tensor sketches

dimensionality  reduction through 
sketching.

    complexity independent of tensor order:

exponential gain!

+1

+1

-1

sketch s

tensor t

tensor sketches

randomized id84
through sketching.

    complexity independent of tensor order:

exponential gain!

+1

+1

-1

sketch s

tensor t

applications

tensor decomposition via sketching by  wang, tung, smola, a.  nips   15.
compact tensor pooling for visual question and answering by shi, anubhai, 
furlanello, a,  cvpr  2017 vqa  workshop. 

tensor sketches

randomized id84
through sketching.

    complexity independent of tensor order:

exponential gain!

+1

+1

-1

sketch s

tensor t

applications

tensor decomposition via sketching by  wang, tung, smola, a.  nips   15.
compact tensor pooling for visual question and answering by shi, anubhai, 
furlanello, a,  cvpr  2017 vqa  workshop. 

id98

h

c

w

mct

a
v
g
p
o
o

l
i

n
g

f
c

r
e
u

l

b
a
t
c
h
n
o
r
m

f
c

s
o
f
t

m
a
x

"banana"

      what is the 
mustach made of?

id56

l

employing tensor contractions in alexnet

replace fully connected layer with tensor contraction layer

enabling tensor contraction layer in mxnet

performance	of	the	tcl

    trained	end-to-end

    on	id163	with	vgg:	

    65.9%	space	savings
    performance	drop	of	0.6%	only

    on	id163	with	alexnet:		

    56.6%	space	savings
    performance	improvement	of	0.5%

low-rank	tensor	regression

tensor	regression	networks,		j.	kossaifi,	z.c.lipton,	a.khanna,	
t.furlanello and	a.anandkumar,		arxiv pre-publication	

performance	and	rank

speeding up tensor contractions

1 tensor contractions are a core primitive of multiid202.

2 blas 3: unbounded compute intensity (no. of ops per i/o)

consider single-index contractions: cc = aa bb

e.g. cmnp = amnk bkp

==a(:,1,:)a(:,2,:)a422b21c421speeding up tensor contractions

what do we have?

tensor computation libraries
1 arbitrary/restricted tensor
operation of any order and
dimension

e   cient computing frame
1 static analysis solutions

1 ppcg [isl] (polyhedral)
2 tce (dsl)

1 tensortoolbox (matlab)
2 ftensor (c++)
3 cyclops (c++)
4 btas (c++)
5 all the python...

2 parallel and distributed primitives

1 blas, cublas
2 blis, blasx, cublasxt

speeding up tensor contraction

explicit permutation dominates,
especially for small tensors.

consider cmnp = akm bpkn.
1 akm     amk
2 bpkn     bkpn
3 cmnp     cmpn
4 cm(pn) = amk bk(pn)
5 cmpn     cmnp

(top) cpu. (bottom) gpu. the fraction of time
spent in copies/transpositions. lines are shown with
1, 2, 3, and 6 transpositions.

10020030040050000.20.40.60.81nexisting primitives

gemm

suboptimal for many small matrices.

pointer-to-pointer batchedgemm

available in mkl 11.3   and cublas 4.1

c[p] =    op(a[p]) op(b[p]) +    c[p]

cublas<t>gemmbatched(cublashandle_t handle,

cublasoperation_t transa, cublasoperation_t transb,
int m, int n, int k,
const t* alpha,
const t** a, int lda,
const t** b, int ldb,
const t* beta,
t** c, int ldc,
int batchcount)

tensor contraction with extended blas primitives

cmnp = a          b         

cmnp     c[m + n    ldc1 + p    ldc2]

n[p]k

p[n]k

pk[n]

nk[p]

kernel2

kernel1

(np)k cmn[p] = amkb(cid:62)

case contraction
1.1 amkbknp cm(np) = amkbk(np) cmn[p] = amkbkn[p]
1.2 amkbkpn
cmn[p] = amkbk[p]n cm[n]p = amkbkp[n]
cmn[p] = amkb(cid:62)
1.3 amkbnkp
cm[n]p = amkb(cid:62)
1.4 amkbpkn
cm(np) = amkb(cid:62)
1.5 amkbnpk
cm[n]p = amkb(cid:62)
1.6 amkbpnk
2.1 akmbknp cm(np) = a(cid:62)
kmbk(np) cmn[p] = a(cid:62)
cmn[p] = a(cid:62)
kmbk[p]n cm[n]p = a(cid:62)
2.2 akmbkpn
cmn[p] = a(cid:62)
kmb(cid:62)
2.3 akmbnkp
cm[n]p = a(cid:62)
kmb(cid:62)
2.4 akmbpkn
cm(np) = a(cid:62)
kmb(cid:62)
2.5 akmbnpk
cm[n]p = a(cid:62)
kmb(cid:62)
2.6 akmbpnk
cmn[p] = b(cid:62)
km[p]a(cid:62)
3.1 ankbkmp
cmn[p] = b(cid:62)
k[p]ma(cid:62)
3.2 ankbkpm
cmn[p] = bmk[p]a(cid:62)
3.3 ankbmkp
3.4 ankbpkm
3.5 ankbmpk
3.6 ankbpmk

(np)k cmn[p] = a(cid:62)

cmn[p] = bm[p]ka(cid:62)

kmbkn[p]
kmbkp[n]

kmb(cid:62)

nk[p]

pk[n]

p[n]k

n[p]k

nk

nk

nk

nk

kernel2

cmn[p] = bm[p]kakn

pk cm[n]p = b(cid:62)

kernel1
cmn[p] = b(cid:62)
km[p]akn
cmn[p] = b(cid:62)
k[p]makn
cmn[p] = bmk[p]akn

case contraction
4.1 aknbkmp
4.2 aknbkpm
4.3 aknbmkp
4.4 aknbpkm
4.5 aknbmpk
4.6 aknbpmk
k(mn)a(cid:62)
5.1 apkbkmn c(mn)p = b(cid:62)
k[n]ma(cid:62)
cm[n]p = b(cid:62)
5.2 apkbknm
cm[n]p = bmk[n]a(cid:62)
5.3 apkbmkn
5.4 apkbnkm
5.5 apkbmnk
5.6 apkbnmk
6.1 akpbkmn c(mn)p = b(cid:62)
k(mn)akp cm[n]p = b(cid:62)
cm[n]p = b(cid:62)
6.2 akpbknm
k[n]makp
6.3 akpbmkn
cm[n]p = bmk[n]akp
6.4 akpbnkm
6.5 akpbmnk
6.6 akpbnmk

c(mn)p = b(mn)ka(cid:62)

pk

pk

pk cm[n]p = bm[n]ka(cid:62)

pk

km[n]akp

c(mn)p = b(mn)kakp cm[n]p = bm[n]kakp

km[n]a(cid:62)

pk

tensor contraction with extended blas primitives

case contraction
1.1 amkbknp cm(np) = amkbk(np) cmn[p] = amkbkn[p] cm[n]p = amkbk[n]p
6.1 akpbkmn c(mn)p = b(cid:62)

k(mn)akp cm[n]p = b(cid:62)

km[n]akp

kernel1

kernel2

kernel3

example: mappings to level 3 blas routines

case 1.1, kernel2: cmn[p] = amkbkn[p]
cublasdgemmstridedbatched(handle,

cublas_op_n, cublas_op_n,
m, n, k,
&alpha,
a, lda1, 0,
b, ldb1, ldb2,
&beta,
c, ldc1, ldc2,
p)

existing primitives

pointer-to-pointer batchedgemm

a new primitive: stridedbatchedgemm

c[p] =    op(a[p]) op(b[p]) +    c[p]

pointer-to-pointer batchedgemm requires memory allocation and
pre-computation.
solution: stridedbatchedgemm with    xed strides.
(cid:73) special case of pointer-to-pointer batchedgemm.
(cid:73) no pointer-to-pointer data structure or overhead.

cublas<t>gemmstridedbatched(cublashandle_t handle,

cublasoperation_t transa, cublasoperation_t transb,
int m, int n, int k,
const t* alpha,
const t* a, int lda1, int stridea,
const t* b, int ldb1, int strideb,
const t* beta,
t* c, int ldc1, int stridec,
int batchcount)

a new primitive: stridedbatchedgemm

performance on par with pure gemm (p100 and beyond).

stridedbatchedgemm

documentation in cublas 8.0:

$$ grep stridedbatched -a 17 /usr/local/cuda/include/cublas_api.h
2320:cublasapi cublasstatus_t cublassgemmstridedbatched (cublashandle_t handle,
2321-
2322-
2323-
2324-
2325-
2326-
2327-
2328-
2329-
2330-
2331-
2332-
2333-
2334-
2335-
2336-
2337-
...

cublasoperation_t transa,
cublasoperation_t transb,
int m,
int n,
int k,
const float *alpha, // host or device pointer
const float *a,
int lda,
long long int stridea,
const float *b,
int ldb,
long long int strideb,
const float *beta,
float *c,
int ldc,
long long int stridec,
int batchcount);

// host or device pointer

// purposely signed

applications: tucker decomposition

tmnp = gijkamibnjcpk

main steps in the algorithm

ymjk = tmnpbt
yink = tmnpat+1
yijp = tmnpbt+1

njct
pk
mi ct
pk
nj at+1
mi

performance on tucker decomposition:

  mnp    ijk   mi  njtgab   pkc2040608010012010   2100102104106ntime(sec)tensortoolboxbtascyclopscpubatchedgpubatchedlow-communication fft for multiple gpus involves tensor contractions.

applications: fft

stridedbatchedgemm composes 75%+ of the runtime.

(cid:73) essential to the performance.
(cid:73) two custom kernels are precisely interleaved gemms.

2 p100 gpus: 1.3x over cufftxt.

8 p100 gpus: 2.1x over cufftxt.

c. cecka    low-communication fft with fast multipole method    gtc 2017.

tensor sketches

randomized id84
through sketching.

    complexity independent of tensor order:

exponential gain!

+1

+1

-1

sketch s

tensor t

applications

tensor decomposition via sketching 
 visual question and answering

id98

h

c

w

mct

a
v
g
p
o
o

l
i

n
g

f
c

r
e
u

l

b
a
t
c
h
n
o
r
m

f
c

s
o
f
t

m
a
x

"banana"

      what is the 
mustach made of?

id56

l

tensor sketching

id84 through sketching.

count sketch for vector x

c[h[i]]+ = s[i]x[i], for s[i]     {   1, +1}n

count sketch for outer products x     y

convolution of count sketches

c(x     y, h, s) = c(x, h, s)     c(y, h, s)
= f f t    1(f f t (c(x, h, s))f f t (c(y, h, s)))

+1

+1

-1

sketch c

vector x

+1

+1

-1

sketch s

tensor t

accelerated tensor low-rank decomposition

symmetric tensor cp decomposition
for symmetric tensor t     rn  n  n,    nd {(  i , ui )}k

(cid:107)t    (cid:80)k

i=1   i u   3
i (cid:107)2
f .

i=1 to minimize

wide application in data mining and latent variable models.
tensor power iteration: u(t+1) = t (i , u(t), u(t))/(cid:107)t (i , u(t), u(t))(cid:107)2.
accelerated tensor power iteration via sketching:
tensorsketch: s(t )     rb, for n < b (cid:28) n3.

[t (i , u, u)]i     (cid:104)s(t ),

s(u     u     ei )(cid:105)

= (cid:104)f(s(t )), f(s(u)).     f(s(u)).     f(s(ei ))(cid:105)
= (cid:104)f   1(f(s(t )).     f(s(u)).     f(s(u))),

s(ei )).

time complexity: o(n3)     o(n + b log b).

wang et al. (cmu and uci)

fast and guaranteed tensor decomposition

nips    15

2 / 1

e   cient spectral method for id96

id96
v : vocabulary size; k: number of topics. recover topic distributions
  1,       ,   k     rv from n unlabeled documents.

figure 1: negative log-likelihood and running time (min) on wikipedia dataset.

wang et al. (cmu and uci)

fast and guaranteed tensor decomposition

nips    15

3 / 1

9101112131415167.888.28.4log hash lengthnegative log   likelihood  k=50k=100k=200exact, k=50exact, k=100exact, k=200id150, 100 iterations, 145 minsmultimodal tensor pooling

h

spatial sketch

d3

d2

d1

c

w

image feature

3d fft

3d ifft

1d fft

(optional)

d3

d2

d1

l

text feature

count sketch

d4

mct in visual question & answering

id98

t is the 

id56

w  

musta

c          

h

 

m	


c

l

f
c

r
e
u

l

 m

f
c

s
o
f
t

m
a

"    

na
"

a
v

 
 
 

b
 
 
 
 
 

x
tensor decompositions

i

s
c
s
s
a
c

l

t

h
a
m

alice

bob

carol

dave

eve

example: discovering latent factors

i

s
c
s
y
h
p

i

c
s
u
m

list of scores for students in di   erent tests

learn hidden factors for verbal and mathematical
intelligence [c. spearman 1904]

score (student,test) = studentverbal-intlg    testverbal
+ studentmath-intlg    testmath

matrix decomposition: discovering latent factors

i

s
c
s
s
a
c

l

i

s
c
s
y
h
p

i

c
s
u
m

verbal

math

=

+

h

t

a
m

alice

bob

carol

dave

eve

identifying hidden factors in   uencing the observations

characterized as matrix decomposition

tensor: shared matrix decomposition

i

s
c
s
s
a
c

l

i

s
c
s
y
h
p

i

c
s
u
m

verbal

math

=

=

+

+

h
t
a
m

alice

bob

carol

dave

eve

alice

bob

carol

dave

eve

(oral)

(written)

shared decomposition with di   erent scaling factors

combine matrix slices as a tensor

tensor decomposition

verbal

math

=

+

i

c
s
u
m

i

s
c
s
y
h
p

written
oral

alice

bob

carol

dave

eve

h

t

a
m

i

s
c
s
s
a
c

l

outer product notation:

t = u     v     w +   u       v       w

m

t i1,i2,i3 = ui1    vi2    wi3 +   ui1      vi2      wi3

identi   ability under tensor decomposition

=

+

+         

t =   1a1

   3 +   2a2

   3 +          ,

uniqueness of tensor decomposition [j. kruskal 1977]

above tensor decomposition: unique when rank one
pairs are linearly independent

matrix case: when rank one pairs are orthogonal

  2a2

  1a1

  2a2

  2a2

  1a1

  1a1

unsupervised learning via probabilistic models

data     model     learning algorithm     predictions

hidden

h

choice variable

k1

k2

k3

k4

k5

topics

a

a

a

a

a

gene data dna rna

life
observed variables

words

challenges in high dimensional learning

dimension of x     dim. of latent variable h.
learning is like    nding needle in a haystack.

computationally & statistically challenging.

extracting topics from documents

e

d

u

c

s

a

p

c

(cid:1)

o

r
i

o

r

m

t

n

s

e

campus

police

witness

campus

police

witness

campus

police

witness

campus

police

witness

topics

topic proportion

a., d. p. foster, d. hsu, s.m. kakade, y.k. liu.   two svds su   ce: spectral decompositions

for probabilistic id96 and id44,    nips 2012.

tensor methods for id96

campus

police
witness

topic-word matrix p[word = i|topic = j]

linearly independent columns

moment tensor: co-occurrence of word triplets

w

it

n

p

c

a

m

olic

e

e

s

s

p

u

s

campus

police

witness

u s

p

p

m

e s s

olic e
w it n

c a

=

+

+

e d uca (cid:1) o n

s p orts

cri m e

tensors vs. variational id136

criterion: perplexity = exp[   likelihood].

learning topics from pubmed on spark, 8mil articles

10   104

e
m
t

i

i

g
n
n
n
u
r

8

6

4

2

0

variational

105

y tensor
t
i
x
e
l
p
r
e
p

104

103

learning network communities from social network data
facebook n     20k, yelp n     40k, dblp-sub n     1e5, dblp n     1e6.

106

105

104

103

e
m
t

i

i

g
n
n
n
u
r

101

100

10-1

r
o
r
r
e

102

fb

yp

dblpsub dblp

10-2

fb

yp

dblpsub dblp

f. huang, u.n. niranjan, m. hakeem, a,    online tensor methods for training latent variable models,    jmlr 2014.

tensors vs. variational id136

criterion: perplexity = exp[   likelihood].

learning topics from pubmed on spark, 8mil articles

10   104

e
m
t

i

i

g
n
n
n
u
r

8

6

4

2

0

106

105

104

e
m
t

i

i

g
n
n
n
u
r

103

102

o r d

e r s

fb

t e

u r a

c

c

a

variational

105

y tensor
t
i
x
e
l
p
r
e
p

104

m o r e

&

103

s t e r

learning network communities from social network data
facebook n     20k, yelp n     40k, dblp-sub n     1e5, dblp n     1e6.

a

f

e

d

u

n it

g

o f m a

101

100

10-1

r
o
r
r
e

yp

dblpsub dblp

10-2

fb

yp

dblpsub dblp

f. huang, u.n. niranjan, m. hakeem, a,    online tensor methods for training latent variable models,    jmlr 2014.

tensorly: tensor learning in python

(cid:73) pure python
(cid:73) integrated in the python ecosystem
(cid:73) minimal dependencies (numpy, scipy and matplotlib)
(cid:73) easy to use and extend
(cid:73) fast
(cid:73) extensively tested (unit-tests)
(cid:73) exhaustive documentation
(cid:73) open source, bsd licensed

tensorly yours,

try it: pip install tensorly

https://tensorly.github.io

contributions welcome!

ensorlyoutline

1

introduction

2 distributed deep learning using mxnet

3 learning in multiple dimensions

4 conclusion

conclusion

distributed deep learning at scale

mxnet has many attractive features

    flexible programming
    portable
    highly e   cient

easy to deploy large-scale dl on aws cloud

    deep learning ami
    cloud formation templates

tensors are the future of ml

tensor contractions: space savings in deep architectures.

new primitives speed up tensor contractions: extended blas

t

=

+

u

v

+

+

=

+

....

