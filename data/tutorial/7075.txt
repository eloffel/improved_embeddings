advanced 

id23

george konidaris
gdk@cs.brown.edu

id23

   : s     a

max

 

r =

 trt

  t=0

the world

discrete rl

real-valued states
what if the states or actions are real-valued?

need real-valued:

     policies 
     value functions
     environmental models

key issues:

     uncountable in   nity 
     may never revisit states
     must generalize

vs

2.5

2

1.5

1

0.5

0

100

80

60

40

20

0
0

10

20

30

40

50

60

70

80

90

function approximation
exactly as we have seen before.

     represent function       in parametrized form:

f (x)

 
    for some parameter vector w. 

f (x, w)

      write an objective function in terms of w.

     optimize (typically id119).

function approximation

value 
function

policy

model

function approximation

value 
function

policy

model

value function approximation
represent q function:

q(s, a, w) : rn ! r

objective function?

samples of form:

(si, ai, ri, si+1, ai+1)

minimize summed squared td error:

min

w

nxi=0

(q(si, ai, w)   ri    q(si+1, ai+1, w))2

value function approximation
given a function approximator, compute the gradient and 
descend it.

simplest thing you can do:

    linear value function approximation.
    use set of basis functions
 1, ...,  n
    q is a linear function of them:

  q(s, a) = w     (s, a) =

wi (si, ai)

nxi=1

function approximation
one choice of basis functions: 

    just use state variables directly: 

[1, x, y]

more powerful:

    polynomials in state variables.

    1st order:  
    2nd order: 

[1, x, y, xy]
[1, x, y, xy, x2, y2, x2y, y2x, x2y2]

    this is like a taylor expansion.

function approximation
another:

    fourier terms on state variables.

     [1, cos(  x), cos(  y), cos(  [x + y])]

objective function minimization
first, let   s do stochastic id119.

as each data point (transition) comes in

    compute gradient of objective w.r.t. data point
    descend gradient a little bit

  q(s, a) = w     (s, a)
nxi=0

min

w

(w     (si, ai)   ri    w     (si+1, ai+1))2

gradient
for each weight wj:

@
@wj

= 2

nxi=0
nxi=0

(w     (si, ai)   ri    w     (si+1, ai+1))2

(w     (si, ai)   ri    w     (si+1, ai+1))  j(si, ai)

td error

so for each si the contribution is:

(w     (si, ai)   ri    w     (si+1, ai+1))  j(si, ai)

make a step:
wj,i+1 = wj,i +     (w     (si, ai)   ri    w     (si+1, ai+1))  j(si, ai)
wi+1 = wi +      (si, ai)

  -gradient

the same logic applies when using eligibility traces.

wi+1 = wi +      (si, ai)

becomes

where

wi+1 = wi +     e

et =   et 1 +  (st, at)
e0 =   0

[sutton and barto, 1998]

acrobot

acrobot

least-squares td
minimize:

min

w

nxi=0

(w     (si, ai)   ri    w     (si+1, ai+1))2

error function has a bowl shape, so unique minimum. just go 
right there!

   
least-squares td
derivative set to zero:
nxi=1
(w     (si, ai)   ri    w     (si+1, ai+1))  (si, ai)t = 0
nxi=1

(w     (si, ai)    w     (si+1, ai+1))  t (si, ai) =

nxi=1

wt

ri t (si, ai)

w = a 1b

( (si, ai)     (si+1, ai+1))  t (si, ai)

a =

b =

nxi=1
nxi=1

ri t (si, ai)

[bradtke and barto, 1996]

lstd(  )
can derive the least-squares version of lstd(  ) in this way.
try it at home!

    write down the objective function    
    sample ri replaced by complex reward estimate.

    you will get a trace vector if you do some clever algebra.
    trace vector is the same size as w.

[boyan, 1999]

lstd(  )

one inversion solves for w!

but:
    computationally expensive.
    a may not be invert-able.
    least-squares behavior sometimes unstable outside of data.

    lspi: least squares policy iteration
    requires recomputing a over historical data.

    ai+1 changes with the policy

[lagoudakis and parr, 2003]

linear methods don   t scale
why not?

    they   re complete.
    they have nice properties (bowl-shaped error).
    they are easy to use!

how many basis functions in a complete nth order taylor 
series of d variables?

(n + 1)d

function approximation

td-gammon: tesauro (circa 1992-1995)

    at or near best human level
    learn to play backgammon through self-play
    1.5 million games
    neural network function approximator
    td(  )

changed the way the best human players played.

arcade learning environment

[bellemare 2013]

deep q-networks

[mnih et al., 2015]

atari

[mnih et al., 2015]

video: two minute papers

atari

[mnih et al., 2015]

function approximation

value 
function

policy

model

policy search
represent policy directly:

   (s, a,    ) : rn, rm ! [0, 1]

why?

objective function?

hill climbing
   
what if you can   t differentiate   ?

sample-based optimization:

     sample some   values near your current best  .
   
     adjust your current best to the highest value  .
   

   

aibo gait optimization
from kohl and stone, icra 2004.

power and pi2
more recently, two closely related algorithms:

    generate some sample    values.
    next    is sum of prior samples weighted by reward.

   

   

(theodorou and schaal 2010, kober and peters 2011)

reinforce
if we can differentiate       
   
    compute and ascend
    this is the gradient of return w.r.t policy parameters

   r/     

reinforce: one particularly popular sample-based estimate 
of the gradient.

    t =    rtr   (st, at,    )
   (st, at,    )

policy search
slightly more general theorem - policy gradient theorem.

therefore, one way is to learn q and then ascend gradient.
q need only be de   ned using basis functions computed from  .  

[sutton et al. 1999]

deep policy search

[levine et al., 2016]

deep policy search

[levine et al., 2016]

robotics

[levine et al., 2016]

function approximation

value 
function

policy

model

learning a model
learn a model:

why?

t (si+1|si, ai, w)

objective function?

samples of form:

(si, ai, ri, si+1, ai+1)

maximize likelihood of observed transitions:

max

w

   n
i=1t (si+1|si, ai, w)

procedure
model-based rl algorithms roughly look like:

     get some transition data
     learn a model
     run rl on samples from that model to convergence
     repeat

advantages?

this never works. why?

pilco
the main issue is that your model is never exactly right.

    policy specialized to model.
    typically assume predictions are    correct   .
    but the model is uncertain!

recent breakthrough: bayesian policy search:

zm

e"xt

r(st)#

[deisenroth et al, 2011]

pilco
combine gaussian process dynamics learning with analytic 
id189.

deep models

[weber et al., 2017]

deep models

[weber et al., 2017]

deep models

[weber et al., 2017]

deep models

[weber et al., 2017]

function approximation

value 
function

policy

model

hierarchical rl

skill hierarchies
hierarchical rl: base hierarchical control on skills.

    component of behavior.
    performs continuous, low-level control.
    can treat as discrete action.

behavior is modular and compositional.

skills are like subroutines.
def abs(x):
   if(x > 0):
     return x
   else:
     return -x 

                                                             [wilkes, wheeler and gill, 1951]

hierarchical rl
rl typically solves a single problem monolithically. 

hierarchical rl:

     create and use higher-level macro-actions.
     problem now contains subproblems.
     each subproblem is also an rl problem.

options framework: theoretical basis for skill acquisition, 
learning and planning using higher-level actions (options). 

hierarchical rl

!

hierarchical rl
skill 

problem

the options framework
an option is one formal model of a skill.
an option o is a policy unit:

    initiation set
    termination condition
    option policy

io : s !{ 0, 1}
   o : s     a ! [0, 1]

 o : s ! [0, 1]

[sutton, precup and singh 1999]

actions as options
a primitive action a can be represented by an option:

     
ia(s) = 1,8s 2 s
      
 a(s) = 1,8s 2 s
     

   a(s, b) =    1 a = b

0

otherwise

a primitive action can be executed anywhere, lasts exactly one 
time step, and always chooses action a.

questions
given an mdp:

(s, a, r, t,  )

 ... let   s replace a with a set of options o (some of which may 
be primitive actions). 

    how do we characterize the resulting problem?
    how do we plan using options?
    how do we learn using options?
    how do we characterize the resulting policies?

smdps
the resulting problem is a semi-(markov decision process).
this consists of:

                                     set of states
                                     set of options
                                     transition model
                                     reward function
                                     discount factor (per step)

s
o
p (s0, t|o, s)
r(s0, s, t)
 

in this case:

    all times are natural numbers.
       semi    here means transitions can last t timesteps.
    transition and reward function involve time taken for 

option to execute.

easy

q   (s, o) = et,s0[r(s0, s, t)] + et,s0[ t   (s0, o0)q   (s0, o0)]

all things    ow from bellman.

example

(sutton, precup and singh, aij 1999)

example

(sutton, precup and singh, aij 1999)

example

(sutton, precup and singh, aij 1999)

what are skills for?
lots of things! 

a few salient points:

    rewiring.
    transfer.
    skill-speci   c abstractions.

rewiring
adding an option changes the connectivity of the mdp.
this affects:

    learning and planning.
    exploration.
    state-visit distribution.
    diameter of problem. 

(sutton, precup and singh, aij 1999)

transfer
use experience gained while solving one problem to improve 
performance in another.

skill transfer:

    use options as mechanism for transfer.
    transfer components of solution.
    can drastically improve performance
     ... even if it takes a lot of effort to learn them.

general principle: subtasks recur. 

transfer
tasks drawn from parametrized family.

    common features present.
    options de   ned using only common features.

(konidaris and barto, ijcai 2007)

skill-speci   c abstractions
skill-speci   c abstractions
options provide opportunities for abstraction

    split high-dimensional problem into subproblems ...
     ... such that each one supports a solution using an 

abstraction. 

working hypothesis: behavior is piecewise low-dimensional. 

skill discovery
where do skills come from?

discover options autonomously, through interaction with an 
environment.

    typically subgoal options. 
    this means that we must determine    .
 o
    sometimes also     .
ro

the question then becomes:

    which states are good subgoals?

betweenness centrality
consider an mdp as a graph.

    states are vertices.
    edges indicate possible transition between two states.

further, let us assume a task distribution over start states and 
goal pairs: 

     pt (s, e)

(simsek and barto, 2008)

betweenness centrality
we can de   ne the betweenness centrality of a vertex (state) as:

 se(v)

 se

wse

xs,e

this indicates it id203 of being on a shortest path from s 
to e; if we de   ne:

    shortest path as optimal solution.
        

wse = pt (s, e)

... then we get something sensible for rl.

(simsek and barto, 2008)

betwenness centrality

(simsek and barto, 2008)

skill acquisition

    a robot learning to solve a task
    extracting skills from solution
    deploying them in a new task

[konidaris et al., 2011]

training room

acquired skills

the test room

the test room

the test room

900

800

700

600

e
m
t

i

500

400

300

innate controllers

acquired skills

[konidaris et al., 2011]

state abstraction

key idea
how can we create a model of an environment that is 
maximally abstract but still allows the agent to plan?

what is the fundamental question of probabilistic planning?

given a state and a sequence of high-level actions:

     what is the id203 of being able to execute it?
     what is the expected reward?  

[konidaris et al., 2014, 2015]

symbols for planning
a plan p = {o1, ..., on} from a state distribution z is a sequence 
of actions to be executed from a state drawn from z. 

starting from the corridor ...

    gotodoor
    turnhandle
    pushdooropen
    enterroom ...

so:     which mathematical objects do we need to 
determine the id203 with which we 
can execute any plan p?

symbols for planning
we need one classi   er and one operator per skill.

initiation classi   er:                   

...

symbols for planning
we need one classi   er and one operator per skill.

image distribution:

...

.
.
.

probabilistic planning
must deal with distributions over states in the future.

...

probabilistic initiation set

zs    id203 of

execution

probabilistic
distribution    
over states

distribution over states

de   ning a symbol

what do operations on our symbols mean?

(concrete boolean algebra)

probabilistic symbols
learning symbolic representations

    execute options and get some data

(s, o, s0, r) (s, io?)

    for each option:

    partition into ~abstract subgoal options
    for each partitioned option:

    probabilistic classi   er for init distribution
    density estimator for image distribution
    regression for reward model

learning symbolic representations

symbolic planning

learning symbolic representations

symbolic representations

symbol0

symbol1

symbolic representations

symbol1

symbol3

symbol4

symbol5

symbolic representations

symbol8 and symbol12

symbol0

symbol8 and symbol11

symbol2

symbol3

symbolic representations

symbol1

symbol5 and symbol6

symbol3

symbol2

symbolic planning

true abstraction hierarchies
base mdp: 
successive mdps: 

m0 = {s0, a0, r0, p0}

mi = {si, ai, ri, pi}

3

2

1

taxi
options:
1. up, down, left, right, pick up, drop off
2. drive to each depot, pick up, drop off
3. passenger-to-depot

[konidaris, ijcai 2016]

id23

thank you!
questions?

