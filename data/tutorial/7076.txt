convolutional networks @ dl indaba

nando de freitas, martin gorner, karen simonyan 

lecture outline

    recap.
    convolutional layers.
    convolutional neural networks.
    going deeper: the challenges and how to solve them.
    beyond image classification.

2

recap 

3

>tensorflow and deep learning_
 >tensorflow and deep learning_

without a phd
without a phd

   deep
science !

deep
 code ...

#tensorflow                                                                               @martin_gorner

hello world: handwritten digits classification - mnist

?

mnist = mixed national institute of standards and technology - download the dataset at http://yann.lecun.com/exdb/mnist/

very simple model: softmax classification

28x28 
pixels

784 pixels

...

weighted sum of all 

pixels + bias

softmax

...

0

1

2

9

neuron outputs

                                                                 @martin_gorner

in matrix notation, 100 images at a time

10 columns

w0,0 w0,1 w0,2 w0,3     w0,9
w1,0 w1,1 w1,2 w1,3     w1,9
w2,0 w2,1 w2,2 w2,3     w2,9
w3,0 w3,1 w3,2 w3,3     w3,9
w4,0 w4,1 w4,2 w4,3     w4,9
w5,0 w5,1 w5,2 w5,3     w5,9
w6,0 w6,1 w6,2 w6,3     w6,9
w7,0 w7,1 w7,2 w7,3     w7,9
w8,0 w8,1 w8,2 w8,3     w8,9
            
w783,0 w783,1 w783,2     w783,9

7
8
4
 
l
i
n
e
s

x : 100 images,
one per line, 

flattened

x

x

x

x

x

x

x

x

broadcast

l0,0 l0,1 l0,2 l0,3     l0,9
                                +    b0 b1 b2 b3     b9
l0,0 
l1,0 l1,1 l1,2 l1,3     l1,9    
l2,0 l2,1 l2,2 l2,3     l2,9    
l3,0 l3,1 l3,2 l3,3     l3,9    
l4,0 l4,1 l4,2 l4,3     l4,9    
            
l99,0 l99,1 l99,2       l99,9    

+ same 10 biases 
on all lines

784 pixels

softmax, on a batch of images

 predictions                         images      weights   biases
  y[100, 10]                        x[100, 784]   w[784,10]   b[10]

applied line 

by line

matrix multiply

broadcast 
on all lines

tensor shapes in [ ]

                                                                 @martin_gorner

now in tensorflow (python)

tensor shapes:  x[100, 784]   w[748,10]   b[10]

y = tf.nn.softmax(tf.matmul(x, w) + b)

matrix multiply

broadcast 
on all lines

                                                                 @martin_gorner

success ?

cross id178:

0          1        2        3       4        5        6        7        8        9
0
0

1

0

0

0

0

0

0

0

actual probabilities,    one-hot    encoded

this is a    6   

computed probabilities

0.1 0.2 0.1 0.3 0.2 0.1 0.9 0.2 0.1 0.1
0          1        2        3       4        5        6        7        8        9

                                                                 @martin_gorner

tensorflow - initialisation

import tensorflow as tf

this will become the batch size, 100

x = tf.placeholder(tf.float32, [none, 28, 28, 1])
w = tf.variable(tf.zeros([784, 10]))
b = tf.variable(tf.zeros([10]))

28 x 28 grayscale images

init = tf.initialize_all_variables()

training = computing variables w and b

                                                                 @martin_gorner

tensorflow - success metrics

flattening images

# model
y = tf.nn.softmax(tf.matmul(tf.reshape(x, [-1, 784]), w) + b)
# placeholder for correct answers
y_ = tf.placeholder(tf.float32, [none, 10])

   one-hot    encoded

# id168
cross_id178 = -tf.reduce_sum(y_ * tf.log(y))

# % of correct answers found in batch
is_correct = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))

   one-hot    decoding

                                                                 @martin_gorner

tensorflow - training

learning rate

optimizer = tf.train.gradientdescentoptimizer(0.003)
train_step = optimizer.minimize(cross_id178)

                                                                 @martin_gorner

id168

tensorflow - run !

sess = tf.session()
sess.run(init)

for i in range(1000):

running a tensorflow 
computation, feeding 
placeholders

# load batch of images and correct answers
batch_x, batch_y = mnist.train.next_batch(100)
train_data={x: batch_x, y_: batch_y}

# train
sess.run(train_step, feed_dict=train_data)

tip:

do this 
every 100 
iterations

# success ?
a,c = sess.run([accuracy, cross_id178], feed_dict=train_data)

# success on test data ?
test_data={x: mnist.test.images, y_: mnist.test.labels}
a,c = sess.run([accuracy, cross_id178], feed=test_data)

tensorflow - full python code

import tensorflow as tf

initialisation

optimizer = tf.train.gradientdescentoptimizer(0.003)
train_step = optimizer.minimize(cross_id178)

x = tf.placeholder(tf.float32, [none, 28, 28, 1])
w = tf.variable(tf.zeros([784, 10]))
b = tf.variable(tf.zeros([10]))
init = tf.initialize_all_variables()

model

sess = tf.session()
sess.run(init)

for i in range(10000):

# model
y=tf.nn.softmax(tf.matmul(tf.reshape(x,[-1, 784]), w) + b)

# load batch of images and correct answers
batch_x, batch_y = mnist.train.next_batch(100)
train_data={x: batch_x, y_: batch_y}

training step

# placeholder for correct answers
y_ = tf.placeholder(tf.float32, [none, 10])

# id168
cross_id178 = -tf.reduce_sum(y_ * tf.log(y))

success metrics

# train
sess.run(train_step, feed_dict=train_data)

run

# success ? add code to print it
a,c = sess.run([accuracy, cross_id178], feed=train_data)

# % of correct answers found in batch
is_correct = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))

# success on test data ?
test_data={x:mnist.test.images, y_:mnist.test.labels}
a,c = sess.run([accuracy, cross_id178], feed=test_data)

                                                                 @martin_gorner

|

e p   !|
|g o   d e

|

let   s try 5 fully-connected layers !

;

-
)

overkill

200

100

60

30

10

784

sigmoid function

0

1

2

...

9

softmax

                                                                 @martin_gorner

tensorflow - initialisation

k = 200
l = 100
m = 60
n = 30

weights initialised 
with random values

w1 = tf.variable(tf.truncated_normal([28*28, k] ,stddev=0.1))
b1 = tf.variable(tf.zeros([k]))

w2 = tf.variable(tf.truncated_normal([k, l], stddev=0.1))
b2 = tf.variable(tf.zeros([l]))

w3 = tf.variable(tf.truncated_normal([l, m], stddev=0.1))
b3 = tf.variable(tf.zeros([m]))
w4 = tf.variable(tf.truncated_normal([m, n], stddev=0.1))
b4 = tf.variable(tf.zeros([n]))
w5 = tf.variable(tf.truncated_normal([n, 10], stddev=0.1))
b5 = tf.variable(tf.zeros([10]))

                                                                 @martin_gorner

tensorflow - the model

x = tf.reshape(x, [-1, 28*28])

weights and biases

y1 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)
y2 = tf.nn.sigmoid(tf.matmul(y1, w2) + b2)
y3 = tf.nn.sigmoid(tf.matmul(y2, w3) + b3)
y4 = tf.nn.sigmoid(tf.matmul(y3, w4) + b4)
y = tf.nn.softmax(tf.matmul(y4, w5) + b5)

                                                                 @martin_gorner

demo - slow start ?

                                                                 @martin_gorner

relu !

relu
relu = rectified linear unit

y = tf.nn.relu(tf.matmul(x, w) + b)

                                                                 @martin_gorner

relu

                                                                 @martin_gorner

overfitting

cross-id178 loss

overfitting ?

                                                                 @martin_gorner

dropout

dropout

pkeep =
tf.placeholder(tf.float32)

training
pkeep=0.75

evaluation 

pkeep=1

yf = tf.nn.relu(tf.matmul(x, w) + b)
y  = tf.nn.dropout(yf, pkeep)

                                                                 @martin_gorner

deep learning research is like playing with lego

not like this

but rather like this

combinatorial re-use is
robust and amazing for 
creativity

general artificial intelligence

convolutional layers 

28

motivation: locality and translation invariance

    locality: objects tend to have a local spatial support
    translation invariance: object appearance is independent of location

the bird occupies a local area and looks the same in different parts of an image.

we should construct neural nets which exploit these properties!

29

incorporating locality assumptions

    make fully-connected layer locally-connected
    each unit/neuron is connected to a local rectangular area     receptive field
    different units connected to different locations

    output (   feature map   ) lies on a grid itself 

locality

fully-connected unit

locally-connected units
with 3  3 receptive field

30

incorporating invariance assumptions

    weight sharing

    units connected to different locations have the same weights
    equivalently, each unit is applied to all locations

    convolutional layer     locally-connected layer with weight sharing (translation invariance)

weight 
sharing

locally-connected units
with 3  3 receptive field

convolutional units

with 3  3 receptive field

31

correlation and convolution

convolution as searching for patterns

convolutional networks

[matthew zeiler & rob fergus]

hubel-wiesel -> fukushima -> lecun and hinton

36

conv layer mechanics

single-channel scenario

    weight matrix of conv layer is called conv kernel (or filter)
    to compute the output feature map

    slide the receptive field of the filter over the input and compute dot products
    receptive field size == filter size

filter has 

3*3=9 weights

animation credit: https://github.com/vdumoulin/conv_arithmetic 

evaluation of a single conv filter with 3   3 receptive 

field on 4   4 input produces 2   2 output

37

conv layer mechanics

multi-channel scenario

    conv layer input and output can have multiple channels

    e.g. 3-channel rgb image or 16-channel feature map

feature maps are 3-d tensors

height     width     channels

4   4   1 input, 2   2   1 output

3*3=9 filter weights

4   4   3 input, 2   2   1 output
3*3*3=27 filter weights 

4   4   3 input, 2   2   2 output
3*3*3*2=54 filter weights

38

conv layer mechanics

output size

    we   ll assume multi-channel input & output from now on

    for n   n input and kernel size k   k the output size is m = n - k + 1

    we consider all receptive fields lying fully within

the input: known as    valid    convolution

4   4   cin input
2   2   cout output

39

conv layer variants

padded convolution

   

increase (pad) the input with p zeros on both sides

   

sometimes implemented as a separate padding layer
purpose: control output resolution (e.g. preserve resolution)

   
    common settings
   valid   : p=0
   same   : p = (k - 1)/2 on each side for kernel size k
receptive fields go beyond the original input
output has the same spatial size as the input

   
   

   
   

4   4   cin input, 2   2   cout output

   valid    padding

5   5   cin input, 5   5   cout output

   same    padding

40

conv layer variants

strided convolution

    conv filter can be applied with a step (   stride   ) between receptive fields
    purposes

    reduce spatial resolution for faster processing
    achieve invariance to local translation 

    output size:                                            for input size n, kernel size k, padding p, and stride s 

m   m output

n   n input

n=5, k=3, p=0, s=2     m=2

n=5, k=3, p=1, s=2     m=3

n=6, k=3, p=1, s=2     m=3

41

conv layer variants

dilated convolution

    conv filter is applied with a step (   dilation rate   ) between kernel elements

     k   k kernel dilated to size k   =k+(k-1)(r-1), where r is dilation rate

    purposes

    output size 

    large receptive field with a small kernel
    fast alternative to large kernels

    computed based on the dilated kernel size k   

   

m   m output

n   n input

k=3, r=2      k   =5

n=7, k   =5, p=0, s=1     m=3

42

conv layer variants

transposed convolution

    also known as: up-convolution, de-convolution, fractionally-strided convolution
    purpose: increase the resolution
    does the opposite of strided convolution

    implemented by swapping forward and backward operations of standard convolution

    output size 

m   m output

n   n output

n   n input

m   m input

   

strided convolution

transposed convolution

43

convolutional layer

+padding

stride

convolutional
subsampling

convolutional
subsampling

convolutional
subsampling

w1[4, 4, 3]
w2[4, 4, 3]

w[4, 4, 3, 2]

filter 
size

input 
channels

output 
channels

                                                                 @martin_gorner

pooling layer

    purposes (same as strided convolution)

    reduce spatial resolution for faster processing
    achieve invariance to local translation

    average pooling

    computes the average input over the receptive field
    same as k   k strided convolution with weights fixed to 1/(k*k)

    max pooling

    global pooling

    computes the max input over the receptive field

    pooling with the whole input as the receptive field
    gets rid of spatial dimensions, full invariance to location
    can be average or max

45

image convolution layer

modularity - never forget the lego image!

linear layer

relu layer

conv layer

conv layer

pooling layer

convolutional networks

stacking the layers together

54

 hacker   s tip

all

convolu-

tional

convolutional neural network

28x28x1 

28x28x4

14x14x8

7x7x12

200
10

+ biases on 
all layers

convolutional layer, 4 channels
w1[5, 5, 1, 4] stride 1

convolutional layer, 8 channels
w2[4, 4, 4, 8] stride 2

convolutional layer, 12 channels
w3[4, 4, 8, 12] stride 2

fully connected layer   w4[7x7x12, 200]
softmax readout layer  w5[200, 10]

tensorflow - initialisation

k=4
l=8
m=12

filter 
size

input 
channels

output 
channels

w1 = tf.variable(tf.truncated_normal([5, 5, 1, k] ,stddev=0.1))
b1 = tf.variable(tf.ones([k])/10)
w2 = tf.variable(tf.truncated_normal([5, 5, k, l] ,stddev=0.1))
b2 = tf.variable(tf.ones([l])/10)
w3 = tf.variable(tf.truncated_normal([4, 4, l, m] ,stddev=0.1))
b3 = tf.variable(tf.ones([m])/10)

n=200

w4 = tf.variable(tf.truncated_normal([7*7*m, n] ,stddev=0.1))
b4 = tf.variable(tf.ones([n])/10)
w5 = tf.variable(tf.truncated_normal([n, 10] ,stddev=0.1))
b5 = tf.variable(tf.zeros([10])/10)

weights initialised 
with random values

tensorflow - the model

input image batch
x[100, 28, 28, 1]

weights

stride

biases

y1 = tf.nn.relu(tf.nn.conv2d(x, w1, strides=[1, 1, 1, 1], padding='same') + b1)
y2 = tf.nn.relu(tf.nn.conv2d(y1, w2, strides=[1, 2, 2, 1], padding='same') + b2)
y3 = tf.nn.relu(tf.nn.conv2d(y2, w3, strides=[1, 2, 2, 1], padding='same') + b3)

yy = tf.reshape(y3, shape=[-1, 7 * 7 * m])

y4 = tf.nn.relu(tf.matmul(yy, w4) + b4)
y  = tf.nn.softmax(tf.matmul(y4, w5) + b5)

flatten all values for 
fully connected layer  

y3 [100, 7, 7, 12]

yy [100, 7x7x12]

                                                                 @martin_gorner

wtfh ???

???

                                                                 @martin_gorner

bigger convolutional network + dropout

28x28x1 

28x28x6

14x14x12

7x7x24

200
10

+ biases on 
all layers

convolutional layer, 6 channels
convolutional layer, 6 channels
w1[6, 6, 1, 6] stride 1
w1[6, 6, 1, 6] stride 1
convolutional layer, 12 channels
convolutional layer, 12 channels
w2[5, 5, 6, 12] stride 2
w2[5, 5, 6, 12] stride 2
convolutional layer, 24 channels
convolutional layer, 24 channels
w3[4, 4, 12, 24] stride 2
w3[4, 4, 12, 24] stride 2

+dropout 
p=0.75

fully connected layer   w4[7x7x24, 200]
softmax readout layer  w5[200, 10]

yeah !

                                                                 @martin_gorner

with dropout

convolutional networks

for image classification

    now we are ready to build an image classification network 

using conv layers

    activation function: relu(x) = max(x, 0)

    typical structure for image classification

    image     [[conv    ] * m     pool] * n     [linear] * k     softmax

62

case study 1: mnist classification

lenet-5 [lecun et al., 1998]

task
    hand-written digit classification
    10 classes

63

case study 1: mnist classification

lenet-5 [lecun et al., 1998]

layer configuration:
    5   5 conv, stride=1,    valid    padding, sigmoid activation
    2   2 average pool, stride=2

64

id163 challenge

    large-scale image recognition challenge

    major id161 benchmark
    running since 2010 (stanford, unc)
    http://www.image-net.org/challenges/lsvrc/  

    1.4m images, 1000 classes
    main tasks

    classify an image into 1 of the classes

   

   

top-1 error

top-5 error

   

predicted class should be correct

   

predict 5 classes, the correct one
should be among them
    detect all objects in an image

65

id163 challenge

overview of the classification task

    2010-11: hand-crafted computer

vision pipelines

    2012-2016: convnets

    2012: alexnet

    major deep learning success

    2013: zfnet

improvements over alexnet

    2014

    2015

    2016

   

   
   

   

   

vggnet: deeper, simpler
inceptionnet: deeper, faster

resnet: even deeper

ensembled networks, results have saturated

66

case study 2: alexnet

krizhevsky et al., 2012

224x224x3
rgb input

1000
class likelihoods

    8-layer convnet: 5 conv layers, 3 fc layers
    ingredients for success

    architecture

    infrastructure

   
   

   
   

relu non-linearities
regularisation: dropout, weight decay (l2 penalty)

large dataset with random augmentation
two gpus (model split across gpus), 6 days of training

two important components
of successful deep learning models:
architecture and infrastructure

67

case study 2: alexnet

krizhevsky et al., 2012

layer

output size

d
e
p
t
h

input image

conv-11x11x96/4

maxpool/2

conv-5x5x256

maxpool/2

conv-3x3x384

conv-3x3x384

conv-3x3x256

maxpool/2

fc-4096

fc-4096

fc-1000

224x224x3

56x56x96

28x28x96

28x28x256

14x14x256

14x14x384

14x14x384

14x14x256

7x7x256

4096

4096

1000

with depth: higher-level representations, 
more spatial invariance
    spatial resolution is reduced
    #channels is increased

linear layers at the bottom of alexnet 
contain a lot of parameters

68

deeper is better

    each weight layer performs a linear operation, followed by non-linearity

    layer can be seen as a linear classifier itself

    more layers     more non-linearities

    leads to a more discriminative (more powerful) model

    what limits the number of layers in convnets?

    early convnet models used pooling after each conv. layer

input image resolution sets the limit: log(n) for n   n input

   

    computational complexity

69

building very deep convnets

    stack several conv. layers between pooling

    #conv. layers  >>  #pooling layers
    #conv. layers will not affect resolution if 

each layer preserves spatial resolution

    stride = 1 & input padding (   same    convolution)

    more generally, interleave deep multi-layer blocks 

with resolution reduction layers
    strided conv instead of pooling

70

building very deep convnets

    use stacks of small (3   3) conv. layers

    in most cases, the only kernel size you need
    a cheap way of building a deep convnet

    stacks have a large receptive field

    two 3   3 layers     5   5 field
    three 3   3 layers     7   7 field

    less parameters than a single layer 

with a large kernel

71

72

73

case study 3: vggnet 

simonyan & zisserman, 2014

    straightforward implementation of very deep nets:

    stacks of conv. layers followed by max-pooling 
    3x3 conv. kernels, stride=1
    relu non-linearities
    regularisation: dropout, weight decay (l2 penalty)

    a family of architectures

    derived by injecting more conv. layers

    infrastructure

    trained on 4 gpus (training data split across gpus)
    2-3 weeks

74

vggnet incarnations

75

vggnet incarnations

76

vggnet incarnations

77

vggnet incarnations

78

vggnet incarnations

79

vggnet incarnations

80

vggnet incarnations

81

vggnet layer pattern

    multi-layer stacks (conv. layers, stride=1)

interleaved with resolution reduction 
(max-pooling, stride=2)

    other very deep nets (discussed later) 

follow a similar pattern

82

vggnet error vs depth

    error reduces with depth
    plateaus after 16 layers

    we   ll discuss how to fix that

83

going deeper
and how to solve them

challenges of training very deep convnets 

84

challenges of training very deep convnets

    we have seen that depth is important
    why not to keep adding layers to vggnet?

two main reasons:
    computational complexity

    convnet will be too slow to train and evaluate

    optimisation

    we won   t be able to train such nets

85

optimisation

    model optimisation is important

    some architectures are hard to train     in particular very deep nets

    a plethora of gradient-based optimisation methods

    weight update rules are different: sgd, rms-prop, adam, etc.
    sgd with momentum     typical choice for convnets

    major problem: gradient instability

    when we backprop through many layers, compute a product of weights
    if the weights are small, the gradients vanish (get too small)
    if the weights are large, the gradients explode (get too large)

86

 the superpower: batch normalisation

batch normalisation

ioffe and szegedy, 2015

    motivation: the distribution of 

activations changes during 
training, making it harder

    batchnorm layer normalises 

the input to zero mean 
and unit variance

    can be placed anywhere in 

the network
    typically after each conv layer 

before activation

88

batch normalisation (2)

    requires batched training
    batchnorm is differentiable
    means and variances are (slightly) different for different batches

    adds randomness, which is a good regulariser
    nets with batchnorm need less regularisation, dropout is rarely needed

    less sensitive to initialisation, can use n(0, 0.01)

89

residual connections

motivation

    construction to facilitate training of ultra deep nets (100-1000 layers)

    complementary to batchnorm

    motivation: after certain depth, deeper nets have higher training error

error curves for vgg-like nets (3   3 conv throughout)

with batchnorm

90

residual connections

    identity connection which skips a few layers
    we only need to learn the residual
    becomes easier to learn identity, if need to

    just set the weights to 0 

    backprop perspective

    gradient skips weight layers     no vanishing
    improves gradient flow through layers

91

resnets

he at al., 2015-16

    a family of models 

    won the classification task of id163-2015

    simple network design

    inspired by vggnet, but 10x deeper
    residual connections & batchnorm

 

92

resnets

deeper resnets have lower training and test errors

93

resnet id163 results

94

beyond id163 classification

95

fully convolutional networks

shelhamer et al., 2014

    convnet w/o linear layers (   fully convolutional   )

    pre-trained on id163 classification
    penultimate conv layer has 21 channel

    20 classes & background

    convnet contains pooling layers

    which reduce resolution
    compensated by transposed conv 

in the end

pre-trained on id163

(vggnet, resnet) 

transposed conv

increases 

spatial resolution

96

two-stream convnet for video

simonyan & zisserman, 2014

    appearance and motion are processed separately 
    spatial stream convnet

    input: rgb frame

    temporal stream convnet

    input: motion vector field between several frames

    each convnet can be pre-trained (again!)

optical flow

97

lipreading (convnets for video)

98

finding poverty in satellite images (stanford)

99

atari with deep rl

visualizing what nets attend to

wang et al (2016)

general artificial intelligence

what is go?

one of the four arts to be mastered by a true scholar (confucius)
40 million players, 2000 pros: go schools in japan, china and s. korea
simple rules leading to profound complexity
10^170 possible board configurations > no. of atoms in the universe!

why is it hard for computers to play?

sheer complexity of the game means that exhaustive search intractable
branching factor is 200 in go compared to 20 in chess
primarily a game about intuition rather than brute calculation
writing evaluation fn to determine who is winning, thought impossible
combines pattern recognition with search and planning
   beating a professional go player    a long-standing grand challenge of ai

training the deep neural networks

human expert

positions

supervised learning

policy network

id23

policy network

generates new data

(30 mil. positions)

value network

general artificial intelligence

two networks: policy and value nets
value network

policy 
network

general artificial intelligence

internal testing

calibration

external testing

alphago (may 2017)

wins 3/3 matches

alphago (mar 2016)

wins     matches

alphago (oct 2015)

wins 5/5 matches

ke jie (9p)

world number 1

lee sedol (9p)
top player of 
past decade

fan hui (2p)

3-times reigning 
euro champion

thank you!

