id23:

an introduction

deep learning indaba 

september 2017

vukosi marivate and benjamin rosman

1

contents

contents

1. what is id23?

2. value-based methods

3. model-based methods and policy search

4.

inverse id23 and applications

2

what is id23?

we   ve seen how to solve many cool problems around 
supervised and unsupervised learning

but a major component of intelligence is decision making

3

what is id23?

id23 is the branch of machine learning 
relating to learning in sequential decision making 
settings

behaviour learning

4

from supervised to reinforcement

supervised learning, single decision point

multiple decision points
    how do i know if i   m doing the right thing?
    how do my decisions now impact the future?
    actions affect the environment!

5

interacting with an environment

decision maker (agent) exists 
within an environment

6

interacting with an environment

decision maker (agent) exists 
within an environment

agent takes actions based on 
the environment state

7

interacting with an environment

decision maker (agent) exists 
within an environment

agent takes actions based on 
the environment state

environment state updates
agent receives feedback as 
rewards

8

a model for decision making

markov decision process (mdp)

m =    s, a, t, r,      

9

a model for decision making

markov decision process (mdp)

m =    s, a, t, r,      

    states: encode world configurations

    actions: choices made by agent

10

a model for decision making

markov decision process (mdp)

m =    s, a, t, r,      

transition function: how the world

evolves under actions

11

a model for decision making

markov decision process (mdp)

m =    s, a, t, r,      

rewards: feedback signal to agent

12

a model for decision making

markov decision process (mdp)

m =    s, a, t, r,      

       [0,1] discounting for future rewards

13

a model for decision making

markov decision process (mdp)

m =    s, a, t, r,      

markov:

   future is independent of the past, given the present    

14

an example

cleaning robot 

actions:

reward:
    +1 for finding dirt
   
   

-1 for falling into hole
-0.001 for every move

15

an example

states:
    position on grid e.g.
    s is (1,1), goal (4,3)

actions:

1

0

0

reward:
    +1 for finding dirt
   
   

-1 for falling into hole
-0.001 for every move

16

what is the optimal policy?

0.8

0.1

0.1

17

what is the optimal policy?

change the action transitions?

0.1

0.45

0.45

18

what is the optimal policy?

change the action transitions?

0.1

0.45

0.45

19

practically, why rl?

    treating disease in an individual 
    chronic disease (hiv, cancer, schizophrenia, etc.)

    not a single decision event 

information about:
    patient (demographics, 

family history)

    body (test results, etc.)
    disease (genomics, 

progression etc.)

20

how do we find the best treatment 
strategy?

evaluating behaviours

many different trajectories

are possible through a space

use the total discounted
accumulated rewards
to evaluate them 

21

42

-18

37.6

rewards

scalar feedback signal 
encode (un)desirable features of behaviours:

winning/losing, collisions, taking expensive actions, ...

    sparse
    delayed
    only have relative value

22

the rats of hanoi

23

policies

a policy (or behaviour or strategy) (cid:7586) is any mapping from 
states to actions

    deterministic or stochastic

optimal policy (cid:7586)*
    accumulates maximal rewards over a trajectory
    this is what we want to learn!

24

immediate vs delayed rewards

cannot just rely on the instantaneous reward function
tradeoff: don   t just act myopically (short term)

1 step

5 steps

notion of value to codify the goodness of a state, 
considering a policy running into the future

    represented as a value function

25

value functions

value function:

accumulated reward

the expected return (r) starting at state s and then 
executing policy (cid:7586)

   how good is s under (cid:7586)?   

26

example value functions

reward -1 for 
every move

27

example value functions

random policy:

28

example value functions

optimal policy:

29

so what?

how do we use these ideas

to do something useful?

30

value functions: recursion

v(s)     expected return starting at s and following (cid:7586)

suggests dependence on v(s   ) from next state s   

bellman equation:

value of s

immediate 

reward

for all 
possible 

next 
states

31

value of s   

id203 

the 

of 

reaching 
that state 
with (cid:7586)

value functions: optimality

similarly, for an optimal policy (cid:7586)* with optimal value 
function v*:  

bellman optimality equation:

take the 

best 

possible 
action

32

value functions

action-value function:

transition 
id203

the expected return (r) starting at state s and executing 
action a, and then following policy (cid:7586)

   how good is a in s under (cid:7586)?   

33

optimal policies and value functions

(cid:7586)*(a|s) :=

1 if a = argmax  q*(s,a),
0 otherwise

move in 
direction of 
greatest value

finding q* (or v*) is equivalent to finding (cid:7586)*

every mdp has an optimal policy

34

the goal of rl

given this formulation, 

how do we learn a policy?

35

solving bellman

given the bellman equation 

solve this as a large system of value function equations
    but: non-linear (max operator)
    so: solve iteratively

what are we trying to do here?
    learn how good each state of the world is, when looking perfectly into the 

future

36

id145

value iteration: id145

(t,r,s,a)

    iteratively update v (synchronous version)
    at each iteration i:

    for all states s in s:

    update v(s)

but: this requires the full mdp!!
in general, t and r are unknown

37

value based methods

38

algorithm setup

value based methods:
    no transition model
    no reward model
    access to environment for experiment or 

access to training data (s,a,r,s   )

    goal: learn value of states, state-actions

    policy through learned values

39

data generation

t and r unknown!

instead, generate samples of
training data (s,a,r,s   ) from
environment

-5

0

40

learning from experience

we need
    a method to choose actions

    some model to keep track of

and learn

    value function

41

the bandit problem

consider a row of one-arm bandit

machines in a casino

set of    arms    (actions) that 
each generate rewards from 
different distributions

exploration vs exploitation

42

action selection

the exploration-exploitation tradeoff!

maximizing expected returns means balancing between: 

    exploiting gained knowledge (greedy)

    take the best known action

    exploring new actions/states (random)

    try something new 

43

action selection strategies

  -greedy (0 <        1):
    with id203 1-    exploit

    choose the best action for a state

    with id203    explore
    randomly choose action

   usually higher at beginning of learning, decay later
softmax 
    sample action given softmax

44

learning from experience

we need
    a method to choose actions

    some model to keep track of

and learn

    value function

45

td learning

temporal difference (td) learning:
   
    for each experience tuple (s,r,s   ) under policy (cid:7586):

initialise v for all s in s

(t,r,s,a)

    update v:

46

estimated return 
(td target)

td error

eligibility traces

- keep track of where agent has been
- more efficient updates

47

td(0)

td(0) learning:
   
    for each trajectory/episode:

initialise v for all s

   

for all s
   

e(s) = 0

   
   
   

   

e(s) = e(s) + 1

for all s in s
   
e(s) = 0

    for each experience tuple (s,r,s   ) under policy (cid:7586) in episode:

we are back to normal td learning. 

48

(t,r,s,a)

td rollouts

(t,r,s,a)

49

td(1)

td(1) learning:
   
    for each trajectory/episode:

initialise v for all s

   

for all s
   

e(s) = 0

    for each experience tuple (s,r,s   ) under policy (cid:7586) in episode:
mark whole trajectory

e(s) = e(s) + 1

for all s in s
   
e(s) =   e(s)

decay trace

   
   
   

   

50

(t,r,s,a)

tuning the decay

td(0) 
no traces

  td(1)
traces 
decay with    

td((cid:7581))

control the 
decay rate

51

td((cid:7581))

td((cid:7581)) learning:
   
    for each trajectory/episode:

initialise v for all s

   

for all s
   

e(s) = 0

    for each experience tuple (s,r,s   ) under policy (cid:7586) in episode:

e(s) = e(s) + 1

   
   
   

for all s in s
   
e(s) =   (cid:7581)e(s)

control the speed of decay

   

52

(t,r,s,a)

intermission

15 minutes

53

onwards from td

recap: we can now learn by estimating v from experience

but:
    not using actions a

    we would rather learn q,

for easier policy extraction!
v requires a one-step lookahead model

54

sarsa

initialise q for all s, a

   
    for each episode

    initialise 
    choose       in       from q
    for each step t in episode

act

learn from s, a, r, s   , a   

(t,r,s,a)

    take     , observe 
    choose           in          from q

look ahead

learn

   

 

55

sarsa

where did we get the 
    taking the next action under q
    this is an on policy algorithm

 ?

what about off policy?
    learn about optimal policy while exploring
    reuse experience from other policies
    learn from observations

56

(t,r,s,a)

id24

initialise q for all s, a

   
    for each episode

    initialise 
    for each step t in episode
    choose     in     from q
    take     , observe

57

(t,r,s,a)

act

learn

take best next action 

(so far)

id24 demo

(t,r,s,a)

shreyas skandan: https://www.youtube.com/watch?v=rtu7g0y4os4

58

typical learning curves

59

generalising...

what about extending behaviour to different tasks?

what about building a simulator?

ask questions about the domain

solution: we need a model!!!

60

model based methods

61

from values to environment models

model based id23

learn a model (t and r) from experience

supervised learning problem

models let you predict next state and reward

reason about uncertainty

62

algorithm setup

(t,r,s,a)

model based rl:
    no transition model
    no reward model
    access to environment for experiment or 

access to training data (s,a,r,s   )

    goal: learn transition and reward models

    policy through learned models. 

63

model based rl

learn a transition and reward model

(t,r,s,a)

on receiving experience                        :

 
 

64

dyna q algorithm

for each step t in episode
    choose     in     from q 
    take     , observe 
    update q:
    given 
    update t and r
    repeat n times:

id24

model update

    sample previously observed s
    sample previously taken a (in s)
    get r and s    from model
    update q:

65

sample model to 
update q

what else can i do with a model?

quantify uncertainty in value functions

inherent stochasticity

uncertainty from:
    data sparsity
   
    latent structure
approaches:
    monte carlo sampling
    simulation

66

a little bit of overkill?

ok, so we   ve gone to all this trouble to learn t, r      q    

can   t we just learn the policy?

67

policy search

68

algorithm setup

direct policy learning:
    no transition model
    no reward model
    access to environment for experiment or 

access to training data (s,a,r,s   )

    goal: learn policy directly

(t,r,s,a)

69

policy gradient

parametrise policy: 

choices:
    linear combination of basis functions
    set of state features
    deep neural network 

goal: find best (cid:7578)

optimisation problem!

70

optimising the policy

define cost function j((cid:7578)):

start value, average reward per time step   

find (cid:7578) that maximises j((cid:7578))

e.g. gradient ascent on:

policy gradient

71

why policy gradient?

+ high-dimensional action spaces
+ continuous action spaces
+ many recent successes in robotics

- local convergence
- policy evaluation high variance

72

recap - rl approaches

policy 
search
s

(cid:7586)

a

value 

function 
based
s a

q

a

model 
based
s a

t, r

s    r

73

inverse id23

74

inferring a reward function

designing reward functions is hard!
    often not clear what should be done or how it should be 

rewarded

    where do these come from?

learn the incentives that explain observed behaviour
    from an    expert    

we do not observe the reward, but want to learn it 

75

inverse id23

environment

reward

rl

policy/

behaviour

76

inverse id23

environment

reward

irl

policy/

behaviour

77

algorithm setup

inverse rl:
    transition model (can be learned)
    no reward model
    observe training data (s,a,s   )
    goal: learn a reward model to explain the 

behaviour observed through the training data

(t,r,s,a)

78

irl: from paths to rewards

    observe trajectory/trajectories (s,a,s   )
    would like to know:

    what was the goal of the agent?
    what was the reward? 

get to g and avoid water?

79

maximum likelihood irl

ml irl algorithm (intuition):
    given sample trajectories d
   
initialise a reward function r
    calculate policy from r, t
    calculate p(d|(cid:7586))
    calculate gradient, update r

possible reward function

80

irl: from paths to rewards

what about different teachers?

information not in the data when 
we get it.

mlirl with multiple intentions!!! 

81

m babes et. al. apprenticeship learning about multiple intentions

irl

learn from demonstration
    id104

    showing tasks to robots

    learning from experts

82

(some) id23 

applications

83

application areas

randomised controlled trials

efficacy in sequential multiple 
assignment randomized trial

84

an introduction to dynamic treatment regimes: marie 
davidian

application areas

advertising :(

nuff said!!!

85

application areas

strategies to improve donations or collecting taxes :)

86

tax collections optimization for new york state - gerard miller 
et. al.

application areas

mobile health interventions

experimental design & machine learning opportunities in mobile health: 
susan murphy

87

hiv treatment: possible formulation

features: 
baseline viral load, cd4 count,
   
   
baseline cd4 percentage, 
    age, # previous treatments.

states: 
    viral load tracked monthly over 24 

months. 

    patient   s treatment stage
   

bins for the viral load, in copies/ml, 
were [0.0,50,100,1k,100k].

actions:  
   

therapy/drug cocktail groups 
occurring in the data set. 

reward:
    negated auc 

88

v marivate: improved empirical methods in reinforcement-learning 
evaluation

application areas

robotics:
learning 
behaviours

89

rl application areas

games
    standardised testbeds
    long decision horizons

90

application areas

automated trading

1:

2: ???

3:

91

thank you + resources

2nd edition draft 
recommended. draft 
available online 
http://incompleteideas.net/
sutton/book/the-book-2nd.
html

9292

rl class: 
https://www.udacity.com/course/reinforcement-l
earning--ud600

vukosi marivate and benjamin rosman

vmarivate@csir.co.za, brosman@csir.co.za

