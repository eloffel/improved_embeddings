cs 224d: deep learning for nlp1
lecture notes: part iii2
spring 2015

keyphrases: neural networks. forward computation. backward
propagation. neuron units. max-margin loss. gradient checks.
xavier parameter initialization. learning rates. adagrad.

this set of notes introduces single and multilayer neural networks,

and how they can be used for classi   cation purposes. we then dis-
cuss how they can be trained using a distributed id119
technique known as id26. we will see how the chain
rule can be used to make parameter updates sequentially. after a
rigorous mathematical discussion of neural networks, we will discuss
some practical tips and tricks in training neural networks involving:
neuron units (non-linearities), gradient checks, xavier parameter
initialization, learning rates, adagrad, etc. lastly, we will motivate
using recurrent neural networks as a language model.

1 neural networks: foundations

we established in our previous discussions the need for non-linear
classi   ers since most data are not linearly separable and thus, our
classi   cation performance on them is limited. neural networks are
a family of classi   ers with non-linear decision boundary as seen in
figure 1. now that we know the sort of decision boundaries neural
networks create, let us see how they manage doing so.

1.1 a neuron

a neuron is a generic computational unit that takes n inputs and
produces a single output. what differentiates the outputs of different
neurons is their parameters (also referred to as their weights). one of
the most popular choices for neurons is the "sigmoid" or "binary lo-
gistic regression" unit. this unit takes an n-dimensional input vector
x and produces the scalar activation (output) a. this neuron is also
associated with an n-dimensional weight vector, w, and a bias scalar,
b. the output of this neuron is then:

a =

1

1 + exp(wtx + b)

1 course instructor: richard socher

2 author: rohit mundra, richard socher

figure 1: we see here how a non-linear
decision boundary separates the data
very well. this is the prowess of neural
networks.
fun fact:

neural networks are biologically in-

spired classi   ers which is why they are
often called "arti   cial neural networks"
to distinguish them from the organic
kind. however, in reality human neural
networks are so much more capable
and complex from arti   cial neural net-
works that it is usually better to not
draw too many parallels between the
two.

we can also combine the weights and bias term above to equiva-
lently formulate:

a =

1
1 + exp([wt

b]    [x 1])

neuron:

a neuron is the fundamental building

block of neural networks. we will
see that a neuron can be one of many
functions that allows for non-linearities
to accrue in the network.

cs 224d: deep learning for nlp

2

this formulation can be visualized in the manner shown in fig-

ure 2.

1.2 a single layer of neurons

we extend the idea above to multiple neurons by considering the
case where the input x is fed as an input to multiple such neurons as
shown in figure 3.
if we refer to the different neurons    weights as {w(1),         , w(m)}
and the biases as {b1,         , bm}, we can say the respective activations
are {a1,         , am}:

a1 =
...
am =

1

1 + exp(w(1)tx + b1)

1

1 + exp(w(m)tx + bm)

figure 2: this image captures how in
a sigmoid neuron, the input vector x is
   rst scaled, summed, added to a bias
unit, and then passed to the squashing
sigmoid function.

let us de   ne the following abstractions to keep the notation simple
and useful for more complex networks:

1

1

...

1+exp(zm)

1+exp(z1)

            
            
                 rm
             b1
             w(1)t    

    w(m)t    

...
bm

        

  (z) =

b =

w =

              rm  n

we can now right the output of scaling and biases as:

the activations of the sigmoid function can then be written as:

z = wx + b

             =   (z) =   (wx + b)

             a(1)

...
a(m)

so what do these activations really tell us? well, one can think
of these activations as indicators of the presence of some weighted
combination of features. we can then use a combination of these
activations to perform classi   cation tasks.

figure 3: this image captures how
multiple sigmoid units are stacked on
the right, all of which receive the same
input x.

cs 224d: deep learning for nlp

3

1.3 feed-forward computation
so far we have seen how an input vector x     rn can be fed to a
layer of sigmoid units to create activations a     rm. but what is the
intuition behind doing so? let us consider the following named-
entity recognition (ner) problem in nlp as an example:

"museums in paris are amazing"

here, we want to classify whether or not the center word "paris" is
a named-entity. in such cases, it is very likely that we would not just
want to capture the presence of words in the window of word vectors
but some other interactions between the words in order to make the
classi   cation. for instance, maybe it should matter that "museums"
is the    rst word only if "in" is the second word. such non-linear de-
cisions can often not be captured by inputs fed directly to a softmax
function but instead require the scoring of the intermediate layer
discussed in section 1.2. we can thus use another matrix u     rm  1
to generate an unnormalized score for a classi   cation task from the
activations:

s = uta = ut  (wx + b)

analysis of dimensions: if we represent each word using a 4-
dimensional word vector and we use a 5-word window as input (as
in the above example), then the input x     r20. if we use 8 sigmoid
units in the hidden layer and generate 1 score output from the activa-
tions, then w     r8  20, b     r8, u     r8  1, s     r.

1.4 maximum margin objective function

like most machine learning models, neural networks also need an
optimization objective, a measure of error or goodness which we
want to minimize or maximize respectively. here, we will discuss a
popular error metric known as the maximum margin objective. the
idea behind using this objective is to ensure that the score computed
for "true" labeled data points is higher than the score computed for
"false" labeled data points.

using the previous example, if we call the score computed for the

"true" labeled window "museums in paris are amazing" as s and the
score computed for the "false" labeled window "not all museums in
paris" as sc (subscripted as c to signify that the window is "corrupt").
then, our objective function would be to maximize (s     sc) or to
minimize (sc     s). however, we modify our objective to ensure that
error is only computed if sc > s     (sc     s) > 0. the intuition
behind doing this is that we only care the the "true" data point have
a higher score than the "false" data point and that the rest does not

dimensions for a single hidden layer
neural network: if we represent each
word using a 4-dimensional word
vector and we use a 5-word window
as input, then the input x     r20. if we
use 8 sigmoid units in the hidden layer
and generate 1 score output from the
activations, then w     r8  20, b     r8,
u     r8  1, s     r. the stage-wise
feed-forward computation is then:

z = wx + b

a =   (z)
s = uta

figure 4: this image captures how a
simple feed-forward network might
compute its output.

cs 224d: deep learning for nlp

4

matter. thus, we want our error to be (sc     s) if sc > s else 0. thus,
our optimization objective is now:

minimize j = max(sc     s, 0)

however, the above optimization objective is risky in the sense that

it does not attempt to create a margin of safety. we would want the
"true" labeled data point to score higher than the "false" labeled data
point by some positive margin    . in other words, we would want
error to be calculated if (s     sc <    ) and not just when (s     sc < 0).
thus, we modify the optimization objective:

minimize j = max(    + sc     s, 0)

we can scale this margin such that it is     = 1 and let the other
parameters in the optimization problem adapt to this without any
change in performance. for more information on this, read about
functional and geometric margins - a topic often covered in the study
of support vector machines. finally, we de   ne the following opti-
mization objective which we optimize over all training windows:

minimize j = max(1 + sc     s, 0)

in the above formulation sc = ut  (wxc + b) and s = ut  (wx + b).

1.5 training with id26     elemental

in this section we discuss how we train the different parameters in
the model when the cost j discussed in section 1.4 is positive. no
parameter updates are necessary if the cost is 0. since we typically
update parameters using id119 (or a variant such as sgd),
we typically need the gradient information for any parameter as
required in the update equation:

  (t+1) =   (t)            (t) j

id26 is technique that allows us to use the chain rule
of differentiation to calculate loss gradients for any parameter used
in the feed-forward computation on the model. to understand this
further, let us understand the toy network shown in figure 5 for
which we will perform id26.

here, we use a neural network with a single hidden layer and a
single unit output. let us establish some notation that will make it
easier to generalize this model later:

    xi is an input to the neural network.

    s is the output of the neural network.

the max-margin objective function
is most commonly associated with
support vector machines (id166s)

figure 5: this is a 4-2-1 neural network
where neuron j on layer k receives input
z(k)
and produces activation output a(k)
.
j
j

cs 224d: deep learning for nlp

5

    each layer (including the input and output layers) has neurons
which receive an input and produce an output. the j-th neuron
of layer k receives the scalar input z(k)
and produces the scalar
j
activation output a(k)
j

.

    we will call the backpropagated error calculated at z(k)
j
    layer 1 refers to the input layer and not the    rst hidden layer. for

as   

(k)
j

.

the input layer, xj = z(1)

j = a(1)
j

.

    w(k) is the transfer matrix that maps the output from the k-th layer
to the input to the (k + 1)-th thus, w(1) = w and w(2) = u to put
this new generalized notation in perspective of section 1.3.
let us begin: suppose the cost j = (1 + sc     s) is positive and
14 (in figure 5 and

1 and
1 . this fact is crucial to understanding id26    

we want to perform the update of parameter w(1)
figure 6), we must realize that w(1)
thus a(2)
backpropagated gradients are only affected by values they contribute
to. a(2)
is consequently used in the forward computation of score by
1
multiplication with w(2)
. we can see from the max-margin loss that:
1

14 only contributes to z(2)

   j
   s

=        j
   sc

=    1

therefore we will work with    s
   w(1)
ij

here for simplicity. thus,

id26 notation:
    xi is an input to the neural network.
    s is the output of the neural net-

work.

    the j-th neuron of layer k receives
and produces

the scalar input z(k)
j
the scalar activation output a(k)
j

.

    for the input layer, xj = z(1)
    w(k) is the transfer matrix that maps

j = a(1)
j

.

the output from the k-th layer to
the input to the (k + 1)-th. thus,
w(1) = w and w(2) = ut using
notation from section 1.3.

=

   w(2)

a(2)
i
i
   w(1)
ij

= w(2)

i

i

   a(2)
   w(1)
ij

   s

   w(1)
ij
   a(2)
   w(1)
ij

i

    w(2)
i

   w(2)a(2)

=

i

i

   w(1)
ij
   a(2)
   z(2)
  (z(2)
i
   z(2)

i

i

i

= w(2)

= w(2)

= w(2)
i   

= w(2)
i   

(cid:48)(z(2)

i

(cid:48)(z(2)

i

)

= w(2)
i   

(cid:48)(z(2)

i

(cid:48)(z(2)
= w(2)
i   
   a(1)
(2)
=   
j
i

i

i

i

)

   z(2)
   w(1)
ij
   z(2)
   w(1)
ij
   z(2)
   w(1)
ij
   

)

i

   w(1)
ij
   

)

   w(1)
ij
)a(1)

j

(b(1)
i + a(1)

1 w(1)

i1 + a(1)

2 w(1)

i2 + a(1)

3 w(1)

i3 + a(1)

4 w(1)
i4 )

i +    
(b(1)

k

k w(1)
a(1)
ik )

cs 224d: deep learning for nlp

6

j where
is essentially the error propagating backwards from the i-th neu-

(2)
we see above that the gradient reduces to the product   
i
(2)
  
i
ron in layer 2. a(1)
scaled by wij.

is an input fed to i-th neuron in layer 2 when

j

   a(1)

let us discuss the "error sharing/distribution" interpretation of

id26 better using figure 6 as an example. say we were to
update w(1)
14 :
1. we start with the an error signal of 1 propagating backwards from

a(3)
1 .

figure 6: this subnetwork shows the
relevant parts of the network required
to update w(1)
ij

2. we then multiply this error by the local gradient of the neuron

to a(3)

which maps z(3)
1
the error is still 1. this is now known as   

(3)
1 = 1.
3. at this point, the error signal of 1 has reached z(3)

1 . this happens to be 1 in this case and thus,

1 . we now need

to distribute the error signal so that the "fair share" of the error
reaches to a(2)
1 .

4. this amount is the (error signal at z(3)

1 =   

(3)

1 )  w(2)

1 = w(2)
1

. thus,

the error at a(2)

1 = w(2)
1

.

5. as we did in step 2, we need to move the error across the neuron
1 . we do this by multiplying the error signal

to a(2)

1 by the local gradient of the neuron which happens to be
1 )(1       (z(2)

1 )).

which maps z(2)
1
at a(2)
  (z(2)

6. thus, the error signal at z(2)
1

known as   

(2)
1 .

is   (z(2)

1 )(1       (z(2)

1 ))w(2)
1

. this is

7. finally, we need to distribute the "fair share" of the error to w(1)
14
by simply multiplying it by the input it was responsible for for-
warding, which happens to be a(1)
4 .

8. thus, the gradient of the loss with respect to w(1)

14 is calculated to

be a(1)

4   (z(2)

1 )(1       (z(2)

1 ))w(2)
1

.

notice that the result we arrive at using this approach is exactly
the same as that we arrived at using explicit differentiation earlier.
thus, we can calculate error gradients with respect to a parameter
in the network using either the chain rule of differentiation or using
an error sharing and distributed    ow approach     both of these ap-
proaches happen to do the exact same thing but it might be helpful

to think about them one way or another.

cs 224d: deep learning for nlp

7

1 ) are mathematically equivalent
1 ) as long as the

bias updates: bias terms (such as b(1)
to other weights contributing to the neuron input (z(2)
input being forwarded is 1. as such, the bias gradients for neuron
(k)
i on layer k is simply   
i
instead of w(1)
  (z(2)

14 above, the gradient would simply be   (z(2)
.

. for instance, if we were updating b(1)
1
1 )(1    

1 ))w(2)
1

generalized steps to propagate   (k) to   (k   1):
1. we have error   

(k)
i propagating backwards from z(k)
i

at layer k. see figure 7.

, i.e. neuron i

figure 7: propagating error from   (k) to
  (k   1)

2. we propagate this error backwards to a(k   1)

(k)
by multiplying   
i

by

j

the path weight w(k   1)

.

ij

3. thus, the error received at a(k   1)
4. however, a(k   1)

j

j

(k)

i w(k   1)

ij

is   

.

may have been forwarded to multiple nodes in

the next layer as shown in figure 8. it should receive responsibility
for errors propagating backward from node m in layer k too, using
the exact same mechanism.
5. thus, error received at a(k   1)

m w(k   1)

is   

+   

(k)

(k)

.

mj

i w(k   1)
ij
6. in fact, we can generalize this to be    i   
7. now that we have the correct error at a(k   1)

j

, we move it across

(k)

i w(k   1)

ij

.

neuron j at layer k     1 by multiplying with with the local gradient
  (cid:48)(z(k   1)

).

j

j

j

8. thus, the error that reaches z(k   1)

j

, called   

(k   1)
j

is

  (cid:48)(z(k   1)

)    i   

(k)

i w(k   1)

ij

1.6 training with id26     vectorized

so far, we discussed how to calculate gradients for a given parameter
in the model. here we will generalize the approach above so that
we update weight matrices and bias vectors all at once. note that
these are simply extensions of the above model that will help build
intuition for the way error propagation can be done at a matrix-
vector level.

figure 8: propagating error from   (k) to
  (k   1)

cs 224d: deep learning for nlp

8

, we identi   ed that the error gradient is

for a given parameter w(k)
ij

   a(k)
j

(k+1)
. as a reminder, w(k) is the matrix that maps a(k)
simply   
i
to z(k+1). we can thus establish that the error gradient for the entire
matrix w(k) is:

              

   w(k) =

(k+1)
1
(k+1)
  
2

a(k)
1
a(k)
1

a(k)
2
a(k)
2

(k+1)
  
1
(k+1)
  
2

...

        
        
...

...

             =   (k+1)a(k)t

error propagates from layer (k + 1) to
(k) in the following manner:

  (k) = f (cid:48)(z(k))     (w(k)t   (k+1))
of course, this assumes that in the
forward propagation the signal z(k)    rst
goes through activation neurons f to
generate activations a(k) and are then
linearly combined to yield z(k+1) via
transfer matrix w(k).

thus, we can write an entire matrix gradient using the outer prod-
uct of the error vector propagating into the matrix and the activations
forwarded by the matrix.

now, we will see how we can calculate the error vector   (k). we
w(k)
ij

j =   (cid:48)(z(k)

established earlier using figure 8 that   
this can easily generalize to matrices such that:
(cid:48)(z(k))     (w(k)t  (k+1))

(k+1)
)    i   
i

  (k) =   

(k)

j

.

in the above formulation, the     operator corresponds to an element
wise product between elements of vectors (    : rn    rn     rn).

computational ef   ciency: having explored element-wise updates
as well as vector-wise updates, we must realize that the vectorized
implementations run substantially faster in scienti   c computing
environments such as matlab or python (using numpy/scipy
packages). thus, we should use vectorized implementation in prac-
tice. furthermore, we should also reduce redundant calculations
in id26 - for instance, notice that   (k) depends directly
on   (k+1). thus, we should ensure that when we update w(k) using
  (k+1), we save   (k+1) to later derive   (k)     and we then repeat this for
(k     1) . . . (1). such a recursive procedure is what makes backpropa-
gation a computationally affordable procedure.

2 neural networks: tips and tricks

having discussed the mechanics of neural networks, we will now
dive into some tips and tricks commonly employed when using neu-
ral networks in practice.

2.1 gradient check

we have discussed how to calculate error gradients for parameters
in a neural network model using calculus. here we discuss another

cs 224d: deep learning for nlp

9

technique of approximating the gradient without the use of error
id26:

   j
     

    lim
h   0

j(   + h)     j(       h)
(   + h)     (       h)

= lim
h   0

j(   + h)     j(       h)

2h

we know the above holds true from the de   nition of differentia-
tion, but how does this apply to getting an error gradient? well, the
term j(   + h) is simply the error calculated on a forward pass for a
given dataset when we change perturb the parameter    by +h. sim-
ilarly, the term j(       h) is the error calculated on a forward pass for
the same dataset when we change perturb the parameter    by    h.
thus, using two forward passes, we can approximate the gradient
with respect to any given parameter in the model. of course, consid-
ering how many operations are required for even a forward pass, this
is an extremely expensive procedure of evaluating gradients     this
is a great way of verifying implementations of the id26
however.

a naive implementation of gradient check can be implemented in

the following manner:

snippet 2.1
def eval_numerical_gradient(f, x):

"""
a naive implementation of numerical gradient of f at x
- f should be a function that takes a single argument
- x is the point (numpy array) to evaluate the gradient
at
"""

fx = f(x) # evaluate function value at original point
grad = np.zeros(x.shape)
h = 0.00001

# iterate over all indexes in x
it = np.nditer(x, flags=[   multi_index   ],
op_flags=[   readwrite   ])

while not it.finished:

# evaluate function at x+h
ix = it.multi_index
old_value = x[ix]
x[ix] = old_value + h # increment by h
fxh = f(x) # evaluate f(x + h)
x[ix] = old_value # restore to previous value (very

gradient checks are a great way to
compare analytical and numerical
gradients. analytical gradients should
be close and numerical gradients can be
calculated using:
    lim
h   0

j(   + h)     j(       h)

   j
     

2h

j(   + h) and j(       h) can be evalu-
ated using two forward passes. an
implementation of this can be seen in
snippet 2.1.

cs 224d: deep learning for nlp

10

important!)

# compute the partial derivative
grad[ix] = (fxh - fx) / h # the slope
it.iternext() # step to next dimension

return grad

2.2 id173

(cid:13)(cid:13)(cid:13)w(i)(cid:13)(cid:13)(cid:13)f

i=1

like most classi   ers, neural networks are also prone to over   tting
which causes their validation and test performances to be subop-
timal. we can implement l2 id173 such that the loss with
id173, jr, is calculated to be:
l   

jr = j +   

(cid:13)(cid:13)(cid:13)w(i)(cid:13)(cid:13)(cid:13)f

in the above formulation,

is the frobenius norm of the
matrix w(i) and    is the relative weight assigned to id173
in the weighted-sum objective. adding this type of id173
penalizes weights for being large in magnitude by contributing
to the cost quadratically with it. this reduces the    exibility of the
target function (i.e. classi   er) thereby reducing the over   tting phe-
nomenon. imposing such a constraint can be interpreted as the prior
bayesian belief that the optimal weights are close to zero. how close
you wonder? well, that is what    controls     large values of    lead to
a stronger belief that the weights should be chose to zero. it must be
noted that the bias terms are not regularized and do not contribute to
the cost term above     try thinking about why this is the case.

2.3 neuron units

so far we have discussed neural networks that contain sigmoidal
neurons to allow nonlinearities, however in many applications better
networks can be designed using other id180. some
common choices are listed here with their function and gradient
de   nitions and these can be substituted with the sigmoidal functions
discussed above.

sigmoid: this is the default choice we have discussed and the activa-
tion function    is:

  (z) =

1

1 + exp(   z)

figure 9: the response of a sigmoid
nonlinearity

cs 224d: deep learning for nlp

11

where   (z)     (0, 1)

the gradient of   (z) is:
(cid:48)(z) =

  

    exp(   z)
1 + exp(   z)

=   (z)(1       (z))

tanh: the tanh functionis an alternative to the sigmoid function that
is often found to converge faster in practice. the primary difference
between tanh and sigmoid is that tanh output ranges from    1 to 1
while the sigmoid ranges from 0 to 1.

tanh(z) =

exp(z)     exp(   z)
exp(z) + exp(   z)

where tanh(z)     (   1, 1)

= 2  (2z)     1

figure 10: the response of a tanh
nonlinearity

the gradient of tanh(z) is:

tanh(cid:48)(z) = 1    

(cid:18) exp(z)     exp(   z)

exp(z) + exp(   z)

(cid:19)2

= 1     tanh2(z)

hard tanh: the hard tanh function is sometimes preferred over the
tanh function since it is computationally cheaper. it does however
saturate for magnitudes of z greater than 1. the activation of the
hard tanh is:

hardtanh(z) =

: z <    1
:    1     z     1
: z > 1

figure 11: the response of a hard tanh
nonlinearity

the derivative can also be expressed in a piecewise functional form:

                   1
(cid:40)

z
1

hardtanh(cid:48)(z) =

:    1     z     1
: otherwise

1
0

soft sign: the soft sign function is another nonlinearity which can
be considered an alternative to tanh since it too does not saturate as
easily as hard clipped functions:

softsign(z) =

z

1 + |z|

the derivative is the expressed as:

softsign(cid:48)(z) =

sgn(z)
(1 + z)2

figure 12: the response of a soft sign
nonlinearity

where sgn is the signum function which returns    1 depending on the sign of z

cs 224d: deep learning for nlp

12

relu: the relu (recti   ed linear unit) function is a popular choice
of activation since it does not saturate even for larger values of z and
has found much success in id161 applications:

rect(z) = max(z, 0)

figure 13: the response of a relu
nonlinearity

figure 14: the response of a leaky
relu nonlinearity

the derivative is then the piecewise function:

rect(cid:48)(z) =

1
0

: z > 0
: otherwise

leaky relu: traditional relu units by design do not propagate
any error for non-positive z     the leaky relu modi   es this such
that a small error is allowed to propagate backwards even when z is
negative:

leaky(z) = max(z, k    z)

where 0 < k < 1

this way, the derivative is representable as:

(cid:40)

(cid:40)

leaky(cid:48)(z) =

1
k

: z > 0
: otherwise

2.4 xavier parameter initialization
in understanding the difficulty of training deep feedfor-
ward neural networks (2010), xavier et al study the effect
of different weight and bias initialization schemes on training dy-
namics. the empirical    ndings suggest that for sigmoid and tanh
activation units, lower error rates are achieved and faster convergence
occurs when the weights of a matrix w     rn(l+1)  n(l) are initialized
randomly with a uniform distribution with the following range:

(cid:20)

w     u

   

   

6

(cid:21)

   

6

n(l) + n(l+1) ,

n(l) + n(l+1)

where n(l) is the number of input units to w

and n(l+1) is the number of output units from w

in this parameter initialization scheme, bias units are initialized to 0.
this approach attempts to maintain activation variances as well as
backpropagated gradient variances across layers. without such ini-
tialization, the gradient variances (which are a proxy for information)
generally decrease with id26 across layers.

cs 224d: deep learning for nlp

13

2.5 learning rate

the speed at which a parameter in the model updates can be con-
trolled using the learning rate. in the following naive gradient de-
scent formulation,    is the learning rate:

  (t+1) =   (t)            (t) j

you might think that for fast convergence rates, we should set    to
larger values however faster convergence is not guaranteed with
larger convergence rates. in fact, with very large learning rates, we
might experience that the id168 actually diverges because the
parameters update causes the model to overshoot the convex minima
as shown in figure 15. in non-convex models (most of those we work
with), the outcome of a large learning rate is unpredictable but the
chances of diverging id168s are very high.

the simple solution to avoiding a diverging loss is to use a very
small learning rate so that we carefully scan the parameter space.
furthermore, we can keep this learning rate    xed for all parameters
in the model instead of having a more complex scheme in which each
parameter has its own learning rate.

since training is the most expensive phase in a deep learning

system, some research has attempted to improve this naive approach
to setting learning learning rates. for instance, ronan collobert
scales the learning rate of a weight wij (where w     rn(l+1)  n(l)) by
the inverse square root of the fan-in of the neuron (n(l)). another
approach is to allow the learning rate to decrease over time such that:

  (t) =

  (0)  

max(t,   )

in the above scheme,   (0) is a tunable parameter and represents the
starting learning rate.    is also a tunable parameter and represents
the time at which the learning rate should start reducing. in practice,
this method has been found to work quite well. in the next section
we discuss another method for adaptive id119 which does
not require hand-set learning rates.

2.6 subgradient optimization using adagrad

adagrad is an implementation of standard stochastic gradient de-
scent (sgd) with one key difference: the learning rate can vary for
each parameter. the learning rate for each parameter depends on
the history of gradient updates of that parameter in a way such that
parameters with a scarce history of updates are updated faster using
a larger learning rate. in other words, parameters that have not been

figure 15: here we see that updating
parameter w2 with a large learning rate
can lead to divergence of the error.

cs 224d: deep learning for nlp

14

updated much in the past are likelier to have higher learning rates
now. formally:

  (t) =   (t   1)    

(cid:113)

  

  =1(      j(  ))2
   t

      j(t)

in this technique, we see that if the rms of the history of gradients

is extremely low, the learning rate is very high. a simple implemen-
tation of this technique is:

snippet 2.2
# assume the gradient dx and parameter vector x
cache += dx**2
x += - learning_rate * dx / np.sqrt(cache + 1e-8)

