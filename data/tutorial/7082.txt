cs 224d: deep learning for nlp1
lecture notes: part iv2
spring 2015

1 course instructor: richard socher

2 author: milad mohammadi, rohit
mundra, richard socher

keyphrases: language models. id56. bi-directional id56. deep
id56. gru. lstm.

1 language models

language models compute the id203 of occurrence of a number
of words in a particular sequence. the id203 of a sequence of
t words {w1, ..., wt} is denoted as p(w1, ..., wt). since the number
of words coming before a word, wi, varies depending on its location
in the input document, p(w1, ..., wt) is usually conditioned on a
window of n previous words rather than all previous words:

p(w1, ..., wt) =

i=t   

i=1

p(wi|w1, ..., wi   1)     i=t   

i=1

p(wi|wi   (n   1), ..., wi   1)
(1)

equation 1 is especially useful for speech and translation systems
when determining whether a word sequence is an accurate transla-
tion of an input sentence. in existing language translation systems,
for each phrase / sentence translation, the software generates a num-
ber of alternative word sequences (e.g. {i have, i had, i has, me have, me
had}) and scores them to identify the most likely translation sequence.
in machine translation, the model chooses the best word ordering

for a an input phrase by assigning a goodness score to each output
word sequence alternative. to do so, the model may choose between
different word ordering or word choice alternatives. it would achieve
this objective by running all word sequences candidates through a
id203 function that assigns each a score. the sequence with
the highest score is the output of the translation. for example, the
machine would give a higher score to "the cat is small" compared
to "small the is cat", and a higher score to "walking home after school"
compare do "walking house after school". to compute these proba-
bilities, the count of each id165 would be compared against the
frequency of each word. for instance, if the model takes bi-grams,
the frequency of each bi-gram, calculated via combining a word with
its previous word, would be divided by the frequency of the corre-
sponding uni-gram. equations 2 and 3 show this relationship for

cs 224d: deep learning for nlp

2

bigram and trigram models.

p(w2|w1) =

count(w1, w2)

count(w1)

p(w3|w1, w2) =

count(w1, w2, w3)

count(w1, w2)

(2)

(3)

the relationship in equation 1 focuses on making predictions

based on a    xed window of context (i.e. the n previous words) used
to predict the next word. in some cases the window of past con-
secutive n words may not be suf   cient to capture the context. for
instance, consider a case where an article discusses the history of
spain and france and somewhere later in the text, it reads "the two
countries went on a battle"; clearly the information presented in this
sentence alone is not suf   cient to identify the name of the two coun-
tries. bengio et al. introduces the    rst large-scale deep learning for
natural language processing model that enables capturing this type
of context via learning a distributed representation of words; figure 1
shows the neural network architecture. in this model, input word
vectors are used by both to the hidden layer and the output layer.
equation 4 shows the parameters of the softmax() function consisting
of the standard tanh() function (i.e. the hidden layer) as well as the
linear function, w(3)x + b(3), that captures all the previous n input
word vectors.

  y = so f tmax(w(2)tanh(w(1)x + b(1)) + w(3)x + b(3))

(4)

in all conventional language models, the memory requirements

of the system grows exponentially with the window size n making it
nearly impossible to model large word windows without running out
of memory.

2 recurrent neural networks (id56)

unlike the conventional translation models, where only a    nite win-
dow of previous words would be considered for conditioning the
language model, recurrent neural networks (id56) are capable of
conditioning the model on all previous words in the corpus.

figure 2 introduces the id56 architecture where rectangular box
is a hidden layer at a time-step, t. each such layer holds a number
of neurons, each of which performing a linear matrix operation on
its inputs followed by a non-linear operation (e.g. tanh()). at each
time-step, the output of the previous step along with the next word
vector in the document, xt, are inputs to the hidden layer to produce
a prediction output   y and output features ht (equations 5 and 6). the

figure 1: the    rst deep neural network
architecture model for nlp presented
by bengio et al.

cs 224d: deep learning for nlp

3

inputs and outputs of each single neuron are illustrated in figure 3.

ht = w f (ht   1) + w(hx)xt

  y = w(s) f (ht)

(5)

(6)

below are the details associated with each parameter in the net-

work:

    x1, ..., xt   1, xt, xt+1, ...xt: the word vectors corresponding to a cor-

pus with t words.

    ht =   (whhht   1 + whxxt): the relationship to compute the hidden

layer output features at each time-step t
    xt     rd: input word vector at time t.
    whx     rdh  d: weights matrix used to condition the input word

vector, xt

    whh     rdh  dh: weights matrix used to condition the output of

figure 2: a recurrent neural network
(id56). three time-steps are shown.

the previous time-step, ht   1

    ht   1     rdh: output of the non-linear function at the previous

time-step, t     1. h0     rdh is an initialization vector for the
hidden layer at time-step t = 0.

      (): the non-linearity function (sigmoid here)

      yt = so f tmax(w(s)ht): the output id203 distribution over the
vocabulary at each time-step t. essentially,   yt is the next predicted
word given the document context score so far (i.e. ht   1) and the
last observed word vector x(t). here, w(s)     r|v|  dh and   y     r|v|
where |v| is the vocabulary.

the id168 used in id56s is often the cross id178 error
introduced in earlier notes. equation 7 shows this function as the
sum over the entire vocabulary at time-step t.

j(t)(  ) =     j=|v|
   
j=1

yt,j    log(   yt,j)

the cross id178 error over a corpus of size t is:

j =     1
t

t   

t=1

j(t)(  ) =     1
t

j=|v|
   
j=1

t   

t=1

yt,j    log(   yt,j)

equation 9 is called the perplexity relationship; it is basically 2 to
the power of the negative log id203 of the cross id178 error
function shown in equation 8. perplexity is a measure of confusion

(7)

(8)

figure 3: the inputs and outputs to a
neuron of a id56

xt   1 xt xt+1 ht   1 ht ht+1 w"w"yt   1 yt yt+1 where lower values imply more con   dence in predicting the next
word in the sequence (compared to the ground truth outcome).

cs 224d: deep learning for nlp

4

perplexity = 2j

(9)
the amount of memory required to run a layer of id56 is propor-
tional to the number of words in the corpus. for instance, a sentence
with k words would have k word vectors to be stored in memory.
also, the id56 must maintain two pairs of w, b matrices. while the
size of w could be very large, it does not scale with the size of the
corpus (unlike the traditional language models). for a id56 with
1000 recurrent layers, the matrix would be 1000    1000 regardless of
the corpus size.

figure 4 is an alternative representation of id56s used in some
publications. it representsid56s the id56 hidden layer as a loop.

figure 4: the illustration of a id56 as a
loop over time-steps

2.1 vanishing gradient & gradient explosion problems

recurrent neural networks propagate weight matrices from one time-
step to the next. recall the goal of a id56 implementation is to en-
able propagating context information through faraway time-steps.
for example, consider the following two sentences:

sentence 1

"jane walked into the room. john walked in too. jane said hi to ___"

sentence 2

"jane walked into the room. john walked in too. it was late in the
day, and everyone was walking home after a long day at work. jane

said hi to ___"

in both sentences, given their context, one can tell the answer to both
blank spots is most likely "john". it is important that the id56 predicts
the next word as "john", the second person who has appeared several
time-steps back in both contexts. ideally, this should be possible given
what we know about id56s so far. in practice, however, it turns out
id56s are more likely to correctly predict the blank spot in sentence 1
than in sentence 2. this is because during the back-propagation phase,
the contribution of gradient values gradually vanishes as they propa-
gate to earlier time-steps. thus, for long sentences, the id203 that
"john" would be recognized as the next word reduces with the size of
the context. below, we discuss the mathematical reasoning behind the
vanishing gradient problem.

consider equations 5 and 6 at a time-step t; to compute the id56
error, de/dw, we sum the error at each time-step. that is, det/dw for

ht cs 224d: deep learning for nlp

5

every time-step, t, is computed and accumulated.

   e
   w =

t   

t=1

   et
   w

(10)

the error for each time-step is computed through applying the
chain rule differentiation to equations 6 and 5; equation 11 shows
the corresponding differentiation. notice dht/dhk refers to the partial
derivative of ht with respect to all previous k time-steps.

   et
   w =

t   

k=1

   et
   yt

   yt
   ht

   ht
   hk

   hk
   w

(11)

equation 12 shows the relationship to compute each dht/dhk; this
is simply a chain rule differentiation over all hidden layers within the
[k, t] time interval.

   ht
   hk

=

t   

j=k+1

   hj
   hj   1

=

t   

j=k+1

wt    diag[ f (cid:48)(jj   1)]

(12)

because h     rdn, each    hj/   hj   1 is the jacobian matrix for h:

   hj
   hj   1

= [

   hj

   hj   1,1

...

   hj

   hj   1,dn

] =

                                 

   hj,1
   hj   1,1

.
.
.

   hj,dn
   hj   1,1

.

.

.

.

.

.

.

.

.

   hj,1

   hj   1,dn

.
.
.

   hj,dn
   hj   1,dn

                                 

(13)

putting equations 10, 11, 12 together, we have the following rela-

tionship.

   e
   w =

t   

t   

t=1

k=1

   et
   yt

   yt
   ht

(

t   

j=k+1

   hj
   hj   1

)

   hk
   w

(14)

equation 15 shows the norm of the jacobian matrix relationship
in equation 13. here,   w and   h represent the upper bound values
for the two matrix norms. the norm of the partial gradient at each
time-step, t, is therefore, calculated through the relationship shown in
equation 15.

(cid:107)    hj
   hj   1

(cid:107)   (cid:107) wt (cid:107)(cid:107) diag[ f (cid:48)(hj   1)] (cid:107)      w   h

(15)

the norm of both matrices is calculated through taking their l2-
norm. the norm of f (cid:48)(hj   1) can only be as large as 1 given the sigmoid
non-linearity function.

(cid:107)    ht
   hk

(cid:107)=(cid:107)

t   

j=k+1

   hj
   hj   1

(cid:107)    (  w   h)t   k

(16)

cs 224d: deep learning for nlp

6

the exponential term (  w   h)t   k can easily become a very small or
large number when   w   h is much smaller or larger than 1 and t     k
is suf   ciently large. recall a large t     k evaluates the cross id178
error due to faraway words. the contribution of faraway words to
predicting the next word at time-step t diminishes when the gradient
vanishes early on.

during experimentation, once the gradient value grows extremely
large, it causes an over   ow (i.e. nan) which is easily detectable at
runtime; this issue is called the gradient explosion problem. when the
gradient value goes to zero, however, it can go undetected while dras-
tically reducing the learning quality of the model for far-away words
in the corpus; this issue is called the vanishing gradient problem.

to gain practical intuition about the vanishing gradient problem,

you may visit the following example website.

2.2 solution to the exploding & vanishing gradients

now that we gained intuition about the nature of the vanishing gradi-
ents problem and how it manifests itself in deep neural networks, let
us focus on a simple and practical heuristic to solve these problems.

to solve the problem of exploding gradients, thomas mikolov    rst
introduced a simple heuristic solution that clips gradients to a small
number whenever they explode. that is, whenever they reach a cer-
tain threshold, they are set back to a small number as shown in algo-
rithm 1.

  g        e
   w
if (cid:107)   g (cid:107)    threshold then
  g     threshold
(cid:107)   g (cid:107)

  g

end if

algorithm 1: psudo-code for norm clip-
ping in the gradients whenever they ex-
plode

figure 5 visualizes the effect of gradient clipping. it shows the de-
cision surface of a small recurrent neural network with respect to its
w matrix and its bias terms, b. the model consists of a single unit
of recurrent neural network running through a small number of time-
steps; the solid arrows illustrate the training progress on each gradient
descent step. when the id119 model hits the high error wall
in the objective function, the gradient is pushed off to a far-away loca-
tion on the decision surface. the clipping model produces the dashed
line where it instead pulls back the error gradient to somewhere close
to the original gradient landscape.

to solve the problem of vanishing gradients, we introduce two tech-
niques. the    rst technique is that instead of initializing w(hh) ran-
domly, start off from an identify matrix initialization.

figure 5: gradient explosion clipping
visualization

onthedi cultyoftrainingrecurrentneuralnetworksfigure6.weplottheerrorsurfaceofasinglehiddenunitrecurrentnetwork,highlightingtheexistenceofhighcur-vaturewalls.thesolidlinesdepictsstandardtrajectoriesthatgradientdescentmightfollow.usingdashedarrowthediagramshowswhatwouldhappenifthegradientsisrescaledtoa   xedsizewhenitsnormisaboveathreshold.explodesodoesthecurvaturealongv,leadingtoawallintheerrorsurface,liketheoneseeninfig.6.ifthisholds,thenitgivesusasimplesolutiontotheexplodinggradientsproblemdepictedinfig.6.ifboththegradientandtheleadingeigenvectorofthecurvaturearealignedwiththeexplodingdirectionv,itfollowsthattheerrorsurfacehasasteepwallperpen-diculartov(andconsequentlytothegradient).thismeansthatwhenstochasticgradientdescent(sgd)reachesthewallanddoesagradientdescentstep,itwillbeforcedtojumpacrossthevalleymovingperpen-diculartothesteepwalls,possiblyleavingthevalleyanddisruptingthelearningprocess.thedashedarrowsinfig.6correspondtoignoringthenormofthislargestep,ensuringthatthemodelstaysclosetothewall.thekeyinsightisthatallthestepstakenwhenthegradientexplodesarealignedwithvandignoreotherdescentdirection(i.e.themodelmovesperpendiculartothewall).atthewall,asmall-normstepinthedirectionofthegradientthere-foremerelypushesusbackinsidethesmootherlow-curvatureregionbesidesthewall,whereasaregulargradientstepwouldbringusveryfar,thusslowingorpreventingfurthertraining.instead,withaboundedstep,wegetbackinthatsmoothregionnearthewallwheresgdisfreetoexploreotherdescentdirections.theimportantadditioninthisscenariototheclassicalhighcurvaturevalley,isthatweassumethattheval-leyiswide,aswehavealargeregionaroundthewallwhereifwelandwecanrelyon   rstordermethodstomovetowardsthelocalminima.thisiswhyjustclippingthegradientmightbesu cient,notrequiringtheuseasecondordermethod.notethatthisalgo-rithmshouldworkevenwhentherateofgrowthofthegradientisnotthesameastheoneofthecurvature(acaseforwhichasecondordermethodwouldfailastheratiobetweenthegradientandcurvaturecouldstillexplode).ourhypothesiscouldalsohelptounderstandthere-centsuccessofthehessian-freeapproachcomparedtoothersecondordermethods.therearetwokeydif-ferencesbetweenhessian-freeandmostothersecond-orderalgorithms.first,itusesthefullhessianmatrixandhencecandealwithexplodingdirectionsthatarenotnecessarilyaxis-aligned.second,itcomputesanewestimateofthehessianmatrixbeforeeachup-datestepandcantakeintoaccountabruptchangesincurvature(suchastheonessuggestedbyourhypothe-sis)whilemostotherapproachesuseasmoothnessas-sumption,i.e.,averaging2ndordersignalsovermanysteps.3.dealingwiththeexplodingandvanishinggradient3.1.previoussolutionsusinganl1orl2penaltyontherecurrentweightscanhelpwithexplodinggradients.giventhattheparame-tersinitializedwithsmallvalues,thespectralradiusofwrecisprobablysmallerthan1,fromwhichitfollowsthatthegradientcannotexplode(seenecessarycondi-tionfoundinsection2.1).theid173termcanensurethatduringtrainingthespectralradiusneverexceeds1.thisapproachlimitsthemodeltoasim-pleregime(withasinglepointattractorattheorigin),whereanyinformationinsertedinthemodelhastodieoutexponentiallyfastintime.insucharegimewecannottrainageneratornetwork,norcanweexhibitlongtermmemorytraces.doya(1993)proposestopre-programthemodel(toinitializethemodelintherightregime)ortouseteacherforcing.the   rstproposalassumesthatifthemodelexhibitsfromthebeginningthesamekindofasymptoticbehaviourastheonerequiredbythetarget,thenthereisnoneedtocrossabifurcationboundary.thedownsideisthatonecannotalwaysknowtherequiredasymptoticbehaviour,and,evenifsuchinformationisknown,itisnottrivialtoinitial-izeamodelinthisspeci   cregime.weshouldalsonotethatsuchinitializationdoesnotpreventcross-ingtheboundarybetweenbasinsofattraction,which,asshown,couldhappeneventhoughnobifurcationboundaryiscrossed.teacherforcingisamoreinteresting,yetanotverywellunderstoodsolution.itcanbeseenasawayofinitializingthemodelintherightregimeandtherightcs 224d: deep learning for nlp

7

the second technique is to use the recti   ed linear units (relu) in-
stead of the sigmoid function. the derivative for the relu is either 0 or
1. this way, gradients would    ow through the neurons whose deriva-
tive is 1 without getting attenuated while propagating back through
time-steps.

2.3 deep bidirectional id56s

so far, we have focused on id56s that look into the past words to
predict the next word in the sequence. it is possible to make predic-
tions based on future words by having the id56 model read through
the corpus backwards. irsoy et al. shows a bi-directional deep neu-
ral network; at each time-step, t, this network maintains two hidden
layers, one for the left-to-right propagation and another for the right-
to-left propagation. to maintain two hidden layers at any time, this
network consumes twice as much memory space for its weight and
bias parameters. the    nal classi   cation result,   yt, is generated through
combining the score results produced by both id56 hidden layers. fig-
ure 6 shows the bi-directional network architecture, and equations 17
and 18 show the mathematical formulation behind setting up the bi-
directional id56 hidden layer. the only difference between these two
relationships is in the direction of recursing through the corpus. equa-
tion 19 shows the classi   cation relationship used for predicting the
next word via summarizing past and future word representations.

(17)

(18)

figure 6: a bi-directional id56 model

      
h t = f (
      
h t = f (

      
w xt +
      
w xt +

      
v
      
v

      
b )
      
b )

      
h t   1 +
      
h t+1 +
      
      
h t] + c)
h t;

  yt = g(uht + c) = g(u[

(19)
figure 7 shows a multi-layer bi-directional id56 where each lower
layer feeds the next layer. as shown in this    gure, in this network
architecture, at time-step t each intermediate neuron receives one set
of parameters from the previous time-step (in the same id56 layer),
and two sets of parameters from the previous id56 hidden layer; one
input comes from the left-to-right id56 and the other from the right-
to-left id56.

to construct a deep id56 with l layers, the above relationships are
modi   ed to the relationships in equations 20 and 21 where the input
to each intermediate neuron at level i is the output of the id56 at
layer i     1 at the same time-step, t. the output,   y, at each time-step is
the result of propagating input parameters through all hidden layers
(equation 22).

figure 7: a deep bi-directional id56
with three id56 layers.

bidirectionality h!t=f(w!"!xt+v!"h!t   1+b!)h!t=f(w!""xt+v!"h!t+1+b!)yt=g(u[h!t;h!t]+c)yhx                 now represents (summarizes) the past and future around a single token. h=[h!;h!]going deep h!(i)t=f(w!"!(i)ht(i   1)+v!"(i)h!(i)t   1+b!(i))h!(i)t=f(w!""(i)ht(i   1)+v!"(i)h!(i)t+1+b!(i))yt=g(u[h!t(l);h!t(l)]+c)yh(3)xeach memory layer passes an intermediate sequential representation to the next. h(2)h(1)      
h (i)
      
h (i)

t = f (

t = f (

t

      
w (i)h(i   1)
      
w (i)h(i   1)

t

+

+

  yt = g(uht + c) = g(u[

t   1 +

v (i)      
      
h (i)
v (i)      
      
h (i)
t+1 +
      
      
h (l)
h (l)

;

t

t

cs 224d: deep learning for nlp

8

      
b (i))
      
b (i))

] + c)

(20)

(21)

(22)

2.4 application: id56 translation model

traditional translation models are quite complex; they consist of nu-
merous machine learning algorithms applied to different stages of the
language translation pipeline. in this section, we discuss the potential
for adopting id56s as a replacement to traditional translation mod-
ules. consider the id56 example model shown in figure 8; here, the
german phrase echt dicke kiste is translated to awesome sauce. the    rst
three hidden layer time-steps encode the german language words into
some language word features (h3). the last two time-steps decode h3
into english word outputs. equation 23 shows the relationship for
the encoder stage and equations 24 and 25 show the equation for the
decoder stage.

ht =   (ht   1, xt) = f (w(hh)ht   1 + w(hx)xt)

ht =   (ht   1) = f (w(hh)ht   1)

yt = so f tmax(w(s)ht)

(23)

(24)

(25)

one may naively assume this id56 model along with the cross-
id178 function shown in equation 26 can produce high-accuracy
translation results. in practice, however, several extensions are to be
added to the model to improve its translation accuracy performance.

max  

1
n

   
n=1

n    log(p  (y(  )|x(n)))

(26)

extension i:
train different id56 weights for encoding and decoding.
this decouples the two units and allows for more accuracy prediction
of each of the two id56 modules. this means the   () functions in
equations 23 and 24 would have different w(hh) matrices.

extension ii:
different inputs:

compute every hidden state in the decoder using three

    the previous hidden state (standard)

figure 8: a id56-based translation
model. the    rst three id56 hidden
layers belong to the source language
model encoder, and the last two belong
to the destination language model
decoder.

x1 x2 x3 h1 h2 h3 w	
   w	
   y1 y2 awesome	
   	
   	
   	
   	
   	
   	
   	
   sauce	
   cs 224d: deep learning for nlp

9

figure 9: language model with
three inputs to each decoder neuron:
(ht   1, c, yt   1)

    last hidden layer of the encoder (c = ht in figure 9)
    previous predicted output word,   yt   1

combining the above three inputs transforms the    function in the
decoder function of equation 24 to the one in equation 27. figure 9
illustrates this model.

ht =   (ht   1, c, yt   1)

(27)

extension iii:
train deep recurrent neural networks using multiple
id56 layers as discussed earlier in this chapter. deeper layers often
improve prediction accuracy due to their higher learning capacity. of
course, this implies a large training corpus must be used to train the
model.

extension iv:
lar to what was discussed earlier in this chapter.

train bi-directional encoders to improve accuracy simi-

extension v: given a word sequence a b c in german whose transla-
tion is x y in english, instead of training the id56 using a b c     x
y, train it using c b a     x y. the intutition behind this technique is
that a is more likely to be translated to x. thus, given the vanishing
gradient problem discussed earlier, reversing the order of the input
words can help reduce the error rate in generating the output phrase.

3 id149

beyond the extensions discussed so far, id56s have been found to per-
form better with the use of more complex units for activation. so far,
we have discussed methods that transition from hidden state h(t   1)
to h(t) using an af   ne transformation and a point-wise nonlinearity.
here, we discuss the use of a gated activation function thereby mod-
ifying the id56 architecture. what motivates this? well, although
id56s can theoretically capture long-term dependencies, they are very
hard to actually train to do this. id149 are designed in
a manner to have more persistent memory thereby making it easier for
id56s to capture long-term dependencies. let us see mathematically
how a gru uses h(t   1) and x(t) to generate the next hidden state h(t).
we will then dive into the intuition of this architecture.

z(t) =   (w(z)x(t) + u(z)h(t   1))
r(t) =   (w(r)x(t) + u(r)h(t   1))
  h(t) = tanh(r(t)     uh(t   1) + wx(t))
h(t) = (1     z(t))       h(t) + z(t)     h(t   1)

(update gate)
(reset gate)
(new memory)
(hidden state)

cs 224d: deep learning for nlp

10

the above equations can be thought of a gru   s four fundamental op-
erational stages and they have intuitive interpretations that make this
model much more intellectually satisfying (see figure 10):
1. new memory generation: a new memory   h(t) is the consolidation
of a new input word x(t) with the past hidden state h(t   1). an-
thropomorphically, this stage is the one who knows the recipe of
combining a newly observed word with the past hidden state h(t   1)
to summarize this new word in light of the contextual past as the
vector   h(t).

2. reset gate: the reset signal r(t) is responsible for determining how
important h(t   1) is to the summarization   h(t). the reset gate has the
ability to completely diminish past hidden state if it    nds that h(t   1)
is irrelevant to the computation of the new memory.

3. update gate: the update signal z(t) is responsible for determining
how much of h(t   1) should be carried forward to the next state.
for instance, if z(t)     1, then h(t   1) is almost entirely copied out to
h(t). conversely, if z(t)     0, then mostly the new memory   h(t) is
forwarded to the next hidden state.

4. hidden state: the hidden state h(t) is    nally generated using the
past hidden input h(t   1) and the new memory generated   h(t) with
the advice of the update gate.

it is important to note that to train a gru, we need to learn all
the different parameters: w, u, w(r), u(r), w(z), u(z). these follow the

figure 10: the detailed internals of a
gru

cs 224d: deep learning for nlp

11

same id26 procedure we have seen in the past.

4 long-short-term-memories

long-short-term-memories are another type of complex activation unit
that differ a little from grus. the motivation for using these is similar
to those for grus however the architecture of such units does differ.
let us    rst take a look at the mathematical formulation of lstm units
before diving into the intuition behind this design:

i(t) =   (w(i)x(t) + u(i)h(t   1))
f (t) =   (w( f )x(t) + u( f )h(t   1))
o(t) =   (w(o)x(t) + u(o)h(t   1))
  c(t) = tanh(w(c)x(t) + u(c)h(t   1))
c(t) = f (t)       c(t   1) + i(t)       c(t)
h(t) = o(t)     tanh(c(t))

(input gate)
(forget gate)
(output/exposure gate)
(new memory cell)
(final memory cell)

we can gain intuition of the structure of an lstm by thinking of its

figure 11: the detailed internals of a
lstm

cs 224d: deep learning for nlp

12

architecture as the following stages:
1. new memory generation: this stage is analogous to the new mem-
ory generation stage we saw in grus. we essentially use the input
word x(t) and the past hidden state h(t   1) to generate a new mem-
ory   c(t) which includes aspects of the new word x(t).

2. input gate: we see that the new memory generation stage doesn   t
check if the new word is even important before generating the new
memory     this is exactly the input gate   s function. the input gate
uses the input word and the past hidden state to determine whether
or not the input is worth preserving and thus is used to gate the new
memory. it thus produces i(t) as an indicator of this information.

3. forget gate: this gate is similar to the input gate except that it
does not make a determination of usefulness of the input word    
instead it makes an assessment on whether the past memory cell is
useful for the computation of the current memory cell. thus, the
forget gate looks at the input word and the past hidden state and
produces f (t).

4. final memory generation: this stage    rst takes the advice of the
forget gate f (t) and accordingly forgets the past memory c(t   1). sim-
ilarly, it takes the advice of the input gate i(t) and accordingly gates
the new memory   c(t). it then sums these two results to produce the
   nal memory c(t).

5. output/exposure gate: this is a gate that does not explicitly ex-
ist in grus. it   s purpose is to separate the    nal memory from the
hidden state. the    nal memory c(t) contains a lot of information
that is not necessarily required to be saved in the hidden state. hid-
den states are used in every single gate of an lstm and thus, this
gate makes the assessment regarding what parts of the memory c(t)
needs to be exposed/present in the hidden state h(t). the signal it
produces to indicate this is o(t) and this is used to gate the point-
wise tanh of the memory.

