cs 224d: deep learning for nlp1
lecture notes: part v 2
spring 2015

1 course instructor: richard socher

2 author: francois chaubard, richard
socher

keyphrases: id56, id56s, mv-id56, rntn

this set of notes discusses and describes the many variants on
the id56 (id56s) and their application and
successes in the    eld of nlp.

1 id56s

in these notes, we introduce and discuss a new type of model that
is indeed a superset of the previously discussed recurrent neural
network. id56s (id56s) are perfect for settings
that have nested hierarchy and an intrinsic recursive structure. well
if we think about a sentence, doesn   t this have such a structure? take
the sentence    a small crowd quietly enters the historical church   .
first, we break apart the sentence into its respective noun phrase,
verb phrase,    a small crowd    and    quietly enters the historical
church   , respectively. but there is a noun phrase, verb phrase within
that verb phrase right?    quietly enters    and    historical church   . etc,
etc. seems pretty recursive to me.

the syntactic rules of language are highly recursive. so we take
advantage of that recursive structure with a model that respects it!
another added bene   t of modeling sentences with id56   s is that
we can now input sentences of arbitrary length, which was a huge
head scratcher for using neural nets in nlp, with very clever tricks
to make the input vector of the sentence to be of equal size despite
the length of the sentences not being equal. (see bengio et al., 2003;
henderson, 2003; collobert & weston, 2008)

let   s imagine our task is to take a sentence and represent it as
a vector in the same semantic space as the words themselves. so
that phrases like    i went to the mall yesterday   ,    we went shopping
last week   , and    they went to the store   , would all be pretty close
in distance to each other. well we have seen ways to train unigram
word vectors, should we do the same for bigrams, trigrams, etc. this
very well may work but there are two major issues with this thinking.
1) there are literally an in   nite amount of possible combinations
of words. storing and training an in   nite amount of vectors would
just be absurd. 2) some combinations of words while they might be
completely reasonable to hear in language, may never be represented
in our training/dev corpus. so we would never learn them.

we need a way to take a sentence and its respective words vectors,

figure 1: a standard recursive neural
network

figure 2: parse tree of a sentence.

cs 224d: deep learning for nlp

2

and derive what the embedded vector should be. now lets    rst ask
a very debated question. is it naive to believe that the vector space
that we used to represent all words, is suf   ciently expressive to also
be able to represent all sentences of any length? while this may be
unintuitive, the performance of these models suggest that this is
actually a reasonable thing to do.

let   s    rst discuss the difference between semantic and grammat-

ical understanding of a sentence. semantic analysis is an under-
standing of the meaning of a sentence, being able to represent the
phrase as a vector in a structured semantic space, where similar sen-
tences are very nearby, and unrelated sentences are very far away.
the grammatical understanding is one where we have identi   ed the
underlying grammatical structure of the sentence, which part of the
sentence depends on which other part, what words are modifying
what other words, etc. the output of such an understanding is usu-
ally represented as a parse tree as displayed in figure 2.

now for the million dollar question. if we want to know the se-

mantic representation, is it an advantage, nay, required, to have
a grammatical understanding? well some might disagree but for
now we will treat this semantic composition task the following way.
first, we need to understand words. then, we need to know the way
words are put together, then,    nally, we can get to a meaning of a
phrase or sentence by leveraging these two previous concepts.

so lets begin with our    rst model built on this principle. let   s

imagine we were given a sentence, and we knew the parse tree for
that sentence, such as the one displayed in figure 2, could we    gure
out an encoding for the sentence and also perhaps a sentiment score
just from the word vectors that are in the sentence? we observe how
a simple id56 can perform this task. (as will you in pset 3!)

1.1 a simple single layer id56
lets walk through the model displayed in figure 3 above. we    rst
take a sentence parse tree and the sentence word vectors and begin
to walk up the tree. the lowest node in the graph is node 3, so we
concatenate l29 and l430 to form a vector     r2d and feed it into our
network to compute:

h(1) = tanh(w(1)

+ b(1))

(1)

since w(1)     rd  2d and b(1)     rd, h(1)     rd. we can now think
of h(1) as a point in the same word vector space for the bigram    this
assignment   , in which we did not need to learn this representation
separately, but rather derived it from its constituting word vectors.

(cid:35)

(cid:34)

l29
l430

figure 3: an example standard id56
applied to a parsed sentence    i love this
assignment   

cs 224d: deep learning for nlp

3

we now take h(1) and put it through a softmax layer to get a score

over a set of sentiment classes, a discrete set of known classes that
represent some meaning. in the case of positive/negative sentiment
analysis, we would have 5 classes, class 0 implies strongly negative,
class 1 implies negative, class 2 is neutral, class 3 is positive, and
   nally class 4 is strongly positive.

now we do the same thing with the    i    and    love    to produce
the vector h(1) for the phrase    i love   . again, we compute a score
over the semantic classes again for that phrase. finally, for the most
interesting step, we need to merge the two phrases    i love    and    this
assignment   . here we are concatenating word phrases, rather than
word vectors! we do this in the same manner, concatenating the two
h(1) vectors and compute

       h(1)

le f t
h(1)
right

       + b(1))

h(1) = tanh(w(1)

(2)

now we have a vector in the word vector space that represents

the full sentence    i love this assignment   . furthermore, we can put
this h(1) through the same softmax layer as before, and compute
sentiment probabilities for the full sentence. of course the model will
only do this reliably once trained, but that left for you to do in pset3.
:)

now lets take a step back. first, is it naive to think we can use

the same matrix w to concatenate all words together and get a very
expressive h(1) and yet again use that same matrix w to concatenate
all phrase vectors to get even deeper phrases? these criticisms are
valid and we can address them in the following twist on the simple
id56.

1.2 syntactically untied su-id56

as we discussed in the criticisms of the previous section, using the
same w to bring together a noun phrase and verb phrase and to
bring together a prepositional phrase and another word vector seems
intuitively wrong. and maybe we are bluntly merging all of these
functionalities into too weak of a model.

what we can do to remedy this shortcoming is to    syntactically
untie    the weights of these different tasks. by this we mean, there
is no reason to expect the optimal w for one category of inputs to
be at all related to the optimal w for another category of inputs. so
we let these w   s be different and relax this constraint. while this for
sure increases our weight matrices to learn, the performance boost
we gain is non-trivial.

figure 4: using different w   s for
different categories of inputs is more
natural than having just one w for all
categories

cs 224d: deep learning for nlp

4

as in figure 4 above, we notice our model is now conditioned

upon what the syntactic categories of the inputs are. note, we deter-
mine what the categories are via a very simple probabilistic context
free grammar (pid18) which is more or less learned by computing
summary statistics over the penn tree bank to learn rules such as
   the    is always a dt, etc, etc. no deeper understanding of this part
is really necessary, just know it  s really simple.

the only major other difference in this model is that we initialize
the w   s to the identity. this way the default thing to do is to average
the two word vectors coming in. slowly but surely, the model learns
which vector is more important and also any rotation or scaling of
the vectors that improve performance. we observe in figure 5 that
the trained weight matrices learn actually meaning! for example, the
dt-np rule or determiner followed by a noun phrase such as    the
cat    or    a man   , puts more emphasis on the noun phrase than on
the determiner. (this is obvious because the right diagonals are red
meaning higher weightl). this is called the notion of soft head words,
which is something that linguists have long observed to be true for
sometime, however the model learned this on its own just by looking
at data. pretty cool!

the su-id56 does indeed outperform previously discussed mod-
els but perhaps it is still not expressive enough. if we think of mod-
ifying words, such as adverbs like    very   , any interpolation with
this word vector and the following one, is de   nitely not what the
understood nature of    very    is.

as an adverb, it   s literal de   nition is    used for emphasis   . how

can we have a vector that emphasizes any other vector that is to
follow when we are solely performing a linear interpolation? how
can we construct a vector that will    scale    any other vector this way?
truth is we can not. we need to have some form of multiplication of
word on another word. we uncover two such compositions below
that enable this. the    rst utilizes word matrices and the other utilizes
a quadratic equation over the typical af   ne.

1.3 mv-id56   s (matrix-vector id56s)

we now augment our word representation, to not only include a
word vector, but also a word matrix! so the word    very    will have
a word vector vvery     rd but also vvery     rd  d. this gives us the
expressive ability to not only embed what a word means, but we also
learn the way that words    modify    other words. the word matrix en-
ables the latter. to feed two words, a and b, into a id56, we take their
word matrices a and b, to form our input vector x as the concatena-
tion of vector ab and ba. for our example of    very   , vvery could just

figure 5: the learnt w weights for
dt-np composition match linguists
theory

figure 6: an example mv-id56

cs 224d: deep learning for nlp

5

be the identity times any scalar above one. which would scale any
neighboring word vector by that number! this is the type of expres-
sive ability we desired. while the new word representation explodes
our feature space, we can express much better the way words modify
each other.

by observing the errors the model makes, we see even the mv-

id56 still can not express certain relations. we observe three major
classes of mistakes.

first, negated positives. when we say something positive but

one word turns it negative, the model can not weigh that one word
strong enough to    ip the sentiment of the entire sentence. figure
7 shows such an example where the swap of the word    most    to
   least    should    ip the entire sentiment of the sentence, but the mv-
id56 does not capture this successfully.

the second class of mistakes is the negated negative case. where

we say something is not bad, or not dull, as in figure 8. the mv-
id56 can not recognize that the word    not    lessens the sentiment
from negative to neutral.

the    nal class of errors we observe is the    x but y conjunction   
displayed in figure 9. here the x might be negative but if the y is
positive then the model   s sentiment output for the sentence should be
positive! mv-id56s struggle with this.

thus, we must look for an even more expressive composition

algorithm that will be able to fully capture these types of high level
compositions.

1.4 rntns (recursive neural tensor network)

the    nal id56 we will cover here is by far the most successful on
the three types of errors we left off with. the recursive neural
tensor network does away with the notion of a word matrix, and
furthermore, does away with the traditional af   ne transformation
pre-tanh/sigmoid concept. to compose two word vectors or phrase
vectors, we again concatenate them to form a vector     r2d but in-
stead of putting it through an af   ne function then a nonlinear, we put
it through a quadratic    rst, then a nonlinear, such as:

h(1) = tanh(xtvx + wx)

(3)

note that v is a 3rd order tensor in     r2d  2d  d. we compute
xtv[i]x    i     [1, 2, ...d] slices of the tensor outputting a vector    
rd. we then add wx and put it through a nonlinear function. the
quadratic shows that we can indeed allow for the multiplicative type
of interaction between the word vectors without needing to maintain
and learn word matrices!

figure 7: negated positives

figure 8: negated negatives

figure 9: using a recursive neural net
can correctly classify the sentiment of
the contrastive conjunction x but y but
the mv-id56 can not

figure 10: one slice of a rntn. note
there would be d of these slices.

cs 224d: deep learning for nlp

6

as we see in figure 11, the rntn is the only model that is capable

of succeeding on these very hard datasets.

we will continue next time with a model that actually outperforms

the rntn in some aspects and it does not require an input parse
tree! this model is the dynamic convolutional neural network, and
we will talk about that soon. good luck on your midterm!

