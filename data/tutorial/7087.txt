ensemble methods in machine learning

thomas g(cid:2) dietterich

oregon state university(cid:2) corvallis(cid:2) oregon(cid:2) usa(cid:2)

tgd(cid:2)cs(cid:3)orst(cid:3)edu(cid:2)

www home page(cid:3) http(cid:4)(cid:5)(cid:5)www(cid:3)cs(cid:3)orst(cid:3)edu(cid:5)(cid:6)tgd

abstract(cid:2) ensemble methods are learning algorithms that construct a
set of classi(cid:4)ers and then classify new data points by taking a (cid:5)weighted(cid:6)
vote of their predictions(cid:7) the original ensemble method is bayesian aver(cid:8)
aging(cid:2) but more recent algorithms include error(cid:8)correcting output coding(cid:2)
id112(cid:2) and boosting(cid:7) this paper reviews these methods and explains
why ensembles can often perform better than any single classi(cid:4)er(cid:7) some
previous studies comparing ensemble methods are reviewed(cid:2) and some
new experiments are presented to uncover the reasons that adaboost
does not over(cid:4)t rapidly(cid:7)

 

introduction

consider the standard supervised learning problem(cid:2) a learning program is given
training examples of the form f(cid:3)x (cid:0) y (cid:4)(cid:0) (cid:2) (cid:2) (cid:2) (cid:0) (cid:3)xm(cid:0) ym(cid:4)g for some unknown func(cid:5)
tion y (cid:6) f (cid:3)x(cid:4)(cid:2) the xi values are typically vectors of the form hxi(cid:2) (cid:0) xi(cid:2) (cid:0) (cid:2) (cid:2) (cid:2) (cid:0) xi(cid:2)ni
whose components are discrete(cid:5) or real(cid:5)valued such as height(cid:7) weight(cid:7) color(cid:7) age(cid:7)
and so on(cid:2) these are also called the features of xi(cid:2) let us use the notation xij
to refer to the j(cid:5)th feature of xi(cid:2) in some situations(cid:7) we will drop the i subscript
when it is implied by the context(cid:2)

the y values are typically drawn from a discrete set of classes f (cid:0) (cid:2) (cid:2) (cid:2) (cid:0) kg
in the case of classi(cid:2)cation or from the real line in the case of regression(cid:2) in
this chapter(cid:7) we will consider only classi(cid:9)cation(cid:2) the training examples may be
corrupted by some random noise(cid:2)

given a set s of training examples(cid:7) a learning algorithm outputs a classi(cid:2)er(cid:2)
the classi(cid:9)er is an hypothesis about the true function f (cid:2) given new x values(cid:7) it
predicts the corresponding y values(cid:2) i will denote classi(cid:9)ers by h (cid:0) (cid:2) (cid:2) (cid:2) (cid:0) hl(cid:2)

an ensemble of classi(cid:9)ers is a set of classi(cid:9)ers whose individual decisions are
combined in some way (cid:3)typically by weighted or unweighted voting(cid:4) to classify
new examples(cid:2) one of the most active areas of research in supervised learning has
been to study methods for constructing good ensembles of classi(cid:9)ers(cid:2) the main
discovery is that ensembles are often much more accurate than the individual
classi(cid:9)ers that make them up(cid:2)

a necessary and su(cid:10)cient condition for an ensemble of classi(cid:9)ers to be more
accurate than any of its individual members is if the classi(cid:9)ers are accurate and
diverse (cid:3)hansen (cid:11) salamon(cid:7)  		 (cid:4)(cid:2) an accurate classi(cid:9)er is one that has an
error rate of better than random guessing on new x values(cid:2) two classi(cid:9)ers are

 

diverse if they make di(cid:14)erent errors on new data points(cid:2) to see why accuracy
and diversity are good(cid:7) imagine that we have an ensemble of three classi(cid:9)ers(cid:15)
fh (cid:0) h (cid:0) h g and consider a new case x(cid:2) if the three classi(cid:9)ers are identical (cid:3)i(cid:2)e(cid:2)(cid:7)
not diverse(cid:4)(cid:7) then when h (cid:3)x(cid:4) is wrong(cid:7) h (cid:3)x(cid:4) and h (cid:3)x(cid:4) will also be wrong(cid:2)
however(cid:7) if the errors made by the classi(cid:9)ers are uncorrelated(cid:7) then when h (cid:3)x(cid:4)
is wrong(cid:7) h (cid:3)x(cid:4) and h (cid:3)x(cid:4) may be correct(cid:7) so that a majority vote will correctly
classify x(cid:2) more precisely(cid:7) if the error rates of l hypotheses h(cid:3) are all equal to
p (cid:3)  (cid:4)  and if the errors are independent(cid:7) then the id203 that the majority
vote will be wrong will be the area under the binomial distribution where more
than l(cid:4)  hypotheses are wrong(cid:2) figure   shows this for a simulated ensemble
of    hypotheses(cid:7) each having an error rate of  (cid:2) (cid:2) the area under the curve for
   or more hypotheses being simultaneously wrong is  (cid:2)   (cid:7) which is much less
than the error rate of the individual hypotheses(cid:2)

y
t
i
l
i
b
a
b
o
r
p

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

0

5

10

number of classifiers in error

15

20

fig(cid:2)  (cid:2) the id203 that exactly (cid:0) (cid:5)of   (cid:6) hypotheses will make an error(cid:2) assuming
each hypothesis has an error rate of  (cid:7)  and makes its errors independently of the other
hypotheses(cid:7)

of course(cid:7) if the individual hypotheses make uncorrelated errors at rates ex(cid:5)
ceeding  (cid:2) (cid:7) then the error rate of the voted ensemble will increase as a result of
the voting(cid:2) hence(cid:7) one key to successful ensemble methods is to construct indi(cid:5)
vidual classi(cid:9)ers with error rates below  (cid:2)  whose errors are at least somewhat
uncorrelated(cid:2)

this formal characterization of the problem is intriguing(cid:7) but it does not
address the question of whether it is possible in practice to construct good en(cid:5)
sembles(cid:2) fortunately(cid:7) it is often possible to construct very good ensembles(cid:2) there
are three fundamental reasons for this(cid:2)

 

the (cid:9)rst reason is statistical(cid:2) a learning algorithm can be viewed as search(cid:5)
ing a space h of hypotheses to identify the best hypothesis in the space(cid:2) the
statistical problem arises when the amount of training data available is too small
compared to the size of the hypothesis space(cid:2) without su(cid:10)cient data(cid:7) the learn(cid:5)
ing algorithm can (cid:9)nd many di(cid:14)erent hypotheses in h that all give the same
accuracy on the training data(cid:2) by constructing an ensemble out of all of these
accurate classi(cid:9)ers(cid:7) the algorithm can (cid:20)average(cid:21) their votes and reduce the risk
of choosing the wrong classi(cid:9)er(cid:2) figure  (cid:3)top left(cid:4) depicts this situation(cid:2) the
outer curve denotes the hypothesis space h(cid:2) the inner curve denotes the set of
hypotheses that all give good accuracy on the training data(cid:2) the point labeled f
is the true hypothesis(cid:7) and we can see that by averaging the accurate hypotheses(cid:7)
we can (cid:9)nd a good approximation to f (cid:2)

statistical

h

computational

h

h1

h4

h2
f

h3

h1

h2

f

h3

representational

h

h1

h2

f

h3

fig(cid:2)  (cid:2) three fundamental reasons why an ensemble may work better than a single
classi(cid:4)er

 

the second reason is computational(cid:2) many learning algorithms work by per(cid:5)
forming some form of local search that may get stuck in local optima(cid:2) for ex(cid:5)
ample(cid:7) neural network algorithms employ id119 to minimize an error
function over the training data(cid:7) and decision tree algorithms employ a greedy
splitting rule to grow the decision tree(cid:2) in cases where there is enough training
data (cid:3)so that the statistical problem is absent(cid:4)(cid:7) it may still be very di(cid:10)cult
computationally for the learning algorithm to (cid:9)nd the best hypothesis(cid:2) indeed(cid:7)
optimal training of both neural networks and decisions trees is np(cid:5)hard (cid:3)hya(cid:9)l
(cid:11) rivest(cid:7)  	  (cid:23) blum (cid:11) rivest(cid:7)  	  (cid:4)(cid:2) an ensemble constructed by running the
local search from many di(cid:14)erent starting points may provide a better approxi(cid:5)
mation to the true unknown function than any of the individual classi(cid:9)ers(cid:7) as
shown in figure   (cid:3)top right(cid:4)(cid:2)

the third reason is representational(cid:2) in most applications of machine learn(cid:5)
ing(cid:7) the true function f cannot be represented by any of the hypotheses in h(cid:2)
by forming weighted sums of hypotheses drawn from h(cid:7) it may be possible
to expand the space of representable functions(cid:2) figure   (cid:3)bottom(cid:4) depicts this
situation(cid:2)

the representational issue is somewhat subtle(cid:7) because there are many learn(cid:5)
ing algorithms for which h is(cid:7) in principle(cid:7) the space of all possible classi(cid:9)ers(cid:2) for
example(cid:7) neural networks and id90 are both very (cid:25)exible algorithms(cid:2)
given enough training data(cid:7) they will explore the space of all possible classi(cid:9)ers(cid:7)
and several people have proved asymptotic representation theorems for them
(cid:3)hornik(cid:7) stinchcombe(cid:7) (cid:11) white(cid:7)  		 (cid:4)(cid:2) nonetheless(cid:7) with a (cid:9)nite training sam(cid:5)
ple(cid:7) these algorithms will explore only a (cid:9)nite set of hypotheses and they will
stop searching when they (cid:9)nd an hypothesis that (cid:9)ts the training data(cid:2) hence(cid:7)
in figure  (cid:7) we must consider the space h to be the e(cid:14)ective space of hypotheses
searched by the learning algorithm for a given training data set(cid:2)

these three fundamental issues are the three most important ways in which
existing learning algorithms fail(cid:2) hence(cid:7) ensemble methods have the promise of
reducing (cid:3)and perhaps even eliminating(cid:4) these three key shortcomings of stan(cid:5)
dard learning algorithms(cid:2)

  methods for constructing ensembles

many methods for constructing ensembles have been developed(cid:2) here we will
review general purpose methods that can be applied to many di(cid:14)erent learning
algorithms(cid:2)

 (cid:3)  bayesian voting(cid:5) enumerating the hypotheses

in a bayesian probabilistic setting(cid:7) each hypothesis h de(cid:9)nes a conditional prob(cid:5)
ability distribution(cid:15) h(cid:3)x(cid:4) (cid:6) p (cid:3)f (cid:3)x(cid:4) (cid:6) yjx(cid:0) h(cid:4)(cid:2) given a new data point x and a
training sample s(cid:7) the problem of predicting the value of f (cid:3)x(cid:4) can be viewed
as the problem of computing p (cid:3)f (cid:3)x(cid:4) (cid:6) yjs(cid:0) x(cid:4)(cid:2) we can rewrite this as weighted

 

sum over all hypotheses in h(cid:15)

p (cid:3)f (cid:3)x(cid:4) (cid:6) yjs(cid:0) x(cid:4) (cid:6) xh h

h(cid:3)x(cid:4)p (cid:3)hjs(cid:4)(cid:2)

we can view this as an ensemble method in which the ensemble consists of all of
the hypotheses in h(cid:7) each weighted by its posterior id203 p (cid:3)hjs(cid:4)(cid:2) by bayes
rule(cid:7) the posterior id203 is proportional to the likelihood of the training
data times the prior id203 of h(cid:15)

p (cid:3)hjs(cid:4) (cid:2) p (cid:3)sjh(cid:4)p (cid:3)h(cid:4)(cid:2)

in some learning problems(cid:7) it is possible to completely enumerate each h   h(cid:7)
compute p (cid:3)sjh(cid:4) and p (cid:3)h(cid:4)(cid:7) and (cid:3)after id172(cid:4)(cid:7) evaluate this bayesian
(cid:20)committee(cid:2)(cid:21) furthermore(cid:7) if the true function f is drawn from h according to
p (cid:3)h(cid:4)(cid:7) then the bayesian voting scheme is optimal(cid:2)

bayesian voting primarily addresses the statistical component of ensem(cid:5)
bles(cid:2) when the training sample is small(cid:7) many hypotheses h will have signif(cid:5)
icantly large posterior probabilities(cid:7) and the voting process can average these to
(cid:20)marginalize away(cid:21) the remaining uncertainty about f (cid:2) when the training sam(cid:5)
ple is large(cid:7) typically only one hypothesis has substantial posterior id203(cid:7)
and the (cid:20)ensemble(cid:21) e(cid:14)ectively shrinks to contain only a single hypothesis(cid:2)

in complex problems where h cannot be enumerated(cid:7) it is sometimes possible
to approximate bayesian voting by drawing a random sample of hypotheses
distributed according to p (cid:3)hjs(cid:4)(cid:2) recent work on id115
methods (cid:3)neal(cid:7)  		 (cid:4) seeks to develop a set of tools for this task(cid:2)

the most idealized aspect of the bayesian analysis is the prior belief p (cid:3)h(cid:4)(cid:2) if
this prior completely captures all of the knowledge that we have about f before
we obtain s(cid:7) then by de(cid:9)nition we cannot do better(cid:2) but in practice(cid:7) it is often
di(cid:10)cult to construct a space h and assign a prior p (cid:3)h(cid:4) that captures our prior
knowledge adequately(cid:2) indeed(cid:7) often h and p (cid:3)h(cid:4) are chosen for computational
convenience(cid:7) and they are known to be inadequate(cid:2) in such cases(cid:7) the bayesian
committee is not optimal(cid:7) and other ensemble methods may produce better
results(cid:2) in particular(cid:7) the bayesian approach does not address the computational
and representational problems in any signi(cid:9)cant way(cid:2)

 (cid:3)  manipulating the training examples

the second method for constructing ensembles manipulates the training exam(cid:5)
ples to generate multiple hypotheses(cid:2) the learning algorithm is run several times(cid:7)
each time with a di(cid:14)erent subset of the training examples(cid:2) this technique works
especially well for unstable learning algorithms(cid:26)algorithms whose output clas(cid:5)
si(cid:9)er undergoes major changes in response to small changes in the training data(cid:2)
decision(cid:5)tree(cid:7) neural network(cid:7) and rule learning algorithms are all unstable(cid:2) lin(cid:5)
ear regression(cid:7) nearest neighbor(cid:7) and linear threshold algorithms are generally
very stable(cid:2)

 

the most straightforward way of manipulating the training set is called bag(cid:3)
ging(cid:2) on each run(cid:7) id112 presents the learning algorithm with a training set
that consists of a sample of m training examples drawn randomly with replace(cid:5)
ment from the original training set of m items(cid:2) such a training set is called a
bootstrap replicate of the original training set(cid:7) and the technique is called boot(cid:3)
strap aggregation (cid:3)from which the term id112 is derived(cid:23) breiman(cid:7)  		 (cid:4)(cid:2) each
bootstrap replicate contains(cid:7) on the average(cid:7)   (cid:2) (cid:27) of the original training set(cid:7)
with several training examples appearing multiple times(cid:2)

another training set sampling method is to construct the training sets by
leaving out disjoint subsets of the training data(cid:2) for example(cid:7) the training set
can be randomly divided into    disjoint subsets(cid:2) then    overlapping training
sets can be constructed by dropping out a di(cid:14)erent one of these    subsets(cid:2)
this same procedure is employed to construct training sets for   (cid:5)fold cross(cid:5)
validation(cid:7) so ensembles constructed in this way are sometimes called cross(cid:3)
validated committees (cid:3)parmanto(cid:7) munro(cid:7) (cid:11) doyle(cid:7)  		 (cid:4)(cid:2)

the third method for manipulating the training set is illustrated by the
adaboost algorithm(cid:7) developed by freund and schapire (cid:3) 		 (cid:7)  		 (cid:7)  		 (cid:7)
 		 (cid:4)(cid:2) like id112(cid:7) adaboost manipulates the training examples to generate
multiple hypotheses(cid:2) adaboost maintains a set of weights over the training
examples(cid:2) in each iteration (cid:5)(cid:7) the learning algorithm is invoked to minimize
the weighted error on the training set(cid:7) and it returns an hypothesis h(cid:3)(cid:2) the
weighted error of h(cid:3) is computed and applied to update the weights on the
training examples(cid:2) the e(cid:14)ect of the change in weights is to place more weight
on training examples that were misclassi(cid:9)ed by h(cid:3) and less weight on examples
that were correctly classi(cid:9)ed(cid:2) in subsequent iterations(cid:7) therefore(cid:7) adaboost
constructs progressively more di(cid:10)cult learning problems(cid:2)

the (cid:9)nal classi(cid:9)er(cid:7) hf (cid:3)x(cid:4) (cid:6) p(cid:3) w(cid:3)h(cid:3)(cid:3)x(cid:4)(cid:7) is constructed by a weighted vote

of the individual classi(cid:9)ers(cid:2) each classi(cid:9)er is weighted (cid:3)by w(cid:3)(cid:4) according to its
accuracy on the weighted training set that it was trained on(cid:2)

recent research (cid:3)schapire (cid:11) singer(cid:7)  		 (cid:4) has shown that adaboost can be
viewed as a stage(cid:5)wise algorithm for minimizing a particular error function(cid:2) to
de(cid:9)ne this error function(cid:7) suppose that each training example is labeled as (cid:28) 
or (cid:4) (cid:7) corresponding to the positive and negative examples(cid:2) then the quantity
mi (cid:6) yih(cid:3)xi(cid:4) is positive if h correctly classi(cid:9)es xi and negative otherwise(cid:2) this
quantity mi is called the margin of classi(cid:9)er h on the training data(cid:2) adaboost
can be seen as trying to minimize

exp(cid:2)(cid:4)yix(cid:3)

xi

w(cid:3)h(cid:3)(cid:3)xi(cid:4)(cid:3) (cid:0)

(cid:3) (cid:4)

which is the negative exponential of the margin of the weighted voted classi(cid:9)er(cid:2)
this can also be viewed as attempting to maximize the margin on the training
data(cid:2)

 

 (cid:3)  manipulating the input features

a third general technique for generating multiple classi(cid:9)ers is to manipulate
the set of input features available to the learning algorithm(cid:2) for example(cid:7) in a
project to identify volcanoes on venus(cid:7) cherkauer (cid:3) 		 (cid:4) trained an ensemble
of    neural networks(cid:2) the    networks were based on   di(cid:14)erent subsets of
the   	 available input features and   di(cid:14)erent network sizes(cid:2) the input feature
subsets were selected (cid:3)by hand(cid:4) to group together features that were based on
di(cid:14)erent image processing operations (cid:3)such as principal component analysis and
the fast fourier transform(cid:4)(cid:2) the resulting ensemble classi(cid:9)er was able to match
the performance of human experts in identifying volcanoes(cid:2) tumer and ghosh
(cid:3) 		 (cid:4) applied a similar technique to a sonar dataset with    input features(cid:2)
however(cid:7) they found that deleting even a few of the input features hurt the
performance of the individual classi(cid:9)ers so much that the voted ensemble did
not perform very well(cid:2) obviously(cid:7) this technique only works when the input
features are highly redundant(cid:2)

 (cid:3)  manipulating the output targets

a fourth general technique for constructing a good ensemble of classi(cid:9)ers is to
manipulate the y values that are given to the learning algorithm(cid:2) dietterich (cid:11)
bakiri (cid:3) 		 (cid:4) describe a technique called error(cid:5)correcting output coding(cid:2) suppose
that the number of classes(cid:7) k(cid:7) is large(cid:2) then new learning problems can be
constructed by randomly partioning the k classes into two subsets a(cid:3) and b(cid:3)(cid:2)
the input data can then be re(cid:5)labeled so that any of the original classes in set
a(cid:3) are given the derived label   and the original classes in set b(cid:3) are given
the derived label  (cid:2) this relabeled data is then given to the learning algorithm(cid:7)
which constructs a classi(cid:9)er h(cid:3)(cid:2) by repeating this process l times (cid:3)generating
di(cid:14)erent subsets a(cid:3) and b(cid:3)(cid:4)(cid:7) we obtain an ensemble of l classi(cid:9)ers h (cid:0) (cid:2) (cid:2) (cid:2) (cid:0) hl(cid:2)
now given a new data point x(cid:7) how should we classify it(cid:30) the answer is to
have each h(cid:3) classify x(cid:2) if h(cid:3)(cid:3)x(cid:4) (cid:6)  (cid:7) then each class in a(cid:3) receives a vote(cid:2) if
h(cid:3)(cid:3)x(cid:4) (cid:6)  (cid:7) then each class in b(cid:3) receives a vote(cid:2) after each of the l classi(cid:9)ers
has voted(cid:7) the class with the highest number of votes is selected as the prediction
of the ensemble(cid:2)

an equivalent way of thinking about this method is that each class j is
encoded as an l(cid:5)bit codeword cj(cid:7) where bit (cid:5) is   if and only if j   b(cid:3)(cid:2) the
(cid:5)(cid:5)th learned classi(cid:9)er attempts to predict bit (cid:5) of these codewords(cid:2) when the l
classi(cid:9)ers are applied to classify a new point x(cid:7) their predictions are combined
into an l(cid:5)bit string(cid:2) we then choose the class j whose codeword cj is closest (cid:3)in
hamming distance(cid:4) to the l(cid:5)bit output string(cid:2) methods for designing good error(cid:5)
correcting codes can be applied to choose the codewords cj (cid:3)or equivalently(cid:7)
subsets a(cid:3) and b(cid:3)(cid:4)(cid:2)

dietterich and bakiri report that this technique improves the performance of
both the c (cid:2)  decision tree algorithm and the id26 neural network
algorithm on a variety of di(cid:10)cult classi(cid:9)cation problems(cid:2) recently(cid:7) schapire

 

(cid:3) 		 (cid:4) has shown how adaboost can be combined with error(cid:5)correcting out(cid:5)
put coding to yield an excellent ensemble classi(cid:9)cation method that he calls ad(cid:2)
aboost(cid:3)oc(cid:2) the performance of the method is superior to the ecoc method
(cid:3)and to id112(cid:4)(cid:7) but essentially the same as another (cid:3)quite complex(cid:4) algorithm(cid:7)
called adaboost(cid:3)m (cid:2) hence(cid:7) the main advantage of adaboost(cid:3)oc is imple(cid:5)
mentation simplicity(cid:15) it can work with any learning algorithm for solving  (cid:5)class
problems(cid:2)

ricci and aha (cid:3) 		 (cid:4) applied a method that combines error(cid:5)correcting out(cid:5)
put coding with feature selection(cid:2) when learning each classi(cid:9)er(cid:7) h(cid:3)(cid:7) they apply
feature selection techniques to choose the best features for learning that classi(cid:9)er(cid:2)
they obtained improvements in   out of    tasks with this approach(cid:2)

 (cid:3)  injecting randomness

the last general purpose method for generating ensembles of classi(cid:9)ers is to
inject randomness into the learning algorithm(cid:2) in the id26 algorithm
for training neural networks(cid:7) the initial weights of the network are set randomly(cid:2)
if the algorithm is applied to the same training examples but with di(cid:14)erent
initial weights(cid:7) the resulting classi(cid:9)er can be quite di(cid:14)erent (cid:3)kolen (cid:11) pollack(cid:7)
 		 (cid:4)(cid:2)

while this is perhaps the most common way of generating ensembles of neu(cid:5)
ral networks(cid:7) manipulating the training set may be more e(cid:14)ective(cid:2) a study by
parmanto(cid:7) munro(cid:7) and doyle (cid:3) 		 (cid:4) compared this technique to id112 and to
  (cid:5)fold cross(cid:5)validated committees(cid:2) they found that cross(cid:5)validated committees
worked best(cid:7) id112 second best(cid:7) and multiple random initial weights third
best on one synthetic data set and two medical diagnosis data sets(cid:2)

for the c (cid:2)  decision tree algorithm(cid:7) it is also easy to inject randomness
(cid:3)kwok (cid:11) carter(cid:7)  		 (cid:23) dietterich(cid:7)     (cid:4)(cid:2) the key decision of c (cid:2)  is to choose a
feature to test at each internal node in the decision tree(cid:2) at each internal node(cid:7)
c (cid:2)  applies a criterion known as the information gain ratio to rank(cid:5)order the
various possible feature tests(cid:2) it then chooses the top(cid:5)ranked feature(cid:5)value test(cid:2)
for discrete(cid:5)valued features with v values(cid:7) the decision tree splits the data into
v subsets(cid:7) depending on the value of the chosen feature(cid:2) for real(cid:5)valued features(cid:7)
the decision tree splits the data into   subsets(cid:7) depending on whether the value
of the chosen feature is above or below a chosen threshold(cid:2) dietterich (cid:3)    (cid:4)
implemented a variant of c (cid:2)  that chooses randomly (cid:3)with equal id203(cid:4)
among the top    best tests(cid:2) figure   compares the performance of a single
run of c (cid:2)  to ensembles of     classi(cid:9)ers over    di(cid:14)erent data sets(cid:2) for each
data set(cid:7) a point is plotted(cid:2) if that point lies below the diagonal line(cid:7) then the
ensemble has lower error rate than c (cid:2) (cid:2) we can see that nearly all of the points
lie below the line(cid:2) a statistical analysis shows that the randomized trees do
statistically signi(cid:9)cantly better than a single decision tree on    of the data sets
and statistically the same in the remaining  	 data sets(cid:2)

ali (cid:11) pazzani (cid:3) 		 (cid:4) injected randomness into the foil algorithm for learn(cid:5)
ing prolog(cid:5)style rules(cid:2) foil works somewhat like c (cid:2)  in that it ranks possible
conditions to add to a rule using an information(cid:5)gain criterion(cid:2) ali and pazzani

	

)
r
o
r
r
e
 
t
n
e
c
r
e
p
(
 
5
.
4
c
 
d
e
z
i
m
o
d
n
a
r
 
d
l
o
f
-
0
0
2

60

50

40

30

20

10

0

0

10

20

30

c4.5 (percent error)

40

50

60

fig(cid:2)  (cid:2) comparison of the error rate of c (cid:7)  to an ensemble of     id90
constructed by injecting randomness into c (cid:7)  and then taking a uniform vote(cid:7)

computed all candidate conditions that scored within   (cid:27) of the top(cid:5)ranked can(cid:5)
didate(cid:7) and then applied a weighted random choice algorithm to choose among
them(cid:2) they compared ensembles of    classi(cid:9)ers to a single run of foil and
found statistically signi(cid:9)cant improvements in    out of  	 tasks and statistically
signi(cid:9)cant loss of performance in only one task(cid:2) they obtained similar results
using   (cid:5)fold cross(cid:5)validation to construct the training sets(cid:2)

raviv and intrator (cid:3) 		 (cid:4) combine bootstrap sampling of the training data
with injecting noise into the input features for the learning algorithm(cid:2) to train
each member of an ensemble of neural networks(cid:7) they draw training examples
with replacement from the original training data(cid:2) the x values of each training
example are perturbed by adding gaussian noise to the input features(cid:2) they
report large improvements in a synthetic benchmark task and a medical diagnosis
task(cid:2)

finally(cid:7) note that id115 methods for constructing bayesian

ensembles also work by injecting randomness into the learning process(cid:2) however(cid:7)
instead of taking a uniform vote(cid:7) as we did with the randomized id90(cid:7)
each hypothesis receives a vote proportional to its posterior id203(cid:2)

  comparing di(cid:4)erent ensemble methods

several experimental studies have been performed to compare ensemble methods(cid:2)
the largest of these are the studies by bauer and kohavi (cid:3) 			(cid:4) and by dietterich
(cid:3)    (cid:4)(cid:2) table   summarizes the results of dietterich(cid:31)s study(cid:2) the table shows
that adaboost often gives the best results(cid:2) id112 and randomized trees give

  

similar performance(cid:7) although randomization is able to do better in some cases
than id112 on very large data sets(cid:2)

table  (cid:2) all pairwise combinations of the four ensemble methods(cid:7) each cell contains
the number of wins(cid:2) losses(cid:2) and ties between the algorithm in that row and the algorithm
in that column(cid:7)

c (cid:7) 

random c (cid:7)     (cid:19)   (cid:19)  	
bagged c (cid:7)     (cid:19)   (cid:19)   
adaboost c (cid:7)     (cid:19)   (cid:19)   

adaboost c (cid:7)  bagged c (cid:7) 
  (cid:19)   (cid:19)   

  (cid:19)   (cid:19)   
  (cid:19)   (cid:19)   

most of the data sets in this study had little or no noise(cid:2) when   (cid:27) arti(cid:9)cial
classi(cid:9)cation noise was added to the 	 domains where id112 and adaboost
gave di(cid:14)erent performance(cid:7) the results shifted radically as shown in table  (cid:2)
under these conditions(cid:7) adaboost over(cid:9)ts the data badly while id112 is
shown to work very well in the presence of noise(cid:2) randomized trees did not do
very well(cid:2)

table  (cid:2) all pairwise combinations of c (cid:7) (cid:2) adaboosted c (cid:7) (cid:2) bagged c (cid:7) (cid:2) and
randomized c (cid:7)  on 	 domains with   (cid:20) synthetic class label noise(cid:7) each cell contains
the number of wins(cid:2) losses(cid:2) and ties between the algorithm in that row and the algorithm
in that column(cid:7)

c (cid:7)  adaboost c (cid:7)  bagged c (cid:7) 

random c (cid:7)    (cid:19)   (cid:19)  
bagged c (cid:7)    (cid:19)   (cid:19)  
adaboost c (cid:7)    (cid:19)   (cid:19)  

  (cid:19)   (cid:19)  
  (cid:19)   (cid:19)  

  (cid:19)   (cid:19)  

the key to understanding these results is to return again to the three short(cid:5)
comings of existing learning algorithms(cid:15) statistical support(cid:7) computation(cid:7) and
representation(cid:2) for the decision(cid:5)tree algorithm c (cid:2) (cid:7) all three of these prob(cid:5)
lems can arise(cid:2) id90 essentially partition the input feature space into
rectangular regions whose sides are perpendicular to the coordinate axes(cid:2) each
rectangular region corresponds to one leaf node of the tree(cid:2)

if the true function f can be represented by a small decision tree(cid:7) then
c (cid:2)  will work well without any ensemble(cid:2) if the true function can be correctly
represented by a large decision tree(cid:7) then c (cid:2)  will need a very large training
data set in order to (cid:9)nd a good (cid:9)t(cid:7) and the statistical problem will arise(cid:2)

the computational problem arises because (cid:9)nding the best (cid:3)i(cid:2)e(cid:2)(cid:7) smallest(cid:4)
decision tree consistent with the training data is computationally intractable(cid:7) so
c (cid:2)  makes a series of decisions greedily(cid:2) if one of these decisions is made incor(cid:5)
rectly(cid:7) then the training data will be incorrectly partitioned(cid:7) and all subsequent
decisions are likely to be a(cid:14)ected(cid:2) hence(cid:7) c (cid:2)  is highly unstable(cid:7) and small

  

changes in the training set can produce large changes in the resulting decision
tree(cid:2)

the representational problem arises because of the use of rectangular parti(cid:5)
tions of the input space(cid:2) if the true decision boundaries are not orthogonal to
the coordinate axes(cid:7) then c (cid:2)  requires a tree of in(cid:9)nite size to represent those
boundaries correctly(cid:2) interestingly(cid:7) a voted combination of small id90
is equivalent to a much larger single tree(cid:7) and hence(cid:7) an ensemble method can
construct a good approximation to a diagonal decision boundary using several
small trees(cid:2) figure   shows an example of this(cid:2) on the left side of the (cid:9)gure
are plotted three decision boundaries constructed by three id90(cid:7) each
of which uses   internal nodes(cid:2) on the right is the boundary that results from
a simple majority vote of these trees(cid:2) it is equivalent to a single tree with   
internal nodes(cid:7) and it is much more accurate than any one of the three individual
trees(cid:2)

class 1

class 1

class 2

class 2

fig(cid:2)  (cid:2) the left (cid:4)gure shows the true diagonal decision boundary and three staircase
approximations to it (cid:5)of the kind that are created by decision tree algorithms(cid:6)(cid:7) the
right (cid:4)gure shows the voted decision boundary(cid:2) which is a much better approximation
to the diagonal boundary(cid:7)

now let us consider the three algorithms(cid:15) adaboost(cid:7) id112(cid:7) and ran(cid:5)
domized trees(cid:2) id112 and randomization both construct each decision tree
independently of the others(cid:2) id112 accomplishes this by manipulating the in(cid:5)
put data(cid:7) and randomization directly alters the choices of c (cid:2) (cid:2) these methods
are acting somewhat like bayesian voting(cid:23) they are sampling from the space of
all possible hypotheses with a bias toward hypotheses that give good accuracy
on the training data(cid:2) consequently(cid:7) their main e(cid:14)ect will be to address the sta(cid:5)
tistical problem and(cid:7) to a lesser extent(cid:7) the computational problem(cid:2) but they do
not directly attempt to overcome the representational problem(cid:2)

in contrast(cid:7) adaboost constructs each new decision tree to eliminate (cid:20)resid(cid:5)
ual(cid:21) errors that have not been properly handled by the weighted vote of the
previously(cid:5)constructed trees(cid:2) adaboost is directly trying to optimize the weighted
vote(cid:2) hence(cid:7) it is making a direct assault on the representational problem(cid:2) di(cid:5)

  

rectly optimizing an ensemble can increase the risk of over(cid:9)tting(cid:7) because the
space of ensembles is usually much larger than the hypothesis space of the orig(cid:5)
inal algorithm(cid:2)

this explanation is consistent with the experimental results given above(cid:2) in
low(cid:5)noise cases(cid:7) adaboost gives good performance(cid:7) because it is able to opti(cid:5)
mize the ensemble without over(cid:9)tting(cid:2) however(cid:7) in high(cid:5)noise cases(cid:7) adaboost
puts a large amount of weight on the mislabeled examples(cid:7) and this leads it to
over(cid:9)t very badly(cid:2) id112 and randomization do well in both the noisy and
noise(cid:5)free cases(cid:7) because they are focusing on the statistical problem(cid:7) and noise
increases this statistical problem(cid:2)

finally(cid:7) we can understand that in very large datasets(cid:7) randomization can
be expected to do better than id112 because bootstrap replicates of a large
training set are very similar to the training set itself(cid:7) and hence(cid:7) the learned
decision tree will not be very diverse(cid:2) randomization creates diversity under all
conditions(cid:7) but at the risk of generating low(cid:5)quality id90(cid:2)

despite the plausibility of this explanation(cid:7) there is still one important open
question concerning adaboost(cid:2) given that adaboost aggressively attempts
to maximize the margins on the training set(cid:7) why doesn(cid:31)t it over(cid:9)t more often(cid:30)
part of the explanation may lie in the (cid:20)stage(cid:5)wise(cid:21) nature of adaboost(cid:2) in
each iteration(cid:7) it reweights the training examples(cid:7) constructs a new hypothesis(cid:7)
and chooses a weight w(cid:3) for that hypothesis(cid:2) it never (cid:20)backs up(cid:21) and modi(cid:9)es
the previous choices of hypotheses or weights that it has made to compensate
for this new hypothesis(cid:2)

to test this explanation(cid:7) i conducted a series of simple experiments on syn(cid:5)
thetic data(cid:2) let the true classi(cid:9)er f be a simple decision rule that tests just one
feature (cid:3)feature  (cid:4) and assigns the example to class (cid:28)  if the feature is  (cid:7) and
to class (cid:4)  if the feature is  (cid:2) now construct training (cid:3)and testing(cid:4) examples by
generating feature vectors of length     at random as follows(cid:2) generate feature
  (cid:3)the important feature(cid:4) at random(cid:2) then generate each of the other features
randomly to agree with feature   with id203  (cid:2)  and to disagree otherwise(cid:2)
assign labels to each training example according to the true function f (cid:7) but
with   (cid:27) random classi(cid:9)cation noise(cid:2) this creates a di(cid:10)cult learning problem
for simple decision rules of this kind (cid:3)decision stumps(cid:4)(cid:7) because all     features
are correlated with the class(cid:2) still(cid:7) a large ensemble should be able to do well on
this problem by voting separate decision stumps for each feature(cid:2)

i constructed a version of adaboost that works more aggressively than stan(cid:5)
dard adaboost(cid:2) after every new hypothesis h(cid:3) is constructed and its weight
assigned(cid:7) my version performs a id119 search to minimize the negative
exponential margin (cid:3)equation  (cid:4)(cid:2) hence(cid:7) this algorithm reconsiders the weights
of all of the learned hypotheses after each new hypothesis is added(cid:2) then it
reweights the training examples to re(cid:25)ect the revised hypothesis weights(cid:2)

figure   shows the results when training on a training set of size   (cid:2) the plot
con(cid:9)rms our explanation(cid:2) the aggressive adaboost initially has much higher
error rates on the test set than standard adaboost(cid:2) it then gradually im(cid:5)
proves(cid:2) meanwhile(cid:7) standard adaboost initially obtains excellent performance

on the test set(cid:7) but then it over(cid:9)ts as more and more classi(cid:9)ers are added to the
ensemble(cid:2) in the limit(cid:7) both ensembles should have the same representational
properties(cid:7) because they are both minimizing the same function (cid:3)equation  (cid:4)(cid:2)
but we can see that the exceptionally good performance of standard adaboost
on this problem is due to the stage(cid:5)wise optimization process(cid:7) which is slow to
(cid:9)t the data(cid:2)

  

t
e
s
 
a
t
a
d
 
t
s
e
t
 
e
h
t
 

n
o

 
)
0
0
0
1

 
f
o

 
t
u
o
(
 
s
r
o
r
r
e

210

205

200

195

190

185

180

175

170

165

160

aggressive adaboost

standard adaboost

1

10

iterations of adaboost

100

1000

fig(cid:2)  (cid:2) aggressive adaboost exhibits much worse performance than standard ad(cid:2)
aboost on a challenging synthetic problem

  conclusions

ensembles are well(cid:5)established as a method for obtaining highly accurate classi(cid:5)
(cid:9)ers by combining less accurate ones(cid:2) this paper has provided a brief survey of
methods for constructing ensembles and reviewed the three fundamental reasons
why ensemble methods are able to out(cid:5)perform any single classi(cid:9)er within the
ensemble(cid:2) the paper has also provided some experimental results to elucidate
one of the reasons why adaboost performs so well(cid:2)

one open question not discussed in this paper concerns the interaction be(cid:5)
tween adaboost and the properties of the underlying learning algorithm(cid:2) most
of the learning algorithms that have been combined with adaboost have been
algorithms of a global character (cid:3)i(cid:2)e(cid:2)(cid:7) algorithms that learn a relatively low(cid:5)
dimensional decision boundary(cid:4)(cid:2) it would be interesting to see whether local
algorithms (cid:3)such as radial basis functions and nearest neighbor methods(cid:4) can be
pro(cid:9)tably combined via adaboost to yield interesting new learning algorithms(cid:2)

bibliography

ali(cid:7) k(cid:2) m(cid:2)(cid:7) (cid:11) pazzani(cid:7) m(cid:2) j(cid:2) (cid:3) 		 (cid:4)(cid:2) error reduction through learning multiple

descriptions(cid:2) machine learning(cid:7)    (cid:3) (cid:4)(cid:7)        (cid:2)

bauer(cid:7) e(cid:2)(cid:7) (cid:11) kohavi(cid:7) r(cid:2) (cid:3) 			(cid:4)(cid:2) an empirical comparison of voting classi(cid:9)cation
algorithms(cid:15) id112(cid:7) boosting(cid:7) and variants(cid:2) machine learning(cid:7)    (cid:3) ! (cid:4)(cid:7)
      	(cid:2)

blum(cid:7) a(cid:2)(cid:7) (cid:11) rivest(cid:7) r(cid:2) l(cid:2) (cid:3) 	  (cid:4)(cid:2) training a  (cid:5)node neural network is np(cid:5)
complete (cid:3)extended abstract(cid:4)(cid:2) in proceedings of the  	   workshop on
computational learning theory(cid:7) pp(cid:2) 	    san francisco(cid:7) ca(cid:2) morgan
kaufmann(cid:2)

breiman(cid:7) l(cid:2) (cid:3) 		 (cid:4)(cid:2) id112 predictors(cid:2) machine learning(cid:7)    (cid:3) (cid:4)(cid:7)        (cid:2)
cherkauer(cid:7) k(cid:2) j(cid:2) (cid:3) 		 (cid:4)(cid:2) human expert(cid:5)level performance on a scienti(cid:9)c
image analysis task by a system using combined arti(cid:9)cial neural net(cid:5)
works(cid:2)
in chan(cid:7) p(cid:2) (cid:3)ed(cid:2)(cid:4)(cid:7) working notes of the aaai workshop
on integrating multiple learned models(cid:7) pp(cid:2)      (cid:2) available from
http(cid:2)(cid:3)(cid:3)www(cid:4)cs(cid:4)fit(cid:4)edu(cid:3)(cid:5)imlm(cid:3)(cid:2)

dietterich(cid:7) t(cid:2) g(cid:2) (cid:3)    (cid:4)(cid:2) an experimental comparison of three methods for
constructing ensembles of id90(cid:15) id112(cid:7) boosting(cid:7) and random(cid:5)
ization(cid:2) machine learning(cid:2)

dietterich(cid:7) t(cid:2) g(cid:2)(cid:7) (cid:11) bakiri(cid:7) g(cid:2) (cid:3) 		 (cid:4)(cid:2) solving multiclass learning problems via
error(cid:5)correcting output codes(cid:2) journal of arti(cid:2)cial intelligence research(cid:7)
 (cid:7)        (cid:2)

freund(cid:7) y(cid:2)(cid:7) (cid:11) schapire(cid:7) r(cid:2) e(cid:2) (cid:3) 		 (cid:4)(cid:2) a decision(cid:5)theoretic generalization of
on(cid:5)line learning and an application to boosting(cid:2) tech(cid:2) rep(cid:2)(cid:7) at(cid:11)t bell
laboratories(cid:7) murray hill(cid:7) nj(cid:2)

freund(cid:7) y(cid:2)(cid:7) (cid:11) schapire(cid:7) r(cid:2) e(cid:2) (cid:3) 		 (cid:4)(cid:2) experiments with a new boosting algo(cid:5)
rithm(cid:2) in proc(cid:11)   th international conference on machine learning(cid:7) pp(cid:2)
       (cid:2) morgan kaufmann(cid:2)

hansen(cid:7) l(cid:2)(cid:7) (cid:11) salamon(cid:7) p(cid:2) (cid:3) 		 (cid:4)(cid:2) neural network ensembles(cid:2) ieee trans(cid:11)

pattern analysis and machine intell(cid:11)(cid:7)   (cid:7) 		      (cid:2)

hornik(cid:7) k(cid:2)(cid:7) stinchcombe(cid:7) m(cid:2)(cid:7) (cid:11) white(cid:7) h(cid:2) (cid:3) 		 (cid:4)(cid:2) universal approximation
of an unknown mapping and its derivatives using multilayer feedforward
networks(cid:2) neural networks(cid:7)  (cid:7)        (cid:2)

hya(cid:9)l(cid:7) l(cid:2)(cid:7) (cid:11) rivest(cid:7) r(cid:2) l(cid:2) (cid:3) 	  (cid:4)(cid:2) constructing optimal binary id90 is

np(cid:5)complete(cid:2) information processing letters(cid:7)   (cid:3) (cid:4)(cid:7)      (cid:2)

kolen(cid:7) j(cid:2) f(cid:2)(cid:7) (cid:11) pollack(cid:7) j(cid:2) b(cid:2) (cid:3) 		 (cid:4)(cid:2) back propagation is sensitive to initial
conditions(cid:2) in advances in neural information processing systems(cid:7) vol(cid:2)  (cid:7)
pp(cid:2)         san francisco(cid:7) ca(cid:2) morgan kaufmann(cid:2)

kwok(cid:7) s(cid:2) w(cid:2)(cid:7) (cid:11) carter(cid:7) c(cid:2) (cid:3) 		 (cid:4)(cid:2) multiple id90(cid:2) in schachter(cid:7) r(cid:2) d(cid:2)(cid:7)
levitt(cid:7) t(cid:2) s(cid:2)(cid:7) kannal(cid:7) l(cid:2) n(cid:2)(cid:7) (cid:11) lemmer(cid:7) j(cid:2) f(cid:2) (cid:3)eds(cid:2)(cid:4)(cid:7) uncertainty in ar(cid:3)
ti(cid:2)cial intelligence  (cid:7) pp(cid:2)        (cid:2) elsevier science(cid:7) amsterdam(cid:2)

  

neal(cid:7) r(cid:2) (cid:3) 		 (cid:4)(cid:2) probabilistic id136 using id115 meth(cid:5)
ods(cid:2) tech(cid:2) rep(cid:2) crg(cid:5)tr(cid:5)	 (cid:5) (cid:7) department of computer science(cid:7) univer(cid:5)
sity of toronto(cid:7) toronto(cid:7) ca(cid:2)

parmanto(cid:7) b(cid:2)(cid:7) munro(cid:7) p(cid:2) w(cid:2)(cid:7) (cid:11) doyle(cid:7) h(cid:2) r(cid:2) (cid:3) 		 (cid:4)(cid:2)

improving committee
diagnosis with resampling techniques(cid:2) in touretzky(cid:7) d(cid:2) s(cid:2)(cid:7) mozer(cid:7) m(cid:2) c(cid:2)(cid:7)
(cid:11) hesselmo(cid:7) m(cid:2) e(cid:2) (cid:3)eds(cid:2)(cid:4)(cid:7) advances in neural information processing
systems(cid:7) vol(cid:2)  (cid:7) pp(cid:2)         cambridge(cid:7) ma(cid:2) mit press(cid:2)

raviv(cid:7) y(cid:2)(cid:7) (cid:11) intrator(cid:7) n(cid:2) (cid:3) 		 (cid:4)(cid:2) id64 with noise(cid:15) an e(cid:14)ective regu(cid:5)

larization technique(cid:2) connection science(cid:7)   (cid:3)   (cid:4)(cid:7)        (cid:2)

ricci(cid:7) f(cid:2)(cid:7) (cid:11) aha(cid:7) d(cid:2) w(cid:2) (cid:3) 		 (cid:4)(cid:2) extending local learners with error(cid:5)correcting
output codes(cid:2) tech(cid:2) rep(cid:2)(cid:7) naval center for applied research in arti(cid:9)cial
intelligence(cid:7) washington(cid:7) d(cid:2)c(cid:2)

schapire(cid:7) r(cid:2) e(cid:2) (cid:3) 		 (cid:4)(cid:2) using output codes to boost multiclass learning prob(cid:5)
lems(cid:2) in proceedings of the fourteenth international conference on ma(cid:3)
chine learning(cid:7) pp(cid:2)         san francisco(cid:7) ca(cid:2) morgan kaufmann(cid:2)

schapire(cid:7) r(cid:2) e(cid:2)(cid:7) freund(cid:7) y(cid:2)(cid:7) bartlett(cid:7) p(cid:2)(cid:7) (cid:11) lee(cid:7) w(cid:2) s(cid:2) (cid:3) 		 (cid:4)(cid:2) boosting the mar(cid:5)
gin(cid:15) a new explanation for the e(cid:14)ectiveness of voting methods(cid:2) in fisher(cid:7)
d(cid:2) (cid:3)ed(cid:2)(cid:4)(cid:7) machine learning(cid:13) proceedings of the fourteenth international
conference(cid:2) morgan kaufmann(cid:2)

schapire(cid:7) r(cid:2) e(cid:2)(cid:7) (cid:11) singer(cid:7) y(cid:2) (cid:3) 		 (cid:4)(cid:2)

improved boosting algorithms using
con(cid:9)dence(cid:5)rated predictions(cid:2) in proc(cid:11)   th annu(cid:11) conf(cid:11) on comput(cid:11) learn(cid:3)
ing theory(cid:7) pp(cid:2)    	 (cid:2) acm press(cid:7) new york(cid:7) ny(cid:2)

tumer(cid:7) k(cid:2)(cid:7) (cid:11) ghosh(cid:7) j(cid:2) (cid:3) 		 (cid:4)(cid:2) error correlation and error reduction in ensemble

classi(cid:9)ers(cid:2) connection science(cid:7)   (cid:3)   (cid:4)(cid:7)        (cid:2)

