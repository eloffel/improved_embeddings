a tutorial on principal component analysis

derivation, discussion and singular value decomposition

jon shlens | jonshlens@ucsd.edu

25 march 2003 | version 1

principal component analysis (pca) is a mainstay
of modern data analysis - a black box that is widely
used but poorly understood. the goal of this paper is
to dispel the magic behind this black box. this tutorial
focuses on building a solid intuition for how and why
principal component analysis works; furthermore, it
crystallizes this knowledge by deriving from    rst prin-
cipals, the mathematics behind pca . this tutorial
does not shy away from explaining the ideas infor-
mally, nor does it shy away from the mathematics.
the hope is that by addressing both aspects, readers
of all levels will be able to gain a better understand-
ing of the power of pca as well as the when, the how
and the why of applying this technique.

1 overview

principal component analysis (pca) has been called
one of the most valuable results from applied lin-
ear algebra. pca is used abundantly in all forms
of analysis - from neuroscience to computer graphics
- because it is a simple, non-parametric method of
extracting relevant information from confusing data
sets. with minimal additional e   ort pca provides
a roadmap for how to reduce a complex data set to
a lower dimension to reveal the sometimes hidden,
simpli   ed dynamics that often underlie it.

the goal of this tutorial is to provide both an intu-
itive feel for pca, and a thorough discussion of this
topic. we will begin with a simple example and pro-
vide an intuitive explanation of the goal of pca. we
will continue by adding mathematical rigor to place
it within the framework of id202 and explic-
itly solve this problem. we will see how and why
pca is intimately related to the mathematical tech-
nique of singular value decomposition (svd). this
understanding will lead us to a prescription for how
to apply pca in the real world. we will discuss both

the assumptions behind this technique as well as pos-
sible extensions to overcome these limitations.

the discussion and explanations in this paper are
informal in the spirit of a tutorial. the goal of this
paper is to educate. occasionally, rigorous mathe-
matical proofs are necessary although relegated to
the appendix. although not as vital to the tutorial,
the proofs are presented for the adventurous reader
who desires a more complete understanding of the
math. the only assumption is that the reader has a
working knowledge of id202. nothing more.
please feel free to contact me with any suggestions,
corrections or comments.

2 motivation: a toy example

here is the perspective: we are an experimenter. we
are trying to understand some phenomenon by mea-
suring various quantities (e.g. spectra, voltages, ve-
locities, etc.) in our system. unfortunately, we can
not    gure out what is happening because the data
appears clouded, unclear and even redundant. this
is not a trivial problem, but rather a fundamental
obstacle to experimental science. examples abound
from complex systems such as neuroscience, photo-
science, meteorology and oceanography - the number
of variables to measure can be unwieldy and at times
even deceptive, because the underlying dynamics can
often be quite simple.

take for example a simple toy problem from
physics diagrammed in figure 1. pretend we are
studying the motion of the physicist   s ideal spring.
this system consists of a ball of mass m attached to
a massless, frictionless spring. the ball is released a
small distance away from equilibrium (i.e. the spring
is stretched). because the spring is    ideal,    it oscil-
lates inde   nitely along the x-axis about its equilib-
rium at a set frequency.

this is a standard problem in physics in which the

1

that we need to deal with air, imperfect cameras or
even friction in a less-than-ideal spring. noise con-
taminates our data set only serving to obfuscate the
dynamics further. this toy example is the challenge
experimenters face everyday. we will refer to this
example as we delve further into abstract concepts.
hopefully, by the end of this paper we will have a
good understanding of how to systematically extract
x using principal component analysis.

3 framework: change of basis

the goal: principal component analysis computes
the most meaningful basis to re-express a noisy, gar-
bled data set. the hope is that this new basis will
   lter out the noise and reveal hidden dynamics. in
the example of the spring, the explicit goal of pca is
to determine:    the dynamics are along the x-axis.   
in other words, the goal of pca is to determine that
  x - the unit basis vector along the x-axis - is the im-
portant dimension. determining this fact allows an
experimenter to discern which dynamics are impor-
tant and which are just redundant.

3.1 a naive basis

with a more precise de   nition of our goal, we need
a more precise de   nition of our data as well. for
each time sample (or experimental trial), an exper-
imenter records a set of data consisting of multiple
measurements (e.g. voltage, position, etc.). the
number of measurement types is the dimension of
the data set. in the case of the spring, this data set
has 12,000 6-dimensional vectors, where each camera
contributes a 2-dimensional projection of the ball   s
position.

in general, each data sample is a vector in m-
dimensional space, where m is the number of mea-
surement types. equivalently, every time sample is
a vector that lies in an m-dimensional vector space
spanned by an orthonormal basis. all measurement
vectors in this space are a linear combination of this
set of unit length basis vectors. a naive and simple

figure 1: a diagram of the toy example.

motion along the x direction is solved by an explicit
function of time. in other words, the underlying dy-
namics can be expressed as a function of a single vari-
able x.

however, being ignorant experimenters we do not
know any of this. we do not know which, let alone
how many, axes and dimensions are important to
measure. thus, we decide to measure the ball   s posi-
tion in a three-dimensional space (since we live in a
three dimensional world). speci   cally, we place three
movie cameras around our system of interest. at
200 hz each movie camera records an image indicat-
ing a two dimensional position of the ball (a projec-
tion). unfortunately, because of our ignorance, we
do not even know what are the real    x   ,    y    and    z   
axes, so we choose three camera axes {~a, ~b, ~c} at some
arbitrary angles with respect to the system. the an-
gles between our measurements might not even be
90o! now, we record with the cameras for 2 minutes.
the big question remains: how do we get from this
data set to a simple equation of x?

we know a-priori that if we were smart experi-
menters, we would have just measured the position
along the x-axis with one camera. but this is not
what happens in the real world. we often do not
know what measurements best re   ect the dynamics
of our system in question. furthermore, we some-
times record more dimensions than we actually need!
also, we have to deal with that pesky, real-world
in the toy example this means

problem of noise.

2

choice of a basis b is the identity matrix i.

b =

b1
b2
...
bm

   
            

   
            

=

1 0
0 1
...
...
0 0

   
            

      
      
. . .
      

0
0
...
1

   
            

= i

with this assumption pca is now limited to re-
expressing the data as a linear combination of its ba-
sis vectors. let x and y be m  n matrices related by
a linear transformation p. x is the original recorded
data set and y is a re-representation of that data set.

px = y

(1)

where each row is a basis vector bi with m compo-
nents.

also let us de   ne the following quantities2.

to summarize, at one point in time, camera a
records a corresponding position (xa(t), ya(t)). each
trial can be expressed as a six dimesional column vec-
tor ~x.

~x =

xa
ya
xb
yb
xc
yc

   

                     

   

                     

where each camera contributes two points and the
entire vector ~x is the set of coe   cients in the naive
basis b.

3.2 change of basis

with this rigor we may now state more precisely
what pca asks: is there another basis, which is a
linear combination of the original basis, that best re-
expresses our data set?

a close reader might have noticed the conspicuous
addition of the word linear. indeed, pca makes one
stringent but powerful assumption:
linearity. lin-
earity vastly simpli   es the problem by (1) restricting
the set of potential bases, and (2) formalizing the im-
plicit assumption of continuity in a data set. a subtle
point it is, but we have already assumed linearity by
implicitly stating that the data set even characterizes
the dynamics of the system! in other words, we are
already relying on the superposition principal of lin-
earity to believe that the data characterizes or pro-
vides an ability to interpolate between the individual
data points1.

1to be sure, complex systems are almost always nonlinear
and often their main qualitative features are a direct result of
their nonlinearity. however, locally linear approximations usu-
ally provide a good approximation because non-linear, higher
order terms vanish at the limit of small perturbations.

    pi are the rows of p
    xi are the columns of x
    yi are the columns of y.

equation 1 represents a change of basis and thus can
have many interpretations.

1. p is a matrix that transforms x into y.

2. geometrically, p is a rotation and a stretch

which again transforms x into y.

3. the rows of p, {p1, . . . , pm}, are a set of new

basis vectors for expressing the columns of x.

the latter interpretation is not obvious but can be
seen by writing out the explicit dot products of px.

px =    
      
y =    
      

   
         x1

p1
...
pm
p1    x1

...

pm    x1

       xn   
       p1    xn
. . .
       pm    xn

...

   
      

we can note the form of each column of y.

yi =    
      

p1    xi

...

pm    xi

   
      

is a dot-
we recognize that each coe   cient of yi
product of xi with the corresponding row in p. in

2in this section xi and yi are column vectors, but be fore-

warned. in all other sections xi and yi are row vectors.

3

other words, the jth coe   cient of yi is a projection
on to the jth row of p. this is in fact the very form
of an equation where yi is a projection on to the basis
of {p1, . . . , pm}. therefore, the rows of p are indeed
a new set of basis vectors for representing of columns
of x.

3.3 questions remaining

by assuming linearity the problem reduces to    nd-
ing the appropriate change of basis. the row vectors
{p1, . . . , pm} in this transformation will become the
principal components of x. several questions now
arise.

    what is the best way to    re-express    x?
    what is a good choice of basis p?

these questions must be answered by next asking our-
selves what features we would like y to exhibit. ev-
idently, additional assumptions beyond linearity are
required to arrive at a reasonable result. the selec-
tion of these assumptions is the subject of the next
section.

4 variance and the goal

now comes the most important question: what does
   best express    the data mean? this section will build
up an intuitive answer to this question and along the
way tack on additional assumptions. the end of this
section will conclude with a mathematical goal for
deciphering    garbled    data.

in a linear system    garbled    can refer to only two
potential confounds: noise and redundancy. let us
deal with each situation individually.

figure 2: a simulated plot of (xa, ya) for camera a.
the signal and noise variances   2
noise are
graphically represented.

signal and   2

measurement. a common measure is the the signal-
to-noise ratio (snr), or a ratio of variances   2.

sn r =

  2
signal
  2

noise

(2)

a high snr (   1) indicates high precision data,
while a low snr indicates noise contaminated data.
pretend we plotted all data from camera a from
the spring in figure 2. any individual camera should
record motion in a straight line. therefore, any
spread deviating from straight-line motion must be
noise. the variance due to the signal and noise are
indicated in the diagram graphically. the ratio of the
two, the snr, thus measures how    fat    the oval is - a
range of possiblities include a thin line (snr    1), a
perfect circle (snr = 1) or even worse. in summary,
we must assume that our measurement devices are
reasonably good. quantitatively, this corresponds to
a high snr.

4.1 noise

noise in any data set must be low or - no matter the
analysis technique - no information about a system
can be extracted. there exists no absolute scale for
noise but rather all noise is measured relative to the

4.2 redundancy

redundancy is a more tricky issue. this issue is par-
ticularly evident in the example of the spring.
in
this case multiple sensors record the same dynamic

4

calculate something like the variance. i say    some-
thing like    because the variance is the spread due to
one variable but we are concerned with the spread
between variables.

consider two sets of simultaneous measurements

with zero mean3.

a = {a1, a2, . . . , an} , b = {b1, b2, . . . , bn}

the variance of a and b are individually de   ned as
follows.

a = haiaiii ,   2
  2

b = hbibiii

where the expectation4 is the average over n vari-
ables. the covariance between a and b is a straigh-
forward generalization.

covariance of a and b       2

ab = haibiii

two important facts about the covariance.

      2

ab = 0 if and only if a and b are entirely
uncorrelated.

ab =   2

a if a = b.

      2

we can equivalently convert the sets of a and b into
corresponding row vectors.

a = [a1 a2 . . . an]
b = [b1 b2 . . . bn]

figure 3: a spectrum of possible redundancies in
data from the two separate recordings r1 and r2 (e.g.
xa, yb). the best-   t line r2 = kr1 is indicated by
the dashed line.

information. consider figure 3 as a range of possi-
bile plots between two arbitrary measurement types
r1 and r2. panel (a) depicts two recordings with no
redundancy. in other words, r1 is entirely uncorre-
lated with r2. this situation could occur by plotting
two variables such as (xa, humidity).

however, in panel (c) both recordings appear to be
strongly related. this extremity might be achieved
by several means.

    a plot of (xa,   xa) where xa is in meters and   xa

is in inches.

    a plot of (xa, xb) if cameras a and b are very

nearby.

so that we may now express the covariance as a dot
product matrix computation.

clearly in panel (c) it would be more meaningful to
just have recorded a single variable, the linear com-
bination r2     kr1, instead of two variables r1 and r2
separately. recording solely the linear combination
r2     kr1 would both express the data more concisely
and reduce the number of sensor recordings (2     1
indeed, this is the very idea behind di-
variables).
mensional reduction.

4.3 covariance matrix

the snr is solely determined by calculating vari-
ances. a corresponding but simple way to quantify
the redundancy between individual recordings is to

5

  2
ab    

abt

1

n     1

(3)

where the beginning term is a constant for normal-
ization5.

finally, we can generalize from two vectors to an
arbitrary number. we can rename the row vectors
x1     a, x2     b and consider additional indexed row
3these data sets are in mean deviation form because the

means have been subtracted o    or are zero.

4h  ii denotes the average over values indexed by i.
5the simplest possible id172 is 1

n . however, this
provides a biased estimation of variance particularly for small
n. it is beyond the scope of this paper to show that the proper
id172 for an unbiased estimator is

1

1 .

n

   

vectors x3, . . . , xm. now we can de   ne a new m    n
matrix x.

x =    
      

x1
...
xm

   
      

(4)

one interpretation of x is the following. each row
of x corresponds to all measurements of a particular
type (xi). each column of x corresponds to a set
of measurements from one particular trial (this is ~x
from section 3.1). we now arrive at a de   nition for
the covariance matrix sx.

sx    

1

n     1

xxt

(5)

consider how the matrix form xxt , in that order,
computes the desired value for the ijth element of sx.
speci   cally, the ijth element of the variance is the dot
product between the vector of the ith measurement
type with the vector of the jth measurement type.

    the ijth value of xxt is equivalent to substi-

tuting xi and xj into equation 3.

    sx is a square symmetric m    m matrix.
    the diagonal terms of sx are the variance of

particular measurement types.

    the o   -diagonal terms of sx are the covariance

between measurement types.

computing sx quanti   es the correlations between all
possible pairs of measurements. between one pair of
measurements, a large covariance corresponds to a
situation like panel (c) in figure 3, while zero covari-
ance corresponds to entirely uncorrelated data as in
panel (a).

sx is special. the covariance matrix describes all
relationships between pairs of measurements in our
data set. pretend we have the option of manipulat-
ing sx. we will suggestively de   ne our manipulated
covariance matrix sy. what features do we want to
optimize in sy?

4.4 diagonalize the covariance matrix

if our goal is to reduce redundancy, then we would
like each variable to co-vary as little as possible with
other variables. more precisely, to remove redun-
dancy we would like the covariances between separate
measurements to be zero. what would the optimized
covariance matrix sy look like? evidently, in an    op-
timized    matrix all o   -diagonal terms in sy are zero.
therefore, removing redundancy diagnolizes sy.

there are many methods for diagonalizing sy. it
is curious to note that pca arguably selects the eas-
iest method, perhaps accounting for its widespread
application.

pca assumes that all basis vectors {p1, . . . , pm}
are orthonormal (i.e. pi    pj =   ij). in the language
of id202, pca assumes p is an orthonormal
matrix. secondly pca assumes the directions with
the largest variances are the most    important    or in
other words, most principal. why are these assump-
tions easiest?

envision how pca might work. by the variance
assumption pca    rst selects a normalized direction
in m-dimensional space along which the variance in
x is maximized - it saves this as p1. again it    nds
another direction along which variance is maximized,
however, because of the orthonormality condition, it
restricts its search to all directions perpindicular to
all previous selected directions. this could continue
until m directions are selected. the resulting ordered
set of p   s are the principal components.

in principle this simple pseudo-algorithm works,
however that would bely the true reason why the
orthonormality assumption is particularly judicious.
namely, the true bene   t to this assumption is that it
makes the solution amenable to id202. there
exist decompositions that can provide e   cient, ex-
plicit algebraic solutions.

notice what we also gained with the second as-
sumption. we have a method for judging the    im-
portance    of the each prinicipal direction. namely,
the variances associated with each direction pi quan-
tify how principal each direction is. we could thus
rank-order each basis vector pi according to the cor-
responding variances.

we will now pause to review the implications of all

6

the assumptions made to arrive at this mathematical
goal.

4.5 summary of assumptions and limits

this section provides an important context for un-
derstanding when pca might perform poorly as well
as a road map for understanding new extensions to
pca. the    nal section provides a more lengthy dis-
cussion about recent extensions.

that the data has a high snr. hence, princi-
pal components with larger associated vari-
ances represent interesting dynamics, while
those with lower variancees represent noise.

iv. the principal components are orthogonal.

this assumption provides an intuitive sim-
pli   cation that makes pca soluble with
id202 decomposition techniques.
these techniques are highlighted in the two
following sections.

i. linearity

linearity frames the problem as a change
of basis.
several areas of research have
explored how applying a nonlinearity prior
to performing pca could extend this algo-
rithm - this has been termed kernel pca.

we have discussed all aspects of deriving pca -
what remains is the id202 solutions. the
   rst solution is somewhat straightforward while the
second solution involves understanding an important
decomposition.

ii. mean and variance are su   cient statistics.
the formalism of su   cient statistics cap-
tures the notion that the mean and the vari-
ance entirely describe a id203 distribu-
tion. the only zero-mean id203 dis-
tribution that is fully described by the vari-
ance is the gaussian distribution.

in order for this assumption to hold, the
id203 distribution of xi must be gaus-
sian. deviations from a gaussian could in-
validate this assumption6. on the    ip side,
this assumption formally guarantees that
the snr and the covariance matrix fully
characterize the noise and redundancies.

iii. large variances have important dynamics.
this assumption also encompasses the belief

6a sidebar question: what if xi are not gaussian dis-
tributed? diagonalizing a covariance matrix might not pro-
duce satisfactory results. the most rigorous form of removing
redundancy is statistical independence.

p (y1, y2) = p (y1)p (y2)

where p (  ) denotes the id203 density. the class of algo-
rithms that attempt to    nd the y1, , . . . , ym that satisfy this
statistical constraint are termed independent component anal-
ysis (ica). in practice though, quite a lot of real world data
are gaussian distributed - thanks to the central limit theo-
rem - and pca is usually a robust solution to slight deviations
from this assumption.

5 solving pca: eigenvectors of

covariance

we will derive our    rst algebraic solution to pca
using id202. this solution is based on an im-
portant property of eigenvector decomposition. once
again, the data set is x, an m   n matrix, where m is
the number of measurement types and n is the num-
ber of data trials. the goal is summarized as follows.

find some orthonormal matrix p where
y = px such that sy     1
1 yyt is diag-
n   
onalized. the rows of p are the principal
components of x.

we begin by rewriting sy in terms of our variable of
choice p.

sy =

=

=

=

sy =

7

yyt

(px)(px)t

pxxt pt

p(xxt )pt

papt

1

1

1

n     1
n     1
n     1
n     1
n     1

1

1

note that we de   ned a new matrix a     xxt , where
a is symmetric (by theorem 2 of appendix a).
our roadmap is to recognize that a symmetric ma-
trix (a) is diagonalized by an orthogonal matrix of
its eigenvectors (by theorems 3 and 4 from appendix
a). for a symmetric matrix a theorem 4 provides:

in practice computing pca of a data set x entails (1)
subtracting o    the mean of each measurement type
and (2) computing the eigenvectors of xxt . this so-
lution is encapsulated in demonstration matlab code
included in appendix b.

a = edet

(6)

6 a more general solution: sin-

gular value decomposition

where d is a diagonal matrix and e is a matrix of
eigenvectors of a arranged as columns.

the matrix a has r     m orthonormal eigenvectors
where r is the rank of the matrix. the rank of a is
less than m when a is degenerate or all data occupy
a subspace of dimension r     m. maintaining the con-
straint of orthogonality, we can remedy this situation
by selecting (m     r) additional orthonormal vectors
to       ll up    the matrix e. these additional vectors
do not e   ect the    nal solution because the variances
associated with these directions are zero.

now comes the trick. we select the matrix p to
be a matrix where each row pi is an eigenvector of
xxt . by this selection, p     et. substituting into
equation 6, we    nd a = pt dp. with this relation
1 = pt ) we can
and theorem 1 of appendix a (p   
   nish evaluating sy.

sy =

=

=

=

sy =

1

1

1

n     1
n     1
n     1
n     1
n     1

1

1

papt

p(pt dp)pt

(ppt )d(ppt )

(pp   

1)d(pp   

1)

d

it is evident that the choice of p diagonalizes sy.
this was the goal for pca. we can summarize the
results of pca in the matrices p and sy.

    the principal components of x are the eigenvec-

tors of xxt ; or the rows of p.

    the ith diagonal value of sy is the variance of

x along pi.

this section is the most mathematically involved and
can be skipped without much loss of continuity. it is
presented solely for completeness. we derive another
algebraic solution for pca and in the process,    nd
that pca is closely related to singular value decom-
position (svd). in fact, the two are so intimately
related that the names are often used interchange-
ably. what we will see though is that svd is a more
general method of understanding change of basis.

we begin by quickly deriving the decomposition.
in the following section we interpret the decomposi-
tion and in the last section we relate these results to
pca.

6.1 singular value decomposition
let x be an arbitrary n    m matrix7 and xt x be a
rank r, square, symmetric n    n matrix. in a seem-
ingly unmotivated fashion, let us de   ne all of the
quantities of interest.

    {  v1,   v2, . . . ,   vr} is

set of orthonormal
m    1 eigenvectors with associated eigenvalues
{  1,   2, . . . ,   r} for the symmetric matrix xt x.

the

(xt x)  vi =   i  vi

      i          i are positive real and termed the sin-

gular values.

    {  u1,   u2, . . . ,   ur} is the set of orthonormal n    1

vectors de   ned by   ui     1

  i

x  vi.

we obtain the    nal de   nition by referring to theorem
5 of appendix a. the    nal de   nition includes two
new and unexpected properties.

7notice that in this section only we are reversing convention
from m  n to n  m. the reason for this derivation will become
clear in section 6.3.

8

      ui      uj =   ij
    kx  vik =   i

these properties are both proven in theorem 5. we
now have all of the pieces to construct the decompo-
sition. the    value    version of singular value decom-
position is just a restatement of the third de   nition.

x  vi =   i  ui

(7)

this result says a quite a bit. x multiplied by an
eigenvector of xt x is equal to a scalar times an-
other vector. the set of eigenvectors {  v1,   v2, . . . ,   vr}
and the set of vectors {  u1,   u2, . . . ,   ur} are both or-
thonormal sets or bases in r-dimensional space.
we can summarize this result for all vectors in
one id127 by following the prescribed
construction in figure 4. we start by constructing a
new diagonal matrix   .

      

    1

   

                           

. . .

0

    r

0

0

. . .

   

                           

0

where     1         2     . . .         r are the rank-ordered set of
singular values. likewise we construct accompanying
orthogonal matrices v and u.

v = [  v  1   v  2 . . .   v   m]
u = [  u  1   u  2 . . .   u  n]

where we have appended an additional (m     r) and
(n     r) orthonormal vectors to       ll up    the matri-
ces for v and u respectively8. figure 4 provides a
graphical representation of how all of the pieces    t
together to form the matrix version of svd.

xv = u  

(8)

where each column of v and u perform the    value   
version of the decomposition (equation 7). because

8this is the same procedure used to    xe the degeneracy in

the previous section.

v is orthogonal, we can multiply both sides by
1 = vt to arrive at the    nal form of the decom-
v   
position.

x = u  vt

(9)

although it was derived without motivation, this de-
composition is quite powerful. equation 9 states that
any arbitrary matrix x can be converted to an or-
thogonal matrix, a diagonal matrix and another or-
thogonal matrix (or a rotation, a stretch and a second
rotation). making sense of equation 9 is the subject
of the next section.

6.2 interpreting svd

the    nal form of svd (equation 9) is a concise but
thick statement to understand. let us instead rein-
terpret equation 7.

xa = kb

where a and b are column vectors and k is a scalar
constant. the set {  v1,   v2, . . . ,   vm} is analogous
to a and the set {  u1,   u2, . . . ,   un} is analogous to
b. what is unique though is that {  v1,   v2, . . . ,   vm}
and {  u1,   u2, . . . ,   un} are orthonormal sets of vectors
which span an m or n dimensional space, respec-
tively. in particular, loosely speaking these sets ap-
pear to span all possible    inputs    (a) and    outputs   
(b). can we formalize the view that {  v1,   v2, . . . ,   vn}
and {  u1,   u2, . . . ,   un} span all possible    inputs    and
   outputs   ?
we can manipulate equation 9 to make this fuzzy

hypothesis more precise.

x = u  vt

ut x =   vt
ut x = z

where we have de   ned z       vt .
note that
the previous columns {  u1,   u2, . . . ,   un} are now
rows in ut . comparing this equation to equa-
tion 1, {  u1,   u2, . . . ,   un} perform the same role as
{  p1,   p2, . . . ,   pm}. hence, ut is a change of basis
from x to z. just as before, we were transform-
ing column vectors, we can again infer that we are

9

the    value    form of svd is expressed in equation 7.

x  vi =   i  ui

the mathematical intuition behind the construction of the matrix form is that we want to express all
n    value    equations in just one equation. it is easiest to understand this process graphically. drawing
the matrices of equation 7 looks likes the following.

we can construct three new matrices v, u and   . all singular values are    rst rank-ordered
    1         2     . . .         r, and the corresponding vectors are indexed in the same rank order. each pair
of associated vectors   vi and   ui is stacked in the ith column along their respective matrices. the cor-
responding singular value   i is placed along the diagonal (the iith position) of   . this generates the
equation xv = u  , which looks like the following.

the matrices v and u are m    m and n    n matrices respectively and    is a diagonal matrix with
a few non-zero values (represented by the checkerboard) along its diagonal. solving this single matrix
equation solves all n    value    form equations.

figure 4: how to construct the matrix form of svd from the    value    form.

transforming column vectors. the fact that the or-
thonormal basis ut (or p) transforms column vec-
tors means that ut is a basis that spans the columns
of x. bases that span the columns are termed the
column space of x. the column space formalizes the
notion of what are the possible    outputs    of any ma-
trix.

there is a funny symmetry to svd such that we

can de   ne a similar quantity - the row space.

xv =   u

(xv)t = (  u)t
vt xt = ut   

10

vt xt = z

where we have de   ned z     ut  . again the rows of
vt (or the columns of v) are an orthonormal basis
for transforming xt into z. because of the trans-
pose on x, it follows that v is an orthonormal basis
spanning the row space of x. the row space likewise
formalizes the notion of what are possible    inputs   
into an arbitrary matrix.

we are only scratching the surface for understand-
ing the full implications of svd. for the purposes of
this tutorial though, we have enough information to
understand how pca will fall within this framework.

6.3 svd and pca

with similar computations it is evident that the two
methods are intimately related. let us return to the
original m    n data matrix x. we can de   ne a new
matrix y as an n    m matrix9.
   n     1

y    

xt

1

where each column of y has zero mean. the de   ni-
tion of y becomes clear by analyzing yt y.

yt y =    1

xt  t    1

   n     1

xt  

xt t xt

1

   n     1
n     1
n     1

1

xxt

=

=

yt y = sx

by construction yt y equals the covariance matrix
of x. from section 5 we know that the principal
components of x are the eigenvectors of sx. if we
calculate the svd of y, the columns of matrix v
contain the eigenvectors of yt y = sx. therefore,
the columns of v are the principal components of x.
this second algorithm is encapsulated in matlab code
included in appendix b.

what does this mean? v spans the row space of
y     1
1 xt . therefore, v must also span the col-
   n   
1 x. we can conclude that    nding
umn space of
the principal components10 amounts to    nding an or-
thonormal basis that spans the column space of x.

   n   

1

7 discussion and conclusions

7.1 quick summary

performing pca is quite simple in practice.

9y is of the appropriate n    m dimensions laid out in the
derivation of section 6.1. this is the reason for the       ipping   
of dimensions in 6.1 and figure 4.

10if the    nal goal is to    nd an orthonormal basis for the
coulmn space of x then we can calculate it directly without
constructing y. by symmetry the columns of u produced by
the svd of

x must also be the principal components.

1
   n

1
   

figure 5: data points (black dots) tracking a person
on a ferris wheel. the extracted principal compo-
nents are (p1, p2) and the phase is     .

1. organize a data set as an m    n matrix, where
m is the number of measurement types and n is
the number of trials.

2. subtract o    the mean for each measurement type

or row xi.

3. calculate the svd or the eigenvectors of the co-

variance.

in several    elds of literature, many authors refer to
the individual measurement types xi as the sources.
the data projected into the principal components
y = px are termed the signals, because the pro-
jected data presumably represent the    true    under-
lying id203 distributions.

7.2 dimensional reduction

one bene   t of pca is that we can examine the vari-
ances sy associated with the principle components.
often one    nds that large variances associated with

11

in the example of the spring, k = 1.

the    rst k < m principal components, and then a pre-
cipitous drop-o   . one can conclude that most inter-
esting dynamics occur only in the    rst k dimensions.
like-
in figure 3 panel (c), we recognize k = 1
wise,
along the principal component of r2 = kr1. this
process of of throwing out the less important axes
can help reveal hidden, simpli   ed dynamics in high
dimensional data. this process is aptly named
dimensional reduction.

7.3 limits and extensions of pca

both the strength and weakness of pca is that it is
a non-parametric analysis. one only needs to make
the assumptions outlined in section 4.5 and then cal-
culate the corresponding answer. there are no pa-
rameters to tweak and no coe   cients to adjust based
on user experience - the answer is unique11 and inde-
pendent of the user.

this same strength can also be viewed as a weak-
ness. if one knows a-priori some features of the dy-
namics of a system, then it makes sense to incorpo-
rate these assumptions into a parametric algorithm -
or an algorithm with selected parameters.

consider the recorded positions of a person on a
ferris wheel over time in figure 5. the id203
distributions along the axes are approximately gaus-
sian and thus pca    nds (p1, p2), however this an-
swer might not be optimal. the most concise form of
dimensional reduction is to recognize that the phase
(or angle along the ferris wheel) contains all dynamic
information. thus, the appropriate parametric algo-
rithm is to    rst convert the data to the appropriately
centered polar coordinates and then compute pca.
this prior non-linear transformation is sometimes
termed a kernel transformation and the entire para-
metric algorithm is termed kernel pca. other com-
mon kernel transformations include fourier and

11to be absolutely precise, the principal components are not
uniquely de   ned. one can always    ip the direction by multi-
plying by    1.
in addition, eigenvectors beyond the rank of
a matrix (i.e.   i = 0 for i > rank) can be selected almost
at whim. however, these degrees of freedom do not e   ect the
qualitative features of the solution nor a dimensional reduc-
tion.

12

figure 6: non-gaussian distributed data causes pca
to fail.
in exponentially distributed data the axes
with the largest variance do not correspond to the
underlying basis.

gaussian transformations. this procedure is para-
metric because the user must incorporate prior
knowledge of the dynamics in the selection of the ker-
nel but it is also more optimal in the sense that the
dynamics are more concisely described.

sometimes though the assumptions themselves are
too stringent. one might envision situations where
the principal components need not be orthogonal.
furthermore, the distributions along each dimension
(xi) need not be gaussian. for instance, figure 6
contains a 2-d exponentially distributed data set.
the largest variances do not correspond to the mean-
ingful axes of thus pca fails.

this less constrained set of problems is not triv-
ial and only recently has been solved adequately via
independent component analysis (ica). the formu-
lation is equivalent.

find a matrix p where y = px such that
sy is diagonalized.

however it abandons all assumptions except linear-
ity, and attempts to    nd axes that satisfy the most
formal form of redundancy reduction - statistical in-
dependence. mathematically ica    nds a basis such
that the joint id203 distribution can be factor-

ized12.

because at a = i, it follows that a   

1 = at .

p (yi, yj) = p (yi)p (yj)

for all i and j, i 6= j. the downside of ica is that it
is a form of nonlinear optimizaiton, making the solu-
tion di   cult to calculate in practice and potentially
not unique. however ica has been shown a very
practical and powerful algorithm for solving a whole
new class of problems.

experience

writing this paper has been an extremely in-
that
structional
this paper helps
to demystify the motivation
and results of pca, and the underlying assump-
tions behind this
technique.

important analysis

for me.

i hope

8 appendix a: id202

this section proves a few unapparent theorems in
id202, which are crucial to this paper.

1. the inverse of an orthogonal matrix is

its transpose.

the goal of this proof is to show that if a is an

orthogonal matrix, then a   
let a be an m    n matrix.

1 = at .

a = [a1 a2 . . . an]

where ai is the ith column vector. we now show that
at a = i where i is the identity matrix.

at a. the ijth element of at a is (at a)ij = ai

let us examine the ijth element of the matrix
t aj.
remember that the columns of an orthonormal ma-
trix are orthonormal to each other. in other words,
the dot product of any two columns is zero. the only
exception is a dot product of one particular column
with itself, which equals one.

2.

if a is any matrix, the matrices at a

and aat are both symmetric.

let   s examine the transpose of each in turn.

(aat )t = at t at = aat
(at a)t = at at t = at a

the equality of the quantity with its transpose
completes this proof.

3. a matrix is symmetric if and only if it is

orthogonally diagonalizable.

because this statement is bi-directional, it requires
a two-part    if-and-only-if    proof. one needs to prove
the forward and the backwards    if-then    cases.

let us start with the forward case. if a is orthog-
onally diagonalizable, then a is a symmetric matrix.
by hypothesis, orthogonally diagonalizable means
that there exists some e such that a = edet ,
where d is a diagonal matrix and e is some special
matrix which diagonalizes a. let us compute at .

at = (edet )t = et t dt et = edet = a

evidently, if a is orthogonally diagonalizable, it

must also be symmetric.

the reverse case is more involved and less clean so
it will be left to the reader. in lieu of this, hopefully
the    forward    case is suggestive if not somewhat
convincing.

4. a symmetric matrix is diagonalized by a

matrix of its orthonormal eigenvectors.

(at a)ij = ai

t aj =    1 i = j
0 i 6= j

at a is the exact description of the identity matrix.
1a = i. therefore,
the de   nition of a   

1 is a   

12equivalently, in the language of id205 the
goal is to    nd a basis p such that the mutual information
i(yi, yj) = 0 for i 6= j.

restated in math,

let a be a square n    n
symmetric matrix with associated eigenvectors
{e1, e2, . . . , en}. let e = [e1 e2 . . . en] where the
ith column of e is the eigenvector ei. this theorem
asserts that there exists a diagonal matrix d where
a = edet .

this theorem is an extension of the previous theo-
rem 3. it provides a prescription for how to    nd the

13

matrix e, the    diagonalizer    for a symmetric matrix.
it says that the special diagonalizer is in fact a matrix
of the original matrix   s eigenvectors.

this proof is in two parts. in the    rst part, we see
that the any matrix can be orthogonally diagonalized
if and only if it that matrix   s eigenvectors are all lin-
early independent. in the second part of the proof,
we see that a symmetric matrix has the special prop-
erty that all of its eigenvectors are not just linearly
independent but also orthogonal, thus completing our
proof.

in the    rst part of the proof, let a be just some
matrix, not necessarily symmetric, and let it have
independent eigenvectors (i.e. no degeneracy). fur-
thermore, let e = [e1 e2 . . . en] be the matrix of
eigenvectors placed in the columns. let d be a diag-
onal matrix where the ith eigenvalue is placed in the
iith position.

we will now show that ae = ed. we can exam-
ine the columns of the right-hand and left-hand sides
of the equation.

left hand side : ae = [ae1 ae2 . . . aen]
right hand side : ed = [  1e1   2e2 . . .   nen]

evidently, if ae = ed then aei =   iei for all i.
this equation is the de   nition of the eigenvalue equa-
tion. therefore, it must be that ae = ed. a lit-
1, completing
tle rearrangement provides a = ede   
the    rst part the proof.

for the second part of the proof, we show that
a symmetric matrix always has orthogonal eigenvec-
tors. for some symmetric matrix, let   1 and   2 be
distinct eigenvalues for eigenvectors e1 and e2.

  1e1    e2 = (  1e1)t e2
= (ae1)t e2
t at e2
= e1
t ae2
= e1
t (  2e2)
= e1
  1e1    e2 =   2e1    e2
can

relation we

the

last

that
by
since we have conjectured
(  1       2)e1    e2 = 0.
that the eigenvalues are in fact unique, it must be

equate

14

the case that e1    e2 = 0. therefore, the eigenvectors
of a symmetric matrix are orthogonal.
let us back up now to our original postulate that
a is a symmetric matrix. by the second part of the
proof, we know that the eigenvectors of a are all
orthonormal (we choose the eigenvectors to be nor-
malized). this means that e is an orthogonal matrix
so by theorem 1, et = e   
1 and we can rewrite the
   nal result.

a = edet

. thus, a symmetric matrix is diagonalized by a
matrix of its eigenvectors.

5. for any arbitrary m    n matrix x, the
symmetric matrix xt x has a set of orthonor-
mal eigenvectors of {  v1,   v2, . . . ,   vn} and a set
of associated eigenvalues {  1,   2, . . . ,   n}. the
set of vectors {x  v1, x  v2, . . . , x  vn} then form
an orthogonal basis, where each vector x  vi is
of length      i.

all of these properties arise from the dot product

of any two vectors from this set.

(x  vi)    (x  vj) = (x  vi)t (x  vj)

=   vt
i xt x  vj
=   vt
i (  j   vj)
=   j   vi      vj

(x  vi)    (x  vj) =   j  ij

the last relation arises because the set of eigenvectors
of x is orthogonal resulting in the kronecker delta.
in more simpler terms the last relation states:

(x  vi)    (x  vj) =      j

0

i = j
i 6= j

this equation states that any two vectors in the set
are orthogonal.

the second property arises from the above equa-
tion by realizing that the length squared of each vec-
tor is de   ned as:

kx  vik2 = (x  vi)    (x  vi) =   i

9 appendix b: code

is written for matlab 6.5

code

this
lease 13)
not
tory

from mathworks13.
e   cient

computationally
(terse

comments

the
but
begin with

(re-
code
is
explana-
a %).

data - mxn matrix of input data
(m dimensions, n trials)

% pca2: perform pca using svd.
%
%
% signals - mxn matrix of projected data
%
%

v - mx1 matrix of variances

pc - each column is a pc

this    rst version follows section 5 by examin-
ing the covariance of the data set.

[m,n] = size(data);

function [signals,pc,v] = pca1(data)
% pca1: perform pca using covariance.
data - mxn matrix of input data
%
%
(m dimensions, n trials)
% signals - mxn matrix of projected data
%
%

v - mx1 matrix of variances

pc - each column is a pc

[m,n] = size(data);

% subtract off the mean for each dimension
mn =
data = data - repmat(mn,1,n);

mean(data,2);

% calculate the covariance matrix
covariance = 1 / (n-1) * data * data   ;

% subtract off the mean for each dimension
mn =
data = data - repmat(mn,1,n);

mean(data,2);

% construct the matrix y
y = data    / sqrt(n-1);

% svd does it all
[u,s,pc] = svd(y);

% calculate the variances
s = diag(s);
v = s .* s;

% project the original data
signals = pc    * data;

% find the eigenvectors and eigenvalues
[pc, v] = eig(covariance);

10 references

% extract diagonal of matrix as vector
v = diag(v);

% sort the variances in decreasing order
[junk, rindices] = sort(-1*v);
v = v(rindices);
pc = pc(:,rindices);

% project the original data set
signals = pc    * data;

this second version follows section 6 computing

pca through svd.

function [signals,pc,v] = pca2(data)

13http://www.mathworks.com

(1997)    the
bell, anthony and sejnowski, terry.
independent components of natural scenes are
edge filters.    vision research 37(23), 3327-3338.
[a paper from my    eld of research that surveys and explores
di   erent forms of decorrelating data sets. the authors
examine the features of pca and compare it with new ideas
in redundancy reduction, namely independent component
analysis.]

bishop, christopher. (1996) neural networks for

pattern recognition. clarendon, oxford, uk.
[a challenging but brilliant text on statistical pattern
recognition (neural networks). although the derivation of
pca is touch in section 8.6 (p.310-319), it does have a
great discussion on potential extensions to the method and
it puts pca in context of other methods of dimensional

15

reduction. also, i want to acknowledge this book for several
ideas about the limitations of pca.]

lay, david.

(2000).

id202 and it   s

applications. addison-wesley, new york.
[this is a beautiful text. chapter 7 in the second edition
(p.
intuitive derivation and
discussion of svd and pca. extremely easy to follow and
a must read.]

441-486) has an exquisite,

mitra, partha and pesaran, bijan. (1999)    anal-
ysis of dynamic brain imaging data.    biophysical
journal. 76, 691-708.
[a comprehensive and spectacular paper from my    eld of
research interest. it is dense but in two sections    eigenmode
analysis: svd    and    space-frequency svd    the authors
discuss the bene   ts of performing a fourier transform on
the data before an svd.]

will, todd (1999)    introduction to the sin-
gular value decomposition    davidson college.
www.davidson.edu/academic/math/will/svd/index.html
[a math professor wrote up a great web tutorial on svd
intuitive explanations, graphics and
with tremendous
animations. although it avoids pca directly,
it gives a
great intuitive feel for what svd is doing mathematically.
also, it is the inspiration for my    spring    example.]

16

