deep learning for ai 

from machine perception to machine cognition

li deng 

chief scientist of ai, 

microsoft applications/services group (asg) & 

msr deep learning technology center  (dltc)

a plenary presentation at ieee-icassp, march 24, 2016

thanks go to many colleagues at dltc & msr, collaborating universities, 

and at microsoft   s engineering groups (asg+)

definition

deep learning is a class of machine learning algorithms that[1](pp199   200)

    use a cascade of many layers of nonlinear processing .
    are part of the broader machine learning field of learning 

representations of data facilitating end-to-end optimization.

    learn multiple levels of representations that correspond to 

hierarchies of concept abstraction

       ,    

2

artificial intelligence (ai) is the intelligence exhibited by 
machines or software. it is also the name of the academic field 
of study on how to create computers and computer software
that are capable of intelligent behavior.

artificial general intelligence (agi) is the intelligence of a (hypothetical) machine 
that could successfully perform any intellectual task that a human being can. it is 
a primary goal of artificial intelligence research and an important topic for science 
fiction writers and futurists. artificial general intelligence is also referred to as 
"strong ai      

3

ai/(a)gi & deep learning: the main thesis

ai/gi = machine perception (speech, image, video, gesture, 

touch...)    

+ machine cognition (natural language, reasoning, 

attention, memory/learning,
knowledge, decision making, action, 
interaction/conversation,    )

gi: ai that is flexible, general, adaptive, learning from 1st
principles       
deep learning + reinforcement/unsupervised learning 
   ai/gi 

4

ai/gi & deep learning: how alphago fits

ai/gi = machine perception (speech, image, video, gesture, 

touch...)    

+ machine cognition (natural language, reasoning, 

attention, memory/learning,
knowledge, decision making, action, 
interaction/conversation,    )

agi: ai that is flexible, general, adaptive, learning from 1st
principles       
deep learning + reinforcement/unsupervised learning 
   ai/agi 

5

outline

    deep learning for machine perception

    speech
    image

    deep learning for machine cognition

    semantic modeling
    natural language
    multimodality
    reasoning, attention, memory (ram)
    id99/management/exploitation
    optimal decision making (by deep id23)

    three hot areas/challenges of deep learning & ai research

6

deep learning research: centered at nips
(neural information processing systems) 

deep
learning
tutorial 

dec 7-12, 2015
musk & ram
& openai

zuckerberg &
lecun,  

2013

hinton & id163
&    bidding    2012 
hinton &
2009
msr 

7

8

scientists see promise in deep-learning programs
john markoff
november 23, 2012

tianjin, china, october, 25, 2012

the universal 
translator    comes true!

deep learning 
technology enabled 
speech-to-speech 
translation

microsoft research

a voice recognition program translated a speech given by 

richard f. rashid, microsoft   s top scientist, into mandarin chinese. 

9

id50 for phone recognition, nips, december 2009; 2012

investigation of full-sequence training of dbns for id103., interspeech, sept 2010
binary coding of speech spectrograms using a deep auto-encoder,        interspeech, sept 2010
roles of pre-training & fine-tuning in cd-dbn-id48s for real-world asr,  nips, dec. 2010
large vocabulary continuous id103 with cd-dnn-id48s, icassp,  april 2011 
conversational speech transcription using contxt-dependent dnn,interspeech, aug. 2011 

making id50 effective for lvcsr, asru, dec. 2011 
application of pretrained dnns to large vocabulary id103., icassp, 2012 
                         2.0                      2011, 2015

cd-dnn-id48 invented, 2010 

microsoft research

10

microsoft research

11

across-the-board deployment of dnn in speech industry
(+ in university labs & darpa programs)                                                       (2012-2014)

12

microsoft research

13

in the academic world

   this joint paper (2012) 
from the major speech 
recognition laboratories 

details the first major 

industrial application of 

deep learning.   

14

state-of-the-art id103 today
(& tomorrow --- roles of unsupervised learning)

15

asr: neural network architectures at 

single channel:

lstm acoustic model trained with 

connectionist temporal classification (ctc)

results on a 2,000-hr english voice search 

task show an 11% relative improvement

papers: [h. sak et al - icassp 2015, 

interspeech 2015, a. senior et al - asru 
2015]

model 

lstm w/ conventional 
modeling

wer

14.0

lstm w/ ctc

12.9%

(sainath, senior, sak, vinyals)

multi-channel:

multi-channel raw-waveform input for each channel

initial network layers factored to do spatial and 

spectral filtering

output passed to a cldnn acoustic model, entire 

network trained jointly

results on a 2,000-hr english voice search task 

show more than 10% relative improvement

papers: [t. n. sainath et al - asru 2015, icassp 

2016]

model 

raw-waveform, 1ch

delay+sum, 8 channel

mvdr, 8 channel

factored raw-waveform, 2ch

wer

19.2

18.7

18.8

17.1

(slide credit: tara sainath & andrew senior)

baidu   s deep speech 2 
end-to-end dl system for mandarin and english

paper: bit.ly/deepspeech2

    human-level mandarin 

recognition on short queries:
    deepspeech:  3.7% - 5.7% cer
    humans:             4% - 9.7% cer

   

   

   

   

trained on 12,000 hours of 
conversational, read, mixed 
speech.

9 layer id56 with ctc cost:

2d invariant convolution
7 recurrent layers
fully connected output

trained with sgd on heavily-
optimized hpc system.  
   sortagrad    curriculum 
learning.

   batch dispatch    framework 
for low-latency production 
deployment.

(slide credit: andrew ng & adam coates)

learning transition probabilities in dnn-id48 asr

dnn outputs include not only state posterior outputs but also id48 transition 
probabilities

real-time reduction of 16%
wer reduction of 10%

state posteriors

transition probs

siri data

matthias paulik,    improvements to the pruning behavior of dnn acoustic 
models   . interspeech 2015

(slide: alex acero)

id122n-based lvcsr system 

    feed-forward sequential memory 

network(id122n)

    results on 10,000 hours mandarin 

short message dictation task

    8 hidden layers
    memory block with -/+ 15 frames
    ctc training criteria

    comparable results to dblstm with 

smaller model size

    training costs only 1 day using 16 gpus 

and asgd algorithm

model

#para.(m)

cer (%)

relu dnn

lstm

blstm

id122n

40

27.5

45

19.8

6.40

5.25

4.67

4.61

shiliang zhang, cong liu, hui jiang, si wei, lirong dai, yu hu.    feedforward sequential memory networks: 
a new structure to learn long-term dependency    . arxiv:1512.08031, 2015.

(slide credit: cong liu & yu hu)

english conversational telephone id103*

key ingredients:
   

joint id56/id98 acoustic model trained on 
2000 hours of publicly available audio 

    maxout activations
    exponential and nn language models

wer results on switchboard hub5-2000:

wer swb wer ch

model

id98

id56

joint id56/id98

10.4

9.9

9.3

+ lm rescoring

8.0%

17.9

16.3

15.6

14.1

*saon et al.    the ibm 2015 english conversational telephone id103 system   , interspeech 2015.

(slide credit: g. saon & b. kingsbury)

conv. layerconv. layeroutput layerbottleneckhidden layerhidden layerhidden layerhidden layerhidden layerid56 featuresid98 featuresrecurrent layerbottleneck    sp-p14.5:    scalable training of deep learning machines 

by incremental block training with intra-block 
parallel optimization and blockwise model-update 
filtering,    by kai chen and qiang huo

(slide credit: xuedong huang)

cntk/phily

*google updated that tensorflow can now scale to support multiple machines recently; 
comparisons have not been made yet
    recent research at ms (icassp-2016):
-   scalable training of deep learning machines by incremental block training with intra-
block parallel optimization and blockwise model-update filtering   
-   highway lstm id56s for distance id103   
-   self-stabilized deep neural networks   

deep learning also shattered image recognition
(since 2012)

23

4th year

3.567%

3.581%

super-deep: 152 layers 

microsoft research

24

microsoft research

25

depth is of crucial importance

alexnet, 8 layers

(ilsvrc 2012)

vgg, 19 layers
(ilsvrc 2014)

googlenet, 22 layers

(ilsvrc 2014)

ilsvrc (large scale visual recognition challenge) 

(slide credit: jian sun, msr)

11x11 conv, 96, /4, pool/25x5 conv, 256, pool/23x3 conv, 3843x3 conv, 3843x3 conv, 256, pool/2fc, 4096fc, 4096fc, 10003x3 conv, 643x3 conv, 64, pool/23x3 conv, 1283x3 conv, 128, pool/23x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 256, pool/23x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 512, pool/23x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 512, pool/2fc, 4096fc, 4096fc, 1000inputconv7x7+2(s)maxpool 3x3+2(s)localrespnormconv1x1+1(v)conv3x3+1(s)localrespnormmaxpool 3x3+2(s)convconvconvconv1x1+1(s)3x3+1(s)5x5+1(s)1x1+1(s)convconvmaxpool 1x1+1(s)1x1+1(s)3x3+1(s)depthconcatconvconvconvconv1x1+1(s)3x3+1(s)5x5+1(s)1x1+1(s)convconvmaxpool 1x1+1(s)1x1+1(s)3x3+1(s)depthconcatmaxpool 3x3+2(s)convconvconvconv1x1+1(s)3x3+1(s)5x5+1(s)1x1+1(s)convconvmaxpool 1x1+1(s)1x1+1(s)3x3+1(s)depthconcatconvconvconvconv1x1+1(s)3x3+1(s)5x5+1(s)1x1+1(s)convconvmaxpool 1x1+1(s)1x1+1(s)3x3+1(s)averagepool 5x5+3(v)depthconcatconvconvconvconv1x1+1(s)3x3+1(s)5x5+1(s)1x1+1(s)convconvmaxpool 1x1+1(s)1x1+1(s)3x3+1(s)depthconcatconvconvconvconv1x1+1(s)3x3+1(s)5x5+1(s)1x1+1(s)convconvmaxpool 1x1+1(s)1x1+1(s)3x3+1(s)depthconcatconvconvconvconv1x1+1(s)3x3+1(s)5x5+1(s)1x1+1(s)convconvmaxpool 1x1+1(s)1x1+1(s)3x3+1(s)averagepool 5x5+3(v)depthconcatmaxpool 3x3+2(s)convconvconvconv1x1+1(s)3x3+1(s)5x5+1(s)1x1+1(s)convconvmaxpool 1x1+1(s)1x1+1(s)3x3+1(s)depthconcatconvconvconvconv1x1+1(s)3x3+1(s)5x5+1(s)1x1+1(s)convconvmaxpool 1x1+1(s)1x1+1(s)3x3+1(s)depthconcataveragepool 7x7+1(v)fcconv1x1+1(s)fcfcsoftmaxactivationsoftmax0conv1x1+1(s)fcfcsoftmaxactivationsoftmax1softmaxactivationsoftmax2depth is of crucial importance

alexnet, 8 layers

(ilsvrc 2012)

vgg, 19 layers
(ilsvrc 2014)

resnet, 152 layers

(ilsvrc 2015)

ilsvrc (large scale visual recognition challenge) 

(slide credit: jian sun, msr)

3x3 conv, 643x3 conv, 64, pool/23x3 conv, 1283x3 conv, 128, pool/23x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 256, pool/23x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 512, pool/23x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 512, pool/2fc, 4096fc, 4096fc, 100011x11 conv, 96, /4, pool/25x5 conv, 256, pool/23x3 conv, 3843x3 conv, 3843x3 conv, 256, pool/2fc, 4096fc, 4096fc, 10001x1 conv, 643x3 conv, 641x1 conv, 2561x1 conv, 643x3 conv, 641x1 conv, 2561x1 conv, 643x3 conv, 641x1 conv, 2561x2 conv, 128, /23x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 256, /23x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 512, /23x3 conv, 5121x1 conv, 20481x1 conv, 5123x3 conv, 5121x1 conv, 20481x1 conv, 5123x3 conv, 5121x1 conv, 2048ave pool, fc 10007x7 conv, 64, /2, pool/2depth is of crucial importance

resnet, 152 layers

(slide credit: jian sun, msr)

1x1 conv, 643x3 conv, 641x1 conv, 2561x1 conv, 643x3 conv, 641x1 conv, 2561x1 conv, 643x3 conv, 641x1 conv, 2561x2 conv, 128, /23x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 1283x3 conv, 1281x1 conv, 5121x1 conv, 256, /23x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 2563x3 conv, 2561x1 conv, 10241x1 conv, 512, /23x3 conv, 5121x1 conv, 20481x1 conv, 5123x3 conv, 5121x1 conv, 20481x1 conv, 5123x3 conv, 5121x1 conv, 2048ave pool, fc 10007x7 conv, 64, /2, pool/2outline

    deep learning for machine perception

    speech
    image

    deep learning for machine cognition

    semantic modeling
    natural language
    multimodality
    reasoning, attention, memory (ram)
    id99/management/exploitation
    optimal decision making (by deep id23)

    three hot areas/challenges of deep learning & ai research

29

deep semantic model for symbol embedding

similar

apart

semantic vector

letter-trigram 
embedding matrix

letter-trigram encoding
matrix (fixed)

bag-of-words vector

input word/phrase

        

ws,4

ws,3

ws,2

d=300

d=500

d=500

dim = 50k

ws,1

            

wt,4

wt,3

wt,2

wt,1

d=300

d=500

d=500

dim = 50k

            

wt,4

wt,3

wt,2

wt,1

d=300

d=500

d=500

dim = 50k

      ,   

dim = 100m

dim = 100m

dim = 100m

s:    racing  car   

t1:    formula one   

t2:    racing to me   

huang, p., he, x., gao, j., deng, l., acero, a., and heck, l. learning deep structured semantic models for 
web search using clickthrough data. in acm-cikm, 2013.

microsoft research

30

many applications of deep semantic modeling:
learning semantic relationship between    source    and    target   
tasks

source 

target 

word semantic embedding

web search

query intent detection

id53

machine translation

query auto-suggestion

query auto-completion

apps recommendation

context

search query

search query

word

web documents

use intent

pattern / mention (in nl)

relation / entity (in knowledge base)

sentence in language a

translated sentences in language b

search query

partial search query

user profile

distillation of survey feedbacks

feedbacks in text

automatic image captioning

image

id162

natural user interface

ads selection

ads click prediction

email analysis: people prediction

email search

email declutering

text query

command (text / speech / gesture)

search query

search query

email content

search query

email contents

suggested query

completed query

recommended apps

relevant feedbacks

text caption

images

actions

ad keywords

ad documents

recipients, senders

email content

email contents in similar threads

knowledge-base construction

entity from source

entity fitting desired relationship

contextual entity search

automatic highlighting

text summarization

key phrase / context

documents in reading

long text 

entity / its corresponding page

key phrases to be highlighted

31

summarized short text

automatic image captioning (msr system)

detector models,
deep neural net 

features,    

computer 

vision 
system

street

signs

under

on

light

stop

red

sign

bus

city

pole

building

traffic

language 

model

a red stop sign sitting under a traffic light on a city street
a stop sign at an intersection on a street
a stop sign with two street signs on a pole on a sidewalk
a stop sign at an intersection on a city street
   
a stop sign
a red traffic light

caption 

generation 

system

a stop sign at an intersection on a city street

dssm model

semantic 
ranking 
system

fang, gupta, iandola, srivastava, deng, dollar, gao, he, mitchell, platt, zitnick, zweig,    from captions to 
visual concepts and back,    cvpr, 2015

a

b

machine:

human:

coco challenge results (cvpr-2015, boston)

tied for 
1st prize

microsoft research

36

deep learning for machine cognition

--- deep id23
---    optimal    actions: control and business decision making 

37

id23 from    non-working    to    working   , due to 
deep learning (much like dnn for speech)

38

deep q-network (id25)

    input layer: image vector of     
    output layer: a single output q-value for each action     ,     (    ,     ,     )
    dnn parameters:     

id23 

--- optimizing long-term values

short-term

long-term

playing the
breakout game

optimizing
business 
decision
making

maximize
immediate
reward

optimize life-time revenue,
service usages, and 
customer satisfaction

self play to improve skills

41

dnn learning pipeline in  

42

dnn architecture used in  

43

analysis of four dnns in 

dnns

properties

architecture

additional details

                      

                         

                      

    (    )

slow, accurate stochastic 
supervised learning policy, 
trained on 30m (s,a) pairs

13 layer network; alternating 
convnets and rectifier non-
linearities; output dist. over 
all legal moves

evaluation time: 
3 ms
accuracy vs. corpus:   57%
train time: 

3 weeks

fast, less accurate stochastic 
sl policy, trained on 30m 
(s,a) pairs

stochastic rl policy, trained 
by self-play

linear softmax of small pattern 
features

evaluation time: 
2 us
accuracy vs. corpus:   24%

same as

            

win vs.         :

            

80%

value function: % chance of 
            
state s

winning by starting in 

            

same as        , but with one 
output (% chance of 
winning)

15k less computation than evaluating 
with roll-outs

            

id169 in 

a

s

s

a

s

v(s)

a

s

a

s

s

s

end

z

    (    ) = argmax         (    ,     )
         ,      =             ,      +     (    ,     )

roll-out 
estimate

exploration 

bonus

            ,      =

1

    (    ,     )

   

    

mixture weight
    
     +             

1                       

# of times action a 

value function 

taken in state s

computed in advance

win/loss result of 1 
roll-out with                          

         ,      =                       (    |    )

           (    ,     )
1 +     (    ,     )

   
   
   
   
   

think of this mcts component as a highly efficient    decoder   , a concept familiar to asr
-> id67 and fast match in id103 literature during 80   s-90   s
this is tree search (go-specific), not graph search (a*)
speech is a relatively simple signal     sequential id125 sufficient, no need for a* or tree
key innovation in alphago:    scores    in mcts computed by dnns with rl

deep learning for machine cognition

--- memory & attention (applied to machine translation)

47

long short-term memory id56 

lstm

(hochreiter & schmidhuber, 1997)

lstm cell unfolding over time

(jozefowics, zarembe, sutskever, 
icml 2015)

49

gated recurrent unit (gru)

(simpler than lstm; no output gates)

(jozefowics, zarembe, sutskever, icml 2015; google
kumar et al., arxiv, july, 2015; metamind)

50

seq-2-seid24 (id4)
deep    thought   -vector approach to mt

lstm/gru decoder

lstm/gru encoder

id4

   thought vector   

(forcada&  eco, 1997; 
casta  o&casacuberta, 1997;
kalchbrenner&blunsom, 2013; 
sutskever et al., 2014; 
cho et al., 2014)

(slide credit: kyunghyun cho, 2016)

52

id4

    this model replying    thought vector    does not perform well

    especially for long source sentences

    because:

   you can   t cram the meaning of a whole 
%&!$# sentence into a single $&!#* vector!   

ray mooney

(modified from: kyunghyun cho, 2016)

53

id4 with attention

attention-based model

    encoder: bidirectional id56

    a set of annotation vectors

    attention-based decoder

(1) compute attention weights

(2) weighted-sum of the annotation vectors

(3) use       to replace    though vector    

(modified from: kyunghyun cho, 2016)
(modified from: kyunghyun cho, 2016)

54

benchmark: wmt   14 en-de

30

24

phrase-based mt
(buck et al., 2014)

18

12

6

0

oov replacement
(jean et al., 2015;
luong et al., 2015)

attention-based id4 
(bahdanau et al., 2015)

dec 2014

(modified from: kyunghyun cho)

large target vocabulary

(jean et al., 2015;
luong et al., 2015)

location+content, local+global

attention

(luong et al., 2015a)

june 2015

models for global & local attention

global: all source states.

local: subset of source states.

(luong et al., 2015)

benchmark: wmt   15 en-de

large target vocabulary 

+ oov replacement

+ ensemble

(jean et al., 2015)

bpe-based sub words
+ monolingual corpus
(sennrich et al., 2015a)

bpe-based sub words
(sennrich et al., 2015)

27.5

26.25

25

23.75

22.5

21.25

20

large target vocabulary

+ oov replacement
(jean et al., 2015)

(modified from: kyunghyun cho)

bpe-based sub words
+ monolingual corpus

+ ensemble

(sennrich et al., 2015a)

syntax-based mt

(sennrich & haddow, 2015)

same attention model applied to

58

image captioning

topics: beyond natural languages 
    image id134

    conditional language modelling

    encoder: convolutional network

    pretrained as a classifier or autoencoder

    decoder: recurrent neural network

    id56 language model

    with attention mechanism (xu et al., 2015)

deep learning for machine cognition

--- neural reasoning: memory network
--- better neural reasoning: tensor product representations

(tpr) with structured id99

59

memory networks for reasoning

    rather than placing 

   attention    to part of a 
sentence, it can be placed 
to cognitive space with 
many sentences

    this allows    reasoning   
    embedding input

         =             
         =             
     =         

    attention over memories

         = softmax(                )

    generating the final 

answer
     =                        

     = softmax(          +      )

[sukhbaatar, szlm, weston, fergus:    end-to-end memory networks,    nips, 2015]

[kumar, irsoy,    socher:    ask me anything: dynamic memory networks for nlp,    nips, 2015]

61

[xiong, merity, socher:    dynamic memory networks for visual & textual id53,   arxiv, mar 4, 2016]
reported in new york times, mar 6, 2016

62

tpr: neural representation of structure

    structured embedding vectors via tensor-product rep (tpr)

symbolic semantic parse tree (complex relation)

then, reasoning in symbolic-space (traditional ai) can be beautifully 
carried out in the continuous-space in human cognitive and neural-
net terms

paul smolensky & g. legendre: 
the harmonic mind, mit press, 2006
from neural computation to optimality-theoretic grammar 
volume i: cognitive architecture; volume 2: linguistic implications

63

outline

    deep learning for machine perception

    speech
    image

    deep learning for machine cognition

    semantic modeling
    natural language
    multimodality
    reasoning, attention, memory (ram)
    id99/management/exploitation
    optimal decision making (by deep id23)

    three hot areas/challenges of deep learning & ai research

64

challenges for future research

1. structured embedding for better reasoning: integrate   
symbolic/neural representations

2. integrate deep discriminative & generative/bayesian 
models

3. deep unsupervised learning

65

few leaders are admired by george bush

admire(george bush, few leaders)

  (s) = cons(ex1(ex0(ex1(s))),

cons(ex1(ex1(ex1(s))), ex0(s)))
0wex
1] +
1)+wcons

w = wcons
0(wex

0[wex
1wex

1wex
1wex

1[wcons

1(wex

0)]

wcons

v

a

p

meaning (lf)

f

g

b

d

c

patient

output

  

input

p

aux

v

by

a

   passive sentence   

  

isomorphism

b

d

c

aux

patien

t

f b
y

e g
agent

w

slide from paul smolensky, 2015

recurrent nn vs. dynamic system 

parameterization:
              ,            ,            : all unstructured

regular matrices

parameterization:
               =m(      ); sparse system matrix
           =(      ); gaussian-mix params; mlp 
       =         

67

deep discriminative nn

deep generative (bayesian)

structure

graphical; info flow: bottom-up

graphical; info flow: top-down

incorp constraints & 
domain knowledge

harder; less fine-grained

easier; more fine grained

semi/unsupervised

hard or impossible

easier, at least possible

interpretability

harder

easy (generative    story    on data and hidden variables)

representation

distributed

localist (mostly); can be distributed also

id136/decode

easy

harder (but note recent progress)

scalability/compute

easier (regular computes/gpu)

harder  (but note recent progress)

incorp. uncertainty

hard

easy

empirical goal

classification, id171,    

classification (via bayes rule), latent 
variable id136   

terminology

neurons, activation/gate functions, 
weights     

random vars, stochastic    neurons   , 
potential function, parameters    

learning algorithm

a single, unchallenged, algorithm --
backprop

a major focus of open research, many 
algorithms, & more to come

evaluation

on a black-box score     end 
performance

on almost every intermediate quantity

implementation

hard (but increasingly easier)

standardized but insights needed

experiments

massive, real data

modest, often simulated data

parameterization

dense matrices

sparse (often pdfs); can be dense

deep unsupervised learning

    unsupervised learning (ul) has recently been a very hot topic in 

deep learning 

    need to have a task to ground ul --- e.g. help improve prediction
    examples of id103 and image captioning:

    3000 hrs of paired acoustics (x) & word label (y)
    how can we exploit 300,000+ hrs of speech acoustics with no paired 

labels?

    4 sources of knowledge

    strong structure prior of    labels    y (sequences)
    strong structure prior of input data x (conventional ul)
    dependency of x on y (generative modeling for embedding knowledge)
    dependency of y on x (state of the art systems w. supervised learning)

69

end 

(of chapter 1)

thank you!

q/a

71

72

73

tensor product rep for reasoning

    facebook   s reasoning task (babi):

accepted to iclr, may 2016

74

structured id99 & reasoning via 

tpr

    given containee-container relationship 
   

encode all entities (e.g., actors (mary), objects (football), and locations (nowhere, kitchen, 
garden)) by vectors
encode each statement by a matrix via binding (tensor product of two vectors),             

   
    reasoning (transitivity) by id127,                                =                                 =             
    generate answer (e.g., where is the football in #5) via unbinding (inner product)
a. left-multiply by          all statements prior to the current time. (yields                         ,                         )
b. pick the most recent container where 2-norms of the multiplications in (a) are approximately 1.0. 

(yields          .)

tpr results on fb   s babi task

76

