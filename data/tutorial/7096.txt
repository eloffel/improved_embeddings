tensor   ow review session

joshua achiam

uc berkeley

september 8, 2017

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

1 / 33

automatic di   erentiation

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

2 / 33

what is tensor   ow?

tensor   ow is a library for building and manipulating computation
graphs on tensors.

what is a computation graph? a directed, acyclic graph where

bottom nodes (no arrows going in) are inputs (external inputs (data)
or internal inputs (variables))
top nodes (no arrows going out) are outputs
middle nodes are functions.

in tensor   ow, all inputs, outputs, and function outputs are tensors
(multi-dimensional arrays).

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

3 / 33

computation graphs

simple example: input x, output o, various intermediate functions.
chain rule gives us a way to compute gradient of o with respect to x
by going backward through intermediate nodes:

note do/dl = 1.
first,    nd    l/   g and    l/   h.
get gradients dg /dx and dh/dx.
aggregate at x:

do
dx

=

   l
   g

dg
dx

+

   l
   h

dh
dx

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

4 / 33

automatic di   erentiation

given a computation graph f : x     y , we can automatically generate
a new computation graph that returns dy /dx

(very) roughly, each op in the graph is replaced by a gradients op
that runs in the reverse-direction. instead of the original op

we place

o : x1, ..., xn     y ,

o(cid:48) :

      
   y

          

   x1

, ...,

      
   xn

this procedure allows us to get gradients of any scalar signal (e.g.
id168) with respect to any inputs to a graph (e.g. trainable
parameters)!

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

5 / 33

automatic di   erentiation

you will almost never have to worry about what happens under the
hood here

libraries like tensor   ow, theano, pytorch, ca   e, and others take
care of this for you

takeaway: computation graph libraries let you de   ne really
complicated graphs (especially neural net architectures), get gradients
for no extra programming e   ort, and easily optimize via    rst-order
methods!

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

6 / 33

tensor   ow basics

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

7 / 33

basic graph building

create points of data entry through tf.placeholder()
create parameter variables through tf.variable() or
tf.get_variable()1
apply operations to data
can be simple/atomic

in some places numpy syntax is supported (including broadcasting)
e.g. a + b and tf.add(a,b) produce same output

or composite: see tf.layers package for neural network layers
e.g. tf.layers.dense(), which creates standard feedforward
   densely connected    neural net layer
great for fast prototyping   most of the work already done for you!
snap pieces together like lego

1see https://stackover   ow.com/questions/37098546/di   erence-between-variable-

and-get-variable-in-tensor   ow

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

8 / 33

example graph building

from mnist tutorial:

# create the model
x = tf.placeholder(

dtype=tf.float32,
shape=[none, 784]
)

w = tf.variable(tf.zeros([784, 10]))
b = tf.variable(tf.zeros([10]))
y = tf.matmul(x, w) + b

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

9 / 33

operations do not run at de   ne time

caution! operations produce tensors as outputs, not data.

in [3]: np.add(5,5)
out[3]: 10

in [4]: tf.add(5,5)
out[4]: <tf.tensor    add:0    shape=() dtype=int32>

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

10 / 33

session, run, and initialization

to compute outputs of cgs in tf, you need a session. several ways
to get one:

tf.session()
tf.interactivesession() (automatically sets as default)
tf.get_default_session() (only if a default session exists)
use the run() command from a session to perform computations
run() requires a feed dict for placeholders

sess = tf.interactivesession()
x_ph = tf.placeholder(shape=(none,100) ,dtype=tf.float32)
y_ph = tf.placeholder(shape=(none,10), dtype=tf.float32)
loss, update_op = build_network(x_ph, y_ph)
x_batch, y_batch = data_gen.next()
out = sess.run(

[loss, update_op],
feed_dict={x_ph: x_batch, y_ph: y_batch}
)

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

11 / 33

further notes on run

run() will only compute necessary pieces of computation graph to
get outputs you ask for (nice   avoids excess compute!)
as on previous slide, run() can get multiple outputs at once (with a
single pass through necessary nodes in computation graph)
if you have variables in your computation graph, nothing will work
until you initialize them

to do this easily, after making session and graph, but before training:

sess.run(tf.global_variables_initializer())

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

12 / 33

syntactic sugar for run

if just running one op / evaluating one tensor, and a default
session exists, you can use .run() and .eval()2
.run() works for operations

sess = tf.session()
with sess.as_default():

tf.global_variables_initializer().run()

.eval() works for tensors

sess = tf.interactivesession()
x, y = make_inputs()
accuracy = build_network(x, y)
x_batch, y_batch = data_gen.next()
print(accuracy.eval(feed_dict={x : x_batch, y : y_batch}))

(in above snippet, recall that interactivesession becomes
default automatically!)

2see https://stackover   ow.com/questions/38987466/eval-and-run-in-tensor   ow

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

13 / 33

id168s

tensor   ow makes common id168s easy! example,
cross-id178 loss for classi   cation:

# create the model
x = tf.placeholder(tf.float32, [none, 784])
y = build_logits_network(x)

# define loss and optimizer
y_ = tf.placeholder(tf.float32, [none, 10])

cross_id178 = tf.reduce_mean(

tf.nn.softmax_cross_id178_with_logits(

labels=y_,
logits=y
)

)

see tf.losses for more (huber_loss, hinge_loss, etc.)

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

14 / 33

id168s

write your own custom losses. for instance, in policy gradients:

lr = tf.exp( logli - logli_old )
adv = tf.placeholder(shape=(none,), dtype=tf.float32)
surrogate_loss = -tf.reduce_mean( lr * adv )

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

15 / 33

optimizers

after building networks and id168s, add an optimizer to
minimize loss.

make an optimzer object, and set hyperparameters via constructor
method (like momentum, rmsprop coe   cients, adam coe   cients) or
leave at safe defaults

call minimize on loss to get training op:

optimizer = tf.train.adamoptimizer(learning_rate=1e-3)
train_op = optimizer.minimize(loss)

to perform one step of training, just run training op!

sess.run(train_op, feed_dict)

nb: if you want to, you can specify which variables the optimizer acts
on as an argument to minimize

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

16 / 33

mnist example

def main(_):

# import data
mnist = input_data.read_data_sets(flags.data_dir, one_hot=true)

# create the model
x = tf.placeholder(tf.float32, [none, 784])
w = tf.variable(tf.zeros([784, 10]))
b = tf.variable(tf.zeros([10]))
y = tf.matmul(x, w) + b

# define loss and optimizer
y_ = tf.placeholder(tf.float32, [none, 10])

cross_id178 = tf.reduce_mean(

tf.nn.softmax_cross_id178_with_logits(labels=y_, logits=y)
)

train_step = tf.train.gradientdescentoptimizer(0.5).minimize(cross_id178)

sess = tf.interactivesession()
tf.global_variables_initializer().run()
# train
for _ in range(1000):

batch_xs, batch_ys = mnist.train.next_batch(100)
sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

# test trained model
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(sess.run(

accuracy,
feed_dict={x: mnist.test.images, y_: mnist.test.labels}
))

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

17 / 33

building advanced computation graphs

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

18 / 33

common neural network operations

many standard neural network layers are already implemented, and
enable high customization (for instance, custom initializers)
dense / fully-connected layers (wx + b):

tf.layers.dense
tf.contrib.layers.fully_connected

conv layers

tf.layers.conv2d
tf.contrib.layers.conv2d

id180: tf.nn.{relu, sigmoid, tanh, elu}
neural network tricks: tf.layers.dropout
tensor utilities: tf.reshape, tf.contrib.layers.flatten,
tf.concat, tf.reduce_sum, tf.reduce_mean

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

19 / 33

recurrent neural networks

to build a recurrent neural network (id56),    rst specify an id56
cell, which de   nes the computation at each time step. note, this is
not the output tensor you will run!

tf.nn.id56_cell.basicid56cell (ht =   (wxt + rht   1 + b))
tf.nn.id56_cell.grucell
tf.nn.id56_cell.lstmcell

get a sequence of outputs and the    nal hidden state of an id56
computed with that cell by calling tf.nn.dynamic_id56

in [1]: import tensorflow as tf
in [2]: x = tf.placeholder(shape=(none,none,10),dtype=tf.float32)
in [3]: cell = tf.nn.id56_cell.grucell(20)
in [4]: outputs, final = tf.nn.dynamic_id56(cell, x, time_major=false, dtype=tf.float32)
in [5]: sess = tf.interactivesession()
in [6]: tf.global_variables_initializer().run()
in [7]: import numpy as np
in [8]: o, f = sess.run([outputs, final], {x: np.random.rand(32,50,10)})
in [9]: o.shape
out[9]: (32, 50, 20)
in [10]: f.shape
out[10]: (32, 20)

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

20 / 33

variable scoping and reuse

organize variables by name with variable scopes

out = input

with tf.variable_scope(   conv1   ):

out = tf.layers.batch_id172(out,...)
out = tf.conv2d(out,...)

with tf.variable_scope(   conv2   ):

out = tf.layers.batch_id172(out,...)
out = tf.conv2d(out,...)

scopes can nest

scope names become part of directory structure for variable names;
makes possible to access them through more arcane methods

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

21 / 33

variable scoping and reuse

can reuse variables (ie make two nodes with tied weights) via reuse

in [1]: import tensorflow as tf

in [2]: x = tf.placeholder(shape=(none,10),dtype=tf.float32)
...: y = tf.placeholder(shape=(none,10),dtype=tf.float32)
...: with tf.variable_scope(   test   ):
...:
o1 = tf.layers.dense(x, 10)
...: with tf.variable_scope(   test   ,reuse=true):
...:
...:

o2 = tf.layers.dense(x, 10)

in [3]: o2
out[3]: <tf.tensor    test_1/dense/biasadd:0    shape=(?, 10) dtype=float32>

in [4]: o1
out[4]: <tf.tensor    test/dense/biasadd:0    shape=(?, 10) dtype=float32>

in [5]: tf.global_variables()
out[5]:
[<tf.variable    test/dense/kernel:0    shape=(10, 10) dtype=float32_ref>,

<tf.variable    test/dense/bias:0    shape=(10,) dtype=float32_ref>]

o1 and o2 are di   erent dense operations, there is only one set of
variables!

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

22 / 33

gradients and stop gradients

if you want to write a custom optimizer you may want to work with
gradients directly; can do this using tf.gradients
you may want to prevent id26 through a particular
tensor: can do this with tf.stop_gradient

example: in id25, where we want to minimize a mean-square bellman
error with respect to params of current network but not target network
o = tf.placeholder(shape=(none,dim_o),dtype=tf.float32)
a = tf.placeholder(shape=(none,),dtype=tf.int32)
o2 = tf.placeholder(shape=(none,dim_o),dtype=tf.float32)
r = tf.placeholder(shape=(none,),dtype=tf.float32)
with tf.variable_scope(   main   ):

q = build_network(o)

with tf.variable_scope(   target   ):

q_targ = build_network(o2)

q_a = tf.reduce_sum(q * tf.one_hot(a, num_actions),1)
q_targ_a = tf.reduce_max(q_targ,1)
target = r + gamma * q_targ_a
target = tf.stop_gradient(target)
loss = tf.reduce_mean(tf.square(q_a - target))
# later on, make assign statement so q_targ lags q

# b x a

# b x a
# b
# b

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

23 / 33

logging and debugging

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

24 / 33

logging

tensor   ow has native operations for saving data through
tf.summary
declare summary ops as functions of other tensors or ops3

def variable_summaries(var):

"""attach a lot of summaries to a tensor (for tensorboard visualization)."""
with tf.name_scope(   summaries   ):

mean = tf.reduce_mean(var)
tf.summary.scalar(   mean   , mean)
with tf.name_scope(   stddev   ):

stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))

tf.summary.scalar(   stddev   , stddev)
tf.summary.scalar(   max   , tf.reduce_max(var))
tf.summary.scalar(   min   , tf.reduce_min(var))
tf.summary.histogram(   histogram   , var)

summary ops are never called unless you run them

for convenience, merge all summary ops via

merged_summary_op = tf.summary.merge_all()

3example from https:

//www.tensorflow.org/get_started/summaries_and_tensorboard
september 8, 2017

joshua achiam (uc berkeley)

tensor   ow review session

25 / 33

logging

make a tf.summary.filewriter to save summaries to    le.4
...create a graph...
# launch the graph in a session.
sess = tf.session()
# create a summary writer, add the    graph    to the event file.
writer = tf.summary.filewriter(<some-directory>, sess.graph)

passing the graph into filewriter allows you to inspect the
computation graph later when you visualize in tensorboard.

to save summaries with the filewriter, run the merged summary op
and then use add_summary:

for i in range(n_steps):

feed_dict = get_next_feed_dict()
summary, _ = sess.run([merged_summary_op, train_op], feed_dict)
writer.add_summary(summary, i)

4https:

//www.tensorflow.org/api_docs/python/tf/summary/filewriter
september 8, 2017

joshua achiam (uc berkeley)

tensor   ow review session

26 / 33

logging

invoke tensorboard with
tensorboard --logdir=path/to/log-directory

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

27 / 33

debugging

explore with interactivesession in ipython
common issue   tensor shapes are wrong. can check with
<tensor>.get_shape().as_list().
want to look at the list of all variables?
tf.global_variables().
in [1]: import tensorflow as tf

in [2]: x = tf.placeholder(shape=(none,10), dtype=tf.float32)

in [3]: with tf.variable_scope(   test   ):

...:
...:

y = tf.layers.dense(x, 20)

in [4]: tf.global_variables()
out[4]:
[<tf.variable    test/dense/kernel:0    shape=(10, 20) dtype=float32_ref>,

<tf.variable    test/dense/bias:0    shape=(20,) dtype=float32_ref>]

good scoping makes it easier to    nd problem areas

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

28 / 33

debugging

sometimes, look at raw inputs and outputs of networks!

if outputs all look the same despite di   erent inputs, maybe a hidden
layer   s activations are saturating

be super careful about batchnorm and other neural network tricks
that have di   erent behavior at training time and test time

see reference5 for a good guide on using batchnorm correctly.

make sure the scale of inputs to your network is reasonable
(empirically it helps sometimes for data to have mean zero and std=1)

if you are using default values anywhere (in layers, optimizers, etc.),
check them and make sure they make sense

5http://ruishu.io/2016/12/27/batchnorm/

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

29 / 33

what else is out there?

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

30 / 33

computation graph libraries

tensor   ow (google) - huge community, well-supported, somewhat
clunky api but great distributed performance

theano (u of montreal) - long history, widely-used, slow to compile

ca   e (berkeley / facebook)

torch (facebook and others) - now available in python (previously
just lua), de   ne-by-run makes for fast and    exible prototyping,
comparable speed to tensor   ow

chainer (preferred networks) - another de   ne-by-run like torch

for some performance comparisons:
https://github.com/soumith/convnet-benchmarks

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

31 / 33

alternate apis/wrappers for tensor   ow

tf-contrib (active development code in tensor   ow available to user,
usually stabilizes into main code eventually)

keras (o   cially supported by google)

tf-slim (also supported by go   wait a second... how many apis did
google make for this thing?)

sonnet (supported by deepmind! which is owned by google)

tflearn

sk   ow

lots of fragmentation, but things seem to have stabilized around core
tensor   ow / keras. sonnet may be worth watching, though, because of
deepmind.

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

32 / 33

that   s all folks

questions?

joshua achiam (uc berkeley)

tensor   ow review session

september 8, 2017

33 / 33

