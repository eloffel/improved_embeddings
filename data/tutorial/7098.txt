hardware architectures for 

deep neural networks 

micro tutorial 
october 16, 2016 

website: http://eyeriss.mit.edu/tutorial.html  

1 

2 

speakers 

joel emer 

senior distinguished  
research scientist 

nvidia 
professor 

mit 

vivienne sze 

professor 

mit 

yu-hsin chen 
phd candidate 

mit 

2 

outline 

       overview of deep neural networks 
       dnn development resources 
       survey of dnn computation 
       dnn accelerators 
       network optimizations 
       benchmarking metrics for evaluation 
       dnn training 

3 

participant takeaways 

       understand the key design considerations for 

dnns  

       be able to evaluate different implementations of 
dnn with benchmarks and comparison metrics  

       understand the tradeoffs between various 

architectures and platforms 

       assess the utility of various optimization 

approaches 

       understand recent implementation trends and 

opportunities 

4 

background of  
deep neural networks 

5 

ai and machine learning 

artificial intelligence 
machine learning 

   field of study that gives computers the ability 
to learn without being explicitly programmed    
     arthur samuel, 1959 

6 

brain-inspired machine learning 

artificial intelligence 
machine learning 

brain-inspired 

an  algorithm  that  takes  its  basic 
functionality  from  our  understanding 
of how the brain operates 

7 

how does the brain work? 

       the basic computational unit of the brain is a neuron 

   86b neurons in the brain 

       neurons are connected with nearly 1014     1015 synapses 
       neurons  receive  input  signal  from  dendrites  and  produce 
output  signal  along axon,  which  interact  with  the  dendrites  of 
other neurons via synaptic weights 

       synaptic weights     learnable & control influence strength 

image source: stanford 

8 

spiking-based machine learning 

artificial intelligence 
machine learning 

brain-inspired 

spiking 

9 

spiking architecture 

       brain-inspired 
       integrate and fire 
       example: ibm truenorth 

[merolla et al., science 2014; esser et al., pnas 2016] 
http://www.research.ibm.com/articles/brain-chip.shtml 

10 

machine learning with neural networks 

artificial intelligence 
machine learning 

brain-inspired 

spiking 

 

neural 
networks 

11 

neural networks: weighted sum 

image source: stanford 

12 

many weighted sums 

image source: stanford 

13 

deep learning 

artificial intelligence 
machine learning 

brain-inspired 

spiking 

 

neural 
networks 

deep 

learning 

14 

what is deep learning? 

image 

   volvo 
xc90    

image source: [lee et al., comm. acm 2011] 

15 

why is deep learning hot now? 

big data 
availability 

gpu 

acceleration 

new ml 

techniques 

350m images 
uploaded per 
day 
2.5 petabytes 
of customer 
data hourly 

300 hours of 
video uploaded 
every minute 

16 

id163 challenge 

image classification task: 

 1.2m training images     1000 object categories 

 id164 task: 

 456k training images     200 object categories 

17 

id163: image classification task 

30 

25 

20 

15 

10 

5 

0 

top 5 classification error (%) 
large error rate reduction 
due to deep id98 

2010 

2011 

2012 

2013 

2014 

2015 

human 

hand-crafted feature- 

based designs 

deep id98-based designs 

[russakovsky et al., ijcv 2015] 

18 

gpu usage for id163 challenge 

19 

deep learning on images 

      
image classification 
       object localization 
       id164 

      
image segmentation 
       action recognition 
      
image generation 

20 

deep learning for speech 

       id103 
       natural language processing 
       speech translation 
       audio generation 

21 

deep learning on games 

google deepmind alphago 

22 

medical applications of deep learning 

       brain cancer detection 

image source: [jermyn et al., jbo 2016] 

23 

deep learning for self-driving cars 

24 

connectomics     finding synapses 

(1) em 

(2) ml 
membrane 
detection 

machine learning requires orders of  
magnitude more computation than other parts 

(3) watershed 

(4) agglomeration 

(5) merging 

(6) synapses 

(7) skeletons 

(8) graph 

image source: mit 

25 

mature applications 

       image 

o    classification: image to object class 
o    recognition: same as classification (except for faces) 
o    detection: assigning bounding boxes to objects 
o    segmentation: assigning object class to every pixel 

       speech & language 

o    id103: audio to text 
o    translation 
o    natural language processing: text to meaning 
o    audio generation: text to audio 

       games 

26 

emerging applications 
       medical (cancer detection, pre-natal) 
       finance (trading, energy forecasting, risk) 
       infrastructure (structure safety and traffic) 
       weather forecasting and id37 

this tutorial will focus on image classification 
http://www.nextplatform.com/2016/09/14/next-wave-deep-learning-applications/ 

27 

opportunities 

$500b market over 10 years! 

image source: tractica 

28 

opportunities 

from ee times     september 27, 2016 
 
   today  the  job  of  training  machine  learning  models  is 
limited  by  compute,  if  we  had  faster  processors  we   d 
run bigger models   in practice we train on a reasonable 
subset of data that can finish in a matter of months. we 
could use improvements of several orders of magnitude 
    100x or greater.    

       greg diamos, senior researcher, svail, baidu 

29 

overview of  
deep neural networks 

30 

dnn timeline 

       1940s: neural networks were proposed 
       1960s: deep neural networks were proposed 
       1990s: early hardware for shallow neural nets 

       example: intel etann (1992) 

       1998: lenet for mnist 
       2011: id103 using dnn (microsoft)  
       2012: deep learning starts supplanting traditional ml 

       alexnet for image classification 

       early 2010s: rise of dnn accelerator research 

       examples: neuflow, diannao, etc. 

31 

publications at architecture conferences 

       micro, isca, hpca, asplos 

32 

so many neural networks! 

http://www.asimovinstitute.org/neural-network-zoo/ 

33 

dnn terminology 101 

neurons 

image source: stanford 

34 

dnn terminology 101 

synapses 

image source: stanford 

35 

dnn terminology 101 

each synapse has a weight for neuron activation 

                 =                                        (       =                                                          ) 

x1 

x2 

x3 

w11 

w34 

y1 

y2 

y3 

y4 

image source: stanford 

36 

dnn terminology 101 

weight sharing: multiple synapses use the same weight value 

                 =                                        (       =                                                          ) 

x1 

x2 

x3 

w11 

w34 

y1 

y2 

y3 

y4 

image source: stanford 

37 

dnn terminology 101 

l1 input neurons 
e.g. image pixels 

layer 1 

l1 output neurons 
a.k.a. activations 

image source: stanford 

38 

dnn terminology 101 

l2 input  
activations 

layer 2 

l2 output  
activations 

image source: stanford 

39 

dnn terminology 101 

fully-connected: all i/p neurons connected to all o/p neurons 
sparsely-connected 

image source: stanford 

40 

dnn terminology 101 

feed forward 

feedback 

image source: stanford 

41 

popular types of dnns 

       fully-connected nn 

       feed forward, a.k.a. multilayer id88 (mlp) 

       convolutional nn (id98) 

       feed forward, sparsely-connected w/ weight sharing 

       recurrent nn (id56)  

       feedback 

       long short-term memory (lstm) 

       feedback + storage 

42 

id136 vs. training  

       training: determine weights 

       supervised:  

       reinforcement: 

       training set has inputs and outputs, i.e., labeled 

       output assessed via rewards and punishments 

       unsupervised:  

       training set is unlabeled 

       semi-supervised:  

       training set is partially labeled  

       id136: apply weights to determine output  

 

 

43 

deep convolutional neural networks 

modern deep id98: 5     1000 layers 

conv 
layer 

low-level 
features 

    

high-level 
features 

conv 
layer 

fc 
layer 

classes 

1     3 layers 

44 

deep convolutional neural networks 

conv 
layer 

low-level 
features 

    

high-level 
features 

conv 
layer 

fc 
layer 

classes 

convolution  activation 

  	

45 

deep convolutional neural networks 

conv 
layer 

low-level 
features 

    

high-level 
features 

conv 
layer 

fc 
layer 

classes 

fully 

connected 

activation 

  	

46 

deep convolutional neural networks 

optional layers in between  

conv and/or fc layers 

conv 
layer 

norm 
layer 

pool 
layer 

conv 
layer 

high-level 
features 

fc 
layer 

classes 

id172 

pooling 

47 

deep convolutional neural networks 

conv 
layer 

norm 
layer 

pool 
layer 

conv 
layer 

high-level 
features 

fc 
layer 

classes 

  
  

  
  

  
  

convolutions  account  for  more 
than 90% of overall computation, 
dominating runtime and energy 
consumption 

48 

convolution (conv) layer 

a plane of input activations 

a.k.a. input feature map (fmap) 

filter (weights) 

r 

s 

h 

w 

49 

convolution (conv) layer 

input fmap 

h 

filter (weights) 

r 

s 

w 

element-wise 
multiplication 

50 

convolution (conv) layer 

input fmap 

output fmap 

filter (weights) 

r 

s 

h 

e 

element-wise 
multiplication 

w 

f 
partial sum (psum) 

accumulation 

an output  
activation 

51 

convolution (conv) layer 

input fmap 

output fmap 

filter (weights) 

r 

s 

h 

w 

an output  
activation 

e 

f 

sliding window processing 

52 

convolution (conv) layer 

filter 

c 

r 

s 

input fmap 
c 

h 

w 

many input channels (c) 

output fmap 

e 

f 

53 

convolution (conv) layer 

many 

filters (m) 
c 

r 

 

1 
s 
   
c 

input fmap 
c 

h 

w 

output fmap 

e 

m 

f 
many 

output channels (m) 

r 

m 
s 

54 

convolution (conv) layer 

filters 

c 

r 

 

s 
   
c 

r 

s 

many 

input fmaps (n) 

c 

h 

1 

 

w 
   

c 

h 

n 

  

  

w 

  

  

many 

output fmaps (n) 

m 

e 

1 

 

f 
   

e 

n 

f 

55 

conv layer implementation 

output fmaps   biases 

input fmaps   filter weights 

56 

conv layer implementation 

na  ve 7-layer for-loop implementation: 

for each output fmap value 

for	(n=0;	n<n;	n++)	{	
				for	(m=0;	m<m;	m++)	{	
								for	(x=0;	x<f;	x++)	{	
												for	(y=0;	y<e;	y++)	{	
	
																o[n][m][x][y]	=	b[m];	
																for	(i=0;	i<r;	i++)	{	
convolve  
																				for	(j=0;	j<s;	j++)	{	
a window 
																								for	(k=0;	k<c;	k++)	{	
and apply 
																												o[n][m][x][y]	+=	i[n][k][ux+i][uy+j]	  	w[m][k][i][j];	
																								}	
activation 
																				}	
																}	
	
																o[n][m][x][y]	=	activation(o[n][m][x][y]);	
												}																	
								}	
				}	
}	

57 

traditional id180 

sigmoid 

hyperbolic tangent 
1 

0 

-1 

-1 

0 

1 

0 

1 

1 

0 

-1 

-1 

y=1/(1+e-x)	

y=(ex-e-x)/(ex+e-x)	

image source: caffe tutorial 

58 

modern id180 

rectified linear unit 

(relu) 

leaky relu 

exponential lu 

1 

0 

1 

0 

1 

0 

-1 

-1 

0 

-1 
-1 

1 

0 

1 

-1 

y=max(0,x)	

y=max(  x,x)	
   = small const. (e.g. 0.1) 

0 

-1 

1 
x   0	
				x,							
y=	
				  (ex-1),	
x<0	

image source: caffe tutorial 

59 

fully-connected (fc) layer 

       height and width of output fmaps are 1 (e = f = 1) 
       filters as large as input fmaps (r = h, s = w) 
      

implementation: id127 

filters 
chw 

input fmaps 

output fmaps 

n 

n 

m 

chw 

   

m = 

60 

fc layer     from conv layer pov 

filters 

input fmaps 
c 

c 

h 

 

w 
   
c 

h 

w 

h 

1 

 

w 
   

c 

h 

n 

  
  

  

  

  

w 

  

  

output fmaps 

m 

1 

1 

 

1 
   

  

1 

1 

n 

61 

pooling (pool) layer 

       reduce resolution of each channel independently 
      
       overlapping or non-overlapping    depending on stride 

increase translation-invariance and noise-resilience  

image source: caffe tutorial 

62 

pool layer implementation 

na  ve 6-layer for-loop max-pooling implementation: 

for each pooled value 

for	(n=0;	n<n;	n++)	{	
				for	(m=0;	m<m;	m++)	{	
								for	(x=0;	x<f;	x++)	{	
												for	(y=0;	y<e;	y++)	{	
	
																max	=	-inf;		
																for	(i=0;	i<r;	i++)	{	
																				for	(j=0;	j<s;	j++)	{	
																								if	(i[n][m][ux+i][uy+j]	>	max)	{	
																												max	=	i[n][m][ux+i][uy+j];	
																								}	
																				}	
																}	
	
																o[n][m][x][y]	=	max;	
												}																	
								}	
				}	
}	

find the max  
with in a window 

63 

id172 (norm) layer 

       batch id172 (bn) 

       normalize  activations  towards  mean=0  and  std. 

dev.=1 based on the statistics of the training dataset 
       put in between conv/fc and activation function 

conv 
layer 

convolution 

activation 

bn 

  	

believed to be key to getting high accuracy and  
faster training on very deep neural networks. 

[ioffe et al., icml 2015] 

64 

bn layer implementation 

       the normalized value is further scaled and shifted, the 

parameters of which are learned from training 

data mean 

learned scale factor 

data std. dev. 

learned shift factor 

small const. to avoid 
numerical problems 

65 

id172 (norm) layer 

       local response id172 (lrn) 

       tries to mimic the inhibition scheme in the brain 

now deprecated! 
image source: caffe tutorial 

66 

relevant components for tutorial 

       typical operations that we will discuss: 

       convolution (conv) 
       fully-connected (fc) 
       max pooling 
       relu 

67 

