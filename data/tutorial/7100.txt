bayesian concept learning
id84
id91
evaluation

unsupervised learning

alta de waal

department of statistics

university of pretoria, south africa

13 september 2017

deep learning indaba     sept 13th, 2017

1/36

bayesian concept learning
id84
id91
evaluation

overview

bayesian concept learning

id84

id91

evaluation

resources

deep learning indaba     sept 13th, 2017

2/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

how does a child learn a word?

deep learning indaba     sept 13th, 2017

3/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

how does a child learn a word?

positive examples

deep learning indaba     sept 13th, 2017

3/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

how does a child learn a word?

positive examples

active learning involves negative examples

deep learning indaba     sept 13th, 2017

3/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

how does a child learn a word?

positive examples

active learning involves negative examples

phsychological research has shown that people can learn
concepts from positive examples alone

deep learning indaba     sept 13th, 2017

3/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

concept learning

learning the meaning of a word is equivalent to concept
learning, which in turn is equivalent to binary classi   cation.

de   nition
de   ne f (x) = 1 if x is an example of the concept c and
f (x) = 0 otherwise. the goal is to learn the indicator function
f , which de   nes which elements are in the set c

deep learning indaba     sept 13th, 2017

4/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

concept learning

learning the meaning of a word is equivalent to concept
learning, which in turn is equivalent to binary classi   cation.

de   nition
de   ne f (x) = 1 if x is an example of the concept c and
f (x) = 0 otherwise. the goal is to learn the indicator function
f , which de   nes which elements are in the set c

learn from positive examples
note that standard binary classi   cation techniques require
positive and negative examples. by contrast, we will devise a
way to learn from positive examples alone.

deep learning indaba     sept 13th, 2017

4/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

number game (tennenbaum, 1999)

the concept c

deep learning indaba     sept 13th, 2017

5/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

number game (tennenbaum, 1999)

the concept c

integers between 1 and 100

deep learning indaba     sept 13th, 2017

5/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

number game (tennenbaum, 1999)

the concept c

integers between 1 and 100
suppose i tell you d = {16} is a positive example of the
concept.

deep learning indaba     sept 13th, 2017

5/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

number game (tennenbaum, 1999)

the concept c

integers between 1 and 100
suppose i tell you d = {16} is a positive example of the
concept.

what other numbers do you think are positive?

deep learning indaba     sept 13th, 2017

5/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

number game (tennenbaum, 1999)

the concept c

integers between 1 and 100
suppose i tell you d = {16} is a positive example of the
concept.

what other numbers do you think are positive?

presumably numbers that are similar in some sense to 16
are more likely.

deep learning indaba     sept 13th, 2017

5/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

number game (tennenbaum, 1999)

the concept c

integers between 1 and 100
suppose i tell you d = {16} is a positive example of the
concept.

what other numbers do you think are positive?

presumably numbers that are similar in some sense to 16
are more likely.

but similar in what why?

deep learning indaba     sept 13th, 2017

5/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

number game (tennenbaum, 1999)

the concept c

integers between 1 and 100
suppose i tell you d = {16} is a positive example of the
concept.

what other numbers do you think are positive?

presumably numbers that are similar in some sense to 16
are more likely.

but similar in what why?

suppose i update the positive examples to
d = {2, 8, 16, 64}

deep learning indaba     sept 13th, 2017

5/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

number game (tennenbaum, 1999)

the concept c

integers between 1 and 100
suppose i tell you d = {16} is a positive example of the
concept.

what other numbers do you think are positive?

presumably numbers that are similar in some sense to 16
are more likely.

but similar in what why?

suppose i update the positive examples to
d = {2, 8, 16, 64}
what other numbers do you think are positive?

deep learning indaba     sept 13th, 2017

5/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

human experiment

deep learning indaba     sept 13th, 2017

6/36

481216202428323640444852566064687276808488929610000.5116examples481216202428323640444852566064687276808488929610000.5160481216202428323640444852566064687276808488929610000.5116   8   2  64481216202428323640444852566064687276808488929610000.5116  23  19  20481216202428323640444852566064687276808488929610000.5160  80  10  30481216202428323640444852566064687276808488929610000.5160  52  57  55481216202428323640444852566064687276808488929610000.5181  25   4  36481216202428323640444852566064687276808488929610000.5181  98  86  93bayesian concept learning
id84
id91
evaluation

background
number game
background

plausible concepts:
powers of two

even numbers

powers of two except 32

prime numbers

odd numbers

deep learning indaba     sept 13th, 2017

7/36

bayesian concept learning
id84
id91
evaluation

background
number game
background

bayesian concept learning

deep learning indaba     sept 13th, 2017

8/36

00.10.2powers of 2     {32}powers of 2 + {37}allpowers of 10powers of 9powers of 8powers of 7powers of 6powers of 5powers of 4powers of 3powers of 2ends in 9ends in 8ends in 7ends in 6ends in 5ends in 4ends in 3ends in 2ends in 1mult of 10mult of 9mult of 8mult of 7mult of 6mult of 5mult of 4mult of 3squaresoddevenprior00.20.405101520253035likdata = 1600.20.405101520253035post00.10.2powers of 2     {32}powers of 2 + {37}allpowers of 10powers of 9powers of 8powers of 7powers of 6powers of 5powers of 4powers of 3powers of 2ends in 9ends in 8ends in 7ends in 6ends in 5ends in 4ends in 3ends in 2ends in 1mult of 10mult of 9mult of 8mult of 7mult of 6mult of 5mult of 4mult of 3squaresoddevenprior012x 10   305101520253035likdata = 16   8   2  6400.5105101520253035postbayesian concept learning
id84
id91
evaluation

background
number game
background

bayesian concept learning

deep learning indaba     sept 13th, 2017

8/36

00.10.2powers of 2     {32}powers of 2 + {37}allpowers of 10powers of 9powers of 8powers of 7powers of 6powers of 5powers of 4powers of 3powers of 2ends in 9ends in 8ends in 7ends in 6ends in 5ends in 4ends in 3ends in 2ends in 1mult of 10mult of 9mult of 8mult of 7mult of 6mult of 5mult of 4mult of 3squaresoddevenprior00.20.405101520253035likdata = 1600.20.405101520253035post00.10.2powers of 2     {32}powers of 2 + {37}allpowers of 10powers of 9powers of 8powers of 7powers of 6powers of 5powers of 4powers of 3powers of 2ends in 9ends in 8ends in 7ends in 6ends in 5ends in 4ends in 3ends in 2ends in 1mult of 10mult of 9mult of 8mult of 7mult of 6mult of 5mult of 4mult of 3squaresoddevenprior012x 10   305101520253035likdata = 16   8   2  6400.5105101520253035postbayesian concept learning
id84
id91
evaluation

background
number game
background

unsupervised learning

supervised learning: predict labels based on labelled
training data

no reference to any known labels

id84

id91

deep learning indaba     sept 13th, 2017

9/36

bayesian concept learning
id84
id91
evaluation

pca

principal component analysis (pca)

id84
visualisation
noise    ltering
feature extraction

deep learning indaba     sept 13th, 2017

10/36

bayesian concept learning
id84
id91
evaluation

pca

principal component analysis (pca)

id84
visualisation
noise    ltering
feature extraction

deep learning indaba     sept 13th, 2017

10/36

bayesian concept learning
id84
id91
evaluation

pca

pca for id84

deep learning indaba     sept 13th, 2017

11/36

bayesian concept learning
id84
id91
evaluation

pca

pca for id84

deep learning indaba     sept 13th, 2017

11/36

bayesian concept learning
id84
id91
evaluation

pca

pca for visualisation

deep learning indaba     sept 13th, 2017

12/36

bayesian concept learning
id84
id91
evaluation

pca

pca for visualisation

deep learning indaba     sept 13th, 2017

12/36

bayesian concept learning
id84
id91
evaluation

pca

pca for noise    ltering and feature selection

reconstruction of images from just 150 of the    3000 initial
features.

dimensionality of the data is reduced by nearly a factor of
20

the projected images contain enough information that we
might, by eye, recognise the individuals in the image

deep learning indaba     sept 13th, 2017

13/36

bayesian concept learning
id84
id91
evaluation

pca

pca - summary

e   ective in a wide variety of contexts
good starting point in order to visualize:

the relationship between observations
the main variance in the data

understand the intrinsic dimensionality of the data

o   ers a straightforward and e   cient path to gain insight
into high-dimensional data
weaknesses:

highly a   ected by outliers in the data
doesn   t perform well with non-linear relationships in data

manifold learning
multidimensional scaling (mds)

deep learning indaba     sept 13th, 2017

14/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

id116

id91 seek to learn an optimal division or discrete labeling
of groups of points.
the id116 algorithm searches for a pre-determined
number of clusters within an unlabeled multidimensional
dataset.
simple conception of what the optimal id91 looks like

the    cluster center    is the arithmetic mean of all the
points belonging to the cluster.

each point is closer to its own cluster center than to other
cluster centers.

deep learning indaba     sept 13th, 2017

15/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

how does it work?

deep learning indaba     sept 13th, 2017

16/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

how does it work?

objective

subdivide data points of a dataset into clusters based on
nearest mean values
minimise the distance between points in each cluster

deep learning indaba     sept 13th, 2017

16/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

how does it work?

objective

subdivide data points of a dataset into clusters based on
nearest mean values
minimise the distance between points in each cluster

k

denotes the number of clusters in the data
must be speci   ed (not determined by algorithm)

deep learning indaba     sept 13th, 2017

16/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

how does it work?

objective

subdivide data points of a dataset into clusters based on
nearest mean values
minimise the distance between points in each cluster

k

denotes the number of clusters in the data
must be speci   ed (not determined by algorithm)

input

x     n data points (1, 2, n-dimensional)
k     number of clusters

deep learning indaba     sept 13th, 2017

16/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

how does it work?

objective

subdivide data points of a dataset into clusters based on
nearest mean values
minimise the distance between points in each cluster

k

denotes the number of clusters in the data
must be speci   ed (not determined by algorithm)

input

x     n data points (1, 2, n-dimensional)
k     number of clusters

output

a set of k cluster centroids
labeling of x that assigns each of the points in x to a
unique cluster

deep learning indaba     sept 13th, 2017

16/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

em algorithm

deep learning indaba     sept 13th, 2017

17/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

em algorithm

1 guess some cluster centers

deep learning indaba     sept 13th, 2017

17/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

em algorithm

1 guess some cluster centers
2 repeat until converged

deep learning indaba     sept 13th, 2017

17/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

em algorithm

1 guess some cluster centers
2 repeat until converged

1 e-step: assign points to the nearest cluster center

deep learning indaba     sept 13th, 2017

17/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

em algorithm

1 guess some cluster centers
2 repeat until converged

1 e-step: assign points to the nearest cluster center
2 m-step: set the cluster centers to the mean

deep learning indaba     sept 13th, 2017

17/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

em algorithm

1 guess some cluster centers
2 repeat until converged

1 e-step: assign points to the nearest cluster center
2 m-step: set the cluster centers to the mean

e-step: involves updating our expectation of which cluster
each point belongs to

deep learning indaba     sept 13th, 2017

17/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

em algorithm

1 guess some cluster centers
2 repeat until converged

1 e-step: assign points to the nearest cluster center
2 m-step: set the cluster centers to the mean

e-step: involves updating our expectation of which cluster
each point belongs to

m-step: involves maximizing some    tness function that
de   nes the location of the cluster centersin this case,by
taking a simple mean of the data in each cluster

deep learning indaba     sept 13th, 2017

17/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

example

deep learning indaba     sept 13th, 2017

18/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

1. guess some cluster centers ( initialise   i)

deep learning indaba     sept 13th, 2017

19/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

2. repeat until converged

deep learning indaba     sept 13th, 2017

20/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

2. repeat until converged

deep learning indaba     sept 13th, 2017

20/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

2. repeat until converged

deep learning indaba     sept 13th, 2017

20/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

2. repeat until converged

deep learning indaba     sept 13th, 2017

20/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

2. repeat until converged

deep learning indaba     sept 13th, 2017

20/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

2. repeat until converged

deep learning indaba     sept 13th, 2017

20/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

2. repeat until converged

deep learning indaba     sept 13th, 2017

20/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

final id91

deep learning indaba     sept 13th, 2017

21/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

colour compression

deep learning indaba     sept 13th, 2017

22/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

id116 - summary

limited to linear cluster boundaries

can be slow for a large number of samples
lazy algorithm

doesn   t learn a discriminative function from training data,
but memorises training data
in e   ect it means that id116 doesn   t have a training step
with each prediction, the distances are calculated again

https://datasciencelab.wordpress.com/2013/12/12/
id91-with-id116-in-python/

deep learning indaba     sept 13th, 2017

23/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

gaussian mixture models (gmms)

motivation

id116 has no intrinsic measure of id203 or
uncertainty of cluster assignments.
places a circle (for 2-d) at the center of each cluster
radius of circle acts as a hard cuto    for cluster assignment
within the training set
any point outside this circle is not considered a member of
the cluster

deep learning indaba     sept 13th, 2017

24/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

id116 has no built-in way of accounting for elliptical clusters

deep learning indaba     sept 13th, 2017

25/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

gmm as alternative

gmm

a gaussian mixture model (gmm) attempts to    nd a
mixture of multi-dimensional gaussian id203
distributions that best model any input dataset

in the simplest case, gmms can be used for    nding clusters
in the same manner as id116.

probabilistic in nature -    soft    cluster assignments

deep learning indaba     sept 13th, 2017

26/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

deep learning indaba     sept 13th, 2017

27/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

de   ne the covariance

deep learning indaba     sept 13th, 2017

28/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

gmms as density estimation

deep learning indaba     sept 13th, 2017

29/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

gmms as density estimation

deep learning indaba     sept 13th, 2017

29/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

deep learning indaba     sept 13th, 2017

30/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

mixture of 16 gaussians
cannot    nd separated clusters of data
rather    t the overall distribution of the data
generative model of the distribution
the gmm gives us the recipe to generate new random
data distributed similarly to our input

deep learning indaba     sept 13th, 2017

30/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

digits dataset generated using gmm

deep learning indaba     sept 13th, 2017

31/36

bayesian concept learning
id84
id91
evaluation

id116 id91
gaussian mixture models

digits dataset generated using gmm

deep learning indaba     sept 13th, 2017

31/36

bayesian concept learning
id84
id91
evaluation

deep learning indaba     sept 13th, 2017

32/36

bayesian concept learning
id84
id91
evaluation

deep learning indaba     sept 13th, 2017

33/36

bayesian concept learning
id84
id91
evaluation

deep learning indaba     sept 13th, 2017

34/36

bayesian concept learning
id84
id91
evaluation

evaluation techniques

generative models - likelihood of the data under the model
analytic criterion

akaike information criterion (aic)
bayesian information criterion (bic)

stability based methods

deep learning indaba     sept 13th, 2017

35/36

bayesian concept learning
id84
id91
evaluation

resources

https://github.com/jakevdp/pythondatasciencehandbook
https://rare-technologies.com/blog/
https://chrisalbon.com/
https://machinelearningflashcards.com/

deep learning indaba     sept 13th, 2017

36/36

