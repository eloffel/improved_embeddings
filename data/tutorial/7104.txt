   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]ml review
     * [9]       contribute
     * [10]            review.com newsletter
     __________________________________________________________________

understanding lstm and its diagrams

   [11]go to the profile of shi yan
   [12]shi yan (button) blockedunblock (button) followfollowing
   mar 13, 2016

   i just want to reiterate what   s said here:
   [13]understanding id137
   posted on august 27, 2015 humans don't start their thinking from
   scratch every second. as you read this essay, you   colah.github.io

   i   m not better at explaining lstm, i want to write this down as a way
   to remember it myself. i think the above blog post written by
   christopher olah is the best lstm material you would find. please visit
   the original link if you want to learn lstm. (but i did create some
   nice diagrams.)
     __________________________________________________________________

   although we don   t know how brain functions yet, we have the feeling
   that it must have a logic unit and a memory unit. we make decisions by
   reasoning and by experience. so do computers, we have the logic units,
   cpus and gpus and we also have memories.

   but when you look at a neural network, it functions like a black box.
   you feed in some inputs from one side, you receive some outputs from
   the other side. the decision it makes is mostly based on the current
   inputs.

   i think it   s unfair to say that neural network has no memory at all.
   after all, those learnt weights are some kind of memory of the training
   data. but this memory is more static. sometimes we want to remember an
   input for later use. there are many examples of such a situation, such
   as the stock market. to make a good investment judgement, we have to at
   least look at the stock data from a time window.

   the naive way to let neural network accept a time series data is
   connecting several neural networks together. each of the neural
   networks handles one time step. instead of feeding the data at each
   individual time step, you provide data at all time steps within a
   window, or a context, to the neural network.

   a lot of times, you need to process data that has periodic patterns. as
   a silly example, suppose you want to predict christmas tree sales. this
   is a very seasonal thing and likely to peak only once a year. so a good
   strategy to predict christmas tree sale is looking at the data from
   exactly a year back. for this kind of problems, you either need to have
   a big context to include ancient data points, or you have a good
   memory. you know what data is valuable to remember for later use and
   what needs to be forgotten when it is useless.

   theoretically the naively connected neural network, so called recurrent
   neural network, can work. but in practice, it suffers from two
   problems: vanishing gradient and exploding gradient, which make it
   unusable.

   then later, [14]lstm (long short term memory) was invented to solve
   this issue by explicitly introducing a memory unit, called the cell
   into the network. this is the diagram of a lstm building block.
   [1*lah0_xxekfe0lkju54gkfq.png]

   at a first sight, this looks intimidating. let   s ignore the internals,
   but only look at the inputs and outputs of the unit. the network takes
   three inputs. x_t is the input of the current time step. h_t-1 is the
   output from the previous lstm unit and c_t-1 is the    memory    of the
   previous unit, which i think is the most important input. as for
   outputs, h_t is the output of the current network. c_t is the memory of
   the current unit.

   therefore, this single unit makes decision by considering the current
   input, previous output and previous memory. and it generates a new
   output and alters its memory.
   [1*s0y1a3kxyo7_esug_ksk-q.png]

   the way its internal memory c_t changes is pretty similar to piping
   water through a pipe. assuming the memory is water, it flows into a
   pipe. you want to change this memory flow along the way and this change
   is controlled by two valves.
   [1*os5tavakciii1qnduynlja.jpeg]

   the first valve is called the forget valve. if you shut it, no old
   memory will be kept. if you fully open this valve, all old memory will
   pass through.
   [1*a0louibzykb6j-wsfwwq1q.jpeg]

   the second valve is the new memory valve. new memory will come in
   through a t shaped joint like above and merge with the old memory.
   exactly how much new memory should come in is controlled by the second
   valve.
   [1*dvhmv-0wery0kjqpaishba.png]

   on the lstm diagram, the top    pipe    is the memory pipe. the input is
   the old memory (a vector). the first cross     it passes through is the
   forget valve. it is actually an element-wise multiplication operation.
   so if you multiply the old memory c_t-1 with a vector that is close to
   0, that means you want to forget most of the old memory. you let the
   old memory goes through, if your forget valve equals 1.

   then the second operation the memory flow will go through is this +
   operator. this operator means piece-wise summation. it resembles the t
   shape joint pipe. new memory and the old memory will merge by this
   operation. how much new memory should be added to the old memory is
   controlled by another valve, the     below the + sign.

   after these two operations, you have the old memory c_t-1 changed to
   the new memory c_t.
   [1*9zgzxrsvavjesaowtz69yg.png]

   now lets look at the valves. the first one is called the forget valve.
   it is controlled by a simple one layer neural network. the inputs of
   the neural network is h_t-1, the output of the previous lstm block,
   x_t, the input for the current lstm block, c_t-1, the memory of the
   previous block and finally a bias vector b_0. this neural network has a
   sigmoid function as activation, and it   s output vector is the forget
   valve, which will applied to the old memory c_t-1 by element-wise
   multiplication.
   [1*lndznhvxlskjepcsp4kd4w.png]

   now the second valve is called the new memory valve. again, it is a one
   layer simple neural network that takes the same inputs as the forget
   valve. this valve controls how much the new memory should influence the
   old memory.

   the new memory itself, however is generated by another neural network.
   it is also a one layer network, but uses tanh as the activation
   function. the output of this network will element-wise multiple the new
   memory valve, and add to the old memory to form the new memory.
   [1*exmq2zw1o0k5vsm-buj2mg.png]

   these two     signs are the forget valve and the new memory valve.
   [1*mohp-z4km0qm6y0ry7ygmq.png]

   and finally, we need to generate the output for this lstm unit. this
   step has an output valve that is controlled by the new memory, the
   previous output h_t-1, the input x_t and a bias vector. this valve
   controls how much new memory should output to the next lstm unit.

   the above diagram is inspired by christopher   s blog post. but most of
   the time, you will see a diagram like below. the major difference
   between the two variations is that the following diagram doesn   t treat
   the memory unit c as an input to the unit. instead, it treats it as an
   internal thing    cell   .

   i like the christopher   s diagram, in that it explicitly shows how this
   memory c gets passed from the previous unit to the next. but in the
   following image, you can   t easily see that c_t-1 is actually from the
   previous unit. and c_t is part of the output.

   the second reason i don   t like the following diagram is that the
   computation you perform within the unit should be ordered, but you
   can   t see it clearly from the following diagram. for example to
   calculate the output of this unit, you need to have c_t, the new memory
   ready. therefore, the first step should be evaluating c_t.

   the following diagram tries to represent this    delay    or    order    with
   dash lines and solid lines (there are errors in this picture). dash
   lines means the old memory, which is available at the beginning. some
   solid lines means the new memory. operations require the new memory
   have to wait until c_t is available.
   [1*hil-mwfis_6l4nabnbuqxg.png]

   but these two diagrams are essentially the same. here, i want to use
   the same symbols and colors of the first diagram to redraw the above
   diagram:
   [1*6dhvid122cpwdm9lmvwslkag.png]

   this is the forget gate (valve) that shuts the old memory:
   [1*oouvkl3kimurbwnfveutpg.png]

   this is the new memory valve and the new memory:
   [1*ocscuhnvng74yrsjfn4eoa.png]

   these are the two valves and the element-wise summation to merge the
   old memory and the new memory to form c_t (in green, flows back to the
   big    cell   ):
   [1*z6j0hie_eufmtexgtzt4ua.png]

   this is the output valve and output of the lstm unit:
   [1*2jodip6jvulo-kntkyb96a.png]

     * [15]neural networks
     * [16]recurrent neural network
     * [17]deep learning
     * [18]machine learning
     * [19]lstm

   (button)
   (button)
   (button) 6.7k claps
   (button) (button) (button) 50 (button) (button)

     (button) blockedunblock (button) followfollowing
   [20]go to the profile of shi yan

[21]shi yan

   software engineer & wantrepreneur. interested in computer graphics,
   bitcoin and deep learning.

     (button) follow
   [22]ml review

[23]ml review

   highlights from machine learning research, projects and learning
   materials. from and for ml scientists, engineers an enthusiasts.

     * (button)
       (button) 6.7k
     * (button)
     *
     *

   [24]ml review
   never miss a story from ml review, when you sign up for medium.
   [25]learn more
   never miss a story from ml review
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/37e2f46f1714
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714&source=--------------------------nav_reg&operation=register
   8. https://medium.com/mlreview?source=logo-lo_fdsussqag8eq---33272dbbd858
   9. https://medium.com/mlreview/publish-with-ml-review-c814c54ca28d
  10. http://mlreview.com/?medium
  11. https://medium.com/@shiyan?source=post_header_lockup
  12. https://medium.com/@shiyan
  13. http://colah.github.io/posts/2015-08-understanding-lstms/
  14. https://en.wikipedia.org/wiki/long_short-term_memory
  15. https://medium.com/tag/neural-networks?source=post
  16. https://medium.com/tag/recurrent-neural-network?source=post
  17. https://medium.com/tag/deep-learning?source=post
  18. https://medium.com/tag/machine-learning?source=post
  19. https://medium.com/tag/lstm?source=post
  20. https://medium.com/@shiyan?source=footer_card
  21. https://medium.com/@shiyan
  22. https://medium.com/mlreview?source=footer_card
  23. https://medium.com/mlreview?source=footer_card
  24. https://medium.com/mlreview
  25. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  27. http://colah.github.io/posts/2015-08-understanding-lstms/
  28. https://medium.com/p/37e2f46f1714/share/twitter
  29. https://medium.com/p/37e2f46f1714/share/facebook
  30. https://medium.com/p/37e2f46f1714/share/twitter
  31. https://medium.com/p/37e2f46f1714/share/facebook
