   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

neural networks building blocks

   [9]go to the profile of eugenio culurciello
   [10]eugenio culurciello (button) blockedunblock (button)
   followfollowing
   nov 1, 2017

   neural networks are made of smaller modules or building blocks,
   similarly to atoms in matter and logic gates in digital circuits.

   once you know what the blocks are, you can combine them to solve a
   variety of problems. many new building blocks are added constantly, as
   smart minds discover them

   what are these blocks? how can we use them? follow me   

weights

   weights implement the product of an input i with a weight value w, to
   produce an output o. seems easy, but addition and multiplications are
   at the hearth of neural networks.
   [1*6vepc2js6kqp4-8syet_ew.png]
   weights implement the product of an input i with a weight value w to
   give an output o

neurons

   artificial neurons are one basic building block.
   [1*9mndvdtrf0le-nignpukxg.png]
   neuron, inputs i, output o, weight w

   they sum up some inputs i and use a non-linearity (added as separate
   block) to output (o) a non-liner version of the sum.

   neurons use weights w to take inputs and scale them, before summing the
   values.

identity layers

   identity layers just pass the input to the output. seem pretty bare,
   but they are an essential block of neural networks.
   [1*4z4y-plsd3ccykuscgryiq.png]
   identity: i=o

   see below why. you will not be disappointed!

non-linearities

   [11]http://pytorch.org/docs/master/nn.html#non-linear-activations

   they take the inner value of a neuron and transform it with a non-liner
   function to produce a neuron output.
   [1*09smnzf5tbtnc9213bhkaa.png]
   non-linear module

   the most popular are: relu, tang, sigmoid.

linear layers

   [12]http://pytorch.org/docs/master/nn.html#linear-layers

   these layers are an array of neurons. each take multiple inputs and
   produce multiple outputs.
   [1*nqqo_uzpeyx1cqky7rb3lg.png]
   linear layer: input i and output o are vectors of any and different
   length. a weight matrix (left) and how it may be fully connected
   (right) show different version of the same linear layer.

   the input and output number is arbitrary and of uncorrelated length.

   these building blocks are basically an array of linear combinations of
   inputs scaled by weights. weights multiply inputs with an array of
   weight values, and they are usually learned with a learning algorithm.

   linear layers do not come with non-linearities, you will have to add it
   after each layer. if you stack multiple layers, you will need a
   non-linearity between them, or they will all collapse to a single
   linear layer.

convolutional layers

   [13]http://pytorch.org/docs/master/nn.html#convolution-layers

   they are exactly like a linear layer, but each output neuron is
   connected to a locally constrained group of input neurons.
   [1*qxgjipqjifnpwf5gcxlsiw.png]
   convolution is a 3d matrix operation: an inputs cuboid of data is
   multiplied by multiple kernels to produce an output cuboid of data.
   these are just a lot of multiplication and additions

   this group is often called receptive-field, borrowing the name from
   neuroscience.

   convolutions can be performed in 1d, 2d, 3d    etc.

   these basic block takes advantage of the local features of data, which
   are correlated in some kind of inputs. if the inputs are correlated,
   then it makes more sense to look at a group of inputs rather than a
   single value like in a linear layer. linear layer can be thought of a
   convolutions with 1 value per filters.

recurrent neural network

   [14]http://pytorch.org/docs/master/nn.html#id56

   recurrent neural networks are a special kind of linear layer, where the
   output of each neuron is fed back ad additional input of the neuron,
   together with actual input.
   [1*ct6qi3fmqwnheu4jrzuxdw.png]
   id56: outputs or an additional state are fed back (values at time t-1)
   as an input together with input i (at time t)

   you can think of id56 as a combination of the input at instant t and the
   state/output of the same neuron at time t-1.

   id56 can remember sequences of values, since they can recall previous
   outputs, which again are linear combinations of their inputs.

attention modules

   attention modules are simply a gating function for memories. if you
   want to specify which values in an array should be passed through
   attention, you use a linear layer to gate each input by some weighting
   function.
   [1*z8dudzw7macx6cipxnjcrw.png]
   attention module: an input i vector is linearly combined by multiplying
   each value by a weight matrix and producing one or multiple outputs

   attention modules can be soft when the weights are real-valued and the
   inputs are thus multiplied by values. attention is hard when weight are
   binary, and inputs are either 0 or passing through. outputs are also
   called attention head outputs. more info: [15]nice blog post and
   [16]another.

memories

   a basic building block that saves multiple values in a table.
   [1*gpnm13ajfum5wwyr4t07rw.png]
   memory, input i, ouput o, optional addressing a

   this is basically just a multi-dimensional array. the table can be of
   any size and any dimensions. it can also be composed of multiple banks
   or heads. generally memory have a write function writes to all
   locations and reads from all location. an attention-like module can
   focus reading and writing to specific locations. see attention module
   below. more info: [17]nice blog post.

residual modules

   residual modules are a simple combination of layers and pass-through
   layers or identity layers (ah-ah! told you they were important!).
   [1*lailtsfgeq6icufitj-1tg.png]
   residual module can combine input with a cascade of other layers

   they became famous in resnet: [18]https://arxiv.org/abs/1512.03385, and
   they have revolutionized neural networks since.

lstm, gru, etc.

   [19]http://pytorch.org/docs/master/nn.html#recurrent-layers

   these units use an id56, residual modules and multiple attention modules
   to gate inputs, outputs and state values (and more) of an id56, to
   produce augmented id56 that can remember longer sequence in the future.
   more info and figures: [20]great post and [21]nice blog post.

neural networks

   now the hard part: how do you [22]combine these modules to make neural
   networks that can actually solve interesting problems in the world?

   so far it has been human smart minds that architected these neural
   network topologies.

   but why, did you not tell me that neural network were all about
   learning from data? and yet the most important things or all, the
   network architecture is still designed by hand? what?

this list will grow, stay tuned.

      

   ps1: i do wish some smart minds can help me to find automatic ways to
   learn neural network architectures from data and the problem they need
   to solve.

   if we can do this, we can also create machines that can build their own
   circuits topologies, and self-evolve. this would be a blast. and my
   work would be done. ah, i can finally enjoy life!

about the author

   i have almost 20 years of experience in neural networks in both
   hardware and software (a rare combination). see about me here:
   [23]medium, [24]webpage, [25]scholar, [26]linkedin, and more   

     * [27]artificial intelligence

   (button)
   (button)
   (button) 865 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of eugenio culurciello

[29]eugenio culurciello

   i dream and build new technology

     * (button)
       (button) 865
     * (button)
     *
     *

   [30]go to the profile of eugenio culurciello
   never miss a story from eugenio culurciello, when you sign up for
   medium. [31]learn more
   never miss a story from eugenio culurciello
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a5c47bcd7c8d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@culurciello/neural-networks-building-blocks-a5c47bcd7c8d&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@culurciello/neural-networks-building-blocks-a5c47bcd7c8d&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@culurciello?source=post_header_lockup
  10. https://medium.com/@culurciello
  11. http://secure-web.cisco.com/114bva72ujh6sqvx2mzhai1vtqk7on18k7oj7l0iytoi4u4blofhd0ihztgtvc8yczua_5yemskijkaobqhsrmyguck6zd9vixvesbqfzjg8vmqxn3qrbtpojzzlxlx8boracptjxy1vrzqdco8qrct1fv9fmmop7cnj-nveq6fbj8jfwlbq7nd5ujvjausk8_t7i3ed-3weuvp6d51isbdhhyj2o2sbeaevwbq6biu6yvv8eanh9e5foetaihzxkxwvd2dg8cstwaejc_gelsvt5hw0yeyqv3b25iv92zs89ux-drhxnwec4zdzcj9ec9ziefyjjpeitk_898axoihasratvsv4o0a9jkoja52e/http://pytorch.org/docs/master/nn.html#non-linear-activations
  12. http://secure-web.cisco.com/1azqi165a3_srk7tge3e5ro3auaumbjkxzbbe2ruxapvumu6bdjodgiiao5pfij_ziuso2qcomr9iglwrvimvsq_26i4regz1mla7hywnijfhuzzwbcqere8ml7_yhfr2mwyws9fijwtkht0ls0ce8sc6b6nkflvofei5b12dlkat7punw7vidj71qpepbvtpstjqs0ubwsloah4bthwe2n7gvsqbzfsaxwup9yec8iacjvfmx9htlzkgsnapzbsjs62vqo7oiwgkshuc77n52qwzznxahg5t9ximxfchisewnz8f7ad9cfkatn4xqjjlvewfscfuey73d7jpobyg_zf0fjm1dzjkvnw6ty12hdu/http://pytorch.org/docs/master/nn.html#linear-layers
  13. http://secure-web.cisco.com/1pvqjlvacnarzbxkctengxko4ndou7fp0mre674v_so7siqgsr49x-9xdzbosoh04ljq0pbcwwyjxke2dac5spdkouu1rbzqpakz8rx1aj3vg93d6emzymegreiwvyntdcjddmiqqrqqnft0uivhhiqp0iz-u4jh0no9d0lfc9nfsqy3gpse99f8jws0ugaht-km2q8sifvc55mbcmpfplgf7rjh3nssauybs0r05kthxofyv5yqj9let29wurrze2muo6d-atrhrf2fpcmbuwqt-vp8rnrpxip0alzue2yzecp1m8xa1eiiocezu7sus7txkzvcaow0as0dg6uz9r1flwgqj31gmid-n6xfb5li/http://pytorch.org/docs/master/nn.html#convolution-layers
  14. http://secure-web.cisco.com/13gqo-wuq9ecat6mwpaaaunlxa--8o7lhosl6-iixmnrydb8tuolw1auv565yrycbpekbhdkg2chj4vkc_th3mtzp1uwmbu6eqkcwj2b-xpiunby_njizewxero4qzqmjl-8gg44mmkrwtrowmf4lggzpptqgi05xfznmmj4ib46h9hi-ir2mwqm_7aywcxmzrrfqnhlusxcu8yozstb1jl4pjwjcathh7-fxpht8iuw7g6_blxjw2gif9tmzpoxq8c3gemnictzb3lam6qd4ohl0q2gtkaagb_8g4lrxzp9-kasstdss9ueowtieanorqktmv0v6_u4m6aynymv1aixzna-f2uhnvwklfd9ct8y/http://pytorch.org/docs/master/nn.html#id56
  15. https://distill.pub/2016/augmented-id56s/
  16. http://akosiorek.github.io/ml/2017/10/14/visual-attention.html
  17. https://distill.pub/2016/augmented-id56s/
  18. https://secure-web.cisco.com/1m2kbt459siun25js1lqr8kdnmvj54a39cjcvfzo6jb3jscevu9repxk3lnulgqjyzpxzyv5nxouushqdqobwz0taiupxn52w6vlz36gmmisc5bifxzu0b_kmg9--gw3lsdswo7vpekjlwn9loehrxun51gfc6tsc2zazn4yz4zcst_ujadmyhxhylunkqztoudu0xezx79txbuqnq3nwsgaaqrfy-vkkd4m9kb5txd_axs0nngahld93crub9qdw_hoirfhcznj3ecxcrjnwrpdjtxfhjtc88kxxejjj0llv1ualmmyasrarm-li4fofz_ymlfe9dti3bgurmkb8cuyym6vjtj6n_m7ckp9ebfi/https://arxiv.org/abs/1512.03385
  19. http://secure-web.cisco.com/1ejbhpjd78ykx2hledyupz07txkc-gqfrpp7vyjqv9x_nbtym9kjwfx-snxuupyvfkcawfzuefwz1isqwyfj8jnoaba_6as6f6edvwu6u_3bkvcmkpcpfr7xhmujvy1es5yv0vid98mfy5gvkkvci79uw29aouvzjns1yx4d4zqpmlbi6gxeu0blydsvk73yvgk_r9zeifp6u223abauftab6oscog-6ocupoc0qy6pnsidxmqnr1gw4j7mqhyfsfjel90m6fqf5uxgrhbq2jpofmaxlh0i5hwvdrthnvikc8swakxkxjf-uwrceyw2bprnv4ncex3f5ovtf70lyx9wrkgqkbf_sofqvse1ls-5h0/http://pytorch.org/docs/master/nn.html#recurrent-layers
  20. http://colah.github.io/posts/2015-08-understanding-lstms/
  21. https://distill.pub/2016/augmented-id56s/
  22. https://towardsdatascience.com/neural-network-architectures-156e5bad51ba
  23. https://medium.com/@culurciello/
  24. https://e-lab.github.io/html/contact-eugenio-culurciello.html
  25. https://scholar.google.com/citations?user=segmqkiaaaaj
  26. https://www.linkedin.com/in/eugenioculurciello/
  27. https://medium.com/tag/artificial-intelligence?source=post
  28. https://medium.com/@culurciello?source=footer_card
  29. https://medium.com/@culurciello
  30. https://medium.com/@culurciello
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/a5c47bcd7c8d/share/twitter
  34. https://medium.com/p/a5c47bcd7c8d/share/facebook
  35. https://medium.com/p/a5c47bcd7c8d/share/twitter
  36. https://medium.com/p/a5c47bcd7c8d/share/facebook
