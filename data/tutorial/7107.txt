   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

memory, attention, sequences

   [16]go to the profile of eugenio culurciello
   [17]eugenio culurciello (button) blockedunblock (button)
   followfollowing
   jul 11, 2017

   we have seen the rise and success of [18]categorization neural
   networks. the next big step in neural network is to make sense of
   complex spatio-temporal data coming from observing and interacting with
   the real world. we talked before about the [19]new wave of neural
   networks that operate in this space.

   but how can we use these network to learn complex tasks in the real
   world? for example, how can i tell my advanced vacuum cleaning robot:
      roomby: you forgot to vacuum the spot under the red couch in the
   living room!    and get an proper response?

   for this to happen, we need to parse spatio-temporal information with
   attention mechanism, so we can understand complex instruction and how
   they relate to our environment.

   let us consider a couple of example applications: summarization of text
   or video. consider this text:

        a woman in a blue dress was approached by a woman in a white dress.
     she cut a few slices of an apple. she then gave a slice to the woman
     in blue.   

   in order to answer to the question:    who offered a slice of apple?    we
   need to focus on    apple   ,    apple owner   ,    give    words and concepts. the
   rest of the story is not relevant. these part of the story need our
   attention.

   a similar situation occurs in video summarization, where a long video
   can be summarized in a small set of sequences of frames where important
   actions are performed, and which again require our attention. imagine
   you are looking for the car keys, or your shoes, you would focus on
   different part of the video, and the scene. for each action and for
   each goal, we need attention to focus on the important data, and ignore
   the rest.
   [1*wi6vuu1lh2iw2tayk32zuw.jpeg]
   example of video summarization

   well if you think about it, summarization and a focused set of data is
   important for every temporal sequence, be it translation of a document,
   or action recognition in video, or the combination of a sentence
   description of a task and the execution in an environment.

     all these tasks need to reduce the data to focal set, and pay
     attention to the set in order to provide an answer or action

   attention is then one of the most important components of neural
   networks adept to understand sequences, be it a video sequence, an
   action sequence in real life, or a sequence of inputs, like voice or
   text or any other data. it is no wonder that [20]our brain implements
   attention at many levels, in order to select only the important
   information to process, and eliminate the overwhelming amount of
   background information that is not needed for the task at hand.

   a great review of attention in neural network is [21]given here. i
   report here some important diagrams as a reference:
   [1*ol7jld1cgbuhawotkphd-w.png]
   an attention model is a method that takes n arguments y_1     y_n and a
   context c. it returns a vector z which is the summary of the y_i
   focusing on the information linked to context c. more formally, it
   returns a weighted arithmetic mean of the y_i and the weights are
   chosen according the relevance of each y_i given the context c.
   [1*jdxssdfra2haw7zgtyy97g.png]
   implementation of the attention model. notice that m_i = tanh(w1 c + w2
   y_i), meaning that both y_i and c are linearly combined.
   [1*sjh08sptuzdm8kbdnzc9ta.png]
   attention model with dot-products used to define relevance of inputs
   vs context.

   both the last 2 figures above implement    soft    attention. hard
   attention is implemented by randomly picking one of the inputs y_i with
   id203 s_i. this is a rougher choice than the averaging of soft
   attention. soft attention use is preferred because it can be trained
   with back-propagation.

   attention and memory systems are also[22] described here with nice
   visualizations.

   attention can come at a cost, as they mention in [23]this article, but
   in reality this cost can be minimized by hierarchical attention
   modules, such as the ones [24]implemented here.

now see how attention can [25]implement an entire id56 for translation:

   it can do so by stacking multiple layer of attention modules, and with
   an architecture such as this one:
   [1*au1tiiuainffepyowy1cbg.jpeg]
   sequence to sequence system: an encoder takes in an input sequence x,
   and produces an embedding z. a decoder produces an output sequence y,
   by taking as input the embedding z and the previous output y of t-1.
   [1*dh-bvcxsdr6dtn-ub9loea.jpeg]
   module used for attention [26]here. q = query, k = key, v = values. q
   and k are multiplied together and scaled to compute a    similarity
   metric   . this metric produces a weight that modulates the values v.

   multiple attention modules can be used in parallel in a multi-head
   attention:
   [1*4nt8dpqx6d7uodwv_5dtmq.jpeg]
   multiple attention heads are used in parallel to focus on different
   parts of a sequence in parallel. here v,q,k are projected with neural
   network layers to another space, so they can be scaled and mixed.

   the entire attention-based network is called    transformer    network:
   [1*2nqu3lpphoangdi9ki4gwg.jpeg]

   in id56, time is encoded in the sequence, as inputs and outputs flow one
   at a time. in a feed-forward neural network, time needs to be
   represented to preserve the positional encoding. in these
   attention-driven networks, time is encoded as an added extra input, a
   sine wave. it is basically a signal that is added to inputs and outputs
   to represent the passing of time. notice here the biological parallel
   with brain waves and [27]neural oscillations.

     but why do we want to use attention-based neural network instead of
     the id56/lstm we have been [28]using so far? because they use a lot
     less computation!

   if you read the paper to table 2, you will see these network can save
   2   3 orders of magnitude of operations! that is some serious saving!

   i believe this attention-based network will slowly supplant id56 in many
   application of neural networks.

   [29]here you can find a great explanation of the transformer
   architecture and data flow!

memory

   one important piece of work that is also interesting is [30]fast
   weights. this work implements a neural associative memory         this is a
   kind of short-term memory that sits in between neural weight
   (long-term) and recurrent weights (very-fast weights based on input
   activities). fast weights implements a kind of memory that is similar
   to the neural attention mechanism seen above, where we compare current
   inputs to a set of stored previous inputs. this is basically what
   happens in the    attention model with dot-products    digram seen above.

   in fast weights the input x(t) is the context used to compare to
   previously stored values h in the figure below.
   [1*k8sqv7yc_aexedynktj04w.jpeg]
   fast associative memory implemented in [31]fast weights

   if you read the [32]paper you can see that this kind of neural network
   associative memory can outperform id56 and id137 again, in the
   same way that attention can.

   i thinks this is again mounting evidence that many of the task
   currently performed by id56 today, could be replaced by less
   computationally expensive (not to mention using less memory bandwidth
   and parameters) algorithms like this one.

   please also look at: [33]attentive recurrent comparators, which also
   combine attention and recurrent layers to understand the details of a
   learning unit.

about the author

   i have almost 20 years of experience in neural networks in both
   hardware and software (a rare combination). see about me here:
   [34]medium, [35]webpage, [36]scholar, [37]linkedin, and more   

donations

   [1*gcbudlipi6szcmfkhuqrcg.jpeg]

   if you found this article useful, please consider a [38]donation to
   support more tutorials and blogs. any contribution can make a
   difference!

     * [39]machine learning
     * [40]towards data science
     * [41]id56
     * [42]attention based nn
     * [43]lstm

   (button)
   (button)
   (button) 865 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [44]go to the profile of eugenio culurciello

[45]eugenio culurciello

   i dream and build new technology

     (button) follow
   [46]towards data science

[47]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 865
     * (button)
     *
     *

   [48]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [49]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/37456d271992
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/memory-attention-sequences-37456d271992&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/memory-attention-sequences-37456d271992&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_7glmd683eqma---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@culurciello?source=post_header_lockup
  17. https://towardsdatascience.com/@culurciello
  18. https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba
  19. https://medium.com/towards-data-science/a-new-kind-of-deep-neural-networks-749bcde19108
  20. https://en.wikipedia.org/wiki/attention
  21. https://blog.heuritech.com/2016/01/20/attention-mechanism/
  22. http://distill.pub/2016/augmented-id56s/
  23. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  24. https://arxiv.org/abs/1706.03762
  25. https://arxiv.org/abs/1706.03762
  26. https://arxiv.org/abs/1706.03762
  27. https://en.wikipedia.org/wiki/neural_oscillation
  28. https://arxiv.org/abs/1512.02595
  29. https://jalammar.github.io/illustrated-transformer/
  30. https://arxiv.org/abs/1610.06258
  31. https://arxiv.org/abs/1610.06258
  32. https://arxiv.org/abs/1610.06258
  33. https://medium.com/@sanyamagarwal/understanding-attentive-recurrent-comparators-ea1b741da5c3
  34. https://medium.com/@culurciello/
  35. https://e-lab.github.io/html/contact-eugenio-culurciello.html
  36. https://scholar.google.com/citations?user=segmqkiaaaaj
  37. https://www.linkedin.com/in/eugenioculurciello/
  38. https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=q3fhe3bwsc72w
  39. https://towardsdatascience.com/tagged/machine-learning?source=post
  40. https://towardsdatascience.com/tagged/towards-data-science?source=post
  41. https://towardsdatascience.com/tagged/id56?source=post
  42. https://towardsdatascience.com/tagged/attention-based-nn?source=post
  43. https://towardsdatascience.com/tagged/lstm?source=post
  44. https://towardsdatascience.com/@culurciello?source=footer_card
  45. https://towardsdatascience.com/@culurciello
  46. https://towardsdatascience.com/?source=footer_card
  47. https://towardsdatascience.com/?source=footer_card
  48. https://towardsdatascience.com/
  49. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  51. https://medium.com/p/37456d271992/share/twitter
  52. https://medium.com/p/37456d271992/share/facebook
  53. https://medium.com/p/37456d271992/share/twitter
  54. https://medium.com/p/37456d271992/share/facebook
