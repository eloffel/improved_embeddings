   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]ml review
     * [9]       contribute
     * [10]            review.com newsletter
     __________________________________________________________________

l1 norm id173 and sparsity explained for dummies

   [11]go to the profile of shi yan
   [12]shi yan (button) blockedunblock (button) followfollowing
   aug 27, 2016

   well, i think i   m just dumb. when understanding an
   abstract/mathematical idea, i have to really put it into images, i have
   to see and touch it in my head. i need the geometry, the object, the
   intuition behind the idea and better with vivid metaphors in real life.

   sometimes when i found people don   t think or at least don   t explain
   things this way, pointing me to equations and papers, saying there are
   no simple explanations, i got angry. and often after i thought stuff
   through, i could find silly intuitive explanations to those ideas. one
   such an experience was yesterday when i tried to understand l1 norm
   id173 applied to machine learning. thus, i   d like to make this
   silly but intuitive piece to explain this idea to fellow dummies like
   myself.

   when performing a machine learning task on a small dataset, one often
   suffers from the over-fitting problem, where the model accurately
   remembers all training data, including noise and unrelated features.
   such a model often performs badly on new test or real data that have
   not been seen before. because the model treats the training data too
   seriously, it failed to learn any meaningful pattern out of it, but
   simply memorizing everything it has seen.

   now, one solution to solve this issue is called id173. the
   idea is applying an l1 norm to the solution vector of your machine
   learning problem (in case of deep learning, it   s the neural network
   weights.), and trying to make it as small as possible. so if your
   initial goal is finding the best vector x to minimize a id168
   f(x), your new task should incorporate the l1 norm of x into the
   formula, finding the minimum (f(x) + l1norm(x)). the big claim they
   often throw at you is this: an x with small l1 norm tends to be a
   sparse solution. being sparse means that the majority of x   s components
   (weights) are zeros, only few are non-zeros. and a sparse solution
   could avoid over-fitting.

   that   s it, that   s how they explain it in most of the articles,
   textbooks, materials. giving an idea without explanation feels like
   pushing a spear through the back of my head.
   [1*t7deoupm6rbhw2rt0qg3hq.gif]

   not sure about you guys, but the reason for using an l1 norm to ensure
   sparsity and therefore avoid over-fitting wasn   t so obvious to me. it
   took me some time to figure out why. essentially, i had these
   questions:
    1. why does a small l1 norm give a sparse solution?
    2. why does a sparse solution avoid over-fitting?
    3. what does id173 do really?

   my initial confusion came from the fact that i only looked at the l1
   norm and only thought about what it means for l1 norm to be small. what
   i should really do, however, is thinking the id168 and the l1
   norm penalty as a whole.

   let me explain it from the beginning, the over-fitting problem. i   d
   like to use a concrete example. suppose you purchased a robot and you
   want to teach him to classify the chinese characters by looking at the
   following example:
   [1*wx0sua_cja5mnd4yxnb63q.png]

   the first 5 characters belong to the first category, the last 5 are the
   second category. and these 10 characters are the only training data you
   have.
   [1*9yq6foltpdsguw7djv0xoa.png]

   now, unfortunately, the robot is too smart for the task. it has large
   enough memory to remember 5 characters. after seeing all the 10
   characters, the robot learned a way to categorize them: it remembers
   all the first 5 characters exactly. as long as a character is not one
   of those 5, the robot will put the character into the second category.

   of course, this method will work very well on the 10 training
   characters, as the robot can achieve 100% accuracy. however, you
   provide a new character:
   [1*bozhrokfjdhe4qzmcsd7hw.png]

   this character should belong to the first category. but because it
   never appeared in the training data, the robot hasn   t seen it before.
   based on its algorithm, the robot will put this character into the
   second category, which is wrong.
   [1*miulihwinxwbvcxwrqc_cg.png]

   it should be pretty obvious for us human to see the pattern here. all
   characters that belong to the first category have a common part. the
   robot failed the task because it   s too smart and the training data is
   too small.

   this is the problem of over-fitting. but what is id173 and why
   can sparsity avoid over-fitting?
   [1*8k1trclpcaa4uswqtfwtda.gif]

   now suppose you got angry at your robot. you banged the head of the
   robot with a hammer, and while doing it, you shook some of its memory
   chips off his head. you essentially have made the robot dumber. now,
   instead of being able to memorize 5 characters, the robot can only
   remember a character part.

   you let the robot do the training again by looking at all 10 characters
   and still force him to achieve the same accuracy. because he can   t
   remember all 5 characters this time, you essentially force him to look
   for a simpler pattern. now he discovers the common part of all the
   category a characters!

   this is exactly what l1 norm id173 does. it bangs on your
   machine (model) to make it    dumber   . so instead of simply memorizing
   stuff, it has to look for simpler patterns from the data. in the case
   of the robot, when he could remember 5 characters, his    brain    has a
   vector of size 5: [   ,    ,    ,    ,    ]. now after id173 (banging),
   4 slots of his memory became unusable. therefore the newly learned
   vector is: [   , 0, 0, 0, 0] and clearly, this is a sparse vector.

   more formally, when you are solving a large vector x with less training
   data. the solutions to x could be a lot.
   [1*u_ytcgvmsafqfrj4_my9vw.png]

   here a is a matrix that contains all the training data. x is the
   solution vector you are looking for. b is the label vector.

   when data is not enough and your model   s parameter size is large, your
   matrix a will not be    tall    enough and your x is very long. so the
   above equation will look like this:
   [1*utjb7cwbkiye1zcxib64lw.png]

   for a system like this, the solutions to x could be infinite. to find a
   good one out of those solutions, you want to make sure each component
   of your selected solution x captures a useful feature of your data. by
   l1 id173, you essentially make the vector x smaller (sparse),
   as most of its components are useless (zeros), and at the same time,
   the remaining non-zero components are very    useful   .

   another metaphor i can think of is this: suppose you are the king of a
   kingdom that has a large population and an ok overall gdp, but the per
   capita is very low. each one of your citizens is lazy and unproductive
   and you are mad. therefore you command    be productive, strong and hard
   working, or you die!    and you enforce the same gdp as before. as a
   result, many people died due to your harshness, those who survived your
   tyranny became really capable and productive. you can think the
   population here is the size of your solution vector x, and commanding
   people to be productive or die is essentially id173. in the
   regularized sparse solution, you ensure that each component of the
   vector x is very capable. each component must capture some useful
   feature or pattern of the data.

   another way of id173 in deep learning is dropout. the idea is
   simple, removing some random neural connections from the neural network
   while training and adding them back after a while. essentially this is
   still trying to make your model    dumber    by reducing the size of the
   neural network and put more responsibilities and pressure on the
   remaining weights to learn something useful. once those weights have
   learned good features, then adding back other connections to embrace
   new data. i   d like to think this adding back connection thing as
      introducing immigrants to your kingdom when your are in short hands   
   in the above metaphor.

   based on this    making model dumber    idea, i guess we can come up with
   other similar ways to avoid over-fitting, such as starting with a small
   network and gradually adding new neurons and connections to the network
   when more data is available. or performing a pruning while training to
   get rid of connections that are close to zero.

   so far we have demonstrated why sparsity can avoid over-fitting. but
   why adding an l1 norm to the id168 and forcing the l1 norm of
   the solution to be small can produce sparsity?

   yesterday when i first thought about this, i used two example vectors
   [0.1, 0.1] and [1000, 0]. the first vector is obviously not sparse, but
   it has the smaller l1 norm. that   s why i was confused, because looking
   at the l1 norm alone won   t make this idea understandable. i have to
   consider the entire id168 as a whole.

   let   s go back to the problem of ax = b, with a simple and concrete
   example. suppose we want to find a line that matches a set of points in
   2d space. we all know that you need at least 2 points to fix a line.
   but what if the training data has only one point? then you will have
   infinite solutions: every line that passes through the point is a
   solution. suppose the point is at [10, 5], and a line is defined as a
   function y = a * x + b. then the problem is finding a solution to this
   equation:
   [1*rzfzrjnxx8wupsxw957njw.gif]

   since b = 5     10 * a, all points on this following line b = 5     10 * a
   should be a solution:
   [1*sms5qc_2o6h87l_nf0b8mw.gif]

   but how to find the sparse one with l1 norm?

   l1 norm is defined as the summation of absolute values of a vector   s
   all components. for example, if a vector is [x, y], its l1 norm is |x|
   + |y|.

   now if we draw all points that have a l1 norm equals to a constant c,
   those points should form something (in red) like this:
   [1*uhxe9qibzdqiebfje7hggw.png]

   this shape looks like a tilted square. in high dimension space, it will
   be an octahedron. notice that on this red shape, not all points are
   sparse. only on the tips, points are sparse. that is, either x or y
   component of a point is zero. now the way to find a sparse solution is
   enlarging this red shape from the origin by giving an ever-growing c to
      touch    the blue solution line. the intuition is that the touch point
   is most likely at a tip of the shape. since the tip is a sparse point,
   the solution defined by the touch point is also a sparse solution.
   [1*0qrbxi6dlivroqcsfqeyha.png]

   as an example, in this graph, the red shape grows 3 times till it
   touches the blue line b = 5   10 * a. the touch point, as you can see, is
   at a tip of the red shape. the touch point [0.5, 0] is a sparse vector.
   therefore we say, by finding the solution point with the smallest l1
   norm (0.5) out of all possible solutions (points on the blue line), we
   find a sparse solution [0.5, 0] to our problem. at the touch point, the
   constant c is the smallest l1 norm you could find within all possible
   solutions.

   the intuition of using l1 norm is that the shape formed by all points
   whose l1 norm equals to a constant c has many tips (spikes) that happen
   to be sparse (lays on one of the axises of the coordinate system). now
   we grow this shape to touch the solutions we find for our problem
   (usually a surface or a cross-section in high dimension). the
   id203 that the touch point of the 2 shapes is at one of the
      tips    or    spikes    of the l1 norm shape is very high. that   s why you
   want to put l1 norm into your id168 formula so that you can
   keep looking for a solution with a smaller c (at the    sparse    tip of
   the l1 norm). (so in the real id168 case, you are essentially
   shrinking the red shape to find a touch point, not enlarging it from
   the origin.)

   does l1 norm always touch the solution at a tip and find us a sparse
   solution? not necessarily. suppose we still want to find a line out of
   2d points, but this time, the only training data is a point [1, 1000].
   in this case, the solution line b = 1000 -a is in parallel to one of
   the edges of the l1 norm shape:
   [1*bgiz0bxu7fqordvb1i1r4w.png]

   eventually, they touch on an edge, not by a tip. not only you can   t
   have a unique solution this time, most of your regularized solutions
   are still not sparse (other than the two tip points.)

   but again, the id203 of touching a tip is very high. i guess this
   is even more true for high dimension, real-world problems. as when your
   coordinate system has more axises, your l1 norm shape should have more
   spikes or tips. it must look like a cactus or a hedgehog! i can   t
   imagine.
   [1*yq9q8vdefr4sm8olqdlsgg.jpeg]
   [1*97qyaukolsnrsqyd90yhhq.jpeg]

   if you push a person towards a cactus, the id203 of he being
   pricked by the needles is pretty high. that   s also why they invented
   this pervert weapon and that   s why they want to use l1 norm.
   [1*fpzrodf7-x3818ngnib8iq.jpeg]

   but is the l1 norm the best kind of norm to find a sparse solution?
   well, it turns out that the lp norm when 0 <= p < 1 gives the best
   result. this can be explained by looking at the shapes of different
   norms:
   [1*olfn24vf_c3y5p3tiz4_5a.png]

   as you can see, when p < 1, the shape is more    scary   , with more
   sharpen, outbreaking spikes. whereas when p = 2, the shape becomes a
   smooth, non-threatening ball. then why not letting p < 1? that   s
   because when p < 1, there are calculation difficulties.

   in conclusion, over-fitting is a problem you see when your machine
   learning model is too large (has too many parameters) comparing to your
   available training data. in this case, the model tends to remember all
   training cases including noisy to achieve better training score. to
   avoid this, id173 is applied to the model to (essentially)
   reduce its size. one way of id173 is making sure the trained
   model is sparse so that the majority of it   s components are zeros.
   those zeros are essentially useless, and your model size is in fact
   reduced.

   the reason for using l1 norm to find a sparse solution is due to its
   special shape. it has spikes that happen to be at sparse points. using
   it to touch the solution surface will very likely to find a touch point
   on a spike tip and thus a sparse solution.

   just think about this:
   [1*ruvj6qu2_so5wmi4pdsu9g.png]

   when there is a zombie outbreak, which one should be the weapon of
   choice?
   [1*sk7d7ax_tw1htbsixv8nwg.jpeg]

     * [13]machine learning
     * [14]artificial intelligence
     * [15]deep learning
     * [16]neural networks

   (button)
   (button)
   (button) 3k claps
   (button) (button) (button) 20 (button) (button)

     (button) blockedunblock (button) followfollowing
   [17]go to the profile of shi yan

[18]shi yan

   software engineer & wantrepreneur. interested in computer graphics,
   bitcoin and deep learning.

     (button) follow
   [19]ml review

[20]ml review

   highlights from machine learning research, projects and learning
   materials. from and for ml scientists, engineers an enthusiasts.

     * (button)
       (button) 3k
     * (button)
     *
     *

   [21]ml review
   never miss a story from ml review, when you sign up for medium.
   [22]learn more
   never miss a story from ml review
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5b0e4be3938a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/mlreview/l1-norm-id173-and-sparsity-explained-for-dummies-5b0e4be3938a&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/mlreview/l1-norm-id173-and-sparsity-explained-for-dummies-5b0e4be3938a&source=--------------------------nav_reg&operation=register
   8. https://medium.com/mlreview?source=logo-lo_kpkjr80z1zww---33272dbbd858
   9. https://medium.com/mlreview/publish-with-ml-review-c814c54ca28d
  10. http://mlreview.com/?medium
  11. https://medium.com/@shiyan?source=post_header_lockup
  12. https://medium.com/@shiyan
  13. https://medium.com/tag/machine-learning?source=post
  14. https://medium.com/tag/artificial-intelligence?source=post
  15. https://medium.com/tag/deep-learning?source=post
  16. https://medium.com/tag/neural-networks?source=post
  17. https://medium.com/@shiyan?source=footer_card
  18. https://medium.com/@shiyan
  19. https://medium.com/mlreview?source=footer_card
  20. https://medium.com/mlreview?source=footer_card
  21. https://medium.com/mlreview
  22. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  24. https://medium.com/p/5b0e4be3938a/share/twitter
  25. https://medium.com/p/5b0e4be3938a/share/facebook
  26. https://medium.com/p/5b0e4be3938a/share/twitter
  27. https://medium.com/p/5b0e4be3938a/share/facebook
