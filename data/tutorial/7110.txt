   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*ihxppxallvwq3pddu-lhyw.jpeg]

id173 in machine learning

   [16]go to the profile of prashant gupta
   [17]prashant gupta (button) blockedunblock (button) followfollowing
   nov 15, 2017

   one of the major aspects of training your machine learning model is
   avoiding overfitting. the model will have a low accuracy if it is
   overfitting. this happens because your model is trying too hard to
   capture the noise in your training dataset. by noise we mean the data
   points that don   t really represent the true properties of your data,
   but random chance. learning such data points, makes your model more
   flexible, at the risk of overfitting.

   the concept of balancing bias and variance, is helpful in understanding
   the phenomenon of overfitting.
   [18]balancing bias and variance to control errors in machine learning
   in the world of machine learning, accuracy is everything. you strive to
   make your model more accurate by tuning and   medium.com

     one of the ways of avoiding overfitting is using cross validation,
     that helps in estimating the error over test set, and in deciding
     what parameters work best for your model.

   [19]cross-validation in machine learning
   there is always a need to validate the stability of your machine
   learning model. i mean you just can   t fit the model to   medium.com

   this article will focus on a technique that helps in avoiding
   overfitting and also increasing model interpretability.

id173

   this is a form of regression, that constrains/ regularizes or shrinks
   the coefficient estimates towards zero. in other words, this technique
   discourages learning a more complex or flexible model, so as to avoid
   the risk of overfitting.

   a simple relation for id75 looks like this. here y
   represents the learned relation and    represents the coefficient
   estimates for different variables or predictors(x).

   y       0 +   1x1 +   2x2 +    +   pxp

   the fitting procedure involves a id168, known as residual sum
   of squares or rss. the coefficients are chosen, such that they minimize
   this id168.
   [1*dy3-iagchjjlg7oyxx1o3a.png]

   now, this will adjust the coefficients based on your training data. if
   there is noise in the training data, then the estimated coefficients
   won   t generalize well to the future data. this is where id173
   comes in and shrinks or regularizes these learned estimates towards
   zero.

ridge regression

   [1*ciqz8lhwxi5c4d1nv24w4g.png]

   above image shows ridge regression, where the rss is modified by adding
   the shrinkage quantity. now, the coefficients are estimated by
   minimizing this function. here,    is the tuning parameter that decides
   how much we want to penalize the flexibility of our model. the increase
   in flexibility of a model is represented by increase in its
   coefficients, and if we want to minimize the above function, then these
   coefficients need to be small. this is how the ridge regression
   technique prevents coefficients from rising too high. also, notice that
   we shrink the estimated association of each variable with the response,
   except the intercept   0, this intercept is a measure of the mean value
   of the response when xi1 = xi2 =    = xip = 0.

   when    = 0, the penalty term has no e   ect, and the estimates produced
   by ridge regression will be equal to least squares. however, as         ,
   the impact of the shrinkage penalty grows, and the ridge regression
   coe   cient estimates will approach zero. as can be seen, selecting a
   good value of    is critical. cross validation comes in handy for this
   purpose. the coefficient estimates produced by this method are also
   known as the l2 norm.

   the coefficients that are produced by the standard least squares method
   are scale equivariant, i.e. if we multiply each input by c then the
   corresponding coefficients are scaled by a factor of 1/c. therefore,
   regardless of how the predictor is scaled, the multiplication of
   predictor and coefficient(xj  j) remains the same. however, this is not
   the case with ridge regression, and therefore, we need to standardize
   the predictors or bring the predictors to the same scale before
   performing ridge regression. the formula used to do this is given
   below.
   [1*6kradbf-capfpr7gaszasa.png]

lasso

   [1*thj4sspyv0bdr8xxediwxa.png]

   lasso is another variation, in which the above function is minimized.
   its clear that this variation differs from ridge regression only in
   penalizing the high coefficients. it uses |  j|(modulus)instead of
   squares of   , as its penalty. in statistics, this is known as the l1
   norm.

   lets take a look at above methods with a different perspective. the
   ridge regression can be thought of as solving an equation, where
   summation of squares of coefficients is less than or equal to s. and
   the lasso can be thought of as an equation where summation of modulus
   of coefficients is less than or equal to s. here, s is a constant that
   exists for each value of shrinkage factor   . these equations are also
   referred to as constraint functions.

   consider their are 2 parameters in a given problem. then according to
   above formulation, the ridge regression is expressed by   1   +   2       s.
   this implies that ridge regression coefficients have the smallest
   rss(id168) for all points that lie within the circle given by
     1   +   2       s.

   similarly, for lasso, the equation becomes,|  1|+|  2|    s. this implies
   that lasso coefficients have the smallest rss(id168) for all
   points that lie within the diamond given by |  1|+|  2|    s.

   the image below describes these equations.
   [1*xc-8thomxro3oghkylrfra.png]
   credit : an introduction to statistical learning by gareth james,
   daniela witten, trevor hastie, robert tibshirani

   the above image shows the constraint functions(green areas), for
   lasso(left) and ridge regression(right), along with contours for
   rss(red ellipse). points on the ellipse share the value of rss. for a
   very large value of s, the green regions will contain the center of the
   ellipse, making coefficient estimates of both regression techniques,
   equal to the least squares estimates. but, this is not the case in the
   above image. in this case, the lasso and ridge regression coefficient
   estimates are given by the    rst point at which an ellipse contacts the
   constraint region. since ridge regression has a circular constraint
   with no sharp points, this intersection will not generally occur on an
   axis, and so the ridge regression coe   cient estimates will be
   exclusively non-zero. however, the lasso constraint has corners at each
   of the axes, and so the ellipse will often intersect the constraint
   region at an axis. when this occurs, one of the coe   cients will equal
   zero. in higher dimensions(where parameters are much more than 2), many
   of the coe   cient estimates may equal zero simultaneously.

   this sheds light on the obvious disadvantage of ridge regression, which
   is model interpretability. it will shrink the coefficients for least
   important predictors, very close to zero. but it will never make them
   exactly zero. in other words, the final model will include all
   predictors. however, in the case of the lasso, the l1 penalty has the
   e   ect of forcing some of the coe   cient estimates to be exactly equal to
   zero when the tuning parameter    is su   ciently large. therefore, the
   lasso method also performs variable selection and is said to yield
   sparse models.

what does id173 achieve?

   a standard least squares model tends to have some variance in it, i.e.
   this model won   t generalize well for a data set different than its
   training data. id173, significantly reduces the variance of
   the model, without substantial increase in its bias. so the tuning
   parameter   , used in the id173 techniques described above,
   controls the impact on bias and variance. as the value of    rises, it
   reduces the value of coefficients and thus reducing the variance. till
   a point, this increase in    is beneficial as it is only reducing the
   variance(hence avoiding overfitting), without loosing any important
   properties in the data. but after certain value, the model starts
   loosing important properties, giving rise to bias in the model and thus
   underfitting. therefore, the value of    should be carefully selected.

   this is all the basic you will need, to get started with
   id173. it is a useful technique that can help in improving the
   accuracy of your regression models. a popular library for implementing
   these algorithms is [20]scikit-learn. it has a wonderful api that can
   get your model up an running with just a few lines of code in python.
     __________________________________________________________________

   if you liked this article, be sure to show your support by clapping for
   this article below and if you have any questions, leave a comment and i
   will do my best to answer.

   for being more aware of the world of machine learning, follow me. it   s
   the best way to find out when i write more articles like this.

   you can also follow me on [21]twitter, [22]email me directly or
   [23]find me on linkedin. i   d love to hear from you.

   that   s all folks, have a nice day :)
     __________________________________________________________________

credit

   content for this article is inspired and taken from, an introduction to
   statistical learning by gareth james, daniela witten, trevor hastie,
   robert tibshirani

     * [24]machine learning
     * [25]data science
     * [26]deep learning
     * [27]artificial intelligence
     * [28]towards data science

   (button)
   (button)
   (button) 3.7k claps
   (button) (button) (button) 20 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of prashant gupta

[30]prashant gupta

   machine learning engineer, android developer, tech enthusiast

     (button) follow
   [31]towards data science

[32]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3.7k
     * (button)
     *
     *

   [33]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [34]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/76441ddcf99a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/id173-in-machine-learning-76441ddcf99a&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/id173-in-machine-learning-76441ddcf99a&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_f8b9pro38bai---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@prashantgupta17?source=post_header_lockup
  17. https://towardsdatascience.com/@prashantgupta17
  18. https://medium.com/towards-data-science/balancing-bias-and-variance-to-control-errors-in-machine-learning-16ced95724db
  19. https://medium.com/towards-data-science/cross-validation-in-machine-learning-72924a69872f
  20. https://becominghuman.ai/implementing-decision-trees-using-scikit-learn-5057b27221ec
  21. https://twitter.com/prashant_1722
  22. mailto:pr.span24@gmail.com
  23. https://www.linkedin.com/in/prashantgupta17/
  24. https://towardsdatascience.com/tagged/machine-learning?source=post
  25. https://towardsdatascience.com/tagged/data-science?source=post
  26. https://towardsdatascience.com/tagged/deep-learning?source=post
  27. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  28. https://towardsdatascience.com/tagged/towards-data-science?source=post
  29. https://towardsdatascience.com/@prashantgupta17?source=footer_card
  30. https://towardsdatascience.com/@prashantgupta17
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/?source=footer_card
  33. https://towardsdatascience.com/
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://medium.com/towards-data-science/balancing-bias-and-variance-to-control-errors-in-machine-learning-16ced95724db
  37. https://medium.com/towards-data-science/cross-validation-in-machine-learning-72924a69872f
  38. https://medium.com/p/76441ddcf99a/share/twitter
  39. https://medium.com/p/76441ddcf99a/share/facebook
  40. https://medium.com/p/76441ddcf99a/share/twitter
  41. https://medium.com/p/76441ddcf99a/share/facebook
