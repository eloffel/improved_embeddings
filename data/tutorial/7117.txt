   [1]

linkedin

     * [2]sign in
     * [3]join now

   tensorflow: what parameters to optimize?

                    tensorflow: what parameters to optimize?

   published on october 16, 2017october 16, 2017     8 likes     0 comments

   [4]ahmed gad

[5]ahmed gad[6]follow

job seeker. fritz/kdnuggets/tds contributor, t.a. & deep learning | machine
learning | id161 researcher

     * (button) like8
     * (button) comment0
     * [ ] share
          + (button) linkedin
          + (button) facebook
          + (button) twitter
       3

   this article targets whom have a basic understanding for tensorflow
   core api.

   learning tensorflow core api, which is the lowest level api in
   tensorflow, is a very good step for starting learning tensorflow
   because it let you understand the kernel of the library. here is a very
   simple example of tensorflow core api in which we create and train a
   id75 model.

   the steps are as follows:
    1. read the trainable parameters of the model (just a weight and a
       bias in this example).
    2. read the training data into placeholders.
    3. create the id75 model function.
    4. create a id168 to assess the prediction errors of the
       model.
    5. create a tensorflow session.
    6. initialize the trainable parameters.
    7. run the session to train the regression model.

1.  import tensorflow
2.
3.  #trainable parameters
4.  w = tensorflow.variable([.6], dtype=tensorflow.float32)
5.  b = tensorflow.variable([.2], dtype=tensorflow.float32)
6.
7.  #training data (inputs/outputs)
8.  x = tensorflow.placeholder(dtype=tensorflow.float32)
9.  y = tensorflow.placeholder(dtype=tensorflow.float32)
10.
11. #linear model
12. linear_model = w * x + b
13.
14. #id75 id168 - sum of the squares
15. squared_deltas = tensorflow.square(linear_model - y)
16. loss = tensorflow.reduce_sum(squared_deltas)
17.
18. #creating a session
19. sess = tensorflow.session()
20.
21. #initializing variables
22. init = tensorflow.global_variables_initializer()
23. sess.run(init)
24.
25. #print the loss
26. print(sess.run(loss, feed_dict={ x: [1, 2, 3, 4], y: [0, 1, 2, 3]}))
27.
28. sess.close()


   the loss returned is 53.76. existence of error, specially for large
   error, means that the parameters used must be updated. these parameters
   are expected to be updated automatically but we can start updating it
   manually until reaching zero error.
     * for w=0.8 and b=0.4, the loss is 3.44
     * for w=1.0 and b=0.8, the loss is 12.96
     * for w=1.0 and b=-0.5, the loss is 1.0
     * for w=1.0 and b=-1.0, the loss is 0.0

   thus when w=1.0 and b=-1.0 the desired results is identical to the
   predicted results and thus we can`t enhance the model more than this.
   we reached the optimal values for the parameters but not using the
   optimal way. the optimal way of calculating the parameters is
   automatic.

   there are a number of optimizers already exist in tensorflow for making
   things simpler. these optimizers exist in apis in tensorflow such as:
     * tensorflow.train
     * tensorflow.estimator

   here is how we can use tensorflow.train for updating the parameters
   automatically.

tensorflow.train api

   there are a number of optimizers that tensorflow provides that makes
   the previous manual work of calculating the best values for the model
   parameters automatically.

   the simplest optimizer is the id119 that changes the values
   of each parameter slowly until reaching the value that minimizes the
   loss. id119 modifies each variable according to the
   magnitude of the derivative of loss with respect to the variable.

   because doing such operations of calculating the derivatives is complex
   and error prone, tensorflow can calculate the gradients automatically.
   after calculating the gradients, you need to optimize the parameters
   yourself.

   but tensorflow makes things easier and easier by providing optimizers
   that will calculate the derivatives in addition to optimizing the
   parameters.

   tensorflow.train api contains a class called gradientdescentoptimizer
   that can both calculate the derivatives and optimizing the parameters.
   for example, the following code shows how to minimize the loss using
   the gradientdescentoptimizer:
1.  import tensorflow
2.
3.  #trainable parameters
4.  w = tensorflow.variable([0.3], dtype=tensorflow.float32)
5.  b = tensorflow.variable([-0.2], dtype=tensorflow.float32)
6.
7.  #training data (inputs/outputs)
8.  x = tensorflow.placeholder(dtype=tensorflow.float32)
9.  y = tensorflow.placeholder(dtype=tensorflow.float32)
10.
11. x_train = [1, 2, 3, 4]
12. y_train = [0, 1, 2, 3]
13.
14. #linear model
15. linear_model = w * x + b
16.
17. #id75 id168 - sum of the squares
18. squared_deltas = tensorflow.square(linear_model - y_train)
19. loss = tensorflow.reduce_sum(squared_deltas)
20.
21. #id119 optimizer
22. optimizer = tensorflow.train.gradientdescentoptimizer(learning_rate=0.01)
23. train = optimizer.minimize(loss=loss)
24.
25. #creating a session
26. sess = tensorflow.session()
27.
28. writer = tensorflow.summary.filewriter("/tmp/log/", sess.graph)
29.
30. #initializing variables
31. init = tensorflow.global_variables_initializer()
32. sess.run(init)
33.
34. #optimizing the parameters
35. for i in range(1000):
36.     sess.run(train, feed_dict={x: x_train, y: y_train})
37.
38. #print the parameters and loss
39. curr_w, curr_b, curr_loss = sess.run([w, b, loss], {x: x_train, y: y_train})

40. print("w : ", curr_w, ", b : ", curr_b, ", loss : ", curr_loss)
41.
42. writer.close()
43.
44. sess.close()

    here is the result returned by the optimizer. it seems that it deduced
   the right values of the parameters automatically in order to get the
   least loss.

   w :  [ 0.99999809] , b :  [-0.9999944] , loss :  2.05063e-11

   there is some advantages of using such built-in optimizers rather than
   building it manually. this is because the code for creating such simple
   id75 model using tensorflow core api is not complex. but
   it won`t be like that when working with much more complex models. thus
   it is preferred to use the frequently used tasks from high-level apis
   in tensorflow.

   here is the dataflow graph of the previous program when visualized
   using tensorboard (tb).

   [:0]

   but there is a very important question. how the optimizer deduced the
   parameters that it should change? how it deduced that we are to
   optimize the weight (w) and bias (b)? we have not explicitly told the
   optimizer that these are the parameters that it will change in order to
   reduce the loss but it deduced it itself. so, how?

   in line 35, we run the session and asked to evaluate the train tensor.
   tensorflow will follow the chain of graph nodes to evaluate that
   tensor. in line 23, tensorflow found that to evaluate the train tensor
   it should evaluate the optimizer.minimize operation. this operation
   will try to minimize its input arguments as much as possible.

   following back, to evaluate the minimize operation it will accept one
   tensor which is the loss. so, the goal now is to minimize the loss
   tensor. but how to minimize the loss? it will still follow the graph
   back and it will find it is evaluated using the tensorflow.reduce_sum()
   operation. so, our goal now is to minimize the result of the
   tensorflow.reduce_sum() operation.

   following back, this operation is evaluated using one tensor as input
   which is squared_deltas. so, rather than having our goal to minimize
   the tensorflow.reduce_sum() operation, our goal now is to minimize the
   squared_deltas tensor.

   following the chain back, we find that the squared_deltas tensor
   depends on the tensorflow.square() operation. so, we should minimize
   tensorflow.square() the operation. minimizing that operation will ask
   us to minimize its input tensors which are linear_model and y_train.
   looking for thse two tensors, which one can be modified? the tensors of
   type variables can be modified. because y_train is not a variable but
   placeholder, then we can`t modify it and thus we can modify the
   linear_model to minimize the result.

   in line 15, the linear_model tensor is calculated based on three inputs
   which are w, x, and b. looking for these tensors, only w and b can be
   changed because they are variables. so, our goal is to minimize these
   two tensors w and b.

   this is how tensorflow deduced that to minimize the loss it should
   minimize the weight and bias parameters.

   [7]ahmed gad

[8]ahmed gad

job seeker. fritz/kdnuggets/tds contributor, t.a. & deep learning | machine
learning | id161 researcher

   [9]follow

   0 comments
   article-comment__guest-image
   [10]sign in to leave your comment
     __________________________________________________________________

more from ahmed gad

   [11]27 articles
   from y=x to building a complete id158

[12]from y=x to building a complete artificial   

   march 29, 2019

   feature reduction using genetic algorithm with python

[13]feature reduction using genetic algorithm   

   january 29, 2019

   id158s optimization using genetic algorithm with
   python

[14]id158s optimization   

   january 24, 2019

     *    2019
     * [15]about
     * [16]user agreement
     * [17]privacy policy
     * [18]cookie policy
     * [19]copyright policy
     * [20]brand policy
     * [21]manage subscription
     * [22]community guidelines
     * [ ]
          + (button) bahasa indonesia
          + (button) bahasa malaysia
          + (button)   e  tina
          + (button) dansk
          + (button) deutsch
          + (button) english
          + (button) espa  ol
          + (button)             
          + (button) fran  ais
          + (button)          
          + (button) italiano
          + (button)             
          + (button) nederlands
          + (button)          
          + (button) norsk
          + (button) polski
          + (button) portugu  s
          + (button) rom  n  
          + (button)               
          + (button) svenska
          + (button) tagalog
          + (button)                      
          + (button) t  rk  e
          + (button)               
       languagelanguage

references

   1. https://www.linkedin.com/?trk=header_logo
   2. https://www.linkedin.com/uas/login?trk=header_signin
   3. https://www.linkedin.com/start/join?trk=header_join
   4. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_image
   5. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_title
   6. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/tensorflow-what-hyperparameters-optimize-ahmed-gad&trk=author-info__follow-button
   7. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_image
   8. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_title
   9. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/tensorflow-what-hyperparameters-optimize-ahmed-gad&trk=author-info__follow-button-bottom
  10. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/tensorflow-what-hyperparameters-optimize-ahmed-gad&trk=article-reader_leave-comment
  11. https://www.linkedin.com/today/author/ahmedfgad
  12. https://www.linkedin.com/pulse/from-yx-building-complete-artificial-neural-network-ahmed-gad?trk=related_artice_from y=x to building a complete id158_article-card_title
  13. https://www.linkedin.com/pulse/feature-reduction-using-genetic-algorithm-ahmed-gad?trk=related_artice_feature reduction using genetic algorithm with python_article-card_title
  14. https://www.linkedin.com/pulse/artificial-neural-networks-optimization-using-genetic-ahmed-gad?trk=related_artice_id158s optimization using genetic algorithm with python_article-card_title
  15. https://press.linkedin.com/about-linkedin?trk=article_reader_footer_footer-about
  16. https://www.linkedin.com/legal/user-agreement?trk=article_reader_footer_footer-user-agreement
  17. https://www.linkedin.com/legal/privacy-policy?trk=article_reader_footer_footer-privacy-policy
  18. https://www.linkedin.com/legal/cookie-policy?trk=article_reader_footer_footer-cookie-policy
  19. https://www.linkedin.com/legal/copyright-policy?trk=article_reader_footer_footer-copyright-policy
  20. https://brand.linkedin.com/policies?trk=article_reader_footer_footer-brand-policy
  21. https://www.linkedin.com/psettings/guest-controls?trk=article_reader_footer_footer-manage-sub
  22. https://www.linkedin.com/help/linkedin/answer/34593?lang=en&trk=article_reader_footer_footer-community-guide
