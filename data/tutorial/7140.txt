   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]cracking the data science interview
     * [9]algorithms
     * [10]machine learning
     * [11]deep learning
     * [12]statistics
     * [13]database
     * [14]advice
     * [15]industry
     __________________________________________________________________

   [1*cpi-6ztpyfmyv3btt8eumq.jpeg]

the 10 deep learning methods ai practitioners need to apply

   go to the profile of james le
   [16]james le (button) blockedunblock (button) followfollowing
   nov 17, 2017

   interest in machine learning has exploded over the past decade. you see
   machine learning in computer science programs, industry conferences,
   and the wall street journal almost daily. for all the talk about
   machine learning, many conflate what it can do with what they wish it
   could do. fundamentally, machine learning is using algorithms to
   extract information from raw data and represent it in some type of
   model. we use this model to infer things about other data we have not
   yet modeled.

   neural networks are one type of model for machine learning; they have
   been around for at least 50 years. the fundamental unit of a neural
   network is a node, which is loosely based on the biological neuron in
   the mammalian brain. the connections between neurons are also modeled
   on biological brains, as is the way these connections develop over time
   (with    training   ).

   in the mid-1980s and early 1990s, many important architectural
   advancements were made in neural networks. however, the amount of time
   and data needed to get good results slowed adoption, and thus interest
   cooled. in the early 2000s, computational power expanded exponentially
   and the industry saw a    cambrian explosion    of computational techniques
   that were not possible prior to this. deep learning emerged from that
   decade   s explosive computational growth as a serious contender in the
   field, winning many important machine learning competitions. the
   interest has not cooled as of 2017; today, we see deep learning
   mentioned in every corner of machine learning.

   to get myself into the craze, i took [17]udacity   s    deep learning   
   course, which is a great introduction to the motivation of deep
   learning and the design of intelligent systems that learn from complex
   and/or large-scale datasets in tensorflow. for the class projects, i
   used and developed neural networks for image recognition with
   convolutions, natural language processing with embeddings and character
   based text generation with recurrent neural network / long short-term
   memory. all the code in jupiter notebook can be found on [18]this
   github repository.

   here is an outcome of one of the assignments, a id167 projection of
   word vectors, clustered by similarity.
   [1*5guqfloyqrb4kgjaicn0og.png]

   most recently, i have started reading academic papers on the subject.
   from my research, here are several publications that have been hugely
   influential to the development of the field:
     * nyu   s [19]gradient-based learning applied to document recognition
       (1998), which introduces convolutional neural network to the
       machine learning world.
     * toronto   s [20]deep id82s (2009), which presents a new
       learning algorithm for id82s that contain many layers
       of hidden variables.
     * stanford & google   s [21]building high-level features using
       large-scale unsupervised learning (2012), which addresses the
       problem of building high-level, class-specific feature detectors
       from only unlabeled data.
     * berkeley   s [22]decaf         a deep convolutional activation feature for
       generic visual recognition (2013), which releases decaf, an
       open-source implementation of the deep convolutional activation
       features, along with all associated network parameters to enable
       vision researchers to be able to conduct experimentation with deep
       representations across a range of visual concept learning
       paradigms.
     * deepmind   s [23]playing atari with deep id23
       (2016), which presents the 1st deep learning model to successfully
       learn control policies directly from high-dimensional sensory input
       using id23.

   there is an abundant amount of great knowledge about deep learning i
   have learnt via research and learning. here i want to share the 10
   powerful deep learning methods ai engineers can apply to their machine
   learning problems. but first of all, let   s define what deep learning
   is. deep learning has been a challenge to define for many because it
   has changed forms slowly over the past decade. to set deep learning in
   context visually, the figure below illustrates the conception of the
   relationship between ai, machine learning, and deep learning.
   [1*k5p2e-b_rhh2x4u9qmsrwg.jpeg]

   the field of ai is broad and has been around for a long time. deep
   learning is a subset of the field of machine learning, which is a
   subfield of ai. the facets that differentiate deep learning networks in
   general from    canonical    feed-forward multilayer networks are as
   follows:
     * more neurons than previous networks
     * more complex ways of connecting layers
     *    cambrian explosion    of computing power to train
     * automatic feature extraction

   when i say    more neurons   , i mean that the neuron count has risen over
   the years to express more complex models. layers also have evolved from
   each layer being fully connected in multilayer networks to locally
   connected patches of neurons between layers in convolutional neural
   networks and recurrent connections to the same neuron in recurrent
   neural networks (in addition to the connections from the previous
   layer).

   deep learning then can be defined as neural networks with a large
   number of parameters and layers in one of four fundamental network
   architectures:
     * unsupervised pre-trained networks
     * convolutional neural networks
     * recurrent neural networks
     * id56s

   in this post, i am mainly interested in the latter 3 architectures. a
   convolutional neural network is basically a standard neural network
   that has been extended across space using shared weights. id98 is
   designed to recognize images by having convolutions inside, which see
   the edges of an object recognized on the image. a recurrent neural
   network is basically a standard neural network that has been extended
   across time by having edges which feed into the next time step instead
   of into the next layer in the same time step. id56 is designed to
   recognize sequences, for example, a speech signal or a text. it has
   cycles inside that implies the presence of short memory in the net. a
   id56 is more like a hierarchical network where
   there is really no time aspect to the input sequence but the input has
   to be processed hierarchically in a tree fashion. the 10 methods below
   can be applied to all of these architectures.

1         back-propagation

   back-prop is simply a method to compute the partial derivatives (or
   gradient) of a function, which has the form as a function composition
   (as in neural nets). when you solve an optimization problem using a
   gradient-based method (id119 is just one of them), you want
   to compute the function gradient at each iteration.
   [1*gramdohlaf-afgybfsa0jq.png]

   for a neural nets, the objective function has the form of a
   composition. how do you compute the gradient? there are 2 common ways
   to do it: (i) analytic differentiation. you know the form of the
   function. you just compute the derivatives using the chain rule (basic
   calculus). (ii) approximate differentiation using finite difference.
   this method is computationally expensive because the number of function
   evaluation is o(n), where n is the number of parameters. this is
   expensive, compared to analytic differentiation. finite difference,
   however, is commonly used to validate a back-prop implementation when
   debugging.

2         stochastic id119

   an intuitive way to think of id119 is to imagine the path of
   a river originating from top of a mountain. the goal of gradient
   descent is exactly what the river strives to achieve         namely, reach
   the bottom most point (at the foothill) climbing down from the
   mountain.

   now, if the terrain of the mountain is shaped in such a way that the
   river doesn   t have to stop anywhere completely before arriving at its
   final destination (which is the lowest point at the foothill, then this
   is the ideal case we desire. in machine learning, this amounts to
   saying, we have found the global mimimum (or optimum) of the solution
   starting from the initial point (top of the hill). however, it could be
   that the nature of terrain forces several pits in the path of the
   river, which could force the river to get trapped and stagnate. in
   machine learning terms, such pits are termed as local minima solutions,
   which is not desirable. there are a bunch of ways to get out of this
   (which i am not discussing).
   [1*fbyskqwqf8uqau-1she4jg.png]

   id119 therefore is prone to be stuck in local minimum,
   depending on the nature of the terrain (or function in ml terms). but,
   when you have a special kind of mountain terrain (which is shaped like
   a bowl, in ml terms this is called a convex function), the algorithm is
   always guaranteed to find the optimum. you can visualize this picturing
   a river again. these kind of special terrains (a.k.a convex functions)
   are always a blessing for optimization in ml. also, depending on where
   at the top of the mountain you initial start from (ie. initial values
   of the function), you might end up following a different path.
   similarly, depending on the speed at the river climbs down (ie. the
   learning rate or step size for the id119 algorithm), you
   might arrive at the final destination in a different manner. both of
   these criteria can affect whether you fall into a pit (local minima) or
   are able to avoid it.

3         learning rate decay

   [1*jzivdhozzrhv9m8cestisa.jpeg]

   adapting the learning rate for your stochastic id119
   optimization procedure can increase performance and reduce training
   time. sometimes this is called learning rate annealing or adaptive
   learning rates. the simplest and perhaps most used adaptation of
   learning rate during training are techniques that reduce the learning
   rate over time. these have the benefit of making large changes at the
   beginning of the training procedure when larger learning rate values
   are used, and decreasing the learning rate such that a smaller rate and
   therefore smaller training updates are made to weights later in the
   training procedure. this has the effect of quickly learning good
   weights early and fine tuning them later.

   two popular and easy to use learning rate decay are as follows:
     * decrease the learning rate gradually based on the epoch.
     * decrease the learning rate using punctuated large drops at specific
       epochs.

4         dropout

   deep neural nets with a large number of parameters are very powerful
   machine learning systems. however, overfitting is a serious problem in
   such networks. large networks are also slow to use, making it difficult
   to deal with overfitting by combining the predictions of many different
   large neural nets at test time. dropout is a technique for addressing
   this problem.
   [1*pghkz1k2lepg01egfbtkoq.jpeg]

   the key idea is to randomly drop units (along with their connections)
   from the neural network during training. this prevents units from
   co-adapting too much. during training, dropout samples from an
   exponential number of different    thinned    networks. at test time, it is
   easy to approximate the effect of averaging the predictions of all
   these thinned networks by simply using a single untwined network that
   has smaller weights. this significantly reduces overfitting and gives
   major improvements over other id173 methods. dropout has been
   shown to improve the performance of neural networks on supervised
   learning tasks in vision, id103, document classification
   and computational biology, obtaining state-of-the-art results on many
   benchmark datasets.

5         max pooling

   max pooling is a sample-based discretization process. the object is to
   down-sample an input representation (image, hidden-layer output matrix,
   etc.), reducing its dimensionality and allowing for assumptions to be
   made about features contained in the sub-regions binned.
   [1*mab72pbcgfsg707fgdoxga.jpeg]

   this is done in part to help over-fitting by providing an abstract form
   of the representation. as well, it reduces the computational cost by
   reducing the number of parameters to learn and provides basic
   translation invariance to the internal representation. max pooling is
   done by applying a max filter to usually non-overlapping subregions of
   the initial representation.

6         batch id172

   naturally, neural networks including deep networks require careful
   tuning of weight initialization and learning parameters. batch
   id172 helps relaxing them a little.

   weights problem:
     * whatever the initialization of weights, be it random or empirically
       chosen, they are far away from the learned weights. consider a mini
       batch, during initial epochs, there will be many outliers in terms
       of required feature activations.
     * the deep neural network by itself is ill-posed, i.e. a small
       perturbation in the initial layers, leads to a large change in the
       later layers.

   during back-propagation, these phenomena causes distraction to
   gradients, meaning the gradients have to compensate the outliers,
   before learning the weights to produce required outputs. this leads to
   the requirement of extra epochs to converge.
   [1*xkr2e_wmf_hlx6mz4domlg.png]

   batch id172 regularizes these gradient from distraction to
   outliers and flow towards the common goal (by normalizing them) within
   a range of the mini batch.

   learning rate problem: generally, learning rates are kept small, such
   that only a small portion of gradients corrects the weights, the reason
   is that the gradients for outlier activations should not affect learned
   activations. by batch id172, these outlier activations are
   reduced and hence higher learning rates can be used to accelerate the
   learning process.

7         long short-term memory:

   a lstm network has the following three aspects that differentiate it
   from an usual neuron in a recurrent neural network:
    1. it has control on deciding when to let the input enter the neuron.
    2. it has control on deciding when to remember what was computed in
       the previous time step.
    3. it has control on deciding when to let the output pass on to the
       next time stamp.

   the beauty of the lstm is that it decides all this based on the current
   input itself. so if you take a look at the following diagram:
   [1*u1wu_6kvzln8wls6hncdsw.png]

   the input signal x(t) at the current time stamp decides all the above 3
   points. the input gate takes a decision for point 1. the forget gate
   takes a decision on point 2 and the output gate takes a decision on
   point 3. the input alone is capable of taking all these three
   decisions. this is inspired by how our brains work and can handle
   sudden context switches based on the input.

8         skip-gram:

   the goal of id27 models is to learn a high-dimensional dense
   representation for each vocabulary term in which the similarity between
   embedding vectors shows the semantic or syntactic similarity between
   the corresponding words. skip-gram is a model for learning word
   embedding algorithms.

   the main idea behind the skip-gram model (and many other id27
   models) is as follows: two vocabulary terms are similar, if they share
   similar context.
   [1*0zcp-z033it0uykvwbplbq.jpeg]

   in other words, assume that you have a sentence, like    cats are
   mammals   . if you use the term    dogs    instead of    cats   , the sentence is
   still a meaningful sentence. so in this example,    dogs    and    cats    can
   share the same context (i.e.,    are mammals   ).

   based on the above hypothesis, you can consider a context window (a
   window containing k consecutive terms. then you should skip one of
   these words and try to learn a neural network that gets all terms
   except the one skipped and predicts the skipped term. therefore, if two
   words repeatedly share similar contexts in a large corpus, the
   embedding vectors of those terms will have close vectors.

9         continuous bag of words:

   in natural language processing problems, we want to learn to represent
   each word in a document as a vector of numbers such that words that
   appear in similar context have vectors that are close to each other. in
   continuous id159, the goal is to be able to use the
   context surrounding a particular word and predict the particular word.
   [1*-itr-hwiywz46gldcs4zyq.jpeg]

   we do this by taking lots and lots of sentences in a large corpus and
   every time we see a word, we take the surrounding word. then we input
   the context words to a neural network and predict the word in the
   center of this context.

   when we have thousands of such context words and the center word, we
   have one instance of a dataset for the neural network. we train the
   neural network and finally the encoded hidden layer output represents
   the embedding for a particular word. it so happens that when we train
   this over a large number of sentences, words in similar context get
   similar vectors.

10         id21:

   let   s think about how an image would run through a convolutional neural
   networks. say you have an image, you apply convolution to it, and you
   get combinations of pixels as outputs. let   s say they   re edges. now
   apply convolution again, so now your output is combinations of edges   
   or lines. now apply convolution again, so your output is combinations
   of lines and so on. you can think of it as each layer looking for a
   specific pattern. the last layer of your neural network tends to get
   very specialized. perhaps if you were working on id163, your
   networks last layer would be looking for children or dogs or airplanes
   or whatever. a couple layers back you might see the network looking for
   eyes or ears or mouth or wheels.
   [1*ovnmgv2yulrn6mnpzoskug.jpeg]

   each layer in a deep id98 progressively builds up higher and higher
   level representations of features. the last couple layers tend to be
   specialized on whatever data you fed into the model. on the other hand,
   the early layers are much more generic, there are many simple patterns
   common among a much larger class of pictures.

   id21 is when you take a id98 trained on one dataset, chop
   off the last layer(s), retrain the models last layer(s) on a different
   dataset. intuitively, you   re retraining the model to recognized
   different higher level features. as a result, training time gets cut
   down a lot so id21 is a helpful tool when you don   t have
   enough data or if training takes too much resources.

   this article only shows the general overview of these methods. i
   suggest reading the articles below for more detailed explanations:
     * andrew beam   s [24]   deep learning 101   
     * andrey kurenkov   s [25]   a brief history of neural nets and deep
       learning   
     * adit deshpande   s [26]   a beginner   s guide to understanding
       convolutional neural networks   
     * chris olah   s [27]   understanding id137   
     * algobean   s [28]   id158s   
     * andrej karpathy   s [29]   the unreasonable effectiveness of recurrent
       neural networks   

   deep learning is strongly technique-focused. there are not much
   concrete explanations for each of the new ideas. most new ideas came
   out with experimental results attached to prove that they work. deep
   learning is like playing lego. mastering lego is as challenging as any
   other arts, but getting into it is easier.

               
   if you enjoyed this piece, i   d love it if you hit the clap button      so
   others might stumble upon it. you can find my own code on [30]github,
   and more of my writing and projects at [31]https://jameskle.com/. you
   can also follow me on [32]twitter, [33]email me directly or [34]find me
   on linkedin. [35]sign up for my newsletter to receive my latest
   thoughts on data science, machine learning, and artificial intelligence
   right at your inbox!

     * [36]deep learning
     * [37]artificial intelligence
     * [38]neural networks
     * [39]data science
     * [40]brain

   (button)
   (button)
   (button) 4.4k claps
   (button) (button) (button) 9 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of james le

[41]james le

   medium member since apr 2019

   blue ocean thinker ([42]https://jameskle.com/)

     (button) follow
   [43]cracking the data science interview

[44]cracking the data science interview

   your ultimate guide to data science interviews

     * (button)
       (button) 4.4k
     * (button)
     *
     *

   [45]cracking the data science interview
   never miss a story from cracking the data science interview, when you
   sign up for medium. [46]learn more
   never miss a story from cracking the data science interview
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/885259f402c1
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/cracking-the-data-science-interview/the-10-deep-learning-methods-ai-practitioners-need-to-apply-885259f402c1&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/cracking-the-data-science-interview/the-10-deep-learning-methods-ai-practitioners-need-to-apply-885259f402c1&source=--------------------------nav_reg&operation=register
   8. https://medium.com/cracking-the-data-science-interview?source=logo-lo_bv6sap5dexgq---a095e0538d84
   9. https://medium.com/cracking-the-data-science-interview/tagged/algorithms
  10. https://medium.com/cracking-the-data-science-interview/tagged/machine-learning
  11. https://medium.com/cracking-the-data-science-interview/tagged/deep-learning
  12. https://medium.com/cracking-the-data-science-interview/tagged/statistics
  13. https://medium.com/cracking-the-data-science-interview/tagged/database
  14. https://medium.com/cracking-the-data-science-interview/tagged/advice
  15. https://medium.com/cracking-the-data-science-interview/tagged/industry
  16. https://medium.com/@james_aka_yale
  17. https://www.udacity.com/course/deep-learning--ud730
  18. https://github.com/khanhnaid1131994/deep-learning
  19. http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
  20. http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf
  21. http://icml.cc/2012/papers/73.pdf
  22. http://proceedings.mlr.press/v32/donahue14.pdf
  23. https://www.cs.toronto.edu/~vmnih/docs/id25.pdf
  24. http://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html
  25. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/
  26. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks/
  27. http://colah.github.io/posts/2015-08-understanding-lstms/
  28. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/
  29. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  30. https://github.com/khanhnaid1131994
  31. https://jameskle.com/
  32. https://twitter.com/@james_aka_yale
  33. mailto:khanhle.1013@gmail.com
  34. http://www.linkedin.com/in/khanhnaid11394
  35. http://eepurl.com/dewjzb
  36. https://medium.com/tag/deep-learning?source=post
  37. https://medium.com/tag/artificial-intelligence?source=post
  38. https://medium.com/tag/neural-networks?source=post
  39. https://medium.com/tag/data-science?source=post
  40. https://medium.com/tag/brain?source=post
  41. https://medium.com/@james_aka_yale
  42. https://jameskle.com/
  43. https://medium.com/cracking-the-data-science-interview?source=footer_card
  44. https://medium.com/cracking-the-data-science-interview?source=footer_card
  45. https://medium.com/cracking-the-data-science-interview
  46. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  48. https://medium.com/@james_aka_yale?source=post_header_lockup
  49. https://medium.com/p/885259f402c1/share/twitter
  50. https://medium.com/p/885259f402c1/share/facebook
  51. https://medium.com/@james_aka_yale?source=footer_card
  52. https://medium.com/p/885259f402c1/share/twitter
  53. https://medium.com/p/885259f402c1/share/facebook
