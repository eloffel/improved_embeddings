   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*i2rfaffmufu48vepw9j0ww.jpeg]

naive bayes in machine learning

   [16]go to the profile of prashant gupta
   [17]prashant gupta (button) blockedunblock (button) followfollowing
   nov 6, 2017

   bayes    theorem finds many uses in the id203 theory and
   statistics. there   s a micro chance that you have never heard about this
   theorem in your life. turns out that this theorem has found its way
   into the world of machine learning, to form one of the highly decorated
   algorithms. in this article, we will learn all about the naive bayes
   algorithm, along with its variations for different purposes in machine
   learning.

   as you might have guessed, this requires us to view things from a
   probabilistic point of view. just as in machine learning, we have
   attributes, response variables and predictions or classifications.
   using this algorithm, we will be dealing with the id203
   distributions of the variables in the dataset and predicting the
   id203 of the response variable belonging to a particular value,
   given the the attributes of a new instance. lets start by reviewing the
   bayes    theorem.

bayes    theorem

   this lets us examine the id203 of an event based on the prior
   knowledge of any event that related to the former event. so for
   example, the id203 that price of a house is high, can be better
   assessed if we know the facilities around it, compared to the
   assessment made without the knowledge of location of the house. bayes   
   theorem does exactly that.
   [1*c0pjiclo_opqkoduknsihq.png]
   image taken from wikipedia

   above equation gives the basic representation of the bayes    theorem.
   here a and b are two events and,

   p(a|b) : the id155 that event a occurs , given that b
   has occurred. this is also known as the posterior id203.

   p(a) and p(b) : id203 of a and b without regard of each other.

   p(b|a) : the id155 that event b occurs , given that a
   has occurred.

   now, let   s see how this suits well to the purpose of machine learning.

   take a simple machine learning problem, where we need to learn our
   model from a given set of attributes(in training examples) and then
   form a hypothesis or a relation to a response variable. then we use
   this relation to predict a response, given attributes of a new
   instance. using the bayes    theorem, its possible to build a learner
   that predicts the id203 of the response variable belonging to
   some class, given a new set of attributes.

   consider the previous equation again. now, assume that a is the
   response variable and b is the input attribute. so according to the
   equation, we have

   p(a|b) : id155 of response variable belonging to a
   particular value, given the input attributes. this is also known as the
   posterior id203.

   p(a) : the prior id203 of the response variable.

   p(b) : the id203 of training data or the evidence.

   p(b|a) : this is known as the likelihood of the training data.

   therefore, the above equation can be rewritten as
   [1*n4d8bklvcurng1xkshtsxg.png]
   image taken from wikipedia

   let   s take a problem, where the number of attributes is equal to n and
   the response is a boolean value, i.e. it can be in one of the two
   classes. also, the attributes are categorical(2 categories for our
   case). now, to train the classifier, we will need to calculate p(b|a),
   for all the values in the instance and response space. this means, we
   will need to calculate 2*(2^n -1), parameters for learning this model.
   this is clearly unrealistic in most practical learning domains. for
   example, if there are 30 boolean attributes, then we will need to
   estimate more than 3 billion parameters.

naive bayes algorithm

   the complexity of the above bayesian classifier needs to be reduced,
   for it to be practical. the naive bayes algorithm does that by making
   an assumption of conditional independence over the training dataset.
   this drastically reduces the complexity of above mentioned problem to
   just 2n.

   the assumption of conditional independence states that, given random
   variables x, y and z, we say x is conditionally independent of y given
   z, if and only if the id203 distribution governing x is
   independent of the value of y given z.

   in other words, x and y are conditionally independent given z if and
   only if, given knowledge that z occurs, knowledge of whether x occurs
   provides no information on the likelihood of y occurring, and knowledge
   of whether y occurs provides no information on the likelihood of x
   occurring.

     this assumption makes the bayes algorithm, naive.

   given, n different attribute values, the likelihood now can be written
   as
   [1*2m9bpdqhlzj2kealohtbsg.png]

   here, x represents the attributes or features, and y is the response
   variable. now, p(x|y) becomes equal to the products of, id203
   distribution of each attribute x given y.

maximizing a posteriori

   what we are interested in, is finding the posterior id203 or
   p(y|x). now, for multiple values of y, we will need to calculate this
   expression for each of them.

   given a new instance xnew, we need to calculate the id203 that y
   will take on any given value, given the observed attribute values of
   xnew and given the distributions p(y) and p(x|y) estimated from the
   training data.

   so, how will we predict the class of the response variable, based on
   the different values we attain for p(y|x). we simply take the most
   probable or maximum of these values. therefore, this procedure is also
   known as maximizing a posteriori.

maximizing likelihood

   if we assume that the response variable is uniformly distributed, that
   is it is equally likely to get any response, then we can further
   simplify the algorithm. with this assumption the priori or p(y) becomes
   a constant value, which is 1/categories of the response.

   as, the priori and evidence are now independent of the response
   variable, these can be removed from the equation. therefore, the
   maximizing the posteriori is reduced to maximizing the likelihood
   problem.

feature distribution

   as seen above, we need to estimate the distribution of the response
   variable from training set or assume uniform distribution. similarly,
   to estimate the parameters for a feature   s distribution, one must
   assume a distribution or generate nonparametric models for the features
   from the training set. such assumptions are known as event models. the
   variations in these assumptions generates different algorithms for
   different purposes. for continuous distributions, the gaussian naive
   bayes is the algorithm of choice. for discrete features, multinomial
   and bernoulli distributions as popular. detailed discussion of these
   variations are out of the scope of this article.

   naive bayes classifiers work really well in complex situations, despite
   the simplified assumptions and naivety. the advantage of these
   classifiers is that they require small number of training data for
   estimating the parameters necessary for classification. this is the
   algorithm of choice for text categorization. this is the basic idea
   behind naive bayes classifiers, that you need to start experimenting
   with the algorithm.
     __________________________________________________________________

   if you liked this article, be sure to show your support by clapping for
   this article below and if you have any questions, leave a comment and i
   will do my best to answer.

   for being more aware of the world of machine learning, follow me. it   s
   the best way to find out when i write more articles like this.

   you can also follow me on [18]twitter, [19]email me directly or
   [20]find me on linkedin. i   d love to hear from you.

   that   s all folks, have a nice day :)

     * [21]machine learning
     * [22]data science
     * [23]data
     * [24]artificial intelligence
     * [25]deep learning

   (button)
   (button)
   (button) 521 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of prashant gupta

[27]prashant gupta

   machine learning engineer, android developer, tech enthusiast

     (button) follow
   [28]towards data science

[29]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 521
     * (button)
     *
     *

   [30]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [31]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f49cc8f831b4
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/naive-bayes-in-machine-learning-f49cc8f831b4&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/naive-bayes-in-machine-learning-f49cc8f831b4&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_rzhtjfpx5flz---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@prashantgupta17?source=post_header_lockup
  17. https://towardsdatascience.com/@prashantgupta17
  18. https://twitter.com/prashant_1722
  19. mailto:pr.span24@gmail.com
  20. https://www.linkedin.com/in/prashantgupta17/
  21. https://towardsdatascience.com/tagged/machine-learning?source=post
  22. https://towardsdatascience.com/tagged/data-science?source=post
  23. https://towardsdatascience.com/tagged/data?source=post
  24. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  25. https://towardsdatascience.com/tagged/deep-learning?source=post
  26. https://towardsdatascience.com/@prashantgupta17?source=footer_card
  27. https://towardsdatascience.com/@prashantgupta17
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/f49cc8f831b4/share/twitter
  34. https://medium.com/p/f49cc8f831b4/share/facebook
  35. https://medium.com/p/f49cc8f831b4/share/twitter
  36. https://medium.com/p/f49cc8f831b4/share/facebook
