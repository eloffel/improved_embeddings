   [tr?id=190747804793608&ev=pageview &noscript=1]

     * [1]

physics
     * [2]

mathematics
     * [3]

biology
     * [4]

computer science
     * [5]

all articles

     * (button)
     * (button)
     * (button)
     * (button)

   (button)

new theory cracks open the black box of deep learning

   (button)

share

     (button)
     *

comments
     * (button)

read later

   previous: wired to learn: the next aia brain built from atomic switches
   can learn
   series

[6]wired to learn: the next ai

new theory cracks open the black box of deep learning

by [7]natalie wolchover

   september 21, 2017
   a new idea called the    information bottleneck    is helping to explain
   the puzzling success of today   s artificial-intelligence algorithms    
   and might also explain how human brains learn.
   (button)
   information dog bottleneck information dog bottleneck

   [8]eric nyquist for quanta magazine
   [infobottleneck_2880x1620-2880x1620.jpg]
   [quantateam_natalie1.jpg]

natalie wolchover

   senior writer/editor
     __________________________________________________________________

   september 21, 2017
     __________________________________________________________________

   [9]view pdf/print mode
   [10]artificial intelligence[11]computer science[12]deep
   learning[13]machine learning[14]podcast[15]random walk[16]wired to
   learn: the next ai
   [17]alice and bob meet the wall of fire - the biggest ideas in science
   from quanta     available now! [18]alice and bob meet the wall of fire -
   the biggest ideas in science from quanta     available now!

   even as machines known as    deep neural networks    have learned to
   converse, drive cars, [19]beat video games and [20]go champions, dream,
   paint pictures and help make scientific discoveries, they have also
   confounded their human creators, who never expected so-called
      deep-learning    algorithms to work so well. no underlying principle has
   guided the design of these learning systems, other than vague
   inspiration drawn from the architecture of the brain (and no one really
   understands how that operates either).

   like a brain, a deep neural network has layers of neurons     artificial
   ones that are figments of computer memory. when a neuron fires, it
   sends signals to connected neurons in the layer above. during deep
   learning, connections in the network are strengthened or weakened as
   needed to make the system better at sending signals from input data    
   the pixels of a photo of a dog, for instance     up through the layers to
   neurons associated with the right high-level concepts, such as    dog.   
   after a deep neural network has    learned    from thousands of sample dog
   photos, it can identify dogs in new photos as accurately as people can.
   the magic leap from special cases to general concepts during learning
   gives deep neural networks their power, just as it underlies human
   reasoning, creativity and the other faculties collectively termed
      intelligence.    experts wonder what it is about deep learning that
   enables generalization     and to what extent brains apprehend reality in
   the same way.

   last month, a [21]youtube video of a conference talk in berlin, shared
   widely among artificial-intelligence researchers, offered a possible
   answer. in the talk, [22]naftali tishby, a computer scientist and
   neuroscientist from the hebrew university of jerusalem, presented
   evidence in support of a new theory explaining how deep learning works.
   tishby argues that deep neural networks learn according to a procedure
   called the    information bottleneck,    which he and two
   collaborators [23]first described in purely theoretical terms in 1999.
   the idea is that a network rids noisy input data of extraneous details
   as if by squeezing the information through a bottleneck, retaining only
   the features most relevant to general concepts. striking new
   [24]computer experiments by tishby and his student ravid shwartz-ziv
   reveal how this squeezing procedure happens during deep learning, at
   least in the cases they studied.

   tishby   s findings have the ai community buzzing.    i believe that the
   information bottleneck idea could be very important in future deep
   neural network research,    said [25]alex alemi of google research, who
   has already [26]developed new approximation methods for applying an
   information bottleneck analysis to large deep neural networks. the
   bottleneck could serve    not only as a theoretical tool for
   understanding why our neural networks work as well as they do
   currently, but also as a tool for constructing new objectives and
   architectures of networks,    alemi said.

   some researchers remain skeptical that the theory fully accounts for
   the success of deep learning, but [27]kyle cranmer, a particle
   physicist at new york university who uses machine learning to analyze
   particle collisions at the large hadron collider, said that as a
   general principle of learning, it    somehow smells right.   

   [28]geoffrey hinton, a pioneer of deep learning who works at google and
   the university of toronto, emailed tishby after watching his berlin
   talk.    it   s extremely interesting,    hinton wrote.    i have to listen to
   it another 10,000 times to really understand it, but it   s very rare
   nowadays to hear a talk with a really original idea in it that may be
   the answer to a really major puzzle.   

   according to tishby, who views the information bottleneck as a
   fundamental principle behind learning, whether you   re an algorithm, a
   housefly, a conscious being, or a physics calculation of emergent
   behavior, that long-awaited answer    is that the most important part of
   learning is actually forgetting.   

the bottleneck

   tishby began contemplating the information bottleneck around the time
   that other researchers were first mulling over deep neural networks,
   though neither concept had been named yet. it was the 1980s, and tishby
   was thinking about how good humans are at id103     a major
   challenge for ai at the time. tishby realized that the crux of the
   issue was the question of relevance: what are the most relevant
   features of a spoken word, and how do we tease these out from the
   variables that accompany them, such as accents, mumbling and
   intonation? in general, when we face the sea of data that is reality,
   which signals do we keep?

      this notion of relevant information was mentioned many times in
   history but never formulated correctly,    tishby said in an interview
   last month.    for many years people thought id205 wasn   t
   the right way to think about relevance, starting with misconceptions
   that go all the way to shannon himself.   

   claude shannon, the founder of id205, in a sense liberated
   the study of information starting in the 1940s by allowing it to be
   considered in the abstract     as 1s and 0s with purely mathematical
   meaning. shannon took the view that, as tishby put it,    information is
   not about semantics.    but, tishby argued, this isn   t true. using
   id205, he realized,    you can define    relevant    in a
   precise sense.   

   imagine x is a complex data set, like the pixels of a dog photo, and y
   is a simpler variable represented by those data, like the word    dog.   
   you can capture all the    relevant    information in x about y by
   compressing x as much as you can without losing the ability to predict
   y. in their 1999 paper, tishby and co-authors [29]fernando pereira, now
   at google, and [30]william bialek, now at princeton university,
   formulated this as a mathematical optimization problem. it was a
   fundamental idea with no killer application.

      i   ve been thinking along these lines in various contexts for 30
   years,    tishby said.    my only luck was that deep neural networks became
   so important.   

eyeballs on faces on people on scenes

   though the concept behind deep neural networks had been kicked around
   for decades, their performance in tasks like speech and image
   recognition only took off in the early 2010s, due to improved training
   regimens and more powerful computer processors. tishby recognized their
   potential connection to the information bottleneck principle in 2014
   after reading a [31]surprising paper by the physicists [32]david schwab
   and [33]pankaj mehta.

   the [34]duo discovered that a deep-learning algorithm invented by
   hinton called the    deep belief net    works, in a particular case,
   exactly like reid172, a technique used in physics to zoom out
   on a physical system by coarse-graining over its details and
   calculating its overall state. when schwab and mehta applied the deep
   belief net to a model of a magnet at its    critical point,    where the
   system is fractal, or self-similar at every scale, they found that the
   network automatically used the reid172-like procedure to
   discover the model   s state. it was a stunning indication that, as the
   biophysicist [35]ilya nemenman said at the time,    extracting relevant
   features in the context of statistical physics and extracting relevant
   features in the context of deep learning are not just similar words,
   they are one and the same.   

   the only problem is that, in general, the real world isn   t fractal.
      the natural world is not ears on ears on ears on ears; it   s eyeballs
   on faces on people on scenes,    cranmer said.    so i wouldn   t say [the
   reid172 procedure] is why deep learning on natural images is
   working so well.    but tishby, who at the time was undergoing
   chemotherapy for pancreatic cancer, realized that both deep learning
   and the coarse-graining procedure could be encompassed by a broader
   idea.    thinking about science and about the role of my old ideas was an
   important part of my healing and recovery,    he said.

   in 2015, he and his student noga zaslavsky [36]hypothesized that deep
   learning is an information bottleneck procedure that compresses noisy
   data as much as possible while preserving information about what the
   data represent. tishby and shwartz-ziv   s new experiments with deep
   neural networks reveal how the bottleneck procedure actually plays out.
   in one case, the researchers used small networks that could be trained
   to label input data with a 1 or 0 (think    dog    or    no dog   ) and gave
   their 282 neural connections random initial strengths. they then
   tracked what happened as the networks engaged in deep learning with
   3,000 sample input data sets.

   the basic algorithm used in the majority of deep-learning procedures to
   tweak neural connections in response to data is called    stochastic
   id119   : each time the training data are fed into the
   network, a cascade of firing activity sweeps upward through the layers
   of artificial neurons. when the signal reaches the top layer, the final
   firing pattern can be compared to the correct label for the image     1
   or 0,    dog    or    no dog.    any differences between this firing pattern
   and the correct pattern are    back-propagated    down the layers, meaning
   that, like a teacher correcting an exam, the algorithm strengthens or
   weakens each connection to make the network layer better at producing
   the correct output signal. over the course of training, common patterns
   in the training data become reflected in the strengths of the
   connections, and the network becomes expert at correctly labeling the
   data, such as by recognizing a dog, a word, or a 1.

   in their experiments, tishby and shwartz-ziv tracked how much
   information each layer of a deep neural network retained about the
   input data and how much information each one retained about the output
   label. the scientists found that, layer by layer, the networks
   converged to the information bottleneck theoretical bound: a
   theoretical limit derived in tishby, pereira and bialek   s original
   paper that represents the absolute best the system can do at extracting
   relevant information. at the bound, the network has compressed the
   input as much as possible without sacrificing the ability to accurately
   predict its label.

   tishby and shwartz-ziv also made the intriguing discovery that deep
   learning proceeds in two phases: a short    fitting    phase, during which
   the network learns to label its training data, and a much longer
      compression    phase, during which it becomes good at generalization, as
   measured by its performance at labeling new test data.

   as a deep neural network tweaks its connections by stochastic gradient
   descent, at first the number of bits it stores about the input data
   stays roughly constant or increases slightly, as connections adjust to
   encode patterns in the input and the network gets good at fitting
   labels to it. some experts have compared this phase to memorization.

   then learning switches to the compression phase. the network starts to
   shed information about the input data, keeping track of only the
   strongest features     those correlations that are most relevant to the
   output label. this happens because, in each iteration of stochastic
   id119, more or less accidental correlations in the training
   data tell the network to do different things, dialing the strengths of
   its neural connections up and down in a [37]random walk. this
   randomization is effectively the same as compressing the system   s
   representation of the input data. as an example, some photos of dogs
   might have houses in the background, while others don   t. as a network
   cycles through these training photos, it might    forget    the correlation
   between houses and dogs in some photos as other photos counteract it.
   it   s this forgetting of specifics, tishby and shwartz-ziv argue, that
   enables the system to form general concepts. indeed, their experiments
   revealed that deep neural networks ramp up their generalization
   performance during the compression phase, becoming better at labeling
   test data. (a deep neural network trained to recognize dogs in photos
   might be tested on new photos that may or may not include dogs, for
   instance.)

   it remains to be seen whether the information bottleneck governs all
   deep-learning regimes, or whether there are other routes to
   generalization besides compression. some ai experts see tishby   s idea
   as one of many important theoretical insights about deep learning to
   have emerged recently. [38]andrew saxe, an ai researcher and
   theoretical neuroscientist at harvard university, noted that certain
   very large deep neural networks don   t seem to need a drawn-out
   compression phase in order to generalize well. instead, researchers
   program in something called early stopping, which cuts training short
   to prevent the network from encoding too many correlations in the first
   place.

   tishby argues that the network models analyzed by saxe and his
   colleagues differ from standard deep neural network architectures, but
   that nonetheless, the information bottleneck theoretical bound defines
   these networks    generalization performance better than other methods.
   questions about whether the bottleneck holds up for larger neural
   networks are partly addressed by tishby and shwartz-ziv   s most recent
   experiments, not included in their preliminary paper, in which they
   train much larger, 330,000-connection-deep neural networks to recognize
   handwritten digits in the 60,000-image [39]modified national institute
   of standards and technology database, a well-known benchmark for
   gauging the performance of deep-learning algorithms. the scientists saw
   the same convergence of the networks to the information bottleneck
   theoretical bound; they also observed the two distinct phases of deep
   learning, separated by an even sharper transition than in the smaller
   networks.    i   m completely convinced now that this is a general
   phenomenon,    tishby said.

humans and machines

   the mystery of how brains sift signals from our senses and elevate them
   to the level of our conscious awareness drove much of the early
   interest in deep neural networks among ai pioneers, who hoped to
   reverse-engineer the brain   s learning rules. ai practitioners have
   since largely abandoned that path in the mad dash for technological
   progress, instead slapping on bells and whistles that boost performance
   with little regard for biological plausibility. still, as their
   thinking machines achieve ever greater feats     even stoking fears that
   [40]ai could someday pose an existential threat     many researchers hope
   these explorations will uncover general insights about learning and
   intelligence.

   [41]brenden lake, an assistant professor of psychology and data science
   at new york university who studies similarities and differences in how
   humans and machines learn, said that tishby   s findings represent    an
   important step towards opening the black box of neural networks,    but
   he stressed that the brain represents a much bigger, blacker black box.
   our adult brains, which boast several hundred trillion connections
   between 86 billion neurons, in all likelihood employ a bag of tricks to
   enhance generalization, going beyond the basic image- and
   sound-recognition learning procedures that occur during infancy and
   that may in many ways resemble deep learning.

   for instance, lake said the fitting and compression phases that tishby
   identified don   t seem to have analogues in the way children learn
   handwritten characters, which he studies. children don   t need to see
   thousands of examples of a character and compress their mental
   representation over an extended period of time before they   re able to
   recognize other instances of that letter and write it themselves. in
   fact, they can learn from a single example. [42]lake and his
   colleagues    models suggest the brain may deconstruct the new letter
   into a series of strokes     previously existing mental constructs    
   allowing the conception of the letter to be tacked onto an edifice of
   prior knowledge.    rather than thinking of an image of a letter as a
   pattern of pixels and learning the concept as mapping those features   
   as in standard machine-learning algorithms, lake explained,    instead i
   aim to build a simple causal model of the letter,    a shorter path to
   generalization.

   such brainy ideas might hold lessons for the ai community, furthering
   the back-and-forth between the two fields. tishby believes his
   information bottleneck theory will ultimately prove useful in both
   disciplines, even if it takes a more general form in human learning
   than in ai. one immediate insight that can be gleaned from the theory
   is a better understanding of which kinds of problems can be solved by
   real and id158s.    it gives a complete
   characterization of the problems that can be learned,    tishby said.
   these are    problems where i can wipe out noise in the input without
   hurting my ability to classify. this is natural vision problems, speech
   recognition. these are also precisely the problems our brain can cope
   with.   

   meanwhile, both real and id158s stumble on problems
   in which every detail matters and minute differences can throw off the
   whole result. most people can   t quickly multiply two large numbers in
   their heads, for instance.    we have a long class of problems like this,
   logical problems that are very sensitive to changes in one variable,   
   tishby said.    classifiability, discrete problems, cryptographic
   problems. i don   t think deep learning will ever help me break
   cryptographic codes.   

   generalizing     traversing the information bottleneck, perhaps     means
   leaving some details behind. this isn   t so good for doing algebra on
   the fly, but that   s not a brain   s main business. we   re looking for
   familiar faces in the crowd, order in chaos, salient signals in a noisy
   world.

   this article was reprinted on [43]wired.com.

share this article
     __________________________________________________________________

newsletter

   get quanta magazine delivered to your inbox
   [44](button) subscribe now
   [45]most recent newsletter
   [template]
   [quantateam_natalie1.jpg]

natalie wolchover

   senior writer/editor
     __________________________________________________________________

   september 21, 2017
     __________________________________________________________________

   [46]view pdf/print mode
   [47]artificial intelligence[48]computer science[49]deep
   learning[50]machine learning[51]podcast[52]random walk[53]wired to
   learn: the next ai
   [54]alice and bob meet the wall of fire - the biggest ideas in science
   from quanta     available now! [55]alice and bob meet the wall of fire -
   the biggest ideas in science from quanta     available now!

share this article
     __________________________________________________________________

newsletter

   get quanta magazine delivered to your inbox
   [56](button) subscribe now
   [57]most recent newsletter
   [58]prev

the quanta newsletter

   get highlights of the most important news delivered to your email inbox
   ____________________
   (button) subscribe

[59]most recent newsletter

comment on this article

   quanta magazine moderates comments to facilitate an informed,
   substantive, civil conversation. abusive, profane, self-promotional,
   misleading, incoherent or off-topic comments will be rejected.
   moderators are staffed during regular business hours (new york time)
   and can only accept comments written in english.
   (button) show comments
   brain made of wires
   brain made of wires

next article

a brain built from atomic switches can learn
     __________________________________________________________________

     * [60]about quanta
     * [61]archive
     * [62]contact us
     * [63]terms & conditions
     * [64]privacy policy
     * [65]simons foundation

   all rights reserved    2019

references

   visible links
   1. https://www.quantamagazine.org/physics/
   2. https://www.quantamagazine.org/mathematics/
   3. https://www.quantamagazine.org/biology/
   4. https://www.quantamagazine.org/computer-science/
   5. https://www.quantamagazine.org/archive/
   6. https://www.quantamagazine.org/tag/wired-to-learn-the-next-ai/
   7. https://www.quantamagazine.org/authors/natalie/
   8. https://www.ericnyquist.com/
   9. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  10. https://www.quantamagazine.org/tag/artificial-intelligence/
  11. https://www.quantamagazine.org/tag/computer-science/
  12. https://www.quantamagazine.org/tag/deep-learning/
  13. https://www.quantamagazine.org/tag/machine-learning/
  14. https://www.quantamagazine.org/tag/podcast/
  15. https://www.quantamagazine.org/tag/random-walk/
  16. https://www.quantamagazine.org/tag/wired-to-learn-the-next-ai/
  17. https://www.amazon.com/dp/026253634x/
  18. https://www.amazon.com/dp/026253634x/
  19. https://www.quantamagazine.org/clever-machines-learn-how-to-be-curious-20170919/
  20. https://www.quantamagazine.org/is-alphago-really-such-a-big-deal-20160329/
  21. https://www.youtube.com/watch?v=blqjhjxihk8&feature=youtu.be
  22. http://www.cs.huji.ac.il/~tishby/
  23. https://arxiv.org/pdf/physics/0004057.pdf
  24. https://arxiv.org/abs/1703.00810
  25. https://research.google.com/pubs/104980.html
  26. https://arxiv.org/pdf/1612.00410.pdf
  27. https://as.nyu.edu/content/nyu-as/as/faculty/kyle-s-cranmer.html
  28. http://www.cs.toronto.edu/~hinton/
  29. https://research.google.com/pubs/author1092.html
  30. http://www.princeton.edu/~wbialek/wbialek.html
  31. https://arxiv.org/abs/1410.3831
  32. https://biophysics.princeton.edu/people/david-j-schwab
  33. http://physics.bu.edu/~pankajm/
  34. https://www.quantamagazine.org/deep-learning-relies-on-reid172-physicists-find-20141204/
  35. http://www.physics.emory.edu/home/people/faculty/nemenman-ilya.html
  36. https://arxiv.org/abs/1503.02406
  37. https://www.quantamagazine.org/a-unified-theory-of-randomness-20160802/
  38. http://www.people.fas.harvard.edu/~asaxe/
  39. http://yann.lecun.com/exdb/mnist/
  40. https://www.quantamagazine.org/artificial-intelligence-aligned-with-human-values-qa-with-stuart-russell-20150421/
  41. http://cims.nyu.edu/~brenden/
  42. http://cims.nyu.edu/~brenden/lakeetal2015science.pdf
  43. https://www.wired.com/story/new-theory-deep-learning/
  44. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/#newsletter
  45. http://us1.campaign-archive2.com/home/?u=0d6ddf7dc1a0b7297c8e06618&id=f0cb61321c
  46. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  47. https://www.quantamagazine.org/tag/artificial-intelligence/
  48. https://www.quantamagazine.org/tag/computer-science/
  49. https://www.quantamagazine.org/tag/deep-learning/
  50. https://www.quantamagazine.org/tag/machine-learning/
  51. https://www.quantamagazine.org/tag/podcast/
  52. https://www.quantamagazine.org/tag/random-walk/
  53. https://www.quantamagazine.org/tag/wired-to-learn-the-next-ai/
  54. https://www.amazon.com/dp/026253634x/
  55. https://www.amazon.com/dp/026253634x/
  56. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/#newsletter
  57. http://us1.campaign-archive2.com/home/?u=0d6ddf7dc1a0b7297c8e06618&id=f0cb61321c
  58. https://www.quantamagazine.org/a-brain-built-from-atomic-switches-can-learn-20170920/
  59. http://us1.campaign-archive2.com/home/?u=0d6ddf7dc1a0b7297c8e06618&id=f0cb61321c
  60. https://www.quantamagazine.org/about/
  61. https://www.quantamagazine.org/archive
  62. https://www.quantamagazine.org/contact-us/
  63. https://www.quantamagazine.org/terms-conditions/
  64. https://www.quantamagazine.org/privacy-policy/
  65. http://www.simonsfoundation.org/

   hidden links:
  67. https://www.quantamagazine.org/
  68. https://www.quantamagazine.org/
  69. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/#comments
  70. http://www.facebook.com/sharer.php?u=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  71. https://twitter.com/share?url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/&text=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning&via=quantamagazine
  72. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/#0
  73. mailto:?subject=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning&body=a%20new%20idea%20called%20the%20%e2%80%9cinformation%20bottleneck%e2%80%9d%20is%20helping%20to%20explain%20the%20puzzling%20success%20of%20today%e2%80%99s%20artificial-intelligence%20algorithms%20%e2%80%94%20and%20might%20also%20explain%20how%20human%20brains%20learn.%0a%0ahttps://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  74. https://getpocket.com/save?url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/&title=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning
  75. http://digg.com/submit?url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  76. https://news.ycombinator.com/submitlink?u=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/&t=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning
  77. https://share.flipboard.com/bookmarklet/popout?v=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning&url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  78. https://www.quantamagazine.org/a-brain-built-from-atomic-switches-can-learn-20170920/
  79. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/#comments
  80. https://www.quantamagazine.org/a-brain-built-from-atomic-switches-can-learn-20170920/
  81. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/#comments
  82. https://www.quantamagazine.org/authors/natalie/
  83. http://www.facebook.com/sharer.php?u=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  84. https://twitter.com/share?url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/&text=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning&via=quantamagazine
  85. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/#0
  86. mailto:?subject=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning&body=a%20new%20idea%20called%20the%20%e2%80%9cinformation%20bottleneck%e2%80%9d%20is%20helping%20to%20explain%20the%20puzzling%20success%20of%20today%e2%80%99s%20artificial-intelligence%20algorithms%20%e2%80%94%20and%20might%20also%20explain%20how%20human%20brains%20learn.%0a%0ahttps://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  87. https://getpocket.com/save?url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/&title=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning
  88. http://digg.com/submit?url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  89. https://news.ycombinator.com/submitlink?u=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/&t=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning
  90. https://share.flipboard.com/bookmarklet/popout?v=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning&url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  91. https://www.quantamagazine.org/authors/natalie/
  92. http://www.facebook.com/sharer.php?u=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  93. https://twitter.com/share?url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/&text=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning&via=quantamagazine
  94. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/#0
  95. mailto:?subject=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning&body=a%20new%20idea%20called%20the%20%e2%80%9cinformation%20bottleneck%e2%80%9d%20is%20helping%20to%20explain%20the%20puzzling%20success%20of%20today%e2%80%99s%20artificial-intelligence%20algorithms%20%e2%80%94%20and%20might%20also%20explain%20how%20human%20brains%20learn.%0a%0ahttps://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  96. https://getpocket.com/save?url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/&title=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning
  97. http://digg.com/submit?url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
  98. https://news.ycombinator.com/submitlink?u=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/&t=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning
  99. https://share.flipboard.com/bookmarklet/popout?v=new%20theory%20cracks%20open%20the%20black%20box%20of%20deep%20learning&url=https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/
 100. https://www.quantamagazine.org/a-brain-built-from-atomic-switches-can-learn-20170920/
 101. https://www.quantamagazine.org/
 102. https://www.facebook.com/quantanews
 103. https://twitter.com/quantamagazine
 104. http://youtube.com/c/quantamagazineorgnews
 105. https://instagram.com/quantamag
 106. http://www.dogstudio.be/
