   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]ml review
     * [9]       contribute
     * [10]            review.com newsletter
     __________________________________________________________________

how to predict quora question pairs using siamese manhattan lstm

   [11]go to the profile of elior cohen
   [12]elior cohen (button) blockedunblock (button) followfollowing
   jun 7, 2017

   the article is about manhattan lstm (malstm)         a siamese deep network
   and its appliance to [13]kaggle   s quora pairs competition.
   i will do my best to explain the network and go through the keras code
   (if you are only here for the code, scroll down :)
   [14]full code on github
     __________________________________________________________________

   in the past few years, deep learning is all the fuss in the tech
   industry.
   to keep up on things i like to get my hands dirty implementing
   interesting network architectures i come across in article readings.

   few months ago i came across a very nice article called [15]siamese
   recurrent architectures for learning sentence similarity.it offers a
   pretty straightforward approach to the common problem of sentence
   similarity.
   named malstm (   ma    for manhattan distance), its architecture is
   depicted in figure 1 (diagram excludes the sentence preprocessing
   part).
   notice that since this is a siamese network, it is easier to train
   because it shares weights on both sides.
   [1*szm2gdnr-otx9ytvkqeuog.png]
   figure 1 malstm   s architecture         similar color means the weights are
   shared between the same-colored elements

network explained

   (i will be using [16]keras, so some technical details are related to
   the implementation)

   so first of all, what is a    siamese network   ?
   siamese networks are networks that have two or more identical
   sub-networks in them.
   siamese networks seem to perform well on similarity tasks and have been
   used for tasks like sentence semantic similarity, recognizing forged
   signatures and many more.

   in malstm the identical sub-network is all the way from the embedding
   up to the last lstm hidden state.
   id27 is a modern way to represent words in deep learning
   models. more about it can be found in this nice [17]blog post.
   essentially it   s a method to give words semantic meaning in a vector
   representation.

   inputs to the network are zero-padded sequences of word indices. these
   inputs are vectors of fixed length, where the first zeros are being
   ignored and the nonzeros are indices that uniquely identify words.
   those vectors are then fed into the embedding layer. this layer looks
   up the corresponding embedding for each word and encapsulates all them
   into a matrix. this matrix represents the given text as a series of
   embeddings.
   i use [18]google   s id97 embedding, same as in the original paper.
   the process is depicted in figure 2.
   [1*5pv_jnm4plt3hqxg-4ixaw.png]
   figure 2 embedding process

   we have two embedded matrices that represent a candidate of two similar
   questions. then we feed them into the lstm (practically, there is only
   one) and the final state of the lstm for each question is a
   50-dimensional vector. it is trained to capture semantic meaning of the
   question.
   in figure 1, this vector is denoted by the letter h.
   if you don   t entirely understand lstms, i suggest reading [19]this
   wonderful post.

   by now we have the two vectors that hold the semantic meaning of each
   question. we put them through the defined similarity function (below)
   [1*w6aqlzu6yzqlc5pgcbmtmg.png]
   malstm similarity function

   and since we have an exponent of a negative the output (the prediction
   in our case) will be between 0 and 1.

training

   the optimizer of choice in the article is the [20]adadelta optimizer,
   which can be read about in [21]this article. we also use gradient
   clipping to avoid the exploding gradient problem. you may find a nice
   explanation of the gradient clipping in [22]this video from the udacity
   deep learning course.

   this is where i will diverge a little from the original paper. for the
   sake of simplicity, i do not use a specific weight initialization
   scheme and do not pretrain it on a different task.

   other parameters such as batch size, epochs, and the gradient clipping
   norm value are chosen by me.
     __________________________________________________________________

code

   my full implementation can be found in [23]this jupyter notebook         keep
   following this post to see only the significant parts.

   here (and in the notebook) i   ve excluded all of the data analysis part,
   again to keep things simple and the article readable.

preprocessing

   we get the data as raw text, so our first mission is to take the text
   and convert it into lists of word indices.
   when first opening the training data files in pandas, this is what you
   get.
   [1*8inl5nynsmecqkopmewgja.png]
   figure 3 first lines of the raw training dataset

   our columns of interest are question1, question2, and is_duplicate
   which are self-explanatory.
   the training data is stored in train_df and the test data in test_df
   and both are pandas dataframes.
   only difference between train_df and test_df is that the latter doesn   t
   have the is_duplicate column
train_df = pd.read_csv(train_csv)
test_df = pd.read_csv(test_csv)

   next, i created a helper function named text_to_word_list(text) which
   takes a string as input and outputs a list where each entry is a single
   word from the text and does some preprocessing (removing specific signs
   etc).

   now our aim is to have the ability to turn a word into its embedding
   given by id97, in order to do that we will need to build:
     * vocabulary which is a dict where the keys are words (str) and
       values are the corresponding indices (a unique id as int).
     * inverse_vocabulary which is a list of words (str) where the index
       in the list is the matching id (fromvocabulary).
       we reserve the first place for an all zeros embedding         this is
       needed for the zero padding to be ignored.

   we also use gensim.models.keyedvectors to load the id97 embeddings.
   throughout the code only 2 functions of this class will be used, .vocab
   which will hold all of the id97 words and .word_vec(word) which
   takes a word and returns its embedding.
   finally we will use nltk's english stopwords and store them in stops.

   creating vocabulary and inverse_vocabulary:
vocabulary = dict()
inverse_vocabulary = ['<unk>']
# '<unk>' will never be used, it is only a placeholder for the
# [0, 0, ....0] embedding
id97 = keyedvectors.load_id97_format(embedding_file,binary=true)
questions_cols = ['question1', 'question2']
# iterate over the questions only of both training and test datasets
for dataset in [train_df, test_df]:
    for index, row in dataset.iterrows():
        # iterate through the text of both questions of the row
        for question in questions_cols:
            q2n = []  # q2n -> question numbers representation
            for word in text_to_word_list(row[question]):
                # check for unwanted words
                if word in stops and word not in id97.vocab:
                    continue
                if word not in vocabulary:
                    vocabulary[word] = len(inverse_vocabulary)
                    q2n.append(len(inverse_vocabulary))
                    inverse_vocabulary.append(word)
                else:
                    q2n.append(vocabulary[word])
            # replace questions with lists of word indices
            dataset.set_value(index, question, q2n)

   so now we have vocabulary, inverse_vocabulary and both train_df and
   test_df converted to word indices, screenshot below.
   [1*xup6dhoez1nk4xbj_btrmg.png]
   figure 4 first lines of the converted to word indices training dataset.

   notice we start at index 1, index 0 is reserved for the zero padding.
   also, notice i do not exclude stopwords if they have embeddings, i will
   later give them a random representation         this is done for the sake of
   simplicity. a far better approach will be to train your own embeddings
   to better capture the context of the problem.

embedding matrix

   our next goal is to create the embedding matrix.
   we will assign each word its id97 embedding and leave the
   unrecognized ones (less than 0.5%) random.
   also, we keep the first index all zeros.
embedding_dim = 300
# this will be the embedding matrix
embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)
embeddings[0] = 0  # so that the padding will be ignored
# build the embedding matrix
for word, index in vocabulary.items():
    if word in id97.vocab:
        embeddings[index] = id97.word_vec(word)

   great, we have our embedding matrix in place.

data preparation

   in order to prepare our data for use in keras we have to do two things:
     * split our data to    left    and    right    inputs (one for each side of
       the malstm)
     * pad all of the word number sequences with zeros

   we will also create a validation dataset, to measure our model using
   scikit-learn   s train_test_split function         it keeps the labels
   distribution between the datasets by default.
   in max_seq_length we have the length of the longest question, and here
   is the code
# split to train validation
validation_size = 40000
training_size = len(train_df) - validation_size
x = train_df[questions_cols]
y = train_df['is_duplicate']
x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=
validation_size)
# split to dicts
x_train = {'left': x_train.question1, 'right': x_train.question2}
x_validation = {'left': x_validation.question1, 'right': x_validation.question2}
x_test = {'left': test_df.question1, 'right': test_df.question2}
# convert labels to their numpy representations
y_train = y_train.values
y_validation = y_validation.values
# zero padding
for dataset, side in itertools.product([x_train, x_validation], ['left', 'right'
]):
    dataset[side] = pad_sequences(dataset[side],  maxlen=max_seq_length)

   itertools.product simply gives all the combinations between the two
   lists.

model

   now we create the model itself.
   most of the code is pretty clear but i would like to take a moment to
   talk about keras merge layer.
   the merge layer allows us to merge elements with some built-in methods,
   but also supports custom methods. this is where it comes in handy since
   we need to    merge    our two lstms output using the malstm similarity
   function.
   first let   s define the malstm similarity function.
def exponent_neg_manhattan_distance(left, right):
    return k.exp(-k.sum(k.abs(left-right), axis=1, keepdims=true))

   now lets build the model (using functional api)
# the visible layer
left_input = input(shape=(max_seq_length,), dtype='int32')
right_input = input(shape=(max_seq_length,), dtype='int32')
embedding_layer = embedding(len(embeddings), embedding_dim, weights=[embeddings]
, input_length=max_seq_length, trainable=false)
# embedded version of the inputs
encoded_left = embedding_layer(left_input)
encoded_right = embedding_layer(right_input)
# since this is a siamese network, both sides share the same lstm
shared_lstm = lstm(n_hidden)
left_output = shared_lstm(encoded_left)
right_output = shared_lstm(encoded_right)
# calculates the distance as defined by the malstm model
malstm_distance = lambda(function=lambda x: exponent_neg_manhattan_distance(x[0]
, x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])
# pack it all up into a model
malstm = model([left_input, right_input], [malstm_distance])

   don   t you love it how simple keras is?
   next we define the optimizer and compile our model.
# adadelta optimizer, with gradient clipping by norm
optimizer = adadelta(clipnorm=gradient_clipping_norm)
malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accurac
y'])

   now all that is left is to train it!

training and results

   i launched it on my local machine, which has a gtx 1070.
   the whole script and including preparation and training took about 21
   hours.
malstm_trained = malstm.fit([x_train['left'], x_train['right']], y_train, batch_
size=batch_size, nb_epoch=n_epoch,
                            validation_data=([x_validation['left'], x_validation
['right']], y_validation),
                            callbacks=[checkpointer])

   to properly evaluate the model performance, lets plot training data vs
   validation data accuracy and loss

   accuracy:
   [1*cykvcnkx4vyrne5jolpj2w.png]

   loss:
   [1*vof67dmhdhy_4k-wi0qalg.png]

   so just like that out of the box, seems that malstm is doing ok,
   getting an 82.5% accuracy rate on the validation data.

improvements

   this article and the code as well was written with simplicity in mind.
   to achieve state of the art results tuning and adjusting to your
   specific use case will always be needed.

   here are some thoughts of mine where can we go from here:
     * use id21 like in the article, to pretrain your lstm
     * train id27s, on the data questions.
     * create new data. from my data exploration (is not present in this
       article) there are some duplicate questions that appear twice
       against different questions. it is possible that using this we can
       create more data.
     * create new data using data augmentation swapping words from the
       text with synonyms
     * choose a different optimizer. i   ve read a lot that adadelta doesn   t
       perform as well as other methods when finely tuned.

   the double-edged sword of deep learning is that this list is infinite.
   i   ve put there a tiny bit of possible things that keep us within the
   malstm architecture.

   if you want to explore further and to get the best results possible, i
   advise you to look at the discussion about the competition         they
   achieved some really impressive results there using various models and
   ensembling.
     __________________________________________________________________

   the purpose of this post was to put into work a good article that
   implements some things that you don   t really see in tutorials and
   stuff, i hope this post and the code has taught you a thing or two.
   happy coding :)

     * [24]machine learning
     * [25]data science
     * [26]python
     * [27]keras
     * [28]recurrent neural network

   (button)
   (button)
   (button) 1.1k claps
   (button) (button) (button) 19 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of elior cohen

[30]elior cohen

   data scientist, pythonista

     (button) follow
   [31]ml review

[32]ml review

   highlights from machine learning research, projects and learning
   materials. from and for ml scientists, engineers an enthusiasts.

     * (button)
       (button) 1.1k
     * (button)
     *
     *

   [33]ml review
   never miss a story from ml review, when you sign up for medium.
   [34]learn more
   never miss a story from ml review
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/8b31b0b16a07
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07&source=--------------------------nav_reg&operation=register
   8. https://medium.com/mlreview?source=logo-lo_6gxc1ktfzz5t---33272dbbd858
   9. https://medium.com/mlreview/publish-with-ml-review-c814c54ca28d
  10. http://mlreview.com/?medium
  11. https://medium.com/@eliorcohen?source=post_header_lockup
  12. https://medium.com/@eliorcohen
  13. https://www.kaggle.com/c/quora-question-pairs
  14. https://github.com/eliorc/medium/blob/master/malstm.ipynb
  15. http://www.mit.edu/~jonasm/info/muellerthyagarajan_aaai16.pdf
  16. https://keras.io/
  17. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
  18. https://code.google.com/archive/p/id97/
  19. http://colah.github.io/posts/2015-08-understanding-lstms/
  20. https://arxiv.org/abs/1212.5701
  21. https://arxiv.org/abs/1212.5701
  22. https://www.youtube.com/watch?v=vuamhbewewa
  23. https://github.com/eliorc/medium/blob/master/malstm.ipynb
  24. https://medium.com/tag/machine-learning?source=post
  25. https://medium.com/tag/data-science?source=post
  26. https://medium.com/tag/python?source=post
  27. https://medium.com/tag/keras?source=post
  28. https://medium.com/tag/recurrent-neural-network?source=post
  29. https://medium.com/@eliorcohen?source=footer_card
  30. https://medium.com/@eliorcohen
  31. https://medium.com/mlreview?source=footer_card
  32. https://medium.com/mlreview?source=footer_card
  33. https://medium.com/mlreview
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://medium.com/p/8b31b0b16a07/share/twitter
  37. https://medium.com/p/8b31b0b16a07/share/facebook
  38. https://medium.com/p/8b31b0b16a07/share/twitter
  39. https://medium.com/p/8b31b0b16a07/share/facebook
