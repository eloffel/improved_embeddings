   #[1]stanford literary lab    feed [2]stanford literary lab    comments
   feed [3]stanford literary lab    distributions of words across narrative
   time in 27,266 novels comments feed [4]how many novels have been
   published in english? (an attempt) [5]a hierarchical cluster of words
   across narrative time [6]alternate [7]alternate

   [8]stanford literary lab

[9]stanford literary lab

   director: mark algee-hewitt

   menu
     * [10]about
     * [11]people
     * [12]pamphlets
     * [13]projects
     * [14]techne
     * [15]events

distributions of words across narrative time in 27,266 novels

   over the course of the last few months here at the literary lab, i   ve
   been working on a little project that looks at the distributions of
   individual words inside of novels, when averaged out across lots and
   lots of texts. this is incredibly simple, really     the end result is
   basically just a time-series plot for a word, similar to a historical
   frequency trend. but, the units are different     instead of historical
   time, the x-axis is what matt jockers calls    narrative time,    the space
   between the beginning and end of a book.

   in a certain sense, this grew out of a project i worked on a couple
   years ago that did something similar in the context of an individual
   text     i wrote a program called [16]textplot that tracked the positions
   of words inside of novels and then found words that    flock    together,
   that show up in similar regions of the narrative. this got me thinking
       what if you did this with lots of novels, instead of just one? beyond
   any single text     are there general trends that govern the positions of
   words inside of novels at a kind of narratological level, split away
   from any particular plot? or would everything wash out in the
   aggregate? averaged out across thousands of texts     do individual words
   rise and fall across narrative time, in the way they do across
   historical time? if so     what   s the    shape    of narrative, writ large?

   this draws inspiration, of course, from a bunch of really interesting
   projects that have looked at different versions of this question in the
   last couple years. most well-known is probably matt jockers   
   [17]syuzhet, which tracks the fluctuation of    sentiment    across
   narrative time, and then clusters the resulting trend lines into a set
   of archetypal shapes. andrew piper, [18]writing in new literary
   history, built a model of the    conversional    narrative     based on the
   confessions, in which there   s a shift in the semantic register at the
   moment of conversion     and then traced this signature forward through
   literary history. and, here at stanford, a number of projects have
   looked at the movement of different types of literary    signals    across
   the x-axis of narrative. the suspense project has been using a neural
   network to score the    suspensefulness    of chunks of novels; in
   [19]pamphlet 7, holst katsma traces the    loudness    of speech across
   chapters in the idiot; and mark algee-hewitt has looked at the
   dispersion of words related to the    sublime    across long narratives.

   methodologically, though, the closest thing is probably ben schmidt   s
   fascinating    [20]plot arceology    project, which does something very
   similar except with movies and tv shows     schmidt trained a topic model
   on a corpus of screenplays and then traced the distributions of topics
   across the scripts, finding, among other things, the footprint of the
   prototypical cop drama, a crime at the beginning and a trial at the
   end. i was fascinated by this, and immediately started to daydream
   about what it would look like to replicate it with a corpus of novels.
   but, picking up where textplot left off     what could be gained by
   taking a kind of zero-abstraction approach to the question and just
   looking at individual words? instead of starting with a higher-order
   concept like sentiment, suspense, loudness, conversion, sublimity, or
   even topic     any of which may or may not be the most concise way to
   hook onto the    priors    that sit behind narratives     what happens if you
   start with the smallest units of meaning, and build up from there?

   it   s sort of the lowest-level version of the question, maybe right on
   the line where literary studies starts to shade into corpus
   linguistics. which words are most narratologically    focused,    the most
   distinctive of beginnings, endings, middles, ends, climaxes,
   denouements? how strong are the effects? which types of words encode
   the most information about narrative structure     is it just the
   predictable stuff like    death    and    marriage,    or does it sift down
   into the more architectural layers of language     articles, pronouns,
   verb tenses, punctuation, parts-of-speech? over historical time     do
   words    move    inside of narratives, migrating from beginnings to
   middles, middles to ends, ends to beginnings? or, to pick up on a
   question [21]ted underwood posed recently     would it be possible to use
   this kind of word-level information to build a predictive model of
   narrative sequence, something that could reliably    unshuffle    the
   chapters in a book? it   s like a    basic science    of narratology     if you
   just survey the interior axis of literature in as simple a way as
   possible, what falls out?

   stacking texts

   one nice thing about this question is that the feature extraction step
   is really easy     we just need to count words in a particular way. at
   the lowest level     for a given word in a single text, we can compute
   its    dispersion    across narrative time, the set of positions where the
   word appears. for example,    death    in moby dick:

   so, just eyeballing it, maybe    death    leans slightly towards the end,
   especially with that little cluster around word 220k? once we can do
   this with a single text, though, it   s easy to merge together data from
   lots of texts. we can just compute this over and over again for each
   novel, map the x-axis onto a normalized 0-1 scale, and then stack
   everything up into a big vertical column. for example, in 100 random
   novels:

   or, in 1,000:

   and then, to merge everything together, we can just sum everything up
   into a big histogram, basically     split the x-axis into 100
   evenly-spaced bins, and count up the number of times the word appears
   in each column. it   s sort of like one of those visualizations of
   id203 distributions where marbles get dropped into different
   slots     each word in each text is a marble, and its relative position
   inside the text determines which slot the marble goes into. at the end,
   everything stacks up into a big multinomial distribution that captures
   the aggregate dispersion of the word across lots of texts. eg, from the
   sample of 1,000 above:

   this can then be expanded to all 27,266 texts in our corpus of american
   novels     18,177 from gale   s american fiction corpus (1820-1930), and
   another 9,089 from the chicago novel corpus (1880-2000), which together
   weigh in at about 2.5 billion words. for death:

   so it   s true! not a surprise, but useful as a sanity check. now, with
      death,    it happened that we could already see a sort of blurry,
   pixelated version of this just with a handful of texts. the nice thing
   about this, though, is that it also becomes possible to surface
   well-sampled distributions even for words that are much more
   infrequent, to the degree that they don   t appear in any individual text
   with enough frequency to infer any kind of general tendency. (by zipf   s
   law, this is a large majority of words, in fact     most words will just
   appear a handful of times in a given novel, if at all.) for example,
   take a word like    athletic,    which appears exactly once in moby dick,
   about half-way through:

   and, even in the same sample of 1,000 random novels, the data is still
   very sparse:

   if you squint at this, maybe you can see something, but it   s hard to
   say. with the full corpus, though, the clear trend emerges:

      athletic        along with a great many words used to describe people, as
   we   ll see shortly     concentrates really strongly at the beginning of
   the novel, where characters are getting introduced for the first time.
   if someone is athletic, it tends to get mentioned the first time they
   appear, not the second or third or last.

   significance,    interestingness   

   are these trends significant, though? for    death    and    athletic    they
   certainly look meaningful, just eyeballing the histograms. but what
   about for a word like    irony   :

   which looks much more like a flat line, with some random noise in the
   bin counts? if we take the null hypothesis to be the uniform
   distribution     a flat line, no relationship between the position in
   narrative time and the observed frequency of the word     then the simple
   way to test for significance is just a basic chi-squared test between
   the observed distribution and the expected uniform distribution, where
   the frequency of the word is spread out evenly across the bins. for
   example,    death    appears 593,893 times in the corpus, so, under the
   uniform assumption, we   d expect each of the 100 bins to have 1/100th of
   the total count, ~5,938 each. for    death,    then, the chi-squared
   statistic between this theoretical baseline and the observed counts is
   ~16,925, and the pvalue is so small that it literally rounds down to 0
   in python. for    athletic,    chi-squared is ~1,467, with p also comically
   small at 3.15e-242. whereas, for    irony        chi-squared is 98, with p at
   0.49, meaning we can   t say with confidence that there   s any kind of
   meaningful trend.

   under this, it turns out that almost all words that appear with any
   significant frequency are non-uniform. if we skim off the set of all
   words that appear at least 10,000 times in the corpus (of which there
   are 10,908 in total)     of these, 10,513 (96%) are significantly
   different from the uniform distribution at p<0.05, 10,227 (94%) at
   p<0.01, and 9,815 (90%) at p<0.001. and even if we crank the exponent
   down quite a lot - at p<1e-10, 7,830 (72%) words still reject the null
   hypothesis. what to make of this? i guess there's some kind of very
   high (or low) level insight here - narrative structure does, in fact,
   manifest at the level of individual words? which, once the numbers pop
   up in the jupyter notebook, is one of those things that seems either
   interesting or obvious, depending on how you look at it. but, from an
   interpretive standpoint, the question becomes - if everything is
   significant, then which words are the most significant? which words are
   the most non-uniform, the most surprising, which encode the most
   narratological information? where to focus attention? how to rank the
   words, essentially, from most to least interesting?

   this seemed like a simple question at first, but over the course of the
   last couple months i   ve gone around and around on it, and i   m still not
   confident that i have a good way of doing this. i think the problem,
   basically, is that this notion of    interestingness    or    non-uniformity   
   is actually less cut-and-dry that it seemed to me at first.
   specifically, it   s hard to meaningfully compare words with extremely
   different overall frequencies in the corpus. it   s easy to compare, say,
      sleeping    (which is basically flat) and    ancient    (which has a huge
   spike at the beginning), both of which show up right around 100,000
   times. but, it   s much harder to compare    ancient    and    the,    which
   appears over 100 million times, and represents slightly less than 5% of
   all words in all of the novels. this starts to feel sort of
   apples-to-oranges-ish, in various ways (more on this later), and is
   further confounded by the fact that the data gets so sparse at the very
   top of the frequency chart     since a small handful of words have
   exponentially larger frequencies than everything else, it   s harder to
   say what you   d    expect    to see with those words, since there are so few
   examples of them.

   to get a sense of how this plays out across the whole dictionary     one
   very simple way to quantify the    non-uniformity    of the distributions
   is just to take the raw variance of the bin counts, which can then be
   plotted against frequency:

   this is unreadable with the linear scales because the graph gets
   dominated by a handful of function words; but, on a log-log scale, a
   fairly clean power law falls out:

   but, with that really wide vertical    banding,    which is basically the
   axis of surprise that we care about     at any given word frequency on
   the x-axis, words at the bottom of the band will have comparatively
      flat    distributions across narrative time, and words at the top will
   have the most non-uniform / narratologically interesting distributions.

   the nice thing about just using the raw variance to quantify this is
   that it   s easy to compare it against what you   d expect, in theory, at
   any given level of word frequency. for a multinomial distribution, the
   variance you   d expect to see for the count in any one of the bins is n
   * p (1-p), where n is the number of observations (the word count, in
   this context) and p is 1/100, the id203 that a word will fall
   into any given bin under the uniform assumption. almost all of the
   words fall above this line:

   which, to loop back, corresponds to the fact that almost all words,
   under the chi-squared test, appear not to come from the uniform
   distribution     almost all have higher variances than you   d expect if
   they were uniformly distributed.

   i   m unsure about this, but     to get a crude rank ordering for the words
   in a way that (at least partially) controls for frequency, i think it   s
   reasonable just to divide the observed variances by the expected value
   at each word   s frequency, which gives a ratio that captures how many
   times greater a word   s actual variance is than what you   d expect if it
   didn   t fluctuate at all across narrative time. (mathematically, this is
   basically equivalent to the original chi-squared statistic.)

   so, then, if we skim off the top 500 words:

   does this make sense, statistically? i don   t love the fact that it
   still correlates with frequency     almost all of the highest-frequency
   words make the cut. but that might also just be surfacing something
   true about the high frequency words, which do clearly rise up higher
   above the line. (counterintuitively?) there are other ways of doing
   this     namely, if you flatten everything out into density functions    
   that reward low-frequency words that have the most    volatile    or
      spiky    distributions. but, these are problematic in other ways     this
   is a rabbit hole, which i   ll try circle back to in a bit.

   anyway, under this (imperfect) heuristic, here are the 500 most
   narratologically non-uniform words, ordered by the    center of mass    of
   the distributions across narrative time, running from the beginning to
   the end. there   s way too much here to look at all at once, but, in the
   broadest sense, it looks like     beginnings are about youth, education,
   physical size, (good) appearance, color, property, hair, and noses? and
   endings     forgiveness, criminal justice, suffering, joy, murder,
   marriage, arms, and hands? and the middle? i gloss it as a mixture of
   dialogism and psychological interiority        say,       think,       feel,    and
   all the contractions and dialog punctuation marks     though it   s less
   clear-cut.

   the really wacky stuff, though, is in the stopwords     which deserve
   their own post.

   the feature extraction code for this project can be found [22]here, and
   the analysis code is [23]here. thanks mark algee-hewitt, franco
   moretti, matt jockers, scott enderle, dan jurafsky, christof sch  ch,
   and ryan heuser for helping me think through this project at various
   stages. any mistakes are mine!
   distributions of words across narrative time in 27,266 novels

   [24]david mcclure [25]july 10, 2017july 10, 2017 [26]experiments
     * [27]    how many novels have been published in english? (an attempt)
     * [28]a hierarchical cluster of words across narrative time    

3 thoughts on    distributions of words across narrative time in 27,266 novels   

     *
   [29]ben schmidt
       july 10, 2017 at 6:46 pm
       [30]permalink
       very interesting stuff.
       a couple methodological questions.
       1. i found that in a lot of cases there are words that peak in the
       middle and aren   t used much at either the beginning or the end.
       this probably correlates some with what you all call    abstract    and
          concrete    words in previous lit lab work. might a chi-square test
       be worse at finding these kinds of words than those that
       demonstrate a continuous trend up or down. (or maybe even those
       whose hump is at the 25th or 75th percentile, vs. the 50th?) there
       should be an easy way to test this, but it   s escaping me. maybe
       using absolute error instead of square error would help.
       2. dividing into 100 chunks of narrative time (jockers does this,
       too) means that you   ll get a lot of jumpiness, at a finer
       resolution than anyone actually perceives while reading. if you
       want to do significance testing, this may be penalizing the rare
       words more than the common words. do the chi-square tests results
       change if you use (say) 12 bins instead of 100?
          +
        david mcclurepost author
            july 11, 2017 at 8:41 pm
            [31]permalink
            hey ben,
            thanks for this. about the words in the middle vs. ends     i
            don   t believe chi-squared will favor one over the other, since
            the test just operates on a per-bin level     it   s just
            ((observed - expected)^2 / expected), summed over each of the
            bins. if anything, i think a potential problem with this is
            that it doesn   t have any notion of the ordering of the bins    
            it just treats them as independent    events,    like the result
            of rolling a dice. so, if you took one of the smooth,
            downward-trending curves and randomly shuffled the bins     it
            would have the exact same chi-squared test statistic. this
            seems weird, and makes me wonder if there   s some kind of
               correlational    approach that would explicitly hook onto the
            ordering. i think this would work for doing pairwise
            comparisons between the words, but, in the    ranking    context,
            when comparing against a flat distribution, where there   s no
            trend to correlate against     i   m not sure how to factor in the
            ordering, or if it matters. i guess generally     it   s not clear
            to me if it makes sense to think of these as    distributions   
            or as    time series.   
            i like the idea of dropping down the bin size, i   ll try that.
            thanks!
     *
   [32]ben schmidt
       july 13, 2017 at 9:12 am
       [33]permalink
       on the chi-square, the effect i   m thinking of is a little more
       subtle. if end and beginning-weighted terms make graphs that look
       like a    /    and    \   , respectively, the chi-square results would be
       the same if the middle weighted terms looked like    v   . but they
       don   t: they look like a    u   , or more often like (i   m going to test
       the completeness of your unicode font here) a          . if you reorder
       those from most to least, the chi-square values may be less or more
       depending on the shape, i guess   it certainly looks like you might
       have some bias towards words that have a massive over/underuse in
       the first and/or 99th percentile. (anyhow, i don   t think this is
       super important, just a little interesting.)
       what i   ve usually done in this case is to pull the first two
       principal components out of a matrix; the first tends to show
       linear change, and the second middle-weightedness. [34]here   s a
       rather crazy plot-of-plots in the space. the percentage of variance
       explain by the first pc is roughly similar to the r^2 of a linear
       model. (which itself is an ok representation here). that also does
       not account for strength of the effect, and can end up
       overweighting common words.

   comments are closed.

   copyright    2019 [35]stanford literary lab. powered by [36]wordpress.
   theme: spacious by [37]themegrill.

references

   visible links
   1. https://litlab.stanford.edu/feed/
   2. https://litlab.stanford.edu/comments/feed/
   3. https://litlab.stanford.edu/distributions-of-words-27k-novels/feed/
   4. https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/
   5. https://litlab.stanford.edu/hierarchical-cluster-across-narrative-time/
   6. https://litlab.stanford.edu/wp-json/oembed/1.0/embed?url=https://litlab.stanford.edu/distributions-of-words-27k-novels/
   7. https://litlab.stanford.edu/wp-json/oembed/1.0/embed?url=https://litlab.stanford.edu/distributions-of-words-27k-novels/&format=xml
   8. https://litlab.stanford.edu/
   9. https://litlab.stanford.edu/
  10. https://litlab.stanford.edu/
  11. https://litlab.stanford.edu/people/
  12. https://litlab.stanford.edu/pamphlets/
  13. https://litlab.stanford.edu/projects/
  14. https://litlab.stanford.edu/techne
  15. https://litlab.stanford.edu/events/
  16. http://dclure.org/essays/mental-maps-of-texts/
  17. http://www.matthewjockers.net/2015/02/02/syuzhet/
  18. http://muse.jhu.edu/article/581921
  19. https://litlab.stanford.edu/literarylabpamphlet7.pdf
  20. http://sappingattention.blogspot.com/2014/12/fundamental-plot-arcs-seen-through.html
  21. https://twitter.com/ted_underwood/status/873884672383647744
  22. https://github.com/davidmcclure/literary-interior/tree/shuffle
  23. https://github.com/davidmcclure/lint-analysis
  24. https://litlab.stanford.edu/author/david/
  25. https://litlab.stanford.edu/distributions-of-words-27k-novels/
  26. https://litlab.stanford.edu/category/experiments/
  27. https://litlab.stanford.edu/how-many-novels-have-been-published-in-english-an-attempt/
  28. https://litlab.stanford.edu/hierarchical-cluster-across-narrative-time/
  29. http://benschmidt.org/
  30. https://litlab.stanford.edu/distributions-of-words-27k-novels/#comment-1109
  31. https://litlab.stanford.edu/distributions-of-words-27k-novels/#comment-1111
  32. http://benschmidt.org/
  33. https://litlab.stanford.edu/distributions-of-words-27k-novels/#comment-1113
  34. http://benschmidt.org/mimnoclone/
  35. https://litlab.stanford.edu/
  36. https://wordpress.org/
  37. https://themegrill.com/themes/spacious

   hidden links:
  39. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/how-often.png
  40. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/death-moby-dick.png
  41. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/death-100.png
  42. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/death-1000.png
  43. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/death-1000-bars.png
  44. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/death-27k.png
  45. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/athletic-moby-dick.png
  46. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/athletic-1000.png
  47. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/athletic-27k.png
  48. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/irony.png
  49. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/variance-linear.png
  50. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/variance-log-log.png
  51. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/variance-exp.png
  52. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/variance-bands.png
  53. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/variance-500.png
  54. https://web.stanford.edu/group/litlab/cgi-bin/wordpress/wp-content/uploads/2017/07/500-1.png
  55. https://litlab.stanford.edu/distributions-of-words-27k-novels/#masthead
