   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

supervised machine learning         id75 in python

   go to the profile of alvin chung
   [14]alvin chung (button) blockedunblock (button) followfollowing
   nov 18, 2017 unlisted
   [1*cjeblfjut-hjcbh_6lknsq.jpeg]
   [15]source/cco
update [17/11/17]: the full implementation of supervised id75 can b
e [16]found here.

introduction

   the concept of machine learning has somewhat become a fad as late, with
   companies from small start-ups to large enterprises screaming to be
   technologically enabled through the quote on quote, integration of
   complex automation and predictive analysis.

   this has lead to an impression that machine learning is highly
   nebulous, with systems on integration beyond the comprehension of the
   general public. this is far from the truth. rather, i view machine
   learning more towards the field of computational statistics instead of
   some enigmatic black box, where someone waves a wand, abracadabra, and
   is able to conjure up some magical prediction.

motivation

   the motivation of this series is to enable anyone who is interested in
   the field of machine learning to be able to develop, understand and
   implement their own machine learning algorithms.

   this will be a first of a series of articles which i plan to write; i
   hope you enjoy.

   a few important terminologies before we start:
     * independent variables (features): an independent variable is a
       variable that is manipulated to determine the value of a dependent
       variable. simply, they are the features which we want to use to
       predict some given value of y. it can be also called an explanatory
       variable
     * dependent variable(target): the dependent variable depends on the
       values of the independent variable. simply put, it is the feature
       which we are trying to predict. this can also be commonly known as
       a response variable.
     __________________________________________________________________

simple and multiple id75

   what is your expected income from your years of education? what is
   expected final exam results given your previous marks? what are your
   chances of winning over the girl of your dreams (i   m kidding)

   nonetheless, id75 is one of the strongest tools available
   in statistics and machine learning and can be used to predict some
   value (y) given a set of traits or features (x).

   given that it is such a powerful tool, it is a great starting point for
   individuals to who are excited in the field of data science and machine
   learning to learn about,    how machines learn to make predictions   .

   to illustrate how id75 works, we may examine a common
   problem students face when attending university. what is my expected
   final exam mark, given my previous results in the subject? this problem
   can be mathematically defined as some function between our independent
   variables (x) and the corresponding final exam mark (y).
   [1*6t7y8lps5k-lsoswawnm9a.png]
x (input) = assignment results
y (output) = final exam mark
f = function which describes the relationship between x and y
e (epsilon) = random error term (positive or negative) with a mean zero (there a
re move assumptions for our residuals, however we won't be covering them)

   from experience, you may come to the derivation that,    if my assignment
   mark is 73%, that i generally score a mark of 1.1x for final exam mark,
   which is approximately 80.3%   . while, this may be true, such an
   approximation method is rather unorthodox, and lacks accuracy as us
   humans are implicitly biased in our approximations. furthermore, this
   comes increasingly difficult to predict as we add more independent
   variables.

   computers on the other hand are optimised to perform extremely well
   when provided a set of logical sequences, as they do not suffer from
   biases in comparison to humans. moreover, computers are more efficient
   in both accuracy and computational speed; we can use computers to our
   advantage in predicting our desired feature which we are interested in
   understanding.

   for our example, we will be using a supervised    training and test    set
   to predict a student   s expected final exam result predicated on their
   assignment scores. we will achieve this through splitting our data set
   into a training and test set. the purpose of the training set is to
   enable the machine to learn the relationship between the a student   s
   assignment results and their respective final exam mark. in doing so,
   we can then use the learned function to estimate a student   s final exam
   result, and apply it to our unlabelled test set in order to predict a
   student   s expected final exam score.
   [1*hw9b8_ggbo3gtfiuen86eg.png]
regression
y = f(x) + e, where x = (x1, x2...xn)
training: machine learns (fits) f from labelled training set
test: machine predicts y from unlabeled test set
note: f(x) can be derived through matrices to perform least square linear regres
sion. however this beyond the scope of this tutorial, if you'd like to learn how
 to derive regression lines [17]here is a good link . also, x can be a tensor wi
th any number of dimensions. a 1d tensor is a vector (1 row, many columns), 2d t
ensor is a matrix (many rows, many columns), and higher dimensional tensor.

   for simplicity, we will only use one independent variable(assignment)
   in predicting our estimated final exam score, which is a 2d tensor.
     __________________________________________________________________

id75 (ordinary least squares)

     how to predict the future by drawing a straight line. yes, this
     counts as machine learning.

   the objective of ordinary least square regression (ols) is to learn a
   linear model (line) in which we can use to predict (y), while
   consequently attempting to reduce the error (error term). by reducing
   our error term, we inversely increase the accuracy of our prediction.
   thereby, improving our learned function.
   [1*dzb0d9unu22fvnjp1pjlsa.png]
   source: [18]wikipedia    id75   

   since id75 is a parametric method: where the sample data
   comes from a population that follows a id203 distribution based
   on a fixed set of parameters. as such, various assumptions must be
   satisfied about the form of the function relating x and y         see in the
   attached notes for further reading. our model will be a function that
   predicts y-hat given a specific x:
   [1*asvqlkzccvyqkai8otcrwq.png]
   this can be interpreted as, for a one unit increase in x, holding all
   else constant, y increases   1
     __________________________________________________________________

interpretation

     0: is the y-intercept when x = 0. i.e. when your assignment results is
   0, your predicted final exam mark is the y intercept (   0 )

     1: is the slope of our line. i.e. how much does our final exam mark
   increase for a one unit increase in your assignment mark.

   reminder, our objective is to learn the model parameters which
   minimises our error term, thereby increasing the accuracy of our
   model   s prediction.

     to find the best model parameters:

     1. define a cost function, or id168, that measures how
     inaccurate our model   s prediction is.

     2. find the parameter that minimises loss, i.e. make our model as
     accurate as possible.

   graphically this can be represented in a cartesian plane, as our model
   is two dimensional. this would change into a plane for three
   dimensions, etc   
   [1*8xasz3p6hhstbask1nscla.png]
   where y is our final exam mark, and x is our assignment mark

     note for dimensionality: our example is two-dimensional for
     simplicity, however, this is unrealistic and typically you will have
     more features (x   s) and coefficients in your model, as generally you
     will have more than one feature that is significant in explaining
     your dependent variable. in addition, id75 suffers
     enormously from the [19]curse of dimensionality, as once we deal
     with high-dimensional spaces, every data point becomes an outlier.
     __________________________________________________________________

cost function

   the formula for the cost function may be daunting for you at first.
   however, it is extremely simple and intuitive to understand.
   [1*dyzqshni7wqryfb01aoxkg.png]
   cost function (error term) of our linear model

   simply, the cost function says to take the difference between each real
   data point (y) and our model   s prediction (  ), square the differences
   to avoid negative numbers and penalise larger differences. finally, add
   them up and take the average. except rather than dividing it by n, we
   divide it by 2*n. this is because mathematicians have decided that it
   is easier to derive. feel free to take this to the mathematics court of
   justice. however, for simplicity just remember that we take 2*n.

   for problems that are 2 dimensional, we can could simply derive the
   optimal beta parameters that minimise our id168. however, as
   the model grows increasingly complex, computing the beta parameters for
   each variable becomes no longer feasible. as such, a method known as
   id119 will be necessary in allowing us to minimise our loss
   function.
     __________________________________________________________________

id119: learning our parameters

   [1*lbvlg7msu05pzkmhuaitsq.png]
   [20]source: youtube [21]mwamba capital

     imagine you   re standing somewhere on a mountain (point a). you want
     to get as low as possible as fast as possible, so you decide to take
     the following steps:

     - you check your current altitude, your altitude a step north, a
     step south, a step east, a step west. using this, you figure out
     which direction you should step to reduce your altitude as much as
     possible in this step.

              repeat until stepping any direction will cause you to go up again
     (point b).

     this is id119

   currently, id119 is one of the most popular methods used for
   parameter optimisation. it is often used with neural networks, which we
   will cover later. nonetheless, it is important to understand what it
   does, and how it works.

   the goal of id119 is to find the minimum point of our
   model   s cost function by iteratively getting a better and better
   approximation. this is achieved through iteratively, moving in which
   direction the ground is sloping down most steeply, until stepping any
   direction will cause you to go up again.
   [1*ua1je11ju6__jl5bugcyyw.gif]
   [22]source: pydata

   taking a look at our id168 we saw in regression:
   [1*dyzqshni7wqryfb01aoxkg.png]

   we see that this is really a function of two variables:   0 and   1. all
   the rest of the variables are determined, since x, y and n are given
   during training. hence, we want to try and minimise this function.
   [1*wsbakff2geh1zgy4hjbwfq.gif]
   source: [23]github alykhan tejani

   the function is (   0 ,   1 ) = z. to begin id119, we start by
   making a guess of the parameters b0 and b1 that minimise the function.

   next, we take the partial derivative of the id168 with respect
   to each beta parameters: [dz/d  0, dz/d  1]. the partial derivative
   indicates how much total loss increased or decreased if you increase   0
   or   1 by a very small amount. if the partial derivative of dz/d  1 is a
   negative number, then increasing   1 is good as it will reduce our total
   loss. if it is a positive number, you want to decrease   1. if it is
   zero, we don   t change   1, as it means we have reached optimum.

   we do this until we reach the bottom, i.e, the algorithm converges and
   the loss has been minimised.
     __________________________________________________________________

overfitting

     overfitting:   sherlock, your explanation of what just happened is too
     specific to the situation   . regularisation:    don   t overcomplicate
     things, sherlock.    i   ll punch you for every extra word.
     hyperparameter (    ):      here   s the strength with which i will punch
     you for every extra word            [24]credits to vishal maini

   overfitting occurs when the trained model performs too well on the
   training data that the model learned on, but does not generalise well
   on the test data. the problem of overfitting is not limited to
   computers, humans are often no better. for instance, say you had a bad
   experience with an xyz airline, maybe the service wasn   t good, or that
   the airline was riddled with delays. you might be tempted to say that
   all flights on xyz airline sucks. this is called overfitting whereby we
   overgeneralise something, which otherwise, might have been us just
   having a bad day.
   [1*3o3ib-4dcvedonhau9a5za.png]
   source: [25]quora: luis argerich

   overfitting occurs when a model over-learns from the training data to
   the point where it starts picking up idiosyncrasies that aren   t
   representative of patterns in the real world. this becomes especially
   problematic as you make your model increasingly complex. underfitting
   is related to the issue where your model is not complex enough to
   capture the underlying trends in the data.
   [1*za8baiam4hihaa5kpn94hw.png]
   source: [26]scott fortmann-roe
id160
bias: is the amount of error introduced by approximating real-world phenomena wi
th a simplified model.
variance: is how much your model   s test error changes based on variation in the
training data. it reflects the model   s sensitivity to the idiosyncrasies of the
data set it was trained on
as a model increases in complexity and it becomes flexible, its bias decreases (
it does a good job of explaining the training data), but variance increases (it
doesn   t generalise as well).

     ultimately, in order to have a good model, you need one with low
     bias and low variance

   remember, the only thing we care about is how well the model performs
   on the test data. you want to predict the mark of a student   s final
   exam result, before they are marked, not just build a model that is
   100% accurate in classifying a students mark based of the training set.

   two ways to combat overfitting:

   1. use more training data: the more you have, the harder it is to
   overfit the data by learning too much from any single training example.

   2. user regularisation: add in a penalty in the id168 for
   building a model that assigns too much explanatory power to any one
   feature or allows to many features to be taken into account
   [1*rft6mtu45dit0ojhlgdcbg.png]

   the first piece of the sum above is our normal cost function. the
   second piece is a regularisation term that adds a penalty for large
   beta coefficients that give too much explanatory power to any specific
   feature.

     with these two elements in place, the cost function now balances
     between two priorities: explaining the training data and preventing
     that explanation from becoming overly specific.

   the lambda coefficient of the regularisation term in the cost function
   is a hyperparameter: a general setting of your model that can be
   increased or decreased (i.e. tuned) in order to improve performance. a
   higher lambda value harshly penalises large beta coefficients that
   could lead to potential overfitting. to decide on the best value of
   lambda (  ), you   d use a method known as cross-validation which involves
   holding our a portion of the training data during training, then seeing
   how well your model explains the held-out portion. we   ll go over this
   in more depth in future series'.
for those whom are interested in the full implementation of supervised learning:
 id75. it can be [27]found here
     __________________________________________________________________

wooh!! you   ve just covered the essentials of supervised learning: linear
regression!

   here   s what we covered in this section:
     * how supervised data can be used to enable computers to learn a
       function without explicitly being programmed.
     * id75, the fundamentals of parametric algorithms
     * learning parameters with id119
     * overfitting and regularisation

further reading & practice

   for a more comprehensive understanding of id75, i
   recommend    elements of statistical learning    by trevor hastie. it   s a
   fantastic book which covers the essentials of statistical learning
   using id202.

   for id28, i recommend    applied id28    by
   david w. hosmer. it goes into much more detail in the different methods
   and approaches available for id28.

   for practice:

   for practice, i recommend playing around with datasets used to predict
   housing prices, [28]boston housing data is the most popular. otherwise,
   consider [29]salary prediction.

   for datasets to considering implementing supervised learning: linear
   regression. i recommend going on [30]reddit/datasets or [31]kaggle, to
   practice and see how accurate of a prediction you can create.
   [32]supervised machine learning         dimensional reduction and principal
   component analysis
      a primer to id84 and principle component analysis
   with python   hackernoon.com

   unlisted thanks to [33]angelo atho.
     * [34]machine learning
     * [35]id75
     * [36]data science
     * [37]artificial intelligence
     * [38]python

   (button)
   (button)
   (button) 262 claps
   (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of alvin chung

[39]alvin chung

   medium member since jan 2019

   data scientist, coffee lover, and passionate thinker

     (button) follow
   [40]hacker noon

[41]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 262
     * (button)

   [42]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [43]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/541a5d8141ce
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/supervised-machine-learning-linear-regression-in-python-541a5d8141ce&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/supervised-machine-learning-linear-regression-in-python-541a5d8141ce&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_fajqi9kxucqt---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@alvinchungg
  15. https://hackernoon.com/r/?url=https://deathtothestockphoto.com/
  16. https://hackernoon.com/r/?url=https://github.com/chippasaur/supervised-learning-linear-regression
  17. https://hackernoon.com/r/?url=https://www.youtube.com/watch?v=qa_fi92_qo8
  18. https://hackernoon.com/r/?url=https://commons.wikimedia.org/wiki/file:linear_regression.svg
  19. https://hackernoon.com/r/?url=https://stats.stackexchange.com/questions/169156/explain-curse-of-dimensionality-to-a-child
  20. https://hackernoon.com/r/?url=https://www.youtube.com/watch?v=riplxsnf_zs
  21. https://hackernoon.com/r/?url=https://www.youtube.com/channel/uclr00rtnrucjr3l36kytzga
  22. https://hackernoon.com/r/?url=http://songhuiming.github.io/pages/2017/05/13/gradient-descent-in-solving-linear-regression-and-logistic-regression/
  23. https://hackernoon.com/r/?url=https://alykhantejani.github.io/a-brief-introduction-to-gradient-descent/
  24. https://medium.com/@v_maini
  25. https://hackernoon.com/r/?url=https://www.quora.com/what-is-overfitting
  26. https://hackernoon.com/r/?url=http://scott.fortmann-roe.com/docs/biasvariance.html
  27. https://hackernoon.com/r/?url=https://github.com/chippasaur/supervised-learning-linear-regression
  28. https://hackernoon.com/r/?url=http://www.cs.toronto.edu/~delve/data/boston/bostondetail.html
  29. https://hackernoon.com/r/?url=https://www.kaggle.com/c/job-salary-prediction/data
  30. https://hackernoon.com/r/?url=https://www.reddit.com/r/datasets/
  31. https://hackernoon.com/r/?url=https://www.kaggle.com/kernels
  32. https://hackernoon.com/supervised-machine-learning-dimensional-reduction-and-principal-component-analysis-614dec1f6b4c
  33. https://medium.com/@athoangelo?source=post_page
  34. https://hackernoon.com/tagged/machine-learning?source=post
  35. https://hackernoon.com/tagged/linear-regression?source=post
  36. https://hackernoon.com/tagged/data-science?source=post
  37. https://hackernoon.com/tagged/artificial-intelligence?source=post
  38. https://hackernoon.com/tagged/python?source=post
  39. https://hackernoon.com/@alvinchungg
  40. https://hackernoon.com/?source=footer_card
  41. https://hackernoon.com/?source=footer_card
  42. https://hackernoon.com/
  43. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  45. https://hackernoon.com/@alvinchungg?source=post_header_lockup
  46. https://hackernoon.com/supervised-machine-learning-dimensional-reduction-and-principal-component-analysis-614dec1f6b4c
  47. https://hackernoon.com/@alvinchungg?source=footer_card
