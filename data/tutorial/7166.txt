   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

                 deep learning revolutionizes conversational ai

   recent ai breakthroughs transform id103.

   by [27]yishay carmiel

   august 9, 2017

   soundwave. soundwave. (source: [28]pixabay)

   [29]check out the "ai models and methods sessions" at the ai conference
   in new york, april 29 - may 2, 2018.

   the dream of id103 is a system that truly understands
   humans speaking   in different environments, with a variety of accents
   and languages. for decades, people tackled this problem with no
   success. pinpointing effective strategies for creating such a system
   seemed impossible.

   in the past years, however, breakthroughs in ai and deep learning have
   changed everything in the quest for id103. applying deep
   learning techniques enabled remarkable results. today, we see the leap
   forward in development manifesting in a wide range of products, such as
   amazon echo, apple siri, and many more. in this post, i   ll review
   recent advances in id103, examine the elements that have
   contributed to the rapid progress, and discuss the futureand how far we
   may be from solving the problem completely.

a little background

   for years, one of the most important tasks of ai has been to understand
   humans. people want machines to understand not only what they say but
   also what they mean, and to take particular actions based on that
   information. this goal is the essence of conversational ai.

   conversational ai encompasses two main categories: man-machine
   interface, and human-to-human interface. in a man-machine interface, a
   human interacts with a machine through voice or text, and the machine
   understands the human (even if in a limited manner), and takes some
   kind of action. figure 1 demonstrates that the machine can be either a
   personal assistant (siri, alexa, or similar) or some kind of chatbot.
   man-machine ai figure 1. man-machine ai. source: yishay carmiel.

   in a human-to-human interaction, the ai forms a bridge between two or
   more humans having conversations, interacting, or creating insights
   (see figure 2). an example might be an ai that listens to conference
   calls, then creates a summary of the call, following up with the
   relevant people.
   human-to-human ai figure 2. human-to-human ai. source: yishay carmiel.

machine perception and cognition

   in order to understand the challenges and the technologies behind
   conversational ai, we must examine the basic concepts in ai: machine
   perception and machine cognition.

   machine perception is the ability of a machine to analyze data in a
   manner similar to the way humans use their senses to relate to the
   world around them; in other words, essentially giving a machine human
   senses. a lot of the recent ai algorithms that employ computer cameras,
   such as id164 and recognition, fall into the category of
   machine perception   they concern the sense of vision. id103
   and profiling are machine perception technologies that use the sense of
   hearing.

   machine cognition is the reasoning on top of the metadata generated
   from the machine perception. machine cognition includes
   decision-making, id109, action taking, user intent, and more.
   generally, without machine cognition, there isn   t any impact from the
   outcome of the ai   s perception; machine perception delivers the proper
   metadata information for a decision and action.

   in conversational ai, machine perception includes all speech analysis
   technologies, such as recognition and profiling, and machine cognition
   includes all the language understanding-related technologies, which are
   part of natural language processing (nlp).

the evolution of id103

   the research and development of id103 occurred over three
   major time periods:

before 2011

   active research on id103 has been taking place for
   decades; in fact, even in the 1950s and 1960s, there were attempts to
   build id103 systems. however, before 2011 and the advent
   of deep learning, big data, and cloud computing, these solutions were
   far from sufficient for mass adoptions and commercial use. essentially,
   the algorithms were not good enough, there was not enough data on which
   to train the algorithms, and the lack of available high-performance
   computing prevented researchers from running more complicated
   experiments.

2011     2014

   the first major effect of deep learning occurred in 2011, when a group
   of researchers from microsoft, li deng, dong yu, and alex acero,
   together with geoffrey hinton and his student george dahl, created the
   first id103 system that is based on deep learning. the
   impact was immediate: more than 25% relative reduction in the error
   rate. this system was the starting point for massive developments and
   improvements in the fields. together with more data, available cloud
   computing, and adoption from big companies like apple (siri), amazon
   (alexa), and google, there were significant improvements in the
   performance and the products released to the market.

2015     today

   at the end of 2014, recurrent neural networks gained much more
   emphasis. together with id12, memory networks, and other
   techniques, this era comprised the third wave of progress. today,
   almost every type of algorithm or solution uses some type of neural
   model, and, in fact, almost all of research in speech has shifted
   toward deep learning.

recent advances of neural models in speech

   the past six years of id103 has created more breakthroughs
   then the previous 40+ years. this extraordinary recent advancement is
   due to neural networks. in order to understand the impact of deep
   learning and the part it plays, we need to understand first how speech
   recognition works.

   although id103 has been in active research for almost 50
   years, building machines that understand human speech is one of the
   most challenging problems in ai. it   s much harder then it may appear.
   id103 consists of well-defined tasks: given some kind
   human speech, try to convert the speech into words. however, the speech
   can be part of a noisy signal, requiring the work of extracting the
   speech from the noise and converting the relevant parts into meaningful
   words.

the basic building blocks of a id103 system

   id103 is basically divided into three main sections:

   the signal level: the aim of the signal level is to extract speech from
   the signal, enhance it (if needed), do proper pre-processing and
   cleaning, and feature extraction. this level is very similar to every
   machine learning task; in other words, given some data, we need to do
   proper data pre-processing and feature extraction.

   the acoustic level: the aim of the acoustic level is to classify the
   features into different sounds. put another way, sounds themselves do
   not provide a precise enough criteria, but rather sub-sounds that are
   sometimes called acoustic states.

   the language level: since we assume the sounds are produced by a human
   and have meaning, we take the sounds, combine them into words, and then
   take the words and combine them into sentences. these techniques in the
   language levels are usually different types of nlp techniques.

improvements from deep learning

   deep learning made a significant impact on the field of speech
   recognition. the impact is so far-reaching that even today, almost
   every solution in the field of id103 probably has one or
   more neural-based algorithms embedded within it.

   usually the evaluation of id103 systems is based on an
   industry standard called switchboard (swbd). swbd is a corpus of speech
   assembled from conversations improvised over the phone. swbd includes
   audio and human-level transcriptions.

   the evaluation of a id103 system is based on a metric
   called word error rate (wer). wer refers to how many words are
   misrecognized by the id103 system. figure 3 shows the
   improvement in wer from 2008     2017.
   improvement in word error rate figure 3. improvement in word error
   rate. source: yishay carmiel.

   from 2008 to 2011, the wer was in a steady state, around 23 to 24%;
   deep learning started to appear in 2011; and as a result, reduced the
   wer from 23.6% to 5.5%. this development was a game changer for speech
   recognition, an almost 77% relative improvement. now there are a wide
   range of applications, such as apple siri, amazon alexa, microsoft
   cortana, and google now. we also see a variety of appliances activated
   by id103, such as amazon echo and google home.

the secret sauce

   so, what improved the system so drastically? is there a technique that
   reduced the wer from 23.6% to 5.5%? unfortunately, there isn   t a single
   method. deep learning and id103 are so entangled that
   creating a state-of-the-art system involves a variety of different
   techniques and methods.

   at the signal level, for instance, there are different neural-based
   techniques to extract and enhance the speech from the signal itself
   (figure 4). also, there are techniques that replace the classical
   feature extraction methods with more complex and efficient neural-based
   methods.
   signal level analysis figure 4. signal level analysis. source: yishay
   carmiel.

   the acoustic and language levels also encapsulate a variety of
   different deep learning techniques, from acoustic state classification
   using different types of neural-based architectures to neural-based
   language models in the language level (see figure 5).
   acoustic and language-level analysis figure 5. acoustic and
   language-level analysis. source: yishay carmiel.

   creating a state-of-the-art system is not an easy task, and building it
   involves implementing and integrating all these different techniques
   into the system.

cutting edge research

   with so many recent breakthroughs in id103, it   s natural
   to ask, what   s next? three primary areas seem likely to be the main
   focus of research in the near future: algorithms, data, and
   scalability.

algorithms

   with the success of amazon echo and google home, many companies are
   releasing smart speakers and home devices that understand speech. these
   devices introduce a new problem, however: the user is often not talking
   closely into a microphone, as with a mobile phone, for instance.
   dealing with distant speech is a challenging problem that is being
   actively researched by a lot of groups. today, innovative deep learning
   and signal processing techniques can improve the quality of
   recognition.

   one of the most interesting topics of research today is finding new and
   exotic topologies of neural networks. we see some promising results
   both in language models and acoustic models that are being applied. two
   examples are grid-lstm in acoustic models and attention-based memory
   networks for language models.

data

   one of the key issues in a id103 system is the lack of
   real-life data. for example, it is hard to get high-quality data of
   distance speech. however, there is a lot of available data from other
   sources. one question is: can we create proper synthesizers to generate
   data for training? generating synthesized data and training systems
   based on that is getting good attention today.

   in order to train a id103 system, we need data sets that
   have both audio and transcriptions. manual transcription is tedious
   work and sometimes can cause problems on large amounts of audio. as a
   result, there is active research on semi-supervised training and
   building proper confidence measure for the recognizers.

scalability

   since deep learning is so entangled with id103, it
   consumes a non-trivial amount of cpu and memory. with users    massive
   adoption of id103 systems, building a cost-effective cloud
   solution is a challenging and important problem. there is ongoing
   research into how to reduce the computation cost and develop more
   efficient solutions. today, most id103 systems are
   cloud-based, and have two specific issues that must be solved: latency
   and continuous connectivity. latency is a key issue in devices that
   need an immediate response, such as robotics. and in a system that is
   listening all the time, continuous connectivity is a problem due to
   bandwidth cost. as a result, there is research toward edge speech
   recognition that preserves the quality of a cloud-based system.

solving the problems of id103

   in the recent years, there has been a quantum leap in speech
   recognition performance and adoption. how far are we from solving it
   completely? are we five years, or maybe 10 from declaring victory? the
   answer is, probably   but there are still challenging problems that will
   take some time to solve.

   the first problem is the issue of sensitivity to noise. a speech
   recognition system works pretty well from a close microphone and
   non-noisy environment; however, distant speech and noisy data degrades
   the system rapidly. the second problem that must be addressed is
   language expansion: there are roughly 7,000 languages in the world, and
   most id103 systems support around 80. scaling the systems
   poses a tremendous challenge. in addition, we lack data on many
   languages, and a id103 system is difficult to create with
   low data resources.

conclusion

   deep learning has made its mark on id103 and
   conversational ai. due to recent breakthroughs, we are truly on the
   edge of a revolution. the big question is, are we going to achieve a
   triumph, solve the challenges of id103, and begin to use
   it like any other commoditized technology? or is a new solution waiting
   to be discovered? after all, the recent advances in id103
   are just one piece of the puzzle: language understanding itself is a
   complex mystery   maybe an even greater one.

   [30]check out the "ai models and methods sessions" at the ai conference
   in new york, april 29 - may 2, 2018.
   article image: soundwave. (source: [31]pixabay).

   share
    1. [32]tweet
    2.
    3.
     __________________________________________________________________

   [33]yishay carmiel

[34]yishay carmiel

   yishay carmiel is the head of spoken labs, the strategic artificial
   intelligence and machine learning research arm of spoken
   communications. spoken labs develops and implements industry-leading
   deep learning and ai technologies for automatic id103
   (asr), natural language processing (nlp) and advanced voice data
   extraction. yishay and his team are currently working on bleeding-edge
   innovations that make the real-time customer experience a reality     at
   scale. yishay has nearly 20 years    experience as an algorithm scientist
   and te...
   [35]more
     __________________________________________________________________

   [36]bots landscape.

   [37]ai

[38]infographic: the bot platform ecosystem

   by [39]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [40]vertical forest, milan.

   [41]ai

[42]evolve ai

   by [43]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [44]welcome sign at o'reilly ai conference 2016

   [45]ai

[46]highlights from the o'reilly ai conference in new york 2016

   by [47]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [48]close up of uber's self driving car in pittsburgh.

   [49]ai

[50]how ai is propelling driverless cars, the future of surface transport

   by [51]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [52]our company
     * [53]teach/speak/write
     * [54]careers
     * [55]customer service
     * [56]contact us

site map

     * [57]ideas
     * [58]learning
     * [59]topics
     * [60]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [61]terms of service     [62]privacy policy     [63]editorial independence

   animal

   iframe: [64]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/bd311-yishay-carmiel
  28. https://pixabay.com/en/sound-wave-voice-listen-856770/
  29. https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/topic/2528?intcmp=il-data-confreg-lp-ainy18_20180302_new_site_deep_learning_revolutionizes_conversational_ai_top_cta
  30. https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/topic/2528?intcmp=il-data-confreg-lp-ainy18_20180302_new_site_deep_learning_revolutionizes_conversational_ai_end_cta
  31. https://pixabay.com/en/sound-wave-voice-listen-856770/
  32. https://twitter.com/share
  33. https://www.oreilly.com/people/bd311-yishay-carmiel
  34. https://www.oreilly.com/people/bd311-yishay-carmiel
  35. https://www.oreilly.com/people/bd311-yishay-carmiel
  36. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  37. https://www.oreilly.com/topics/ai
  38. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  39. https://www.oreilly.com/people/b1d73-jon-bruner
  40. https://www.oreilly.com/ideas/evolve-ai
  41. https://www.oreilly.com/topics/ai
  42. https://www.oreilly.com/ideas/evolve-ai
  43. https://www.oreilly.com/people/14d38-naveen-rao
  44. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  45. https://www.oreilly.com/topics/ai
  46. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  47. https://www.oreilly.com/people/0d2c1-mac-slocum
  48. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  49. https://www.oreilly.com/topics/ai
  50. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  51. https://www.oreilly.com/people/c7521-shahin-farshchi
  52. http://oreilly.com/about/
  53. http://oreilly.com/work-with-us.html
  54. http://oreilly.com/careers/
  55. http://shop.oreilly.com/category/customer-service.do
  56. http://shop.oreilly.com/category/customer-service.do
  57. https://www.oreilly.com/ideas
  58. https://www.oreilly.com/topics/oreilly-learning
  59. https://www.oreilly.com/topics
  60. https://www.oreilly.com/all
  61. http://oreilly.com/terms/
  62. http://oreilly.com/privacy.html
  63. http://www.oreilly.com/about/editorial_independence.html
  64. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  66. https://www.oreilly.com/
  67. https://www.facebook.com/oreilly/
  68. https://twitter.com/oreillymedia
  69. https://www.youtube.com/user/oreillymedia
  70. https://plus.google.com/+oreillymedia
  71. https://www.linkedin.com/company/oreilly-media
  72. https://www.oreilly.com/
