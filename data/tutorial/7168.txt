   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

can a deep neural network compose music?

   [16]go to the profile of justin svegliato
   [17]justin svegliato (button) blockedunblock (button) followfollowing
   jul 19, 2017

   when i first started grad school last september, i wanted to jump right
   into the deep learning craze as soon as i could. while i was working as
   a software developer in nyc, i kept hearing amazing things about deep
   learning: [18]deepface could recognize people   s faces just as well as
   you and i could, [19]alphago was destroying players in a game that
   originally seemed elusive to ai, and [20]gans were just starting to
   gain momentum. so when i learned that my department wanted to run
   [21]their first real deep learning class, i signed up immediately.
   especially since it would be taught by a big name in id161,
   [22]erik learned-miller, who runs the [23]id161 lab here. by
   the way, to give credit where credit is due, [24]the original course
   was developed by [25]fei-fei li, the director of the [26]stanford
   vision lab, and [27]andrej karpathy.

   fast forward past a few huge assignments that made us implement the
   nitty-gritty of a neural network framework including dense layers,
   dropout layers, batch norm layers, convolutional layers, recurrent
   layers, every activation layer you can think of, all of the optimizers
   available on the market, id26, and a bunch of applications
   in id161 like object recognition, image captioning, and image
   generation, [28]sam witty and i finally had to pick what we wanted to
   do for our final project. since sam is the keyboardist in a band called
   [29]white cassette, that meant he was infinitely qualified on
   everything related to music. with his expertise combined with my
   serious lack of musical talent, we decided to see if we could use deep
   learning to compose music just like a person could.
   [1*ybianyx-pgxvowctnynchw.jpeg]
   source: [30]music lover stock

   this led us to [31]a pretty sweet post in a blog called [32]hexahadria
   by [33]daniel johnson. daniel built a deep neural network that can
   compose classical piano pieces. with the help of that post, sam and i
   worked on our final project         which we regrettably named deep jammer
   without thinking         that can endlessly compose classical piano music by
   learning from a ton of music from bach, beethoven, mozart, and other
   famous composers. you can check out [34]our report, [35]our poster, and
   [36]our code if you want the details. we also used id21 to
   compose jazz, but we   ll save that for another post. while deep jammer
   is based on what daniel did, we refactored a lot of his [37]theano code
   (which he even admitted was messy), optimized the design and the
   hyperparameters, and implemented a cleaner version in [38]keras that
   works but just not as well. to cut to the chase, here   s a preview of a
   classical piano piece composed by deep jammer:

   iframe: [39]/media/aeec16b5c871c55220f2983993c119f7?postid=f89b6ba4978d

   enough backstory! let   s get on with the show. since the obvious answer
   to the title of this post is a resounding yes, let   s ask a different
   question now.

   how can a deep neural network compose music?

overview

   to give you the basic idea of what we did, let   s get an overview of our
   final project. take a look at this:
   [1*wvx52crdn9omttjn__cbtq.png]

   as you can see, the diagram just has three parts:
     * an input matrix that represents a slice of music.
     * an output matrix that represents a prediction of what to play next.
     * a neural network that learns patterns by listening to a lot of
       music.

   since this probably seems pretty vague to you right now, let   s slowly
   walk through the input matrix, the output matrix, and the neural
   network. it   s not all that bad.

input

   the input to our network is a slice of music that we convert to a
   particular representation. to be fancy, we call this a segment. it   s
   just a matrix with three dimensions:
     * time: the segment has 128 time steps. as an analogy, every time
       step is like 100 milliseconds of a song. similar to a piece of
       sheet music, this will help us learn how a segment changes over
       time.
     * note: each time step has 88 notes. we have 88 of them because there
       are 88 keys on a piano. again, if we think about sheet music, this
       represents all of the lines and spaces on the staff.
     * attribute: each note has 78 attributes that describe the state of
       that note, which we   ll cover right after this. basically, for each
       note, we need to know a few things about it.

   since talking about a three-dimensional matrix might be confusing,
   let   s put everything together now. first, the input is just a segment.
   second, each segment has 128 time steps to represent time. third, each
   time step has 88 notes for each key on a piano. fourth, each note has
   78 attributes that summarize the state of that note. in a few words,
   the input         which is the three-dimensional matrix that we called a
   segment         just describes a slice of music.

   up until this point we haven   t talked about what the attributes of each
   note look like. so what are they? take a look below:
     * position: the position of the key on a piano of the corresponding
       note. this ranges from 1 to 88 since there are 88 keys on a piano.
       more importantly, we need this since the network has to understand
       the difference between notes of the same pitch like when we have
       two as in different octaves.
     * pitch class: an array where each element corresponds to a pitch (c,
       d, e, f, g, a, and b along with the flats and sharps). the pitch
       associated with the corresponding note is set to true. every other
       pitch is set to false.
     * vicinity: an array that contains the state of the notes that
       neighbor the corresponding note. we store two values for every
       neighboring note: we store (1) true if it was just played (and
       false otherwise) and (2) true if it was just held down (and, again,
       false otherwise). to keep it simple, we only capture the
       neighboring notes in the upper and lower octave of the
       corresponding note.
     * beat: the location of the corresponding note in its measure. for
       example, a note is associated with an integer ranging from 0 to 16
       since our time resolution is at 16th notes. however, this could
       change with the resolution of the classical piano pieces in the
       training set, which we   ll chat about.

output

   the output from our network predicts what should be played at the very
   next time step after the input segment. again, to be fancy, we call
   this a prediction. it   s just matrix with two dimensions:
     * note: the prediction has 88 notes for each key of a piano like the
       input.
     * attributes: each note has 2 attributes that describe the predicted
       state of that note.

   so what are the attributes? check them out below:
     * play id203: the id203 of the note being played at the
       next time step.
     * articulate id203: the id203 of the note being held at
       the next time step.

architecture

   now that we understand the input and the output of our network, let   s
   take a look at the role of each layer. however, before we do that, take
   a look at a simplified diagram that gives a rough but nice overview of
   what   s going on:
   [1*vk_bwilrng7a-mmfy8abla.png]

   the layers look and sound way more confusing than they actually are.
   it   s pretty simple if we break each one down. by the way, if you don   t
   know what an lstm is, it   s just a special type of an id56. check out
   [40]this post on [41]colah   s blog if you want to learn more. in any
   case, check out each layer:
     * time layer: an lstm layer that captures the temporal patterns of
       music. we treat the time dimension from before as the temporal
       dimension of the lstm. this means that the network learns how the
       attributes of each note changes with time. for example, you can
       think of arpeggios and scales here.
     * transpose layer: a processing layer that transposes the time
       dimension and the note dimension. why do we flip these two
       dimensions? this sets the note dimension as the new temporal
       dimension of the lstm, which prepares it for the next layer.
     * note layer: an lstm layer that captures the spatial (or note)
       patterns of music. we treat the note dimension from before as the
       temporal dimension of the lstm. this enables the network to learn
       how the attributes vary with the notes. the best example is a chord
       or a group of notes that are played together. (i   m with you there.
       this is a weird idea to think about.)
     * dense layer: a fully-connected layer that reduces the
       high-dimensional output from the note layer to an unnormalized
       prediction. this layer summarizes what the time layer and the note
       layer have learned.
     * activation layer: a layer that normalizes         or squashes         each
       element of the unnormalized prediction from the dense layer. we use
       the sigmoid activation function to do this:

   [1*kykb86z_cfkx2e35jp1prw.png]

   note that there are several things we can adjust in the architecture
   like the the number of time, note, and dense layers and the number of
   nodes in each layer. we can also add dropout layers if we   d like as
   well. i   ll gloss over this since they   re implementation details.

training

   after walking through everything about our network, it   s time to train
   it. it   s really not any different from how you train any network.
   here   s what we did:
    1. we collected 325 classical music pieces from [42]here. they   re midi
       files. if you don   t know what that is, i wouldn   t sweat it. midi is
       just a format that encodes music as a sequence of events. let   s
       skip the details for the sake of brevity.
    2. we made a training set once we got the midi files. to do this, we
       sampled randomly from the midi files and converted them into a
       representation that fit with our network. in our training set, a
       training example is a pair of two matrices (i.e., the input and
       output matrices that we already talked about):
       (a) the features are just a segment (the three-dimensional matrix
       with the time, note, and attribute dimensions).
       (b) the label is a prediction (the two-dimensional matrix with the
       note and attribute dimensions).
    3. we designed a custom id168 that we   ll omit since we skipped
       too much notation. the paper goes over that if you   re interested.
       along with the id168, we derived its gradient for
       id26 too.
    4. we trained our network using [43]adadelta, which is an adaptive
       learning rate id119 method, for 5000 epochs with a batch
       size of 5 segments.

   if you   re interested, you can take a look at a nice plot of the
   training loss below. there   s nothing too surprising here. it decreases
   like most training loss curves.
   [1*xgzfc1jq2ffrd7u6u34jzq.png]

   every 100 epochs, we composed music with our network to see what the
   music looks like as the training loss continued to decrease. take a
   look at the visualization below. note that the time axis represents
   time steps and the notes axis represents the 88 piano keys. you can see
   how the music gets better and better as the network trains more and
   more. by the very end, you can definitely see arpeggios, scales, and
   chords.
   [1*xfkattyzfrtquz0cdg5vaa.png]

generation

   now that we   ve trained our network, let   s compose some classical piano
   music. to do this, we repeated the following steps over and over again:
    1. we selected a random segment from the training set.
    2. we fed that segment into the network as input.
    3. we got a prediction from the network as output.
    4. we converted the prediction into a new segment.
    5. jump to step 2.

   to put matter simply, we fed in a segment, got back a prediction, and
   fed the prediction back into the network. if we keep track of each
   prediction, we can string together a classical piano piece that doesn   t
   sound too bad.

results

   to evaluate the classical piano music composed by our network, we sent
   out a survey where participants had to rate a bunch of different pieces
   out of 10. for good measure, we mixed in a real classical piano piece
   from a famous composer unbeknownst to the participants. since [44]our
   poster is outdated, here are a few up-to-date interesting stats:
     * our network   s best piece scored an average rating of 7.5
     * the real piece scored an average rating of 7.9
     * strangely, 3 people didn   t believe that a human composed the real
       piece
     * 12 people liked our network   s best piece more than the real piece

   now that we   re at the end, it   s time for some classical piano music
   composed by our network! i hope you enjoy the show. stay awhile and
   listen:

   iframe: [45]/media/aeec16b5c871c55220f2983993c119f7?postid=f89b6ba4978d

   iframe: [46]/media/1eb7f6603d00e7ae419e7a0dc02f9e4a?postid=f89b6ba4978d

   iframe: [47]/media/c955b7051d26a9f63581575ec9a0198a?postid=f89b6ba4978d

   iframe: [48]/media/798dedb7f6ea191c7403b9fa995f4be2?postid=f89b6ba4978d

   iframe: [49]/media/530818361bd7f36ee53929dbf3ebcfb4?postid=f89b6ba4978d

     * [50]machine learning
     * [51]deep learning
     * [52]neural networks
     * [53]artificial intelligence
     * [54]towards data science

   (button)
   (button)
   (button) 143 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [55]go to the profile of justin svegliato

[56]justin svegliato

   cs phd student at umass amherst. former software developer on wall
   street. i like to build intelligent systems that reason and learn.

     (button) follow
   [57]towards data science

[58]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 143
     * (button)
     *
     *

   [59]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [60]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f89b6ba4978d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/can-a-deep-neural-network-compose-music-f89b6ba4978d&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/can-a-deep-neural-network-compose-music-f89b6ba4978d&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_fyxxag3eehwu---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@justinsvegliato?source=post_header_lockup
  17. https://towardsdatascience.com/@justinsvegliato
  18. http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/taigman_deepface_closing_the_2014_cvpr_paper.pdf
  19. https://storage.googleapis.com/deepmind-media/alphago/alphagonaturepaper.pdf
  20. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
  21. https://compsci697l.github.io/
  22. http://people.cs.umass.edu/~elm/
  23. http://vis-www.cs.umass.edu/
  24. http://cs231n.stanford.edu/
  25. http://vision.stanford.edu/feifeili/
  26. http://vision.stanford.edu/
  27. http://cs.stanford.edu/people/karpathy/
  28. https://samwitty.github.io/
  29. http://www.whitecassette.com/
  30. http://music-lover-stock.deviantart.com/
  31. http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
  32. http://www.hexahedria.com/
  33. http://www.hexahedria.com/about
  34. https://www.justinsvegliato.com/pdf/deep-jammer-report.pdf
  35. https://www.justinsvegliato.com/pdf/deep-jammer-poster.pdf
  36. https://github.com/justinsvegliato/deep-jammer
  37. http://deeplearning.net/software/theano/
  38. https://keras.io/
  39. https://towardsdatascience.com/media/aeec16b5c871c55220f2983993c119f7?postid=f89b6ba4978d
  40. http://colah.github.io/posts/2015-08-understanding-lstms/
  41. http://colah.github.io/
  42. http://www.piano-midi.de/
  43. https://arxiv.org/pdf/1212.5701.pdf
  44. https://www.justinsvegliato.com/pdf/deep-jammer-poster.pdf
  45. https://towardsdatascience.com/media/aeec16b5c871c55220f2983993c119f7?postid=f89b6ba4978d
  46. https://towardsdatascience.com/media/1eb7f6603d00e7ae419e7a0dc02f9e4a?postid=f89b6ba4978d
  47. https://towardsdatascience.com/media/c955b7051d26a9f63581575ec9a0198a?postid=f89b6ba4978d
  48. https://towardsdatascience.com/media/798dedb7f6ea191c7403b9fa995f4be2?postid=f89b6ba4978d
  49. https://towardsdatascience.com/media/530818361bd7f36ee53929dbf3ebcfb4?postid=f89b6ba4978d
  50. https://towardsdatascience.com/tagged/machine-learning?source=post
  51. https://towardsdatascience.com/tagged/deep-learning?source=post
  52. https://towardsdatascience.com/tagged/neural-networks?source=post
  53. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  54. https://towardsdatascience.com/tagged/towards-data-science?source=post
  55. https://towardsdatascience.com/@justinsvegliato?source=footer_card
  56. https://towardsdatascience.com/@justinsvegliato
  57. https://towardsdatascience.com/?source=footer_card
  58. https://towardsdatascience.com/?source=footer_card
  59. https://towardsdatascience.com/
  60. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  62. https://medium.com/p/f89b6ba4978d/share/twitter
  63. https://medium.com/p/f89b6ba4978d/share/facebook
  64. https://medium.com/p/f89b6ba4978d/share/twitter
  65. https://medium.com/p/f89b6ba4978d/share/facebook
