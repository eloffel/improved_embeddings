   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets   
   building a wikipedia text corpus for natural language processing
   comments feed [5]taming the python visualization jungle, nov 29 webinar
   [6]cartoon: thanksgiving, big data, & turkey data science.

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2017    [28]nov    [29]tutorials,
   overviews    building a wikipedia text corpus for natural language
   processing ( [30]17:n45 )

building a wikipedia text corpus for natural language processing

   [31][prv.gif] previous post
   [32]next post [nxt.gif]


   tags: [33]datasets, [34]natural language processing, [35]nlp, [36]text
   mining, [37]wikidata, [38]wikipedia

   wikipedia is a rich source of well-organized textual data, and a vast
   collection of knowledge. what we will do here is build a corpus from
   the set of english wikipedia articles, which is freely and conveniently
   available online.
     __________________________________________________________________

   by [39]matthew mayo, kdnuggets.

   one of the first things required for natural language processing (nlp)
   tasks is a corpus. in linguistics and nlp, corpus (literally latin for
   body) refers to a collection of texts. such collections may be formed
   of a single language of texts, or can span multiple languages -- there
   are numerous reasons for which multilingual corpora (the plural of
   corpus) may be useful. corpora may also consist of themed texts
   (historical, biblical, etc.). corpora are generally solely used for
   statistical linguistic analysis and hypothesis testing.

   the good thing is that the internet is filled with text, and in many
   cases this text is collected and well oganized, even if it requires
   some finessing into a more usable, precisely-defined format. wikipedia,
   in particular, is a rich source of well-organized textual data. it's
   also a vast collection of knowledge, and the unhampered mind can dream
   up all sorts of uses for just such a body of text.

   what we will do here is build a corpus from the set of english
   wikipedia articles, which is freely and conveniently available online.

   wikipedia


   install gensim

   in order to easily build a text corpus void of the wikipedia article
   markup, we will use [40]gensim, a [41]id96 library for
   python. specifically, the gensim.corpora.wikicorpus.wikicorpus class is
   made just for this task:

     construct a corpus from a wikipedia (or other mediawiki-based)
     database dump.

   in order to properly progress through the following steps, you will
   need to have [42]gensim installed. it's a simple enough process; using
   pip:
$ pip install gensim

   moving on...


   download the wikipedia dump file

   a wikipedia dump file is also required for this procedure, quite
   obviously. the latest such files can be found [43]here.

   a warning: the latest such english wikipedia database dump file is ~14
   gb in size, so downloading, storing, and processing said file is not
   exactly trivial.

   the file i aquired and used for this task was
   enwiki-latest-pages-articles.xml.bz2. go ahead and download it or
   another similar file to use in the next steps.


   make the corpus

   i wrote a simple python script (with inspiration from [44]here) to
   build the corpus by stripping all wikipedia markup from the articles,
   using gensim. you can read up on the wikicorpus class (mentioned above)
   [45]here.

   the code is pretty straightforward: the wikipedia dump file is opened
   and read article by article using the get_texts() method of the
   wikicorpus class, all of which are ultimately written to a single text
   file. both the wikipedia dump file and the resulting corpus file must
   be specified on the command line.

$ python make_wiki_corpus enwiki-latest-pages-articles.xml.bz2 wiki_en.txt

processed 10000 articles
processed 20000 articles
processed 30000 articles
processed 40000 articles
processed 50000 articles
processed 60000 articles
processed 70000 articles
processed 80000 articles
processed 90000 articles
processed 100000 articles
...

   after several hours, the above code leaves me with a corpus file named
   wiki_en.txt.


   check the corpus

   a second script then checks the corpus text file we just built.

   now, keep in mind that this large wikipedia dump file then resulted in
   a very large corpus file. given its enormous size, you may have
   dificulty reading the full file into memory at one time.

   this script, then, starts by reading 50 lines -- which equates to 50
   full articles -- from the text file and outputting them to the
   terminal, after which you can press a key to output another 50, or type
   'stop' to quit. if you do stop, the script then proceeds to load the
   entire file into memory. which could be a problem for you. you can,
   however, verify the text by batches of lines, in order to satisfy your
   curiousity that something good happened as a result of running the
   first script.

   if you are planning on working on such a large text file, you may need
   some workarounds for its large size in comparison to your machine's
   memory.

   the corpus file must be specified at the command line to execute.
$ python check_wiki_corpus.py wiki_en.txt

...
best loved patriotic songs harperresource external links mp and realaudio
recordings available at the united states library of congress words sheet
music midi file at the cyber hymnal america the beautiful park in colorado
springs named for katharine lee bates words archival collection of america
the beautiful lantern slides from the another free sheet music

>>> type 'stop' to quit or hit enter key for more <<<

   and that's it. some simple code to accomplish what gensim makes a
   simple task. now that you are armed with an ample corpus, the natural
   language processing world is your oyster. time for something fun.


   related:
     * [46]a framework for approaching textual data science tasks
     * [47]natural language processing key terms, explained
     * [48]5 free resources for getting started with deep learning for
       natural language processing
     __________________________________________________________________

   [49][prv.gif] previous post
   [50]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [51]another 10 free must-read books for machine learning and data
       science
    2. [52]9 must-have skills you need to become a data scientist, updated
    3. [53]who is a typical data scientist in 2019?
    4. [54]the pareto principle for data scientists
    5. [55]what no one will tell you about data science job applications
    6. [56]19 inspiring women in ai, big data, data science, machine
       learning
    7. [57]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [58]id158s optimization using genetic algorithm
       with python
    2. [59]who is a typical data scientist in 2019?
    3. [60]8 reasons why you should get a microsoft azure certification
    4. [61]the pareto principle for data scientists
    5. [62]r vs python for data visualization
    6. [63]how to work in data science, ai, big data
    7. [64]the deep learning toolset     an overview

[65]latest news

     * [66]download your datax guide to ai in marketing
     * [67]kdnuggets offer: save 20% on strata in london
     * [68]training a champion: building deep neural nets for big ...
     * [69]building a recommender system
     * [70]predict age and gender using convolutional neural netwo...
     * [71]top tweets, mar 27     apr 02: here is a great ex...

   [72]kdnuggets home    [73]news    [74]2017    [75]nov    [76]tutorials,
   overviews    building a wikipedia text corpus for natural language
   processing ( [77]17:n45 )
      2019 kdnuggets. [78]about kdnuggets.  [79]privacy policy. [80]terms
   of service

   [81]subscribe to kdnuggets news
   [82][tw_c48.png] [83]facebook [84]linkedin
   x

   [envelope.png] [85]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html/feed
   5. https://www.kdnuggets.com/2017/11/anaconda-taming-python-visualization-jungle.html
   6. https://www.kdnuggets.com/2017/11/cartoon-thanksgiving-turkey-data-science.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2017/index.html
  28. https://www.kdnuggets.com/2017/11/index.html
  29. https://www.kdnuggets.com/2017/11/tutorials.html
  30. https://www.kdnuggets.com/2017/n45.html
  31. https://www.kdnuggets.com/2017/11/anaconda-taming-python-visualization-jungle.html
  32. https://www.kdnuggets.com/2017/11/cartoon-thanksgiving-turkey-data-science.html
  33. https://www.kdnuggets.com/tag/datasets
  34. https://www.kdnuggets.com/tag/natural-language-processing
  35. https://www.kdnuggets.com/tag/nlp
  36. https://www.kdnuggets.com/tag/text-mining
  37. https://www.kdnuggets.com/tag/wikidata
  38. https://www.kdnuggets.com/tag/wikipedia
  39. https://www.kdnuggets.com/author/matt-mayo
  40. https://radimrehurek.com/gensim/index.html
  41. https://en.wikipedia.org/wiki/topic_model
  42. https://radimrehurek.com/gensim/install.html
  43. https://dumps.wikimedia.org/enwiki/latest/
  44. https://github.com/panyang/wikipedia_id97/blob/master/v1/process_wiki.py
  45. https://radimrehurek.com/gensim/install.html
  46. https://www.kdnuggets.com/2017/11/framework-approaching-textual-data-tasks.html
  47. https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html
  48. https://www.kdnuggets.com/2017/07/5-free-resources-getting-started-deep-learning-nlp.html
  49. https://www.kdnuggets.com/2017/11/anaconda-taming-python-visualization-jungle.html
  50. https://www.kdnuggets.com/2017/11/cartoon-thanksgiving-turkey-data-science.html
  51. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  52. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  53. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  54. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  55. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  56. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  57. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  58. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  59. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  60. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  61. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  62. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  63. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  64. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  65. https://www.kdnuggets.com/news/index.html
  66. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  67. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  68. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  69. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  70. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  71. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  72. https://www.kdnuggets.com/
  73. https://www.kdnuggets.com/news/index.html
  74. https://www.kdnuggets.com/2017/index.html
  75. https://www.kdnuggets.com/2017/11/index.html
  76. https://www.kdnuggets.com/2017/11/tutorials.html
  77. https://www.kdnuggets.com/2017/n45.html
  78. https://www.kdnuggets.com/about/index.html
  79. https://www.kdnuggets.com/news/privacy-policy.html
  80. https://www.kdnuggets.com/terms-of-service.html
  81. https://www.kdnuggets.com/news/subscribe.html
  82. https://twitter.com/kdnuggets
  83. https://facebook.com/kdnuggets
  84. https://www.linkedin.com/groups/54257
  85. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  87. https://www.kdnuggets.com/
  88. https://www.kdnuggets.com/
