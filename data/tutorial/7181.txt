   #[1]the morning paper    feed [2]the morning paper    comments feed
   [3]the morning paper    opening the black box of deep neural networks
   via information     part ii comments feed [4]opening the black box of
   deep neural networks via information     part i [5]mastering the game of
   go without human knowledge [6]alternate [7]alternate [8]the morning
   paper [9]wordpress.com

   [10]skip to content

   [11]the morning paper

an interesting/influential/important paper from the world of cs every weekday
morning, as selected by adrian colyer

     * [12]home
     * [13]about
     * [14]infoq qr editions
     * [15]subscribe
     * [16]privacy

opening the black box of deep neural networks via information     part ii

   november 16, 2017
   tags: [17]deep learning

   [18]opening the black box of deep neural networks via information
   schwartz-viz & tishby, icri-ci 2017

   yesterday we looked at the [19]id205 of deep learning,
   today in part ii we   ll be diving into experiments using that
   id205 to try and understand what is going on inside of
   dnns. the experiments are done on a network with 7 fully connected
   hidden layers, and widths 12-10-7-5-4-3-2 neurons.

   the network is trained using sgd and cross-id178 id168, but
   no other explicit id173. the id180 are
   hyperbolic tangent in all layers but the final one, where a sigmoid
   function is used. the task is to classify 4096 distinct patterns of the
   input variable x into one of two output classes. the problem is
   constructed such that i(x;y) \approx 0.99 bits.

   output activations are discretised into 30 bins, and the markov chain y
   \rightarrow x \rightarrow t_i is used to calculate the joint
   distributions for every hidden layer t_i . using the joint
   distributions, it is then possible to calculate the encoder and decoder
   mutual information, i(x;t) and i(t;y) , for each hidden layer in the
   network. the calculations were repeated 50 times over, with different
   random initialisation of network works and different random selections
   of training samples.

a phase shift during training

   we can plot the mutual information retained in each layer on a graph.
   the following chart shows the situation before any training has been
   done (i.e., random initial weights of each of the 50 generated
   networks).

   the different colours in the chart represent the different hidden
   layers (and there are multiple points of each colour because we   re
   looking at 50 different runs all plotted together). on the x-axis is
   i(x;t) , so as we move to the right on the x-axis, the amount of mutual
   information between the hidden layer and the input x increases. on the
   y-axis is i(t;y) , so as we move up on the y-axis, the amount of mutual
   information between the hidden layer and the output y increases.

   i   m used to thinking of progressing through the network layers from
   left to right, so it took a few moments for it to sink in that it   s the
   lowest layer which appears in the top-right of this plot (maintains the
   most mutual information), and the top-most layer which appears in the
   bottom-left (has retained almost no mutual information before any
   training). so the information path being followed goes from the
   top-right corner to the bottom-left traveling down the slope.

   part-way through training (here at 400 epochs), we can see that much
   more i(t;y) information is being retained through the layers .

   after 9000 epochs we   re starting to see a pretty flat information path,
   which means that we   re retaining mutual information needed to predict y
   all the way through the network layers.

   something else happens during training though, and the best
   demonstration of it is to be found in watching this short video.

   iframe:
   [20]https://www.youtube.com/embed/q45lpv9rev0?version=3&rel=1&fs=1&auto
   hide=2&showsearch=0&showinfo=1&iv_load_policy=1&wmode=transparent

   what you should hopefully have noticed is that early on the points
   shoot up and to the right, as the hidden layers learn to retain more
   mutual information both with the input and also as needed to predict
   the output. but after a while, a phase shift occurs, and points move
   more slowly up and to the left.

   the following chart shows the average layer trajectories when training
   with 85% of the data, giving a static representation of the same
   phenomenon. the green line shows the phase change point, and the yellow
   points are the final positions.

     the two optimization phases are clearly visible in all cases. during
     the fast     empirical error minimization (erm)     phase, which takes a
     few hundred epochs, the layers increase information on the labels
     (increase i_y ) while preserving the dpi order (lower layers have
     higher information). in the second and much longer training phase
     the layer   s information on the input, i_x , decreases and the layers
     lose irrelevant information until convergence (the yellow points).
     we call this phase the representation compression phase.

   you can also see the phase change clearly (the vertical grey line) when
   looking at the normalized means and standard deviations of the layer   s
   stochastic gradients during the optimization process:

     we claim that these distinct sg phases (grey line in the figure
     above), correspond and explain the erm and compression phases we
     observe in the information plane.

the drift phase

   the first, erm, phase is a drift phase. here the gradient means are
   much larger than their standard deviations, indicating small gradient
   stochasticity (high snr). the increase in i_y is what we expect to see
   from cross-id178 loss minimisation.

the diffusion phase

   the existence of the compression phase is more surprising. in this
   phase the gradient means are very small compared to the batch-to-batch
   fluctuations, and the gradients behave like gaussian noise with very
   small means for each layer (low snr). this is a diffusion phase.

        the diffusion phase mostly adds random noise to the weights, and
     they evolve like [21]weiner processes [think brownian motion], under
     the training error or label information constraint.

   this has the effect of maximising id178 of the weights distribution
   under the training error constraint. this in turn minimises the mutual
   information i(x;t_i)     in other words, we   re discarding information in
   x that is irrelevant to the task at hand. the fancy name for this
   process of id178 maximisation by adding noise is stochastic
   relaxation.

   compression by diffusion is exponential in the number of time steps it
   takes (optimisation epochs) to achieve a certain compression level (and
   hence why you see the points move more slowly during this phase).

   one interesting consequence of this phase is the randomised nature of
   the final weights of the dnn:

     this indicates that there is a huge number of different networks
     with essentially optimal performance, and attempts to interpret
     single weights or even single neurons in such networks can be
     meaningless.

convergence to the information bottleneck bound

   as can be clearly seen in the charts, different layers converge to
   different points in the information plane, and this is related to the
   critical slowing down of the stochastic relaxation process near the
   phase transitions on the information bottleneck curve.

   recall from yesterday that the information curve is a line of optimal
   representations separating the achievable and unachievable regions in
   the information plane. testing the information values in each hidden
   layer and plotting them against the information curve shows that the
   layers do indeed approach this bound.

     how exactly the dnn neurons capture the optimal ib representations
     is another interesting issue to be discussed elsewhere, but there
     are clearly many different layers that correspond to the same ib
     representation.

why does deep work so well?

   to understand the benefits of more layers the team trained 6 different
   architectures with 1 to 6 hidden layers and did 50 runs of each as
   before. the following plots show how the information paths evolved
   during training for each of the different network depths:

   from this we can learn four very interesting things:
    1. adding hidden layers dramatically reduces the number of training
       epochs for good generalization. one hidden layer was unable to
       achieve good i_y values even after 10^4 iterations, but six layers
       can achieve full relevant information capture after just 400.
    2. the compression phase of each layer is shorter when it starts from
       a previous compressed layer. for example, the convergence is much
       slower with 4 layers than with 5 or 6.
    3. the compression is faster for the deeper (narrower and closer to
       the output) layers. in the diffusion phase the top layers compress
       first and    pull    the lower layers after them. adding more layers
       seems to add intermediate representations which accelerate
       compression.
    4. even wide layers eventually compress in the diffusion phase. adding
       extra width does not help.

training sample size

   training sample size seems to have the biggest impact on what happens
   during the diffusion phase. here are three charts showing the
   information paths when training with 5% (left), 45% (middle), and 85%
   (right of the data):

what does all this tell us about future directions for dl?

     we are currently working on new learning algorithms that utilize the
     claimed ib optimality of the layers. we argue that sgd seems an
     overkill during the diffusion phase, which consumes most of the
     training epochs, and the much simpler optimization algorithms, such
     as monte-carlo relaxations, can be more efficient.

   furthermore, the analytic connections between encoder and decoder
   distributions can be exploited during training: combining the ib
   iterations with stochastic relaxation methods may significantly boost
   dnn training.

     to conclude, it seems fair to say, based on our experiments and
     analysis, that deep learning with dnn are in essence learning
     algorithms that effectively find efficient representations that are
     approximate minimal statistics in the ib sense. if our findings hold
     for general networks and tasks, the compression of the sgd and the
     convergence of the layers to the ib bound can explain the phenomenal
     success of deep learning.

share this:

     * [22]twitter
     * [23]linkedin
     * [24]email
     * [25]print
     *

like this:

   like loading...

related

   from     [26]uncategorized
   [27]    opening the black box of deep neural networks via information    
   part i
   [28]mastering the game of go without human knowledge    
   5 comments [29]leave one    
    1. [30]david banville [31]permalink
       november 16, 2017 6:37 pm
       just to make sure i understand the information curve: green
       represents shallow layers and orange represent deep layers.
       so the deeper we go into an untrained network the less information
       we maintain about both x and y (the part that is contained is x at
       least). this is due to the random noise introduced by
       initialization of the weights which shuffles everything up. so the
       whole goal of the optimization process is to maintain as much
       information about y while at the same time reject as much
       information about x (spurious info). just like a distillation
       process in chemistry.
       why is random initilization so important? because starting with
       deterministic weights the points would be all clustered up on the
       top right part of the information graph. the optimization would
       take forever to converge because all it would try to do is to
       reject the gigantic amount of information about x contained in x
       (it would start in phase ii so to speak). while if we start with
       randomized weights, than we take a shortcut by removing all the
       info from x before starting to optimize (of course we also lose the
       info about y, but that   s part of the game).
       makes sense to me anyway ^^
       [32]reply
          + [33]anmol chachra [34]permalink
            november 25, 2017 6:04 am
            i did not understand which layer retains most information (say
            for a network of 6 layers) after training for like 100 epochs
            (erm phase). before training, initial layers were the more
            informative (right!)?
            [35]reply
               o [36]adriancolyer [37]permalink*
                 november 28, 2017 6:25 am
                 i think of it like this: before training, each layer is
                 in essence randomly scrambling its inputs. after the
                 first layer we   ve scrambled them up a little bit, but
                 we   ve still retained a reasonable amount of information.
                 as we go through successive scramblings (also think of
                 e.g., shuffling a deck of cards that was originally
                 sorted) less and less of that information gets retained.
    2. ninja [38]permalink
       june 22, 2018 1:47 am
       i don   t understand why the initial layers have the highest mutual
       information with both input and output even when they are randomly
       initialized ? can somebody throw more light please ?
       [39]reply

trackbacks

    1. [40]on the information bottleneck theory of deep learning | the
       morning paper

leave a reply [41]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [42]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [43]log out /
   [44]change )
   google photo

   you are commenting using your google account. ( [45]log out /
   [46]change )
   twitter picture

   you are commenting using your twitter account. ( [47]log out /
   [48]change )
   facebook photo

   you are commenting using your facebook account. ( [49]log out /
   [50]change )
   [51]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

   this site uses akismet to reduce spam. [52]learn how your comment data
   is processed.
     * subscribe

                        [53][mail-new-icon.png?w=600]
      never miss an issue! the morning paper delivered straight to your
                                   inbox.
     * search
       ____________________
     * archives archives [select month________]
     * most read in the last few days
          + [54]ginseng: keeping secrets in registers when you distrust
            the operating system
          + [55]amazon aurora: design considerations for high throughput
            cloud-native id208
          + [56]establishing software root of trust unconditionally
          + [57]amazon aurora: on avoiding distributed consensus for i/os,
            commits, and membership changes
          + [58]the amazing power of word vectors
          + [59]calvin: fast distributed transactions for partitioned
            database systems
          + [60]on the criteria to be used in decomposing systems into
            modules
          + [61]neural ordinary differential equations
          + [62]applying the universal scalability law to organisations
          + [63]programming paradigms for dummies: what every programmer
            should know
     * rss
          + [64]rss - posts
          + [65]rss - comments
     * live on twitter[66]my tweets

   [67]blog at wordpress.com.

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [68]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [69]likes-master

   %d bloggers like this:

references

   visible links
   1. https://blog.acolyer.org/feed/
   2. https://blog.acolyer.org/comments/feed/
   3. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/feed/
   4. https://blog.acolyer.org/2017/11/15/opening-the-black-box-of-deep-neural-networks-via-information-part-i/
   5. https://blog.acolyer.org/2017/11/17/mastering-the-game-of-go-without-human-knowledge/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/&for=wpcom-auto-discovery
   8. https://blog.acolyer.org/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#content
  11. https://blog.acolyer.org/
  12. https://blog.acolyer.org/
  13. https://blog.acolyer.org/about/
  14. https://blog.acolyer.org/infoq-quarterly-review-editions/
  15. https://blog.acolyer.org/email-subs/
  16. https://blog.acolyer.org/privacy/
  17. https://blog.acolyer.org/tag/deep-learning/
  18. https://arxiv.org/abs/1703.00810
  19. https://blog.acolyer.org/2017/11/15/opening-the-black-box-of-deep-neural-networks-via-information-part-i
  20. https://www.youtube.com/embed/q45lpv9rev0?version=3&rel=1&fs=1&autohide=2&showsearch=0&showinfo=1&iv_load_policy=1&wmode=transparent
  21. https://en.m.wikipedia.org/wiki/wiener_process
  22. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/?share=twitter
  23. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/?share=linkedin
  24. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/?share=email
  25. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#print
  26. https://blog.acolyer.org/category/uncategorized/
  27. https://blog.acolyer.org/2017/11/15/opening-the-black-box-of-deep-neural-networks-via-information-part-i/
  28. https://blog.acolyer.org/2017/11/17/mastering-the-game-of-go-without-human-knowledge/
  29. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#respond
  30. https://www.facebook.com/app_scoped_user_id/10154918425867321/
  31. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#comment-18148
  32. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/?replytocom=18148#respond
  33. http://codeitplease.wordpress.com/
  34. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#comment-18547
  35. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/?replytocom=18547#respond
  36. https://adriancolyer.wordpress.com/
  37. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#comment-18672
  38. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#comment-28491
  39. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/?replytocom=28491#respond
  40. https://blog.acolyer.org/2017/11/24/on-the-information-bottleneck-theory-of-deep-learning/
  41. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#respond
  42. https://gravatar.com/site/signup/
  43. javascript:highlandercomments.doexternallogout( 'wordpress' );
  44. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/
  45. javascript:highlandercomments.doexternallogout( 'googleplus' );
  46. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/
  47. javascript:highlandercomments.doexternallogout( 'twitter' );
  48. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/
  49. javascript:highlandercomments.doexternallogout( 'facebook' );
  50. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/
  51. javascript:highlandercomments.cancelexternalwindow();
  52. https://akismet.com/privacy/
  53. http://eepurl.com/bbzpm9
  54. https://blog.acolyer.org/2019/04/05/ginseng-keeping-secrets-in-registers-when-you-distrust-the-operating-system/
  55. https://blog.acolyer.org/2019/03/25/amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases/
  56. https://blog.acolyer.org/2019/04/03/establishing-software-root-of-trust-unconditionally/
  57. https://blog.acolyer.org/2019/03/27/amazon-aurora-on-avoiding-distributed-consensus-for-i-os-commits-and-membership-changes/
  58. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
  59. https://blog.acolyer.org/2019/03/29/calvin-fast-distributed-transactions-for-partitioned-database-systems/
  60. https://blog.acolyer.org/2016/09/05/on-the-criteria-to-be-used-in-decomposing-systems-into-modules/
  61. https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/
  62. https://blog.acolyer.org/2015/04/29/applying-the-universal-scalability-law-to-organisations/
  63. https://blog.acolyer.org/2019/01/25/programming-paradigms-for-dummies-what-every-programmer-should-know/
  64. https://blog.acolyer.org/feed/
  65. https://blog.acolyer.org/comments/feed/
  66. https://twitter.com/519408925733425153
  67. https://wordpress.com/?ref=footer_blog
  68. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#cancel
  69. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  71. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#comment-form-guest
  72. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#comment-form-load-service:wordpress.com
  73. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#comment-form-load-service:twitter
  74. https://blog.acolyer.org/2017/11/16/opening-the-black-box-of-deep-neural-networks-via-information-part-ii/#comment-form-load-service:facebook
