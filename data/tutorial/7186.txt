   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

structured deep learning

   [16]go to the profile of kerem turgutlu
   [17]kerem turgutlu (button) blockedunblock (button) followfollowing
   nov 24, 2017
   [1*2l2c8akpzs8cgogxellfsa.jpeg]
     * fast
     * no domain knowledge requiring
     * high performing

   this blog will mainly focus on a not very widely known application area
   of deep learning, structured data.

   in machine learning/deep learning or any kind of predictive modeling
   task data comes before the algorithm/methodology. this is the main
   reason why machine learning requires a lot of feature engineering
   before certain tasks such as image classification, nlp, and many other
      unusual    data that can   t be directly fed into a id28 or
   a id79 model. in contrary, these type of tasks are done
   significantly well by deep learning without any requirement of nasty
   and time consuming feature engineering. most of the time these features
   require domain knowledge, creativity and a lot of trial and error. of
   course domain expertise and clever feature engineering are still very
   valuable but the techniques i will be mentioning throughout this post
   will be enough for you to aim for top 3 in a kaggle competition
   ([18]http://blog.kaggle.com/2016/01/22/rossmann-store-sales-winners-int
   erview-3rd-place-cheng-gui/) without any prior knowledge on the
   subject.
   [1*pnfywqns7q2jmrrgf1zm2g.jpeg]
   fig 1. a cute dog and an angry cat (labeled by me)

   due to complex nature and ability of feature generation (e.g.
   convolutional layers of id98s), deep learning is abundantly applied to
   various kinds of image, textual and audio data problems. these are
   without a doubt very important problems for advancement of ai, and
   there must be a very good reason why every year top researchers in the
   field compete to classify cats, dogs, and ships better than the
   previous year. but these are rarely the cases we see in industry.
   companies work with databases involving structured datasets, and these
   are the domains that shape every day lives.

   let   s define structured data to be more clear for the rest of this
   post. here you can think of rows as collected individual data points or
   observations and columns as fields that represents a single attribute
   of each observation. for example data from an online retail store might
   have rows as sales made by customers and columns as item bought,
   quantity, price, time stamp, and so on    .

   below we have online seller data, rows as each unique sales and columns
   describing that particular sales.
   [1*rfe1wmduhflyiyamsphc1a.png]
   fig 2. pandas dataframe an example to structured data

   let   s talk about how we can leverage neural networks for structured
   data task. actually at a theoretical level it   s very trivial to create
   a fully connected network with any desired architecture, then using
      rows    as inputs. given the id168 after a couple of dot
   products and id26 we would end up having a trained network
   which then can be used to make predictions.

   even though as it seems very straightforward there are major reasons
   why people prefer tree based methods over neural networks when it comes
   to structured data. this can be understand from the algorithm   s point
   of view, by exploring how actually algorithms see and treat our data.

   the main separation in terms of having structured and unstructured data
   is that with unstructured data even though it   s    unusual   , we often
   deal with single entities with single units like pixels, voxels, audio
   frequencies, radar back-scatters, sensor measurements    so on so forth.
   in contrary, with structured data we often need to deal with many
   different type of data under two main groups; numerical and
   categorical. categorical data requires processing prior to training,
   because most of the algorithms as well as neural networks can   t handle
   them directly yet.

   there are various options available for encoding variables such as
   label/numerical encoding and one-hot encoding. but these techniques are
   problematic in terms of memory and real representation of categorical
   levels. the former property is probably more obvious and can be
   illustrated with an example.

   let   s say we have day of week information as a column. if we one-hot
   encode or arbitrarily label encode this variable we would assume equal
   and arbitrary distance/difference among levels respectively.
   [1*zsykxea1qrkegiplnnfhyq.jpeg]
   fig 3. one-hot encoding and label encoding

   but both of these two approaches assume that the difference between
   each pair of days are equal but in reality we easily know that this is
   not true, so should our algorithm!

        the continuous nature of neural networks limits their applicability
     to categorical variables. therefore, naively applying
     neural networks on structured data with integer
     representation for category variables does not work well    [1]

   tree algorithms do not require any assumptions on continuity of
   categorical variables since they can find states by splitting as
   needed, but with neural networks this is not the case. here comes the
   entity embedding to help. entity embedding is used to map discrete
   values into multi-dimensional space where values with similar function
   outputs are closer to each other. you may think of if we were embedding
   states in a country for a sales problem, similar states in terms of
   sales would be closer to each in this projected space.

   since we don   t want to make arbitrary assumptions between the levels of
   our categorical variables, we will learn a better representation of
   each in euclidean space. this representation will be equal to nothing
   but the dot product of one-hot encoded data and learn-able weights.

   embeddings are very widely used in nlp, as each word is represented as
   a vector. two famous embedding examples are [19]glove and [20]id97.
   we can see how powerful embeddings are from the figure 4 [2]. these
   vectors are readily available for you to download them and use them as
   it fits your goal, which is actually very cool thinking of the
   information they hold.
   [1*sxnxyfaqflueidxpco130w.png]
   fig 4. id97 from tensorflow tutorials

   even though embeddings can be applied to different context either with
   supervised or unsupervised manner, our main objective is to understand
   how to do these projections for categorical variables.
     __________________________________________________________________

entity embeddings

   even though entity embeddings have a different name, they are not much
   different then the use case we have seen in id27s. after all,
   the only thing we care is to have higher dimensional vector
   representation of our for our grouped data; this may be words, day of
   weeks, countries and many others you can imagine. this transaction from
   id27s to meta(categorical in our case) data embeddings
   enabled yoshua bengio et al. to win a kaggle
   competition([21]https://www.kaggle.com/c/pkdd-15-predict-taxi-service-t
   rajectory-i) back in 2015 with a single simple automated approach,
   which is not the usual case for winning solutions.

        to deal with the discrete meta-data, consisting of client id, taxi
     id, date and time information, we learn embeddings jointly with the
     model for each of these information. this is inspired by neural
     id38 approaches [2], in which each word is mapped to a
     vector space of fixed size (the vector is called the embedding of
     the word).   [3]

   [1*wfydi5x5kkfgjc1wsrieyq.png]
   fig 5. taxi metadata embeddings visualization with id167 2d projections

   step by step we will explore how to learn these features within a
   neural network. define a fully connected neural network, separate
   numerical and categorical variables.

   for each categorical variable:
    1. initialize a random embedding matrix as m x d.

   m: number of unique levels of categorical variable (monday, tuesday,    )

   d: desired dimension for representation, a hyperparameter which can be
   between 1 and m-1 (if 1 then it will be label encoding, if m it will be
   one-hot encoding)
   [1*wf-yxfeohwwi7ekorrvkog.png]
   fig 6. embedding matrix

   2. then for each forward pass through neural network we do a lookup for
   the given level (e.g monday for    dow   ) from the embedding matrix, which
   will give us a vector as 1 x d.
   [1*jsx1eejrfs-k9tnuarqrnq.png]
   fig 7, embedding vector after lookup

   3. append this 1 x d vector to our input vector (numerical vector).
   think this process as augmenting a matrix, where we add an embedding
   vector for each category that been embedded by doing lookup for each
   particular row.
   [1*qa5oh3-bxfjhsxed-afiea.png]
   fig 8. after adding embedding vectors

   4. while doing id26 we are also updating these embedding
   vectors in the way of gradient which minimizes our id168.

   usually inputs are not updated but for embedding matrices we have this
   special case where we allow our gradient to flow all the way back to
   these mapped features and hence optimize them.

   we can think this as a process, which allows our categorical embeddings
   to be better represented at every iteration.

   note: rule of thumb is to keep categories that don   t have very high
   cardinality. as in if a variable has unique levels for 90% of the
   observations then it wouldn   t be a very predictive variable and we may
   very well get rid of it.
     __________________________________________________________________

good news

   we can very well implement above mentioned architecture in our favorite
   framework (preferably a dynamic one) by doing lookup and allowing
   requires_grad=true in our embedding vectors and learning them. but all
   these steps and more is already done in fast.ai. beyond making
   structural deep learning easy this library also provides many
   state-of-art features like differential learning rates, sgdr, cyclical
   learning rate, learning rate finder and so on. these are all the things
   we would like to take advantage of. you can read further about these
   topics from these very cool other blog posts:

   [22]https://medium.com/@bushaev/improving-the-way-we-work-with-learning
   -rate-5e99554f163b

   [23]https://medium.com/@surmenok/estimating-optimal-learning-rate-for-a
   -deep-neural-network-ce32f2556ce0

   [24]https://medium.com/@markkhoffmann/exploring-stochastic-gradient-des
   cent-with-restarts-sgdr-fa206c38a74e
   [25]teleported.in
   learning rate (lr) is one of the most important hyperparameters to be
   tuned and holds key to faster and effective   teleported.in
     __________________________________________________________________

walk through with fast.ai

   in this part we will take a look at how we can bypass all these steps
   mentioned and build a neural network that is more effective with
   structured data.

   for this purpose we will take a look at an active kaggle competition
   [26]https://www.kaggle.com/c/mercari-price-suggestion-challenge/. in
   this challenge we are trying to predict the price of an item sold by an
   online seller. this is a very suitable example to entity embeddings
   since data is mostly categorical with relatively high cardinality (not
   too much) and there isn   t much else.

   data:

   ~1.4 m rows
     * item_condition_id: condition of the item (cardinality: 5)
     * category_name: category name (cardinality: 1287)
     * brand_name: name of the brand (cardinality: 4809)
     * shipping: whether shipping is included in price (cardinality 2)

   important note: i won   t be having a validation set in this example
   since i   ve already find my best model parameters but you should always
   do hyperparameter tuning with a validation set.

   step 1:

   fill missing values as a level, since missing-ness itself is an
   important information.
train.category_name = train.category_name.fillna('missing').astype('category')
train.brand_name = train.brand_name.fillna('missing').astype('category')
train.item_condition_id = train.item_condition_id.astype('category')
test.category_name = test.category_name.fillna('missing').astype('category')
test.brand_name = test.brand_name.fillna('missing').astype('category')
test.item_condition_id = test.item_condition_id.astype('category')

   step 2:

   preprocess data, scale for numerical columns since neural networks like
   normalized data or in other words equally scaled data. if you don   t
   scale your data one feature may be over emphasized by the network since
   it   s all about dot products and gradients. it would be better if we
   were scaling both training and test by training statistics, but this
   shouldn   t matter much. think of dividing each pixel by 255, same logic.

   i   ve combined training and test data since we want same levels to have
   same encodings.
combined_x, combined_y, nas, _ = proc_df(combined, 'price', do_scale=true)

   step 3:

   create model data object. path is where fast.ai stores models and
   activations.
path = '../data/'
md = columnarmodeldata.from_data_frame(path, test_idx, combined_x, combined_y, c
at_flds=cats, bs= 128

   step 4:

   decide d (dimension of embedding). cat_sz is a list of tuples
   (col_name, cardinality + 1) for each categorical columns.
# we said that d (dimension of embedding) is an hyperparameter
# but here is jeremy howard's rule of thumb
emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]
# [(6, 3), (1312, 50), (5291, 50), (3, 2)]

   step 5:

   create a learner, this is the core object of fast.ai library.
# params: embedding sizes, number of numerical cols, embedding dropout, output,
layer sizes, layer dropouts
m = md.get_learner(emb_szs, len(combined_x.columns)-len(cats),
                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)

   step 6:

   this part is explained in much more detail in other posts that i   ve
   mentioned before.

   take advantage of full fast.ai joy.

   we are picking our learning rate to be from a point before where loss
   starts increasing   
# find best lr
m.lr_find()
# find best lr
m.sched.plot()

   [1*ijhid113dgx1sph-vlspkj3g.png]
   fig 9. learning rate loss plot

   fit
we can see that with just 3 epochs we have
lr = 0.0001
m.fit(lr, 3, metrics=[lrmse])

   [1*bemetx2zqviub-26jvl76a.png]

   fit more
m.fit(lr, 3, metrics=[lrmse], cycle_len=1)

   [1*f4qscxjoq5y3ceja_ndjxw.png]

   and some more   
m.fit(lr, 2, metrics=[lrmse], cycle_len=1)

   [1*x2eupurhah9ond53kjnyjq.png]

   so, these brief but effective steps can take you to top ~10% without
   any further need within minutes. if you really want to aim high, i
   would suggest you to exploit item_description column and make it
   multiple categorical variables. then leave the job to entity
   embeddings, of course don   t forget to stack and ensemble :)

   this was my very first blog post i hope you enjoyed it! i must admit
   this thing is kind of addictive, so i might come back very soon   

   appreciate all the claps you have for me :)

   bio: i am a current graduate student in usf, master   s of analytics
   program. i   ve been applying machine learning for 3 years now and
   currently practicing deep learning with fast.ai.

   linkedin: [27]https://www.linkedin.com/in/kerem-turgutlu-12906b65/en

   references:

   [1] cheng guo, felix berkhahn (2016, april, 22) entity embeddings of
   categorical variables. retrieved from
   [28]https://arxiv.org/abs/1604.06737.

   [2] tensorflow tutorials:
   [29]https://www.tensorflow.org/tutorials/id97

   [3] yoshua bengio, et al. id158s applied to taxi
   destination prediction. retrieved from
   [30]https://arxiv.org/pdf/1508.00021.pdf.

     * [31]machine learning
     * [32]deep learning
     * [33]structured data
     * [34]kaggle
     * [35]towards data science

   (button)
   (button)
   (button) 3k claps
   (button) (button) (button) 21 (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of kerem turgutlu

[37]kerem turgutlu

   machine learning engineer

     (button) follow
   [38]towards data science

[39]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3k
     * (button)
     *
     *

   [40]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [41]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b8ca4138b848
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/structured-deep-learning-b8ca4138b848&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/structured-deep-learning-b8ca4138b848&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_urgaxf0llaoy---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@keremturgutlu?source=post_header_lockup
  17. https://towardsdatascience.com/@keremturgutlu
  18. http://blog.kaggle.com/2016/01/22/rossmann-store-sales-winners-interview-3rd-place-cheng-gui/
  19. https://nlp.stanford.edu/projects/glove/
  20. https://en.wikipedia.org/wiki/id97
  21. https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i
  22. https://medium.com/@bushaev/improving-the-way-we-work-with-learning-rate-5e99554f163b
  23. https://medium.com/@surmenok/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0
  24. https://medium.com/@markkhoffmann/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e
  25. http://teleported.in/posts/cyclic-learning-rate/
  26. https://www.kaggle.com/c/mercari-price-suggestion-challenge/l
  27. https://www.linkedin.com/in/kerem-turgutlu-12906b65/en
  28. https://arxiv.org/abs/1604.06737
  29. https://www.tensorflow.org/tutorials/id97
  30. https://arxiv.org/pdf/1508.00021.pdf
  31. https://towardsdatascience.com/tagged/machine-learning?source=post
  32. https://towardsdatascience.com/tagged/deep-learning?source=post
  33. https://towardsdatascience.com/tagged/structured-data?source=post
  34. https://towardsdatascience.com/tagged/kaggle?source=post
  35. https://towardsdatascience.com/tagged/towards-data-science?source=post
  36. https://towardsdatascience.com/@keremturgutlu?source=footer_card
  37. https://towardsdatascience.com/@keremturgutlu
  38. https://towardsdatascience.com/?source=footer_card
  39. https://towardsdatascience.com/?source=footer_card
  40. https://towardsdatascience.com/
  41. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  43. http://teleported.in/posts/cyclic-learning-rate/
  44. https://medium.com/p/b8ca4138b848/share/twitter
  45. https://medium.com/p/b8ca4138b848/share/facebook
  46. https://medium.com/p/b8ca4138b848/share/twitter
  47. https://medium.com/p/b8ca4138b848/share/facebook
