   #[1]rare technologies    feed [2]rare technologies    comments feed
   [3]rare technologies    new download api for pretrained nlp models and
   datasets in gensim comments feed [4]alternate [5]alternate

   [tr?id=1761346240851963&ev=pageview&noscript=1]

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld

   [7]pragmatic machine learning rare technologies [8]navigation

     * [9]services
     * [10]products
          + [11]pii tools
          + [12]scaletext
     * [13]corporate training
          + [14]overview
          + [15]python best practices
          + [16]practical machine learning
          + [17]topic modelling
          + [18]deep learning in practice
     * [19]for students
          + [20]open source
          + [21]incubator
          + [22]competitions
     * [23]company
          + [24]careers
          + [25]our team
     * [26]blog
     * [27]contact
     * [28]search

     * [29]services
     * [30]products
          + [31]pii tools
          + [32]scaletext
     * [33]corporate training
          + [34]overview
          + [35]python best practices
          + [36]practical machine learning
          + [37]topic modelling
          + [38]deep learning in practice
     * [39]for students
          + [40]open source
          + [41]incubator
          + [42]competitions
     * [43]company
          + [44]careers
          + [45]our team
     * [46]blog
     * [47]contact
     * [48]search

new download api for pretrained nlp models and datasets in gensim

   [49]chaitali saini 2017-11-27[50] datasets, [51]gensim, [52]open
   source, [53]student incubator[54] 4 comments

   there   s no shortage of websites and repositories that aggregate various
   machine learning datasets and pre-trained models ([55]kaggle, [56]uci
   mlr, [57]deepdive, individual repos like [58]glove, [59]fasttext,
   [60]quora, [61]blogs, individual university pages   ). the only problem
   is, they all use widely different formats, cover widely different
   use-cases and go out of service with worrying regularity.

   for this reason, we decided to include free datasets and models
   relevant to unsupervised text analysis (gensim   s sweet spot), directly
   in gensim, using a stable data repository (github) and a common data
   format and access api. gensim has been around for nearly 10 years, and
   deserves its own stable, reliable set of resources.
   in the spirit of the good old standards proliferation    (image: xkcd)

   in the spirit of the good old standards proliferation    (image:
   [62]xkcd)

   this is a graduating blog post by chaitali saini, one of our wonderful
   [63]incubator students.

api design

   to make the life of our users easier, we had a look at how other
   popular packages (such as scikit-learn, nltk or spacy) deal with
   dataset access, packaging and upgrades.

   our design goals were:
     * ease of use: users must be able to load up a pre-packaged dataset
       (text corpus or pretrained model) and use it in a single line of
       code.
     * efficiency: accessing the datasets must be streamed and efficient,
       i.e. no unnecessary decompressing (disk or ram)
     * extendibility: users must be able to share their own
       domain-specific dataset or model, to be included in the gensim
       repository.

   all of this using open source tools and licenses, naturally.

   did we succeed? the rest of this post introduces this new
   functionality, so you be the judge.

core api concepts

where are the corpora / models stored?

   globally, the datasets live in a new central dedicated github
   repository, [64]gensim-data. its readme contains a list of resources
   available right now (more in the making, domain-specific contributions
   welcome!).

   locally, on your machine after you download them, the datasets live
   inside the ~/gensim-data folder. the relevant files are retrieved from
   github and re-assembled here if necessary (more on that below).

   github allows storing an unlimited number of files, acts as a good cdn,
   has (mostly) good world-wide availability (except some shenanigans by
   china), dos protection. the only limitation is that all files must be
   smaller than 2gb, which is annoying because many nlp corpora hit that
   limit easily.

   we solved that problem by splitting large files into several smaller
   files on storage (in github), then re-assembling them transparently
   during the download (your disk).

   note that you don   t have to clone or install this gensim-data
   repository, it   s a technical detail you can simply ignore. gensim knows
   the data location and when you call something like
   gensim.downloader.api.load(   text8   ), gensim will automatically locate
   and download the correct file.

what   s in a dataset?

   in nlp, both corpora and models are typically a result of a longer
   pipeline, which includes things like crawling (a specific website or
   database), text filtering (removing boilerplate or document
   subsampling), preprocessing (text id172, encoding,
   id121) etc.

   these are all critical values to know, as a different choice of
   parameters leads to a different dataset     even if it   s just running the
   same pipeline code at a later date.

   for this reason, we also store metadata about each dataset. this
   typically includes the original authors, related published paper(s)
   (where applicable), things like the vectorizer, dimensionality and so
   on. this is how you   d retrieve this metadata:

# from cli
$ python -m gensim.downloader -i dataset_name

# from python
>>> print(api.info('text8'))
{
    u'description': u'first 100,000,000 bytes of plain text from wikipedia. used
 for testing purposes; see wiki-english-* for proper full wikipedia datasets.',
    u'read_more': [u'http://mattmahoney.net/dc/textdata.html']}
    u'file_size': 33182058,
    u'num_records': 1701,
    u'record_format': u'list of str (tokens)',
    u'file_name': u'text8.gz',
    u'reader_code': u'https://github.com/rare-technologies/gensim-data/releases/
download/text8/__init__.py',
    u'license': u'public domain',
    u'parts': 1,
    u'checksum': u'68799af40b6bda07dfa47a32612e5364',
    ...
}

what datasets are there?

   a core concept of the new api is a dataset name, a unique string
   identifier of each dataset. you can get a list of all available dataset
   names:

# using the command line
$ python -m gensim.downloader --info

   downloader_info

   output for python -m gensim.downloader --info

# from python
>>> import gensim.downloader as api
>>> print(api.info())

accessing a dataset

   finally, this is how you   d download a dataset locally and load it up,
   ready for use in python:

# load a corpus
>>> text8_corpus = api.load('text8')
# load a pre-trained model; the api is the same
>>> glove_model = api.load('glove-twitter-200')

   the commands above will both download the dataset (if not already
   present) and load it as a python object, ready for use:

>>> print(next(iter(text8_corpus))[:10])  # corpora are streamed iterables
[u'anarchism', u'originated', u'as', u'a', u'term', u'of', u'abuse', u'first', u
'used', u'against']
>>> print(glove_model.most_similar("science", topn=3))
[('physics', 0.7786292433738708), ('biology', 0.754408061504364), ('math', 0.744
5053458213806)]

   note that the loaded corpora are not loaded into ram as whole, but
   rather streamed from disk, document-by-document as iterables, which is
   gensim   s native format for scalability reasons. (confused about
   python   s iterators, iterables and data streaming? see our tutorial blog
   post [65]data streaming in python: generators, iterators, iterables.)

   if you want to download a corpus or model without loading it as a
   python object, use the optional return_path parameter instead:

>>> text8corpus = api.load('text8', return_path=true)
>>> print(text8corpus)  # now it   s just a string
'/home/username/gensim-data/text8/text8.gz'

end-to-end example

   we   ll end with one end-to-end example. by now, it should be clear
   what   s happening and how to integrate the prepackaged resources into
   your own machine learning pipeline:

>>> import gensim.downloader as api
>>> from gensim.models import id97
>>> print(api.info('text8'))
{u'source': u'http://mattmahoney.net/dc/textdata', u'checksum': u'68799af40b6bda
07dfa47a32612e5364', u'parts': 1, u'description': u'cleaned small sample from wi
kipedia', u'file_name': u'text8.gz'}
>>> text8_corpus = api.load('text8')
>>> w2v_model = id97(text8_corpus)  # train id97 model on text8 corpus
>>> print(w2v_model.similarity('tree', 'leaf'))  # sanity check
0.702394119049

   we hope you enjoy this new functionality. let us know in the comments
   below or [66]on twitter what you think!

faq

   q1: when does this functionality become available?
   a: in the next gensim release: gensim>=3.2.0 (planned for december
   2017). edit:[67]already released!

   this is an initial release and we have several domain-specific datasets
   in the pipeline (patents, medical, hr).

   q2: how to add a new corpus / model?
   a: we welcome datasets in any language, and especially domain
   domain-specific dataset (legal, hr, engineering, medical   ). the only
   restriction is that the dataset must fit within gensim   s mission:
   unsupervised text analysis for humans. and obviously have a license
   that allows (non-commercial) redistribution.

   to contribute a new dataset, look at the [68]gensim-data readme.
   compress your data to a single .gz file, share it with us through any
   file-sharing service. then [69]create a new issue with a detailed
   description with how the dataset was created, any related links/papers,
   parameters or settings used. important: include a motivating use-case
   and a concrete python example for how to load and use the dataset! a
   dataset is useless if people don   t know how to work with it.

   q3: how do you update the existing datasets?
   a: short answer: never. the published datasets are immutable, published
   in their original form forever, no changes. we think this is the best
   way to ensure reproducibility.

   at the same time, no one prevents you from adding new, updated models
   under a new name, as a new dataset. for example, we published the
      wiki-english-20171001    corpus and once we release a new version, we   ll
   make a new dataset with a name like    wiki-english-20171201    (note the
   difference in the timestamp). but previous dataset versions will remain
   always available, under their original name.

   q4: what   s the dataset naming policy?
   a: reasonably short, all lowercase, no spaces, hyphens instead of
   underscores. example:    glove-twitter-25    as a model;
      wiki-english-20171001    for a dataset. do not stress about the name,
   we   ll choose something suitable or make adjustments.

   q5: how do i delete all gensim datasets from my computer?
   a: remove the ~/gensim-data folder, all data is stored in there.

   q6: i don   t have enough disk space under ~/gensim-data for new
   datasets, what should i do?
   a: even though gensim can stream from remote storages (s3, hdfs) using
   [70]smart_open, a locally downloaded copy of the dataset is required
   here, for performance reasons. if you have another disk with more
   space, we recommend you create a [71]symbolic link (symlink) and point
   ~/gensim-data to a location on this larger disk.

   q7: are you planning to expand the api?
   a: probably. your suggestion and feedback welcome. one thing we already
   know we   ll implement in future versions is dataset tags, for simpler
   dataset filtering and navigation (tags for language, domain, purpose   ).

   [72]corpora[73]fasttext[74]glove[75]nlp[76]pretrained
   models[77]wikipedia

comments 4

    1.
   [78]ibrahim sharaf
       [79]2017-12-10 at 9:45 am
       amazing improvement! thank you to all who made this happen.
       [80]reply
         1. radim   eh    ek
        [81]radim   eh    ek
            [82]2017-12-16 at 8:46 am
            thank you ibrahim.
            if you have any interesting nlp datasets, let us know     
            [83]reply
    2. pingback: [84]another twitter id31 with python     part
       10 (neural network with    | copy paste programmers
    3.
   ovidiu
       [85]2019-01-22 at 9:59 pm
       hi, is there a way to have a dataset based on a folder of text
       files whereas the json dictionary would include the path to those
       files that have to be read? or is it the only way to embed the
       texts pers se directly into one big dictionary?
       [86]reply

leave a reply [87]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   submit

   current [88][email protected] * 4.2_________________

   leave this field empty ____________________

author of post

   chaitali saini

chaitali saini's bio:

   undergraduate student at cluster innovation centre, du majoring in it
   and maths. interested in nlp and likes contributing to open source.

need expert consulting in ml and nlp?

   ________________________________________

   ________________________________________


   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   please leave this field empty. ________________________________________

   send

categories

   categories[select category___________]

archives

   archives [select month__]

recent posts

     * [89]export pii drill-down reports
     * [90]personal data analytics
     * [91]scanning office 365 for sensitive pii information
     * [92]pivoted document length normalisation
     * [93]sent2vec: an unsupervised approach towards learning sentence
       embeddings

stay ahead of the curve

get our latest tutorials, updates and insights delivered straight to your
inbox.

   ____________________

   ____________________

   subscribe
   ____________________
   1-2 times a month, if lucky. your information will not be shared.

   [94][footer-logo.png]
     * [95]services
     * [96]careers
     * [97]our team
     * [98]corporate training
     * [99]blog
     * [100]incubator
     * [101]contact
     * [102]competitions
     * [103]site map

   rare technologies [104][email protected] sv  tova 5, prague, czech
   republic [105](eu) +420 776 288 853
   type and press    enter    to search ____________________

references

   visible links
   1. https://rare-technologies.com/feed/
   2. https://rare-technologies.com/comments/feed/
   3. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/feed/
   4. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/
   5. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/&format=xml
   6. https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld
   7. https://rare-technologies.com/
   8. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/
   9. https://rare-technologies.com/services/
  10. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/
  11. https://pii-tools.com/
  12. https://scaletext.com/
  13. https://rare-technologies.com/corporate-training/
  14. https://rare-technologies.com/corporate-training/
  15. https://rare-technologies.com/python-best-practices/
  16. https://rare-technologies.com/practical-machine-learning/
  17. https://rare-technologies.com/topic-modelling-training/
  18. https://rare-technologies.com/deep_learning_training/
  19. https://rare-technologies.com/incubator
  20. https://github.com/rare-technologies/
  21. https://rare-technologies.com/incubator/
  22. https://rare-technologies.com/competitions/
  23. https://rare-technologies.com/#braintrust
  24. https://rare-technologies.com/careers/
  25. https://rare-technologies.com/our-team/
  26. https://rare-technologies.com/blog/
  27. https://rare-technologies.com/contact/
  28. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/
  29. https://rare-technologies.com/services/
  30. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/
  31. https://pii-tools.com/
  32. https://scaletext.com/
  33. https://rare-technologies.com/corporate-training/
  34. https://rare-technologies.com/corporate-training/
  35. https://rare-technologies.com/python-best-practices/
  36. https://rare-technologies.com/practical-machine-learning/
  37. https://rare-technologies.com/topic-modelling-training/
  38. https://rare-technologies.com/deep_learning_training/
  39. https://rare-technologies.com/incubator
  40. https://github.com/rare-technologies/
  41. https://rare-technologies.com/incubator/
  42. https://rare-technologies.com/competitions/
  43. https://rare-technologies.com/#braintrust
  44. https://rare-technologies.com/careers/
  45. https://rare-technologies.com/our-team/
  46. https://rare-technologies.com/blog/
  47. https://rare-technologies.com/contact/
  48. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/
  49. https://rare-technologies.com/author/chaitalisaini/
  50. https://rare-technologies.com/category/datasets/
  51. https://rare-technologies.com/category/gensim/
  52. https://rare-technologies.com/category/open-source/
  53. https://rare-technologies.com/category/student-incubator/
  54. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/#comments
  55. https://www.kaggle.com/datasets
  56. https://archive.ics.uci.edu/ml/index.php
  57. http://deepdive.stanford.edu/opendata/
  58. https://nlp.stanford.edu/projects/glove/
  59. https://fasttext.cc/docs/en/english-vectors.html
  60. https://data.quora.com/first-quora-dataset-release-question-pairs
  61. https://medium.com/startup-grind/fueling-the-ai-gold-rush-7ae438505bc2
  62. https://xkcd.com/927/
  63. https://rare-technologies.com/incubator
  64. https://github.com/rare-technologies/gensim-data
  65. https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/
  66. https://twitter.com/gensim_py/
  67. https://github.com/rare-technologies/gensim/releases/tag/3.2.0
  68. https://github.com/rare-technologies/gensim-data#want-to-add-a-new-corpus-or-model
  69. https://github.com/rare-technologies/gensim-data/issues
  70. https://github.com/rare-technologies/smart_open
  71. https://en.wikipedia.org/wiki/symbolic_link
  72. https://rare-technologies.com/tag/corpora/
  73. https://rare-technologies.com/tag/fasttext/
  74. https://rare-technologies.com/tag/glove/
  75. https://rare-technologies.com/tag/nlp/
  76. https://rare-technologies.com/tag/pretrained-models/
  77. https://rare-technologies.com/tag/wikipedia/
  78. http://ibrahimsharafelden.me/
  79. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/#comment-2694
  80. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/?replytocom=2694#respond
  81. http://radimrehurek.com/
  82. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/#comment-2697
  83. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/?replytocom=2697#respond
  84. http://copypasteprogrammers.com/another-twitter-sentiment-analysis-with-python-part-10-neural-network-with-a6441269aa3c/
  85. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/#comment-2794
  86. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/?replytocom=2794#respond
  87. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/#respond
  88. https://rare-technologies.com/cdn-cgi/l/email-protection
  89. https://rare-technologies.com/personal-data-reports/
  90. https://rare-technologies.com/pii_analytics/
  91. https://rare-technologies.com/pii-scan-o365-connector/
  92. https://rare-technologies.com/pivoted-document-length-normalisation/
  93. https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
  94. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/
  95. https://rare-technologies.com/services/
  96. https://rare-technologies.com/careers/
  97. https://rare-technologies.com/our-team/
  98. https://rare-technologies.com/corporate-training/
  99. https://rare-technologies.com/blog/
 100. https://rare-technologies.com/incubator/
 101. https://rare-technologies.com/contact/
 102. https://rare-technologies.com/competitions/
 103. https://rare-technologies.com/sitemap
 104. https://rare-technologies.com/cdn-cgi/l/email-protection#157c7b737a5567746770386170767d7b7a797a727c70663b767a78
 105. tel:+420 776 288 853

   hidden links:
 107. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/#top
 108. https://www.facebook.com/raretechnologies
 109. https://twitter.com/raretechteam
 110. https://www.linkedin.com/company/6457766
 111. https://github.com/piskvorky/
 112. https://rare-technologies.com/feed/
