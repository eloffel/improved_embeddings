squashing
computational linguistics

noah a. smith

paul g. allen school of computer science  & engineering

university of washington

seattle, usa
@nlpnoah

research supported in part by:  nsf, darpa  deft,  darpa  cwc,  facebook,  google,  samsung, university  of washington.

data

applications of nlp in 2017

    conversation, ie, mt, qa, summarization,  text categorization

applications of nlp in 2017

    conversation, ie, mt, qa, summarization,  text categorization
    machine-in-the-loop tools for (human) authors

chenhao tan

elizabeth clark

revise your 
message with help 
from nlp

tremoloop.com

collaborate with an nlp 
model through an 
   exquisite corpse    
storytelling game

applications of nlp in 2017

    conversation, ie, mt, qa, summarization,  text categorization
    machine-in-the-loop tools for (human) authors
    analysis tools for measuring social phenomena

lucy lin

dallas card

sensationalism in 
science news

bit.ly/sensational-news
    bookmark this survey!

track ideas, propositions, 
frames in discourse over 
time

data

?

squash

squash networks

    parameterized differentiable functions composed out of simpler 
parameterized differentiable functions,  some nonlinear

squash networks

    parameterized differentiable functions composed out of simpler 
parameterized differentiable functions,  some nonlinear

*yes,  rectified  linear  units (relus)  are only half-squash;  hat-tip martha  white.

from jack  (2010), dynamic 
system modeling and 
control, goo.gl/pgvjps

squash networks

    parameterized differentiable functions composed out of simpler 
parameterized differentiable functions,  some nonlinear

    estimate parameters using leibniz (1676)

from existentialcomics.com

who wants an all-squash diet?

much festive

many dropout

very curcurbita

wow

linguistic structure prediction

output (structure)

input (text)

linguistic structure prediction

sequences, 

trees, 

graphs,    

output (structure)

input (text)

linguistic structure prediction

sequences, 

trees, 

graphs,    

   gold    output

output (structure)

input (text)

linguistic structure prediction

sequences, 

trees, 

graphs,    

   gold    output

output (structure)

input representation

input (text)

linguistic structure prediction

sequences, 

trees, 

graphs,    

   gold    output

clusters, lexicons, 
embeddings,    

output (structure)

input representation

input (text)

linguistic structure prediction

sequences, 

trees, 

graphs,    

   gold    output

clusters, lexicons, 
embeddings,    

training  objective

output (structure)

input representation

input (text)

linguistic structure prediction

probabilistic, 
cost-aware,    

sequences, 

trees, 

graphs,    

   gold    output

clusters, lexicons, 
embeddings,    

training  objective

output (structure)

input representation

input (text)

linguistic structure prediction

probabilistic, 
cost-aware,    

sequences, 

trees, 

graphs,    

   gold    output

clusters, lexicons, 
embeddings,    

training  objective

output (structure)

part representations

input representation

input (text)

linguistic structure prediction

probabilistic, 
cost-aware,    

sequences, 

trees, 

graphs,    

   gold    output

segments/spans, arcs, 
graph fragments,    

clusters, lexicons, 
embeddings,    

training  objective

output (structure)

part representations

input representation

input (text)

linguistic structure prediction

   gold    output

training  objective

output (structure)

part representations

input representation

input (text)

linguistic structure prediction

   task   

r
e
g
u
l
a
r
i
z
a
t
i
o
n

annotation
conventions

& theory

   gold    output

error definitions

& weights

constraints &
independence
assumptions

n
o
i
t
c
e
l
e
s
 
a
t
a
d

training  objective

output (structure)

part representations

input representation

input (text)

inductive bias

    what does your learning 
algorithm assume?

    how will it choose among good 
predictive functions?

see also:
no free lunch theorem
(mitchell,  1980; wolpert,  1996)

bias

data

three new models

    parsing sentences into 
predicate-argument structures

    fillmore frames
    semantic dependency graphs

    language models that 
dynamically  track entities

when democrats wonder why there is so much resentment of clinton, they don   t need to look much 
further than the big lie about philandering that stephanopoulos,  carville helped to put over in 1992.

original  story  on slate.com:   http://goo.gl/hp89td

frame-semantic analysis

when democrats wonder why there is so much resentment of clinton, they don   t need to look much 
further than the big lie about philandering that stephanopoulos,  carville helped to put over in 1992.

framenet:   https://framenet.icsi.berkeley.edu

frame-semantic analysis

when democrats wonder why there is so much resentment of clinton, they don   t need to look much 
further than the big lie about philandering that stephanopoulos,  carville helped to put over in 1992.

framenet:   https://framenet.icsi.berkeley.edu

frame-semantic analysis

cognizer:   democrats
topic:  why     clinton

when democrats wonder why there is so much resentment of clinton, they don   t need to look much 
further than the big lie about philandering that stephanopoulos,  carville helped to put over in 1992.

framenet:   https://framenet.icsi.berkeley.edu

frame-semantic analysis

landmark  event:   democrats      clinton
trajector event:   they    1992

degree:   so
mass:   resentment  of clinton

cognizer:   democrats
topic:  why     clinton

entity:   so     clinton

time:   when     clinton
cognizer agent:   they
ground:   much     1992
sought entity:   ?

time:   when     clinton
required  situation:   they     to look     1992

explanation:   why
degree:  so much
content:   of clinton
experiencer:   ?

when democrats wonder why there is so much resentment of clinton, they don   t need to look much 
further than the big lie about philandering that stephanopoulos,  carville helped to put over in 1992.

topic:   about      1992

helper:  stephanopoulos      carville
goal:  to put over
time:  in 1992
benefited_party:   ?

framenet:   https://framenet.icsi.berkeley.edu

trajector event:   the big lie     over
landmark  period:  1992

brood, consider, 
contemplate, deliberate,    

translate

bracket, 
categorize, 
class, classify

appraise, 
assess, 
evaluate,    

commit to 
memory, learn, 
memorize,    

agonize, fret, fuss, 
lose sleep,    

framenet:   https://framenet.icsi.berkeley.edu

frame-semantic analysis

landmark  event:   democrats      clinton
trajector event:   they    1992

degree:   so
mass:   resentment  of clinton

cognizer:   democrats
topic:  why     clinton

entity:   so     clinton

time:   when     clinton
cognizer agent:   they
ground:   much     1992
sought entity:   ?

time:   when     clinton
required  situation:   they     to look     1992

explanation:   why
degree:  so much
content:   of clinton
experiencer:   ?

when democrats wonder why there is so much resentment of clinton, they don   t need to look much 
further than the big lie about philandering that stephanopoulos,  carville helped to put over in 1992.

topic:   about      1992

helper:  stephanopoulos      carville
goal:  to put over
time:  in 1992
benefited_party:   ?

framenet:   https://framenet.icsi.berkeley.edu

trajector event:   the big lie     over
landmark  period:  1992

when democrats wondercogitation why there is so much resentment of clinton, they don   t need     words  + frame

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

bilstm

(contextualized 
word vectors)
words  + frame

   

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

parts:   segments 
up to length d

scored  by 

another  bilstm, 

with labels

bilstm

(contextualized 
word vectors)
words  + frame

output:  covering  sequence  of nonoverlapping segments

   

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

parts:   segments 
up to length d

scored  by 

another  bilstm, 

with labels

bilstm

(contextualized 
word vectors)
words  + frame

segmental id56 
(lingpeng kong, chris dyer, n.a.s.,  iclr 2016)

training  objective:

log loss

output:  covering  sequence  of nonoverlapping segments,  recovered  in o(ldn);  see sarawagi & cohen, 2004

   

parts:   segments 
up to length d

scored  by 

another  bilstm, 

with labels

bilstm

(contextualized 
word vectors)
input sequence

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

   

cognizer

topic

   

   

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

   

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

cognizer

topic

   

   
wondercogitation

wondercogitation

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

   

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

cognizer

topic

   

   
wondercogitation

wondercogitation

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

   

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

cognizer

topic

   

   
wondercogitation

wondercogitation

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

   

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

cognizer

topic

   

   
wondercogitation

wondercogitation

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

id136 via id145 in o(ldn)

   

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

cognizer

topic

   

   
wondercogitation

wondercogitation

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

open-sesame
(swabha swayamdipta,  sam thomson, 
chris dyer, n.a.s., arxiv:1706.09528)

training  objective:

recall-oriented 
softmax margin 
(gimpel et al., 

2010)

cognizer:   democrats
topic: why there is so much  resentment of clinton

output:  labeled 
argument  spans

   

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

parts:   segments 
with role labels, 

scored  by 

another  bilstm

bilstm

(contextualized 
word vectors)
words  + frame

   

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

cognizer

topic

   

   
wondercogitation

wondercogitation

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

   

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

cognizer

topic

   

   
wondercogitation

wondercogitation

syntax  features?

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

id32  (marcus  et al., 1993)

no

yes

no

yes

yes

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

   

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

n
o
i
t
a
t
i

g
o
c
r
e
d
n
o
w

cognizer

topic

   
wondercogitation

wondercogitation

main task

   

no

yes

no

yes

yes

scaffold  task

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

multitask representation learning
(caruana,  1997)

main task:  find and 
label semantic 
arguments

output structures  need not be consistent

scaffold task:  predict 
syntactic constituents

training  objective

training  objective

   gold    output

output (structure)

output (structure)

   gold    output

part  representations

input representation

input (text)

shared

part  representations

input representation

input (text)

training  datasets  need not overlap

f1 on frame-id29 (frames & arguments), framenet 1.5 test set.

72

71

70

69

68

67

66

65

semafor 1.0 

(das et al., 

2014)

ours

semafor 2.0 
(kshirsagar et 

al., 2015)

open-sesame     with 
syntactic 
scaffold

    with syntax 

features

framat (roth, 

2016)

fitzgerald et 

al. (2015)

single model

ensemble

open-source  systems

f1 on frame-id29 (frames & arguments), framenet 1.5 test set.

72

71

70

69

68

67

66

65

semafor 1.0 

(das et al., 

2014)

ours

semafor 2.0 
(kshirsagar et 

al., 2015)

open-sesame     with 
syntactic 
scaffold

    with syntax 

features

framat (roth, 

2016)

fitzgerald et 

al. (2015)

single model

ensemble

open-source  systems

training  objective:

recall-oriented 
softmax margin 
(gimpel et al., 

2010)

cognizer:   democrats
topic: why there is so much  resentment of clinton

output:  labeled 
argument  spans

bias?

   

segments
get scores

parts:   segments 
with role labels, 

scored  by 

another  bilstm

bilstm

(contextualized 
word vectors)
words  + frame

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

training  objective:

recall-oriented 
softmax margin 
(gimpel et al., 

2010)

cognizer:   democrats
topic: why there is so much  resentment of clinton

output:  labeled 
argument  spans

bias?

   

syntactic scaffold

segments
get scores

parts:   segments 
with role labels, 

scored  by 

another  bilstm

bilstm

(contextualized 
word vectors)
words  + frame

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

recall-oriented 

cost

training  objective:

recall-oriented 
softmax margin 
(gimpel et al., 

2010)

cognizer:   democrats
topic: why there is so much  resentment of clinton

output:  labeled 
argument  spans

bias?

   

syntactic scaffold

segments
get scores

parts:   segments 
with role labels, 

scored  by 

another  bilstm

bilstm

(contextualized 
word vectors)
words  + frame

when democrats wondercogitation why there is so much resentment of clinton, they don   t need    

semantic dependency graphs
(delph-in  minimal recursion  semantics-derived  representation;     dm   )

oepen et al. (semeval 2014; 2015), see also  http://sdp.delph-in.net

tanh(c[hwonder; hdemocrats] + b)     arg1

arg1

democrats  wonder

when democrats wonderwhy there is so much resentment of clinton, they don   t need    

arg1

is resentment

comp_so

so much

arg1

much  resentment

arg2

arg1

arg1

arg1

when democrats 

democrats  wonder

why is

resentment  of

arg1

of clinton

when democrats wonderwhy there is so much resentment of clinton, they don   t need    

arg1

is resentment

comp_so

so much

arg1

much  resentment

arg1

wonder why

arg2

arg1

arg1

arg1

when democrats 

democrats  wonder

why is

resentment  of

arg1

of clinton

when democrats wonderwhy there is so much resentment of clinton, they don   t need    

arg1

wonder is

arg1

wonder there

id136 via ad3
(alternating  directions id209; martins et al., 2014)

arg1

is resentment

comp_so

so much

arg1

much  resentment

arg1

wonder why

arg2

arg1

arg1

arg1

when democrats 

democrats  wonder

why is

resentment  of

arg1

of clinton

when democrats wonderwhy there is so much resentment of clinton, they don   t need    

arg1

wonder is

arg1

wonder there

neurboparser
(hao peng, sam thomson, n.a.s., acl  2017)

   

when democrats wonderwhy there is so much resentment of clinton, they don   t need    

training  objective:
structured  hinge 

loss

output:  labeled 

semantic 

dependency 
graph with 
constraints

parts:   labeled 

bilexical

dependencies

bilstm

(contextualized 
word vectors)

words

three formalisms, three separate parsers

arg1

arg1

act

democrats  wonder

democrats  wonder

democrats  wonder

formalism  1 (dm)

formalism  2 (pas)

formalism  3 (psd)

when democrats wonderwhy there is so much resentment of clinton, they don   t need    

shared input representations
(daum  , 2007)

arg1

arg1

act

democrats  wonder

democrats  wonder

democrats  wonder

when democrats wonderwhy there is so much resentment of clinton, they don   t need    

formalism  1 (dm)

formalism  2 (pas)

formalism  3 (psd)

shared  across  all

cross-task parts

arg1

arg1

act

democrats  wonder

democrats  wonder

democrats  wonder

when democrats wonderwhy there is so much resentment of clinton, they don   t need    

shared  across  all

both:  shared input representations
& cross-task parts

arg1

arg1

act

democrats  wonder

democrats  wonder

democrats  wonder

when democrats wonderwhy there is so much resentment of clinton, they don   t need    

formalism  1 (dm)

formalism  2 (pas)

formalism  3 (psd)

shared  across  all

multitask learning:  many possibilities

    shared input representations, parts?  which parts?
    joint decoding?
    overlapping training data?
    scaffold  tasks?

formalism  1 (dm)

formalism  2 (pas)

formalism  3 (psd)

   gold    output

   gold    output

   gold    output

f1 averaged on three semantic  dependency  parsing formalisms, semeval 2015 test set.

88
87
86
85
84
83
82
81
80

du et al., 2015

almeida & 

martins, 2015 
(no syntax)

almeida & 

martins, 2015 

(syntax)

neurboparser     with shared 

ours

input 

representations 
and cross-task 

parts

wsj (in-domain)

brown  (out-of-domain)

neurboparser f1 on three semantic  dependency  parsing formalisms, semeval 2015 test set.

100

90

80

70

60

50

good enough?

dm

wsj (in-domain)

pas
brown  (out-of-domain)

psd

bias?

cross-formalism

sharing

   

when democrats wonderwhy there is so much resentment of clinton, they don   t need    

training  objective:
structured  hinge 

loss

output:  labeled 

semantic 

dependency 
graph with 
constraints

parts:   labeled 

bilexical

dependencies

bilstm

(contextualized 
word vectors)

words

text

text     sentences

larger context

generative language models

history next word

p(w | history)

generative language models

history next word

p(w | history)

entity language model
(yangfeng ji, chenhao tan, sebastian martschat, 
yejin choi, n.a.s.,  emnlp 2017)

when democrats wonderwhy there is so much resentment of

entity language model
(yangfeng ji, chenhao tan, sebastian martschat, 
yejin choi, n.a.s.,  emnlp 2017)

entity 1

when democrats wonderwhy there is so much resentment of

entity language model
(yangfeng ji, chenhao tan, sebastian martschat, 
yejin choi, n.a.s.,  emnlp 2017)

entity 2

entity 1

1. new entity  with new 

vector

2. mention  word will be 

   clinton   

when democrats wonderwhy there is so much resentment of clinton, 

entity language model
(yangfeng ji, chenhao tan, sebastian martschat, 
yejin choi, n.a.s.,  emnlp 2017)

entity 2

entity 1

when democrats wonderwhy there is so much resentment of clinton, 

entity language model
(yangfeng ji, chenhao tan, sebastian martschat, 
yejin choi, n.a.s.,  emnlp 2017)

entity 2

entity 1

1. coreferent of entity 1 
(previously  known as 
   democrats   )

2. mention  word will be 

   they   

3. embedding  of entity  1 

will be updated

when democrats wonderwhy there is so much resentment of clinton, they

entity language model
(yangfeng ji, chenhao tan, sebastian martschat, 
yejin choi, n.a.s.,  emnlp 2017)

entity 2

entity 1

when democrats wonderwhy there is so much resentment of clinton, they

entity language model
(yangfeng ji, chenhao tan, sebastian martschat, 
yejin choi, n.a.s.,  emnlp 2017)

entity 2

entity 1

1. not part of an entity 

mention
2.    don   t   

when democrats wonderwhy there is so much resentment of clinton, they don   t

conll 2012 coreference evaluation.

perplexity on conll 2012 test set.
140

136

132

128

75
65
55
45
35
25

5-gram lm id56 lm

accuracy:  inscript.

entity 

language 

model

always 

new

shallow 
features

modi et 
al. (2017)

entity lm human

75

70

65

60

55

50

martschat  and 
strube (2015)
conll muc f1

reranked with entity 

lm

b3

ceaf

bias?

entities

history next word

p(w | history)

bias in the future?

    linguistic  scaffold tasks.

bias in the future?

    linguistic  scaffold tasks.

    language is by and about people.

bias in the future?

    linguistic  scaffold tasks.

    language is by and about people.

    nlp is needed when texts are costly to read.

bias in the future?

    linguistic  scaffold tasks.

    language is by and about people.

    nlp is needed when texts are costly to read.

    polyglot learning.

bias

data

thank you!

