cs229 lecture notes

andrew ng

part iv
generative learning algorithms

so far, we   ve mainly been talking about learning algorithms that model
p(yjx; (cid:18)), the conditional distribution of y given x. for instance, logistic
regression modeled p(yjx; (cid:18)) as h(cid:18)(x) = g((cid:18)t x) where g is the sigmoid func-
tion. in these notes, we   ll talk about a di(cid:11)erent type of learning algorithm.
consider a classi(cid:12)cation problem in which we want to learn to distinguish
between elephants (y = 1) and dogs (y = 0), based on some features of
an animal. given a training set, an algorithm like id28 or
the id88 algorithm (basically) tries to (cid:12)nd a straight line|that is, a
decision boundary|that separates the elephants and dogs. then, to classify
a new animal as either an elephant or a dog, it checks on which side of the
decision boundary it falls, and makes its prediction accordingly.

here   s a di(cid:11)erent approach. first, looking at elephants, we can build a
model of what elephants look like. then, looking at dogs, we can build a
separate model of what dogs look like. finally, to classify a new animal, we
can match the new animal against the elephant model, and match it against
the dog model, to see whether the new animal looks more like the elephants
or more like the dogs we had seen in the training set.

algorithms that try to learn p(yjx) directly (such as id28),
or algorithms that try to learn mappings directly from the space of inputs x
to the labels f0; 1g, (such as the id88 algorithm) are called discrim-
inative learning algorithms. here, we   ll talk about algorithms that instead
try to model p(xjy) (and p(y)). these algorithms are called generative
learning algorithms. for instance, if y indicates whether a example is a dog
(0) or an elephant (1), then p(xjy = 0) models the distribution of dogs   
features, and p(xjy = 1) models the distribution of elephants    features.

after modeling p(y) (called the class priors) and p(xjy), our algorithm

1

2

can then use bayes rule to derive the posterior distribution on y given x:

p(yjx) =

p(xjy)p(y)

p(x)

:

here, the denominator is given by p(x) = p(xjy = 1)p(y = 1) + p(xjy =
0)p(y = 0) (you should be able to verify that this is true from the standard
properties of probabilities), and thus can also be expressed in terms of the
quantities p(xjy) and p(y) that we   ve learned. actually, if were calculating
p(yjx) in order to make a prediction, then we don   t actually need to calculate
the denominator, since

arg max

y

p(yjx) = arg max

y

p(xjy)p(y)

p(x)

= arg max

y

p(xjy)p(y):

1 gaussian discriminant analysis

the (cid:12)rst generative learning algorithm that we   ll look at is gaussian discrim-
inant analysis (gda). in this model, we   ll assume that p(xjy) is distributed
according to a multivariate normal distribution. lets talk brie(cid:13)y about the
properties of multivariate normal distributions before moving on to the gda
model itself.

1.1 the multivariate normal distribution

the multivariate normal distribution in n-dimensions, also called the multi-
variate gaussian distribution, is parameterized by a mean vector (cid:22) 2 rn
and a covariance matrix (cid:6) 2 rn(cid:2)n, where (cid:6) (cid:21) 0 is symmetric and positive
semi-de(cid:12)nite. also written \n ((cid:22); (cid:6))", its density is given by:

p(x; (cid:22); (cid:6)) =

1

(2(cid:25))n=2j(cid:6)j1=2 exp(cid:18)(cid:0)

1
2

(x (cid:0) (cid:22))t (cid:6)(cid:0)1(x (cid:0) (cid:22))(cid:19) :

in the equation above, \j(cid:6)j" denotes the determinant of the matrix (cid:6).

for a random variable x distributed n ((cid:22); (cid:6)), the mean is (unsurpris-

ingly,) given by (cid:22):

e[x] =zx

x p(x; (cid:22); (cid:6))dx = (cid:22)

the covariance of a vector-valued random variable z is de(cid:12)ned as cov(z) =

e[(z (cid:0) e[z])(z (cid:0) e[z])t ]. this generalizes the notion of the variance of a

3

real-valued random variable. the covariance can also be de(cid:12)ned as cov(z) =
e[zz t ] (cid:0) (e[z])(e[z])t . (you should be able to prove to yourself that these
two de(cid:12)nitions are equivalent.) if x (cid:24) n ((cid:22); (cid:6)), then

cov(x) = (cid:6):

here   re some examples of what the density of a gaussian distribution

look like:

0.25

0.2

0.15

0.1

0.05

0.25

0.2

0.15

0.1

0.05

0.25

0.2

0.15

0.1

0.05

3

2

1

0

   1

2

1

0

   1

   2

   3

   3

   2

3

2

3

1

0

   1

   2

   3

   3

   2

2

1

0

   1

3

2

3

1

0

   1

   2

   3

   3

   2

0

   1

3

2

1

the left-most (cid:12)gure shows a gaussian with mean zero (that is, the 2x1
zero-vector) and covariance matrix (cid:6) = i (the 2x2 identity matrix). a gaus-
sian with zero mean and identity covariance is also called the standard nor-
mal distribution. the middle (cid:12)gure shows the density of a gaussian with
zero mean and (cid:6) = 0:6i; and in the rightmost (cid:12)gure shows one with , (cid:6) = 2i.
we see that as (cid:6) becomes larger, the gaussian becomes more \spread-out,"
and as it becomes smaller, the distribution becomes more \compressed."

lets look at some more examples.

0.25

0.2

0.15

0.1

0.05

3

2

1

0

   1

   2

0.25

0.2

0.15

0.1

0.05

3

2

1

   3

   3

   2

0

   1

3

2

1

0

   1

   2

3

2

0

   1

1

   3

   3

   2

0.25

0.2

0.15

0.1

0.05

3

2

1

0

   1

   2

   3

   3

   2

0

   1

3

2

1

the (cid:12)gures above show gaussians with mean 0, and with covariance

matrices respectively

(cid:6) =(cid:20) 1

0

0

1 (cid:21) ; (cid:6) =(cid:20) 1

0.5

0.5

1 (cid:21) ;

:(cid:6) =(cid:20) 1

0.8

0.8

1 (cid:21) :

the leftmost (cid:12)gure shows the familiar standard normal distribution, and we
see that as we increase the o(cid:11)-diagonal entry in (cid:6), the density becomes more
\compressed" towards the 45(cid:14) line (given by x1 = x2). we can see this more
clearly when we look at the contours of the same three densities:

4

3

2

1

0

   1

   2

   3

   3

3

2

1

0

   1

   2

   3

   3

3

2

1

0

   1

   2

   3

   3

3

2

1

0

   1

   2

   3

   3

   2

   1

0

1

2

3

   2

   1

0

1

2

3

   2

   1

0

1

2

3

here   s one last set of examples generated by varying (cid:6):

3

2

1

0

   1

   2

   3

   3

3

2

1

0

   1

   2

   3

   3

   2

   1

0

1

2

3

   2

   1

0

1

2

3

the plots above used, respectively,

   2

   1

0

1

2

3

(cid:6) =(cid:20) 1

-0.5

-0.5

1 (cid:21) ; (cid:6) =(cid:20) 1

-0.8

-0.8

1 (cid:21) ;

:(cid:6) =(cid:20) 3

0.8

0.8

1 (cid:21) :

from the leftmost and middle (cid:12)gures, we see that by decreasing the diagonal
elements of the covariance matrix, the density now becomes \compressed"
again, but in the opposite direction. lastly, as we vary the parameters, more
generally the contours will form ellipses (the rightmost (cid:12)gure showing an
example).

as our last set of examples, (cid:12)xing (cid:6) = i, by varying (cid:22), we can also move

the mean of the density around.

0.25

0.2

0.15

0.1

0.05

0.25

0.2

0.15

0.1

0.05

0.25

0.2

0.15

0.1

0.05

3

2

1

0

   1

2

1

0

   1

   2

   3

   3

   2

3

2

3

1

0

   1

   2

   3

   3

   2

2

1

0

   1

3

2

3

1

0

   1

   2

   3

   3

   2

0

   1

3

2

1

the (cid:12)gures above were generated using (cid:6) = i, and respectively

(cid:22) =(cid:20) 1

0 (cid:21) ; (cid:22) =(cid:20) -0.5

0 (cid:21) ; (cid:22) =(cid:20) -1

-1.5 (cid:21) :

5

1.2 the gaussian discriminant analysis model

when we have a classi(cid:12)cation problem in which the input features x are
continuous-valued random variables, we can then use the gaussian discrim-
inant analysis (gda) model, which models p(xjy) using a multivariate nor-
mal distribution. the model is:

y (cid:24) bernoulli((cid:30))

xjy = 0 (cid:24) n ((cid:22)0; (cid:6))
xjy = 1 (cid:24) n ((cid:22)1; (cid:6))

writing out the distributions, this is:

p(y) = (cid:30)y(1 (cid:0) (cid:30))1(cid:0)y

p(xjy = 0) =

p(xjy = 1) =

1

(2(cid:25))n=2j(cid:6)j1=2 exp(cid:18)(cid:0)
(2(cid:25))n=2j(cid:6)j1=2 exp(cid:18)(cid:0)

1

1
2
1
2

(x (cid:0) (cid:22)0)t (cid:6)(cid:0)1(x (cid:0) (cid:22)0)(cid:19)
(x (cid:0) (cid:22)1)t (cid:6)(cid:0)1(x (cid:0) (cid:22)1)(cid:19)

here, the parameters of our model are (cid:30), (cid:6), (cid:22)0 and (cid:22)1. (note that while
there   re two di(cid:11)erent mean vectors (cid:22)0 and (cid:22)1, this model is usually applied
using only one covariance matrix (cid:6).) the log-likelihood of the data is given
by

   ((cid:30); (cid:22)0; (cid:22)1; (cid:6)) = log

= log

m

m

yi=1
yi=1

p(x(i); y(i); (cid:30); (cid:22)0; (cid:22)1; (cid:6))

p(x(i)jy(i); (cid:22)0; (cid:22)1; (cid:6))p(y(i); (cid:30)):

by maximizing     with respect to the parameters, we (cid:12)nd the maximum like-
lihood estimate of the parameters (see problem set 1) to be:

6

1fy(i) = 1g

i=1 1fy(i) = 0gx(i)
i=1 1fy(i) = 0g
i=1 1fy(i) = 1gx(i)
i=1 1fy(i) = 1g
m

m

1
m

(cid:30) =

xi=1
(cid:22)0 = pm
pm
(cid:22)1 = pm
pm
xi=1

(cid:6) =

1
m

(x(i) (cid:0) (cid:22)y(i))(x(i) (cid:0) (cid:22)y(i))t :

pictorially, what the algorithm is doing can be seen in as follows:

1

0

   1

   2

   3

   4

   5

   6

   7
   2

   1

0

1

2

3

4

5

6

7

shown in the (cid:12)gure are the training set, as well as the contours of the
two gaussian distributions that have been (cid:12)t to the data in each of the
two classes. note that the two gaussians have contours that are the same
shape and orientation, since they share a covariance matrix (cid:6), but they have
di(cid:11)erent means (cid:22)0 and (cid:22)1. also shown in the (cid:12)gure is the straight line
giving the decision boundary at which p(y = 1jx) = 0:5. on one side of
the boundary, we   ll predict y = 1 to be the most likely outcome, and on the
other side, we   ll predict y = 0.

1.3 discussion: gda and id28

the gda model has an interesting relationship to id28. if we
view the quantity p(y = 1jx; (cid:30); (cid:22)0; (cid:22)1; (cid:6)) as a function of x, we   ll (cid:12)nd that it

7

can be expressed in the form

p(y = 1jx; (cid:30); (cid:6); (cid:22)0; (cid:22)1) =

1

1 + exp((cid:0)(cid:18)t x)

;

where (cid:18) is some appropriate function of (cid:30); (cid:6); (cid:22)0; (cid:22)1.1 this is exactly the form
that id28|a discriminative algorithm|used to model p(y =
1jx).

when would we prefer one model over another? gda and logistic regres-
sion will, in general, give di(cid:11)erent decision boundaries when trained on the
same dataset. which is better?

we just argued that if p(xjy) is multivariate gaussian (with shared (cid:6)),
then p(yjx) necessarily follows a logistic function. the converse, however,
is not true; i.e., p(yjx) being a logistic function does not imply p(xjy) is
multivariate gaussian. this shows that gda makes stronger modeling as-
sumptions about the data than does id28.
it turns out that
when these modeling assumptions are correct, then gda will (cid:12)nd better (cid:12)ts
to the data, and is a better model. speci(cid:12)cally, when p(xjy) is indeed gaus-
sian (with shared (cid:6)), then gda is asymptotically e(cid:14)cient. informally,
this means that in the limit of very large training sets (large m), there is no
algorithm that is strictly better than gda (in terms of, say, how accurately
they estimate p(yjx)). in particular, it can be shown that in this setting,
gda will be a better algorithm than id28; and more generally,
even for small training set sizes, we would generally expect gda to better.
in contrast, by making signi(cid:12)cantly weaker assumptions, logistic regres-
sion is also more robust and less sensitive to incorrect modeling assumptions.
there are many di(cid:11)erent sets of assumptions that would lead to p(yjx) taking
the form of a logistic function. for example, if xjy = 0 (cid:24) poisson((cid:21)0), and
xjy = 1 (cid:24) poisson((cid:21)1), then p(yjx) will be logistic. id28 will
also work well on poisson data like this. but if we were to use gda on such
data|and (cid:12)t gaussian distributions to such non-gaussian data|then the
results will be less predictable, and gda may (or may not) do well.

to summarize: gda makes stronger modeling assumptions, and is more
data e(cid:14)cient (i.e., requires less training data to learn \well") when the mod-
eling assumptions are correct or at least approximately correct. logistic
regression makes weaker assumptions, and is signi(cid:12)cantly more robust to
deviations from modeling assumptions. speci(cid:12)cally, when the data is in-
deed non-gaussian, then in the limit of large datasets, id28 will

1this uses the convention of rede(cid:12)ning the x(i)   s on the right-hand-side to be n + 1-

dimensional vectors by adding the extra coordinate x

(i)
0 = 1; see problem set 1.

8

almost always do better than gda. for this reason, in practice logistic re-
gression is used more often than gda. (some related considerations about
discriminative vs. generative models also apply for the naive bayes algo-
rithm that we discuss next, but the naive bayes algorithm is still considered
a very good, and is certainly also a very popular, classi(cid:12)cation algorithm.)

2 naive bayes

in gda, the feature vectors x were continuous, real-valued vectors. lets now
talk about a di(cid:11)erent learning algorithm in which the xi   s are discrete-valued.
for our motivating example, consider building an email spam (cid:12)lter using
machine learning. here, we wish to classify messages according to whether
they are unsolicited commercial (spam) email, or non-spam email. after
learning to do this, we can then have our mail reader automatically (cid:12)lter
out the spam messages and perhaps place them in a separate mail folder.
classifying emails is one example of a broader set of problems called text
classi(cid:12)cation.

lets say we have a training set (a set of emails labeled as spam or non-
spam). we   ll begin our construction of our spam (cid:12)lter by specifying the
features xi used to represent an email.

we will represent an email via a feature vector whose length is equal to
the number of words in the dictionary. speci(cid:12)cally, if an email contains the
i-th word of the dictionary, then we will set xi = 1; otherwise, we let xi = 0.
for instance, the vector

x =

2

6666666664

1
0
0
...
1
...
0

3

7777777775

a
aardvark
aardwolf
...
buy
...
zygmurgy

is used to represent an email that contains the words \a" and \buy," but not
\aardvark," \aardwolf" or \zygmurgy."2 the set of words encoded into the

2actually, rather than looking through an english dictionary for the list of all english
words, in practice it is more common to look through our training set and encode in our
feature vector only the words that occur at least once there. apart from reducing the
number of words modeled and hence reducing our computational and space requirements,

9

feature vector is called the vocabulary, so the dimension of x is equal to
the size of the vocabulary.

having chosen our feature vector, we now want to build a discriminative
model. so, we have to model p(xjy). but if we have, say, a vocabulary of
50000 words, then x 2 f0; 1g50000 (x is a 50000-dimensional vector of 0   s and
1   s), and if we were to model x explicitly with a multinomial distribution over
the 250000 possible outcomes, then we   d end up with a (250000 (cid:0)1)-dimensional
parameter vector. this is clearly too many parameters.

to model p(xjy), we will therefore make a very strong assumption. we will
assume that the xi   s are conditionally independent given y. this assumption
is called the naive bayes (nb) assumption, and the resulting algorithm is
called the naive bayes classi(cid:12)er. for instance, if y = 1 means spam email;
\buy" is word 2087 and \price" is word 39831; then we are assuming that if
i tell you y = 1 (that a particular piece of email is spam), then knowledge
of x2087 (knowledge of whether \buy" appears in the message) will have no
e(cid:11)ect on your beliefs about the value of x39831 (whether \price" appears).
more formally, this can be written p(x2087jy) = p(x2087jy; x39831). (note that
this is not the same as saying that x2087 and x39831 are independent, which
would have been written \p(x2087) = p(x2087jx39831)"; rather, we are only
assuming that x2087 and x39831 are conditionally independent given y.)

we now have:

p(x1; : : : ; x50000jy)

= p(x1jy)p(x2jy; x1)p(x3jy; x1; x2) (cid:1) (cid:1) (cid:1) p(x50000jy; x1; : : : ; x49999)
= p(x1jy)p(x2jy)p(x3jy) (cid:1) (cid:1) (cid:1) p(x50000jy)

=

n

yi=1

p(xijy)

the (cid:12)rst equality simply follows from the usual properties of probabilities,
and the second equality used the nb assumption. we note that even though
the naive bayes assumption is an extremely strong assumptions, the resulting
algorithm works well on many problems.

our model is parameterized by (cid:30)ijy=1 = p(xi = 1jy = 1), (cid:30)ijy=0 = p(xi =
1jy = 0), and (cid:30)y = p(y = 1). as usual, given a training set f(x(i); y(i)); i =

this also has the advantage of allowing us to model/include as a feature many words
that may appear in your email (such as \cs229") but that you won   t (cid:12)nd in a dictionary.
sometimes (as in the homework), we also exclude the very high frequency words (which
will be words like \the," \of," \and,"; these high frequency, \content free" words are called
stop words) since they occur in so many documents and do little to indicate whether an
email is spam or non-spam.

10

1; : : : ; mg, we can write down the joint likelihood of the data:

l((cid:30)y; (cid:30)ijy=0; (cid:30)ijy=1) =

p(x(i); y(i)):

m

yi=1

maximizing this with respect to (cid:30)y; (cid:30)ijy=0 and (cid:30)ijy=1 gives the maximum
likelihood estimates:

(cid:30)jjy=1 = pm
(cid:30)jjy=0 = pm
(cid:30)y = pm

pm
pm

m

i=1 1fx(i)

j = 1 ^ y(i) = 1g

i=1 1fy(i) = 1g

i=1 1fx(i)

j = 1 ^ y(i) = 0g

i=1 1fy(i) = 0g

i=1 1fy(i) = 1g

in the equations above, the \^" symbol means \and." the parameters have
a very natural interpretation. for instance, (cid:30)jjy=1 is just the fraction of the
spam (y = 1) emails in which word j does appear.

having (cid:12)t all these parameters, to make a prediction on a new example

with features x, we then simply calculate

p(y = 1jx) =

p(xjy = 1)p(y = 1)

p(x)

=

(qn

i=1 p(xijy = 1)) p(y = 1)

i=1 p(xijy = 0)) p(y = 0)

;

(qn
i=1 p(xijy = 1)) p(y = 1) + (qn

and pick whichever class has the higher posterior id203.

lastly, we note that while we have developed the naive bayes algorithm
mainly for the case of problems where the features xi are binary-valued, the
generalization to where xi can take values in f1; 2; : : : ; kig is straightforward.
here, we would simply model p(xijy) as multinomial rather than as bernoulli.
indeed, even if some original input attribute (say, the living area of a house,
as in our earlier example) were continuous valued, it is quite common to
discretize it|that is, turn it into a small set of discrete values|and apply
naive bayes. for instance, if we use some feature xi to represent living area,
we might discretize the continuous values as follows:

living area (sq. feet) < 400

400-800

800-1200

1200-1600 >1600

xi

1

2

3

4

5

thus, for a house with living area 890 square feet, we would set the value
of the corresponding feature xi to 3. we can then apply the naive bayes

11

algorithm, and model p(xijy) with a multinomial distribution, as described
previously. when the original, continuous-valued attributes are not well-
modeled by a multivariate normal distribution, discretizing the features and
using naive bayes (instead of gda) will often result in a better classi(cid:12)er.

2.1 laplace smoothing

the naive bayes algorithm as we have described it will work fairly well
for many problems, but there is a simple change that makes it work much
better, especially for text classi(cid:12)cation. lets brie(cid:13)y discuss a problem with
the algorithm in its current form, and then talk about how we can (cid:12)x it.

consider spam/email classi(cid:12)cation, and lets suppose that, after complet-
ing cs229 and having done excellent work on the project, you decide around
june 2003 to submit the work you did to the nips conference for publication.
(nips is one of the top machine learning conferences, and the deadline for
submitting a paper is typically in late june or early july.) because you end
up discussing the conference in your emails, you also start getting messages
with the word \nips" in it. but this is your (cid:12)rst nips paper, and until this
time, you had not previously seen any emails containing the word \nips";
in particular \nips" did not ever appear in your training set of spam/non-
spam emails. assuming that \nips" was the 35000th word in the dictionary,
your naive bayes spam (cid:12)lter therefore had picked its maximum likelihood
estimates of the parameters (cid:30)35000jy to be
i=1 1fx(i)

(cid:30)35000jy=1 = pm
(cid:30)35000jy=0 = pm

i=1 1fx(i)

pm
pm

35000 = 1 ^ y(i) = 1g
i=1 1fy(i) = 1g
35000 = 1 ^ y(i) = 0g
i=1 1fy(i) = 0g

= 0

= 0

i.e., because it has never seen \nips" before in either spam or non-spam
training examples, it thinks the id203 of seeing it in either type of email
is zero. hence, when trying to decide if one of these messages containing
\nips" is spam, it calculates the class posterior probabilities, and obtains

p(y = 1jx) =

=

i=1 p(xijy = 1)p(y = 1)

qn

i=1 p(xijy = 1)p(y = 1) +qn
qn

:

0
0

i=1 p(xijy = 0)p(y = 0)

this is because each of the terms \qn

i=1 p(xijy)" includes a term p(x35000jy) =
0 that is multiplied into it. hence, our algorithm obtains 0=0, and doesn   t
know how to make a prediction.

12

stating the problem more broadly, it is statistically a bad idea to estimate
the id203 of some event to be zero just because you haven   t seen it be-
fore in your (cid:12)nite training set. take the problem of estimating the mean of
a multinomial random variable z taking values in f1; : : : ; kg. we can param-
eterize our multinomial with (cid:30)i = p(z = i). given a set of m independent
observations fz(1); : : : ; z(m)g, the maximum likelihood estimates are given by

(cid:30)j = pm

i=1 1fz(i) = jg

:

m

as we saw previously, if we were to use these maximum likelihood estimates,
then some of the (cid:30)j   s might end up as zero, which was a problem. to avoid
this, we can use laplace smoothing, which replaces the above estimate
with

(cid:30)j = pm

i=1 1fz(i) = jg + 1

:

m + k

pk

here, we   ve added 1 to the numerator, and k to the denominator. note that
j=1 (cid:30)j = 1 still holds (check this yourself!), which is a desirable property
since the (cid:30)j   s are estimates for probabilities that we know must sum to 1.
also, (cid:30)j 6= 0 for all values of j, solving our problem of probabilities being
estimated as zero. under certain (arguably quite strong) conditions, it can
be shown that the laplace smoothing actually gives the optimal estimator
of the (cid:30)j   s.

returning to our naive bayes classi(cid:12)er, with laplace smoothing, we

therefore obtain the following estimates of the parameters:

(cid:30)jjy=1 = pm
(cid:30)jjy=0 = pm

i=1 1fx(i)

j = 1 ^ y(i) = 1g + 1

i=1 1fy(i) = 1g + 2

i=1 1fx(i)

j = 1 ^ y(i) = 0g + 1

i=1 1fy(i) = 0g + 2

pm
pm

(in practice, it usually doesn   t matter much whether we apply laplace smooth-
ing to (cid:30)y or not, since we will typically have a fair fraction each of spam and
non-spam messages, so (cid:30)y will be a reasonable estimate of p(y = 1) and will
be quite far from 0 anyway.)

2.2 event models for text classi(cid:12)cation

to close o(cid:11) our discussion of generative learning algorithms, lets talk about
one more model that is speci(cid:12)cally for text classi(cid:12)cation. while naive bayes

as we   ve presented it will work well for many classi(cid:12)cation problems, for text
classi(cid:12)cation, there is a related model that does even better.

13

in the speci(cid:12)c context of text classi(cid:12)cation, naive bayes as presented uses
the what   s called the multi-variate bernoulli event model. in this model,
we assumed that the way an email is generated is that (cid:12)rst it is randomly
determined (according to the class priors p(y)) whether a spammer or non-
spammer will send you your next message. then, the person sending the
email runs through the dictionary, deciding whether to include each word i
in that email independently and according to the probabilities p(xi = 1jy) =

i=1 p(xijy).

(cid:30)ijy. thus, the id203 of a message was given by p(y)qn

here   s a di(cid:11)erent model, called the multinomial event model. to de-
scribe this model, we will use a di(cid:11)erent notation and set of features for
representing emails. we let xi denote the identity of the i-th word in the
email. thus, xi is now an integer taking values in f1; : : : ; jv jg, where jv j
is the size of our vocabulary (dictionary). an email of n words is now rep-
resented by a vector (x1; x2; : : : ; xn) of length n; note that n can vary for
di(cid:11)erent documents. for instance, if an email starts with \a nips . . . ,"
then x1 = 1 (\a" is the (cid:12)rst word in the dictionary), and x2 = 35000 (if
\nips" is the 35000th word in the dictionary).

in the multinomial event model, we assume that the way an email is
generated is via a random process in which spam/non-spam is (cid:12)rst deter-
mined (according to p(y)) as before. then, the sender of the email writes the
email by (cid:12)rst generating x1 from some multinomial distribution over words
(p(x1jy)). next, the second word x2 is chosen independently of x1 but from
the same multinomial distribution, and similarly for x3, x4, and so on, until
all n words of the email have been generated. thus, the overall id203 of
i=1 p(xijy). note that this formula looks like the
one we had earlier for the id203 of a message under the multi-variate
bernoulli event model, but that the terms in the formula now mean very dif-
ferent things. in particular xijy is now a multinomial, rather than a bernoulli
distribution.

a message is given by p(y)qn

the parameters for our new model are (cid:30)y = p(y) as before, (cid:30)ijy=1 =
p(xj = ijy = 1) (for any j) and (cid:30)ijy=0 = p(xj = ijy = 0). note that we have
assumed that p(xjjy) is the same for all values of j (i.e., that the distribution
according to which a word is generated does not depend on its position j
within the email).

if we are given a training set f(x(i); y(i)); i = 1; : : : ; mg where x(i) =
1 ; x(i)
ni ) (here, ni is the number of words in the i-training example),

2 ; : : : ; x(i)

(x(i)

the likelihood of the data is given by

m

14

l((cid:30); (cid:30)ijy=0; (cid:30)ijy=1) =

=

p(x(i); y(i))

m

yi=1
yi=1  ni
yj=1

p(x(i)

j jy; (cid:30)ijy=0; (cid:30)ijy=1)! p(y(i); (cid:30)y):

maximizing this yields the maximum likelihood estimates of the parameters:

j = k ^ y(i) = 1g

j=1 1fx(i)
i=1 1fy(i) = 1gni
j=1 1fx(i)
i=1 1fy(i) = 0gni

j = k ^ y(i) = 0g

(cid:30)kjy=1 = pm
(cid:30)kjy=0 = pm
(cid:30)y = pm

i=1pni
pm
i=1pni
pm

m

i=1 1fy(i) = 1g

:

if we were to apply laplace smoothing (which needed in practice for good
performance) when estimating (cid:30)kjy=0 and (cid:30)kjy=1, we add 1 to the numerators
and jv j to the denominators, and obtain:

(cid:30)kjy=1 = pm
(cid:30)kjy=0 = pm

i=1pni
pm
i=1pni
pm

j = k ^ y(i) = 1g + 1

j=1 1fx(i)
i=1 1fy(i) = 1gni + jv j
j=1 1fx(i)
i=1 1fy(i) = 0gni + jv j

j = k ^ y(i) = 0g + 1

:

while not necessarily the very best classi(cid:12)cation algorithm, the naive bayes
classi(cid:12)er often works surprisingly well. it is often also a very good \(cid:12)rst thing
to try," given its simplicity and ease of implementation.

