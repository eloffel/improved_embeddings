probabilistic machine learning:

foundations and frontiers

zoubin ghahramani1,2,3,4

1 university of cambridge

2 alan turing institute

3 leverhulme centre for the future of intelligence

4 uber ai labs

zoubin@eng.cam.ac.uk

http://mlg.eng.cam.ac.uk/zoubin/

sackler forum, national academy of sciences

washington dc, 2017

machine learning

zoubin ghahramani

2 / 53

many related terms

artificial intelligence

statistical modelling

neural networks

data mining

machine learning

data analytics

deep learning

pattern recognition

data science

zoubin ghahramani

3 / 53

many related fields

computer science

engineering

computational 
neuroscience

machine learning

economics

statistics

applied mathematics

cognitive science

physics

zoubin ghahramani

4 / 53

many many applications

bioinformatics

robotics

id161

scientific data analysis

information retrieval

machine learning

natural language  

processing

id103

signal processing

machine translation

recommender systems

medical informatics

targeted advertising

finance

zoubin ghahramani

data compression

5 / 53

machine learning

    machine learning is an interdisciplinary field that develops both the 

mathematical foundations and practical applications of  systems 
that learn from data. 

main conferences and journals: nips, icml, aistats, uai, kdd, jmlr, ieee tpami

zoubin ghahramani

6 / 53

canonical problems in machine learning

zoubin ghahramani

7 / 53

classification

x

x

x

x

x

x

x

x

x

x

o

x

o

o

o o
o

o

o

    task: predict discrete class label from input data 

    applications: face recognition, image recognition, medical diagnosis    

    methods: id28, support vector machines (id166s), 
neural networks, id79s, gaussian process classifiers   

zoubin ghahramani

8 / 53

regression

d

y

x

    task: predict continuous quantities from input data 

    applications: financial forecasting, click-rate prediction,     

    methods: id75, neural networks, gaussian processes,    

zoubin ghahramani

9 / 53

id91

    task: group data together so that similar points are in the same group 

)

    applications: bioinformatics, astronomy, document modelling, 

network modelling,     

    methods: id116, gaussian mixtures, dirichlet process mixtures,    

zoubin ghahramani

10 / 53

id84

2

1.5

1

0.5

0

!0.5

!1

!1.5

!2
1

0.5

0

!0.5

!1

0

1

0.5

    task: map high-dimensional data onto low dimensions while 

preserving relevant information 

    applications: any where the raw data is high-dimensional 

    methods: pca, factor analysis, mds, lle, isomap, gplvm,   

zoubin ghahramani

11 / 53

semi-supervised learning

   

   

   

   

   

   

    task: learn from both labelled and unlabelled data 

    applications: any where labelling data is expensive, e.g. vision,speech    

    methods: probabilistic models, graph-based ssl, transductive id166s   

zoubin ghahramani

12 / 53

id161:  

object, face and handwriting recognition, 

image captioning

computer games

autonomous vehicles

neural networks and deep learning

zoubin ghahramani

14 / 53

neu ral networks

outputs

weights

hidden
units

weights

inputs

y

x

neural networks
data: d = {(x(n), y(n))}n
parameters    are weights of neural net.
neural nets model p(y(n)|x(n),   ) as a nonlin-
ear function of    and x, e.g.:

n=1 = (x, y)

p(y(n) = 1|x(n),   ) =   (!i

  ix

(n)
i

)

multilayer neural networks model the overall function as a
composition of functions (layers), e.g.:

y(n) = !j

  

(2)

j   (!i

  

(1)
ji x

(n)
i

) +   (n)

usually trained to maximise likelihood (or penalised likelihood) using
variants of stochastic id119 (sgd) optimisation.

zoubin ghahramani

15 / 53

dee p learning

deep learning systems are neural network models similar to
those popular in the    80s and    90s, with:

    some architectural and algorithmic innovations (e.g. many

layers, relus, dropout, lstms)

    vastly larger data sets (web-scale)

    vastly larger-scale compute resources (gpu, cloud)

    much better software tools (theano, torch, tensorflow)

    vastly increased industry investment and media hype

zoubin ghahramani

16 / 53

   gure from http://www.andreykurenkov.com/

limitations of deep learning

neural networks and deep learning systems give amazing
performance on many benchmark tasks but they are generally:

    very data hungry (e.g. often millions of examples)

    very compute-intensive to train and deploy (cloud gpu

resources)

    poor at representing uncertainty

    easily fooled by adversarial examples

       nicky to optimise: non-convex + choice of architecture,

learning procedure, initialisation, etc, require expert
knowledge and experimentation

    uninterpretable black-boxes, lacking in trasparency,

dif   cult to trust

zoubin ghahramani

17 / 53

beyond deep learning

zoubin ghahramani

18 / 53

machine learning as
probabilistic modelling

    a model describes data that one could observe

from a system

    if we use the mathematics of id203

theory to express all forms of uncertainty and
noise associated with our model...

    ...then inverse id203 (i.e. bayes rule)

allows us to infer unknown quantities, adapt
our models, make predictions and learn from
data.

zoubin ghahramani

19 / 53

bayes rule

p(hypothesis|data) =

p(hypothesis)p(data|hypothesis)

"h p(h)p(data|h)

    bayes rule tells us how to do id136
about hypotheses (uncertain quantities)
from data (measured quantities).

    learning and prediction can be seen as

forms of id136.

reverend thomas bayes (1702-1761)

zoubin ghahramani

20 / 53

one slide on bayesian machine learning

everything follows from two simple rules:

sum rule:
product rule: p(x, y) = p(x)p(y|x)

p(x) = "y p(x, y)

learning:

p(  |d, m) =

p(d|  , m)p(  |m)

p(d|m)

p(d|  , m)
p(  |m)
p(  |d, m)

likelihood of parameters    in model m
prior id203 of   
posterior of    given data d

prediction:

p(x|d, m) = # p(x|  , d, m)p(  |d, m)d  

model comparison:

zoubin ghahramani

p(m|d) =

p(d|m)p(m)

p(d)

21 / 53

why should we care?

calibrated model and prediction uncertainty: getting
systems that know when they don   t know.

automatic model complexity control and structure learning
(bayesian occam   s razor)

zoubin ghahramani

22 / 53

what do i mean by being bayesian?

let   s return to the example of neural networks / deep learning:
dealing with all sources of parameter uncertainty
also potentially dealing with structure uncertainty

outputs

weights

hidden
units

weights

inputs

y

x

feedforward neural nets model p(y(n)|x(n),   )

parameters    are weights of neural net.

structure is
the choice of architecture,
number of hidden units and layers, choice of
id180, etc.

zoubin ghahramani

23 / 53

when do we need probabilities?

zoubin ghahramani

25 / 53

whe n is the probabilistic approach
essential?

many aspects of learning and intelligence depend crucially on
the careful probabilistic representation of uncertainty:

    forecasting

    decision making

    learning from limited, noisy, and missing data

    learning complex personalised models

    data compression

    automating scienti   c modelling, discovery, and

experiment design

zoubin ghahramani

26 / 53

automating model discovery:

the automatic statistician

zoubin ghahramani

28 / 53

the aut omatic statistician

language of models

translation

data

search

model

prediction

report

evaluation

checking

problem: data are now ubiquitous; there is great value from
understanding this data, building models and making
predictions... however, there aren   t enough data scientists,
statisticians, and machine learning experts.

zoubin ghahramani

29 / 53

the aut omatic statistician

language of models

translation

data

search

model

prediction

report

evaluation

checking

problem: data are now ubiquitous; there is great value from
understanding this data, building models and making
predictions... however, there aren   t enough data scientists,
statisticians, and machine learning experts.

solution: develop a system that automates model discovery
from data:

    processing data, searching over models, discovering a good
model, and explaining what has been discovered to the user.

zoubin ghahramani

29 / 53

background: gaussian processes

consider the problem of nonid75: you want to
learn a function f with error bars from data d = {x, y}

y

x

a gaussian process de   nes a distribution over functions p(f ) which
can be used for bayesian regression:

p(f |d) =

p(f )p(d|f )

p(d)

de   nition: p(f ) is a gaussian process if for any    nite subset
{x1, . . . , xn}     x , the marginal distribution over that subset p(f) is
multivariate gaussian.

gps can be used for regression, classi   cation, ranking, dim. reduct...

zoubin ghahramani

31 / 53

automatic statistician for regression

and time-series models

zoubin ghahramani

33 / 53

the atoms of our language of models

five base kernels

0

0

0

0

0

squared

periodic

exp. (se)

(per)

linear

(lin)

constant

white

(c)

noise (wn)

encoding for the following types of functions

smooth

periodic

linear

constant

gaussian

functions

functions

functions

functions

noise

zoubin ghahramani

35 / 53

the composition rules of our language

    two main operations: addition, multiplication

0

0

lin    lin

quadratic
functions

se    per

locally
periodic

0

lin + per

periodic plus
linear trend

0

se + per

periodic plus
smooth trend

zoubin ghahramani

36 / 53

mod el search: m auna loa keeling curve

    

      

      

      

   

         

start

se

rq

lin

per

            

            

            

se + rq

. . .

per + rq

. . .

per    rq

se + per + rq

. . .

se    (per + rq)

. . .

zoubin ghahramani

37 / 53

. . .

. . .

. . .

mod el search: m auna loa keeling curve

      

      

      

      

   

                          

start

se

rq

lin

per

            

            

            

se + rq

. . .

per + rq

. . .

per    rq

se + per + rq

. . .

se    (per + rq)

. . .

zoubin ghahramani

37 / 53

. . .

. . .

. . .

mod el search: m auna loa keeling curve

      

      

      

      

      

                                     

start

se

rq

lin

per

            

            

            
se + rq

. . .

per + rq

. . .

per    rq

se + per + rq

. . .

se    (per + rq)

. . .

zoubin ghahramani

37 / 53

. . .

. . .

. . .

mod el search: m auna loa keeling curve

      

      

      

   

                                                      

start

se

rq

lin

per

            

            

            
se + rq

. . .

per + rq

. . .

per    rq

se + per + rq

. . .

se    (per + rq)

. . .

zoubin ghahramani

37 / 53

. . .

. . .

. . .

exa mple: an entirely automatic analysis

raw data

full model posterior with extrapolations

700

600

500

400

300

200

100

700

600

500

400

300

200

100

0

1950

1952

1954

1956

1958

1960

1962

1950

1952

1954

1956

1958

1960

1962

four additive components have been identi   ed in the data

    a linearly increasing function.

    an approximately periodic function with a period of 1.0 years and

with linearly increasing amplitude.

    a smooth function.

    uncorrelated noise with linearly increasing standard deviation.

zoubin ghahramani

38 / 53

goo d predictive performance as well

standardised rmse over 13 data sets

 

e
s
m
r
d
e
s
d
r
a
d
n
a
s

t

i

5
3

.

0
3

.

5
2

.

0
2

.

5
1

.

0
.
1

abcd

abcd

accuracy

interpretability

spectral
kernels

trend, cyclical

bayesian

irregular

mkl

eureqa

changepoints

squared

linear

exponential

regression

    tweaks can be made to the algorithm to improve accuracy

or interpretability of models produced. . .

    . . . but both methods are highly competitive at extrapolation

zoubin ghahramani

40 / 53

automating id136:

probabilistic programming

zoubin ghahramani

42 / 53

probabilistic programming

problem: probabilistic model development and the derivation
of id136 algorithms is time-consuming and error-prone.

zoubin ghahramani

43 / 53

probabilistic programming

problem: probabilistic model development and the derivation
of id136 algorithms is time-consuming and error-prone.
solution:

    develop probabilistic programming languages for

expressing probabilistic models as computer programs that
generate data (i.e. simulators).

    derive universal id136 engines for these languages
that do id136 over program traces given observed data
(bayes rule on computer programs).

zoubin ghahramani

43 / 53

probabilistic programming

problem: probabilistic model development and the derivation
of id136 algorithms is time-consuming and error-prone.
solution:

    develop probabilistic programming languages for

expressing probabilistic models as computer programs that
generate data (i.e. simulators).

    derive universal id136 engines for these languages
that do id136 over program traces given observed data
(bayes rule on computer programs).

example languages: bugs, infer.net, blog, stan, church,
venture, anglican, probabilistic c, stochastic python*, haskell*,
turing*, ...

example id136 algorithms: metropolis-hastings, variational
id136, particle    ltering, particle cascade, slice sampling*, particle
mcmc, nested particle id136*, austerity mcmc*

zoubin ghahramani

43 / 53

automating optimisation:

bayesian optimisation

zoubin ghahramani

45 / 53

con clusions

probabilistic modelling offers a framework for building
systems that reason about uncertainty and learn from data,
going beyond traditional pattern recognition problems.

i have brie   y reviewed some of the frontiers of our research,
centred around the theme of automating machine learning,
including:

    the automatic statistician

    probabilistic programming

    bayesian optimisation

ghahramani, z. (2015) probabilistic machine learning and arti   cial
intelligence. nature 521:452   459.

http://www.nature.com/nature/journal/v521/n7553/full/nature14541.html

zoubin ghahramani

47 / 53

col laborators

ryan p. adams
yutian chen
david duvenaud
yarin gal
hong ge
michael a. gelbart
roger grosse
jos   miguel hern  ndez-lobato
matthew w. hoffman

james r. lloyd
david j. c. mackay
adam   scibior
amar shah
emma smith
christian steinruecken
joshua b. tenenbaum
andrew g. wilson

zoubin ghahramani

48 / 53

pap ers

general:

ghahramani, z. (2013) bayesian nonparametrics and the probabilistic approach to modelling.
philosophical trans. royal society a 371: 20110553.

ghahramani, z. (2015) probabilistic machine learning and arti   cial intelligence nature
521:452   459. http://www.nature.com/nature/journal/v521/n7553/full/nature14541.html

automatic statistician:

website: http://www.automaticstatistician.com

duvenaud, d., lloyd, j. r., grosse, r., tenenbaum, j. b. and ghahramani, z. (2013) structure
discovery in nonparametric regression through compositional kernel search. icml 2013.

lloyd, j. r., duvenaud, d., grosse, r., tenenbaum, j. b. and ghahramani, z. (2014) automatic
construction and natural-language description of nonparametric regression models aaai
2014. http://arxiv.org/pdf/1402.4304v2.pdf

lloyd, j. r., and ghahramani, z. (2015) statistical model criticism using kernel two sample
tests. http://mlg.eng.cam.ac.uk/lloyd/papers/kernel-model-checking.pdf. nips 2015.

bayesian optimisation:

hern  ndez-lobato, j. m., hoffman, m. w., and ghahramani, z. (2014) predictive id178
search for ef   cient global optimization of black-box functions. nips 2014

hern  ndez-lobato, j.m., gelbart, m.a., adams, r.p., hoffman, m.w., ghahramani, z. (2016)
a general framework for constrained bayesian optimization using information-based search.
journal of machine learning research. 17(160):1   53.

zoubin ghahramani

49 / 53

pap ers ii

probabilistic programming:

turing: https://github.com/yebai/turing.jl

chen, y., mansinghka, v., ghahramani, z. (2014) sublinear-time approximate mcmc
transitions for probabilistic programs. arxiv:1411.1690

ge, hong, adam scibior, and zoubin ghahramani (2016) turing: rejuvenating probabilistic
programming in julia. (in preparation).

bayesian neural networks:

jos   miguel hern  ndez-lobato and ryan adams. probabilistic id26 for scalable
learning of bayesian neural networks. icml, 2015.

yarin gal and zoubin ghahramani. dropout as a bayesian approximation: representing model
uncertainty in deep learning. icml, 2016.

yarin gal and zoubin ghahramani. a theoretically grounded application of dropout in recurrent
neural networks. nips, 2016.

jos   miguel hern  ndez-lobato, yingzhen li, daniel hern  ndez-lobato, thang bui, and
richard e turner. black-box alpha divergence minimization. icml, 2016.

zoubin ghahramani

50 / 53

bayesian neural networks and gaussian

processes

outputs

weights

hidden
units

weights

inputs

y

x

bayesian neural network
data: d = {(x(n), y(n))}n
n=1 = (x, y)
parameters    are weights of neural net

prior
posterior
prediction

p(  |  )
p(  |  , d)     p(y|x,   )p(  |  )

p(y   |d, x   ,   ) =$ p(y   |x   ,   )p(  |d,   ) d  

a neural network with one hidden layer, in   nitely

many hidden units and gaussian priors on the weights
    a gp (neal, 1994). he also analysed in   nitely deep

y

networks.

x

zoubin ghahramani

51 / 53

mod el checking and criticism

    good statistical modelling should include model criticism:

    does the data match the assumptions of the model?

    our automatic statistician does posterior predictive checks,

dependence tests and residual tests

    we have also been developing more systematic

nonparametric approaches to model criticism using kernel
two-sample testing:

    lloyd, j. r., and ghahramani, z. (2015) statistical model criticism using

kernel two sample tests. nips 2015.

zoubin ghahramani

52 / 53

bayesian optimisation

figure 4. classi   cation error of a 3-hidden-layer neural network
constrained to make predictions in under 2 ms.

(work with j.m. hern  ndez-lobato, m.a. gelbart, m.w. hoffman, & r.p.

adams) arxiv:1511.09422 arxiv:1511.07130 arxiv:1406.2541

zoubin ghahramani

53 / 53

