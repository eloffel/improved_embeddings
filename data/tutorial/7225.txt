deep learning for speech/language 

processing

--- machine learning & signal processing perspectives

li deng 

deep learning technology center

microsoft research, redmond, usa

tutorial given at interspeech, sept 6, 2015

thanks go to many colleagues at dltc/msr, collaborating universities, 

and at microsoft   s engineering groups 

outline

    part i: basics of machine learning (deep 

and shallow) and of signal processing

    part ii: speech

    part iii: language
(in case you did not get link to slides, send email to: 
alexander.raake@tu-ilmenau.de)

2

reading material

books:
bengio, yoshua (2009). "learning deep 
architectures for ai".

l. deng and d. yu (2014) "deep 
learning: methods and applications" 

http://research.microsoft.com/pubs/209355/deeplearning-nowpublishing-vol7-sig-039.pdf

d. yu and l. deng (2014). "automatic 
id103: a deep learning 
approach    (publisher: springer).

a deep-learning 
approach

3

reading material

4

5

6

reading material (cont   d)

wikipedia:
https://en.wikipedia.org/wiki/deep_learning

papers:
g. e. hinton, r. salakutdinov. "reducing the dimensionality of data with neural networks". science 313: 
504   507, 2016.

g. e. hinton, l. deng, d. yu, etc. "deep neural networks for acoustic modeling in id103: the 
shared views of four research groups," ieee signal processing magazine, pp. 82   97, november 2012.

g. dahl, d. yu, l. deng, a.  acero. "context-dependent pre-trained deep neural networks for large-
vocabulary id103". ieee trans. audio, speech, and language processing, vol 20(1): 30   42, 
2012.  (plus other papers in the same special issue)

y. bengio, a. courville, and p. vincent. "representation learning: a review and new perspectives," ieee 
trans. pami, special issue learning deep architectures, 2013.

j. schmidhuber.    deep learning in neural networks: an overview,    arxiv, october 2014.

y. lecun, y. bengio, and g. hinton.    deep learning   , nature, vol. 521, may 2015.

j. bellegarda and c. monz.    state of the art in statistical methods for language and speech processing,    
computer speech and language, 2015

7

part i: machine learning (deep/shallow)

and signal processing

8

machine learning basics

- survey of audience background 

part i:  basics
part ii: advanced topics (tomorrow)

cognitive
science

revised slide from: pascal vincent, 2015

9

machine learning & deep learning

machine learning

data

analysis/

statistics

programs

deep learning

machine 
learning

what is deep learning?

deep learning (deep machine learning, or deep structured
learning, or hierarchical
learning, or sometimes dl) is a
branch of machine learning based on a set of algorithms that
attempt to model high-level abstractions in data by using
model architectures, with complex structures or otherwise,
composed of multiple non-linear transformations.[1](p198)[2][3][4]

11

(slide from: yoshua bengio)

12

shallow

boosting

id88

ae

d-ae

deep
y lecun

ma ranzato

modified from

neural net

id56

conv. net

id166

rbm

dbn dbm

gmm

sparse 
coding

bayesnp

bayes nets

sp

decisiontree

shallow

boosting

neural networks

id88

ae

d-ae

deep
y lecun

ma ranzato

modified from

deep neural 
net
id56

conv. net

id166

rbm

dbn dbm

sparse 
coding

gmm

bayesnp

bayes nets

sp

probabilistic models

decisiontree

shallow

boosting

neural networks

id88

ae

d-ae

deep
y lecun

ma ranzato

modified from

deep neural 
net
id56

conv. net

id166

rbm

dbn dbm

?gmm

sparse 
coding

bayesnp

?bayes nets

sp

probabilistic models

decisiontree

supervised

unsupervised

supervised

signal processing     information processing
image/
animation/
graphics

audio/music

speech

signals 

processing

video

text/
language

coding/
compression

audio
coding

speech 
coding

image 
coding

video
coding

document
compression/
summary

communication

voice over ip, dab,etc

4g/5g networks, dvb, home 

networking, etc

security

multimedia watermarking, encryption, etc.

enhancement/
analysis

de-noising/
source separation

speech 
enhancement/
feature extraction

image/video enhancement (clear 
type), segmentation, feature 
extraction

grammar 
checking, text 
parsing

synthesis/
rendering

computer 
music

speech
synthesis
(text-to-speech)

computer 
graphics/

video 
synthesis

user-interface

multi-modal human computer interaction (hci --- input methods)

recognition

understanding
(semantic ie)

auditory
scene analysis
(computer 
audition; e.g. 
melody detection 
& singer id)

automatic 
speech/speaker
recognition

image 
recognition

spoken 
language
understanding

image 
understanding

computer
vision
(e.g. 3-d object 
recognition)

natural
language
generation

document
recognition

natural 
language 
understanding

retrieval/mining

music
retrieval

translation

spoken document 
retrieval & 
voice/mobile search
speech translation
speech translation

social media apps

.

.

image 
retrieval

video 
search

text search
(info retrieval)

photo sharing 
(e.g. flickr)

video sharing
(e.g. youtube, 

machine translation

16

blogs, wiki, 
del.icio.us   

machine learning basics

17

1060-1089

18

input

output

decision function

id168

generative models

generative or
discriminative models

discriminative loss (form 
varies)

inductive 

transductive 

generative

discriminative

supervised

unsupervised

semi-supervised

active

adaptive

multitask

tsftsiiy   }{);,(ln   yxpl      uiix   }{tsiitriiixyx      }{ and)},{(triiiyx   )},{(triiiyx   )},{(uiitriiixyx      }{ and)},{(uliiuiitriiiyxyx            }{ and}{ and)},{(adiiitryxf   )},{( and kyxktriii allfor )},{(   tsfkyfktsiik allfor }{or     supervised machine learning 

(classification)

training phase (usually offline)

training data set

learned model

training
algorithm

measurements (features)

& 

associated    class    labels

(colors used to show class labels)

parameters/weights
(and sometimes structure)

supervised machine learning 

(classification)

test phase (run time, online)

input test data point

learned model

output

measurements (features) only

structure + parameters

predicted class label or 
label sequence (e.g. sentence)

a key ml concept: generalization

under-fitting

r
o
r
r
e

over-fitting

best

generalization

test set error

training set

error

model capacity (e.g. size, depth)

    to avoid over-fitting,
    the need for id173 (or make model simpler, or to add more training)
        move the training objective away from (empirical) error rate

generalization     effects of more data

under-fitting

r
o
r
r
e

over-fitting

best

generalization

test set error

training set

error

model capacity

a variety of ml methods

    id90/forests/jungles, boosting
    support vector machines (id166s)
    model-based (id114, often generative models: 

sparse connections w. interpretability)
    model tailored for each new application and incorporates prior 

knowledge

    bayesian statistics exploited to    invert the model    & infer variables of 

interest

    neural networks (dnn, id56, dense connections)

these two types of methods can be made deep: deep generative models and dnns

recipe for (supervised) deep

learning with big data

(i.e., underfitting)

(i.e., overfitting)

deeper

contrast with signal processing approaches

    strong focus on sophisticated objective 

functions for optimization (e.g. mce, mwe, 
mpe, mmi, string-level, super-string-level,    )
    can be regarded as    end2end    learning in asr
    almost always non-id76 

(praised by deep-ml researchers)

    weaker focus on id173 & overfitting
    why?
    our asr community has been using shallow, 
low-capacity models for too long (e.g., gmm-
id48)

    less need for overcoming overfitting
    now, deep models add a new dimension for 

increasing model capacity

    id173 becomes essential for dnn
    e.g. dbn pre-training,    dropout    method, 

stdp spiking neurons, etc.

2008

26

deep neural net (dnn) basics

--- why gradient vanishes & how to rescue it

27

(shallow) neural networks for asr

(prior to the rise of deep learning)

temporal & time-delay (1-d convolutional) neural nets
   

atlas, homma, and marks,    an id158 for spatio-temporal bipolar patterns, 
application to phoneme classification,    nips, 1988.

    waibel, hanazawa, hinton, shikano, lang.    phoneme recognition using time-delay neural 

networks.    ieee transactions on acoustics, speech and signal processing, 1989.

hybrid neural nets-id48
    morgan and bourlard.    continuous id103 using mlp with id48s,    icassp, 1990.
recurrent neural nets
   

bengio.    id158s and their application to speech/sequence recognition   , ph.d. 
thesis, 1991.
robinson.    a real-time recurrent error propagation network word recognition system,    icassp 
1992.

   

neural-net nonlinear prediction
   

deng, hassanein, elmasry.    analysis of correlation structure for a neural predictive model with 
applications to id103,    neural networks, vol. 7, no. 2, 1994.

schuster, paliwal. "id182," ieee trans. signal processing, 1997.

bidirectional recurrent neural nets
   
neural-net tandem
   

hermansky, ellis, sharma. "tandem connectionist feature extraction for conventional id48 
systems." icassp 2000.

    morgan, zhu, stolcke, sonmez, sivadas, shinozaki, ostendorf, jain, hermansky, ellis, doddington, 
chen, cretin, bourlard, athineos,    pushing the envelope - aside [id103],    ieee signal 
processing magazine, vol. 22, no. 5, 2005.
    darpa ears program 2001-2004: novel approach i  (novel approach ii: deep generative model)

bottle-neck features extracted from neural-nets
   

grezl, kara   at, kontar & cernocky.    probabilistic and bottle-neck features for lvcsr of meetings,    
icassp, 2007.

1988
1989

1990

1991

1992

1994
1997

2000

2005

2007

28

one-hidden-layer neural networks

    starting from (nonlinear) regression

    replace each fj with a variable zj,

where

and h() is a fixed activation function

    the outputs obtained from

where s() is another fixed function

   

in all, we have (simplifying biases):

multi-layer neural networks

denote all activation 
functions by h

    the sum is over those values of j with 

instantiated weights wkj

unchallenged learning algorithm: back propagation (bp)

    for regression, we consider a squared error cost 

function:

e(w) =    sn sk ( tnk     yk(xn,w) )2

which corresponds to a gaussian density p(t|x)

    we can substitute

and use a general purpose optimizer to estimate w, 
but it is much more efficient to exploit derivatives of e, 
the essence of bp

learning neural networks

e(w) =    sn sk ( tnk     yk(xn,w) )2

    recall that for id75:

   e(w)/   wm = -sn  ( tn - yn )   xnm

weight in-between error 
signal and input signal

error signal

input signal

    we   ll use the chain rule of differentiation to derive a 

similar-looking expression, where

    local input signals are forward-propagated from the input

    local error signals are back-propagated from the output 

local signals needed for learning

    for clarity, consider the error for one training case:

    to compute    en/   wji, note that wji appears in only one 

term of the overall expression, namely

if wji is in the 1st layer, 
zi is actually input xi

    using the chain rule of differentiation, we have

weight

local 
error 
signal

local 
input 
signal

where

forward-propagating local input signals

    forward propagation gives all the a   s and z   s

back-propagating local error signals

t2

t1

    back-propagation gives all the d    s

back-propagating error signals

    to compute    en/   aj (i.e., dj), aj (also called    logit   ) appears in 
all those expressions ak = si wki h(ai) that depend on aj

    using the chain rule, we have

    the sum is over k s.t. unit j is connected to unit k and 

for each such term,    ak/   aj = wkj h   (aj)

    noting that    en/   ak = dk, we get the back-propagation 

rule:

    for output units:          -

putting the propagations together

    for each training case n, apply forward propagation 

and back-propagation to compute

for each weight wji

    sum these over training cases to compute

    use these derivatives for steepest descent learning 

(too slow for large set of training data)

    minibatch learning: after a small set of input samples, 

use the above gradient to update the weights (so 
update more often)

why gradients tend to vanish for dnn

    recall bp for adjacent layer pair:
    for sigmoid units: if the = h       (1-h      )
    if there is one hidden layer,

gradients are not likely to vanish

    problem becomes serious when nets

get deep

39

why gradients tend to vanish for dnn

y

i

w3,b3

h2

w2,b2

h1

w1,b1

x

y

i

w3,b3

h2

w2,b2

h1

w1,b1

x

upward pass

downward pass

3    (1)inininininderroryydds      323  ()iijjijywhbs         2212()iijjijhwhbs         111()iijjijhwxbs         22233(1)jnjnjnijinupstreamihhwdd         11122(1)jnknknknjkupstreamjhhwdd         why gradients tend to vanish for dnn

    to illustrate to problem, let   s use matrix form of error bp:

]+1

[

l+2

l+2

   ,    ,    
   ,    ,    

it suffers from all problems associated with linear processes

    so even if forward pass is nonlinear, error backprop is a linear process
   
    many terms of    (1-
   
   
   
   

in addition, many terms in the product of w   s
if any sigmoid unit saturates in either direction, the error gradient becomes zero
if ||w||<1, the product will shrink fast for high depths
if ||w||>1, the product may grow fast for high depths

) for sigmoid units

how to rescue it

    pre-train the dnn by generative dbn (solution by 2010)

    complicated process

    discriminative pre-training (much easier to do)
    still random initialization but with carefully set variance values; e.g.

    layer-dependent variance values
    for lower layers (with more terms in the product),

make the variance closer to 0 (e.g. < 0.1)

    use bigger training data to reduce the chance 

of vanishing gradients for each epoch

    use relu units: only one side of zero gradient

instead of two as for sigmoid units 

the power of understanding root causes!!! 
(mid 2010 at msr redmond)

42

an alternative way of training nn 

    backprop takes partial derivatives:

   

if the output layer is linear, total (instead of partial) 
derivative can be computed

    this is basic learning method for deep convex/stacking net 
(dsn), designed to be easily parallelizable by batch training

    using the total derivative is equivalent to coordinate 

descent algorithm with an    in   nite    step size to achieve the 
global optimum along the    coordinate    of updating u while 
   xing w.

43

deep stacking nets

   

   

   

   

   

learn weight matrices u 
and w in individual 
modules separately.
given w and linear output 
layer, u can be expressed 
as explicit nonlinear 
function of w.
this nonlinear function is 
used as the constraint in 
solving nonlinear least 
square for learning w.
initializing w with rbm 
(bottom layer)
for higher layers, part of w 
is initialized with the 
optimized w from the 
immediately lower layer 
and part of it with random 
numbers

(deng, yu, platt, icassp-2012; hutchinson, deng, yu, ieee t-pami, 2013)

44

...............w2u2...............w1u1wrand..................w3wrandu3.....................wrandw4u4a neat way of learning dsn weights

e =

1
2

   
    

||                     ||2,

where          =                  =                               =         (    ,     )

        
        
e =

= 2                                         =                 1
1
2

       ||        (    ,     )             ||2, subject to u= f(    ), 

             = f(    ),   where          =                      

use of lagrange multiplier method:

e =

1
2

       ||        (    ,     )             ||2 +      ||u    f      ||

to learn w (& then u)     full derivation       in closed form
(i.e. no longer recursion on partial derivation as in id26

y

h

x

u

w

    advantages found: 

--- less noise in gradient than using chain rule which ignores explicit constraint u= f(    )
--- batch learning is effective, aiding parallel training 

45

how the brain may do backprop

    canadian psychology, vol 44, pp 10-13, 2003.

feedback system in biological neural nets

   
    key roles of stdp (spike-time-dependent plasticity) --- temporal derivative
   

to provide a way to encode error derivatives

46

how the brain may do backprop

   

   

   

   

   

backprop algorithm requires that feedforward and feedback weights are the same
this is clearly not true for biological neural nets
how to reconcile this discrepancy?
recent studies showed that use of random feedback weights in bp performs close to rigorous bp
implications for regularizing bp learning (like dropout, which may not make sense at 1st glance) 

47

recurrent neural net (id56) basics

--- why memory decays fast or explode (1990   s)
--- how to rescue it (2010   s, in the new deep learning era)

48

basic architecture of an id56

        is the hidden layer that carries the information from time 0~    
where         :                                                   ,         :                                                  
         =                                               ,                             =     (                   1 +                  )

    

    

    1

    1

    1

    

    

    2

    2

    

    2

    

    3

    3

    

    

    3

    
   

        

        

        

    

    

used for slot filling in slu

[mesnil, he, deng, bengio, 2013; yao, zweig, hwang, shi, yu, 2013]

microsoft research

49

back-propagation through time (bptt)

                    3

at time      = 3

1. forward propagation

2. generate output

3. calculate error

4. back propagation

5. back prop. through time

    1

    

    1

    

    1

    

    

    

    2

    2

    2

    

    3

    

    3

    

    3

microsoft research

50

http://karpathy.github.io/2015/05/21/id56-effectiveness/

microsoft research

51

some early attempts to examine 

difficulties in learning id56s

    yoshua bengio   s ph.d. thesis at mcgill university (1991)
    based on attractor properties of nonlinear dynamic systems
    his recent, more intuitive explanation --- in extreme of 
nonlinearity, discrete functions and gradients vanish or 
explode:

    an alternative analysis: based on perturbation analysis of 

nonlinear differential equations (next several slides)

52

    nn for nonlinear sequence prediction (like nn language model used today)
    memory (temporal correlation) proved to be stronger than linear prediction
    no gpus to use; very slow to train with bp; did not make nn big and deep, etc.

    conceptually easy to make it deep using (state-space) signal processing & id114    

by moving away from nn   

53

54

55

56

57

why gradients vanish/explode for id56

    the easiest account is to follow the analysis for 

dnn

    except that    depth    of id56 is much larger: the 

length of input sequence

    especially serious for speech sequence (not as 

bad for text input)

    tony robinson group was the only one that made 

id56 work for timit phone recognition (1994)

58

how to rescue it

   

echo state nets (   lazy    approach)
    avoiding problems by not training input & recurrent weights
    h. jaeger.    short term memory in echo state networks   , 2001

    but if you figure out smart ways to train them, you get much better results

(nips-ws, 2013)

59

how to rescue it

    by better optimization
    hessian-free method 
    primal-dual method

60

how to rescue it

    use of lstm (long short-term memory) cells

    sepp hochreiter & j  rgen schmidhuber (1997). "long short-

term memory" neural computation 9 (8): 1735   1780.

    many earlier-to-read materials, especially after 2013

    the best way so far to train id56s well

    increasingly popular in speech/language processing
    attracted big attention from asr community at icassp-2013   s 

dnn special session (graves et al.)

    huge progress since then

61

many ways to show an lstm cell

(slide revised from: koutnik & schmidhuber, 2015)

62

many ways to show an lstm cell

63

many ways to show an lstm cell

64

huh?....

don   t worry, we   ll come back to this shortly

lstm cells in an id56

66

lstm cell unfolding over time

(jozefowics, zarembe, sutskever, 
icml 2015)

68

gated recurrent unit (gru)

(simpler than lstm; no output gates)

(jozefowics, zarembe, sutskever, icml 2015; google
kumar et al., arxiv, july, 2015; metamind)

69

70

(2006, 2012)

71

part ii: speech

72

deep/dynamic structure in human 
speech production and perception

(part of my tutorial at 2009 nips ws)

73

production & perception: closed-loop chain

speaker

listener

message

decoded
message

internal
model

speech acoustics in 

closed-loop chain

encoder: two-stage production mechanisms

speaker

message

phonology (higher level):
   symbolic encoding of linguistic message
   discrete representation by phonological features
   loosely-coupled multiple feature tiers
   overcome beads-on-a-string phone model
   theories of distinctive features, feature geometry
& articulatory phonology
    account for partial/full sound deletion/modification
in casual speech 

id102 (lower level):
   convert discrete linguistic features to
continuous acoustics
   mediated by motor control & articulatory     
dynamics
   mapping from articulatory variables to
vt area function to acoustics 
   account for co-articulation and reduction   
(target undershoot), etc.

speech acoustics

encoder: phonological modeling

speaker

computational phonology:
    represent pronunciation variations as
constrained factorial markov chain 
    constraint: from articulatory phonology
    language-universal representation

message

ten themes

/  t           

n        

i:   m      z /

tongue
tip

tongue
body

mid / front

high / front

speech acoustics

encoder: phonetic modeling

speaker

message

computational id102:
    segmental factorial id48 for sequential target 
in articulatory or vocal tract resonance domain
    switching trajectory model for target-directed
articulatory dynamics
    switching nonlinear state-space model for
dynamics in speech acoustics
    illustration:

speech acoustics

decoder i: auditory reception

listener

decoded
message

internal
model

    convert speech acoustic waves into
efficient & robust auditory representation
    this processing is largely independent 
of phonological units
    involves processing stages in cochlea
(ear), cochlear nucleus, soc, ic,   , all
the way to a1 cortex
    principal roles: 
1) combat environmental acoustic 

message

distortion;

2) detect relevant speech features 
3) provide temporal landmarks to aid

decoding

    key properties: 
1) critical-band freq scale, logarithmic compression,
2) adapt freq selectivity, cross-channel correlation,
3) sharp response to transient sounds
4) modulation in independent frequency bands,
5) binaural noise suppression, etc. 

decoder ii: cognitive perception

listener

decoded
message

internal
model

    cognitive process: recovery of linguistic
message
    relies on 
1)    internal    model: structural knowledge of 

the encoder (production system)

message

2) robust auditory representation of features
3) temporal landmarks
    child speech acquisition process is one that 
gradually establishes the    internal    model
    strategy: analysis by synthesis
    i.e., probabilistic id136 on (deeply) 
hidden linguistic units using the internal
model
    no motor theory: the above strategy 
requires no articulatory recovery from 
speech acoustics

human speech perception (decoder)

listener

decoded
message

internal
model

    convert speech acoustic waves into

efficient & robust auditory representation

    this processing is largely independent 

of phonological units

    involves processing stages in cochlea
(ear), cochlear nucleus, soc, ic,   , all
the way to a1 cortex
    two principal roles: 

message

1) combat environmental acoustic 

distortion; 

2) provide temporal landmarks to aid

decoding

    key properties: 

1) critical-band freq scale, logarithmic compression,
2) adapt freq selectivity, cross-channel correlation,
3) sharp response to transient sounds (cn),
4) modulation in independent frequency bands,
5) binaural noise suppression, etc. 

types of speech perception 

theories 

    active vs. passive

    bottom up vs./and top down

    autonomous vs. interactive

active vs. passive

    active theories suggests that speech 

perception and production are closely related
    listener knowledge of how sounds are produced 

facilitates recognition of sounds

    passive theories emphasizes the sensory 

aspects of speech perception
    listeners utilize internal filtering mechanisms

    knowledge of vocal tract characteristics plays a minor 

role, for example when listening in noise conditions

bottom up vs. & top down

    top-down processing works with 
knowledge a listener has about a 
language, context, experience, 
etc.
    listeners use stored information 
about language and the world to 
make sense of the speech 

    bottom-up processing works in 

the absence of a knowledge base 
providing top-down information
    listeners receive auditory information, 

convert it into a neural signal and 
process the phonetic feature 
information 

specific speech perception 

theories

    motor theory
    acoustic invariance theory
    direct realism
    trace model (based on neural nets)
    cohort theory
    fuzzy logic model of perception
    native language magnet theory

motor theory

    postulates speech is perceived by 

reference to how it is produced
    when perceiving speech, listeners access 

their own knowledge of how phonemes 
are articulated

    articulatory gestures (such as rounding or 

pressing the lips together) are units of 
perception that directly provide the 
listener with phonetic information

liberman, cooper, shankweiler, & studdert-
kennedy, 1967

acoustic invariance theory

    listeners inspect the incoming signal for the so-
called acoustic landmarks which are particular 
events in the spectrum carrying information 
about gestures which produced them. 

    gestures are limited by the capacities of 

humans    articulators and listeners are sensitive 
to their auditory correlates, the lack of 
invariance simply does not exist in this model. 

    the acoustic properties of the landmarks 

constitute the basis for establishing the 
distinctive features. 

    bundles of the distinctive features uniquely 

specify phonetic segments (phonemes, 
syllables, words).

stevens, k.n. (2002). "toward a model of lexical access based on 
acoustic landmarks and distinctive features" (pdf). journal of the 
acoustical society of america 111 (4): 1872   1891.

trace model

    for example, a listener hears the 

beginning of bald, and the words bald, 
ball, bad, bill become active in memory. 
then, soon after, only bald and ball 
remain in competition (bad, bill have 
been eliminated because the vowel 
sound doesn't match the input). 

    soon after, bald is recognized. 

    trace simulates this process by 

representing the temporal dimension of 
speech, allowing words in the lexicon to 
vary in activation strength, and by having 
words compete during processing. 

a deep/generative model of speech production/perception

--- perception as    variational id136   

speaker

targets

articulation

message

distortion-free acoustics

distorted acoustics

distortion factors & 

speech acoustics

feedback to articulation

deep generative models, 

variational interference/learning, & 

applications to speech

90

deep learning 

   

neural networks, deep 
in space & time (recurrent lstm), & deep id56

+

generative models, deep 

in space & time (dynamic), & deep/hidden dynamic models

+ 

   ,    ,    

91

deep neural nets

deep generative models

structure

graphical; info flow: bottom-up

graphical; info flow: top-down

incorp constraints & 
domain knowledge

hard

easy

semi/unsupervised

harder or impossible

easier, at least possible

interpretation

harder

easy (generative    story    on data and hidden variables)

representation

distributed

localist (mostly); can be distributed also

id136/decode

easy

harder (but note recent progress)

scalability/compute

easier (regular computes/gpu)

harder  (but note recent progress)

incorp. uncertainty

hard

easy

empirical goal

classification, id171,    

classification (via bayes rule), latent 
variable id136   

terminology

neurons, activation/gate functions, 
weights     

random vars, stochastic    neurons   , 
potential function, parameters    

learning algorithm

a single, unchallenged, algorithm --
backprop

a major focus of open research, many 
algorithms, & more to come

evaluation

on a black-box score     end performance

on almost every intermediate quantity

implementation

hard (but increasingly easier)

standardized but insights needed

experiments

massive, real data

modest, often simulated data

parameterization

dense matrices

sparse (often pdfs); can be dense

example: (shallow) generative model

   topics   
as hidden layer

war

animals

   

computers

iraqi 

the

matlab

slide revised from: max welling

another example: medical diagnosis

diseases

symptoms

id136 problem: 
what is the most 
probable disease 

given the 
symptoms?

94

example: (deep) generative model

targets

articulation

distortion-free acoustics

distorted acoustics

distortion factors & 

speech acoustics

feedback to articulation

deep generative/graphical model id136

    key issues:  

    representation:  syntax and semantics (directed/undirected,variables/factors,..)
    id136:   computing probabilities and most likely assignments/explanations
    learning:   of model parameters based on observed data.  relies on id136!
id136 is np-hard (incl. approximation hardness)

   
    exact id136:   works for very limited subset of models/structures

    e.g., chains or low-treewidth trees

    approximate id136:   highly computationally intensive

    deterministic:   variational, loopy belief propagation, expectation propagation
    numerical sampling (monte carlo):    id150

    variational learning: 

    em algorithm
    e-step uses variational id136 (recent new advances in ml)

variational id136/learning is not 

trivial: example

diseases

difficulty: 

explaining away: 

observation introduces 
correlation of nodes in 
the parent hidden layer

symptoms

97

variational em 

step 1: maximize the bound with respect to q

(q approximates true posterior, often by factorizing it) 

step 2: fix q, maximize with respect to  

note in traditional em, q is precise; e.g. posteriors computed by forward/backward 
algorithm for id48s

98

),(maxarg  :step) (e)()1(kqkqlq         ),(maxarg  :step) (m)1()1(                  kkqldeterministic
hdm

statistical hdm
(hidden dynamic
model)

bridle, deng, picone, richards, ma, kamm, schuster, pike, reagan. final report for workshop on language engineering, 
johns hopkins university, 1998.  (experiments on switchboard tasks)

99

icassp-2004

auxiliary function:

102

surprisingly good id136 results for 

continuous hidden states

    by-product: accurately 

tracking dynamics of 
resonances (formants) in 
vocal tract (timit & swbd).

    best formant tracker by 
then in speech analysis; 
used as basis to form a 
formant database as 
   ground truth   

    we thought we solved the 

asr problem, except
   intractable    for decoding

   

deng & huang, challenges in adopting id103, communications of the acm, vol. 47, pp. 69-75, 2004.
deng, cui, pruvenok, huang, momen, chen, alwan, a database of vocal tract resonance trajectories for research in speech , icassp, 2006.

103

deep generative models in id103

(prior to the rising of deep learning) 

segment & nonstationary-state models
   

digalakis, rohlicek, ostendorf.    ml estimation of a stochastic linear system with the em alg & 
application to id103,    ieee t-sap, 1993
deng, aksmanovic, sun, wu, id103 using id48 with polynomial regression functions 
as nonstationary states,    ieee t-sap, 1994.

hidden dynamic models (hdm)
   

deng, ramsay, sun.    production models as a structural basis for automatic id103,    
speech communication, vol. 33, pp. 93   111, 1997.
bridle et al.    an investigation of segmental hidden dynamic models of speech coarticulation for 
id103,    final report workshop on language engineering, johns hopkins u, 1998.
picone et al.    initial evaluation of hidden dynamic models on conversational speech,    icassp, 1999.
deng and ma.    spontaneous id103 using a statistical co-articulatory model for the 
vocal tract resonance dynamics,    jasa, 2000.

structured hidden trajectory models (htm)
   

zhou, et al.    coarticulation modeling by embedding a target-directed hidden trajectory model into 
id48,    icassp, 2003.     darpa ears program 2001-2004: novel approach ii 
deng, yu, acero.    structured speech modeling,    ieee trans. on audio, speech and language 
processing, vol. 14, no. 5, 2006.

switching nonlinear state-space models 
   

deng.    switching dynamic system models for speech articulation and acoustics,    in mathematical foundations of 
speech and language processing, vol. 138, pp. 115 - 134, springer, 2003.
lee et al.    a multimodal variational approach to learning and id136 in switching state space 
models,    icassp, 2004.

   

   

   

   

   

   

1993

1994

1997
1998

1999
2000

2003
2006

104

other deep generative models 

(developed outside speech)

    sigmoid belief nets & wake/sleep alg. (1992) 
    deep belief nets (dbn, 2006);
    start of deep learning 
    totally non-obvious result:

stacking many rbms (undirected)

    not deep id82 (dbm, undirected)
    but a dbn (directed, generative model)
    excellent in generating images & id133

    similar type of deep generative models to hdm
    but simpler: no temporal dynamics
    with very different parameterization
    most intriguing of dbn: id136 is easy 

(i.e. no need for approximate id58)
       restriction    of connections in rbm  

    pros/cons analysis    hinton coming to msr 2009

105

this is a very different kind of deep generative model

(mohamed, dahl, hinton, 2009, 2012)

(deng et al., 2006; deng & yu, 2007)

(after adding backprop to the generative dbn)

error analysis

d

    elegant model formulation & knowledge 

incorporation

    strong empirical results: 96% timit accuracy 
with nbest=1001; 75.2% lattice decoding w. 
monophones; fast approx. training 

    still very expensive for decoding; could not 

ship (very frustrating!)

-- dbn/dnn made many new errors 

on short, undershoot vowels

-- 11 frames contain too much    noise   

107

early successes of deep learning

in id103

108

academic-industrial collaboration (2009,2010)

    i invited geoff hinton to work with me at msr, redmond
    well-timed academic-industrial collaboration:

    asr industry searching for new solutions when    principled    

deep generative approaches could not deliver

    academia developed deep learning tools (e.g. dbn 2006) 

looking for applications

    add backprop to deep generative models (dbn)     dnn 

(hybrid generative/discriminative)

    advent of gpu computing (nvidia cuda library released 

2007/08)

    big training data in id103 were already available  

109

invitee 1: give me one week 
to decide    ,    
not worth my time to fly to 
vancouver for this   

mohamed, dahl, hinton, id50 for phone recognition, nips 2009 workshop on deep learning, 2009
yu, deng, wang, learning in the deep-structured id49, nips 2009 workshop on deep learning, 2009
   ,    ,    

110

expanding dnn at industry scale

   

   

   
   
   

   
   

   

   

   

scale dnn   s success to large speech tasks (2010-2011)
    grew output neurons from context-independent phone states (100-200) to context-dependent 

ones (1k-30k)     cd-dnn-id48 for bing voice search and then to swbd tasks

    motivated initially by saving huge msft investment in the speech decoder software 

infrastructure 

    cd-dnn-id48 also gave much higher accuracy than ci-dnn-id48
    earlier nns made use of context only as appended inputs, not coded directly as outputs
    discovered that with large training data backprop works well without dbn pre-training by 
understanding why gradients often vanish (patent filed for    discriminative pre-training    2011)

engineering for large speech systems:
    combined expertise in dnn (esp. with gpu implementation) and id103
    collaborations among msrr, msra, academic researchers

yu, deng, dahl, roles of pre-training and fine-tuning in context-dependent dbn-id48s for real-world id103, in nips workshop on deep learning, 2010. 
dahl, yu, deng, acero, large vocabulary continuous id103 with context-dependent dbn-id48s, in proc. icassp, 2011.
dahl, yu, deng, acero, context-dependent pre-trained deep neural networks for large vocabulary id103, in ieee transactions on audio, speech, and language 
processing (2013 ieee sps best paper award) , vol. 20, no. 1, pp. 30-42, january 2012.
seide, li, yu, "conversational speech transcription using context-dependent deep neural networks", interspeech 2011, pp. 437-440.
hinton, deng, yu, dahl, mohamed, jaitly, senior, vanhoucke, nguyen, sainath, kingsbury, deep neural networks for acoustic modeling in speechrecognition, in ieee signal 
processing magazine, vol. 29, no. 6, pp. 82-97, november 2012
sainath, t., kingsbury, b., ramabhadran, b., novak, p., and mohamed, a.    making id50 effective for large vocabulary continuous id103,    proc. 
asru, 2011.
sainath, t., kingsbury, b., soltau, h., and ramabhadran, b.    optimization techniques to improve training speed of deep neural networks for large speech tasks,    ieee 
transactions on audio, speech, and language processing, vol.21, no.11, pp.2267-2276, nov. 2013.
jaitly, n., nguyen, p., senior, a., and vanhoucke, v.    application of pretrained deep neural networks to large vocabulary id103,    proc. interspeech, 2012.

112

(replacing gmm only; longer mfcc/filter-back windows w. no transformation)

dnn-id48

model tied triphone states directly

many layers of 

nonlinear 

feature 

transformation 

+ softmax

113

dnn vs. pre-dnn prior-art

   

table: timit phone recognition (3 hours of training)

features

pre-dnn

dnn

setup

error rates

deep generative model

5 layers x 2048

24.8%

23.4%  

   

table: voice search ser (24-48 hours of training)
features

setup

error rates

pre-dnn

gmm-id48 with mpe

dnn

5 layers x 2048

    table: switchboard wer (309 hours training)

36.2%

30.1%  

~10% relative
improvement

~20% relative
improvement

features

pre-dnn

dnn

setup

error rates

gmm-id48 with bmmi

7 layers x 2048

23.6%

15.8% 

~30% relative
improvement

for dnn, the more data, the better!

114

scientists see promise in deep-learning programs
john markoff
november 23, 2012
rick rashid in tianjin, china, october, 25, 2012

deep learning 
technology enabled 
speech-to-speech 
translation

a voice recognition program translated a speech given by 

richard f. rashid, microsoft   s top scientist, into mandarin chinese. 

cd-dnn-id48 

dahl, yu, deng, and acero,    context-dependent pre-
trained deep neural networks for large vocabulary 
id103,    ieee trans. aslp, jan. 2012 (also 
icassp 2011)
seide et al, interspeech, 2011.

after no improvement for 10+ years 
by the research community   
   msr reduced error from ~23% to
<13% (and under 7% for rick 
rashid   s s2s demo in 2012)!

impact of deep learning in speech technology

cortana

118

microsoft research

119

(slide from bengio, 2015)

120

121

in academic world

   this joint paper from the major 
id103 laboratories 

was the first major industrial 
application of deep learning.   

122

dnn: (fully-connected) deep neural  networks
   dnn for acoustic modeling in id103,    in ieee spm, nov. 2012

first train a stack of n models each of 
which has one hidden layer. each model in 
the stack treats the hidden variables of the 
previous model as data.

then compose them into 
a single deep belief 
network (dbn).

then add outputs 
and train the dnn 
with backprop.

asr issues

how to reduce the runtime without 
accuracy loss? 
how to do speaker adaptation with 
low footprints?
how to be robust to noise?

how to reduce accuracy gap 
between large and small dnn?
how to deal with large variety of 
data?
how to enable languages with 
limited training data?

solutions

svd

svd-based adaptation

variable component id98

teacher-student learning using 
output posterior
dnn factorization, mixed band 
training
multi-lingual dnn

microsoft research

124

more recent development of 

deep learning for speech

125

chapter 7

126

innovation: better optimization

    sequence discriminative training for dnn:
- mohamed, yu, deng:    investigation of full-sequence training of deep
belief networks for id103,    interspeech, 2010.
- kingsbury, sainath, soltau.    scalable minimum bayes risk training of    
dnn acoustic models using distributed hessian-free optimization,    
interspeech, 2012.
- su, li, yu, seide.    error back propagation for sequence training of cd 
deep networks for conversational speech transcription,    icassp, 2013.
- vesely, ghoshal, burget, povey.    sequence-discriminative training of 
deep neural networks, interspeech, 2013.
    distributed asynchronous sgd 
- dean, corrado,   senior, ng.    large scale distributed deep networks,    
nips, 2012.
- sak, vinyals, heigold, senior, mcdermott, monga, mao.    sequence 
discriminative distributed training of long short-term memory 
recurrent neural networks,    interspeech,2014.

127

input data x

innovation: towards raw inputs

    bye-bye mfccs (no more cosine transform, mel-scaling?) 
- deng, seltzer, yu, acero, mohamed, hinton.    binary coding of speech spectrograms using a deep 
auto-encoder,    interspeech, 2010.
- mohamed, hinton, penn.    understanding how id50 perform acoustic modeling,    
icassp, 2012.
- li, yu, huang, gong,    improving wideband id103 using mixed-bandwidth training data 
in cd-dnn-id48    slt, 2012
- deng, j. li, huang, yao, yu, seide, seltzer, zweig, he, williams, gong, acero.    recent advances in 
deep learning for speech research at microsoft,    icassp, 2013.
- sainath, kingsbury, mohamed, ramabhadran.    learning filter banks within a deep neural network 
framework,    asru, 2013.
    bye-bye fourier transforms?
- jaitly and hinton.    learning a better representation of speech sound waves using rbms,    icassp, 
2011.
- tuske, golik, schluter, ney.    acoustic modeling with deep neural networks using raw time signal for 
lvcsr,    interspeech, 2014.
- golik et al,    convolutional nns for acoustic modeling of raw time signals in lvcsr,    interspeech, 
2015.
- sainath et al.    learning the speech front-end with raw waveform cldnns,    interspeech, 2015
    dnn as hierarchical nonlinear feature extractors: 
- seide, li, chen, yu.    feature engineering in context-dependent deep neural networks for 
conversational speech transcription, asru, 2011.
- yu, seltzer, li, huang, seide.    id171 in deep neural networks - studies on speech 
recognition tasks,    iclr, 2013.
- yan, huo, xu.    a scalable approach to using dnn-derived in gmm-id48 based acoustic modeling in 
lvcsr,    interspeech, 2013.
- deng, chen.    sequence classification using high-level features extracted from deep neural 
networks,    icassp, 2014.

128

input data x

innovation: transfer/multitask learning

& adaptation

adaptation to speakers & 
environments (i-vectors)

    too many references to list & organize

129

innovation: better id173 & nonlinearity

x

x

x

x

x

x

input data x

130

innovation: better architectures

    recurrent nets (bi-directional id56/lstm) 
and conv nets (id98) are superior to fully-
connected dnns

    sak, senior, beaufays.    lstm recurrent neural 
network architectures for large scale acoustic 
modeling,    interspeech,2014. 

    soltau, saon, sainath.    joint training of 

convolutional and non-convolutional neural 
networks,    icassp, 2014. 

131

innovation: ensemble deep learning

    ensembles of id56/lstm, dnn, & conv nets (id98) give 

   

   

huge gains:
t. sainath,  o. vinyals, a. senior, h. sak.    convolutional, long short-term memory, fully 
connected deep neural networks,    icassp 2015.
l. deng and john platt, ensemble deep learning for id103, interspeech, 
2014.

    g. saon, h. kuo, s. rennie, m. picheny.    the ibm 2015 english conversational telephone 

id103 system,    arxiv, may 2015. (8% wer on swb-309h)

132

innovation: better learning objectives/methods

    use of ctc as a new objective in id56/lstm with 

end2end learning drastically simplifies asr 
systems

    predict graphemes or words directly; no pron. 

dictionaries; no cd; no id90 

    use of    blank    symbols may be equivalent to a 

special id48 state tying scheme

    ctc/id56 has not replaced id48 (left-to-right)
    relative 8% gain by ctc has been shown by a very

limited number of labs

   

   

   

   

   

a. graves and n. jaitly.    towards end-to-end id103 
with recurrent neural networks,    icml, 2014.
a. hannun, a. ng et al.    deepspeech: scaling up end-to-end 
id103,    arxiv nov. 2014.
a. maas et al.    lexicon-free conversational asr with nn,    naacl, 
2015
h. sak et al.    learning acoustic frame labeling for asr with id56,    
icassp, 2015
h. sak, a. senior, k. rao, f. beaufays.    fast and accurate recurrent 
neural network acoustic models for id103,    
interspeech, 2015

133

innovation: a new paradigm for id103

    seq2seid24 with attention 

mechanism (borrowed from nlp-mt)

    w. chan, n. jaitly, q. le, o. vinyals.    listen, attend, and spell,   

   

arxiv, 2015.
j. chorowski, d. bahdanau, d. serdyuk, k. cho, y. bengio. 
   attention-based models for id103,    arxiv, 2015.

134

a perspective on recent innovations of asr

    all above deep learning innovations are based on 

supervised, discriminative learning of dnn and recurrent 
variants

    capitalizing on big, labeled data
    incorporating monotonic-sequential structure of speech 

(non-monotonic for language, later)

    hard to incorporate many other aspects of speech 

knowledge with (e.g. speech distortion model)

    hard to do semi- and unsupervised learning
   deep generative modeling may overcome such difficulties

li deng and roberto togneri, chapter 6: deep dynamic models for learning hidden representations of speech features, pp. 153-196, 
springer, december 2014.

li deng and navdeep jaitly, chapter 2: deep discriminative and generative models for pattern recognition, ~30 pages, in handbook of 
pattern recognition and id161: 5th edition, world scientific publishing, jan 2016.

135

structure

graphical; info flow: bottom-up graphical; info flow: top-down

deep neural nets

deep generative models

incorp constraints & 
domain knowledge

hard

easy

unsupervised

interpretation

harder or impossible

easier, at least possible

harder

easier (generative    story    on data and hidden variables)

representation

distributed

localist (mostly); can be distributed also

id136/decode

easy

harder (but note recent progress)

scalability/compute

easier (regular computes/gpu)

harder  (but note recent progress)

incorp. uncertainty

hard

easy

empirical goal

classification, id171,    

classification (via bayes rule), latent 
variable id136   

terminology

neurons, activation/gate functions, 
weights     

random vars, stochastic    neurons   , 
potential function, parameters    

learning algorithm

a single, unchallenged, algorithm --
backprop

a major focus of open research, many 
algorithms, & more to come

evaluation

on a black-box score     end performance

on almost every intermediate quantity

implementation

hard (but increasingly easier)

standardized but insights needed

experiments

massive, real data

modest, often simulated data

parameterization

dense matrices

sparse (often pdfs); can be dense

example 1: interpretable deep learning    
using deep topic models (nips-2015)

137

recall: (shallow) generative model

   topics   
as hidden layer

war

animals

   

computers

iraqi 

the

matlab

slide revised from: max welling

interpretable deep learning: deep generative model

    constructing interpretable dnns based on generative topic models 
    end-to-end learning by mirror-descent id26
    to maximize posterior id203 p(y|x)

    y: output (win/loss), and x: input feature vector

mirror descent 
algorithm (mda) 

j. chen, j. he, y. shen, l. xiao, x. he, j. gao, x. song, and l. deng,    end-to-end learning of id44 by mirror-descent back propagation   , submitted to nips2015.

example 2: unsupervised learning using 

deep generative model (acl, 2013)

    distorted character string images    text
    easier than unsupervised  speech    text
    47% error reduction over google   s open-source ocr system

motivated me to 
think about 
unsupervised asr
and nlp

140

power: character-level lm & generative modeling for 

unsupervised learning 

   image    data are naturally    generated    by the model quite accurately (like    computer graphics   )
i had the same idea for unsupervised generative speech-to-text in 90   s

   
   
    not successful because 1) deep generative models were too simple for generating speech waves

2) id136/learning methods for deep generative models not mature then
3) computers were too slow   

141

deep generative model for image-text
(berg-kirkpatrick et al., 2013, 2015)

deep generative model for speech-text

(deng, 1998; deng et al, 1997, 2000, 2003, 2006)

l.deng, a dynamic, feature-based approach to the interface between phonology
& id102 for speech modeling and recognition, speech communication, 
vol. 24, no. 4, pp. 299-323, 1998.

142

deep generative model for image-text
(berg-kirkpatrick et al., 2013, 2015)

deep generative model for speech-text

(deng, 1998; deng et al, 2000, 2003, 2006)

word-level

language model

plus

feature-level

pronunciation model

143

deep generative model for image-text
(berg-kirkpatrick et al., 2013, 2015)

deep generative model for speech-text

(deng, 1998; deng et al, 2000, 2003, 2006)

articulatory
dynamics

easy: likely no    explaining away    problem in 

id136 and learning

hard: pervasive    explaining away    problem
144

due to speech dynamics

deep generative model for image-text
(berg-kirkpatrick et al., 2013, 2015)

deep generative model for speech-text

(deng, 1998; deng et al, 2000, 2003, 2006)

articulatory

to

acoustics
mapping

145

   

in contrast, articulatory-to-acoustics 
mapping in asr is much more complex

    during 1997-2000, shallow nns were used

for this as    universal approximator   

    not successful
    now we have better dnn tool 
    even id56/lstm/ctc tool for dynamic 

modeling

    essence: exploit the strong prior of lm: 

trained with billions of words/text 

    no need to pair the text with acoustics; 

hence unsupervised learning

    think of it as using a new objective function 
based on distribution matching between lm 
prior and the distribution of words 
predicted from acoustic models

146

very simple, & easy to model accurately

further thoughts on unsupervised asr

    deep generative modeling experiments not successful in 90   s

    computers were too slow   
    models were too simple from text to speech waves 

    still true     need speech scientists to work harder with technologists
    and when generative models are not good enough, discriminative models 

and learning (e.g., id56) can help a lot 

    further, can iterate between the two, like wake-sleep (algorithm)

    id136/learning methods for deep generative models not mature 

at that time 
    only partially true today 
        due to recent big advances in machine learning 
    based on new ways of thinking about generative graphical modeling 

motivated by the availability of deep learning tools (e.g. dnn)

    a brief review next

147

advances in id136 algms for deep generative models

kingma & welling 2014, salakhutdinov et al, 2015

icml-2014 talk monday june 23, 15:20

in track f (deep learning ii)

   efficient gradient based id136 
through transformations between

bayes nets and neural nets    

other solutions to solve the "large variance problem    in variational id136:

-id58ian id136 with stochastic search [d.m. blei, m.i. jordan and j.w. paisley, 2012]
-fixed-form variational posterior approximation through stochastic id75 [t. salimans and a. knowles, 2013].
-black box variational id136. [r. ranganath, s. gerrish and d.m. blei. 2013]
-stochastic variational id136 [m.d. hoffman, d. blei, c. wang and j. paisley, 2013]
-estimating or propagating gradients through stochastic neurons. [y. bengio, 2013].
-neural variational id136 and learning in belief networks. [a. mnih and k. gregor, 2014, icml]
-stochastic backprop & approximation id136 in deep generative models [d. rezende, s. mohamed, d. wierstra, 2014]
-semi-supervised learning with deep generative models [k. kingma, d. rezende, s. mohamed, m. welling, 2014, nips] 
-auto-encoding id58 [k. kingma, m. welling, 2014, icml]
-learning stochastic recurrent networks [bayer and osendorfer, 2015 iclr] 
-draw: a recurrent neural network for image generation. [k. gregor, danihelka, rezende, wierstra, 2015]
- plus a number of nips-2015 papers, to appear.

slide provided by max welling (icml-2014 tutorial) w. my updates on references of 2015 and late 2014 

148

further thoughts on unsupervised asr

    deep generative modeling experiments not successful in 90   s

    computers were too slow   
    models were too simple from text to speech waves 

    still true     need speech scientists to work harder with technologists
    and when generative models are not good enough, discriminative models 

and learning (e.g., id56) can help a lot; but to do it? a hint next 
    further, can iterate between the two, like wake-sleep (algorithm)

    id136/learning methods for deep generative models not mature 

at that time 
    only partially true today 
        due to recent big advances in machine learning 
    based on new ways of thinking about generative graphical modeling 

motivated by the availability of deep learning tools (e.g. dnn)

    a brief review next

149

id56           vs.  generative hdm

parameterization:
              ,            ,            : all unstructured

regular matrices

parameterization:
               =m(      ); sparse system matrix
           =(      ); gaussian-mix params; mlp 
       =         

150

generative hdm

151

id56           vs.  generative hdm

~dnn

~dbn

e.g. generative pre-training

(analogous to generative dbn pretraining for dnn)

nips-2015 paper to appear on simpler dynamic models for a non-asr application

better ways of integrating deep generative/discriminative models are possible
- hint: example 1 where generative models are used to define the dnn architecture 

end of 

part ii: speech

-deep supervised learning shattered asr via dnn/lstm
-deep unsupervised learning may impact more in the future
-no more low-hanging fruit

deep learning also shattered the entire field of 
image recognition and id161 (since 2012)

153

krizhevsky, sutskever, hinton,    id163
classification with deep convolutional neural 
networks.    nips, dec. 2012

googlenet
6.67%

4.94%

deep id98 
univ. toronto team

microsoft research

154

part iii: language

moving from perception: phonetic/word recognition, image/gesture  recognition, etc

cognition: memory, attention, reasoning, q/a, & decision making, etc.

to

embedding enables exploring models for these human cognitive functions

155

embedding linguistic entities

in low-dimensional continuous space

and

examples of nlp applications

156

                  =

a.k.a the 1-hot
word vector

id27 
vector in the 
semantic space

the index of    cat    in 
the vocabulary

deerwester, dumais, furnas, landauer, 
harshman, "indexing by latent 
semantic analysis," jasis 1990

microsoft research

157

nn id27

bengio, ducharme, vincent, jauvin,    a 
neural probabilistic language model.     
jmlr, 2003

scoring: 

training:

where 

                         1,     2,     3,     4,     5 =             (         1,     2,     3,     4,     5 +     )

     = max 0, 1 +                 +

update the model until     + > 1 +        

    + =                          1,     2,     3,     4,     5
        =                          1,     2,        ,     4,     5

and
<     1,     2,     3,     4,     5 > is a valid 5-gram
<     1,     2,        ,     4,     5 > is a    negative sample    constructed
by replacing the word     3 with a random word        

e.g., a negative example:    cat chills x a mat   

collobert, weston, bottou, karlen, 
kavukcuoglu, kuksa,    natural language 
processing (almost) from scratch,    jmlr 
2011

id27

microsoft research

159

u

w

id27

cat

microsoft research

160

chases

   

is

mikolov, yih,  zweig,    linguistic 
regularities in continuous space 
word representations,    naacl 
2013

continuous bag-of-words

the cbow architecture (a) on the left, and the skip-gram architecture (b) on the right. 
[mikolov et al., 2013 iclr].

microsoft research

161

dim

# words

id27 

matrix

          =

w1,w2,                                                                                                 wn

w

    however, for large scale nl tasks a decomposable, robust representation is preferable

    vocabulary of real-world  big data tasks could be huge (scalability)

>100m unique words in a modern commercial search engine log, and keeps growing

    new words, misspellings, and word fragments frequently occur 

(generalizability) 

microsoft research

162

embedding vector

                     

embedding vector

dim=500

    

id27 
matrix: 500    100    

dim = 100m

1-hot word vector

huang, he, gao, deng, acero, heck,    learning deep structured 
semantic models for web search using clickthrough data,    cikm, 2013

microsoft research

163

dim=500

    

dim = 50k

    

dim = 100m

swu embedding 
matrix: 500    50    

swu encoding
matrix

1-hot word vector

could go up to extremely large

microsoft research

164

example: cat     #cat#     #-c-a, c-a-t, a-t-# 
(w/ word boundary mark #)

dim

letter-trigram embedding 

matrix

                  =

        

(                ,        

)

    

   
    =1

.   1,...0                         1,       1,   
c-a-t ... a-t-#

    #-c-a                 

   

# total letter-trigrams

count of ltg(k)
in the word    cat        :the vector of ltg(k)

two words has the same ltg: 
collision rate     0.004%

microsoft research

165

supervised embeddings for

semantic modeling with applications

--- embedding linguistic symbols by backprop
--- mining distant supervision signals

microsoft research

166

deep structured semantic model (dssm)

    build word/phrase, or sentence-level semantic 

vector representation

    trained by a similarity-driven objective

    projecting semantically similar phrases to vectors close to each 

other

    projecting semantically different phrases to vectors far apart

microsoft research

167

dssm for learning semantic embedding

initialization:

neural networks are initialized with random weights

semantic vector

letter-trigram 
embedding matrix

        

w4

w3

w2

d=300

d=500

d=500

        +

           

d=300

d=500

d=500

d=300

d=500

d=500

letter-trigram encoding
matrix (fixed)

dim = 50k

w1

bag-of-words vector

dim = 100m

input word/phrase

s:    racing  car   

dim = 50k

dim = 50k

dim = 100m
t+:    formula one   

dim = 100m
t -:    racing to me   

microsoft research

168

dssm for learning semantic embedding

training:

compute cosine similarity between semantic vectors 

compute 
gradients

    

            (                      ,         + )

   
         ={    +,       }             (                      ,             )

    w

cos(        ,         +)

cos(        ,            )

semantic vector

letter-trigram 
embedding matrix

        

w4

w3

w2

d=300

d=500

d=500

        +

           

d=300

d=500

d=500

d=300

d=500

d=500

letter-trigram encoding
matrix (fixed)

dim = 50k

w1

bag-of-words vector

dim = 100m

input word/phrase

s:    racing  car   

dim = 50k

dim = 50k

dim = 100m
t+:    formula one   

dim = 100m
t -:    racing to me   

microsoft research

169

dssm for learning semantic embedding

runtime:

semantic vector

letter-trigram 
embedding matrix

        

ws,4

ws,3

ws,2

d=300

d=500

d=500

letter-trigram encoding
matrix (fixed)

dim = 50k

ws,1

bag-of-words vector

dim = 100m

input word/phrase

s:    racing  car   

similar

apart

            

wt,4

wt,3

wt,2

wt,1

d=300

d=500

d=500

dim = 50k

            

wt,4

wt,3

wt,2

wt,1

d=300

d=500

d=500

dim = 50k

dim = 100m

t1:    formula one   

dim = 100m

t2:    racing to me   

microsoft research

170

context <-> word 
query <-> clicked-doc 
pattern<-> relationship 

         +      =

exp (                      ,     + )

              exp(                      ,      )

         +     

microsoft research

171

word semantic embedding

web search

query intent detection

id53

machine translation

query auto-suggestion

query auto-completion

apps recommendation

many applications of dssm (many low-hanging fruits):
learning semantic similarity between x and y
tasks

source x

target y

context

search query

search query

word

web documents

use intent

pattern / mention (in nl)

relation / entity (in kb)

sentence in language a

translated sentences in language b

search query

partial search query

user profile

distillation of survey feedbacks

feedbacks in text

automatic image captioning

id162

natural user interface

ads selection

ads click prediction

email analysis: people prediction

email search

email declutering

image

text query

command (text / speech / gesture)

search query

search query

email content

search query

email contents

suggested query

completed query

recommended apps

relevant feedbacks

text caption

images

actions

ad keywords

ad documents

recipients, senders

email content

email contents in similar threads

knowledge-base construction

entity from source

entity fitting desired relationship

contextual entity search

automatic highlighting

text summarization

key phrase / context

documents in reading

long text 

entity / its corresponding page

key phrases to be highlighted

172

summarized short text

automatic image captioning (msr system)

detector models,
deep neural net 

features,    

computer 

vision 
system

street

signs

under

on

light

stop

red

sign

bus

city

pole

building

traffic

a stop sign at an intersection on a city street

dssm model

semantic 
ranking 
system

language 

model

a red stop sign sitting under a traffic light on a city street
a stop sign at an intersection on a street
a stop sign with two street signs on a pole on a sidewalk
a stop sign at an intersection on a city street
   
a stop sign
a red traffic light

caption 

generation 

system

fang, gupta, iandola, srivastava, deng, dollar, 
gao, he, mitchell, platt, zitnick, zweig,    from 
captions to visual concepts and back,    accepted 
to appear in cvpr, 2015; in arxiv 2014

microsoft system (msr):
use of dssm for global 
semantic matching

174

a

b

machine:

human:

competition results (cvpr-2015, june, boston)

won 1st prize at ms coco captioning challenge 2015!

measure the quality of 
the captions by human 
judge (e.g., turing test).

note: even a human 
cannot guarantee to 
pass turing test by 100%

the top teams and the state-of-the-art 

msr

google

msr captivator    

montreal/toronto

berkeley lrcn

%  of  captions that 
pass the turing test

official 
rank

32.2% 

31.7%

30.1%

27.2%

26.8%

1st(tie)

1st(tie)

3rd(tie)

3rd(tie)

5th

other groups: baidu/ucla, stanford, tsinghua, etc.

human

67.5%

--

word semantic embedding

web search

query intent detection

id53

machine translation

query auto-suggestion

query auto-completion

apps recommendation

many applications of dssm (many low-hanging fruits):
learning semantic similarity between x and y
tasks

source x

target y

context

search query

search query

word

web documents

use intent

pattern / mention (in nl)

relation / entity (in kb)

sentence in language a

translated sentences in language b

search query

partial search query

user profile

distillation of survey feedbacks

feedbacks in text

automatic image captioning

id162

natural user interface

ads selection

ads click prediction

email analysis: people prediction

email search

email declutering

image

text query

command (text / speech / gesture)

search query

search query

email content

search query

email contents

suggested query

completed query

recommended apps

relevant feedbacks

text caption

images

actions

ad keywords

ad documents

recipients, senders

email content

email contents in similar threads

knowledge-base construction

entity from source

entity fitting desired relationship

contextual entity search

key phrase / context

entity / its corresponding page

automatic highlighting

text summarization

documents in reading

long text 

key phrases to be highlighted

181

summarized short text

http://blogs.bing.com/search/2014/12/10/bing-brings-the-worlds-knowledge-straight-to-you-with-insights-for-office/

182

scenario: contextual search in microsoft office/word 

when    lincoln    is selected, pages of a car company, movie, or the town in nebraska will not appear 

183

towards modeling cognitive functions: 

memory and attention

-seq-to-seid24 via lstm with attention mechanism
-memory nets and id63s
-dynamic memory nets 
-from id195 to seq2struct and to struct2struct

microsoft research

184

deep    thought   -vector approach to mt

microsoft research

185

deep    thought   -vector approach to mt

microsoft research

186

   attention    mechanism 
used for    softly    selecting 
relevant input portion from 
memory

microsoft research

189

microsoft research

190

popular theories of human memory/attention

the attention and memory models discussed so far are far from human memory/attention 
mechanisms (https://en.wikipedia.org/wiki/atkinson%e2%80%93shiffrin_memory_model):

hopfield nets store (associative) memories as 
attractors of the dynamic network

191

lstm mainly models short-term memory

192

lstm does not model long-term memory well

lstm makes short-term memory lasting via a simple    unit-loop    mechanism, very

   
different from long-term memory in human cognition
    review of a very recent modeling study on episodic and semantic memories, extending 
the basic lstm formulation

193

slides from: coursera, 2014

194

going beyond l-stm --- towards more realist long-term memory (episodic & semantic)

195

196

towards modeling cognitive functions: 

memory and attention

-seq-to-seid24 via lstm with attention mechanism
-memory nets and id63s
-dynamic memory nets 
-from id195 to seq2struct & to struct2struct

microsoft research

197

review: embedding in the form of    flat    vectors

    a linguistic or physical entity or a simple    relation   

a low-dim continuous-space vector or embedding

mapping via distributed representations by nn

special issue, vol. 46 (1990)
connectionist symbol processing
(4 articles)

pdp book, 1986

198

extension:    flat    vectors    structures (tree/graph)

   structured embedding vectors via tensor-product rep.

symbolic semantic parse tree (complex relation)

then, reasoning in symbolic-space (traditional ai) can be beautifully carried out in the continuous-space in human 
cognitive and neural-net (i.e., connectionist) terms

smolensky & legendre: the harmonic mind, mit press, 2006
from neural computation to optimality-theoretic grammar 
volume i: cognitive architecture; volume 2: linguistic implications

rogers & mcclelland
semantic cognition
mit press, 2006

199

few leaders are admired by george bush

admire(george bush, few leaders)

  (s) = cons(ex1(ex0(ex1(s))),

cons(ex1(ex1(ex1(s))), ex0(s)))
0wex
1] +
1)+wcons

w = wcons
0(wex

0[wex
1wex

1wex
1wex

1[wcons

1(wex

0)]

wcons

v

a

p

meaning (lf)

f

g

b

d

c

patient

output

  

input

p

aux

v

by

a

   passive sentence   

  

isomorphism

b

d

c

aux

patien

t

f b
y

e g
agent

w

slide from paul smolensky, 2015

summary & perspective

    id103 is the first success example of deep learning at industry 

scale

    deep learning is very effective in id103, speech translation (skype 

translator), image recognition (onedrive image tagging), image captioning, 
language understanding (cortana), semantic intelligence, multimodal and 
multitask learning, web search, advertising, entity search (insights for ms 
office), user and business activity prediction, etc.

    enabling factors:

    big datasets for training deep models
    powerful gpgpu computing
    innovations in deep learning architectures and algorithms 

    how to discover distant supervision signals free from human labeling
    how to build deep learning systems grounded on exploiting such    smart    signals (example: dssm)
       

203

summary & perspective

    id103: all low-hanging fruits are taken 
    i.e. more innovation and hard work needed than before

    image recognition: most low-hanging fruits are taken
    natural language: does not seem there is much low-hanging fruit there 

    i.e. even more innovation and hard work needed than before 

    big data analytics (e.g. user behavior, business activities, etc):

- a new frontier

    small data: deep learning may still win (e.g. 2012 kaggle   s drug discovery) 
    perceptual data: deep learning methods always win, and win big
    be careful: data with adversarial nature; data with odd variability

204

issues for    near    future of deep learning

    for perceptual tasks (e.g. speech, image/video, gesture, etc.)

    with supervised data: what will be the limit for growing accuracy wrt increasing 

amounts of labeled data?

    beyond this limit or when labeled data are exhausted or non-economical to 

collect, will novel and effective unsupervised deep learning emerge and what 
will they be (e.g. deep generative models)? 

    many new innovations are to come, likely in the area of unsupervised learning

    for cognitive tasks (e.g. natural language, reasoning, 
knowledge, decision making, etc.)
    will supervised deep learning (e.g. mt) beat the non-deep-learning state of the 

art like speech/image recognition?

    how to distill/exploit    distant    supervision signals for supervised deep learning?
    will dense vector embedding be sufficient for language? do we really need to 

directly encode and recover syntactic/semantic structure of language?

    even more new innovations are to come, likely in the area of new architectures 

and learning methods pertaining to distant supervised learning

205

the future of deep learning

    continued rapid progress in language processing methods 
and applications by both industry and academia
    from image to video processing/understanding
    from supervised learning  (huge success already) to 
unsupervised learning (not much success yet but ideas abound)
    from perception to cognition

    more exploration of attention modeling
    combine representation learning with complex knowledge 

extraction & reasoning

    modeling human memory functions more faithfully
    learning to act and control (deep id23)

    successes in business applications will propel more rapid 
advances in deep learning (positive feedbacks)

206

   
   
   

   
   
   

   
   

   

   

   

   

   
   
   

   

auli, m., galley, m., quirk, c. and zweig, g., 2013. joint language and translation modeling with recurrent neural networks. in emnlp.
auli, m., and gao, j., 2014. decoder integration and expected id7 training for recurrent neural network language models. in acl.
baker, j., li deng, jim glass, s. khudanpur, c.-h. lee, n. morgan, and d. o'shgughnessy, research developments and directions in id103 and 
understanding, part 1, in ieee signal processing magazine, vol. 26, no. 3, pp. 75-80, 2009.

updated minds report on id103 and understanding

bengio, y., 2009. learning deep architectures for ai. foundumental trends machine learning, vol. 2.
bengio, y., courville, a., and vincent, p. 2013. representation learning: a review and new perspectives. ieee trans. pami, vol. 38, pp. 1798-1828.
bengio, y., ducharme, r., and vincent, p., 2000. a neural probabilistic language model, in nips. 

collobert, r., weston, j., bottou, l., karlen, m., kavukcuoglu, k., and kuksa, p., 2011. natural language processing (almost) from scratch. in jmlr, vol. 12.
dahl, g., yu, d., deng, l., and acero, 2012. a. context-dependent, pre-trained deep neural networks for large vocabulary id103, ieee trans. audio, 
speech, & language proc., vol. 20 (1), pp. 30-42.
deerwester, s., dumais, s. t., furnas, g. w., landauer, t., and harshman, r. 1990. indexing by latent semantic analysis. j. american society for information science, 
41(6): 391-407
deng, l.  a dynamic, feature-based approach to the interface between phonology  & id102 for speech modeling and recognition, speech communication, vol. 24, no. 
4, pp. 299-323, 1998.

computational models for speech production
switching dynamic system models for speech articulation and acoustics

deng l., g. ramsay, and d. sun, production models as a structural basis for automatic id103," speech communication (special issue on speech production 
modeling), in speech communication, vol. 22, no. 2, pp. 93-112, august 1997.
deng l. and j. ma, spontaneous id103 using a statistical coarticulatory model for the vocal tract resonance dynamics, journal of the acoustical society 
of america, 2000.

deep learning: methods and applications

machine learning paradigms for id103: an overview

deng l. and xuedong huang, challenges in adopting id103, in communications of the acm, vol. 47, no. 1, pp. 11-13, january 2004.
deng, l., seltzer, m., yu, d., acero, a., mohamed, a., and hinton, g., binary coding of speech spectrograms using a deep auto-encoder, interspeech, 2010.
deng, l., tur, g, he, x, and hakkani-tur, d. 2012. use of kernel deep convex networks and end-to-end learning for spoken language understanding, proc. ieee 
workshop on spoken language technologies.
deng, l., yu, d. and acero, a. 2006. structured speech modeling, ieee trans. on audio, speech and language processing, vol. 14, no. 5, pp. 1492-1504.
new types of deep neural network learning for id103 and related applications: an overview

use of differential cepstra as acoustic features in hidden trajectory modeling for phonetic recognition

deep convex network: a scalable architecture for speech pattern classification

deep stacking networks for information retrieval

microsoft research

207

a multimodal variational approach to learning and id136 in switching state space models

microsoft research

208

learning with recursive perceptual representations

microsoft research

209

210

