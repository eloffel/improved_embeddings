character-aware neural language models

yoon kim yacine jernite david sontag alexander rush

harvard seas

new york university

to appear in aaai 2016

code: https://github.com/yoonkim/lstm-char-id98

kim, jernite, sontag, rush

character-aware neural language models

1 / 79

language model

language model (lm): id203 distribution over a sequence of words.
p(w1, . . . , wt ) for any sequence of length t from a vocabulary v (with
wi     v for all i).
important for many downstream applications:

machine translation

id103

text generation

kim, jernite, sontag, rush

character-aware neural language models

2 / 79

count-based language models

t(cid:89)

by the chain rule, any distribution can be factorized as:

p(w1, . . . , wt ) =

p(wt|w1, . . . , wt   1)

t=1

id165 language models make a markov assumption:

p(wt|w1, . . . , wt   1)     p(wt|wt   n, . . . , wt   1)

needs smoothing to deal with rare id165s.

kim, jernite, sontag, rush

character-aware neural language models

3 / 79

neural language models

neural language models (nlm)

represent words as dense vectors in rn (id27s).
wt     r|v| : one-hot representation of word     v at time t
    xt = xwt : id27 (x     rn  |v|, n < |v|)

kim, jernite, sontag, rush

character-aware neural language models

4 / 79

neural language models

neural language models (nlm)

represent words as dense vectors in rn (id27s).
wt     r|v| : one-hot representation of word     v at time t
    xt = xwt : id27 (x     rn  |v|, n < |v|)

train a neural net that composes history to predict next word.
exp(pj    g (x1, . . . , xt   1) + qj )
exp(pj(cid:48)    g (x1, . . . , xt   1) + qj(cid:48)

p(wt = j|w1, . . . , wt   1) =

(cid:88)

j(cid:48)   v

)

= softmax(pg (x1, . . . , xt   1) + q)

pj     rm, qj     r : output id27/bias for word j     v

g : composition function

kim, jernite, sontag, rush

character-aware neural language models

4 / 79

feed-forward nlm (bengio, ducharme, and vincent 2003)

kim, jernite, sontag, rush

character-aware neural language models

5 / 79

feed-forward nlm (bengio, ducharme, and vincent 2003)

kim, jernite, sontag, rush

character-aware neural language models

6 / 79

feed-forward nlm (bengio, ducharme, and vincent 2003)

kim, jernite, sontag, rush

character-aware neural language models

7 / 79

feed-forward nlm (bengio, ducharme, and vincent 2003)

kim, jernite, sontag, rush

character-aware neural language models

8 / 79

recurrent neural network lm (mikolov et al. 2011)

maintain a hidden state vector ht that is recursively calculated.

kim, jernite, sontag, rush

character-aware neural language models

9 / 79

recurrent neural network lm (mikolov et al. 2011)

maintain a hidden state vector ht that is recursively calculated.

ht = f (wxt + uht   1 + b)

ht     rm : hidden state at time t (summary of history)

w     rm  n : input-to-hidden transformation
u     rm  m : hidden-to-hidden transformation

f (  ) : non-linearity

kim, jernite, sontag, rush

character-aware neural language models

9 / 79

recurrent neural network lm (mikolov et al. 2011)

maintain a hidden state vector ht that is recursively calculated.

ht = f (wxt + uht   1 + b)

ht     rm : hidden state at time t (summary of history)

w     rm  n : input-to-hidden transformation
u     rm  m : hidden-to-hidden transformation

f (  ) : non-linearity

apply softmax to ht.

kim, jernite, sontag, rush

character-aware neural language models

9 / 79

recurrent neural network lm (mikolov et al. 2011)

kim, jernite, sontag, rush

character-aware neural language models

10 / 79

recurrent neural network lm (mikolov et al. 2011)

kim, jernite, sontag, rush

character-aware neural language models

11 / 79

recurrent neural network lm (mikolov et al. 2011)

kim, jernite, sontag, rush

character-aware neural language models

12 / 79

recurrent neural network lm (mikolov et al. 2011)

kim, jernite, sontag, rush

character-aware neural language models

13 / 79

id27s (collobert et al. 2011; mikolov et al. 2012)

key ingredient in neural language models.

after training, similar words are close in the vector space.

(not unique to nlms)

kim, jernite, sontag, rush

character-aware neural language models

14 / 79

nlm performance (on id32)

di   cult/expensive to train, but performs well.

language model

perplexity

5-gram count-based (mikolov and zweig 2012)
id56 (mikolov and zweig 2012)
deep id56 (pascanu et al. 2013)
lstm (zaremba, sutskever, and vinyals 2014)

141.2
124.7
107.5
78.4

renewed interest in id38.

kim, jernite, sontag, rush

character-aware neural language models

15 / 79

nlm issue

issue: the fundamental unit of information is still the word

separate embeddings for    trading   ,    leading   ,    training   , etc.

kim, jernite, sontag, rush

character-aware neural language models

16 / 79

nlm issue

issue: the fundamental unit of information is still the word

separate embeddings for    trading   ,    trade   ,    trades   , etc.

kim, jernite, sontag, rush

character-aware neural language models

17 / 79

nlm issue

no parameter sharing across orthographically similar words (e.g.,
spelled similarly).

orthography contains much semantic/syntactic information.

how can we leverage subword information for id38?

can we exploit this to perform better id38 with rare
words?

kim, jernite, sontag, rush

character-aware neural language models

18 / 79

previous (nlm-based) work

use morphological segmenter as a preprocessing step

unfortunately     unpre    fortunatestm    lysuf

luong, socher, and manning 2013: id56 over
morpheme embeddings

botha and blunsom 2014: sum over word/morpheme embeddings

kim, jernite, sontag, rush

character-aware neural language models

19 / 79

this work

main idea: no explicit morphology, use characters directly.

kim, jernite, sontag, rush

character-aware neural language models

20 / 79

this work

main idea: no explicit morphology, use characters directly.

convolutional neural networks (id98) (lecun et al. 1989)

central to deep learning systems in vision.

shown to be e   ective for nlp tasks (collobert et al. 2011).

id98s in nlp typically involve temporal (rather than spatial)
convolutions over words.

kim, jernite, sontag, rush

character-aware neural language models

20 / 79

network architecture: overview

kim, jernite, sontag, rush

character-aware neural language models

21 / 79

character-level id98 (charid98)

kim, jernite, sontag, rush

character-aware neural language models

22 / 79

character-level id98 (charid98)

c     rd  l : matrix representation of word (of length l)
h     rd  w : convolutional    lter matrix

d : dimensionality of character embeddings (e.g., 15)

w : width of convolution    lter (e.g., 1,2,3,4,5)

kim, jernite, sontag, rush

character-aware neural language models

23 / 79

character-level id98 (charid98)

c     rd  l : matrix representation of word (of length l)
h     rd  w : convolutional    lter matrix

d : dimensionality of character embeddings (e.g., 15)

w : width of convolution    lter (e.g., 1,2,3,4,5)

1. apply a convolution between c and h to obtain a vector f     rl   w +1

f[i] = (cid:104)c[   , i : i + w     1], h(cid:105)

where (cid:104)a, b(cid:105) = tr(abt ) is the frobenius inner product.

kim, jernite, sontag, rush

character-aware neural language models

23 / 79

character-level id98 (charid98)

c     rd  l : matrix representation of word (of length l)
h     rd  w : convolutional    lter matrix

d : dimensionality of character embeddings (e.g., 15)

w : width of convolution    lter (e.g., 1,2,3,4,5)

1. apply a convolution between c and h to obtain a vector f     rl   w +1

f[i] = (cid:104)c[   , i : i + w     1], h(cid:105)

where (cid:104)a, b(cid:105) = tr(abt ) is the frobenius inner product.
2. take the max-over-time (with bias and nonlinearity)

y = tanh(max

i

{f[i]} + b)

as the feature corresponding to the    lter h (for a particular word).

kim, jernite, sontag, rush

character-aware neural language models

23 / 79

character-level id98 (charid98)

kim, jernite, sontag, rush

character-aware neural language models

24 / 79

character-level id98 (charid98)

c     rd  l : representation of absurdity

kim, jernite, sontag, rush

character-aware neural language models

25 / 79

character-level id98 (charid98)

h     rd  w : convolutional    lter matrix of width w = 3

kim, jernite, sontag, rush

character-aware neural language models

26 / 79

character-level id98 (charid98)

f[1] = (cid:104)c[   , 1 : 3], h(cid:105)

kim, jernite, sontag, rush

character-aware neural language models

27 / 79

character-level id98 (charid98)

f[1] = (cid:104)c[   , 1 : 3], h(cid:105)

kim, jernite, sontag, rush

character-aware neural language models

28 / 79

character-level id98 (charid98)

f[2] = (cid:104)c[   , 2 : 4], h(cid:105)

kim, jernite, sontag, rush

character-aware neural language models

29 / 79

character-level id98 (charid98)

f[t     2] = (cid:104)c[   , t     2 : t ], h(cid:105)

kim, jernite, sontag, rush

character-aware neural language models

30 / 79

character-level id98 (charid98)

y [1] = max

i

{f[i]}

kim, jernite, sontag, rush

character-aware neural language models

31 / 79

character-level id98 (charid98)

each    lter picks out a character id165

kim, jernite, sontag, rush

character-aware neural language models

32 / 79

character-level id98 (charid98)

f(cid:48)[1] = (cid:104)c[   , 1 : 2], h(cid:48)(cid:105)

kim, jernite, sontag, rush

character-aware neural language models

33 / 79

character-level id98 (charid98)

y [2] = max

i

{f(cid:48)[i]}

kim, jernite, sontag, rush

character-aware neural language models

34 / 79

character-level id98 (charid98)

many    lter matrices (25   200) per width (1   7)

kim, jernite, sontag, rush

character-aware neural language models

35 / 79

character-level id98 (charid98)

add bias, apply nonlinearity

kim, jernite, sontag, rush

character-aware neural language models

36 / 79

0.7$1.5$1.1$0.2$1.7$0.8$1.0$0.9$0.5$1.1$~bcharacter-level id98 (charid98)

for roughly the same number of parameters (20 million),

before

now

id27

output from charid98

ptb perplexity: 85.4

ptb perplexity: 84.6

charid98 is slower, but convolution operations on gpu have been very
optimized.

kim, jernite, sontag, rush

character-aware neural language models

37 / 79

character-level id98 (charid98)

for roughly the same number of parameters (20 million),

before

now

id27

output from charid98

ptb perplexity: 85.4

ptb perplexity: 84.6

charid98 is slower, but convolution operations on gpu have been very
optimized.

can we model more complex interactions between character id165s
picked up by the    lters?

kim, jernite, sontag, rush

character-aware neural language models

37 / 79

highway network

kim, jernite, sontag, rush

character-aware neural language models

38 / 79

highway network

y : output from charid98

multilayer id88

z = g (wy + b)

kim, jernite, sontag, rush

character-aware neural language models

39 / 79

highway network

y : output from charid98

multilayer id88

z = g (wy + b)

highway network
(srivastava, gre   , and schmidhuber 2015)

z = t (cid:12) g (wh y + bh ) + (1     t) (cid:12) y

wh , bh : a   ne transformation

t =   (wt y + bt ) : transform gate

1     t : carry gate

hierarchical, adaptive composition of character id165s.

kim, jernite, sontag, rush

character-aware neural language models

39 / 79

highway network

kim, jernite, sontag, rush

character-aware neural language models

40 / 79

input from charid98 input to lstm highway network

model

word model

no highway layers
one mlp layer
one highway layer
two highway layers

perplexity

85.4

84.6
92.6
79.7
78.9

no more gains with 3+ layers.

kim, jernite, sontag, rush

character-aware neural language models

41 / 79

results: english id32

kn-5 (mikolov et al. 2012)
id56 (mikolov et al. 2012)
deep id56 (pascanu et al. 2013)
sum-prod net (cheng et al. 2014)
lstm-medium (zaremba, sutskever, and vinyals 2014)
lstm-huge (zaremba, sutskever, and vinyals 2014)

lstm-word-small
lstm-char-small
lstm-word-large
lstm-char-large

ppl

size

141.2
124.7
107.5
100.0
82.7
78.4

97.6
92.3
85.4
78.9

2 m
6 m
6 m
5 m
20 m
52 m

5 m
5 m
20 m
19 m

kim, jernite, sontag, rush

character-aware neural language models

42 / 79

what about morphologically rich languages?

data-s
|c|
51
101
74
72
76
62

|v|
10 k
46 k
37 k
27 k
25 k
62 k

english (en)
czech (cs)
german (de)
spanish (es)
french (fr)
russian (ru)
|v| = word vocab size
|c| = character vocab size
t = number of tokens in training set.

t

|v|
60 k
1 m
1 m 206 k
1 m 339 k
1 m 152 k
1 m 137 k
1 m 497 k

data-l
|c|
197
195
260
222
225
111

t

20 m
17 m
51 m
56 m
57 m
25 m

kim, jernite, sontag, rush

character-aware neural language models

43 / 79

what about morphologically rich languages?

data-s
|c|
51
101
74
72
76
62

|v|
10 k
46 k
37 k
27 k
25 k
62 k

t

|v|
60 k
1 m
1 m 206 k
1 m 339 k
1 m 152 k
1 m 137 k
1 m 497 k

data-l
|c|
197
195
260
222
225
111

t

20 m
17 m
51 m
56 m
57 m
25 m

english (en)
czech (cs)
german (de)
spanish (es)
french (fr)
russian (ru)

|v| varies quite a bit by language.

(e   ectively use the full vocabulary)

kim, jernite, sontag, rush

character-aware neural language models

44 / 79

baselines

kneser-ney lm: count-based baseline

word lstm: id27s as input

morpheme lbl (botha and blunsom 2014)

input for word k is

xk(cid:124)(cid:123)(cid:122)(cid:125)

id27

+

(cid:88)
(cid:124) (cid:123)(cid:122) (cid:125)

j   mk

mj

morpheme embeddings

morpheme lstm: same input as above, but with lstm architecture

morphemes obtained from running an unsupervised morphological tagger
morfessor cat-map (creutz and lagus 2007).

kim, jernite, sontag, rush

character-aware neural language models

45 / 79

perplexity on data-s (1 m tokens)

kim, jernite, sontag, rush

character-aware neural language models

46 / 79

perplexity on data-s (1 m tokens)

kim, jernite, sontag, rush

character-aware neural language models

47 / 79

perplexity on data-s (1 m tokens)

kim, jernite, sontag, rush

character-aware neural language models

48 / 79

perplexity on data-s (1 m tokens)

kim, jernite, sontag, rush

character-aware neural language models

49 / 79

perplexity on data-s (1 m tokens)

kim, jernite, sontag, rush

character-aware neural language models

50 / 79

perplexity on data-l (17-57 m tokens)

kim, jernite, sontag, rush

character-aware neural language models

51 / 79

how does performance vary with corpus/vocab size?

experiment on german large dataset:

take the most frequent k words as the vocabulary and replace rest
with <unk>
compare % perplexity reduction going from word to character lstm.

kim, jernite, sontag, rush

character-aware neural language models

52 / 79

how does performance vary with corpus/vocab size?

experiment on german large dataset:

take the most frequent k words as the vocabulary and replace rest
with <unk>
compare % perplexity reduction going from word to character lstm.

vocabulary size

10 k

25 k

50 k

100 k

training

size

1 m
5 m
10 m
25 m

17
8
9
9

16
14
9
8

21
16
12
9

   
21
15
10

character model outperforms word model in all scenarios.

kim, jernite, sontag, rush

character-aware neural language models

52 / 79

learned word representations

kim, jernite, sontag, rush

character-aware neural language models

53 / 79

learned word representations

kim, jernite, sontag, rush

character-aware neural language models

54 / 79

learned word representations

kim, jernite, sontag, rush

character-aware neural language models

55 / 79

learned word representations (in vocab)

(based on cosine similarity)

while

although
letting
though
minute

his

your
her
my
their

in vocabulary

you

richard

trading

conservatives

jonathan

we
guys

i

robert

neil
nancy

advertised
advertising
turnover
turnover

word

embedding

kim, jernite, sontag, rush

character-aware neural language models

56 / 79

learned word representations (in vocab)

(based on cosine similarity)

while

although
letting
though
minute

chile
whole

word

embedding

characters

(before highway) meanwhile

white

in vocabulary

you

richard

trading

conservatives

jonathan

we
guys

i

your
young
four
youth

robert

neil
nancy

hard
rich
richer
richter

advertised
advertising
turnover
turnover

heading
training
reading
leading

his

your
her
my
their

this
hhs
is
has

kim, jernite, sontag, rush

character-aware neural language models

56 / 79

learned word representations (in vocab)

(based on cosine similarity)

while

although
letting
though
minute

chile
whole

word

embedding

characters

(before highway) meanwhile

characters

(after highway)

white

meanwhile

whole
though

nevertheless

his

your
her
my
their

this
hhs
is
has

hhs
this
their
your

in vocabulary

you

richard

trading

conservatives

jonathan

we
guys

i

your
young
four
youth

we
your
doug

i

robert

neil
nancy

hard
rich
richer
richter

eduard
gerard
edward

carl

advertised
advertising
turnover
turnover

heading
training
reading
leading

trade

training
traded
trader

kim, jernite, sontag, rush

character-aware neural language models

56 / 79

learned word representations (in vocab)

(based on cosine similarity)

while

although
letting
though
minute

chile
whole

word

embedding

characters

(before highway) meanwhile

characters

(after highway)

white

meanwhile

whole
though

nevertheless

his

your
her
my
their

this
hhs
is
has

hhs
this
their
your

in vocabulary

you

richard

trading

conservatives

jonathan

we
guys

i

your
young
four
youth

we
your
doug

i

robert

neil
nancy

hard
rich
richer
richter

eduard
gerard
edward

carl

advertised
advertising
turnover
turnover

heading
training
reading
leading

trade

training
traded
trader

kim, jernite, sontag, rush

character-aware neural language models

57 / 79

learned word representations (in vocab)

(based on cosine similarity)

while

although
letting
though
minute

chile
whole

word

embedding

characters

(before highway) meanwhile

characters

(after highway)

white

meanwhile

whole
though

nevertheless

his

your
her
my
their

this
hhs
is
has

hhs
this
their
your

in vocabulary

you

richard

trading

conservatives

jonathan

we
guys

i

your
young
four
youth

we
your
doug

i

robert

neil
nancy

hard
rich
richer
richter

eduard
gerard
edward

carl

advertised
advertising
turnover
turnover

heading
training
reading
leading

trade

training
traded
trader

kim, jernite, sontag, rush

character-aware neural language models

58 / 79

learned word representations (in vocab)

(based on cosine similarity)

while

although
letting
though
minute

chile
whole

word

embedding

characters

(before highway) meanwhile

characters

(after highway)

white

meanwhile

whole
though

nevertheless

his

your
her
my
their

this
hhs
is
has

hhs
this
their
your

in vocabulary

you

richard

trading

conservatives

jonathan

we
guys

i

your
young
four
youth

we
your
doug

i

robert

neil
nancy

hard
rich
richer
richter

eduard
gerard
edward

carl

advertised
advertising
turnover
turnover

heading
training
reading
leading

trade

training
traded
trader

kim, jernite, sontag, rush

character-aware neural language models

59 / 79

learned word representations (in vocab)

(based on cosine similarity)

while

although
letting
though
minute

chile
whole

word

embedding

characters

(before highway) meanwhile

characters

(after highway)

white

meanwhile

whole
though

nevertheless

his

your
her
my
their

this
hhs
is
has

hhs
this
their
your

in vocabulary

you

richard

trading

conservatives

jonathan

we
guys

i

your
young
four
youth

we
your
doug

i

robert

neil
nancy

hard
rich
richer
richter

eduard
gerard
edward

carl

advertised
advertising
turnover
turnover

heading
training
reading
leading

trade

training
traded
trader

kim, jernite, sontag, rush

character-aware neural language models

60 / 79

learned word representations (oov)

computer-aided

out-of-vocabulary
misinformed

looooook

characters

(before highway)

computer-guided

computerized

disk-drive
computer

informed
performed
transformed

inform

characters

computer-guided
computer-driven

(after highway)

computerized

computer

informed
performed

outperformed
transformed

look
cook
looks
shook

look
looks
looked
looking

kim, jernite, sontag, rush

character-aware neural language models

61 / 79

learned word representations (oov)

computer-aided

out-of-vocabulary
misinformed

looooook

characters

(before highway)

computer-guided

computerized

disk-drive
computer

informed
performed
transformed

inform

characters

computer-guided
computer-driven

(after highway)

computerized

computer

informed
performed

outperformed
transformed

look
cook
looks
shook

look
looks
looked
looking

kim, jernite, sontag, rush

character-aware neural language models

62 / 79

learned word representations (oov)

computer-aided

out-of-vocabulary
misinformed

looooook

characters

(before highway)

computer-guided

computerized

disk-drive
computer

informed
performed
transformed

inform

characters

computer-guided
computer-driven

(after highway)

computerized

computer

informed
performed

outperformed
transformed

look
cook
looks
shook

look
looks
looked
looking

kim, jernite, sontag, rush

character-aware neural language models

63 / 79

what is the highway network doing?

q: might we simply be learning to carry some dimensions and to combine
others? is the transform gate truly a function of the input word?

kim, jernite, sontag, rush

character-aware neural language models

64 / 79

input from charid98 input to lstm what is the highway network doing?

a: no. for all dimensions, on some words   (  )     0, and for others     1.

kim, jernite, sontag, rush

character-aware neural language models

65 / 79

what is the convolutional layer doing?

q: does each    lter truly pick out a character id165?

kim, jernite, sontag, rush

character-aware neural language models

66 / 79

0.4$%0.8$2.2$0.1$0.5$%0.4$0.4$%0.4$0.1$0.1$1.2$1.5$%0.8$%1.5$0.2$0.1$1.2$0.7$0.2$0.1$%1.2$0.2$%0.2$0.3$0.2$%1.3$%0.1$%0.2$%0.5$0.1$0.2$%0.3$0.3$%0.1$1.0$%0.3$a    b    s    u    r   d    i    t    y 0.1$0.7$0.2$%0.1$0.2$%0.4$0.5$0.7$concatena3on$of$character$embeddings$max%over%3me$pooling$single$   lter$output$what is the convolutional layer doing?

for each length-6    lter, the 100 substrings with highest    lter response.

kim, jernite, sontag, rush

character-aware neural language models

67 / 79

what is the convolutional layer doing?

for each length-6    lter, the 100 substrings with highest    lter response.

kim, jernite, sontag, rush

character-aware neural language models

68 / 79

what is the convolutional layer doing?

for each length-6    lter, the 100 substrings with highest    lter response.

kim, jernite, sontag, rush

character-aware neural language models

69 / 79

conclusion

a character-aware language model that relies only on character-level
inputs: charid98 + highway network + lstm.

outperforms strong word/morpheme lstm baselines.

much recent work on character inputs:

santos and zadrozny 2014: id98 over characters concatenated with
id27s into crf.
zhang and lecun 2015: deep id98 over characters for document
classi   cation.
ballesteros, dyer, and smith 2015: lstm over characters for parsing.
ling et al. 2015: lstm over characters into another lstm for
id38/pos-tagging.

more broadly, suggests new ways to think about representation
learning.

kim, jernite, sontag, rush

character-aware neural language models

70 / 79

appendix: character id165 representations

pre   xes, su   xes, hyphenated, others

pre   xes: character id165s that start with    start-of-word    character, such

as {un, {mis. su   xes de   ned similarly.

kim, jernite, sontag, rush

character-aware neural language models

71 / 79

appendix: hyperparameters

small

large

d
w [1, 2, 3, 4, 5, 6]
h
f

15
[25    w ]
tanh

15
[1, 2, 3, 4, 5, 6, 7]
[min{200, 50    w}]
tanh

id98

hw-net

lstm

l
g

1
relu

2

l
m 300

2
relu

2
650

kim, jernite, sontag, rush

character-aware neural language models

72 / 79

appendix: results on data-s

b&b

small

large

kn-4
mlbl

word
morph
char

word
morph
char

cs de es

fr ru

545
465

503
414
401

366
296

305
278
260

241
200

212
197
182

274
225

229
216
189

396
304

352
290
278

286
263

357
493
398
271
371 239 165 184 261

200
177

222
196

kim, jernite, sontag, rush

character-aware neural language models

73 / 79

appendix: results on data-l

cs de es

fr ru en

b&b

small

kn-4
mlbl

word
morph
char

862
643

463
404

219
203

243
227

347
331

701
202
615
209
578 305 169 190

186
189

390
300

353
331
313

291
273

236
233
216

kim, jernite, sontag, rush

character-aware neural language models

74 / 79

appendix: e   ect of highway layers (ptb)

small

large

no highway layers
one highway layer
two highway layers
multilayer id88

100.3
92.3
90.1
111.2

84.6
79.7
78.9
92.6

no more gains with 2+ layers (may be language dependent).

kim, jernite, sontag, rush

character-aware neural language models

75 / 79

references i

bengio, yoshua, rejean ducharme, and pascal vincent (2003).    a neural

probabilistic language model   .
research 3, pp. 1137   1155.

in: journal of machine learning

mikolov, tomas et al. (2011).    empirical evaluation and combination of

advanced id38 techniques   .
interspeech.

in: proceedings of

collobert, ronan et al. (2011).    natural language processing (almost)

from scratch   .
pp. 2493   2537.

in: journal of machine learning research 12,

mikolov, tomas et al. (2012).    subword id38 with neural

networks   .

in: preprint: www.   t.vutbr.cz/  imikolov/id56lm/char.pdf.

mikolov, tomas and geo   rey zweig (2012).    context dependent

recurrent neural network language model   .

in: proceedings of slt.

pascanu, razvan et al. (2013).    how to construct deep neural

networks   .

in: arxiv:1312.6026.

kim, jernite, sontag, rush

character-aware neural language models

76 / 79

references ii

zaremba, wojciech, ilya sutskever, and oriol vinyals (2014).    recurrent

neural network id173   .

in: arxiv:1409.2329.

luong, minh-thang, richard socher, and chris manning (2013).    better

word representations with id56s for morphology   .
in: proceedings of conll.

botha, jan and phil blunsom (2014).    compositional morphology for
in: proceedings of

word representations and language modelling   .
icml.

lecun, yann et al. (1989).    handwritten digit recognition with a

id26 network   .

in: proceedings of nips.

srivastava, rupesh kumar, klaus gre   , and jurgen schmidhuber (2015).

   training very deep networks   .

in: arxiv:1507.06228.

cheng, wei chen et al. (2014).    id38 with sum-product

networks   .

in: proceedings of interspeech.

kim, jernite, sontag, rush

character-aware neural language models

77 / 79

references iii

creutz, mathias and krista lagus (2007).    unsupervised models for

morpheme segmentation and morphology learning   .
of the acm transations on speech and language processing.

in: proceedings

santos, cicero nogueira dos and bianca zadrozny (2014).    learning
in:

character-level representations for part-of-speech tagging   .
proceedings of icml.

zhang, xiang and yann lecun (2015).    text understanding from

scratch   .

in: arxiv:1502.01710.

ballesteros, miguel, chris dyer, and noah a. smith (2015).    improved

transition-based parsing by modeling characters instead of words with
lstms   .

in: proceedings of emnlp 2015.

ling, wang et al. (2015).    finding function in form: compositional
in:

character models for open vocabulary word representation   .
proceedings of emnlp.

kim, jernite, sontag, rush

character-aware neural language models

78 / 79

references iv

hochreiter, sepp and j  urgen schmidhuber (1997).    long short-term

memory   .

in: neural computation 9, pp. 1735   1780.

kim, jernite, sontag, rush

character-aware neural language models

79 / 79

