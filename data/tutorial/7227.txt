deep	reinforcement	learning	

lecture	1:	mo5va5on	+	overview	+	mdps	+	exact	

solu5on	methods		

	

	

pieter	abbeel	

openai		/	uc	berkeley	/	gradescope	

	

many	slides	made	with	john	schulman,	yan	(rocky)	duan	and	xi	(peter)	chen	

	

organizers	/	instructors	/	tas	

logislcs	

n   

n   

n   

n   

laptop	setup	for	labs	
n    check	your	email!	

communicalon	with	sta   	

n   

in-person	

n    piazza	

n    help	us	out!	j	

laptops	

n   

stay	charged	

tomorrow	
n    make	sure	to	have	your	badges!	

bootcamp	goals	

n    understand	mathemalcal	and	algorithmic	foundalons	of	

deep	rl	

n    have	implemented	many	of	the	core	algorithms	

schedule	--	saturday	

8:30:	breakfast	/	check-in	
9-10:	core	lecture	1	intro	to	mdps	and	exact	solulon	methods	(pieter	abbeel)	
10-10:10	brief	sponsor	intros	
10:10-11:10	core	lecture	2	sample-based	approximalons	and	fi]ed	learning	(rocky	duan)	
11:10-11:30:	co   ee	break	
11:30-12:30	core	lecture	3	id25	+	variants	(vlad	mnih)	
12:30-1:30	lunch	[catered]	
1:30-2:15	core	lecture	4a	policy	gradients	and	actor	crilc	(pieter	abbeel)	
2:15-3:00	core	lecture	4b	pong	from	pixels	(andrej	karpathy)	
3:00-3:30	core	lecture	5	natural	policy	gradients,	trpo,	and	ppo	(john	schulman)	
3:30-3:50	co   ee	break	
3:50-4:30	core	lecture	6	nuts	and	bolts	of	deep	rl	experimentalon		(john	schulman)	
4:30-7	labs	1-2-3			
7-8	fronlers	lecture	i:	recent	advances,	fronlers	and	future	of	deep	rl	(vlad	mnih)	

	

schedule	--	sunday	

8:30:	breakfast		
9-10:	core	lecture	7	svg,	ddpg,	and	stochaslc	computalon	graphs	(john	schulman)	
10-11:	core	lecture	8	derivalve-free	methods	(peter	chen)	
11-11:30:	co   ee	break	
11:30-12:30	core	lecture	9	model-based	rl	(chelsea	finn)	
12:30-1:30	lunch	[catered]	
1:30-2:30	core	lecture	10	ulliles	/	inverse	rl	(pieter	abbeel	/	chelsea	finn)	
2:30-3:10	two-minute	presentalons	by	each	ta	
3:10-3:30	co   ee	break	
3:30-6	labs	4-5	
6-7	fronlers	lecture	ii:	recent	advances,	fronlers	and	future	of	deep	rl	(sergey	levine)	

markov	decision	process	

assumplon:	agent	gets	to	observe	the	state	

[drawing	from	su]on	and	barto,	reinforcement	learning:	an	introduclon,	1998]	

some	reinforcement	learning	success	stories	

kohl	and	stone,	2004	

ng	et	al,	2004	

tedrake	et	al,	2005	

kober	and	peters,	2009	

mnih	et	al	2013	(id25)	
mnih	et	al,	2015	(a3c)	

silver	et	al,	2014	(dpg)	

lillicrap	et	al,	2015	(ddpg)	

schulman	et	al,	

2016	(trpo	+	gae)	

levine*,	finn*,	et	

al,	2016	
(gps)	

silver*,	huang*,	et	

al,	2016	
(alphago)	

rl	algorithms	landscape	

rl	algorithms	landscape	

markov	decision	process	(mdp)	

an	mdp	is	de   ned	by:	
n    set	of	states		s		
n    set	of	aclons		a 
n    transilon	funclon		p(s    | s, a) 
n    reward	funclon		r(s, a, s   ) 
n    start	state		s0 
n    discount	factor	   
n    horizon	h 

example	mdp:	gridworld	

an	mdp	is	de   ned	by:	
n    set	of	states		s		
n    set	of	aclons		a 
n    transilon	funclon		p(s    | s, a) 
n    reward	funclon		r(s, a, s   ) 
n    start	state		s0 
n    discount	factor	   
n    horizon	h 

goal: 

  : 

outline	

n    oplmal	control	

													=		

given	an	mdp	(s, a, p, r,   , h)	
   nd	the	oplmal	policy	  *	

n    exact	methods:	
n    value	itera*on	
n    policy	iteralon	

for	now:	small	discrete	state-aclon	spaces	as	they	are	simpler	to	get	the	main	
concepts	across.		we	will	consider	large	/	conlnuous	spaces	later!	

oplmal	value	funclon	v*	
 tr(st, at, st+1) |    , s0 = s#

    e" hxt=0

v    (s) = max

=	sum	of	discounted	rewards	when	starlng	from	state	s	and	aclng	oplmally	

oplmal	value	funclon	v*	
 tr(st, at, st+1) |    , s0 = s#

    e" hxt=0

v    (s) = max

=	sum	of	discounted	rewards	when	starlng	from	state	s	and	aclng	oplmally	

let   s	assume:		
aclons	determinislcally	successful,		gamma	=	1,	h	=	100	
v*(4,3)	=	1	
v*(3,3)	=	1	
v*(2,3)	=	1	
v*(1,1)	=	1	
v*(4,2)	=	-1	

oplmal	value	funclon	v*	
 tr(st, at, st+1) |    , s0 = s#

    e" hxt=0

v    (s) = max

=	sum	of	discounted	rewards	when	starlng	from	state	s	and	aclng	oplmally	

let   s	assume:		
aclons	determinislcally	successful,		gamma	=	0.9,	h	=	100	
v*(4,3)	=	1	
v*(3,3)	=	0.9	
v*(2,3)	=	0.9*0.9	=	0.81	
v*(1,1)	=	0.9*0.9*0.9*0.9*0.9	=	0.59		
v*(4,2)	=	-1	

oplmal	value	funclon	v*	
 tr(st, at, st+1) |    , s0 = s#

    e" hxt=0

v    (s) = max

=	sum	of	discounted	rewards	when	starlng	from	state	s	and	aclng	oplmally	

let   s	assume:		
aclons	successful	w/id203	0.8,		gamma	=	0.9,	h	=	100	
v*(4,3)	=	1	
v*(3,3)	=	0.8	*	0.9	+	0.1	*	0.9	*	v*(3,3)	+	0.1	*	0.9	*	v*(3,2)	
v*(2,3)	=		
v*(1,1)	=	
v*(4,2)	=		

value	iteralon	

n   

n   

n   

n   

v    1 (s) = max

v    0 (s)
														=	oplmal	value	for	state	s	when	h=0	
n    		
v    1 (s)
													=	oplmal	value	for	state	s	when	h=1	
n    			
p (s0|s, a)(r(s, a, s0) +  v    0 (s0))
v    2 (s)
												=	oplmal	value	for	state	s	when	h=2			
p (s0|s, a)(r(s, a, s0) +  v    1 (s0))
n    		
v    k (s)
											=	oplmal	value	for	state	s	when	h	=	k	
n    				

v    0 (s) = 0 8s
a xs0
a xs0
a xs0

v    k (s) = max

v    2 (s) = max

p (s0|s, a)(r(s, a, s0) +  v    k 1(s0))

value	iteralon	

algorithm:	

start	with																								for	all	s.	
for	k	=	1,	   	,	h:	
								for	all	states	s	in	s:		

a xs0
p (s0|s, a) r(s, a, s0) +  v    k 1(s0) 
v    k (s)   max
a xs0
p (s0|s, a) r(s, a, s0) +  v    k 1(s0) 
      k(s)   arg max

	
	
												this	is	called	a	value	update	or	bellman	update/back-up	

value	iteralon	

k	=	0	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	0	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	1	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	2	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	3	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	4	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	5	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	6	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	7	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	8	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	9	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	10	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	11	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	12	

noise = 0.2
discount = 0.9

value	iteralon	

k	=	100	

noise = 0.2
discount = 0.9

value	iteralon	convergence	

theorem.			value	itera*on	converges.		at	convergence,	we	have	found	the	
op*mal	value	func*on	v*	for	the	discounted	in   nite	horizon	problem,	which	
sa*s   es	the	bellman	equa*ons	
v    (s) = max

p (s0|s, a) [r(s, a, s0) +  v    (s0)]

					

a xs0

      now	we	know	how	to	act	for	in   nite	horizon	with	discounted	rewards!	

run	value	iteralon	lll	convergence.	
this	produces	v*,	which	in	turn	tells	us	how	to	act,	namely	following:	

      (s) = arg max

p (s0|s, a) [r(s, a, s0) +  v    (s0)]

     

note:	the	in   nite	horizon	oplmal	policy	is	stalonary,	i.e.,	the	oplmal	aclon	at	
a	state	s	is	the	same	aclon	at	all	lmes.		(e   cient	to	store!)	

a xs0

	

     
     

convergence:	intuilon	

n   

n   

v    (s)
															=	expected	sum	of	rewards	accumulated	starlng	from	state	s,	aclng	oplmally	for									steps	
v    h(s)
															=	expected	sum	of	rewards	accumulated	starlng	from	state	s,	aclng	oplmally	for	h	steps	

1

n    addilonal	reward	collected	over	lme	steps	h+1,	h+2,	   		

 h+1r(sh+1) +  h+2r(sh+2) + . . .      h+1rmax +  h+2rmax + . . . =
	goes	to	zero	as	h	goes	to	in   nity	
v    h

h!1    ! v    

			hence							

	

 h+1
1    

rmax

for	simplicity	of	notalon	in	the	above	it	was	assumed	that	rewards	are	always	greater	than	or	equal	to	zero.		if	rewards	can	be	negalve,	a	
similar	argument	holds,	using	max	|r|	and	bounding	from	both	sides.	

convergence	and	contraclons	

n    de   ne	the	max-norm:	

n    theorem:	for	any	two	approximalons	u	and	v	

n   

i.e.,	any	dislnct	approximalons	must	get	closer	to	each	other,	so,	in	parlcular,	any	approximalon	
must	get	closer	to	the	true	u	and	value	iteralon	converges	to	a	unique,	stable,	oplmal	solulon	

n    theorem:	

n   

i.e.	once	the	change	in	our	approximalon	is	small,	it	must	also	be	close	to	correct	

exercise	1:	e   ect	of	discount	and	noise	

(a)	prefer	the	close	exit	(+1),	risking	the	cli   	(-10)	

(1)	  	=	0.1,	noise	=	0.5	

(b)	prefer	the	close	exit	(+1),	but	avoiding	the	cli   	(-10)	

(2)	  	=	0.99,	noise	=	0	

(c)	prefer	the	distant	exit	(+10),	risking	the	cli   	(-10)	

(d)	prefer	the	distant	exit	(+10),	avoiding	the	cli   	(-10)	

(3)	   =	0.99,	noise	=	0.5	

(4)	   =	0.1,	noise	=	0	

	

exercise	1	solulon	

(a)	prefer	close	exit	(+1),	risking	the	cli   	(-10)

	---	

	(4)	  	=	0.1,	noise	=	0	

exercise	1	solulon	

(b)	prefer	close	exit	(+1),	avoiding	the	cli   	(-10)

	---

	(1)		  	=	0.1,	noise	=	0.5	

exercise	1	solulon	

(c)	prefer	distant	exit	(+1),	risking	the	cli   	(-10)

	---

	(2)	  	=	0.99,	noise	=	0	

exercise	1	solulon	

(d)	prefer	distant	exit	(+1),	avoid	the	cli   	(-10)

	---

	(3)	  	=	0.99,	noise	=	0.5	

q-values	

q*(s, a) = expected utility starting in s, taking action a, and (thereafter) 
acting optimally

bellman equation:

q-value iteration:

q   k+1(s, a)  xs0

p (s0|s, a)(r(s, a, s0) +   max

a0

q   k(s0, a0))

q-value	iteralon	
p (s0|s, a)(r(s, a, s0) +   max

a0

q   k+1(s, a)  xs0

k = 100

q   k(s0, a0))

noise = 0.2
discount = 0.9

outline	

n    oplmal	control	

													=		

given	an	mdp	(s, a, p, r,   , h)	
   nd	the	oplmal	policy	  *	

n    exact	methods:	
n    value	itera*on	
n    policy	iteralon	

for	now:	small	discrete	state-aclon	spaces	as	they	are	simpler	to	get	the	main	
concepts	across.		we	will	consider	large	/	conlnuous	spaces	later!	

policy	evalualon	

p (s0|s, a) r(s, a, s0) +  v    k 1(s0) 

n    policy	evalualon	for	a	given												:	

   (s)

p (s0|s,    (s))(r(s,    (s), s0) +  v    

k 1(s))

n    recall	value	iteralon:	

v    

a xs0
v    k (s)   max
k (s)  xs0
8s v    (s)  xs0

	at	convergence:	

p (s0|s,    (s))(r(s,    (s), s0) +  v    (s))

exercise	2	

consider a stochastic policy    (a|s), where    (a|s) is the id203 of taking
action a when in state s. which of the following is the correct update to perform
policy evaluation for this stochastic policy?

1. v    

2. v    

3. v    

k+1(s)   maxaps0 p (s0|s, a) (r(s, a, s0) +  v    
k+1(s)  ps0pa    (a|s)p (s0|s, a) (r(s, a, s0) +  v    
k+1(s)  pa    (a|s) maxs0 p (s0|s, a) (r(s, a, s0) +  v    

k (s0))

k (s0))

k (s0))

policy	iteralon	

one	itera5on	of	policy	itera5on:	
   k
n    policy	evalualon	for	current	policy									:	

n   

iterate	unll	convergence	

v    k

i+1(s)  xs0

p (s0|s,    k(s)) [r(s,    (s), s0) +  v    k

i

(s0)]

n    policy	improvement:	   nd	the	best	aclon	according	to	one-step	

look-ahead	

   k+1(s)   arg max

a xs0

p (s0|s, a) [r(s, a, s0) +  v    k (s0)]

n   

n   

repeat	unll	policy	converges	

at	convergence:	oplmal	policy;	and	converges	faster	than	value	iteralon	under	some	condilons	

policy	evalualon	revisited	

n   

idea	1:	modify	bellman	updates	

v    

i+1(s)  xs0

p (s0|s,    (s)) [r(s,    (s), s0) +  v    

i (s0)]

n   

idea	2:	it	is	just	a	linear	system,	solve	with	numpy	(or	whatever)	

	variables:	v  (s)	
	constants:	p,	r	

8s v    (s) =xs0

p (s0|s,    (s)) [r(s,    (s), s0) +  v    (s0)]

policy	iteralon	guarantees	

policy	iteralon	iterates	over:	

n    policy	evalualon	for	current	policy								:	

   k

n   

iterate	unll	convergence	

p (s0|s,    k(s)) [r(s,    (s), s0) +  v    k

i

(s0)]

n    policy	improvement:	   nd	the	best	aclon	according	to	

v    k

i+1(s)  xs0

one-step	look-ahead	
   k+1(s)   arg max

a xs0

p (s0|s, a) [r(s, a, s0) +  v    k (s0)]

theorem.		policy	iteralon	is	guaranteed	to	converge	and	at	convergence,	the	current	policy	
and	its	value	funclon	are	the	oplmal	policy	and	the	oplmal	value	funclon!	

proof	sketch:			
(1)    guarantee	to	converge:	in	every	step	the	policy	improves.		this	means	that	a	given	policy	can	be	encountered	

at	most	once.		this	means	that	awer	we	have	iterated	as	many	lmes	as	there	are	di   erent	policies,	i.e.,	
(number	aclons)(number	states),	we	must	be	done	and	hence	have	converged.	
(2)    op*mal	at	convergence:	by	de   nilon	of	convergence,	at	convergence	  k+1(s)	=	  k(s)	for	all	states	s.		this	

means	

	

	

	

	

		

							hence						sals   es	the	bellman	equalon,	which	means							is	equal	to	the	oplmal	value	funclon	v*.	

outline	

n    oplmal	control	

													=		

given	an	mdp	(s, a, p, r,   , h)	
   nd	the	oplmal	policy	  *	

n    exact	methods:	
n    value	itera*on	
n    policy	itera*on	

limita*ons:		

      

iteralon	over	/	storage	for	all	states	and	aclons:	requires	small,	
discrete	state-aclon	space	

       update	equalons	require	access	to	dynamics	model	

