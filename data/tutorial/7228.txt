lecture	2:	sampling-based	approximations

and

function	fitting

yan	(rocky)	duan

berkeley	ai	research	lab

many	slides	made	with	john	schulman,	xi	(peter)	chen	and	pieter	abbeel

quick	one-slide	recap

n optimal	control

=	

given	an	mdp	(s, a, p, r,   , h)
find	the	optimal	policy	  *

limitations:	

n exact	methods:
n value	iteration
n policy	iteration

   

   

update	equations	require	access	to	dynamics	
model

->	sampling-based	approximations

iteration	over	/	storage	for	all	states	and	actions:	
requires	small,	discrete	state-action	space

->	q/v	function	fitting

sampling-based	approximation

n q	value	iteration
n value	iteration?
n policy	iteration

n policy	evaluation
n policy	improvement?

recap	q-values

q*(s, a) = expected utility starting in s, taking action a, and (thereafter) 
acting optimally

bellman equation:

q-value iteration:

(tabular)	id24

n q-value	iteration:

n rewrite	as	expectation:	

qk+1   es0   p (s0|s,a)hr(s, a, s0) +   max

a0

qk(s0, a0)i

n (tabular)	id24:	replace	expectation	by	samples
s0     p (s0|s, a)

n for	an	state-action	pair	(s,a),	receive:
n consider	your	old	estimate:
qk(s, a)
n consider	your	new	sample	estimate:

n

incorporate	the	new	estimate	into	a	running	average:
qk+1(s, a)   (1      )qk(s, a) +     [target(s0)]

(tabular)	id24

algorithm:

q0(s, a)

start	with	
get	initial	state	s
for	k =	1,	2,	   	till	convergence

for	all	s,	a.

sample	action	a,	get	next	state	s   
if	s   	is	terminal:

target = r(s, a, s0)
sample	new	initial	state	s   

else:

target = r(s, a, s0) +   max
a0

qk(s0, a0)
qk+1(s, a)   (1      )qk(s, a) +     [target]
s   s0

how	to	sample	actions?

n choose random actions?

n choose action that maximizes

qk(s, a)

(i.e.	greedily)?

n

  -greedy:	choose	random	action	with	prob.	  ,	otherwise	choose	
action	greedily

id24	properties

n amazing	result:	id24	converges	to	optimal	policy	--

even	if	you   re	acting	suboptimally!

n this	is	called	off-policy	learning

n caveats:

n you	have	to	explore	enough
n you	have	to	eventually	make	the	learning	rate

small	enough

n    	but	not	decrease	it	too	quickly

id24	properties

n technical	requirements.	

n all	states	and	actions	are	visited	infinitely	often

n basically,	in	the	limit,	it	doesn   t	matter	how	you	select	actions	(!)
n learning	rate	schedule	such	that	for	all	state	and	action	

pairs	(s,a):

1xt=0

   t(s, a) = 1

1xt=0

   2
t (s, a) < 1

for details, see tommi jaakkola, michael i. jordan, and satinder p. singh. on the convergence of stochastic iterative 
id145 algorithms. neural computation, 6(6), november 1994.

id24	demo:	gridworld

states:	11	cells

   
    actions:	{up,	down,	left,	right}
    deterministic	transition	function
   
    discount:	1
    reward:	+1	for	getting	diamond,	-1	for	falling	into	trap

learning	rate:	0.5

id24	demo:	crawler

states:	discretized	value	of	2d	state:	(arm	angle,	hand	angle)

   
    actions:	cartesian	product	of	{arm	up,	arm	down}	and	{hand	up,	hand	down}
    reward:	speed	in	the	forward	direction

sampling-based	approximation

n q	value	iteration	   (tabular)	id24
n value	iteration?
n policy	iteration

n policy	evaluation
n policy	improvement?

value	iteration	w/	samples?

n value	iteration

v    i+1(s)   max

a es0   p (s0|s,a) [r(s, a, s0) +  v    i (s0)]

n unclear	how	to	draw	samples	through	max   ...

sampling-based	approximation

n q	value	iteration	   (tabular)	id24
n value	iteration?
n policy	iteration

n policy	evaluation
n policy	improvement?

recap:	policy	iteration

one	iteration	of	policy	iteration:
   k
n policy	evaluation	for	current	policy									:

n

iterate	until	convergence
v    k
i+1(s)   es0   p (s0|s,   k(s))[r(s,    k(s), s0) +  v    k

i

(s0)]

n

can	be	approximated	by	samples
this	is	called	temporal	difference	(td)	learning
policy	improvement:	find	the	best	action	according	to	one-step	
look-ahead
   k+1(s)   arg max

a

es0   p (s0|s,a)[r(s, a, s0) +  v    k (s0)]

unclear	what	to	do	with	the	max	(for	now)

sampling-based	approximation

n q	value	iteration	   (tabular)	id24
n value	iteration?
n policy	iteration

n policy	evaluation	   (tabular)	td-learning
n policy	improvement	(for	now)

quick	one-slide	recap

n optimal	control

=	

given	an	mdp	(s, a, p, r,   , h)
find	the	optimal	policy	  *

limitations:	

n exact	methods:
n value	iteration
n policy	iteration

   

   

update	equations	require	access	to	dynamics	
model

->	sampling-based	approximations

iteration	over	/	storage	for	all	states	and	actions:	
requires	small,	discrete	state-action	space

->	q/v	function	fitting

can	tabular	methods	scale?

n discrete	environments

gridworld

10^1

tetris
10^60

atari

10^308 (ram)   10^16992 (pixels)

can	tabular	methods	scale?
n continuous	environments	(by	crude	discretization)

crawler
10^2

hopper
10^10

humanoid
10^100

generalizing	across	states

n basic	id24	keeps	a	table	of	all	q-values

n

in	realistic	situations,	we	cannot	possibly	learn	
about	every	single	state!

n

n

too	many	states	to	visit	them	all	in	training
too	many	states	to	hold	the	q-tables	in	memory

n

instead,	we	want	to	generalize:

n

learn	about	some	small	number	of	training	states	from	
experience

n generalize	that	experience	to	new,	similar	situations
this	is	a	fundamental	idea	in	machine	learning,	and	
we   ll	see	it	over	and	over	again

n

n

n

approximate	id24

instead	of	a	table,	we	have	a	parametrized	q	function:
n can	be	a	linear	function	in	features:	

q   (s, a)

q   (s, a) =    0f0(s, a) +    1f1(s, a) +        +    nfn(s, a)

n or	a	complicated	neural	net

learning	rule:
n remember:	
n update:

target(s0) = r(s, a, s0) +   max
a0

q   k (s0, a0)

   k+1      k      r       1

2

(q   (s, a)   target(s0))2        =   k

connection	to	tabular	id24

n suppose	

    2 r|s|   |a|, q   (s, a)        sa

2

2

(q   (s, a)   target(s0))2 
(   sa   target(s0))2 

r   sa    1
= r   sa    1
=    sa   target(s0)
   sa      sa      (   sa   target(s0))
= (1      )   sa +    [target(s0)]

n plug	into	update:

n compare	with	tabular	id24	update:

qk+1(s, a)   (1      )qk(s, a) +     [target(s0)]

engineered	approximation	example:	tetris

n

n

n

state:	na  ve	board	configuration	+	shape	of	the	falling	piece	~1060 states!

action:	rotation	and	translation	applied	to	the	falling	piece

22	features	aka	basis	functions	

 i

n

n

n

n

n

ten	basis	functions,	0,	.	.	.	,	9,	mapping	the	state	to	the	height	h[k]	of	each	column.

nine	basis	functions,	10,	.	.	.	,	18,	each	mapping	the	state	to	the	absolute	difference	
between	heights	of	successive	columns:	|h[k+1]	   	h[k]|,	k	=	1,	.	.	.	,	9.

one	basis	function,	19,	that	maps	state	to	the	maximum	column	height:	maxk h[k]
one	basis	function,	20,	that	maps	state	to	the	number	of	   holes   	in	the	board.

one	basis	function,	21,	that	is	equal	to	1	in	every	state.

  v (s) =

 i   i(s) =  >   (s)

21xi=0

[bertsekas &	ioffe,	1996	(td);	bertsekas &	tsitsiklis 1996	(td);	kakade 2002	(policy	gradient);	farias &	van	roy,	2006	(approximate	lp)]

deep	reinforcement	learning

pong

enduro

beamrider

q*bert

    from	pixels	to	actions
    same	algorithm	(with	effective	tricks)
    id98	function	approximator,	w/	3m	free	parameters

lab	1

n we	have	now	covered	enough	materials	for	lab	1.
n will	be	released	on	piazza	by	this	afternoon.
n covers	value	iteration,	policy	iteration,	and	tabular	id24.

convergence	of	approximate	id24

n the	bad:	it	is	not	guaranteed	to	converge   

n even	if	the	function	approximation	is	expressive	enough	to	

represent	the	true	q	function

r=0

x2

2  

r=0

x1

  

function	approximator:		[1	2]	*	  

simple	example**

composing	operators**

n definition.		an	operator	g	is	a	non-expansion with	respect	to	a	norm	||	.	||		if

n fact. if	the	operator	f	is	a	  -contraction	with	respect	to	a	norm	||	.	||	and	the	

operator	g	is	a	non-expansion	with	respect	to	the	same	norm,	then	the	
sequential	application	of	the	operators	g	and	f	is	a	  -contraction,	i.e.,	

n corollary. if	the	supervised	learning	step	is	a	non-expansion,	then	iteration	in	
value	iteration	with	function	approximation	is	a	  -contraction,	and	in	this	case	
we	have	a	convergence	guarantee.

averager function	approximators are	non-expansions**

n examples:	

n nearest	neighbor	(aka	state	aggregation)
n linear	interpolation	over	triangles	

(tetrahedrons,	   )

averager function	approximators are	non-expansions**

linear	regression	l **

example	taken	from	gordon,	1995

guarantees	for	fixed	point**

n

i.e.,	if	we	pick	a	non-expansion	function	approximator which	can	approximate	
j*	well,	then	we	obtain	a	good	value	function	estimate.

n to	apply	to	discretization:	use	continuity	assumptions	to	show	that	j*	can	be	

approximated	well	by	chosen	discretization	scheme.

