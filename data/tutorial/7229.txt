deep q-networks

volodymyr mnih

deep rl bootcamp, berkeley

recap: id24

   

learning a parametric q function:

    remember: 

    update:

    for tabular function,                               , we recover the familiar update:

    converges to optimal values (*)

    does it work with a neural network q functions?

    yes but with some care

recap: (tabular) id24

recap: approximate id24

id25
    high-level idea - make id24 look like supervised learning.
    two main ideas for stabilizing id24.
    apply q-updates on batches of past experience instead of online:

    experience replay (lin, 1993).
    previously used for better data efficiency.
    makes the data distribution more stationary.

    use an older set of weights to compute the targets (target network):

    keeps the target function from changing too quickly.

   human-level control through deep id23   , mnih, kavukcuoglu, silver et al. (2015)

target network intuition

    changing the value of one action will 

change the value of other actions 
and similar states.

    the network can end up chasing its 
own tail because of id64.
    somewhat surprising fact - bigger 

networks are less prone to this 
because they alias less.

s

s   

id25 training algorithm

id25 details

    uses huber loss instead of squared loss on bellman error:

    uses rmsprop instead of vanilla sgd.

    optimization in rl really matters.

    it helps to anneal the exploration rate.

    start    at 1 and anneal it to 0.1 or 0.05 over the first million frames.

id25 on atari

pong

enduro

beamrider

q*bert

    49 atari 2600 games.
    from pixels to actions.
    the change in score is the reward.
    same algorithm.
    same function approximator, w/ 3m free parameters.
    same hyperparameters.
    roughly human-level performance on 29 out of 49 games.

atari network architecture

    convolutional neural network architecture:

    history of frames as input.
    one output per action - expected reward for that action q(s, a).
    final results used a slightly bigger network (3 convolutional + 1 fully-connected hidden layers).

16 8x8 filters

4x84x84

32 4x4 filters

256 hidden units

fully-connected linear 

output layer

stack of 4 previous

 frames

convolutional layer
 of rectified linear units

convolutional layer
 of rectified linear units

fully-connected layer
 of rectified linear units

stability techniques

atari results

   human-level control through deep id23   , mnih, kavukcuoglu, silver et al. (2015)

id25 playing atari

action values on pong

learned value functions

sacrificing immediate rewards

id25 source code

    the id25 source code (in lua+torch) is available:

https://sites.google.com/a/deepmind.com/id25/

neural fitted q iteration
    nfq (riedmiller, 2005) trains neural networks with id24.
    alternates between collecting new data and fitting a new q-function to all previous 

experience with batch id119.

    id25 can be seen as an online variant of nfq.

lin   s networks

    long-ji lin   s thesis    id23 for robots using neural 

networks    (1993) also trained neural nets with id24.

    introduced experience replay among other things.
    lin   s networks did not share parameters among actions.

lin   s architecture

q(a1,s)

q(a2,s)

q(a3,s)

id25
q(a1,s) q(a2,s)

q(a3,s)

double id25

    there is an upward bias in maxa q(s, a;   ).
    id25 maintains two sets of weight    and   -, so reduce bias by using:

       for selecting the best action.
      - for evaluating the best action.

    double id25 loss:

   double id23 with double id24   , van hasselt et al. (2016)

prioritized experience replay
    replaying all transitions with equal id203 is highly suboptimal.
    replay transitions in proportion to absolute bellman error:

    leads to much faster learning.

   prioritized experience replay   , schaul et al. (2016)

dueling id25

    value-advantage decomposition of q:

    dueling id25 (wang et al., 2015):

id25

dueling
id25

q(s,a)

v(s)

q(s,a)

a(s,a)

atari results

   dueling network architectures for deep id23   , wang et al. (2016)

noisy nets for exploration

    add noise to network parameters for better exploration [fortunato, azar, piot et al. (2017)].
    standard linear layer:
    noisy linear layer:
      w and   b contain noise.
      w and   b are learned parameters that determine the amount of noise.

   noisy nets for exploration   , fortunato, azar, piot et al. (2017)
also see    parameter space noise for exploration   , plappert et al. (2017)

questions?

