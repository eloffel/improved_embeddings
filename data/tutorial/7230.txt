policy	gradients	

	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

slides	authored	with	john	schulman	and	xi	(peter)	chen	

	

schedule	--	saturday	

8:30:	breakfast	/	check-in	
9-10:	core	lecture	1	intro	to	mdps	and	exact	soluqon	methods	(pieter	abbeel)	
10-10:10	brief	sponsor	intros	
10:10-11:10	core	lecture	2	sample-based	approximaqons	and	fited	learning	(rocky	duan)	
11:10-11:30:	co   ee	break	
11:30-12:30	core	lecture	3	id25	+	variants	(vlad	mnih)	
12:30-1:30	lunch	[catered]	
1:30-2:15	core	lecture	4a	policy	gradients	and	actor	cri<c	(pieter	abbeel)	
2:15-3:00	core	lecture	4b	pong	from	pixels	(andrej	karpathy)	
3:00-3:30	core	lecture	5	natural	policy	gradients,	trpo,	and	ppo	(john	schulman)	
3:30-3:50	co   ee	break	
3:50-4:30	core	lecture	6	nuts	and	bolts	of	deep	rl	experimentaqon		(john	schulman)	
4:30-7	labs	1-2-3			
7-8	fronqers	lecture	i:	recent	advances,	fronqers	and	future	of	deep	rl	(vlad	mnih)	

	

reinforcement	learning	

ut

[figure	source:	suton	&	barto,	1998]	

policy	opqmizaqon	in	the	rl	landscape	

policy	opqmizaqon	

      (u|s)

ut

[figure	source:	suton	&	barto,	1998]	

policy	opqmizaqon	

n    consider	control	policy	parameterized	

by	parameter	vector	

   

max

   

e[

hxt=0

r(st)|      ]

	

n    stochasqc	policy	class	(smooths	out	

the	problem):	
																				:	id203	of	acqon	u	in	state	s		
      (u|s)

      (u|s)

ut

[figure	source:	suton	&	barto,	1998]	

why	policy	opqmizaqon	

n    oien						can	be	simpler	than	q	or	v	

   

n    e.g.,	roboqc	grasp	

n    v:	doesn   t	prescribe	acqons	

n    would	need	dynamics	model	(+	compute	1	bellman	back-up)	

n    q:	need	to	be	able	to	e   ciently	solve	

arg max

q   (s, u)

u

n    challenge	for	conqnuous	/	high-dimensional	acqon	spaces*	

*some	recent	work	(parqally)	addressing	this:		

	naf:	gu,	lillicrap,	sutskever,	levine	icml	2016	
	input	convex	nns:	amos,	xu,	kolter	arxiv	2016	
	deep	energy	q:	haarnoja,	tang,	abbeel,	levine,	icml	2017		

policy	opqmizaqon					dynamic	programming	

n    conceptually:	

optimize what you care 
about 

indirect, exploit the problem 
structure, self-consistency 

n    empirically:	

more compatible with rich 
architectures (including 
recurrence)  
 
more versatile 
 
more compatible with 
auxiliary objectives 

more compatible with 
exploration and off-policy 
learning 
 
more sample-efficient when 
they work 
 
 

likelihood	raqo	policy	gradient	

likelihood	raqo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartlet,	2001]	

likelihood	raqo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartlet,	2001]	

likelihood	raqo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartlet,	2001]	

likelihood	raqo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartlet,	2001]	

likelihood	raqo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartlet,	2001]	

likelihood	raqo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartlet,	2001]	

likelihood	raqo	gradient:	validity	

ru (   )       g =

1
m

mxi=1

r    log p (    (i);    )r(    (i))

n    valid	even	when		

n    r	is	disconqnuous	and/or	unknown	
n    sample	space	(of	paths)	is	a	discrete	set		

likelihood	raqo	gradient:	intuiqon	

ru (   )       g =
n    gradient	tries	to:	

1
m

mxi=1

r    log p (    (i);    )r(    (i))

n   

increase	id203	of	paths	with	
posiqve	r	

n    decrease	id203	of	paths	with	

negaqve	r	

!	likelihood	raqo	changes	probabiliqes	of	experienced	paths,	
does	not	try	to	change	the	paths	(<->	path	derivaqve)	

let   s	decompose	path	into	states	and	acqons	

let   s	decompose	path	into	states	and	acqons	

let   s	decompose	path	into	states	and	acqons	

let   s	decompose	path	into	states	and	acqons	

likelihood	raqo	gradient	esqmate	

derivaqon	from	importance	sampling	

p (   |   old)

r(    ) 
u (   ) = e         old    p (   |   )
r(    ) 
r   u (   ) = e         old   r   p (   |   )
= e         old   r    p (   |   )|   old
r(    ) 
r(    )   
= e         old   r    log p (   |   )|   old

p (   |   old)

p (   |   old)

r    u (   )|   =   old

note:	suggests	we	can	also	look	at	more	than	just	gradient!	

[tang&abbeel,	nips	2011]	

derivaqon	from	importance	sampling	

p (   |   old)

r(    ) 
u (   ) = e         old    p (   |   )
r(    ) 
r   u (   ) = e         old   r   p (   |   )
= e         old   r    p (   |   )|   old
r(    ) 
r(    )   
= e         old   r    log p (   |   )|   old

p (   |   old)

p (   |   old)

r    u (   )|   =   old

note:	suggests	we	can	also	look	at	more	than	just	gradient!	

[tang&abbeel,	nips	2011]	

derivaqon	from	importance	sampling	

p (   |   old)

r(    ) 
u (   ) = e         old    p (   |   )
r(    ) 
r   u (   ) = e         old   r   p (   |   )
= e         old   r    p (   |   )|   old
r(    ) 
r(    )   
= e         old   r    log p (   |   )|   old

p (   |   old)

p (   |   old)

r    u (   )|   =   old

note:	suggests	we	can	also	look	at	more	than	just	gradient!	

[tang&abbeel,	nips	2011]	

derivaqon	from	importance	sampling	

p (   |   old)

r(    ) 
u (   ) = e         old    p (   |   )
r(    ) 
r   u (   ) = e         old   r   p (   |   )
= e         old   r    p (   |   )|   old
r(    ) 
r(    )   
= e         old   r    log p (   |   )|   old

p (   |   old)

p (   |   old)

r    u (   )|   =   old

note:	suggests	we	can	also	look	at	more	than	just	gradient!	

[tang&abbeel,	nips	2011]	

derivaqon	from	importance	sampling	

p (   |   old)

r(    ) 
u (   ) = e         old    p (   |   )
r(    ) 
r   u (   ) = e         old   r   p (   |   )
= e         old   r    p (   |   )|   old
r(    ) 
r(    )   
= e         old   r    log p (   |   )|   old

p (   |   old)

p (   |   old)

suggests	we	can	also	look	at	more	than	just	gradient!	

r    u (   )|   =   old

[tang&abbeel,	nips	2011]	

e.g.,	can	use	importance	sampled	objecqve	as	   surrogate	loss   	(locally)	

likelihood	raqo	gradient	esqmate	

n    as	formulated	thus	far:	unbiased	but	very	noisy	
n    fixes	that	lead	to	real-world	pracqcality	

n    baseline	
n    temporal	structure	

n    also:	kl-divergence	trust	region	/	natural	gradient	(=	general	trick,	
equally	applicable	to	perturbaqon	analysis	and	   nite	di   erences)	

likelihood	raqo	gradient:	intuiqon	

ru (   )       g =
n    gradient	tries	to:	

1
m

mxi=1

r    log p (    (i);    )r(    (i))

n   

increase	id203	of	paths	with	
posiqve	r	

n    decrease	id203	of	paths	with	

negaqve	r	

!	likelihood	raqo	changes	probabiliqes	of	experienced	paths,	
does	not	try	to	change	the	paths	(<->	path	derivaqve)	

likelihood	raqo	gradient:	baseline	

ru (   )       g =

1
m

mxi=1

      consider	baseline	b:	

ru (   )       g =

r    log p (    (i);    )r(    (i))
mxi=1
1
m

r    log p (    (i);    )(r(    (i))   b)

sqll	unbiased!	

[williams	1992]	

ok	as	long	as	baseline	doesn   t	depend	on	acqon	
in	logprob(acqon)	
p (    )) = b     0

= br   (x   

p (    ;    )r    log p (    ;    )b
p (    ;    )r   p (    ;    )
p (    ;    )

b

							
e [r    log p (    ;    )b]
=x   
=x   
=x    r   p (    ;    )b
=r    x   
p (    )b!

=r    (b)

likelihood	raqo	and	temporal	structure	
	current	esqmate:	

  g =

r    log p (    (i);    )(r(    (i))   b)

n   

1
m

1
m

mxi=1
mxi=1 h 1xt=0
mxi=1  h 1xt=0

=

1
m

r    log       (u(i)

t

r(s(i)
t

|s(i)

t )! h 1xt=0
t )"  t 1xk=0

|s(i)

, u(i)

t )   b!
k )  +  h 1xk=t

r    log       (u(i)

t

r(s(i)

k , u(i)

r(s(i)

k , u(i)

k )    b#!

	

	

=

doesn   t	depend	on		u(i)
t

ok	to	depend	on		s(i)
t

n    removing	terms	that	don   t	depend	on	current	acqon	can	lower	variance:	

1
m

mxi=1

h 1xt=0

r    log       (u(i)

t

t ) h 1xk=t

|s(i)

r(s(i)

k , u(i)

t )!
k )   b(s(i)

[policy	gradient	theorem:	suton	et	al,	nips	1999;	gpomdp:	bartlet	&	baxter,	jair	2001;	survey:	peters	&	schaal,	iros	2006]		

baseline	choices	

n    good	choice	for	b?		

1
m

mxi=1

r(    (i))

b = e [r(    )]    

n    constant	baseline:	
n    opqmal	constant	baseline:	
mxi=1
n    time-dependent	baseline:		
n    state-dependent	expected	return:		
	
  	increase	logprob	of	acqon	proporqonally	to	how	much	its	returns	are	
beter	than	the	expected	return	under	the	current	policy	

b(st) = e [rt + rt+1 + rt+2 + . . . + rh 1]

= v    (st)

h 1xk=t

k , u(i)
k )

r(s(i)

1
m

bt =

[see:	greensmith,	bartlet,	baxter,	jmlr	2004	for	variance	reducqon	techniques.]		

esqmaqon	of						

v    

r(s(i)

k , u(i)

k )!
k )   v    (s(i)

1
m

mxi=1

h 1xt=0

r    log       (u(i)

t

t ) h 1xk=t

|s(i)

n   

v    
 0

init		
n    collect	trajectories		
n    regress	against	empirical	return:	

   1, . . . ,    m

 i+1   arg min

 

1
m

mxi=1

h 1xt=0 v    

    (s(i)

t )    h 1xk=t

how to estimate? 

r(s(i)

k , u(i)

k ) !2

esqmaqon	of						

v    

n    bellman	equaqon	for		

v    

v    (s) =xu

   (u|s)xs0

p (s0|s, u)[r(s, u, s0) +  v    (s0)]

n   

v    
 0

init		
n    collect	data	{s,	u,	s   ,	r}	
n    fited	v	iteraqon:	
 i+1   min

  x(s,u,s0,r)

kr + v    

 i(s0)   v (s)k2

2 +  k     ik2

2

vanilla	policy	gradient	

~	[williams,	1992]	

recall	our	likelihood	raqo	pg	esqmator	

1
m

mxi=1

h 1xt=0

r    log       (u(i)

t

t ) h 1xk=t

|s(i)

r(s(i)

k , u(i)

k )!
k )   v    (s(i)

n    esqmaqon	of	q	from	single	roll-out	

q   (s, u) = e[r0 + r1 + r2 +       |s0 = s, a0 = a]
n    =	high	variance	per	sample	based	/	no	generalizaqon	used	

n    reduce	variance	by	discounqng	
n    reduce	variance	by	funcqon	approximaqon	(=criqc)				

recall	our	likelihood	raqo	pg	esqmator	

1
m

mxi=1

h 1xt=0

r    log       (u(i)

t

t ) h 1xk=t

|s(i)

r(s(i)

k , u(i)

k )!
k )   v    (s(i)

n    esqmaqon	of	q	from	single	roll-out	

q   (s, u) = e[r0 + r1 + r2 +       |s0 = s, a0 = a]
n    =	high	variance	per	sample	based	/	no	generalizaqon	used	

n    reduce	variance	by	discounqng	
n    reduce	variance	by	funcqon	approximaqon	(=criqc)				

recall	our	likelihood	raqo	pg	esqmator	

1
m

mxi=1

h 1xt=0

r    log       (u(i)

t

t ) h 1xk=t

|s(i)

r(s(i)

k , u(i)

k )!
k )   v    (s(i)

n    esqmaqon	of	q	from	single	roll-out	

q   (s, u) = e[r0 + r1 + r2 +       |s0 = s, a0 = a]
n    =	high	variance	per	sample	based	/	no	generalizaqon	used	

n    reduce	variance	by	discounqng	
n    reduce	variance	by	funcqon	approximaqon	(=criqc)				

recall	our	likelihood	raqo	pg	esqmator	

1
m

mxi=1

h 1xt=0

r    log       (u(i)

t

t ) h 1xk=t

|s(i)

r(s(i)

k , u(i)

k )!
k )   v    (s(i)

n    esqmaqon	of	q	from	single	roll-out	

q   (s, u) = e[r0 + r1 + r2 +       |s0 = s, a0 = a]
n    =	high	variance	per	sample	based	/	no	generalizaqon	

n    reduce	variance	by	discounqng	
n    reduce	variance	by	funcqon	approximaqon	(=criqc)				

further	re   nements	
h 1xt=0

t ) h 1xk=t

r    log       (u(i)

k , u(i)

r(s(i)

|s(i)

t

k )!
k )   v    (s(i)

1
m

mxi=1

n    esqmaqon	of	q	from	single	roll-out	

q   (s, u) = e[r0 + r1 + r2 +       |s0 = s, a0 = a]
n    =	high	variance	per	sample	based	/	no	generalizaqon	

n    reduce	variance	by	discounqng	
n    reduce	variance	by	funcqon	approximaqon	(=criqc)				

recall	our	likelihood	raqo	pg	esqmator	

1
m

mxi=1

h 1xt=0

r    log       (u(i)

t

t ) h 1xk=t

|s(i)

r(s(i)

k , u(i)

k )!
k )   v    (s(i)

n    esqmaqon	of	q	from	single	roll-out	

q   (s, u) = e[r0 + r1 + r2 +       |s0 = s, a0 = a]
n    =	high	variance	per	sample	based	/	no	generalizaqon	

n    reduce	variance	by	discounqng	
n    reduce	variance	by	funcqon	approximaqon	(=criqc)				

variance	reducqon	by	discounqng	

q   (s, u) = e[r0 + r1 + r2 +       |s0 = s, a0 = a]

  	introduce	discount	factor	as	a	hyperparameter	to	improve	
esqmate	of	q:	

q   , (s, u) = e[r0 +  r1 +  2r2 +       |s0 = s, a0 = a]

reducing	variance	by	funcqon	approximaqon	

q   , (s, u) = e[r0 +  r1 +  2r2 +        | s0 = s, u0 = u]

= e[r0 +  v    (s1) | s0 = s, u0 = u]
= e[r0 +  r1 +  2v    (s2) | s0 = s, u0 = u]
= e[r0 +  r1 + + 2r2 +  3v    (s3) | s0 = s, u0 = u]
=       

n    generalized	advantage	es<ma<on	uses	an	exponenqally	

weighted	average	of	these	

n    ~	td(lambda)	

reducing	variance	by	funcqon	approximaqon	

q   , (s, u) = e[r0 +  r1 +  2r2 +        | s0 = s, u0 = u]

= e[r0 +  v    (s1) | s0 = s, u0 = u]
= e[r0 +  r1 +  2v    (s2) | s0 = s, u0 = u]
= e[r0 +  r1 + + 2r2 +  3v    (s3) | s0 = s, u0 = u]
=       

n    generalized	advantage	es<ma<on	uses	an	exponenqally	

weighted	average	of	these	

n    ~	td(lambda)	

reducing	variance	by	funcqon	approximaqon	

q   , (s, u) = e[r0 +  r1 +  2r2 +        | s0 = s, u0 = u]

= e[r0 +  v    (s1) | s0 = s, u0 = u]
= e[r0 +  r1 +  2v    (s2) | s0 = s, u0 = u]
= e[r0 +  r1 + + 2r2 +  3v    (s3) | s0 = s, u0 = u]
=       

n    generalized	advantage	es<ma<on	uses	an	exponenqally	

weighted	average	of	these	

n    ~	td(lambda)	

reducing	variance	by	funcqon	approximaqon	

q   , (s, u) = e[r0 +  r1 +  2r2 +        | s0 = s, u0 = u]

= e[r0 +  v    (s1) | s0 = s, u0 = u]
= e[r0 +  r1 +  2v    (s2) | s0 = s, u0 = u]
= e[r0 +  r1 + + 2r2 +  3v    (s3) | s0 = s, u0 = u]
=       

n    async	advantage	actor	cri<c	(a3c)	[mnih	et	al,	2016]	

  q
												one	of	the	above	choices	(e.g.		k=5	step	lookahead)	

n   

reducing	variance	by	funcqon	approximaqon	

q   , (s, u) = e[r0 +  r1 +  2r2 +        | s0 = s, u0 = u]

= e[r0 +  v    (s1) | s0 = s, u0 = u]
= e[r0 +  r1 +  2v    (s2) | s0 = s, u0 = u]
= e[r0 +  r1 + + 2r2 +  3v    (s3) | s0 = s, u0 = u]
=       

(1    )
(1    ) 
(1    ) 2

(1    ) 3

n    generalized	advantage	es<ma<on	(gae)	[schulman	et	al,	iclr	2016]	

  q
							=	lambda	exponenqally	weighted	average	of	all	the	above	

n   

n    ~	td(lambda)	/	eligibility	traces	[suton	and	barto,	1990]	

reducing	variance	by	funcqon	approximaqon	

q   , (s, u) = e[r0 +  r1 +  2r2 +        | s0 = s, u0 = u]

= e[r0 +  v    (s1) | s0 = s, u0 = u]
= e[r0 +  r1 +  2v    (s2) | s0 = s, u0 = u]
= e[r0 +  r1 + + 2r2 +  3v    (s3) | s0 = s, u0 = u]
=       

(1    )
(1    ) 
(1    ) 2

(1    ) 3

n    generalized	advantage	es<ma<on	(gae)	[schulman	et	al,	iclr	2016]	

  q
							=	lambda	exponenqally	weighted	average	of	all	the	above	

n   

n    ~	td(lambda)	/	eligibility	traces	[suton	and	barto,	1990]	

actor-criqc	with	a3c	or	gae	

n    policy	gradient	+	generalized	advantage	esqmaqon:	

n   

init		

      0

v    
 0

n    collect	roll-outs	{s,	u,	s   ,	r}	and		
n    update:			

 i+1   min

  qi(s, u)
  x(s,u,s0,r)
k   qi(s, u)   v    
h 1xt=0
mxk=1

r    log       i(u(k)

1
m

t

  (s)k2

2

2 +    k     ik2
)      qi(s(k)

, u(k)

t

t

|s(k)

t

)   v    

 i(s(k)

t

)   

   i+1      i +    

note:	many	variaqons,	e.g.	could	instead	use	1-step	for	v,	full	roll-out	for	pi:	

 i+1   min

   i+1      i +    

  x(s,u,s0,r)
kr + v    
h 1xt=0
mxk=1

1
m

 i(s0)   v (s)k2
2 +  k     ik2
t0 )!
) h 1xt0=t

r(k)
t0   v    

 i(s(k)

|s(k)

2

t

r    log       i(u(k)

t

async	advantage	actor	criqc	(a3c)	

n    [mnih	et	al,	icml	2016]	

n    likelihood	raqo	policy	gradient	
n    n-step	advantage	esqmaqon	

a3c	--	labyrinth	

gae:	e   ect	of	gamma	and	lambda	

[schulman	et	al,	2016	--	gae]	

learning	locomoqon	(trpo	+	gae)	

[schulman,	moritz,	levine,	jordan,	abbeel,	2016]	

in	contrast:	darpa	roboqcs	challenge	

