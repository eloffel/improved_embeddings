pong from pixels

deep rl bootcamp

andrej karpathy, aug 26, 2017

mdp

policy network

policy network

e.g.,

height width
[80 x 80]
array of

policy network

height width
[80 x 80]
array

policy network

height width
[80 x 80]
array

e.g. 200 nodes in the hidden network, so:

[(80*80)*200 + 200] + [200*1 + 1] = ~1.3m parameters

layer 1

layer 2

network does not see this. network sees 80*80 = 6,400 numbers.
it gets a reward of +1 or -1, some of the time.
q: how do we efficiently find a good setting of the 1.3m parameters?

problem is easy if you want to be inefficient...

1. repeat forever:
2.
3.
4.
5. return the best policy

    sample 1.3m random numbers
    run the policy for a while
    if the performance is best so far, save it

problem is easy if you want to be inefficient...

1. repeat forever:
2.
3.
4.
5. return the best policy

    sample 1.3m random numbers
    run the policy for a while
    if the performance is best so far, save it

problem is easy if you want to be inefficient...

1. repeat forever:
2.
3.
4.
5. return the best policy

    sample 1.3m random numbers
    run the policy for a while
    if the performance is best so far, save it

policy gradients

suppose we had the training labels    
(we know what to do in any state)

(x1,up)
(x2,down)
(x3,up)
...

suppose we had the training labels    
(we know what to do in any state)

(x1,up)
(x2,down)
(x3,up)
...

suppose we had the training labels    
(we know what to do in any state)

(x1,up)
(x2,down)
(x3,up)
...

maximize:

except, we don   t have labels...

should we go up or down?

except, we don   t have labels...

   try a bunch of stuff and see 
what happens. do more of the 
stuff that worked in the future.   

-rl

let   s just act according to our current policy...

rollout the policy 
and collect an 
episode

collect many rollouts...

4 rollouts:

not sure whatever we did here, but 
apparently it was good.

not sure whatever we did here, but it was bad.

pretend every action we took here 
was the correct label.

pretend every action we took 
here was the wrong label.

maximize:

maximize:

supervised learning
maximize:

for images x_i and their 
labels y_i.

supervised learning
maximize:

id23

for images x_i and their 
labels y_i.

supervised learning
maximize:

id23
1) we have no labels so we sample:

for images x_i and their 
labels y_i.

supervised learning
maximize:

id23
1) we have no labels so we sample:

for images x_i and their 
labels y_i.

2) once we collect a batch of rollouts:
maximize:

supervised learning
maximize:

id23
1) we have no labels so we sample:

for images x_i and their 
labels y_i.

2) once we collect a batch of rollouts:
maximize:

we call this the advantage, it   s a 
number, like +1.0 or -1.0 based on how 
this action eventually turned out.

supervised learning
maximize:

id23
1) we have no labels so we sample:

for images x_i and their 
labels y_i.

2) once we collect a batch of rollouts:
maximize:

+ve advantage will make that action more 
likely in the future, for that state.
-ve advantage will make that action less 
likely in the future, for that state.

discounting

blame each action assuming that its effects have 
exponentially decaying impact into the future.

reward +1.0

reward -1.0

discounting

blame each action assuming that its effects have 
exponentially decaying impact into the future.

0.21

0.24

discounted rewards
0.27

-0.81

-0.9

-1

0

0

\gamma = 0.9

reward +1.0

reward -1.0

time

discounted reward

episode start

ball gets past our paddle

-1 reward

we screwed up 
somewhere here

this was all hopeless and only 
contributes noise to the gradient

130 line gist, numpy as the only dependency.
https://gist.github.com/karpathy/a4166c7fe253700972fcbc77
e4ea32c5

nothing too scary over here.

we use openai gym.
and start the main training loop.

get the current image and 
preprocess it.

bookkeeping so that we can do 
id26 later. if you were 
to use pytorch or something, this 
would not be needed.

a small piece of backprop:
derivative of the [log id203 of the taken action 
given this image] with respect to the [output of the 
network (before sigmoid)]

recall: loss:

a small piece of backprop:
derivative of the [log id203 of the taken action 
given this image] with respect to the [output of the 
network (before sigmoid)]

recall: loss:

more compact:

step the environment

(execute the action, 
get new state and 
record the reward)

once a rollout is done,
concatenate together all images, hidden 
states, etc. that were seen in this batch.

again, if using pytorch, no need to do this.

advantage modulation

backprop!!!!!!1

use rmsprop for the 
parameter update.

prints etc

in summary

1. initialize a policy network at random
2. repeat forever:
3.     collect a bunch of rollouts with the policy
4.     increase the id203 of actions that worked well
5. ???
6. profit.

thank you! questions?

