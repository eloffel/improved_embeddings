advanced id189:
natural gradient, trpo, and more

john schulman

august 26, 2017

two limitations of    vanilla    id189

(cid:73) hard to choose stepsizes

(cid:73) input data is nonstationary due to changing policy: observation and reward

distributions change

(cid:73) bad step is more damaging than in supervised learning, since it a   ects

visitation distribution

(cid:73) step too far     bad policy
(cid:73) next batch: collected under bad policy
(cid:73) can   t recover   collapse in performance

(cid:73) sample e   ciency

(cid:73) only one gradient step per environment sample
(cid:73) dependent on scaling of coordinates

reducing id23 to optimization

(cid:73) much of modern ml: reduce learning to numerical optimization problem

(cid:73) supervised learning: minimize training error

(cid:73) rl: how to use all data so far and compute the best policy ?

(cid:73) id24: can (in principle) include all transitions seen so far, however,

(cid:73) id189: yes stochastic gradients, but no optimization

we   re optimizing the wrong objective
problem   

(cid:73) this lecture: write down an optimization problem that allows you to do a
small update to policy    based on data sampled from    (on-policy data)

what loss to optimize?

(cid:73) policy gradients

  g =   et

(cid:73) can di   erentiate the following loss

(cid:104)      log     (at | st)   at
(cid:104)

log     (at | st)   at

(cid:105)
(cid:105)

.

lpg (  ) =   et

but don   t want to optimize it too far

(cid:73) equivalently di   erentiate

lis
  old

(  ) =   et

just the chain rule:       log f (  )(cid:12)(cid:12)  old

=

(cid:20)     (at | st)
(cid:12)(cid:12)  old

    old(at | st)

     f (  )
f (  old) =      

  at

.

(cid:21)
(cid:16) f (  )

f (  old)

(cid:17)(cid:12)(cid:12)  old

at    =   old, state-actions are sampled using   old. (is = importance sampling)

surrogate loss: importance sampling interpretation

(cid:73) importance sampling interpretation

,at        [a  (st, at)]

(cid:20)     (at | st)
(cid:20)     (at | st)

    old(at | st)
    old(at | st)

(cid:21)

  at

,at       old

,at       old

est       old
= est       old
= est       old
= lis
(  )
  old

(cid:21)

a    old (st, at)

(importance sampling)

(replace a   with estimator)

(cid:73) kakade et al.1 and s. et al.2 analyze how lis approximates the actual

performance di   erence between    and   old.

(cid:73) in practice, lis is not much di   erent than the logprob version

lpg (  ) =   et

log     (at | st)   at

, for reasonably small policy changes.

(cid:104)

(cid:105)

1s. kakade and j. langford.    approximately optimal approximate id23   .
2j. schulman, s. levine, p. moritz, m. i. jordan, and p. abbeel.    trust region policy optimization   .

icml. 2002.

icml (2015).

trust region policy optimization

(cid:73) de   ne the following trust region update:

maximize

  et
  et[kl[    old(   | st),     (   | st)]]       .
(cid:73) also worth considering using a penalty instead of a constraint

subject to

  at

  

(cid:21)

(cid:20)     (at | st)

    old(at | st)
(cid:21)

(cid:20)     (at | st)

    old(at | st)

maximize

  

  et

  at

         et[kl[    old(   | st),     (   | st)]]

(cid:73) method of lagrange multipliers: optimality point of   -constrained problem

is also an optimality point of   -penalized problem for some   .

(cid:73) in practice,    is easier to tune, and    xed    is better than    xed   

monotonic improvement result

(cid:73) consider kl penalized objective

(cid:20)     (at | st)

    old(at | st)

(cid:21)

  at

         et[kl[    old(   | st),     (   | st)]]

maximize

  

  et

(cid:73) theory result: if we use max kl instead of mean kl in penalty, then we get

a lower (=pessimistic) bound on policy performance

trust region policy optimization: pseudocode

(cid:73) pseudocode:

for iteration=1, 2, . . . do

run policy for t timesteps or n trajectories
estimate advantage function at all timesteps
    (an | sn)
    old(an | sn)
kl    old

n(cid:88)

subject to

maximize

n=1

  

  an
(    )       

end for

(cid:73) can solve constrained optimization problem e   ciently by using conjugate

gradient

(cid:73) closely related to natural policy gradients (kakade, 2002), natural actor

critic (peters and schaal, 2005), reps (peters et al., 2010)

solving kl penalized problem
(    )           kl    old
(cid:73) maximize   l    old
(cid:73) make linear approximation to l    old

(    )

and quadratic approximation to kl term:
2 (         old)t f (         old)

maximize

  

where g =

g    (         old)       
   
     

(    )(cid:12)(cid:12)  =  old

l    old

,

f =

   2
   2  

kl    old

(    )(cid:12)(cid:12)  =  old

(cid:73) quadratic part of l is negligible compared to kl term
(cid:73) f is positive semide   nite, but not if we include hessian of l

(cid:73) solution:          old = 1

   f    1g , where f is fisher information matrix, g is

policy gradient. this is called the natural policy gradient3.

3s. kakade.    a natural policy gradient.    nips. 2001.

solving kl constrained problem

(cid:73) method of lagrange multipliers: solve penalized problem to get      (  ). then

substitute      (  ) into original problem and solve for   .

(cid:73)    only a   ects scaling of solution, not direction. compute scaling as follows:

(cid:73) compute s = f    1g (soln with    = 1)
(cid:73) rescale          old =   s so that constraint is satis   ed. quadratic approx

kl    old

(    )     1

2 (         old)t f (         old), so we want

(  s)t f (  s) =   

1
2

(cid:113)

   =

2  /(s t fs)

(cid:73) even better, we can do a line search to solve the original nonlinear problem.

maximize l    old

(    )     1[kl    old

(    )       ]

try   ,   /2,   /4, . . . until line search objective improves

   proximal    policy optimization: kl penalty version

(cid:73) use penalty instead of constraint

n(cid:88)

n=1

maximize

  

    (an | sn)
    old(an | sn)

  an     c    kl    old

(    )

(cid:73) pseudocode:

for iteration=1, 2, . . . do

run policy for t timesteps or n trajectories
estimate advantage function at all timesteps
do sgd on above objective for some number of epochs
if kl too high, increase   . if kl too low, decrease   .

end for

(cid:73)     same performance as trpo, but only    rst-order optimization

review

(cid:73) suggested optimizing surrogate loss lpg or lis
(cid:73) suggested using kl to constrain size of update
(cid:73) corresponds to natural gradient step f    1g under linear quadratic

approximation

(cid:73) can solve for this step approximately using conjugate gradient method

connection between trust region problem and other things

n(cid:88)

n=1

maximize

  

subject to

    (an | sn)
    old(an | sn)
kl    old

  an
(    )       

(cid:73) linear-quadratic approximation + penalty     natural gradient
(cid:73) no constraint     policy iteration
(cid:73) euclidean penalty instead of kl     vanilla policy gradient

limitations of trpo

(cid:73) hard to use with architectures with multiple outputs, e.g. policy and value

function (need to weight di   erent terms in distance metric)

(cid:73) empirically performs poorly on tasks requiring deep id98s and id56s, e.g.

atari benchmark

(cid:73) cg makes implementation more complicated

calculating natural gradient step with kfac

(cid:73) summary: do blockwise approximation to fim, and approximate each block

using a certain factorization

(cid:73) alternate expression for fim as outer product

(instead of second deriv. of kl):

(cid:2)      log     (at | st)t      log     (at | st)(cid:3)

  et

r. grosse and j. martens.    a kronecker-factored approximate fisher matrix for convolution layers   . 2016
j. martens and r. grosse.    optimizing neural networks with kronecker-factored approximate curvature   . 2015

calculating natural gradient step with kfac

(cid:73) consider network with weight matrix w appearing once in network, with

y = wx. then

   wij l = xi   yj l = xi y j
f =  et
fij,i(cid:48)j(cid:48) =   et

(cid:2)   wij log     (at | st)t   wij log     (at | st)(cid:3)
(cid:2)xi y j xi(cid:48)y j(cid:48)(cid:3)
(cid:2)y j y j(cid:48)(cid:3)
(cid:2)xi y j xi(cid:48)y j(cid:48)(cid:3)       et[xi xi(cid:48)]  et

fij,i(cid:48)j(cid:48) =   et

(cid:73) kfac approximation:

(cid:73) approximating fisher block as tensor product a     b, where a is nin    nin

and b is nout    nout. (a     b)   1 = a   1     b   1 and we can compute
matrix-vector product without forming full matrix.

(cid:73) maintain running estimate of covariance matrices   et[xi xj ],   et

(cid:2)y i(cid:48)y j(cid:48)(cid:3) and

periodically compute matrix inverses (small overhead)

acktr: combine a2c with kfac natural gradient

(cid:73) combined with a2c, gives excellent on atari benchmark and continuous

control from images4.

(cid:73) note: we   re already computing       log     (at | st) for policy gradient: no

extra gradient computation needed

(cid:73) matrix inverses can be computed asynchronously
(cid:73) limitation: works straightforwardly for feedforward nets (including

convolutions), less straightforward for id56s or architectures with shared
weights

4y. wu, e. mansimov, s. liao, r. grosse, and j. ba.    scalable trust-region method for deep id23 using kronecker-factored

approximation   .

(2017).

proximal policy optimization: kl penalized version

(cid:73) back to penalty instead of constraint

n(cid:88)

n=1

maximize

  

    (an | sn)
    old(an | sn)

  an     c    kl    old

(    )

(cid:73) pseudocode:

for iteration=1, 2, . . . do

run policy for t timesteps or n trajectories
estimate advantage function at all timesteps
do sgd on above objective for some number of epochs
if kl too high, increase   . if kl too low, decrease   .

end for

(cid:73)     same performance as trpo, but only    rst-order optimization

proximal policy optimization: clipping objective

(cid:73) recall the surrogate objective

(cid:20)     (at | st)

(cid:21)

(cid:104)

(cid:105)

rt(  )   at

.

=   et
(cid:73) form a lower bound via clipped importance ratios

    old(at | st)

lis(  ) =   et

  at

(cid:104)

lclip(  ) =   et

min(rt(  )   at, clip(rt(  ), 1      , 1 +  )   at)

(cid:105)

(1)

(2)

(cid:73) forms pessimistic bound on objective, can be optimized using sgd

01linear interpolation factor0.020.000.020.040.060.080.100.12et[klt]lcpi=et[rtat]et[clip(rt,1,1+)at]lclip=et[min(rtat,clip(rt,1,1+)at)]proximal policy optimization

(cid:73) pseudocode:

for iteration=1, 2, . . . do

run policy for t timesteps or n trajectories
estimate advantage function at all timesteps
do sgd on lclip(  ) objective for some number of epochs

end for

(cid:73) a bit better than trpo on continuous control, much better on atari
(cid:73) compatible with multi-output networks and id56s

further reading

icml (2016)

(cid:73) s. kakade.    a natural policy gradient.    nips. 2001
(cid:73) s. kakade and j. langford.    approximately optimal approximate id23   .
(cid:73) j. peters and s. schaal.    natural actor-critic   . neurocomputing (2008)
(cid:73) j. schulman, s. levine, p. moritz, m. i. jordan, and p. abbeel.    trust region policy optimization   .
(cid:73) y. duan, x. chen, r. houthooft, j. schulman, and p. abbeel.    benchmarking deep id23 for continuous control   .
(cid:73) j. martens and i. sutskever.    training deep and recurrent networks with hessian-free optimization   . springer, 2012
(cid:73) z. wang, v. bapst, n. heess, v. mnih, r. munos, et al.    sample e   cient actor-critic with experience replay   .
(cid:73) y. wu, e. mansimov, s. liao, r. grosse, and j. ba.    scalable trust-region method for deep id23 using kronecker-factored
(cid:73) j. schulman, f. wolski, p. dhariwal, a. radford, and o. klimov.    proximal policy optimization algorithms   .
(cid:73) blog.openai.com: recent posts on baselines releases

approximation   .

icml (2015)

icml. 2002

(2017)

(2016)

(2017)

that   s all. questions?

