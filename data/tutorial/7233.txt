the nuts and bolts of deep rl research

john schulman

august 26, 2017

outline

approaching new problems

ongoing development and tuning

general tuning strategies for rl

policy gradient strategies

id24 strategies

miscellaneous advice

approaching new problems

new algorithm? use small test problems

(cid:73) run experiments quickly
(cid:73) do hyperparameter search
(cid:73) interpret and visualize learning process: state visitation, value function, etc.
(cid:73) construct toy problems where your idea will be strongest and weakest,

where you have a sense of what it should do

(cid:73) counterpoint: don   t over   t algorithm to contrived problem
(cid:73) useful to have medium-sized problems that you   re intimately familiar with

new task? make it easier until signs of life

(cid:73) provide good input features
(cid:73) shape reward function

pomdp design

(cid:73) visualize random policy: does it sometimes exhibit desired behavior?
(cid:73) human control

(cid:73) atari: can you see game features in downsampled image?

(cid:73) plot time series for observations and rewards. are they on a reasonable

scale?

(cid:73) hopper.py in gym:

reward = 1.0 - 1e-3 * np.square(a).sum() + delta x / delta t

(cid:73) histogram observations and rewards

run your baselines

(cid:73) don   t expect them to work with default parameters
(cid:73) recommended:

(cid:73) cross-id178 method1
(cid:73) well-tuned policy gradient method2
(cid:73) well-tuned id24 + sarsa method

1istv  an szita and andr  as l  orincz (2006).    learning tetris using the noisy cross-id178 method   .
2https://github.com/openai/baselines, https://github.com/rll/rllab

in: neural computation.

run with more samples than expected

(cid:73) early in tuning process, may need huge number of samples

(cid:73) don   t be deterred by published work

(cid:73) examples:

(cid:73) trpo on atari: 100k timesteps per batch for kl= 0.01
(cid:73) id25 on atari: update freq=10k, replay bu   er size=1m

ongoing development and tuning

it works! but don   t be satis   ed

(cid:73) explore sensitivity to each parameter

(cid:73) if too sensitive, it doesn   t really work, you just got lucky

(cid:73) look for health indicators

(cid:73) vf    t quality
(cid:73) policy id178
(cid:73) update size in output space and parameter space
(cid:73) standard diagnostics for deep networks

continually benchmark your code

(cid:73) if reusing code, regressions occur
(cid:73) run a battery of benchmarks occasionally

always use multiple random seeds

always be ablating

(cid:73) di   erent tricks may substitute

(cid:73) especially whitening

(cid:73)    regularize    to favor simplicity in algorithm design space

(cid:73) as usual, simplicity     generalization

automate your experiments

(cid:73) don   t spend all day watching your code print out numbers
(cid:73) consider using a cloud computing platform (microsoft azure, amazon ec2,

google compute engine)

general tuning strategies for rl

whitening / standardizing data

(cid:73) if observations have unknown range, standardize

(cid:73) compute running estimate of mean and standard deviation
(cid:73) x(cid:48) = clip((x       )/  ,   10, 10)

(cid:73) rescale the rewards, but don   t shift mean, as that a   ects agent   s will to live
(cid:73) standardize prediction targets (e.g., value functions) the same way

generally important parameters

(cid:73) discount

(cid:73) returnt = rt +   rt+1 +   2rt+2 + . . .
(cid:73) e   ective time horizon: 1 +    +   2 +        = 1/(1       )

(cid:73) i.e.,    = 0.99     ignore rewards delayed by more than 100 timesteps

(cid:73) low    works well for well-shaped reward
(cid:73) in td(  ) methods, can get away with high    when    < 1

(cid:73) action frequency

(cid:73) solvable with human control (if possible)
(cid:73) view random exploration

general rl diagnostics

(cid:73) look at min/max/stdev of episode returns, along with mean
(cid:73) look at episode lengths: sometimes provides additional information

(cid:73) solving problem faster, losing game slower

policy gradient strategies

id178 as diagnostic

(cid:73) premature drop in policy id178     no learning
(cid:73) alleviate by using id178 bonus or kl penalty

kl as diagnostic

(cid:73) compute kl(cid:2)  old(   | s),   (   | s)(cid:3)

(cid:73) kl spike     drastic loss of performance
(cid:73) no learning progress might mean steps are too large

(cid:73) batchsize=100k converges to di   erent result than batchsize=20k.

baseline explained variance

(cid:73) explained variance = 1   var[empirical return   predicted value]

var [empirical return]

policy initialization

(cid:73) more important than in supervised learning: determines initial state

visitation

(cid:73) zero or tiny    nal layer, to maximize id178

id24 strategies

(cid:73) optimize memory usage carefully: you   ll need it for replay bu   er
(cid:73) learning rate schedules
(cid:73) exploration schedules
(cid:73) be patient. id25 converges slowly

(cid:73) on atari, often 10-40m frames to get policy much better than random

thanks to szymon sidor for suggestions

miscellaneous advice

(cid:73) read older textbooks and theses, not just conference papers
(cid:73) don   t get stuck on problems   can   t solve everything at once

(cid:73) exploration problems like cart-pole swing-up
(cid:73) id25 on atari vs cartpole

(cid:73) techniques from supervised learning don   t necessarily work in rl: batch

norm, dropout, big networks

that   s all. questions?

