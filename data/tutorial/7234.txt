stochastic computation graphs, and

pathwise derivative id189

john schulman

august 27, 2017

stochastic computation graphs

motivation

(cid:73) gradient estimation is at the core of much of modern machine learning
(cid:73) backprop is mechanical: this us more freedom to tinker with architectures

and id168s

(cid:73) rl (and certain probabilistic models) involve components you can   t

backpropagate through   need reinforce-style estimators logprob *
advantage

(cid:73) often we need a combination of backprop-style and reinforce-style

gradient estimation

(cid:73) id56 policies   (at | ht), where ht = f (ht   1, at   1, ot)
(cid:73) instead of reward, we have a known di   erentiable function, e.g. classi   er

with hard attention. objective = logprob of correct label.

(cid:73) each paper to propose such a model provides a new convoluted derivation

gradients of expectations

want to compute      e [f ]. where   s   ?

(cid:73)    inside    distribution, e.g., ex   p(   |   ) [f (x)]
(cid:73)      ex [f (x)] = ex [f (x)      log px (x;   )] .
(cid:73) score function (sf) estimator
(cid:73) example: reinforce policy gradients, where x is the trajectory

(cid:73) inside expectation: ez   n (0,1) [f (  , z)]

     ez [f (x(z,   ))] = ez [     f (x(z,   ))] .

(cid:73) pathwise derivative (pd) estimator
(cid:73) example: neural net with dropout

(cid:73) often, we can reparametrize, to change from one form to another

(cid:73) not a property of problem: equivalent ways of writing down the same

stochastic process

(cid:73) what if f depends on    in complicated way, a   ecting distribution and f ?

m. c. fu.    gradient estimation   .

in: handbooks in operations research and management science 13 (2006), pp. 575   616

stochastic computation graphs

(cid:73) stochastic computation graph is a dag, each node corresponds to a

deterministic or stochastic operation

(cid:73) can automatically derive unbiased gradient estimators, with variance

reduction

j. schulman, n. heess, t. weber, et al.    gradient estimation using stochastic computation graphs   .

in: nips. 2015

john schulman1, nicolas heess2, th  ophane weber2, pieter abbeel1 1university of california, berkeley              2google deepmindgeneralize id26 to deal with random variables that we can   t differentiate through.  why can   t we differentiate through them?gradient estimation using stochastic computation graphs1) discrete random variables 2) unmodeled external world, in rl / controlcomputation graphsstochastic computation graphsstochastic nodemotivation / applicationspolicy gradients in 
id23variational id136(hard) id12(hard) memory read/write   s1s2...sta1a2...atr1r2...rtdistributionisdifferentfromthedistributionweareevaluating:forparameter   2   ,   =   oldisusedforsampling,butweareevaluatingat   =   new.ev c|   new[  c]=ev c|   old2664  cyv c,    dvpv(v|depsv\   ,   new)pv(v|depsv\   ,   old)3775(17)   ev c|   old2664  c0bb@log0bb@yv c,    dvpv(v|depsv\   ,   new)pv(v|depsv\   ,   old)1cca+11cca3775(18)wherethesecondlineusedtheinequalityx logx+1,andthesignisreversedsince  cisnegative.summingoverc2candrearrangingwegetes|   new"xc2c  c#   es|   old"xc2c  c+xv2slog   p(v|depsv\   ,   new)p(v|depsv\   ,   old)     qv#(19)=es|   old"xv2slogp(v|depsv\   ,   new)  qv#+const(20)equation(20)allowsformajorization-minimizationalgorithms(liketheemalgorithm)tobeusedtooptimizewithrespectto   .infact,similarequationshavebeenderivedbyinterpretingrewards(negativecosts)asprobabilities,andthentakingthevariationallowerboundonlog-id203(e.g.,[24]).cexamplesc.1generalizedemalgorithmandvariationalid136.thegeneralizedemalgoritid48aximizeslikelihoodinaprobabilisticmodelwithlatentvariables[18].supposetheprobabilisticmodelde   nesaid203distributionp(x,z;   )wherexisob-served,zisalatentvariable,and   isaparameterofthedistribution.thegeneralizedemalgoritid48aximizesthevariationallowerbound,whichisde   nedbyanexpectationoverq:l(   ,q)=ez   q   log   p(x,z;   )q(z)    .(21)thegeneralizedemalgorithmcantakemanydifferentforms,leadingtodifferentgradientestima-tionproblems.xh1h2h3r1r2r3 1 2 3   1   2   3neuralvariationalid136.[14]proposeageneral-izedemalgorithmformulti-layeredlatentvariablemod-elssuchassigmoidalbeliefnetworksthatemploysanin-ferencenetwork,anexplicitparameterizationofqasafunctionoftheobserveddatax,toallowforfastapprox-imateid136.thegenerativemodelandid136net-worktaketheformp   (x)=xh1,h2p   1(x|h1)p   2(h1|h2)p   3(h2)q (h1,h2|x)=q 1(h1|x)q 2(h2|h1),andthusl(   , )=eh   q 26664logp   1(x|h1)q 1(h1|x)|{z}=r1+logp   2(h1|h2)p   3(h2)q 2(h2|h1)|{z}=r237775.11xh1...hngenerativemodelxh1...hnid136modelxystochasticstochasticfigure1:stochasticfeedforwardneuralnetworks.left:networkdiagram.rednodesarestochasticandbinary,whiletherestofthehiddensaredeterministicsigmoidnodes.right:motivationastowhymultimodaloutputsareneeded.giventhetophalfofthefacex,themouthinycanbedifferent,leadingtodifferentexpressions.themean-   eldapproximationwasproposedin[4]toimprovethelearningofsbns.adrawbackofthevariationalapproachisthat,similartogibbs,ithastocyclethroughthehiddennodesoneatatime.moreover,besidethestandardmean-   eldvariationalparameters,additionalparametersmustbeintroducedtolower-boundanintractabletermthatshowsupintheexpectedfreeenergy,makingthelower-boundlooser.gaussian   eldsareusedin[5]forid136bymakinggaussianapproximationstounits   input,butthereisnolongeralowerboundonthelikelihood.inthispaper,weintroducethestochasticfeedforwardneuralnetwork(sfnn)formodelingcon-ditionaldistributionsp(y|x)overcontinuousreal-valuedyoutputspace.unlikesbns,tobettermodelcontinuousdata,sfnnshavehiddenlayerswithbothstochasticanddeterministicunits.theleftpaneloffig.1showsadiagramofsfnnswithmultiplehiddenlayers.givenaninputvectorx,differentstatesofthestochasticunitscangeneratesdifferentmodesiny.forlearning,wepresentanovelmontecarlovariantofthegeneralizedexpectationmaximizationalgorithm.importancesamplingisusedforthee-stepforid136,whileerrorid26isusedbythem-steptoimproveavariationallowerboundonthedatalog-likelihood.sfnnshaveseveralattractiveproperties,including:   wecandrawsamplesfromtheexactmodeldistributionwithoutresortingtomcmc.   stochasticunitsformadistributedcodetorepresentanexponentialnumberofmixturecompo-nentsinoutputspace.   asadirectedmodel,learningdoesnotneedtodealwithaglobalpartitionfunction.   combinationofstochasticanddeterministichiddenunitscanbejointlytrainedusingtheback-propagationalgorithm,asinstandardfeed-forwardneuralnetworks.thetwomainalternativemodelsareconditionalgaussianrestrictedboltzmannmachines(c-grbms)[6]andmixturedensitynetworks(mdns)[1].notethatgaussianprocesses[7]andgaussianrandomfields[8]areunimodalandthereforeincapableofmodelingamultimodaly.conditionalrandomfields[9]arewidelyusedinnlpandvision,butoftenassumeytobedis-creteratherthancontinuous.c-grbmsarepopularmodelsusedforhumanmotionmodeling[6],structuredprediction[10],andasahigher-orderpotentialinimagesegmentation[11].whilec-grbmshavetheadvantageofexactid136,theyareenergybasedmodelsthatde   nedifferentpartitionfunctionsfordifferentinputx.learningalsorequiresgibbssamplingwhichispronetopoormixing.mdnsuseamixtureofgaussianstorepresenttheoutputy.thecomponents   means,mixingproportions,andtheoutputvariancesareallpredictedbyamlpconditionedonx.aswithsfnns,theid26algorithmcanbeusedtotrainmdnsef   ciently.however,thenumberofmixturecomponentsintheoutputyspacemustbepre-speci   edandthenumberofparametersislinearinthenumberofmixturecomponents.incontrast,withnhstochastichiddennodes,sfnnscanuseitsdistributedrepresentationtomodelupto2nhmixturecomponentsintheoutputy.2stochasticfeedforwardneuralnetworkssfnnscontainbinarystochastichiddenvariablesh2{0,1}nh,wherenhisthenumberofhiddennodes.forclarityofpresentation,weconstructasfnnfromaone-hidden-layermlpbyreplacingthesigmoidnodeswithstochasticbinaryones.notethatothertypesstochasticunitscanalsobeused.theconditionaldistributionofinterest,p(y|x),isobtainedbymarginalizingoutthelatentstochastichiddenvariables:p(y|x)=php(y,h|x).sfnnsaredirectedgraphicalmodelswherethegenerativeprocessstartsfromx,   owsthroughh,andthengeneratesoutputy.thus,wecanfactorizethejointdistributionas:p(y,h|x)=p(y|h)p(h|x).tomodelreal-valuedy,wehave2amortized id136:stochastic neural networksit   s all about gradients of expectations!general formula@@   e264xcostnodecc375=xstochasticnodexdeterministicallyin   uencedby   0bbb@@@   logp(x|parents(x))xcostnodecin   uencedbyxc1ccca+@@   xcostnodecdeterministicallyin   uencedbyxc1@@   e264xcostnodecc375=xstochasticnodexdeterministicallyin   uencedby   0bbb@@@   logp(x|parents(x))xcostnodecin   uencedbyxc1ccca+@@   xcostnodecdeterministicallyin   uencedbyxc baseline(allnodesnotin   uencedbyx)1just differentiate the    surrogate    function@@   e264xcostnodecc375=xstochasticnodexdeterministicallyin   uencedby   0bbb@@@   logp(x|parents(x))xcostnodecin   uencedbyxc1ccca+@@   xcostnodecdeterministicallyin   uencedbyxc baseline(allnodesnotin   uencedbyx)=@@   26666664xstochasticnodexdeterministicallyin   uencedby   0bbb@logp(x|parents(x))0bbb@xcostnodecin   uencedbyx  c baseline1ccca1ccca+xcostnodecdeterministicallyin   uencedbyxc(   )377777751under certain conditions, surrogate is a lower bound on expected cost, related to variational lower bound. see appendix cequivalently, use backprop, but introduce terms    logp(x) * (sum of downstream costs)  at stochastic nodes. see algorithm 1 in paper.ref: [reinforce]ref: [glimpse]ref: [sffnn]ref: [rlntm]refs: [nvil], [vae], [dlgm]-[nvil] mnih & gregor. neural variational id136 and learning in belief networks (2014). -[vae] kingma & welling. autoencoding id58 (2013) -[dlgm] rezende, mohamed & wierstra, stochastic id26 and approximate id136 in deep generative models. (2014) -[glimpse] mnih, heess, graves & kavukcuoglu. recurrent models of visual attention. (2014) -[rlntm] zaremba & sutskever. id23 id63s. (2015) -[sffnn] tang & salakhudinov, learning stochastic feedforward networks. (2013) -[reinforce] williams simple statistical gradient-following algorithms for connectionist id23 (1992) -fu, gradient estimation (2006) -neal & hinton. a view of the em algorithm that justi   es incremental, sparse, and other variants (1998) referencestwo flavors of gradient estimation   inputnodedeterministicnodestochasticnode   xfgivessfestimator   zxfgivespdestimator2.3simpleexamplesseveralsimpleexamplesthatillustratethestochasticcomputationgraphformalismareshownbelow.thegradientestimatorscanbedescribedbywritingtheexpectationsasintegralsanddifferentiating,aswiththesimplerestimatorsfromsection2.1.however,theyarealsoimpliedbythegeneralresultsthatwewillpresentinsection3.stochasticcomputationgraphobjectivegradientestimator(1)   xyf(2)   xyf(3)   xyf(4)   xyf(5)   x0x1x2f1f2@x@   @@xlogp(y|x)f(y)@@   logp(x|   )f(y(x))@@   logp(x|   )f(y)@@   logp(x|   )f(x,y(   ))+@y@   @f@y@@   logp(x1|   ,x0)(f1(x1)+f2(x2))+@@   logp(x2|   ,x1)f2(x2)ey[f(y)]ex[f(y(x))]ex,y[f(y)]ex[f(x,y(   ))]ex1,x2[f1(x1)+f2(x2)]figure1:simplestochasticcomputationgraphsthesesimpleexamplesillustrateseveralimportantmotifs,wherestochasticanddeterministicnodesarearrangedinseriesorinparallel.forexample,notethatin(2)thederivativeofydoesnotappearintheestimator,sincethepathfrom   tofis   blocked   byx.similarly,in(3),p(y|x)doesnotappear(thistypeofbehaviorisparticularlyusefulifweonlyhaveaccesstoasimulatorofasystem,butnotaccesstotheactuallikelihoodfunction).ontheotherhand,(4)hasadirectpathfrom   tof,whichcontributesatermtothegradientestimator.(5)resemblesaparameterizedmarkovrewardprocess,anditillustratesthatwe   llobtainscorefunctiontermsoftheformgradlog-id203   futurecosts.xh1h2w1w2b1b2soft-maxy=labelcross-id178losstheexamplesaboveallhaveoneinput   ,buttheformal-ismaccommodatesmodelswithmultipleinputs,forex-ampleastochasticneuralnetworkwithmultiplelayersofweightsandbiases,whichmayin   uencedifferentsub-setsofthestochasticandcostnodes.seeappendixcfornontrivialexampleswithstochasticnodesandmulti-pleinputs.the   gureontherightshowsadeterministiccomputationgraphrepresentingclassi   cationlossforatwo-layerneuralnetwork,whichhasfourparameters(w1,b1,w2,b2)(weightsandbiases).ofcourse,thisdeterministiccomputationgraphisaspecialtypeofstochasticcomputationgraph.4@@   e264xcostnodecc375=xstochasticnodexdeterministicallyin   uencedby   0bbb@@@   logp(x|parents(x))xcostnodecin   uencedbyxc1ccca+@@   xcostnodecdeterministicallyin   uencedbyxc baseline(allnodesnotin   uencedbyx)=@@   26666664xstochasticnodexdeterministicallyin   uencedby   0bbb@logp(x|parents(x))0bbb@xcostnodecin   uencedbyx  c baseline1ccca1ccca+xcostnodecdeterministicallyin   uencedbyxc(   )37777775scorefunctionestimator:givenx   px(x;   )@@   ex[f(x)]=ex   f(x)@@   logpx(x;   ) .pathwisederivativeestimator:givenz   pz(z),deterministicx(z,   )@@   ez[f(x(z,   ))]=ez   @@   f(x(z,   )) .1overviewllwhy   s it useful?  1)no need to  rederive every time  2)enable generic softwareadcbe    @@   e264xcostnodecc375=xstochasticnodexdeterministicallyin   uencedby   0bbb@@@   logp(x|parents(x))xcostnodecin   uencedbyxc1ccca+@@   xcostnodecdeterministicallyin   uencedbyxc baseline(allnodesnotin   uencedbyx)=@@   26666664xstochasticnodexdeterministicallyin   uencedby   0bbb@logp(x|parents(x))0bbb@xcostnodecin   uencedbyx  c baseline1ccca1ccca+xcostnodecdeterministicallyin   uencedbyxc(   )37777775scorefunctionestimator:givenx   px(x;   )@@   ex[f(x)]=ex   f(x)@@   logpx(x;   ) .pathwisederivativeestimator:givenz   pz(z),deterministicx(z,   )@@   ez[f(x(z,   ))]=ez   @@   f(x(z,   )) .@a@   @p(b|a,d)@a@p(b|a,d)@a@c@b@e@d@p(d| )@ @c@b@b@d1@@   e264xcostnodecc375=xstochasticnodexdeterministicallyin   uencedby   0bbb@@@   logp(x|parents(x))xcostnodecin   uencedbyxc1ccca+@@   xcostnodecdeterministicallyin   uencedbyxc baseline(allnodesnotin   uencedbyx)=@@   26666664xstochasticnodexdeterministicallyin   uencedby   0bbb@logp(x|parents(x))0bbb@xcostnodecin   uencedbyx  c baseline1ccca1ccca+xcostnodecdeterministicallyin   uencedbyxc(   )37777775scorefunctionestimator:givenx   px(x;   )@@   ex[f(x)]=ex   f(x)@@   logpx(x;   ) .pathwisederivativeestimator:givenz   pz(z),deterministicx(z,   )@@   ez[f(x(z,   ))]=ez   @@   f(x(z,   )) .@a@   @logp(b|a,d)@a@logp(b|a,d)@a@c@b@e@d@logp(d| )@ @c@b@b@d1worked exampleconclusions-we generalize computation graph formalism to allow for stochastic nodes that    block    derivative propagation -we generalize    baseline    (from policy gradient lit.) so it depends on all non-descendants of stochastic node -gradient estimator can be computed with modi   cation of id26 algorithm -automatically reproduce estimators from previous work@a@   @logp(b|a,d)@a@logp(b|a,d)@a@c@b@e@d@logp(d| )@ (c+e)@c@b@b@d@logp(b|a,d)@dc@d@    @logp(b|a,d)@dc+@e@d   2@a@   @logp(b|a,d)@a@logp(b|a,d)@a@c@b@e@d@logp(d| )@ (c+e)@c@b@b@d@logp(b|a,d)@dc@d@    @logp(b|a,d)@dc+@e@d   2@a@   @logp(b|a,d)@ac@logp(b|a,d)@ac@c@b@e@d@logp(d| )@ (c+e)@c@b@b@d@logp(b|a,d)@dc@d@    @logp(b|a,d)@dc+@e@d   2@a@   @logp(b|a,d)@ac@logp(b|a,d)@ac@c@b@e@d@logp(d| )@ (c+e)@c@b@b@d@logp(b|a,d)@dc@d@    @logp(b|a,d)@dc+@e@d   2example: hard attention

(cid:73) look at image (o1)
(cid:73) determine crop window (a)
(cid:73) get cropped image (o2)
(cid:73) version 1

(cid:73) output label (y )
(cid:73) get reward of 1 for correct label

(cid:73) version 2

(cid:73) output parameter specifying distribution over label (  )
(cid:73) reward = prob or logprob of correct label (log p)

o1o2  ayro1o2  a  logpabstract example

(cid:73) l = c + e. want to compute d
(cid:73) treat stochastic node b) as constants, and introduce losses

d   e [l] and d

d  e [l].

logprob     (futurecost     optionalbaseline) at each stochastic node

(cid:73) obtain unbiased gradient estimate by di   erentiating surrogate:

surrogate(  ,   ) = c + e

(cid:124)(cid:123)(cid:122)(cid:125)(1)

+ log p(  b | a, d)  c
(cid:125)
(cid:124)

(cid:123)(cid:122)

(2)

(1): how parameters in   uence cost through deterministic dependencies
(2): how parameters a   ect distribution over random variables.

adcbe    results

(cid:73) general method for calculating gradient estimator

(cid:73) surrogate loss version: each stochastic node gives a loss expression

logprob(node value) * (sum of    downstream    costs treated as constants).
add these to the sum of costs treated as variables.

(cid:73) alternative generalized backprop: as usual, do reverse topological sort.

instead of backpropagating using jacobian, backpropagate gradlogprob *
futurecosts.

(cid:73) generalized version of baselines for variance reduction (without bias):

logp(node) * (sum of downstream costs treated as constants minus
baseline(nondescendents of node))

pathwise derivative id189

deriving the policy gradient, reparameterized

(cid:73) episodic mdp:

want to compute      e [rt ]. we   ll use       log   (at | st;   )

  s1s2...sta1a2...atrtderiving the policy gradient, reparameterized

(cid:73) episodic mdp:

want to compute      e [rt ]. we   ll use       log   (at | st;   )

(cid:73) reparameterize: at =   (st, zt;   ). zt is noise from    xed distribution.

  s1s2...sta1a2...atrt  s1s2...sta1a2...atz1z2...ztrtderiving the policy gradient, reparameterized

(cid:73) episodic mdp:

want to compute      e [rt ]. we   ll use       log   (at | st;   )

(cid:73) reparameterize: at =   (st, zt;   ). zt is noise from    xed distribution.

(cid:73) only works if p(s2 | s1, a1) is known   (cid:95)

  s1s2...sta1a2...atrt  s1s2...sta1a2...atz1z2...ztrtusing a q-function

d
d  

e [rt ] = e(cid:34) t(cid:88)t=1
= e(cid:34) t(cid:88)t=1

drt
dat

dat

d  (cid:35) = e(cid:34) t(cid:88)t=1

d
dat

e [rt | at]

dat

d  (cid:35)

dq(st, at)

dat

dat

d  (cid:35) = e(cid:34) t(cid:88)t=1

q(st,   (st, zt;   ))(cid:35)

d
d  

  s1s2...sta1a2...atz1z2...ztrtsvg(0) algorithm

(cid:73) learn q   to approximate q   ,  , and use it to compute gradient estimates.

n. heess, g. wayne, d. silver, et al.    learning continuous control policies by stochastic value gradients   .

in: nips. 2015

svg(0) algorithm

(cid:73) learn q   to approximate q   ,  , and use it to compute gradient estimates.
(cid:73) pseudocode:

for iteration=1, 2, . . . do

execute policy      to collect t timesteps of data
t=1 q(st,   (st, zt;   ))
t=1(q  (st, at)       qt)2, e.g. with td(  )

update      using g          (cid:80)t
update q   using g          (cid:80)t

end for

n. heess, g. wayne, d. silver, et al.    learning continuous control policies by stochastic value gradients   .

in: nips. 2015

svg(1) algorithm

(cid:73) instead of learning q, we learn

(cid:73) state-value function v     v   ,  
(cid:73) dynamics model f , approximating st+1 = f (st, at) +   t

(cid:73) given transition (st, at, st+1), infer   t = st+1     f (st, at)
(cid:73) q(st, at) = e [rt +   v (st+1)] = e [rt +   v (f (st, at) +   t)], and at =   (st,   ,   t)

  s1s2...sta1a2...atz1z2...ztrtsvg(   ) algorithm

(cid:73) just learn dynamics model f
(cid:73) given whole trajectory, infer all noise variables
(cid:73) freeze all policy and dynamics noise, di   erentiate through entire deterministic

computation graph

  s1s2...sta1a2...atz1z2...ztrtsvg results

(cid:73) applied to 2d robotics tasks

(cid:73) overall: di   erent gradient estimators behave similarly

n. heess, g. wayne, d. silver, et al.    learning continuous control policies by stochastic value gradients   .

in: nips. 2015

deterministic policy gradient

(cid:73) for gaussian actions, variance of score function policy gradient estimator goes to

in   nity as variance goes to zero

(cid:73) but svg(0) gradient is    ne when        0

     (cid:88)t

q(st,   (st,   ,   t))

(cid:73) problem: there   s no exploration.
(cid:73) solution: add noise to the policy, but estimate q with td(0), so it   s valid

o   -policy

(cid:73) policy gradient is a little biased (even with q = q   ), but only because state

distribution is o      it gets the right gradient at every state

d. silver, g. lever, n. heess, et al.    deterministic policy gradient algorithms   .

in: icml. 2014

deep deterministic policy gradient

(cid:73) incorporate replay bu   er and target network ideas from id25 for increased

stability

t. p. lillicrap, j. j. hunt, a. pritzel, et al.    continuous control with deep id23   .

in: iclr (2015)

deep deterministic policy gradient

(cid:73) incorporate replay bu   er and target network ideas from id25 for increased

stability

(cid:73) use lagged (polyak-averaging) version of q   and      for    tting q   (towards

q   ,  ) with td(0)

  qt = rt +   q  (cid:48)(st+1,   (st+1;   

(cid:48)))

t. p. lillicrap, j. j. hunt, a. pritzel, et al.    continuous control with deep id23   .

in: iclr (2015)

deep deterministic policy gradient

(cid:73) incorporate replay bu   er and target network ideas from id25 for increased

stability

(cid:73) use lagged (polyak-averaging) version of q   and      for    tting q   (towards

q   ,  ) with td(0)

  qt = rt +   q  (cid:48)(st+1,   (st+1;   

(cid:48)))

(cid:73) pseudocode:

for iteration=1, 2, . . . do

act for several timesteps, add data to replay bu   er
sample minibatch

update      using g          (cid:80)t
update q   using g          (cid:80)t

end for

t=1 q(st,   (st, zt;   ))
t=1(q  (st, at)       qt)2,

t. p. lillicrap, j. j. hunt, a. pritzel, et al.    continuous control with deep id23   .

in: iclr (2015)

ddpg results

applied to 2d and 3d robotics tasks and driving with pixel input

t. p. lillicrap, j. j. hunt, a. pritzel, et al.    continuous control with deep id23   .

in: iclr (2015)

id189: comparison

y. duan, x. chen, r. houthooft, et al.    benchmarking deep id23 for continuous control   .

in: icml (2016)

