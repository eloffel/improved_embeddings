derivative-free	and	evolutionary	methods

xi	(peter)	chen	    uc	berkeley

slides	authored	with	john	schulman	(openai) and	pieter	abbeel	(openai /	uc	berkeley)

reinforcement	learning

[figure	source:	sutton	&	barto,	 1998]

policy	optimization	in	the	rl	landscape

max

   

u (   ) = max

   

e[

hxt=0

r(st)|      ]

evolutionary	methods

max

   

u (   ) = max

   

e[

hxt=0

r(st)|      ]

n general	algorithm:

n make	some	random	change	to	the	parameters
n if	the	result	improves,	keep	the	change
n repeat

cross-id178	method

cem:
initialize			
for iteration	=	1,	2,	   

>0

   2 rd,   2 rd
sample	n	parameters
for	each					,	perform	one	rollout	to	get	return	
select	the	top	k%	of				,	and	fit	a	new	diagonal	gaussian	

   i     n (  , diag( 2))

r(   i)

   i

   

to	those	samples.	update
endfor

  ,  

cross-id178	method

n can	work	embarrassingly	well

   2 r22

[nips	2013]

closely	related	approaches

(   (e), u (e))

n

n

n

n

   xe

   xe

reward	weighted	regression	(rwr)

n

dayan	&	hinton,	nc	1997;	peters	&	schaal,	icml	2007

  (i+1) = arg max

q(u (e), p  (   (e))) log p  (   (e))

policy	improvement	with	path	integrals	(pi2)

n

pi2:	theodorou,	 buchli,	 schaal jmlr2010;	kappen,	2007;	(pi2-cma:	stulp &	sigaud icml2012)

  (i+1) = arg max

exp( u (e)) log p  (   (e))

covariance	matrix	adaptation	evolutionary	strategy	(cma-es)

n

cma:	hansen	&	ostermeier 1996;		(cma-es:	hansen,	muller,	 koumoutsakos 2003)

(  (i+1),    (i+1)) = arg max

  ,    x  e

power

w(u (  e)) log n (   (  e);   ,    )

n

kober &	peters,	nips	2007	(also	applies	importance	 sampling	for	sample	re-use)

  (i+1) =   (i) + xe

(   (e)     (i))u (e)! / xe

u (e)!

cma-es

covariance	matrix	adaptation	(cma)	has	
become	standard	in	graphics	[hansen,	
ostermeier,	1996]

   2 r120

however,	drl	humanoid	 policy	usually	have	100k+	params

outline

n evolutionary	methods

n cross-id178	method	works	well	on	relatively	low-dim	problems
n can	es	work	well	on	optimizing	deep	net	policies?

learning	a	distribution	of	policies

n consider	a	distribution	of	policy	parameters:

n goal:	maximize
n stochastic	gradient	ascent	by	policy	gradient/lr:

e      p  (   ),             [r(    )]

        p  (   )

r  e      p  (   ),             [r(    )] = e      p  (   ),             [r   log p  (   )r(    )]

n ignore	most	information	about	trajectory	  :	states,	actions,	rewards..

n only	specific	params    and	return	r(  )	are	used
n this	turns	out	to	enable	very	scalable	distributed	training	(more	on	

this	later)

a	concrete	example

n suppose																				is	a	gaussian	distribution	with	mean					,	

  

        p  (   )

and	covariance	matrix

 2i

log p  (   ) =  ||        ||2
2 2
        
r   log p  (   ) =
 2

+ const

n if	we	draw	two	parameter	samples											,	and	obtain	two	

   1,    2

trajectories										:
   1,    2

e      p  (   ),             [r   log p  (   )r(    )]    

1

2   r(   1)

   1     
 2 + r(   2)

   2     

 2  

connection	to	finite	difference

n antithetic	sampling

n sample	a	pair	of	policies	with	mirror	noise
n get	a	pair	of	rollouts	from	environment
n spsa:	finite	difference	with	random	direction

(   +,     )

(   + =    +     ,      =          )

r  e [r(    )]    

=

=

1

2   r(   +)
2   r(   +)

1

   
2 

         

 2

 

   +     
 2 + r(    )
 2  
 2 + r(    )     

    

[r(   +)   r(    )]

vs

finite difference

practical	tricks

n loss	transformation to	be	robust	against	reward	scaling

n rank-based:
n learn	from	the	best:

r(    ) = percentile of    

r(    ) = 1[    in best k%]

n    cross-id178	method!

n parametrizations	matter

n normal	deep	nets	that	work	with	sgd	might	not	work	well	with	es	[duan

et	al.,	icml	2016]

n virtual	batch	norm	leads	to	competitive	performance	on	modern	 deep	rl	

benchmark	[salimans et	al.,	nips	2016]

n other	tricks	that	helped:	discretization	of	continuous	 action	space,	

densenet [huang	et	al.,	2016]...

es	competitive	on	deeprl tasks

[salimans,	ho,	chen,	sutskever,	2017]

considerations

n pros:

n work	with	arbitrary	parameterization,	

even	non-differentiable
embarrassingly	easy	to	parallelize

n

n cons:

n not	very	sample	efficient	since	ignores	

temporal	structure

n harder	to	tune

[salimans,	ho,	chen,	sutskever,	2017]

considerations

n pros:

n work	with	arbitrary	parameterization,	

even	non-differentiable
embarrassingly	easy	to	parallelize

n

n cons:

n not	very	sample	efficient	since	ignores	

temporal	structure

n harder	to	tune

[salimans,	ho,	chen,	sutskever,	2017]

distributed	deep	learning

worker 1

worker 2

worker 6

worker 3

worker 5

worker 4

distributed	deep	learning

each	worker	sends	
big	gradient	vectors

worker 1
worker 1

worker 2
worker 2

worker 6
worker 6

all

reduce

worker 3
worker 3

worker 5
worker 5

worker 4
worker 4

distributed	evolution

worker 1

worker 2

worker 6

what	need	to	be	sent??

worker 3

worker 5

worker 4

recap:	learning	a	distribution	of	policies

n consider	a	distribution	of	policy	parameters:

n goal:	maximize
n stochastic	gradient	ascent	by	policy	gradient/lr:

e      p  (   ),             [r(    )]

        p  (   )

r  e      p  (   ),             [r(    )] = e      p  (   ),             [r   log p  (   )r(    )]

n ignore	most	information	about	trajectory	  :	states,	actions,	rewards..

n only	specific	params    and	return	r(  )	are	used
n this	turns	out	to	enable	very	scalable	distributed	training	(more	on	

this	later)

distributed	evolution

worker 1

worker 2

worker 6

   and r(  )?

   is big!

but

    =    +     

worker 3

same for all workers

only need seed of random number generator!

scalability

[salimans,	ho,	chen,	sutskever,	2017]

scalability

[salimans,	ho,	chen,	sutskever,	2017]

distributed	evolution

each	worker	
broadcasts		
tiny	scalars

worker 1

worker 2

worker 6

worker 3

worker 5

worker 4

distributed	evolution

each	worker	
broadcasts		
tiny	scalars

worker 1

worker 2

worker 6

worker 3

worker 5

worker 4

distributed	evolution

each	worker	
broadcasts		
tiny	scalars

worker 1

worker 2

worker 6

worker 3

worker 5

worker 4

pro:	scalability

[salimans,	ho,	chen,	sutskever,	2017]

con:	sample	efficiency

[salimans,	ho,	chen,	sutskever,	2017]

cross-id178	/	evolutionary	methods

n full	episode	evaluation,	parameter	perturbation

n more	structured	exploration
n not	biased	by	discount	factor
n easy	to	scale

n open	question:	policy	gradient	at	action	level	or	parameter	level?

n

important	to	remember	both	are	doing	finite	difference	/	random	search

n when	episode	is	very	long,	maybe	finite	difference	in	parameter	space	is	more	

efficient?

n maybe	there	is	a	better	space	to	be	discovered

outline

n evolutionary	methods
n finite	difference	gradients

black	box	gradient	computation

challenge:	noise	can	dominate

r(t)

r(t)

e       [r(    )]

   

solution	1:		average	over	many	samples

r(t)

r(t)

e       [r(    )]

   

solution	2:	fix	random	seed

r(t)

r(t)

e       [r(    )]
fixed	random	
seed	sample

   

solution	2:	fix	random	seed

n randomness	in	policy	and	dynamics

n but	can	often	only	control	randomness	in	policy..

n example:		wind	influence	on	a	helicopter	is	stochastic,	but	if	

we	assume	the	same	wind	pattern	across	trials,	this	will	make	
the	different	choices	of	   more	readily	comparable

n note:	equally	applicable	to	evolutionary	methods

[ng	&	jordan,	2000]	provide	theoretical	analysis	of	gains	from	fixing	randomness	(   pegasus   )

[policy search was done in simulation]

[ng + al, iser 2004]

learning	to	hover

