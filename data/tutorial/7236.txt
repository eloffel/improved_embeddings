model-based [deep] 
id23

chelsea finn 
deep rl bootcamp

outline

1. why use model-based id23? 
2. main model-based rl approaches 
3. using local models & guided policy search 
4. handling high-dimensional observations

outline

1. why use model-based id23? 
2. main model-based rl approaches 
3. using local models & guided policy search 
4. handling high-dimensional observations

why use model-based id23?

- sample efficiency 

gradient-free methods 

(e.g. nes, cma, etc.)

10x

fully online methods 

half-cheetah (slightly different version)

(e.g. a3c)

10x

id189 

(e.g. trpo)

10x

trpo+gae (schulman et al.    16)

half-cheetah

replay buffer value estimation methods 

(id24, ddpg, naf, etc.)

10x

model-based deep rl 

(e.g. guided policy search)

10x

model-based    shallow    rl 

(e.g. pilco)

gu et al.    16

100,000,000 steps 
(100,000 episodes) 
(~ 15 days real time)

wang et al.    17

10,000,000 steps 
(10,000 episodes) 
(~ 1.5 days real time)

1,000,000 steps 
(1,000 episodes) 
(~ 3 hours real time)

about 20 minutes of 
experience on a real 
robot

10x gap

slide from sergey levine

chebotar et al.    17 (note log scale)

why use model-based id23?

- sample efficiency 
- transferability & generality

a model can be reused for achieving different tasks.

[more examples later]

outline

1. why use model-based id23? 
2. main model-based rl approaches 
3. using local models & guided policy search 
4. handling high-dimensional observations

the anatomy of a id23 problem

slide from sergey levine

model-based id23

supervised learning

backprop through model to optimize policy

backpropagate

algorithm v0:

does it work?

yes!

    essentially how system identification works in classical robotics 
    some care should be taken to design a good base policy 
    particularly effective if we can hand-engineer a dynamics 
representation using our knowledge of physics, and fit just a few 
parameters

slide adapted from sergey levine

does it work?

no!

go right to get higher!

    distribution mismatch problem becomes exacerbated as we use more 
expressive model classes

slide adapted from sergey levine

can we do better?

algorithm v1:

what if we make a mistake?

can we do better?

algorithm v2a:

r e p l a n n i n g   h e l p s   w i t h   m o d e l   e r r o r s

 

s
p
e
t
s
n
y
r
e
v
e

 

model-predictive control (mpc)

 

s
p
e
t
s
n
y
r
e
v
e

 

an alternative way to choose actions

can instead sample to choose actions: 
a. sample action sequences from some distribution 

(e.g. uniformly at random) 

b. run actions through model to prediction future 
c. choose action leading to the best future

nagabandi et al. arxiv    17

summary so far
    version 0: collect random samples, train dynamics, plan 

    pro: simple, no iterative procedure 
    con: distribution mismatch problem 

    version 1: iteratively collect data, refit model 

    pro: simple, solves distribution mismatch 
    con: might make mistakes with imperfect model 

    version 2: iteratively collect data using mpc (replan at each step) 

    pro: robust to small model errors 
    con: computationally expensive, but have a planning algorithm available

two ways to optimize policy w.r.t. model: 

    backprop through model into policy 
    sampling-based optimization 

what kind of models can we use?

gaussian process

neural network

other

image: punjani & abbeel    14

slide adapted from sergey levine

video prediction? 
more on this later

outline

1. why use model-based id23? 
2. main model-based rl approaches 
3. using local models & guided policy search 
4. handling high-dimensional observations

the trouble with global models

    planner will seek out regions where the model is erroneously optimistic 
    need to find a very good model in most of the state space to converge 
on a good solution

the trouble with global models

    planner will seek out regions where the model is erroneously optimistic 
    need to find a very good model in most of the state space to converge 
on a good solution 
    in some tasks, the model is much more complex than the policy

local models

backpropagate

local models

local models

how to fit the dynamics?

can we do better?

what if we go too far?

how to stay close to old controller?

local models approach summary

levine & abbeel nips    14

e.g. using id75

e.g. using iterative lqr

case study: local models & iterative lqr

case study: local models & iterative lqr

local models approach summary

levine & abbeel nips    14

e.g. using id75

guided policy search: supervise one global policy using multiple local policies

end result: single local policy

e.g. using iterative lqr

case study: guided policy search

case study: guided policy search

guided policy search: learning

(levine*, finn*, et al. jmlr    16)

guided policy search: learned behaviors

(levine*, finn*, et al. jmlr    16)

outline

1. why use model-based id23? 
2. main model-based rl approaches 
3. using local models & guided policy search 
4. handling high-dimensional observations

only access to high-dimensional observations (i.e. images)?

also: no reward signal with only observations

only access to high-dimensional observations (i.e. images)?

one option: provide image of goal

also: no reward signal with only observations

approaches 
1. learn model in latent space 
2. learn model of observations (e.g. video) 
3.

inverse models [won   t cover]

learning in latent space

key idea: learn embedding

, then learn model in latent space

learning in latent space

key idea: learn embedding

, then do model-based rl in latent space

nips 2015

icra 2016

learning in latent space

what is reward for optimizing policy? 

reward signal:

learning in latent space

how to optimize latent embedding g?

finn et al.    16

watter et al.    15

learn embedding & model jointly

embedding is smooth and structured

learning in latent space

~300 trials = ~25 min of robot time (per task)

watter et al. nips    15

learning in latent space

125 trials = 11 min of robot time (per task)

finn et al. icra    16

learning in latent space

pros: 
+ learn complex visual skills very efficiently 
+ structured representation enables effective learning 
cons: 
-  reconstruction objectives might not recover the right representation

aside: low-dimensional embedding can also be useful for model-free approaches

model-free rl in latent space

use embedding for reward function

sermanet et al. rss    17

video demonstration

learned policy

fqi in latent space 

lange et al.    12

trpo in latent space 
ghadirzadeh et al.    17

acquire reward using 
id163 features

+  model-free rl

if you have a reward, you can predict it to form better latent space. 

(jaderberg et al.    17, shelhamer et al.    17)

modeling directly in observation space

recall mpc

 

s
p
e
t
s
n
y
r
e
v
e

 

action-conditioned video prediction

finn et al. nips    16, finn & levine icra    17

planning with visual foresight

1. consider potential action sequences 
2. predict the future for each action 

sequence 

3. pick best future & execute 

corresponding action 

4. repeat 1-3 to replan in real time

finn & levine icra    17

modeling directly in observation space

<2 days of unsupervised robot time 

finn & levine icra    17

only human involvement: programming initial motions and providing objects to play with.

modeling directly in observation space

model can be reused for different tasks

ebert et al.    17

modeling directly in observation space

pros: 
+ entirely self-supervised 
+ learn for a variety of tasks 
+ more efficient than single-task model-free learning 
cons: 
-  can   t [yet] handle as complex skills as model-free methods

outline

1. why use model-based id23? 
2. main model-based rl approaches 
3. using local models & guided policy search 
4. handling high-dimensional observations

model-based rl review

supervised learning

improve the 
policy

e.g., backprop through model

correcting for model errors: 
refitting model with new data, replanning with mpc, using local models 
model-based rl from raw observations: 
learn latent space, typically with unsupervised learning, or  
model & plan directly in observational space

suggested reading on model-based rl

tassa et al. iros    12. synthesis and stabilization of complex behaviors. good 
introduction to mpc with a known model 
levine*, finn* et al. jmlr    16. end-to-end learning of deep visuomotor policies. thorough 
paper on guided policy search for learning real robotic vision-based skills 
heess et al. nips    15. stochastic value gradients. backdrop through dynamics to assist 
model-free learner 
watter et al. nips    15. embed-to-control, learn latent space and use model-baed rl in 
learned latent space to reach image of goal 
finn & levine icra    17. deep visual foresight for planning robot motion. plan using 
learned action-conditioned video prediction model

further reading on model-based rl
use known model: tassa et al. iros    12, tan et al. tog    14, mordatch et al. tog    14  
guided policy search: levine*, finn* et al. jmlr    16, mordatch et al. rss    14, nips    15  
backprop through model: deisenroth et al. icml    11, heess et al. nips    15, mishra et al. icml 
   17, degrave et al.    17, henaff et al.    17 
inverse models: agrawal et al. nips    16 
mbrl in latent space: watter et al. nips    15, finn et al. icra    16  
mpc with deep models: lenz et al. rss    15, finn & levine icra    17 
combining model-based & model-free:
- use roll-outs from model as experience: sutton    90, gu et al. icml    16 
- use model as baseline: chebotar et al. icml    17 
- use model for exploration: stadie et al. arxiv    15, oh et al. nips    16 
- model-free policy with planning capabilities:  tamar et al. nips    16, pascanu et al.    17 
- model-based look-ahead: guo et al. nips    14, silver et al. nature    16

model-based vs. model-free algorithms
models: 
+ easy to collect data in a scalable way (self-supervised) 
+ possibility to transfer across tasks 
+ typically require a smaller quantity of supervised data 
- models don   t optimize for task performance 
- sometimes harder to learn than a policy 
- often need assumptions to learn complex skills (continuity, resets) 
model-free: 
+ makes little assumptions beyond a reward function 
+ effective for learning complex policies 
- require a lot of experience (slower) 
- not transferable across tasks

ultimately we will want both!

closing remarks

model-based rl is an under-explored area of research

two active, exciting areas:  
- model-based approaches with high-dimensional observations 
- combining elements of model-based planning & model-free policies

questions?

