deep	reinforcement	learning	bootcamp	

lecture	10a:	u7li7es	

	

	

pieter	abbeel	

openai		/	uc	berkeley	/	gradescope	

	

slides	made	with	dan	klein	

	

schedule	--	sunday	

8:30:	breakfast		
9-10:	core	lecture	7	svg,	ddpg,	and	stochasmc	computamon	graphs	(john	schulman)	
10-11:	core	lecture	8	derivamve-free	methods	(peter	chen)	
11-11:30:	co   ee	break	
11:30-12:30	core	lecture	9	model-based	rl	(chelsea	finn)	
12:30-1:30	lunch	[catered]	
1:30-2:30	core	lecture	10	u1li1es	/	inverse	rl	(pieter	abbeel	/	chelsea	finn)	
2:30-3:10	two-minute	presentamons	by	each	ta	
3:10-3:30	co   ee	break	
3:30-6	labs	4-5	
6-7	fronmers	lecture	ii:	recent	advances,	fronmers	and	future	of	deep	rl	(sergey	levine)	

umlimes	

our	premise	so	far	

      maximize:		

u (   ) = e" hxt=0

r(st, at, st+1) |       #

why?	

maximum	expected	umlity	

      why	should	we	average	umlimes?			

why	not,	e.g.,	worst-case	reasoning?	

      principle	of	maximum	expected	umlity:	

      a	ramonal	agent	should	chose	the	acmon	that	maximizes	its	expected	umlity,	given	its	

knowledge	

      quesmons:	

      where	do	umlimes	come	from?	
      how	do	we	know	such	umlimes	even	exist?	
      how	do	we	know	that	averaging	even	makes	sense?	
      what	if	our	behavior	(preferences)	can   t	be	described	by	umlimes?	

umlimes	

      umlimes	are	funcmons	from	outcomes	
(states	of	the	world)	to	real	numbers	
that	describe	an	agent   s	preferences	

      where	do	umlimes	come	from?	
in	a	game,	may	be	simple	(+1/-1)	

     
      umlimes	summarize	the	agent   s	goals	
      theorem:	any	   ramonal   	preferences	can	

be	summarized	as	a	umlity	funcmon	

      we	hard-wire	umlimes	and	let	

behaviors	emerge	
      why	don   t	we	let	agents	pick	umlimes?	
      why	don   t	we	prescribe	behaviors?	

	

umlimes:	uncertain	outcomes	

gejng	ice	cream	

get	single	

get	double	

oops	

whew!	

preferences	

      an	agent	must	have	preferences	among:	

      prizes:	a, b,	etc.	
      loleries:	situamons	with	uncertain	prizes	

      notamon:	

      preference:	
     
indi   erence:	

		a	prize 

		a	lolery 

a 

p                1-p 

a                  b 

ramonality	

ramonal	preferences	

      we	want	some	constraints	on	preferences	before	we	call	them	ramonal,	such	as:	

axiom	of	transimvity:	

(

ba

   

)

(

cb

   

)

   

   

(

ca

   

)

      for	example:	an	agent	with	intransimve	preferences	can	

	be	induced	to	give	away	all	of	its	money	
     
     
     

if	b	>	c,	then	an	agent	with	c	would	pay	(say)	1	cent	to	get	b	
if	a	>	b,	then	an	agent	with	b	would	pay	(say)	1	cent	to	get	a	
if	c	>	a,	then	an	agent	with	a	would	pay	(say)	1	cent	to	get	c	

ramonal	preferences	

the	axioms	of	ramonality	

theorem:	ramonal	preferences	imply	behavior	describable	as	maximizamon	of	expected	umlity	

meu	principle	

      theorem	[ramsey,	1931;	von	neumann	&	morgenstern,	1944]	

      given	any	preferences	samsfying	these	constraints,	there	exists	a	real-valued	

	funcmon	u	such	that:	

     

i.e.	values	assigned	by	u	preserve	preferences	of	both	prizes	and	loleries!	

      maximum	expected	umlity	(meu)	principle:	
      choose	the	acmon	that	maximizes	expected	umlity	
      note:	an	agent	can	be	enmrely	ramonal	(consistent	with	meu)	without	ever	represenmng	or	
      e.g.,	a	lookup	table	for	perfect	mc-tac-toe,	a	re   ex	vacuum	cleaner	

manipulamng	umlimes	and	probabilimes	

human	umlimes	

umlity	scales	

      normalized	umlimes:	u+	=	1.0,	u-	=	0.0	
      micromorts:	one-millionth	chance	of	death,	useful	for	

paying	to	reduce	product	risks,	etc.	

      qalys:	quality-adjusted	life	years,	useful	for	medical	

decisions	involving	substanmal	risk	

      note:	behavior	is	invariant	under	posimve	linear	

transformamon	

      with	determinismc	prizes	only	(no	lolery	choices),	only	

ordinal	umlity	can	be	determined,	i.e.,	total	order	on	prizes	

human	umlimes	

      umlimes	map	states	to	real	numbers.	which	numbers?	
      standard	approach	to	assessment	(elicitamon)	of	human	umlimes:	

      compare	a	prize	a	to	a	standard	lolery	lp	between	

         best	possible	prize   	u+	with	id203	p	
         worst	possible	catastrophe   	u-	with	id203	1-p	
      adjust	lolery	id203	p	unml	indi   erence:	a	~	lp	
      resulmng	p	is	a	umlity	in	[0,1]	

pay	$30	

0.999999																														0.000001	

no	change	

instant	death	

money	

      money	does	not	behave	as	a	umlity	funcmon,	but	we	can	talk	about	the	

umlity	of	having	money	(or	being	in	debt)	

      given	a	lolery	l	=	[p,	$x;	(1-p),	$y]	

      the	expected	monetary	value	emv(l)	is	p*x	+	(1-p)*y	
      u(l)	=	p*u($x)	+	(1-p)*u($y)	
      typically,	u(l)	<	u(	emv(l)	)	
     
      when	deep	in	debt,	people	are	risk-prone	

in	this	sense,	people	are	risk-averse	

example:	insurance	

      consider	the	lolery	[0.5,	$1000;		0.5,	$0]	
      what	is	its	expected	monetary	value?		($500)	
      what	is	its	certainty	equivalent?	

      monetary	value	acceptable	in	lieu	of	lolery	
      $400	for	most	people	

      di   erence	of	$100	is	the	insurance	premium	
      there   s	an	insurance	industry	because	people	

will	pay	to	reduce	their	risk	

      if	everyone	were	risk-neutral,	no	insurance	

needed!	

     

it   s	win-win:	you   d	rather	have	the	$400	and	
the	insurance	company	would	rather	have	the	
lolery	(their	umlity	curve	is	   at	and	they	have	
many	loleries)	

