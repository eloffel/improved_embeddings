inverse id23

chelsea finn 
deep rl bootcamp

where does the reward come from?

computer games

reward

real world scenarios

robotics

dialog

autonomous driving

mnih et al.    15

what is the reward?
often use a proxy

*frequently easier to provide expert data* 

approach: infer reward function from roll-outs of expert policy

why infer the reward?

behavioral cloning/direct imitation: mimic actions of expert 
- but no reasoning about outcomes or dynamics 
- the expert might have different degrees of freedom 

can we reason about what the expert is trying to achieve?

inverse optimal control / inverse id23:

infer reward function from demonstrations 

(ioc/irl)

(kalman    64, ng & russell    00)

given: 
- state & action space 
- roll-outs from   * 
- dynamics model (sometimes)

goal: 
- recover reward function 
- then use reward to get policy

challenges 

underdefined problem 

difficult to evaluate a learned reward 

demonstrations may not be precisely optimal

maximum id178 inverse rl

(ziebart et al.    08)

handle ambiguity using probabilistic model of behavior

notation:

maxent formulation:

(energy-based model for behavior)

maximum id178 irl optimization

maximum id178 irl optimization

{

blackboard

maximum id178 inverse rl

(ziebart et al.    08)

handle ambiguity using probabilistic model of behavior

how can we: 
(1) handle unknown dynamics? (2) avoid solving the mdp in the inner loop

sample adaptively to estimate z 

sample to estimate z

[by constructing a policy]

guided cost learning algorithm

(finn et al. icml    16)

policy   

generate policy 
samples from   

update reward using 

samples & demos

policy   

update    w.r.t. reward

reward r

guided cost learning algorithm

(finn et al. icml    16)

policy   

generate policy 
samples from   

generator

update reward using 

samples & demos

update    w.r.t. reward 
take 1 policy opt. step

reward r

policy   

update reward in inner loop of policy optimization

ho & ermon, nips    16

discriminator

aside: id3 

(goodfellow et al.    14)

similar to inverse rl, gans learn an objective for generative modeling.

zhu et al.    17

arjovsky et al.    17

isola et al.    17

inverse rl

trajectory   
policy   ~q(  )

reward r

sample x

generator g

gans

discriminator d

(finn*, christiano*, et al.    16)

connection to id3

inverse rl

trajectory   
policy   ~q(  )

reward r

(goodfellow et al.    14)

sample x

generator g

discriminator d

gans

reward/discriminator optimization:

data distribution p

gcl:

both:

gail:

some classi   er

(finn*, christiano*, et al.    16)

connection to id3

inverse rl

trajectory   
policy   ~q(  )

reward r

(goodfellow et al.    14)

sample x

generator g

discriminator d

gans

policy/generator optimization:

data distribution p

unknown dynamics: train generator/policy with rl

baram et al. icml    17: use learned dynamics model to backdrop through discriminator
(finn*, christiano*, et al.    16)

*id178-regularized rl*

guided cost learning experiments

real-world tasks

dish placement

pouring almonds

state includes goal plate pose

state includes unsupervised 
visual features [finn et al.    16]

comparison

relative id178 irl 
(boularias et al.    11)

comparisons

path integral irl 

(kalakrishnan et al.    13)

relative id178 irl 
(boularias et al.    11)

generate policy 
samples from q

update reward using 

samples & demos

reward r

dish placement, demos

dish placement, standard cost

dish placement, relent irl

    video of dish baseline method

dish placement, gcl policy

    video of dish our method - samples & reoptimizing

pouring, demos

    video of pouring demos

pouring, relent irl

    video of pouring baseline method

pouring, gcl policy

    video of pouring our method - samples

generative adversarial imitation learning experiments
(ho & ermon nips    16)

- demonstrations from trpo-optimized policy 
- use trpo as a policy optimizer

conclusion: irl requires fewer demonstrations than behavioral cloning

generative adversarial imitation learning experiments
(ho & ermon nips    16)

learned behaviors from human motion capture

merel et al.    17

walking

falling & getting up

gcl & gail: pros & cons

(with an efficiency policy optimizer)

inverse id23 review

acquiring a reward function is important (and challenging!)

goal of inverse rl: infer reward function underlying expert demonstrations

evaluating the partition function: 
- initial approaches solve the mdp in the inner loop and/or 

assume known dynamics 

- with unknown dynamics, estimate z using samples

connection to id3: 
- sampling-based maxent irl is a gan with a special form of 

discriminator and uses rl to optimize the generator

suggested reading on inverse rl

classic papers: 
abbeel & ng icml    04. apprenticeship learning via inverse id23. 
good introduction to inverse id23 
ziebart et al. aaai    08. maximum id178 inverse id23. 
introduction to probabilistic method for inverse id23
modern papers: 
wulfmeier et al. arxiv    16. deep maximum id178 inverse id23. 
maxent inverse rl using deep reward functions 
finn et al. icml    16. guided cost learning. sampling based method for maxent irl 
that handles unknown dynamics and deep reward functions 
ho & ermon nips    16. generative adversarial imitation learning. inverse rl method 
using id3

further reading on inverse rl

maxent-based irl: ziebart et al. aaai    08, wulfmeier et al. arxiv    16, finn et al. icml    16 
adversarial irl: ho & ermon nips    16, finn*, christiano* et al. arxiv    16, baram et al. icml    17 
handling multimodality: li et al. arxiv    17, hausman et al. arxiv    17, wang, merel et al. arxiv    17  
handling domain shift: stadie et al. iclr    17 

questions?

ioc is under-defined

need id173:
    encourage slowly changing cost 

    cost of demos decreases strictly monotonically in time

id173 ablation

