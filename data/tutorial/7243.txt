monte carlo

id136 methods

iain murray
university of edinburgh

http://iainmurray.net

monte carlo and insomnia

enrico fermi (1901   1954) took
great delight in astonishing his
colleagues with his remarkably
accurate predictions of
experimental results. . .

. . . his    guesses    were really
derived from the statistical
sampling techniques that he used
to calculate with whenever
insomnia struck!

   the beginning of the monte carlo method, n. metropolis

overview

gaining insight from random samples

id136 / computation
what does my data imply? what is still uncertain?

sampling methods:
importance, rejection, metropolis   hastings, gibbs, slice

practical issues / debugging

id75

y =   1x +   2,

p(  ) = n (  ; 0, 0.42i)

-2024-6-4-2024priorp(  )xyid75

y(n) =   1x(n) +   2 +  (n),

 (n)     n (0, 0.12)

-2024-6-4-2024p(  |data)   p(data|  )p(  )xyid75 (zoomed in)

-2024-1.5-1-0.50p(  |data)   p(data|  )p(  )xymodel mismatch

what will bayesian id75 do?

   202   6   4   202quiz

given a (wrong) linear assumption, which explanations are
typical of the posterior distribution?

a

b

c

d all of the above

e none of the above

z not sure

   3   2   10123   4   202   3   2   10123   4   202   3   2   10123   4   202   under   tting   

posterior very certain despite blatant mis   t. peaked around least bad option.

   4   2024   6   4   2024roadmap

    looking at samples

    monte carlo computations

    how to actually get the samples

simple monte carlo integration

(cid:90) f (  )   (  ) d   =    average over    of f    
  (s)       

f (  (s)),

1
s

   

s(cid:88)s=1

unbiased
variance     1/s

prediction

p(y    | x   ,d) =(cid:90) p(y    | x   ,   ) p(   |d) d  

   

1

s(cid:88)s

p(y    | x   ,   (s)),

  (s)     p(   |d)

xx   yprediction

p(y    | x   ,d) =(cid:90) p(y    | x   ,   ) p(   |d) d  

   

1

s(cid:88)s

p(y    | x   ,   (s)),

  (s)     p(   |d)

p(y   |x   ,  (s))xx   yprediction

p(y    | x   ,d) =(cid:90) p(y    | x   ,   ) p(   |d) d  

   

1

s(cid:88)s

p(y    | x   ,   (s)),

  (s)     p(   |d)

p(y   |x   ,  (s))11212xs=1p(y   |x   ,  (s))xx   yprediction

p(y    | x   ,d) =(cid:90) p(y    | x   ,   ) p(   |d) d  

   

1

s(cid:88)s

p(y    | x   ,   (s)),

  (s)     p(   |d)

p(y   |x   ,d)1100100xs=1p(y   |x   ,  (s))xx   ymore interesting models

    noisy function values: y(n)     p(y | f (x(n); w),   )
    what weights and noise are plausible? p(  ), p(w |   )
    some observations corrupted?

if z(n) = 1, then y(n)     p(y | 0,   n),

. . . p(z = 1) =  

. . . a lot of choices. joint id203:

p(y(n) | z(n), x(n), w,   ,   n) p(z(n) |  ) p(x(n))(cid:35) p( ) p(  n) p(  ) p(w |   ) p(  )

(cid:34)(cid:89)n

id136

observe data: d =(cid:8)x(n), y(n)}
unknowns:    =(cid:8)w,   ,  ,   ,{z(n)}, . . .(cid:9)

p(   |d) =

p(d |   ) p(  )

p(d)

    p(d,   )

marginalization

interested in particular parameter   i

p(  i |d) =(cid:90) p(   |d) d  \i

sampling solution:

    sample everything:   (s)     p(   |d)
      (s)
comes from marginal p(  i |d)

i

(but see also    rao   blackwellization   )

roadmap

    looking at samples

    monte carlo computations

    how to actually get the samples
standard generators, markov chains

    practical issues

sampling simple distributions

use library routines for
univariate distributions
(and some other special cases)

this book (free online) explains
how some of them work

http://luc.devroye.org/rnbookindex.html

target distribution

  (  ) =

,

     (  )
z
e.g.,      (  ) = p(d |   ) p(  )

sampling discrete values

u     uniform[0, 1]
u = 0.4        = b

large number of samples? 1) alias method, devroye book; 2) ex 6.3 mackay book

sampling from a density

math: a(s)     uniform[0, 1],

  (s) =      1(a(s))

where cdf   (  ) =(cid:82)   

         (  (cid:48)) d  (cid:48)

geometry:
sample under curve

     (  )    (2)  (3)  (1)  (4)rejection sampling

while true:

       q
h     uniform[0, kq(  )]
if h <      (  ):

return   

     (  )kq(  )    (1)acceptrejectrejectrejectrejection sampling

while true:

       q
h     uniform[0, kq(  )]
if h <      (  ):

return   

koptq(  )     (  )kq(  )    (1)acceptrejectrejectrejectimportance sampling

rewrite integral: expectation under simple distribution q:

(cid:90) f (  )   (  ) d   = (cid:90) f (  )
s(cid:88)s=1

1
s

   

  (  )
q(  )

q(  ) d  ,

f (  (s))

  (  (s))
q(  (s))

,

  (s)     q

unbiased if q(  ) > 0 where   (  ) > 0. can have in   nite variance.

importance sampling 2

can   t evaluate   (  ) =

alternative version:

     (  )
z

, z =(cid:90)      (  ) d  

  (s)     q,

r   (s) =

     (  (s))
q(  (s))

,

r(s) =

biased but consistent estimator:

r   (s)

(cid:80)s(cid:48) r   (s(cid:48))

(cid:90) f (  )   (  ) d      

s(cid:88)s=1

f (  (s)) r(s)

id75

60 samples from prior:

-2024-1.5-1-0.50  (s)   prior,p(  )xyid75

60 samples from prior, importance reweighted:

-2024-1.5-1-0.50r(  (s))   p(data|  (s))xyid75

10,000 samples from prior, importance reweighted:

-2024-1.5-1-0.50r(  (s))   p(data|  (s))xyhigh dimensional   

12 curves from prior and 12 curves from posterior

-4-2024-505p(  |data)   p(data|  )p(  )xyhigh dimensional   

10,000 samples from prior, importance reweighted:

-4-2024-505r(  (s))   p(data|  (s))xyhigh dimensional   

100,000 samples from prior, importance reweighted:

-4-2024-505r(  (s))   p(data|  (s))xyhigh dimensional   

1,000,000 samples from prior, importance reweighted:

-4-2024-505r(  (s))   p(data|  (s))xyroadmap

    looking at samples

    monte carlo computations

    how to actually get the samples
standard generators, markov chains

    practical issues

>30,000 citations

marshall rosenbluth   s account:
http://dx.doi.org/10.1063/1.1887186

markov chains

p(  (s+1) |   (1) . . .   (s)) = t (  (s+1)      (s))

divergent

e.g., random walk di   usion
t (  (s+1)      (s)) = n (  (s+1);   (s),   2)

absorbing states

cycles

      s  markov chain equilibrium   (  )

p(  (s)) =   (  (s))

lim
s      

   ergodic   
if true for all   (s=0)

(other de   nitions of ergodic exist)

possible to get anywhere in k steps,
(t k(  (cid:48)

      ) > 0 for all pairs)

    no cycles or islands

invariant/stationary condition

if   (s) is a sample from   ,

  (s+1) is also a sample from   .

p(  (cid:48)) =(cid:90) t (  (cid:48)      )   (  ) d   =   (  (cid:48))

metropolis   hastings

  (cid:48)     q(  (cid:48);   (s))

if accept:

else:

  (s+1)       (cid:48)
  (s+1)       (s)

p (accept) = min(cid:32)1,

     (  (cid:48)) q(  (s);   (cid:48))

     (  (s)) q(  (cid:48);   (s))(cid:33)

example / warning

|   |                           |
10
0 1

0 <   (s) < 1
  (s+1) = (  (s)     1)/9, 1 <   (s) < 10

proposal: (cid:40)  (s+1) = 9  (s) + 1,
min(cid:18)1,

     (  ) q(  (cid:48);   )(cid:19) = min(cid:18)1,

accept move with id203:

     (  )(cid:19)

     (  (cid:48)) q(  ;   (cid:48))

     (  (cid:48))

(wrong!)

example / warning

accept   (cid:48) with id203:
q(  ;   (cid:48))      (  (cid:48))

min(cid:18)1,

really, green (1995):

q(  (cid:48);   )      (  )(cid:19) = min(cid:18)1,
min(cid:18)1, (cid:12)(cid:12)(cid:12)(cid:12)

     (  )(cid:19) = min(cid:18)1, 9

     (cid:12)(cid:12)(cid:12)(cid:12)

     (  (cid:48))

     (cid:48)

     (  (cid:48))

     (  )(cid:19)

1
1/9

     (  (cid:48))

     (  )(cid:19)

       matlab/octave code for demo

function samples = metropolis ( init , log_pstar_fn , iters , sigma )

d = numel ( init );
samples = zeros (d , iters );

state = init ;
lp_state = log_pstar_fn ( state );
for ss = 1: iters

% propose
prop = state + sigma * randn ( size ( state ));
lp_prop = log_pstar_fn ( prop );
if rand () < exp ( lp_prop - lp_state )

% accept
state = prop ;
lp_state = lp_prop ;

end
samples (: , ss ) = state (:);

end

step-size demo

explore n (0, 1) with di   erent step sizes   

sigma = @(s) plot(metropolis(0, @(x)-0.5*x*x, 1e3, s));

sigma(0.1)

99.8% accepts

sigma(1)

68.4% accepts

sigma(100)

0.5% accepts

01002003004005006007008009001000   4   202401002003004005006007008009001000   4   202401002003004005006007008009001000   4   2024id115 (mcmc)

user chooses   (  )

explore some plausible   

for large s:
p(  (s)) =   (  (s))
p(  (s+1)) =   (  (s+1))

e(cid:104)f (  (s))(cid:105) = e(cid:104)f (  (s+1))(cid:105) =(cid:90) f (  )  (  ) d   = lim

s      

1

s(cid:88)s

f (  (s))

markov chain mixing
initialization +     2000 steps     x    sample   

p(  (s=100))

p(  (s=2000))       (  )

creating an mcmc scheme

m   h gives t (  (cid:48)      ) with invariant   
lots of options for q(  (cid:48);   ):

    local di   usions
    appproximations of   
    update one variable or all?
    . . .

multiple valid operators ta, tb, tc, . . .

composing operators

if p(  (1)) =   (  (1))

  (2)     ta(        (1))     p(  (2)) =   (  (2))
  (3)     tb(        (2))     p(  (3)) =   (  (3))
  (4)     tc(        (3))     p(  (4)) =   (  (4))
composition tctbta leaves    invariant

valid mcmc method if ergodic overall

id150
pick variables in turn or randomly,

and resample p(  i |   j(cid:54)=i)

ti(  (cid:48)      ) = p(  (cid:48)i |   j(cid:54)=i)   (  (cid:48)j(cid:54)=i       j(cid:54)=i)

lhs adapted from fig 11.11 bishop prml

p(  1|  2)  1  2?id150 correctness

p(  ) = p(  i |   \i) p(  \i)
simulate by drawing   \i, then   i |   \i

draw   \i: sample   , throw initial   i away

blocking / collapsing

infer    = (w, z) given d = {x(n), y(n)}. model:

w     n (0, i)
zn     bernoulli(0.1)

y(n)    (cid:40)n (w(cid:62)x(n), 0.12)

n (0, 1)

zn = 0
zn = 1

block gibbs: resample p(w | z,d) and p(z| w,d)

collapsing: run mcmc on p(z|d) or p(w |d)

auxiliary variables

collapsing: analytically integrate variables out

auxiliary methods: introduce extra variables;

integrate by mcmc

explore:   (  , h), where

(cid:90)   (  , h) dh =   (  )

swendsen   wang, hamiltonian monte carlo (hmc), slice sampling,
pseudo-marginal methods. . .

slice sampling idea

sample uniformly under curve      (  )       (  )

p(h|   ) = uniform[0,      (  )]

p(   | h)    (cid:40)1      (  )     h

otherwise

0

=    uniform on the slice   

  h(  ,h)     (  )slice sampling

unimodal conditionals

rejection sampling p(   | h) using broader uniform

  h(  ,h)slice sampling

unimodal conditionals

adaptive rejection sampling p(   | h)

  h(  ,h)slice sampling

unimodal conditionals

quickly    nd new   

no rejections recorded

  h(  ,h)slice sampling
multimodal conditionals

use updates that leave p(   | h) invariant:
    place bracket randomly around point
    linearly step out until ends are o    slice
    sample on bracket, shrinking as before

  h(  ,h)     (  )slice sampling

advantages of slice-sampling:
    easy     only requires      (  )       (  )
    no rejections
    step sizes adaptive

other versions:
neal (2003): http://www.cs.toronto.edu/   radford/slice-aos.abstract.html
elliptical slice sampling: http://iainmurray.net/pub/10ess/
pseudo-marginal slice sampling: http://arxiv.org/abs/1510.02958

roadmap

    looking at samples

    monte carlo computations

    how to actually get the samples

standard generators, markov chains

    practical issues

empirical diagnostics

recommendations

rasmussen (2000)

for diagnostics:
standard software packages like r-coda

for opinion on thinning, multiple runs, burn in, etc.
practical id115
charles j. geyer, statistical science. 7(4):473   483, 1992.
http://www.jstor.org/stable/2246094

getting it right

we write mcmc code to update    |d

idea: also write code to sample d |   

both codes leave p(  ,d) invariant

run codes alternately. check      s match prior

geweke, jasa, 99(467):799   804, 2004.

  dother consistency checks

do i get the right answer on tiny versions
of my problem?

can i make good id136s about synthetic data
drawn from my model?

posterior model checking: gelman et al.
bayesian data analysis textbook and papers.

summary

write down the id203 of everything p(d,   )

condition on data, d,

explore unknowns    by mcmc

samples give plausible explanations

    look at them
    average their predictions

which method?

simulate / sample with known distribution:
exact samples, rejection sampling

posterior distribution, small, noisy problem
importance sampling

posterior distribution, interesting problem
start with mcmc
slice sampling, m-h if careful, gibbs if clever
hamiltonian methods, hmc, uses gradients

acknowledgements / references
my    rst reading: mackay   s book and neal   s review:
www.id136.phy.cam.ac.uk/mackay/itila/
www.cs.toronto.edu/   radford/review.abstract.html

handbook of id115

(brooks, gelman, jones, meng eds, 2011)

http://www.mcmchandbook.net/handbooksamplechapters.html

some relevant workshops (apologies for omissions)

    abc in montreal
    scalable monte carlo methods for bayesian analysis of big data
    black box learning and id136
    bayesian nonparametrics: the next generation

alternatives: probabilistic integration; advances in approximate bayesian id136

practical examples

my exercise sheet:
http://iainmurray.net/teaching/09mlss/

bugs examples and more in stan:
https://github.com/stan-dev/example-models/

kaggle entry:
http://iainmurray.net/pub/12kaggle dark/

appendix slides

reverse operator

r(        (cid:48)) =

t (  (cid:48)      )   (  )

(cid:82) t (  (cid:48)      )   (  ) d  

necessary condition:

=

t (  (cid:48)      )   (  )

  (  (cid:48))

t (  (cid:48)      )   (  ) = r(        (cid:48))   (  (cid:48)),

     ,   (cid:48)

if r = t , known as detailed balance (not necessary)

balance condition

t (  (cid:48)       )   (  ) = r(         (cid:48))   (  (cid:48))

implies that   (  ) is left invariant:

(cid:90) t (  (cid:48)      )   (  ) d   =   (  (cid:48))

(cid:90) r(        (cid:48)) d  
                        :1

metropolis   hastings

  (cid:48)     q(  (cid:48);   (s))

if accept:

else:

  (s+1)       (cid:48)
  (s+1)       (s)

transitions satisfy detailed balance:

(cid:48)

t (  

      ) =         

q(  (cid:48);   ) min(cid:16)1,   (  (cid:48)) q(  ;  (cid:48))

. . .

  (  ) q(  (cid:48);  )(cid:17)   (cid:48)
      )  (  ) symmetric in   ,   (cid:48)

(cid:54)=   
  (cid:48) =   

t (  (cid:48)

http://www.kaggle.com/c/darkworlds

. . .

