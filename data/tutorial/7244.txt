machine learning                        

   

                                                 srihari 

id203 theory 

sargur n. srihari 

srihari@cedar.buffalo.edu 

1 

machine learning                        

   

                                                 srihari 

id203 theory  in machine learning 

       id203 is key concept is dealing with 

uncertainty 
      arises due to finite size of data sets and noise on 
measurements 

       id203 theory 

      framework for quantification and manipulation of 
uncertainty 
      one of the central foundations of machine learning 

2 

machine learning                        

   

                                                 srihari 

random variable (r.v.) 

       takes values subject to chance  

      e.g., x  is the result of coin toss with values head 
and tail which are non - numeric  
      x can be denoted by a r.v. x which has values of 1 and 0  

      each value of x has an associated id203  

       id203 distribution 

      mathematical function that describes  

1.   possible values of a r.v.  
2.   and associated probabilities 

3 

machine learning                        

id203 with two variables 

   

                                                 srihari 

       key concepts:  

      conditional & joint probabilities of variables 

       random variables: b and f 

      box b, fruit f 

      f has two values orange (o) or apple (a) 
      b has values red (r) or blue (b) 

2 apples 
6 oranges 

3 apples 
1 orange 

p(f=o)=3/4 and p(f=a)=1/4 
let p(b=r)=4/10 and p(b=b)=6/10 

given the above data we are interested in 
several probabilities of interest: 
marginal, conditional and joint 
described next 

4 

machine learning                        

   

                                                 srihari 

probabilities of interest 

       marginal id203 

      what is the id203 of an 
apple? p(f=a) 
       note that we have to consider p(b) 

       id155 

      given that we have an orange 
what is the id203 that we 
chose the blue box? p(b=b|f=o) 

       joint id203 

      what is the id203 of orange 
and blue box? p(b=b,f=o) 

2 apples 
6 oranges 

3 apples 
1 orange 

5 

machine learning                        

   

                                                 srihari 

sum rule of id203 theory 

       consider two random variables 
       x can take on values xi, i=1,, m 
       y can take on values yi, i=1,..l 
       n trials sampling both x and y 
       no of trials with x=xi and y=yj is nij 

joint id203 p(x = xi,y = y j ) =

nij
n

       marginal id203 

p(x = xi) =

ci
n
    ,   p(x = xi) =
nij

since ci =
   

j

l

   

j=1

p(x = xi,y = y j )
6 

machine learning                        

   

                                                 srihari 

product rule of id203 theory 

       consider only those instances for which x=xi 
       then fraction of those instances for which y=yj is 
       called id155 
       relationship between joint and id155: 

written as p(y=yj|x=xi) 

p(y = y j | x = xi) =

nij
ci
nij
n

p(x = xi,y = y j ) =
                            = p(y = y j | x = xi)p(x = xi)
   

=

   

nij
ci

ci
n

7 

machine learning                        

   

                                                 srihari 

id47 

       from the product rule together with the symmetry 

property p(x,y)=p(y,x) we get 
p(x |y )p(y )

p(y | x) =
   

p(x)

       which is called bayes    theorem 
       using the sum rule the denominator is expressed as 
id172 constant to  
ensure sum of conditional 
id203 on lhs 
sums to 1 over all values of y 

p(x |y )p(y )

   

  

 

p(x) =
   

y

8 

machine learning                        

   

                                                 srihari 

rules of id203  

       given random variables x and y 
       sum rule gives marginal id203 
ci
n

p(x = xi,y = y j )

   

l

p(x = xi) =
   
       product rule: joint id203 in terms of conditional and 

=

j=1

marginal 

p(x,y ) =
   

nij
n

= p(y | x)p(x) =

nij
ci

  

ci
n

       combining we get bayes rule 

p(y | x) =
   

p(x |y )p(y )

where 

p(x)

p(x) =
   

viewed as 
posterior  a  likelihood x prior 

p(x |y )p(y )

   

y

9 

machine learning                        

                                                 srihari 
ex: joint distribution over two variables 

   

x takes nine possible values, y takes two values 

n = 60 data points 

histogram  
of y 
(fraction of 
data points 
having each 
value of y) 

histogram  
of x given y=1 

histogram  
of x 

fractions would equal the id203 as n   oo 

10 

machine learning                        

   

                                                 srihari 

bayes rule applied to fruit problem 

       id203 that box is red given 

that fruit picked is  orange 

p(f = o | b = r)p(b = r)

p(f = o)

p(b = r | f = o) =

                          =

4
3
4   
10
9
20

2
3 = 0.66

=

the a posteriori id203 of 0.66 is 
different from the a priori id203 of 0.4 

       id203 that fruit is orange 

    

      from sum and product rules 
p(f = o) = p(f = o,b = r) + p(f = o,b = b)
              = p(f = o | b = r)p(b = r) + p(f = o | b = b)p(b = b)
              =

1
4   

6
10 =

9
20 = 0.45

6
8   

4
10 +

the marginal id203 of 0.45 is lower 
than average id203 of 7/12=0.58 

11 

machine learning                        

   

                                                 srihari 

independent variables 

       if p(x,y)=p(x)p(y) then x and y are said to be 

independent 

       why? 
       from product rule: 

p(y | x) =
   

p(x,y )
p(x)

= p(y )

       in fruit example if each box contained same 

fraction of apples and oranges then p(f|b)=p(f)  

12 

machine learning                        

   

                                                 srihari 
id203 density function (pdf) 
       continuous variables 
       if id203 that x falls in 
interval (x,x+  x) is given 
by p(x)dx for   x   0  
 then p(x) is a pdf of x 
       id203 x lies in 

cumulative 
distribution 
function 

interval (a,b) is 

b

p(x     (a,b)) = p(x)dx
   

a

   

id203 that x lies in 
interval (-   ,z) is 

z

p(z) = p(x)dx
   

      

   

13 

machine learning                        

   

                                                 srihari 

several variables 

       if there are several continuous variables x1,   ,xd 

denoted by vector x then we can define a joint 
id203 density p(x)=p(x1,..,xd) 

       multivariate id203 density must satisfy 
 
 

   

 p(x)     0 
  
   
p(x)dx
   
      

= 1

14 

machine learning                        

                                                 srihari 
sum, product, bayes for continuous 

   

       rules apply for continuous, or combinations of 

discrete and continuous variables 

p(x) = p(x,y)dy

   

p(x,y) = p(y | x)p(x)
p(x | y)p(y)

p(y | x) =
   

p(x)

       formal justification of sum, product rules for 

continuous variables requires measure theory 

15 

machine learning                        

   

                                                 srihari 

expectation 

       expectation is average value of some function f(x) under the 
       for a discrete distribution 

id203 distribution p(x) denoted e[f] 

 

 e[f] =   x p(x) f(x) 

 

  

   

       for a continuous distribution 

examples of f(x) 
of use in ml:  
f(x)=x;     e[f] is mean 
f(x)=ln p(x); e[f] is id178 
e[f ] = p(x)f (x)dx
f(x)=-ln[q(x)/p(x)];   k-l divergence 
   
if there are n points drawn from a  pdf, then expectation can be 
approximated as 
 e[f] = (1/n)  n
 

this approximation is extremely important  
when we use 
sampling to determine expected value 

       conditional expectation with respect to a conditional distribution 

=1  f(xn) 
n

      

 

 ex[f] =   x p(x|y) f(x) 

16 

machine learning                        

   

                                                 srihari 

variance 

       measures how much variability there is in f(x) 

around its mean value e[f(x)] 
       variance of f(x) is denoted as 
 var[f] = e[(f(x)     e[f(x)])2] 
 
       expanding the square 
 
 var[f] = e[(f(x)2]     e[f(x)]2 
       variance of the variable x itself 
 

 var[x] = e[x2]     e[x]2 

 

17 

machine learning                        

   

                                                 srihari 

covariance 

       for two random variables x and y their covariance is 
        

 cov[x,y] = ex,y [{x-e[x]} {y-e[y]}] 
   

   = ex,y [xy] - e[x]e[y] 

 
       expresses how x and y vary together 

vanishes 

       if x and y are independent then their covariance 
       if x and y are two vectors of random variables 
       if we consider covariance of components of vector x 

covariance is a matrix 

with each other then we denote it as   cov[x] =cov [x,x] 

18 

machine learning                        

   

                                                 srihari 

bayesian probabilities 
       classical or frequentist view of probabilities 

       id203 is frequency of random, repeatable event 
       frequency of a tossed coin coming up heads is 1/2 

       bayesian view 

       id203 is a quantification of uncertainty 
       degree of belief in propositions that do not involve random 

variables 

       examples of uncertain events as probabilities:  

       whether arctic sea ice cap will disappear 
       whether moon was once in its own orbit around the sun 
       whether thomas jefferson had a child by one of his slaves 
       whether a signature on a check is genuine 

19 

machine learning                        

                                                 srihari 
whether arctic sea cap will disappear 

   

       we have some idea of how 
quickly polar ice is melting 

       revise it on the basis of 
fresh evidence (satellite 
observations) 

nasa video 

an uncertain event 
answered by general bayesian interpretation 

       assessment will affect 

actions we take (to reduce 
greenhouse gases) 

20 

machine learning                        

   

                                                 srihari 

bayesian representation of uncertainty 

       use of id203 to represent uncertainty is not an 
       if numerical values are used to represent degrees of 

ad-hoc choice 

belief, then simple set of axioms for manipulating 
degrees of belief leads to sum and product rules of 
id203 (cox   s theorem) 

       id203 theory can be regarded as an extension of 

boolean logic to situations involving uncertainty 
(jaynes) 

21 

machine learning                        

   

                                                 srihari 

bayesian approach 

       quantify uncertainty around choice of parameters w 

       e.g., w is vector of parameters in curve fitting 

y(x, w) = w0 + w1x + w2x 2 + ..+ wmx m =
   

m

   

wjx j

       uncertainty before observing data expressed by p(w) 
       given observed data d ={ t1, . . tn } 

       uncertainty in w after observing d, by bayes rule: 

j=0

 
 

p(w | d) =
    

p(d | w)p(w)

p(d)

       quantity p(d|w) is evaluated for observed data 

       it can be viewed as function of w 
       it represents how probable the data set is for  different parameters w 
       it is called the likelihood function 
       not a id203 distribution over w 

22 

machine learning                        

   

id47 in words 

                                                 srihari 

       uncertainty in w expressed as 

 
       id47 in words:   

p(w | d) =
    

p(d)

p(d | w)p(w)

 

 posterior      likelihood     prior 

       denominator is id172 factor  

       involves marginalization over w 

                       
p(d) = p(d | w)p(w)dw
    

   

  by sum rule

machine learning                        

   

role of likelihood function 

                                                 srihari 

       likelihood function plays central role in both 
       frequentist:  

bayesian and frequentist paradigms 

       w is a fixed parameter determined by an estimator;  
       error bars on estimate are obtained from possible 
data sets d 
       bayesian:   

       there is a single data set d 
       uncertainty in parameters expressed as id203 
distribution over w 

machine learning                        

   

                                                 srihari 

maximum likelihood approach 
       in frequentist setting w is a fixed parameter 

       w is set to value that maximizes likelihood function p(d|w) 
       in ml, negative log of likelihood function is called error 

function since maximizing likelihood is equivalent to 
minimizing error 

       error bars 

      bootstrap approach to creating l data sets 

       from n data points new data sets are created by drawing n points at 

random with replacement  

       repeat l times to generate l data sets 
       accuracy of parameter estimate can be evaluated by variability of 

predictions between different bootstrap sets   

25 

machine learning                        

   

                                                 srihari 

bayesian: prior and posterior 

inclusion of prior knowledge arises naturally 

      
       coin toss example 

       fair looking coin is tossed three times and lands head each time 
       classical m.l.e of the id203 of landing heads is 1 implying all 
       bayesian approach with reasonable prior will lead to less 

future tosses will land heads 

extreme conclusion 

  =p(h) 

p(  ) 

p(  |h) 

26 

machine learning                        

   

                                                 srihari 

practicality of bayesian approach 

 

       marginalization over whole parameter space is 

required to make predictions or compare 
models 

       factors making it practical: 

       sampling methods such as id115 
methods  
       increased speed and memory of computers  

       deterministic approximation schemes such as 
id58 and expectation propagation 
are alternatives to sampling 

27 

machine learning                        

   

the gaussian distribution 

                                                 srihari 

       for single real-valued variable x 

n(x |   ,  2) =
    

1

   
      
(2    2)1/2 exp    
   
         

1
2  2 (x       )2

   
      
   
         

what is an exponential:  
y=ex, where e=2.718 
its value for argument 0 is 1 
it is its own derivative 

       it has two parameters:  

       mean   , variance    2, 
       standard deviation   

       precision    =1/   2 

       can find expectations of functions of 

x under gaussian 

e[x] = n(x |   ,  2)

   

   

      
   

e[x 2] = n(x |   ,  2)

x 2dx =   2 +   2

   

      

var[x] = e[x 2]    e[x]2 =   2
    

maximum of a distribution is its mode 
for a gaussian, mode coincides with mean 

  = 0,    =1 

   

machine learning                        

                                                 srihari 
multivariate gaussian distribution 
       for single real-valued variable x 
   
      
(x      )t      1(x      )
   
         

(2  )d/2

1
2

1

n(x |   ,  ) =

   
      
1
1/2 exp    
   
         
  
       it has parameters:  

     

       mean   , a d-dimensional vector 
       covariance matrix    

       which is a d   d matrix 

machine learning                        

   

                                                 srihari 

likelihood function for gaussian 

       given n scalar observations x=[x1,.. xn]t 

       which are independent and identically 
       id203 of data set is given by 

distributed 

likelihood function 

n

p(x |   ,  2) = n(xn
    

    |   ,  2)

n=1

       log-likelihood function is 

data: black points 
likelihood= product of blue values 
pick mean and variance to maximize 
this product 

ln p(x |   ,  2) =    
    

1
2  2

n

   

(xn       )2    

ln   2    

ln(2  )

n
2

n
2

       maximum likelihood solutions are given by 

n=1

 

1
n
1
n

  ml =

2 =
  ml
    

n

   

n=1
n

n=1

          ml)2

(xn

xn

which is the sample mean 

which is the sample variance 

30 

machine learning                        

   

bias in maximum likelihood 

                                                 srihari 

       maximum likelihood 

systematically 
underestimates variance 
       e[  ml]=  
       e[   2

ml]=((n-1)/n)   2 

      not an issue as n increases 
       problem is related to over-

fitting problem 

green curve is true distribution 
averaged across three data sets 
mean is correct 
variance is underestimated 
because it is estimated relative 
to sample mean and not true mean 

31 

machine learning                        

   

                                                 srihari 

curve fitting probabilistically 

       goal is to predict for target 
variable t given a new value 
of the input variable x 

       given n input values 

x=(x1,..xn)t and corresponding 
target values t=(t1,..,tn)t 

       assume given value of x, value 
of t has a gaussian distribution 
with mean equal to y(x,w) of 
polynomial curve 
 p(t|x,w,  )=n(t|y(x,w),  -1) 

y(x, w) = w0 + w1x + w2x 2 + ..+ wmx m =
    

m

   

j=0

wjx j

gaussian conditional distribution 
for t given x. 
mean is given by  
polynomial function y(x,w) 
precision given by    

32 

   

machine learning                        

                                                 srihari 
curve fitting with maximum likelihood 
       likelihood function is 
       logarithm of the likelihood function is 

p(t | x, w,   ) = n(tn
      

    | y(xn, w),      1)

n=1

n

ln p(t | x, w,   ) =    
     

n

   

  
2

{y(xn, w)

   tn}2 +

ln       

ln(2  )

n
2

n
2

       to find maximum likelihood solution for 

n=1

polynomial coefficients wml 
       maximize w.r.t w 
       can omit last two terms -- don   t depend on w 
       can replace   /2 with    (since it is constant wrt w) 
       minimize negative log-likelihood 
       identical to sum-of-squares error function 

33 

machine learning                        

   

                                                 srihari 

precision parameter with id113 
       maximum likelihood can also be used to 
       maximizing likelihood wrt     gives 

determine     of gaussian conditional distribution 

1
  ml
     

=

1
n

n

   

n=1

{
y(xn, wml)   tn

2

}

       first determine parameter vector wml governing 
the mean and subsequently use this to find 
precision   ml 

34 

machine learning                        

   

                                                 srihari 

predictive distribution 

       knowing parameters  w and     
       predictions for new values of x can be made 

using 

 p(t|x,wml,  ml)=n(t|y(x,wml),  ml

-1) 

       instead of a point estimate we are now giving a 

id203 distribution over t 

35 

machine learning                        

   

                                                 srihari 

a more bayesian treatment 

       introducing a prior distribution over polynomial 

coefficients w  
  
  
  

   
            
   

  
2  

(m +1)/2
   
            
   

   
      
exp    
   
         

  
2

   
      
wtw
   
         

p(w |   ) = n(w | 0,     1i ) =
     
         where    is the precision of the distribution 
        m+1 is total no. of parameters for an mth order polynomial  
          are model parameters also called hyperparameter 

      

 they control distribution of model parameters 

36 

machine learning                        

   

                                                 srihari 

posterior distribution 

       using id47, posterior distribution for w is 

proportional to product of prior distribution and 
likelihood function 
 p(w|x,t,  ,  )          p(t|x,w,  )p(w|  ) 
       w can be determined by finding the most probable 
value of w given the data, ie. maximizing posterior 
distribution 

       this is equivalent (by taking logs) to minimizing  

n

   

  
2
     

2

{
y(xn, w )   tn

}

+

  
2

wtw

n=1

       same as sum of squared errors function with a 

id173 parameter given by   =  /   

37 

machine learning                        

bayesian curve fitting 

   

                                                 srihari 

       previous treatment still makes point estimate of w 
       in fully bayesian approach consistently apply sum and 

product rules and integrate over all values of w 

       given training data x and t and new test point x , goal 

is to predict value of t 
       i.e, wish to evaluate predictive distribution p(t|x,x,t) 
       applying sum and product rules of id203 

       predictive distribution can be written in the form 
p(t | x, x,t) = p(t, w |

x, x, t)dw                 by sum rule (marginalizing over w)

                 = p(t|x,w,x,t) p(

w | x, x,t)  by product rule

                  = p(t|x,w)p(w|x,t)dw
    

      by eliminating unnecessary variables

     p(t | x, w) = n(t | y(x , w),      1)

posterior distribution over parameters 
also a gaussian 

38 

   

   
   

machine learning                        

bayesian curve fitting 
       predictive distribution is also gaussian 

   

                                                 srihari 

    p(t | x, x,t) = n(t | m(x),s 2(x))

      where the mean and variance are dependent on x 

m(x) =     (x)t s

n

   

n=1

  (xn)tn

s 2(x) =      1 +   (x)t s  (x)
      (x)t

s    1 =   i +      (xn)

n

n=1

first term is uncertainty in predicted value due to noise in target 
second term is uncertainty in parameters due to bayesian treatment 

  (x)  has elements   i(x) = x i  for i = 0,..m
     

predictive distribution is a m=9 polynomial 
   = 5 x 10-3 
   =11.1 
red curve is mean 
red region is +1 std dev 

39 

machine learning                        

   

                                                 srihari 

model selection 

40 

machine learning                        

   

                                                 srihari 

models in curve fitting 

       in polynomial curve fitting:  

       an optimal order of polynomial gives best 
generalization 

       order of the polynomial controls  

      the number of free parameters in the model and 
thereby model complexity 

       with regularized least squares l also controls 

model complexity 

41 

machine learning                        

   

                                                 srihari 

validation set to select model 
       performance on training set is not a good 

indicator of predictive performance 

       if there is plenty of data,  

       use some of the data to train a range of models or a 
given model with a range of values for its parameters 

       compare them on an independent set, called 

validation set 

       select one having best predictive performance 

       if data set is small then some over-fitting can 

occur and it is necessary to keep aside a test set 

42 

machine learning                        

   

                                                 srihari 

s-fold cross validation 

partitioned into s groups 

       supply of data is limited 
       all available data is 
       s-1 groups are used to train 
and evaluated on remaining 
group 
       repeat for all s choices of 

held-out group 

       performance scores from s 

runs are averaged 

s=4 

if s=n this is the 
leave-one-out method 

43 

machine learning                        

   

                                                 srihari 

bayesian information criterion 
       criterion for choosing model 
       akaike information criterion (aic) chooses 

 ln p(d|wml)    m 

model for which the quantity 
 
       is highest 
       where m is number of adjustable parameters 
       bic is a variant of this quantity 

44 

machine learning                        

   

                                                 srihari 

the curse of dimensionality 

need to deal with spaces with many 

variables in machine learning 

45 

machine learning                        

   

                                                 srihari 

example clasification problem 
       three classes 

       12 variables: 

two shown 
       100 points 
       learn to 

classify from 
data 

which class  
should x 
belong to? 

46 

machine learning                        

   

                                                 srihari 

cell-based classification 

       na  ve approach of 
cell based voting 
will fail because of 
exponential growth 
of cells with 
dimensionality 

       hardly any points in 

each cell 

47 

machine learning                        

   

                                                 srihari 
volume of sphere in high dimensions 
       sphere is of radius r =1 in 
       what fraction of volume 

d-dimensions 

lies between radius  
 r = 1-    and  r =1? 
       vd(r)=kdrd 
       this fraction is given by 1-
       as d increases high 

(1-  )d 

proportion of volume lies 
near outer shell 

fraction of volume of sphere 
lying in range r =1-     to r = 1 
for various dimensions d 

48 

machine learning                        

   

                                                 srihari 
gaussian in high-dimensional space 
       x-y space converted to r-

space using polar 
coordinates 

       most of the id203 

mass is located in a thin 
shell at a specific radius 

49 

