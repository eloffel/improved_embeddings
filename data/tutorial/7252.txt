recent advances in non-id76

and its implications to learning

anima anandkumar

..

u.c. irvine

..

icml 2016 tutorial

optimization at the heart of machine learning

most learning problems can be cast as optimization.

unsupervised learning

id91
id116, hierarchical . . .

maximum likelihood estimator
probabilistic latent variable models

supervised learning

optimizing a neural network with
respect to a id168

output

neuron

input

convex vs. non-id76

guarantees for mostly convex..

but non-convex is trending!

images taken from https://www.facebook.com/nonconvex

convex vs. nonid76

unique optimum: global/local.

multiple local optima

guaranteed approaches for reaching global optima?

non-id76 in high dimensions

critical/statitionary points: x :    xf (x) = 0.

curse of dimensionality: exponential number of critical points.

local maxima

saddle points

local minima

guaranteed approaches for reaching local optima?

outline

1

introduction

2 escaping saddle points

3 avoiding local optima

4 conclusion

non-id76 in high dimensions

critical/statitionary points: x :    xf (x) = 0.

curse of dimensionality: exponential number of critical points.

escaping saddle points in high dimensions?

can sgd escape in bounded time?

local maxima

saddle points

local minima

why are saddle points ubiquitous?

symmetries in optimization landscapes

f (  ) invariant to permutations: f (x1, x2, . . .) = f (x2, x1, . . .) = . . .
e.g. deep learning, mixture models . . .
peril of non-convexity: avg. of optimal solutions is a critical point
but not optimal!

why are saddle points ubiquitous?

symmetries in optimization landscapes

f (  ) invariant to permutations: f (x1, x2, . . .) = f (x2, x1, . . .) = . . .
e.g. deep learning, mixture models . . .
peril of non-convexity: avg. of optimal solutions is a critical point
but not optimal!

optimization in deep learning

exponentially more equivalent solutions with no. of neurons, layers . . .

no free lunch: rich expressivity of large deep networks comes at a cost.

figures obtained from rong ge.

more critical points in overspeci   ed models
overspeci   cation: more capacity (neurons) than required.

result

each critical point in smaller network (over which function is realized)
generates a set of critical points in larger network.

even global optima in smaller network can generate local optima and
saddle points in larger network.

training data

neural network

learning trajectory

y

  (  )

  (  )

x

x1

x2

a long time spent in the manifold of singularities.

k fukumizu, s amari. local minima and plateaus in hierarchical structures of multilayer

id88s. 2000.

local optima vs. saddle points

optimization function: f (x). critical points:    xf (x) = 0.
local minima: a critical point where    2f (x) (cid:3) 0
local maxima: a critical point where    2f (x)     0
non-degenerate saddle point: a critical point where    2f (x) has
strictly positive and negative eigenvalues.
indeterminate critical points: degenerate hessian    2f (x) (cid:5) 0 or
   2f (x) (cid:6) 0.

guaranteed approaches for reaching local optima?

how to escape saddle points?

saddle point

saddle point has 0 gradient.

how to escape saddle points?

stuck

saddle point has 0 gradient.

escape

how to escape saddle points?

stuck

escape

saddle point has 0 gradient.
non-degenerate saddle: hessian has both    eigenvalues.

how to escape saddle points?

stuck

escape

saddle point has 0 gradient.
non-degenerate saddle: hessian has both    eigenvalues.
negative eigenvalue: direction of escape.

how to escape saddle points?

stuck

escape

saddle point has 0 gradient.
non-degenerate saddle: hessian has both    eigenvalues.
negative eigenvalue: direction of escape.

second order method: use hessian information to escape.

(cid:2) cubic id173 of id77, nestorov & polyak

first order method: noisy stochastic id119 works!
(cid:2) escaping from saddle points     online stochastic gradient for tensor

decomposition, r. ge, f. huang, c. jin, y. yuan

second-order methods for escaping saddle points

recall newton   s method

   1   f (x), where h(x) :=    2f (x)

  x = h(x)
better convergence rate for convex functions than id119.

second-order methods for escaping saddle points

recall newton   s method

   1   f (x), where h(x) :=    2f (x)

  x = h(x)
better convergence rate for convex functions than id119.

how does newton   s method perform on non-convex functions?

analysis of newton   s method

local quadratic approximation
assume x   
let   vi be change along each eigenvector vi.

is a non-degenerate saddle, i.e. hessian has    ve eigenvalue.

f (x   

+   x)     f (x   

) + 0.5

  i  v2
i .

(cid:2)

analysis of id119

i

preserves the sign of   i: movement in correct direction.
but if initialized at x   

, stays there and movement in nbd very slow.

analysis of newton   s method

local quadratic approximation
assume x   
let   vi be change along each eigenvector vi.

is a non-degenerate saddle, i.e. hessian has    ve eigenvalue.

f (x   

+   x)     f (x   

) + 0.5

  i  v2
i .

(cid:2)

analysis of id119

i

preserves the sign of   i: movement in correct direction.
but if initialized at x   

, stays there and movement in nbd very slow.

analysis of newton   s method

rescales each eigen direction as      1
: better convergence.
if   i < 0: wrong direction since rescaling cancels the sign.

i

newton   s method converges to a saddle point!

   identifying and attacking the saddle point problem in high-dimensional non-convex

optimization    by y. dauphin et al 2014.

saddle free trust region methods

escaping non-degenerate saddle point

moving along eigenvector with   i < 0 improves f (x).

saddle point

simple method: switch between id119 & hessian methods

saddle free trust region methods

escaping non-degenerate saddle point

moving along eigenvector with   i < 0 improves f (x).

stuck

escape

simple method: switch between id119 & hessian methods

saddle free trust region methods

escaping non-degenerate saddle point

moving along eigenvector with   i < 0 improves f (x).

stuck

escape

simple method: switch between id119 & hessian methods

more sophisticated: cubic id173 of id77 (nestorov
& polyak)

first order method for escaping saddle points?
second order methods expensive due to hessian computation.
approximate hessian: not easy to analyze for non-convex methods.

escaping from saddle points     online stochastic gradient for tensor decomposition, r. ge,

f. huang, c. jin, y. yuan

first order method for escaping saddle points?
second order methods expensive due to hessian computation.
approximate hessian: not easy to analyze for non-convex methods.

shortcoming of id119

can get stuck at saddle point: for
certain initializations.

escaping from saddle points     online stochastic gradient for tensor decomposition, r. ge,

f. huang, c. jin, y. yuan

first order method for escaping saddle points?
second order methods expensive due to hessian computation.
approximate hessian: not easy to analyze for non-convex methods.

shortcoming of id119

can get stuck at saddle point: for
certain initializations.

stochastic id119 with noise
noisy gradient cheaper to compute
(sgd vs. gd).

exact gradient at saddle useless, but
with su   cient noise escapes.

escaping from saddle points     online stochastic gradient for tensor decomposition, r. ge,

f. huang, c. jin, y. yuan

first order method for escaping saddle points?
second order methods expensive due to hessian computation.
approximate hessian: not easy to analyze for non-convex methods.

shortcoming of id119

can get stuck at saddle point: for
certain initializations.

stochastic id119 with noise
noisy gradient cheaper to compute
(sgd vs. gd).

exact gradient at saddle useless, but
with su   cient noise escapes.

theorem: for smooth, twice-di    fn. with non-degenerate saddle points,
noisy sgd converges to a local optimum in polynomial steps.

escaping from saddle points     online stochastic gradient for tensor decomposition, r. ge,

f. huang, c. jin, y. yuan

problems satisfying non-degenerate saddle property

matrix eigenvector

dictionary learning

=

orthogonal tensor decomposition

=

+

....

challenging to establish this property.

what about other kinds of saddle points?

higher order local optima

beyond second order saddle points

non-degenerate saddle point: a critical point where    2f (x) has
strictly positive and negative eigenvalues.
indeterminate critical points: degenerate hessian    2f (x) (cid:5) 0.

higher order local optima

beyond second order saddle points

non-degenerate saddle point: a critical point where    2f (x) has
strictly positive and negative eigenvalues.
indeterminate critical points: degenerate hessian    2f (x) (cid:5) 0.

weaker notions of local optimality under degeneracy

higher order local optima

beyond second order saddle points

non-degenerate saddle point: a critical point where    2f (x) has
strictly positive and negative eigenvalues.
indeterminate critical points: degenerate hessian    2f (x) (cid:5) 0.

weaker notions of local optimality under degeneracy

a critical point x is pth order local optimum if
f (x)     f (y) = o((cid:8)x     y(cid:8)p) for every neighbor y.

e   cient approaches for escaping higher order saddle points in non-id76 by a. ,

r. ge, colt 2016.

examples of degenerate saddle points

second order local min.
not third order local min.

third order local min.
not fourth order local min.

10
0
-10
-20
2

0

-2 -2

2

0

10
5
0
-5
2

0

-2 -2

2

0

examples of degenerate saddle points

connected set of degenerate
hessian.

connected set of degenerate
hessian.

all local optima

all saddle points.

2

1

0
2

0

-2 -2

2

0

2

1

0

-1
2

0

-2 -2

2

0

escaping degenerate saddle points

a point is third order local minimum i   

it is a critical point with    2f (x) (cid:5) 0.
   u in null space of    2f (x), i.e.    2f (x)u = 0,

(cid:2)

i,j,k

uiujuk   

   3

   xixjxk

f (x) = 0.

e   cient approaches for escaping higher order saddle points in non-id76 by a. ,

r. ge, colt 2016.

escaping degenerate saddle points

a point is third order local minimum i   

it is a critical point with    2f (x) (cid:5) 0.
   u in null space of    2f (x), i.e.    2f (x)u = 0,

(cid:2)

i,j,k

uiujuk   

   3

   xixjxk

f (x) = 0.

first method to escape third order saddle in polynomial time.

combination of second order and third order steps.

second order: cubic reg. newton. conv. to second-order local opt.
third order: max. tensor norm of    3f (x) in null-space of hessian.
run third order only if second order does not make progress.

e   cient approaches for escaping higher order saddle points in non-id76 by a. ,

r. ge, colt 2016.

concluding this section

summary

saddle points abound in high dimensional non-id76.

symmetries and over-speci   cation compound the problem.
saddle points slow down convergence.

solutions for escaping non-degenerate saddle points: second order
trust methods and noisy sgd.

degenerate saddle points are harder to escape: polynomial time for
third order but np-hard for higher order.

outlook

noisy sgd has worse scaling with dimension vs. second order
methods. can this be improved?

what are the saddle structures of popular problems, e.g. deep
learning?

can noisy sgd escape higher order saddle points in bounded time?

outline

1

introduction

2 escaping saddle points

3 avoiding local optima

4 conclusion

holy grail: reaching global optimum

so far: escaping saddle points using local information, i.e. derivatives.

can global optimum be reached using only local information?

local maxima

saddle points

local minima

holy grail: reaching global optimum

so far: escaping saddle points using local information, i.e. derivatives.

can global optimum be reached using only local information?

yes in special cases, e.g. id76, spectral optimization

local maxima

saddle points

local minima

holy grail: reaching global optimum

so far: escaping saddle points using local information, i.e. derivatives.

can global optimum be reached using only local information?

yes in special cases, e.g. id76, spectral optimization

plan: attaining global optimality through id106.

local maxima

saddle points

local minima

(cid:3)
matrix eigen-analysis

find decomposition of matrix m =

i   iviv(cid:3)
i .

optimization:    nd top eigenvector.
(cid:10)v, m v(cid:11) s.t. (cid:8)v(cid:8) = 1, v     r
d.

max

v

max

min

local optimum     global optimum!

(cid:3)
matrix eigen-analysis

find decomposition of matrix m =

i   iviv(cid:3)
i .

optimization:    nd top eigenvector.
(cid:10)v, m v(cid:11) s.t. (cid:8)v(cid:8) = 1, v     r
d.

max

v

lagrangian: (cid:10)v, m v(cid:11)       ((cid:8)v(cid:8)2     1).

local optimum     global optimum!

max

min

(cid:3)
matrix eigen-analysis

find decomposition of matrix m =

i   iviv(cid:3)
i .

optimization:    nd top eigenvector.
(cid:10)v, m v(cid:11) s.t. (cid:8)v(cid:8) = 1, v     r
d.

max

v

lagrangian: (cid:10)v, m v(cid:11)       ((cid:8)v(cid:8)2     1).
critical points: m v =   v (all eigenvectors).

local optimum     global optimum!

max

min

(cid:3)
matrix eigen-analysis

find decomposition of matrix m =

i   iviv(cid:3)
i .

optimization:    nd top eigenvector.
(cid:10)v, m v(cid:11) s.t. (cid:8)v(cid:8) = 1, v     r
d.

max

v

lagrangian: (cid:10)v, m v(cid:11)       ((cid:8)v(cid:8)2     1).
critical points: m v =   v (all eigenvectors).
all saddle points (at most d) are non-degenerate.

local optimum     global optimum!

max

min

(cid:3)
matrix eigen-analysis

find decomposition of matrix m =

i   iviv(cid:3)
i .

optimization:    nd top eigenvector.
(cid:10)v, m v(cid:11) s.t. (cid:8)v(cid:8) = 1, v     r
d.

max

v

lagrangian: (cid:10)v, m v(cid:11)       ((cid:8)v(cid:8)2     1).
critical points: m v =   v (all eigenvectors).
all saddle points (at most d) are non-degenerate.
no. of local optima: 1

local optimum     global optimum!

max

min

(cid:3)
matrix eigen-analysis

find decomposition of matrix m =

i   iviv(cid:3)
i .

optimization:    nd top eigenvector.
(cid:10)v, m v(cid:11) s.t. (cid:8)v(cid:8) = 1, v     r
d.

max

v

max

min

lagrangian: (cid:10)v, m v(cid:11)       ((cid:8)v(cid:8)2     1).
critical points: m v =   v (all eigenvectors).
all saddle points (at most d) are non-degenerate.
no. of local optima: 1

local optimum     global optimum!

algorithmic implication

gradient ascent (power method) converges to global optimum!

saddle points avoided by random initialization!

from matrices to tensors

matrix: pairwise moments

d  d is a second order tensor.

e[x     x]     r
e[x     x]i1,i2 = e[xi1xi2].
for matrices: e[x     x] = e[xx(cid:3)
m = uu(cid:3)

is rank-1 and mi,j = uiuj.

].

tensor: higher order moments

d  d  d is a third order tensor.

e[x     x     x]     r
e[x     x     x]i1,i2,i3 = e[xi1xi2xi3].
t = u     u     u is rank-1 and ti,j,k = uiujuk.

notion of tensor contraction

extends the notion of matrix product

(cid:2)

matrix product
vjmj
m v =

j

=

+

(cid:2)

tensor contraction

t (u, v,  ) =

uivjti,j,:

i,j

=

+

+

+

problem of tensor decomposition

computationally hard for general tensors.
orthogonal tensors t =

i   [k]   iui     ui     ui : ui     uj for i (cid:16)= j.

(cid:3)

decomposition through tensor norm max.

t (v, v, v), (cid:8)v(cid:8) = 1, v     r

d.

max

v

lagrangian: t (v, v, v)     1.5  ((cid:8)v(cid:8)2     1).

max

min

multiple local optima, but they correspond to components!

exponentially many saddle points!

problem of tensor decomposition

computationally hard for general tensors.
orthogonal tensors t =

i   [k]   iui     ui     ui : ui     uj for i (cid:16)= j.

(cid:3)

decomposition through tensor norm max.

t (v, v, v), (cid:8)v(cid:8) = 1, v     r

d.

max

v

lagrangian: t (v, v, v)     1.5  ((cid:8)v(cid:8)2     1).
critical points: t (v, v,  ) =   v.

max

min

multiple local optima, but they correspond to components!

exponentially many saddle points!

problem of tensor decomposition

computationally hard for general tensors.
orthogonal tensors t =

i   [k]   iui     ui     ui : ui     uj for i (cid:16)= j.

(cid:3)

decomposition through tensor norm max.

t (v, v, v), (cid:8)v(cid:8) = 1, v     r

d.

max

v

lagrangian: t (v, v, v)     1.5  ((cid:8)v(cid:8)2     1).
critical points: t (v, v,  ) =   v.
no. of eigenvectors: exp(d)!

max

min

multiple local optima, but they correspond to components!

exponentially many saddle points!

problem of tensor decomposition

computationally hard for general tensors.
orthogonal tensors t =

i   [k]   iui     ui     ui : ui     uj for i (cid:16)= j.

(cid:3)

decomposition through tensor norm max.

t (v, v, v), (cid:8)v(cid:8) = 1, v     r

d.

max

v

lagrangian: t (v, v, v)     1.5  ((cid:8)v(cid:8)2     1).
critical points: t (v, v,  ) =   v.
no. of eigenvectors: exp(d)!
all saddle points are non-degenerate.

max

min

multiple local optima, but they correspond to components!

exponentially many saddle points!

problem of tensor decomposition

computationally hard for general tensors.
orthogonal tensors t =

i   [k]   iui     ui     ui : ui     uj for i (cid:16)= j.

(cid:3)

decomposition through tensor norm max.

t (v, v, v), (cid:8)v(cid:8) = 1, v     r

d.

max

v

lagrangian: t (v, v, v)     1.5  ((cid:8)v(cid:8)2     1).
critical points: t (v, v,  ) =   v.
no. of eigenvectors: exp(d)!
all saddle points are non-degenerate.
local optima: {ui} for i = 1, . . . , k.

multiple local optima, but they correspond to components!

exponentially many saddle points!

max

min

implication: guaranteed tensor decomposition

(cid:3)

i   [k]   iui     ui     ui.

given orthogonal tensor t =

recover components one by one

run projected sgd on max
v:(cid:5)v(cid:5)=1
guaranteed to recover a local optimum {ui} (upto scale).
find all components {ui} by de   ation!

t (v, v, v).

implication: guaranteed tensor decomposition

(cid:3)

i   [k]   iui     ui     ui.

given orthogonal tensor t =

recover components one by one

run projected sgd on max
v:(cid:5)v(cid:5)=1
guaranteed to recover a local optimum {ui} (upto scale).
find all components {ui} by de   ation!

t (v, v, v).

alternative: simultaneous recovery of components (ge et al    15)

(cid:2)
i(cid:7)=j

t (vi, vi, vj, vj).

for fourth order tensor t min

   i,(cid:5)vi(cid:5)=1

all saddle points are non-degenerate.

all local optima are global.

sgd recovers the orthgonal tensor components optimally

perturbation analysis for tensor decomposition

well understood for matrix decomposition vs. hard for polynomials.
subtle analysis for tensor decomposition.

a., r. ge, d. hsu, s. kakade, m. telgarsky,    tensor decompositions for learning latent
variable models,    jmlr 2014.

perturbation analysis for tensor decomposition

well understood for matrix decomposition vs. hard for polynomials.
subtle analysis for tensor decomposition.

t     r
  t = t + e,

d  d  d: orthogonal tensor. e: noise tensor.

t =

  ivi     vi     vi,

(cid:8)e(cid:8) := max
x:(cid:5)x(cid:5)=1

|e(x, x, x)|.

(cid:2)

i

theorem: when (cid:8)e(cid:8) <   min   
(cid:8)e(cid:8) with linear no. of restarts.

d , power method recovers {vi} up to error

a., r. ge, d. hsu, s. kakade, m. telgarsky,    tensor decompositions for learning latent
variable models,    jmlr 2014.

perturbation analysis for tensor decomposition

v3

v1

v2

theorem: when (cid:8)e(cid:8) <   min   
(cid:8)e(cid:8) with linear no. of restarts.

d , power method recovers {vi} up to error

a., r. ge, d. hsu, s. kakade, m. telgarsky,    tensor decompositions for learning latent
variable models,    jmlr 2014.

non-orthogonal tensor decomposition

=

+

+       

t = v1

   3 + v2

   3 +        ,

a., r. ge, d. hsu, s. kakade, m. telgarsky,    tensor decompositions for learning latent

variable models,    jmlr 2014.

non-orthogonal tensor decomposition

orthogonalization

input tensor t

a., r. ge, d. hsu, s. kakade, m. telgarsky,    tensor decompositions for learning latent

variable models,    jmlr 2014.

non-orthogonal tensor decomposition

orthogonalization

t (w, w, w ) =   t

a., r. ge, d. hsu, s. kakade, m. telgarsky,    tensor decompositions for learning latent

variable models,    jmlr 2014.

non-orthogonal tensor decomposition

orthogonalization
v1 v2 w

  v1   v2

t (w, w, w ) =   t

a., r. ge, d. hsu, s. kakade, m. telgarsky,    tensor decompositions for learning latent

variable models,    jmlr 2014.

non-orthogonal tensor decomposition

orthogonalization
v1 v2 w

  v1   v2

t (w, w, w ) =   t

  t = t (w, w, w ) =   v1

   3 +   v2

   3 +        ,

=

+

a., r. ge, d. hsu, s. kakade, m. telgarsky,    tensor decompositions for learning latent

variable models,    jmlr 2014.

non-orthogonal tensor decomposition

orthogonalization
v1 v2 w

  v1   v2

t (w, w, w ) =   t

find w using svd of matrix slice

m = t (  ,  ,   ) =

+

a., r. ge, d. hsu, s. kakade, m. telgarsky,    tensor decompositions for learning latent

variable models,    jmlr 2014.

non-orthogonal tensor decomposition

orthogonalization
v1 v2 w

  v1   v2

t (w, w, w ) =   t

orthogonalization: invertible when vi   s linearly independent.

a., r. ge, d. hsu, s. kakade, m. telgarsky,    tensor decompositions for learning latent

variable models,    jmlr 2014.

unsupervised learning via tensor methods

data     model     learning algorithm     predictions

hidden

h

choice variable

k1

k2

k3

k4

k5

topics

a

a

a

a

a

lifegene datadna rna
observed variables

words

=

+

+

guaranteed learning through tensor methods

replace the objective function
max likelihood vs. best tensor decomp.

preserves global optimum (in   nite samples)

arg max

p(x;   ) = arg min

  

(cid:4)t (x): empirical tensor, t (  ): low rank tensor based on   .

  

f

(cid:8)(cid:4)t (x)     t (  )(cid:8)2

guaranteed learning through tensor methods

replace the objective function
max likelihood vs. best tensor decomp.

preserves global optimum (in   nite samples)

arg max

p(x;   ) = arg min

  

(cid:4)t (x): empirical tensor, t (  ): low rank tensor based on   .

  

f

(cid:8)(cid:4)t (x)     t (  )(cid:8)2

finding globally opt tensor decomposition
simple algorithms succeed under mild and natural
conditions for many learning problems.

guaranteed learning through tensor methods

replace the objective function
max likelihood vs. best tensor decomp.

preserves global optimum (in   nite samples)

arg max

p(x;   ) = arg min

  

(cid:4)t (x): empirical tensor, t (  ): low rank tensor based on   .

  

f

(cid:8)(cid:4)t (x)     t (  )(cid:8)2

finding globally opt tensor decomposition
simple algorithms succeed under mild and natural
conditions for many learning problems.

dataset 1

dataset 2

model
class

tensors vs. variational id136
criterion: perplexity = exp[   likelihood].
learning topics from pubmed on spark, 8mil articles

  104

10

e
m
t

i

i

g
n
n
n
u
r

8

6

4

2

0

105

y tensor

variational

t
i
x
e
l
p
r
e
p

104

103

f. huang, u.n. niranjan, m. hakeem, a,    online tensor methods for training latent variable models,    jmlr 2014.

tensors vs. variational id136
criterion: perplexity = exp[   likelihood].
learning topics from pubmed on spark, 8mil articles

  104

10

e
m
t

i

i

g
n
n
n
u
r

8

6

4

2

0

105

y tensor

variational

t
i
x
e
l
p
r
e
p

104

103

learning network communities from social network data
facebook n     20k, yelp n     40k, dblp-sub n     1e5, dblp n     1e6.

106

105

104

103

e
m
t

i

i

g
n
n
n
u
r

101

100

10-1

r
o
r
r
e

102

fb

yp

dblpsub dblp

10-2

fb

yp

dblpsub dblp

f. huang, u.n. niranjan, m. hakeem, a,    online tensor methods for training latent variable models,    jmlr 2014.

tensors vs. variational id136
criterion: perplexity = exp[   likelihood].
learning topics from pubmed on spark, 8mil articles

  104

10

e
m
t

i

i

g
n
n
n
u
r

8

6

4

2

0

u r a t e

c

c

a

105

y tensor

variational

m o r e

&

t
i
x
e
l
p
r
e
p

104

103

s t e r

101

100

10-1

r
o
r
r
e

learning network communities from social network data
facebook n     20k, yelp n     40k, dblp-sub n     1e5, dblp n     1e6.

a

f

e

d

n it u

g

o f m a

106

105

104

e
m
t

i

i

g
n
n
n
u
r

103

o r d

102

e r s

fb

yp

dblpsub dblp

10-2

fb

yp

dblpsub dblp

f. huang, u.n. niranjan, m. hakeem, a,    online tensor methods for training latent variable models,    jmlr 2014.

sample screenshot
infrastructure: aws 6 nodes with 36 cores each.
data: pubmed 8.1 million articles, 140k vocabulary. 50 topics.
total runtime: 27 mins.
https://github.com/furonghuang/spectrallda-tensorspark

learning representations using id106

e   cient tensor decomposition with shifted components

=

+...+

+

+...+

f1

sf1

f2

sf2

shift-invariant dictionary

dictionary

image

convolutional model

=

   

+

   

fast text embeddings through tensor methods

car accident

id27s

f. huang, a. unsupervised learning of word-sequence representations from scratch via

convolutional tensor decomposition. 2016.

fast text embeddings through tensor methods

car accident

sentence 

it   s paraphrase

id27s

sentence embeddings

f. huang, a. unsupervised learning of word-sequence representations from scratch via

convolutional tensor decomposition. 2016.

fast text embeddings through tensor methods

paraphrase detection on msr corpus with     5000 sentences

activation 
    maps

stack

one-hot encoding
         matrix

   principal 
components

projected
    data

=

=

=

* +

*

* +

*

* +

*

=

=

=

projected
    data

f. huang, a. unsupervised learning of word-sequence representations from scratch via

convolutional tensor decomposition. 2016.

fast text embeddings through tensor methods

paraphrase detection on msr corpus with     5000 sentences

method
vector similarity (baseline)
tensor (proposed)
skipthought (id56)

f score no. of samples
    4k
75%
81%     4k
    74mil
82%

unsupervised learning of embeddings.

no outside info for tensor vs. large book corpus (74 million) for
skipthought vectors.

f. huang, a. unsupervised learning of word-sequence representations from scratch via

convolutional tensor decomposition. 2016.

training neural networks with tensors

output y

neurons   (  )

input x

input x score s(x)

weights

=

+

e
e[y    s(x)]

m. janzamin, h. sedghi, and a.,    beating the perils of non-convexity: guaranteed training of

neural networks using tensor methods,    june. 2015.

training neural networks with tensors

input x score s(x)

weights

output y

neurons   (  )

input x

=

+

e
e[y    s(x)]

given input pdf p(  ), sm(x) := (   1)m    (m)p(x)
gaussian x     hermite polynomials.

p(x)

.

y
sm(x)
x

m. janzamin, h. sedghi, and a.,    beating the perils of non-convexity: guaranteed training of

neural networks using tensor methods,    june. 2015.

id23 of pomdps

learning in adaptive environments

rewards from hidden state.

actions drive hidden state
evolution.

id23 of pomdps

learning in adaptive environments

rewards from hidden state.

actions drive hidden state
evolution.

partially observable markov decision process
learning using tensor methods under memoryless policies

hi   1

xi   1

hi

xi

ri   1

ai   1

hi+1

xi+1

ri

ai

ri+1

ai+1

playing atari game

observation window

average reward vs. time.

sm-ucrl-pomdp

dnn

0.018

0.016

0.014

0.012

0.01

0.008

0.006

0.004

0.002

 

d
r
a
w
e
r
e
g
a
r
e
v
a

0

0

2

4

6

time

8

10

12
  105

pomdp model with 3 hidden states (trained using tensor methods)
vs. nn with 3 hidden layers 10 neurons each (trained using
rmsprop).

playing atari game

observation window

average reward vs. time.

sm-ucrl-pomdp

dnn

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

d
r
a
w
e
r
 
e
g
a
r
e
v
a

0

0

2

4

6

time

8

10

  10

pomdp model with 8 hidden states (trained using tensor methods)
vs. nn with 3 hidden layers 30 neurons each (trained using
rmsprop).

playing atari game

observation window

average reward vs. time.

s .

0.08

0.07

0.06

0.05

d
r
a
w
e
r
 
e
g
a
r
e
v
a

0.04

o l u ti o

0.03

0.02

d

o

m e t h

sm-ucrl-pomdp

v i a

n

o r

s

n

t e

dnn

0.01

0

0

2

4

6

time

8

10

  10

s

e t t e r

b

t o

e

c

n

e

e r g

v

n

o

pomdp model with 8 hidden states (trained using tensor methods)
s t e r
vs. nn with 3 hidden layers 30 neurons each (trained using
rmsprop).

c

a

f

outline

1

introduction

2 escaping saddle points

3 avoiding local optima

4 conclusion

guaranteed non-id76

conclusion

non-id76 requires new theoretical frameworks.
escaping saddle points an important challenge for non-convex
optimization.

(cid:2) symmetry and overspeci   cation lead to explosion of saddle points.
(cid:2) sgd can escape non-degenerate points in bounded time.
(cid:2) e   cient algorithms for escape under degeneracy.

matrix and tensor methods have desirable guarantees on reaching
global optimum.

(cid:2) applicable to unsupervised, supervised and id23.
(cid:2) polynomial computational and sample complexity.
(cid:2) faster and better performance in practice.

steps forward

how to analyze saddle point structure of well known problems, e.g.
deep learning.
scaling up tensor methods: sketching algorithms, extended blas, . . .
uni   ed conditions on when non-id76 is tractable?

resources and research connections

http://www.offconvex.org/ blog.
https://www.facebook.com/nonconvex group.

http://newport.eecs.uci.edu/anandkumar/
icml and nips workshops. (upcoming one on thursday).

resources and research connections

http://www.offconvex.org/ blog.
https://www.facebook.com/nonconvex group.

http://newport.eecs.uci.edu/anandkumar/
icml and nips workshops. (upcoming one on thursday).

collaborators

jennifer chayes, christian borgs,

prateek jain, alekh agarwal &

praneeth netrapalli (msr), srinivas

turaga (janelia), michael hawrylycz

& ed lein (allen brain), allesandro

lazaric (inria), alex smola (cmu),

rong ge (duke), daniel hsu

(columbia), sham kakade (uw),

hossein mobahi (mit).

