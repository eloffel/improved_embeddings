derivatives, id26, and vectorization

justin johnson

september 6, 2017

1 derivatives

1.1 scalar case

you are probably familiar with the concept of a derivative in the scalar case:
given a function f : r     r, the derivative of f at a point x     r is de   ned as:

f(cid:48)(x) = lim
h   0

f (x + h)     f (x)

h

derivatives are a way to measure change. in the scalar case, the derivative
of the function f at the point x tells us how much the function f changes as the
input x changes by a small amount   :

f (x +   )     f (x) +   f(cid:48)(x)

for ease of notation we will commonly assign a name to the output of f ,
say y = f (x), and write    y
   x for the derivative of y with respect to x. this
notation emphasizes that    y
   x is the rate of change between the variables x and
y; concretely if x were to change by    then y will change by approximately       y
   x .
we can write this relationship as

x     x +    x =    y        y +

   y
   x

   x

you should read this as saying    changing x to x +    x implies that y will
change to approximately y +    x    y
   x    . this notation is nonstandard, but i like
it since it emphasizes the relationship between changes in x and changes in y.
the chain rule tells us how to compute the derivative of the compositon of
functions. in the scalar case suppose that f, g : r     r and y = f (x), z = g(y);
then we can also write z = (g   f )(x), or draw the following computational graph:

f       y
the (scalar) chain rule tells us that

x

g       z

   z
   x

=

   z
   y

   y
   x

1

this equation makes intuitive sense. the derivatives    z

   y and    y

   x give:

x     x +    x =    y        y +
y     y +    y =    z        z +

   y
   x
   z
   y

   x

   y

combining these two rules lets us compute the e   ect of x on z: if x changes
   x    x. if y changes by
   y
   x    x which is exactly what the chain rule

   x    x, so we have    y =    y

by    x then y will change by    y
   y then z will change by    z
tells us.

   y    y =    z

   y

1.2 gradient: vector in, scalar out

this same intuition carries over into the vector case. now suppose that f :
rn     r takes a vector as input and produces a scalar. the derivative of f at
the point x     rn is now called the gradient, and it is de   ned as:

   xf (x) = lim
h   0

f (x + h)     f (x)

(cid:107)h(cid:107)

now the gradient    xf (x)     rn is a vector, with the same intuition as the

scalar case. if we set y = f (x) then we have the relationship

x     x +    x =    y        y +

      x

   y
   x

the formula changes a bit from the scalar case to account for the fact that
   x are now vectors in rn while y is a scalar. in particular when
   x by    x we use the dot product, which combines two vectors to

x,    x, and    y
multiplying    y
give a scalar.

one nice outcome of this formula is that it gives meaning to the individual
elements of the gradient    y
   x . suppose that    x is the ith basis vector, so that
the ith coordinate of    is 1 and all other coordinates of    are 0. then the dot
product    y
   x ; thus the ith coordinate of
   y
   x tells us the approximate amount by which y will change if we move x along
the ith coordinate axis.

   x       x is simply the ith coordinate of    y

this means that we can also view the gradient    y

   x as a vector of partial

derivatives:

   y
   x

=

(cid:18)    y

,

   y
   x2

, . . . ,

   y
   xn

   x1

(cid:19)

where xi is the ith coordinate of the vector x, which is a scalar, so each

partial derivative    y
   xi

is also a scalar.

2

1.3 jacobian: vector in, vector out
now suppose that f : rn     rm takes a vector as input and produces a vector
as output. then the derivative of f at a point x, also called the jacobian, is
the m    n matrix of partial derivatives. if we again set y = f (x) then we can
write:

             y1

   x1

...

   ym
   x1

         

      
. . .
      

   y1
   xn

...

   ym
   xn

   y
   x

=

the jacobian tells us the relationship between each element of x and each
, so it tells us the amount

element of y: the (i, j)-th element of    y
by which yi will change if xj is changed by a small amount.

   x is equal to    yi
   xj

just as in the previous cases, the jacobian tells us the relationship between

changes in the input and changes in the output:

x     x +    x =    y        y +

   y
   x

   x

   x is a m    n matrix and    x is an n -dimensional vector, so the
   x    x is a matrix-vector multiplication resulting in an m -dimensional

here    y
product    y
vector.
the chain rule can be extended to the vector case using jacobian matrices.
suppose that f : rn     rm and g : rm     rk. let x     rn , y     rm , and
z     rk with y = f (x) and z = g(y), so we have the same computational graph
as the scalar case:

the chain rule also has the same form as the scalar case:

f       y

g       z

x

   z
   x

=

   z
   y

   y
   x

however now each of these terms is a matrix:    z

   x is a k    n matrix; the multiplication of    z

   y is a k    m matrix,    y
   y and    y

   x is
   x is

a m    n matrix, and    z
id127.

3

1.4 generalized jacobian: tensor in, tensor out

just as a vector is a one-dimensional list of numbers and a matrix is a two-
dimensional grid of numbers, a tensor is a d-dimensional grid of numbers1.

many operations in deep learning accept tensors as inputs and produce
tensors as outputs. for example an image is usually represented as a three-
dimensional grid of numbers, where the three dimensions correspond to the
height, width, and color channels (red, green, blue) of the image. we must
therefore develop a derivative that is compatible with functions operating on
general tensors.
suppose now that f : rn1          ndx     rm1          mdy . then the input to f
is a dx-dimensional tensor of shape n1              ndx , and the output of f is a
dy-dimensional tensor of shape m1           mdy . if y = f (x) then the derivative
   y
   x is a generalized jacobian, which is an object with shape
(m1              mdy )    (n1              ndx )

note that we have separated the dimensions of    y

   x into two groups: the
   rst group matches the dimensions of y and the second group matches the
dimensions of x. with this grouping, we can think of the generalized jacobian
as generalization of a matrix, where each    row    has the same shape as y and
each    column    has the same shape as x.
now if we let i     zdy and j     zdx be vectors of integer indices, then we

can write

(cid:18)    y

(cid:19)

=

   yi
   xj

   x

i,j

in this equation note that yi and xj are scalars, so the derivative    yi
   xj

is
also a scalar. using this notation we see that like the standard jacobian, the
generalized jacobian tells us the relative rates of change between all elements
of x and all elements of y.

the generalized jacobian gives the same relationship between inputs and

outputs as before:

x     x +    x =    y        y +

   y
   x

   x

the di   erence is that now    x is a tensor of shape n1              ndx and    y
is a generalized matrix of shape (m1              mdy )    (n1              ndx ). the
product    y
   x    x is therefore a generalized matrix-vector multiply, which results in
a tensor of shape m1              mdy .

   x

the generalized matrix-vector multipy follows the same algebraic rules as a

traditional matrix-vector multiply:

1the word tensor is used in di   erent ways in di   erent    elds; you may have seen the term
before in physics or abstract algebra. the machine learning de   nition of a tensor as a d-
dimensional grid of numbers is closely related to the de   nitions of tensors in these other
   elds.

4

(cid:18)    y

   x

(cid:19)

   x

=

(cid:18)    y

(cid:19)

(cid:88)

j

i

   x

i,j

(   x)i =

(cid:18)    y
(cid:19)
(cid:16)    y

   x

j,:

(cid:17)

      x

   x

j,:

the only di   erence is that the indicies i and j are not scalars; instead they
is the jth    row   

are vectors of indicies. in the equation above the term

of the generalized matrix    y
   x , which is a tensor with the same shape as x. we
have also used the convention that the dot product between two tensors of the
same shape is an elementwise product followed by a sum, identical to the dot
product between vectors.

the chain rule also looks the same in the case of tensor-valued functions.
suppose that y = f (x) and z = g(y), where x and y have the same shapes as
above and z has shape k1              kdz . now the chain rule looks the same as
before:

   z
   x

=

   z
   y

   y
   x

   y is a generalized matrix of shape (k1          kdz )  
the di   erence is that now    z
(m1              mdy ), and    y
   z is a generalized matrix of shape (m1              mdy )   
(n1              ndx); the product    z
   y
   x is a generalized matrix-matrix multiply,
resulting in an object of shape (k1              kdz )    (n1              ndx ). like
the generalized matrix-vector multiply de   ned above, the generalized matrix-
matrix multiply follows the same algebraic rules as the traditional matrix-matrix

   y

(cid:18)    z

(cid:19)

(cid:88)

(cid:18)    y

(cid:19)

(cid:18)    z

(cid:19)

=

(cid:18)    y

(cid:19)

  

   y

i,k

   x

k,j

   y

i,:

   x

:,j

=

i,j

k

in this equation the indices i, j, k are vectors of indices, and the terms

are the ith    row    of    z

   y and the jth    column    of    y

i,:
   x respectively.

(cid:19)

multiply: (cid:18)    z
(cid:16)    y

(cid:17)

   x

and

   x

:,j

(cid:16)    z

(cid:17)

   y

2 id26 with tensors

in the context of neural networks, a layer f is typically a function of (tensor)
inputs x and weights w; the (tensor) output of the layer is then y = f (x, w).
the layer f is typically embedded in some large neural network with scalar loss
l.

during id26, we assume that we are given    l

   y and our goal is to

compute    l

   x and    l

   w . by the chain rule we know that

   l
   x

=

   l
   y

   y
   x

   l
   w

=

   l
   y

   y
   w

therefore one way to proceed would be to form the (generalized) jacobians
   x and    l
   w .

   w and use (generalized) id127 to compute    l

   x and    y

   y

5

however, there   s a problem with this approach: the jacobian matrices    y
   y
   w are typically far too large to    t in memory.

   x and

the jacobian    y

as a concrete example, let   s suppose that f is a linear layer that takes as
input a minibatch of n vectors, each of dimension d, and produces a minibatch
of n vectors, each of dimension m . then x is a matrix of shape n    d, w is a
matrix of shape d    m , and y = f (x, w) = xw is a matrix of shape n    m .
   x then has shape (n    m )    (n    d). in a typical neural
network we might have n = 64 and m = d = 4096; then    y
   x consists of
64    4096    64    4096 scalar values; this is more than 68 billion numbers; using
32-bit    oating point, this jacobian matrix will take 256 gb of memory to store.
therefore it is completely hopeless to try and explicitly store and manipulate
the jacobian matrix.

however it turns out that for most common neural network layers, we can
   l
   y without explicitly forming
   x . even better, we can typically derive this expression without
   x ; in many cases we

derive expressions that compute the product    y
   x
the jacobian    y
even computing an explicit expression for the jacobian    y
can work out a small case on paper and then infer the general formula.

let   s see how this works out for the case of the linear layer f (x, w) = xw.

set n = 1, d = 2, m = 3. then we can explicitly write

y1,3

y1,2

(cid:1)(cid:18)w1,1 w1,2 w1,3

y =(cid:0)y1,1
(cid:1) = xw
=(cid:0)x1,1 x1,2
      t
      x1,1w1,1 + x1,2w2,1

w2,1 w2,2 w2,3

x1,1w1,2 + x1,2w2,2
x1,1w1,3 + x1,2w2,3

=

(cid:19)

(1)

(2)

(3)

during id26 we assume that we have access to    l
   y which tech-
nically has shape (1)    (n    m ); however for notational convenience we will
instead think of it as a matrix of shape n    m . then we can write

=(cid:0)dy1,1

   l
   y

(cid:1)

dy1,2

dy1,3

our goal now is to derive an expression for    l

   x in terms of x, w, and    l
   y ,
without explicitly forming the entire jacobian    y
   x . we know that    l
   x will have
shape (1)   (n    d), but as is typical for representing gradients we instead view
   x as a matrix of shape n    d. we know that each element of    l
   x is a scalar

   l

giving the partial derivatives of l with respect to the elements of x:

(cid:16)    l

   x1,1

(cid:17)

   l

   x1,2

   l
   x

=

thinking one element at a time, the chain rule tells us that

6

   l
   x1,1
   l
   x1,2

=

=

   l
   y
   l
   y

   y

   x1,1

   y

   x1,2

(4)

(5)

viewing these derivatives as generalized matrices,    l

has shape (n  m )  (1); their product    l

and    y
   x1,1
we instead view    l
matrix product is simply the dot product    l

   y and    y
   x1,1

   y has shape (1)  (n  m )
then has shape (1)  (1). if
as matrices of shape n   m , then their generalized

   x1,1

now we compute

   y

   x1,1

   y

   x1,2

=

=

(cid:16)    y1,1
(cid:16)    y1,1

   x1,1

   x1,2

.

   x1,1

   y       y
(cid:17)
(cid:17)

=(cid:0)w1,1 w1,2 w1,3
=(cid:0)w2,1 w2,2 w2,3

(cid:1)
(cid:1)

(6)

(7)

   y1,2
   x1,1

   y1,2
   x1,2

   y1,3
   x1,1

   y1,3
   x1,2

where the    nal equality comes from taking the derivatives of equation 3 with
respect to x1,1.

we can now combine these results and write

   l
   x1,1
   l
   x1,2

=

=

   l
   y
   l
   y

      y
   x1,1
      y
   x1,2

= dy1,1w1,1 + dy1,2w1,2 + dy1,3w1,3

= dy1,1w2,1 + dy1,2w2,2 + dy1,3w2,3

this gives us our    nal expression for    l
   x :

(cid:17)

(cid:16)    l
(cid:18)dy1,1w1,1 + dy1,2w1,2 + dy1,3w1,3

   x1,1

   x1,2

   l

dy1,1w2,1 + dy1,2w2,2 + dy1,3w2,3

   l
   y

xt

(cid:19)t

   l
   x

=

=

=

(8)

(9)

(10)

(11)

(12)

this    nal result    l
   y xt is very interesting because it allows us to
e   ciently compute    l
   x . we have
only derived this formula for the speci   c case of n = 1, d = 2, m = 3 but it in
fact holds in general.

   x =    l
   x without explicitly forming the jacobian    y

by a similar thought process we can derive a similar expression for    l

   w with-
   w . you should try and work through this

out explicitly forming the jacobian    y
as an exercise.

7

