lecture notes:

some notes on id119

marc toussaint

machine learning & robotics lab, fu berlin

arnimallee 7, 14195 berlin, germany

may 3, 2012

i   ll brie   y cover the following topics about gradient de-

scent:

       steepest descent   
    how the    size    of the gradient might be misleading.

this leads to methods for stepsize adaptation.
    how to guarantee monotonous convergence.
    reconsideration of what    steepest descent    should
mean in the case of a non-euclidean metric. this
leads to the so-called covariant or natural gradient.
    a brief comment on co- and contra-variance (of the

gradient) under linear transformations.

    the relation of the covariant gradient to the newton

method.
    rprop.
simple id119 is a very    handy    method for
optimization. that is, while id119 is often not
the most ef   cient method, it is an absolutely essential
tool to prototype optimization algorithms and for pre-
liminary testing of models. once the model formulation
is stable one might want to invest more in considering
better optimization methods, especially 2nd order meth-
ods, iterated line search, existing professional optimizers
(e.g. snopt), mathematical programming (e.g. cplex),
etc. nonetheless, in these notes i cover some points that
lead to simple yet ef   cient variants of id119,
very useful for prototyping. points i am not covering but
should be on the list is:

    stochastic id119
    iterative line search
    many more...

1 id119
given a scalar function f (x) with x     rn. we want to
   nd its minimum

min

x

f (x) .

(1)

figure 1: illustration of steepest descent.

the gradient    f (x)
   x at location x points toward a direction
where the function increases. the negative        f (x)
   x is usu-
ally called steepest descent direction   in section 3 we will
discuss in which sense this really is the steepest descent
direction and in section 4 whether    f (x)
really is a    vec-
   x
tor   .

the plain    id119 method    to    nd the min-
imum of f (x) starts from an initial point x0, then iter-
atively takes a step along the steepest descent direction
(optionally scaled by a stepsize), until convergence. the
algorithm and an illustration are given in figure 1.

algorithm 1 plain id119
input:

starting point x     rn, a function    f (x)

   x , stepsize

  , tolerance   

output: some x hopefully minimizing f
1: repeat
2:
3: until    x <    for 10 iterations in sequence

(cid:62)
x     x           f (x)

   x

lecture notes:
some notes on id119, marc toussaint   may 3, 2012

2

rection of the gradient is ok, but the size questionable.
therefore, robust gradient methods must permanently
rescale the stepsize empirically depending on local prop-
erties of the function. there are two simple heuristics:

    when you   ve done a step and the function value in-
creases, your step was too large; undo the step and
decrease the stepsize.

    when you   ve done a successful step and the function
value decreases, perhaps your step could have been
larger; increase the stepsize.

algorithm 2 presents one version of such a robust step-
size adaptation.

the    undo    in the    rst heuristic might seem overly
conservative and a waste   one is tempted to not fully
undo the step but do something more heuristic (half the
step, etc, etc). in my experience this ends up in overly
complicated and eventually non-robust hacks. the undo
step has the great advantage of guaranteeing monotonic-
ity! which is great! never underestimate the relieve of
a programmer (esp. in a prototyping context) of having a
method that is really guaranteed to converge.

3 steepest descent and the covariant

[natural] gradient

the origin of the word    covariant    will be explained in
the next section   here let   s simply ask: what is really the
direction of steepest?

consider the following de   nition:

de   nition 1 (really steepest descent). given f (x) and
a point x0. de   ne b = {x     rn : d(x, x0) =  } as
the set of points with distance   to x0. here we pre-
sume the existence of a distance function d(x, x0). let
x    = argminx   b f (x) be the point with smallest f-value
and distance   to x0. as the steepest descent direction we
de   ne the direction from x0 to x   , that is, (x        x0)/  in
the limit       0.

i think that de   nition is very reasonable. it makes ex-
plicit that it is    fair    to compare the function decrease
along different directions only when we walk the same
distance along these directions. note that the partial gra-
dient    f (x)
   x does not acknowledge any notion of distance
in the space   how can it make a statement about steepest
descent then?

let us assume that the distance is (at least locally...)

given in terms of a metric a:

d(x, x0) = (x     x0)(cid:62)a(x     x0)

(2)

since, by de   nition of the partial gradient g(cid:62) =    f (x0)
function f (x) can be locally approximated as

   x , the

f (x0 +   )   =f (x0) + g(cid:62)  

(3)

figure 2: should a small gradient really imply a small
stepsize? and a large gradient a large stepsize? no! in-
stead robust online stepsize adaptation is necessary. al-
gorithm 2 is one version;    determines the absolute step-
size since g/|g| is normalized.

2 the stepsize issue and monotonic-

ity

   x

(cid:12)(cid:12)    f (x)

the    rst pitfall with id119 is the stepsize,
which in algorithm 1 is proportional to the gradient size

(cid:12)(cid:12). first, be aware that the scaling of the x-axis is re-

ally arbitrary. ideally an optimization algorithm should
be invariant under rescaling of the x-axis. the same
holds for the y-axis. the plain gradient    f (x)
is of course
   x
not rescaling/transformation invariant (detailed in sec-
tion 4). furthermore, functions may have very differ-
ent characteristics in different regions: consider the func-
tion in figure 2; while it is near    at (=small gradient) in
one region, it is very steep (=large gradient) in other re-
gions. should a small (large) gradient really always im-
ply a small (large) step?

the more one thinks about such examples, the more
one might come to the following conclusion: never trust
(interpret too much into) the size of the gradient. the di-

algorithm 2 monotonous id119 with stepsize
adaptation
input:

starting point x     rn, a scalar function f (x) and

its gradient    f (x)

   x , initial stepsize   , tolerance   

output: some x hopefully minimizing f
1: initialize fx = f (x)     r, g =    f (x)
2: repeat
3:
4:
5:

y     x        g/|g| ,
if fy < fx then

fy     f (y)

   x

(cid:62)     rn

x     y ,
fx     fy
(cid:62)
g        f (x)
a     1.2a

   x

else

6:
7:
8:
9:
10:
11: until |y     x| <    for 10 iterations in sequence

a     0.5a

end if

// increase stepsize

// decrease stepsize

large gradient     large step?small gradient     small step?lecture notes:
some notes on id119, marc toussaint   may 3, 2012
the x        b with minimal f-value and distance   to x0 is
given as

x        x0 = argmin

(cid:104)

  

s.t.

g(cid:62)  

  (cid:62)a   =  2
let a =  2b(cid:62)b and z = b  
z(cid:62)z = 1

  (cid:62)g

s.t.

(cid:105)

= argmin

  

= b-1 argmin

z

(b-1z)(cid:62)g
z(cid:62)b-(cid:62)g

s.t.

s.t.

z(cid:62)z = 1
z(cid:62)z = 1

= b-1 argmin
    b-1[   b-(cid:62)g]        a-1g

z

(4)

(5)

(6)

(7)

(8)

(9)

(the b-1 in front of the argmin in equation (7) is because
the return value of argminz has to be transformed to be-
come a   !!) this tells us that the    real    steepest descent di-
rection is    a-1g (we neglected constants because, as dis-
cussed, one should not trust the gradient size anyway.)

in conclusion, if a metric a is given, the covariant gra-

dient descent is given as

   x        a-1g .

(10)

the next section will give an intuitive example. the co-
variant gradient is also called natural gradient     especially
in the context when x describes a id203 distribution
and one uses the so-called fisher metric in the space of
distributions (literature: amari).

4

invariance under transformation:
the gradient is a covariant thing

consider the following example: we have a function f (x)
over a two dimensional space, x     r2. we express the
coordinates in this space as x = (x1, x2), and let   s sim-
ply assume f (x) = x1 + x2. the function   s gradient is of
course    f

   x = (1 1).

now let   s transform the coordinates of the space: we
introduce new coordinates (z1, z2) = (2x1, x2) or z = bx
with b =

            . the same function, written in the new

            2 0

0 1

coordinates, is f (z) = 1
function, written in these new coordinates, is    f

2 z1 + z2. the gradient of that same
2 1).

   z = ( 1

let   s summarize what happened:
    we have a transformation of a vector space described

by a coordinate transformation matrix b.

    coordinate vectors of course transform as z = bx.
    however, the partial gradient of a function w.r.t. the

coordinates transforms as    f
   z

(cid:62)

= b-(cid:62)    f

   x

(cid:62)
.

    therefore, there seems to exist one type of mathe-
matical objects (e.g. coordinate vectors) which trans-
form with b, and a second type of mathematical ob-
jects (e.g. the partial gradient of a function w.r.t. co-
ordinates) which transform with b-(cid:62).

3

these two types are called contra-variant and co-variant.
this should at least tell us that indeed the so-called
   gradient-vector    is somewhat different to a    normal
vector   : it behaves inversely under coordinate transfor-
mations.
ordinary id119 of the form x     x +       f
   x
adds objects of different types: a contra-variant coordi-
nate vector x with a co-variant coordinate gradient    f
   x .
clearly, adding two such different types leads to an ob-
ject who   s transformation under coordinate transforms is
strange   and indeed the ordinary id119 is not
invariant under transformations.

so why is the covariant id119 called co-
variant? let   s check how a-1g transforms under a linear
transformation b: as we showed, the new    vector    of
partial coordinate derivatives will be g(cid:48) = b-(cid:62)g. the new
metric matrix in the new coordinate system will be a(cid:48) =
b-(cid:62)ab-1 (a metric matrix is doubly co-variant, which we
didn   t show). therefore the new covariant gradient in the
new coordinates is a(cid:48)-1g(cid:48) = (b-(cid:62)ab-1)-1b-(cid:62)g = b(a-1g),
which is the old covariant gradient in the old coordinate
simply transformed forward as a normal vector. there-
fore the inverse metric converts the co-variant gradient to
become contra-variant. in that sense, the covariant gradi-
ent is    the only sensible thing to do    from the view of
correct behavior under coordinate transforms.

in the remainder,

just for the fun, i   ll try to give a
slightly more detailed explanation of the meaning of co-
and contra-variant.

suppose we have two (mathematical) objects and if we
multiply them together this gives a scalar. the scalar
shouldn   t depend on any choice of coordinate system
and is therefore invariant against coordinate transforms.
then, if one of the objects transforms in a co-variant
(   transforming with the transformation   ) manner, the
other object must transform in a contra-variant (   trans-
forming contrary to the transformation   ) manner to en-
sure that the resulting scalar is invariant. this is a gen-
eral principle: whenever two things multiply together to
give a scalar, one should transform co- the other contra-
variant.

the gradient    f

   x of a function can be de   ned as an ob-
ject (a so-called 1-form) that multiplies to a point varia-
tion   x to give the function variation   f =    f
   x   x. there-
fore, if coordinate vectors are contra-variant, the gradient
must be co-variant. normal (contra-variant) coordinate
vectors are written as columns. they multiply with row
vectors to give a scalar; row vectors are co-variant.1
it

1there is more confusion to add: a vector itself (as a coordinate-free
algebraic object) is a co-variant object and should be distinguished from
its coordinate vector (given a basis), which is a contra-variant object
(it must be because coordinate coef   cients multiply to basis vectors).
a coordinate-free gradient itself (a 1-form) is a contra-variant object
and should be distinguished from its coordinate representation (given
a basis), which is a co-variant object. a metric (or 2-form) is    double-
contravariant    since it multiplies to two vectors to give a scalar; its in-
verse is    double-covariant   ; and again, moving from algebraic objects
to coordinate representations this is    ipped.

4

lecture notes:
some notes on id119, marc toussaint   may 3, 2012

therefore makes sense to write a gradient as a row vector.
however, note that row/column notations are just a con-
vention anyway   and the wast majority of researchers
have the convention to write gradients also as column
vector.

5 relation between covariant gradi-

ent and the id77

the id77 computes the 2nd order approxima-
tion (called 2nd order taylor expansion) of the function:

f (x0 +   )     f (x0) +

   f (x0)

   x

   +   (cid:62)h  

(11)

algorithm 3 an rprop variance (irprop   , igel, hsken)
input:
   x , starting point x0, initial

functions f (x) and    f (x)

stepsize   , tolerance   

   x

if gig(cid:48)

i > 0 then

(cid:62)
g        f (x)
x(cid:48)     x
for i = 1 : n do
  i     1.2  i
else if gig(cid:48)
  i     0.5  i
gi     0

output: some x hopefully minimizing f
1: initialize x = x0, all   i =   , all g(cid:48)
i = 0
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: until |x(cid:48)     x| <    for 10 iterations in sequence

end if
optionally: cap   i     [  min,   max]
xi     xi       i sign(gi)

end for
g(cid:48)     g

// force gig(cid:48)

i < 0 then

// same direction as last time

// change of direction

i = 0 in next iteration

// store the gradient for next iteration

the matrix h is called hessian and, intuitively, describes
the local n-dimensional parabola curvature that approxi-
mates f. if h is positive-de   nite, the parabola is indeed
positively curved, meaning that it goes up in all direc-
tions.
(would h have a negative eigenvalue it would
describe a saddle-like function, going down along the
eigen-vectors.) if h is positive-de   nite the parabola has
a de   nite minimum

(cid:62)
      =    h-1    f (x0)

   x

.

(12)

the id77 iterates exactly this step: it com-
putes a local 2nd order approximation of f, jumps to the
minimum of this parabola, and iterates from there.

this is exactly the same as covariant id119,
but with the local hessian h replacing the metric a. a
difference is that the hessian h of a function is local and
typically different for any point in space, whereas the
metric is usually assumed constant throughout the space.

6 rprop

rprop stands for    resilient back propagation   ; where
   back propagation    refers to traditional methods of train-
ing neural networks and    resilient   , in my interpretation,
for    robustness by undoing steps   .
(literature: ried-
miller, igel, hsken.) at    rst sight the algorithm might
seem a terrible hack   but it is simple, robust and ef   cient.
it takes the heuristic of    don   t trust the gradient size   
to an extreme and maintains a separate online-adapted
stepsize for each input dimension. by rescaling the gradi-
ent in each dimension separately it also ignores the pre-
cise gradient direction   which could be justi   ed by not
knowing the true underlying metric: instead of assum-
ing a known metric and using it for a covariant gradient
method, it heuristically and locally rescales each dimen-
sion via an online stepsize adaptation. in fact, rprop is
fully invariant against separate linear transformations of
each dimension (    a diagonal metric) and, empirically,
also pretty robust against any linear transformation of
the problem. it could therefore be interpreted as a (very
heuristic) attempt to mimic covariance id119.

