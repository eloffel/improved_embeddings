neural network for 
machine learning

                 | jahan@nvidia.com
                 | hryu@nvidia.com /                  | hanbyuly@nvidia.com

1

agenda

background of neural network

scoring / loss / id166 / softmax / id173

neural network architecture

optimization / training

id158

activation function

2

deep learning

learning representation/features

the traditional model of pattern recognition (since the late 50's)
fixed/engineered features (or fixed kernel) + trainable  classifier

hand-crafted  
feature extractor

   simple    trainable  

classifier

end-to-end learning /  id171 /  deep learning

trainable features (or kernel) + trainable classifier

trainable 

feature extractor

trainable 
classifier

3

this basic model has not evolved much since the 50's

    the first learning machine: the id88

   

built at cornell in 1960

    the id88 was a linear classifier on top of a  simple 

feature extractor

    the vast majority of practical applications of ml today 

use glorified linear classifiers or glorified template 
matching.

    designing a feature extractor requires  considerable 

efforts by experts.

t

a f

e
a
u
r
e
e
x
t
r
a
c
t
o
r

wi

n

)
y = sign    i= 1

w i f i (x )+ b)

4

scoring

5

summary

6

scoring

id136

        #,    ,     =        #+    

input data

class score

weights

bias

   parameters   

7

a linear classifier for image

8

interpretation

9

the bias trick

10

define a loss/cost/objective function

we have scores and labels:

example:

    #

we want to define a id168         that describes how unhappy we are with 
these scores.

11

loss example: the multiclass id166 loss

12

loss example: the multiclass id166 loss

13

loss example: the multiclass id166 loss

vectorized implementation

14

what does the id168 look like?

example: 3 examples with 3 classes

15

what does the id168 look like?

16

softmax classifier

extension of id28 to multiple classes

17

example

18

id173

penalizing extreme weights

19

id173 parameter

20

summary

21

summary

22

neural network architecture and

optimization

23

neural network architecture

id158

input 
layer

hidden 
layer

output 
layer

neuron

synapse

24

id136

0.5

0.9

-0.3

h1

h2

h3

o1

o2

h1 weights = (1.0, -2.0, 2.0)
h2 weights = (2.0, 1.0, -4.0)
h3 weights = (1.0, -1.0, 0.0)

o1 weights = (-3.0, 1.0, -3.0)
o2 weights = (0.0, 1.0, 2.0)

input feed-forward to outputs

25

id136

0.5

0.9

-0.3

0.13

0.96

0.40

o1

o2

h1 weights = (1.0, -2.0, 2.0)
h2 weights = (2.0, 1.0, -4.0)
h3 weights = (1.0, -1.0, 0.0)

o1 weights = (-3.0, 1.0, -3.0)
o2 weights = (0.0, 1.0, 2.0)

h1 = s(0.5 * 1.0 + 0.9 * -2.0 + -0.3 * 2.0) = s(-1.9) = .13
h2 = s(0.5 * 2.0 + 0.9 * 1.0 + -0.3 * -4.0) = s(3.1) = .96
h3 = s(0.5 * 1.0 + 0.9 * -1.0 + -0.3 * 0.0) = s(-0.4) = .40

26

id136

0.5

0.9

-0.3

0.13

0.96

0.40

0.35

0.85

h1 weights = (1.0, -2.0, 2.0)
h2 weights = (2.0, 1.0, -4.0)
h3 weights = (1.0, -1.0, 0.0)

o1 weights = (-3.0, 1.0, -3.0)
o2 weights = (0.0, 1.0, 2.0)

o1 = s(.13 * -3.0 + .96 * 1.0 + .40 * -3.0) = s(-.63) = .35
o2 = s(.13 * 0.0 + .96 * 1.0 + .40 * 2.0) = s(1.76) = .85

27

matrix formulation

h1 weights = (1.0, -2.0, 2.0)
h2 weights = (2.0, 1.0, -4.0)
h3 weights = (1.0, -1.0, 0.0)

hidden layer weights

inputs

s(

1.0

2.0

1.0

-2.0

1.0

-1.0

2.0

-4.0

0.0

*

0.5

0.9

-0.3

) = s(

-1.9

3.1

-0.4

) = 

.13

.96

0.4

hidden layer outputs

28

id26

    problem: which weights should be updated and by how much?

insight: use the derivative of the error with respect to weight to assign 
   blame   

   

i1

i2

i3

h1

h2

h3

o1

o2

backward to input to update parameters with loss

29

id26 example

o

t (ground truth)

i

w

.35

0.9

.85

0.5

0.9

-0.3

.13

.96

.40

30

id119

    id119 minimizes the neural network   s error

    at each time step the error of the network is calculated on the training data
    then the weights are modified to reduce the error

    id119 terminates when 

    the error is sufficiently small
    the max number of time steps has been exceeded

31

id119
getting to a minimum

the derivative of a function in 1 dimension is:

in n dimensions,  the gradient is the vector of (partial) derivatives.

32

id119

33

dynamics of id119

intuition

id166

softmax

34

gradient intuition

if i increase x by h, how would the input of f change?

35

forward pass and id26 intuition

3

36

summary

input feed-forward to outputs

i1

i2

i3

h1

h2

h3

o1

o2

backward to input to update parameters with loss

37

id158

38

ann architecture
fully connected layers

2-layer neural nets or 
1-hidden neural nets

39

ann architecture

trainable parameters

number of neurons: 4 + 2 = 6
number of weights: [4x3 + 2x4] = 20
number of parameters: 6 + 20 = 26

40

ann architecture

trainable parameters

modern id98s: ~10 million neurons
human visual cortex: ~5 billion neurons

41

the artificial neuron

inputs

neuron

f()

output

42

the artificial neuron

inputs are weighted before

the function is applied

w1x1

neuron

f()

y

output

w2x2

inputs

x1

x2

43

the artificial neuron

inputs are weighted before

the function is applied

w1x1

neuron

f()

y

output

w2x2

inputs

x1

x2

44

biological inspiration

inputs

output

45

the artificial neuron

more detailed view

activation function f

inputs

x1

x2

x3

w1x1

w2x2

w3x3

b

f xi

wixi + b!

y

wixi + b f

xi

46

the artificial neuron

more detailed view

activation function f

inputs

x1

x2

x3

w1x1

w2x2

w3x3

b

f xi

wixi + b!

y

wixi + b f

xi

example python 
implementation:

47

id180

id180 are applied 
to the inputs at each neuron
a range of non-linear functions 
have been used
non-linearity is important 
because a multi-layer ann with 
linear activations would still 
only be able to learn composite 
linear functions

48

id180

sigmoid

squashes numbers to the range 
[0,1]
historically popular, but they 
have big practical problems that 
prevent network training

49

id180

hyperbolic tangent

squashes numbers to range [-1,1]

zero-centered is good

still have practical issues

50

id180

rectified linear units

f (x) = max(0, x)

most popular in current dl work
computationally efficient, relu
neurons can be trained much 
faster
addresses the practical issues of 
sigmoid and tanh
neurons can    die    during 
training 

51

id180

leaky relu

computationally efficient
will not    die    like relu

52

id180

in practice

use relu
try out leaky relu
you can try out tanh but it probably won   t work as 
well as relu
never use sigmoid

53

training neural networks

    procedure for training neural networks
perform id136 on the training set

   
    calculate the error between the predictions and actual labels of the training set
    determine the contribution of each neuron to the error
    modify the weights of the neural network to minimize the error

    error contributions are calculated using id26
    error minimization is achieved with id119

54

setting the number or layers and their size

overfitting, generalization and outliers

55

effect of id173

overfitting, generalization and outliers

56

network size and overfitting

takeaway:
    don   t be afraid of overfitting: use a network as big as you can train
    use id173 to control overfitting

57

vectorized feed-forward computation

feed forward between two fully connected layers as a matrix-vector multiplication 
(followed by activation)
this is very good for gpu acceleration

58

example feed-forward computation of network

59

conclusion

60

anns in deep learning

traditional machine perception     hand tuned features

raw data

feature extraction

classifier/
detector

result

e.g. id166  
neural net,

   

61

anns in deep learning
deep learning machine perception

raw data

feature extraction

classifier/
detector

result

62

hyperparameter

you will see every point saying    it depends   

63

64

