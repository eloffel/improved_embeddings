cs294-129: designing, visualizing and 
understanding deep neural networks 

john canny 

fall 2016 
cn: 34652 

with considerable help from andrej 

karpathy, fei-fei li, justin johnson, ian 

goodfellow and several others 

deep learning: hype or hope?  

deep learning: hype or hope?  

hype: (n)    extravagant or intensive publicity or promotion    

 

 

 

 

hope: (n)    expectation of fulfillment or success    

milestones: digit recognition 

lenet 1989: recognize zip codes, yann lecun, bernhard 
boser and others, ran live in us postal service 

milestones: image classification 

convolutional nns: alexnet (2012): trained on 200 gb of 
id163 data 

 

 

 

 

human performance 
5.1% error 

milestones: id103 

recurrent nets: lstms (1997): 

milestones: language translation 

sequence-to-sequence models with lstms and attention: 

 

 

 

 

 

 

 

 

source luong, cho, manning acl tutorial 2016.  

 milestones: deep id23 

8 

in 2013, deep mind   s  arcade player bests human expert 
on six atari games. acquired by google in 2014,. 

 

 

 

 

 

 

in 2016, deep mind   s  

alphago defeats former  
world champion lee sedol   

deep learning: is it hype or hope?  

deep learning: is it hype or hope?  

yes ! 

other deep learning courses at berkeley: 

11 

fall 2016: 

 

cs294-43: special topics in deep learning: trevor darrell, 
alexei efros, marcus rohrbach, mondays 10-12 in newton 
room (250 sdh). emphasis on id161 research. 
readings, presentations. 

 

cs294-yy: special topics in deep learning: trevor 
darrell, sergey levine, and dawn song, weds 10-12, 
https://people.eecs.berkeley.edu/~dawnsong/cs294-dl.html 
emphasis on security and other emerging applications.  

readings, presentations, project.  

 

other deep learning courses at berkeley: 

12 

spring xx: 

 

cs294-yy: deep id23, john shulman 
and sergey levine, http://rll.berkeley.edu/deeprlcourse/. 
should happen in spring 2017. broad/introductory 
coverage, assignments, project.  

 

stat 212b: topics in deep learning: joan bruna 
(statistics), last offered spring 2016. next ?? 

paper reviews and course project. 

 

learning about deep neural networks 

13 

yann lecun quote: dnns require:    an interplay between 
intuitive insights, theoretical modeling, practical 
implementations, empirical studies, and scientific analyses    

 

i.e. there isn   t a framework or core set of principles to 
explain everything (c.f. id114 for machine 
learning). 

 

we try to cover the ground in lecun   s quote.   

 

this course (please interrupt with questions) 

14 

goals: 

    introduce deep learning to a broad audience. 

    review principles and techniques (incl. visualization) for 

understanding deep networks. 

    develop skill at designing networks for applications. 

 

materials: 

    book(s) 

    notes 

    lectures 

    visualizations 

 

the role of animation 

15 

 

 

 

 

 

 

 

 

 

from a. karpathy   s cs231n notes. 

this course 

16 

work: 

    class participation: 10% 

    midterm: 20% 

    final project (in groups): 40% 

    assignments : 30% 

 

audience: eecs grads and undergrads.  

in the long run will probably be    normalized    to a 100/200-
course numbered    mezzanine    offering.  

 

 

prerequisites 

17 

    knowledge of calculus and id202, math 53/54 or 

equivalent.  

 

    id203 and statistics, cs70 or stat 134. cs70 is bare 

minimum preparation, a stat course is better. 

 

    machine learning, cs189. 

 

    programming, cs61b or equivalent. assignments will 

mostly use python.  

logistics 

18 

    course number: cs 294-129 fall 2016, uc berkeley 

    on bcourses, publicly readable. 

    ccn: 34652  

    instructor: john canny lastname@berkeley.edu  

    time: mw 1pm - 2:30pm 

    location: 306 soda hall 

    discussion: join piazza for announcements and to ask 

questions about the course 

    office hours:  

    john canny - m 2:30-3:30, in 637 soda 

 

 

course project 

    more info later 

 

    can be combined with other course projects 

 

    encourage    open-source    projects that can be archived 

somewhere.  

 

outline     basics/id161 

 

outline     applications/advanced topics 

 

some history 

    reading: the deep learning book, introduction 

phases of neural network research 

    1940s-1960s: cybernetics: brain like electronic systems, 

morphed into modern control theory and signal processing. 

    1960s-1980s: digital computers, automata theory, 

computational complexity theory: simple shallow circuits are 
very limited    

    1980s-1990s: connectionism: complex, non-linear networks, 

back-propagation. 

    1990s-2010s: computational learning theory, graphical 

models: learning is computationally hard, simple shallow 
circuits are very limited    

    2006   : deep learning: end-to-end training, large datasets, 

explosion in applications. 

citations of the    lenet    paper 

    recall the lenet was a modern visual classification network 
that recognized digits for zip codes. its citations look like this: 

 

 

 

 

 

 

second phase 

deep learning    winter    

third phase 

    the 2000s were a golden age for machine learning, and 

marked the ascent of id114. but not so for neural 
networks.  

why the success of dnns is surprising 

    from both complexity and learning theory perspectives, 

simple networks are very limited. 

    can   t compute parity with a small network. 

    np-hard to learn    simple    functions like 3sat formulae, 

and i.e. training a dnn is np-hard. 

 

why the success of dnns is surprising 

    the most successful dnn training algorithm is a version of 

id119 which will only find local optima. in other 
words, it   s a greedy algorithm. backprop: 

                                   loss = f(g(h(y))) 

             d loss/dy = f   (g) x g   (h) x h   (y) 

 

 

    id192 are even more limited in what they can 

represent and how well they learn. 

 

    if a problem has a greedy solution, its regarded as an    easy    

problem.  

why the success of dnns is surprising 

    in id114, values in a network represent random 
variables, and have a clear meaning. the network structure 
encodes dependency information, i.e. you can represent rich 
models.  

 

    in a dnn, node activations encode nothing in particular, and 
the network structure only encodes (trivially) how they derive 
from each other.  

 

why the success of dnns is surprising obvious 

    hierarchical representations are ubiquitous in ai. computer 

vision:  

why the success of dnns is surprising obvious 

    natural language: 

 

why the success of dnns is surprising obvious 

    human learning: is deeply layered.  

 

deep expertise 

why the success of dnns is surprising obvious 

    what about greedy optimization?  

    less obvious, but it looks like many learning problems (e.g. 

image classification) are actually    easy    i.e. have reliable 
steepest descent paths to a good model.  

 

ian goodfellow     iclr 2015 tutorial 

