convolutional neural 
network models (case study)

                 | jahan@nvidia.com

                 | hryu@nvidia.com /                  | hanbyuly@nvidia.com

1

agenda

id163 challenge

alexnet

googlenet

residual network (resnet)

inception

polynet

2

    1,000 object classes (categories)
    images:

    1,281,167 train images (140g)
    100k test images

    driven by stanford vision lab

    prof. fei-fei li

3

deep learning revolution

id163

accuracy rate

100%

traditional cv

deep learning

90%

80%

70%

60%

50%

40%

30%

20%

10%

0%

2009 2010 2011 2012 2013 2014 2015 2016

models

alexnet

challenge year

# of layers

top-5 error

data augmentation

inception(nin)

# of convolutions

conv. kernel sizes

# of fcn

fcn sizes

dropout

local response id172

batch id172

2012

8

16.4%

+

   

5

11,5,3

3

vgg

2014

19

7.3%

+

   

16

3

3

4096,4096,10

4096,4096,1

00

+

+

   

000

+

   

   

googlenet

resnet

2014

22

6.7%

+

+

21

2015

152

3.57%

+

   

151

7,1,3,5

7,1,3,5

1

1

1000

1000

+

+

   

+

   

+

4

lenet-5

once upon a time,

[lecun et al., 1998]

conv filters were 5x5, applied at stride 1
subsampling (pooling) layers were 2x2 applied at stride 2
i.e. architecture is [conv-pool-conv-pool-conv-fc]

5

alexnet

convolution neural network with 2 gpus

5x convolution layers

3x fully 

connected layer

gpu 1

gpu 2

max pooling layers / local response id172 layers

6

parameter size

kernel size

[11x11x3]

[5x5x96]

[3x3x256]

[3x3x192]

[3x3x192]

# of kernels

[96]

[256]

[384]

[384]

[256]

7

conv1

96 11x11x3 filter, stride 4, pad 0

pool1

3x3 filters, stride 2

norm1

local response norm

sizing

[55x55x96]

[27x27x96]

[27x27x96]

parameters

flops

35k

105m

output
volume

290k

conv2

256 5x5x96 filter, stride 1, pad 2

[27x27x256]

614k

448m

187k

pool2

3x3 filters, stride 2

norm2

local response norm

[13x13x256]

[13x13x256]

conv3

384 3x3x256 filters, stride 1, pad 1

[13x13x384]

conv4

384 3x3x384 filters, stride 1, pad 1

[13x13x384]

conv5

256 3x3x384 filters, stride 1, pad 1

[13x13x384]

pool3

3x3 filters, stride 2

[6x6x256]

fc6

fc7

fc8

4096 neurons

4096 neurons

1000 neurons

[4096]

[4096]

[1000]

884k

1.3m

884k

150m

224m

150m

65k

65k

65k

8

vggnet

only 3x3 conv stride 1, pad 1
and  2x2 max pool stride 2

best model

[simonyan and zisserman, 2014]

9

vggnet

(not counting biases)

memory

params

flops

15m * 4 byte ~= 60m / image (id136)
138m parameters

conv3-64

conv3-64

maxpool

conv3-128

conv3-128

maxpool

conv3-256

conv3-256

conv3-256

maxpool

conv3-512

conv3-512

conv3-512

maxpool

conv3-512

conv3-512

conv3-512

maxpool

fc-4096

fc-4096

fc-1000

[224x224x64]

[224x224x64]

[112x112x64]

3,211,264 

3,211,264 

802,816 

1,728 

86,704,128 

heavy compute

36,864 

1,849,688,064 

[112x112x128]

1,605,632 

73,728 

924,844,032 

[112x112x128]

1,605,632 

147,456 

1,849,688,064 

[56x56x128]

[56x56x256]

[56x56x256]

[56x56x256]

[28x28x256]

[28x28x512]

[28x28x512]

[28x28x512]

[14x14x512]

[14x14x512]

[14x14x512]

[14x14x512]

[7x7x512]

[1x1x4096]

[1x1x4096]

[1x1x1000]

401,408 

802,816 

802,816 

802,816 

200,704 

294,912 

924,844,032 

589,824 

1,849,688,064 

589,824 

1,849,688,064 

401,408 

1,179,648 

924,844,032 

401,408 

2,359,296 

1,849,688,064 

401,408 

2,359,296 

1,849,688,064 

100,352 

100,352 

2,359,296 

462,422,016 

100,352 

2,359,296 

462,422,016 

100,352 

2,359,296 

462,422,016 

25,088 

4,096 

102,760,448 

102,760,448 

4,096 

16,777,216 

16,777,216 

1,000 

4,096,000 

4,096,000 

15,087,080 

138,344,128  15,470,264,320 

[simonyan and zisserman, 2014]

10

googlenet

inception module

[szegedy et al., 2014]

11

googlenet
in depth view

- only 5 million params!
(removes fc layers 
completely)

compared to alexnet:
- 12x less params
- 2x more compute
- 6.67% (vs. 16.4%)

12

resnet
152 layers !!

[he et al., 2015]

13

cifar10 experiments

14

resnet running time

2-3 weeks of training 
on 8 gpu machine

at runtime: faster 
than a vggnet! 
(even though it has 
8x more layers)

15

resnet technique

spatial dimension 
only 56x56!

16

resnet: residual net / bottleneck

17

1x1 convolution?

convolution for channel direction only

18

resnet: depth view

19

ilsvrc2016

16% better performance

20

21

