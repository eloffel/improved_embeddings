neural network for 
machine learning 2

                 | jahan@nvidia.com

                 | hryu@nvidia.com /                  | hanbyuly@nvidia.com

1

agenda

training neural network

stochastic id119

learning rate control

id174

weight initialization

data augmentation

id173 & dropout

hyper parameter exploration

2

training of neural networks 

1. divide data on 2 sets: training set and validation set
2. define network and training procedure

   

   

   

   

network architecture 

id168

preprocessing  and data augmentation 

training algorithm and parameters (batch size, initialization,   )

3. training:

for (t = 0; t < t; t++)

train net(w) on training set

4.

test  net(w) on validation set
if  (not state-of-the art) goto 2

3

training of neural networks is difficult 

id168
network architecture:  # of layers, and # of channels/layer

a lot of hyper-parameters to choose:
   
   
    weight initialization
   
   

sgd algorithm and learning rate policy
id174: scaling, augmentation    

4

optimization algorithms for id98 training

5

batch id119

we want to minimize  loss over training set with n samples (xn, yn): 

1
    

          =

         (             ,      ,         )

     =1
batch  optimization: 
1. accumulate gradients over all samples in training set 
            (    ,           1)

            (    ,           1)

        

=

        
            

  

               1

; 

               1

        
            

=

        
            

  

            

2. update w:

          + 1 =                       

1
    

    

     =1

        
        

(         ,      ,         )

issue:

id163 has 106 images    gradient computation for whole set is expensive

6

stochastic id119

stochastic id119 (on-line learning):

1. randomly choose sample (xk, yk): 
2.           + 1 =                       

(         ,      ,         )

        
        

stochastic id119 with mini-batches:

1. divide the dataset into small mini-batches, choose samples from different 

classes 

2. compute the gradient using a single m-batch, make an update
3. move to the  next mini-batch    

don   t forget to shuffle / shift data between epochs!

7

stochastic id119

n = batch: gradient computation is heavy, step is small
n =1 (on-line training): gradient is very noisy, zig-zags around    true    gradient 

mini-batch training follows the curve of 
gradient: 
the expected value of the weight change for 
on-line training is continuously pointing in the 
direction of the gradient at the current point in 
weight space.

batch

mini-batch

wilson,    on the general inefficiency of batch training for id119 learning, 2003

8

9

mini-batch id119

sgd with mini-batch:

1. faster than batch
2. more robust to redundant data 
3. behaves better in local minima/ saddle.

   use stochastic id119 when training time is the bottleneck    

leon bottou, stochastic id119 tricks

10

mini-batch size and learning rate

how to change learning rate when we change the batch size?
   classical ml    rule: on-line / mini-batch training with large learning rate is much 
more stable than batch training with the same learning rate.
convolutional nn (for problems with large number of classes) 
alex krizhevsky (   one weird trick on parallelizing id98   ):
1. theory:  multiply the learning rate by k1/2 when increase the batch size by k to 

keep the variance in the gradient expectation constant.

2. practice: to multiply the learning rate by k when multiplying the batch size by k. 

this rule does not work when mini-batch size become too large !

warning:

11

example: cifar0-10

12

learning rate & its control

13

example: cifar-10 training

training and testing accuracy 

batch =100, initial lr =0.001

batch =1000, initial lr =0.01

14

example: cifar-10

training and testing accuracy 

batch =100,initial lr =0.001

batch =1000, intial lr =0.001

15

learning rate adaptation

          + 1 =                   (    )    

        
        

classical learning rate annealing:

   

     =1

1

    2(    )

<     and      =1

    1

    (    )

=     , 

e.g.   (t) =

c
    

; 

other learning rate policies:  
1. manual:             =                     
2. exp:                    =                      
[
                

    

]

3.
4.

step :                  =     0         
inverse:             =     0     (1 +              )       

16

example: cifar-10 training

initial lr = 0.001, decrease lr by 10x  for 10,000 and 20,000 iterations

17

learning rate

no silver bullet for learning rate

choose proper size with explaration

18

why accuracy improves when we decrease lr? 

motivational example

let   s take a quadratic function       = 10         2 + 2.5         2 and use sgd to find minimum. 

basic sgd converges if learning rate <

1
10

. 

lr=0.05

lr=0.09

lr=0.10

lr=0.20

19

adaptive learning rate 

adapt
1.

learning rate per weight or per layer
   

use only the sign of the gradient

   

divide the learning rate for a weight by a running average of the magnitudes of recent 
gradients for that weight (adagrad, adadelta, rmsprop,..)

2.

learning rate and momentum
   

natural gradient

   

adam (knigmna and ba)

20

sgd with weight decay

          + 1 =                        (

        
        

+              (    ) ) 

weight decay works to keep weights size under control (   id173   ). 
this is equivalent to adding penalty on weights to id168:

             =           +

    
2

        2

21

sgd with momentum

          + 1 =           +              + 1

             +      =                                   

        
        

momentum works as weighted average of gradients . 

        t+1 =             (      =1

    +1         +1           

        
        

k )

22

w(t-1)w(t)w(t+1v(t+1)optimization near saddle points

   the problem with convnets cost functions is not local min, but local saddle points. 
how sgd methods behave near saddle point?   

r. pascanu,    on the saddle point problem for non-id76   , http://arxiv.org/abs/1405.4604

dauphin,    identifying and attacking the saddle point problem in high-dimensional non-id76   ,  
http://arxiv.org/pdf/1406.2572v1.pdf

24

advanced optimization algorithms

a lot of interesting optimization algorithms to speed-up training and get 
better results:
    nesterov accelerated gradient 
    adagrad
    adadelta
    rmsprop/rprop  
    varience-based sgd
    averaged sgd
    adam 

25

adagrad, adadelta, and adam

adagrad:  adapt learning rate for each weight

                     + 1 =    

    

    +    (

     

        
                

(    ))    

   

        
                

(     + 1)

adadelta: accumulate the denominator over last k gradients (sliding window): 

          + 1 =             +1

    +1

                     + 1 =    

        
(
                
    

    (    +1)

(    ))2
        
                

   

(     + 1)

this requires to keep k gradients. instead we can use simpler formula:

          + 1 =                    + 1              (
        
                

                     + 1 =    

         +1 +    

   

    

(     + 1))2

        
                
(     + 1)

26

performance near saddle points

by alec radford alecradford http://imgur.com/a/hqolp

27

id174

28

preprocessing the data

29

dimension reduction

in practice, you may also see pca and whitening of the data

(data has diagonal 
covariance matrix)

(covariance matrix 
is the identity 
matrix)

30

tldr: in practice for images: center only

e.g. consider cifar-10 example with [32,32,3] images
- subtract the mean image (e.g. alexnet)

(mean image = [32,32,3] array)

- subtract per-channel mean (e.g. vggnet)

(mean along each channel = 3 numbers)

not common to normalize 
variance, to do pca or 
whitening

31

case: deepface architecture

yaniv taigman, etc (facebook) . deepface: closing the gap to human-level performance in face 
verification, cvpr 2014

32

weight initialization

33

initialization for neural network

34

gradient banishment

35

understanding the difficulty of 
training  convolutional networks

example: mlp with 4  fully  connected layers, with sigmoid non-linear function.
measure mean and standard deviation of the activation (output of the sigmoid) for 4 
hidden layers

the top hidden layer quickly saturates at 0. it drives the 
grdaient de/dy multiplied by 0 and therefore causing the 
id119 to stall, slowing down all learning. near 
epoch 100 it slowly de-saturates releasing other layers

x. glorot ,y. bengio, understanding the difficulty of training deep feedforward neural networks 

36

why?

playground.tensorflow.org

all neurons will be all zero

during back-propagation

37

xavier initialization

reasonable initialization.
(mathematical derivation 
assumes linear activations)

38

xavier initialization with relu

40

he initialization

41

id173 | data augmentation

54

neural network id173

empirical

explicit

dropout
dropconnect
stochastic pooling
artificial data

early stopping
limiting number of parameters
weight decay

l1/l2 id173
max norm constraints

https://en.wikipedia.org/wiki/id173_(mathematics)

55

data augmentation

image translations  
re-scale (both up and down) before crop

the common method to    enlarge    training set:
   
   
    horizontal and vertical reflections (    flip   )
    elastic deformation with random interpolations ((bilinear, area, nearest neighbor 

and cubic, with equal id203) (simard, 2003) 

    photometric distortion and  altering the intensities of the rgb channels in training 

images (a.g. howard., 2013)

56

data augmentation (1)

1. horizontal filps

2. random crops/scales

training: sample random crops / scales
resnet:
1. pick random l in range [256, 480]
2. resize training image, short side = l
3. sample random 224 x 224 patch

testing: average a fixed set of crops
resnet:
1. resize image at 5 scales:  {224, 256, 384, 480, 640}
2. for each size, use 10 224 x 224 crops: 4 corners + 

center, + flips

57

3. color jitter

data augmentation 

4. others
random mix/combinations of :
-
-
-
-
-

translation
rotation
stretching
shearing, 
lens distortions,      (go crazy)

58

data augmentation (plankton competition)

pre-processed image

augmented image

http://benanne.github.io/2015/03/17/plankton.html

59

id173 dropout

60

dropout layer

randomly set some neurons to zero in the forward pass

srivastava et al., 2014

61

code example

example forward 
pass with a 3-
layer network 
using dropout

cs231n

62

why this is good?
it helps higher accuracy

63

using for training / id136

64

hyperparameter optimizations

65

parameters affecting training

learning rate
id173s
filter size
model depth
stride
dropout
model architecture
   

66

random search vs. grid search

random search for hyper-parameter optimization
bergstra and bengio, 2012

67

search optimal hyper-parameters

example

adjust range

cs231n

53% - relatively 
good for a 2-layer 
neural net with 50 
hidden neurons.

but this best 
cross-validation 
result is worrying. 
why?

68

monitor and visualize the loss curve

69

70

