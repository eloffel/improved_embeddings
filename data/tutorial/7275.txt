convolutional neural 
network

                 | jahan@nvidia.com

                 | hryu@nvidia.com /                  | hanbyuly@nvidia.com

1

agenda

id98 models

visualization

localization and detection

segmentation

id21

deep dream and neural style

2

id161 tasks

classification

classification 
+ localization

id164

segmentation

cat

cat

cat, dog, duck

cat, dog, duck

single object

multiple objects

3

visualization

[ understanding id98 ]

4

deep visualization

https://youtu.be/agkfiq4igam

5

deconv approach

1. feed image into net

2. pick a layer, set the gradient there to be all zero except for one 1 for 
some neuron of interest
3. backprop to image:

   guided 
id26:   
instead

cs231n

6

guided id26

[visualizing and understanding convolutional networks, zeiler and fergus 2013]
[deep inside convolutional networks: visualising image classification models and saliency maps, simonyan et al., 2014]
[striving for simplicity: the all convolutional net, springenberg, dosovitskiy, et al., 2015]

7

deconv approaches

bit weird

8

visualization arbitrary neurons

9

visualization arbitrary neurons

10

visualization arbitrary neurons

11

localization and detection

12

object localization goals

car

detector

given an image output two things:

object class (integer)
x1,y1,x2,y2 bounding box coordinates

13

localization pipeline

traditional 
detection:

image

proposal
generator

(cv)

classifier

(cv or dnn)

object list

deep learning 
approach:

image

dnn

detector

object list

in one shot, get an object list

goal is real-time detection rates

traditional detection pipelines can generate 1000   s of proposals, which may get in 

the way of realtime

14

slide credit: ross girshick, microsoft research

15

idea: train a localization network

very deep convolutional networks for large-scale image recognition, 
simonyan et al., 2014 

overfeat: integrated recognition, localization and detection using convolutional 
networks, sermanet et al., 2014

take out softmax loss, swap in 
l2 (regression) loss, fine-tune 
the classification network.

slide credit: fei-fei li & andrej karpathy, stanford cs231n

16

idea: train a localization network

very deep convolutional networks for large-scale image recognition, 
simonyan et al., 2014 

overfeat: integrated recognition, localization and detection using convolutional 
networks, sermanet et al., 2014

take out softmax loss, swap in 
l2 (regression) loss, fine-tune 
the classification network.

slide credit: fei-fei li & andrej karpathy, stanford cs231n

17

idea: train a localization network

very deep convolutional networks for large-scale image recognition, 
simonyan et al., 2014 

overfeat: integrated recognition, localization and detection using convolutional 
networks, sermanet et al., 2014

in practice:

it works better to predict a 4d 
vector for every class (e.g. 4000d 
vector for 1000 id163 classes). 
during training only backprop the 
loss for the correct class
apply at multiple locations and 
scales

slide credit: fei-fei li & andrej karpathy, stanford cs231n

18

slide credit: fei-fei li & andrej karpathy, stanford cs231n

19

slide credit: fei-fei li & andrej karpathy, stanford cs231n

20

slide credit: fei-fei li & andrej karpathy, stanford cs231n

21

slide credit: fei-fei li & andrej karpathy, stanford cs231n

22

detection

given an image, output 3 things:

confidence
class (integer)
x1,y1,x2,y2 bounding box 
coordinates

slide credit: fei-fei li & andrej karpathy, stanford cs231n

23

slide credit: fei-fei li & andrej karpathy, stanford cs231n

24

slide credit: fei-fei li & andrej karpathy, stanford cs231n

25

slide credit: fei-fei li & andrej karpathy, stanford cs231n

26

27

detection software

compiled c++ binaries, torch and python apis for overfeat: 
http://cilvr.nyu.edu/doku.php?id=software:overfeat:start
caffe fork implementing r-id98 (requires matlab): https://github.com/rbgirshick/rid98
caffe fork implementing fast r-id98: https://github.com/rbgirshick/fast-rid98
theano based python library with overfeat wrapper: http://sklearn-theano.github.io/
torch based r-id98 implementation: https://github.com/fmassa/object-detection.torch

28

practice

id164 for

pascal voc
kitti dataset

29

segmentation

30

a classification network

slide credit: jon long and evan shelhamer, cvpr 2015

31

becoming fully convolutional

slide credit: jon long and evan shelhamer, cvpr 2015

32

becoming fully convolutional

slide credit: jon long and evan shelhamer, cvpr 2015

33

upsampling output

slide credit: jon long and evan shelhamer, cvpr 2015

34

end-to-end pixels-to-pixels network

slide credit: jon long and evan shelhamer, cvpr 2015

35

applications

36

segmentation performance

37

software

caffe branch implementing fcn: fcn.berkeleyvision.org

38

practice

image segmentation

pascal voc

39

neural style

40

41

42

jitter regularizer

   image update   

43

inception_4c/output

deepdream modifies the image in a way that    boosts    all activations, at any layer

this creates a feedback loop: e.g. any slightly detected dog face will be made more 
and more dog like over time

44

inception_4c/output

inception_3b/5x5_reduce

deepdream modifies the image in a way that    boosts    all activations, at any layer

45

neural style

46

practice

google deep dream

47

id21

48

should i training all this?

    dataset sizes have increased 

greatly over time

    what about your problem?

the deep learning book, ian goodfellow and yoshua bengio and aaron courville

49

id21

instead of initializing network with random weights we use the weights that have 
been learned on a different dataset
we then continue to train the network (or part of it)     called fine-tuning     using our 
true dataset of interest
three modes:

if new dataset is small and similar to original, just train a linear classifier on high-level 
features output by original network.  fine-tuning whole network would overfit.

if new dataset is large and similar, fine-tune the whole network

if new dataset is small and different, train linear classifier on mid-level features from 
original network

if new dataset is large and different, still fine-tune the whole network as convergence is 
often still faster than starting from scratch

50

id21

what is it?

new data

transfer 
weights

id98

myth:  you need a lot of data to 

train deep neural networks

object 
classifier

dog vs. 

cat

top 10 in 

10 mins after 

finetuning

51

id21

what is it?

new data

transfer 
weights

id98

myth:  you need a lot of data to 

train deep neural networks

object 
classifier

dog vs. 

cat

top 10 in 

10 mins after 

finetuning

52

pretrained model
starting with other   s effort

53

id21 with id98s

1. train on 
id163

cs231n

54

id21 with id98s

1. train on 
id163

2. small dataset:
feature extractor

freeze these

train this

cs231n

55

id21 with id98s

1. train on 
id163

2. small dataset:
feature extractor

freeze these

train this

3. medium dataset:
finetuning

more data = retrain more of 
the network (or all of it)

freeze these

tip: use only ~1/10th of 
the original learning rate 
in finetuning top layer, 
and ~1/100th on 
intermediate layers

train this

cs231n

56

57

pretrained model
starting with other   s effort

58

   using 4 nvidia kepler gpus and 
optimizations described below, 
training took from 3.5 days for the 
18-layer model to 14 days for the 
101-layer model.   

- torch & facebook tech blog

http://torch.ch/blog/2016/02/04/resnets.html

59

practice

food dataset-101 + k-foods

pascal voc

60

61

