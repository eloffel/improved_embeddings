a statistical view of

deep learning

shakir mohamed

4 july 2015

i   ve taken to writing this series of posts on a statistical
view of deep learning with two principal motivations in
mind. the    rst was as a personal exercise to make con-
crete and to test the limits of the way that i think about
and use deep learning in my every day work. the second,
was to highlight important statistical connections and im-
plications of deep learning that i have not seen made in
the popular courses, reviews and books on deep learn-
ing, but which are extremely important to keep in mind.
this document forms a collection of these essays originally
posted at blog.shakirm.com.

1

c o n t e n t s

1 recursive generalised linear models

3

1.1 generalised linear models
1.2 recursive generalised linear models
1.3 learning and estimation
1.4 summary

6

3

5

2 auto-encoders and free energy

4

7

2.1 generalised denoising auto-encoders
2.2 separating model and id136
8
2.3 approximate id136 in latent variable models
2.4 summary

10

7

3 memory and kernels

11

3.1 basis functions and neural networks
3.2 kernel methods
3.3 gaussian processes
3.4 summary

14

14

12

12

4 recurrent networks and dynamical systems

8

15

4.1 recurrent neural networks
4.2 probabilistic dynamical systems
4.3 prediction, filtering and smoothing
4.4 summary

18

15

17

18

5 generalisation and regularisation

20

invariant map estimators

5.1 regularisers and priors
5.2
5.3 dropout: with and without id136
5.4 summary
6 what is deep?

23

24

21

6.1 deep and id187
6.2 characterising deep models
6.3 beyond hierarchies of the mean
6.4 summary

28

24

26

27

20

22

2

1

r e c u r s i v e g e n e r a l i s e d l i n e a r m o d e l s

deep learning and the use of deep neural networks [1] are now estab-
lished as a key tool for practical machine learning. neural networks
have an equivalence with many existing statistical and machine learn-
ing approaches and i would like to explore one of these views in this
post. in particular, i   ll look at the view of deep neural networks as re-
cursive generalised linear models (rglms). generalised linear mod-
els form one of the cornerstones of probabilistic modelling and are
used in almost every    eld of experimental science, so this connection
is an extremely useful one to have in mind. i   ll focus here on what
are called feed-forward neural networks and leave a discussion of the
statistical connections to recurrent networks to another post.

1.1 generalised linear models

the basic id75 model is a linear mapping from p-dimensional
input features (or covariates) x, to a set of targets (or responses) y, us-
ing a set of weights (or regression coef   cients)    and a bias (offset)
  0 . the outputs can also by multivariate, but i   ll assume they are
scalar here. the full probabilistic model assumes that the outputs are
corrupted by gaussian noise of unknown variance   2.

   =   (cid:62)x +   0

y =    +  

      n(0,   2)

in this formulation,    is the systematic component of the model and
  is the random component. generalised linear models (glms)[2] al-
low us to extend this formulation to problems where the distribution
on the targets is not gaussian but some other distribution (typically a
distribution in the exponential family). in this case, we can write the
generalised regression problem, combining the coef   cients and bias
for more compact notation, as:

   =   (cid:62)x,

   = [     ,   0], x = [  x, 1]

e[y] =    = g   1(  )

where g(  ) is the link function that allows us to move from natural
parameters    to mean parameters   . if the inverse link function used
in the de   nition of    above were the logistic sigmoid, then the mean
parameters correspond to the probabilities of y being a 1 or 0 under

3

table 1: correspondence between link and activations functions in

generalised regression.

target
real
binary

regression
linear
logistic

link
identity
logit log   
1     

binary

probit

binary

gumbel

binary

logistic

gauss

inv
cdf      1(  )
compl.
log-log
log(   log(  ))

categorical multinomial

counts
counts
non-neg.
sparse
ordered

poisson
poisson
gamma
tobit
ordinal

(cid:112)(  )

log(  )

reciprocal 1
  

inv link
identity
sigmoid

1

cdf

1+exp(     )
gauss
  (  )
gumbel cdf
e   e   x

hyperbolic
tangent
tanh(  )
multin. logit

  i(cid:80)

j   j

activation

sigmoid

probit

tanh

softmax

exp(  )
  2
1
  
max max(0;   ) relu
cum.
  (  k       )

logit

the bernoulli distribution.

there are many link functions that allow us to make other distribu-
tional assumptions for the target (response) y. in deep learning, the
link function is referred to as the activation function and i list in the
table below the names for these functions used in the two    elds. from
this table we can see that many of the popular approaches for speci-
fying neural networks that have counterparts in statistics and related
literatures under (sometimes) very different names, such multinomial
regression in statistics and softmax classi   cation in deep learning, or
recti   er in deep learning and tobit models is statistics.

1.2 recursive generalised linear models

constructing a recursive glm or deep deep feed-forward neural net-
work using the linear predictor as the basic building block. glms
have a simple form: they use a linear combination of the input using
weights   , and pass this result through a simple non-linear function.
in deep learning, this basic building block is called a layer. it is easy
to see that such a building block can be easily repeated to form more
complex, hierarchical and non-id75 functions. this recur-
sive application of the basic regression building block is why models
in deep learning are described as having multiple layers and are de-
scribed as deep.

4

figure 1: constructing a recursive glm or deep deep feedforward
neural network using the linear predictor as the basic build-
ing block.

if an arbitrary regression function h, for layer l, with linear predic-
tor , and inverse link or activation function f, is speci   ed as:

hl(x) = fl(  l)

then we can easily specify a recursive glm by iteratively applying

or composing this basic building block:

e[y] =   l = hl     . . .     h1     ho(x)

this composition is exactly the speci   cation of an l-layer deep neu-
ral network model. there is no mystery in such a construction (and
hence in feedforward neural networks) and the utility of such a model
is easy to see, since it allows us to extend the power of our regressors
far beyond what is possible using only linear predictors.

this form also shows that recursive glms and neural networks are
one way of performing basis function regression. what such a for-
mulation adds is a speci   c mechanism by which to specify the basis
functions: by application of recursive linear predictors.

1.3

learning and estimation

given the speci   cation of these models, what remains is an approach
for training them, i.e. estimation of the regression parameters    for
every layer. this is where deep learning has provided a great deal of
insight and has shown how such models can be scaled to very high-
dimensional inputs and on very large data sets.

5

g()   = >xg()   l= >lxl   e[y]building block:  linear predictor or layera natural approach is to use the negative log-id203 as the loss
function and id113 [3]:

l =     log p(y|  l)

where if using the gaussian distribution as the likelihood function
we obtain the squared loss, or if using the bernoulli distribution we
obtain the cross id178 loss. estimation or learning in deep neural
networks corresponds directly to id113 in
recursive glms. we can now solve for the regression parameters by
computing gradients w.r.t. the parameters and updating using gradi-
ent descent. deep learning methods now always train such models
using stochastic approximation (using stochastic id119),
using automated tools for computing the chain rule for derivatives
throughout the model (i.e. back-propagation), and perform the com-
putation on powerful distributed systems and gpus. this allows
such a model to be scaled to millions of data points and to very large
models with potentially millions of parameters [4].

from the maximum likelihood theory, we know that such estimators
can be prone to over   tting and this can be reduced by incorporat-
ing model regularisation, either using approaches such as penalised
regression and shrinkage, or through bayesian regression. the impor-
tance of regularisation has also been recognised in deep learning and
further exchange here could be bene   cial.

1.4

summary

deep feed-forward neural networks have a direct correspondence to
recursive generalised linear models and basis function regression in
statistics     which is an insight that is useful in demystifying deep
networks and an interpretation that does not rely on analogies to
sequential processing in the brain. the training procedure is an ap-
plication of (regularised) id113, for which
we now have a large set of tools that allow us to apply these models to
very large-scale, real-world systems. a statistical perspective on deep
learning points to a broad set of knowledge that can be exchanged
between the two    elds, with the potential for further advances in ef-
   ciency and understanding of these regression problems. it is thus
one i believe we all bene   t from by keeping in mind. there are other
viewpoints such as the connection to id114, or for recur-
rent networks, to dynamical systems, which i hope to think through
in the future.

6

2

a u t o - e n c o d e r s a n d f r e e e n e r g y

with the success of discriminative modelling using deep feedforward
neural networks (or using an alternative statistical lens, recursive gen-
eralised linear models) in numerous industrial applications, there is
an increased drive to produce similar outcomes with unsupervised
learning. in this post, i   d like to explore the connections between de-
noising auto-encoders as a leading approach for unsupervised learn-
ing in deep learning, and density estimation in statistics. the statisti-
cal view i   ll explore casts learning in denoising auto-encoders as that
of id136 in latent factor (density) models. such a connection has a
number of useful bene   ts and implications for our machine learning
practice.

2.1 generalised denoising auto-encoders

denoising auto-encoders are an important advancement in unsuper-
vised deep learning, especially in moving towards scalable and ro-
bust representations of data. for every data point y, denoising auto-
encoders begin by creating a perturbed version of it y(cid:48), using a known
corruption process c(y(cid:48)|y). we then create a network that given the
perturbed data y(cid:48), reconstructs the original data y. the network is
grouped into two parts, an encoder and a decoder, such that the out-
put of the encoder z can be used as a representation/features of the
data. the objective function is [5]:

perturbation: y(cid:48)     c(y(cid:48)|y)

encoder:

z(y(cid:48)) = f  (y(cid:48))

decoder: y     g  (z)

objective: ldae = log p(y|z)

where log p(  ) is an appropriate likelihood function for the data,
and the objective function is averaged over all observations. gener-
alised denoising auto-encoders (gdaes) realise that this formulation
may be limited due to    nite training data, and introduce an addi-
tional penalty term r(  ) for added regularisation [6]:
lgdae = log p(y|z)       r(y, y(cid:48))

gdaes exploit the insight that perturbations in the observation
space give rise to robustness and insensitivity in the representation

7

z. two key questions that arise when we use gdaes are: how to
choose a realistic corruption process, and what are appropriate regu-
larisation functions.

2.2

separating model and id136

the dif   culty in reasoning statistically about auto-encoders is that
they do not maintain or encourage a distinction between a model of
the data (statistical assumptions about the properties and structure
we expect) and the approach for id136/estimation in that model
(the ways in which we link the observed data to our modelling as-
sumptions). the auto-encoder framework provides a computational
pipeline, but not a statistical explanation, since to explain the data
(which must be an outcome of our model), you must know it before-
hand and use it as an input. not maintaining the distinction between
model and id136 impedes our ability to correctly evaluate and
compare competing approaches for a problem, leaves us unaware of
relevant approaches in related literatures that could provide useful
insight, and makes it dif   cult for us to provide the guidance that al-
lows our insights to be incorporated into our community   s broader
knowledge-base.

to ameliorate these concerns we typically re-interpret the auto-encoder
by seeing the decoder as the statistical model of interest (and is in-
deed how many interpret and use auto-encoders in practice). a prob-
abilistic decoder provides a generative description of the data, and
our task is id136/learning in this model. for a given model,
there are many competing approaches for id136, such as maxi-
mum likelihood (ml) and maximum a posteriori (map) estimation,
noise-contrastive estimation, id115 (mcmc),
variational id136, cavity methods, integrated nested laplace ap-
proximations (inla), etc. the role of the encoder is now clear: the
encoder is one mechanism for id136 in the model described by
the decoder. its structure is not tied to the model (decoder), and it is
just one from the smorgasbord of available approaches with its own
advantages and tradeoffs.

2.3 approximate id136 in latent variable models

encoder-decoder view of id136 in latent variable models. another
dif   culty with daes is that robustness is obtained by considering per-
turbations in the data space such a corruption process will, in general,
not be easy to design. furthermore, by carefully reasoning about the
induced probabilities, we can show [5] that the dae objective func-
tion ldae corresponds to a lower bound obtained by applying the
variational principle to the log-density of the corrupted data log p(y(cid:48))
this though, is not a quantity we are interested in reasoning about.

a way forward would be to instead apply the variational principle

8

figure 2: encoder-decoder view of

id136 in latent variable

models.

to the quantity we are interested in, the log-marginal id203 of
the observed data log p(y) [7] [8]. the objective function obtained by
applying the variational principle to the generative model (probabilis-
tic decoder) is known as the variational free energy:

lvfe = eq(z)[log p(y|z)]     kl[q(z)(cid:107)p(z)]

by inspection, we can see that this matches the form of the gdae

objective. there are notable differences though:

    instead of considering perturbations in the observation space,
we consider perturbations in the hidden space, obtained by us-
ing a prior p(z). the hidden variables are now random, latent
variables. auto-encoders are now generative models that are
straightforward to sample from.

    the encoder q(z|y) is a mechanism for approximating the true

posterior distribution of the latent/hidden variables p(z|y).

    we are now able to explain the introduction of the penalty func-
tion in the gdae objective in a principled manner. rather than
designing the penalty by hand, we are able to derive the form
this penalty should take, appearing as the kl divergence be-
tween the the prior and the encoder distribution.

auto-encoders reformulated in this way, thus provide an ef   cient

way of implementing approximate bayesian id136. using an encoder-
decoder structure, we gain the ability to jointly optimise all param-
eters using the single computational graph; and we obtain an ef   -
cient way of doing id136 at test time, since we only need a sin-
gle forward pass through the encoder. the cost of taking this ap-

9

data yid136/encoderq(z |y)z ~ q(z | y)model/decoderp(y |z)y ~ p(y | z)zproach is that we have now obtained a potentially harder optimisa-
tion, since we have coupled the id136s for the latent variables
together through the parameters of the encoder. approaches that do
not implement the q-distribution as an encoder have the ability to
deal with arbitrary missingness patterns in the observed data and we
lose this ability, since the encoder must be trained knowing the miss-
ingness pattern it will encounter. one way we explored these connec-
tions is in a model we called deep latent gaussian models (dlgm)
with id136 based on stochastic variational id136 (and imple-
mented using an encoder) [7], and is now the basis of a number of
extensions [9] [10].

2.4

summary

auto-encoders address the problem of statistical id136 and pro-
vide a powerful mechanism for id136 that plays a central role in
our search for more powerful unsupervised learning. a statistical
view, and variational reformulation, of auto-encoders allows us to
maintain a clear distinction between the assumed statistical model
and our approach for id136, gives us one ef   cient way of im-
plementing id136, gives us an easy-to-sample generative model,
allows us to reason about the statistical quantity we are actually in-
terested in, and gives us a principled id168 that includes the
important regularisation terms. this is just one perspective that is
becoming increasingly popular, and is worthwhile to re   ect upon as
we continue to explore the frontiers of unsupervised learning.

10

3

m e m o r y a n d k e r n e l s

memory, the ways in which we remember and recall past experiences
and data to reason about future events, is a term used frequently in
current literature. all models in machine learning consist of a mem-
ory that is central to their usage. we have two principal types of
memory mechanisms, most often addressed under the types of mod-
els they stem from: parametric and non-parametric (but also all the
shades of grey in-between). deep networks represent the archetypical
parametric model, in which memory is implemented by distilling the
statistical properties of observed data into a set of model parameters
or weights. the poster-child for non-parametric models would be
kernel machines (and nearest neighbours) that implement their mem-
ory mechanism by actually storing all the data explicitly. it is easy to
think that these represent fundamentally different ways of reasoning
about data, but the reality of how we derive these methods points to
far deeper connections and a more fundamental similarity.

deep networks, kernel methods and gaussian processes form a con-
tinuum of approaches for solving the same problem - in their    nal
form, these approaches might seem very different, but they are fun-
damentally related, and keeping this in mind can only be useful for
future research. this connection is what i explore in this post.

figure 3: connecting machine learning methods for regression.

11

deep networkskernel machinesgaussian processesdual functionsbayesian id136in   nite limits 3.1

basis functions and neural networks

all the methods in this post look at regression: learning discrimina-
tive or input-output mappings. all such methods extend the humble
linear model, where we assume that linear combinations of the input
data x, or transformations of it   (x), explain the target values y. the
  (x) are basis functions that transform the data into a set of more
interesting features. features such as sift for images or mfccs for
audio have been popular in the past     in these cases, we still have
a id75, since the basis functions are    xed. neural net-
works give us the ability to use adaptive basis functions, allowing
us to learn what the best features are from data instead of designing
these by-hand, and allowing for a non-id75.

a useful probabilistic formulation separates the regression into sys-
tematic and random components: the systematic component is a func-
tion f we wish to learn, and the targets are noisy realisations of this
function. to connect neural networks to the linear model, i   ll explic-
itly separate the last linear layer of the neural network from the layers
that appear before it. thus for an l-layer deep neural network, i   ll de-
note the    rst l     1 layers by the mapping   (x;   ) with parameters
  , and the    nal layer weights w; the set of all model parameters is
q = {  , w}.

sytematic:

f = w(cid:62)  (x;   )

q     n(0,   2

qi),

random:

y = f(x) +  

      n(0,   2
y)

once we have speci   ed our probabilistic model, this implies an
objective function for optimising the model parameters given by the
negative log joint-id203. we can now apply back-propagation
and learn all the parameters, performing map estimation in the neu-
ral network model. memory in this model is maintained in the para-
metric modelling framework; we do not save the data but compactly
represent it by the parameters of our model. this formulation has
many nice properties: we can encode properties of the data into the
function f, such as being a 2d image for which convolutions are sensi-
ble, and we can choose to do a stochastic approximation for scalability
and perform id119 using mini-batches instead of the en-
tire data set. the id168 for the output weights is of particular
interest, since it will offers us a way to move from neural networks to
other types of regression.

n(cid:88)

n=1

j(w) =

1
2

(yn     w(cid:62)  (xn;   ))2 +

  
2

w(cid:62)w.

3.2 kernel methods

if you stare a bit longer at this last objective function, especially as
formulated by explicitly representing the last linear layer, you   ll very

12

(cid:88)

1
  

quickly be tempted to compute its dual function [11, pp. 293]. we   ll
do this by    rst setting the derivative w.r.t. w to zero and solving for
it:

   j(w) = 0 =    w =
(cid:88)

w =

  n  (xn) =   (cid:62)  

n

(yn     w(cid:62)  (xn))  (xn)

n

  n =    

1
  

(w(cid:62)  (xn)     yn)

we   ve combined all basis functions/features for the observations
into the matrix   . by taking this optimal solution for the last layer
weights and substituting it into the id168, two things emerge:
we obtain the dual id168 that is completely rewritten in terms
of a new parameter   , and the computation involves the matrix prod-
uct or gram matrix k =     (cid:62). we can repeat the process and solve
the dual loss for the optimal parameter , and obtain:

   j(  ) = 0 =       = (k +   in)   1y

and this is where the kernel machines deviate from neural net-
works. since we only need to consider inner products of the features
  )x) (implied by maintaining k), instead of parameterising them us-
ing a non-linear mapping given by a deep network, we can use ker-
nel substitution (aka, the kernel trick) and get the same behaviour
by choosing an appropriate and rich id81 k(x, x(cid:48)). this
highlights the deep relationship between deep networks and kernel
machines: they are more than simply related, they are duals of each
other.

the memory mechanism has now been completely transformed into
a non-parametric one - we explicitly represent all the data points
(through the matrix k). the advantage of the kernel approach is that
is is often easier to encode properties of the functions we wish to
represent e.g., functions that are up to p-th order differentiable or
periodic functions, but stochastic approximation is now not possible.
predictions for a test point x* can now be written in a few different
ways:

(cid:88)

n

f = w(cid:62)map  (x   ) =   (cid:62)  (x)  (x   ) =

  nk(x   , xn) = k(x, x   )(cid:62)(k +   i)   1y

the last equality is a form of solution implied by the representer
theorem and shows that we can instead think of a different formu-
lation of our problem: one that directly penalises the function we
are trying to estimate, subject to the constraint that the function lies
within a hilbert space (and providing a direct non-parametric view):

n(cid:88)

n=1

j(f) =

1
2

(yn     f(xn))2 +

  

2(cid:107)f(cid:107)2
h.

13

3.3 gaussian processes

we can go even one step further and obtain not only a map estimate
of the function f, but also its variance. we must now specify a prob-
ability model that yields the same id168 as this last objective
function. this is possible since we now know what a suitable prior
over functions is, and this probabilistic model corresponds to gaus-
sian process (gp) regression [12]:

p(f) = n(0, k)

p(y|f) = n(y|f,   )

we can now apply the standard rules for gaussian conditioning to
obtain a mean and variance for any predictions x   . what we obtain
is:

p(f   |x, y, x   ) = n(e[f   ], v[f   ])

e[f   ] = k(x, x   )(cid:62)(k +   i)   1y

v[f   ] = k(x   , x   )     k(x, x   )(cid:62)(k +   i)   1k(x, x   )

conveniently, we obtain the same solution for the mean whether
we use the kernel approach or the gaussian conditioning approach.
we now also have a way to compute the variance of the functions of
interest, which is useful for many problems (such as active learning
and optimistic exploration). memory in the gp is also of the non-
parametric    avour, since our problem is formulated in the same way
as the kernel machines. gps form another nice bridge between kernel
methods and neural networks: we can see gps as derived by bayesian
reasoning in kernel machines (which are themselves dual functions of
neural nets), or we can obtain a gp by taking the number of hidden
units in a one layer neural network to in   nity [13].

3.4

summary

deep neural networks, kernel methods and gaussian processes are
all different ways of solving the same problem - how to learn the best
regression functions possible. they are deeply connected: starting
from one we can derive any of the other methods, and they expose
the many interesting ways in which we can address and combine ap-
proaches that are ostensibly in competition. i think such connections
are very interesting, and should prove important as we continue to
build more powerful and faithful models for regression and classi   -
cation.

14

4

r e c u r r e n t n e t w o r k s a n d d y n a m i c a l
s y s t e m s

recurrent neural networks (id56s) are now established as one of the
key tools in the machine learning toolbox for handling large-scale se-
quence data. the ability to specify highly powerful models, advances
in stochastic id119, the availability of large volumes of
data, and large-scale computing infrastructure, now allows us to ap-
ply id56s in the most creative ways. from handwriting generation,
image captioning, language translation and voice recognition, id56s
now routinely    nd themselves as part of large-scale consumer prod-
ucts.

on a    rst encounter, there is a mystery surrounding these models.
we refer to them under many different names: as recurrent networks
in deep learning, as state space models in probabilistic modelling, as
dynamical systems in signal processing, and as autonomous and non-
autonomous systems in mathematics. since they attempt to solve the
same problem, these descriptions are inherently bound together and
many lessons can be exchanged between them: in particular, lessons
on large-scale training and deployment for big data problems from
deep learning, and even more powerful sequential models such as
changepoint, factorial or switching state-space models. this post is
an initial exploration of these connections.

4.1 recurrent neural networks

recurrent networks [14] take a functional viewpoint to sequence mod-
elling. they describe sequence data using a function built using recur-
sive components that use feedback from hidden units at time points
in the past to inform computations of the sequence at the present.
what we obtain is a neural network where activations of one of the

figure 4: equivalent models:

recurrent networks and state-space

models.

15

xt+1htht 1time delayxtxt+1xtxt 1htht 1ht+1p(ht|f(ht 1))htht 1ht+1xt+1xtxt 1xtxt+1xt+2f(ht 1)simple recurrent networknetwork unfolded over timestate-space graphical modelhidden layers feeds back into the network along with the input (see
   gures). such a recursive description is unbounded and to practically
use such a model, we unfold the network in time and explicitly rep-
resent a    xed number of recurrent connections. this transforms the
model into a feed-forward network for which our familiar techniques
can be applied.

if we consider an observed sequence x, we can describe a loss func-
tion for id56s unfolded for t steps as:

feedback : ht = f`(h<t, xt   1)

t(cid:88)

loss : j(  ) =

t=1

d (xt, ht)

the model and corresponding id168 is that of a feed-forward
network, with d(  ) an appropriate distance function for the data be-
ing predicted, such as the squared loss. the difference from stan-
dard feed-forward networks is that the parameters of the recursive
function f are the same for all time points, i.e.
they are shared
across the model. we can perform parameter estimation by averaging
over a mini-batch of sequences and using stochastic id119
with application of the id26 algorithm. for recurrent net-
works, this combination of unfolding in time and id26 is
referred to as id26 through time (bptt) [15].

since we have simpli   ed our task by always considering the learn-
ing algorithm as the application of sgd and backprop, we are free to
focus our energy on creative speci   cations of the recursive function.
the simplest and common recurrent networks use feedback from one
past hidden layer earlier examples include the elman or jordan net-
works. but the true workhorse of current recurrent deep learning is
the long short-term memory (lstm) network [16]. the transition
function in an lstm produces two hidden vectors: a hidden layer h,
and a memory cell c, and applies the function f composed of soft-
gating using sigmoid functions   (  ) and a number of weights and
biases (e.g., a, b, a, b):

input : it =   (axt + bht   1 + dct   1 + a)

forget : ft =   (ext + fht   1 + gct   1 + b)

cell : ct = ftct   1 + it tanh(hxt + ght   1 + d)

output : ot =   (kxt + lht   1 + mct + e)

hidden : ht = ot tanh(ct)

16

4.2

probabilistic dynamical systems

we can also view the recurrent network construction above using a
probabilistic framework (relying on reasoning used in part i of this
series). instead of viewing the recurrent network as a recursive func-
tion followed by unfolding for t time steps, we can directly model
a sequence of length t with latent (or hidden) dynamics and spec-
ify a probabilistic graphical model. both the latent states h and the
observed data x are assumed to be probabilistic. the transition proba-
bility is the same for all time, so this is equivalent to assuming the pa-
rameters of the transition function are shared. we could refer to these
models as stochastic recurrent networks; the established convention
is to refer to them as dynamical systems or state-space models.

in probabilistic modelling, the core quantity of interest is the prob-
ability of the observed sequence x, computed as follows:

(cid:90)

(cid:89)

p(x1, . . . xt ) =

p(xt, ht)dht

t

p(xt, ht) = p(xt|ht)p(ht|ht   1)

using id113, we can obtain a loss func-
tion based on the log of this marginal likelihood. since for recurrent
networks the transition dynamics is assumed to be deterministic, we
can easily recover the id56 id168:

det. dynamics : p  (ht|ht   1) =    (ht = f  (ht   1, xt   1))

(cid:90)

(cid:88)
(cid:88)

t

loss : j(  ) =

log

p(ht|ht   1)p(xt|ht)dht

=    j(  ) =

t

log p(xt|f  (ht   1, xt   1))

which recovers the original id168 with the distance function
given by the log of the chosen likelihood function. it is no surprise
that the id56 loss corresponds to id113
with deterministic dynamics.

as machine learners we never really trust our data, so in some cases
we will wish to consider noisy observations and stochastic transitions.
we may also wish to explore estimation beyond maximum likelihood.
a great deal of power is obtained by considering stochastic transi-
tions that transform recurrent networks into probabilistic generative
temporal models [17, 18] models that account for missing data, al-
low for denoising and built-in regularisation, and that model the se-
quence density. we gain new avenues for creativity in our transitions:
we can now consider states that jump and random times between
different operational modes, that might reset to a base state, or that
interact with multiple sequences simultaneously.

17

but when the hidden states h are random, we are faced with the prob-
lem of id136. for certain assumptions such as discrete or gaus-
sian transitions, algorithms for id48 and kalman
   lters, respectively, demonstrate ways in which this can be done.
more recent approaches use variational id136 or particle mcmc
[17]. in general, ef   cient id136 for large-scale state-space models
remains an active research area.

4.3

prediction, filtering and smoothing

dynamical systems are often described to make three different types
of id136 problems explicit: prediction,    ltering and smoothing
[18].

    prediction (inferring the future) is the    rst use of most ma-
chine learning models. having seen training data we are asked
to forecast the behaviour of the sequence at some point k time-
steps in the future. here, we compute the predictive distribu-
tion of the hidden state, since knowing this allows us to predict
or generate what would be observed:

p(ht+k|y1,...t)

    filtering (inferring the present) is the task of computing the
marginal distribution of the hidden state given only the past
states and observations.

p(ht|y1,...,t)

    smoothing (inferring the past) is the task of computing the
marginal distribution of the hidden state given knowledge of
the past and future observations.

p(ht|y1,...,t ), t < t

.

these operations neatly separate the different types of computations
that must be performed to correctly reason about the sequence with
random hidden states. for id56s, due to their deterministic nature,
computing predictive distributions and    ltering are realised by the
feed-forward operations in the unfolded network. smoothing is an
operation that does not have a counterpart, but architectures such as
bi-directional recurrent nets attempt to    ll this role.

4.4

summary

recurrent networks and state space models attempt to solve the same
problem: how to best reason from sequential data. as we continue

18

research in this area, it is the intersection of deterministic and prob-
abilistic approaches that will allow us to further exploit the power
of these temporal models. recurrent networks have been shown to
be powerful, scalable, and applicable to an incredibly diverse set of
problems. they also have much to teach in terms of initialisation, sta-
bility issues, gradient management and the implementation of large-
scale temporal models. probabilistic approaches have much to offer
in terms of better regularisation, different types of sequences we can
model, and the wide range of probabilistic queries we can make with
models of sequence data. there is much more that can be said, but
these initial connections make clear the way forward.

19

5

g e n e r a l i s at i o n a n d r e g u l a r i s at i o n

we now routinely build complex, highly-parameterised models in an
effort to address the complexities of modern data sets. we design our
models so that they have enough    capacity   , and this is now second
nature to us using the layer-wise design principles of deep learning.
but some problems continue to affect us, those that we encountered
even in the low-data regime, the problem of over   tting and seeking
better generalisation.

the classical description of deep feedforward networks in part 1 or
of recurrent networks in part 4 established maximum likelihood as
the the underlying estimation principle for these models. maximum
likelihood (ml) [19] is an elegant, conceptually simple and easy to
implement estimation framework. and it has several statistical advan-
tages, including consistency and asymptotic ef   ciency. deep learning
has shown just how effective ml can be. but it is not without its
disadvantages, the most prominent being a tendency for over   tting.
over   tting is the problem of all statistical sciences, and ways of deal-
ing with this are abound. the general solution reduces to considering
an estimation framework other than maximum likelihood this penul-
timate post explores some of the available alternatives.

5.1 regularisers and priors

the principle technique for addressing over   tting in deep learning
is by regularisation adding additional penalties to our training ob-
jective that prevents the model parameters from becoming large and
from    tting to the idiosyncrasies of the training data. this trans-
forms our estimation framework from maximum likelihood into a
maximum penalised likelihood, or more commonly maximum a pos-
teriori (map) estimation (or a shrinkage estimator). for a deep model
with id168 l(  ) and parameters   , we instead use the modi   ed
loss that includes a regularisation function r:

(cid:88)

l(  ) =    

n

log p(yn|xn,   ) + 1
  

r(  )

   is a regularisation coef   cient that is a hyperparameter of the
model. it is also commonly known that this formulation can be de-
rived by considering a probabilistic model that instead of a penalty,

20

table 2: common priors and regularisers

name
l2/gaussian/weight decay
l1/laplace/lasso
p-norms
total variation
fused lasso
cauchy

1

2

1

r(   )
  (cid:107)  (cid:107)2
  (cid:107)  (cid:107)1
(cid:107)  (cid:107)p; p > 0
(cid:80)
  |     |;       = (  j       j   1)
  |  | +   |     |
i log(  2
   

i +   2)

p(  )
n(  |0;   )
l(cid:97)   (  |0;   )
exp(     (cid:107)  (cid:107)p)

1
    

  2

(       )2+  2

figure 5: contours showing the shrinkage effects of different priors.

introduces a prior id203 distribution over the parameters. the
id168 is the negative of the log joint id203 distribution:

(cid:88)

log p(y,   ) =

log p(yn|xn,   ) + log p(  |  )

n

the table shows some common regularisers, of which the l1 and
l2 penalties are used in deep learning. most other regularisers in the
probabilistic literature cannot be added as a simple penalty function,
but are instead given by a hierarchical speci   cation (and whose op-
timisation is also more involved, requiring some form of alternating
optimisation). amongst the most effective are the sparsity inducing
penalties such as automatic relevance determination, the normal-
inverse gaussian, the horseshoe, and the general class of gaussian
scale-mixtures.

5.2

invariant map estimators

while these regularisers may prevent over   tting to some extent, the
underlying estimator still has a number of disadvantages. one of
these is that map estimators are not invariant to smooth reparameter-
isations of the model. map estimators reason only using the density
of the posterior distribution on parameters and their solution thus
depends arbitrarily on the units of measurement we use. the effect
of this is that we get very different gradients depending on our units,
with different scaling and behaviour that impacts our optimisation.
the most general way of addressing this is to reason about the entire
distribution on parameters instead. another approach is to design an
invariant map estimator [20], where we instead maximise the modi-
   ed probabilistic model:

21

p(x|  )p(  )|i(  )|    1

2

where i(  ) is the fisher information matrix. it is the introduction
of the fisher information that gives us the transformation invariance,
although using this objective is not practically feasible (requiring up
to 3rd order derivatives). but this is an important realisation that
highlights an important property we seek in our estimators. inspired
by this, we can use the fisher information in other ways to obtain
invariant estimators (and better-behaved gradients). this builds the
link to, and highlights the importance of the natural gradient in deep
learning, and the intuition and use of the minimum message length
from id205 [21].

5.3 dropout: with and without id136

since the l2 regularisation corresponds to a gaussian prior assump-
tion on the parameters, this induces a gaussian distribution on the
hidden variables of a deep network.
it is thus equally valid to in-
troduce regularisation on the hidden variables directly. this is what
dropout [22], one of the major innovations in deep learning, uses
to great effect. dropout also moves a bit further away from map
estimation and closer to a bayesian statistical approach by using ran-
domness and averaging to provide robustness.

consider an arbitrary linear transformation layer of a deep network
with link/activation function   (  ), input h, parameters w and the
dimensionality of the hidden variable d. rather than describing the
computation as a warped linear transformation, dropout uses a mod-
i   ed probabilistic description. for i = 1, ...d, we have two types of
dropout:

bernoulli: zi     bern(zi|  i)   i = 1

2 (default)

gaussian: zi     n(zi|1,   2)   2 = 1(default)

dropout layer: y =   (w(h     z) + b)

in the bernoulli case, we draw a 1/0 indicator for every variable in
the hidden layer and include the variable in the computation if it is
1 and drop it out otherwise. the hidden units are now random and
we typically call such variables latent variables. dropout introduces
sparsity into the latent variables, which in recent times has been the
subject of intense focus in machine learning and an important way to
regularise models. a feature of dropout is that it assumes that the
the dropout (or sparsity id203) is always known and    xed for
the training period. this makes it simple to use and has shown to
provide an invaluable form of regularisation.

you can view the indicator variables z as a way of selecting which

22

of the hidden features are important for computation of the current
data point. it is natural to assume that the best subset of hidden fea-
tures is different for every data point and that we should    nd and
use the best subset during computation. this is the default view-
point in probabilistic modelling, and when we make this assumption
the dropout description above corresponds to an equally important
tool in probabilistic modelling that of models with spike-and-slab
priors [23]. a corresponding spike-and-slab-based model, where the
indicator variables are called the spikes and the hidden units, the
slabs, would be:

(cid:89)

spike and slab: zi     bern(zi|  i)

p(y|z, h,   ) =

n(yi|zihi, zi  2
i )

i

we can apply spike-and-slab priors    exibly: it can be applied to
individual hidden variables, to groups of variables, or to entire lay-
ers. in this formulation, we must now infer the sparsity id203
p(z|y, h)
this is the hard problem dropout sought to avoid by as-
suming that the id203 is always known. nevertheless, there has
been much work in the use of models with spike-and-slab priors and
their id136, showing that these can be better than competing ap-
proaches [24]. but an ef   cient mechanism for large-scale computation
remains elusive.

5.4

summary

the search for more ef   cient parameter estimation and ways to over-
come over   tting leads us to ask fundamental statistical questions
about our models and of our chosen approaches for learning. the
popular id113 has the desirable consistency
properties, but is prone to over   tting. to overcome this we moved to
map estimation that help to some extent, but its limitations such as
lack of transformation invariance leads to scale and gradient sensi-
tivities that we can seek to ameliorate by incorporating the fisher
information into our models. we could also try other probabilis-
tic regularisers whose unknown distribution we must average over.
dropout is one way of achieving this without dealing with the prob-
lem of id136, but were we to consider id136, we would hap-
pily use spike-and-slab priors. ideally, we would combine all types
of regularisation mechanisms, those that penalise both the weights
and activations, assume they are random and that average over their
unknown con   guration. there are many diverse views on this issue;
all point to the important research still to do.

23

6

w h at i s d e e p ?

throughout this series, we have discussed deep networks by exam-
ining prototypical instances of these models, e.g., deep feed-forward
networks, deep auto-encoders, deep generative models, but have not
yet interrogated the key word we have been using. we have not posed
the question what does    deep    mean, and what makes a model deep.
there is little in way of a detailed discussion as to what constitutes
a    deep    model and can be hard to do     it seems appropriate as a
closing attempt to provide one such view.

arguably, deep learning today means much more than a description
of a class of useful models. it espouses the use of powerful non-linear
models, models that provide uni   ed id168s that allow for end-
to-end training, machine learning approaches designed from the be-
ginning to be scalable and amenable to large data sets, and to com-
putational methods that fully exploit modern computing resources.
while these other factors have been most remarkably demonstrated
with deep learning, these are goals shared with all other areas of
machine learning. what is of central importance is    deep    as a char-
acterisation of models and their desirable features.

6.1 deep and id187

if we look into the existing literature, deep learning is generally de-
scribed as the machine learning of deep models. and a deep model
is any model that involves multiple levels of computation, in partic-
ular, computation achieved by the repeated application of non-linear
transformations [25] . this is a general framework and the number
of transformations used forms the depth of the model. this is well-
suited as a description of neural networks (or recursive glms), since
we can easily construct a model by recursively applying a linear trans-
formation followed by an element-wise non-linearity, allowing us to
move from id75 models that use only one (non-)linear
transformation (so called    shallow    models) to more complex non-
id75s that use three or more non-linear transformations
(i.e.    deep    models).

to provide a statistical view, we need a slightly more precise frame-
work     and will use that of id187. as a start, we will
characterise deep feed-forward models and then generalise from this

24

figure 6: deep and id187 are abound in machine

learning.

type of model. feed-forward networks are constructed from a succes-
sion of non-linear transformations that form a mapping from inputs
x to targets y with parameters   . representing each transformation
using a    layer      l(z) = fl(wz), the    nal output is obtained by the
composition of layers:

!y =   l       l   1     . . .       0(x);

   = {w}l

l=0

for sigmoidal networks, the activation function f is a sigmoid func-
tion. we were previously able to recast this model as a general
probabilistic regression model p(y|g(x;   ))p(  ), corresponding to a fa-
miliar regularised deep neural network whose loss, with regulariser
r = log p(  ), is:

l = log p(y|g(x;   )) + r(  )

as a probabilistic model, we could instead think of the output of
every layer   l as a random (latent) variable, with the property that
the expectation of a layer is given by the non-linear transformation,
and the sequence of expectations producing the output:

e[hl] =   l = fl(w(l)z) e[y] =   l = e[e[. . . e[h0(x)]]]

it is this characterisation that brings us to id187: mod-
els where its (prior) id203 distributions can be decomposed into
a sequence of conditional distributions [26, ch. 10]:

p(z) = p(z1|z2)p(z2|z3) . . . p(zl   1|zl)p(zl)

this speci   cation implies that the prior is composed of a sequence
of stochastic computations, and satis   es the aspirations we estab-
lished for deep models. a hierarchical construction is not restricted
to regression and is a general framework for all models, including

25

xh2h1yh2h1yh3gatesxh2xh2xh2gatesh2gatesxh2xh2xh2h2yh1yh   1xz2z1h2h   2deep feed-forward regressiondeep directed generative modelhierarchical mixture of expertsdeep multi-view model/information bottleneckdensity estimation, time-series models, spatial statistics, etc. our sig-
moidal networks can instead be written as the following hierarchical
regression model:

p(y|g(x;   )) = p(y|hl)bern(hl|wl   1hl   1)) . . . bern(h1|w0x0))p(  )

at layer l, the inputs from the previous layer

hl

are transformed using a linear mapping into natural parameters of
the bernoulli distribution (into the pre-synaptic activations). since
we perform id113 in the canonical or mean
parameters of this model, there is an implicit transformation of the
natural parameters using a sigmoid function     a link function for
the bernoulli distribution.

the conclusion from this is that one way to view deep feed-forward
networks are as hierarchical probabilistic models. what is impor-
tant though, is that this hierarchy is a hierarchy formed through the
means of the layer-distributions. only the mean parameters at every
layer of the hierarchy depend on computations from previous parts
of the hierarchy, i.e. hierarchies whose dependency is through the
   rst-order structure at every layer of the model.

6.2 characterising deep models

almost all models we use in deep learning are models formed through
hierarchies of the mean. deep generative models are another popular
model class with this characteristic. one widely-known example is a
sigmoid belief network (sbn), a deep directed id114 with
bernoulli latent variables. the hierarchical formulation through the
means is:

p(h2) = bern(h2|  ) p(h1|h2) = bern(h1|w2h2) p(x|h1) = bern(x|w1h1)

other examples are not hard to    nd:

    non-linear gaussian belief networks (id86bns) follow the
same hierarchy as sbns, but use gaussian latent variables, and
form the hierarchy through the gaussian means. and is closely
related to hierarchical ica.
    deep latent gaussian models (dlgms) [7] and deep auto-
regressive networks (darn)[27] form their hierarchy through
the means of gaussian and auto-regressive bernoulli distribu-
tions, respectively.

    deep gaussian processes, a non-parametric analog of the nl-
gbns, are formed through a hierarchical dependency through
its mean functions.

26

    deep exponential families (def), similar to the description
above for deep feed-forward networks, construct a hierarchical
model using one-parameter exponential families. this single
(canonical) parameter controls all moments of the distribution
and often directly encodes the mean, so any hierarchy formed
in this way is a hierarchical model of the means.

    deep id82s (dbm) are graphical undirected
models (i.e. all conditional probabilities and restrictions are
fully speci   ed by its graphical depiction) also form hierarchi-
cal id148 using one-parameter exponential families.

the intuition we obtain from deep learning is that every stage of
computation, every non-linear transformation, allows us to form in-
creasingly abstract representations of the data. statistically, every hid-
den layer allows us to capture more long-range and higher order cor-
relations in the data (after being integrated out). in either view, these
hierarchies are important since they provide a highly ef   cient way
to construct complex models, e.g., in a mixture model we can use
a 2-layer hierarchy using k and l clusters at each layer, effectively
modelling kl clusters     something infeasible with a standard (   at)
mixture model. parts of our hierarchy far away from the data are
indeed more abstract (diffuse and close to the prior), since they have
a small effect on the data distribution: this implies that to effectively
learn with and use deep models, we require large amounts of data.

6.3

beyond hierarchies of the mean

deep models forming their hierarchies through the mean parameters
are amongst the most powerful and    exible models in the machine
learning toolbox. if you are going to build any hierarchical model, a
hierarchy through the mean is a very good idea indeed. there are
two aspects that follow from this:    rstly, there are many models that
are formed through mean-hierarchies that are not labelled as deep;
secondly, a hierarchy through the mean represents just one way to
build such a hierarchical model.

there are many other interesting models that are formed through
hierarchical constructions, and some include:

    hierarchies on variances:

this is a natural step and is used
in many bayesian models where learning variances is involved.
this does raise interesting research questions as to what as-
sumptions and distributions to use beyond the simple one-parameter
exponential families that are widely used.

    hierarchical mixture models, mixed-membership models and
admixture models: these models form a mixture of mixture-
models. these are not typically called deep, though they could
be called that. as mentioned above, we can easily can represent
kl mixture components using such constructions. they show

27

how different representations can be combined in useful ways,
e.g., if the mixture is over deep feed-forward networks. and
some other interesting instances:

    id110s and multi-level regression models.
    hierarchical dirichlet processes.

as wray buntine points out (see comment on original post)
these are also id187 of the mean, and points
to another aspect that has not been discussed: that of dis-
tributed and partitioned representations. deep learning
emphasises distributed representations (using multivariate
continuous latent variables), and models such as the hdp
show how both distributed and partitioned representations
can be used together to provide powerful and adaptive
models.

    canonical correlation, information bottleneck and multi-

view models.

    multi-level spike-and-slab models.

6.4

summary

one way to characterise models described as deep are as hierarchi-
cal models of means: id187 where the mean at every
layer of the hierarchy depends on computation in previous parts of
the hierarchy. this is a correspondence we    nd in almost all models
characterised as deep in our current practice, whether these be deep
neural networks, deep latent gaussian models, deep exponential fam-
ilies or deep gaussian processes. this is always our    rst approach in
building modern machine learning models, since we can capture a
great deal of the structure underlying our data in this way. but we
also know how to extend our hierarchies in ways that allow us to
specify other structural aspects we may be interested in. while it is
conceptually easy to extend our hierarchies in many ways, techniques
for dealing with hierarchies other than the mean in computationally
ef   cient ways are still missing, and remains one of the important re-
search questions that we face in machine learning.

28

a s h o r t r e v i e w

each post is necessarily short since my aim was to test how concrete
i could frame my thinking within around 1200 words (posts are on
average 1500 words though). thus there are many more discussions,
references and connections that could be added , and is one limita-
tion of these essays.
i do not explicitly discuss convolutional net-
works anywhere. since convolution is a special linear operation we
will not need any special reasoning to form a statistical view. what
does require more reasoning is the statistical connections to pooling
operations and something i   ll hopefully cement in the future. the
invariant map estimators discussed in part 5 show that you could
get an update rule that will involve the inverse fisher, but which is
different from that obtained using the natural gradient, and is a con-
nection that i was unable to establish directly. i did not provide many
examples of the ways that popular deep and statistical methods can
be combined. kernel methods (in part 3) and deep learning can easily
be combined by parameterising the kernel with a neural network, giv-
ing the best of both worlds. i have chosen to view dropout (in part 5)
as a prior assumption that does not require id136, and connected
this to spike-and-slab priors. but there are many other views that are
complementary and valid for this, making a longer discussion of just
this topic something for the future.

i have enjoyed writing and learning through this series; it has been a
wonderful exploration and fun to write. thanks to the many people
who have read, shared and sent me feedback.

29

b i b l i o g r a p h y

[1] c. m. bishop,    neural networks for pattern recognition,    1995.
[2] p. mccullagh and j. a. nelder,    generalized linear models.,   

1989.

[3] p. j. bickel and k. a. doksum,    mathematical statistics, volume

i,    2001.

[4] l. bottou, stochastic id119 tricks. 2012.
[5] p. vincent, h. larochelle, y. bengio, and p.-a. manzagol,    ex-
tracting and composing robust features with denoising autoen-
coders,    in proceedings of the 25th international conference on ma-
chine learning, pp. 1096   1103, 2008.

[6] y. bengio, l. yao, g. alain, and p. vincent,    generalized denois-
ing auto-encoders as generative models,    in advances in neural
information processing systems, pp. 899   907, 2013.

[7] d. j. rezende, s. mohamed, and d. wierstra,    stochastic back-
propagation and approximate id136 in deep generative mod-
els,    in proceedings of the 31st international conference on machine
learning, pp. 1278   1286, 2014.

[8] d. p. kingma and m. welling,    auto-encoding id58,   

arxiv preprint arxiv:1312.6114, 2014.

[9] d. p. kingma, s. mohamed, d. j. rezende, and m. welling,
   semi-supervised learning with deep generative models,    in ad-
vances in neural information processing systems, pp. 3581   3589,
2014.

[10] k. gregor, i. danihelka, a. graves, and d. wierstra,    draw: a
recurrent neural network for image generation,    arxiv preprint
arxiv:1502.04623, 2015.

[11] c. m. bishop, pattern recognition and machine learning, vol. 4,

ch. kernel methods, p. 293. springer new york, 2006. pp. 293.

[12] c. e. rasmussen, gaussian processes for machine learning. mit

press, 2006.

[13] r. m. neal, bayesian learning for neural networks, ch. priors for

in   nite networks, pp. 29   53. 1994.

[14] y. bengio, i. goodfellow, and a. courville, deep learning. mit

press (to appear), 2015.

[15] p. j. werbos,    id26 through time: what it does and
how to do it,    proceedings of the ieee, vol. 78, no. 10, pp. 1550   
1560, 1990.

30

[16] f. gers, long short-term memory in recurrent neural networks.
phd thesis, cole polytechnique fdrale de lausanne, lausanne,
switzerland, 2011.

[17] d. barber, a. t. cemgil, and s. chiappa, bayesian time series mod-

els. cambridge university press, 2011.

[18] s. sarkka, bayesian    ltering and smoothing, vol. 3. cambridge uni-

versity press, 2013.

[19] l. le cam,    maximum likelihood: an introduction,    interna-
tional statistical review/revue internationale de statistique, pp. 153   
171, 1990.

[20] p. druilhet, j.-m. marin, et al.,    invariant {hpd} credible sets and
{map} estimators,    bayesian analysis, vol. 2, no. 4, pp. 681   691,
2007.

[21] i. h. jermyn,    invariant bayesian estimation on manifolds,    an-

nals of statistics, pp. 583   605, 2005.

[22] n. srivastava, g. hinton, a. krizhevsky,

i. sutskever, and
r. salakhutdinov,    dropout: a simple way to prevent neural net-
works from over   tting,    the journal of machine learning research,
vol. 15, no. 1, pp. 1929   1958, 2014.

[23] h. ishwaran and j. s. rao,    spike and slab variable selection:
frequentist and bayesian strategies,    annals of statistics, pp. 730   
773, 2005.

[24] s. mohamed, z. ghahramani, and k. a. heller,    bayesian and
l1 approaches for sparse unsupervised learning,    in proceedings
of the 29th international conference on machine learning (icml-12),
pp. 751   758, 2012.

[25] y. bengio,    learning deep architectures for ai,    foundations and

trends r(cid:13) in machine learning, vol. 2, no. 1, pp. 1   127, 2009.

[26] c. robert, the bayesian choice: from decision-theoretic foundations to
computational implementation. springer science & business media,
2007.

[27] k. gregor, i. danihelka, a. mnih, c. blundell, and d. wierstra,
   deep autoregressive networks,    in proceedings of the 31st inter-
national conference on machine learning, 2014.

31

