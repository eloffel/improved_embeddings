recurrent neural network

                 | jahan@nvidia.com

                 | hryu@nvidia.com /                  | hanbyuly@nvidia.com

1

agenda

introduction to recurrent neural network

2

introduction to id56

3

sequence classification and labeling

the sequence labeling:  
    handwriting recognition
    id103
    language translation
special case of pattern classification: individual data points cannot be assumed to 
be independent. both the inputs and the labels are strongly correlated.

4

example

problem: find linear function which  predicts the next term x(t+1) in the input 
sequence based on n previous elements { x(t-n).. x(t-1) }
auto-regression is classical solution.

w

x(t-3)

x(t-2)

x(t-1)

x(t)

5

feed forward time-delay nn

feed-forward nn (e.g. conv nn) is natural extension of auto-regression:
we use network (non-linear function) with input composed from fixed number of 
sequence elements

auto-regression problem
auto-regression problem

w

x(t-3)

x(t-2)

x(t-1)

x(t)

6

recurrent nn

time delay-nn have fixed, limited temporal memory. how can we predict sequences
with long or multi-scale time-dependencies? 
recurrent neural networks (id56s) use their hidden states work as memory on 
previous inputs:

output           =                   1

hidden state          =                ,                   

input          

7

neural network with memory

previous input affects current output

    we use this for future prediction

current state

previous state

current input

current input

8

an unrolled recurrent neural network

9

flexible id56s

examples

vanilla

nn

image

captioning

sentiment

classification

machine

translation

video

classification

10

character level example

vocabulary:
[h,e,l,o]

example training
sequence:
   hello

11

recurrent nn

let   s unfold id56 in time:

          =     3             

          + 1 =     3              + 1

             1

         =     1              

+     2                  1

         + 1 =     1               + 1

+     2             

         

          + 1

warning: we skip bias and non-linear function for simplicity!

12

recurrent nn: training

id56 training is done by using gradient back-propagation recursively back in time 

from t to 1. first let propagate 

        
        

    

        
        

         1

        
       

         1

        
        

         1

        
        

    

        
       

     =     3    

        
        

     +     2    

        
       

     + 1

        
       

     + 1

        
        

     =     1    

        
       

    

13

recurrent nn: training

after we have 

        
       

     we compute 

        
        

to the weights over all t=1..t

     : we sum the    instant    derivatives with respect 

        
        

    

        
        3

=  

        
        

               (t)

        

=  

        
       

     + 1        (t)

        
       

    

=  

        
       

             (t)

         

14

        
        2
        
        1

bi-directional id56

for many problems (nlp, handwriting, id103, protein structure 
prediction,   ) it is beneficial to have access to future as well as to past context.
bidirectional id56: split recurrent layers into two separate recurrent layers both of 
which are connected to the same output layer. there is no information flows 
between the forward and backward hidden layers to make sure that the unfolded 
graph is acyclic

15

bi-directional id56

forward pass:  
1.
2.

the input sequence is presented in opposite directions to the two hidden layers
the output layer is not updated until both hidden layers have processed the 
entire input sequence

16

bi-id56: back-propagation

back-propagation is similar to unidirectional id56 except that:
1. compute  all the gradients for output layer
2.

send gradients back to the two hidden layers in opposite directions

17

id103 with id56

18

traditional id103 flow

front
end:
mfcc

acoustic-1 

acoustic-2

gmm

id48 / wfst

word 

pronunciation

id48 / wfst

id165

language

model

1. the waveform spoken by a user is split into small consecutive slices or    frames    of 10 

milliseconds of audio. each frame is analyzed for its frequency content, and the 
resulting feature vector is passed to an acoustic model 

2. acoustic model gmm outputs a id203 distribution over all the phonemes (sounds) 

3. a hidden markov model (id48) impose temporal structure on this sequence of id203 

distributions. 

4. this is then combined with other knowledge sources such as a pronunciation model that 

links sequences of sounds to valid words in the target language

5.

language model that expresses how likely given word sequences.

19

nn-based id103 flow

front
end:
mfcc

acoustic model

id56 

word 

pronunciation

id48 / wfst

nn

language

model

1. the waveform spoken by a user is split into small consecutive slices or    frames    of 10 

milliseconds of audio. each frame is analyzed for its frequency content, and the 
resulting feature vector is passed to an acoustic model 

2. new acoustic model based on recurrent nn

3. this is then combined with other knowledge sources such as a pronunciation model that 

links sequences of sounds to valid words in the target language

4. nn-based langiuage model

20

time-delay neural network (td-nn)

td-nn was proposed for phoneme recognition 
in  [weibel, hinton, 1989]. 

3 layers nn:
    8 units in 1st layer,  3 frames
    3 units in 2nd layer; 5 frames
    higher layers to make decisions based on 

wider range in time

21

acoustic recognition with nn

the basic approach:

1. transform speech signal into sequence of 

spectrograms (2d representation of an 
acoustic signal based on  fft applied to 
window of speech). 

2. apply nn technique to    read    this 2d 

spectrogram

22

google voice search

google just announced that they improved their voice search using acoustic models 
based on id56 and ctc (connectionist temporal classification).

example: "how cold is it outside". 
the ctc model outputs spikes as it identifies various phonemes (shown in different colors) in 
the input speech signal. the x-axis shows the acoustic input timing for phonemes and y-axis 
shows the posterior probabilities as predicted by the neural network. the dotted line shows 
where the model chooses not to output a phoneme.

https://www.youtube.com/watch?v=5_9soz3d41g&feature=youtu.be
http://googleresearch.blogspot.co.uk/2015/09/google-voice-search-faster-and-more.html

23

deepspeech (baidu)

id56-based automatic  id103
1. transcribe acoustic directly into letters
2. decode transcription (fix    spelling   )

transcription is based on id56 only.
    no hand-designed models for background noise, 

reverberation, or speaker variation 

    no phoneme dictionary
    no id48 / wfst

24

deepspeech

input: the spectrogram frame x(t) 
along with a context of c frames on 
each side

25

deepspeech

layers h1   h2-h3 : fc with clipped relu

     = g                     
       
          = min(max 0,      , 20)

       1 +         

26

deepspeech

layer h4 is bi-directional recurrent 
layer, splitted into 2 sub-layers:
                   1
                +1

     =          4            
       
     =          4            
       

     +    4
     +    4

3 +        
3 +        

     must be computed sequentially: 1 -> t 
       
     must be computed in reverse : t -> 1
       

27

deepspeech

last layer h5  is regular fc layer:
    ) +     5

3 =          5     (       
       

    +       

28

practice

whale sound classification

29

natural language processing with id56

30

feed forward nn     based language model

feed-forward nn (e.g. conv. nn) can be used to predict next word from k previous
words

x(t-k)

x(t-2)

x(t-1)

x(t)

dictionary= 100,000     net has 100,000 outputs. can we scale nn to 100,000 outputs? 
is there a better way to deal with such a large number of outputs?

31

id56 - based language model

id56 allows to extend fixed input window used by td-nn

input layer w and output layer y have the same dimensionality as 
the vocabulary (10k - 200k). 

hidden layer s is small (50 - 1000 neurons)

network parameters:

u - weights between input and hidden layer
v - weights between hidden and output layer
w     recurrent weights

32

training of id56 language model

id56 is trained by unfolding in time and training 
as a deep feed-forward nn.
gradients are propagated back through time

33

training of id56 language model

example: batch training on input sequence 

34

id56 vs id165 language models

wsj-20k, kaldi

3

35

id27

the classical nlp regards words as atomic symbols. each word is represented as one   hot vector 
[0000010000]

motel [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] and
hotel  [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] = 0 

id27 - a word is represented as a dense vector 

36

code a word through its context 

socher-manning tutorial   http://www.socher.org/index.php/deeplearningtutorial/deeplearningtutorial

37

nn to learn id27

the basic idea: 

1. define score function  that is 1 on true sequence and 0 on corrupted:

score(cat chills on a mat)    ->  1
score(cat chills jeju a mat) ->  0

2. each word is associated with vector rn : [            ]
3. build and train nn 

38

practice

id56 for natural language processing

39

long-short temporal memory id56

40

problem of long term dependencies

id56 has short memory: the influence of a given input 
on the hidden layer, and therefore on the network 
output, either decays or blows up exponentially:

    1 =     1               +     2         0
    2 =     1          2 +     2         1
=     1          2 +     2     (    1          1 +     2         0)
2         0
=     1          2 +                                     +     2
         =     1               +     2         1                   1 +    
+         

                                       +     2

             0

41

long short-term memory (lstm)

long short-term memory (lstm) architecture was introduced to address id56 short 
memory. it has a n new basic element - memory cell 

all gates use logistic 
sigmoid: 

0 - gate closed,
1 - gate open 

output gate    switch-
off    output of the cell 

input gate cut-off 
the input of the cell 

forget gate reset the 
cell's  state 

hochreiter and schmidhuber,1997.

42

id56 sequence representation

43

lstm representation

44

lstm representation

45

cell state

lstm operation
store

forget

output

46

image captioning

47

image captioning

recurrent neural network

convolutional neural network

explain images with multimodal recurrent neural networks, mao et al.
deep visual-semantic alignments for generating image descriptions, karpathy and fei-fei
show and tell: a neural image caption generator, vinyals et al.
long-term recurrent convolutional networks for visual recognition and description, donahue et al.
learning a recurrent visual representation for image id134, chen and zitnick

48

48

test image

49

test image

50

test image

x

51

test image

x0

<sta
rt>

<start>

52

test image

wih

y0

h0

x0

<sta
rt>

before:

h = tanh(wxh * x + whh * h)

now:

h = tanh(wxh * x + whh * h + wih * v)

v

<start>

53

test image

y0

h0

sample!

x0

<sta
rt>

straw

<start>

54

test image

55

y0

y1

h0

h1

x0

<sta
rt>

straw

<start>

test image

y0

y1

h0

h1

x0

<sta
rt>

straw

hat

sample!

<start>

56

test image

y0

y1

y2

h0

h1

h2

x0

<sta
rt>

straw

hat

<start>

57

test image

sample
<end> token
=> finish.

y0

y1

y2

h0

h1

h2

x0

<sta
rt>

straw

hat

<start>

58

id21 with id98s is pervasive   

id164 
(faster r-id98)

image captioning: id98 + id56

id98 pretrained 
on id163

word vectors pretrained 
from id97

59

image sentence datasets

microsoft coco
[tsung-yi lin et al. 2014]
mscoco.org

currently:
~120k images
~5 sentences each

60

61

62

63

