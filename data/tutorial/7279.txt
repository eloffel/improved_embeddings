practical deep learning

                 | jahan@nvidia.com

                 | hryu@nvidia.com /                  | hanbyuly@nvidia.com

1

agenda

power of small filters

gpu computing & parallelism

floating point precision 

2

power of small filters

3

stacking convolution layers

suppose we stack two 3x3 conv layers (stride 1)
each neuron sees 3x3 region of previous activation map

input

first conv

second conv

4

stacking one more layer

x

x

x

three 3 x 3 conv gives similar
representational power as a single
7 x 7 convolution

5

1x(7x7) convolution vs. 3x(3x3) convolution

one conv with 7 x 7 filters

three conv with 3 x 3 filters

number of weights:
= c x (7 x 7 x c) = 49 c2

number of multiply-adds:
= (h x w x c) x (7 x 7 x c)
= 49 hwc2

number of weights:
= 3 x c x (3 x 3 x c) = 27 c2

number of multiply-adds:
= 3 x (h x w x c) x (3 x 3 x c)
= 27 hwc2

more weight
more compute

less weight
less compute

6

convolution filter smaller than 3x3?

7

1x1 convolution

bottleneck layer

h x w x c

conv 1x1, c/2 filters

h x w x (c / 2)

conv 3x3, c/2 filters

h x w x (c / 2)

conv 1x1, c filters

1.    bottleneck    1 x 1 conv

to reduce dimension

2. 3 x 3 conv at reduced 

dimension

3. restore dimension 

with another 1 x 1 conv

h x w x c

[seen in lin et al,    network in network   , 
googlenet, resnet]

8

h x w x c

conv 1x1, c/2 filters

h x w x (c / 2)

conv 3x3, c/2 filters

h x w x (c / 2)

conv 1x1, c filters

h x w x c

more nonlinearity,

fewer params, 
less compute!

3.25 c2

parameters

h x w x c

conv 3x3, c filters

9 c2

parameters

h x w x c

9

breaking 3x3 filter

divide & conquer

h x w x c

conv 1x3, c filters

h x w x c

conv 3x1, c filters

h x w x c

6 c2

parameters

h x w x c

conv 3x3, c filters

9 c2

parameters

h x w x c

10

recent googlenet
new more inception layers

szegedy et al,    rethinking the inception architecture for id161   

11

gpu computing

12

gpu for computing
a good friend for computing

13

tesla: gpu accelerated computing platform

focused on co-design for accelerated data center

fast gpu

engineered for high throughput

tflops

nvidia gpu

x86 cpu

productive

programming
model & tools

5.5

5.0

4.5

4.0

3.5

3.0

2.5

2.0

1.5

1.0

0.5

0.0

k20

m2090

m1060

p100

k80

fast gpu

+

strong cpu

2008

2010

2012

2014

2016

expert 
co-design

application

middleware

sys sw

large systems

processor

accessibility

14

no more free lunch

so many things to consider for high performance

15

gpu are really good at id127

which neural network does

gpu: nvida tesla k40
with cublas

cpu: intel e5-2697 v2
12 core @ 2.7 ghz
with mkl

16

extraordinary strong scaling
one strong node faster than lots of weak nodes

caffe alexnet performance

vasp performance

single p100 nvlink-enabled node vs lots of weak nodes

single p100 pcie node vs lots of weak nodes

8x p100

4x p100

2x p100

64x

48x

32x

16x

e
d
o
n
 
r
e
v
r
e
s
 
u
p
c
 
1
 
s
v
 
p
u
-
d
e
e
p
s

0x

0

32 cpu nodes

64 cpu nodes

8x p100

4x p100

2x p100

e
d
o
n
 
r
e
v
r
e
s
 
u
p
c
 
1
 
s
v
 
p
u
-
d
e
e
p
s

12x

10x

8x

6x

4x

2x

0x

16

32

48

64

0

4

8

12

16

20

24

28

32

# of cpu server nodes

# of cpu server nodes

cpu: dual socket intel e5-2680v3 12 cores, 128 gb ddr4 per node, fdr ib
vasp 5.4.1_05feb16, si-huge dataset.  16, 32 nodes are estimated based on same scaling from 4 to 8 nodes
caffe alexnet scaling data: https://software.intel.com/en-us/articles/caffe-training-on-multi-node-distributed-memory-systems-based-on-intel-xeon-processor-e5

17

ten years of gpu computing

world   s first atomic 
model of hiv capsid

gpu-trained ai machine 

beats world champion in go

oak ridge deploys world   s 

fastest supercomputer w/ gpus

fermi: world   s 
first hpc gpu

alexnet beats expert code 
by huge margin using gpus

stanford builds ai 
machine using gpus

google outperforms 
humans in id163

world   s first gpu 

top500 system

cuda launched

discovered how h1n1 
mutates to resist drugs

world   s first 3-d mapping 

of human genome

2006

2008

2010

2012

2014

18
2016

need more computing power?

even with gpus, training can be slow

vgg: ~2-3 weeks training with 4 gpus
resnet 101: 2-3 weeks with 4 gpus

nvidia titan blacks
~$1k each

resnet reimplemented in torch: http://torch.ch/blog/2016/02/04/resnets.html

19

multigpu training

alex krizhevsky,    one weird trick for parallelizing convolutional neural networks   

20

tensorflow approach
synchronous data parallelism

abadi et al,    tensorflow: large-scale machine learning on heterogeneous distributed systems   

21

caffe   s approach 

tree reduction

exchange

update

the current implementation uses a tree reduction strategy. e.g. if there are 4 gpus in the 
system, 0:1, 2:3 will exchange gradients, then 0:2 (top of the tree) will exchange gradients, 
0 will calculate updated model, 0->2, and then 0->1, 2->3.

22

distributed cpu training

data parallelism

model parallelism

[large scale distributed deep networks, jeff dean et al., 2013]

23

bottlenecks

24

cpu     gpu communication bottleneck

gpu communicate path

cpu: data augment and prefetching

with multi-threads

gpu: training

25

storage bottleneck

    hdd is slow to read
    pre-processed images stored contiguously in files, read as raw byte stream from 

ssd disk

id163 recipes from a practical view, yangqing jia

26

gpu memory bottleneck

titan x: 12 gb 
tesla p100: 16 gb
tesla p40: 24gb

e.g.
alexnet: ~3gb needed with batch size 256

27

floating point precision

28

floating precision

32 bit    single    precision is typically used for 
id98s for performance

prediction and statues: 
16 bit    half    precision will be the new standard
gpu already support fp16 on memory
hardware support is ready for p100
frameworks need to be ready for hw support

benchmarks on titan x, from 

https://github.com/soumith/convnet-benchmarks

29

fp16 training?

how low can we go?

gupta et al, 2015: 
train with 16-bit fixed point with stochastic rounding

gupta et al,    deep learning with limited numerical precision   , icml 2015

30

trained quantization

han et al. deep compression: compressing deep neural networks with pruning, trained 
quantization and huffman coding, iclr 2016 (best paper)

31

precision

32

int8 id136

nvidia readied such hardware accelerator

33

2-6 bits/weight sufficient

id136 for ann/id98

han et al. deep compression: compressing deep neural networks with pruning, trained 
quantization and huffman coding, iclr 2016 (best paper)

34

lower computing cost

in energy and area

energy numbers are from mark horowitz    computing   s energy problem (and what we can do about it)   , isscc 2014 area numbers are from synthesized 
result using design compiler under tsmc 45nm tech node. fp units used designware library.

35

deep learning r&d infra

36

users

storage

network

compute node

network storage

dgx-1

dgx-1

dgx-1

file server

login server

100gps edr ib

1gps or 10gps ethernet

os storage :  480gb ssd

local cache : 7tb ssd with raid0

lustre/
gfs/hfs

100gps edr ib

ethernet

fc for storage

37

scenario

network

step1

step7

step3

step5

step6

file server

login server

users

step2

step4

compute node

net storage

dgx-1

dgx-1
dgx-1dgx-1

1gps or 10gps ethernet

100gps edr ib

step1. login server 
step2. assign dedicate dgx-1 in scheduler  (slrum/lsf/pbs/sge)
step3.  pull 3tb dataset from network file system to local cache
step4. training with dgx-1
step5. result param in local cache
step6. push result to network storage
step7. release node & logout 

100gps edr ib

ethernet

fc for storage

38

caffe/digits interactive mode

step1 login node 

ssh hryu@psgcluster.nvidia.com

step2 assign empty dgx-1 node
srun --comment=docker --pty bash    i

step3 load dl framework

docker pull nvidia/caffe or  nvidia/digits

nvidia-docker run --rm -p 5000:34448 -v $(pwd):/data -v $(pwd):/jobs nvidia/digits 

39

40

tensorflow interactive mode

step1 login node 

ssh hryu@psgcluster.nvidia.com

step2 assign empty dgx-1 node
srun --comment=docker --pty bash    i

step3 load dl framework

docker pull gcr.io/tensorflow/tensorflow:latest-gpu

nvidia-docker run -it -p 8888:8888 -p 6006:6006 gcr.io/tensorflow/tensorflow:latest-gpu

41

42

assign gpus for docker

use environment variables nv_gpu
nv_gpu=   {gpu index}    nvidia-docker run {docker options}

$nv_gpu=   1,3    nvidia-docker run  

43

gpus in docker 
only 2 gpus within docker

$nv_gpu=   1,3    nvidia-docker run  

2 gpus is assigned in docker

scheduler (slrum/lsf/pbs/sge)

manage resources 

nv_gpu environment

allocate resource to nv_gpu variables

nvdocker

load nv_gpu variables 

44

45

