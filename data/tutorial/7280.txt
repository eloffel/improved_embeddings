deep learning frameworks 
review

                 | jahan@nvidia.com

                 | hryu@nvidia.com /                  | hanbyuly@nvidia.com

1

agenda

sequentials

why tensorflow is slower than others

2

sequentials

3

caffe script for model

simple id98

layer {

name: "pool1"
type: "pooling"
bottom: "conv1"
top: "pool1"
pooling_param {

pool: max
kernel_size: 2
stride: 2

}

}

nvidia confidential. do not distribute.

4

layer {

name: "conv1"
type: "convolution"
bottom: "scale"
top: "conv1"
param {

lr_mult: 0.0

}
param {

lr_mult: 0.0

}

convolution_param {

num_output: 20
kernel_size: 5
stride: 1
weight_filler {

type: "xavier"

}
bias_filler {

type: "constant"

}

}

}

keras sequentials

simple ann

from keras.models import sequential 

from keras.layers import dense, dropout, activation 

model = sequential() 

model.add(dense(64, input_dim=20, init='uniform')) 

model.add(activation('tanh')) 

model.add(dropout(0.5)) 

model.add(dense(64, init='uniform')) 

model.add(activation('tanh')) 

model.add(dropout(0.5)) 

model.add(dense(10, init='uniform')) 

model.add(activation('softmax'))

nvidia confidential. do not distribute.

5

keras sequentials

simple id98

model = sequential() 
model.add(convolution2d(32, 3, 3, border_mode='valid', input_shape=(3, 100, 100))) model.add(activation('relu')) 
model.add(convolution2d(32, 3, 3)) 
model.add(activation('relu')) 
model.add(maxpooling2d(pool_size=(2, 2))) 
model.add(dropout(0.25))

model.add(convolution2d(64, 3, 3, border_mode='valid')) 
model.add(activation('relu')) 
model.add(convolution2d(64, 3, 3)) 
model.add(activation('relu')) 
model.add(maxpooling2d(pool_size=(2, 2))) 
model.add(dropout(0.25)) 

model.add(flatten()) 
model.add(dense(256)) 
model.add(activation('relu')) 
model.add(dropout(0.5))

model.add(dense(10)) 
model.add(activation('softmax'))

nvidia confidential. do not distribute.

6

mxnet sequentials : symbols

alexnet

def get_symbol(num_classes, **kwargs): 

input_data = mx.symbol.variable(name="data") 

conv1 = mx.symbol.convolution( data=input_data, kernel=(11, 11), stride=(4, 4), num_filter=96) 
relu1 = mx.symbol.activation(data=conv1, act_type="relu") 
pool1 = mx.symbol.pooling( data=relu1, pool_type="max", kernel=(3, 3), stride=(2,2)) 
lrn1 = mx.symbol.lrn(data=pool1, alpha=0.0001, beta=0.75, knorm=1, nsize=5) 

conv2 = mx.symbol.convolution( data=lrn1, kernel=(5, 5), pad=(2, 2), num_filter=256) 
relu2 = mx.symbol.activation(data=conv2, act_type="relu") 
pool2 = mx.symbol.pooling(data=relu2, kernel=(3, 3), stride=(2, 2), pool_type="max") 
lrn2 = mx.symbol.lrn(data=pool2, alpha=0.0001, beta=0.75, knorm=1, nsize=5) 

conv3 = mx.symbol.convolution( data=lrn2, kernel=(3, 3), pad=(1, 1), num_filter=384) 
relu3 = mx.symbol.activation(data=conv3, act_type="relu") 
conv4 = mx.symbol.convolution( data=relu3, kernel=(3, 3), pad=(1, 1), num_filter=384) 
relu4 = mx.symbol.activation(data=conv4, act_type="relu") 
conv5 = mx.symbol.convolution( data=relu4, kernel=(3, 3), pad=(1, 1), num_filter=256) 
relu5 = mx.symbol.activation(data=conv5, act_type="relu") 
pool3 = mx.symbol.pooling(data=relu5, kernel=(3, 3), stride=(2, 2), pool_type="max") 
flatten = mx.symbol.flatten(data=pool3) 

fc1 = mx.symbol.fullyconnected(data=flatten, num_hidden=4096) 
relu6 = mx.symbol.activation(data=fc1, act_type="relu") 
dropout1 = mx.symbol.dropout(data=relu6, p=0.5) 
fc2 = mx.symbol.fullyconnected(data=dropout1, num_hidden=4096) 

nvidia confidential. do not distribute.

7

torch sequentials

simple ann

nn.sequential() 

local lenet = nn.sequential() 
lenet:add(nn.mulconstant(0.0125))  
lenet:add(backend.spatialconvolution(channels,20,5,5,1,1,0))  
lenet:add(backend.spatialmaxpooling(2, 2, 2, 2)) 
lenet:add(backend.spatialconvolution(20,50,5,5,1,1,0))  
lenet:add(backend.spatialmaxpooling(2,2,2,2))  
lenet:add(nn.view(-1):setnuminputdims(3))  
lenet:add(nn.linear(800,500))  
lenet:add(backend.relu()) 
lenet:add(nn.linear(500, nclasses))  
lenet:add(nn.logsoftmax()) 

nvidia confidential. do not distribute.

8

troch inception 

features:add(nn.mulconstant(0.02))

features:add(convlayer(nchannels,64,7,7,2,2,3,3)):add(backend.spatialbatchid172(64,1e-3)):add(backend.relu(true))
features:add(backend.spatialmaxpooling(3,3,2,2):ceil())
features:add(convlayer(64,64,1,1)):add(backend.spatialbatchid172(64,1e-3)):add(backend.relu(true))
features:add(convlayer(64,192,3,3,1,1,1,1)):add(backend.spatialbatchid172(192,1e-3)):add(backend.relu(true))
features:add(backend.spatialmaxpooling(3,3,2,2):ceil())
features:add(inception( 192, {{ 64},{ 64, 64},{ 64, 96},{'avg', 32}})) -- 3(a)
features:add(inception( 256, {{ 64},{ 64, 96},{ 64, 96},{'avg', 64}})) -- 3(b)
features:add(inception( 320, {{  0},{128,160},{ 64, 96},{'max',  0}})) -- 3(c)
features:add(convlayer(576,576,2,2,2,2)):add(backend.spatialbatchid172(576,1e-3))
features:add(inception( 576, {{224},{ 64, 96},{ 96,128},{'avg',128}})) -- 4(a)
features:add(inception( 576, {{192},{ 96,128},{ 96,128},{'avg',128}})) -- 4(b)
features:add(inception( 576, {{160},{128,160},{128,160},{'avg', 96}})) -- 4(c)
features:add(inception( 576, {{ 96},{128,192},{160,192},{'avg', 96}})) -- 4(d)

local function inception(input_size, config)

local concat = nn.concat(2)

...

local conv3 = nn.sequential()

...

local conv3xx = nn.sequential()
local pool = nn.sequential()

end

local conv3 = nn.sequential() 

conv3:add(convlayer( input_size, config[2][1],1,1,1,1)):add(backend.relu(true)) 
conv3:add(convlayer(config[2][1], config[2][2],3,3,1,1,1,1)):add(backend.relu
concat:add(conv3

9

why tensorflow is slow?

10

one day   

we found that tensorflow is really slow

1.2

1

0.8

0.6

0.4

0.2

0

o
i
t
a
r
 
e
c
n
a
m
r
o
f
r
e
p

alexnet_owt

googlenet

caffe

tensorflow

resnet

11

nvidia profile result - tensorflow

cifar10 example, 10 iteration

invocations

total bytes (mb)

avg. throughput

duration (ms)

cpu     gpu

gpu     cpu

gpu     gpu

107912

107883

64615

123.256

220.186

258.46

556.42 mb/s

1.455 gb

3.005 mb/s

221.538

151.341

86.024

12

nvidia profile result - caffe

cifar10 example, 10 iteration

invocations

total bytes (mb)

avg. throughput (gb/s)

duration (ms)

htod

dtoh

dtod

379

571

450

172.955

17.973

508.779

11.496

7.872

202.713

15.045

2.283

2.51

13

what we found is,

    tensorflow is gpu operation is somewhat coarse

    gpu utilization is low
    heavy cpu-gpu latency bottleneck

   

low cpu-gpu communication bandwidth

   

it mainly uses eigen3 open source library for id202, not cudnn

14

15

