capturing dependency syntax 
with "deep" sequential models

yoav goldberg 
depling 2017

b i u
n l p

capturing dependency syntax 
with "deep" sequential models

yoav goldberg 
depling 2017

b i u
n l p

capturing dependency syntax 
with "deep" sequential models

yoav goldberg 
depling 2017

eva's talk: "deep" sentential structure

b i u
n l p

b i u
n l p

deep learning

b i u
n l p

deep learning

it learns on its own. 
it works like the brain.

it can do anything.

b i u
n l p

my experience  

with deep learning for language

``i'm sorry dave, 

i'm afraid i can't do that.''

(not in the scary sense)

b i u
n l p

my experience  

with deep learning for language
    with proper tools, easy to produce "innovative" models. 
    not so easy to get good results. 
    with feed-forward nets, hard to beat linear models w/ 

human engineered feature combinations. 

    on 20-newsgroups, naivebayes+tfidf wins over deep 

feed-forward-nets and convnets. 

    semi-sup learning sort-of easy with word-embeddings. 
    id56s (in particular lstms) are really really cool.

b i u
n l p

my experience  

with deep learning for language
    with proper tools, easy to produce "innovative" models. 
    not so easy to get good results. 
    with feed-forward nets, hard to beat linear models w/ 

human engineered feature combinations. 

    on 20-newsgroups, naivebayes+tfidf wins over deep 

feed-forward-nets and convnets. 

    semi-sup learning sort-of easy with word-embeddings. 
    id56s (in particular lstms) are really really cool.

id97

id97
b i u
n l p

i dog

i cat, dogs, dachshund, rabbit, puppy, poodle, rottweiler,

mixed-breed, doberman, pig

i sheep

i cattle, goats, cows, chickens, sheeps, hogs, donkeys,

herds, shorthorn, livestock

i november

i october, december, april, june, february, july, september,

january, august, march

i jerusalem

i tiberias, jaffa, haifa, israel, palestine, nablus, damascus

katamon, ramla, safed

i teva

i p   zer, schering-plough, novartis, astrazeneca,

glaxosmithkline, sano   -aventis, mylan, sano   , genzyme,
pharmacia

b i u
n l p

my experience  

with deep learning for language
    with proper tools, easy to produce "innovative" models. 
    not so easy to get good results. 
    with feed-forward nets, hard to beat linear models w/ 

human engineered feature combinations. 

    on 20-newsgroups, naivebayes+tfidf wins over deep 

feed-forward-nets and convnets. 

    semi-sup learning sort-of easy with word-embeddings. 
    id56s (in particular lstms) are really really cool.

b i u
n l p

my experience  

with deep learning for language
    with proper tools, easy to produce "innovative" models. 
    not so easy to get good results. 
    with feed-forward nets, hard to beat linear models w/ 

human engineered feature combinations. 

    on 20-newsgroups, naivebayes+tfidf wins over deep 

feed-forward-nets and convnets. 

    semi-sup learning sort-of easy with word-embeddings. 
    id56s (in particular lstms) are really really cool.

b i u
n l p

their
very
existence
?

5
3
3
3

0
3
5
5

also an important predictor of reading time. phrases
that are often removed in sentence compression   
like fronted phrases, parentheticals,    oating quanti-
   ers, etc.   are often associated with non-local de-
pendencies. also, there is evidence that people are
more likely to    xate on the    rst word in a con-
stituent than on its second word (hy  on  a and pol-
latsek, 2000). being able to identify constituent
borders is important for sentence compression, and
reading    xation data may help our model learn a rep-
resentation of our data that makes it easy to identify
constituent boundaries.

l

l

l

r

r

r

np

vp

cut

np

vp

vp

vp

vb

their

root

np

vp

np

vp

vb

risks

nns

prp$

take

xcomp

risks

l

nns

nns

their

prp$

improving sequence to sequence learning for 

c
u
t

n
p

v
b

embeddings:

t
h
e
i
r

r
i
s
k
s

p
r
p
$

bilstms are

works for both

lemma
pos
dependency label
direction

vb
state-of-the-art
cut

subj

subj

expected

id33

euclidean distance

simple & e   ective
feature extractors

parsing accuracies
with minimal e   ort

figure 1: example sentence from the dundee corpus

eliyahu kiperwasser and yoav goldberg

graph-based & transition-based
r v
r
p

simple and accurate id33

using bidirectional lstm feature representations

figure 2: illustration of the symmetry scoring component that takes into account the conjuncts syntactic structures. each conjunct
i
tree is decomposed into paths that are fed into the path-lstms (squares). the resulting vectors are fed into the symmetry lstm
i
function (circles). the outcome vectors (blue circles) are then fed into the euclidean distance function.

improving preposition sense disambiguation

doing stuff with lstms

regression duration measures the total
time
spent    xating a word after the gaze has already left
it once. this measure belongs to the group of late
measures, i.e., measures that are sensitive to the later
emnlp 2016 submission ***. con   dential review copy. do not distribute.
cognitive processing stages including interpretation
and integration of already decoded words. since
in the experiments below, we learn models to pre-
the reader by de   nition has already had a chance to
dict the    rst pass duration of word    xations and the
recognize the word, regressions are associated with
total duration of regressions to a word. these two
semantic confusion and contradiction, incongruence
measures constitute a perfect separation of the to-
and syntactic complexity, as famously experienced
tal reading time of each word split between the    rst
in garden path sentences. for this measure the value
pass and subsequent passes. both measures are de-
0 indicates that the word was read at most once by
scribed below. they are both discretized into six
this reader.
bins as follows with only non-zero values contribut-
ing to the calculation of the standard deviation (sd):
1. the task
abstract
v
parsing
p
0: measure = 0 or
              
1: measure < 1 sd below reader   s average or
context rich feature
2: measure < .5 sd below reader   s average or
          
many hand-crafted features:
3: measure < .5 above reader   s average or
4: measure > .5 sd above reader   s average or
5: measure > 1 sd above reader   s average
writing
classi   cation

288
289
290
291
292
293
294
295
296
be
to
297
298
299
300
301
figure 2: an illustration of term-pair classi   cation. each term-pair is represented by several paths. each path is a sequence of
edges, and each edge consists of four components: lemma, pos, dependency label and dependency direction. each edge vector
is fed in sequence into the lstm, resulting in a path embedding vector ~op. the averaged path vector becomes the term-pair   s
302
prepositions are very common, very ambiguous and tend to carry different 
feature vector, used for classi   cation. the dashed ~vwx , ~vwy vectors refer to the integrated network described in section 3.2.
m lp
meanings in different contexts. 
303
the 50-dimensional and 100-dimensional embed-
vjumped
ding vectors and selected the ones that yield bet-
304
concat
ter performance on the validation set.4 the other
preposition-sense disambiguation is a task of assigning a category to a 
embeddings, as well as out-of-vocabulary lemmas,
preposition in context: 
305
are initialized randomly. we update all embedding
lst m b
lst m b
vectors during training.
306
yf
4
3.2
lst m f
307
the network presented in section 3.1 classi   es
each (x, y) term-pair based on the paths that con-
xjumped
308
nect x and y in the corpus. our goal was to im-
prove upon previous path-based methods for hy-
- can we improve performance by using unannotated data?
pernymy detection, and we show in section 6
309
that our network indeed outperforms them. yet,
- are translations of prepositions to other languages predictive for this task? 
as path-based and distributional methods are con-
310
t
u
- how can we use multilingual corpora for learning a representation of the 
sidered complementary, we present a simple way
u
0
to integrate distributional features in the network,
context that can be used for sense-disambiguation?
311
yielding improved performance.
312

term-pair classi   cation each (x, y) term-pair
is represented by the multiset of lexico-syntactic
paths that connected x and y in the corpus, de-
noted as paths(x, y), while the supervision is
given for the term pairs. we represent each (x, y)
s2sb
2
term-pair as the weighted-average of its path vec-
tors, by applying average pooling on its path vec-
   you should book a room for 2 nights            duration
tors, as follows:
   for some reason, he is not here yet            explanation 
~vxy = ~vpaths(x,y) = pp2paths(x,y) fp,(x,y)   ~op
   i went there to get a present for my mother           beneficiary
pp2paths(x,y) fp,(x,y)
where fp,(x,y) is the frequency of p in paths(x, y).
we then feed this path vector to a single-layer net-
work that performs binary classi   cation to decide
whether y is a hypernym of x.

first pass duration measures the total time spent
given two spans of lengths k and m with cor-
2. previous work
reading a word    rst time it is    xated, including
any immediately following re-   xations of the same
responding vector sequences u1:k and v1:m we en-
word. this measure correlates with word length, fre-
code each sequences using an lstm, and take the
quency and ambiguity because long words are likely
to attract several    xations in a row unless they are
euclidean distance between the resulting representa-
particularly easily predicted or recognized. this ef-
tions:
fect arises because long words are less likely to    t
inside the fovea of the eye. note that for this mea-
sure the value 0 indicates that the word was not    x-
v   
ated by this reader.
concat

the network is trained such that the distance is min-
imized for compatible spans and large for incompat-
ible ones in order to learn that vectors that represents
correct conjuncts are closer than vectors that do not
represent conjuncts.
transition-based parsing algorithm (greedy optimization)
i m    n
i
5    n
what are the elements in the sequences to be com-
c = sof tmax(w    ~vxy)
arc-hybrid neural parser
pared? one choice is to take the vectors ui to cor-

see table 1 for an example of    rst pass duration
and regression duration annotations for one reader
n
l v
n
r
and sentence.
n
p
p
s
feminine, present, passive, singular

morphological inflection generation
336
337
roee aharoni and yoav goldberg       yonatan belinkov
338
bar ilan university nlp lab                  mit csail      
339
340
341
i
342
i
343
344
i
n
 
 
345
f
f
346
347
348
349
350
351
352
353
354
e step
355
5,5
4,5
f
f
356
357
358
359
360

this decomposition captures the syntactic context
5. ndst network architecture
of each word, but does not uniquely determine the
our second approach, the neural discriminative string transducer (ndst):
structure of the tree. to remedy this, we add to
the paths special symbols, r and l, which marks
the lowest common ancestors with the right and left
step
3,3
words respectively. these are added to the path
f
above the corresponding nodes. for example con-
morphological  templates 
sider the following paths which corresponds to the
instead  of  training  the  network  to 
predict only a speci   c character at 
above syntactic structure:
each step, we train the network to 
either predict a character or copy a 
character at a given position in the 

figure 2: multitask and cascaded bi-lstms for sentence com-
pression. layer l 1 contain pre-trained embeddings. gaze
prediction and id35-tag prediction are auxiliary training tasks,
and loss on all tasks are propagated back to layer l0.

with representations learned from multilingual data

hila gonen and yoav goldberg
yoav.goldberg@gmail.com
hilagonen87@gmail.com

...
+
few context rich, learned,
bilstm features.

sym(u1:k, v1:m) = ||lst m (u1:k)   lst m (v1:m)||

morpho-syntactic  attribute 
embeddings  allows  us  to  train 
joint  models  over  examples  that 
share  only  the  part  of  speech 
rather than all the attributes

(2)
c is a 2-dimensional vector whose components
the
sum to 1, and we classify a pair as positive if

graph-based parsing algorithm (global optimization)

posi, posi+1, posj 1, posj
posi 1, posi, posj 1, posj
posi, posi+1, posj, posj+1
posi 1, posi, posj, posj+1

   
   
+

first-order neural parser

the factored sequence to sequence approach (faruqui et. al., 2016)

pos=v 
mood=ind
tense=prs/fut
gender=fem
person=3
voice=act
num=du 
aspect=ipfv/pfv

        
          

we extended the network to take into account
dog

2. full model  

3. novel methods

1.motivation

1
3
a m a
f
f

future, singular  +  past, plural

pos=v 
mood=ind
tense=prs/fut
gender=fem
person=3

x/noun/dobj/> define/verb/root/-

past, passive

integrated network
sf
4

x/noun/nsubj/> be/verb/root/-

nns
<w>
risks

written

   

pos=v 
mood=imper
num=pl
aspect=ipfv

r
j
2 3

term-pair classi   er

y/noun/pobj/<

y/noun/attr/<

as/adp/prep/<

prp$

p
r
o
   

<w>
t
f

their

average
pooling

t
a
k
e

m a

<w>
0,0
f

path lstm

v
b

m   

step
1,0
f

step   

step
2,1
f

v
p

step
4,4
f

vb

vp

vp

np

vp

np

  
3,4
f

  
2,2
f

  
2,3
f

  
1,1
f

cut

(softmax)

  
 
f

5
 
f

lst m f

lst m f

lst m f

lst m f

lst m b

lst m b

lst m b

good

t
a
f

u
r
f

0
j
f

+

+

pro   ts

4. ms2s network architecture
our    rst approach, the bi-directional sequence to sequence model (bs2s):
our    rst approach, the morphological sequence to sequence architecture  (ms2s):

n
n
s

n
p

v
p

vbrown

concat

concat

concat

xbrown

+

i
 
f

a
1

bad

was

t
s

pro   ts

acomp

(1)

l

acomp

m lp

m lp

m lp

vthe

(x, y)

vfox

nns

~vwx

xthe

~vwy

xfox

take

~vxy

</w>

</w>

</w>

f

t
t

x   

vb

vp

np

<w>

yb
3

yb
5

yb
1

yb
4

yb
2

a

a

  

  

aux

yf
1

yf
2

yf
3

yf
5

  

  

i
i

  

  

  

  

  

  

  

2

u

a

n

u

0

1

2

3

5

  

n

~op

s3sb
3

s4sb
4

s0sb
0

s1sb
1

sb
5

sf
0

sf
1

sf
2

sf
3

sf
5

step

step

step

step

</w>

  

  

r

f

f

f

r

t

t

t

t

j

,

j

the

over

jumped

(cid:1877)=argmax(cid:3037)(cid:1839)(cid:1838)(cid:1842)(cid:3046)(cid:3032)(cid:3041)(cid:3046)(cid:3032)((cid:2038)(cid:1871),(cid:1861))[(cid:1862)]

lazy

id33 is the task of extracting a dependency tree for a given sentence.
dependency tree is a directed tree where each word modi   es (i.e. modi   er) the parent   s

b i u
n l p

their
very
existence
?

5
3
3
3

0
3
5
5

also an important predictor of reading time. phrases
that are often removed in sentence compression   
like fronted phrases, parentheticals,    oating quanti-
   ers, etc.   are often associated with non-local de-
pendencies. also, there is evidence that people are
more likely to    xate on the    rst word in a con-
stituent than on its second word (hy  on  a and pol-
latsek, 2000). being able to identify constituent
borders is important for sentence compression, and
reading    xation data may help our model learn a rep-
resentation of our data that makes it easy to identify
constituent boundaries.

l

l

l

r

r

r

np

vp

cut

np

vp

vp

vp

vb

their

root

np

vp

np

vp

vb

risks

nns

prp$

take

xcomp

risks

l

nns

nns

their

prp$

improving sequence to sequence learning for 

c
u
t

n
p

v
b

embeddings:

t
h
e
i
r

r
i
s
k
s

p
r
p
$

bilstms are

works for both

lemma
pos
dependency label
direction

vb
state-of-the-art
cut

subj

subj

expected

id33

euclidean distance

simple & e   ective
feature extractors

parsing accuracies
with minimal e   ort

figure 1: example sentence from the dundee corpus

eliyahu kiperwasser and yoav goldberg

graph-based & transition-based
r v
r
p

simple and accurate id33

using bidirectional lstm feature representations

figure 2: illustration of the symmetry scoring component that takes into account the conjuncts syntactic structures. each conjunct
i
tree is decomposed into paths that are fed into the path-lstms (squares). the resulting vectors are fed into the symmetry lstm
i
function (circles). the outcome vectors (blue circles) are then fed into the euclidean distance function.

improving preposition sense disambiguation

doing stuff with lstms

regression duration measures the total
time
spent    xating a word after the gaze has already left
it once. this measure belongs to the group of late
measures, i.e., measures that are sensitive to the later
emnlp 2016 submission ***. con   dential review copy. do not distribute.
cognitive processing stages including interpretation
and integration of already decoded words. since
in the experiments below, we learn models to pre-
the reader by de   nition has already had a chance to
dict the    rst pass duration of word    xations and the
recognize the word, regressions are associated with
total duration of regressions to a word. these two
semantic confusion and contradiction, incongruence
measures constitute a perfect separation of the to-
and syntactic complexity, as famously experienced
tal reading time of each word split between the    rst
in garden path sentences. for this measure the value
pass and subsequent passes. both measures are de-
0 indicates that the word was read at most once by
scribed below. they are both discretized into six
this reader.
bins as follows with only non-zero values contribut-
ing to the calculation of the standard deviation (sd):
1. the task
abstract
v
parsing
p
0: measure = 0 or
              
1: measure < 1 sd below reader   s average or
context rich feature
2: measure < .5 sd below reader   s average or
          
many hand-crafted features:
3: measure < .5 above reader   s average or
4: measure > .5 sd above reader   s average or
5: measure > 1 sd above reader   s average
writing
classi   cation

288
289
290
291
292
293
294
295
296
be
to
297
298
299
300
301
figure 2: an illustration of term-pair classi   cation. each term-pair is represented by several paths. each path is a sequence of
edges, and each edge consists of four components: lemma, pos, dependency label and dependency direction. each edge vector
is fed in sequence into the lstm, resulting in a path embedding vector ~op. the averaged path vector becomes the term-pair   s
302
prepositions are very common, very ambiguous and tend to carry different 
feature vector, used for classi   cation. the dashed ~vwx , ~vwy vectors refer to the integrated network described in section 3.2.
m lp
meanings in different contexts. 
303
the 50-dimensional and 100-dimensional embed-
vjumped
ding vectors and selected the ones that yield bet-
304
concat
ter performance on the validation set.4 the other
preposition-sense disambiguation is a task of assigning a category to a 
embeddings, as well as out-of-vocabulary lemmas,
preposition in context: 
305
are initialized randomly. we update all embedding
lst m b
lst m b
vectors during training.
306
yf
4
3.2
lst m f
307
the network presented in section 3.1 classi   es
each (x, y) term-pair based on the paths that con-
xjumped
308
nect x and y in the corpus. our goal was to im-
prove upon previous path-based methods for hy-
- can we improve performance by using unannotated data?
pernymy detection, and we show in section 6
309
that our network indeed outperforms them. yet,
- are translations of prepositions to other languages predictive for this task? 
as path-based and distributional methods are con-
310
t
u
- how can we use multilingual corpora for learning a representation of the 
sidered complementary, we present a simple way
u
0
to integrate distributional features in the network,
context that can be used for sense-disambiguation?
311
yielding improved performance.
312

term-pair classi   cation each (x, y) term-pair
is represented by the multiset of lexico-syntactic
paths that connected x and y in the corpus, de-
noted as paths(x, y), while the supervision is
given for the term pairs. we represent each (x, y)
s2sb
2
term-pair as the weighted-average of its path vec-
tors, by applying average pooling on its path vec-
   you should book a room for 2 nights            duration
tors, as follows:
   for some reason, he is not here yet            explanation 
~vxy = ~vpaths(x,y) = pp2paths(x,y) fp,(x,y)   ~op
   i went there to get a present for my mother           beneficiary
pp2paths(x,y) fp,(x,y)
where fp,(x,y) is the frequency of p in paths(x, y).
we then feed this path vector to a single-layer net-
work that performs binary classi   cation to decide
whether y is a hypernym of x.

first pass duration measures the total time spent
given two spans of lengths k and m with cor-
2. previous work
reading a word    rst time it is    xated, including
any immediately following re-   xations of the same
responding vector sequences u1:k and v1:m we en-
word. this measure correlates with word length, fre-
code each sequences using an lstm, and take the
quency and ambiguity because long words are likely
to attract several    xations in a row unless they are
euclidean distance between the resulting representa-
particularly easily predicted or recognized. this ef-
tions:
fect arises because long words are less likely to    t
inside the fovea of the eye. note that for this mea-
sure the value 0 indicates that the word was not    x-
v   
ated by this reader.
concat

the network is trained such that the distance is min-
imized for compatible spans and large for incompat-
ible ones in order to learn that vectors that represents
correct conjuncts are closer than vectors that do not
represent conjuncts.
transition-based parsing algorithm (greedy optimization)
i m    n
i
5    n
what are the elements in the sequences to be com-
c = sof tmax(w    ~vxy)
arc-hybrid neural parser
pared? one choice is to take the vectors ui to cor-

see table 1 for an example of    rst pass duration
and regression duration annotations for one reader
n
l v
n
r
and sentence.
n
p
p
s
feminine, present, passive, singular

morphological inflection generation
336
337
roee aharoni and yoav goldberg       yonatan belinkov
338
bar ilan university nlp lab                  mit csail      
339
340
341
i
342
i
343
344
i
n
 
 
345
f
f
346
347
348
349
350
351
352
353
354
e step
355
5,5
4,5
f
f
356
357
358
359
360

this decomposition captures the syntactic context
5. ndst network architecture
of each word, but does not uniquely determine the
our second approach, the neural discriminative string transducer (ndst):
structure of the tree. to remedy this, we add to
the paths special symbols, r and l, which marks
the lowest common ancestors with the right and left
step
3,3
words respectively. these are added to the path
f
above the corresponding nodes. for example con-
morphological  templates 
sider the following paths which corresponds to the
instead  of  training  the  network  to 
predict only a speci   c character at 
above syntactic structure:
each step, we train the network to 
either predict a character or copy a 
character at a given position in the 

figure 2: multitask and cascaded bi-lstms for sentence com-
pression. layer l 1 contain pre-trained embeddings. gaze
prediction and id35-tag prediction are auxiliary training tasks,
and loss on all tasks are propagated back to layer l0.

with representations learned from multilingual data

hila gonen and yoav goldberg
yoav.goldberg@gmail.com
hilagonen87@gmail.com

...
+
few context rich, learned,
bilstm features.

sym(u1:k, v1:m) = ||lst m (u1:k)   lst m (v1:m)||

morpho-syntactic  attribute 
embeddings  allows  us  to  train 
joint  models  over  examples  that 
share  only  the  part  of  speech 
rather than all the attributes

(2)
c is a 2-dimensional vector whose components
the
sum to 1, and we classify a pair as positive if

graph-based parsing algorithm (global optimization)

posi, posi+1, posj 1, posj
posi 1, posi, posj 1, posj
posi, posi+1, posj, posj+1
posi 1, posi, posj, posj+1

   
   
+

first-order neural parser

the factored sequence to sequence approach (faruqui et. al., 2016)

pos=v 
mood=ind
tense=prs/fut
gender=fem
person=3
voice=act
num=du 
aspect=ipfv/pfv

        
          

we extended the network to take into account
dog

2. full model  

3. novel methods

1.motivation

1
3
a m a
f
f

future, singular  +  past, plural

pos=v 
mood=ind
tense=prs/fut
gender=fem
person=3

x/noun/dobj/> define/verb/root/-

past, passive

integrated network
sf
4

x/noun/nsubj/> be/verb/root/-

nns
<w>
risks

written

   

pos=v 
mood=imper
num=pl
aspect=ipfv

r
j
2 3

term-pair classi   er

y/noun/pobj/<

y/noun/attr/<

as/adp/prep/<

prp$

p
r
o
   

<w>
t
f

their

average
pooling

t
a
k
e

m a

<w>
0,0
f

path lstm

v
b

m   

step
1,0
f

step   

step
2,1
f

v
p

step
4,4
f

vb

vp

vp

np

vp

np

  
3,4
f

  
2,2
f

  
2,3
f

  
1,1
f

cut

(softmax)

  
 
f

5
 
f

lst m f

lst m f

lst m f

lst m f

lst m b

lst m b

lst m b

good

t
a
f

u
r
f

0
j
f

+

+

pro   ts

4. ms2s network architecture
our    rst approach, the bi-directional sequence to sequence model (bs2s):
our    rst approach, the morphological sequence to sequence architecture  (ms2s):

n
n
s

n
p

v
p

vbrown

concat

concat

concat

xbrown

+

i
 
f

a
1

bad

was

t
s

pro   ts

acomp

(1)

l

acomp

m lp

m lp

m lp

vthe

(x, y)

vfox

nns

~vwx

xthe

~vwy

xfox

take

~vxy

</w>

</w>

</w>

f

t
t

x   

vb

vp

np

<w>

yb
3

yb
5

yb
1

yb
4

yb
2

a

a

  

  

aux

yf
1

yf
2

yf
3

yf
5

  

  

i
i

  

  

  

  

  

  

  

2

u

a

n

u

0

1

2

3

5

  

n

~op

s3sb
3

s4sb
4

s0sb
0

s1sb
1

sb
5

sf
0

sf
1

sf
2

sf
3

sf
5

step

step

step

step

</w>

  

  

r

f

f

f

r

t

t

t

t

j

,

j

the

over

jumped

(cid:1877)=argmax(cid:3037)(cid:1839)(cid:1838)(cid:1842)(cid:3046)(cid:3032)(cid:3041)(cid:3046)(cid:3032)((cid:2038)(cid:1871),(cid:1861))[(cid:1862)]

lazy

id33 is the task of extracting a dependency tree for a given sentence.
dependency tree is a directed tree where each word modi   es (i.e. modi   er) the parent   s

b i u
n l p

id56s/lstms  
and syntax

b i u
n l p

brief intro to id56s

b i u
n l p

recurrent neural networks

    very strong models of sequential data. 
    function from n vectors to a single vector.

b i u
n l p

recurrent neural networks

v(what)

v(is)

v(your) v(name)

    very strong models of sequential data. 
    function from n vectors to a single vector.

b i u
n l p

recurrent neural networks

v(what)

v(is)

v(your) v(name)

????

    very strong models of sequential data. 
    function from n vectors to a single vector.

b i u
n l p

recurrent neural networks

v(what)

v(is)

v(your) v(name)

v(beer)

?

    very strong models of sequential data. 
    function from n vectors to a single vector.

b i u
n l p

recurrent neural networks

v(what)

v(is)

v(your) v(name)

enc(what is your name)

    very strong models of sequential data. 
    function from n vectors to a single vector.

b i u
n l p

recurrent neural networks

v(what)

v(is)

v(your) v(name)

enc(what is your name)

    very strong models of sequential data. 
    trainable function from n vectors to a single vector.

b i u
n l p

recurrent neural networks

    there are different variants (implementations). 

    we'll focus on the interface level.

b i u
n l p

recurrent neural networks

v(what)

v(is)

v(your) v(name)

enc(what is your name)

    very strong models of sequential data. 
    trainable function from n vectors to a single vector.

b i u
n l p

recurrent neural networks

rn n (s0, x1:n) = sn, yn

xi 2 rdin, yi 2 rdout, si 2 rf (dout)

    very strong models of sequential data. 
    trainable function from n vectors to a single* vector.

b i u
n l p

figure 5: graphical representation of an id56 (recursive).

recurrent neural networks

this presentation follows the recursive de   nition, and is correct for arbitrary long sequences.
however, for a    nite sized input sequence (and all input sequences we deal with are    nite)
one can unroll the recursion, resulting in the structure in figure 6.

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

s5

x1

x2

x3

x4

x5

   

figure 6: graphical representation of an id56 (unrolled).

si

while not usually shown in the visualization, we include here the parameters     in order
to highlight the fact that the same parameters are shared across all time steps. di   erent

    input vectors         , output vector 
    think of      as "memory".  
    the output vector       depends on all inputs x1:i

x1:i

yi

yi

47

b i u
n l p

figure 5: graphical representation of an id56 (recursive).

recurrent neural networks

this presentation follows the recursive de   nition, and is correct for arbitrary long sequences.
however, for a    nite sized input sequence (and all input sequences we deal with are    nite)
one can unroll the recursion, resulting in the structure in figure 6.

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

s5

x1

x2

x3

x4

x5

   

rn n (s0, x1:n) = sn, yn
rn n (s0, x1:n) = sn, yn

figure 6: graphical representation of an id56 (unrolled).

    recursively de   ned. 

while not usually shown in the visualization, we include here the parameters     in order
to highlight the fact that the same parameters are shared across all time steps. di   erent

xi 2 rdin, yi 2 rdout, si 2 rf (dout)

    there's a vector       for every pre   x  x1:i

yi

47

si = r(si 1, xi)
yi = o(si)

b i u
n l p

   

xi

recurrent neural networks
    what are the vectors      good for?

this presentation follows the recursive de   nition, and is correct for arbitrary long sequences.
however, for a    nite sized input sequence (and all input sequences we deal with are    nite)
one can unroll the recursion, resulting in the structure in figure 6.

figure 5: graphical representation of an id56 (recursive).

yi

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

s5

x1

x2

x3

x4

x5

    on their own? nothing. 

figure 6: graphical representation of an id56 (unrolled).

   

while not usually shown in the visualization, we include here the parameters     in order
to highlight the fact that the same parameters are shared across all time steps. di   erent

47

b i u
n l p

   

xi

recurrent neural networks
    what are the vectors      good for?

this presentation follows the recursive de   nition, and is correct for arbitrary long sequences.
however, for a    nite sized input sequence (and all input sequences we deal with are    nite)
one can unroll the recursion, resulting in the structure in figure 6.

figure 5: graphical representation of an id56 (recursive).

yi

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

s5

x1

x2

x3

x4

x5

   

figure 6: graphical representation of an id56 (unrolled).

    on their own? nothing. 
    but we can train them.

while not usually shown in the visualization, we include here the parameters     in order
to highlight the fact that the same parameters are shared across all time steps. di   erent

47

b i u
n l p

   

xi

recurrent neural networks
    what are the vectors      good for?

this presentation follows the recursive de   nition, and is correct for arbitrary long sequences.
however, for a    nite sized input sequence (and all input sequences we deal with are    nite)
one can unroll the recursion, resulting in the structure in figure 6.

figure 5: graphical representation of an id56 (recursive).

yi

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

s5

x1

x2

x3

x4

x5

   

figure 6: graphical representation of an id56 (unrolled).

    on their own? nothing. 
    but we can train them.

while not usually shown in the visualization, we include here the parameters     in order
to highlight the fact that the same parameters are shared across all time steps. di   erent

de   ne function form
de   ne loss

47

b i u
n l p

   

xi

recurrent neural networks
    what are the vectors      good for?

this presentation follows the recursive de   nition, and is correct for arbitrary long sequences.
however, for a    nite sized input sequence (and all input sequences we deal with are    nite)
one can unroll the recursion, resulting in the structure in figure 6.

figure 5: graphical representation of an id56 (recursive).

yi

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

s5

x1

x2

x3

x4

x5

   

trained parameters.

figure 6: graphical representation of an id56 (unrolled).

    on their own? nothing. 
    but we can train them.

while not usually shown in the visualization, we include here the parameters     in order
to highlight the fact that the same parameters are shared across all time steps. di   erent

de   ne function form
de   ne loss

47

simpleid56:

   

xi

recurrent neural networks
    what are the vectors      good for?

this presentation follows the recursive de   nition, and is correct for arbitrary long sequences.
however, for a    nite sized input sequence (and all input sequences we deal with are    nite)
one can unroll the recursion, resulting in the structure in figure 6.

rsid56 (si 1, xi) = tanh(ws    si 1 + wx    xi)
figure 5: graphical representation of an id56 (recursive).

looks simple. 
theoretically powerful. 
practically, not so much.

yi

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

s5

x1

x2

x3

x4

x5

   

trained parameters.

figure 6: graphical representation of an id56 (unrolled).

    on their own? nothing. 
    but we can train them.

while not usually shown in the visualization, we include here the parameters     in order
to highlight the fact that the same parameters are shared across all time steps. di   erent

de   ne function form
de   ne loss

47

lstm:

rlst m (sj 1, xj) =[cj; hj]

figure 5: graphical representation of an id56 (recursive).

this presentation follows the recursive de   nition, and is correct for arbitrary long sequences.
however, for a    nite sized input sequence (and all input sequences we deal with are    nite)
one can unroll the recursion, resulting in the structure in figure 6.

looks complex, and is. 
very strong in practice.

   

xi
cj =cj 1   f + g   i
hj = tanh(cj)   o
i = (wxi    xj + whi    hj 1)
f = (wxf    xj + whf    hj 1)
o = (wxo    xj + who    hj 1)
g = tanh(wxg    xj + whg    hj 1)

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

s5

x1

x2

x3

x4

x5

   

trained parameters.

figure 6: graphical representation of an id56 (unrolled).

    on their own? nothing. 
    but we can train them.

while not usually shown in the visualization, we include here the parameters     in order
to highlight the fact that the same parameters are shared across all time steps. di   erent

de   ne function form
de   ne loss

47

b i u
n l p

   

xi

recurrent neural networks
    what are the vectors      good for?

this presentation follows the recursive de   nition, and is correct for arbitrary long sequences.
however, for a    nite sized input sequence (and all input sequences we deal with are    nite)
one can unroll the recursion, resulting in the structure in figure 6.

figure 5: graphical representation of an id56 (recursive).

yi

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

s5

x1

x2

x3

x4

x5

   

figure 6: graphical representation of an id56 (unrolled).

    on their own? nothing. 
    but we can train them.

while not usually shown in the visualization, we include here the parameters     in order
to highlight the fact that the same parameters are shared across all time steps. di   erent

de   ne function form
de   ne loss

47

b i u
n l p

inspired by (ling et al., 2015b)), an id56 that reads in a sentence and, based on the    nal
state decides if it conveys positive or negative sentiment (this is inspired by (wang et al.,
2015b)) or an id56 that reads in a sequence of words and decides whether it is a valid
noun-phrase. the loss in such cases is de   ned in terms of a function of yn = o(sn), and
the error gradients will backpropagate through the rest of the sequence (see figure 7).30
the loss can take any familiar form     cross id178, hinge, margin, etc.

recurrent neural networks
de   ning the loss.

loss

predict &
calc loss

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

x1

x2

x3

x4

x5

acceptor: predict something from end state. 
figure 7: acceptor id56 training graph.
backprop the error all the way back. 
train the network to capture meaningful information

encoder similar to the acceptor case, an encoder supervision uses only the    nal output
vector, yn. however, unlike the acceptor, where a prediction is made solely on the basis
of the    nal vector, here the    nal vector is treated as an encoding of the information in the

b i u
n l p

inspired by (ling et al., 2015b)), an id56 that reads in a sentence and, based on the    nal
state decides if it conveys positive or negative sentiment (this is inspired by (wang et al.,
2015b)) or an id56 that reads in a sequence of words and decides whether it is a valid
noun-phrase. the loss in such cases is de   ned in terms of a function of yn = o(sn), and
the error gradients will backpropagate through the rest of the sequence (see figure 7).30
the loss can take any familiar form     cross id178, hinge, margin, etc.

recurrent neural networks
de   ning the loss.

loss

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

predict &
calc loss

y5

x1

x2

x3

x4

x5

the    nal vector is a good 
"summary" of the sequence

acceptor: predict something from end state. 
figure 7: acceptor id56 training graph.
backprop the error all the way back. 
train the network to capture meaningful information

encoder similar to the acceptor case, an encoder supervision uses only the    nal output
vector, yn. however, unlike the acceptor, where a prediction is made solely on the basis
of the    nal vector, here the    nal vector is treated as an encoding of the information in the

inspired by (ling et al., 2015b)), an id56 that reads in a sentence and, based on the    nal
state decides if it conveys positive or negative sentiment (this is inspired by (wang et al.,
2015b)) or an id56 that reads in a sequence of words and decides whether it is a valid
noun-phrase. the loss in such cases is de   ned in terms of a function of yn = o(sn), and
the error gradients will backpropagate through the rest of the sequence (see figure 7).30
the loss can take any familiar form     cross id178, hinge, margin, etc.

    predict sentiment of the sentence based on all words. 
recurrent neural networks
    predict word i based on words 1,...,i-1.
    what are the vectors      good for?

yi

loss

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

predict &
calc loss

y5

x1

x2

x3

x4

x5

the    nal vector is a good 
"summary" of the sequence

acceptor: predict something from end state. 
figure 7: acceptor id56 training graph.
backprop the error all the way back. 
train the network to capture meaningful information

encoder similar to the acceptor case, an encoder supervision uses only the    nal output
vector, yn. however, unlike the acceptor, where a prediction is made solely on the basis
of the    nal vector, here the    nal vector is treated as an encoding of the information in the

inspired by (ling et al., 2015b)), an id56 that reads in a sentence and, based on the    nal
state decides if it conveys positive or negative sentiment (this is inspired by (wang et al.,
2015b)) or an id56 that reads in a sequence of words and decides whether it is a valid
noun-phrase. the loss in such cases is de   ned in terms of a function of yn = o(sn), and
the error gradients will backpropagate through the rest of the sequence (see figure 7).30
the loss can take any familiar form     cross id178, hinge, margin, etc.

    predict sentiment of the sentence based on all words. 
recurrent neural networks
    predict word i based on words 1,...,i-1.
    what are the vectors      good for?

yi

loss

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

predict &
calc loss

y5

x1

x2

x3

x4

x5

the    nal vector is a good 
"summary" of the sequence

acceptor: predict something from end state. 
figure 7: acceptor id56 training graph.
backprop the error all the way back. 
train the network to capture meaningful information

encoder similar to the acceptor case, an encoder supervision uses only the    nal output
vector, yn. however, unlike the acceptor, where a prediction is made solely on the basis
of the    nal vector, here the    nal vector is treated as an encoding of the information in the

b i u
n l p

recurrent neural networks

loss

sum

predict &
calc loss

predict &
calc loss

predict &
calc loss

predict &
calc loss

predict &
calc loss

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

x1

x2

x3

x4

x5

transducer: predict something from each state. 
backprop the sum of errors all the way back. 
figure 8: transducer id56 training graph.
train the network to capture meaningful information

language models are shown to provide much better perplexities than traditional language
models (mikolov et al., 2010; sundermeyer, schl  uter, & ney, 2012; mikolov, 2012).

using id56s as transducers allows us to relax the markov assumption that is tradition-

b i u
n l p

10.4 bi-id56

"deep id56s"

y1

y2

y3

y4

y5

s3
0

s2
0

s1
0

y3
1

r3,o3

y2
1

r2,o2

y1
1

r1,o1

s3
1

s2
1

s1
1

y3
2

r3,o3

y2
2

r2,o2

y1
2

r1,o1

s3
2

s2
2

s1
2

y3
3

r3,o3

y2
3

r2,o2

y1
3

r1,o1

s3
3

s2
3

s1
3

y3
4

r3,o3

y2
4

r2,o2

y1
4

r1,o1

s3
4

s2
4

s1
4

y3
5

r3,o3

y2
5

r2,o2

y1
5

r1,o1

s3
5

s2
5

s1
5

x1

x2

x3

x4

x5

id56 can be stacked 

figure 10: a 3-layer (   deep   ) id56 architecture.

deeper is better! 
(better how?)

a useful elaboration of an id56 is a bidirectional-id56 (bi-id56) (schuster & paliwal, 1997;

b i u
n l p

story so far:

    there is a thing called a (deep) id56. 

    we can feed it a list of vectors. 

    each vector represents a word. 

    at the end it spits out a vector summarizing the list 

of vectors. 

    we in   uence the summarization with training.

b i u
n l p

story so far:

    there is a thing called a (deep) id56. 
sequence in, vector out. 

    we can feed it a list of vectors. 

but human language is not (only) a sequence!
    each vector represents a word. 

    at the end it spits out a vector summarizing the list 

of vectors. 

    we in   uence the summarization with training.

b i u
n l p

can id56s learn hierarchy?

b i u
n l p

can id56s learn hierarchy?

(joint work with tal linzen and emmanuel dupoux)

assessing the ability of lstms to learn syntax-sensitive dependencies

tal linzen1,2

emmanuel dupoux1

lscp1 & ijn2, cnrs,

ehess and ens, psl research university

{tal.linzen,

emmanuel.dupoux}@ens.fr

yoav goldberg

computer science department

bar ilan university

yoav.goldberg@gmail.com

abstract

(hochreiter and schmidhuber, 1997) or gated recur-
rent units (gru) (cho et al., 2014), has led to sig-

b i u
n l p

the case for syntax

    some natural-language phenomena    

       are indicative of hierarchical structure. 

    for example, subject verb agreement. 

the boy kicks the ball
the boys kick the ball

b i u
n l p

the case for syntax

    some natural-language phenomena    

       are indicative of hierarchical structure. 

    for example, subject verb agreement. 

the boy with the white shirt with the blue collar kicks the ball
the boys with the white shirts with the blue collars kick the ball

b i u
n l p

the case for syntax

    some natural-language phenomena    

       are indicative of hierarchical structure. 

    for example, subject verb agreement. 

the boy (with the white shirt (with the blue collar)) kicks the ball
the boys (with the white shirts (with the blue collars)) kick the ball

b i u
n l p

the case for syntax

    some natural-language phenomena    

       are indicative of hierarchical structure. 

    for example, subject verb agreement. 

the boy (with the white shirt (with the blue collar)) kicks the ball
the boys (with the white shirts (with the blue collars)) kick the ball

nsubj

b i u
n l p

can a sequence lstm 

learn agreement?

b i u
n l p

can a sequence lstm 

learn agreement?

some prominent    gures in the history of philosophy who have 
defended moral rationalism are plato and immanuel kant .

b i u
n l p

can a sequence lstm 

learn agreement?

some prominent    gures in the history of philosophy who have 
defended moral  nn   are plato and immanuel kant .

replace rare words with their pos

b i u
n l p

can a sequence lstm 

learn agreement?

some prominent    gures in the history of philosophy who have 
defended moral  nn   are plato and immanuel kant .

choose a verb with a subject

b i u
n l p

can a sequence lstm 

learn agreement?

some prominent    gures in the history of philosophy who have 
defended moral  nn  ____

cut the sentence at the verb

b i u
n l p

can a sequence lstm 

learn agreement?

some prominent    gures in the history of philosophy who have 
defended moral  nn  ____

plural or singular?

binary prediction task

b i u
n l p

...

v(have)

v(defended) v(moral)

v(nn)

plural / singular

predict

b i u
n l p

can a sequence lstm 

learn agreement?

some prominent    gures in the history of philosophy who have 
defended moral  nn  ____

plural or singular?

binary prediction task

b i u
n l p

can a sequence lstm 

learn agreement?

some prominent    gures in the history of philosophy who have 
defended moral  nn  ____

plural or singular?

b i u
n l p

can a sequence lstm 

learn agreement?

some prominent    gures in the history of philosophy who have 
defended moral  nn  ____

plural or singular?

b i u
n l p

can a sequence lstm 

learn agreement?

some prominent    gures in the history of philosophy who have 
defended moral  nn  ____

plural or singular?

b i u
n l p

can a sequence lstm 

learn agreement?

some prominent    gures in the history of philosophy who have 
defended moral  nn  ____

plural or singular?

in order to answer:

need to learn the concept of number.
need to identify the subject (ignoring irrelevant words)

b i u
n l p

somewhat harder task

b i u
n l p

somewhat harder task

some prominent    gures in the history of philosophy who have 
defended moral  nn   are plato and immanuel kant .

choose a verb with a subject

b i u
n l p

somewhat harder task

some prominent    gures in the history of philosophy who have 
defended moral  nn   are plato and immanuel kant .

some prominent    gures in the history of philosophy who have 
defended moral  nn   is plato and immanuel kant .

choose a verb with a subject

and    ip its number.

b i u
n l p

somewhat harder task

some prominent    gures in the history of philosophy who have 
defended moral  nn   are plato and immanuel kant .
v

some prominent    gures in the history of philosophy who have 
defended moral  nn   is plato and immanuel kant .
x

can the lstm learn to 

distinguish good from bad sentences?

b i u
n l p

...

v(boy) v(kicks)

v(the)

v(ball)

v

predict

b i u
n l p

...

v(boy)

v(kick)

v(the)

v(ball)

x

predict

b i u
n l p

can a sequence lstm 

learn agreement?

lstms learn agreement remarkably well.

predicts number with 99% accuracy.
...but most examples are very easy  

(look at last noun).

b i u
n l p

can a sequence lstm 

learn agreement?

lstms learn agreement remarkably well.

predicts number with 99% accuracy.
...but most examples are very easy  

(look at last noun).

(b)

(a)

b i u
n l p

can a sequence lstm 

learn agreement?

lstms learn agreement remarkably well.

predicts number with 99% accuracy.
...but most examples are very easy  

(look at last noun).

when restricted to cases  

of at least one intervening noun:

97% accuracy

b i u
n l p

can a sequence lstm 

learn agreement?

emnlp 2016 submission ***. con   dential review copy. do not distribute.

lstms learn agreement remarkably well.

384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401

learns number of nouns

figure 5: targeted training: last intervening (note
that the    none    category is missing     all dependencies
had at least one intervening noun).

figure 3: embeddings of singular nouns (in red) and

4 targeted training

b i u
n l p

can a sequence lstm 

learn agreement?

lstms learn agreement remarkably well.

more errors as the number of intervening nouns 

of opposite number increases

b i u
n l p

(c)

can a sequence lstm 

learn agreement?

lstms learn agreement remarkably well.

predicts number with 99% accuracy.
...but most examples are very easy  

(look at last noun).

when restricted to cases  

of at least one intervening noun:

~97% accuracy

(f)

b i u
n l p

(c)

can a sequence lstm 

learn agreement?

lstms learn agreement remarkably well.

predicts number with 99% accuracy.
...but most examples are very easy  

(look at last noun).

when restricted to cases  

of at least one intervening noun:

~97% accuracy

but < 16% err
for 4 misleading

nouns...

(f)

b i u
n l p

can a sequence lstm 

learn agreement?

lstms learn agreement remarkably well.

but we trained it on the agreement task. 
does a language model learn agreement?

b i u
n l p

(b)

can a sequence lstm 

learn agreement?

does a language model learn agreement?

b i u
n l p

can a sequence lstm 

learn agreement?

does a language model learn agreement?

what if we used the best lm in the world?

b i u
n l p

(c)

can a sequence lstm 

learn agreement?

does a language model learn agreement?

(d)

google's beast lm  
does better than ours 
but still struggles 
considerably.

b i u
n l p

can a sequence lstm 

learn agreement?

does a language model learn agreement?

lstm-lm does not learn agreement. 
explicit error signal is required.

but with explicit signal, 

lstms can learn agreement very well.

b i u
n l p

can a sequence lstm 

learn agreement?

where do lstms fail?

in many and diverse cases. 

but we did manage to    nd some common trends.

forward agreement attraction errors; others were dif-
b i u
   cult to interpret. we mention here three classes of
n l p
errors that can motivate future experiments.

can a sequence lstm 

learn agreement?

where do lstms fail?

the networks often misidenti   ed the heads of
noun-noun compounds. in (17), for example, the
models predict a singular verb even though the num-
ber of the subject conservation refugees should be
determined by its head refugees. this suggests that
the networks didn   t master the structure of english
noun-noun compounds.14

noun compounds can be tricky

(17)

(18)

conservation refugees live in a world col-
ored in shades of gray; limbo.
information technology (it) assets com-
monly hold large volumes of con   dential
data.

8 related work
the majority of nlp work on neural networks eval-
uates them on their performance in a task such as
id38 or machine translation (sunder-
meyer et al., 2012; bahdanau et al., 2015). these
evaluation setups average over many different syn-
tactic constructions, making it dif   cult to isolate the
network   s syntactic capabilities.

other studies have tested the capabilities of id56s
to learn simple arti   cial languages. gers and schmid-
huber (2001) showed that lstms can learn the
context-free language anbn, generalizing to ns as
high as 1000 even when trained only on n 2
{1, . . . , 10}. simple recurrent networks struggled
with this language (rodriguez et al., 1999; rodriguez,

b i u
n l p

can a sequence lstm 

number. the attractor is often itself a subject of an
irrelevant verb, making a potential    agree with the
most recent subject    strategy unreliable. finally, the
existence of a relative clause is sometimes not overtly
indicated by a function word (relativizer), as in (11)
(for comparison, see the minimally different (12)):

learn agreement?

where do lstms fail?

(11)

(12)

the landmarks this article lists here are
also run-of-the-mill and not notable.

relative clauses are hard.

the landmarks that this article lists here
are also run-of-the-mill and not notable.

for data sparsity reasons we restricted our attention
to dependencies with a single attractor and no other
intervening nouns. as figure 2d shows, attraction
errors were more frequent in dependencies with an

on the following patterns:

(13)
(14)

these patterns differ by exactly one function word,
which determines the type of the modi   er of the main
clause subject: a prepositional phrase (pp) in the    rst
sentence and a relative clause (rc) in the second. in
pp sentences the correct number of the upcoming
verb is determined by the main clause subject toy(s);
in rc sentences it is determined by the embedded
subject boy(s).

we generated all four versions of each pattern, and
repeated the process ten times with different lexical
items (the house(s) of/that the girl(s), the computer(s)
of/that the student(s), etc.), for a total of 80 sentences.

b i u
n l p

learn agreement?

can a sequence lstm 

are likely to be fairly challenging, for several rea-
sons. they typically contain a verb that agrees with
the attractor, reinforcing the misleading cue to noun
number. the attractor is often itself a subject of an
irrelevant verb, making a potential    agree with the
most recent subject    strategy unreliable. finally, the
existence of a relative clause is sometimes not overtly
indicated by a function word (relativizer), as in (11)
reduced relative clauses are harder.
(for comparison, see the minimally different (12)):

where do lstms fail?

(11)

(12)

the landmarks this article lists here are
also run-of-the-mill and not notable.

the landmarks that this article lists here
are also run-of-the-mill and not notable.

for data sparsity reasons we restricted our attention
to dependencies with a single attractor and no other

plify the analysis, we deviate from our practice in the
rest of this paper and use constructed sentences.

we    rst constructed sets of sentence pre   xes based

on the following patterns:

(13)
(14)

these patterns differ by exactly one function word,
which determines the type of the modi   er of the main
clause subject: a prepositional phrase (pp) in the    rst
sentence and a relative clause (rc) in the second. in
pp sentences the correct number of the upcoming
verb is determined by the main clause subject toy(s);
in rc sentences it is determined by the embedded
subject boy(s).

we generated all four versions of each pattern, and
repeated the process ten times with different lexical

b i u
n l p

can a sequence lstm 

learn agreement?

where do lstms fail?

no relative clause 
overt relative clause 
reduced relative clause

error
3.2% 
9.9% 
25%

b i u
n l p

can a sequence lstm 

learn agreement?

where do lstms fail?

no relative clause 
overt relative clause 
reduced relative clause

error
3.2% 
9.9% 
25%

humans also fail much more on reduced relatives.

b i u
n l p

the agreement experiment: 

recap

    we wanted to show lstms can't learn hierarchy. 

    --> we sort-of failed. 

    lstms learn to cope with natural-language 

patterns that exhibit hierarchy, based on minimal 
and indirect supervision. 

    but some sort of relevant supervision is required.

b i u
n l p

what happens    
beyond english?

    english is a simple language. 

    we started exploring more interesting ones. 

    if you want to collaborate on cool agreement 

patterns in your favorite language, let's discuss!

b i u
n l p

story so far:

    id56s are very    exible sequence encoders. 

    we can train them to encode rather intricate 

syntactic structures. 

    can we use them for parsing?

b i u
n l p

story so far:

    id56s are very    exible sequence encoders. 

    we can train them to encode rather intricate 

syntactic structures. 

    can we use them for parsing?

b i u
n l p

parsing with lstms, take 1

easy-first id33 with hierarchical tree lstms

eliyahu kiperwasser

computer science department

bar-ilan university
ramat-gan, israel

yoav goldberg

computer science department

bar-ilan university
ramat-gan, israel

elikip@gmail.com

yoav.goldberg@gmail.com

abstract

we suggest a compositional vector represen-
tation of parse trees that relies on a recursive

do not cope well with trees with arbitrary branch-
ing factors     most work require the encoded trees to
be binary-branching, or have a    xed maximum arity.

b i u
n l p

core idea

    lstms are sota at modeling sequences. 

    encode sequence of modi   ers as an lstm. 

    combine in a recursive manner. 

 great for dependency trees. 

b i u
n l p

core idea

t.l3

t.l2

t.l1

t

t.w

t.r1

t.r2

t.r3

t.r4

enc(t.l3)

enc(t.l2)

enc(t.l1)

enc(t.r1)

enc(t.r2)

enc(t.r3)

enc(t.r4)

l

l

l

r

r

r

r

l r

concat

enc(t)

    two lstms 
    head + left modi   ers encoded w/ lstm-l  
    head + right modi   ers encoded w/ lstm-r 
    the left and right end states are concatenated

b i u
n l p

core idea

t.l3

t.l2

t.l1

t

t.w

t.r1

t.r2

t.r3

t.r4

enc(t.l3)

enc(t.l2)

enc(t.l1)

enc(t.r1)

enc(t.r2)

enc(t.r3)

enc(t.r4)

l

l

l

r

r

r

r

l r

concat

enc(t)

    two lstms 
    head + left modi   ers encoded w/ lstm-l  
    head + right modi   ers encoded w/ lstm-r 
    the left and right end states are concatenated

b i u
n l p

core idea
core idea

t.l3

t.l2

t.l1

t

t.w

t.r1

t.r2

t.r3

t.r4

enc(t.l3)

enc(t.l2)

enc(t.l1)

enc(t.r1)

enc(t.r2)

enc(t.r3)

enc(t.r4)

l

l

l

r

r

r

r

l r

concat

enc(t)

    two lstms 
    head + left modi   ers encoded w/ lstm-l  
    head + right modi   ers encoded w/ lstm-r 
    the left and right end states are concatenated

b i u
n l p

core idea

t.l3

t.l2

t.l1

t

t.w

t.r1

t.r2

t.r3

t.r4

enc(t.l3)

enc(t.l2)

enc(t.l1)

enc(t.r1)

enc(t.r2)

enc(t.r3)

enc(t.r4)

l

l

l

r

r

r

r

l r

concat

enc(t)

    two lstms 
    head + left modi   ers encoded w/ lstm-l  
    head + right modi   ers encoded w/ lstm-r 
    the left and right end states are concatenated

b i u
n l p

the black fox who really likes apples did not jump over a lazy dog yesterday

l
the black fox who really likes apples

l

did

l

not

l
jumpl

r

foxr

r

who really likes apples

r
jumpr

r

over a lazy dog

r

yesterday

l r

whol

whor

r

really likes apples

l r

overl

overr

r

a lazy dog

l

the

l
black

l

foxl

l

l r

really

likesl

likesr

r
apples

l

a

l

lazy

l r

dogl

dogr

l r

l r

l r

l r

l r

l r

l r

l r

l r

the

black

fox

who

really

likes

apples

did

not

jump

over

a

lazy

dog

yesterday

b i u
n l p

the black fox who really likes apples did not jump over a lazy dog yesterday

l
the black fox who really likes apples

l

did

l

not

l
jumpl

r

foxr

r

who really likes apples

r
jumpr

r

over a lazy dog

r

yesterday

l r

whol

whor

r

really likes apples

l r

overl

overr

r

a lazy dog

l

the

l
black

l

foxl

l

l r

really

likesl

likesr

r
apples

l

a

l

lazy

l r

dogl

dogr

l r

l r

l r

l r

l r

l r

l r

l r

l r

the

black

fox

who

really

likes

apples

did

not

jump

over

a

lazy

dog

yesterday

b i u
n l p

the black fox who really likes apples did not jump over a lazy dog yesterday

l
the black fox who really likes apples

l

did

l

not

l
jumpl

r

foxr

r

who really likes apples

r
jumpr

r

over a lazy dog

r

yesterday

l r

whol

whor

r

really likes apples

l r

overl

overr

r

a lazy dog

l

the

l
black

l

foxl

l

l r

really

likesl

likesr

r
apples

l

a

l

lazy

l r

dogl

dogr

l r

l r

l r

l r

l r

l r

l r

l r

l r

the

black

fox

who

really

likes

apples

did

not

jump

over

a

lazy

dog

yesterday

b i u
n l p

the black fox who really likes apples did not jump over a lazy dog yesterday

l
the black fox who really likes apples

l

did

l

not

l
jumpl

r

foxr

r

who really likes apples

r
jumpr

r

over a lazy dog

r

yesterday

l r

whol

whor

r

really likes apples

l r

overl

overr

r

a lazy dog

l

the

l
black

l

foxl

l

l r

really

likesl

likesr

r
apples

l

a

l

lazy

l r

dogl

dogr

l r

l r

l r

l r

l r

l r

l r

l r

l r

the

black

fox

who

really

likes

apples

did

not

jump

over

a

lazy

dog

yesterday

b i u
n l p

the black fox who really likes apples did not jump over a lazy dog yesterday

l
the black fox who really likes apples

l

did

l

not

l
jumpl

r

foxr

r

who really likes apples

r
jumpr

r

over a lazy dog

r

yesterday

l r

whol

whor

r

really likes apples

l r

overl

overr

r

a lazy dog

l

the

l
black

l

foxl

l

l r

really

likesl

likesr

r
apples

l

a

l

lazy

l r

dogl

dogr

l r

l r

l r

l r

l r

l r

l r

l r

l r

the

black

fox

who

really

likes

apples

did

not

jump

over

a

lazy

dog

yesterday

hierarchical tree lstm (example)

root

the

brown

fox

jumped with joy

over

the

fence

12

hierarchical tree lstm (example)

root

the

brown

fox

jumped with joy

over

the

fence

the brown fox

sub tree (fox)

leftchildren(lstm)
               
the

brown

fox

rightchildren(lstm)
               !

fox

12

hierarchical tree lstm (example)

root

the

brown

fox

jumped with joy

over

the

fence

the brown fox

with joy

sub tree (fox)

sub tree (with)

leftchildren(lstm)
               
the

brown

fox

rightchildren(lstm)
               !

fox

leftchildren(lstm)
               

with

rightchildren(lstm)
               !

with

joy

12

hierarchical tree lstm (example)

root

the

brown

fox

jumped with joy

over

the

fence

the brown fox

with joy

over

the

fence

sub tree (fox)

sub tree (with)

sub tree (over)

leftchildren(lstm)
               
the

brown

fox

rightchildren(lstm)
               !

fox

leftchildren(lstm)
               

with

rightchildren(lstm)
               !

with

joy

leftchildren(lstm)
               

rightchildren(lstm)
               !

over

over

the

fence

12

hierarchical tree lstm (example)

root

the

brown

fox

jumped with joy

over

the

fence

sub tree (jumped)

leftchildren(lstm)
               
the brown fox

jumped

rightchildren(lstm)
               !

jumped

with joy

over the fence

the brown fox

with joy

over

the

fence

sub tree (fox)

sub tree (with)

sub tree (over)

leftchildren(lstm)
               
the

brown

fox

rightchildren(lstm)
               !

fox

leftchildren(lstm)
               

with

rightchildren(lstm)
               !

with

joy

leftchildren(lstm)
               

rightchildren(lstm)
               !

over

over

the

fence

12

hierarchical tree lstm (example)

root

the

brown

fox

jumped with joy

over

the

fence

sub tree (jumped)

leftchildren(lstm)
               
the brown fox

jumped

rightchildren(lstm)
               !

jumped

with joy

over the fence

the brown fox

with joy

over

the

fence

sub tree (fox)

sub tree (with)

sub tree (over)

leftchildren(lstm)
               
the

brown

fox

rightchildren(lstm)
               !

fox

leftchildren(lstm)
               

with

rightchildren(lstm)
               !

with

joy

leftchildren(lstm)
               

rightchildren(lstm)
               !

over

over

the

fence

12

hierarchical tree lstm (example)

root

the

brown

fox

jumped with joy

over

the

fence

sub tree (jumped)

leftchildren(lstm)
               
the brown fox

jumped

rightchildren(lstm)
               !

jumped

with joy

over the fence

the brown fox

with joy

over

the

fence

sub tree (fox)

sub tree (with)
sub tree (with)

sub tree (over)

leftchildren(lstm)
               
the

brown

fox

rightchildren(lstm)
               !

fox

leftchildren(lstm)
               

with

rightchildren(lstm)
               !

with

joy

leftchildren(lstm)
               

rightchildren(lstm)
               !

over

over

the

fence

12

hierarchical tree lstm (example)

root

the

brown

fox

jumped with joy

over

the

fence

sub tree (jumped)

leftchildren(lstm)
               
the brown fox

jumped

rightchildren(lstm)
               !

jumped

with joy over the fence

the brown fox

with joy

over

the

fence

sub tree (fox)

sub tree (with)

sub tree (over)
sub tree (over)

leftchildren(lstm)
               
the

brown

fox

rightchildren(lstm)
               !

fox

leftchildren(lstm)
               

with

rightchildren(lstm)
               !

with

joy

leftchildren(lstm)
               

rightchildren(lstm)
               !

over

over

the

fence

12

how do we capture leafs?

root

the

brown

fox

jumped with joy

over

the

fence

sub tree (jumped)

leftchildren(lstm)
               
the brown fox jumped

rightchildren(lstm)
               !

jumped

with joy

over the fence

the brown fox

with joy

over

the

fence

sub tree (fox)

sub tree (with)

sub tree (over)

leftchildren(lstm)
               
the

brown

fox

rightchildren(lstm)
               !

fox

leftchildren(lstm)
               

with

rightchildren(lstm)
               !

with

joy

leftchildren(lstm)
               

rightchildren(lstm)
               !

over

over

the

fence

12

easy-first parsing (score function)

subtree(the)

subtree(fox)

subtree(jumped)

subtree(with)

sattachleft

the

fox

jumped

with

joy

sattachright

brown

18

easy-first parsing (score function)

mlp

subtree(the)

subtree(fox)

subtree(jumped)

subtree(with)

sattachleft

the

fox

jumped

with

joy

sattachright

brown

18

easy-first parsing (score function)

(sattachleft, sattachright)

mlp

subtree(the)

subtree(fox)

subtree(jumped)

subtree(with)

sattachleft

the

fox

jumped

with

joy

sattachright

brown

18

easy-first parsing (score function)

(sattachleft, sattachright)

mlp

subtree(the)

subtree(fox)

subtree(jumped)

subtree(with)

sattachleft

the

fox

jumped

with

joy

sattachright

brown

how do we e ciently compute the hierarchical tree lstm
representation for each pending sub-tree?

18

bottom-up tree representation building

19

bottom-up tree representation building

the

fox

jumped

with

joy

brown

19

bottom-up tree representation building

the

fox

jumped

with

joy

brown

19

bottom-up tree representation building

the

fox

jumped

with

joy

brown

+

fox

jumped

with

joy

the

brown

19

bottom-up tree representation building

the

fox

jumped

with

joy

brown

+

fox

jumped

with

joy

the

brown

sub tree (fox)

leftchildren(lstm)
               
brown

fox

rightchildren(lstm)
               !

fox

19

bottom-up tree representation building

the

fox

jumped

with

joy

brown

+

fox

jumped

with

joy

the

brown

sub tree (fox)

leftchildren(lstm)
               
brown

fox

rightchildren(lstm)
               !

fox

)

19

bottom-up tree representation building

the

fox

jumped

with

joy

brown

+

fox

jumped

with

joy

the

brown

sub tree (fox)

sub tree (fox)

leftchildren(lstm)
               
brown

fox

rightchildren(lstm)
               !

fox

)

leftchildren(lstm)
               
the
fox

brown

rightchildren(lstm)
               !

fox

19

bottom-up tree representation building

the

fox

jumped

with

joy

brown

constant

+

fox

jumped

the

brown

time!

with

joy

sub tree (fox)

sub tree (fox)

leftchildren(lstm)
               
brown

fox

rightchildren(lstm)
               !

fox

)

leftchildren(lstm)
               
the
fox

brown

rightchildren(lstm)
               !

fox

19

easy-first parsing (continued)

fox

jumped

with

joy

the

brown

fox

jumped

with

the

brown

joy

fox

jumped

the

brown

with

jumped

fox

with

the

brown

joy

joy

20

b i u
n l p

easy first parsing with 
hierarchical tree lstms

    it works! we get nice results. 
    but. 
    turns out can actually do something much simpler.

b i u
n l p

parsing with lstms, take 2

simple and accurate id33

using bidirectional lstm feature representations

eliyahu kiperwasser

computer science department

bar-ilan university
ramat-gan, israel

yoav goldberg

computer science department

bar-ilan university
ramat-gan, israel

elikip@gmail.com

yoav.goldberg@gmail.com

abstract

we present a simple and effective scheme
for id33 which is based on

arc-factored (   rst order) models (mcdonald, 2006),
in which the scoring function for a tree decomposes
over the individual arcs of the tree. more elaborate
models look at larger (overlapping) parts, requiring

b i u
n l p

parsing with lstms, take 2

simple and accurate id33

using bidirectional lstm feature representations

eliyahu kiperwasser

computer science department

bar-ilan university
ramat-gan, israel

yoav goldberg

computer science department

bar-ilan university
ramat-gan, israel

elikip@gmail.com

yoav.goldberg@gmail.com

abstract

we present a simple and effective scheme
for id33 which is based on

arc-factored (   rst order) models (mcdonald, 2006),
in which the scoring function for a tree decomposes
over the individual arcs of the tree. more elaborate
models look at larger (overlapping) parts, requiring

b i u
n l p

bi-directional id56s

b i u
n l p

id56s so far:

loss

sum

predict &
calc loss

predict &
calc loss

predict &
calc loss

predict &
calc loss

predict &
calc loss

y1

y2

y3

y4

y5

s0

s1

r,o

r,o

s2

r,o

s3

r,o

s4

r,o

x1

x2

x3

x4

x5

each state encodes the entire history up to that state. 
this is not bad. but what about the future?

figure 8: transducer id56 training graph.

language models are shown to provide much better perplexities than traditional language
models (mikolov et al., 2010; sundermeyer, schl  uter, & ney, 2012; mikolov, 2012).

using id56s as transducers allows us to relax the markov assumption that is tradition-

b i u
n l p

bidirectional id56s

ythe

ybrown

yfox

yjumped

y   

concat

concat

concat

concat

concat

yb
5

yb
4

yb
3

yb
2

yb
1

sb
5

s4sb
4

rb,ob

rb,ob

s3sb
3

s2sb
2

rb,ob

s1sb
1

rb,ob

s0sb
0

rb,ob

yf
1

yf
2

yf
3

yf
4

yf
5

sf
0

rf ,of

sf
1

rf ,of

sf
2

rf ,of

sf
3

rf ,of

sf
4

rf ,of

sf
5

xthe

xbrown

xfox

xjumped

x   

one id56 runs left to right.   
another runs right to left. 
encode both future and history of a word.

figure 11: bi-id56 over the sentence    the brown fox jumped .   .

looking at the k top-most elements of the stack, the id56 framework can be used to provide

b i u
n l p

bidirectional id56s

ythe

ybrown

yfox

yjumped

y   

concat

concat

concat

concat

concat

yb
5

yb
4

yb
3

yb
2

yb
1

sb
5

s4sb
4

rb,ob

rb,ob

s3sb
3

s2sb
2

rb,ob

s1sb
1

rb,ob

s0sb
0

rb,ob

yf
1

yf
2

yf
3

yf
4

yf
5

sf
0

rf ,of

sf
1

rf ,of

sf
2

rf ,of

sf
3

rf ,of

sf
4

rf ,of

sf
5

xthe

xbrown

xfox

xjumped

x   

one id56 runs left to right.   
another runs right to left. 
encode both future and history of a word.

figure 11: bi-id56 over the sentence    the brown fox jumped .   .

looking at the k top-most elements of the stack, the id56 framework can be used to provide

b i u
n l p

bidirectional id56s

ythe

ybrown

yfox

yjumped

y   

concat

concat

concat

concat

concat

yb
5

yb
4

yb
3

yb
2

yb
1

sb
5

s4sb
4

rb,ob

rb,ob

s3sb
3

s2sb
2

rb,ob

s1sb
1

rb,ob

s0sb
0

rb,ob

yf
1

yf
2

yf
3

yf
4

yf
5

sf
0

rf ,of

sf
1

rf ,of

sf
2

rf ,of

sf
3

rf ,of

sf
4

rf ,of

sf
5

xthe

xbrown

xfox

xjumped

x   

one id56 runs left to right.   
another runs right to left. 
encode both future and history of a word.

figure 11: bi-id56 over the sentence    the brown fox jumped .   .

looking at the k top-most elements of the stack, the id56 framework can be used to provide

b i u
n l p

bi-id56s

an in   nite window
 around the word.

ythe

ybrown

yfox

yjumped

y   

concat

concat

concat

concat

concat

yb
5

yb
4

yb
3

yb
2

yb
1

sb
5

s4sb
4

rb,ob

rb,ob

s3sb
3

s2sb
2

rb,ob

s1sb
1

rb,ob

s0sb
0

rb,ob

yf
1

yf
2

yf
3

yf
4

yf
5

sf
0

rf ,of

sf
1

rf ,of

sf
2

rf ,of

sf
3

rf ,of

sf
4

rf ,of

sf
5

xthe

xbrown

xfox

xjumped

x   

one id56 runs left to right.   
another runs right to left. 
encode both future and history of a word.

figure 11: bi-id56 over the sentence    the brown fox jumped .   .

looking at the k top-most elements of the stack, the id56 framework can be used to provide

b i u
n l p

bi-id56s

an in   nite window
 around the word.

concat

yf
4

yr
4

s0

lf w d

lf w d

lf w d

lf w d

rrev

rrev

rrev

rrev

s0

xthe

1

xbrown

2

xfox

3

xjumped

xjumped

4

4

xover

5

xthe

6

xdog

7

one id56 runs left to right.   
another runs right to left. 
encode both future and history of a word.

birn n (x1:7, 4) = [yf

4 ; yr
4 ]

yf
4 = rn nf (x1:4)
yr
4 = rn nr(x7:4)

b i u
n l p

deep bi-id56s

ythe

bi3

bi2

bi1

xthe

ybrown

bi3

bi2

bi1

xbrown

yfox

bi3

bi2

bi1

xfox

yjumped

bi3

bi2

bi1

yover

bi3

bi2

bi1

xjumped

xover

bi-id56 can also be stacked

b i u
n l p

(deep) bi-id56s

    provide an "in   nite" window around a focus word. 

    learn to extract what's important. 

    easy to train! 

    very effective for sequence tagging. 

    great as feature extractors!

b i u
n l p

parsing with lstms, take 2

simple and accurate id33

using bidirectional lstm feature representations

eliyahu kiperwasser

computer science department

bar-ilan university
ramat-gan, israel

yoav goldberg

computer science department

bar-ilan university
ramat-gan, israel

elikip@gmail.com

yoav.goldberg@gmail.com

abstract

we present a simple and effective scheme
for id33 which is based on

arc-factored (   rst order) models (mcdonald, 2006),
in which the scoring function for a tree decomposes
over the individual arcs of the tree. more elaborate
models look at larger (overlapping) parts, requiring

parsing background

there are two main frameworks for parsing:

graph-based:

global id136
score factorized over parts
there are    rst, second & third order parsers.

transition-based:

greedy local id136
score relies on current con   guration, which is dependent on all
previous transitions

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

4 / 42

parsing background

there are two main frameworks for parsing:

graph-based:

global id136
score factorized over parts
there are    rst, second & third order parsers.

transition-based:

greedy local id136
score relies on current con   guration, which is dependent on all
previous transitions

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

4 / 42

id170 recipe

predict(x) = arg max

y2y(x)xp2y

score( (p))

    decompose structure to local factors. 

    assign a score to each factor. 

    structure score = sum of local scores. 

    look for highest scoring structure.

graph-based parsing

dobj

nsubj

det

prep

nsubj

score( they

ate

the

pizza

with

anchovies

) =

nsubj

dobj

det

score( they

ate

) + score( ate

pizza

) + score( the

pizza

) +

prep

nsubj

score( pizza

with

) + score( with

anchovies

)

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

5 / 42

graph-based parsing (id136)

input sentence:    they ate pizza   

root

)
y
e
h
t

!

t
o
o
r
(
e
r
o
c
s

they

score(ro

ot
!ate)

score(root!pizza)

s c ore ( ate

! t h e y )
s core(th ey

ate

! ate)

score(ate

score(pizza

!pizza)

!ate)

score(pizza!they )

score(they!pizza)

pizza

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

6 / 42

graph-based parsing (id136)

root

)
y
e
h
t

!

t
o
o
r
(
e
r
o
c
s

they

score(ro

ot
!ate)

score(root!pizza)

s c ore ( ate

! t h e y )
s core(th ey

ate

! ate)

score(ate

score(pizza

!pizza)

!ate)

score(pizza!they )

score(they!pizza)

pizza

spanning tree with maximal score

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

7 / 42

id170 recipe

predict(x) = arg max
predict(x) = arg max

y2y(x)xp2y
y2y(x)xp2y

score( (p))
score( (p))

    feature function extracts useful signals from parts.  

    most work goes into this component.

arc score function

score( modi   er

head ) = ?

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

8 / 42

arc score function

score( modi   er

head ) = f (  (modi   er , head; sentence) )

similar story for transition-based parser
the choice of features is very important

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

9 / 42

b i u
n l p

first-order features

(from ryan mcdonald's phd thesis)

    words and pos of head and mod. 

    words and pos of neighbors of head and mod. 

    pos between head and modi   er. 

    distance between head and modi   er. 

     direction between head and modi   er. 
    many, many combination features.

b i u
n l p

first-order features

(from ryan mcdonald's phd thesis)

a)

b)

c)

d)

c)

b)

d)

second-order features
xi-pos, xk-pos, xj-pos
xk-pos, xj-pos
xk-word, xj-word
xk-word, xj-pos
xk-pos, xj-word

in between pos features
xi-pos, b-pos, xj-pos
surrounding word pos features
xi-pos, xi-pos+1, xj-pos-1, xj-pos
xi-pos-1, xi-pos, xj-pos-1, xj-pos
xi-pos, xi-pos+1, xj-pos, xj-pos+1
xi-pos-1, xi-pos, xj-pos, xj-pos+1

basic bi-gram features
xi-word, xi-pos, xj-word, xj-pos
xi-pos, xj-word, xj-pos
xi-word, xj-word, xj-pos
xi-word, xi-pos, xj-pos
xi-word, xi-pos, xj-word
xi-word, xj-word
xi-pos, xj-pos

second-order features
xi-pos, xk-pos, xj-pos
xk-pos, xj-pos
xk-word, xj-word
xk-word, xj-pos
xk-pos, xj-word

basic bi-gram features
xi-word, xi-pos, xj-word, xj-pos
xi-pos, xj-word, xj-pos
xi-word, xj-word, xj-pos
xi-word, xi-pos, xj-pos
xi-word, xi-pos, xj-word
xi-word, xj-word
xi-pos, xj-pos

in between pos features
xi-pos, b-pos, xj-pos
surrounding word pos features
xi-pos, xi-pos+1, xj-pos-1, xj-pos
xi-pos-1, xi-pos, xj-pos-1, xj-pos
xi-pos, xi-pos+1, xj-pos, xj-pos+1
xi-pos-1, xi-pos, xj-pos, xj-pos+1

basic uni-gram features
xi-word, xi-pos
xi-word
xi-pos
a)
xj-word, xj-pos
basic uni-gram features
xj-word
xi-word, xi-pos
xj-pos
xi-word
xi-pos
xj-word, xj-pos
xj-word
table 3.1: features used by system, f (i, j), where xi is the head and xj the modi   er in
xj-pos
table 3.1: features used by system, f (i, j), where xi is the head and xj the modi   er in
the dependency relation. xi-word: word of head in dependency edge. xj-word: word of
the dependency relation. xi-word: word of head in dependency edge. xj-word: word of
modi   er. xi-pos: pos of head. xj-pos: pos of modi   er. xi-pos+1: pos to the right of
modi   er. xi-pos: pos of head. xj-pos: pos of modi   er. xi-pos+1: pos to the right of
head in sentence. xi-pos-1: pos to the left of head. xj-pos+1: pos to the right of modi   er.
head in sentence. xi-pos-1: pos to the left of head. xj-pos+1: pos to the right of modi   er.
xj-pos-1: pos to the left of modi   er. b-pos: pos of a word in between head and modi   er.
xj-pos-1: pos to the left of modi   er. b-pos: pos of a word in between head and modi   er.

con   guration. the second class of additional features represents the local context of the
attachment, that is, the words before and after the head-modi   er pair. these features take
the form of pos 4-grams: the pos of the head, modi   er, word before/after head and word
before/after modi   er. we also include back-off features to trigrams where one of the local

con   guration. the second class of additional features represents the local context of the
attachment, that is, the words before and after the head-modi   er pair. these features take

manual feature templates

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

11 / 42

core features + feature combinations

replace feature combinations with non-linear learner

figure from chen and manning (2014)
similiar approach in pei et al, weiss et al, andor et al

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

12 / 42

core features + non-linear classi   er

replace feature combinations with non-linear learner
but still need
to de   ne 
good features.

figure from chen and manning (2014)
similiar approach in pei et al, weiss et al, andor et al

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

12 / 42

b i u
n l p

our take on it

let's just use a bidirectional lstm

b i u
n l p

 (x, jumped, f ox)

concat

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n
fox/n

who/p

likes/v

apples/n

jumped/v
jumped/v

over/p

a/d

dog/n

score(h, m, x) = m lp ( (x, h, m))

 (x, h, m) = [birn n (x, h); birn n (x, m)]

b i u
n l p

score

mlp(                        )
 (x, jumped, f ox)

concat

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n
fox/n

who/p

likes/v

apples/n

jumped/v
jumped/v

over/p

a/d

dog/n

score(h, m, x) = m lp ( (x, h, m))

 (x, h, m) = [birn n (x, h); birn n (x, m)]

b i u
n l p

 (x, jumped, f ox)

concat

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n
fox/n

who/p

likes/v

apples/n

jumped/v
jumped/v

over/p

a/d

dog/n

score(h, m, x) = m lp ( (x, h, m))

 (x, h, m) = [birn n (x, h); birn n (x, m)]

b i u
n l p

 (x, f ox, jumped)

concat

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n
fox/n

who/p

likes/v

apples/n

jumped/v
jumped/v

over/p

a/d

dog/n

score(h, m, x) = m lp ( (x, h, m))

 (x, h, m) = [birn n (x, h); birn n (x, m)]

b i u
n l p

 (x, over, f ox)

concat

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n
fox/n

who/p

likes/v

apples/n

jumped/v

over/p
over/p

a/d

dog/n

score(h, m, x) = m lp ( (x, h, m))

 (x, h, m) = [birn n (x, h); birn n (x, m)]

b i u
n l p

 (x, over, jumped)

concat

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n

who/p

likes/v

apples/n

jumped/v
over/p
jumped/n over/p

a/d

dog/n

score(h, m, x) = m lp ( (x, h, m))

 (x, h, m) = [birn n (x, h); birn n (x, m)]

the two bi-id56 vectors give us:

in   nite window around head 
in   nite window around mod 
distance between head and mod 
content between head and mod

concat

and more?

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n
fox/n

who/p

likes/v

apples/n

jumped/v
jumped/v

over/p

a/d

dog/n

score(h, m, x) = m lp ( (x, h, m))

 (x, h, m) = [birn n (x, h); birn n (x, m)]

arc score (intuition)

the bilstm encoding of a word holds information about its
attachment preferences
the score is dependent on the bilstm encoding which in turn
depends on the entire sentence
therefore, the score function focused on a speci   c arc is considering
also the entire sentence attachement preferences

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

21 / 42

tree score

dobj

nsubj

det

prep

nsubj

score( they

ate

the

pizza

with

anchovies

) =

mlp

vthey

mlp

vate

concat

concat

+

vthe

concat

mlp

vpizza

concat

mlp

vwith

mlp

vanchovies

concat

concat

lstm b

s5

lstm b

s4

s3

lstm b

lstm b

s2

s1

lstm b

s0

lstm b

lstm f

lstm f

lstm f

lstm f

lstm f

lstm f

xthey

xate

xthe

xpizza

xwith

xanchovies

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

22 / 42

large margin objective

max(0, 1 

+

)

+

+

mlp

mlp

mlp

mlp

mlp

vthey

concat

vate

concat

vthe

concat

vpizza

concat

vwith

concat

vanchovies

concat

lstm b

lstm b

lstm b

lstm b

lstm b

lstm b

lstm f

lstm f

lstm f

lstm f

lstm f

lstm f

xthey

xate

xthe

xpizza

xwith

xanchovies

mlp

mlp

mlp

mlp

mlp

vthey

concat

vate

concat

vthe

concat

vpizza

concat

vwith

concat

vanchovies

concat

lstm b

lstm b

lstm b

lstm b

lstm b

lstm b

lstm f

lstm f

lstm f

lstm f

lstm f

lstm f

xthey

xate

xthe

xpizza

xwith

xanchovies

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

23 / 42

training objective

gold tree should score a margin above all other trees

x(h,m)2y

m lp ( (x, h, m))   x(h,m)2y06=y

m lp ( (x, h, m)) > 1

 (x, h, m) = [birn n (x, h); birn n (x, m)]

backdrop all the way back through the bi-lstm

graph-based parsing (more details)

cost augmentation: make non-gold attachments more attractive in
training by adding a constant to their score
id72: learning the label on the same bilstm
representation helps both in terms of accuracy and performance.
for speed: simple algebric    trick    reduces the number of matrix
multiplication signi   cantly.

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

24 / 42

graph-based parsing (more details)

cost augmentation: make non-gold attachments more attractive in
training by adding a constant to their score
id72: learning the label on the same bilstm
representation helps both in terms of accuracy and performance.
for speed: simple algebric    trick    reduces the number of matrix
multiplication signi   cantly.

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

24 / 42

arc labels (id72)

dobj

nsubj

det

prep

nsubj

they

ate

the

pizza

with

anchovies

the arc labels hold important additional syntactic information

the labels contribute information useful for the unlabeled case too

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

26 / 42

arc labels (id72)

nsu

p
r

e

p

bj

d

o

et
jd

b

rel a u x

di   erent mlp

mlplbl

vw1

vmodi   er

vw3

concat

concat

concat

vhead

concat

vw5

concat

same bilstm

lstm b

lstm b

lstm b

lstm b

lstm b

lstm f

lstm f

lstm f

lstm f

lstm f

xw1

xmodi   er

xw3

xhead

xw5

enrich bilstm representation by learning labels

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

27 / 42

arc labels (id72)

nsu

p
r

e

p

bj

d

o

et
jd

b

rel a u x

di   erent mlp

mlplbl

vw1

vmodi   er

vw3

concat

concat

concat

vhead

concat

vw5

concat

same bilstm

lstm b

lstm b

lstm b

lstm b

lstm b

lstm f

lstm f

lstm f

lstm f

lstm f

xw1

xmodi   er

xw3

xhead

xw5

enrich bilstm representation by learning labels

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

27 / 42

b i u
n l p

in parsing time

    run (deep) bi-lstm over words+pos. 

    this gives us a vector vi for each word. 

    compute scores for each arc (h,m) via  
    decode using arc scores.

m lp ([vh; vm])

graph-based parsing

and this works:

93.2 uas with two features,

   rst-order parser,

without external embeddings.

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

28 / 42

graph-based parsing

and this works:

93.2 uas with two features,

   rst-order parser,

without external embeddings.

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

28 / 42

b i u
n l p

this is remarkably effective!

b i u
n l p

we can use same trick also 
for transition based parsing

transition-based parsing (oracle)

con   guration:

s2

the

s1

jumped

s0

over

b0

the

b1

lazy

b2

dog

b3

root

fox

brown

scoring:

(scoreleftarc, scorerightarc, scoreshift)

mlp

vthe

concat

vbrown

concat

vfox

vjumped

concat

concat

vover

concat

vthe

vlazy

vdog

concat

concat

concat

vroot

concat

lstm b

s8

lstm b

s7

lstm b

s6

lstm b

s5

lstm b

s4

lstm b

s3

lstm b

s2

s1

lstm b

s0

lstm b

lstm f

lstm f

lstm f

lstm f

lstm f

lstm f

lstm f

lstm f

lstm f

xthe

xbrown

xfox

xjumped

xover

xthe

xlazy

xdog

xroot

eliyahu kiperwasser (bar-ilan university)

simple and accurate id33

logo-biu.png

31 / 42

b i u
n l p

also worth noting:

incremental parsing with minimal features using bi-directional lstm

james cross and liang huang

school of electrical engineering and computer science

oregon state university
corvallis, oregon, usa

{crossj,liang.huang}@oregonstate.edu

abstract

constituency parsing   

transition-based

though such a model can exploit similarities be-
tween words and other embedded categories, and
learn interactions among those atomic features, it
cannot exploit any other details of the text.

recently, neural network approaches for
parsing have largely automated the combi-
nation of individual features, but still rely
on (often a larger number of) atomic fea-
tures created from human linguistic intu-
ition, and potentially omitting important
global context. to further reduce fea-
ture engineering to the bare minimum, we

we aim to reduce the need for manual induction
of atomic features to the bare minimum, by us-
ing bi-directional recurrent neural networks to au-
tomatically learn context-sensitive representations
for each word in the sentence. this approach al-
lows the model to learn arbitrary patterns from the

b i u
n l p

also worth noting:

fast(er) exact decoding and global training for transition-based

id33 via a minimal feature set

tianze shi

cornell university

tianze@cs.cornell.edu

liang huang

oregon state university
liang.huang.sh@gmail.com

lillian lee

cornell university
llee@cs.cornell.edu

publication venue: proceedings of emnlp 2017

id33   

transition-based + id145

abstract

we    rst present a minimal feature set for
transition-based id33, con-
tinuing a recent trend started by kiper-
wasser and goldberg (2016a) and cross

rithms do exist, having been introduced by huang
and sagae (2010) and kuhlmann et al. (2011), un-
fortunately, they are prohibitively costly in prac-
tice, since the number of positions considered can
factor into the exponent of the running time. for
instance, huang and sagae employ a fairly re-

b i u
n l p

also worth noting:

transition-based id33 with stack long short-term memory

chris dyer|  miguel ballesteros}  wang ling  austin matthews  noah a. smith 

|marianas labs }nlp group, pompeu fabra university  carnegie mellon university

chris@marianaslabs.com, miguel.ballesteros@upf.edu,

{lingwang,austinma,nasmith}@cs.cmu.edu

abstract

"id200" parser is very similar to the bilstm

(but does have extra compositionality)

decisions (nivre, 2007; nivre, 2008; nivre, 2009;
choi and mccallum, 2013; bohnet and nivre,
2012), through feature engineering (zhang and
nivre, 2011; ballesteros and nivre, 2014; chen et
al., 2014; ballesteros and bohnet, 2014) and more
recently using neural networks (chen and man-
ning, 2014; stenetorp, 2013).

in retrospect

we propose a technique for learning rep-
resentations of parser states in transition-
based dependency parsers. our primary
innovation is a new control structure for
sequence-to-sequence neural networks   
the id200. like the conventional
stack data structures used in transition-
based parsing, elements can be pushed to
or popped from the top of the stack in
constant time, but, in addition, an lstm

we extend this last line of work by learning
representations of the parser state that are sensi-
tive to the complete contents of the parser   s state:
that is, the complete input buffer, the complete

b i u
n l p

but let's get back to the 
1st-order graph parser

1st order decomposition    

b i u
n l p
graph-based parsing

is incredibly naive

dobj

nsubj

det

prep

nsubj

score( they

ate

the

pizza

with

anchovies

) =

nsubj

dobj

det

score( they

ate

) + score( ate

pizza

) + score( the

pizza

) +

prep

nsubj

score( pizza

with

) + score( with

anchovies

)

logo-biu.png

b i u
n l p

and yet...

rbg parser (lei et al, 2014), 1st order:
turbopasrer (martins et al, 2013), 3rd order:
bilstm (k&g, 2016), 1st order:
bilstm (k&g, 2016), + embeddings:
bilstm (k&g, 2016), + emb, bug    x:
dozat and manning 2017:

91.7 uas
93.1 uas
93.2 uas
92.7 uas
94.0 uas

b i u
n l p

published as a conference paper at iclr 2017

deep biaffine attention for neural
id33

timothy dozat
stanford university
tdozat@stanford.edu

christopher d. manning
stanford university
manning@stanford.edu

abstract

this paper builds off recent work from kiperwasser & goldberg (2016) using neu-
ral attention in a simple graph-based dependency parser. we use a larger but more
thoroughly regularized parser than other recent bilstm-based approaches, with
biaf   ne classi   ers to predict arcs and labels. our parser gets state of the art or

b i u
n l p

score

m lp ([biid56(h); biid56(m)])

mlp(                        )
 (x, jumped, f ox)

concat

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n
fox/n

who/p

likes/v

apples/n

jumped/v
jumped/v

over/p

a/d

dog/n

k&g 2016

b i u
n l p

score

m lp ([biid56(h); biid56(m)])

 (x, jumped, f ox)

concat

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n
fox/n

who/p

likes/v

apples/n

jumped/v
jumped/v

over/p

a/d

dog/n

dozat and manning, 2017

b i u
n l p

score

m lp ([biid56(h); biid56(m)])
biid56(h) m biid56(m)

 (x, jumped, f ox)

concat

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n
fox/n

who/p

likes/v

apples/n

jumped/v
jumped/v

over/p

a/d

dog/n

dozat and manning, 2017

b i u
n l p

score

m lp ([biid56(h); biid56(m)])
biid56(h) m biid56(m)

 (x, jumped, f ox)

concat

bi

bi

bi

bi

bi

bi

bi

bi

bi

the/d

fox/n
fox/n

who/p

likes/v

apples/n

jumped/v
jumped/v

over/p

a/d

dog/n

dozat and manning, 2017

b i u
n l p

and yet...

rbg parser (lei et al, 2014), 1st order:
turbopasrer (martins et al, 2013), 3rd order:
bilstm (k&g, 2016), 1st order:
bilstm (k&g, 2016), + embeddings:
bilstm (k&g, 2016), + emb, bug    x:
dozat and manning 2017:
(bilstm. first order)

91.7 uas
93.1 uas
93.2 uas
92.7 uas
94.0 uas
95.7 uas

b i u
n l p

conll 2017 shared task

b i u
n l p

conll 2017 shared task

b i u
n l p

conll 2017 shared task

bilstm + graph + tuning

dozat and manning
(   rst-order features)

b i u
n l p

conll 2017 shared task

bilstm + graph + tuning

dozat and manning
(   rst-order features)

model of shi, huang and lee
bilstm + transition + dp

(   rst-order features)

b i u
n l p

conll 2017 shared task

bilstm + graph + tuning

dozat and manning
(   rst-order features)

model of shi, huang and lee
bilstm + transition + dp

(   rst-order features)

(both used also character-level lstms for words)

b i u
n l p

the best parsers  
in the world today 
 are based on  

1st-order decomposition 
i    nd this remarkable

over a bilstm 

b i u
n l p

the best parsers  
in the world today 
 are based on  

1st-order decomposition 
i    nd this remarkable

over a bilstm 

b i u
n l p

take home questions

    why does it work? 

    what is encoded in these vectors? 

    where does it fail? 

    how can we improve? (in an interesting way)?

morphology? pre-training? multi-tasking? composition?

b i u
n l p

take home questions

    why does it work? 

    what is encoded in these vectors? 

    where does it fail? 

    how can we improve? (in an interesting way)?

morphology? pre-training? multi-tasking? composition?

thanks for listening!

