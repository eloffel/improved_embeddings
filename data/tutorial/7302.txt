image classification: a core task in id161
cat
(assume given set of discrete labels)
{dog, cat, truck, plane, ...}
source: andrej karpathy & fei-fei li
the problem:
semantic gap
images are represented as 3d arrays of numbers, with integers between [0, 255].

e.g. 
300 x 100 x 3 

(3 for 3 color channels rgb)

source: andrej karpathy & fei-fei li
an image classifier
unlike e.g. sorting a list of numbers,
 
no obvious way to hard-code the algorithm for recognizing a cat, or other classes.
source: andrej karpathy & fei-fei li
data-driven approach:
collect a dataset of images and label them
use machine learning to train an image classifier
evaluate the classifier on a withheld set of test images
example training set
source: andrej karpathy & fei-fei li
first classifier: nearest neighbor classifier
remember all training images and their labels
predict the label of the most similar training image
source: andrej karpathy & fei-fei li
example dataset: cifar-10
10 labels 
50,000 training images
10,000 test images.
for every test image (first column), examples of nearest neighbors in rows
source: andrej karpathy & fei-fei li
how do we compare the images? what is the distance metric?
l1 distance:
source: andrej karpathy & fei-fei li
aside: approximate nearest neighbor
find approximate nearest neighbors quickly
source: andrej karpathy & fei-fei li
the choice of distance is a hyperparameter
l1 (manhattan) distance
l2 (euclidean) distance
two most commonly used special cases of p-norm
source: andrej karpathy & fei-fei li
k-nearest neighbor
find the k nearest images, have them vote on the label
the data
nn classifier
5-nn classifier
http://en.wikipedia.org/wiki/k-nearest_neighbors_algorithm
source: andrej karpathy & fei-fei li
what is the best distance to use?
what is the best value of k to use?

i.e. how do we set the hyperparameters?
source: andrej karpathy & fei-fei li
what is the best distance to use?
what is the best value of k to use?

i.e. how do we set the hyperparameters?


very problem-dependent. 
must try them all out and see what works best.
source: andrej karpathy & fei-fei li

trying out what hyperparameters work best on test set:
very bad idea. the test set is a proxy for the generalization performance
source: andrej karpathy & fei-fei li
      validation data
use to tune hyperparameters
evaluate on test set once at the end
cross-validation
cycle through the choice of which fold is the validation fold, average results.
source: andrej karpathy & fei-fei li
example of
5-fold cross-validation
for the value of k.

each point: single
outcome. 

the line goes
through the mean, bars
indicated standard
deviation

(seems that k = 7 works best
for this data)
source: andrej karpathy & fei-fei li
recap
image classification: we are given a training set of labeled images, asked to predict labels on test set. common to report the accuracy of predictions (fraction of correctly predicted images)
we introduced the k-nearest neighbor classifier, which predicts the labels based on nearest images in the training set
we saw that the choice of distance and the value of k are hyperparameters that are tuned using a validation set, or through cross-validation if the size of the data is small. 
once the best set of hyperparameters is chosen, the classifier is evaluated once on the test set.
source: andrej karpathy & fei-fei li
nearest neighbor classifier
q: what is the complexity of the nn classifier w.r.t training set of n images and test set of m images?


at training time?
o(1)


at test time?
o(nm)
source: andrej karpathy & fei-fei li
linear classification
define a score function

class scores
source: andrej karpathy & fei-fei li
linear classification
define a score function
(assume cifar-10 example so
32 x 32 x 3 images, 10 classes)

weights
[10 x 3072]
bias vector
[10 x 1]
data (image)
[3072 x 1]
class scores
[10 x 1]

source: andrej karpathy & fei-fei li
linear classification
source: andrej karpathy & fei-fei li
interpreting a linear classifier

question:
what can a linear classifier do?
source: andrej karpathy & fei-fei li
interpreting a linear classifier

example training classifiers on cifar-10:
source: andrej karpathy & fei-fei li
interpreting a linear classifier
source: andrej karpathy & fei-fei li
bias trick
source: andrej karpathy & fei-fei li
so far:
we defined a (linear) score function: 
source: andrej karpathy & fei-fei li
2. define a id168 (or cost function, or objective)

scores, label                 loss.
example:
question: if you were to assign a single number to how    unhappy    you are with these scores, what would you do?

source: andrej karpathy & fei-fei li
2. define a id168 (or cost function, or objective)
one (of many ways) to do it: multiclass id166 loss
(one possible generalization of binary support vector machine to multiple classes)
source: andrej karpathy & fei-fei li
2. define a id168 (or cost function, or objective)
one (of many ways) to do it: multiclass id166 loss
loss due to example i


sum over all incorrect labels
difference between the correct class score and incorrect class score

source: andrej karpathy & fei-fei li
loss due to example i


sum over all incorrect labels
difference between the correct class score and incorrect class score

source: andrej karpathy & fei-fei li
example:
e.g. 10
loss = ?
source: andrej karpathy & fei-fei li
example:
e.g. 10
source: andrej karpathy & fei-fei li
there is a bug with the objective   
source: andrej karpathy & fei-fei li
l2 id173

id173 strength
source: andrej karpathy & fei-fei li
l2 id173: motivation
source: andrej karpathy & fei-fei li
source: andrej karpathy & fei-fei li
so far   
score function



id168
source: andrej karpathy & fei-fei li
source: andrej karpathy & fei-fei li
softmax classifier
score function
is the same
(extension of id28 to multiple classes)
source: andrej karpathy & fei-fei li
softmax classifier
score function
is the same


i.e. we   re minimizing the negative log likelihood.

softmax function
source: andrej karpathy & fei-fei li
source: andrej karpathy & fei-fei li
softmax vs. id166
scores:
[10, -2, 3]
[10, 9, 9]
[10, -100, -100]
1
source: andrej karpathy & fei-fei li
softmax vs. id166
1
source: andrej karpathy & fei-fei li
recap
we introduced a parametric approach to image classification
we defined a score function (linear map)
we defined a id168 (id166 / softmax)

one problem remains: how to find w,b ?
source: andrej karpathy & fei-fei li
optimization
source: andrej karpathy & fei-fei li
source: andrej karpathy & fei-fei li
following the gradient
in 1-dimension, the derivative of a function:
in multiple dimension, the gradient is the vector of (partial derivatives).
source: andrej karpathy & fei-fei li
evaluation the 
gradient numerically
   finite difference approximation   
source: andrej karpathy & fei-fei li
evaluation the 
gradient numerically

   centered difference formula   
in practice:
source: andrej karpathy & fei-fei li
original w
negative gradient direction
source: andrej karpathy & fei-fei li
the problems with 
numerical gradient:
approximate
very slow to evaluate
source: andrej karpathy & fei-fei li
we need something better...
calculus
source: andrej karpathy & fei-fei li
source: andrej karpathy & fei-fei li
in summary:

numerical gradient: approximate, slow, easy to write

analytic gradient: exact, fast, error-prone


=>

in practice: always use analytic gradient, but check implementation with numerical gradient. this is called a gradient check.
source: andrej karpathy & fei-fei li
id119
source: andrej karpathy & fei-fei li
mini-batch id119
only use a small portion of the training set to compute the gradient.
common mini-batch sizes are ~100 examples. 
e.g. krizhevsky ilsvrc convnet used 256 examples
source: andrej karpathy & fei-fei li
stochastic id119 (sgd)
use a single example at a time
1
(also sometimes called on-line id119)
source: andrej karpathy & fei-fei li
summary
always use mini-batch id119

incorrectly refer to it as    doing sgd    as everyone else

the mini-batch size is a hyperparameter, but it is not very common to cross-validate over it (usually based on practical concerns, e.g. space/time efficiency)
(or call it batch id119)
source: andrej karpathy & fei-fei li
fun question: suppose you were training with mini-batch size of 100, and now you switch to mini-batch of size 1. your learning rate (step size) should:

increase
decrease
stay the same
become zero
source: andrej karpathy & fei-fei li
the dynamics of id119



always pull the weights down


pull some weights up and some down
source: andrej karpathy & fei-fei li
momentum update
gradient
momentum
update
source: andrej karpathy & fei-fei li
many other ways to perform optimization   

second order methods that use the hessian (or its approximation): bfgs, lbfgs, etc.

currently, the lesson from the trenches is that well-tuned sgd+momentum is very hard to beat for neural networks (to be introduced later).
source: andrej karpathy & fei-fei li
recap:
we looked at image features, and saw that id98s can be thought of as learning the features in end-to-end manner
we explored intuition about what the loss surfaces of linear classifiers look like
we introduced id119 as a way of optimizing id168s, as well as batch id119 and sgd.
numerical gradient: slow :(, approximate :(, easy to write :)
analytic gradient: fast :), exact :), error-prone :(
in practice: gradient check (but be careful)
source: andrej karpathy & fei-fei li
brief aside: image features
in practice, very rare to see id161 applications that train linear classifiers on pixel values


source: andrej karpathy & fei-fei li
brief aside: image features
in practice, very rare to see id161 applications that train linear classifiers on pixel values


source: andrej karpathy & fei-fei li
example: color (hue) histogram
hue bins
+1
source: andrej karpathy & fei-fei li
example: hog features

8x8 pixel region,
quantize the edge orientation into 9 bins

(images from vlfeat.org)
source: andrej karpathy & fei-fei li
example: bag of words
resize patch to a fixed size (e.g. 32x32 pixels)
extract hog on the patch (get 144 numbers)


gives a matrix of size
[number_of_features x 144]
repeat for each detected feature
problem: different images will have different numbers of features. need fixed-sized vectors for linear classification
source: andrej karpathy & fei-fei li
example: bag of words
resize patch to a fixed size (e.g. 32x32 pixels)
extract hog on the patch (get 144 numbers)


gives a matrix of size
[number_of_features x 144]
repeat for each detected feature
source: andrej karpathy & fei-fei li




example: bag of words


144
visual word vectors
learn id116 centroids 
   vocabulary of visual words
e.g. 1000 centroids
1000-d vector
1000-d vector
1000-d vector
histogram of visual words
source: andrej karpathy & fei-fei li
brief aside: image features

source: andrej karpathy & fei-fei li
most recognition systems are build on the same architecture
(slide from yann lecun)
source: andrej karpathy & fei-fei li
most recognition systems are build on the same architecture
(slide from yann lecun)
id98s:
end-to-end
models
source: andrej karpathy & fei-fei li
