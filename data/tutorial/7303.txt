http://swiked.tumblr.com/post/112073818575/guys-please-help-me-is-this-dress-white-and
http://www.wired.com/2015/02/science-one-agrees-color-dress/
4

backprop 

5
source: andrej karpathy & fei-fei li
6
example: x = 4, y = -3.        => f(x,y) = -12
partial derivatives
gradient
question: if i increase x by h, how would the output of f change?
source: andrej karpathy & fei-fei li
7
compound expressions:


source: andrej karpathy & fei-fei li
8
compound expressions:


chain rule:
source: andrej karpathy & fei-fei li
9
another example:
source: andrej karpathy & fei-fei li
10
another example:

-1/(1.37^2) = -0.53
source: andrej karpathy & fei-fei li
11
another example:

[local gradient] x [its gradient]
[1] x [-0.53] = -0.53
source: andrej karpathy & fei-fei li
12
another example:

[local gradient] x [its gradient]
[e^(-1)] x [-0.53] = -0.20
source: andrej karpathy & fei-fei li
13
another example:

[local gradient] x [its gradient]
[-1] x [-0.2] = 0.2
source: andrej karpathy & fei-fei li
14
another example:

[local gradient] x [its gradient]
[1] x [0.2] = 0.2
[1] x [0.2] = 0.2  (both inputs!)

source: andrej karpathy & fei-fei li
15
another example:

[local gradient] x [its gradient]
x0: [2] x [0.2] ~= 0.4
w0: [-1] x [0.2] = -0.2
source: andrej karpathy & fei-fei li
16
every gate during backprop computes, for all its inputs:

           [local gradient] x [gate gradient]
can be computed right away, even during forward pass
the gate receives this during id26
a gate hanging out
source: andrej karpathy & fei-fei li
17
sigmoid function
source: andrej karpathy & fei-fei li
18
sigmoid function

(0.73) * (1 - 0.73) = 0.2
source: andrej karpathy & fei-fei li
19
sigmoid function

source: andrej karpathy & fei-fei li
20
multiple inputs xi
multiple outputs yj

module (layer)
xi
yj

   ..
f
source: andrej karpathy & fei-fei li
21
we are ready:
source: andrej karpathy & fei-fei li
22
in summary
in practice it is rarely needed to derive long gradients of variables on pen and paper
structured your code in stages (layers), where you can derive the local gradients, then chain the gradients during backprop.
caveat: sometimes gradients simplify (e.g. for sigmoid, also softmax). group these.
source: andrej karpathy & fei-fei li
23
neural 
networks
source: andrej karpathy & fei-fei li
24
source: andrej karpathy & fei-fei li
25
source: andrej karpathy & fei-fei li
26
source: andrej karpathy & fei-fei li
27
sigmoid activation function
source: andrej karpathy & fei-fei li
28
a single neuron can be used as a binary linear classifier
source: andrej karpathy & fei-fei li
29
be very careful with your brain analogies:


biological neurons:
many different types
dendrites can perform complex non-linear computations
synapses are not a single weight but a complex non-linear dynamical system
rate code may not be adequate
[dendritic computation. london and hausser]
source: andrej karpathy & fei-fei li
30
id180
source: andrej karpathy & fei-fei li
31
id180
sigmoid
squashes numbers to range [0,1]
historically popular since they have nice interpretation as a saturating    firing rate    of a neuron

a problem: saturated neurons    kill    the gradients
source: andrej karpathy & fei-fei li
32
id180
relu
computes f(x) = max(0,x)

does not saturate
very computationally efficient
converges much faster than sigmoid in practice! (e.g. 6x)

just one annoying problem   

hint: what is the gradient when x < 0?
source: andrej karpathy & fei-fei li
data cloud
33
active relu
dead relu
will never activate 
=> never update
source: andrej karpathy & fei-fei li
34
tldr: in practice:

use relu. be careful with your learning rates
never use sigmoid
source: andrej karpathy & fei-fei li
35
neural 
networks
source: andrej karpathy & fei-fei li
36
neural networks: architectures
   fully-connected    layers
   2-layer neural net   , or
   1-hidden-layer neural net   
   3-layer neural net   , or
   2-hidden-layer neural net   
source: andrej karpathy & fei-fei li
37
neural networks: architectures
number of neurons: ?
number of weights: ?
number of parameters: ?

source: andrej karpathy & fei-fei li
38
neural networks: architectures
number of neurons: 4+2 = 6
number of weights: [4x3 + 2x4] = 20
number of parameters: 20 + 6 = 26 (biases!)

number of neurons: ?
number of weights: ?
number of parameters: ?
source: andrej karpathy & fei-fei li
39
neural networks: architectures
number of neurons: 4+2 = 6
number of weights: [4x3 + 2x4] = 20
number of parameters: 20 + 6 = 26 (biases!)

number of neurons: 4 + 4 + 1 = 9
number of weights: [4x3+4x4+1x4]=32
number of parameters: 32+9 = 41
source: andrej karpathy & fei-fei li
40
neural networks: architectures
modern id98s: ~10 million neurons
human visual cortex: ~5 billion neurons

source: andrej karpathy & fei-fei li
41
what kinds of functions can a neural network represent?
[http://neuralnetworksanddeeplearning.com/chap4.html]
source: andrej karpathy & fei-fei li
42
what kinds of functions 
can a neural network 
represent?
[http://neuralnetworksanddeeplearning.com/chap4.html]

source: andrej karpathy & fei-fei li
43
setting the number of layers and their sizes
more neurons = more capacity
source: andrej karpathy & fei-fei li
44
(you can play with this demo over at convnetjs: http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)
do not use size of neural network as a regularizer. use stronger id173 instead:
source: andrej karpathy & fei-fei li
45
summary

we arrange neurons into fully-connected layers
the abstraction of a layer has a nice property in that it allows us to use efficient vectorized code (matrix multiplies)
neural networks are universal function approximators but this doesn   t mean much.
neural networks are not neural
source: andrej karpathy & fei-fei li
