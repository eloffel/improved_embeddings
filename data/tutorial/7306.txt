deep id23

john schulman

1

mlss, may 2016, cadiz

1berkeley arti   cial intelligence research lab

agenda

introduction and overview

id100

id23 via black-box optimization

id189

variance reduction for policy gradients

trust region and natural gradient methods

open problems

course materials: goo.gl/5wsgbj

introduction and overview

what is id23?

(cid:73) branch of machine learning concerned with taking

sequences of actions

(cid:73) usually described in terms of agent interacting with a
previously unknown environment, trying to maximize
cumulative reward

agentenvironmentactionobservation,rewardmotor control and robotics

robotics:

(cid:73) observations: camera images, joint angles
(cid:73) actions: joint torques
(cid:73) rewards: stay balanced, navigate to target locations,

serve and protect humans

business operations

(cid:73) inventory management

(cid:73) observations: current inventory levels
(cid:73) actions: number of units of each item to purchase
(cid:73) rewards: pro   t

(cid:73) resource allocation: who to provide customer service to

   rst

(cid:73) routing problems: in management of shipping    eet,

which trucks / truckers to assign to which cargo

games

a di   erent kind of optimization problem (min-max) but still
considered to be rl.

(cid:73) go (complete information, deterministic)     alphago2
(cid:73) backgammon (complete information, stochastic)    

td-gammon3

(cid:73) stratego (incomplete information, deterministic)
(cid:73) poker (incomplete information, stochastic)

2david silver, aja huang, et al.    mastering the game of go with deep neural networks and tree search   .

in:

nature 529.7587 (2016), pp. 484   489.

3gerald tesauro.    temporal di   erence learning and td-gammon   .

in: communications of the acm 38.3

(1995), pp. 58   68.

approaches to rl

policy optimizationid145dfo / evolutionpolicy gradientspolicy iterationvalue iterationactor-critic methodsmodi   ed policy iterationid24what is deep rl?

(cid:73) rl using nonlinear function approximators
(cid:73) usually, updating parameters with stochastic gradient

descent

what   s deep rl?

whatever the front half of the cerebral cortex does (motor and
executive cortices)

id100

de   nition

(cid:73) markov decision process (mdp) de   ned by (s,a, p),

where

(cid:73) s: state space
(cid:73) a: action space
(cid:73) p(r , s(cid:48) | s, a): a transition id203 distribution
(cid:73) extra objects de   ned depending on problem setting

(cid:73)   : initial state distribution
(cid:73)   : discount factor

episodic setting

(cid:73) in each episode, the initial state is sampled from   , and
the process proceeds until the terminal state is reached.
for example:

(cid:73) taxi robot reaches its destination (termination = good)
(cid:73) waiter robot    nishes a shift (   xed time)
(cid:73) walking robot falls over (termination = bad)

(cid:73) goal: maximize expected reward per episode

policies

(cid:73) deterministic policies: a =   (s)
(cid:73) stochastic policies: a       (a | s)

(cid:73) parameterized policies:     

episodic setting

s0       (s0)
a0       (a0 | s0)

s1, r0     p(s1, r0 | s0, a0)

a1       (a1 | s1)

s2, r1     p(s2, r1 | s1, a1)
. . .
at   1       (at   1 | st   1)

st , rt   1     p(st | st   1, at   1)

objective:

maximize   (  ), where

  (  ) = e [r0 + r1 +        + rt   1 |   ]

episodic setting

objective:

maximize   (  ), where

  (  ) = e [r0 + r1 +        + rt   1 |   ]

  0a0s0s1a1at-1st  pagentr0r1rt-1environments2parameterized policies

(cid:73) a family of policies indexed by parameter vector        rd

(cid:73) deterministic: a =   (s,   )
(cid:73) stochastic:   (a | s,   )

(cid:73) analogous to classi   cation or regression with input s,
output a. e.g. for neural network stochastic policies:

(cid:73) discrete action space: network outputs vector of

probabilities

(cid:73) continuous action space: network outputs mean and

diagonal covariance of gaussian

id23 via black-box

optimization

derivative free optimization approach

(cid:73) objective:

maximize e [r |   (  ,   )]

(cid:73) view        (cid:4)     r as a black box
(cid:73) ignore all other information other than r collected during

episode

cross-id178 method

(cid:73) evolutionary algorithm
(cid:73) works embarrassingly well

istv  an szita and andr  as l  orincz.    learning
tetris using the noisy cross-id178 method   .
in: neural computation 18.12 (2006),
pp. 2936   2941

victor gabillon, mohammad ghavamzadeh,
and bruno scherrer.    approximate dynamic
programming finally performs well in the
game of tetris   .
in: advances in neural
information processing systems. 2013

cross-id178 method

(cid:73) evolutionary algorithm
(cid:73) works embarrassingly well
(cid:73) a similar algorithm, covariance matrix adaptation, has

become standard in graphics:

cross-id178 method

initialize        rd ,        rd
for iteration = 1, 2, . . . do

collect n samples of   i     n(  , diag(  ))
perform a noisy evaluation ri       i
select the top p% of samples (e.g. p = 20), which we   ll

call the elite set

fit a gaussian distribution, with diagonal covariance,

to the elite set, obtaining a new   ,   .

end for
return the    nal   .

cross-id178 method

(cid:73) analysis: a very similar algorithm is an

minorization-maximization (mm) algorithm, guaranteed
to monotonically increase expected reward

(cid:73) recall that monte-carlo em algorithm collects samples,

reweights them, and them maximizes their logprob

(cid:73) we can derive mm algorithm where each iteration you

maximize(cid:80)

i log p(  i )ri

id189

id189: overview

problem:

maximize e [r |     ]

intuitions: collect a bunch of trajectories, and ...

1. make the good trajectories more probable
2. make the good actions more probable (actor-critic, gae)
3. push the actions towards good actions (dpg, svg)

score function gradient estimator

(cid:73) consider an expectation ex   p(x |   )[f (x)]. want to compute

gradient wrt   

     ex [f (x)] =      

(cid:90)

(cid:90)
(cid:90)
(cid:90)

=

=

dx p(x |   )f (x)
dx      p(x |   )f (x)
dx p(x |   )
dx p(x |   )      log p(x |   )f (x)

     p(x |   )
p(x |   )

f (x)

=
= ex [f (x)      log p(x |   )].

(cid:73) last expression gives us an unbiased gradient estimator. just
sample xi     p(x |   ), and compute   gi = f (xi )      log p(xi |   ).
(cid:73) need to be able to compute and di   erentiate density p(x |   )

wrt   

derivation via importance sampling

alternate derivation using importance sampling

ex      [f (x)] = ex     old
     ex      [f (x)] = ex     old

     ex      [f (x)](cid:12)(cid:12)  =  old

= ex     old

= ex     old

f (x)

p(x |   old)

(cid:21)
(cid:20) p(x |   )
(cid:21)
(cid:20)     p(x |   )
(cid:34)     p(x |   )(cid:12)(cid:12)  =  old
(cid:104)      log p(x |   )(cid:12)(cid:12)  =  old

p(x |   old)

p(x |   old)

f (x)

(cid:35)

f (x)

(cid:105)

f (x)

score function gradient estimator: intuition

  gi = f (xi )      log p(xi |   )

(cid:73) let   s say that f (x) measures how good the

sample x is.

(cid:73) moving in the direction   gi pushes up the

logprob of the sample, in proportion to how
good it is

(cid:73) valid even if f (x) is discontinuous, and

unknown, or sample space (containing x) is a
discrete set

score function gradient estimator: intuition

  gi = f (xi )      log p(xi |   )

score function gradient estimator: intuition

  gi = f (xi )      log p(xi |   )

score function gradient estimator for policies

(cid:73) now random variable x is a whole trajectory

   = (s0, a0, r0, s1, a1, r1, . . . , st   1, at   1, rt   1, st )

     e   [r(   )] = e   [      log p(   |   )r(   )]

(cid:73) just need to write out p(   |   ):

p(   |   ) =   (s0)

[  (at | st,   )p(st+1, rt | st, at)]

t   1(cid:89)

t=0

t   1(cid:88)
(cid:34)

t=0

t   1(cid:88)

t=0

t   1(cid:88)

log p(   |   ) = log   (s0) +

[log   (at | st,   ) + log p(st+1, rt | st, at)]

      log p(   |   ) =      

     e   [r] = e  

log   (at | st,   )

(cid:35)

r     

log   (at | st,   )

(cid:73) interpretation: using good trajectories (high r) as supervised

examples in classi   cation / regression

t=0

policy gradient   slightly better formula

(cid:73) previous slide:

     e   [r] = e  

(cid:33)(cid:35)

      log   (at | st,   )

(cid:73) but we can cut trajectory to t steps and derive gradient

estimator for one reward term rt(cid:48).

     e [rt(cid:48)] = e

rt(cid:48)

      log   (at | st,   )

(cid:73) sum this formula over t, obtaining

(cid:33)(cid:32)t   1(cid:88)

t=0

rt

t(cid:88)
t(cid:48)(cid:88)

t=0

rt(cid:48)

t=0

(cid:34)(cid:32)t   1(cid:88)
(cid:34)
(cid:34)t   1(cid:88)
(cid:34)t   1(cid:88)

t=0

t=0

     e [r] = e

      log   (at | st,   )

= e

t=0

      log   (at | st,   )

(cid:35)

(cid:35)
(cid:35)

t   1(cid:88)

t(cid:48)=t

rt(cid:48)

adding a baseline

(cid:73) suppose f (x)     0,
(cid:73) then for every xi , gradient estimator   gi tries to push up

   x

it   s density

(cid:73) we can derive a new unbiased estimator that avoids this

problem, and only pushes up the density for
better-than-average xi .

     ex [f (x)] =      ex [f (x)     b]

= ex [      log p(x |   )(f (x)     b)]

(cid:73) a near-optimal choice of b is always e [f (x)]

(which must be estimated)

policy gradient with baseline

(cid:73) recall

t   1(cid:88)

t   1(cid:88)

     e   [r] =

      log   (at | st,   )
(cid:73) using the eat [      log   (at | st,   )] = 0, we can show

t(cid:48)=0

rt(cid:48)

t=t

(cid:34)t   1(cid:88)

rt(cid:48)     b(st)

     e   [r] = e  
      log   (at | st,   )
for any    baseline    function b : s     r

t=0

returns(cid:80)t   1

(cid:73) increase logprob of action at proportionally to how much

t=t(cid:48) rt(cid:48) are better than expected

(cid:73) later: use value functions to further isolate e   ect of

action, at the cost of bias

(cid:73) for more general picture of score function gradient

estimator, see stochastic computation graphs 4.

4john schulman, nicolas heess, et al.    gradient estimation using stochastic computation graphs   .

in:

advances in neural information processing systems. 2015, pp. 3510   3522.

(cid:33)(cid:35)

(cid:32)t   1(cid:88)

t=t(cid:48)

that   s all for today

course materials: goo.gl/5wsgbj

variance reduction for policy gradients

review (i)

(cid:73) process for generating trajectory

   = (s0, a0, r0, s1, a1, r1, . . . , st   1, at   1, rt   1, st )

s0       (s0)
a0       (a0 | s0)

s1, r0     p(s1, r0 | s0, a0)

a1       (a1 | s1)

s2, r1     p(s2, r1 | s1, a1)
. . .

at   1       (at   1 | st   1)

st , rt   1     p(st | st   1, at   1)

(cid:73) given parameterized policy   (a | s,   ), the optimization

problem is

maximize

  

e   [r |   (   |   ,   )]

where r = r0 + r1 +        + rt   1.

review (ii)

(cid:73) in general, we can compute gradients of expectations

with the score function gradient estimator

     ex   p(x |   ) [f (x)] = ex [      log p(x |   )f (x)]

(cid:73) we derived a formula for the policy gradient

(cid:34)t   1(cid:88)

t=0

(cid:32)t   1(cid:88)

(cid:33)(cid:35)

rt(cid:48)     b(st)

t=t(cid:48)

     e   [r] = e  

      log   (at | st,   )

value functions

(cid:73) the state-value function v    is de   ned as:

v   (s) = e [r0 + r1 + r2 + . . . | s0 = s]

measures expected future return, starting with state s

(cid:73) the state-action value function q    is de   ned as

q   (s, a) = e [r0 + r1 + r2 + . . . | s0 = s, a0 = a]

(cid:73) the advantage function a   is

a  (s, a) = q   (s, a)     v   (s)

measures how much better is action a than what the
policy    would   ve done.

re   ning the policy gradient formula

(cid:73) recall

     e   [r] = e  

t=0

rt(cid:48)     b(st)

      log   (at | st,   )

(cid:34)t   1(cid:88)
(cid:34)
t   1(cid:88)

(cid:32)t   1(cid:88)
(cid:32)t   1(cid:88)
(cid:34)(cid:32)t   1(cid:88)

(cid:33)(cid:35)
(cid:33)(cid:35)
(cid:33)(cid:35)(cid:35)
(cid:2)      log   (at | st,   )ert st+1...st [q   (st, at)     b(st)](cid:3)

(cid:34)
      log   (at | st,   )ert st+1...st

      log   (at | st,   )

rt(cid:48)     b(st)

rt(cid:48)     b(st)

t=t(cid:48)

t=t(cid:48)

t=t(cid:48)

e  

t=0

=

=

es0...at

t   1(cid:88)
t   1(cid:88)

t=0

=

t=0

es0...at

(cid:73) where the last equality used the fact that

(cid:35)

(cid:34)t   1(cid:88)

t=t(cid:48)

ert st+1...st

rt(cid:48)

= q   (st, at)

re   ning the policy gradient formula

(cid:73) from the previous slide, we   ve obtained

(cid:35)

(cid:35)

(cid:34)t   1(cid:88)

t=0

(cid:34)t   1(cid:88)

     e   [r] = e  

      log   (at | st,   )(q   (st, at)     b(st))

(cid:73) now let   s de   ne b(s) = v   (s), which turns out to be

near-optimal5. we get

     e   [r] = e  

      log   (at | st,   )a  (st, at)

t=0

(cid:73) intuition: increase the id203 of good actions

(positive advantage) decrease the id203 of bad ones
(negative advantage)

5evan greensmith, peter l bartlett, and jonathan baxter.    variance reduction techniques for gradient

estimates in id23   .

in: the journal of machine learning research 5 (2004), pp. 1471   1530.

variance reduction

(cid:35)

(cid:34)t   1(cid:88)

(cid:73) now, we have the following policy gradient formula:

     e   [r] = e  

      log   (at | st,   )a  (st, at)

t=0

(cid:73) a   is not known, but we can plug in a random variable

  at, an advantage estimator

(cid:73) previously, we showed that taking

  at = rt + rt+1 + rt+2 +            b(st)

for any function b(st), gives an unbiased policy gradient
estimator. b(st)     v   (st) gives variance reduction.

the delayed reward problem

(cid:73) one reason rl is di   cult is the long delay between action

and reward

(cid:73)

the delayed reward problem

(cid:73) with id189, we are confounding the

e   ect of multiple actions:

  at = rt + rt+1 + rt+2 +            b(st)

mixes e   ect of at, at+1, at+2, . . .
(cid:73) snr of   at scales roughly as 1/t

(cid:73) only at contributes to signal a  (st, at), but

at+1, at+2, . . . contribute to noise.

var. red. idea 1: using discounts

(cid:73) discount factor   , 0 <    < 1, downweights the e   ect of

rewars that are far in the future   ignore long term
dependencies

(cid:73) we can form an advantage estimator using the

discounted return:

  a  

t = rt +   rt+1 +   2rt+2 + . . .

discounted return

(cid:123)(cid:122)

(cid:124)

(cid:125)

   b(st)

reduces to our previous estimator when    = 1.

(cid:73) so advantage has expectation zero, we should    t baseline

to be discounted value function

(cid:2)r0 +   r1 +   2r2 + . . . | s0 = s(cid:3)

v   ,  (s) = e  

(cid:73)   a  
t

is a biased estimator of the advantage function

var. red. idea 2: value functions in the future

(cid:73) another approach for variance reduction is to use the

value function to estimate future rewards

rt + rt+1 + rt+2 + . . .
   
rt + v (st+1)
rt + rt+1 + v (st+2)
. . .

use empirical rewards

cut o    at one timestep
cut o    at two timesteps

adding the baseline again, we get the advantage
estimators
  at = rt + v (st+1)     v (st)
cut o    at one timestep
  at = rt + rt+1 + v (st+2)     v (st) cut o    at two timesteps
. . .

combining ideas 1 and 2

(cid:73) can combine discounts and value functions in the future, e.g.,

  at = rt +   v (st+1)     v (st), where v approximates
discounted value function v   ,  .

(cid:73) the above formula is called an actor-critic method, where
actor is the policy   , and critic is the value function v .6

(cid:73) going further, the generalized advantage estimator 7

  a  ,  
t =  t + (    )  t+1 + (    )2  t+2 + . . .

where

  t = rt +   v (st+1)     v (st)

(cid:73) interpolates between two previous estimators:

   = 0 :

   = 1 :

rt +   v (st+1)     v (st)
rt +   rt+1 +   2rt+2 +            v (st)

(low v, high b)

(low b, high v)

6vijay r konda and john n tsitsiklis.    actor-critic algorithms.    in: advances in neural information

processing systems. vol. 13. citeseer. 1999, pp. 1008   1014.

7john schulman, philipp moritz, et al.    high-dimensional continuous control using generalized advantage

estimation   .

in: arxiv preprint arxiv:1506.02438 (2015).

alternative approach: reparameterization

(cid:73) suppose problem has continuous action space, a     rd
(cid:73) then d
da q   (s, a) tells use how to improve our action
(cid:73) we can use reparameterization trick, so a is a

deterministic function a = f (s, z), where z is noise. then,

     e   [r] =      q   (s0, a0) +      q   (s1, a1) + . . .

(cid:73) this method is called the deterministic policy gradient8
(cid:73) a generalized version, which also uses a dynamics model,

is described as the stochastic value gradient9

8david silver, guy lever, et al.    deterministic policy gradient algorithms   .

in: icml. 2014;

timothy p lillicrap et al.    continuous control with deep id23   .
arxiv:1509.02971 (2015).

in: arxiv preprint

9nicolas heess et al.    learning continuous control policies by stochastic value gradients   .

in: advances in

neural information processing systems. 2015, pp. 2926   2934.

trust region and natural gradient

methods

optimization issues with policy gradients

(cid:73) hard to choose reasonable stepsize that works for the

whole optimization

(cid:73) we have a gradient estimate, no objective for line search
(cid:73) statistics of data (observations and rewards) change

during learning

(cid:73) they make ine   cient use of data: each experience is only

used to compute one gradient.

(cid:73) given a batch of trajectories, what   s the most we can do

with it?

policy performance function

(cid:73) let   (  ) denote the performance of policy   

  (  ) = e   [r|  ]

(cid:73) the following neat identity holds:

  (    ) =   (  ) + e          [a  (s0, a0) + a  (s1, a1) + a  (s2, a2) + . . . ]

(cid:73) proof: consider nonstationary policy   0  1  2, . . .

  (                    ) =   (              )

+   (                )       (              )
+   (                  )       (                )
+   (                    )       (                  )
+ . . .

(cid:73) tth di   erence term equals a  (st, at)

local approximation

(cid:73) we just derived an expression for the performance of a policy     

relative to   

  (    ) =   (  ) + e          [a  (s0, a0) + a  (s1, a1) + . . . ]
=   (  ) + es0:           [ea0:           [a  (s0, a0) + a  (s1, a1) + . . . ]]

(cid:73) can   t use this to optimize      because state distribution has

complicated dependence.

(cid:73) let   s de   ne l   the local approximation, which ignores change in

state distribution   can be estimated by sampling from   

l  (    ) = es0:         [ea0:           [a  (s0, a0) + a  (s1, a1) + . . . ]]

(cid:35)

(cid:34)t   1(cid:88)
(cid:34)t   1(cid:88)
(cid:34)t   1(cid:88)

t=0

t=0

t=0

= es0:   

= es0:   

= e       

(cid:21)(cid:35)

ea        [a  (st, at)]

(cid:20)     (at | st)

  (at | st)

ea     

    (at | st)
  (at | st)

a  (st, at)

a  (st, at)

(cid:35)

local approximation

(cid:73) now let   s consider parameterized policy,   (a | s,   ). sample with

  old, now write local approximation in terms of   .

    l  old(  ) = es0:   

     l  old(  )(cid:12)(cid:12)  =  0

ea     

(cid:20)     (at | st)
(cid:20)   (at | st,   )

  (at | st)

  (at | st,   old)

(cid:21)(cid:35)
(cid:21)(cid:35)

a  (st, at)

a  (st, at)

(cid:21)(cid:35)
(cid:20)       (at | st,   )
(cid:2)      log   (at | st,   )a  (st, at)(cid:3)(cid:35)

  (at | st,   old)

a  (st, at)

l  (    ) = es0:   

ea     

t=0

t=0

(cid:34)t   1(cid:88)
(cid:34)t   1(cid:88)
(cid:34)t   1(cid:88)
(cid:34)t   1(cid:88)
=        (  )(cid:12)(cid:12)  =  old

= es0:   

= es0:   

t=0

t=0

ea     

ea     

(cid:73) l  old(  ) matches   (  ) to    rst order around   old.

mm algorithm

(cid:73) theorem (ignoring some details)10

(cid:124) (cid:123)(cid:122) (cid:125)
  (  )     l  old(  )

local approx. to   

(cid:124)

s

(cid:123)(cid:122)

    c max

dkl [  (   |   old, s) (cid:107)   (   |   , s)]

(cid:125)

penalty for changing policy

(cid:73)
(cid:73) if   old       new improves lower bound, it   s guaranteed to

improve   

10john schulman, sergey levine, et al.    trust region policy optimization   .

in: arxiv preprint

arxiv:1502.05477 (2015).

l(  )+c  kll(  )  (  )  review

(cid:73) want to optimize   (  ). collected data with policy

parameter   old, now want to do update

(cid:73) derived local approximation l  old(  )
(cid:73) optimizing kl penalized local approximation givesn

guaranteed improvement to   

(cid:73) more approximations gives practical algorithm, called

trpo

trpo   approximations

(cid:73) steps:

(cid:73) instead of max over state space, take mean
(cid:73) linear approximation to l, quadratic approximation to

kl divergence

(cid:73) use hard constraint on kl divergence instead of penalty

(cid:73) solve the following problem approximately

maximize l  old(  )
subject to d kl[  old (cid:107)   ]       

(cid:73) solve approximately through line search in the natural

gradient direction s = f    1g

(cid:73) resulting algorithm is a re   ned version of natural policy

gradient 11

11sham kakade.    a natural policy gradient.    in: nips. vol. 14. 2001, pp. 1531   1538.

empirical results: trpo + gae

(cid:73) trpo, with neural network policies, was applied to learn

controllers for 2d robotic swimming, hopping, and
walking, and playing atari games12

(cid:73) used trpo along with generalized advantage estimation
to optimize locomotion policies for 3d simulated robots13

12john schulman, sergey levine, et al.    trust region policy optimization   .

in: arxiv preprint

arxiv:1502.05477 (2015).

13john schulman, philipp moritz, et al.    high-dimensional continuous control using generalized advantage

estimation   .

in: arxiv preprint arxiv:1506.02438 (2015).

friday, june 6, 14putting in perspective

quick and incomplete overview of recent results with deep rl
algorithms

(cid:73) id189

(cid:73) trpo + gae
(cid:73) standard policy gradient (no trust region) + deep nets

+ parallel implementation14

(cid:73) repar trick15

(cid:73) id2416 and modi   cations17
(cid:73) combining search + supervised learning18
14v. mnih et al.    playing atari with deep id23   .
15nicolas heess et al.    learning continuous control policies by stochastic value gradients   .

in: arxiv preprint arxiv:1312.5602 (2013).

in: advances in

neural information processing systems. 2015, pp. 2926   2934; timothy p lillicrap et al.    continuous control with
deep id23   .

in: arxiv preprint arxiv:1509.02971 (2015).

16v. mnih et al.    playing atari with deep id23   .
17ziyu wang, nando de freitas, and marc lanctot.    dueling network architectures for deep reinforcement

in: arxiv preprint arxiv:1312.5602 (2013).

learning   .
neural information processing systems. 2010, pp. 2613   2621.

in: arxiv preprint arxiv:1511.06581 (2015); hado v hasselt.    double id24   .

in: advances in

18x. guo et al.    deep learning for real-time atari game play using o   ine monte-carlo tree search planning   .

in:

advances in neural information processing systems. 2014, pp. 3338   3346; sergey levine et al.    end-to-end
training of deep visuomotor policies   .
   interactive control of diverse complex characters with neural networks   .
processing systems. 2015, pp. 3114   3122.

in: arxiv preprint arxiv:1504.00702 (2015); igor mordatch et al.

in: advances in neural information

open problems

what   s the right core model-free algorithm?

(cid:73) policy gradients (score function vs. reparameterization,

natural vs. not natural) vs. id24 vs. derivative-free
optimization vs others

(cid:73) desiderata
(cid:73) scalable
(cid:73) sample-e   cient
(cid:73) robust
(cid:73) learns from o   -policy data

exploration

(cid:73) exploration: actively encourage agent to reach unfamiliar

parts of state space, avoid getting stuck in local
maximum of performance

(cid:73) can solve    nite mdps in polynomial time with

exploration19

(cid:73) optimism about new states and actions
(cid:73) maintain distribution over possible models, and plan

with them (bayesian rl, thompson sampling)

(cid:73) how to do exploration in deep rl setting? thompson

sampling20, novelty bonus21

19alexander l strehl et al.    pac model-free id23   .

in: proceedings of the 23rd international

conference on machine learning. acm. 2006, pp. 881   888.

20ian osband et al.    deep exploration via bootstrapped id25   . . in: arxiv preprint arxiv:1602.04621 (2016).
21bradly c stadie, sergey levine, and pieter abbeel.    incentivizing exploration in id23 with

deep predictive models   .

in: arxiv preprint arxiv:1507.00814 (2015).

hierarchy

torque control: 100hz: 107 timesteps /daytask 1         task 2         task 3         task 4               10 timesteps / dayfootstep planning: 1hz: 105 timesteps / daywalk to x     fetch object y     say z     .01 hz: 103 time steps per daymore open problems

(cid:73) using learned models
(cid:73) learning from demonstrations

the end

questions?

