contents

1 singular value decomposition (svd)

2
3
1.1 singular vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2 singular value decomposition (svd) . . . . . . . . . . . . . . . . . . . . .
8
1.3 best rank k approximations
. . . . . . . . . . . . . . . . . . . . . . . . .
1.4 power method for computing the singular value decomposition . . . . . . 11
1.5 applications of singular value decomposition . . . . . . . . . . . . . . . . 16
1.5.1 principal component analysis . . . . . . . . . . . . . . . . . . . . . 16
1.5.2 id91 a mixture of spherical gaussians . . . . . . . . . . . . . 16
1.5.3 an application of svd to a discrete optimization problem . . . . 22
svd as a compression algorithm . . . . . . . . . . . . . . . . . . . 24
1.5.4
1.5.5
spectral decomposition . . . . . . . . . . . . . . . . . . . . . . . . 24
singular vectors and ranking documents . . . . . . . . . . . . . . . 25
1.5.6
1.6 bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
1.7 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

1

1 singular value decomposition (svd)

the singular value decomposition of a matrix a is the factorization of a into the
product of three matrices a = u dv t where the columns of u and v are orthonormal and
the matrix d is diagonal with positive real entries. the svd is useful in many tasks. here
we mention some examples. first, in many applications, the data matrix a is close to a
matrix of low rank and it is useful to    nd a low rank matrix which is a good approximation
to the data matrix . we will show that from the singular value decomposition of a, we
can get the matrix b of rank k which best approximates a; in fact we can do this for every
k. also, singular value decomposition is de   ned for all matrices (rectangular or square)
unlike the more commonly used spectral decomposition in id202. the reader
familiar with eigenvectors and eigenvalues (we do not assume familiarity here) will also
realize that we need conditions on the matrix to ensure orthogonality of eigenvectors. in
contrast, the columns of v in the singular value decomposition, called the right singular
vectors of a, always form an orthogonal set with no assumptions on a. the columns
of u are called the left singular vectors and they also form an orthogonal set. a simple
consequence of the orthogonality is that for a square and invertible matrix a, the inverse
of a is v d   1u t , as the reader can verify.
to gain insight into the svd, treat the rows of an n    d matrix a as n points in a
d-dimensional space and consider the problem of    nding the best k-dimensional subspace
with respect to the set of points. here best means minimize the sum of the squares of the
perpendicular distances of the points to the subspace. we begin with a special case of
the problem where the subspace is 1-dimensional, a line through the origin. we will see
later that the best-   tting k-dimensional subspace can be found by k applications of the
best    tting line algorithm. finding the best    tting line through the origin with respect
to a set of points {xi|1     i     n} in the plane means minimizing the sum of the squared
distances of the points to the line. here distance is measured perpendicular to the line.
the problem is called the best least squares    t.

in the best least squares    t, one is minimizing the distance to a subspace. an alter-
native problem is to    nd the function that best    ts some data. here one variable y is a
function of the variables x1, x2, . . . , xd and one wishes to minimize the vertical distance,
i.e., distance in the y direction, to the subspace of the xi rather than minimize the per-
pendicular distance to the subspace being    t to the data.

returning to the best least squares    t problem, consider projecting a point xi onto a

line through the origin. then

x2
i1 + x2

i2 +        +2

id = (length of projection)2 + (distance of point to line)2 .

see figure 1.1. thus

(distance of point to line)2 = x2

i1 + x2

i2 +        +2

id     (length of projection)2 .

to minimize the sum of the squares of the distances to the line, one could minimize

2

xj

  j

  i

  j

xi

  i

v

min(cid:80)   2
max(cid:80)   2

equivalent to

n(cid:80)

i=1

figure 1.1: the projection of the point xi onto the line through the origin in the direction
of v

n(cid:80)

(x2

i1 + x2

i2 +        +2

id) minus the sum of the squares of the lengths of the projections of

i2 +        +2

(x2

i1 + x2

the points to the line. however,

id) is a constant (independent of the
line), so minimizing the sum of the squares of the distances is equivalent to maximizing
the sum of the squares of the lengths of the projections onto the line. similarly for best-   t
subspaces, we could maximize the sum of the squared lengths of the projections onto the
subspace instead of minimizing the sum of squared distances to the subspace.

i=1

the reader may wonder why we minimize the sum of squared perpendicular distances
to the line. we could alternatively have de   ned the best-   t line to be the one which
minimizes the sum of perpendicular distances to the line. there are examples where
this de   nition gives a di   erent answer than the line minimizing the sum of perpendicular
distances squared. [the reader could construct such examples.] the choice of the objective
function as the sum of squared distances seems arbitrary and in a way it is. but the square
has many nice mathematical properties - the    rst of these is the use of pythagoras theorem
above to say that this is equivalent to maximizing the sum of squared projections. we will
see that in fact we can use the greedy algorithm to    nd best-   t k dimensional subspaces
(which we will de   ne soon) and for this too, the square is important. the reader should
also recall from calculus that the best-   t function is also de   ned in terms of least-squares
   t. there too, the existence of nice mathematical properties is the motivation for the
square.

1.1 singular vectors

we now de   ne the singular vectors of an n   d matrix a. consider the rows of a as n
points in a d-dimensional space. consider the best    t line through the origin. let v be a

3

unit vector along this line. the length of the projection of ai, the ith row of a, onto v is
|ai    v|. from this we see that the sum of length squared of the projections is |av|2. the
best    t line is the one maximizing |av|2 and hence minimizing the sum of the squared
distances of the points to the line.

with this in mind, de   ne the    rst singular vector, v1, of a, which is a column vector,
as the best    t line through the origin for the n points in d-space that are the rows of a.
thus

v1 = arg max
|v|=1

|av|.

the value   1 (a) = |av1| is called the    rst singular value of a. note that   2
sum of the squares of the projections of the points to the line determined by v1.

1 is the

the greedy approach to    nd the best    t 2-dimensional subspace for a matrix a, takes
v1 as the    rst basis vector for the 2-dimenional subspace and    nds the best 2-dimensional
subspace containing v1. the fact that we are using the sum of squared distances will again
help. for every 2-dimensional subspace containing v1, the sum of squared lengths of the
projections onto the subspace equals the sum of squared projections onto v1 plus the sum
of squared projections along a vector perpendicular to v1 in the subspace. thus, instead
of looking for the best 2-dimensional subspace containing v1, look for a unit vector, call
it v2, perpendicular to v1 that maximizes |av|2 among all such unit vectors. using the
same greedy strategy to    nd the best three and higher dimensional subspaces, de   nes
v3, v4, . . . in a similar manner. this is captured in the following de   nitions. there is no
apriori guarantee that the greedy algorithm gives the best    t. but, in fact, the greedy
algorithm does work and yields the best-   t subspaces of every dimension as we will show.

the second singular vector, v2, is de   ned by the best    t line perpendicular to v1

v2 = arg max
v   v1,|v|=1

|av| .

the value   2 (a) = |av2| is called the second singular value of a. the third singular
vector v3 is de   ned similarly by

v3 = arg max

v   v1,v2,|v|=1

|av|

and so on. the process stops when we have found

v1, v2, . . . , vr

as singular vectors and

if instead of    nding v1 that maximized |av| and then the best    t 2-dimensional
subspace containing v1, we had found the best    t 2-dimensional subspace, we might have

|av| = 0.

arg max
v   v1,v2,...,vr

|v|=1

4

done better. this is not the case. we now give a simple proof that the greedy algorithm
indeed    nds the best subspaces of every dimension.
theorem 1.1 let a be an n    d matrix where v1, v2, . . . , vr are the singular vectors
de   ned above. for 1     k     r, let vk be the subspace spanned by v1, v2, . . . , vk. then for
each k, vk is the best-   t k-dimensional subspace for a.

proof: the statement is obviously true for k = 1. for k = 2, let w be a best-   t 2-
dimensional subspace for a. for any basis w1, w2 of w , |aw1|2 + |aw2|2 is the sum of
squared lengths of the projections of the rows of a onto w . now, choose a basis w1, w2
of w so that w2 is perpendicular to v1. if v1 is perpendicular to w , any unit vector in w
will do as w2. if not, choose w2 to be the unit vector in w perpendicular to the projection
of v1 onto w. since v1 was chosen to maximize |av1|2, it follows that |aw1|2     |av1|2.
since v2 was chosen to maximize |av2|2 over all v perpendicular to v1, |aw2|2     |av2|2.
thus

|aw1|2 + |aw2|2     |av1|2 + |av2|2.

hence, v2 is at least as good as w and so is a best-   t 2-dimensional subspace.

for general k, proceed by induction. by the induction hypothesis, vk   1 is a best-   t
k-1 dimensional subspace. suppose w is a best-   t k-dimensional subspace. choose a
basis w1, w2, . . . , wk of w so that wk is perpendicular to v1, v2, . . . , vk   1. then
|aw1|2 + |aw2|2 +        + |awk|2     |av1|2 + |av2|2 +        + |avk   1|2 + |awk|2
since vk   1 is an optimal k -1 dimensional subspace. since wk is perpendicular to
v1, v2, . . . , vk   1, by the de   nition of vk, |awk|2     |avk|2. thus
|aw1|2 + |aw2|2 +        + |awk   1|2 + |awk|2     |av1|2 + |av2|2 +        + |avk   1|2 + |avk|2,

proving that vk is at least as good as w and hence is optimal.

note that the n-vector avi is really a list of lengths (with signs) of the projections of
the rows of a onto vi. think of |avi| =   i(a) as the    component    of the matrix a along
vi. for this interpretation to make sense, it should be true that adding up the squares of
the components of a along each of the vi gives the square of the    whole content of the
matrix a   . this is indeed the case and is the matrix analogy of decomposing a vector
into its components along orthogonal directions.

consider one row, say aj, of a. since v1, v2, . . . , vr span the space of all rows of a,
(aj    vi)2 =

aj    v = 0 for all v perpendicular to v1, v2, . . . , vr. thus, for each row aj,
|aj|2. summing over all rows j,

i=1

n(cid:88)

n(cid:88)

r(cid:88)

r(cid:88)

n(cid:88)

r(cid:88)

r(cid:88)

|aj|2 =

(aj    vi)2 =

(aj    vi)2 =

|avi|2 =

  2
i (a).

r(cid:80)

j=1

j=1

i=1

i=1

j=1

i=1

i=1

5

n(cid:80)

j=1

but

|aj|2 =

n(cid:80)

d(cid:80)

j=1

k=1

a2
jk, the sum of squares of all the entries of a. thus, the sum of

squares of the singular values of a is indeed the square of the    whole content of a   , i.e.,
the sum of squares of all the entries. there is an important norm associated with this
quantity, the frobenius norm of a, denoted ||a||f de   ned as

(cid:115)(cid:88)

j,k

||a||f =

a2
jk.

frobenius norm. that is,(cid:80)   2

i (a) = ||a||2
f .

lemma 1.2 for any matrix a, the sum of squares of the singular values equals the

proof: by the preceding discussion.

a matrix a can be described fully by how it transforms the vectors vi. every vector
v can be written as a linear combination of v1, v2, . . . , vr and a vector perpendicular
to all the vi. now, av is the same linear combination of av1, av2, . . . , avr as v is of
v1, v2, . . . , vr. so the av1, av2, . . . , avr form a fundamental set of vectors associated
with a. we normalize them to length one by

ui =

1

  i(a)

avi.

the vectors u1, u2, . . . , ur are called the left singular vectors of a. the vi are called the
right singular vectors. the svd theorem (theorem 1.5) will fully explain the reason for
these terms.

clearly, the right singular vectors are orthogonal by de   nition. we now show that the

left singular vectors are also orthogonal and that a =

  iuivt
i .

r(cid:80)

i=1

theorem 1.3 let a be a rank r matrix. the left singular vectors of a, u1, u2, . . . , ur,
are orthogonal.

proof: the proof is by induction on r. for r = 1, there is only one ui so the theorem is
trivially true. for the inductive part consider the matrix

b = a       1u1vt
1 .

the implied algorithm in the de   nition of singular value decomposition applied to b is
identical to a run of the algorithm on a for its second and later singular vectors and sin-
gular values. to see this,    rst observe that bv1 = av1       1u1vt
1 v1 = 0. it then follows
that the    rst right singular vector, call it z, of b will be perpendicular to v1 since if it
|z   z1| > |bz|, contradicting the arg max
|bz|
had a component z1 along v1, then,
de   nition of z. but for any v perpendicular to v1, bv = av. thus, the top singular

(cid:12)(cid:12)(cid:12)b z   z1

(cid:12)(cid:12)(cid:12) =

|z   z1|

6

vector of b is indeed a second singular vector of a. repeating this argument shows that
a run of the algorithm on b is the same as a run on a for its second and later singular
vectors. this is left as an exercise.

thus, there is a run of the algorithm that    nds that b has right singular vectors
v2, v3, . . . , vr and corresponding left singular vectors u2, u3, . . . , ur. by the induction
hypothesis, u2, u3, . . . , ur are orthogonal.

it remains to prove that u1 is orthogonal to the other ui. suppose not and for some
1 ui > 0. the proof is symmetric

1 ui (cid:54)= 0. without loss of generality assume that ut

i     2, ut
for the case where ut

1 ui < 0. now, for in   nitesimally small    > 0, the vector

a

(cid:19)

|v1 +   vi|

(cid:18) v1 +   vi
(cid:1)(cid:16)

1 ui

) =(cid:0)  1 +     iut

=

  1u1 +     iui

   

1 +   2

(cid:17)

1       2

2 + o (  4)

=   1 +     iut

1 ui     o(cid:0)  2(cid:1) >   1

has length at least as large as its component along u1 which is

ut
1 (

  1u1 +     iui

   

1 +   2

a contradiction. thus, u1, u2, . . . , ur are orthogonal.

1.2 singular value decomposition (svd)

let a be an n  d matrix with singular vectors v1, v2, . . . , vr and corresponding singular
avi, for i = 1, 2, . . . , r, are the left singular vectors and

values   1,   2, . . . ,   r. then ui = 1
  i
by theorem 1.5, a can be decomposed into a sum of rank one matrices as

r(cid:88)

a =

  iuivt
i .

i=1

we    rst prove a simple lemma stating that two matrices a and b are identical if
av = bv for all v. the lemma states that in the abstract, a matrix a can be viewed as
a transformation that maps vector v onto av.

lemma 1.4 matrices a and b are identical if and only if for all vectors v, av = bv.

proof: clearly, if a = b then av = bv for all v. for the converse, suppose that
av = bv for all v. let ei be the vector that is all zeros except for the ithcomponent
which has value 1. now aei is the ith column of a and thus a = b if for each i, aei = bei.

7

d
r    r

v t
r    d

a
n    d

u
n    r

=

figure 1.2: the svd decomposition of an n    d matrix.

theorem 1.5 let a be an n    d matrix with right singular vectors v1, v2, . . . , vr, left
singular vectors u1, u2, . . . , ur, and corresponding singular values   1,   2, . . . ,   r. then

a =

  iuivt
i .

r(cid:88)

i=1

r(cid:80)
r(cid:80)

i=1

proof: for each singular vector vj, avj =

i vj. since any vector v can be ex-
pressed as a linear combination of the singular vectors plus a vector perpendicular to the

i=1

  iuivt

vi, av =

  iuivt

i v and by lemma 1.4, a =

  iuivt
i .

r(cid:80)

i=1

the decomposition is called the singular value decomposition, svd, of a. in matrix
notation a = u dv t where the columns of u and v consist of the left and right singular
vectors, respectively, and d is a diagonal matrix whose diagonal entries are the singular
values of a.

for any matrix a, the sequence of singular values is unique and if the singular values
are all distinct, then the sequence of singular vectors is unique also. however, when some
set of singular values are equal, the corresponding singular vectors span some subspace.
any set of orthonormal vectors spanning this subspace can be used as the singular vectors.

1.3 best rank k approximations

there are two important matrix norms, the frobenius norm denoted ||a||f and the

2-norm denoted ||a||2. the 2-norm of the matrix a is given by

|av|

max
|v|=1

8

and thus equals the largest singular value of the matrix.

let a be an n    d matrix and think of the rows of a as n points in d-dimensional
space. the frobenius norm of a is the square root of the sum of the squared distance of
the points to the origin. the 2-norm is the square root of the sum of squared distances
to the origin along the direction that maximizes this quantity.

let

a =

  iuivt
i

be the svd of a. for k     {1, 2, . . . , r}, let

r(cid:88)

i=1

k(cid:88)

ak =

  iuivt
i

i=1

be the sum truncated after k terms. it is clear that ak has rank k. furthermore, ak is
the best rank k approximation to a when the error is measured in either the 2-norm or
the frobenius norm.

lemma 1.6 the rows of ak are the projections of the rows of a onto the subspace vk
spanned by the    rst k singular vectors of a.

proof: let a be an arbitrary row vector. since the vi are orthonormal, the projection

of the vector a onto vk is given by

t . thus, the matrix whose rows are the

projections of the rows of a onto vk is given by

avivt

i . this last expression simpli   es

k(cid:80)

i=1

(a    vi)vi

k(cid:80)

i=1

k(cid:88)

k(cid:88)

avivi

t =

  iuivi

t = ak.

to

i=1

i=1

the matrix ak is the best rank k approximation to a in both the frobenius and the
2-norm. first we show that the matrix ak is the best rank k approximation to a in the
frobenius norm.

theorem 1.7 for any matrix b of rank at most k

(cid:107)a     ak(cid:107)f     (cid:107)a     b(cid:107)f

proof: let b minimize (cid:107)a     b(cid:107)2
f among all rank k or less matrices. let v be the space
spanned by the rows of b. the dimension of v is at most k. since b minimizes (cid:107)a     b(cid:107)2
f ,
it must be that each row of b is the projection of the corresponding row of a onto v ,

9

otherwise replacing the row of b with the projection of the corresponding row of a onto
v does not change v and hence the rank of b but would reduce (cid:107)a     b(cid:107)2
f . since each
row of b is the projection of the corresponding row of a, it follows that (cid:107)a     b(cid:107)2
f is
the sum of squared distances of rows of a to v . since ak minimizes the sum of squared
distance of rows of a to any k-dimensional subspace, from theorem (1.1), it follows that
(cid:107)a     ak(cid:107)f     (cid:107)a     b(cid:107)f .

next we tackle the 2-norm. we    rst show that the square of the 2-norm of a     ak is

the square of the (k + 1)st singular value of a,
lemma 1.8 (cid:107)a     ak(cid:107)2

2 =   2

k+1.

r(cid:80)

proof: let a =

  iuivi
t and a     ak =

i=1

t be the singular value decomposition of a. then ak =
t . let v be the top singular vector of a     ak.

  iuivi

  iuivi

k(cid:80)

i=1

express v as a linear combination of v1, v2, . . . , vr. that is, write v =

  ivi. then

i=k+1

r(cid:80)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

r(cid:88)
r(cid:88)

i=k+1

i=k+1

r(cid:80)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

i=1

  i  iuivi

t vi

r(cid:88)

i=k+1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

t

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =
r(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) r(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =

  jvj

i=k+1

j=1

i   2
  2
i .

  iuivi

  i  iui

|(a     ak)v| =

=

the v maximizing this last quantity, subject to the constraint that |v|2 =
occurs when   k+1 = 1 and the rest of the   i are 0. thus, (cid:107)a     ak(cid:107)2
lemma.

2 =   2

r(cid:80)

i=1

  2

i = 1,

k+1 proving the

finally, we prove that ak is the best rank k 2-norm approximation to a.

theorem 1.9 let a be an n    d matrix. for any matrix b of rank at most k

(cid:107)a     ak(cid:107)2     (cid:107)a     b(cid:107)2

proof: if a is of rank k or less, the theorem is obviously true since (cid:107)a     ak(cid:107)2 = 0. thus
assume that a is of rank greater than k. by lemma 1.8, (cid:107)a     ak(cid:107)2
k+1. now suppose
there is some matrix b of rank at most k such that b is a better 2-norm approximation to
a than ak. that is, (cid:107)a     b(cid:107)2 <   k+1. the null space of b, null (b), (the set of vectors
v such that bv = 0) has dimension at least d     k. let v1, v2, . . . , vk+1 be the    rst k + 1
singular vectors of a. by a dimension argument, it follows that there exists a z (cid:54)= 0 in

2 =   2

null (b)     span{v1, v2, . . . , vk+1} .

10

and   1,   2, . . . ,   k not all zero so that (cid:80)d   k

[indeed, there are d     k independent vectors in null(b); say, u1, u2, . . . , ud   k are any
d     k independent vectors in null(b). now, u1, u2, . . . , ud   k, v1, v2, . . . vk+1 are d + 1
vectors in d space and they are dependent, so there are real numbers   1,   2, . . . ,   d   k
i=1   iui
and observe that z cannot be the zero vector.] scale z so that |z| = 1. we now show
that for this vector z, which lies in the space of the    rst k + 1 singular vectors of a,
that (a     b) z       k+1. hence the 2-norm of a     b is at least   k+1 contradicting the
assumption that (cid:107)a     b(cid:107)2 <   k+1. first
(cid:107)a     b(cid:107)2

j=1   jvj. take z = (cid:80)d   k

i=1   iui = (cid:80)k

2     |(a     b) z|2 .

since bz = 0,

since z is in the span{v1, v2, . . . , vk+1}

|az|2 =

  iuivi

t z

=

  2
i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

(cid:107)a     b(cid:107)2

n(cid:88)

(cid:0)vi

t z(cid:1)2

2     |az|2 .
k+1(cid:88)

(cid:0)vi

  2
i

=

i=1

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

(cid:107)a     b(cid:107)2

2       2

k+1

it follows that

t z(cid:1)2       2

k+1

(cid:0)vi

t z(cid:1)2

k+1(cid:88)

i=1

=   2

k+1.

contradicting the assumption that ||a     b||2 <   k+1. this proves the theorem.

1.4 power method for computing the singular value decom-

position

computing the singular value decomposition is an important branch of numerical
analysis in which there have been many sophisticated developments over a long period of
time. here we present an    in-principle    method to establish that the approximate svd
of a matrix a can be computed in polynomial time. the reader is referred to numerical
analysis texts for more details. the method we present, called the power method, is
simple and is in fact the conceptual starting point for many algorithms. it is easiest to
describe    rst in the case when a is square symmetric and has the same right and left
singular vectors, namely,

in this case, we have

(cid:32) r(cid:88)

(cid:33)(cid:32) r(cid:88)

a2 =

  ivivi

t

  jvjvj

t

=

i=1

j=1

r(cid:88)

i,j=1

  i  jvivi

t vjvj

t =

r(cid:88)

i=1

  2
i vivi

t ,

where,    rst we just multiplied the two sums and then observed that if i (cid:54)= j, the dot
t is a matrix
product vi

t vj equals 0 by orthogonality. [caution: the    outer product    vivj

11

a =

  ivivi

t .

r(cid:88)
(cid:33)

i=1

and is not zero even for i (cid:54)= j.] similarly, if we take the k th power of a, again all the
cross terms are zero and we will get

r(cid:88)

ak =

  k
i vivi

t .

if we had   1 >   2, we would have

i=1

ak     v1v1

t .

1
  k
1

now we do not know   1 beforehand and cannot    nd this limit, but if we just take ak
and divide by ||ak||f so that the frobenius norm is normalized to 1 now, that matrix
will converge to the rank 1 matrix v1v1
[this is
still an intuitive description, which we will make precise shortly. first, we cannot make
the assumption that a is square and has the same right and left singular vectors. but,
i , then by

b = aat satis   es both these conditions. if again, the svd of a is (cid:80)

t from which v1 may be computed.

  iuivt

direct multiplication

i

(cid:33)

b = aat =

  iuivt
i

  jvjut
j

(cid:32)(cid:88)

(cid:88)
(cid:88)

i,j

i

=

=

i
  i  juivt
i vjut

j =

i uiut
  2
i ,

(cid:33)(cid:32)(cid:88)
(cid:88)

i,j

j
  i  jui(vt

i    vj)ut

j

since vt
spectral decomposition of b. using the same kind of calculation as above,

i vj is the dot product of the two vectors and is zero unless i = j. this is the

(cid:88)

bk =

i uiut
  2k
i .

i

as k increases, for i > 1,   2k

i /  2k

1 goes to zero and bk is approximately equal to

provided that for each i > 1,   i (a) <   1 (a).

  2k
1 u1ut
1

this suggests a way of    nding   1 and u1, by successively powering b. but there are
two issues. first, if there is a signi   cant gap between the    rst and second singular values
of a matrix, then the above argument applies and the power method will quickly converge
to the    rst left singular vector. suppose there is no signi   cant gap. in the extreme case,
there may be ties for the top singular value. then the above argument does not work.
there are cumbersome ways of overcoming this by assuming a    gap    between   1 and   2;

12

such proofs do have the advantage that with a greater gap, better results can be proved,
but at the cost of some mess. here, instead, we will adopt a clean solution in theorem
1.11 below which states that even with ties, the power method converges to some vector
in the span of those singular vectors corresponding to the    nearly highest    singular values.

a second issue is that computing bk costs k id127s when done in
instead we

a straight-forward manner or o (log k) when done by successive squaring.
compute

bkx

where x is a random unit length vector, the idea being that the component of x in the
direction of u1 would get multiplied by   2
1 each time, while the component of x along
other ui would be multiplied only by   2
i . of course, if the component of x along u1 is zero
to start with, this would not help at all - it would always remain 0. but, this problem is
   xed by picking x to be random as we show in lemma (1.10).

each increase in k requires multiplying b by the vector bk   1x, which we can further

break up into

bkx = a(cid:0)at(cid:0)bk   1x(cid:1)(cid:1) .

this requires two matrix-vector products, involving the matrices at and a.
in many
applications, data matrices are sparse - many entries are zero. [a leading example is the
matrix of hypertext links in the web. there are more than 1010 web pages and the matrix
would be 1010by 1010. but on the average only about 10 entries per row are non-zero;
so only about 1011 of the possible 1020 entries are non-zero.] sparse matrices are often
represented by giving just a linked list of the non-zero entries and their values.
if a
is represented in this sparse manner, then the reader can convince him/herself that we
can a matrix vector product in time proportional to the number of nonzero entries in a.
1    x) is a scalar multiple of u1, u1 can be recovered from bkx by
since bkx       2k
id172.

1 u1(ut

we start with a technical lemma needed in the proof of the theorem.

lemma 1.10 let (x1, x2, . . . , xd) be a unit d-dimensional vector picked at random from
the set {x : |x|     1}. the id203 that |x1|     1
   
proof: we    rst show that for a vector v picked at random with |v|     1, the id203
that v1     1
is at least 9/10. then we let x = v/|v|. this can only increase the value
   
of v1, so the result follows.

is at least 9/10.

20

20

d

d

   
let    = 1
20

. the id203 that |v1|        equals one minus the id203 that
|v1|       . the id203 that |v1|        is equal to the fraction of the volume of the unit
sphere with |v1|       . to get an upper bound on the volume of the sphere with |v1|       ,
consider twice the volume of the unit radius cylinder of height   . the volume of the

d

13

   
1
20

d

figure 1.3: the volume of the cylinder of height
   
of the hemisphere below x1 = 1
20

d

   
1
20

d

is an upper bound on the volume

portion of the sphere with |v1|        is less than or equal to 2  v (d     1) (recall notation
from chapter 2) and

now the volume of the unit radius sphere is at least twice the volume of the cylinder of
height

1     1

and radius

1   

d   1 or

d   1

prob(|v1|       )     2  v (d     1)

v (d)

(cid:113)

v (d)    

2   
d     1

v (d     1)(1     1
d     1

d   1
2

)

using (1     x)a     1     ax

v (d)    

2   
d     1

v (d     1)(1     d     1

2

)     v (d     1)
d     1

   

1
d     1
   

and

prob(|v1|       )     2  v (d     1)
v (d     1)

1   

   

d   1

d     1
   
10
d

    1
10

.

thus the id203 that v1     1
   
theorem 1.11 let a be an n    d matrix and x a random unit length vector. let v be
the space spanned by the left singular vectors of a corresponding to singular values greater

is at least 9/10.

20

d

14

than (1       )   1. let k be    
method, namely,

(cid:16) ln(d/  )

(cid:17)

  

. let w be unit vector after k iterations of the power

w =

(cid:0)aat(cid:1)k x
(cid:12)(cid:12)(cid:12)(aat )k x
(cid:12)(cid:12)(cid:12).
r(cid:88)

  iuivt
i

i=1

n(cid:88)

i=1

x =

ciui.

the id203 that w has a component of at least   perpendicular to v is at most 1/10.

proof: let

a =

be the svd of a. if the rank of a is less than d, then complete {u1, u2, . . . ur} into a
basis {u1, u2, . . . ud} of d-space. write x in the basis of the ui

(cid:48)s as

d(cid:80)

d(cid:80)

  2k
i uiut

since (aat )k =

i , it follows that (aat )kx =

  2k
i ciui. for a random unit
length vector x picked independent of a, the ui are    xed vectors and picking x at random
is equivalent to picking random ci. from lemma 1.10, |c1|     1
   
with id203 at
least 9/10.

i=1

i=1

20

d

suppose that   1,   2, . . . ,   m are the singular values of a that are greater than or equal
to (1       )   1 and that   m+1, . . . ,   n are the singular values that are less than (1       )   1.
now

|(aat )kx|2 =

  2k
i ciui

=

i c2
  4k

i       4k
1 c2

1     1
400d

  4k
1 ,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

n(cid:88)

i=1

with id203 at least 9/10. here we used the fact that a sum of positive quantities is
at least as large as its    rst element and the    rst element is greater than or equal to 1
400d   4k
1
with id203 at least 9/10.
[note: if we did not choose x at random in the begin-
ning and use lemma 1.10, c1 could well have been zero and this argument would not work.]

the component of |(aat )kx|2 perpendicular to the space v is

d(cid:88)

d(cid:88)

  4k
i c2

i     (1       )4k   4k

1

i     (1       )4k   4k
c2
1 ,

i=m+1

i=m+1

i = |x| = 1. thus, the component of w perpendicular to v is at most

(cid:16)   

de      (ln(d/  ))(cid:17)

= o(  )

   
= o(

   
d(1       )2k) = o(

de   2  k) = o

since(cid:80)n

i=1 c2
(1       )2k  2k

1

   
1
20

d

  2k
1

as desired.

15

1.5 applications of singular value decomposition

1.5.1 principal component analysis

the traditional use of svd is in principal component analysis (pca). pca is il-
lustrated by an example - customer-product data where there are n customers buying d
products. let matrix a with elements aij represent the id203 of customer i purchas-
ing product j (or the amount or utility of product j to customer i). one hypothesizes
that there are really only k underlying basic factors like age, income, family size, etc. that
determine a customer   s purchase behavior. an individual customer   s behavior is deter-
mined by some weighted combination of these underlying factors. that is, a customer   s
purchase behavior can be characterized by a k-dimensional vector where k is much smaller
that n and d. the components of the vector are weights for each of the basic factors.
associated with each basic factor is a vector of probabilities, each component of which is
the id203 of purchasing a given product by someone whose behavior depends only
on that factor. more abstractly, a is an n   d matrix that can be expressed as the product
of two matrices u and v where u is an n    k matrix expressing the factor weights for
each customer and v is a k    d matrix expressing the purchase probabilities of products
that correspond to that factor. one twist is that a may not be exactly equal to u v , but
close to it since there may be noise or random perturbations.

taking the best rank k approximation ak from svd (as described above) gives us
such a u, v . in this traditional setting, one assumed that a was available fully and we
wished to    nd u, v to identify the basic factors or in some applications to    denoise    a (if
we think of a     u v as noise). now imagine that n and d are very large, on the order of
thousands or even millions, there is probably little one could do to estimate or even store
a. in this setting, we may assume that we are given just given a few elements of a and
wish to estimate a. if a was an arbitrary matrix of size n    d, this would require    (nd)
pieces of information and cannot be done with a few entries. but again hypothesize that
a was a small rank matrix with added noise. if now we also assume that the given entries
are randomly drawn according to some known distribution, then there is a possibility that
svd can be used to estimate the whole of a. this area is called collaborative    ltering
and one of its uses is to target an ad to a customer based on one or two purchases. we
will not be able to describe it here.

1.5.2 id91 a mixture of spherical gaussians

in id91, we are given a set of points in d   space and the task is to partition the
points into k subsets (clusters) where each cluster consists of    nearby    points. di   erent
de   nitions of the goodness of a id91 lead to di   erent solutions. id91 is an
important area which we will study in detail in chapter ??. here we will see how to solve
a particular id91 problem using singular value decomposition.

in general, a solution to any id91 problem comes up with k cluster centers

16

factors

u

                                          

                                          

      

      

products

v

customers

figure 1.4: customer-product data

which de   ne the k clusters - a cluster is the set of data points which have a particular
cluster center as the closest cluster center. hence the vornoi cells of the cluster centers
determine the clusters. using this observation, it is relatively easy to cluster points in
two or three dimensions. however, id91 is not so easy in higher dimensions. many
problems have high-dimensional data and id91 problems are no exception. clus-
tering problems tend to be np-hard, so we do not have polynomial time algorithms to
solve them. one way around this is to assume stochastic models of input data and devise
algorithms to cluster under such models.

mixture models are a very important class of stochastic models. a mixture is a
id203 density or distribution that is the weighted sum of simple component prob-
ability densities. it is of the form w1p1 + w2p2 +        + wkpk where p1, p2, . . . , pk are the
basic densities and w1, w2, . . . , wk are positive real numbers called weights that add up to
one. clearly, w1p1 + w2p2 +        + wkpk is a id203 density, it integrates to one.

put in picture of a 1-dimensional gaussian mixture
the model    tting problem is to    t a mixture of k basic densities to n samples, each
sample drawn according to the same mixture distribution. the class of basic densities is
known, but the component weights of the mixture are not. here, we deal with the case
where the basic densities are all spherical gaussians. the samples are generated by pick-
ing an integer i from the set {1, 2, . . . , k} with probabilities w1, w2, . . . , wk, respectively.
then, picking a sample according to pi and repeating the process n times. this process
generates n samples according to the mixture where the set of samples is naturally par-
titioned into k sets, each set corresponding to one pi.

the model-   tting problem consists of two sub problems. the    rst sub problem is to
cluster the sample into k subsets where each subset was picked according to one compo-
nent density. the second sub problem is to    t a distribution to each subset. we discuss

17

only the id91 problem here. the problem of    tting a single gaussian to a set of
data points is a lot easier and was discussed in section ?? of chapter 2.

if the component gaussians in the mixture have their centers very close together, then
the id91 problem is unresolvable. in the limiting case where a pair of component
densities are the same, there is no way to distinguish between them. what condition on
the inter-center separation will guarantee unambiguous id91? first, by looking at
1-dimensional examples, it is clear that this separation should be measured in units of the
standard deviation, since the density is a function of the number of standard deviation
from the mean. in one dimension, if two gaussians have inter-center separation at least
ten times the maximum of their standard deviations, then they hardly overlap. what is
the analog of this statement in higher dimensions?

   

for a d-dimensional spherical gaussian, with standard deviation    in each direction1,
it is easy to see that the expected distance squared from the center is d  2. de   ne the
radius r of the gaussian to be the square root of the average distance squared from
   
d  . if the inter-center separation between two spherical gaussians,
the center; so r is
both of radius r is at least 2r = 2
d  , then it is easy to see that the densities hardly
overlap. but this separation requirement grows with d. for problems with large d, this
would impose a separation requirement not met in practice. the main aim is to answer
a   rmatively the question:

can we show an analog of the 1-dimensional statement for large d : in a mixture
of k spherical gaussians in d space, if the centers of each pair of component gaussians
are    (1) standard deviations apart, then we can separate the sample points into the k
components (for k     o(1)).

the central idea is the following: suppose for a moment, we can (magically)    nd the
subspace spanned by the k centers. imagine projecting the sample points to this subspace.
it is easy to see (see lemma (1.12) below) that the projection of a spherical gaussian with
standard deviation    is still a spherical gaussian with standard deviation   . but in the
projection, now, the inter-center separation still remains the same. so in the projection,
   
   
the gaussians are distinct provided the inter-center separation (in the whole space) is
k  ) which is a lot smaller than the    (
d  ) for k << d. interestingly we will see that
   (
the subspace spanned by the k    centers is essentially the best-   t k dimensional subspace
which we can    nd by singular value decomposition.

lemma 1.12 suppose p is a d-dimensional spherical gaussian with center    and stan-
dard deviation   . the density of p projected onto an arbitrary k-dimensional subspace v
is a spherical gaussian with the same standard deviation.

proof: since p is spherical, the projection is independent of the k-dimensional subspace.
pick v to be the subspace spanned by the    rst k coordinate vectors. for a point x =

1since a spherical gaussian has the same standard deviation in every direction, we call it the standard

deviation of the gaussian.

18

1. best    t 1-dimension subspace to
the line

a spherical gaussian is
through its center and the origin.

2. any k-dimensional subspace contain-
ing the line is a best    t k-dimensional
subspace for the gaussian.

3. the best    t k-dimensional subspace
for k gaussians is the subspace con-
taining their centers.

1. best    t 1-dimension subspace to
the line

a spherical gaussian is
through its center and the origin.

2. any k-dimensional subspace contain-
ing the line is a best    t k-dimensional
subspace for the gaussian.

3. the best    t k-dimensional subspace
for k gaussians is the subspace con-
taining their centers.

figure 1.5: best    t subspace to a spherical gaussian.

(x1, x2, . . . , xd), we will use the notation x(cid:48) = (x1, x2, . . . xk) and x(cid:48)(cid:48) = (xk+1, xk+2, . . . , xn).
the density of the projected gaussian at the point (x1, x2, . . . , xk) is

(cid:90)

   |x(cid:48)     (cid:48)|2

2  2

ce

   |x(cid:48)(cid:48)     (cid:48)(cid:48)|2
e

2  2

dx(cid:48)(cid:48) = c(cid:48)e

   |x(cid:48)     (cid:48)|2

2  2

.

this clearly implies the lemma.

x(cid:48)(cid:48)

we now show that the top k singular vectors produced by the svd span the space of
the k centers. first, we extend the notion of best    t to id203 distributions. then
we show that for a single spherical gaussian (whose center is not the origin), the best    t
1-dimensional subspace is the line though the center of the gaussian and the origin. next,
we show that the best    t k-dimensional subspace for a single gaussian (whose center is

19

not the origin) is any k-dimensional subspace containing the line through the gaussian   s
center and the origin. finally, for k spherical gaussians, the best    t k-dimensional sub-
space is the subspace containing their centers. thus, the svd    nds the subspace that
contains the centers.

recall that for a set of points, the best-   t line is the line passing through the origin
which minimizes the sum of squared distances to the points. we extend this de   nition to
id203 densities instead of a set of points.

de   nition 4.1: if p is a id203 density in d space, the best    t line for p is the
line l passing through the origin that minimizes the expected squared (perpendicular)
distance to the line, namely,

(cid:82) dist (x, l)2 p (x) dx.

recall that a k-dimensional subspace is the best-   t subspace if the sum of squared
distances to it is minimized or equivalently, the sum of squared lengths of projections onto
it is maximized. this was de   ned for a set of points, but again it can be extended to a
density as above.

de   nition 4.2: if p is a id203 density in d-space and v is a subspace, then the
expected squared perpendicular distance of v to p, denoted f (v, p), is given by

(cid:90)

f (v, p) =

dist2 (x, v ) p (x) dx,

where dist(x, v ) denotes the perpendicular distance from the point x to the subspace v .

for the uniform density on the unit circle centered at the origin, it is easy to see that

any line passing through the origin is a best    t line for the id203 distribution.
lemma 1.13 let the id203 density p be a spherical gaussian with center    (cid:54)= 0.
the best    t 1-dimensional subspace is the line passing through    and the origin.

proof: for a randomly chosen x (according to p) and a    xed unit length vector v,

e(cid:2)(vt x)2(cid:3) = e

= e

(cid:104)(cid:0)vt (x       ) + vt   (cid:1)2(cid:105)
(cid:104)(cid:0)vt (x       )(cid:1)2
(cid:104)(cid:0)vt (x       )(cid:1)2(cid:105)
(cid:104)(cid:0)vt (x       )(cid:1)2(cid:105)
=   2 +(cid:0)vt   (cid:1)2

= e

= e

+ 2(cid:0)vt   (cid:1)(cid:0)vt (x       )(cid:1) +(cid:0)vt   (cid:1)2(cid:105)
+ 2(cid:0)vt   (cid:1) e(cid:2)vt (x       )(cid:3) +(cid:0)vt   (cid:1)2
+(cid:0)vt   (cid:1)2

20

(cid:104)(cid:0)vt (x       )(cid:1)2(cid:105)

is the variance in the direction v and e(cid:0)vt (x       )(cid:1) = 0. the
lemma follows from the fact that the best    t line v is the one that maximizes (cid:0)vt u(cid:1)2

since e

which is maximized when v is aligned with the center   .

lemma 1.14 for a spherical gaussian with center   , a k-dimensional subspace is a best
   t subspace if and only if it contains   .

proof: by symmetry, every k-dimensional subspace through    has the same sum of
distances squared to the density. now by the svd procedure, we know that the best-
   t k-dimensional subspace contains the best    t line, i.e., contains   . thus, the lemma
follows.

this immediately leads to the following theorem.

theorem 1.15 if p is a mixture of k spherical gaussians whose centers span a k-
dimensional subspace, then the best    t k-dimensional subspace is the one containing the
centers.
proof: let p be the mixture w1p1+w2p2+      +wkpk. let v be any subspace of dimension
k or less. then, the expected squared perpendicular distance of v to p is

f (v, p) =

dist2(x, v )p(x)dx

wi

dist2(x, v )pi(x)dx

(cid:90)

(cid:90)
k(cid:88)
    k(cid:88)

i=1

=

wi( distance squared of pi to its best    t k-dimensional subspace).

i=1

choose v to be the space spanned by the centers of the densities pi. by lemma ?? the
last inequality becomes an equality proving the theorem.

for an in   nite set of points drawn according to the mixture, the k-dimensional svd
subspace gives exactly the space of the centers. in reality, we have only a large number
of samples drawn according to the mixture. however, it is intuitively clear that as the
number of samples increases, the set of sample points approximates the id203 density
and so the svd subspace of the sample is close to the space spanned by the centers. the
details of how close it gets as a function of the number of samples are technical and we
do not carry this out here.

21

1.5.3 an application of svd to a discrete optimization problem

in the last example, svd was used as a dimension reduction technique.

it found
a k-dimensional subspace (the space of centers) of a d-dimensional space and made the
gaussian id91 problem easier by projecting the data to the subspace. here, instead
of    tting a model to data, we have an optimization problem. again applying dimension
reduction to the data makes the problem easier. the use of svd to solve discrete op-
timization problems is a relatively new subject with many applications. we start with
an important np-hard problem, the maximum cut problem for a directed graph g(v, e).

the maximum cut problem is to partition the node set v of a directed graph into
two subsets s and   s so that the number of edges from s to   s is maximized. let a be
the adjacency matrix of the graph. with each vertex i, associate an indicator variable xi.
the variable xi will be set to 1 for i     s and 0 for i       s. the vector x = (x1, x2, . . . , xn)
is unknown and we are trying to    nd it (or equivalently the cut), so as to maximize the
number of edges across the cut. the number of edges across the cut is precisely

(cid:88)

i,j

xi(1     xj)aij.

thus, the maximum cut problem can be posed as the optimization problem

maximize(cid:80)
(cid:88)

i,j

xi(1     xj)aij

subject to xi     {0, 1}.

xi(1     xj)aij = xt a(1     x),

in matrix notation,

k(cid:80)

where 1 denotes the vector of all 1   s . so, the problem can be restated as

i,j

maximize xt a(1     x)

subject to xi     {0, 1}.

(1.1)

the svd is used to solve this problem approximately by computing the svd of a and

replacing a by ak =

  iuivi

t in (1.1) to get

i=1

maximize xt ak(1     x)

subject to xi     {0, 1}.

(1.2)

note that the matrix ak is no longer a 0-1 adjacency matrix.

we will show that:
1. for each 0-1 vector x, xt ak(1     x) and xt a(1     x) di   er by at most

the maxima in (1.1) and (1.2) di   er by at most this amount.

n2   

k+1

. thus,

2. a near optimal x for (1.2) can be found by exploiting the low rank of ak, which by
item 1 is near optimal for (1.1) where near optimal means with additive error of at
most

n2   

.

k+1

22

first, we prove item 1. since x and 1     x are 0-1 n-vectors, each has length at most
   
n. by the de   nition of the 2-norm, |(a     ak)(1     x)|        
n||a     ak||2. now since
xt (a     ak)(1     x) is the dot product of the vector x with the vector (a     ak)(1     x),
|xt (a     ak)(1     x)|     n||a     ak||2.

by lemma 1.8, ||a     ak||2 =   k+1(a). the inequalities,
k+1     ||a||2

2 +          2

k+1       2

(k + 1)  2

1 +   2

f =

(cid:88)

ij     n2
a2

imply that   2

k+1     n2

k+1 and hence ||a     ak||2     n   

k+1

i,j

proving item 1.

for a subset s of {1, 2, . . . , n}, de   ne the 2k-dimensional vector w(s) = (u1(s), v1(   s), u2(s), v2(   s), . . . , uk(s), vk(   s)).

j   s

i=1   i(xt ui)(vi

i=1   iui(s)vi(   s) using id145.

next we focus on item 2. it is instructive to look at the special case when k=1 and a
is approximated by the rank one matrix a1. an even more special case when the left and
right singular vectors u and v are required to be identical is already np-hard to solve ex-
actly because it subsumes the problem of whether for a set of n integers, {a1, a2, . . . , an},
there is a partition into two subsets whose sums are equal. so, we look for algorithms
that solve the maximum cut problem approximately.

t (1     x)) over 0-1 vectors x. a
piece of notation will be useful. for any s     {1, 2, . . . n}, write ui(s) for the sum of
coordinates of the vector ui corresponding to elements in the set s and also for vi. that

for item 2, we want to maximize (cid:80)k
uij. we will maximize(cid:80)k
is, ui(s) = (cid:80)
if we had the list of all such vectors, we could    nd (cid:80)k

i=1   iui(s)vi(   s) for each of them
and take the maximum. there are 2n subsets s, but several s could have the same
w(s) and in that case it su   ces to list just one of them. round each coordinate of
1
nk2 . call the rounded vector   ui. similarly ob-
each ui to the nearest integer multiple of
tain   vi. let   w(s) denote the vector (  u1(s),   v1(   s),   u2(s),   v2(   s), . . . ,   uk(s),   vk(   s)). we
will construct a list of all possible values of the vector   w(s).
[again, if several di   er-
ent s    s lead to the same vector   w(s), we will keep only one copy on the list.] the
list will be constructed by id145. for the recursive step of dynamic
programming, assume we already have a list of all such vectors for s     {1, 2, . . . , i}
and wish to construct the list for s     {1, 2, . . . , i + 1}. each s     {1, 2, . . . , i} leads to
two possible s(cid:48)     {1, 2, . . . , i + 1}, namely, s and s     {i + 1}.
in the    rst case, the
vector   w(s(cid:48)) = (  u1(s) +   u1,i+1,   v1(s),   u2(s) +   u2,i+1,   v2(s), . . . , ...). in the second case,
  w(s(cid:48)) = (  u1(s),   v1(s) +   v1,i+1,   u2(s),   v2(s) +   v2,i+1, . . . , ...). we put in these two vectors
for each vector in the previous list. then, crucially, we prune - i.e., eliminate duplicates.
since ui, vi are unit length vectors, |ui(s)|,|vi(s)|        
as claimed.
nk2 = 1
k2
and similarly for vi. to bound the error, we use an elementary fact: if a, b are reals with
|a|,|b|     m and we estimate a by a(cid:48) and b by b(cid:48) so that |a     a(cid:48)|,|b     b(cid:48)|            m , then

assume that k is constant. now, we show that the error is at most

n. also |  ui(s)    ui(s)|     n

n2   

k+1

|ab     a(cid:48)b(cid:48)| = |a(b     b(cid:48)) + b(cid:48)(a     a(cid:48))|     |a||b     b(cid:48)| + (|b| + |b     b(cid:48)|)|a     a(cid:48)|     3m   .

23

using this, we get that

  i  ui(s)  vi(   s)    

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)     3k  1

  iui(s)vi(s)

k(cid:88)

i=1

   
n/k2     3n3/2/k,

and this meets the claimed error bound.

   
next, we show that the running time is polynomially bounded. |  ui(s)|,|  vi(s)|     2
   
n.
since   ui(s),   vi(s) are all integer multiples of 1/(nk2), there are at most 2/
nk2 possible
   
values of   ui(s),   vi(s) from which it follows that the list of   w(s) never gets larger than
(1/

nk2)2k which for    xed k is polynomially bounded.

we summarize what we have accomplished.

theorem 1.16 given a directed graph g(v, e), a cut of size at least the maximum cut
minus o

can be computed in polynomial time n for any    xed k.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) k(cid:88)

i=1

(cid:17)

(cid:16) n2   

k

it would be quite a surprise to have an algorithm that actually achieves the same
accuracy in time polynomial in n and k because this would give an exact max cut in
polynomial time.

1.5.4 svd as a compression algorithm

suppose a is the pixel intensity matrix of a large image. the entry aij gives the
intensity of the ijth pixel. if a is n   n, the transmission of a requires transmitting o(n2)
real numbers. instead, one could send ak, that is, the top k singular values   1,   2, . . . ,   k
along with the left and right singular vectors u1, u2, . . . , uk, and v1, v2, . . . , vk. this
would require sending o(kn) real numbers instead of o(n2) real numbers. if k is much
smaller than n, this results in savings. for many images, a k much smaller than n can be
used to reconstruct the image provided that a very low resolution version of the image is
su   cient. thus, one could use svd as a compression method.

it turns out that in a more sophisticated approach, for certain classes of pictures one
could use a    xed basis so that the top (say) hundred singular vectors are su   cient to
represent any picture approximately. this means that the space spanned by the top
hundred singular vectors is not too di   erent from the space spanned by the top two
hundred singular vectors of a given matrix in that class. compressing these matrices by
this standard basis can save substantially since the standard basis is transmitted only
once and a matrix is transmitted by sending the top several hundred singular values for
the standard basis.

1.5.5 spectral decomposition

let b be a square matrix.

if the vector x and scalar    are such that bx =   x,
then x is an eigenvector of the matrix b and    is the corresponding eigenvalue. we
present here a spectral decomposition theorem for the special case where b is of the form

24

b = aat for some (possibly rectangular) matrix a. if a is a real valued matrix, then
b is symmetric and positive de   nite. that is, xt bx > 0 for all nonzero vectors x. the
spectral decomposition theorem holds more generally and the interested reader should
consult a id202 book.

theorem 1.17 (spectral decomposition) if b = aat then b = (cid:80)
a =(cid:80)

is the singular valued decomposition of a.

  2
i uiut

i where

i

  iuivt
i

i

proof:

(cid:33)(cid:32)(cid:88)

t

j

(cid:33)t

  jujvt
j

  i  juivi

t vjuj

t

(cid:32)(cid:88)
(cid:88)
(cid:88)
(cid:88)

i

i

  iuivi

j
  2
i uiui

t .

b = aat =

=

=

i

when the   i are all distinct, the ui are the eigenvectors of b and the   2

i are the
if the   i are not distinct, then any vector that is a linear

corresponding eigenvalues.
combination of those ui with the same eigenvalue is an eigenvector of b.

1.5.6 singular vectors and ranking documents

an important task for a document collection is to rank the documents. recall the
term-document vector representation from chapter 2. a n  aive method would be to rank
in order of the total length of the document (which is the sum of the components of its
term-document vector). clearly, this is not a good measure in many cases. this n  aive
method attaches equal weight to each term and takes the projection (dot product) term-
document vector in the direction of the all 1 vector. is there a better weighting of terms,
i.e., a better projection direction which would measure the intrinsic relevance of the doc-
ument to the collection? a good candidate is the best-   t direction for the collection of
term-document vectors, namely the top (left) singular vector of the term-document ma-
trix. an intuitive reason for this is that this direction has the maximum sum of squared
projections of the collection and so can be thought of as a synthetic term-document vector
best representing the document collection.

ranking in order of the projection of each document   s term vector along the best
   t direction has a nice interpretation in terms of the power method. for this, we con-
sider a di   erent example - that of web with hypertext links. the world wide web can
be represented by a directed graph whose nodes correspond to web pages and directed
edges to hypertext links between pages. some web pages, called authorities, are the most

25

prominent sources for information on a given topic. other pages called hubs, are ones
that identify the authorities on a topic. authority pages are pointed to by many hub
pages and hub pages point to many authorities. one is led to what seems like a circular
de   nition: a hub is a page that points to many authorities and an authority is a page
that is pointed to by many hubs.

one would like to assign hub weights and authority weights to each node of the web.
if there are n nodes, the hub weights form a n-dimensional vector u and the authority
weights form a n-dimensional vector v. suppose a is the adjacency matrix representing
the directed graph : aij is 1 if there is a hypertext link from page i to page j and 0
otherwise. given hub vector u, the authority vector v could be computed by the formula

d(cid:88)

vj =

uiaij

i=1

since the right hand side is the sum of the hub weights of all the nodes that point to node
j. in matrix terms,

v = at u.

similarly, given an authority vector v, the hub vector u could be computed by u = av.
of course, at the start, we have neither vector. but the above suggests a power iteration.
start with any v. set u = av; then set v = at u and repeat the process. we know
from the power method that this converges to the left and right singular vectors. so
after su   ciently many iterations, we may use the left vector u as hub weights vector and
project each column of a onto this direction and rank columns (authorities) in order of
this projection. but the projections just form the vector at u which equals v. so we can
just rank by order of vj.

this is the basis of an algorithm called the hits algorithm which was one of the early

proposals for ranking web pages.

a di   erent ranking called page rank is widely used. it is based on a random walk on
the grap described above. (we will study id93 in detail in chapter 5 and the
reader may postpone reading this application until then.)

a random walk on the web goes from web page i to a randomly chosen neighbor of it.
so if pij is the id203 of going from i to j, then pij is just 1/ (number of hypertext links
from i). represent the pij in a matrix p . this matrix is called the transition id203
matrix of the random walk. represent the probabilities of being in each state at time t
by the components of a row vector p (t). the id203 of being in state j at time t is
given by the equation

(cid:88)

pj (t) =

pi (t     1) pij.

then

i

p (t) = p (t     1) p

26

and thus

p (t) = p (0) p t.

the id203 vector p (t) is computed by computing p to the power t. it turns out that
under some conditions, the random walk has a steady state id203 vector that we can
think of as p (   ). it has turned out to be very useful to rank pages in decreasing order
of pj (   ) in essence saying that the web pages with the highest steady state probabilities
are the most important.

in the above explanation, the random walk goes from page i to one of the web pages
pointed to by i, picked uniformly at random. modern technics for ranking pages are more
complex. a more sophisticated random walk is used for several reasons. first, a web
page might not contain any links and thus there is nowhere for the walk to go. second,
a page that has no in links will never be reached. even if every node had at least one
in link and one out link, the graph might not be strongly connected and the walk would
eventually end up in some strongly connected component of the graph. another di   culty
occurs when the graph is periodic, that is, the greatest common divisor of all cycle lengths
of the graph is greater than one. in this case, the random walk does not converge to a
stationary id203 distribution but rather oscillates between some set of id203
distributions. we will consider this topic further in chapter 5.

1.6 bibliographic notes

singular value decomposition is fundamental to numerical analysis and id202.
there are many texts on these subjects and the interested reader may want to study these.
a good reference is [?]. the material on id91 a mixture of gaussians in section 1.5.2
is from [?]. modeling data with a mixture of gaussians is a standard tool in statistics.
several well-known heuristics like the expectation-minimization algorithm are used to
learn (   t) the mixture model to data. recently, in theoretical computer science, there
has been modest progress on provable polynomial-time algorithms for learning mixtures.
some references are [?], [?], [?], [?]. the application to the discrete optimization problem
is from [?]. the section on ranking documents/webpages is from two in   uential papers,
one on hubs and authorities by jon kleinberg [?] and the other on id95 by page,
brin, motwani and winograd [?].

27

1.7 exercises

exercise 1.1 (best    t functions versus best least squares    t) in many experiments
one collects the value of a parameter at various instances of time. let yi be the value of
the parameter y at time xi. suppose we wish to construct the best linear approximation
to the data in the sense that we wish to minimize the mean square error. here error is
measured vertically rather than perpendicular to the line. develop formulas for m and b to
minimize the mean square error of the points {(xi, yi)|1     i     n} to the line y = mx + b.

exercise 1.2 given    ve observed parameters, height, weight, age, income, and blood
pressure of n people, how would one    nd the best least squares    t subspace of the form

a1 (height) + a2 (weight) + a3 (age) + a4 (income) + a5 (blood pressure) = 0

here a1, a2, . . . , a5 are the unknown parameters. if there is a good best    t 4-dimensional
subspace, then one can think of the points as lying close to a 4-dimensional sheet rather
than points lying in 5-dimensions. why is it better to use the perpendicular distance to the
subspace rather than vertical distance where vertical distance to the subspace is measured
along the coordinate axis corresponding to one of the unknowns?

exercise 1.3 what is the best    t line for each of the following set of points?

1. {(0, 1) , (1, 0)}
2. {(0, 1) , (2, 0)}

3. the rows of the matrix

(1) y = x and (2) y = 2x. for
solution: (1) and (2) are easy to do from scratch.
(3), there is no simple method. we will describe a general method later and this can
be applied. but the best    t line is v1 = 1   
correct.
exercise 1.4 let a be a square n    n matrix whose rows are orthonormal. prove that
the columns of a are orthonormal.

at is nonsingular it has an inverse(cid:0)at(cid:1)   1. thus at aat(cid:0)at(cid:1)   1 = at(cid:0)at(cid:1)   1 implying

solution: since the rows of a are orthonormal aat = i and hence at aat = at . since

. fix convince yourself that this is

2

5

that at a = i, i.e., the columns of a are orthonormal.

28

      

       17
4   2 26
(cid:19)
(cid:18) 1

11

7

exercise 1.5 suppose a is a n   n matrix with block diagonal structure with k equal size
blocks where all entries of the ith block are ai with a1 > a2 >        > ak > 0. show that a
has exactly k nonzero singular vectors v1, v2, . . . , vk where vi has the value ( k
n)1/2 in the
coordinates corresponding to the ith block and 0 elsewhere. in other words, the singular
vectors exactly identify the blocks of the diagonal. what happens if a1 = a2 =        = ak?
in the case where the ai are equal, what is the structure of the set of all possible singular
vectors?
hint: by symmetry, the top singular vector   s components must be constant in each block.

exercise 1.6 prove that the left singular vectors of a are the right singular vectors of
at .

solution: a = u dv t , thus at = v du t .

exercise 1.7 interpret the right and left singular vectors for the document term matrix.

solution: the    rst right singular vector is a synthetic document that best matches the
collection of documents. the    rst left singular vector is a synthetic word that best matches
the collection of terms appearing in the documents.

exercise 1.8 verify that the sum of rank one matrices

  iuivi

t can be written as

u dv t , where the ui are the columns of u and vi are the columns of v . to do this,    rst
verify that for any two matrices p and q, we have

(cid:88)

p q =

t

piqi

where pi is the ith column of p and qi is the ith column of q.

i

exercise 1.9

1. show that the rank of a is r where r is the miminum i such that arg max
v   v1,v2,...,vi

2. show that(cid:12)(cid:12)ut

1 a(cid:12)(cid:12) = max

(cid:12)(cid:12)uta(cid:12)(cid:12) =   1.

|u|=1

|a v| = 0.

|v|=1

r(cid:80)

i=1

exercise 1.10 if   1,   2, . . . ,   r are the singular values of a and v1, v2, . . . , vr are the
corresponding right singular vectors, show that

hint: use svd.

r(cid:80)

i=1

1. at a =

  2
i vivi

t

29

2. v1, v2, . . . vr are eigenvectors ofat a.

3. assuming that the set of eigenvectors of a matrix is unique, conclude that the set of

singular values of the matrix is unique.

see the appendix for the de   nition of eigenvectors.

exercise 1.11 let a be a matrix. given an algorithm for    nding

v1 = arg max

|v|=1

|av|

describe an algorithm to    nd the svd of a.

exercise 1.12 compute the singular valued decomposition of the matrix

(cid:19)

(cid:18) 1 2

3 4

a =

exercise 1.13 write a program to implement the power method for computing the    rst
singular vector of a matrix. apply your program to the matrix

                     

a =

3       
2
4       
3
...
...
10 0       
0       
0

1
2
...
9
10

9
10

0
0

10
0
...
0
0

                     

exercise 1.14 modify the power method to    nd the    rst four singular vectors of a matrix
a as follows. randomly select four vectors and    nd an orthonormal basis for the space
spanned by the four vectors. then multiple each of the basis vectors times a and    nd a
new orthonormal basis for the space spanned by the resulting four vectors. apply your
method to    nd the    rst four singular vectors of matrix a of exercise 1.13

exercise 1.15 let a be a real valued matrix. prove that b = aat is positive de   nite.

exercise 1.16 prove that the eigenvalues of a symmetric real valued matrix are real.

exercise 1.17 suppose a is a square invertible matrix and the svd of a is a =(cid:80)
prove that the inverse of a is(cid:80)
r(cid:80)

exercise 1.18 suppose a is square, but not necessarily invertible and has svd a =

i . show that bx = x for all x in the span of the right
i=1
singular vectors of a. for this reason b is sometimes called the pseudo inverse of a and
can play the role of a   1 in many applications.

i . let b =

r(cid:80)

  iuivt

viut
i .

viut

1
  i

1
  i

i=1

i

i

  iuivt
i .

30

exercise 1.19

1. for any matrix a, show that   k     ||a||f   

k

.

2. prove that there exists a matrix b of rank at most k such that ||a     b||2     ||a||f   

k

.

3. can the 2-norm on the left hand side in (b) be replaced by frobenius norm?

exercise 1.20 suppose an n    d matrix a is given and you are allowed to preprocess
a. then you are given a number of d-dimensional vectors x1, x2, . . . , xm and for each of
these vectors you must    nd the vector axi approximately, in the sense that you must    nd a
|ui     axi|       ||a||f|xi|. here    >0 is a given error bound. describe
vector ui satisfying

an algorithm that accomplishes this in time o(cid:0) d+n

(cid:1) per xi not counting the preprocessing

  2

time.

exercise 1.21 (constrained least squares problem using svd) given a, b,
and m, use the svd algorithm to    nd a vector x with |x| < m minimizing |ax    b|. this
problem is a learning exercise for the advanced student. for hints/solution consult golub
and van loan, chapter 12.
exercise 1.22 (document-term matrices): suppose we have a m  n document-term
matrix where each row corresponds to a document where the rows have been normalized
to length one. de   ne the    similarity    between two such documents by their dot product.

1. consider a    synthetic    document whose sum of squared similarities with all docu-
ments in the matrix is as high as possible. what is this synthetic document and how
would you    nd it?

2. how does the synthetic document in (1) di   er from the center of gravity?

3. building on (1), given a positive integer k,    nd a set of k synthetic documents such
that the sum of squares of the mk similarities between each document in the matrix
and each synthetic document is maximized. to avoid the trivial solution of selecting
k copies of the document in (1), require the k synthetic documents to be orthogonal
to each other. relate these synthetic documents to singular vectors.

4. suppose that the documents can be partitioned into k subsets (often called clusters),
where documents in the same cluster are similar and documents in di   erent clusters
are not very similar. consider the computational problem of isolating the clusters.
this is a hard problem in general. but assume that the terms can also be partitioned
into k clusters so that for i (cid:54)= j, no term in the ith cluster occurs in a document
in the jth cluster. if we knew the clusters and arranged the rows and columns in
them to be contiguous, then the matrix would be a block-diagonal matrix. of course

31

the clusters are not known. by a    block    of the document-term matrix, we mean
a submatrix with rows corresponding to the ithcluster of documents and columns
corresponding to the ithcluster of terms . we can also partition any n vector into
blocks. show that any right singular vector of the matrix must have the property
that each of its blocks is a right singular vector of the corresponding block of the
document-term matrix.

5. suppose now that the singular values of all the blocks are distinct (also across blocks).

show how to solve the id91 problem.

hint: (4) use the fact that the right singular vectors must be eigenvectors of at a. show
that at a is also block-diagonal and use properties of eigenvectors.

solution: (1)
(2)
(3): it is obvious that at a is block diagonal. we claim that for any block-diagonal
symmetric matrix b, each eigenvector must be composed of eigenvectors of blocks. to
see this, just note that since for an eigenvector v of b, bv is   v for a real   , for a block
bi of b, biv is also    times the corresponding block of v .
(4): by the above, it is easy to see that each eigenvector of at a has nonzero entries in
just one block.
(e)

exercise 1.23 generate a number of samples according to a mixture of 1-dimensional
gaussians. see what happens as the centers get closer. alternatively, see what happens
when the centers are    xed and the standard deviation is increased.
exercise 1.24 show that maximizing xt uut (1     x) subject to xi     {0, 1} is equivalent
to partitioning the coordinates of u into two subsets where the sum of the elements in both
subsets are equal.

solution: xt uut (1   x) can be written as the product of two scalars(cid:0)xt u(cid:1)(cid:0)ut (1     x)(cid:1).

the    rst scalar is the sum of the coordinates of u corresponding to the subset s and the
second scalar is the sum of the complementary coordinates of u. to maximize the product,
one partitions the coordinates of u so that the two sums are as equally as possible. given
the subset determined by the maximization, check if xt u = ut (1     x).

exercise 1.25 read in a photo and convert to a matrix. perform a singular value decom-
position of the matrix. reconstruct the photo using only 10%, 25%, 50% of the singular
values.

1. print the reconstructed photo. how good is the quality of the reconstructed photo?

2. what percent of the forbenius norm is captured in each case?

32

hint: if you use matlab, the command to read a photo is imread. the types of    les that
can be read are given by imformats. to print the    le use imwrite. print using jpeg format.
to access the    le afterwards you may need to add the    le extension .jpg. the command
imread will read the    le in uint8 and you will need to convert to double for the svd code.
afterwards you will need to convert back to uint8 to write the    le. if the photo is a color
photo you will get three matrices for the three colors used.

exercise 1.26 find a collection of something such as photgraphs, drawings, or charts
and try the svd compression technique on it. how well does the reconstruction work?
exercise 1.27 create a set of 100, 100   100 matrices of random numbers between 0 and
1 such that each entry is highly correlated with the adjacency entries. find the svd of
a. what fraction of the frobenius norm of a is captured by the top 100 singular vectors?
how many singular vectors are required to capture 95% of the frobenius norm?
exercise 1.28 create a 100    100 matrix a of random numbers between 0 and 1 such
that each entry is highly correlated with the adjacency entries and    nd the    rst 100 vectors
for a single basis that is reasonably good for all 100 matrices. how does one do this? what
fraction of the frobenius norm of a new matrix is captured by the basis?
solution: if v1, v2,       , v100 is the basis, then a = av1v1

t +        .

t + av2v2

exercise 1.29 show that the running time for the maximum cut algorithm in section ??
can be carried out in time o(n3 + poly(n)kk), where poly is some polynomial.

exercise 1.30 let x1, x2, . . . , xn be n points in d-dimensional space and let x be the
n   d matrix whose rows are the n points. suppose we know only the matrix d of pairwise
distances between points and not the coordinates of the points themselves. the xij are not
unique since any translation, rotation, or re   ection of the coordinate system leaves the
distances invariant. fix the origin of the coordinate system so that the centroid of the set
of points is at the origin.

1. show that the elements of x t x are given by

(cid:34)

n(cid:88)

j=1

n(cid:88)

i=1

d2
ij +

1
n

n(cid:88)

n(cid:88)

i=1

j=1

(cid:35)

d2
ij

.

i xj =    1
xt
2

ij     1
d2
n

ij     1
d2
n

2. describe an algorithm for determining the matrix x whose rows are the xi.

solution: (1) since the centroid of the set of points is at the origin of the coordinate

axes,(cid:80)n

i=1 xij = 0. write

ij = (xi     xj)t (xi     xj) = xt
d2

i xi + xt

j xj     2xt

i xj

(1.3)

33

(1.4)

(1.5)

(1.6)

(cid:33)

then

(cid:80)n

since 1
n
similarly

i=1 xt

j xj = xt

j xj and 1
n

i=1 xt
i

d2
ij =

i xi + xt
xt

j xj

n(cid:88)
(cid:0)(cid:80)n
n(cid:88)

i=1

1
n

1
n

n(cid:88)
(cid:1) xj = 0.
n(cid:88)

i=1

1
n

1
n

d2
ij =

xt
j xj + xt

i xi

j=1

j=1

summing (1.4) over j gives

n(cid:88)

n(cid:88)

j=1

i=1

n(cid:88)

i=1

d2
ij =

xt
i xi +

n(cid:88)

j=1

n(cid:88)

i=1

xt
i xi

xt
j xj = 2

1
n

rearranging (1.3) and substituting for xt

i xi and xt

j xj from (1.3) and (1.4) yields

(cid:0)d2

i xj =    1
xt
2

ij     xt

i xi     xt

j xj

finally substituting (1.6) yields

ij     xt

i xi     xt

j xj

(cid:0)d2
i xj =    1
xt
2
(cid:80)n
(cid:80)n

n(cid:88)

i=1

d2
ij +

2
n

(cid:32)

2

(cid:1) =    1
(cid:32)

(cid:1) =    1

2

n(cid:88)

j=1

ij     1
d2
n
n(cid:88)

ij     1
d2
n

ij     1
d2
n
n(cid:88)

ij     1
d2
n

j=1

i=1

1
n2

d2
ij +

(cid:80)n

n(cid:88)

i=1

n(cid:88)

j=1

(cid:33)

xt
i xi

n(cid:88)
(cid:80)n

i=1

d2
ij

i=1

j=1 d2

i=1 d2
ij,
ij are the averages of the square of the elements of the ith row, the

note that is d is the matrix of pairwise squared distances, then 1
n
and 1
n2
square of the elements of the jth column and all squared distances respectively.
(2) having constructed x t x we can use an eigenvalue decomposition to determine the
coordinate matrix x. clearly x t x is symmetric and if the distances come from a set of
n points in a d-dimensional space x t x will be positive de   nite and of rank d. thus we
can decompose x t x asx t x = v t   v where the    rst d eigenvalues are positive and the
remainder are zero. since the x t x = v t    1
2 v and thus the coordinates are given by
x = v t    1

k=1 d2

ij, 1
n

2    1

2

exercise 1.31

1. consider the pairwise distance matrix for twenty us cities given below. use the

algorithm of exercise 2 to place the cities on a map of the us.

2. suppose you had airline distances for 50 cities around the world. could you use

these distances to construct a world map?

34

b
o
s
-
400
851
1551
1769
1605
2596
1137
1255
1123
188
1282
271
2300
483
1038
2099
2699
2493
393

b
u
f
400
-
454
1198
1370
1286
2198
803
1181
731
292
883
279
1906
178
662
1699
2300
2117
292

c
h
i
851
454
-
803
920
940
1745
482
1188
355
713
432
666
1453
410
262
1260
1858
1737
597

d
a
l
1551
1198
803
-
663
225
1240
420
1111
862
1374
586
1299
887
1070
547
999
1483
1681
1185

d
e
n
1769
1370
920
663
-
879
831
879
1726
700
1631
488
1579
586
1320
796
371
949
1021
1494

h
o
u
1605
1286
940
225
879
-
1374
484
968
1056
1420
794
1341
1017
1137
679
1200
1645
1891
1220

l
a

2596
2198
1745
1240
831
1374
-
1603
2339
1524
2451
1315
2394
357
2136
1589
579
347
959
2300

m
e
m
1137
803
482
420
879
484
1603
-
872
699
957
529
881
1263
660
240
1250
1802
1867
765

m
i
a
1255
1181
1188
1111
1726
968
2339
872
-
1511
1092
1397
1019
1982
1010
1061
2089
2594
2734
923

m
i
m
1123
731
355
862
700
1056
1524
699
1511
-
1018
290
985
1280
743
466
987
1584
1395
934

boston
bu   alo
chicago
dallas
denver
houston
los angeles
memphis
miami
minneapolis
new york
omaha
philadelphia
phoenix
pittsburgh
saint louis
salt lake city
san francisco
seattle
washington d.c.

35

n
y

188
292
713
1374
1631
1420
2451
957
1092
1018
-
1144
83
2145
317
875
1972
2571
2408
250

o
m
a
1282
883
432
586
488
794
1315
529
1397
290
1144
-
1094
1036
836
354
833
1429
1369
1014

p
h
i
271
279
666
1299
1579
1341
2394
881
1019
985
83
1094
-
2083
259
811
1925
2523
2380
123

p
h
o
2300
1906
1453
887
586
1017
357
1263
1982
1280
2145
1036
2083
-
1828
1272
504
653
1114
1983

p
i
t
483
178
410
1070
1320
1137
2136
660
1010
743
317
836
259
1828
-
559
1668
2264
2138
192

s
t
l
1038
662
262
547
796
679
1589
240
1061
466
875
354
811
1272
559
-
1162
1744
1724
712

s
l
c
2099
1699
1260
999
371
1200
579
1250
2089
987
1972
833
1925
504
1668
1162
-
600
701
1848

s
f

2699
2300
1858
1483
949
1645
347
1802
2594
1584
2571
1429
2523
653
2264
1744
600
-
678
2442

s
e
a
2493
2117
1737
1681
1021
1891
959
1867
2734
1395
2408
1369
2380
1114
2138
1724
701
678
-
2329

d
c

393
292
597
1185
1494
1220
2300
765
923
934
205
1014
123
1963
192
712
1848
2442
2329
-

boston
bu   alo
chicago
dallas
denver
houston
los angeles
memphis
miami
minneapolis
new york
omaha
philadelphia
phoenix
pittsburgh
saint louis
salt lake city
san francisco
seattle
washington d.c.

references
hubs and authorities
golub and van loan
id91 gaussians

36

