contents

1 algorithms for massive data problems

1.1 frequency moments of data streams . . . . . . . . . . . . . . . . . . . . .
1.1.1 number of distinct elements in a data stream . . . . . . . . . . .
1.1.2 counting the number of occurrences of a given element.
. . . . .
1.1.3 counting frequent elements . . . . . . . . . . . . . . . . . . . . . .
1.1.4 the second moment . . . . . . . . . . . . . . . . . . . . . . . . . .

2
3
4
7
8
9
1.2 sketch of a large matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.2.1 id127 using sampling . . . . . . . . . . . . . . . . 14
1.2.2 approximating a matrix with a sample of rows and columns . . . 18
1.3 graph and matrix sparsi   ers
. . . . . . . . . . . . . . . . . . . . . . . . . 20
1.4 sketches of documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1.5 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

1

data. eg. matrix a

ram

aij = ?

aij =       

algorithm

figure 1.1: traditional algorithm model

1 algorithms for massive data problems

massive data, sampling

this chapter deals with massive data problems where the input data (a graph, a ma-
trix or some other object) is too large to be stored in random access memory. one model
for such problems is the streaming model, where the data can be seen only once.
in
the streaming model, the natural technique to deal with the massive data is sampling.
sampling is done    on the    y   . as each piece of data is seen, based on a coin toss, one
decides whether to include the data in the sample. typically, the id203 of including
the data point in the sample may depend on its value. models allowing multiple passes
through the data are also useful; but the number of passes needs to be small. we always
assume that random access memory (ram) is limited, so the entire data cannot be stored
in ram.

to introduce the basic    avor of sampling on the    y, consider the following simple
primitive. we have a stream of n elements, where, n is not known at the start. we wish
to sample uniformly at random one of the n elements. if we have to pick a sample and
then never change it, we are out of luck. [you can convince yourself of this.] so we have to
allow ourselves to    change our mind   : after perhaps drawing one element as a sample, we
must allow ourselves to reject it and instead draw the current element as the new sample.
with this hint, the reader may want to think of the algorithm. instead of describing the

2

algorithm for this problem, we will solve a more general problem:

from a stream of n positive real numbers a1, a2, . . . , an, draw a sample element ai so

that the id203 of picking an element is proportional to its value.
it is easy to see that the following sampling method works. upon seeing a1, a2, . . . , ai,
keep track of the sum a = a1 + a2 +        + ai and a sample aj, j     i, drawn with
id203 proportional to its value. on seeing ai+1, replace the current sample by ai+1
with id203 ai+1
a+ai+1
we can prove by induction that this algorithm does in fact sample element aj with
id203 aj/(a1 + a2 +        + an) : suppose after we have read a1, a2, . . . , ai, we have
picked a sample with id203 of aj being the sample equal to aj/(a1 + a2 +        + ai).
the id203 that we will keep this aj after reading ai+1 is

and update a.

1    

ai+1

a1 + a2 +        + ai+1

a1 + a2 +        + ai
a1 + a2 +        + ai+1

.

=

combining the two, we see that the id203 that aj is the sample after reading ai+1
is precisely aj/(a1 + a2 +        + ai+1) completing the inductive proof.

1.1 frequency moments of data streams

an important class of problems concerns the frequency moments of data streams.
here a data stream a1, a2, . . . , an of length n consists of symbols ai from an alphabet of
m possible symbols which for convenience we denote as {1, 2, . . . , m}. throughout this
section, n, m, and ai will have these meanings and s (for symbol) will denote a generic
element of {1, 2, . . . , m}. the frequency fs of the symbol s is the number of occurrences
of s in the stream. for a nonnegative integer p, the pth frequency moment of the stream
is

m(cid:88)

s=1

f p
s .

note that the p = 0 frequency moment corresponds to the number of distinct symbols
occurring in the stream. the    rst frequency moment is just n, the length of the string.
f 2
s , is useful in computing the variance of the stream.

the second frequency moment,(cid:80)
(cid:18)
m(cid:88)
(cid:18) m(cid:80)

in the limit as p becomes large,

fs     n
m

m(cid:88)

(cid:17)2

(cid:16)

1
m

1
m

s=1

s=1

=

s

ment(s).

s=1

s     2
f 2
(cid:19)1/p

f p
s

(cid:16) n

(cid:17)2(cid:19)

m(cid:88)

s=1

n
m

fs +

m

=

1
m

s     n2
f 2
m2

is the frequency of the most frequent ele-

we will describe sampling based algorithms to compute these quantities for streaming
data shortly. but    rst a note on the motivation for these various problems. the identity
and frequency of the the most frequent item or more generally, items whose frequency

3

exceeds a fraction of n, is clearly important in many applications. if the items are packets
on a network with source destination addresses, the high frequency items identify the
heavy bandwidth users. if the data is purchase records in a supermarket, the high fre-
quency items are the best-selling items. determining the number of distinct symbols is
the abstract version of determining such things as the number of accounts, web users, or
credit card holders. the second moment and variance are useful in networking as well as
in database and other applications. large amounts of network log data are generated by
routers that can record for all the messages passing through them, the source address,
destination address, and the number of packets. this massive data cannot be easily sorted
or aggregated into totals for each source/destination. but it is important to know if some
popular source-destination pairs have a lot of tra   c for which the variance is the natural
measure.

1.1.1 number of distinct elements in a data stream

consider a sequence a1, a2, . . . , an of n elements, each ai an integer in the range 1 to m
where n and m are very large. suppose we wish to determine the number of distinct ai in
the sequence. each ai might represent a credit card number extracted from a sequence of
credit card transactions and we wish to determine how many distinct credit card accounts
there are. the model is a data stream where symbols are seen one at a time. we    rst
show that any deterministic algorithm that determines the number of distinct elements
exactly must use at least m bits of memory.

lower bound on memory for exact deterministic algorithm

suppose we have seen the    rst k symbols of the stream and k > m. the set of distinct
symbols seen so far could be any of the 2m subsets of {1, 2, . . . , m}. each subset must
result in a di   erent state for our algorithm and hence m bits of memory are required. to
see this, suppose    rst that two di   erent size subsets of distinct symbols lead to the same
internal state. then our algorithm would produce the same count of distinct symbols for
both inputs, clearly an error for one of the input sequences. if two sequences with the
same number of distinct elements but di   erent subsets lead to the same state, then on
next seeing a symbol that appeared in one sequence but not the other, we would make
an error on at least one of them .

algorithm for the number of distinct elements

let a1, a2, . . . , an be a sequence of elements where each ai     {1, 2, . . . , m}. the number
of distinct elements can be estimated with o(log m) space. let s     {1, 2, . . . , m} be the
set of elements that appear in the sequence. suppose that the elements of s were selected
uniformly at random from {1, 2, . . . , m}. let min denote the minimum element of s.
knowing the minimum element of s allows us to estimate the size of s. the elements of
s partition the set {1, 2, . . . , m} into |s| + 1 subsets each of size approximately m|s|+1. see

4

|s| + 1 subsets

(cid:125)(cid:124)

(cid:123)

(cid:122)

m|s|+1

figure 1.2: estimating the size of s from the minimum element in s which has value

approximately m|s|+1. the elements of s partition the set {1, 2, . . . , m} into |s| + 1 subsets
each of size approximately m|s|+1.

figure 1.2. thus, the minimum element of s should have value close to m|s|+1. solving
min = m|s|+1 yields |s| = m
min     1. since we can determine min, this gives us an estimate
of |s|.

the above analysis required that the elements of s were picked uniformly at random
from {1, 2, . . . , m}. this is generally not the case when we have a sequence a1, a2, . . . , an
of elements from {1, 2, . . . , m}. clearly if the elements of s were obtained by selecting the
|s| smallest elements of {1, 2, . . . , m}, the above technique would give the wrong answer.
if the elements are not picked uniformly at random, can we estimate the number of distinct
elements? the way to solve this problem is to use a hash function h where

h : {1, 2, . . . , m}     {0, 1, 2, . . . , m     1}

to count the number of distinct elements in the input, count the number of elements
in the mapped set {h (a1) , h (a2) , . . .}. the point being that {h(a1), h (a2) , . . .} behaves
like a random subset and so the above heuristic argument using the minimum to estimate
the number of elements may apply. if we needed h (1) , h (2) , . . . to be completely inde-
pendent, the space needed to store the hash function would too high. fortunately, only
2-way independence is needed. we recall the formal de   nition of 2-way independence
below. but    rst recall that a hash function is always chosen at random from a family of
hash functions and phrases like    id203 of collision    refer to the id203 over the
choice of hash function.

universal hash functions

the set of hash functions

h = {h | h : {1, 2, . . . , m}     {0, 1, 2, . . . , m     1}}

is 2-universal if for all x and y in {1, 2, . . . , m}, x (cid:54)= y, and for all z and w in {0, 1, 2, . . . , m     1}

prob [h (x) = z and h (y) = w] = 1
m 2

5

for a randomly chosen h. the concept of a 2-universal family of hash functions is that
given x, h (x) is equally likely to be any element of {0, 1, 2, . . . , m     1} (the reader should
prove this from the de   nition of 2-universal) and for x (cid:54)= y, h (x) and h (y) are indepen-
dent.

we now give an example of a 2-universal family of hash functions. for simplicity let
m be a prime, with m > m. for each pair of integers a and b in the range [0,m -1], de   ne
a hash function

hab (x) = ax + b mod (m )

to store the hash function hab, store the two integers a and b. this requires only o(log m )
space. to see that the family is 2-universal note that h(x) = z and h(y) = w if and only
if

(cid:19)(cid:18) a

(cid:19)

=

b

(cid:19)

(cid:18) z

w

(cid:18) x 1
(cid:18) x 1
(cid:19)

y 1

mod (m )

if x (cid:54)= y , the matrix
a and b. thus, for a and b chosen uniformly at random, the id203 of the equation
holding is exactly 1

m 2 . [we assumed m > m. what goes wrong if this did not hold?]

is invertible modulo m and there is only one solution for

y 1

analysis of distinct element counting algorithm

let b1, b2, . . . , bd be the distinct values that appear in the input. then

s = {h (b1) , h (b2) , . . . h (bd)} is a set of d random and 2-way independent values from the
set {0, 1, 2, . . . , m     1}. we now show that m
min is a good estimate for d, the number of
distinct elements in the input, where min=min(s).

6     m

min     6d, where min is

3, d

(cid:20)

   k h (bk) <

m
6d

(cid:21)

lemma 1.1 assume m >100d. with id203 at least 2
the smallest element of s.

proof: first, we show that prob(cid:2) m

(cid:21)

(cid:20) m

min

6.

min > 6d(cid:3) < 1
(cid:20)
(cid:21)
(cid:26) 1 if h (bi) < m

min <

m
6d

6d

0 otherwise

zi =

for i = 1, 2, . . . , d de   ne the indicator variable

prob

> 6d

= prob

= prob

d(cid:80)

i=1

and let z =

zi. if h (bi) is chosen randomly from {0, 1, 2, . . . , m     1}, then prob [zi = 1] =

6

1

6d . thus, e (zi) = 1

6d and e (z) = 1

6. now

(cid:21)

(cid:20) m

min

prob

> 6d

= prob

(cid:20)
(cid:20)

(cid:21)

min <

m
6d
   k h (bk) <

(cid:21)

m
6d

= prob
= prob (z     1) = prob [z     6e (z)] .

by markov   s inequality prob [z     6e (z)]     1
6.

finally, we show that prob(cid:2) m
(cid:21)

(cid:20) m

(cid:3) < 1
= prob(cid:2)min > 6m

min < d

prob

6.

6

< d
6

min

(cid:3) = prob(cid:2)   k h (bk) > 6m

d

(cid:3)

d

for i = 1, 2, . . . , d de   ne the indicator variable

(cid:26) 0 if h (bi) > 6m

d

1 otherwise

yi =

d(cid:80)

and let y =

yi. now prob (yi = 1) = 6

d, and e (y) = 6. for 2-way
independent random variables, the variance of their sum is the sum of their variances. so
v ar (y) = dv ar (y1). further, it is easy to see since y1 is 0 or 1 that

d, e (yi) = 6

i=1

v ar(y1) = e(cid:2)(y1     e(y1))2(cid:3) = e(y2

1)     e2(y1) = e(y1)     e2(y1)     e (y1) .

thus v ar(y)     e (y). now by the chebychev inequality,

(cid:20) m

min

(cid:21)

<

d
6

prob

(cid:20)

= prob(cid:2)min > 6m

(cid:3) = prob

d

   k h (bi) >

6m
d
= prob (y = 0)     prob [|y     e (y)|     e (y)]
    var(y)
e2 (y)

    1

    1
6

e (y)

(cid:21)

since m
6     m
d

min > 6d with id203 at most 1
min     6d with id203 at least 2
3.

6 and m

min < d

6 with id203 at most 1
6,

1.1.2 counting the number of occurrences of a given element.

to count the number of occurrences of an element in a stream requires at most log n
space where n is the length of the stream. clearly, for any length stream that occurs in
practice, we can a   ord log n space. for this reason, the following material may never be
used in practice, but the technique is interesting and may give insight into how to solve

7

some other problem.

consider a string of 0   s and 1   s of length n in which we wish to count the number
of occurrences of 1   s. clearly if we had log n bits of memory we could keep track of the
exact number of 1   s. however, we can approximate the number with only log log m bits.

let m be the number of 1   s that occur in the sequence. keep a value k such that 2k
is approximately the number of occurrences m. storing k requires only log log n bits of
memory. the algorithm works as follows. start with k=0. for each occurrence of a 1,
add one to k with id203 1/2k. at the end of the string, the quantity 2k     1 is the
estimate of m. to obtain a coin that comes down heads with id203 1/2k,    ip a fair
coin, one that comes down heads with id203 1/2, k times and report heads if the fair
coin comes down heads in all k    ips.

given k, on average it will take 2k ones before k is incremented. thus, the expected

number of 1   s to produce the current value of k is 1 + 2 + 4 +        + 2k   1 = 2k     1.

1.1.3 counting frequent elements

the majority and frequent algorithms

first consider the very simple problem of n people voting. there are m candidates,
{1, 2, . . . , m}. we want to determine if one candidate gets a majority vote and if so
who. formally, we are given a stream of integers a1, a2, . . . , an, each ai belonging to
{1, 2, . . . , m}, and want to determine whether there is some s     {1, 2, . . . , m} which oc-
curs more than n/2 times and if so which s. it is easy to see that to solve the problem
exactly on read only once streaming data with a deterministic algorithm, requires    (n)
space. suppose n is even and the    rst n/2 items are all distinct and the last n/2 items are
identical. after reading the    rst n/2 items, we need to remember exactly which elements
of {1, 2, . . . , m} have occurred. if for two di   erent sets of elements occurring in the    rst
half of the stream, the contents of the memory are the same, then a mistake would occur
if the second half of the stream consists solely of an element in one set, but not the other.
thus, log2

(cid:1) bits of memory, which if m > n is    (n), are needed.

(cid:0) m

n/2

now lets allow the algorithm a random number generator. the reader may want to
think about simple sampling schemes    rst - like picking an element as a sample and then
checking how many times it occurs and modi   cations of this. the following is a simple
low-space algorithm that always    nds the majority vote if there is one.
if there is no
majority vote, the output may be arbitrary. that is, there may be    false positives   , but
no    false negatives   .

majority algorithm

8

store a1 and initialize a counter to one. for each subsequent ai, if ai is the same as
the currently stored item, increment the counter by one. if it di   ers, decrement the
counter by one provided the counter is nonzero. if the counter is zero, then store ai
and set the counter to one.

to analyze the algorithm, it is convenient to view the decrement counter step as    elim-
inating    two items, the new one and the one that caused the last increment in the counter.
it is easy to see that if there is a majority element s, it must be stored at the end. if not,
each occurrence of s was eliminated; but each such elimination also causes another item
to be eliminated and so for a majority item not to be stored at the end, we must have
eliminated more than n items, a contradiction.

next we modify the above algorithm so that not just the majority, but also items
with frequency above some threshold are detected. we will also ensure (approximately)
that there are no false positives as well as no false negatives. indeed the algorithm below
will    nd the frequency (number of occurrences) of each element of {1, 2, . . . , m} to within
n
k+1 using o(k log n) space by keeping k counters instead of just one
an additive term of
counter.

algorithm frequent

maintain a list of items being counted. initially the list is empty. for each item, if
it is the same as some item on the list, increment its counter by one. if it di   ers
from all the items on the list, then if there are less than k items on the list, add the
item to the list with its counter set to one. if there are already k items on the list
decrement each of the current counters by one deleting an element from the list if its
count becomes zero.

theorem 1.2 at the end of algorithm frequent, for each s     {1, 2, . . . , m}, its counter
on the list is at least the number of occurrences of s in the stream minus n/(k+1). in
particular, if some s does not occur on the list, its counter is zero and the theorem asserts
that it occurs fewer than n/(k+1) times in the stream.

proof: view each decrement counter step as eliminating some items. an item is elimi-
nated if it is the current ai being read and there are already k symbols di   erent from it
on the list in which case it and k other items are simultaneously eliminated. thus, the
elimination of each occurrence of an s     {1, 2, . . . , m} is really the elimination of k + 1
items. thus, no more than n/(k+1) occurrences of any symbol can be eliminated. now,
it is clear that if an item is not eliminated, then it must still be on the list at the end.
this proves the theorem.

1.1.4 the second moment

this section focuses on computing the second moment of a stream with symbols from
{1, 2, . . . , m}. let fs denote the number of occurrences of symbol s in the stream. the

9

f 2
s . to calculate the second moment, for each
second moment of the stream is given by
symbol s, 1     s     m, independently set a random variable xs to   1 with id203 1/2.
maintain a sum by adding xs to the sum each time the symbol s occurs in the stream.
at the end of the stream, the sum will equal
xsfs. the expected value of the sum will
be zero where the expectation is over the choice of the   1 value for the xs.

s=1

s=1

m(cid:80)

(cid:32) m(cid:88)

s=1

m(cid:80)
(cid:33)

e

xsfs

= 0.

although the expected value of the sum is zero, its actual value is a random variable and
the expected value of the square of the sum is given by

e

xsfs

= e

x2
sf 2
s

+ 2e

xsxtfsft

=

f 2
s ,

(cid:32) m(cid:88)

(cid:33)2

(cid:32) m(cid:88)

(cid:33)

s=1

s=1

(cid:32)(cid:88)
(cid:33)2

s(cid:54)=t

(cid:32) m(cid:88)

a =

xsfs

the last equality follows since e (xsxt) = 0 for s (cid:54)= t. thus

(cid:33)

m(cid:88)

s=1

m(cid:80)

is an estimator of

s=1

s=1

f 2
s . one di   culty which we will come back to is that to store the xi

requires space m and we want to do the calculation in log m space.

a second issue is that this estimator is depends on its variance which we now compute.

(cid:33)4

(cid:32) m(cid:88)

s=1

(cid:32) (cid:88)

1   s,t,u,v   m

v ar (a)     e

xsfs

= e

xsxtxuxvfsftfufv

(cid:33)

the    rst inequality is because the variance is at most the second moment and the second
equality is by expansion. in the second sum, since the xs are independent, if any one of
s, u, t, or v is distinct from the others, then the expectation of the whole term is zero.
t for t (cid:54)= s and terms of the form
thus, we need to deal only with terms of the form x2
sx2
x4
s. note that this does not need the full power of mutual independence of all the xs, it
only needs 4-way independence, that any four of the x(cid:48)
ss are mutually independent. in

the above sum, there are four indices s, t, u, v and there are(cid:0)4

(cid:1) ways of choosing two of

2

10

them that have the same x value. thus,

v ar (a)    

sx2
x2

t f 2

s f 2
t

2

e

(cid:32) m(cid:88)
(cid:18)4
(cid:19)
m(cid:88)
m(cid:88)
(cid:32) m(cid:88)

(cid:33)2

t=s+1

s=1

s=1

= 6

    3

m(cid:88)

t=s+1

f 2
s f 2

t +

m(cid:88)
(cid:32) m(cid:88)

s=1

f 2
s

+

s=1

s=1

(cid:33)

+ e

(cid:33)

sf 4
x4
s

(cid:32) m(cid:88)

s=1

f 4
s

(cid:33)2

f 2
s

= 4e2 (a) .

the variance can be reduced by a factor of r by taking the average of r independent trials.
with r independent trials the variance would be at most 4
r e2 (a), so to achieve relative

error    in the estimate of

f 2
s , we need o(1/  2) independent trials.

m(cid:80)

s=1

we will brie   y discuss the independent trials here, so as to understand exactly the

amount of independence needed. instead of computing a using the running sum

xsfs

m(cid:80)

s=1

for one random vector x, we independently generate r m-vectors (x1, x2, . . . , xr) at the
outset and compute r running sums

m(cid:88)
(cid:19)2

s=1

x1sfs ,

m(cid:88)
(cid:18) m(cid:80)
(cid:18) m(cid:80)
var(cid:2) 1
r (a1 + a2 +        + ar)(cid:3) = 1

(cid:19)2

x1sfs

s=1

s=1

let a1 =
, . . . , ar =
r (a1 + a2 +        + ar). the variance of this estimator is

, a2 =

x2sfs

s=1

1

m(cid:88)
(cid:18) m(cid:80)

s=1

s=1

x2sfs , . . . ,

xrsfs.

(cid:19)2

r2 [var (a1) + var (a2) +        + var (ar)] = 1

r var(a1),

xrsfs

. our estimate is

where we have assumed that the a1, a2, . . . , ar are mutually independent. now we com-
pute the variance of a1 as we have done for the variance of a. note that this calculation
assumes only 4-way independence between the coordinates of x1. we summarize the as-
sumptions here for future reference:

m(cid:80)

to get an estimate of

f 2
s within relative error    with id203 close to one, say at
least 0.9999, it su   ces to have r = o(1/  2) vectors x1, x2, . . . , xr, each with m coordinates
of   1 with

s=1

1. e (xs) = 0 for all s.

2. x1, x2, . . . , xr are mutually independent. that is for any r vectors v1, v2, . . . , vr with

. . . , xr = vr) = prob(x1 = v1)prob(x2 =

  1 coordinates, prob(x1 = v1, x2 = v2,
v2)       prob(xr = vr).

11

3. any four coordinates of x1 are independent. same for x2, x3, . . . , and xr.

[caution: (2) does not assume that prob(x1 = v1) = 1
2m for all v1; such an assumption
would mean the coordinates of x1 are mutually independent.] the only drawback with the
algorithm as we have described it so far is that we need to keep the r vectors x1, x2, . . . , xr
in memory so that we can do the running sums. this is too space-expensive. we need to
do the problem in space dependent upon the logarithm of the size of the alphabet m, not
m itself. if    is in    (1), then r is in o(1), so it is not the number of trials r which is the
problem. it is the m.

in the next section, we will see that the computation can be done in o(log m) space
by using pseudo-random vectors x1, x2, . . . , xr instead of truly random ones. the pseudo-
random vectors will satisfy (i), (ii), and (iii) and so they will su   ce. this pseudo-
randomness and limited independence has deep connections, so we will go into the con-
nections as well.

error-correcting codes, polynomial interpolation and limited-way indepen-
dence

consider the problem of generating a random m-vector x of   1   s so that any subset of
four coordinates is mutually independent, i.e., for any distinct s, t, u, and v in {1, 2, . . . , m}
and any a, b, c, and d in {-1, +1},

prob(xs = a, xt = b, xu = c, xv = d) = 1
16.

we will see that such an n-dimensional vector may be generated from a truly ran-
dom    seed    of only o(log m) mutually independent bits. thus, we need only store the
o(log m) bits and can generate any of the m coordinates when needed. this allows us to
store the 4-way independent random m-vector using only log m bits. the    rst fact needed
for this is that for any k, there is a    nite    eld f with exactly 2k elements, each of which
can be represented with k bits and arithmetic operations in the    eld can be carried out in
o(k2) time. [in fact f can be taken to be the set of polynomials of degree k    1 with mod
2 coe   cients, where, multiplication is done modulo an irreducible polynomial of degree
k, again over modulo 2.] here, k will be the ceiling of log2 m. we also assume another
basic fact about polynomial interpolation which says that a polynomial of degree at most
three is uniquely determined by its value over any    eld f at four points. more precisely,
for any four distinct points a1, a2, a3, a4     f and any four possibly not distinct values
b1, b2, b3, b4     f , there is a unique polynomial f (x) = f0 + f1x + f2x2 + f3x3 of degree at
most three that with computations done over f , f (a1) = b1, f (a2) = b2, f (a3) = b3, and
f (a4) = b4.

now our de   nition of the pseudo-random    vector x with 4-way independence is
simple. choose four elements f0, f1, f2, f3 at random from f and form the polynomial
f (s) = f0 + f1s + f2s2 + f3s3. this polynomial represents x as follows. for s = 1, 2, . . . , m,

12

xs is the leading bit of the k-bit representation of f (s). thus, the m-dimensional vector
x can be computed using only the 4k bits in f0, f1, f2, f3. (here k = (cid:100)log m(cid:101)).

lemma 1.3 the x de   ned above have 4-way independence.
proof: assume that the elements of f are represented in binary using    instead of the
traditional 0 and 1. let s, t, u, and v be any four coordinates of x and let   ,   ,   ,       
{   1, 1}. there are exactly 2k   1 elements of f whose leading bit is    and similarly for
  ,   , and   . so, there are exactly 24(k   1) 4-tuples of elements b1, b2, b3, b4     f so that
the leading bit of b1 is   , the leading bit of b2 is   , the leading bit of b3 is   , and the
leading bit of b4 is   . for each such b1, b2, b3, and b4, there is precisely one polynomial f
so that

f (s) = b1, f (t) = b2, f (u) = b3, and f (v) = b4

as we saw above. so, the id203 that

xs =   , xt =   , xu =   , and xv =   

is precisely

24(k   1)

total number of f

= 24(k   1)

24k = 1

16 as asserted.

the lemma states how to get one vector x with 4-way independence. however, we
need r = o(1/  2) vectors. also the vectors must be mutually independent. but this is
easy, just choose r polynomials at the outset.

to implement the algorithm with low space, store only the polynomials in memory.
this requires 4k = o(log m) bits per polynomial for a total of o(log m/  2) bits. when a
symbol s in the stream is read, compute x1s, x2s, . . . , xrs and update the running sums.
note that xs1 is just the leading bit of the    rst polynomial evaluated at s; this calculation
is in o(log m) time. thus, we repeatedly compute the xs from the    seeds   , namely the
coe   cients of the polynomials.

this idea of polynomial interpolation is also used in other contexts. error-correcting
codes is an important example. say we wish to transmit n bits over a channel which may
introduce noise. one can introduce redundancy into the transmission so that some channel
errors can be corrected. a simple way to do this is to view the n bits to be transmitted
as coe   cients of a polynomial f (x) of degree n     1. now transmit f evaluated at points
1, 2, 3, . . . , n + m. at the receiving end, any n correct values will su   ce to reconstruct
the polynomial and the true message. so up to m errors can be tolerated. but even if
the number of errors is at most m, it is not a simple matter to know which values are
corrupted. we do not elaborate on this here.

13

1.2 sketch of a large matrix

we will see how to    nd a good approximation of an m  n matrix a, which is read from
external memory, where m and n are large. the approximation consists of a sample of s
columns and r rows of a along with an s    r multiplier matrix. the schematic diagram
is given in figure 1.3.

the crucial point is that uniform sampling will not always work as is seen from simple
examples. we will see that if we sample rows or columns with probabilities proportional
to the squared length of the row or column, then indeed we can get a good approximation.
one may recall that the top k singular vectors of the svd of a, give a similar picture;
but the svd takes more time to compute, requires all of a to be stored in ram, and
does not have the property that the rows and columns are directly from a. however, the
svd does yield the best approximation. error bounds for our approximation are weaker,
though it can be found faster.

we brie   y touch upon two motivations for such a sketch. suppose a is the document-
term matrix of a large collection of documents. we are to    read    the collection at the
outset and store a sketch so that later, when a query (represented by a vector with one
entry per term) arrives, we can    nd its similarity to each document in the collection.
similarity is de   ned by the dot product. in figure 1.3 it is clear that the matrix-vector
product of a query with the right hand side can be done in time o(ns + sr + rm) which
would be linear in n and m if s and r are o(1). to bound errors for this process, we need
to show that the di   erence between a and the sketch of a has small 2-norm. recall that
the 2-norm ||a||2 of a matrix a is max
|x|=1

xt ax.

a second motivation comes from id126s. here a would be a
customer-product matrix whose (i, j)th entry is the preference of customer i for prod-
uct j. the objective is to collect a few sample entries of a and based on them, get an
approximation to a so that we can make future recommendations. these results say that
a few sampled rows of a (all preferences of a few customers) and a few sampled columns
(all customers    preferences for a few products) give a good approximation to a provided
that the samples are drawn according to the length-squared distribution.

we    rst tackle a simpler problem, that of multiplying two matrices. the matrix

multiplication also uses length squared sampling and is a building block to the sketch.

1.2.1 id127 using sampling

suppose a is an m   n matrix and b is an n   p matrix and the product ab is desired.
we show how to use sampling to get an approximate product faster than the traditional
multiplication. let a (:, k) denote the kth column of a. a (:, k) is a m    1 matrix. let

14

                                    

                                    

                                    

   

                                    

sample
columns

n    s

a

n    m

(cid:21)(cid:20) sample rows

r    m

(cid:21)

(cid:20) multi

plier
s    r

figure 1.3: schematic diagram of the approximation of a by a sample of s columns and
r rows.

b (k, :) be the kth row of b. b (k, :) is a 1    n matrix. it is easy to see that

n(cid:88)

n(cid:88)

ab =

a (:, k)b (k, :) =

outer product of k th column of a and the k th row of b.

k=1

k=1

[this is easy to verify since, the (i, j) th entry of the outer product of the k th column of
a and the k th row of b is just aikbkj.] note that for each value of k, a(:, k)b(k, :) is an
m  p matrix each element of which is a single product of elements of a and b. an obvious
use of sampling suggests itself. sample some values for k and compute a (:, k) b (k, :) for
the sampled k   s using their suitably scaled sum as the estimate of ab. it turns out that
nonuniform sampling probabilities are useful. de   ne a random variable z that takes on
values in {1, 2, . . . , n}. let pk denote the id203 that z assumes the value k. the pk
are nonnegative and sum to one. de   ne an associated random matrix variable that has
value

x =

a (:, z) b (z, :)

1
pz

which takes on value 1
pk
denote the entry-wise expectation.

a(:, k)b(k, :) with id203 pk for k = 1, 2, . . . , n. let e (x)

e (x) =

prob(z = k)

1
pk

a (:, k) b (k, :) =

a (:, k)b (k, :) = ab.

this explains the scaling by 1
[in general, if we wish to estimate the sum of n
real numbers a1, a2, . . . , an by drawing a sample z from {1, 2, . . . , n} with probabilities
pz
p1, p2, . . . , pn, then the unbiased estimator of the sum a1 + a2 +        + an is az/pz.]

in x.

we wish to bound the error in this estimate for which the usual quantity of interest is
the variance. but here each entry xij of the matrix random variable may have a di   erent
variance. so, we de   ne the variance of x as the sum of the variances of all its entries. this

15

n(cid:88)

k=1

n(cid:88)

k=1

|a (:, k)|2|b (k, :)|2     ||ab||2
f .

1
pk

natural device of just taking the sum of the variances greatly simpli   es the calculations
as we will see.

m(cid:88)

p(cid:88)

(cid:88)

e(cid:0)x2

ij

(cid:1)   (cid:88)

var(x) =

var (xij) =

(e(xij)2) =

i=1

j=1

ij

ij

i,j

k

(cid:88)

(cid:88)

kj   (cid:88)

ij

pk

1
p2
k

ikb2
a2

(ab)2
ij.

we can simplify by exchanging the order of summations to get

(cid:88)

1
pk

(cid:88)

a2
ik

(cid:88)

k

i

j

var(x) =

(cid:88)

k

kj     ||ab||2
b2
(cid:80)

f =

|a(:,k)|2
||a||2

f

k

if pk is proportional to |a (:, k)|2, i.e, pk =

, we get a simple bound for var(x).

var(x)     ||a||2

f

|b (k, :)|2 = ||a||2

f||b||2
f .

now, if we do s i.i.d. trials,    nd s (matrix-valued) random variables x1, x2, . . . , xs and
take their (entry-wise) averages, the variance goes down by a factor of s (as is always
the case). it will be useful to give this whole process of estimating the product of two
matrices a, b a symbol -    s which we formally de   ne below.

16

algorithm input a, b m   n respectively n   p matrices. algorithm    nds an approx-

imation a    s b to the matrix product ab.,

    a    s b = 1
other xj) by:

s (x1 + x2 +        + xs), where, each xi is computed independently (of

    draw sample of random variable z from {1, 2, . . . , n} with probabilities pro-

portional to the length squared of the columns of a, i.e.,
    prob (z = k) = |a(:, k)|2/||a||2

f for k = 1, 2, . . . , n.

    set xi =

||a||2
|a(:,z)|2 a(:, z)b(z, :).

f

since    s is a randomized algorithm, it does not give us exactly the same output every
time, so a    s b is not strictly speaking a function of a, b.

using probabilities that are proportional to length squared of columns of a turns out
to be useful in other contexts as well and sampling according to them is called    length-
squared sampling   .
the lemma below shows that the error between the correct ab and the estimate a   s b
goes down as s increases. the error is terms of ||a||f||b||f which is natural.
lemma 1.4 suppose a is an m    n matrix and b is an n    p matrix.

e(cid:0)||a    s b     ab||2

f

(cid:1)     1

||a||2

f||b||2
f .

s

proof: taking the average of s independent samples divides the variance by s.

the basic step in the algorithm is computing the s outer products, each of which takes
time o(n) in addition, we need the time to compute the length-squared of the columns
of a (which can be done by reading a once) and drawing the sample.

it is interesting that if a, b come in the correct order, the estimate a    s b can be

found while the data streams.

lemma 1.5 if a is in column order and b is in row order, then as the data streams we
can draw samples and implement the algorithm a    s b with o(s(m + p)) ram space in
the streaming model.

proof: as an extension of our primitive to sample from a stream of positive reals a1, a2, . . . , an
with probabilities proportional to ai, we an devise an algorithm to do length-squared sam-
pling. suppose we have read the    rst i columns of a and maintain the primitive that we
have sampled one of those i columns with id203 proportional to length squared. as
the i + 1 column streams by, we (i) keep it in ram and (ii) compute its length squared
by summing squares of its entries. at the end of reading the column, we can replace the
current sampled column with the i+1 st column with id203 equal to (length squared

17

of the i + 1 st column)/(sum of length squared of columns 1, 2, . . . i + 1), (much as we did
earlier with real numbers - details left to the reader), so at the end we draw one length-
squared sampled column. we would simultaneously draw s samples. then we pick the
corresponding rows of b (no sampling) and    nally carry out the multiplication algorithm
   s with the sample in ram.

an important special case is the multiplication aat where the length-squared distri-
bution can be shown to be the optimal distribution. the variance of x de   ned above for
this case is

(cid:80)

i,j

ij)    (cid:80)

e2(xij) =(cid:80)

ij

k

e(x2

|a(:, k)|4     ||aat||2
f .

1
pk

the second term is independent of the pk. the    rst term is minimized when the pk
are proportional to |a(:, k)|2, hence length-squared distribution minimizes the variance
of this estimator.
[show that for any positive real numbers a1, a2, . . . , an, the minimum
n over all p1, p2, . . . , pn     0 summing to 1 is attained when
2 +       + 1
1 + 1
value of 1
a2
a2
a2
pk     ak.]
p2
p1

pn

1.2.2 approximating a matrix with a sample of rows and columns

we now return to the problem outlined in the beginning of section (1.2). suppose an
m    n (m, n are large) matrix a is read from external memory and a sketch of it is to
be kept for further computation. clearly, just a random sample of columns (or rows) will
not do, since the sample tells us little about the unsampled columns. a simple sketch
idea would be to keep a random sample of rows and a random sample of columns of the
matrix. with uniform random sampling, this can fail to be a good sketch. consider the
case where very few rows and columns of a are nonzero. a small sample will miss them,
storing only zeros. this toy example can be made more subtle by making a few rows
have very high absolute value entries compared to the rest. the sampling probabilities
need to depend on the size of the entries. if we do length-squared sampling of both rows
and columns, then we can get an approximation to a with a bound on the approximation
error. note that the sampling can be achieved in two passes through the matrix, the    rst
to compute running sums of the length-squared of each row and column and the second
pass to draw the sample. only the sampled rows and columns, which is much smaller
than the whole, need to be stored in ram for further computations. the algorithm starts
with:

    pick s columns of a using length-squared sampling.
    pick r rows of a using length-squared sampling (here lengths are row lengths).

we will see that from these sampled columns and rows, we can construct a succinct
approximation to a. for this, we    rst start with what seems to be a strange idea.

18

observe that a = ai, where i is the n    n identity matrix. now, lets approximate ai
by a    s i. the error is given by lemma (1.4):
f     1
s

||a||2
our aim would be to make the error at most   ||a||2
f for which we will need to make
s > ||i||2
f /   = n/   which of course is ridiculous since we want s << n. but this was
caused by the fact that i had large (n) rank. a smaller rank    identity-like    matrix might
work better. for this, we will    nd the svd of r, where, r is the r    n matrix consisting
of the sampled rows of a.

||a     a    s i||2

f||i||2
f .

svd of r : r =

  iuivi

t ,

t(cid:88)

i=1

where, t =rank(r), so that all the   i above are positive.

claim 1 the matrix w =(cid:80)t
proof: since v is in the space of v1, v2, . . . , vt, we can write v =(cid:80)t
get (using the orthogonality of vi): w v =(cid:80)

in the row space of r (from the left). i.e., for such v, w v = v.

i   ivi = v as claimed.

i=1 vivi

t acts as the identity when applied to any vector v

i=1   ivi and then we

we will approximate a by

a     aw     a    s w.

the following lemma shows that these approximations are valid;

lemma 1.6

1. e (||a     aw||2)     1   
2. e (||aw     a    s w||2)        
s||a||f .
r   
3. a    s w can be found from the sampled rows and columns of a.

r||a||f .

proof: ||a     aw||2 = |(a     aw )x| for some unit length vector x by de   nition of ||    ||2.
let y be the component of x in the space spanned by v1, v2, . . . , vt and let v = x     y
be the component of x orthogonal to this space. by claim (1), aw y = ay, so that
(a     aw )y = 0. hence, without loss of generality, we may assume x has no component
in the space spanned by the vi. so, w x = 0 and thus (a     aw )x = ax. also, rx = 0.
now imagine approximating the id127 at a by at   ra. since the columns
of at are rows of a, this can be done using the sampled rows of a. so, it follows from
rx = 0 that at    r ax = 0. using this, we get

|ax|2 = xt at ax = xt (at a     at    r a)x     ||at a     at    r a||2.

19

so, ||aw     a||2

2     ||at a     at    r a||2. thus, we have

e(||a     aw||2)    (cid:0)e(||a     aw||2

2)(cid:1)1/2     e||at a     at    r a||2     1

r

||a||2
f ,

by lemma (1.4) proving (1).

for (2), lemma (1.4) implies that e (||aw     a    s w||2)     1   
r||a||f||w||f . now,
f is the sum of the squared lengths of the rows of w which equals trace of (w w t )

||w||2

and since w w t =(cid:80)t

t , we get that trace of w w t is t which proves (2).

i   1 vivi

the proof of (3) is left to the reader. also left to the reader are choices of s, r to make

the error bounds small.

1.3 graph and matrix sparsi   ers
the problem we consider here is the following: we have a n   d matrix a, where, n >> d.
we are to    nd a subset of some r rows of a and multiply each of these r rows by some
scaler to form a r    d matrix b so that for every d   vector v, we have

|bv|       |av|,

where, for two non-negative real numbers a, b, we say b       a i    b     [(1      )a, (1 +   )a]. [in
words, b is an approximation to a with relative error at most   .] so, we wish the length
of bv to be a relative error approximation to length of av for every v. we give two
motivations for this problem before solving it by showing that with r = o(d log d/  4), we
can indeed solve the problem. [in words, essentially o(d log d) rows of a su   ce, however
large n is.]

start with the familiar example of a document term matrix. suppose we have n
documents and d terms, where, n >> d. each document is represented as before as a
d    vector with one component per term. a very general problem we can ask is for a
summary of a so that for every new document (which is itself represented as a d   vector
v), we should be able to compute an approximation to the vector of similarities of v to
each existing document - i.e., the vector av to relative error   , namely, we want a vector
z approximating av in the sense zi       (av)i for every i. we do not know a solution
of this problem at the current state of knowledge. a simpler question is to ask that the
summary be able to approximate |av|2 which is the sum of squared similarities of the
new document to every document in the collection and this is the problem we solve here.
a second important example is that of graphs. suppose we are given a graph g(v, e)

with d vertices and n edges. n could be as large as (cid:0)d

(cid:1) (and no larger). represent the

graph by a signed edge-vertex incidence matrix a. a has one column per vertex and one
row per edge. each row has exactly two non-zero entries - a +1 for one end vertex of the
edge and a -1 for the other end vertex. [we arbitrarily choose which end vertex gets a 1
and which a -1.] for s     v , the cut (s,   s) is the set of edges between s and   s. it is easy
to show that

2

the number of edges in the cut (s,   s) = |a1s|2,

20

where, 1s is the vector with 1 for each i     s and 0 for each i /    s.
[we leave this to
the reader.] so the problem here would    sparsify    the graph - i.e., produce a subset of
o(d log d) edges to form a sub-graph h and weight them so that for every cut, the
weight of the cut in h is a relative error    approximation to the number of edges in the
cut in g.
theorem 1.7 suppose a is any n    d matrix. we can    nd (in polynomial time) subset
of r =    (d log d/  4) rows of a and multiply each by a scaler to form a r    d matrix b so
that

|bv|      

|av|    d     vectors v.

proof: the proof will use length-squared sampling on a matrix a(cid:48) obtained from a by
scaling the rows of a suitably. first, we argue that we cannot do without this scaling.
consider the case of graphs, where, a is the edge-vertex incidence matrix as above. the
graph might have one (or more) edge(s) which must be included in the sparsi   er. a simple
example is the case when g consists of two cliques of size n/2 each connected by a single
edge. so the cut with one of the clique-vertices forming s has one edge and unless we
pick this edge in our subset, we would have 0 weight across the cut which is not correct.
but if we just use uniform sampling, we are quite unlikely to pick this edge among our
sample of d log d edges since there are in total    (d2) edges.
now to the proof of the theorem: first observe that |av|2 = vt (at a)v and similarly
for b. also observe that for any two non-negative real numbers a, b, b2       a2 implies that
b       a. so intuitively, it su   ces to approximate at a. for this, at    r a suggests itself as
a sampling based method, since, as we saw earlier, at     a may be written as bt b where
b consists of some rows of a scaled.

we start with an important fact about length-squared sampling which is stated below.
[the proof is beyond the scope of the book and so we assume the theorem without proof.]
theorem 1.8 for any n    d matrix a, if r        (d log d/  4), then

||at    r a     at a||2       ||a||2
2.

in other words, o(d log d) sampled rows of a are su   cient to approximate at a in spectral
norm to error ||a||2. recalling that at     a can be written as bt b where, b is an r    d
matrix consisting of the r sampled rows, scaled, (see section (1.2.1)), we get vt (bt b)v =
|bv|2 is within   ||a||2
2 of |av|2. but for relative error, we need the error to be at most
  |av|2 and if ||a||2 > |av|2 which is the case in general, we do not get a relative error
result directly.

to achieve relative error, more work is needed. find the svd of a and suppose it is

t(cid:88)

i=1

a =

  iuivi

t ,

t = rank(a).

i=1 uivi. p has all singular values equal to 1, so that |p v| = |v| for all vectors

let p =(cid:80)t

v in the row space of p .

21

apply the theorem now to p to get that for every non-zero vector v in the row space

of p :

||p t    r p     p t p||2       ||p||2 =   

but p = ad, where, d the    psuedo-inverse    of a is d = (cid:80)t

=   vt p t    r p v       vt p t p v.

t . but then,
vt p t    r p v       vt p t p v   v means vt dt at    r adv       vt dt at adv   v. but then d
is non-singular. suppose w is any vector in the span of vi. w can be written as dv with
v = d   1w and from this it follows that

vivi

1
  i

i=1

wt at    r aw       wt at aw

and the theorem follows.

1.4 sketches of documents

suppose one wished to store all the web pages from the www. since there are billions
of web pages, one might store just a sketch of each page where a sketch is a few hundred
bits that capture su   cient information to do whatever task one had in mind. a web page
or a document is a sequence. we begin this section by showing how to sample a set and
then how to convert the problem of sampling a sequence into a problem of sampling a set.

consider subsets of size 1000 of the integers from 1 to 106. suppose one wished to

compute the resemblance of two subsets a and b by the formula

resemblance (a, b) =

|a   b|
|a   b|

suppose that instead of using the sets a and b, one sampled the sets and compared ran-
dom subsets of size ten. how accurate would the estimate be? one way to sample would
be to select ten elements uniformly at random from a and b. however, this method is
unlikely to produce overlapping samples. another way would be to select the ten smallest
elements from each of a and b. if the sets a and b overlapped signi   cantly one might
expect the sets of ten smallest elements from each of a and b to also overlap. one dif-
   culty that might arise is that the small integers might be used for some special purpose
and appear in essentially all sets and thus distort the results. to overcome this potential
problem, rename all elements using a random permutation.

suppose two subsets of size 1000 overlapped by 900 elements. what would the over-
lap be of the 10 smallest elements from each subset? one would expect the nine smallest
elements from the 900 common elements to be in each of the two subsets for an overlap
of 90%. the resemblance(a, b) for the size ten sample would be 9/11=0.81.

22

another method would be to select the elements equal to zero mod m for some inte-
ger m. if one samples mod m the size of the sample becomes a function of n. sampling
mod m allows us to also handle containment.

in another version of the problem one has a sequence rather than a set. here one con-
verts the sequence into a set by replacing the sequence by the set of all short subsequences
of some length k. corresponding to each sequence is a set of length k subsequences. if
k is su   ciently large, then two sequences are highly unlikely to give rise to the same set
of subsequences. thus, we have converted the problem of sampling a sequence to that of
sampling a set. instead of storing all the subsequences, we need only store a small subset
of the set of length k subsequences.

suppose you wish to be able to determine if two web pages are minor modi   cations
of one another or to determine if one is a fragment of the other. extract the sequence
of words occurring on the page. then de   ne the set of subsequences of k consecutive
words from the sequence. let s(d) be the set of all subsequences of length k occurring
in document d. de   ne resemblance of a and b by

and de   ne containment as

resemblance (a, b) =

|s(a)   s(b)|
|s(a)   s(b)|

containment (a, b) =

|s(a)   s(b)|

|s(a)|

let w be a set of subsequences. de   ne min (w ) to be the s smallest elements in w and
de   ne mod (w ) as the set of elements of w that are zero mod m.

let    be a random permutation of all length k subsequences. de   ne f (a) to be the
s smallest elements of a and v (a) to be the set mod m in the ordering de   ned by the
permutation.

then

and

f (a)     f (b)
f (a)     f (b)

|v (a)   v (b)|
|v (a)   v (b)|

are unbiased estimates of the resemblance of a and b. the value

is an unbiased estimate of the containment of a in b.

|v (a)   v (b)|

|v (a)|

23

1.5 exercises

algorithms for massive data problems

exercise 1.1 given a stream of n positive real numbers a1, a2, . . . , an, upon seeing
a1, a2, . . . , ai keep track of the sum a = a1 + a2 +        + ai and a sample aj, j     i drawn
with id203 proportional to its value. on reading ai+1, with id203 ai+1
replace
a+ai+1
the current sample with ai+1 and update a. prove that the algorithm selects an ai from
the stream with the id203 of picking ai being proportional to its value.

exercise 1.2 given a stream of symbols a1, a2, . . . , an, give an algorithm that will select
one symbol uniformly at random from the stream. how much memory does your algorithm
require?

exercise 1.3 give an algorithm to select an ai from a stream of symbols a1, a2, . . . , an
with id203 proportional to a2
i .

exercise 1.4 how would one pick a random word from a very large book where the prob-
ability of picking a word is proportional to the number of occurrences of the word in the
book?

solution: put your    nger on a random word. the probabilities will be automatically
proportional to the number of occurrences, since each occurrence is equally likely to be
picked.

exercise 1.5 for the streaming model give an algorithm to draw s independent samples
each with the id203 proportional to its value. justify that your algorithm works
correctly.

frequency moments of data streams
number of distinct elements in a data stream
lower bound on memory for exact deterministic algorithm
algorithm for the number of distinct elements
universal hash functions

exercise 1.6 show that for a 2-universal hash family prob (h(x) = z) = 1
x     {1, 2, . . . , m} and z     {0, 1, 2, . . . , m}.

m +1 for all

exercise 1.7 let p be a prime. a set of hash functions

h = {h|{0, 1, . . . , p     1}     {0, 1, . . . , p     1}}

is 3-universal if for all u,v,w,x,y, and z in {0, 1, . . . , p     1}

prob (h (x) = u, h (y) = v, h (z) = w) =

1
p3 .

24

(a) is the set {hab(x) = ax + b mod p|0     a, b < p} of hash functions 3-universal?

(b) give a 3-universal set of hash functions.

solution: no. h (x) = u and h (y) = v determine a and b and hence h (z).

exercise 1.8 give an example of a set of hash functions that is not 2-universal.

solution: consider a hash function hi that maps all x   s to the integer i.

h = {hi (x) = i|1     i     m}

is a set of hash functions where every x is equally likely to be mapped to any i in the
range 1 to m by a hash function chosen at random. however, h(x) and h(y) are clearly
not independent.

analysis of distinct element counting algorithm
counting the number of occurrences of a given element.

exercise 1.9

(a) what is the variance of the method in section 1.1.2 of counting the number of occur-

rences of a 1 with log log n memory?

(b) can the algorithm be iterated to use only log log log n memory? what happens to the

variance?

exercise 1.10 consider a coin that comes down heads with id203 p. prove that the
expected number of    ips before a head occurs is 1/p.

solution: let f be the number of    ips before a heads occurs. then

e [f ] = a + 2 (1     a) a + 3 (1     a)2 a +       

(cid:2)(1     a) + 2 (1     a)2 + 3 (1     a)3(cid:3)

= a
1   a
= a
1   a

1   a
a2 = 1

a

exercise 1.11 randomly generate a string x1x2        xn of 106 0   s and 1   s with id203
1/2 of xi being a 1. count the number of ones in the string and also estimate the number
of ones by the approximate counting algorithm. repeat the process for p=1/4, 1/8, and
1/16. how close is the approximation?

25

counting frequent elements
the majority and frequent algorithms
the second moment

exercise 1.12 construct an example in which the majority algorithm gives a false posi-
tive, i.e., stores a nonmajority element at the end.

exercise 1.13 construct examples where the frequent algorithm in fact does as badly as
in the theorem, i.e., it    under counts    some item by n/(k+1).

exercise 1.14 recall basic statistics on how an average of independent trials cuts down

variance and complete the argument for relative error    estimate of

m(cid:80)

s=1

f 2
s .

error-correcting codes, polynomial interpolation and limited-way indepen-
dence

exercise 1.15 let f be a    eld. prove that for any four distinct points a1, a2, a3, and a4
in f and any four (possibly not distinct) values b1, b2, b3, and b4 in f , there is a unique
polynomial f (x) = f0+f1x+f2x2+f3x3 of degree at most three so that f (a1) = b1, f (a2) =
b2, f (a3) = b3 f (a4) = b4 with all computations done over f .

solution: van der monde matrix etc

sketch of a large matrix

exercise 1.16 suppose we want to pick a row of a matrix at random where the id203
of picking row i is proportional to the sum of squares of the entries of that row. how would
we do this in the streaming model? do not assume that the elements of the matrix are
given in row order.

(a) do the problem when the matrix is given in column order.

(b) do the problem when the matrix is represented in sparse notation: it is just presented

as a list of triples (i, j, aij), in arbitrary order.

solution: pick an element aij with id203 proportional to a2
ij. we have already
described the algorithm for this. then return the i (row number) of the element picked.
the id203 of picking one particular i is just the sum of probabilities of picking each
, exactly as desired. note that this works even in the

element of the row which is(cid:80)

ij(cid:80)

a2

sparse representation.

j

k,l

a2
kl

id127 using sampling

26

exercise 1.17 suppose a and b are two matrices. show that ab =

n(cid:80)

k=1

a (:, k)b (k, :).

exercise 1.18 generate two 100 by 100 matrices a and b with integer values between
1 and 100. compute the product ab both directly and by sampling. plot the di   erence
in l2 norm between the results as a function of the number of samples. in generating
the matrices make sure that they are skewed. one method would be the following. first
generate two 100 dimensional vectors a and b with integer values between 1 and 100. next
generate the ith row of a with integer values between 1 and ai and the ith column of b
with integer values between 1 and bi.

approximating a matrix with a sample of rows and columns

exercise 1.19 show that addt b is exactly

1
s

(cid:18) a (:, k1) b (k1, :)
subject to the constraints xk     0 and (cid:80)

a (:, k2) b (k2, :)

pk2

pk1

+

m(cid:80)

k=1

ak
xk

exercise 1.20 suppose a1, a2, . . . , am are nonnegative reals. show that the minimum
of
xk = 1 is attained when the xk are

proportional to

ak.

k

   

(cid:19)

a (:, ks) b (ks, :)

pks

+        +

sketches of documents

exercise 1.21 consider random sequences of length n composed of the integers 0 through
9. represent a sequence by its set of length k-subsequences. what is the resemblance of
the sets of length k-subsequences from two random sequences of length n for various values
of k as n goes to in   nity?

exercise 1.22 what if the sequences in the exercise 1.21 were not random? suppose the
sequences were strings of letters and that there was some nonzero id203 of a given
letter of the alphabet following another. would the result get better or worse?

exercise 1.23 consider a random sequence of length 10,000 over an alphabet of size
100.

(a) for k = 3 what is id203 that two possible successor subsequences for a given

subsequence are in the set of subsequences of the sequence?

(b) for k = 5 what is the id203?

27

solution: 1.23(a) a subsequence has 100 possible successor subsequences. the proba-
bility that a given subsequence is in the set of 104 subsequences of the sequence is 1/100.

thus the answer is(cid:0)1     1

(cid:1)100 = 1

100

e > 0.6

(b) for k = 5 the universe of possible subsequences has grown to 1010 and the prob-
ability of being a possible successor drops to 10   6. (1-10   6)100=0. however, since the
sequence is of length 104, the id203 that some subsequence   s successor is in the set
is (1     10   6)106
exercise 1.24 how would you go about detecting plagiarism in term papers?

= 1
e .

exercise 1.25 (finding duplicate web pages) suppose you had one billion web pages
and you wished to remove duplicates. how would you do this?

solution: :?? suppose web pages are in html. create a window consisting of each se-
quence of 10 consecutive html commands including text. hash the contents of each window
to an integer and save the lowest 10 integers for each page. to detect exact duplicates,
hash the 10 integers to a single integer and look for collisions. if we want almost exact
duplicates we might ask for eight of the ten integers to be the same. in this case hash
the three lowest integers, the next three lowest, and the last four to integers and look
for collisions. if eight of the ten integers agree one of the above three hashes must agree.
not true.

exercise 1.26 construct two sequences of 0   s and 1   s having the same set of subsequences
of width w.

solution: 1.26 for w = 3, 11101111 and 11110111.

exercise 1.27 consider the following lyrics:

when you walk through the storm hold your head up high and don   t be afraid of the
dark. at the end of the storm there   s a golden sky and the sweet silver song of the
lark.
walk on, through the wind, walk on through the rain though your dreams be tossed
and blown. walk on, walk on, with hope in your heart and you   ll never walk alone,
you   ll never walk alone.

how large must k be to uniquely recover the lyric from the set of all subsequences of
symbols of length k? treat the blank as a symbol.

exercise 1.28 blast: given a long sequence a, say 109 and a shorter sequence b, say
105, how do we    nd a position in a which is the start of a subsequence b(cid:48) that is close to
b? this problem can be solved by id145 but not in reasonable time. find
a time e   cient algorithm to solve this problem.
hint: (shingling approach) one possible approach would be to    x a small length, say
seven, and consider the shingles of a and b of length seven. if a close approximation to b
is a substring of a, then a number of shingles of b must be shingles of a. this should allows
us to    nd the approximate location in a of the approximation of b. some    nal algorithm
should then be able to    nd the best match.

28

