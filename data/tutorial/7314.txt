recent developments of  

content-based recsys

marco de gemmis, pasquale lops, cataldo musto, fedelucio narducci, 

giovanni semeraro

department of computer science
university of bari aldo moro, italy

recent developments of  

content-based recsys

introduction

giovanni semeraro

department of computer science
university of bari aldo moro, italy

about us

marco.degemmis@uniba.it

pasquale.lops@uniba.it

cataldo.musto@uniba.it

fedelucio.narducci@uniba.it

giovanni.semeraro@uniba.it

semantic
web 
access and 
personalization 
   antonio bello    research group 
http://www.di.uniba.it/~swap

in this tutorial   

how to represent content

to improve information access and build a 

new generation of services for 

user modeling and 

recommender systems? 

agenda

why?

why do we need intelligent information access?
why do we need content?
why do we need semantics?

how?

how to introduce semantics?
basics of natural language processing
encoding exogenous semantics,i.e. explicit semantics
encoding endogenous semantics, i.e. implicit semantics

what?

explanation of recommendations
serendipity in recommender systems

agenda

why?

why do we need intelligent information access?
why do we need content?
why do we need semantics?

how?

how to introduce semantics?
basics of natural language processing
encoding exogenous semantics,i.e. explicit semantics
encoding endogenous semantics, i.e. implicit semantics

what?

explanation of recommendations
serendipity in recommender systems

physiologically 

impossible 

to follow the information flow 

in real time

we can handle 
126 bits of 
information/second

we deal with 
393 bits of 
information/second

ratio: more than 3x

source: adrian c.ott,
the 24-hour customer, 
harpercollins, 2010

9

information overload

appeared for the first time in 1964 in   the managing of organizations   

by bertram gross, popularized by alvin toffler in his best-seller             

  future shock   (1970)

by vern evans - flickr: alvin toffler 02, cc by-sa 2.0, https://commons.wikimedia.org/w/index.php?curid=12728920

information overload

appeared for the first time in 1964 in   the managing of organizations   by 

bertram gross, popularized by alvin toffler in his best-seller   future shock   

(1970)

information overload

appeared for the first time in 1964 in   the managing of organizations   by 

bertram gross, popularized by alvin toffler in his best-seller   future shock   (1970)

information overload

   it is not 

information 
overload.

it is filter failure   

clay shirky

talk @web2.0 expo

sept 16-19, 2008

challenge

to effectively cope with 

information overload 

bounded rationality 

&

we need to filter the information flow

we need technologies and algorithms for 

intelligent information access

    and we already have some evidence!

intelligent information access

success stories

information retrieval (search engines)

intelligent information access

success stories

information filtering (recommender systems)

agenda

why?

why do we need intelligent information access?
why do we need content?
why do we need semantics?

how?

how to introduce semantics?
basics of natural language processing
encoding exogenous semantics,i.e. explicit semantics
encoding endogenous semantics, i.e. implicit semantics

what?

explanation of recommendations
serendipity in recommender systems

why do we need content?

search engines need content

trivial: search engines can   t work without content

why do we need content?

recommender systems: not trivial!

why do we need content?

recommender systems can work without content

why do we need content?

several recommender systems 
perfectly work using no content!

id185 (cf), matrix 

factorization (mf) and tensor

factorization (tf) are state-of-the-

art techniques for implementing

recommender systems

acm recsys 2009 paper
by netflix challenge winners

why do we need content?

content can tackle some issues of id185

why do we need content?

id185 issues: sparsity

why do we need content?

id185 issues: new item problem

why do we need content?

id185 issues: lack of transparency!

why do we need content?

who knows the   customers who bought this item      ?

information asymmetry

id185 issues: poor explanations!

why do we need content?

27

accurate but obvious

not useful

    content-based recsys suggest items whose scores are high when 

matched against the user profile

    the user is recommended items similar to those already liked in the 

past

    no straight method for finding something unexpected     overspecialization

obviousness of recommendations!

[mcnee06] s.m. mcnee, j. riedl, and j. konstan. accurate is not always good: how accuracy metrics have hurt 

recommender systems. in extended abstracts of the 2006 acm conference on human factors in computing systems, 

pages 1-5, canada, 2006.

27

recap #1

why do we need content?

    in general: to extend and improve user modeling

    to exploit the information spread on social media

    to overcome typical issues of id185

and id105

    because search engines can   t simply work without

content    

agenda

why?

why do we need intelligent information access?
why do we need content?
why do we need semantics?

how?

how to introduce semantics?
basics of natural language processing
encoding exogenous semantics,i.e. explicit semantics
encoding endogenous semantics, i.e. implicit semantics

what?

explanation of recommendations
serendipity in recommender systems

why do we need semantics?

deep rationality requires a deep comprehension of the 
information conveyed by textual content. to achieve that goal 
it is crucial to improve the quality of user profiles and the 
effectiveness of intelligent information access platforms.

basics: content-based recsys (cbrs)

suggest items similar to those the 

user liked in the past 

recommendations generated by matching 

the description of items with the 
profile of the user   s interests

use of specific features

[lops11] p. lops, m. de gemmis, and g. semeraro. content-based recommender systems: state of the art and trends. in 

f. ricci, l. rokach, b. shapira, and p. b. kantor (eds.), recommender systems handbook, springer, 73   105, 2011.

[pazzani07] pazzani, m. j., & billsus, d. content-based id126s. the adaptive web. lecture 

notes in computer science vol. 4321, 325-341, 2007.

basics: content-based recsys (cbrs)

recommendations
are generated by 
matching the 
features stored in 
the user profile with 
those describing the 
items to be 
recommended.

user profile

items

basics: content-based recsys (cbrs)

recommendations
are generated by 
matching the 
features stored in 
the user profile with 
those describing the 
items to be 
recommended.

x

user profile

items

lack of semantics in user models

   i love turkey. it   s my choice 
for these #holidays!

social media can be helpful to avoid cold start

lack of semantics in user models

   i love turkey. it   s my choice 
for these #holidays!

..but pure content-based representations 

can   t handle polysemy

lack of semantics in user models

   i love turkey. it   s my choice 
for these #holidays!

?

pure content-based representations can easily drive a 

recommender system towards failures!

lack of semantics in social media analysis

?

what are people worried about?

are they worried about the eagle 

or about the city of l   aquila?

lack of semantics in user models

   is not only about

doc1

ai is a branch of 

computer science

doc2

the 2011 

international joint 

conference on 

ar tificial

intelligence will 

be held in spain

user profile

artificial

0.11

doc3

intelligence

0.12

apple launches a 

new product   

apple

ai

   

0.20

0.18

?

multi-word concepts

book recommendation

lack of semantics in user models

   is not only about polysemy

doc1

ai is a branch of 

computer science

doc2

the 2011 

international joint 

conference on 

ar tificial

intelligence will 

be held in spain

user profile

artificial

0.11

doc3

intelligence

0.12

apple launches a 

new product   

apple

ai

   

0.20

0.18

most of the preferences regard ai, 

but   apple   is the most relevant

feature in the profile due to 

synonymy

synonymy

?

book recommendation

lack of semantics in cbrs

italian

english

lack of semantics in cbrs

italian-language

english-language 

news about

basketball

news about

basketball

user profile

items

lack of semantics in cbrs

x

italian-language

english-language 

news about

basketball

news about

basketball

user profile

items

it is likely that the 

algorithm is not able

to suggest a 

(relevant) english 
news since there
exist no overlaps

between the 

features!

lack of semantics in cbrs

x

italian-language

english-language 

news about

basketball

news about

basketball

user profile

items

it is likely that the 

algorithm is not able

to suggest a 

(relevant) english 

news since there exist

no overlaps
between the 

features!

recap #2

why do we need semantics?

   

in general: to improve
content representation in 
intelligent information access
platforms

    to avoid typical issues of 

natural language 
representations (polysemy, 
synonymy, multi-word 
concepts, etc.)

    to model user preferences

in an effective way

    to better understand the 

information spread on social 
media

    to provide multilingual

recommendations

because language is

inherently ambiguous

[deg15] m. de gemmis, p. lops, c. musto, f. narducci and g. 

semeraro. semantics-aware content-based recommender systems. 

in f. ricci, l. rokach, and b. shapira (eds.), recommender systems 

handbook, 2nd ed., springer, 119   159, 2015.

recent developments of 

content-based recsys
basics of nlp and exogenous techniques

pasquale lops

department of computer science
university of bari aldo moro, italy

agenda

why?

why do we need intelligent information access?
why do we need content?
why do we need semantics?

how?

how to introduce semantics?
basics of natural language processing
encoding exogenous semantics,i.e. explicit semantics
encoding endogenous semantics, i.e. implicit semantics

what?

explanation of recommendations
serendipity in recommender systems

information retrieval and filtering

two sides of the same coin (belkin&croft,1992)

information 

retrieval

information

filtering 

information need expressed 

information need expressed 

through a query

goal: retrieve information which 
might be relevant to a 

user 

through a 

user profile
goal: expose users to only the 

information that is 

relevant to them, 
according to personal profiles

it   s all about searching!

[belkin&croft, 1992] belkin, nicholas j., and w. bruce croft. 
"information filtering and information retrieval: two sides of the same 
coin?." communications of the acm 35.12 (1992): 29-38.

search (and content-based recommendation) 

is not so simple as it might seem

4

meno   s paradox of inquiry: 

meno: and how will you enquire, socrates, into that
which you do not know? what will you put forth
as the subject of enquiry? and if you find what
you want, how will you know that this is the
thing you did not know?

socrates: i know, meno, what you mean; but just
see what a tiresome dispute you are introducing.
you argue that a man cannot search either
for what he knows or for what he does not
know; if he knows it, there is no need to search;
and if not, he cannot; he does not know the very
subject about which he is to search.

plato meno 80d-81a

http://www.gutenberg.org/etext/1643

meno   s question at our times: 
the    vocabulary mismatch    problem (revisited)

5

how to discover the concepts that connect us to the 

the information we are seeking (search task) or we want 

to be exposed to (recommendation and user modeling 

tasks) ?

meno   s question at our times: 
the    vocabulary mismatch    problem (revisited)

6

how to discover the concepts that connect us to the 

the information we are seeking (search task) or we want 

to be exposed to (recommendation and user modeling 

tasks) ?

we need some   intelligent   support
(as intelligent information access

technologies)

meno   s question at our times: 
the    vocabulary mismatch    problem (revisited)

7

how to discover the concepts that connect us to the 

the information we are seeking (search task) or we want 

to be exposed to (recommendation and user modeling 

tasks) ?

we need some   intelligent   support
(as intelligent information access

we need to better understand
and represent the content

technologies)

meno   s question at our times: 
the    vocabulary mismatch    problem (revisited)

8

how to discover the concepts that connect us to the 

the information we are seeking (search task) or we want 

to be exposed to (recommendation and user modeling 

tasks) ?

we need some   intelligent   support
(as intelligent information access

we need to better understand
and represent the content

technologies)

   before semantics

some basics

of natural language processing (nlp) 

agenda

why?

why do we need intelligent information access?
why do we need content?
why do we need semantics?

how?

how to introduce semantics?
basics of natural language processing
encoding exogenous semantics,i.e. explicit semantics
encoding endogenous semantics, i.e. implicit semantics

what?

explanation of recommendations
serendipity in recommender systems

scenario

pasquale really loves the movie   the matrix  , and he asks a content-based
recommender system for some suggestions.

question

how can we feed the algorithm with some textual features related to the movie 
to build a (content-based) profile and provide recommendations?

recommendation

engine

?

scenario

(wikipedia page)

the plot can be a rich source of content-based features

scenario

(wikipedia page)

the plot can be a rich source of content-based features

   but we need to properly process it through a pipeline of 

natural language processing techniques

basic nlp operations

    id172 strip unwanted characters/markup  (e.g. 

html/xml tags, punctuation, numbers, etc.)

    id121 break text into tokens
    stopword removal exclude common words having 

little semantic content  

    lemmatization reduce inflectional/variant forms to base 

form (lemma in the dictionary), e.g. am, are, is    be

    id30 reduce terms to their    roots   , e.g. automate(s), 

automatic, automation all reduced to automat

vocabulary

example

starring

keanu reeves,

the matrix is a 1999 american-australian neo-noir
science fiction action film written and directed by the
wachowskis,
laurence
fishburne, carrie-anne moss, hugo weaving, and joe
pantoliano. it depicts a dystopian future in which reality
as perceived by most humans is actually a simulated
reality called "the matrix", created by sentient machines
to subdue the human population, while their bodies' heat
and electrical activity are used as an energy source.
computer programmer "neo" learns this truth and is
drawn into a rebellion against
the machines, which
involves other people who have been freed from the
"dream world".

example

x

x

x

x

x

x
x

starring

keanu reeves,

the matrix is a 1999 american-australian neo-noir
science fiction action film written and directed by the
wachowskis,
laurence
x
fishburne, carrie-anne moss, hugo weaving, and joe
x
pantoliano. it depicts a dystopian future in which reality
as perceived by most humans is actually a simulated
reality called "the matrix", created by sentient machines
to subdue the human population, while their bodies' heat
and electrical activity are used as an energy source.
x
computer programmer "neo" learns this truth and is
drawn into a rebellion against
the machines, which
involves other people who have been freed from the
x
"dream world".
x

x

x

x

x

x

x

x

id172

example

the matrix is a 1999 american australian neo noir
science fiction action film written and directed by the
wachowskis starring keanu reeves laurence fishburne
carrie anne moss hugo weaving and joe pantoliano it
depicts a dystopian future in which reality as perceived
by most humans is actually a simulated reality called the
matrix created by sentient machines to subdue the
human population while their bodies heat and electrical
activity are used as an energy source computer
programmer neo learns this truth and is drawn into a
rebellion against
the machines which involves other
people who have been freed from the dream world

id121

id121 issues

compound words

o science-fiction: break up hyphenated sequence?  
o keanu reeves: one token or two?  how do you decide it is one 

token?

numbers and dates

o 3/20/91
o 55 b.c.
o (800) 234-2333

mar. 20, 1991

20/3/91

id121 issues

language issues

o german noun compounds not segmented

lebensversicherungsgesellschaftsangestellter means 
life insurance company employee

o chinese and japanese have no spaces between words (not always 

guaranteed a unique id121)

                                                         

o arabic (or hebrew) is basically written right to left, but with certain items like 

numbers written left to right

algeria achieved its independence in 1962 after 132 years of french 
occupation

example

x

x

x

x

x

x

x x

the matrix is a 1999 american australian neo noir
science fiction action film written and directed by the
x
wachowskis starring keanu reeves laurence fishburne
carrie anne moss hugo weaving and joe pantoliano it
x
depicts a dystopian future in which reality as perceived
by most humans is actually a simulated reality called the
x
x
matrix created by sentient machines to subdue the
x
human population while their bodies heat and electrical
activity are used as an energy source computer
programmer neo learns this truth and is drawn into a
x x
the machines which involves other
rebellion against
x
x
people who have been freed from the dream world
x

xx
x

x x

x
x

x
x

x
x

x
x

x

x

x

x

x

x

x

x

x

x

stopword removal

example

the matrix is a 1999 american australian neo noir
science fiction action film written and directed by the
wachowskis starring keanu reeves laurence fishburne
carrie anne moss hugo weaving and joe pantoliano it
depicts a dystopian future in which reality as perceived
by most humans is actually a simulated reality called the
matrix created by sentient machines to subdue the
human population while their bodies heat and electrical
activity are used as an energy source computer
programmer neo learns this truth and is drawn into a
rebellion against
the machines which involves other
people who have been freed from the dream world

stopword removal

example

the matrix is a 1999 american australian neo noir
science fiction action film written and directed by the
wachowskis starring keanu reeves laurence fishburne
carrie anne moss hugo weaving and joe pantoliano it
depicts a dystopian future in which reality as perceived
by most humans is actually a simulated reality called the
matrix created by sentient machines to subdue the
human population while their bodyies heat and electrical
activity are used as an energy source computer
programmer neo learns this truth and is drawn into a
rebellion against
the machines which involves other
people who have been freed from the dream world

lemmatization

example

matrix 1999 american australian neo noir science fiction
action film write direct wachowskis star keanu reeves
laurence fishburne carrie anne moss hugo weaving
joe pantoliano depict dystopian future reality perceived
human simulate reality call matrix create sentient
machine subdue human population body heat electrical
activity use energy source computer programmer neo
learn truth draw rebellion against machine involve people
free dream world

next step: to give a weight to each feature

(e.g. through tf-idf)

weighting features: tf-idf

terms frequency     inverse document 

frequency best known weighting scheme in information retrieval. 

weight of a term as product of tf weight and idf weight

tf number of times the term occurs in the document

idf depends on rarity of a term in a collection

tf-idf increases with the number of occurrences within a 
document, and with the rarity of the term in the collection.

)df/log()tflog1(w,,tdtndt         example

matrix 1999 american australian neo noir science fiction
action film write direct wachowskis star keanu reeves
laurence fishburne carrie anne moss hugo weaving
joe pantoliano depict dystopian future reality
perceived human simulate reality call matrix create
sentient machine subdue human population body heat
electrical activity use energy
source computer
programmer neo learn truth draw rebellion against
machine involve people free dream world

green=high idf

red=low idf

the matrix representation

a portion of pasquale   s
content-based profile

matrix

1999

american

australian

science

fiction

hugo

   

world

keywords

given a content-based profile, we

can easily build a basic

recommender system through

vector space model and 

similarity measures

vector space model (vsm)

given a set of n features (vocabulary)

f = {f1, f2 ,..., fn} 

given a set of m items, each item i
represented as a point in a 
n-dimensional vector space

i= (wf1,.....wfn) 
wfi is the weight of feature i in the item

basic content-based recommendations

    documents represented as vectors
    features identified through nlp operations
    features weigthed using tf-idf
    cosine measure for computing similarity 

between vectors

similarity between vectors

cosine similarity 

dot product

unit vectors

                                 viiviiviiijijijjiijijiji12121),cos(                              basic content-based recommendations
drawbacks

a portion of pasquale   s
content-based profile

recommendation: 
notre dame de paris, 

by victor hugo

matrix

1999

american

australian

science

fiction

hugo

   

world

why?
entities as   hugo 
weaving   were not
modeled

basic content-based recommendations
drawbacks

a portion of pasquale   s
content-based profile

recommendation: 

the march of penguins

matrix

1999

american

australian

science

fiction

hugo

   

world

why?
more complex concepts
as   science fiction   were
not modeled as single 
features

basic content-based recommendations
vision

basic content-based recommendations
vision

xx

bad recommendations

recap #3

basics of nlp and keyword-based representation

    natural language processing 

techniques necessary to build a 
content-based profile

    basic content-based

recommender systems can be 
easily built through vsm and 
tf-idf

    keyword-based representation

too poor and can drive to bad
modeling of preferences (and 
bad recommendations)

    we need to shift from 

keywords to concepts

agenda

why?

why do we need intelligent information access?
why do we need content?
why do we need semantics?

how?

how to introduce semantics?
basics of natural language processing
encoding exogenous semantics,i.e. explicit semantics
encoding endogenous semantics, i.e. implicit semantics

what?

explanation of recommendations
serendipity in recommender systems

semantic representations

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

top-down 

approaches based on the 
integration of external 

knowledge for 

representing content. able to 

provide the linguistic, 

cultural and backgroud

knowledge in the 

content representation

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

top-down 

approaches based on the 
integration of external 

knowledge for 

representing content. able to 

provide the linguistic, 

cultural and backgroud

knowledge in the 

content representation

bottom-up 

approaches that determine 
the meaning of a word 
by analyzing the rules of its 

usage in the context of 

ordinary and concrete 

language behavior

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

introduce semantics 

by linking 
the item to

a id13

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

introduce semantics 

by linking 
the item to

a id13

word sense 
disambiguation

entity 
linking

      .

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

introduce semantics 

by linking 
the item to 

a id13

ontologies

linked 

open data

      .

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

introduce semantics 

by linking 
the item to 

a id13

distributional 
semantic models

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

introduce semantics 

by linking 
the item to 

a id13

distributional 
semantic models

explicit 

semantic 
analysis

random
indexing

id97

      

agenda

why?

why do we need intelligent information access?
why do we need content?
why do we need semantics?

how?

how to introduce semantics?
basics of natural language processing
encoding exogenous semantics,i.e. explicit semantics
encoding endogenous semantics, i.e. implicit semantics

what?

semantics-aware recommender systems
cross-lingual content-based recommender systems
explanation of recommendations
real-time semantic analysis of social streams

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

introduce semantics 

by linking the 

item to a 

id13

word sense 
disambiguation

entity 
linking

      .

id51 (wsd)
using linguistic ontologies

wsd selects the proper meaning, i.e. sense, for a word in 
a text by taking into account the context in which it occurs

context

apple     computer
apple

iphone

#12567: computer brand

#22999: fruit

sense repository

dictionaries, ontologies, e.g. id138

giovanni semeraro, marco degemmis, pasquale lops, pierpaolo basile: combining learning and word sense 

disambiguation for intelligent user profiling. ijcai 2007: 2856-2861

id51 (wsd)
using linguistic ontologies

wsd selects the proper meaning, i.e. sense, for a word in 
a text by taking into account the context in which it occurs

context

apple     computer
apple

iphone

#12567: computer brand

#22999: fruit

sense repository

dictionaries, ontologies, e.g. id138

giovanni semeraro, marco degemmis, pasquale lops, pierpaolo basile: combining learning and word sense 

disambiguation for intelligent user profiling. ijcai 2007: 2856-2861

sense repository
id138 linguistic ontology [*]

https://id138.princeton.edu

id138 groups words into sets of synonyms called synsets

it contains nouns, verbs, adjectives, adverbs

word 

meanings

word forms

m1

m2

m3

m   

mm

f1

f2

f3

       

fn

v(1,1)

v(2,1)

v(2,2)

v(3,2)

synonym
word forms
(synset)

v(m,n)

polysemous word:
disambiguation needed

[*] miller, george a. "id138: a lexical database for 
english." communications of the acm 38.11 (1995): 39-41.

sense repository
id138 linguistic ontology 

https://id138.princeton.edu

an example of synset

sense repository
id138 linguistic ontology 

https://id138.princeton.edu

id138 hierarchies

id51

state of the art: jigsaw algorithm [*]

input

o d= {w1, w2,    . , wh} document

output

o x= {s1, s2,    . , sk} 

(k   h)

    each si obtained by disambiguating wi based on the context 

of each word

    some words not recognized by id138 
    groups of words recognized as a single concept

[*] basile, p., de gemmis, m., gentile, a. l., lops, p., & semeraro, g. (2007, june). uniba: 
jigsaw algorithm for id51. inproceedings of the 4th international 
workshop on semantic evaluations (pp. 398-401). association for computational linguistics.

jigsaw wsd algorithm

how to use id138 for wsd? 

semantic similarity between synsets inversely 
proportional to their distance in the id138 is-a 
hierarchy
path length similarity between synsets used to assign 
4
scores to synsets of a polysemous word in order to 
choose the correct sense

placental mammal

carnivore

rodent

3

5

mouse
(rodent)

2

feline, felid

1

cat

(feline mammal)

synset semantic similarity

placental mammal

3

4

carnivore

rodent

5

mouse
(rodent)

2

feline, felid

1

cat

(feline mammal)

sinsim(cat,mouse) =
-log(5/32)=0.806

leacock-chodorow similarity

jigsaw wsd algorithm

   the white cat is hunting the mouse   

white

cat

hunt

mouse

w = cat 

c = {mouse}

w
w

={02037721,00847815}
={02037721,00847815}

cat
cat

t={02244530,03651364}
t={02244530,03651364

02037721: feline 

mammal   

02244530: any of 

numerous small 

rodents   

cat

cat

mouse

mouse

00847815: 

03651364: a hand-

computerized axial 

operated electronic 

tomography   

device    

jigsaw wsd algorithm

   the white cat is hunting the mouse   

white

hunt

w = cat 

c = {mouse}

w

cat

={02037721,00847815}

t={02244530,03651364}

02037721: feline 

mammal   

0.806
0.806
0.806

02244530: any of 

numerous small 

rodents   

cat

cat

0.0

0.0

mouse

00847815: 

0.107

03651364: a hand-

computerized axial 

operated electronic 

tomography   

device    

through wsd can we obtain a

semantics-aware representation 

of textual content

synset-based representation

{09596828} american -- (a native or inhabitant of the united states)

{06281561} fiction -- (a literary work based on the imagination and not necessarily on fact)

{06525881} movie, film, picture, moving picture, moving-picture show, motion picture, 
motion-picture show, picture show, pic, flick -- (a form of entertainment that enacts a story    

{02605965} star -- (feature as the star; "the movie stars dustin 
hoffman as an autistic man")

the matrix representation

through wsd we process the textual
description of the item and we obtain a

semantics-aware

representation of the item as

output

keyword-based features replaced

with the concepts (in this
case id138 synsets) they refer to

matrix

1999

american

{06281561} fiction -- (a literary 
work based on the imagination 
and not necessarily on fact)

{06525881} movie, film, picture, 
moving picture, moving-picture 
show, motion picture, 
motion-picture show, picture show, 
pic, flick -- (a form of entertainment 
that enacts a story    

australian

{02605965} star -- (feature as the 
star; "the movie stars dustin 
hoffman as an autistic man")

science

fiction

hugo

   

world

   

{09596828} american -- (a native or 
inhabitant of the united states)

keywords

synsets

the matrix representation

matrix

1999

american

{06281561} fiction -- (a literary 
work based on the imagination 
and not necessarily on fact)

{06525881} movie, film, picture, 
moving picture, moving-picture 
show, motion picture, 
motion-picture show, picture show, 
pic, flick -- (a form of entertainment 
that enacts a story    

australian

{02605965} star -- (feature as the 
star; "the movie stars dustin 
hoffman as an autistic man")

science

fiction

hugo

   

world

   

{09596828} american -- (a native or 
inhabitant of the united states)

keywords

synsets

id51

recap

polysemy and synonymy

effectively handled

classical nlp techniques helpful to 

remove further noise (e.g. 

stopwords)

potentially language-independent

(later)

entities (e.g. hugo weaving) 

still not recognized

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

introduce semantics 

by linking the 

item to a 

id13

word sense 
disambiguation

entity 
linking

      .

entity linking algorithms

    basic idea

   

input: free text

    e.g. wikipedia 

abstract

    output: 

identification of the 
entities
mentioned in the 
text.

why entity linking?

because we need to identify the entities

mentioned in the textual description

to better catch user preferences and information needs.

several state-of-the-art implementations are already available

    and many more

entity linking algorithms
opencalais

http://www.opencalais.com/opencalais-api/

the matrix representation

matrix

1999

american

{06281561} fiction -- (a literary 
work based on the imagination 
and not necessarily on fact)

{06525881} movie, film, picture, 
moving picture, moving-picture 
show, motion picture, 
motion-picture show, picture show, 
pic, flick -- (a form of entertainment 
that enacts a story    

australian

{02605965} star -- (feature as the 
star; "the movie stars dustin 
hoffman as an autistic man")

neo

science

fiction

   

world

   

   

{09596828} american -- (a native or 
inhabitant of the united states)

keywords

synsets

entities

the matrix representation

matrix

1999

american

{06281561} fiction -- (a literary 
work based on the imagination 
and not necessarily on fact)

{06525881} movie, film, picture, 
moving picture, moving-picture 
show, motion picture, 
motion-picture show, picture show, 
pic, flick -- (a form of entertainment 
that enacts a story    

australian

{02605965} star -- (feature as the 
star; "the movie stars dustin 
hoffman as an autistic man")

neo

science

fiction

   

world

   

   

{09596828} american -- (a native or 
inhabitant of the united states)

keywords

synsets

entities

entities are correctly
recognized and modeled

partially multilingual
(entities are inherently multilingual, 

but other concepts aren   t)

common sense and abstract

concepts now ignored.

entity linking algorithms
tag.me

https://tagme.d4science.org/tagme/

output

very transparent and human-readable content representation
non-trivial nlp tasks automatically performed
(stopwords removal, id165s identification, named entities recognition and 

disambiguation)

entity linking algorithms
tag.me

https://tagme.d4science.org/tagme/

output

each entity identified in the content can be a feature of a 

semantics-aware content representation 

based on entity linking

entity linking algorithms
tag.me

https://tagme.d4science.org/tagme/

output

advantage #1: several common sense

concepts are now identified

entity linking algorithms
tag.me

https://tagme.d4science.org/tagme/

output

advantage #2: each entity is a reference

to a wikipedia page

http://en.wikipedia.org/wiki/the_wachowskis

not a simple textual feature!

entity linking algorithms
tag.me + wikipedia categories

https://tagme.d4science.org/tagme/

we can enrich this entity-based representation 

by exploiting the wikipedia categories    tree

entity linking algorithms
tag.me + wikipedia categories

https://tagme.d4science.org/tagme/

features =

entities

+

wikipedia categories

final representation

of items obtained by 
merging entities

identified in the text with 
the (most relevant) 

wikipedia 

categories each 

entity is linked to

the matrix representation

matrix

1999

american

{06281561} fiction -- (a literary 
work based on the imagination 
and not necessarily on fact)

{06525881} movie, film, picture, 
moving picture, moving-picture 
show, motion picture, 
motion-picture show, picture show, 
pic, flick -- (a form of entertainment 
that enacts a story    

australian

{02605965} star -- (feature as the 
star; "the movie stars dustin 
hoffman as an autistic man")

neo

science

fiction

   

world

   

   

{09596828} american -- (a native or 
inhabitant of the united states)

keywords

synsets

wikipedia pages

the matrix representation

matrix

1999

american

{06281561} fiction -- (a literary 
work based on the imagination 
and not necessarily on fact)

{06525881} movie, film, picture, 
moving picture, moving-picture 
show, motion picture, 
motion-picture show, picture show, 
pic, flick -- (a form of entertainment 
that enacts a story    

australian

{02605965} star -- (feature as the 
star; "the movie stars dustin 
hoffman as an autistic man")

neo

science

fiction

   

world

entities recognized and 
modeled (as in opencalais)

wikipedia-based representation: 

some common sense terms
included, and new interesting

features (e.g.   science-fiction film 

director  ) can be generated

   

   

{09596828} american -- (a native or 
inhabitant of the united states)

terms without a wikipedia 

mapping are ignored

keywords

synsets

wikipedia pages

entity linking algorithms
babelfy

http://babelfy.org/

traditional 
resources

collaborative 

resources

o manually curated by experts
o available for a few languages
o difficult to maintain and update

o collaboratively built by the crowd
o highly multilingual
o up-to-date

entity linking algorithms
babelfy

http://babelfy.org/

we have both named entities and concepts!

entity linking algorithms
babelfy

http://babelfy.org/

entity linking algorithms
babelfy

http://babelfy.org/

the matrix representation

matrix

1999

american

{06281561} fiction -- (a literary 
work based on the imagination 
and not necessarily on fact)

{06525881} movie, film, picture, 
moving picture, moving-picture 
show, motion picture, 
motion-picture show, picture show, 
pic, flick -- (a form of entertainment 
that enacts a story    

australian

{02605965} star -- (feature as the 
star; "the movie stars dustin 
hoffman as an autistic man")

neo

science

fiction

   

world

   

   

{09596828} american -- (a native or 
inhabitant of the united states)

keywords

synsets

babel synsets

the matrix representation

matrix

1999

american

{06281561} fiction -- (a literary 
work based on the imagination 
and not necessarily on fact)

{06525881} movie, film, picture, 
moving picture, moving-picture 
show, motion picture, 
motion-picture show, picture show, 
pic, flick -- (a form of entertainment 
that enacts a story    

australian

{02605965} star -- (feature as the 
star; "the movie stars dustin 
hoffman as an autistic man")

neo

science

fiction

   

world

entities recognized and 
modeled (as in opencalais

and tag.me)

wikipedia-based representation: 

some common sense terms
included, and new interesting
features (e.g.   science-fiction 

director) can be generated

   

   

includes linguistic knowledge
and is able to disambiguate terms

{09596828} american -- (a native or 
inhabitant of the united states)

also multilingual!

keywords

synsets

babel synsets

recap #4

encoding exogenous semantics

by processing textual descriptions

o   exogenous   techniques use 

external knowledge sources to inject
semantics

o id51
algorithms process the textual
description and replace keywords with 
semantic concepts (as synsets)

o entity linking algorithms focus on 

the identification of the entities. some 
recent approaches also able to identify
common sense terms

o combination of both

approaches is potentially the 
best strategy

results in a cultural heritage scenario

cultural heritage fruition & e-learning applications 

of new advanced (multimodal) technologies

in the context of cultural heritage personalization, does the 
integration of ugc and textual description of artwork 
collections cause an increase of the prediction accuracy in the 
process of recommending artifacts to users?

m. de gemmis, p. lops, g. semeraro, and p. basile. integrating tags in a semantic content-based recommender. 

in recsys    08, proceed. of the 2nd acm conference on recommender systems, pages 163   170, october 23-25, 2008, 

lausanne, switzerland, acm, 2008. 

results in a cultural heritage scenario

textual description of 
items (static content)

social tags (from other users): caravaggio, deposition, christ, cross, suffering, religion

passion

5-point rating scale

personal tags

social tags

results in a cultural heritage scenario

user profile

caravaggio, deposition, 

cross, christ, rome,    

passion

caravaggio, deposition, 

christ, cross, suffering, 

religion,    

static 
content

personal 

tags

social tags

collaborative part of 

the user profile

results in a cultural heritage scenario

o artwork representation

o artist
o title
o description
o tags

o change of text representation from vectors of words (bow) into 

vectors of id138 synsets (bos)
o from tags to semantic tags

o supervised learning

o bayesian classifier learned from artworks labeled with user 

ratings and tags

results in a cultural heritage scenario

 

d
e
s
a
b
-
t
n
e
t
n
o
c

 

d
e
s
a
b
-
g
a
t

s
e

l
i
f
o
r
p

s
e

l
i
f
o
r
p

 

d
e
t
n
e
m
g
u
a

s
e

l
i
f
o
r
p

type of content

precision*

recall*

f1*

exp#1: static content

75.86

94.27

84.07

exp#2: personal tags

75.96

92.65

83.48

exp#3: social tags

75.59

90.50

82.37

exp#4: static content + personal tags

78.04

93.60

85.11

exp#5: static content + social tags

78.01

93.19

84.93

* results averaged over the 30 study subjects

overall accuracy f1     85%

recent developments of 
content-based recsys 

id65 

 

fedelucio narducci 

department of computer science 
university of bari aldo moro, italy 

agenda 

why? 

why do we need intelligent information access? 
why do we need content? 
why do we need semantics? 

how? 

how to introduce semantics? 
basics of natural language processing 
encoding exogenous semantics,i.e. explicit semantics 
encoding endogenous semantics, i.e. implicit semantics 

what? 

explanation of recommendations 
serendipity in recommender systems 

insight 

very huge availability of textual content 

insight 

we can use this huge amount of content to 

directly learn a representation of words 

insight 
pass me a peroni! 

i like peroni 

football and peroni, what a perfect saturday! 

what is   peroni   ? 

insight 

pass me a budweiser! 

i like budweiser 

football and budweiser, what a perfect saturday! 

what is   budweiser   ? 

insight 

pass me a budweiser! 

i like budweiser 

football and budweiser, what a perfect saturday! 

what is   budweiser   ? 

insight 
pass me a peroni! 

i like peroni 

football and peroni, what a perfect saturday! 

what is   peroni   ? 

the most famous beer in bari ! 

insight 

the semantics learnt according to  
term usage is called   distributional   

insight 

distributional hypothesis 

  terms used in similar contexts  

share a similar meaning   

id65 

meaning of a word is 

determined by its 

usage 

ludwig wittgenstein 
(austrian philosopher) 

id65 

definition 

by analyzing large corpora of 
textual data it is possible to 
infer information about the 
usage (about the meaning) of 
the terms 

(*) firth, j.r. a synopsis of linguistic theory 
1930-1955. in studies in linguistic analysis, 
pp. 1-32, 1957. 

similar meanings 

id65 

definition 

by analyzing large corpora of 
textual data it is possible to 
infer information about the 
usage (about the meaning) of 
the terms 

(*) firth, j.r. a synopsis of linguistic theory 
1930-1955. in studies in linguistic analysis, 
pp. 1-32, 1957. 

similar meanings 

beer and wine, dog and cat share a similar meaning 

since they are often used in similar contexts 

id65 

term-context matrix 

c1  c2  c3  c4  c5  c6  c7  c8  c9 

         
         

beer 
wine 
spoon      
glass                

         
              

    

    

         
    

a vector-space representation is learnt  

by encoding in which context each term is used 

each row of the matrix is a vector 

 

id65 

term-contexts matrix 
c1  c2  c3  c4  c5  c6  c7  c8  c9 

         
         

beer 
wine 
spoon      
glass                

         
              

    

    

         
    

beer vs wine: good overlap 
similar! 

id65 

term-contexts matrix 
c1  c2  c3  c4  c5  c6  c7  c8  c9 

         
         

beer 
wine 
spoon      
glass                

         
              

    

    

         
    

beer vs spoon: no overlap 
not similar! 

wordspace 

beer 
wine 

mojito 

dog 

a vector space representation (called wordspace) 

is learnt according to terms usage in contexts 

wordspace 

terms sharing a 
similar usage 
are very close 
in the space 

beer 
wine 

mojito 

dog 

a vector space representation (called wordspace) 

is learnt according to terms usage in contexts 

id65 

term-context matrix 

c1  c2  c3  c4  c5  c6  c7  c8  c9 

         
         

beer 
wine 
spoon      
glass                

         
              

    

    

         
    

key question: what is the context? 

id65 

term-context matrix 

c1  c2  c3  c4  c5  c6  c7  c8  c9 

         
         

beer 
wine 
spoon      
glass                

         
              

    

    

         
    

key question: what is the context? 

these approaches are very flexible since the   context   can 

be set according to the granularity required by  the 

representation 

id65 

term-context matrix 

d1  d2  d3  d4  d5  d6  d7  d8  d9 

         
         

beer 
wine 
spoon      
glass                

         
              

    

    

         
    

key question: what is the context? 

coarse-grained granularity:  
context=whole document 

id65 
term-context matrix = term-document matrix 
d1  d2  d3  d4  d5  d6  d7  d8  d9 

         
         

beer 
wine 
spoon      
glass                

         
              

    

    

         
    

key question: what is the context? 

(this is vector space model!) 

vector space model is a distributional model 

id65 

term-contexts matrix 
c1  c2  c3  c4  c5  c6  c7  c8  c9 

         
         

beer 
wine 
spoon      
glass                

         
              

    

    

         
    

key question: what is the context? 

fine-grained granularities:  

context=paragraph, sentence, window of words 

id65 

term-contexts matrix 
c1  c2  c3  c4  c5  c6  c7  c8  c9 

         
         

beer 
wine 
spoon      
glass                

         
              

    

    

         
    

fine-grained granularities:  

 

pros: the more fine-grained the representation, more precise the vectors 

cons: the more fine-grained the representation, the bigger the matrix 

id65 

term-contexts matrix 
c1  c2  c3  c4  c5  c6  c7  c8  c9 

         
         

         
              

beer 
wine 
spoon      
glass                
the flexibility of id65 models  

         
    

    

    

also regards the rows of the matrix 

id65 

term-contexts matrix 
c1  c2  c3  c4  c5  c6  c7  c8  c9 

         
         

concept1 
concept2 
concept3      
concept4                

         
              

    

    

         
    

the flexibility of id65 models  

also regards the rows of the matrix 

keywords can be replaced with concepts  

(as synsets or entities!) 

id65 

keanu reeves 

term-contexts matrix 
c1  c2  c3  c4  c5  c6  c7  c8  c9 
    

    

    

         
    

    

al pacino 
writers      
american 
fishburne      
the flexibility of id65 models  

         
    

laurence 

    

    

    

also regards the rows of the matrix 

keywords can be replaced with concepts  

(as synsets or entities!) 

id65 

term-contexts matrix 
c1  c2  c3  c4  c5  c6  c7  c8  c9 
    

    

    

         
    

keanu reeves 

    

laurence 

al pacino 
writers      
american 
fishburne      
keanu reeves and al pacino 
are   connected   because they 

    

    

both acted in drama films 

    

         
    

id65 

representing documents 
c1  c2  c3  c4  c5  c6  c7  c8  c9 
    

    

    

keanu reeves 

    

laurence 

al pacino 
writers      
american 
fishburne      
given a wordspace, a vector space representation of 
documents (called docspace) is typically built as the 

         
    

    

    

    

centroid vector of word representations 

         
    

id65 

representing documents 
c1  c2  c3  c4  c5  c6  c7  c8  c9 
    

    

    

keanu reeves 

         
    

al pacino 
writers      
american 
fishburne      

laurence 

    

    

    

    

         
    

the matrix                               

         

docspace 

the matrix 

matrix revolutions 

donnie darko 

up! 

it is possible to 
perform similarity 
calculations 
between items 
according to their 
semantic 
representation 

given a wordspace, a vector space representation of 
documents (called docspace) is typically built as the 

centroid vector of word representations 

id65 
       we can exploit the (big) corpora of 
data to directly learn a semantic 
vector-space representation of 
the terms of a language 

       lightweight semantics, not 
       high flexibility: everything is a 

formally defined 

vector: term/term similarity, doc/term, 
term/doc, etc.. 

granularities 

       context can have different 
       huge amount of content is needed 
       matrices are particularly huge and 

difficult to build 
       too many features: need for 

id84 

semantic representations 

explicit (exogenous) 

semantics 

implicit (endogenous) 

semantics 

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts 

introduce semantics  

by linking the  

item to a  

id13 

distributional 
semantic models 

id65 
models share the same 
insight but have important 
distinguishing aspects  

explicit 
semantic 
analysis 

random 
indexing 

id97 

       

semantic representations 

explicit (exogenous) 

semantics 

implicit (endogenous) 

semantics 

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts 

introduce semantics  

by linking the  

item to a  

id13 

distributional 
semantic models 

id65 
models share the same 
insight but have important 
distinguishing aspects  

explicit 
semantic 
analysis 

random 
indexing 

id97 

       

explicit semantic analysis (esa) 

35 

esa matrix 

 

esa is a distributional 
semantic model which 

uses wikipedia 

articles as context 

explicit semantic analysis (esa) 

36 

esa matrix 

 

esa is a distributional 
semantic model which 

uses wikipedia 

articles as context 

wikipedia articles 

 
s
m
r
e
t

esa 
term 1 

    

term k 

context 1 

tf-idf 

tf-idf 

tf-idf 

    

tf-idf 

tf-idf 

tf-idf 

context n 

tf-idf 

tf-idf 

tf-idf 

explicit semantic analysis (esa) 

37 

esa matrix 

 

semantic relatedness  
between a word and a context 
tf-idf score 
 

wikipedia articles 

 
s
m
r
e
t

esa 
term 1 

    

term k 

context 1 

tf-idf 

tf-idf 

tf-idf 

    

tf-idf 

tf-idf 

tf-idf 

context n 

tf-idf 

tf-idf 

tf-idf 

explicit semantic analysis (esa) 

38 

esa matrix 

 

semantic relatedness  
between a word and a context 
tf-idf score 
 

wikipedia articles 

 
s
m
r
e
t

esa 
term 1 

    

term k 

context 1 

tf-idf 

tf-idf 

tf-idf 

    

tf-idf 

tf-idf 

tf-idf 

context n 

tf-idf 

tf-idf 

tf-idf 

explicit semantic analysis (esa) 

every wikipedia article represents a concept 

explicit semantic analysis (esa) 

every wikipedia article represents a concept 

article words are associated with the concept (tf-idf) 

each wikipedia page can be described in terms of 
the words with the highest tf-idf score 

panthera 

cat [0.92] 

leopard [0.84] 

roar [0.77] 

(this is a 

column of esa 

matrix) 

explicit semantic analysis (esa) 

41 

esa 
term 1 

    

term k 

panthera 

tf-idf 

tf-idf 

tf-idf 

    

tf-idf 

tf-idf 

tf-idf 

concept n 

tf-idf 

tf-idf 

tf-idf 

we iterate the process over (almost) all the wikipedia pages and we obtain 

the so-called esa matrix 

explicit semantic analysis (esa) 

42 

esa matrix 

 

esa 
term 1 

    

term k 

panthera 

tf-idf 

tf-idf 

tf-idf 

    

tf-idf 

tf-idf 

tf-idf 

concept n 

tf-idf 

tf-idf 

tf-idf 

each row of the esa matrix is called 

semantic interpretation vector 

(of a term t) 

 

explicit semantic analysis (esa) 

semantic interpretation vector of the term    cat    

(wikipedia articles are ranked in a descending order) 

cat 

cat 

[0.95] 

panthera 
[0.92] 

jane 
fonda 
[0.07] 

explicit semantic analysis (esa) 

semantic interpretation vector of the term    cat    

(wikipedia articles are ranked in a descending order) 

cat 

cat 

[0.95] 

panthera 
[0.92] 

jane 
fonda 
[0.07] 

the semantics of a word is the vector of its 

associations with wikipedia concepts. 

explicit semantic analysis (esa) 

semantic interpretation vector of the term    cat    

(wikipedia articles are ranked in a descending order) 

cat 

cat 

[0.95] 

panthera 
[0.92] 

jane 
fonda 
[0.07] 

the semantics of a word is the vector of its 

associations with wikipedia concepts. 

the highest the score, the more the strength of its 
   semantic connection    with a wikipedia concept 

explicit semantic analysis (esa) 

semantic interpretation vector of text fragment 

(e.g.    mouse button   ) 

is the centroid vector of the terms in the fragment 

mouse 

mouse 
rodent 
[0.91] 

mouse 
computing 
[0.89] 

mickey 
mouse 
[0.81] 

john 

steinbeck 
[0.17] 

button 

centroid vector 

mouse button 

button 
[0.93] 

dick 
button 
[0.84] 

mouse 
computing 
[0.81] 

game 

controller 
[0.32] 

mouse 
button 
computing 
[0.95] 
[0.85] 

dick 
mouse 
rodent 
button 
[0.46] 
[0.92] 

ibm ps/2 
[0.35] 

drag and 

drop 
[0.32] 

explicit semantic analysis (esa) 

  

a semantic representation of an item can be built as the 
centroid vector of the semantic interpretation vectors of 

the terms in the item description 

explicit semantic analysis (esa) 

  

a semantic representation of an item can be built as the 
centroid vector of the semantic interpretation vectors of 

the terms in the item description 

explicit semantic analysis (esa) 

 semantic 
relatedness 
of a pair of text fragments 
(e.g. description of two 

items) computed by 

comparing their 
semantic 

interpretation 
vectors using the 
cosine metric 

the matrix 

matrix revolutions 

donnie darko 

up! 

explicit semantic analysis (esa) 

  

 

another advantage: esa can be also used  

as a feature generation technique 

how can we generate a set of relevant extra  

concepts describing the items? 

explicit semantic analysis (esa) 

  

 

another advantage: esa can be also used  

as a feature generation technique 

how can we generate a set of relevant extra  

concepts describing the items? 

given an item, we first generate its semantic 

interpretation vector 

explicit semantic analysis (esa) 

  

 

another advantage: esa can be also used  

as a feature generation technique. 

how can we generate a set of relevant extra  

concepts describing the items? 

given an item, we first generate its semantic 

interpretation vector 

explicit semantic analysis (esa) 

  

 

another advantage: esa can be also used  

as a feature generation technique. 

how can we generate a set of relevant extra  

concepts describing the items? 

given an item, we first generate its semantic 

interpretation vector 

the matrix 

matrix 
(reloaded) 
[0.91] 

keanu 
reeves 
[0.77] 

ar#   cial	

intelligence	

[0.61]	

turing 
machin
e (0.52] 

explicit semantic analysis (esa) 

  

 

another advantage: esa can be also used  

as a feature generation technique. 

how can we generate a set of relevant extra  

concepts describing the items? 

the pages with the highest tf/idf score in the semantic 

interpretation vector are the most related concepts 

the matrix 

matrix 
(reloaded) 
[0.91] 

keanu 
reeves 
[0.77] 

ar#   cial	

intelligence	

[0.61]	

turing 
machin
e (0.52] 

explicit semantic analysis (esa) 

  

 

another advantage: esa can be also used  

as a feature generation technique. 

how can we generate a set of relevant extra  

concepts describing the items? 

the pages with the highest tf/idf score in the semantic 

interpretation vector are the most related concepts 

extra features 
related to the 

item  

esa effectively used for 

56 

 text categorization [gabri09] 
 experiments on diverse datasets 

 

  

 semantic relatedness of  
 words and texts [gabri09] 
 cosine similarity between vectors of esa concepts 

 
 

 
 

 information retrieval [egozi08, egozi11] 
 esa-based ir algorithm enriching documents and queries 

 
what about esa for information filtering? 

[gabri09] e. gabrilovich and s. markovitch. wikipedia-based semantic interpretation for natural language processing. journal of artificial 
intelligence research 34:443-498, 2009. 
[egozi08] ofer egozi, evgeniy gabrilovich, shaul markovitch: concept-based feature generation and selection for information retrieval. 
aaai 2008, 1132-1137, 2008. 
[egozi11] ofer egozi, shaul markovitch, evgeniy gabrilovich. concept-based information retrieval using explicit semantic analysis.  
acm transactions on information systems 29(2), april 2011.  

information filtering using esa 

57 

german electronic program guides (epg) 

tv-domain 

 

problem 

description of tv shows too short or  

poorly meaningful to feed a  

content-based recommendation algorithm  

 

solution 

explicit semantic analysis exploited to obtain an 

enhanced representation  

[musto12] c. musto, f. narducci, p. lops, g. semeraro, m. de gemmis, m. barbieri, j. h. m. korst, v. pronk, and r. clout. enhanced semantic tv-show 
representation for personalized electronic program guides. umap 2012, pp. 188   199. springer, 2012 

electronic program guides 

58 

[musto12] c. musto, f. narducci, p. lops, g. semeraro, m. de gemmis, m. barbieri, j. h. m. korst, v. pronk, and r. clout. 
enhanced semantic tv-show representation for personalized electronic program guides. umap 2012, pp. 188   199. springer, 2012 

electronic program guides 

59 

[musto12] c. musto, f. narducci, p. lops, g. semeraro, m. de gemmis, m. barbieri, j. h. m. korst, v. pronk, and r. clout. 
enhanced semantic tv-show representation for personalized electronic program guides. umap 2012, pp. 188   199. springer, 2012 

electronic program guides 

wikipedia articles related to the 

tv show are added to the 

description 

60 

[musto12] c. musto, f. narducci, p. lops, g. semeraro, m. de gemmis, m. barbieri, j. h. m. korst, v. pronk, and r. clout. 
enhanced semantic tv-show representation for personalized electronic program guides. umap 2012, pp. 188   199. springer, 2012 

electronic program guides 

61 

user profile 

tv show 

motogp 
sports 

motorbike 

... 

competition 

2012 superbike 
italian grand prix 

[musto12] c. musto, f. narducci, p. lops, g. semeraro, m. de gemmis, m. barbieri, j. h. m. korst, v. pronk, and r. clout. 
enhanced semantic tv-show representation for personalized electronic program guides. umap 2012, pp. 188   199. springer, 2012 

electronic program guides 

62 

tv show 

italian grand prix xno matching! 

2012 superbike 

user profile 

motogp 
sports 

motorbike 

... 

competition 

[musto12] c. musto, f. narducci, p. lops, g. semeraro, m. de gemmis, m. barbieri, j. h. m. korst, v. pronk, and r. clout. 
enhanced semantic tv-show representation for personalized electronic program guides. umap 2012, pp. 188   199. springer, 2012 

electronic program guides 

63 

user profile 

tv show 

motogp 
superbike 

sports 

motorbike 
formula 1 

    

competition 

2012 superbike 
italian grand prix 

through esa we can 
add new features to 
the profile and we 
can improve the 
overlap between 
textual description 

[musto12] c. musto, f. narducci, p. lops, g. semeraro, m. de gemmis, m. barbieri, j. h. m. korst, v. pronk, and r. clout. 
enhanced semantic tv-show representation for personalized electronic program guides. umap 2012, pp. 188   199. springer, 2012 

electronic program guides 

64 

user profile 

motogp 
superbike 

sports 

motorbike 
formula 1 

    

competition 

matching! 

tv show 

   

2012 superbike 
italian grand prix 

[musto12] c. musto, f. narducci, p. lops, g. semeraro, m. de gemmis, m. barbieri, j. h. m. korst, v. pronk, and r. clout. 
enhanced semantic tv-show representation for personalized electronic program guides. umap 2012, pp. 188   199. springer, 2012 

electronic program guides 

results on aprico.tv data 

65 

95 

89.75 

84.5 

79.25 

bow 

ebow (+20) 

ebow (+40) 

ebow (+60) 

ebow = bag of words + wikipedia concepts 

74 

p@5% 

p@10% 

p@25% 

p@50% 

p@75% 

p@100% 

the more wikipedia concepts are added to the textual description  
of the items (ebow+60), the best the precision of the algorithm 

explicit semantic analysis (esa) 

distributional model which uses 
wikipedia article as context 

 
 

very transparent representation 
(columns have an explicit meaning) 

can be used as a  

feature generation tool 

the whole matrix is very huge 

  empirical   tuning of the parameters: 
how many articles? how many terms? 

what is the thresholding? 

 

 

recent developments of 

content-based recsys

endogenous approaches: random indexing & id97

cataldo musto

department of computer science
university of bari aldo moro, italy

dimensions

are important.

when transparency is not so important, 
it is possible to learn a more compact 

vector-space representation of terms and items

when transparency is not so important, 
it is possible to learn a more compact 

vector-space representation of terms and items

id84 techniques

when transparency is not so important, 
it is possible to learn a more compact 

vector-space representation of terms and items

a.k.a. id27 techniques

embedding = a smaller representation of words

is this new?

id84 techniques

latent semantic analysis (lsa) is a widespread

id65 model which builds

a term/context matrix and calculates svd over that matrix.

truncated singular value decomposition

dumais, susan t. "latent semantic 
analysis." annual review of information science 
and technology 38.1 (2004): 188-230.

induces higher-order (paradigmatic) relations through the truncated svd

id84 techniques

singular value decomposition

problem

the huge co-occurrence matrix

solution

don   t build the huge co-occurrence matrix!

use incremental and scalable techniques

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

introduce semantics 

by linking the 

item to a 

id13

distributional 
semantic models

explicit 

semantic 
analysis

random
indexing

id97

      

id84

random indexing

it is an incremental and scalable technique

for id84. 

m. sahlgren. the word-space model: using distributional analysis to represent syntagmatic and paradigmatic relations 
between words in high-dimensional vector spaces. phd thesis, stockholm university, 2006.

id84

random indexing

it is an incremental and scalable technique

for id84. 

insight

    assign a vector to each context (word, documents, etc.). the 

vector can be as big as you want. 

    fill the vector with (almost) randomly assigned values. 
    given a word, collect the contexts where that word appears.
    sum the context and obtain the final representation of the word
    the resulting representation is a smaller but (almost) equivalent

to the original one

m. sahlgren. the word-space model: using distributional analysis to represent syntagmatic and paradigmatic relations 
between words in high-dimensional vector spaces. phd thesis, stockholm university, 2006.

id84

random indexing

it is an incremental and scalable technique

for id84. 

?

insight

    assign a vector to each context (word, documents, etc.). the 

vector can be as big as you want. 

    fill the vector with (almost) randomly assigned values. 
    given a word, collect the contexts where that word appears.
    sum the context and obtain the final representation of the word
    the resulting representation is a smaller but (almost) equivalent

to the original one

m. sahlgren. the word-space model: using distributional analysis to represent syntagmatic and paradigmatic relations 
between words in high-dimensional vector spaces. phd thesis, stockholm university, 2006.

random indexing

algorithm

step 1 - definition of the context granularity:

document? paragraph? sentence? word?

step 2     building the random matrix r 

each    context    (e.g. sentence) is assigned a
context vector

    dimension = k 

    allowed values = {-1, 0, +1}

    values distributed in a random way

    small # of non-zero elements, i.e. sparse vectors

random indexing

context vectors of dimension k = 8

r

1 

r

2

r

3

r

4

r

5

0,

0,

-1, 1,

0,

0,

0,

0

1,

0,

0,

0,

0,

0,

0,

-1

0,

0,

0,

0,

0,

-1, 1,

-1, 1,

0,

0,

0,

0,

0,

1,

0,

0,

-1, 1,

0,

0,

0

0

0

r

n

   

each row is a   context  

random indexing

algorithm

step 3    the vector space representation of a term t is 
obtained by combining the random vectors of the context in 
which it occurs in

r

1 

r

2

r

3

r

4

r

5

   

r

n

0,

0,

-1,

1,

0,

0,

0,

0

1,

0,

0,

0,

0,

0,

0,

-1

0,

0,

0,

0,

0,

-1,

1,

0

-1,

1,

0,

0,

0,

0,

0,

0

1,

0,

0,

-1,

1,

0,

0,

0

   

random indexing

algorithm

step 3     building the representation for t1

t1     {c1, c2, c5}

0,

1,

0,

-1,

1,

0,

0,

0,

1,

0,

-1,

0,

0,

0,

0,

1,

0,

0,

0,

-1,

0,

0,

0,

0,

1,

0,

0,

-1,

0,

0,

0,

0,

1,

0,

0,

0

-1

0

0

0

r1 

r2

r3

r4

r5

   

rn    

random indexing

algorithm

step 3     building the representation for t1

0,

1,

0,

-1,

1,

0,

0,

0,

1,

0,

-1,

0,

0,

0,

0,

1,

0,

0,

0,

-1,

0,

0,

0,

0,

1,

0,

0,

-1,

0,

0,

0,

0,

1,

0,

0,

0

-1

0

0

0

r1 

r2

r3

r4

r5

   

rn    

t1     {c1, c2, c5}

r1 

r2

r5

t1

0,

1,

1,

2,

0,

0,

0,

0,

-1,

0,

0,

1,

0,

-1,

-1,

0,

0,

0,

1,

1,

0,

0,

0,

0,

0,

0,

0,

0,

0

-1

0

-1

+

+

+

random indexing

algorithm

step 3     building the representation for t1

0,

1,

0,

-1,

1,

0,

0,

0,

1,

0,

-1,

0,

0,

0,

0,

1,

0,

0,

0,

-1,

0,

0,

0,

0,

1,

0,

0,

-1,

0,

0,

0,

0,

1,

0,

0,

0

-1

0

0

0

r1 

r2

r3

r4

r5

   

rn    

t1     {c1, c2, c5}

r1 

r2

r5

t1

0,

1,

1,

2,

0,

0,

0,

0,

-1,

0,

0,

1,

0,

-1,

-1,

0,

0,

0,

1,

1,

0,

0,

0,

0,

0,

0,

0,

0,

0

-1

0

-1

+

+

+

output: wordspace

random indexing

algorithm

step 4     building the document space

the vector space representation of a 
document d obtained by 
combining the vector space representation
of the terms that occur in the document 

output: docspace

wordspace and docspace

wordspace

docspace

c1 c2 c3 c4     ck

c1

c2

c3

c4     ck

t1

t2

t3

t4

   

tm

d1

d2

d3

d4

   

dn

k is a simple 
parameter 
of the model 

uniform representation

id84

..even if it sounds weird

theory: johnson-lindenstrauss    lemma [*]

bm,k     am,n rn,k k << n

distances between the points in the reduced space

approximately preserved if 

context vectors are nearly orthogonal

(and they are)

[*] johnson, w. b., & lindenstrauss, j. (1984). extensions of lipschitz mappings 
into a hilbert space. contemporary mathematics, 26(189-206), 1.

id84

..even if it sounds weird

theory: johnson-lindenstrauss    lemma [*]

r

1 

r

2

r

3

r

4

r

5

0,

0,

-1, 1,

0,

0,

0,

0

1,

0,

0,

0,

0,

0,

0,

-1

0,

0,

0,

0,

0,

-1, 1,

0

-1, 1,

0,

0,

0,

0,

0,

0

1,

0,
0,
bm,k     am,n rn,k k << n

-1, 1,

0,

0,

0

distances between the points in the reduced space

   

r

approximately preserved if 

   

n

context vectors are nearly orthogonal

(and they are)

[*] johnson, w. b., & lindenstrauss, j. (1984). extensions of lipschitz mappings 
into a hilbert space. contemporary mathematics, 26(189-206), 1.

id84

..even if it sounds weird

theory: johnson-lindenstrauss    lemma [*]

bm,k     am,n rn,k k << n

distances between the points in the reduced space

approximately preserved if 

context vectors are nearly orthogonal

(and they are)

[*] johnson, w. b., & lindenstrauss, j. (1984). extensions of lipschitz mappings 
into a hilbert space. contemporary mathematics, 26(189-206), 1.

random indexing

incremental and scalable technique for 

learning id27s

smaller vector space

representation

dimension of the space can 

be arbitrarly set

incremental and scalable

not transparent anymore

proper tuning to find the optimal

size of the embeddings

random indexing @work: evsm

    enhanced vector space model [*]

    content-based recommendation framework

    cornerstones

    semantics modeled through distributional models

    random indexing for id84

    negative preferences modeled through quantum negation [^]

    user profiles as centroid vectors of items representation

    recommendations through cosine similarity

[*] musto, cataldo. "enhanced vector space models for content-based recommender systems." proceedings of the fourth 
acm conference on recommender systems. acm, 2010.

[^] widdows, dominic, and stanley peters. "word vectors and quantum logic: experiments with negation and 

disjunction." mathematics of language 8.141-154 (2003).

evsm
experiments

size of the embeddings

the size of the embeddings does not significantly affect the overall

accuracy of evsm (movielens data)

evsm
experiments

quantum negation improves the accuracy of the model 

(movielens data, embedding size=100)

evsm
experiments

evsm significantly overcame all the baselines.

(movielens data, embedding size=400)

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

introduce semantics 

by linking the 

item to a 

id13

distributional 
semantic models

explicit 

semantic 
analysis

random
indexing

id97

      

id97

in a nutshell

    distributional model to learn id27s. 
    uses a two-layers neural network
    training based on the skip-gram methodology
    update of the network through mini-batch and stochastic gradient

descent

id97

(partial) structure of the network

input layer:
    vocabulary v

|v| number of terms
|v| nodes

   
   
    each term is

represented through
a   one hot 
representation  

   .    .

   .

x1

x2

x3

x4

x5

   .

x|v|

id97

(partial) structure of the network

x1

x2

x3

x4

x5

   .

x|v|

h1

h2

input layer:
    vocabulary v

|v| number of terms
|v| nodes

   
   
    one-hot representation

h3

   .    .

hidden layer:
    n nodes

    n = size of the embeddings
    parameter of the model

h4

   .

hn

id97

(partial) structure of the network

x1

x2

x3

x4

x5

   .

x|v|

h1

h2

input layer:
    vocabulary v

|v| number of terms
|v| nodes

   
   
    one-hot representation

h3

   .    .

hidden layer:
    n nodes

    n = size of the embeddings
    parameter of the model

weight of the network:
    randomly set (initially)
    updated through the training

h4

   .

hn

id97

(partial) structure of the network

x1

x2

x3

x4

x5

   .

x|v|

h1

h2

input layer:
    vocabulary v

|v| number of terms
|v| nodes

   
   
    one-hot representation

h3

   .    .

hidden layer:
    n nodes

h4

   .

hn

    n = size of the embeddings
    parameter of the model

weight of the network:
    randomly set (initially)
    updated through the training

final representation for term tk
    weights extracted from the network
    tk=[wtkv1, wtkv2     wtkvn]

id97

training procedure: how to create training examples?

skip-gram methodology

continuous bag-of-words

methodology

given a word w(t), predict its

context w(t-2), t(t-1).. w(t+1), w(t+2)

given a context w(t-2), t(t-1).. 
w(t+1), w(t+2) predict word w(t) 

id97

training procedure: how to create training examples?

skip-gram methodology

given a word w(t), predict its

context w(t-2), t(t-1).. w(t+1), w(t+2)

example
input:    the quick brown fox 
jumped over the lazy dog    

window size: 1

contexts:
   
   
   

([the, brown], quick)
([quick, fox], brown)
([brown, jumped], fox) ...

training examples:
   
   
   
   

(quick, the)
(quick, brown)
(brown, quick)
(brown, fox) ...

id97

training procedure: how to optimize the model?

given a corpus, we create of training examples through skip-gram.

the model tries to maximize the 
id203 of predicting a context    c    
given a word    w   

id97

training procedure: how to optimize the model?

given a corpus, we create of training examples through skip-gram.

the model tries to maximize the 
id203 of predicting a context    c    
given a word    w   

and id203 is calculated through soft-max

id97

training procedure: how to optimize the model?

given a corpus, we create a training examples through skip-gram.

the model tries to maximize the 
id203 of predicting a context c 
given a word w

and id203 is calculated through soft-max

intuitively, id203 is high when scalar product
is close to 1     when vectors are similar!

id97

training procedure: how to optimize the model?

given a corpus, we create a training examples through skip-gram.

the model tries to maximize the 
id203 of predicting a context c 
given a word w

and id203 is calculated through soft-max

intuitively, id203 is high when scalar product
is close to 1     when vectors are similar!

id97 is a distributional model since it learns
a representation such that couples (word,context) 

appearing together have similar vectors

id97

training procedure: how to optimize the model?

given a corpus, we create a training examples through skip-gram.

the model tries to maximize the 
id203 of predicting a context c 
given a word w

and id203 is calculated through soft-max

intuitively, id203 is high when scalar product
is close to 1     when vectors are similar!

the error is collected and weights in the network are updated
accordingly. typically is used stochastic id119

or mini-batch (every 128 or 512 training examples)

id97

learning id27s 

through neural networks: it is not

based on   counting   

co-occurrences. it relies on 

  predicting   the distribution

representation can be really really

small (size<100, typically)

trending     - recent and very hot 

technique

not transparent anymore

needs more computational resources

id97

    empirical comparison of id27 techniques

for content-based recommender systems [*]

    methodology

    build a wordspace using different id27

techniques (and different sizes)

    build a docspace as the centroid vectors of term vectors

    build user profiles as centroid of the items they liked

    provide users with recommendations

    compare the approaches

musto, cataldo, et al. "learning id27s from wikipedia for content-based recommender

systems." european conference on information retrieval.  ecir 2016.

id97

results on dbbook and movielens data

musto, cataldo, et al. "learning id27s from wikipedia for content-based recommender

systems." european conference on information retrieval.  ecir 2016.

id97

results on dbbook and movielens data

id27 techniques

baselines

musto, cataldo, et al. "learning id27s from wikipedia for content-based recommender

systems." european conference on information retrieval.  ecir 2016.

id97

results on dbbook and movielens data

musto, cataldo, et al. "learning id27s from wikipedia for content-based recommender

systems." european conference on information retrieval.  ecir 2016.

   let   s put everything together

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

distributional 
semantic models

work on 

vector space model

work on 

vector space model

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

distributional 
semantic models

work on 

vector space model

work on 

vector space model

can exogenous and endogenous approaches be combined?

   let   s put everything together

c1 c2 c3 c4 c5 c6 c7 c8 c9

concept1

concept2

       

       

       

           

concept3    

   

concept4            

   

       

   

exogenous approaches as entity linking and wsd 

work on the row of the matrix

   let   s put everything together

c1 c2 c3 c4 c5 c6 c7 c8 c9

concept1

concept2

       

       

       

           

concept3    

   

concept4            

   

       

   

exogenous approaches as entity linking and wsd 

work on the row of the matrix

endogenous approaches as esa or id97

work on the columns of the matrix

   let   s put everything together

c1 c2 c3 c4 c5 c6 c7 c8 c9

concept1

concept2

       

       

       

           

concept3    

   

concept4            

   

       

   

exogenous approaches as entity linking and wsd 

work on the row of the matrix

endogenous approaches as esa or id97

work on the columns of the matrix

both approaches can be combined to obtain richer

and more precise semantic representations

(e.g. id97 over textual description processed with wsd)

recent developments of 

content-based recsys

exogenous techniques: 

recsysbased on linked open data

cataldo musto

department of computer science
university of bari aldo moro, italy

semantic representations

explicit (exogenous)

semantics

implicit (endogenous)

semantics

introduce semantics by 
mapping the features 
describing the item with 

semantic concepts

introduce semantics 

by linking the 

item to a 

id13

ontologies

linked 

open data

      .

semantic web

   the semantic web provides a common 
framework that allows data to be shared 
and reused across application enterprise, 

and community boundaries    [*]

[*] berners-lee, tim; james hendler; ora lassila

"the semantic web". scientific american magazine, 2001

semantic web

   the semantic web provides a common 
framework that allows data to be shared 
and reused across application enterprise, 

and community boundaries    [*]

(do we succed?)

[*] berners-lee, tim; james hendler; ora lassila

"the semantic web". scientific american magazine, 2001

from semantic web to linked open data

   the semantic web provides a common 
framework that allows data to be shared 
and reused across application enterprise, 

and community boundaries    [*]

linked open data project

goal: to make structured and 

interconnected the whole

data available on the web 

[^]. 

[*] berners-lee, tim; james hendler; ora lassila

"the semantic web". scientific american magazine, 2001

linked open data

what is it?

linked open data

what is it?

linked open data is a 

methodology

to publish, share and link 

structured data on the 

web

linked open data - cornerstones

1. use of rdf to model the information and 

make data publicly available

linked open data - cornerstones

1. use of rdf to model the information and 

make data publicly available

linked open data - cornerstones

1. use of rdf to model the information and 

make data publicly available

subject

object

predicate

(this is called rdf triple)

linked open data - cornerstones

1. use of rdf to model the information and 

make data publicly available

subject

object

predicate

(this is called rdf triple)

keanu reeves

the matrix

acted

linked open data - cornerstones

1. use of rdf to model the information and 

make data publicly available

subject

object

predicate

(this is called rdf triple)

keanu reeves

the matrix

acted

uri

uri / literal

linked open data - cornerstones

1. use of rdf to model the information and 

make data publicly available

subject

object

predicate

(this is called rdf triple)

dbr:keanu_reeves

dbr:the_matrix

acted

uri

uri / literal

linked open data - cornerstones

1. use of rdf to model the information and 

make data publicly available

subject

object

predicate

(this is called rdf triple)

dbr:keanu_reeves

1964

birthyear

uri

uri / literal

linked open data - cornerstones

1. use of rdf to model the information and 

make data publicly available

dbr:keanu_reeves

dbr:the_matrix

acted

2. re-use existing resources and properties 

in order to make the data inter-connected

dbr:keanu_reeves

dbo:starring

dbr:the_matrix

linked open data

we only use a small subset 

of the    semantic web cake   

we use rdf to model 

our data and we use 

sparql

as query language 

to gather data 

linked open data

do we succeed?

linked open data

this is the 

linked open data cloud

linked open data

this is the 

linked open data cloud

it is a (huge) set of 

interconnected semantic 

datasets

each bubble is a dataset!

linked open data

this is the 

linked open data cloud

it is a (huge) set of 

interconnected semantic 

datasets

each bubble is a dataset!

how many datasets do we have?

149 billions triples
and 9,960 datasets

(source: http://stats.lod2.eu)

linked open data

this is the 

linked open data cloud

it is a (huge) set of 

interconnected semantic 

datasets

each bubble is a dataset!

datasets cover many domains

linked open data

the core of the 

linked open data cloud 

is dbpedia

(http://www.dbpedia.org) 

rdf mapping

of wikipedia

dbpedia

wikipedia

unstructured

content

dbpedia

wikipedia

unstructured

content

dbpedia

structured

data

dbpedia

all the information available in  wikipedia

is modeled in rdf

dbpedia     in a nutshell

we have interesting

features coming

from wikipedia (and 

other sources) and 

the advantage of

formal semantics

defined in rdf

dbpedia     in a nutshell

we have interesting

features coming

from wikipedia (and 

other sources) and 

the advantage of

formal semantics

defined in rdf

we have semantics

without the need of 

building and 

manually populating

an ontology

   one step back

   one step back

sparql comes into play!

sparql

[   ]
select distinct ?city ?name
where { 

?city dct:subject dbc:cities_in_italy . 
?city rdfs:label ?name . 
?city dbo:populationtotal ?population . 
filter (?population > 100000) . 
filter (lang(?name) = 'en')

}

an example of sparql query

sparql

[   ]
select distinct ?city ?name
where { 

?city dct:subject dbc:cities_in_italy . 
?city rdfs:label ?name . 
?city dbo:populationtotal ?population . 
filter (?population > 100000) . 
filter (lang(?name) = 'en')

}

returns

big cities

in italy

(more 

than

100,000 

people)

an example of sparql query

sparql

[   ]
select distinct ?city ?name
where { 

?city dct:subject dbc:cities_in_italy . 
?city rdfs:label ?name . 
?city dbo:populationtotal ?population . 
filter (?population > 100000) . 
filter (lang(?name) = 'en')

}

returns

big cities

in italy

(more 

than

100,000 

people)

how do we exploit sparql?

sparql

[   ]
select distinct ?city ?name
where { 

?city dct:subject dbc:cities_in_italy . 
?city rdfs:label ?name . 
?city dbo:populationtotal ?population . 
filter (?population > 100000) . 
filter (lang(?name) = 'en')

}

returns

big cities

in italy

(more 

than

100,000 

people)

key concept: mapping

sparql

select distinct ?uri, ?title
where {

?uri rdf:type dbpedia-owl:film.
?uri rdfs:label ?title.
filter langmatches(lang(?title), "en") 

.

}

filter regex(?title, "matrix", "i")

we can run a sparql query

to find the corresponding uri 

for the resource

sparql

select distinct ?uri, ?title
where {

?uri rdf:type dbpedia-owl:film.
?uri rdfs:label ?title.
filter langmatches(lang(?title), "en") 

dbr:the_matrix

.

}

filter regex(?title, "matrix", "i")

we want to link   logical   entities

occurring in our data with   physical   

entities occurring in the lod cloud

lod-aware data model

once we have a mapping, 

properties can be extracted

lod-aware recsys

how can we use linked open data

for recommender systems?

motivations: limited content analysis

in some scenarios, we don   t have enough features

to feed our recommendation models.

lod cloud can be helpful

motivations: limited content analysis

several very fine-grained

and interesting features can be 

easily injected by querying dbpedia

motivations: graph-based data model

basic graph-based data model

only collaborative connections are modeled

motivations: graph-based data model

user-1

user-2

extended graph-based data model

richer representation based on properties 

gathered from the lod cloud

motivations: graph-based data model

user-1

user-2

extended graph-based data model

new and unexpected connections may lead to 

more surprising recommendations

lod-aware recsys

1. approaches

based on vector

space models

2. approaches

based on graph-

based models

3. approaches

based on machine 

learning 

techniques

lod-based recommender systems

approaches based on vsm

lod are typically used to cope with 

limited content analysis problem

lod-based recommender systems

approaches based on vsm

lod-based recommender systems

approaches based on vsm

lod-based recommender systems

approaches based on vsm 

lod-based recommender systems

thanks to the lod we can obtain a 
richer vector-space representation

similarity between items

lod-based recommender systems

thanks to the lod we can obtain a 
richer vector-space representation

similarity between items

can we think about more complex models?

lod-based recommender systems

vector space model for lod

in dbpedia each item is

modeled on the ground of 

several facets

each facet is

modeled as a slice

of a tensor.

each slice encodes the 

features describing that

particular facet.

tommaso di noia, roberto mirizzi, vito claudio ostuni, davide romito, markus zanker. linked open data to support content-based
recommender systems. 8th international conference on semantic systems (i-semantics) - 2012 (best paper award)

lod-based recommender systems

vector space model for lod

in dbpedia each item is

modeled on the ground of 

several facets

each facet is

modeled as a slice

of a tensor.

each slice encodes the 

features describing that

particular facet.

tommaso di noia, roberto mirizzi, vito claudio ostuni, davide romito, markus zanker. linked open data to support content-based
recommender systems. 8th international conference on semantic systems (i-semantics) - 2012 (best paper award)

lod-based recommender systems

vector space model for lod

similarity

between items as

linear 

combination of 

the similarity

among dbpedia 

facets (starring, 

directors, 

subject, etc.)

tommaso di noia, roberto mirizzi, vito claudio ostuni, davide romito, markus zanker. linked open data to support content-based
recommender systems. 8th international conference on semantic systems (i-semantics) - 2012 (best paper award)

lod-based recommender systems

vsm content-based recommender

           ,          =

                                         (    )          ,         

   

                                           (        ,         )

|    |

|                            (    )|

predict the rating using a nearest neighbor classifier wherein 
the similarity measure is a linear combination of local property 
similarities

tommaso di noia, roberto mirizzi, vito claudio ostuni, davide romito, markus zanker. linked open data to support content-based
recommender systems. 8th international conference on semantic systems (i-semantics) - 2012 (best paper award)

lod-based recommender systems

property subset evaluation

subject+broader
solution 
better than only 
subject or 
subject+more
broaders

too many broaders
introduce noise

best solution 
achieved 
with 
subject+broader+ge
nres

tommaso di noia, roberto mirizzi, vito claudio ostuni, davide romito, markus zanker. linked open data to support content-based
recommender systems. 8th international conference on semantic systems (i-semantics) - 2012 (best paper award)

lod-aware recsys

1. approaches

based on vector

space models

2. approaches

based on graph-

based models

3. approaches

based on machine 

learning 

techniques

graph-based data model

u2

i1

i4
i2

users = nodes
items = nodes

preferences = edges

(bipartite graph)

u3

u1

u4

i3

i4

very intuitive
representation!

graph-based data model

u2

u3

u1

u4

i1

i4
i2

users = nodes
items = nodes

preferences = edges

(bipartite graph)

i3

i4

basic graph-based data 

models only encode

collaborative data points

we can extend such data 

model by introducing

features gathered from the 

lod cloud

semantic graph-based data model

i1

i4

i3

u2

u3

u1

u4

dbpedia
mapping

semantic graph-based data model

u2

u3

u1

u4

(1-hop)

i1

i4

quentin tarantino

http://dbpedia.org/resource/quentin_tarantino

i3

dcterms:subject

1999 films

http://dbpedia.org/resource/1999_films

dcterms:subject

films about rebellions

http://dbpedia.org/resource/films_about_rebellions

semantic graph-based data model

(2-hop)

i
1
i
4

quentin tarantino

http://dbpedia.org/resource/quentin_

tarantino

american film 

directors

http://dbpedia.org/page/category:am

erican_film_directors

i
3

u4

dcterms:subject

1999 films

dbo:award

lynne thigpen

http://dbpedia.org/resource/1999_films

http://dbpedia.org/resource/lynne_thigpen

dcterms:subject

films about 
rebellions

http://dbpedia.org/resource/films_about_

rebellions

   

u2

u3

u1

semantic graph-based data model

(n-hop)

i
1
i
4

quentin tarantino

http://dbpedia.org/resource/quentin_

tarantino

american film 

directors

http://dbpedia.org/page/category:am

erican_film_directors

i
3

u4

dcterms:subject

1999 films

dbo:award

lynne thigpen

http://dbpedia.org/resource/1999_films

http://dbpedia.org/resource/lynne_thigpen

dcterms:subject

films about 
rebellions

http://dbpedia.org/resource/films_about_

rebellions

   

u2

u3

u1

graph-based recsys

how to get the recommendations?

recommendations 
obtained by mining 
the graph

cataldo musto, pierpaolo basile, pasquale lops, marco de gemmis, giovanni semeraro. introducing linked open data in graph-based 
recommender systems. information processing & management. 53(2): 405-435 (2017)

graph-based recsys

how to get the recommendations?

recommendations 
obtained by mining 
the graph

identification of the 
most relevant (target) 
nodes, according to 
the recommendation
scenario 

cataldo musto, pierpaolo basile, pasquale lops, marco de gemmis, giovanni semeraro. introducing linked open data in graph-based 
recommender systems. information processing & management. 53(2): 405-435 (2017)

graph-based recsys

how to get the recommendations?

recommendations 
obtained by mining 
the graph

identification of the 
most relevant (target) 
nodes, according to 
the recommendation
scenario 

id95
spreading activation
personalized id95
   

cataldo musto, pierpaolo basile, pasquale lops, marco de gemmis, giovanni semeraro. introducing linked open data in graph-based 
recommender systems. information processing & management. 53(2): 405-435 (2017)

graph-based recsys

[*] c. musto, p. basile, p. lops, m. de gemmis, g. semeraro: 

introducing linked open data in graph-based recommender

systems. inf. process. manage. 53(2): 405-435 (2017)

recent work [*]

task: top-n 

recommendation

expansion: 1-hop, all
the properties were

injected

recommendation 
algorithm: id95

with priors

settings: hot start, 

cold start

topologies: nolod , 

lod

graph-based recsys

56

54

52

50

48

46

44

42

no lod vs. lod     hot start scenario (f1@5)

55,02

55,04

47,55

46,35

dbbook

no-lod lod

last.fm

no-lod =  bipartite user-item graph
lod= tripartite graph also including lod properties for items 

graph-based recsys

no lod vs. lod     cold start scenario (f1@5)

53,16

52,94

42,57

45,56

60

50

40

30

20

10

0

dbbook

no-lod lod

last.fm

no-lod =  bipartite user-item graph
lod= tripartite graph also including lod properties for items 

graph-based recsys

no lod vs. lod     cold start scenario (f1@5)

53,16

52,94

42,57

45,56

60

50

40

30

20

10

0

dbbook

no-lod lod

last.fm

no-lod =  bipartite user-item graph
lod= tripartite graph also including lod properties for items 

graph-based recsys

is it necessary to inject all the properties 

available in lod cloud?

graph-based recsys

is it necessary to inject all the properties 

available in lod cloud?

graph-based recsys

is it necessary to inject all the properties 

available in lod cloud?

graph-based recsys

what are the most
promising properties
to include?

manual selection
o domain-specific 

properties

o most frequent properties
o    

automatic selection
o more difficult to 

implement

feature selection

selecting the most promising subset of 

lod-based properties

id95

principal component analysis

chi-square

information gain

information gain ratio

mininum redundancy maximum relevance

cataldo musto, pierpaolo basile, pasquale lops, marco de gemmis, giovanni semeraro. introducing linked open data in graph-based 
recommender systems. information processing & management. 53(2): 405-435 (2017)

graph-based recsys
movielens data / f1@10

baseline

graph-based recsys
dbbook data / f1@10

baseline

graph-based recsys
dbbook data / f1@10

the adoption of features selection techniques

can significantly improve

the predictive accuracy of our recommendation 

algorithm

baseline

graph-based recsys

comparison to state of the art

movielens 100k dataset

lod-aware recsys

1. approaches

based on vector

space models

2. approaches

based on graph-

based models

3. approaches

based on 

machine learning         

techniques

semantic graph-based data model

(recap)

new features describing the item can be 

inferred by mining the

structure of the tripartite graph

average neighbor degree
degree centrality
node redundancy
id91 coefficient

lod-based recommender systems

research question: what is the impact of 
such features on the overall performance of 
the recommendation framework?

cataldo musto, pasquale lops, giovanni semeraro, marco de gemmis. semantics-aware recommender systems exploiting linked open 
data and graph-based features. knowledge-based systems (accepted for publication), 2017. 

lod-based recommender systems

research question: what is the impact of 
such features on the overall performance of 
the recommendation framework?

insight: to build a hybrid classification 
framework exploiting lod-based and 
graph-based features

cataldo musto, pasquale lops, giovanni semeraro, marco de gemmis. semantics-aware recommender systems exploiting linked open 
data and graph-based features. knowledge-based systems (accepted for publication), 2017. 

lod-based recommender systems

methodology

we first model basic features

cataldo musto, pasquale lops, giovanni semeraro, marco de gemmis. semantics-aware recommender systems exploiting linked open 
data and graph-based features. knowledge-based systems (accepted for publication), 2017. 

lod-based recommender systems

methodology

then we introduce extended features
based on the linked open data cloud

cataldo musto, pasquale lops, giovanni semeraro, marco de gemmis. semantics-aware recommender systems exploiting linked open 
data and graph-based features. knowledge-based systems (accepted for publication), 2017. 

lod-based recommender systems

methodology

we used them to feed a 

hybrid classification framework

cataldo musto, pasquale lops, giovanni semeraro, marco de gemmis. semantics-aware recommender systems exploiting linked open 
data and graph-based features. knowledge-based systems (accepted for publication), 2017. 

lod-based recommender systems

results

cataldo musto, pasquale lops, giovanni semeraro, marco de gemmis. semantics-aware recommender systems exploiting linked open 
data and graph-based features. knowledge-based systems (accepted for publication), 2017. 

lod-based recommender systems

results

cataldo musto, pasquale lops, giovanni semeraro, marco de gemmis. semantics-aware recommender systems exploiting linked open 
data and graph-based features. knowledge-based systems (accepted for publication), 2017. 

recap #5

encoding exogenous semantics
through id13s

1. linked open data represent a 

huge data silos, which is freely

available

2. they can easily let overcome

the limited content analysis

problem

3. they can enrich graph-based

data model with interesting data 

points

4. they can feed machine learning

models with new and relevant

features

5. they improve the accuracy of 

recommender systems

recent developments of 
content-based recsys
applications: explanations, obviousness of 

recommendations

marco de gemmis

department of computer science
university of bari aldo moro, italy

agenda

why?

why do we need intelligent information access?
why do we need content?
why do we need semantics?

how?

how to introduce semantics?
basics of natural language processing
encoding exogenous semantics,i.e. explicit semantics
encoding endogenous semantics, i.e. implicit semantics

what?

explanation of recommendations
serendipity in recommender systems

explanatory aims

aim

description

transparency

scrutability

explain how the system works

allow users to tell the system it is wrong

persuasiveness

convince users to try or buy

trust

effectiveness

efficiency

satisfaction

increase users    confidence in the system

help users make good decisions

help users make decisions faster

increase the ease of use or enjoyment

n. tintarev and j. masthoff. evaluating the effectiveness of explanations for recommender systems.
umuai, 22(4-5):399{439, 2012.

3

some examples

   people who liked this movie also liked...    

   you might like this item because it won the oscar   
   it is a funny comedy   

4

explanation strategies

    preferences of similar users

      customers who bought

this item also bought...  

    items similar to those in the user profile

      i recommend star trek because you liked star wars  

    attributes of interest

      you will like forrest gump because tom hanks is in the cast  

5

a detailed explanation for a 

book recommendation

user profile

recommendation

i suggest crime and punishment because 
you like books written by fyodor dostoevskij

furthermore, you often like 

such as the brothers karamazov. 
psychological russian novels such as
anna karenina and a hero of our time.

a detailed explanation for a 

book recommendation

user profile

recommendation

i suggest crime and punishment because 
you like books written by fyodor dostoevskij

furthermore, you often like 

such as the brothers karamazov. 
psychological russian novels such as
anna karenina and a hero of our time.

a hybrid explanation strategy
    preferences of similar users

      customers who bought

this item also bought...  

    items similar to those in the user profile

i suggest crime and punishment because 
you like books written by fyodor dostoevskij

such as the brothers karamazov.

    attributes of interest

i suggest crime and punishment because 
you like books written by fyodor dostoevskij

such as the brothers karamazov.

personalized explanation approach based on user

preferences on items and their properties

8

explaining recommendations

based on the linked open data cloud
    connecting the items the user liked to the 

recommendations through properties in the lod 
cloud
    generation of natural language explanations 

based on    most informative    properties

lod

9

lod-aware representation

dbp:fyodor_dostoyevsky

fyodor 

dostojevsky

dbp:category:prostitution_in_literature

prostitution in 

literature

dcterms:subject

narcisism in 

fiction

dbp:category:narcissism_in_fiction

dbp:category:psychological_novels

psychological

novels

:

t
c
e
j
b
u
s
s
m
r
e
t
c
d

crime and 
punishment

dbp:crime_and_punishment

:

t
c
e
j
b
u
s
s
m
r
e
t
c
d

existentialist

novels

dbp:category:novels_set_in_saint_petersburg

novels set in 
petersburg

saint 

dcterms:publisher

dbp:the_russian_messenger

the russian 
messenger

19th century
russian novels

dbp:category:19th-century_russian_novels

dbp:category:existentialist_novels

10

explod: framework

cataldo musto, fedelucio narducci, pasquale lops, marco de gemmis, giovanni semeraro: explod: a framework for 
explaining recommendations based on the linked open data cloud. recsys 2016: 151-154
11

explod: mapper

profile recommendations

mapper

profile

recommendation

dbp:anna_karenina

dbp:crime_and_pu

nishment

12

explod: builder

dbp:the_brothers_karamazov

dbp:anna_karenina

dbp:a_hero_of_our_time

fyodor 
dostoevskij

dbp:fyodor_dostoyevsky

philosophical

fiction

dbp:category:philosophical_fiction

psychological

russian 
novel

dbp:category:psycological_novel

narcisism in 

fiction

dbp:category:narcissism_in_fiction

dbp:crime_and_punishment

13

explod: ranker

scoring properties in explod

higher score to uncommon properties highly connected to 
the items in both the user profile and the recommendation list

with the items in 
the user profile

the recommendation set

connecting the propertyc

connecting the propertyc

items in the 
user profile

recommendation list

with the items in 

items in the 

property

number of edges

number of edges

14

explod: ranker

fyodor 

dostoevskij:
score: 4.875

dbp:fyodor_dostoyevsky

philosophical
score: 3.534

fiction

dbp:category:philosophical_fiction

psychological

russian 
novel
score: 3.231

dbp:category:psycological_novel

narcisism in 
score: 3.132

fiction

dbp:category:narcissism_in_fiction

dbp:the_brothers_karamazov

dbp:anna_karenina

dbp:a_hero_of_our_time

dbp:crime_and_punishment

15

explod: ranker

returns the top-3 properties

fyodor 

dostoevskij:
score: 4.875

dbp:fyodor_dostoyevsky

philosophical
score: 3.534

fiction

dbp:category:philosophical_fiction

psychological

russian 
novel
score: 3.231

dbp:category:psycological_novel

narcisism in 
score: 3.132

fiction

dbp:category:narcissism_in_fiction

dbp:the_brothers_karamazov

dbp:anna_karenina

dbp:a_hero_of_our_time

dbp:crime_and_punishment

16

explod: generator
output:
    natural language 

explanation

input:
    user profile
    recommended items
    top-k properties

russian 
writers
score: 
russian 
8,599
novels
score: 
pholosophi
6,363
cal novels
score: 
3,544

+

17

explod: generator

dbp:the_brothers_karamazov

dbp:anna_karenina

dbp:crime_and_punishment

i suggest crime and punishment   

dbp:a_hero_of_our_time

18

explod: generator

top-1 property

dbp:the_brothers_karamazov

fyodor 

dostoevskij:
score: 4.875

dbp:fyodor_dostoyevsky

dbp:anna_karenina

dbp:crime_and_punishment

i suggest crime and punishment because you like books 

written by fyodor dostoevskij such as the brothers karamazov. 

dbp:a_hero_of_our_time

19

explod: generator

top-2 properties

dbp:the_brothers_karamazov

fyodor 

dostoevskij:
score: 4.875

dbp:fyodor_dostoyevsky

philosophical
score: 3.534

fiction

dbp:category:philosophical_fiction

dbp:anna_karenina

dbp:crime_and_punishment

i suggest crime and punishment because you like books 

written by fyodor dostoevskij such as the brothers karamazov. 
furthermore, you like philosophical fiction, such as anna karenina.

dbp:a_hero_of_our_time

20

explod: generator

fyodor 

dostoevskij:
score: 4.875

dbp:fyodor_dostoyevsky

philosophical
score: 3.534

fiction

dbp:category:philosophical_fiction

psycological
russian 
novel
score: 3.231

dbp:category:psycological_novel

top-3 properties

dbp:crime_and_punishment

i suggest crime and punishment because you like books 

written by fyodor dostoevskij such as the brothers karamazov. 
furthermore, you like philosophical fiction, such as anna karenina.

finally, you often like psychological russian novel, 

such as anna karenina and a hero of our time.

21

dbp:the_brothers_karamazov

dbp:anna_karenina

dbp:a_hero_of_our_time

experimental evaluation

user study

    movie domain, 308 users involved
    protocol: 

    web application     building user profiles    

recommendations + explanations     questionnaire + ex-
post evaluation
    explanation aims

    transparency, engagement, persuasion, trust, 

effectiveness

three configurations compared

    popularity-based explanation (baseline)
    non-personalized explanation based on lod
    explod

experimental evaluation:

a user study

    gathering movie preferences

    308 users rated 20 movies randomly chosen from the most 

popular movies in imdb

23

experimental evaluation:

a user study

    recommendation and evaluation of explanations

    1 recommendation per user computed by personalized 

id95 + explanation

    the user read the explanation and provided a 5-star rating on 

5 statements to evaluate transparency, persuasion, 
engagement, trust, effectiveness

t. h. haveliwala. topic-sensitive id95: a context-sensitive ranking algorithm for web 
search. ieee trans. knowl. data eng., 15(4):784-796, 2003.

24

experimental evaluation:

a user study

    recommendation and evaluation of explanations

    1 recommendation per user computed by personalized 

id95 + explanation

    the user read the explanation and provided a 5-star rating on 

5 statements to evaluate transparency, persuasion, 
engagement, trust, effectiveness

t. h. haveliwala. topic-sensitive id95: a context-sensitive ranking algorithm for web 
search. ieee trans. knowl. data eng., 15(4):784-796, 2003.

25

experimental evaluation:

a user study

effectiveness

transparency

persuasiveness

engagement

t. h. haveliwala. topic-sensitive id95: a context-sensitive ranking algorithm for web 
search. ieee trans. knowl. data eng., 15(4):784{796, 2003.

trust

26

experimental evaluation:

a user study

    ex-post evaluation

    the user watched the trailer of the recommended movie and 

provided again a 5-star rating  

27

experimental evaluation:

a user study

    three explanation strategies compared

    explod
    popularity-based explanation (baseline)        we suggest this 

item since it is very popular among people who like the 
same movies as you   

    non-personalized explanation based on lod     movie 

properties extracted from dbpedia (without any 
filtering or ranking of properties)

28

experimental evaluation:

a user study

    gathering movie preferences

    308 users rated 20 movies randomly chosen from the most 

popular movies in imdb

    1 recommendation per user computed by personalized 

id95 + explanation
    evaluation of explanations

    the user read the explanation and provided a 5-star rating on 

5 statements to evaluate transparency, persuasion, 
engagement, trust, effectiveness

    ex-post evaluation: the user watched the trailer of the 

recommended movie and provided again a 5-star rating  

t. h. haveliwala. topic-sensitive id95: a context-sensitive ranking algorithm for web 
search. ieee trans. knowl. data eng., 15(4):784{796, 2003.

29

aim

transparency

persuasiveness

engagement

trust

results

statement

i understood why this movie was 
recommended to me
the explanation made the
recommendation more convincing
the explanation helped me discover new 
information about this movie
the explanation increased my
trust in the recommender system

effectiveness

i like this recommendation

explod compared to: 

    popularity-based explanation        we suggest this item since it is very 

popular among people who like the same movies as you   

    non-personalized explanation style     movie properties extracted from 

dbpedia

c. musto, f. narducci, p. lops, m. de gemmis, g. semeraro: explod: a framework for explaining 
recommendations based on the linked open data cloud. proc. acm recsys 2016.

30

explanations - results

explod

non-personalized

baseline (pop)

transparency
persuasion
engagement

trust

effectiveness

4.18
3.41
3.48
3.39
0.72

3.04
2.84
3.28
2.81
0.66

3.01
2.59
2.31
2.67
0.93

results     main findings

explod

non-personalized

baseline (pop)

transparency*

persuasion*
engagement*

trust*

effectiveness**

4.18
3.41
3.48
3.39
0.72

3.04
2.84
3.28
2.81
0.66

3.01
2.59
2.31
2.67
0.93

* average score collected through the user questionnaires
** difference between the pre- and post-trailer ratings

significant improvement in 4 out of 5 metrics
non-significant gaps in terms of effectiveness

c. musto, f. narducci, p. lops, m. de gemmis, g. semeraro: explod: a framework for explaining 
recommendations based on the linked open data cloud. in proc. of the 10th acm conference on 
recommender systems (recsys '16). acm, new york, ny, usa, 151-154. 

explanations - results

aim

question

transparency

i understood why this movie was
recommended to me

topic
director

distributor
composer

persuasion

the explanation made the 
recommendation more convincing

awards
director

location
producer

engagement

the explanation helped me 
discover new information

writer
director

producer
distributor

trust

the explanation increased my
trust in the recommender system

awards
composer

producer
topic

effectiveness

i like this recommendation

director
writer

location
composer

agenda

why?

why do we need intelligent information access?
why do we need content?
why do we need semantics?

how?

how to introduce semantics?
basics of natural language processing
encoding exogenous semantics,i.e. explicit semantics
encoding endogenous semantics, i.e. implicit semantics

what?

explanation of recommendations
serendipity in recommender systems

obviousness of 
recommendations: homophily

    the tendency to surround ourselves by like-minded people [e. zuckerman 

2008]

    the filter bubble [pariser 2011]

    the user is provided with items within her existing range of interests
    cultural

impoverishment:    it   s possible to miss huge trends, changes

and opportunities by talking solely to people who agree with you   

you are 
here!

[e. zuckerman 2008] e. zuckerman. homophily, serendipity, xenophilia. april 25, 2008. 
www.ethanzuckerman.com/blog/2008/04/25/homophily-serendipity-xenophilia/]

[pariser 2011] e. pariser. the filter bubble: what the internet is hiding from you. penguin 
group, may 2011

35

homophily in the digital world

    in the physical world, one of the strongest sources of homophily is 

locality,  due to geographic proximity, family ties, and organizational 
factors (school, work, etc.)

    in the digital world, physical locality is less important. other factors, such 

as common interests, might play a central role

2 main questions

1. are two users more likely to be friends if they share common 

interests? 

2. are two users more likely to share common interests if they are 

friends? 

the answer to both questions is 

yes

[lauw et al. 2010]

[lauw et al. 2010] lauw, h.w., schafer, j.c., agrawal, r., & a. ntoulas. homophily in the digital world: a
livejournal case study. ieee internet computing 14(2):15-23, march-april 2010.

36

the homophily trap

    does homophily hurt recsys?

    try to tell amazon that you liked    star trek      

37

the homophily trap: user-user

recommendations by similar customers

38

serendipitous 

recommendations

39

       suggestions which help the user to find surprisingly interesting items

she might not have discovered by herself    [herlocker et al. 2004]
    both attractive and unexpected

       the experience of receiving an unexpected and fortuitous item 

recommendation    [mcnee et al. 2006]

    surprise or unexpectedness defined with respect to a benchmark model 

that generates expected recommendations [ge10]

[herlocker et al. 2004] herlocker, l., konstan, j.a., terveen, l.g., riedl, j.t.: evaluating id185 recommender systems. acm
transactions on information systems 22(1): 5   53, 2004.
[mcnee et al. 2006] s.m. mcnee, j. riedl, and j. a. konstan. being accurate is not enough: how accuracy metrics have hurt recommender
systems. in chi    06 extended abstracts on human factors in computing systems, chi ea    06, 1097   1101, acm, new york, ny, usa, 2006.
[ge10] ge, m., delgado-battenfeld, c., jannach, d.: beyond accuracy: evaluating recommender systems by coverage and
serendipity. proc. of the acm conference on recommender systems, pp. 257   260. acm (2010)

39

operationally induced 

serendipity

40

    how to introduce serendipity in the 

recommendation process?

    semantic matching is not a solution

    semantic profiles might provide more 

accurate recommendations than 
keyword-based profiles but they could 
be obvious too

40

serendipity in information 

41

seeking

    information seeking metaphor investigated in literature (toms 

2000, andr   et al 2009, bordino et al. 2013)

    toms suggests 4 strategies

    blind luck or    role of chance        random
    pasteur principle or    chance favors only the prepared mind       

flashes of insight don   t just happen, but they are the products of a 
   prepared mind   

    anomalies and exceptions or    searching for dissimilarities       

identification of items dissimilar to those the user liked in the past

    reasoning by analogy     abstraction mechanism allowing the 

system to discover the applicability of an existing schema to a new 
situation

(toms 2000) e. toms. serendipitous information retrieval. proc.1st delos noe workshop on information seeking, searching and querying 
in digital libraries, zurich, switzerland: ercim, 2000. 
(andr   2009) p. andr  , j. teevan, s.t. dumais. from x-rays to silly putty via uranus: serendipity and its role in web search. proc. acm chi 
2009, acm, new york, ny, usa, 2009, 
(bordino et al. 2013) i. bordino, y. mejova, m. lalmas, penguins in sweaters, or serendipitous entity search on user-generated content. 
proc.22nd acm cikm 2013, acm, new york, ny, usa, 2013, pp. 109   118.

41

operationally induced 

serendipity

    how to introduce serendipity in the 

recommendation process?

    build a    prepared mind   !

    need some background knowledge     deep 

understanding of item descriptions
    need some reasoning capabilities    

discovering non-obvious associations 
among items

deep content 

analytics

semantics + 
reasoning

42

42

knowledge infusion: nlp+ai 
    nlp techniques process the unstructured information stored in 

several (open) knowledge sources
    the memory of the system

    spreading activation [and83] as the reasoning mechanism

    the brain of the system

[and83] j. r. anderson. a spreading activation theory of memory. journal of verbal learning and verbal 
behavior, 22:261   295, 1983.

43

43

cultural and linguistic 
background knowledge

the memory: encoding 

knowledge sources as a cu 

repository

    information in long term memory of human beings 

encoded as cognitive units     act theory [and83] 

    cognitive unit (cu) = textual description of a concept
    head = words identifying the concept represented by the cu
    body = words describing the concept
    [head | body]

[and83] j. r. anderson. a spreading activation theory of memory. journal of verbal learning and verbal behavior, 
22:261   295, 1983.

44

encoding a knowledge source 
as cognitive unit repository

head

artificial 0.77  
intelligence 1.22

ai         intelligence       computer
engineering        machine         mind

1.22                 1.10            0.99
0.65           0.55          0.49
                

45

45

body

from wikipedia articles to cu

nlp

articles

cognitive 

units

46

46

cu repositories can be queried

query: machine intelligence

[artificial 0.77
intelligence 1.22
|
ai 1.22
intelligence 1.10
computer 0.99
engineering 0.65
machine 0.55
mind 0.49
. . .
. . .    

relevant

cus

0.85
0.52
0.46

relevance 
score

47

47

cognitive 

units

ki@work

48

clue#1

clue#2

clue#3

clue   

clue#n

background knowledge

knowledge
source #1

knowledge
source #2

knowledge
source #3

knowledge
source #n

. . . 

spreading
activation 
network

keyword1
keyword2

   

new keywords 
associated 
with clues

g. semeraro, m. de gemmis, p. lops, p. basile. an artificial player for a language game. ieee intelligent
systems 27(5): 36-43, 2012.
p. basile, m. de gemmis, p. lops, g. semeraro. solving a complex language game by using knowledge-based
word associations discovery. ieee transactions on computational intelligence and ai in games, 2016 doi:
10.1109/tciaig.2014.2355859.

48

ki@work on movies

sci-fi

conflicts/ 
fights

49

the spreading activation net

50

ki as a novel method for computing 

associations between items

clues

bm25 retrieval

score

51

51

ki as a serendipity engine: 

item-to-item similarity matrix     item-to-

item correlation matrix

wij

wij computed in different ways
    #users co-rated items ii and ij
    cosine similarity between 

descriptions of items ii and ij

    ki correlation index

recommendation list 
computed by               
random walk with 
restart (lovasz 1996)     
working on ki matrix 
(rwr-ki)

(lovasz 1996) l. lovasz. id93 on graphs: a survey. combinatronics 2:1   46, 1996.

52

52

experimental evaluation

    validation of

recommendations
produced by rwr-ki are serendipitous (relevant/attractive
& unexpected/surprising)

the hypothesis

that

    difficulty of assessing unexpectedness

    in-vitro

experiments:

as
deviation from a standard prediction criterion such as
popularity [murakami et al. 2008]

unexpectedness measured

    user studies: how to measure the pleasant surprise that

serendipity should convey?

    user study

    emotions observed in facial expressions are used as implicit
feedback for serendipity     analysis performed using noldus
facereader   

[murakami et al. 2008] t. murakami, k. mori, r. orihara, metrics for evaluating the serendipity of recommendation
lists, in k. satoh, a. inokuchi, k. nagao, t. kawamura (eds.), new frontiers in artificial intelligence, lecture notes in
computer science 4914, pp. 40   46, springer, 2008.

53

noldus facereader   

    recognize basic emotions: 6 categories of emotions,

proposed by ekman (1999)

    happiness

    anger

    sadness

    fear

    disgust

    surprise

(ekman 1999) p. ekman, basic emotions, in t. dalgleish, m.j. power (eds.), handbook of cognition and emotion,
45   60, john wiley & sons, 1999.

54

dataset

    experimental units: 40 master students (engineering,
and

economy,

computer

science

architecture,
humanities)
    26 male (65%), 14 female (35%)
    age distribution: from 20 to 35

    dataset

    2,135 movies released between 2006 and 2011
    movie content     title, poster, plot keywords, cast, director,

summary     crawled from the internet movie database (imdb)

    vocabulary of 32,583 plot keywords
    average: 12.33 keywords/item

55

experimental design (i)

    between-subjects controlled experiment

    20 users randomly assigned to test rwr-ki
    20 users randomly assigned to test random (control
group), a baseline inspired by the blind luck principle
which produces random suggestions

    procedure

    users interact with a web application

    shows details of movies
    20 ratings collected (used only by rwr-ki)
    displays 5 recommendations (movie poster & title)

per user

    recommended items displayed 1 at a time

56

experimental design (ii)

    procedure

    2 binary questions to assess user acceptance

       have you ever heard about this movie?        unexpectedness
       do you like this movie?        relevance
    (no,yes) answers     serendipitous recommendation

    video started when a movie is recommended to the user
and stopped when the answers to the 2 questions were
provided

    5 videos per user    200 videos recorded to assess user

emotional response when exposed to recommendations

57

metrics

relevance@n = #relevant_items/n

   do you like this movie?       yes!

unexpectedness@n = #unexpected_items/n

   have you ever heard about this movie?        no!

serendipity@n = #serendipitous_items/n

= #(relevant_items    unexpected_items)/n

n = size of the recommendation list = 5

58

58

results: questionnaire 

analysis

    serendipity: rwr-ki outperforms random

    statistically significant differences (mann-whitney u test,

p<0.05)

    ~ half

of
serendipitous!

the

recommendations

are

deemed

    rwr-ki: a better relevance-unexpectedness trade-off

    random: more unbalanced towards unexpectedness

59

results: analysis of user 

emotions 

    hypothesis: users    facial expressions convey a mixture of
emotions that helps to measure the perception of
serendipity of recommendations

    serendipity associated to surprise and happiness

    200 videos (40 users x 5 recommendations)

    41 videos filtered out (< 5 seconds) 

        159 videos, facereader    computed the distribution of 

detected emotions + duration (emotions lasting < 1 sec. 
filtered out)

algorithm serend. recomm.
rwr-ki
random

39
30

non-serend. recomm.

39
51

60

results: analysis of user emotions 

associated to serendipitous suggestions

algorithm serend. recomm.
rwr-ki
random

39
30

non-serend. recomm.

39
51

    evidence of happiness and surprise for both algorithms
    rwr-ki > random (in line with the questionnaire results)
    high values of negative emotions (sadness and anger)     ?????

61

results: analysis of user emotions associated 

to non-serendipitous suggestions
non-serend. recomm.

algorithm serend. recomm.
rwr-ki
random

39
30

39
51

    general decrease of surprise and happiness
    high values of negative emotions (sadness and anger), also in this 

case     due to the fact that users assumed troubled expressions since 
they were very concentrated on the task 

62

main findings

    positive emotions: happyness, surprise

    marked difference between rwr-ki and random
    marked difference between serendipitous and non-

serendipitous recommendations

    moderate agreement between explicit feedback 

(questionnaires) & implicit feedback (facial 
expressions/emotions)
    cohen   s kappa coefficient

    emotions can help to assess the actual perception of

serendipity

m. de gemmis, p. lops, g. semeraro, c. musto. an investigation on the serendipity problem in recommender
systems. information processing and management, 2015 doi: 10.1016/j.ipm.2015.06.008

63

readings

open data and graph-based features. umap 2017

semantics-aware recommender systems
o c. musto, g.semeraro, m.de gemmis, p. lops. a hybrid recommendation framework exploiting linked
o c. musto, p. basile, p. lops, m. de gemmis, g. semeraro: introducing linked open data in graph-based
o c. musto, g. semeraro, m. de gemmis, p. lops: tuning personalized id95 for semantics-aware 
o v. w. anelli, v. bellini, t. di noia, w. la bruna, p. tomeo, e. di sciascio: an analysis on time- and session-
o a. ragone, p. tomeo, c. magarelli, t. di noia, m. palmonari, a. maurino, e. di sciascio: schema-

recommendations based on linked open data. eswc (1) 2017: 169-183

recommender systems. inf. process. manage. 53(2): 405-435 (2017)

aware diversification in recommender systems. umap 2017

summarization in linked-data-based feature selection for recommender systems. 32nd acm sigapp 
symposium on applied computing - 2017

systems. inf. sci. 382-383: 234-253(2017)

based recommender systems. ecir 2016: 729-734

id13s. acm tist 8(2): 21:1-21:21 (2017)

o t. di noia, j. rosati, p. tomeo, e. di sciascio: adaptive multi-attribute diversity for recommender
o s. oramas, v. c. ostuni, t. di noia, x. serra, e. di sciascio: sound and music recommendation with 
o c. musto, g. semeraro, m. de gemmis, p. lops: learning id27s from wikipedia for content-
o t. di noia, v. c. ostuni, p. tomeo, e. di sciascio: sprank: semantic path-based ranking for top-n 
o p. tomeo, i. fern  ndez-tob  as, t. di noia, i. cantador: exploiting linked open data in cold-start 
o i. fern  ndez-tob  as, p. tomeo, i. cantador, t. di noia, e. di sciascio: accuracy and diversity in cross-
domain recommendations for cold-start users with positive-only feedback. recsys 2016: 119-122
o c. musto, g. semeraro, m. de gemmis, p. lops: id27 techniques for content-based

recommendations using linked open data. acm tist 8(1): 9:1-9:34 (2016)

recommendations with positive-only feedback. ceri 2016: 11

recommender systems: an empirical evaluation. recsys posters 2015

readings

recommender systems. recommender systems handbook 2015: 119-159

open data features in graph-based recommender systems. cbrecsys@recsys 2015: 10-13

semantics-aware recommender systems
o c. musto, p. basile, m. de gemmis, p. lops, g. semeraro, s. rutigliano: automatic selection of linked
o m. de gemmis, p. lops, c. musto, f.narducci, g. semeraro: semantics-aware content-based
o p. tomeo, t. di noia, m. de gemmis, p. lops, g. semeraro, e. di sciascio: exploiting regression trees as
o t. di noia, v. c. ostuni: recommender systems and linked open data. reasoning web 2015: 88-113
o p. basile, c. musto, m. de gemmis, p. lops, f. narducci, g. semeraro: content-based recommender
systems + dbpedia knowledge = semantics-aware recommender systems. semwebeval@eswc
2014: 163-169

user models for intent-aware multi-attribute diversity. cbrecsys@recsys 2015: 2-9

recommendations. cbrecsys@recsys 2014: 49-56

neighborhood-based graph kernel. ec-web 2014: 89-100

for context-aware content-based recommendation. umap 2014: 381-392

o c. musto, p. basile, p. lops, m. de gemmis, g. semeraro: linked open data-enabled strategies for top-n 
o c. musto, g. semeraro, p. lops, m. de gemmis: combining id65 and entity linking 
o v. c. ostuni, t. di noia, r. mirizzi, e. di sciascio: a linked data recommender system using a 
o v. c. ostuni, t. di noia, e. di sciascio, r. mirizzi: top-n recommendations from implicit feedback 
o c. musto, g. semeraro, p. lops, m. de gemmis: contextual evsm: a content-based context-aware 
o t. di noia, r. mirizzi, v. c. ostuni, d. romito, m. zanker: linked open data to support content-based
o c. musto, f. narducci, p. lops, g. semeraro, m. de gemmis, m. barbieri, j. h. m. korst, v. pronk, r. clout: 

recommendation framework based on id65. ec-web 2013: 125-136

leveraging linked open data. recsys 2013: 85-92

recommender systems. i-semantics 2012: 1-8

enhanced semantic tv-show representation for personalized electronic program guides. umap 
2012: 188-199

readings

semantics-aware recommender systems
o m. degemmis, p. lops, g. semeraro: a content-collaborative recommender that exploits id138-based
o g. semeraro, m. degemmis, p. lops, p. basile: combining learning and id51 for 

user profiles for neighborhood formation. user model. user-adapt. interact. 17(3): 217-255 (2007)

intelligent user profiling. ijcai 2007: 2856-2861

cross-language recommender systems
o f. narducci, p. basile, c. musto, p. lops, a. caputo, m. de gemmis, l. iaquinta, g. semeraro: concept-

based item representations for a cross-lingual content-based recommendation process. inf. sci. 374: 
15-31 (2016)

o c. musto, f. narducci, p. basile, p. lops, m. de gemmis, g. semeraro: cross-language information 
o p. lops, c. musto, f. narducci, m. de gemmis, p. basile, g. semeraro: cross-language personalization 

filtering: id51 vs. distributional models. ai*ia 2011: 250-261

through a semantic content-based recommender system. aimsa 2010: 52-60

explanations
o c. musto, f. narducci, p. lops, m. de gemmis, g. semeraro: explod: a framework for explaining
recommendations based on the linked open data cloud. in proc. of the 10th acm conference on 
recommender systems (recsys '16). acm, new york, ny, usa, 151-154. 

serendipity
o m. de gemmis, p. lops, g. semeraro, c. musto. an investigation on the serendipity problem in 

recommender systems. information processing and management, 2015 doi: 10.1016/j.ipm.2015.06.008

