introduction to convolutional neural networks

jianxin wu

lamda group

national key lab for novel software technology

nanjing university, china

wujx2001@gmail.com

may 1, 2017

contents

1 introduction

2 preliminaries

2.1 tensor and vectorization . . . . . . . . . . . . . . . . . . . . . . .
2.2 vector calculus and the chain rule
. . . . . . . . . . . . . . . . .

3 id98 in a nutshell

3.1 the architecture . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 the forward run . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 stochastic id119 (sgd) . . . . . . . . . . . . . . . . .
3.4 error back propagation . . . . . . . . . . . . . . . . . . . . . . .

4 layer input, output and notations

5 the relu layer

6 the convolution layer

6.1 what is convolution? . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 why to convolve? . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3 convolution as matrix product
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
6.4 the kronecker product
. . . . . . . . . .
6.5 backward propagation: update the parameters
6.6 even higher dimensional indicator matrices
. . . . . . . . . . . .
6.7 backward propagation: prepare supervision signal for the previ-
ous layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.8 fully connected layer as a convolution layer . . . . . . . . . . . .

7 the pooling layer

1

2

3
3
4

5
5
6
6
8

9

10

11
11
13
15
17
17
19

20
22

23

8 a case study: the vgg-16 net

8.1 vgg-verydeep-16 . . . . . . . . . . . . . . . . . . . . . . . . . .
8.2 receptive    eld . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9 remarks

exercises

1

introduction

25
25
27

28

28

this is a note that describes how a convolutional neural network (id98) op-
erates from a mathematical perspective. this note is self-contained, and the
focus is to make it comprehensible to beginners in the id98    eld.

the convolutional neural network (id98) has shown excellent performance
in many id161 and machine learning problems. many solid papers
have been published on this topic, and quite some high quality open source id98
software packages have been made available.

there are also well-written id98 tutorials or id98 software manuals. how-
ever, i believe that an introductory id98 material speci   cally prepared for be-
ginners is still needed. research papers are usually very terse and lack details.
it might be di   cult for beginners to read such papers. a tutorial targeting
experienced researchers may not cover all the necessary details to understand
how a id98 runs.

this note tries to present a document that
    is self-contained. it is expected that all required mathematical background
knowledge are introduced in this note itself (or in other notes for this
course);

    has details for all the derivations. this note tries to explain all the nec-
essary math in details. we try not to ignore an important step in a
derivation. thus, it should be possible for a beginner to follow (although
an expert may feel this note tautological.)

    ignores implementation details. the purpose is for a reader to under-
stand how a id98 runs at the mathematical level. we will ignore those
implementation details. in id98, making correct choices for various im-
plementation details is one of the keys to its high accuracy (that is,    the
devil is in the details   ). however, we intentionally left this part out,
in order for the reader to focus on the mathematics. after understand-
ing the mathematical principles and details, it is more advantageous to
learn these implementation and design details with hands-on experience
by playing with id98 programming.

id98 is useful in a lot of applications, especially in image related tasks. ap-
plications of id98 include image classi   cation, image semantic segmentation,

2

id164 in images, etc. we will focus on image classi   cation (or catego-
rization) in this note. in image categorization, every image has a major object
which occupies a large portion of the image. an image is classi   ed into one of
the classes based on the identity of its main object, e.g., dog, airplane, bird, etc.

2 preliminaries

we start by a discussion of some background knowledge that are necessary in
order to understand how a id98 runs. one can ignore this section if he/she is
familiar with these basics.

2.1 tensor and vectorization

everybody is familiar with vectors and matrices. we use a symbol shown in
boldface to represent a vector, e.g., x     rd is a column vector with d elements.
we use a capital letter to denote a matrix, e.g., x     rh  w is a matrix with
h rows and w columns. the vector x can also be viewed as a matrix with 1
column and d rows.
these concepts can be generalized to higher-order matrices, i.e., tensors. for
example, x     rh  w  d is an order 3 (or third order) tensor. it contains hw d
elements, and each of them can be indexed by an index triplet (i, j, d), with
0     i < h, 0     j < w , and 0     d < d. another way to view an order 3 tensor
is to treat it as containing d channels of matrices. every channel is a matrix
with size h    w . the    rst channel contains all the numbers in the tensor that
are indexed by (i, j, 0). when d = 1, an order 3 tensor reduces to a matrix.

we have interacted with tensors day-to-day. a scalar value is a zeroth-order
(order 0) tensor; a vector is an order 1 tensor; and a matrix is a second order
tensor. a color image is in fact an order 3 tensor. an image with h rows and
w columns is a tensor with size h    w    3: if a color image is stored in the
rgb format, it has 3 channels (for r, g and b, respectively), and each channel
is a h    w matrix (second order tensor) that contains the r (or g, or b) values
of all pixels.

it is bene   cial to represent images (or other types of raw data) as a tensor.
in early id161 and pattern recognition, a color image (which is an
order 3 tensor) is often converted to the gray-scale version (which is a matrix)
because we know how to handle matrices much better than tensors. the color
information is lost during this conversion. but color is very important in various
image (or video) based learning and recognition problems, and we do want to
process color information in a principled way, e.g., as in id98.

tensors are essential in id98. the input, intermediate representation, and
parameters in a id98 are all tensors. tensors with order higher than 3 are
also widely used in a id98. for example, we will soon see that the convolution
kernels in a convolution layer of a id98 form an order 4 tensor.

given a tensor, we can arrange all the numbers inside it into a long vec-
tor, following a pre-speci   ed order. for example, in matlab, the (:) operator

3

converts a matrix into a column vector in the column-   rst order. an example
is:

a =

, a(:) = (1, 3, 2, 4)t =

(1)

(cid:20) 1

3

(cid:21)

2
4

             .

             1

3
2
4

in mathematics, we use the notation    vec    to represent this vectorization
operator. that is, vec(a) = (1, 3, 2, 4)t in the example in equation 1. in order
to vectorize an order 3 tensor, we could vectorize its    rst channel (which is a
matrix and we already know how to vectorize it), then the second channel, . . . ,
till all channels are vectorized. the vectorization of the order 3 tensor is then
the concatenation of the vectorization of all the channels in this order.

the vectorization of an order 3 tensor is a recursive process, which utilizes
the vectorization of order 2 tensors. this recursive process can be applied to
vectorize an order 4 (or even higher order) tensor in the same manner.

2.2 vector calculus and the chain rule

the id98 learning process depends on vector calculus and the chain rule. sup-
pose z is a scalar (i.e., z     r) and y     rh is a vector. if z is a function of y,
then the partial derivative of z with respect to y is a vector, de   ned as

   z
   yi

.

(2)

=

i

in other words,    z

   y is a vector having the same size as y, and its i-th element

is    z
   yi

. also note that    z

   yt =

furthermore, suppose x     rw is another vector, and y is a function of x.

then, the partial derivative of y with respect to x is de   ned as

=

   yi
   xj

.

(3)

   xt

ij

this partial derivative is a h    w matrix, whose entry at the intersection of
the i-th row and j-th column is    yi
   xj

it is easy to see that z is a function of x in a chain-like argument: a function
maps x to y, and another function maps y to z. the chain rule can be used to
compute    z

.

   xt , as

   z
   xt =

   z
   yt

   y
   xt .

(4)

a sanity check for equation 4 is to check the matrix / vector dimensions.

   yt is a row vector with h elements, or a 1  h matrix. (be reminded
   xt is an h    w matrix, the vector / matrix

note that    z
that    z
multiplication between them is valid, and the result should be a row vector with
w elements, which matches the dimensionality of    z

   y is a column vector). since    y

   xt .

4

   y

(cid:20)    z
(cid:16)    z
(cid:17)t
(cid:20)    y

   y

(cid:21)

.

(cid:21)

for speci   c rules to calculate partial derivatives of vectors and matrices,

please refer to the matrix cookbook.

3 id98 in a nutshell

in this section, we will see how a id98 trains and predicts in the abstract level,
with the details left out for later sections.

3.1 the architecture

a id98 usually takes an order 3 tensor as its input, e.g., an image with h
rows, w columns, and 3 channels (r, g, b color channels). higher order tensor
inputs, however, can be handled by id98 in a similar fashion. the input then
sequentially goes through a series of processing. one processing step is usually
called a layer, which could be a convolution layer, a pooling layer, a normal-
ization layer, a fully connected layer, a loss layer, etc. we will introduce the
details of these layers later in this note.1

for now, let us give an abstract description of the id98 structure    rst.

x1        w1        x2                      xl   1        wl   1        xl        wl        z (5)

the above equation 5 illustrates how a id98 runs layer by layer in a forward
pass. the input is x1, usually an image (order 3 tensor). it goes through the
processing in the    rst layer, which is the    rst box. we denote the parameters
involved in the    rst layer   s processing collectively as a tensor w1. the output of
the    rst layer is x2, which also acts as the input to the second layer processing.
this processing proceeds till all layers in the id98 has been    nished, which
outputs xl. one additional layer, however, is added for backward error propa-
gation, a method that learns good parameter values in the id98. let   s suppose
the problem at hand is an image classi   cation problem with c classes. a com-
monly used strategy is to output xl as a c dimensional vector, whose i-th
entry encodes the prediction (posterior id203 of x1 comes from the i-th
class). to make xl a id203 mass function, we can set the processing in the
(l     1)-th layer as a softmax transformation of xl   1 (cf. the distance metric
and data transformation note). in other applications, the output xl may have
other forms and interpretations.

the last layer is a loss layer. let us suppose t is the corresponding target
(ground-truth) value for the input x1, then a cost or id168 can be used
to measure the discrepancy between the id98 prediction xl and the target t.
for example, a simple id168 could be

z =

1
2

(cid:107)t     xl(cid:107)2 ,

(6)

1we will give detailed introductions to three types of layers: convolution, pooling, and
relu, which are the key parts of almost all id98 models. proper id172, e.g., batch
id172 or cross-layer id172 is important in the optimization process for learning
good parameters in a id98. i may add these contents in the next update.

5

although more complex id168s are usually used. this squared (cid:96)2 loss can
be used in a regression problem. in a classi   cation problem, the cross id178
loss is often used. the ground-truth in a classi   cation problem is a categorical
variable t. we    rst convert the categorical variable t to a c dimensional vector
t (cf. the distance metric and data transformation note). now both t and xl
are id203 mass functions, and the cross id178 loss measures the distance
between them. hence, we can minimize the cross id178 (cf. the information
theory note.) equation 5 explicitly models the id168 as a loss layer,
whose processing is modeled as a box with parameters wl.

note that some layers may not have any parameters, that is, wi may be

empty for some i. the softmax layer is one such example.

3.2 the forward run
suppose all the parameters of a id98 model w1, . . . , wl   1 have been learned,
then we are ready to use this model for prediction. prediction only involves run-
ning the id98 model forward, i.e., in the direction of the arrows in equation 5.
let   s take the image classi   cation problem as an example. starting from
the input x1, we make it pass the processing of the    rst layer (the box with
parameters w1), and get x2. in turn, x2 is passed into the second layer, etc.
finally, we achieve xl     rc, which estimates the posterior probabilities of x1
belonging to the c categories. we can output the id98 prediction as

arg max

i

xl
i .

(7)

note that the loss layer is not needed in prediction. it is only useful when
we try to learn id98 parameters using a set of training examples. now, the
problem is: how do we learn the model parameters?

3.3 stochastic id119 (sgd)

as in many other learning systems, the parameters of a id98 model are opti-
mized to minimize the loss z, i.e., we want the prediction of a id98 model to
match the ground-truth labels.

let   s suppose one training example x1 is given for training such parameters.
the training process involves running the id98 network in both directions. we
   rst run the network in the forward pass to get xl to achieve a prediction using
the current id98 parameters. instead of outputting a prediction, we need to
compare the prediction with the target t corresponding to x1, that is, continue
running the forward pass till the last loss layer. finally, we achieve a loss z.

the loss z is then a supervision signal, guiding how the parameters of the
model should be modi   ed (updated). and the sgd way of modifying the pa-
rameters is

wi        wi       

   z
   wi .

(8)

6

figure 1: illustration of the id119 method.

a cautious note about the notation. in most id98 materials, a superscript
indicates the    time    (e.g., training epochs). but in this note, we use the su-
perscript to denote the layer index. please do not get confused. we do not
use an additional index variable to represent time. in equation 8, the        sign
implicitly indicates that the parameters wi (of the i-layer) are updated from
time t to t + 1. if a time index t is explicitly used, this equation will look like

(cid:0)wi(cid:1)t+1

=(cid:0)wi(cid:1)t       

   z

    (wi)t .

(9)

in equation 8, the partial derivative    z

   wi measures the rate of increase of z
with respect to the changes in di   erent dimensions of wi. this partial deriva-
tive vector is called the gradient in mathematical optimization. hence, in a
small local region around the current value of wi, to move wi in the direction
determined by the gradient will increase the objective value z. in order to min-
imize the id168, we should update wi along the opposite direction of the
gradient. this updating rule is called the id119. id119 is
illustrated in figure 1, in which the gradient is denoted by g.

if we move too far in the negative gradient direction, however, the loss
function may increase. hence, in every update we only change the parameters
by a small proportion of the negative gradient, controlled by    (the learning
rate).    > 0 is usually set to a small number (e.g.,    = 0.001). one update
based on x1 will make the loss smaller for this particular training example if the
learning rate is not too large. however, it is very possible that it will make the
loss of some other training examples become larger. hence, we need to update
the parameters using all training examples. when all training examples have
been used to update the parameters, we say one epoch has been processed. one
epoch will in general reduce the average loss on the training set until the learning
system over   ts the training data. hence, we can repeat the id119
updating epochs and terminate at some point to obtain the id98 parameters
(e.g., we can terminate when the average loss on a validation set increases).

7

id119 may seem simple in its math form (equation 8), but it is
a very tricky operation in practice. for example, if we update the parameters
using only gradient calculated from only one training example, we will observe
an unstable id168: the average loss of all training examples will bounce
up and down at very high frequency. this is because the gradient is estimated
using only one training example instead of the entire training set. updating
the parameters using the gradient estimated from a (usually) small subset of
training examples is called the stochastic id119. contrary to single
example based sgd, we can compute the gradient using all training examples
and then update the parameters. however, this batch processing requires a lot
of computations because the parameters are updated only once in an epoch, and
is hence impractical, especially when the number of training examples is large.
a compromise is to use a mini-batch of training examples, to compute the
gradient using this mini-batch, and to update the parameters correspondingly.
for example, we can set 32 or 64 examples as a mini-batch. stochastic gradient
descent (sgd) (using the mini-batch strategy) is the mainstream method to
learn a id98   s parameters. we also want to note that when mini-batch is used,
the input of the id98 becomes an order 4 tensor, e.g., h    w    3    32 if the
mini-batch size is 32.

a new problem now becomes apparent: how to compute the gradient, which

seems a very complex task?

3.4 error back propagation

the last layer   s partial derivatives are easy to compute. because xl is connected
   z
to z directly under the control of parameters wl, it is easy to compute
   wl .
this step is only needed when wl is not empty. in the same spirit, it is also
easy to compute    z
   xl . for example, if the squared (cid:96)2 loss is used, we have an
   xl = xl     t.
   wl , and    z
empty    z

in fact, for every layer, we compute two sets of gradients: the partial deriva-

tives of z with respect to the layer parameters wi, and that layer   s input xi.

    the term    z

(i-th) layer   s parameters;

   wi , as seen in equation 8, can be used to update the current

    the term    z

   xi can be used to update parameters backwards, e.g., to the
(i     1)-th layer. an intuitive explanation is: xi is the output of the
(i     1)-th layer and    z
is how xi should be changed to reduce the loss
function. hence, we could view    z
   xi as the part of the    error    supervision
information propagated from z backward till the current layer, in a layer
by layer fashion. thus, we can continue the back propagation process,
and use    z

   xi to propagate the errors backward to the (i     1)-th layer.

   xi

this layer-by-layer backward updating procedure makes learning a id98 much
easier.

let   s take the i-th layer as an example. when we are updating the i-th layer,
the back propagation process for the (i + 1)-th layer must have been    nished.

8

that is, we already computed the terms
memory and ready for use.

   z

   wi+1 and

   z
   xi+1 . both are stored in

now our task is to compute    z

   wi and    z

   xi . using the chain rule, we have

   z

   (vec(wi)t )

   z

   (vec(xi)t )

=

=

   z

   (vec(xi+1)t )

   z

   (vec(xi+1)t )

    vec(xi+1)
   (vec(wi)t )
    vec(xi+1)
   (vec(xi)t )

,

.

(10)

(11)

   z

since

   z
   xi+1 is already computed and stored in memory, it requires just a
matrix reshaping operation (vec) and an additional transpose operation to get
   (vec(xi+1)t ) , which is the    rst term in the right hand side (rhs) of both equa-
tions. so long as we can compute     vec(xi+1)
   (vec(xi)t ) , we can easily get
what we want (the left hand side of both equations).

   (vec(wi)t ) and     vec(xi+1)

   z

   (vec(wi)t ) and     vec(xi+1)
    vec(xi+1)
   (vec(wi)t ) and

   (vec(xi)t ) are much easier to compute than directly comput-
   (vec(xi)t ) , because xi is directly related to xi+1, through
ing
a function with parameters wi. the details of these partial derivatives will be
discussed in the following sections.

   z

4 layer input, output and notations

now that the id98 architecture is clear, we will discuss in detail the di   erent
types of layers, starting from the relu layer, which is the simplest layer among
those we discuss in this note. but before we start, we need to further re   ne our
notations.
suppose we are considering the l-th layer, whose inputs form an order 3
tensor xl with xl     rh l  w l  dl
. thus, we need a triplet index set (il, jl, dl) to
locate any speci   c element in xl. the triplet (il, jl, dl) refers to one element in
xl, which is in the dl-th channel, and at spatial location (il, jl) (at the il-th row,
and jl-th column). in actual id98 learning, the mini-batch strategy is usually
used. in that case, xl becomes an order 4 tensor in rh l  w l  dl  n where n
is the mini-batch size. for simplicity we assume that n = 1 in this note. the
results in this section, however, are easy to adopt to mini-batch versions.
in order to simplify the notations which will appear later, we follow the
zero-based indexing convention, which speci   es that 0     il < h l, 0     jl < w l,
and 0     dl < dl.

in the l-th layer, a function will transform the input xl to an output y,
which is also the input to the next layer. thus, we notice that y and xl+1 in
fact refers to the same object, and it is very helpful to keep this point in mind.
we assume the output has size h l+1  w l+1  dl+1, and an element in the output
is indexed by a triplet (il+1, jl+1, dl+1), 0     il+1 < h l+1, 0     jl+1 < w l+1,
0     dl+1 < dl+1.

9

5 the relu layer

a relu layer does not change the size of the input, that is, xl and y share the
same size. in fact, the recti   ed linear unit (hence the name relu) can be
regarded as a truncation performed individually for every element in the input:

yi,j,d = max{0, xl

i,j,d} ,

(12)

with 0     i < h l = h l+1, 0     j < w l = w l+1, and 0     d < dl = dl+1.

there is no parameter inside a relu layer, hence no need for parameter

learning in this layer.

based on equation 12, it is obvious that

=(cid:113)xl

i,j,d > 0(cid:121) ,

dyi,j,d
dxl

i,j,d

wise.

where(cid:74)  (cid:75) is the indicator function, being 1 if its argument is true, and 0 other-
hence, we have (cid:20)    z

i,j,d > 0

if xl

(cid:21)

(cid:21)

=

   xl

i,j,d

i,j,d

otherwise

0

.

               
(cid:20)    z

   y

(13)

(14)

note that y is an alias for xl+1.

strictly speaking, the function max(0, x) is not di   erentiable at x = 0, hence
equation 13 is a little bit problematic in theory. in practice, it is not an issue
and relu is safe to use.

if we treat xl

the purpose of relu is to increase the nonlinearity of the id98. since the
semantic information in an image (e.g., a person and a husky dog sitting next
to each other on a bench in a garden) is obviously a highly nonlinear mapping
of pixel values in the input, we want the mapping from id98 input to its output
also be highly nonlinear. the relu function, although simple, is a nonlinear
function, as illustrated in figure 2.
i,j,d as one of the h lw ldl features extracted by id98 layers 1
to l     1, which can be positive, zero or negative. for example, xl
i,j,d may be
positive if a region inside the input image has certain patterns (like a dog   s head
or a cat   s head or some other patterns similar to that); and xl
i,j,d is negative or
zero when that region does not exhibit these patterns. the relu layer will set
all negative values to 0, which means that yl
i,j,d will be activated only for images
possessing these patterns at that particular region. intuitively, this property is
useful for recognizing complex patterns and objects. for example, it is only
a weak evidence to support    the input image contains a cat    if a feature is
activated and that feature   s pattern looks like cat   s head. however, if we    nd
many activated features after the relu layer whose target patterns correspond
to cat   s head, torso, fur, legs, etc., we have higher con   dence (at layer l + 1) to
say that a cat probably exists in the input image.

10

figure 2: the relu function.

1

other nonlinear transformations have been used in the neural network re-
search to produce nonlinearity, for example, the logistic sigmoid function y =
  (x) =
1+exp(   x) . however, logistic sigmoid works signi   cantly worse than
relu in id98 learning. note that 0 < y < 1 if a sigmoid function is used, and
dx = y(1     y), we have dy
dy
4 . hence, in the error back propagation process,
the gradient    z
   y (at most
1
4 ). in other words, a sigmoid layer will cause the magnitude of the gradient
to signi   cantly reduce, and after several sigmoid layers, the gradient will vanish
(i.e., all its components will be close to 0). a vanishing gradient makes gradient
based learning (e.g., sgd) very di   cult.

dx     1
dx will have much smaller magnitude than    z

   x =    z

   y

dy

on the other hand, the relu layer sets the gradient of some features in the
l-th layer to 0, but these features are not activated (i.e., we are not interested
in them). for those activated features, the gradient is back propagated without
any change, which is bene   cial for sgd learning. the introduction of relu to
replace sigmoid is an important change in id98, which signi   cantly reduces the
di   culty in learning id98 parameters and improves its accuracy. there are also
more complex variants of relu, for example, parametric relu and exponential
linear unit.

6 the convolution layer

next, we turn to the convolution layer, which is the most involved one among
those we discuss in this note.

6.1 what is convolution?

let us start by convolving a matrix with one single convolution kernel. suppose
the input image is 3    4 and the convolution kernel size is 2    2, as illustrated
in figure 3.

11

(a) a 2    2 kernel

(b) the convolution input and output

figure 3: illustration of the convolution operation.

if we overlap the convolution kernel on top of the input image, we can
compute the product between the numbers at the same location in the kernel
and the input, and we get a single number by summing these products together.
for example, if we overlap the kernel with the top left region in the input, the
convolution result at that spatial location is: 1    1 + 1    4 + 1    2 + 1    5 = 12.
we then move the kernel down by one pixel and get the next convolution result
as 1   4 + 1   7 + 1   5 + 1   8 = 24. we keep move the kernel down till it reaches
the bottom border of the input matrix (image). then, we return the kernel to
the top, and move the kernel to its right by one element (pixel). we repeat the
convolution for every possible pixel location until we have moved the kernel to
the bottom right corner of the input image, as shown in figure 3.
for order 3 tensors, the convolution operation is de   ned similarly. suppose
the input in the l-th layer is an order 3 tensor with size h l    w l    dl. a
convolution kernel is also an order 3 tensor with size h    w    dl. when we
overlap the kernel on top of the input tensor at the spatial location (0, 0, 0),
we compute the products of corresponding elements in all the dl channels and
sum the hw dl products to get the convolution result at this spatial location.
then, we move the kernel from top to bottom and from left to right to complete
the convolution.
in a convolution layer, multiple convolution kernels are usually used. as-
suming d kernels are used and each kernel is of spatial span h    w , we denote
all the kernels as f . f is an order 4 tensor in rh  w  dl  d. similarly, we use
index variables 0     i < h, 0     j < w , 0     dl < dl and 0     d < d to pinpoint
a speci   c element in the kernels. also note that the set of kernels f refers to
the same object as the notation wl in equation 5. we change the notation a
bit to make the derivation a little bit simpler. it is also clear that even if the
mini-batch strategy is used, the kernels remain unchanged.
as shown in figure 3, the spatial extent of the output is smaller than that
of the input so long as the convolution kernel is larger than 1    1. sometimes
we need the input and output images to have the same height and width, and a
simple padding trick can be used. if the input is h l  w l  dl and the kernel size
is h  w   dl  d, the convolution result has size (h l   h +1)  (w l   w +1)  d.
2 (cid:99) rows above the    rst
for every channel of the input, if we pad (i.e., insert) (cid:98) h   1

12

                                                        2 (cid:99) rows below the last row, and pad (cid:98) w   1

row and (cid:98) h
2 (cid:99) columns to the left of
2 (cid:99) columns to the right of the last column of the input,
the    rst column and (cid:98) w
the convolution output will be h l    w l    d in size, i.e., having the same spatial
extent as the input. (cid:98)  (cid:99) is the    oor functions. elements of the padded rows and
columns are usually set to 0, but other values are also possible.

stride is another important concept in convolution. in figure 3, we convolve
the kernel with the input at every possible spatial location, which corresponds
to the stride s = 1. however, if s > 1, every movement of the kernel skip
s     1 pixel locations (i.e., the convolution is performed once every s pixels both
horizontally and vertically).

in this section, we consider the simple case when the stride is 1 and no
, with h l+1 =

padding is used. hence, we have y (or xl+1) in rh l+1  w l+1  dl+1
h l     h + 1, w l+1 = w l     w + 1, and dl+1 = d.

in precise mathematics, the convolution procedure can be expressed as an

equation:

yil+1,jl+1,d =

h(cid:88)

w(cid:88)

dl(cid:88)

fi,j,dl,d    xl

il+1+i,jl+1+j,dl .

(15)

i=0

j=0

dl=0

equation 15 is repeated for all 0     d     d = dl+1, and for any spatial location
(il+1, jl+1) satisfying 0     il+1 < h l     h + 1 = h l+1, 0     jl+1 < w l     w + 1 =
w l+1. in this equation, xl
il+1+i,jl+1+j,dl refers to the element of xl indexed by
the triplet (il+1 + i, jl+1 + j, dl).

a bias term bd is usually added to yil+1,jl+1,d. we omit this term in this

note for clearer presentation.

6.2 why to convolve?

(cid:104) 1

(cid:105)

figure 4 shows a color input image (4a) and its convolution results using two
di   erent kernels (4b and 4c). a 3    3 convolution matrix k =
is
used. the convolution kernel should be of size 3    3    3, in which we set every
channel to k. when there is a horizontal edge at location (x, y) (i.e., when the
pixels at spatial location (x + 1, y) and (x     1, y) di   er by a large amount), we
expect the convolution result to have high magnitude. as shown in figure 4b,
the convolution results indeed highlight the horizontal edges. when we set every
channel of the convolution kernel to k t (the transpose of k), the convolution
result ampli   es vertical edges, as shown in figure 4c. the matrix (or    lter) k
and k t are called the sobel operators.2

1
0
0
   1    2    1

2
0

if we add a bias term to the convolution operation, we can make the convo-
lution result positive at horizontal (vertical) edges in a certain direction (e.g.,
a horizontal edge with the pixels above it brighter than the pixels below it),
and negative at other locations. if the next layer is a relu layer, the output
of the next layer in fact de   nes many    edge detection features   , which activate

2the sobel operator is named after irwin sobel, an american researcher in digital image

processing.

13

(a) lenna

(b) horizontal edge

(c) vertical edge

figure 4: the lenna image and the e   ect of di   erent convolution kernels.

only at horizontal or vertical edges in certain directions. if we replace the so-
bel kernel by other kernels (e.g., those learned by sgd), we can learn features
that activate for edges with di   erent angles. when we move further down in the
deep network, subsequent layers can learn to activate only for speci   c (but more
complex) patterns, e.g., groups of edges that form a particular shape. these
more complex patterns will be further assembled by deeper layers to activate for
semantically meaningful object parts or even a particular type of object, e.g.,
dog, cat, tree, beach, etc.

one more bene   t of the convolution layer is that all spatial locations share
the same convolution kernel, which greatly reduces the number of parameters
needed for a convolution layer. for example, if multiple dogs appear in an input
image, the same    dog-head-like pattern    feature will be activated at multiple
locations, corresponding to heads of di   erent dogs.

in a deep neural network setup, convolution also encourages parameter shar-
ing. for example, suppose    dog-head-like pattern    and    cat-head-like pattern   
are two features learned by a deep convolutional network. the id98 does not
need to devote two sets of disjoint parameters (e.g., convolution kernels in mul-
tiple layers) for them. the id98   s bottom layers can learn    eye-like pattern   
and    animal-fur-texture pattern   , which are shared by both these more abstract
features. in short, the combination of convolution kernels and deep and hier-
archical structures are very e   ective in learning good representations (features)
from images for visual recognition tasks.

we want to add a note here. although we have used phrases such as    dog-
head-like pattern   , the representation or feature learned by a id98 may not
correspond exactly to semantic concepts such as    dog   s head   . a id98 feature
may activate frequently for dogs    heads and often be deactivated for other types
of patterns. however, there are also possible false activations at other locations,
and possible deactivations at dogs    heads.

in fact, a key concept in id98 (or more generally deep learning) is distributed
representation. for example, suppose our task is to recognize n di   erent types
of objects and a id98 extracts m features from any input image. it is most

14

likely that any one of the m features is useful for recognizing all n object
categories; and to recognize one object type requires the joint e   ort of all m
features.

6.3 convolution as matrix product

equation 15 seems pretty complex. there is a way to expand xl and simplify
the convolution as a matrix product.
let   s consider a special case with dl = d = 1, h = w = 2, and h l = 3,
w l = 4. that is, we consider convolving a small single channel 3   4 matrix (or
image) with one 2    2    lter. using the example in figure 3, we have

       1

4
7

2 3 1
5 6 1
8 9 1

          

(cid:20) 1

1

(cid:21)

1
1

=

(cid:20) 12

24

(cid:21)

16
28

11
17

,

(16)

where the    rst matrix is denoted as a, and     is the convolution operator.

now let   s run a matlab command b=im2col(a,[2 2]), we arrive at a b

matrix that is an expanded version of a:

             1

4
2
5

b =

             .

4
7
5
8

2
5
3
6

5
8
6
9

3
6
1
1

6
9
1
1

it is obvious that the    rst column of b corresponds to the    rst 2    2 region
in a, in a column-   rst order, corresponding to (il+1, jl+1) = (0, 0). similarly,
the second to last column in b correspond to regions in a with (il+1, jl+1) being
(1, 0), (0, 1), (1, 1), (0, 2) and (1, 2), respectively. that is, the matlab im2col
function explicitly expands the required elements for performing each individual
convolution into a column in the matrix b. the transpose of b, bt , is called
the im2row expansion of a.

now, if we vectorize the convolution kernel itself into a vector (in the same

column-   rst order) (1, 1, 1, 1)t , we    nd that3

             =

             1

1
1
1

                        

                         .

12
24
16
28
11
17

bt

(17)

3the notation and presentation of this note is heavily a   ected by the matconvnet software
package   s manual (http://arxiv.org/abs/1412.4564, which is matlab based). the transpose
of an im2col expansion is equivalent to an im2row expansion, in which the numbers involved
in one convolution is one row in the im2row expanded matrix. the derivation in this section
uses im2row, complying with the implementation in matconvnet. ca   e, a widely used id98
software package (http://caffe.berkeleyvision.org/, which is c++ based) uses im2col.
these formulations are mathematically equivalent to each other.

15

if we reshape this resulting vector in equation 17 properly, we get the exact
convolution result matrix in equation 16. that is, the convolution operator is
a linear one. we can multiply the expanded input matrix and the vectorized
   lter to get a result vector, and by reshaping this vector properly we get the
correct convolution results.

we can generalize this idea to more complex situations and formalize them.
if dl > 1 (that is, the input xl has more than one channels), the expansion
operator could    rst expand the    rst channel of xl, then the second, . . . , till all
dl channels are expanded. the expanded channels will be stacked together,
that is, one row in the im2row expansion will have h    w    dl elements, rather
than h    w .

more formally, suppose xl is a third order tensor in rh l  w l  dl

, with one
element in xl being indexed by a triplet (il, jl, dl). we also consider a set of
convolution kernels f , whose spatial extent are all h    w . then, the expansion
operator (im2row) converts xl into a matrix   (xl). we use two indexes (p, q)
to index an element in this matrix. the expansion operator copies the element
at (il, jl, dl) in xl to the (p, q)-th entry in   (xl).

from the description of the expansion process, it is clear that given a    xed

(p, q), we can calculate its corresponding (il, jl, dl) triplet, because obviously

p =

q =
il =
jl =

il+1 + (h l     h + 1)    jl+1 ,
i + h    j + h    w    dl ,
il+1 + i ,
jl+1 + j .

(18)

(19)

(20)

(21)

in equation 19, dividing q by hw and take the integer part of the quotient,
we can determine which channel (dl) does it belong to. similarly, we can get the
o   sets inside the convolution kernel as (i, j), in which 0     i < h and 0     j < w .
q completely determines one speci   c location inside the convolution kernel by
the triplet (i, j, dl).
note that the convolution result is xl+1, whose spatial extent is h l+1 =
h l     h + 1 and w l+1 = w l     w + 1. thus, in equation 18, the remainder
and quotient of dividing p by h l+1 = h l     h + 1 will give us the o   set in the
convolved result (il+1, jl+1), or, the top-left spatial location of the region in xl
(which is to be convolved with the kernel).

based on the de   nition of convolution, it is clear that we can use equa-
tions 20 and 21 to    nd the o   set in the input xl as il = il+1 +i and jl = jl+1 +j.
that is, the mapping from (p, q) to (il, jl, dl) is one-to-one. however, we want
to emphasize that the reverse mapping from (il, jl, dl) to (p, q) is one-to-many, a
fact that is useful in deriving the back propagation rules in a convolution layer.
now we use the standard vec operator to convert the set of convolution
kernels f (order 4 tensor) into a matrix. let   s start from one kernel, which
can be vectorized into a vector in rhw dl
. thus, all convolution kernels can
be reshaped into a matrix with hw dl rows and d columns (remember that
dl+1 = d.) let   s call this matrix f .

16

finally, with all these notations, we have a beautiful equation to calculate

convolution results (cf. equation 17, in which   (xl) is bt ):

vec(y) = vec(xl+1) = vec(cid:0)  (xl)f(cid:1) .

(22)
note that vec(y)     rh l+1w l+1d,   (xl)     r(h l+1w l+1)  (hw dl), and f    
r(hw dl)  d. the id127   (xl)f results in a matrix of size
(h l+1w l+1)    d. the vectorization of this resultant matrix generates a vector
in rh l+1w l+1d, which matches the dimensionality of vec(y).

6.4 the kronecker product

a short detour to the kronecker product is needed to compute the derivatives.
given two matrices a     rm  n and b     rp  q, the kronecker product a    b
is a mp    nq matrix, de   ned as a block matrix

          a11b       

. . .
am1b       

...

          .

a1nb

...

amnb

a     b =

(23)

the kronecker product has the following properties that will be useful for

us:

(a     b)t = at     bt ,
vec(axb) = (bt     a) vec(x) ,

(24)

(25)

for matrices a, x, and b with proper dimensions (e.g., when the matrix mul-
tiplication axb is de   ned.) note that equation 25 can be utilized from both
directions.

with the help of    , we can write down

vec(y) = vec(cid:0)  (xl)f i(cid:1) =(cid:0)i       (xl)(cid:1) vec(f ) ,
vec(y) = vec(cid:0)i  (xl)f(cid:1) = (f t     i) vec(  (xl)) ,

(26)

(27)

where i is an identity matrix of proper size. in equation 26, the size of i is
determined by the number of columns in f , hence i     rd  d in equation 26.
similarly, in equation 27, i     r(h l+1w l+1)  (h l+1w l+1).

the derivation for gradient computation rules in a convolution layer involves
many variables and notations. we summarize the variables used in this deriva-
tion in table 1. note that some of these notations have not been introduced
yet.

6.5 backward propagation: update the parameters

as previously mentioned, we need to compute two derivatives:
    vec(f ) , where the    rst term

    vec(xl) and
    vec(xl) will be used for backward propagation

   z

   z

   z

17

table 1: variables, their sizes and meanings. note that    alias    means a variable
has a di   erent name or can be reshaped into another form.

x
f
y

  (xl)

m
   z
   y
   z
   f
   z
   x

alias
xl
f , wl
y, xl+1 h l+1w l+1    dl+1, the output, dl+1 = d

size & meaning
h lw l    dl, the input tensor
hw dl    d, d kernels, each h    w and dl channels

h l+1w l+1    hw dl, the im2row expansion of xl
h l+1w l+1hw dl    h lw ldl, the indicator matrix for   (xl)
h l+1w l+1    dl+1, gradient for y
hw dl    d, gradient to update the convolution kernels
    vec(xl) h lw l    dl, gradient for xl, useful for back propagation

    vec(f )

    vec(y)

   z

   z

   z

to the previous ((l     1)-th) layer, and the second term will determine how the
parameters of the current (l-th) layer will be updated. a friendly reminder
is to remember that f , f and wi refer to the same thing (modulo reshaping
of the vector or matrix or tensor). similarly, we can reshape y into a matrix
y     r(h l+1w l+1)  d, then y, y and xl+1 refer to the same object (again modulo
reshaping).

from the chain rule (equation 10), it is easy to compute

   z

    vec(f ) as

   z

   (vec(f ))t =

   z

    vec(y)

   (vec(y )t )

   (vec(f )t )

.

(28)

the    rst term in the rhs is already computed in the (l + 1)-th layer as (equiva-
   (vec(xl+1))t . the second term, based on equation 26, is pretty straight-
lently)
forward:

   (cid:0)(cid:0)i       (xl)(cid:1) vec(f )(cid:1)

   z

= i       (xl) .

(29)

    vec(y)

   (vec(f )t )

=

note that we have used the fact    xat
multiplications are well de   ned. this equation leads to

   at = x so long as the matrix

   (vec(f )t )
   a = x or    xa

   z

   (vec(f ))t =

   z

   (vec(y)t )

(i       (xl)) .

making a transpose, we get

   z

    vec(f )

(cid:19)

   z

    vec(y)

=(cid:0)i       (xl)(cid:1)t
(cid:18)    z
=(cid:0)i       (xl)t(cid:1) vec
(cid:19)
(cid:19)

(cid:18)
(cid:18)

  (xl)t    z
   y
  (xl)t    z
   y

= vec

= vec

   y

i

.

18

(30)

(31)

(32)

(33)

(34)

note that both equation 25 (from rhs to lhs) and equation 24 are used in
the above derivation.

thus, we conclude that

   z
   f

=   (xl)t    z
   y

,

(35)

which is a simple rule to update the parameters in the l-th layer: the gradient
with respect to the convolution parameters is the product between   (xl)t (the
im2col expansion) and    z
   y (the supervision signal transferred from the (l+1)-th
layer).

6.6 even higher dimensional indicator matrices
the function   (  ) has been very useful in our analysis. it is pretty high dimen-
sional, e.g.,   (xl) has h l+1w l+1hw dl elements. from the above, we know
that an element in   (xl) is indexed by a pair p and q.

a quick recap about   (xl): 1) from q we can determine dl, which channel
of the convolution kernel is used; and can also determine i and j, the spatial
o   sets inside the kernel; 2) from p we can determine il+1 and jl+1, the spatial
o   sets inside the convolved result xl+1; and, 3) the spatial o   sets in the input
xl can be determined as il = il+1 + i and jl = jl+1 + j.
that is, the mapping m : (p, q) (cid:55)    (il, jl, dl) is one-to-one, and thus is
a valid function. the inverse mapping, however, is one-to-many (thus not a
valid function). if we use m   1 to represent the inverse mapping, we know that
m   1(il, jl, dl) is a set s, where each (p, q)     s satis   es that m(p, q) = (il, jl, dl).
now we take a look at   (xl) from a di   erent perspective. in order to fully
specify   (xl), what information is required? it is obvious that the following
three types of information are needed (and only those). for every element of
  (xl), we need to know
(a) which region does it belong to, i.e., what is the value of p (0     p <

h l+1w l+1)?

(b) which element is it inside the region (or equivalently inside the convolution

kernel), i.e., what is the value of q (0     q < hw dl)?

the above two types of information determines a location (p, q) inside   (xl).
the only missing information is

(c) what is the value in that position, i.e.,(cid:2)  (xl)(cid:3)
(c.1) for (cid:2)  (xl)(cid:3)

can turn [c] into a di   erent but equivalent one:

pq?

since every element in   (xl) is a verbatim copy of one element from xl, we

pq, where is this value copied from? or, what is its original

location inside xl, i.e., an index u that satis   es 0     u < h lw ldl?

(c.2) the entire xl.

19

it is easy to see that the collective information in [a, b, c.1] (for the en-
tire range of p, q and u), and [c.2] (xl) contains exactly the same amount of
information as   (xl).
since 0     p < h l+1w l+1, 0     q < hw dl, and 0     u < h lw ldl, we can
use a a matrix m     r(h l+1w l+1hw dl)  (h lw ldl) to encode the information in
[a, b, c.1]. one row index of this matrix corresponds to one location inside
  (xl) (i.e., a (p, q) pair). one row of m has h lw ldl elements, and each element
can be indexed by (il, jl, dl). thus, each element in this matrix is indexed by a
5-tuple: (p, q, il, jl, dl).

then, we can use the    indicator    method to encode the function m(p, q) =
(il, jl, dl) into m . that is, for any possible element in m , its row index x
determines a (p, q) pair, and its column index y determines a (il, jl, dl) triplet,
and m is de   ned as

(cid:40)

m (x, y) =

1

0

if m(p, q) = (il, jl, dl)
otherwise

.

(36)

the m matrix has the following properties:
    it is very high dimensional;
    but it is also very sparse: there is only 1 non-zero entry in the h lw ldl

elements in one row, because m is a function;

    m , which uses information [a, b, c.1], only encodes the one-to-one cor-
respondence between any element in   (xl) and any element in xl, it does
not encode any speci   c value in xl;

    most importantly, putting together the one-to-one correspondence infor-

mation in m and the value information in xl, obviously we have

vec(  (xl)) = m vec(xl) .

(37)

6.7 backward propagation: prepare supervision signal for

the previous layer

in the l-th layer, we still need to compute
reshape xl into a matrix x     r(h lw l)  dl
(modulo reshaping) interchangeably.

   z

    vec(xl) . for this purpose, we want to
, and use these two equivalent forms

the chain rule states that

   (vec(xl)t ) (cf. equa-
tion 11). we will start by studying the second term in the rhs (utilizing
equations 27 and 37):

   (vec(xl)t ) =

   (vec(y)t )

   z

    vec(y)

   z

    vec(y)

   (vec(xl)t )

=

   (f t     i) vec(  (xl))

   (vec(xl)t )

= (f t     i)m .

thus,

   z

   (vec(xl)t )

=

   z

   (vec(y)t )

(f t     i)m .

(38)

(39)

20

since (using equation 25 from right to left)

   z

   (vec(y)t )

(f t     i) =

=

(cid:18)
(cid:18)

= vec

= vec

(f     i) vec

(f     i)

i

   z
   y

(cid:18)
(cid:18)    z
(cid:18)    z
(cid:18)    z

   y

   y

f t

   y

    vec(y)

   z

   y

(cid:18)    z
(cid:19)t
(cid:19)t
(cid:19)t

f t

,

f t

m ,

(cid:19)

.

f t

(cid:19)t
(cid:19)(cid:19)t

(40)

(41)

(42)

(43)

(44)

(45)

   y f t     r(h l+1w l+1)  (hw dl), and
. on the other hand, m t is an

we have

   z

   (vec(xl)t )

= vec

or equivalently

   z

= m t vec

   (vec(xl))

vec(cid:0)    z
   y f t(cid:1) is a vector in rh l+1w l+1hw dl

let   s have a closer look at the rhs.    z

indicator matrix in r(h lw ldl)  (h l+1w l+1hw dl).
in order to pinpoint one element in vec(xl) or one row in m t , we need an
index triplet (il, jl, dl), with 0     il < h l, 0     jl < w l, and 0     dl < dl.
similarly, to locate a column in m t or an element in    z
   y f t , we need an index
pair (p, q), with 0     p < h l+1w l+1 and 0     q < hw dl.

   (vec(xl)) equals the multiplication of two
vectors: the row in m t (or the column in m ) that is indexed by (il, jl, dl), and

furthermore, since m t is an indicator matrix, in the row vector indexed
by (il, jl, dl), only those entries whose index (p, q) satis   es m(p, q) = (il, jl, dl)
have a value 1, all other entries are 0. thus, the (il, jl, dl)-th entry of

   z

   (vec(xl))

   z

thus, the (il, jl, dl)-th entry of

vec(cid:0)    z
   y f t(cid:1).
equals the sum of these corresponding entries in vec(cid:0)    z
(cid:20)    z

the following succinct equation:

(cid:20)    z

(cid:88)

(cid:21)

transferring the above description into precise mathematical form, we get

   y f t(cid:1).
(cid:21)

f t

=

.

(p,q)

(46)

   x

(il,jl,dl)

(p,q)   m   1(il,jl,dl)

   y

in other words, to compute    z

   x , we do not need to explicitly use the ex-
tremely high dimensional matrix m . instead, equation 46 and equations 18
to 21 can be used to e   ciently    nd    z
   x .
mapping m   1, which is shown in figure 5.

we use the simple convolution example in figure 3 to illustrate the inverse

21

figure 5: illustration of how to compute    z
   x .

in the right half of figure 5, the 6   4 matrix is    z

   y f t . in order to compute
the partial derivative of z with respect to one element in the input x, we need
to    nd which elements in    z
   y f t is involved and add them. in the left half of
figure 5, we show that the input element 5 (shown in larger font) is involved
in 4 convolution operations, shown by the red, green, blue and black boxes,
respectively. these 4 convolution operations correspond to p = 1, 2, 3, 4. for
example, when p = 2 (the green box), 5 is the third element in the convolution,
hence q = 3 when p = 2 and we put a green circle in the (2, 3)-th element of
the    z
   y f t matrix. after all 4 circles are put in the    z
   y f t matrix, the partial
derivative is the sum of elements in these four locations of    z
the set m   1(il, jl, dl) contains at most hw dl elements. hence, equation 46

   y f t .

requires at most hw dl summations to compute one element of    z

   x .4

6.8 fully connected layer as a convolution layer

as aforementioned, one bene   t of the convolution layer is that convolution is a
local operation. the spatial extent of a kernel is often small (e.g., 3    3). one
element in xl+1 is usually computed using only a small number of elements in
its input xl.

a fully connected layer refers to a layer if the computation of any element in
the output xl+1 (or y) requires all elements in the input xl. a fully connected
layer is sometimes useful at the end of a deep id98 model. for example, if after
many convolution, relu and pooling (which will be discussed soon) layers, the
output of the current layer contain distributed representations for the input
image, we want to use all these features in the current layer to build features
with stronger capabilities in the next one. a fully connected layer is useful for
this purpose.
suppose the input of a layer xl has size h l    w l    dl. if we use convolution
kernels whose size is h l    w l    dl, then d such kernels form an order 4 tensor
4in ca   e, this computation is implemented by a function called col2im. in matconvnet,
this operation is operated in a row2im manner, although the name row2im is not explicitly
used.

22

                        in h l    w l    dl    d. the output is y     rd. it is obvious that to compute any
element in y, we need to use all elements in the input xl. hence, this layer is
a fully connected layer, but can be implemented as a convolution layer. hence,
we do not need to derive learning rules for a fully connected layer separately.

7 the pooling layer
we will use the same notation inherited from the convolution layer. let xl    
rh l  w l  dl
be the input to the l-th layer, which is now a pooling layer. the
pooling operation requires no parameter (i.e., wi is null, hence parameter learn-
ing is not needed for this layer). the spatial extent of the pooling (h    w ) is
speci   ed in the design of the id98 structure. assume that h divides h l and w
divides w l and the stride equals the pooling spatial extent,5 the output of pool-
ing (y or equivalently xl+1) will be an order 3 tensor of size h l+1  w l+1  dl+1,
with

h l+1 =

, w l+1 =

, dl+1 = dl .

(47)

h l
h

w l
w

a pooling layer operates upon xl channel by channel independently. within
each channel, the matrix with h l    w l elements are divided into h l+1    w l+1
nonoverlapping subregions, each subregion being h    w in size. the pooling
operator then maps a subregion into a single number.

two types of pooling operators are widely used: max pooling and average
pooling. in max pooling, the pooling operator maps a subregion to its maximum
value, while the average pooling maps a subregion to its average value. in precise
mathematics,

max :

yil+1,jl+1,d =

average :

yil+1,jl+1,d =

1

hw

0   i<h,0   j<w

max

(cid:88)

0   i<h,0   j<w

xl
il+1  h+i,jl+1  w +j,d ,

xl
il+1  h+i,jl+1  w +j,d ,

(48)

(49)

where 0     il+1 < h l+1, 0     jl+1 < w l+1, and 0     d < dl+1 = dl.

pooling is a local operator, and its forward computation is pretty straight-
forward. now we focus on the back propagation. only max pooling is discussed
and we can resort to the indicator matrix again.6 all we need to encode in this
indicator matrix is: for every element in y, where does it come from in xl?

we need a triplet (il, jl, dl) to pinpoint one element in the input xl, and
another triplet (il+1, jl+1, dl+1) to locate one element in y. the pooling output
yil+1,jl+1,dl+1 comes from xl
il,jl,dl , if and only if the following conditions are met:

    they are in the same channel;
    the (il, jl)-th spatial entry belongs to the (il+1, jl+1)-th subregion;
5that is, the strides in the vertical and horizontal direction are h and w , respectively.

the most widely used pooling setup is h = w = 2 with a stride 2.

6average pooling can be dealt with using a similar idea.

23

    the (il, jl)-th spatial entry is the largest one in that subregion.

(50)

(51)

translating these conditions into equations, we get

(cid:23)

dl+1 = dl ,

(cid:22) il

(cid:22) jl

(cid:23)
h
il,jl,dl     yi+il+1  h,j+jl+1  w,dl ,    0     i < h, 0     j < w ,
xl

= jl+1 ,

= il+1,

w

(52)
where (cid:98)  (cid:99) is the    oor function. if the stride is not h (w ) in the vertical (hori-
zontal) direction, equation 51 must be changed accordingly.

given a (il+1, jl+1, dl+1) triplet, there is only one (il, jl, dl) triplet that sat-

is   es all these conditions. thus, we de   ne an indicator matrix

s(xl)     r(h l+1w l+1dl+1)  (h lw ldl) .

(53)

one triplet of indexes (il+1, jl+1, dl+1) speci   es a row in s, while (il, jl, dl)
speci   es a column. these two triplets together pinpoint one element in s(xl).
we set that element to 1 if equations 50 to 52 are simultaneously satis   ed, and
0 otherwise. one row of s(xl) corresponds to one element in y, and one column
corresponds to one element in xl.

with the help of this indicator matrix, we have

vec(y) = s(xl) vec(xl) .

(54)

=

   z

then, it is obvious that

    vec(y)

   (vec(xl)t )

= s(xl),

   z

   (vec(xl)t )

   z

   (vec(y)t )

s(xl) ,

(55)

and consequently

   z

    vec(xl)

= s(xl)t

    vec(y)

.

(56)

s(xl) is very sparse. it has exactly one nonzero entry in every row. thus, we
do not need to use the entire matrix in the computation. instead, we just need
to record the locations of those nonzero entries   there are only h l+1w l+1dl+1
such entries in s(xl).
a simple example can explain the meaning of these equations. let us con-
sider a 2    2 max pooling with stride 2. for a given channel dl, the    rst spatial
subregion contains four elements in the input, with (i, j) = (0, 0), (1, 0), (0, 1)
and (1, 1), and let us suppose the element at spatial location (0, 1) is the largest
among them. in the forward pass, the value indexed by (0, 1, dl) in the input
(i.e., xl
0,1,dl) will be assigned to the element in the (0, 0, dl)-th element in the
output (i.e., y0,0,dl ).

one column in s(xl) contains at most one nonzero element if the strides are
h and w , respectively. in the above example, the column of s(xl) indexed by

24

(0, 0, dl), (1, 0, dl) and (1, 1, dl) are all zero vectors. the column corresponding
to (0, 1, dl) contains only one nonzero entry, whose row index is determined by
(0, 0, dl). hence, in the back propagation, we have

(cid:21)

(cid:20)

   z

    vec(xl)

(cid:21)

   z

    vec(y)

and

(cid:20)

   z

(cid:21)

    vec(xl)

(0,0,dl)

(cid:20)

=

=

(cid:21)

(0,1,dl)

   z

    vec(xl)

(1,0,dl)

,

(cid:21)

(0,0,dl)

   z

    vec(xl)

(1,1,dl)

= 0 .

(cid:20)

=

however, if the pooling strides are smaller than h and w in the vertical
and horizontal directions, respectively, one element in the input tensor may be
the largest element in several pooling subregions. hence, there can have more
than one nonzero entries in one column of s(xl). let us consider the example
input in figure 5. if a 2    2 max pooling is applied to it and the stride is 1 in
both directions, the element 9 is the largest in two pooling regions: [ 5 6
8 9 ] and
9 1 ]. hence, in the column of s(xl) corresponding to the element 9 (indexed by
[ 6 1
(2, 2, dl) in the input tensor), there are two nonzero entries whose row indexes
correspond to (il+1, jl+1, dl+1) = (1, 1, dl) and (1, 2, dl). thus, in this example,

we have (cid:20)

(cid:21)

(cid:20)

=

(cid:20)

+

(cid:21)

   z

   z

   z

    vec(xl)

(2,2,dl)

    vec(y)

(1,1,dl)

    vec(y)

(1,2,dl)

.

(cid:20)

(cid:21)

8 a case study: the vgg-16 net

we have introduced the convolution, pooling, relu and fully connected layers
till now, and have brie   y mentioned the softmax layer. with these layers, we
can build many powerful deep id98 models.

8.1 vgg-verydeep-16

the vgg-verydeep-16 id98 model is a pretrained id98 model released by the
oxford vgg group.7 we use it as an example to study the detailed structure
of id98 networks. the vgg-16 model architecture is listed in table 2.

there are six types of layers in this model.

convolution a convolution layer is abbreviated as    conv   .

its description
includes three parts: number of channels; kernel spatial extent (kernel
size); padding (   p   ) and stride (   st   ) size.

relu no description is needed for a relu layer.

7http://www.robots.ox.ac.uk/~vgg/research/very_deep/

25

table 2: the vgg-verydeep-16 architecture and receptive    eld

type
1 conv
2 relu
3 conv
4 relu
5
pool
6 conv
7 relu
8 conv
9 relu

10
pool
11 conv
12 relu
13 conv
14 relu
15 conv
16 relu
17
pool
18 conv
19 relu

description
64;3x3;p=1,st=1

64;3x3;p=1,st=1

2x2;st=2
128;3x3;p=1,st=1

128;3x3;p=1,st=1

2x2;st=2
256;3x3;p=1,st=1

256;3x3;p=1,st=1

256;3x3;p=1,st=1

2x2;st=2
512;3x3;p=1,st=1

r. size
212
210
210
208
208
104
102
102
100
100
50
48
48
46
46
44
44
22
20

type
20 conv
21 relu
22 conv
23 relu
24
pool
25 conv
26 relu
27 conv
28 relu
29 conv
30 relu
31
32
33 relu
34 drop
35
36 relu
37 drop
38
39

pool
fc

fc
  

fc

r. size
20
18
18
16
16
8
6
6
4
4
2
2
1

description
512;3x3;p=1,st=1

512;3x3;p=1,st=1

2x2;st=2
512;3x3;p=1,st=1

512;3x3;p=1,st=1

512;3x3;p=1,st=1

(7x7x512)x4096

0.5
4096x4096

0.5
4096x1000
(softmax layer)

pool a pooling layer is abbreviated as    pool   . only max pooling is used in
vgg-16. the pooling kernel size is always 2    2 and the stride is always
2 in vgg-16.

fully connected a fully connected layer is abbreviated as    fc   . fully con-
nected layers are implemented using convolution in vgg-16. its size is
shown in the format n1    n2, where n1 is the size of the input tensor, and
n2 is the size of the output tensor. although n1 can be a triplet (such as
7    7    512, n2 is always an integer.

dropout a dropout layer is abbreviated as    drop   . dropout is a technique to
improve the generalization of deep learning methods. it sets the weights
connected to a certain percentage of nodes in the network to 0 (and vgg-
16 set the percentage to 0.5 in the two dropout layers).

softmax it is abbreviated as         .

we want to add a few notes about this example deep id98 architecture.
    a convolution layer is always followed by a relu layer in vgg-16. the

relu layers increase the nonlinearity of the id98 model.

    the convolution layers between two pooling layers have the same number
of channels, kernel size and stride. in fact, stacking two 3    3 convolution
layers is equivalent to one 5   5 convolution layer; and stacking three 3   3
convolution kernels replaces a 7    7 convolution layer. stacking a few (2
or 3) smaller convolution kernels, however, computes faster than a large

26

convolution kernel. in addition, the number of parameters is also reduced,
e.g., 2    3    3 = 18 < 25 = 5    5. the relu layers inserted in between
small convolution layers are also helpful.

    the input to vgg-16 is an image with size 224    224    3. because the
padding is one in the convolution kernels (meaning one row or column is
added outside of the four edges of the input), convolution will not change
the spatial extent. the pooling layers will reduce the input size by a factor
of 2. hence, the output after the last (5th) pooling layer has spatial extent
7    7 (and 512 channels). we may interpret this tensor as 7    7    512 =
25088    features   . the    rst fully connected layer converts them into 4096
features. the number of features remains at 4096 after the second fully
connected layer.

    the vgg-16 is trained for the id163 classi   cation challenge, which is
an object recognition problem with 1000 classes. the last fully connected
layer (4096    1000) output a length 1000 vector for every input image,
and the softmax layer converts this length 1000 vector into the estimated
posterior id203 for the 1000 classes.

8.2 receptive    eld

another important concept in id98 is the receptive    eld size (abbreviated as
   r. size    in table 2). let us look at one element in the input to the    rst fully
connected layer (32|fc). because it is the output of a max pooling, we need
values in a 2    2 spatial extent in the input to the max pool layer to compute
this element (and we only need elements in this spatial extent). this 2    2
spatial extent is called the receptive    eld for this element. in table 2, we listed
the spatial extent for any element in the output of the last pooling layer. note
that because the receptive    eld is square, we only use one number (e.g., 48 for
48    48). the receptive    eld size listed for one layer is the spatial extent in the
input to that layer.
a 3   3 convolution layer will increase the receptive    eld by 2 and a pooling
layer will double the spatial extent. as shown in table 2, receptive    eld size in
the input to the    rst layer is 212   212. in other words, in order to compute any
single element in the 7    7    512 output of the last pooling layer, a 212    212
image patch is required (including the padded pixels in all convolution layers).
it is obvious that the receptive    eld size increases when the network becomes
deeper, especially when a pooling layer is added to the deep net. unlike tra-
ditional id161 and image processing features which depend only on
a small receptive    eld (e.g., 16    16), deep id98 computes its representation
(or features) using large receptive    elds. the larger receptive    eld characteris-
tic is an important reason why id98 has achieved higher accuracy than classic
methods in image recognition.

27

9 remarks

we hope this introductory note on id98 is clear, self-contained, and easy to
understand to our readers.

once a reader is con   dent in his/her understanding of id98 at the math-
ematical level, in the next step it is very helpful to get some hands on id98
experience. for example, one can validate what has been talked about in this
note using the matconvnet software package if you prefer the matlab environ-
ment.8 for c++ lovers, ca   e is a widely used tool.9 the theano package
is a python package for deep learning.10 many more resources for deep learn-
ing (not only id98) are available, e.g., torch,11, tensorflow,12 and more will
emerge soon.

exercises

1. dropout is a very useful technique in training neural networks, which is
proposed by srivastava et al. in a paper titled    dropout: a simple way
to prevent neural networks from over   tting    in jmlr.13 carefully read
this paper and answer the following questions (please organize your answer
to every question in one brief sentence).

(a) how does dropout operate during training?

(b) how does dropout operate during testing?

(c) what is the bene   t of dropout?

(d) why dropout can achieve this bene   t?

2. the vgg16 id98 model (also called vgg-verydeep-16) was publicized
by karen simonyan and andrew zisserman in a paper titled    very deep
convolutional networks for large-scale image recognition    in the arxiv
preprint server.14 and, the googlenet model was publicized by szegedy
et al.
in a paper titled    going deeper with convolutions    in the arxiv
preprint server.15 these two papers were publicized around the same time
and share some similar ideas. carefully read both papers and answer the
following questions (please organize your answer to every question in one
brief sentence).
(a) why do they use small convolution kernels (mainly 3    3) rather than

8http://www.vlfeat.org/matconvnet/
9http://caffe.berkeleyvision.org/
10http://deeplearning.net/software/theano/
11http://torch.ch/
12https://www.tensorflow.org/
13available at http://jmlr.org/papers/v15/srivastava14a.html
14available at https://arxiv.org/abs/1409.1556, later published in iclr 2015 as a confer-

ence track paper.

15available at https://arxiv.org/abs/1409.4842, later published in cvpr 2015.

28

larger ones?

(b) why both networks are quite deep (i.e., with many layers, around 20)?

(c) which di   culty is caused by the large depth? how are they solved in
these two networks?

3. batch id172 (bn) is another very useful technique in training
deep neural networks, which is proposed by sergey io   e and christian
szegedy, in a paper titled    batch id172: accelerating deep net-
work training by reducing internal covariate shift    in icml 2015.16
carefully read this paper and answer the following questions (please or-
ganize your answer to every question in one brief sentence).

(a) what is internal covariate shift?

(b) how does bn deal with this?

(c) how does bn operate in a convolution layer?

(d) what is the bene   t of using bn?

4. resnet is a very deep neural network learning technique proposed by he
et al. in a paper titled    deep residual learning for image recognition    in
cvpr 2016.17 carefully read this paper and answer the following ques-
tions (please organize your answer to every question in one brief sentence).

(a) although vgg16 and googlenet have encountered di   culties in
training networks around 20   30 layers, what enables resnet to train net-
works as deep as 1000 layers?

(b) vgg16 is a feed-forward network, where each layer has only one input
and only one output. while googlenet and resnet are dags (directed
acyclic graph), where one layer can have multiple inputs and multiple
outputs, so long as the data    ow in the network structure does not form
a cycle. what is the bene   t of dag vs. feed-forward?

(c) vgg16 has two fully connected layers (fc6 and fc7), while resnet and
googlenet do not have fully connected layers (except the last layer for
classi   cation). what is used to replace fc in them? what is the bene   t?

5. alexnet refers to the deep convolutional neural network trained on the
ilsvrc challenge data, which is a groundbreaking work of deep id98
for id161 tasks. the technical details of alexnet is reported
in the paper    id163 classi   cation with deep convolutional neural
networks   , by alex krizhevsky, ilya sutskever and geo   rey e. hinton
in nips 25.18 it proposed the relu activation function and creatively
used gpus to accelerate the computations. carefully read this paper

16available at http://jmlr.org/proceedings/papers/v37/io   e15.pdf
17available at https://arxiv.org/pdf/1512.03385.pdf
18this paper is available at http://papers.nips.cc/paper/4824-id163-classi   cation-with-

deep-convolutional-neural-networks

29

and answer the following questions (please organize your answer to every
question in one brief sentence).

(a) describe your understanding of how relu helps its success? and,
how do the gpus help out?

(b) using the average of predictions from several networks help reduce the
error rates. why?

(c) where is the dropout technique applied? how does it help? and what
is the cost of using dropout?

(d) how many parameters are there in alexnet? why the dataset size
(1.2 million) is important for the success of alexnet?

6. we will try di   erent id98 structures on the mnist dataset. we denote
the    baseline    network in the mnist example in matconvnet as base
in this question.19
in this question, a convolution layer is denoted as
   x   y    nin   nout   , whose kernel size is x   y, with nin input and nout
output channels, with stride equal 1 and pad equal 0. the pooling layers
are 2    2 max pooling with stride equal 2. the base network has four
blocks. the    rst consists of a 5  5  1  20 convolution and a max pooling;
the second block is composed of a 5    5    20    50 convolution and a max
pooling; the third block is a 4    4    50    500 convolution (fc) plus a
relu layer; and the    nal block is the classi   cation layer (1   1   500   10
convolution).

(a) the mnist dataset is available at yann.lecun.com/exdb/mnist. read
the instructions in that page, and write a program to transform the data
to formats that suit your favorite deep learning software.

(b) learning deep learning models often involve random numbers. before
the training starts, set the random number generator   s seed to 0. then,
use the base network structure and the    rst 10000 training examples
to learn its parameters. what is test set error rate (on the 10000 test
examples) after 20 training epochs?

(c) from now on, if not otherwise speci   ed, we assume the    rst 10000
training examples and 20 epochs are used. now we de   ne the bn network
structure, which adds a batch id172 layer after every convolution
layer in the    rst three blocks. what is its error rate? what will you say
about bn vs. base?

(d) if you add a dropout layer after the classi   cation layer in the 4th block.
what is the new error rate of base and bn? what you will comment on
dropout?

(e) now we de   ne the sk network structure, which refers to small kernel
size. sk is based on bn. the    rst block (5    5 convolution plus pooling)
now is changed to two 3   3 convolutions, and bn + relu is applied after
19matconvnet version 1.0-beta20. please refer to matconvnet for all the details of base,

such as parameter initialization and learning rate.

30

every convolution. for example, block 1 is now 3    3    1    20 convolution
+ bn + relu + 3   3   20   20 convolution + bn + relu + pool. what
is sk   s error rate? how will you comment on that (e.g., how and why the
error rate changes?)

(f) now we de   ne the sk-s networks structure. the notation    s    refers to
a multiplier that changes the number of channels in convolution layers.
for example, sk is the same as sk-1. and, sk-2 means the number of
channels in all convolution layers (except the one in block 4) are multiplied
by 2. train networks for sk-2, sk-1.5, sk-1, sk-0.5 and sk-0.2. report
their error rates and comment on them.

(g) now we experiment with di   erent training set sizes using the sk-0.2
network structure. using the    rst 500, 1000, 2000, 5000, 10000, 20000, and
60000 (all) training examples, what error rates do you achieve? comment
on your observations.

(h) using the sk-0.2 network structure, study how di   erent training sets
a   ect its performance. train 6 networks, and use the (10000   (i    1) + 1)-
th to (i    10000)-th training examples in training the i-th network. are
id98s stable in terms of di   erent training sets?

(i) now we study how randomness a   ects id98 learning. instead of set
the random number generator   s seed to 0, use 1, 12, 123, 1234, 12345 and
123456 as the seed to train 6 di   erent sk-0.2 networks. what are their
error rates? comment on your observations.

(j) finally, in sk-0.2, change all relu layers to sigmoid layers. how do
you comment on the comparison on error rates of using relu and sigmoid
id180?

31

