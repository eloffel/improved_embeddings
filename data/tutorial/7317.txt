human-level control through deep 

id23

presented by bowen xu

acknowledgment

slides from jiang guo available at: 
http://ir.hit.edu.cn/~jguo/docs/notes/id25-atari.pdf 

slides from dong-kyoung kye available at: http://vi.snu.ac.kr/xe/

towards  general  artificial  intelligence

    playing  atari  with  deep  reinforcement  learning.  arxiv (2013)

    7  atari  games
    the  first  step  towards     general  artificial  intelligence   

    deepmind got  acquired  by  @google  (2014)

    human  level  control  through  deep  reinforcement  learning.  nature  
(2015)
    49  atari  games
    google  patented     deep  reinforcement  learning   

key  concepts

    reinforcement  learning
    markov  decision  process
    discounted  future  reward
    q  learning
    deep  q  network
    exploration  exploitation
    experience  replay
    deep  q  learning  algorithm

reinforcement  learning

    example:  breakout  (one  of  the  atari  games)

    suppose  you  want  to  teach  an  agent  (e.g.  nn)  to  play  this  game

    supervised  training  (expert  players  play  a  million  times)
    reinforcement  learning

that   s  not  how  we  learn!

reinforcement  learning

supervised  learning

target  label  for  each  training  example

ml

reinforcement  learning

sparse and  time  delayed labels

unsupervised  learning

no  label  at  all

pong

breakout

space  invaders

seaquest

beam  rider

rl  is  learning  from  interaction

rl  is  like  life!

markov  decision  process

    0,    0,    1,    1,    1,    2,   ,           1,           1,        ,        

state

action

reward

terminal  state

state  representation

think  about  the  breakout game

    how  to  define  a  state?

location  of  the  paddle
location/direction  of  the  ball

   
   
    presence/absence  of  each  individual  brick

let   s  make  it  more  universal!

screen  pixels

    0,    0,    1,    1,    1,    2,   ,           1,           1,        ,        

mdp

value  function

    future  reward

    =    1+    2+    3+   +        
        =        +        +1+        +2+   +        
        =        +            +1+    2        +2+   +                       
=        +    (        +1+    (        +2+   ))
=        +            +1

    discounted  future  reward  (environment  is  stochastic)

    a  good  strategy  for  an  agent  would  be  to  always  choose  an  action  that  maximizes  

the  (discounted)  future  reward

value-action  function

    we  define  a      (    ,    ) representing  the  maximum  discounted  future  

reward  when  we  perform  action  a in  state  s:

    q  function:  represents  the     quality     of  a  certain  action  in  a  given  state

    imagine  you  have  the  magical  q  function

            ,         =max        +1
    (    ,    )
         =            max
    

        is  the  policy

id24

    how  do  we  get  the  q  function?

    bellman  equation                       

        ,     =    +                           (       ,       )

value  iteration

id24

    in  practice,  value  iteration  is  impractical

    very  limited  states/actions
    cannot  generalize  to  unobserved  states

    think  about  the  breakout game

    image  size:                     (resized)

    state:  screen  pixels

    consecutive  4 images
    grayscale  with  256 gray  levels

                                     rows  in  the  q  table!

function  approximator

        ,    ;               (    ,    )

    use  a  function  (with  parameters)  to  approximate  the  q  function

    linear
    non  linear:  q  network

state

action

network

q  value

    
    

    

q  value  1

state

network

q  value  2

q  value  3

deep id24

    stability issues with deep rl

   na  ve id24 oscillates or diverges with neural nets

1. data is sequential

    successive samples are correlated, non-i.i.d.

2. policy changes rapidly with slight changes to q-values

    policy may oscillate

    distribution of data can swing from one extreme to another

3. scale of rewards and q-values is unknown

    naive id24 gradients can be large unstable when backpropagated

22

deep id24

    deep q-network provides a stable solution to deep value-based rl

1. use experience replay

    break correlations in data, bring us back to i.i.d. setting

    learn from all past policies

    using off-policy id24

2. freeze target q-network

    avoid oscillations

    break correlations between q-network and target

3. clip rewards or normalize network adaptively to sensible range

    robust gradients

23

stable deep rl(1) : experience replay

    to remove correlations, build data-set from agent  s own experience

   take action        according to    -greedy policy
(choose   best   action with id203 1-   , and selects a random action with id203    )
   store transition (      ,      ,            ,                in replay memory     (huge data base to store historical samples)
   sample random mini-batch of transitions    ,   ,   ,      
                	      ,   ,   ,      ~   

         max                   ,      ;                     ,   ;             

   optimize mse between q-network and id24 targets, e.g.

from    

24

stable deep rl(2) : fixed target q-network

    to avoid oscillations, fix parameters used in id24 target

   compute id24 targets w.r.t. old, fixed parameters          

         max                   ,      ;         	   
         max                   ,      ;                        ,   ;             
   periodically update fixed parameters                    

                	      ,   ,   ,      ~   

   optimize mse between q-network and id24 targets

25

stable deep rl(3) : reward / value range

    id25 clips the reward to [-1, +1]

    this prevents q-values from becoming too large

    ensures gradients are well-conditioned

26

stable deep rl

id25

27

how to train the deep q-network

    id168 : 

    differentiating the id168 w.r.t. the weights we arrive at following 

         max                   ,      ;                        ,   ;             
         max                   ,      ;                        ,   ;                            ,   ;         

                	      ,   ,   ,      ~   
                         	      ,   ,   ,      ~   
do id119:                                                        

gradient :

28

how to train the deep q-network

29

how to train the deep q-network

during training

of samples
database 
,   )
(   , 
1 million samples

, 

=mini-batch size

...

(  k2, ak2, rk2,  k2+1 )

(  k1, ak1, rk1,  k1+1 )

descent on parameter    

do mini-batch gradient 

for one step

(  kn, akn, rkn,  kn+1 )

add new data sample to database

                                             ,      
random action       

with id203 1-  

or  

with id203   

play the game for one step

30

image at time t :       
                     ,            ,      
                  

preprocessed sequence

how to train the deep q-network

after training

                                        ,      

play the game for one step

31

id25 in atari

32

id25 in atari

210x160 pixel 
images with a 
128 color 
palette

    the input to the neural network 
consists of an 84x84x4 image 
produced by the pre-processing 

map    

   

input state is stack of raw pixels 
from last 4 frames

33

id25 in atari

210x160 pixel 
images with a 
128 color 
palette

    the first hidden layer convolves 32 

filters of 8x8 with stride 4 with the 
input image and applies a rectifier 
nonlinearity.

   

the second hidden layer convolves 
64 filters of 4x4 with stride 2.

    this is followed by a third 

convolutional layer that convolves 
64 filters of 3x3 with stride 1

34

id25 in atari

210x160 pixel 
images with a 
128 color 
palette

    the final hidden layer is fully-

connected and consists of 512 
rectifier units.

    the output layer is a fully-connected 

linear layer with a single output of 
each valid action.

    the number of valid actions varied 

between 4 and 18 on the games

35

id25 result in atari

36

id25 result in atari

good results

bad results

37

id25 result in atari

  seaquest   id25 gameplay

[https://youtu.be/5wxvj1a0k6q]

38

id25 result in atari

[https://youtu.be/iqxkqf2bose]

39

conclusion

    id23 provides a general-purpose framework for 

a.i.

    rl problems can be solved by end-to-end deep learning

    a single agent can now solve many challenging tasks

    id23 + deep learning

    agent can do stuff that maybe human don  t know how to program

40

