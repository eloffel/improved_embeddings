high-performance hardware for machine learning

nips tutorial

12/7/2015

prof. william dally
stanford university
nvidia corporation

hardware and data enable dnns

the need for speed

larger data sets and models lead to better accuracy but also increase 
computation time. therefore progress in deep neural networks is limited by 
how fast the networks can be computed. 

likewise the application of convnets to low latency id136 problems, 
such as pedestrian detection in self driving car video imagery, is limited by 
how fast a small set of images, possibly a single image, can be classified. 

more data    bigger models   more need for compute
but moore   s law is no longer providing more compute   

lavin & gray, fast algorithms for convolutional neural networks, 2015

outline
    the problem
    baseline
    parallelization
    gpus
    reduced precision
    compression
    better algorithms
    hardware for dnns
    summary

outline
    the problem
    baseline
    parallelization
    gpus
    reduced precision
    compression
    better algorithms
    hardware for dnns
    summary

acceleration

    run a network faster (performance, inf/s)
    run a network more efficiently

    energy (inf/j)
    cost (inf/s$)

    id136 

    just running the network forward

    training

    running the network forward
    back-propagation  of gradient
    update of parameters

what network? dnns, id98s, and id56s

dnn, key operation is dense m x v

bi

=

wij

weight matrix

o
u

t

p
u

t
 

a
c
t
i
v
a

t
i
o
n
s

x

aj

i

n
p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

dnn, key operation is dense m x v

bi

=

wij

repeat for each layer

o
u

t

p
u

t
 

a
c
t
i
v
a

t
i
o
n
s

weight matrix

x

aj

i

n
p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

dnn, key operation is dense m x v

bi

=

wij

x

aj

id26  just does 
this backward

weight matrix

i

n
p
u

t
 

g
r
a
d
e
n
t

i

o
u

t

p
u

t
 

g
r
a
d
e
n

i

t

training, and latency insensitive networks  can be 
batched     operation is m x m     gives re-use of weights

=

wij

x

weight matrix

ajk

i

n
p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

bik

o
u

t

p
u

t
 

a
c
t
i
v
a

t
i
o
n
s

for real time you can   t batch

and there is sparsity in both weights and activations

key operations is spm x spv

bi

=

wij

x

aj

id26  just does 
this backward

weight matrix

i

n
p
u

t
 

g
r
a
d
e
n
t

i

o
u

t

p
u

t
 

g
r
a
d
e
n

i

t

id98s     for image inputs, convolutional stages act as 

trained feature detectors

id98s require convolution

in addition to m x v

kernels
multiple 3d
kuvkj

x

aijaij
axyk

aijaijbxyk

input maps 

axyk

output maps

bxyj

id98s require convolution

in addition to m x v

kernels
multiple 3d
kuvkj

x

6d loop
for each output map j
for each input map k

for each pixel x,y

for each kernel element u,v
bxyj += a(x-u)(y-v)k x kuvkj

aijaij
axyk

aijaijbxyj

input maps

axyk

output maps

bxyj

id56s

some other operations

wij +=   ajgi

pooling

relu

(or other non-linear  function)

weight update

infrastructure

decompress

dnn

eagle.jpg

eagle grabbing a 
fish from water

summary of the problem

    run dnns, id98s, and id56s 

    for training and id136
    can batch if not latency sensitive

    optimize

    speed inf/s
    efficiency inf/j, inf/s$

    key operations are

    m x v

    m x m if batched
    may be sparse (spm x spv)

    convolution

    also

    pooling, non-linear operator (relu), weight update

outline
    the problem
    baseline
    parallelization
    gpus
    reduced precision
    compression
    better algorithms
    hardware for dnns
    summary

baseline performance xeon e5-2698     single core

alexnet     id136, batched 
30 f/s
3.2 f/j
most ops on avx (simd) units

moore   s law made cpus 300x faster than in 1990

but its over   

c moore, data processing in exascale-classcomputer systems, salishan, april 2011

outline
    the problem
    baseline
    parallelization
    gpus
    reduced precision
    compression
    better algorithms
    hardware for dnns
    summary

to go faster, use more processors

lots of parallelism in a dnn

lots of parallelism in a dnn

inputs

   
    points of a feature map
    filters
    elements within a filter

    multiplies within layer are independent
    sums are reductions
    only layers are dependent
    no data dependent operations 

=> can be statically scheduled

data parallel     run multiple inputs in parallel

data parallel     run multiple inputs in parallel

    doesn   t affect latency for one input
    requires p-fold larger batch size
    for training requires coordinated weight update

one method to achieve scale is parallelization

parameter update
p    = p +    p

parameter server

   p

p   

model!
workers

data!
shards

large scale distributed deep networks, jeff dean et al., 2013

large scale distributed deep networks 
j dean et al (2012)

model parallel 

split up the model     i.e. the network

model-parallel convolution

kernels
multiple 3d
kuvkj

x

6d loop
for each output map j
for each input map k

for each pixel x,y

for each kernel element u,v
bxyj += a(x-u)(y-v)k x kuvkj

aijaijaxyk

aijaijbxyj

input maps

axyk

output maps

bxyj

model-parallel convolution     by output region (x,y)

kernels
multiple 3d
kuvkj

x

aijaijaxyk

bxyj
bxyj

bxyj
bxyj

bxyj
bxyj

bxyj
bxyj

bxyj
bxyj

input maps

axyk

output maps

bxyj

6d loop
forall region xy

for each output map j
for each input map k

for each pixel x,y in xy

for each kernel element u,v
bxyj += a(x-u)(y-v)k x kuvkj

model-parallel convolution     by output map j (filter)

kernels
multiple 3d
kuvkj

x

6d loop
forall output map j

for each input map k

for each pixel x,y

for each kernel element u,v
bxyj += a(x-u)(y-v)k x kuvkj

aijaijaxyk

aij
aijaijbxyj

input maps

axyk

output maps

bxyj

model parallel fully-connected layer (m x v)

bi

=

wij

weight matrix

o
u

t

p
u

t
 

a
c
t
i
v
a

t
i
o
n
s

x

aj

i

n
p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

model parallel fully-connected layer (m x v)

=

bi

bi

o
u

t

p
u

t
 

a
c
t
i
v
a

t
i
o
n
s

wij

wij

weight matrix

x

aj

i

n
p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

hyper-parameter parallel

try many alternative networks in parallel

cpu parallelism     core i7     1 core vs 6 cores

300

250

200

s
/
f

150

100

50

0

242

40

1	core

6	cores

nvidia,    whitepaper: gpu-based deep learning id136: a performance and power analysis.    

the moderately sized speech model runs fastest on 8 machines, computing 2.2    faster than using a
single machine. (models were con   gured to use no more than 20 cores per machine.) partitioning

data and model parallel performance

p
u
   
d
e
e
p
s
 

i

g
n
n
a
r
t

i

15

10

5

0
 
1

speech: 42m parameters
images: 80m parameters
images: 330m parameters
images: 1.7b parameters

16

32

64

machines per model instance

 

128

dean et al. large scale distributed deep networks, nips 2012

figure 3: training speed-up for four different deep networks as a function of machines allocated
to a single distbelief model instance. models with more parameters bene   t more from the use of

summary of parallelism

    lots of parallelism in dnns

    16m independent  multiplies in one fc layer
    limited by overhead to exploit a fraction of this

    data parallel

    run multiple training examples in parallel
    limited by batch size

    model parallel

    split model over multiple processors
    by layer
    conv layers by map region
    fully connected layers by output activation

    easy to get 16-64 gpus training one model in parallel

outline
    the problem
    baseline
    parallelization
    gpus
    reduced precision
    compression
    better algorithms
    hardware for dnns
    summary

to go fast, use multiple processors

to go fast, use multiple processors

to be efficient and fast, use gpus

to go fast, use multiple processors

to be efficient and fast, use gpus

to be efficient and go really fast, use multiple gpus

titan x

    3072 cuda cores @ 1 ghz
    6 teraflops fp32
    12gb of gddr5 @ 336 gb/sec
    250w tdp

    24gflops/w
    28nm process

tegra x1

    256 cuda cores @ ~1 ghz
    1 teraflop fp16
    4gb of lpddr4 @ 25.6 gb/s
    15 w tdp (1w idle, <10w typical)

    100gflops/w (fp16)
    20nm process

xeon e5-2698 cpu v.s. titanx gpu

6.8x

4.4x

5.3x

3.6x

nvidia,    whitepaper: gpu-based deep learning id136: a performance and power analysis.    

tegra x1 vs core i7

300

250

200

s
/
f

150

100

50

0

258

242

core i7

tx1

j
/
f

50
45
40
35
30
25
20
15
10
5
0

45

11.5x

3.9

core i7

tx1

nvidia,    whitepaper: gpu-based deep learning id136: a performance and power analysis.    

 
r
e
b
m
u
n
u
p
g

 

 
/
 

p
u
d
e
e
p
s

1.2

1

0.8

0.6

0.4

0.2

0

parallel gpus

50

40

batch size 256
batch size 512
batch size 1024

30

20

 

p
u
d
e
e
p
s

slices=64, model-data parallel
slices=64, data parallel

10

0

1

4

16

32

64

16

gpu number 

 

32

gpu number 

64

 

figure 3: left: the scalability of different parallel approaches. the hybrid parallelism is 
ren wu et al., deep image: scaling up image recognition, arxiv 2015

better when number of gpus is less than 16. the scalability of data parallelism is better with 

and implement. it scales well as we add multiple nodes to the training process.

parallel gpus on deep speech 2

5-3 (2560)
9-7 (1760)

219

218

217

216

215

214

213

212

)
s
d
n
o
c
e
s
(

e
m
t

i

211

20

21

22

23

24

gpus

25

26

27

baidu, deep speech 2: end-to-end id103 in english and mandarin, 2015 

figure 4: scaling comparison of two networks   a 5 layer model with 3 recurrent layers containing 2560
hidden units in each layer and a 9 layer model with 7 recurrent layers containing 1760 hidden units in each

summary of gpus
    titan x ~6x faster, 4x more efficient than xeon e5
    tx1 11.5x more efficient than core i7
    on id136
    larger gains on training

    data parallelism scales easily to 16gpus
    with some effort, linear speedup to 128gpus

outline
    the problem
    baseline
    parallelization
    gpus
    reduced precision
    compression
    better algorithms
    hardware for dnns
    summary

reducing precision 

reduces storage

reduces energy

improves performance

has little effect on accuracy     to a point

dnn, key operation is dense m x v

bi

=

wij

weight matrix

o
u

t

p
u

t
 

a
c
t
i
v
a

t
i
o
n
s

x

aj

i

n
p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

dnn, key operation is dense m x v

    "=           "&
&

    "

bi

=

wij

weight matrix

o
u

t

p
u

t
 

a
c
t
i
v
a

t
i
o
n
s

x

aj

i

n
p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

how much accuracy do we need in the computations:

    "=     )    "&
    "
&
    "&=    "&+      "    &

number representation

23
m

31
m

fp32

fp16

int32

int16

int8

1
s

1
s

1
s

1
s

1
s

8
e

5
e

10
m

15
m

7
m

range

accuracy

10-38 - 1038

.000006%

6x10-5 - 6x104

.05%

0     2x109

0     6x104

0     127

  

  

  

cost of operations

energy (pj)

area (  m2)

0.03
0.05
0.1
0.4
0.9
0.2
3.1
1.1
3.7
5
640

36
67
137
1360
4184
282
3495
1640
7700
n/a
n/a

operation:
8b add
16b add
32b add
16b fp add
32b fp add
8b mult
32b mult
16b fp mult
32b fp mult
32b sram read (8kb)
32b dram read

energy numbers are from mark horowitz    computing   s energy problem (and what we can do about it)   , isscc 2014
area numbers are from synthesized result using design compiler under tsmc 45nm tech node. fp units used designware library.

the importance of staying local

lpddr dram

gb

640pj/word

on-chip sram

mb

50pj/word

local sram

kb

5pj/word

mixed precision

wij

aj

x

+

bi

mixed precision

store weights as 4b using
trained quantization, 
decode to 16b

store activations as 16b

wij

aj

x

+

16b x 16b multiply
round result to 16b

bi

accumulate 24b or 32b 
to avoid saturation

mixed precision

store weights as 4b using
trained quantization, 
decode to 16b

store activations as 16b

wij

aj

x

+

16b x 16b multiply
round result to 16b

bi

accumulate 24b or 32b 
to avoid saturation

batch id172  important to    center    dynamic range

weight update

  

x

aj

gj

x

  wij

+

wij

weight update

learning rate may 

be very small 
(10-5 or less)

  

x

aj

gj

x

  wij

+

wij

  w rounded to 

zero

no learning!

stochastic rounding

learning rate may 

be very small 
(10-5 or less)

  

x

aj

gj

  w very small

x

  wij

sr

  w   ij

+

wij

e(  w   ij) =   wij

reduced precision for id136

multiply energy (pj)

prediction accuracy

32b float

32b int
16b int
arithmetic precision

8b int

y
c
a
r
u
c
c
a

90%
68%
45%
23%
0%

4.0
3.0
2.0
1.0
0.0

)
j
p
(
 
y
g
r
e
n
e

 
l

u
m

reduced precision for training

deep learning with limited numerical precision

figure 2. mnist dataset using id98s: training error (a) and the test error (b) for training using    xed-point number
representation and rounding mode set to either    round to nearest    or    stochastic rounding   . the word length for    xed-
point numbers wl is kept    xed at 16 bits and results are shown for di   erent fractional (integer) lengths for weights and
weight updates: 12(4), and 14(2) bits. layer outputs use h6, 10i format in all cases. results using float are also shown
for comparison.

s. gupta et.al    deep learning with limited numerical precision    icml 15
for training a given network. moreover, the use of the
same word length for all network variables carries with
it the added advantage of simplifying the hardware
implementation.

degradation in either the convergence rate or the clas-
si   cation accuracy. a reduction in the precision below
14 bits begins to negatively impact the network   s
ability to learn when the round-to-nearest scheme is
adopted. this is primarily because at reduced frac-

summary of reduced precision

    can save memory capacity, memory bandwidth, memory power, and 

arithmetic power by using smaller numbers

    fp16 works with little effort

    2x gain in memory, 4x in multiply power

    with care, one can use 

    8b for convolutions
    4b for fully-connected layers

    batch id172     important to    center    ranges
    stochastic rounding     important to retain small increments

outline
    the problem
    baseline
    parallelization
    gpus
    reduced precision
    compression
    better algorithms
    hardware for dnns
    summary

reducing size of network reduces work and storage

prune unneeded connections

pruning

before pruning

after pruning

pruning 
synapses

pruning 
neurons

han et al. learning both weights and connections for efficient neural networks, nips 2015

retrain to recover accuracy

l2 id173 w/o retrain 
l1 id173 w/ retrain 
l2 id173 w/ iterative prune and retrain 

l1 id173 w/o retrain 
l2 id173 w/ retrain 

train connectivity

prune connections

train weights

s
s
o
l
 
y
c
a
r
u
c
c
a

0.5%
0.0%
-0.5%
-1.0%
-1.5%
-2.0%
-2.5%
-3.0%
-3.5%
-4.0%
-4.5%

40%

han et al. learning both weights and connections for efficient neural networks, nips 2015

50%

60%

parametes pruned away

70%
pruned

80%

90%

100%

pruning of alexnet

pruning of vgg-16

pruning neural talk and lstm

pruning neural talk and lstm

speedup of pruning on cpu/gpu

intel core i7 5930k: mkl cblas gemv, mkl spblas csrmv
nvidia geforce gtx titan x: cublas gemv, cusparse csrmv
nvidia tegra k1: cublas gemv, cusparse csrmv

history of pruning

yann lecun, john s. denker, and sara a. solla. optimal brain damage. in advances in 
neural information processing systems, pages 598   605. morgan kaufmann, 1990. 

babak hassibi, david g stork, et al. second order derivatives for network pruning: optimal 
brain surgeon. advances in neural information processing systems, pages 164   164, 1993. 

see poster:

tue dec 8th 07:00 - 11:59 pm @ 210 c #12
learning both weights and connections for
efficient neural network
song han    jeff pool    john tran    bill dally

reduce storage for each remaining weight

trained quantization

(weight sharing)

pruning: less quantity

quantization: less precision

train connectivity

prune connections

original 
network

100% size

   same 
accuracy

10% size

train weights

cluster  the weights

generate code book

quantize the weights 
with code book

retrain code book

   same 
accuracy

3.7% size

han et al. deep compression: compressing deep neural networks with pruning, trained 
quantization and huffman coding, arxiv 2015

weight sharing via id116

   weights 
(32 bit float)

2.09 -0.98 1.48

0.09

0.05 -0.14 -1.08 2.12

cluster

-0.91 1.92

0

-1.03

1.87

0

1.53

1.49

gradient

cluster index
  (2 bit uint)

centroids

fine-tuned 
centroids

3

1

0

3

0

1

3

1

2

0

1

2

1

3

0

2

3:

2.00

2:

1.50

1:

0.00

0:

-1.00

1.96

1.48

-0.04

-0.97

lr

-0.03 -0.01 0.03

0.02

-0.03 0.12

0.02 -0.07

-0.01 0.01 -0.02 0.12

group by

0.03

0.01

-0.02

reduce

-0.01 0.02

0.04

0.01

0.02 -0.01 0.01

0.04

-0.02

-0.07 -0.02 0.01 -0.02

 -0.01 -0.02 -0.01 0.01

0.04

0.02

0.04

-0.03

3

1

0

3

0

1

3

1

1

0

1

2

1

3

0

2

han et al. deep compression: compressing deep neural networks with pruning, trained 
quantization and huffman coding, arxiv 2015

trained quantization

han et al. deep compression: compressing deep neural networks with pruning, trained 
quantization and huffman coding, arxiv 2015

bits per weight

pruning + trained quantization

see workshop poster   

thur dec 10 3:00 - 7:00 pm deep learning symposium @ 210 a, b level 2
deep compression: compressing deep neural
networks with pruning, trained quantization and
huffman coding
song han    huizi mao   bill dally

paper

demo: 

pocket alexnet

summary of compression

under review as a conference paper at iclr 2016

table 1: the compression pipeline can save 35    to 49    parameter storage with no loss of accuracy.

network
lenet-300-100 ref
lenet-300-100 compressed
lenet-5 ref
lenet-5 compressed
alexnet ref
alexnet compressed
vgg-16 ref
vgg-16 compressed

top-1 error top-5 error
1.64%
1.58%
0.80%
0.74%
42.78%
42.78%
31.50%
31.17%

-
-
-
-
19.73%
19.70%
11.32%
10.91%

rate

parameters compress
1070 kb
27 kb
1720 kb
44 kb
240 mb
6.9 mb
552 mb
11.3 mb

40   
39   
35   
49   

table 2: compression statistics for lenet-300-100. p: pruning, q:quantization, h:huffman coding.

layer

#weights weights%

compress neural networks without affecting accuracy by:
1. pruning the unimportant connections => 
compress
weight
2. quantizing the network and enforce weight sharing =>
rate
bits
(p+q)
(p+q)
3. apply huffman encoding 
3.1%
6
3.8%
6
15.7%
6
3.1% (32   )
6

weight
bits
(p+q+h)
4.4
4.4
4.3
5.1

index
bits
(p+q+h)
3.7
4.3
3.2
3.7

(p)
8%
9%
26%
8%(12   )

index
bits
(p+q)
5
5
5
5

235k
30k
1k
266k

ip1
ip2
ip3
total

compress
rate
(p+q+h)
2.32%
3.04%
12.70%
2.49% (40   )

30x     50x compression means

    complex dnns can be put in mobile applications (<100mb total)

    1gb network (250m weights) becomes 20-30mb

    memory bandwidth reduced by 30-50x

    particuarly for fc layers in real-time applications with no reuse

    memory working set fits in on-chip sram

    5pj/word access vs 640pj/word

outline
    the problem
    baseline
    parallelization
    gpus
    reduced precision
    compression
    better algorithms
    hardware for dnns
    summary

before accelerating, make sure you have the fastest 

algorithm 

fft for convolution

pointwise
matrix
multiply

outputs

fft s-1

kernels

ffts

inputs

ffts

figure 1: illustration of the algorithm. note that the matrix-multiplication involves multiplying all
input feature maps by all corresponding kernels.

method requires (n   k + 1)2k2 operations. the complexity of the fft-based method requires
6cn2 log n + 4n2 operations: each fft requires o(n2 log n2) = o(2n2 log n) = 2cn2 log n,
and the pointwise product in the frequency domain requires 4n2 (note that the products are between
two complex numbers). here c represents the hidden constant in the o notation. 2
our algorithm is based on the observation that in all of the operations (1), (2) and (3), each of the

mathieu, henaff, & lecunn, fast training of convolutional networks through ffts, cvpr, 2014

fft for convolution

pointwise
matrix
multiply

outputs

fft s-1

kernels

ffts

inputs

ffts

figure 1: illustration of the algorithm. note that the matrix-multiplication involves multiplying all
input feature maps by all corresponding kernels.

ffts are amortized over k input maps and j output maps
conventional  convolution is kjm2n2 ops for mxm kernel and nxn maps
fft is 4kjn2 + c(k+j)(2n2logn)

faster     even for m=3     with moderate sized k, j.

method requires (n   k + 1)2k2 operations. the complexity of the fft-based method requires
6cn2 log n + 4n2 operations: each fft requires o(n2 log n2) = o(2n2 log n) = 2cn2 log n,
and the pointwise product in the frequency domain requires 4n2 (note that the products are between
two complex numbers). here c represents the hidden constant in the o notation. 2
our algorithm is based on the observation that in all of the operations (1), (2) and (3), each of the
matrices indexed by f is convolved with each of the matrices indexed by f0. we can therefore

mathieu, henaff, & lecunn, fast training of convolutional networks through ffts, 2013

fft performance

)
s
(
 
e
m
t

i

fft

kernel size (m)

mathieu, henaff, & lecunn, fast training of convolutional networks through ffts, 2013

vnet layer formula (2) for a single image i,    lter k, and tile

labeling tile coordinates as (#x,#y), we rewrite the con-
coordinate (#x,#y) as:

winograd convolution

yi,k,!x,!y =

di,c,!x,!y     gk,c

gk,c     rr  r is    lter k in channel c.
g, bt, and at are    lter, data, and inverse transforms.
yk,b     rm  m is output tile b in    lter k.
for k = 0 to k do

for c = 0 to c do

=

c$c=1
c$c=1
= at% c$c=1

at%uk,c     vc,i,!x,!y&a
uk,c     vc,i,!x,!y&a

(10)

for b = 0 to p do

for c = 0 to c do

winograd. arithmetic complexity of computations, volume 33. siam, 1980
lavin & gray, fast algorithms for convolutional neural networks, 2015

thus we can reduce over c channels in transform space,
and only then apply the inverse transform a to the sum.
this amortizes the cost of the inverse transform over the
number of channels.

for    = 0 to    do

for    = 0 to    do

for k = 0 to k do

for b = 0 to p do

summary of algorithms

    fft or winograd convolution
    ~2x faster for 3x3 convolutions
    ~25x faster for 11x11 convolutions

    special purpose hardware running brute-force convolution looses its 

advantage vs. gpu running fft convolutions

    fft convolution cost is independent of convolution size

outline
    the problem
    baseline
    parallelization
    gpus
    reduced precision
    compression
    better algorithms
    hardware for dnns
    summary

to be maximally efficient use special-purpose hardware

unless you are memory limited

diannao (electric brain)

figure 15. layout (65nm).

output 
layer 

hidden 
layer 

input 
neuron 
synapse 

*'

weight'

neuron 
output 

+'

synapses 

x 

ai 

+'

bi 

table'

*'

x 

control#processor#(cp)#
instruc:ons#

nfu)1%

nfu)2%

nfu)3%

#

a
m
d

inst.#

t
n

#

nbin%

#

a
m
d

inst.#

m
e
m
o
r
y
#
i

n
t
e
r
f
a
c
e

t
n
#
x
#
t
n

inst.# d
m
a

#

t
n

#

nbout%

(%)

area
component
or block
in   m2
accelerator 3,023,077
combinational
memory
registers
clock network
filler cell
sb
nbin
nbout
nfu
cp
aximux
other

608,842 (20.14%)
1,158,000 (38.31%)
375,882 (12.43%)
68,721
(2.27%)
811,632 (26.85%)
1,153,814 (38.17%)
427,992 (14.16%)
433,906 (14.35%)
846,563 (28.00%)
(5.69%)
141,809
(0.32%)
9,767
9,226
(0.31%)

critical
(%) path in ns
1.02

power
in mw
485
89 (18.41%)
177 (36.59%)
86 (17.84%)
132 (27.16%)

105 (22.65%)
91 (19.76%)
92 (19.97%)
132 (27.22%)
(6.39%)
31
(2.65%)
8
26
(5.36%)

#

#

5

4

3

2

sb%

(%)

critical path (ns)
area (mm^2)
energy (nj)

figure 11. accelerator.

figure 15. layout (65nm).

figure 9. full hardware implementation of neural networks.

- diannaoimprovedid98computationefficiencybyusingdedicatedfunctional
unitsandmemorybuffersoptimizedfortheid98workload.
- multiplier+addertree+shifter+non-linearlookuporchestratedbyinstructions
- 452gop/s,3.02mm^2and485mw

area
component
or block
in   m2
accelerator 3,023,077
combinational
memory
registers
clock network
filler cell
sb
nbin
nbout
nfu
finally, we have also evaluated a design with tn = 8,
cp
aximux
and thus 64 multipliers in nfu-1. the total area for that
chen et al. diannao: a small-footprint high-throughput accelerator for ubiquitous machine-learning, asplos 2014
other
design is 0.85 mm2, i.e., 3.59x smaller than for tn = 16
due to the reduced buffer width and the fewer number of

logic which is in charge of reading data out of nbin/nbout;
such as the intel etann [18] at the beginning of the 1990s,
figure 16. speedup of accelerator over simd, and of ideal ac-
next versions will focus on how to reduce or pipeline this
not because neural networks were already large at the time,
critical path. the total ram capacity (nbin + nbout + sb
celerator over accelerator.
but because hardware resources (number of transistors) were
+ cp instructions) is 44kb (8kb for the cp ram). the area
naturally much more scarce. the principle was to time-
and power are dominated by the buffers (nbin/nbout/sb) at
share the physical neurons and use the on-chip ram to
respectively 56% and 60%, with the nfu being a close sec-
every cycle (we naturally use 16-bit    xed-point operations
store synapses and intermediate neurons values of hidden
ond at 28% and 27%. the percentage of the total cell power
layers. however, at that time, many neural networks were
in the simd as well). as mentioned in section 7.1, the
is 59.47%, but the routing network (included in the different
small enough that all synapses and intermediate neurons
accelerator performs 496 16-bit operations every cycle for
values could    t in the neural network ram. since this is no
components of the table breakdown) accounts for a signif-
both classi   er and convolutional layers, i.e., 62x more ( 496
8 )
longer the case, one of the main challenges for large-scale
icant share of the total power at 38.77%. at 65nm, due to
than the simd core. we empirically observe that on these
neural network accelerator design has become the interplay
the high toggle rate of the accelerator, the leakage power is
two types of layers, the accelerator is on average 117.87x
between the computational and the memory hierarchy.
almost negligible at 1.73%.
faster than the simd core, so about 2x above the ratio
5. accelerator for large neural networks
of computational operators (62x). we measured that, for
in this section, we draw from the analysis of sections 3 and
classi   er and convolutional layers, the simd core performs
4 to design an accelerator for large-scale neural networks.
2.01 16-bit operations per cycle on average, instead of the

608,842 (20.14%)
1,158,000 (38.31%)
375,882 (12.43%)
68,721
(2.27%)
16x16
8x8
811,632 (26.85%)
1,153,814 (38.17%)
427,992 (14.16%)
433,906 (14.35%)
846,563 (28.00%)
neuron to a neuron of the next layer, and from one synap-
(5.69%)
141,809
tic latch to the associated neuron. for instance, an execution
(0.32%)
9,767
time of 15ns and an energy reduction of 974x over a core
(0.31%)
9,226
has been reported for a 90-10-10 (90 inputs, 10 hidden, 10
outputs) id88 [38].

- weights in off-chip dram

table 6. characteristics of accelerator and breakdown by com-
ponent type (   rst 5 lines), and functional block (last 7 lines).

power
in mw
485
89 (18.41%)
177 (36.59%)
86 (17.84%)
132 (27.16%)

105 (22.65%)
91 (19.76%)
92 (19.97%)
132 (27.22%)
(6.39%)
31
(2.65%)
8
26
(5.36%)

figure 10. energy, critical path and area of full-hardware layers.

critical
(%) path in ns
1.02

128x16

32x32

32x4

64x8

0

1

figure 16. speedup of accelerator over simd, and of ideal ac-
celerator over accelerator.

every cycle (we naturally use 16-bit    xed-point operations
in the simd as well). as mentioned in section 7.1, the
accelerator performs 496 16-bit operations every cycle for
both classi   er and convolutional layers, i.e., 62x more ( 496
than the simd core. we empirically observe that on these
two types of layers, the accelerator is on average 117.87x
faster than the simd core, so about 2x above the ratio
of computational operators (62x). we measured that, for
classi   er and convolutional layers, the simd core performs
2.01 16-bit operations per cycle on average, instead of the
upper bound of 8 operations per cycle. we traced this back
to two major reasons.

bination of preloading and reuse in nbin and sb buffers;
note that we did not implement a prefetcher in the simd
core, which would partly bridge that gap. this explains the
high performance gap for layers class1, class3 and
conv5 which have the largest feature maps sizes, thus
the most spatial locality, and which then bene   t most from
preloading, giving them a performance boost, i.e., 629.92x
on average, about 3x more than other convolutional layers;
we expect that a prefetcher in the simd core would cancel
that performance boost. the spatial locality in nbin is ex-
ploited along the input feature map dimension by the dma,
and with a small ni, the dma has to issue many short mem-
ory requests, which is less ef   cient. the rest of the convolu-
tional layers (conv1 to conv4) have an average speedup
of 195.15x; conv2 has a lesser performance (130.64x) due
to private kernels and less spatial locality. pooling layers

are forward-propagated (so neurons of the previous layer
are the inputs) and then backward-propagated (so neurons
layer are now the inputs). as a result, de-
pending on how data (neurons and synapses) are allocated
they need to be moved between the forward
and backward phases. since there are many more synapses
than neurons (e.g., o(n 2) vs. o(n ) for classi   er layers,
k    k    nif    nof    nx    ny vs. nif    nx    ny for
convolutional layers with private kernels, see section ii), it
is only logical to move neuron outputs instead of synapses.
second, having all synapses (most of the computation input-
s) next to computational operators provides low-energy/low-
latency data (synapses) transfers and high internal band-

central block

tile12 tile13

tile14 tile15

tile10 tile11

tile8 tile9

tile4 tile5

tile6 tile7

tile2 tile3

tile0 tile1

y
h
p
2
t
h

h
t
3
p
h
y

ht0 phy

controller

controller

controller

controller

ht0

ht3

ht2

ht1

-96.3

-96.3

-96.3

-96.3

 

 

  

  

  

  

  

  

  

  

  

  

  

  

ht1 phy

n
k
)

edram2

wires

edram3

n
k
)

diannao and friends

ht2.0 (south link)

figure 4: simpli   ed    oorplan with a single central nfu showing
wire congestion.

1chip

4chips

16chips

data
to sb

64chips

1000

ht2.0 (north link)
tile
tile

100

(cid:83)
(cid:88)
(cid:71)
(cid:72)
(cid:72)
(cid:83)
(cid:54)(cid:54)

tile

tile

h
t
2
.
0
 
(

w
e
s
t
 
l
i
n
k
)

tile

tile

tile

tile

tile

10
edram
router

1

tile

tile

tile

tile

h
t
2
.
0
 
(
e
a
s
t
 
l
i
n
k
)

sb 

edram
bank0

sb 

edram
bank1

nfu

sb 

sb 

dadiannao (bigger computer) uses
multi-chip and edram to fit
larger
models. each chip is 68mm^2 fitting
12 million parameters, consumes 16w

16 
output
neurons

16 
input
neurons

figure 9: snapshot of the node layout.

as shown in table i, layer sizes can range from less than
1mb to about 1gb, most of them ranging in the tens of mb.
(%) power (w )
while srams are appropriate for caching purposes, they
15.97
1.80
(11.27%)
are not dense enough for such large-scale storage. however,
6.15 ( 38.53%)
8.01 ( 50.14%)
edrams are known to have a higher storage density. for
0.01
(0.06%)
instance, a 10mb sram memory requires 20.73mm2 at
6.06
6.12
28nm [36], while an edram memory of the same size and
3.07
0.71
at the same technology node requires 7.27mm2 [50], i.e., a

component/block area (  m2)
67,732,900
whole chip
7,898,081 (11.66%)
central block
30,161,968 (44.53%)
17,620,440 (26.02%)
(8.97%)
6,078,608
(8.82%)
5,973,803
3,979,345
(5.88%)
32207390 (47.55%)
3,348,677
(4.94%)
(0.87%)
586323
27,611,165 (40.76%)

wires
other
combinational
memory
registers
clock network
filler cell

(37.97%)
(38.30%)
(19.25%)
(4.48%)

table vi: node layout characteristics.

moreover, providing suf   cient edram capacity to hold
all synapses on the combined edram of all chips will
edram). the combinational logic and register only account
for 5.88% and 4.94% of the area respectively.
save on off-chip dram accesses, which are particularly
costly energy-wise. for instance, a read access to a 256-
bit wide edram array at 28nm consumes 0.0192nj (50  a,
0.9v, 606 mhz) [25], while a 256-bit read access to a

we used synopsys primepower to estimate the power
consumption of the chip. the peak power consumption is
15.97 w (at a pessimistic 100% toggle rate), i.e., roughly 5-
10% of a state-of-the-art gpu card. the architecture block
breakdown shows that the tiles consume more than one third

benchmarks. we collected 10 id98s from representative
visual recognition applications and used them as our bench-

figure 17: layout of shidiannao (65 nm).

edram
tile
tile
figure 10: speedup w.r.t. the gpu baseline (id136). note that
bank3
ht2.0 (south link)
conv1 and the full nn need a 4-node system, while conv3* and
conv4* even need a 36-node system.

edram
bank2

tile

figure 5: tile-based organization of a node (left) and tile archi-
tecture (right). a node contains 16 tiles, two central edram banks
(%)
and fat tree interconnect; a tile has an nfu, four edram banks
and input/output interfaces to/from the central edram banks.

conv3* and conv4*, need a 36-node system because
their size is respectively 1.29 gb and 1.32 gb. the full
nn contains 59.48m synapses, i.e., 118.96mb (16-bit data),
requiring at least 4 nodes.

shidiannao
16-bit
64
64 kb
64 kb
128 kb
32 kb

table 3: parameter settings of shidiannao and diannao.

data width
# multipliers
nbin sram size
nbout sram size
sb sram size
inst. sram size

above example), and we interleave the synapses rows among
the four banks.

on average, the 1-node, 4-node, 16-node and 64-node
architectures are respectively 21.38x, 79.81x, 216.72x, and
diannao
450.65x faster than the gpu baseline. 1 the    rst reason for
the higher performance is the large number of operators:
16-bit
64
in each node, there are 9216 operators (mostly multipliers
1 kb
and adders), compared to the 2496 macs of the gpu.
1 kb
the second reason is that the on-chip edram provides the
16 kb
necessary bandwidth and low-latency access to feed these
8 kb
many operators.

we placed and routed this design at 28nm (st technology,
lp), and we obtained the    oorplan of figure 4. the nfu
footprint is very small at 0.78mm2 (0.88mm  0.88mm), but
the process imposes an average spacing of 0.2  m between
wires, and provides only 4 horizontal metal layers. as a
result, the 65536 wires connecting the nfu to the edram
energy (nj)
= 3.2768mm, see figure 4.
require a width of 65536  0.2
6048.70 (100%)
5281.09 (87.29%)
consequently, wires occupy 4    3.2768    3.2768     0.88   

nevertheless, the scalability of the different layers varies a
lot. lrn layers scale the best (no inter-node communication)
with a speedup of up to 1340.77x for 64 nodes (lrn2),
conv and pool layers scale almost as well because they
only have inter-node communications on border elements,
e.g., conv1 achieves a speedup of 2595.23x for 64 nodes,

table 4: hardware characteristics of shidiannao at 1ghz,
where power and energy are averaged over 10 benchmarks.

power (mw)
320.10 (100%)
268.82 (83.98%)

area (mm2)
4.86 (100%)
0.66 (13.58%)

accelerator
total
nfu

4

shidiannao (vision computer) it can 
fits small model (up-to 64k parameters) 
on-chip. it maps the computation on 2d 
pe array. the chip is 4.86 mm^2 and 
consumes 320 mw , 

nbinnboutsbibnfuimg

ce
   

f

'
%
&

$
"
#
mn
,[

in 

reduce
c
l
   =

(
img
=
convolution engine

[
reduce
c
k

[
map

   =

c

c

]

lk
],[

,

f

[

lmkn
   
   

,

]]
]
)

out 

 
e
c
u
d
e
r

map 

coefficients 

    convolution engine (ce), is specialized for the convolution-like data-flow that is common in image 

processing.

    ce achieves energy efficiency by capturing data reuse patterns, eliminating data transfer overheads, 

and enabling a large number of operations per memory access. 

shacham@alumni.stanford.edu 

isca'13 

15 

    with restricted the domain in image and video processing, flexible convolution engine improves 

improves energy and area efficiency by 8-15x over a simd engine.

wajahat qadeer et.al, convolution engine: balancing efficiency & flexibility in specialized computing 

connected to each other, and/or to global data wires and
neighbor tiles through an on-chip network of mux-based
routers. the on-chip network, once being con   gured, will form

neuflow

the con   guration data from flow-cpu placed on a runtime
con   guration bus (re-)con   gures most aspects of the grid
at runtime, including connections, operators and smart dma
modes.
b. operation

macbook pro laptop operating at 90 w (30 w for the cpu).
the mgpu and gpu data are obtained from a cuda-based
implementation running on a laptop/mobile nvidia gt335m
operating at 1 ghz and 30 w and on a nvidia gtx480
operating at 1 ghz and 220 w. the fpga performance was

an execution on neuflow typically has the following steps:
(1) the control unit con   gures each tile to be used for the
computation and each connection between the tiles and their
neighbors and/or the global lines, by sending a con   guration
command to each of them, (2) it con   gures the smart dma
to prefetch the data to be processed, and to be ready to write
results back to off-chip memory, (3) when the dma is ready, it
triggers the streaming out, (4) each tile processes its respective
incoming streaming data, and passes the results to another tile,
or back to the smart dma, (5) the control unit is noti   ed of
the end of operations when the smart dma has completed.
the computing grid interconnected by the on-chip network
can perform arbitrary computations on streams of data, from
plain unary operations to complex nested operations. by a
networking of mux-based routers, operators can be easily
fig. 6. chip layout in a 2.5 x 5mm2 die area
cascaded and connected across tiles, independently managing
their    ow by the use of input/output fifos. as illustrated in

tapo (www.tapof   ce.org) for the chip tape-out.

[1] mobileye. [online]. available: http://www.mobileye.com
[2] k. jarrett, k. kavukcuoglu, m. ranzato, and y. lecun,    what is the best
multi-stage architecture for object recognition?    in proc. international
conference on id161 (iccv   09).

[3] neu   ow. [online]. available: http://www.neu   ow.org
[4] m. h. cho, c.-c. cheng, m. kinsy, g. e. suh, and s. devadas,    diastolic
arrays: throughput-driven recon   gurable computing,    in proceedings of
ieee/acm international conference on computer-aided design (ic-
cad), 2008, pp. 457   464.

[5] m. platzner, j. teich, and n. wehn, dynamically recon   gurable systems:

architectures, design methods and applications, 1st ed.
publishing company, incorporated, 2010.

[6] c. farabet, b. martini, b. corda, p. akselrod, e. culurciello, and y. le-
cun,    neu   ow: a runtime recon   gurable data   ow processor for vision,   
in id161 and pattern recognition workshops (cvprw), 2011
ieee computer society conference on, june 2011, pp. 109    116.

[7] p.-h. pham, p. mau, j. kim, and c. kim,    an on-chip network fabric
supporting coarse-grained processor array,    ieee transactions on very
large scale integration (vlsi) systems, 10.1109/tvlsi.2011.2181546
(in press).

[8] r. collobert,    torch,    presented at the workshop on machine learning

open source software, nips, 2008.

    an soc designed  to accelerate neural networks and other complex vision algorithms 

fig. 1. the neuflow architecture

based on large numbers of convolutions  and matrix-to-matrix operations. 

    160 gops, 570 mw, 12.5 mm^2 @ ibm soi 45nm 

p. pham et.al, neuflow: dataflow vision processing system-on-a-chip 

efficient id136 engine

only at the beginning and written at the end of the batch.

distributed leading non-zero detection. input activa-
tions are hierarchically distributed to each pe. to take ad-
vantage of the input vector sparsity, we use leading non-zero
detection logic to select the    rst positive result. each group
of 4 pes does a local leading non-zero detection on input ac-
tivation. the result is sent to a leading non-zero detection
node (lnzd node) illustrated in figure 4. four of lnzd
nodes    nd the next non-zero activation and sends the result
up the lnzd node quadtree. that way the wiring would not
increase as we add pes. at the root lnzd node, the posi-
tive activation is broadcast back to all the pes via a separate

central control unit. the central control unit (ccu)
is the root lnzd node. it communicates with the master
such as cpu and monitors the state of every pe by setting
the control registers. there are two modes in the central
unit: i/o and computing. in the i/o mode, all of the pes
are idle while the activations and weights in every pe can be
accessed by a dma connected with the central unit. in the
computing mode, the ccu will keep collecting and sending
the values from source activation banks in sequential order
until the input length is exceeded. by setting the input length
and starting address of pointer array, eie will be instructed

simulator, rtl and layout. we implemented a custom
cycle-accurate c++ simulator for the accelerator aimed to

total
memory
clock network
register
combinational
   ller cell
act_queue
ptrread
spmatread
arithmunit
actrw
   ller cell

power
(mw)
9.157
5.416
1.874
1.026
0.841

0.112
1.807
4.955
1.162
1.122

(%)

(59.15%)
(20.46%)
(11.20%)
(9.18%)

(1.23%)
(19.73%)
(54.11%)
(12.68%)
(12.25%)

area
(      mmm222)
638,024
594,786
866
9,465
8,946
23,961
758
121,849
469,412
3,110
18,934
23,961

(%)

(93.22%)
(0.14%)
(1.48%)
(1.40%)
(3.76%)
(0.12%)
(19.10%)
(73.57%)
(0.49%)
(2.97%)
(3.76%)

table 2: the implementation results of one pe in eie and
the breakdown by component type (line 3-7), by module
(line 8-13). the critical path of eie is 1.15ns

dense layer, as the caffe library does []. for the compressed
sparse layer, we stored the sparse matrix in in csr format,
and used cusparse csrmv kernel, which is optimized for
sparse matrix-vector multiplication on gpus.

3) mobile gpu. we use nvidia tegra k1 that has 192
cuda cores as our mobile gpu baseline. we used cublas
gemv for the original dense model and cusparse csrmv
for the compressed sparse model. tegra k1 doesn   t have
software interface to report power consumption, so we mea-
sured the total power consumption with a power-meter, then
assumed 15% ac to dc conversion loss, 85% regulator ef-
   ciency and 15% power consumed by peripheral compo-
nents [26, 27] to report the ap+dram power for tegra k1.

pe 

pe 

pe 

pe 

pe 

pe 

pe 

pe 

pe 

pe 

pe 

pe 

(a) 

pe 

pe 

pe 

pe 

spmat

act queue 
ptr_even

act index 

arithm

act value 

ptr_odd

encoded 
weight 

even ptr sram bank 

odd ptr sram bank 

col 
start/
end 
addr 

spmat

sparse 
matrix 
sram 

regs 

decoder 

address 

pointer read 

sparse matrix access 

                      arithmetic unit 

relative 
index 
(b) 

pointer read act r/w act queue sparse matrix access sparse matrix sram                       arithmetic unit regs col start/end addr act index weight decoder address accum dest act regs act sram act value encoded weight relative index src act regs absolute address bypass leading nzero detect even ptr sram bank odd ptr sram bank relu (b) (a) from ne from se from sw leading nzero detect act0 act1 act3 act value s0 s1 s3 from nw act2 s2 nzero index act0,1,2,3 speedup

cpu compressed

gpu  gpu compressed mgpu mgpu compressed

eie

248

507

115

1018

618

92

63

98

60

189

alex-6

alex-7

alex-8

vgg-6

vgg-7

vgg-8

nt-we

nt-wd nt-lstm geo mean

cpu (baseline)

1000x

p
u
d
e
e
p
s

100x

10x

1x

0.1x

energy efficiency

cpu (baseline)

cpu compressed

gpu 

gpu compressed

mgpu

mgpu compressed

eie

100000x
10000x
1000x
100x
10x
1x

y
c
n
e
i
c
i
f
f

e
 
y
g
r
e
n
e

35k

62k

15k

120k

77k

12k

9k

11k

8k

24k

alex-6

alex-7

alex-8

vgg-6

vgg-7

vgg-8

nt-we

nt-wd nt-lstm geo mean

table 4: performance comparison between cpu, gpu, mobile gpu implementations and eie.

scalability and load balancing

nt-

platform

cpu
(core
i7-5930k)
100
gpu
(titan x)
10
mgpu
(tegra k1)
1
eie

64
1pe
1

batch matrix
size
1

type
dense
sparse
dense
sparse
2pes
dense
sparse
dense
sparse
dense
sparse
dense
sparse
alex-7
actual time

64

1

64
alex-6
theoretical time

fc6
7516.2
3066.5
318.4
1417.6
4pes
541.5
134.8
19.8
94.6
12437.2
2879.3
1663.6
4003.9
alex-8
28.1
30.3

alexnet
fc7
6187.1
1282.1
188.9
682.1
8pes
243.0
65.8
8.9
51.5
5765.0
1256.5
2056.8
1372.8
vgg-6
11.7
12.2

16pes

fc8
1134.9
890.5
45.8
407.7
80.5
54.6
5.9
23.2
2252.1
837.0
298.0
576.7
8.9
9.9

fc6
35022.8
3774.3
1056.0
1780.3
1467.8
167.0
53.6
121.5
35427.0
4377.2
2001.4
8024.8
vgg-7
28.1
34.4

vgg16
fc7
5372.8
545.1
188.3
274.9
32pes
243.0
39.8
8.9
24.4
5544.3
626.3
2050.7
660.2
vgg-8
7.9
8.7

y
t
i
l
i

l

b
a
a
c
s

fc8
774.2
777.3
45.7
363.1
64pes
80.5
48.0
5.9
22.0
2243.1
745.1
483.9
544.1
nt-we
7.3
8.4

wd
we
1361.4
605.0
437.4
261.2
69.0
28.7
176.4
117.7
128pes
90.1
65
17.7
41.1
2.3
3.2
11.0
10.9
2565.5
1316
240.6
570.6
956.3
87.8
236.3
187.7
nt-wd
5.2
13.0
13.9
8.0

lstm
470.5
260.0
28.8
107.4
256pes
51.9
18.5
2.5
9.0
956.9
315
95.2
186.5
6.5
7.5

nt-lstm

figure 11: system scalability. the average ef   ciency of single pe    nally decreases as the number of pes increases. on some
very sparse layers, having more pes initially increases the ef   ciency a bit.

fifo=2

fifo=4
2pes

1pe

fifo=8
4pes

fifo=16
8pes

16pes

fifo=32
32pes

fifo=64
64pes

fifo=128
128pes

256pes

fifo=256

 

fifo=1
100%
80%
n
60%
o
i
t
a
40%
t
u
p
20%
m
o
0%
c

u
f
e
s
u

 
l

e
p
#
~

 

 

~

 

l

e
c
n
a
a
b
d
a
o
l

 

o
f
f
#

i

100%
80%
60%
40%
20%
0%
alex-6

alex-8
figure 8: load ef   ciency improves as fifo size increases. when the size is larger than eight, the marginal gain quickly
diminishes. so we choose fifo depth to be eight.
figure 12: the number of padding zeros decreases as the number of pes goes up, leading to less padding zeros and better
compute ef   ciency.

nt-lstm

nt-we

nt-wd

alex-7

alex-6

vgg-6
vgg-6

vgg-7
vgg-7

vgg-8
vgg-8

nt-we

nt-wd

nt-lstm

alex-7

alex-8

   
fpgas
    a field-configurable asic
    fixed-function units have good 

efficiency
    arithmetic units (int and fp)
    rams
    arm cores

    logic built from luts has poor 

efficiency
    30-100x worse ops/j than an asic

microsoft experience

platform  

library/os 

id163 1k 

id136 
throughput 

peak tflops 

effective 
tflops 

estimated  

peak power with 

server 

estimated 
gops/j 

(assuming 
peak power) 

16-core, 2-socket xeon 

e5-2450, 2.1ghz 

caffe + intel mkl  
ubuntu 14.04.1* 

53 images/s 

0.27t 

0.074t (27%) 

~225w 

~0.3 

arria 10 gx1150 

windows server 2012 

369 images/s1 

1.366t 

0.51t (38%) 

~265w 

~1.9 

nervanasys-32 on  

nvidia titan x 

nervanasys-32 on  

ubuntu 14.0.4 

4129 images/s2 

6.1t 

5.75t (94%) 

~475w 

~12.1 

1dense layer time estimated 
2https://github.com/soumith/convnet-benchmarks 

includes server power; however, cpus 
available to other jobs in the datacenter 

33 

ovtcharov et al., toward accelerating deep learning at scale using specialized logic, hot chips 2015

0.0082
0.0734
30.764

1.1320
1.200
136.97

t o    p e_num   

p e_num    t i    t o
datain_port_num

weightin_port_num

t i    t o

compute_weight =

convolver_num
compute is much smaller than tf c

.

load, and

thus the total cycles needed by one fc layer can be estimated as:

t o    p e_num      

p e_num    t i    t o
datain_port_num

0.008
0.462
4.276
0.046
316.64 299.93 17.245 9.075

0.958
1.791

0.046
0.405

4.582
53.933
453.70

17.749 178.26 8.5595 1025.0
17.166 181.23 40.983 1595.7
97.158 102.57 1783.9 3390.0

comparison of fpgas

1.7896
1.3609
67.807

table 7: comparison with other fpga accelerators.

year

platform

clock(mhz)

bandwidth (gb/s)

quantization

strategy
power (w)
problem

complexity (gop)

performance

(gop/s)
resource
ef   ciency

(gop/s/slices)
power ef   ciency

(gop/s/w)

[1]
[13]
2010
virtex5
sx240t

120
   

[2]
[30]
2014
zynq

150
4.2

xc7z045

[3]
[8]
2015
virtex7
vx485t

100
12.8

[4]
ours
2015
zynq

150
4.2

xc7z045

48-bit    xed 16-bit    xed 32-bit    oat

16-bit    xed

14
0.52

16

4.30  10   4

1.14

8

0.552

23.18

   

2.90

18.61
1.33

61.62

9.63
30.76

187.80 (conv)
136.97 (overall)

8.12  10   4 3.58  10   3 (conv)
2.61  10   3 (overall)

3.31

19.50 (conv)
14.22 (overall)

in summary, under the given constraints, the runtime of a con-
v layer and an fc layer can estimated through equation 14 and

[1] s.chakradhar, et.al,   a dynamically configurable coprocessor for convolutional neural networks,    in acm sigarch computer architecture news, 
[2] v. gokhale, et.al,    a 240 g-ops/s mobile coprocessor for deep neural networks,    in id161 and pattern recognition workshops (cvprw), 
[3] c. zhang et.al,    optimizing fpga-based accelerator design for deep convolutional neural networks,    in fpga 2015
[4] j. qiu et.al,    going deeper with embedded fpga platform for convolutional neural network   , to appear in fpga  2016

table 8: projected frame rates on zynq/vc707 board and us-
ing 8/4-bit quantization with vgg-16-svd network.

16-bit quantization 8-bit quantization

total resources

(13)

platform

,

ff

lut
218600 437200
303600 607200

bandwidth # of pe
4.2gbps
4.2gbps

2
3

fps
4.45
5.88

# of pe

4
6

fps
8.9
11.76

nin    nout    row2

convolver_num2    p e_num

.

(14)

zynq
vc707

hardware comparison

table 5: comparison with existing platforms for dnns

tegra k1

a-eye [14] dadiannao[11]

platform
year
platform type
technology
clock (mhz)
memory type
max dnn model size
quantization stategy
area (mm2)
peak throughput (gop/s)
throughput for m   v (gop/s)
power(w)
power ef   ciency (gop/s/w)
power ef   ciency for m         v (gop/s/w)

titan x
2015
gpu
28nm
1075

2014
mgpu
28nm
852

dram+
sram
<3g

32-bit    oat

dram+
sram
<500m
32-bit    oat

-

3225
138.1
250
12.9
0.55

-
365
5.8
8.0
45.6
0.73

2015
fpga

-
150

dram
<500m

16-bit    xed

-
188
1.2
9.63
19.5
0.12

edram+
sram
11.3m

16-bit    xed

2014
asic
28nm
606

67.7
5580
205
15.97
349.4
12.8

eie (ours)

2015
asic
45nm
800

sram
84m

40.8
102
94.6
0.59
172.9
160.3

4-bit ! 16-bit    xed

platforms that are able to store and execute large-scale neu-
ral networks: titan x (gpu), tegra k1 (mobile gpu), a-
eye (fpga) and dadiannao (asic). all other four plat-
forms suffer from low-ef   ciency during matrix-vector mul-
tiplication. a-eye is optimized for conv layers and all of
the parameters are fetched from the external ddr3 mem-
ory, making it extremely sensitive to bandwidth problem.

cision. [43] developed spmv techniques that utilizes large
percentages of peak bandwidth for throughput-oriented ar-
chitectures like the gpu. they achieved over an order of
magnitude performance improvement over a quad-core intel
clovertown system.

to pursue a better computational ef   ciency, several recent
works focus on using fpga as an accelerator for spmv.

[2] s. han et.al,    eie: efficient id136 engine on compressed deep neural network   , submitted to isca 2016

summary of special purpose hardware

    diannao     16 16b multiply-accumulators with buffers optimized for dnns

    all data stored in off-chip dram

    shidiannao     for conv layers     up to 64k parameters on chip
    dadiannao     for fc layers     up to 12m parameters in on-chip edram
    convolution engine     fast convolutions (brute-force algorithm)
    eie     hardware for compressed networks 

    trained quantization and pruning
    no data movement     scalable to 256pes

bottom line

    arithmetic perf/w of special purpose hardware is ~2x a gpu (fp16)
    perf/w on memory limited layers (fc, not batch) is no better than gpu
    big win from special-purpose hardware is 

    when entire network fits on chip
    decompressing  highly-compressed  networks

    fpgas are just inefficient asics

    good arithmetic and on-chip memory
    30-100x less efficient elsewhere

outline
    the problem
    baseline
    parallelization
    gpus
    reduced precision
    compression
    better algorithms
    hardware for dnns
    summary

hardware and data enable dnns

in 1990, cpus had one 100 specint core

c moore, data processing in exascale-classcomputer systems, salishan, april 2011

today they have 6-8 30,000specint cores

(~200,000x) but moore   s law is over   

c moore, data processing in exascale-classcomputer systems, salishan, april 2011

gpus give an additional 5-10x (2,000,000x)

s
/
f

300
250
200
150
100
50
0

258

242

core i7 tx1

j
/
f

50
40
30
20
10
0

45

11.5x

3.9
core i7 tx1

reproducibility and are therefore more dif   cult to debug. synchronous sgd is simple to understand
and implement. it scales well as we add multiple nodes to the training process.

data parallelism can get another 128x (256,000,000x)

more with model and hyper-parameter parallelism

5-3 (2560)
9-7 (1760)

219

218

217

216

215

214

213

212

)
s
d
n
o
c
e
s
(

e
m
t

i

211

20

21

22

23

24

gpus

25

26

27

baidu, deep speech 2: end-to-end id103 in english and mandarin, 2015 

figure 4: scaling comparison of two networks   a 5 layer model with 3 recurrent layers containing 2560

special-purpose hardware can give another 100x 

(25,000,000,000x)

mostly from localizing memory
mgpu
cpu compressed

gpu compressed

gpu 

cpu (baseline)

mgpu compressed

eie

100000x
10000x
1000x
100x
10x
1x

y
c
n
e
i
c
i
f
f

e
 
y
g
r
e
n
e

35k

62k

15k

120k

77k

12k

9k

11k

8k

24k

alex-6

alex-7

alex-8

vgg-6

vgg-7

vgg-8

nt-we

nt-wd nt-lstm geo mean

accelerate the best algorithms
prune the network
compress the network
fft convolutions

so what should you do?

    for training use clusters of 8-16gpus

    best perf, perf/w, perf/$, and memory bandwidth
    easy parallelism

    for id136 in the data center use single gpus

    tesla m4 and m40

    for id136 in mobile devices (automotive, iot)

    use a tx1 (11.5x perf/w of cpu)

    for the absolute best performance and efficiency use an asic

    but make sure the model fits (memory limited asics no better than gpu)
    and that your algorithm isn   t going to change

thank you

