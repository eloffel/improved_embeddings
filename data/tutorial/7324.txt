tutorial on estimation and multivariate

gaussians

stat 27725/cmsc 25400: machine learning

shubhendu trivedi - shubhendu@uchicago.edu

toyota technological institute

october 2015

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

things we will look at today

    id113
    ml for bernoulli random variables
    maximizing a multinomial likelihood: lagrange

multipliers

    multivariate gaussians
    properties of multivariate gaussians
    maximum likelihood for multivariate gaussians
    (time permitting) mixture models

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

the principle of maximum likelihood

suppose we have n data points x = {x1, x2, . . . , xn} (or
{(x1, y1), (x2, y2), . . . , (xn , yn )})
suppose we know the id203 distribution function that
describes the data p(x;   ) (or p(y|x;   ))
suppose we want to determine the parameter(s)   
pick    so as to explain your data best
what does this mean?
suppose we had two parameter values (or vectors)   1 and   2.
now suppose you were to pretend that   1 was really the true
value parameterizing p. what would be the id203 that
you would get the dataset that you have? call this p 1
if p 1 is very small, it means that such a dataset is very
unlikely to occur, thus perhaps   1 was not a good guess

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

the principle of maximum likelihood

we want to pick   m l i.e. the best value of    that explains the
data you have

the plausibility of given data is measured by the    likelihood
function    p(x;   )
maximum likelihood principle thus suggests we pick    that
maximizes the likelihood function

the procedure:

    write the log likelihood function: log p(x;   ) (we   ll see

later why log)

    want to maximize - so di   erentiate log p(x;   ) w.r.t   

and set to zero

    solve for    that satis   es the equation. this is   m l

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

the principle of maximum likelihood

as an aside: sometimes we have an initial guess for   
before seeing the data
we then use the data to re   ne our guess of    using bayes
theorem

this is called map (maximum a posteriori) estimation (we   ll
see an example)

advantages of ml estimation:

    cookbook,    turn the crank    method
       optimal    for large data sizes

disadvantages of ml estimation

    not optimal for small sample sizes
    can be computationally challenging (numerical methods)

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

a gentle introduction: coin tossing

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

problem: estimating bias in coin toss

a single coin toss produces h or t .
a sequence of n coin tosses produces a sequence of values;
n = 4
t ,h,t ,h
h,h,t ,t
t ,t ,t ,h
a probabilistic model allows us to model the uncertainly
inherent in the process (randomness in tossing a coin), as well
as our uncertainty about the properties of the source (fairness
of the coin).

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

probabilistic model

first, for convenience, convert h     1, t     0.
    we have a random variable x taking values in {0, 1}

bernoulli distribution with parameter   :

pr(x = 1;   ) =   .

we will write for simplicity p(x) or p(x;   ) instead of
pr(x = x;   )
the parameter        [0, 1] speci   es the bias of the coin

    coin is fair if    = 1

2

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

reminder: id203 distributions

discrete random variable x taking values in set
x = {x1, x2, . . .}
id203 mass function p : x     [0, 1] satis   es the law of

total id203: (cid:88)

p(x = x) = 1

x   x

hence, for bernoulli distribution we know

p(0) = 1     p(1;   ) = 1       .

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

sequence id203

now consider two tosses of the same coin, (cid:104) x1, x2 (cid:105)
we can consider a number of id203 distributions:

joint distribution p(x1, x2)

conditional distributions p(x1 | x2), p(x2 | x1),

marginal distributions p(x1), p(x2)

we already know the marginal distributions:
p(x1 = 1;   )     p(x2 = 1;   ) =   
what about the conditional?

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

sequence id203 (contd)

we will assume the sequence is i.i.d. - independently
identically distributed.

independence, by de   nition, means

p(x1 | x2) = p(x1),

p(x2 | x1) = p(x2)

i.e., the conditional is the same as marginal - knowing that x2
was h does not tell us anything about x1.
finally, we can compute the joint distribution, using chain rule
of id203:

p(x1, x2) = p(x1)p(x2|x1) = p(x1)p(x2)

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

sequence id203 (contd)

p(x1, x2) = p(x1)p(x2|x1) = p(x1)p(x2)

more generally, for i.i.d. sequence of n tosses,

n(cid:89)

p(x1, . . . , xn;   ) =

p(xi;   ).

i=1

example:    = 1

3 . then,

p(h, t, h;   ) = p(h;   )2p(t ;   ) =

(cid:18) 1

(cid:19)2

3

2
3

  

=

2
27

.

note: the order of outcomes does not matter, only the
number of hs and t s.

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

the parameter estimation problem

given a sequence of n coin tosses x1, . . . , xn     {0, 1}n, we
want to estimate the bias   .
consider two coins, each tossed 6 times:
coin 1 h,h,t ,h,h,h
coin 2 t ,h,t ,t ,h,h
what do you believe about   1 vs.   2?
need to convert this intuition into a precise procedure

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

maximum likelihood estimator

we have considered p(x;   ) as a function of x, parametrized
by   .
we can also view it as a function of   . this is called the
likelihood function.
idea for estimator: choose a value of    that maximizes the
likelihood given the observed data.

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

ml for bernoulli

n(cid:89)

n(cid:89)

likelihood of an i.i.d. sequence x = [x1, . . . , xn]:

l(  ) = p(x;   ) =

p(xi;   ) =

i=1

i=1

  xi (1       )1   xi

log-likelihood:

l(  ) = log p(x;   ) =

n(cid:88)

i=1

[xi log    + (1     xi) log(1       )]

due to monotonicity of log, we have

argmax

  

p(x;   ) = argmax

  

log p(x;   )

we will usually work with log-likelihood (why?)

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

ml for bernoulli (contd)

ml estimate is

(cid:98)  m l = argmax   {

(cid:80)n

to    nd it, set the derivative to zero:

i=1 [xi log    + (1     xi) log(1       )]}

n(cid:88)

xi    

1

1       

j=1

(1     xj) = 0

   
     

log p(x;   ) =

=

1       
  

(cid:98)  m l =

1
  

n(cid:88)
(cid:80)n
(cid:80)n
n(cid:88)

i=1

1
n

i=1

xi

j=1(1     xj)

i=1 xi

ml estimate is simply the fraction of times that h came up.

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

are we done?

(cid:98)  m l =

n(cid:88)
example: h,t ,h,t     (cid:98)  m l = 1
how about: h h h h?     (cid:98)  m l = 1

does this make sense?

1
n

i=1

xi

2

suppose we record a very large number of 4-toss sequences
for a coin with true    = 1
2 .
we can expect to see h,h,h,h about 1/16 of all sequences!
a more extreme case: consider a single toss.

(cid:98)  m l will be either 0 or 1.

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

bayes rule

to proceed, we will need to use bayes rule

we can write the joint id203 of two rv in two ways,
using chain rule:

p(x, y ) = p(x)p(y |x) = p(y )p(x|y ).

from here we get the bayes rule:

p(x|y ) =

p(x)p(y |x)

p(y )

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

bayes rule and estimation

now consider    to be a rv. we have

p(  | x) =

p(x|   )p(  )

p(x)

bayes rule converts prior id203 p(  ) (our belief about   
prior to seeing any data) to posterior p(  |x), using the
likelihood p(x|  ).

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

map estimation

p(  | x) =

p(x|   )p(  )

p(x)

the maximum a-posteriori (map) estimate is de   ned as

(cid:98)  m ap = argmax

  

p(  |x)

note: p(x) does not depend on   , so if we only care about
   nding the map estimate, we can write

p(  |x)     p(x|  )p(  )

what   s p(  )?

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

choice of prior

bayesian approach: try to re   ect our belief about   
utilitarian approach: choose a prior which is computationally
convenient

    later in class: id173 - choose a prior that leads

to better prediction performance

one possibility: uniform p(  )     1 for all        [0, 1].
   uninformative    prior: map is the same as ml estimate

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

constrained optimization: a multinomial likelihood

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

problem: estimating biases in dice

a dice is rolled n times: a single roll produces one of
{1, 2, 3, 4, 5, 6}
let n1, n2, . . . n6 count the outcomes for each value
this is a multinomial distribution with parameters
  1,   2, . . . ,   6
the joint distribution for n1, n2, . . . , n6 is given by

p(n1, n2, . . . , n6; n,   1,   2, . . . ,   6) =

subject to(cid:80)

i   i = 1 and(cid:80)

i ni = n

(cid:32)

n!

n1!n2!n3!n4!n5!n6!

(cid:33) 6(cid:89)

i=1

  ni
i

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

the likelihood is

l(  1,   2, . . . ,   6) =

the log-likelihood is

(cid:32)

l(  1,   2, . . . ,   6) =

log

a false start

(cid:32)

(cid:33) 6(cid:89)

  ni
i

n!

n1!n2!n3!n4!n5!n6!

(cid:33)

+

n!

n1!n2!n3!n4!n5!n6!

i=1

6(cid:88)

i=1

ni log   i

optimize by taking derivative and setting to zero:

   l
     1

=

n1
  1

= 0

therefore:   1 =    
what went wrong?

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

a possible solution

we forgot that(cid:80)6

i=1   i = 1

we could use this constraint to eliminate one of the variables:

5(cid:88)

  i

i=1

(cid:80)5

n6

i=1   i

= 0

  6 = 1    

and then solve the equations

   l
     i

=

n1
  i    

1    

gets messy

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

a more elegant solution: lagrange

multipliers

general constrained optimization problem:

max

  

f (  ) subject to g(  )     c = 0

we can then de   ne the lagrangian

l(  ,   ) = f (  )       (g(  )     c)

is equal to f when the constraint is satis   ed
now do unconstrained optimization over    and   :
optimizing the lagrange multiplier    enforces constraint
more constraints, more multipliers

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

back to rolling dice

recall

(cid:32)

l(  1,   2, . . . ,   6) =

log

(cid:33)

6(cid:88)

i=1

+

ni log   i

n!

n1!n2!n3!n4!n5!n6!

the lagrangian may be de   ned as:

n!(cid:81)

6(cid:88)

i=1

l = log

+

i ni!

ni log   i       

(cid:16) 6(cid:88)

i=1

(cid:17)

  i     1

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

back to rolling dice

taking derivative with respect to   i and setting to 0

   l
     i

= 0

let optimal   i =      
i
ni
     
i       
6(cid:88)
6(cid:88)

i=1

   

  

=

   

= 0 =   

ni
      =   

   
i

6(cid:88)

i=1

ni
      =

   
i = 1

  

ni(cid:80)6

i=1 ni

   
i =

ni =      

i=1

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

multivariate gaussians

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

quick review: discrete/continuous random

variables

a random variable is a function x :     (cid:55)    r

the set of all possible values a random variable x can take is
called its range
discrete random variables can only take isolated values
(id203 of a random variable taking a particular value
reduces to counting)

discrete example: sum of two fair dice

continuous example: speed of a car

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

discrete distributions

assume x is a discrete random variable. we would like to
specify probabilities of events {x = x}
if we can specify the probabilities involving x, we can say
that we have speci   ed the id203 distribution of x
for a countable set of values x1, x2, . . . xn, we have

p(x = xi) > 0, i = 1, 2, . . . , n and(cid:80)

p(x = xi) = 1

i

we can then de   ne the id203 mass function f of x by
f (x) = p(x = x)
sometimes write as fx

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

id203 mass function

example: toss a die and let x be its face value. x is discrete
with range {1, 2, 3, 4, 5, 6}. the pmf is

another example: toss two dice and let x be the largest face
value. the pmf is

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

id203 density functions

the id203 density function of a continuous random
variable x satis   es

a random variable x taking values in set x is said to have a
continuous distribution if p(x = x) = 0 for all x     x
    (cid:82)    
    f (x)         x
    p(a     x     b) =(cid:82) b
p(a     x     b) =(cid:82) b
(cid:82)    

probabilities correspond to areas under the curve f (x)
reminder: no longer need to have

a f (x)dx     1 but must have

a f (x)dx     a, b

       f (x)dx = 1

       f (x)dx = 1

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

why gaussians?

gaussian distributions are widely used in machine learning:

    central limit theorem!

  xn = x1 + x2 +        + xn
   n   xn

(cid:0)x;   ,   2(cid:1)

d

       n

    actually, there are a set of    central limit theorems   

(e.g. corresponding to p-stable distributions)

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

why gaussians?

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

why gaussians?

gaussian distributions are widely used in machine learning:

    central limit theorem!
    gaussians are convenient computationally;
    mixtures of gaussians (just covered in class) are

su   cient to approximate a wide range of distributions;

    closely related to squared loss (have seen earlier in class),

an important error measure in statistics.

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

reminder: univariate gaussian distribution

n (x;   ,   2) =

1

(2    2)1/2 exp

(cid:26)

1
2  2 (x       )2

   

(cid:27)

mean    determines location
variance   2;
standard deviation      2
determines the spread
around   

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

n(x|  ,  2)x2    moments

(cid:90)    

reminder: expectation of a rv x is e [x] (cid:44)(cid:82) xp(x)dx, so
variance of x is var x (cid:44) e(cid:2)(x     e [x])2(cid:3), and

xn (x;   ,   2)dx =   

e [x] =

      

(cid:90)    

var x =

      

(x       )2n (x;   ,   2)dx =   2

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

multivariate gaussian

gaussian distribution of a random vector x in rd:

n (x;   ,   ) =

1

(2  )d/2|  |1/2 exp

1
2

   

(cid:18)

(cid:19)
(x       )t      1(x       )

1

(2  )d/2|  |1/2 factor

the
ensures it   s a pdf (integrates
to one).

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

matrix notation

n (x;   ,   ) =

1

(2  )d/2|  |1/2 exp

(cid:18)

   

1
2

(x       )t      1(x       )

(cid:19)

boldfaced lowercase vectors x, uppercase matrices   .
determinant |  |
matrix inverse      1
transpose xt ,   t

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

mean of the gaussian

by de   nition,

e [x] =

(cid:90)    

(cid:90)    

. . .

      

      

solving this we indeed get

xn (x;   ,   )dx1 . . . dxd

e [x] =   

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

covariance

variance of a rv x with mean   :   2
generalization to two variables: covariance

x = e(cid:2)(x       )2(cid:3)

covx1,x2

(cid:44) e [(x1       1)(x2       2)]

measures how the two variables deviate together from their
means (   co-vary   ).
note: covx,x     var(x) =   2

x

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

correlation vs. covariance

correlation:

cor(a, b) (cid:44) cova,b
  a  b

.

cor     1

   1 < cor < 0

cor     0

cor(a, b) measures the linear relationship between a and b.
   1     cor(a, b)     +1 ; +1 or    1 means a is a linear function
of b.

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

   1.5   1   0.500.511.5   1.5   1   0.500.511.5ab00.10.20.30.40.50.60.70.80.9100.20.40.60.811.21.4ab00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91abcovariance matrix

for a random vector x = [x1, . . . , xd]t with mean   ,

                  2

. . .

x1

covx2,x1

covx (cid:44)

covx1,x2

  2
x2

. . . . . . . covx1,xd
. . . . . . . covx2,xd

covxd,x1 covxd,x2

. . . . .

. . .

. . .
  2
xd

                .

square, symmetric, non-negative main diagonal   why?
variances     0, and cov(x, y) = cov(y, x) by de   nition
one can show (directly from de   nition):

covx = e(cid:2)(x       )(x       )t(cid:3)

i.e. expectation of the outer product of x     e [x] with itself.
note: so far nothing gaussian-speci   c!

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

covariance of the gaussian

we need to calculate e(cid:2)(x       )(x       )t(cid:3)
e(cid:2)xxt(cid:3) =     t +   

with a bit of algebra, we get

now, we already have e [x] =   , and

e(cid:2)(x       )(x       )t(cid:3) = e(cid:2)xxt       xt     x  t +     t(cid:3)
(cid:123)(cid:122)

   

(cid:8)  (e [x])t + e [x]   t         t(cid:9)
(cid:124)
(cid:125)
        t =   

=     t

= e(cid:2)xxt(cid:3)
= e(cid:2)xxt(cid:3)

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

properties of the covariance

consider the eigenvector equation:   u =   u
as a covariance matrix,    is symmetric d    d matrix.
therefore, we have d solutions {  i, ui}d
i=1 where the
eigenvalues   i are real, and the eigenvectors ui are
orthonormal, i.e., inner product

(cid:40)

ut

j ui =

0
1

if i (cid:54)= j,
if i = j.

(cid:88)
(cid:88)

i

1
  i

i

the covariance matrix    then may be written as:
   =

  iuiut
i

thus, the inverse covariance may be written as:
   1 =
  

uiut
i

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

continued..

the quadratic form (x       )t      1(x       ) becomes:

(cid:88)

i

y2
i
  i

i (x       )

where yi = ut
{yi} may be interpreted as a new coordinate system de   ned
by the orthonormal vectors ui that are shifted and rotated
with respect to the original coordinate system
stack the d transposed orthonormal eigenvectors of    into

      . then, y = u(x       ) de   nes rotation (and

       ut

u =

1

      ut

d

possibly re   ection) of x, shifted so that    becomes origin.

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

geometry of the gaussian

     i gives scaling along ui
example in 2d:

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

x1x2  1/21  1/22y1y2u1u2  geometry continued ...

the determinant of the covariance matrix may be written as
the product of its eigenvalues i.e. |  |
thus, in the yi coordinate system, the gaussian distribution
takes the form:

j   

1
2
j

1

(cid:89)

p(y) =

1

(2    j)

1
2

j

exp

   

y2
j
2  j

2 =(cid:81)
(cid:32)

(cid:33)

which is the product of d independent univariate gaussians
the eigenvectors thus de   ne a new set of shifted and rotated
coordinates w.r.t which the joint id203 distribution
factorizes into a product of independent distributions

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

density contours

what are the constant density
contours?

1

(2  )d/2|  |1/2 exp

1
2

   

(cid:18)

(cid:19)
(x       )t      1(x       )
(x       )t      1(x       ) = const

= const

this is a quadratic form, whose solution is an ellipsoid (in 2d,
simply an ellipse)

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

x1x2  1/21  1/22y1y2u1u2  density contours are ellipsoids

we saw that: (x       )t      1(x       ) = const2

(cid:88)

recall that   

   1 =

thus we have:

i

uiut
i

1
  i

(cid:88)

i

where yi = ut

i (x       )

y2
i
  i

= const2

recall the expression for an ellipse in 2d:

(cid:16) x

(cid:17)2

a

(cid:16) y

(cid:17)2

b

+

= 1

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

intuition so far

(cid:18)

(cid:19)

1
2

(x       )t      1(x       )

   

n (x;   ,   ) =

1

(2  )d/2|  |1/2 exp

falls o    exponentially as a
function of (squared)
euclidean distance to the
mean (cid:107)x       (cid:107)2;
the covariance matrix   
determines the shape of the
density;

determinant |  | measures the    spread    (analogous to   2).
n is the joint density of coordinates x1, . . . , xd.

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

   4   3   2   101234   4   3   2   1012340.05 max0.5 max0.9 maxlinear functions of a gaussian rv

for any rv x, and for any a and b,

e [ax + b] = ae [x]+b,

cov(ax+b) = a cov(x)at .

(cid:0)z; a   + b, a  at(cid:1) .

let x     n (  ;   ,   ); then p(z) = n
consider a row vector at that    selects    a single component
from x, i.e., ak = 1 and aj = 0 if j (cid:54)= k. then, z = at x is
simply the coordinate xk.
we have: e [z] = at    =   k, and cov(z) = var(z) =   k,k.
i.e., marginal of a gaussian is also a gaussian

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

conditional and marginal

marginal (   projection    of the gaussian on a subset of
coordinates) is gaussian

conditional (   slice    through gaussian at    xed values for a
subset of coordinates) is gaussian

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

xaxb=0.7xbp(xa,xb)00.5100.51xap(xa)p(xa|xb=0.7)00.510510log-likelihood

n (x;   ,   ) =

1

(2  )d/2|  |1/2 exp

(cid:18)

   

1
2

(x       )t      1(x       )

(cid:19)

take the log, for a single example x:

log n (x;   ,   ) =    

d
2

log 2      

1
2

log |  |   

1
2

(x     )t      1(x     )

can ignore terms independent of parameters:

log n (x;   ,   ) =    

1
2

log |  |   

1
2

(x     )t      1(x     ) + const

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

log-likelihood (contd)

log n (x;   ,   ) =    

1
2

log |  |    

1
2

(x       )t      1(x       ) + const

given a set x of n i.i.d. vectors, we have

log n (x;   ,   ) =    

n
2

log |  |   

1
2

n(cid:88)
(xi     )t      1(xi     ) + const

i=1

we are now ready to compute ml estimates for    and   .

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

ml for parameters

log n (x;   ,   ) =    

n
2

log |  |   

1
2

n(cid:88)

i=1

(xi     )t      1(xi     ) + const

to    nd ml estimate, we use the rule

   
   a

at b =

   
   a

bt a = b,

and set derivative w.r.t.    to zero:

n(cid:88)

i=1

   
     

log n (x;   ,   ) =

     1(xi       ) = 0,

which yields (cid:98)  m l = 1

n

(cid:80)n

i=1 xi.

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

ml for parameters (contd)

a somewhat lengthier derivation produces ml estimate for

n(cid:88)
the covariance:(cid:98)  m l =
(xi       )(xi       )t .
note: the    above is the ml estimate (cid:98)  m l.

1
n

i=1

thus ml estimates for the mean is the sample mean of the
data, and ml estimate for the covariance is the sample
covariance of the data.

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

mixture models and expected log likelihood

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

mixture models

assumptions:

    k underlying types (clusters/components)
    yi is the identity of the component    responsible    for xi
    yi is a hidden (latent) variable: never observed

a mixture model:

p(x;   ) =

k(cid:88)

c=1

p(y = c)p(x|y = c)

  c are called mixing probabilities
the component densities p(x|y = c) needs to be
parameterized

next few slides adapted from ttic 31020 by gregory shakhnarovich

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

parametric mixtures

k(cid:88)

suppose the parameters of the c-th component are   c. then
we can denote    = [  1, . . . ,   k] and write

p(x;   ,   ) =

  cp(x,   c)

any valid setting of    and   , such that(cid:80)k

c=1

a valid pdf

example: mixture of gaussians

c=1   c = 1 produces

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

generative model for a mixture
the generative process with a k-component mixture:
    the parameters   c for each component are    xed
    draw yi     [  1, . . . ,   k]
    given yi, draw xi     p(x|yi;   yi)

the entire generative model for x and y

p(x, y;   ,   ) = p(y;   )p(x|y;   y)

what does this mean? any data point xi could have been
generated in k ways
if the c-th component is gaussian i.e.
p(x|y = c) = n (x;   c,   c)

k(cid:88)

c=1

p(x;   ,   ) =

  cn (x;   c,   c)

where    = [  1, . . . ,   k,   1, . . . ,   k]

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

likelihood of a mixture model

usual idea: estimate set of parameters that maximize
likelihood given observed data
the log-likelihood of   ,    for x = {x1, . . . , xn}:

log p(x;   ,   ) =

log

  cn (xi;   c,   c)

n(cid:88)

k(cid:88)

i=1

c=1

no closed form solution because of sum inside log

how will we estimate parameters?

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

scenario 1: known labels. mixture density

estimation

suppose that we do observe yi     {1, . . . , k} for each
i = 1, . . . , n
let us introduce a set of binary indicator variables
zi = [zi1, . . . , zik], where:

(cid:40)

1
0

zic =

if yi = c
otherwise

the count of examples from c-th component

n(cid:88)

nc =

zic

i=1

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

scenario 1: known labels. mixture density

estimation

if we know zi, the ml estimates of the gaussian components
are simply (as we have seen earlier)

    c =

1
nc

nc
n

n(cid:88)

i=1

zicxi,

zic(xi         c)(xi         c

t

    c =

n(cid:88)

i=1

    c =

1
nc

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

scenario 2: credit assignment

when we don   t know y, we face a credit assignment problem:
which component is responsible for xi?
suppose for a moment that we do know the component
parameters    = [  1, . . . ,   k,   1, . . . ,   k] and mixing
probabilities    = [  1, . . . ,   k]
then, we can compute the posterior of each label using
bayes    theorem:

  ic =   p(y = c|x;   ,   ) =

(cid:80)k

  cp(x;   c,   c)
l=1   lp(x;   l,   l)

we call   ic the responsibility of the c-th component for x

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

expected likelihood

the    complete data    likelihood (when z are known):

p(x, z;   ,   ) =   

(  cn (xi;   c,   c))zic

and the log

p(x, z;   ,   ) = const +

zic(log   c +log n (xi;   c,   c))

k(cid:89)
n(cid:89)
k(cid:88)
n(cid:88)

i=1

c=1

i=1

c=1

we can   t compute it (why?), but can take the expectation
w.r.t the posterior of z, which is just   ic i.e. e[zic] =   ic
the expected likelihood of the data:

e[log p(x, z;   ,   )] = const +

  ic(log   c+log n (xi;   c,   c))

n(cid:88)

k(cid:88)

i=1

c=1

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

expectation maximization

the expected likelihood of the data:

n(cid:88)

k(cid:88)

i=1

c=1

e[log p(x, z;   ,   )] = const +

  ic(log   c+log n (xi;   c,   c))

we can    nd   ,    that maximizes this expected likelihood - by
setting derivatives to zero and for   , using lagrange

multipliers to enforce(cid:80)

c   c = 1

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

expectation maximization

if we know the parameters and indicators (assignments) we
are done

if we know the indicators but not the parameters, we can do
ml estimation of the parameters - and we are done

if we know the parameters but not the indicators, we can
compute the posteriors of the indicators. with known
posteriors, we can estimate parameters that maximize the
expected likelihood - and then we are done

in reality, we know neither the parameters nor the indicators

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

expectation maximization for mixture models

general mixture models: p(x) =(cid:80)k

c=1   cp(x;   c)

initialize   ,   old, and iterate until convergence:

    e-step: compute responsibilities:

  ic =

  old
c p(xi;   old
c )
l=1   old

l p(xi;   old)

    m-step: re-estimate mixture parameters:

(cid:80)k
n(cid:88)

k(cid:88)

i=1

c=1

  old,   new = arg max
  ,  

  ic(log   c + log p(xi;   c))

tutorial on estimation and multivariate gaussians

stat 27725/cmsc 25400

