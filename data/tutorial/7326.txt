foundations of data science   

avrim blum, john hopcroft, and ravindran kannan

thursday 4th january, 2018

   copyright 2015. all rights reserved

1

contents

1 introduction

9

2 high-dimensional space

2.4.1 volume of the unit ball
2.4.2 volume near the equator

12
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.1
. . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.2 the law of large numbers
2.3 the geometry of high dimensions
. . . . . . . . . . . . . . . . . . . . . . 15
2.4 properties of the unit ball . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . . . . . 19
2.5 generating points uniformly at random from a ball
. . . . . . . . . . . . 22
2.6 gaussians in high dimension . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.7 random projection and johnson-lindenstrauss lemma . . . . . . . . . . . 25
2.8 separating gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.9 fitting a spherical gaussian to data . . . . . . . . . . . . . . . . . . . . . 29
2.10 bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.11 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

3 best-fit subspaces and singular value decomposition (svd)

40
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.1
3.2 preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.3 singular vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.4 singular value decomposition (svd) . . . . . . . . . . . . . . . . . . . . . 45
3.5 best rank-k approximations
. . . . . . . . . . . . . . . . . . . . . . . . . 47
3.6 left singular vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.7 power method for singular value decomposition . . . . . . . . . . . . . . . 51
3.7.1 a faster method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.8 singular vectors and eigenvectors . . . . . . . . . . . . . . . . . . . . . . . 54
3.9 applications of singular value decomposition . . . . . . . . . . . . . . . . 54
3.9.1 centering data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
3.9.2 principal component analysis . . . . . . . . . . . . . . . . . . . . . 56
3.9.3 id91 a mixture of spherical gaussians . . . . . . . . . . . . . 56
3.9.4 ranking documents and web pages
. . . . . . . . . . . . . . . . . 62
3.9.5 an application of svd to a discrete optimization problem . . . . 63
3.10 bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.11 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

4 id93 and markov chains

76
4.1 stationary distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.2 id115 . . . . . . . . . . . . . . . . . . . . . . . . . . 81
4.2.1 metropolis-hasting algorithm . . . . . . . . . . . . . . . . . . . . . 83
4.2.2 id150 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.3 areas and volumes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86

2

4.4 convergence of id93 on undirected graphs . . . . . . . . . . . . 88
4.4.1 using normalized conductance to prove convergence . . . . . . . . 94
4.5 electrical networks and id93 . . . . . . . . . . . . . . . . . . . . 97
4.6 id93 on undirected graphs with unit edge weights . . . . . . . 102
4.7 id93 in euclidean space . . . . . . . . . . . . . . . . . . . . . . 109
4.8 the web as a markov chain . . . . . . . . . . . . . . . . . . . . . . . . . . 112
4.9 bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
4.10 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

5 machine learning

129
5.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
5.2 the id88 algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.3 id81s
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
5.4 generalizing to new data . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
5.5 over   tting and uniform convergence . . . . . . . . . . . . . . . . . . . . . 135
5.6
illustrative examples and occam   s razor . . . . . . . . . . . . . . . . . . . 138
5.6.1 learning disjunctions
. . . . . . . . . . . . . . . . . . . . . . . . . 138
5.6.2 occam   s razor
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
5.6.3 application: learning id90 . . . . . . . . . . . . . . . . . 140
5.7 id173: penalizing complexity . . . . . . . . . . . . . . . . . . . . 141
5.8 online learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
5.8.1 an example: learning disjunctions . . . . . . . . . . . . . . . . . . 142
5.8.2 the halving algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 143
5.8.3 the id88 algorithm . . . . . . . . . . . . . . . . . . . . . . . 143
5.8.4 extensions: inseparable data and hinge loss
. . . . . . . . . . . . 145
5.9 online to batch conversion . . . . . . . . . . . . . . . . . . . . . . . . . . 146
5.10 support-vector machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.11 vc-dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
5.11.1 de   nitions and key theorems . . . . . . . . . . . . . . . . . . . . . 149
5.11.2 examples: vc-dimension and growth function . . . . . . . . . . . 151
5.11.3 proof of main theorems . . . . . . . . . . . . . . . . . . . . . . . . 153
5.11.4 vc-dimension of combinations of concepts . . . . . . . . . . . . . 156
5.11.5 other measures of complexity . . . . . . . . . . . . . . . . . . . . . 156
5.12 strong and weak learning - boosting . . . . . . . . . . . . . . . . . . . . . 157
5.13 stochastic id119 . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.14 combining (sleeping) expert advice . . . . . . . . . . . . . . . . . . . . . 162
5.15 deep learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
5.15.1 id3 (gans) . . . . . . . . . . . . . . . 170
5.16 further current directions . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
5.16.1 semi-supervised learning . . . . . . . . . . . . . . . . . . . . . . . 171
5.16.2 active learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
5.16.3 id72 . . . . . . . . . . . . . . . . . . . . . . . . . . 174
5.17 bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175

3

5.18 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176

6 algorithms for massive data problems: streaming, sketching, and

181
sampling
6.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
6.2 frequency moments of data streams . . . . . . . . . . . . . . . . . . . . . 182
6.2.1 number of distinct elements in a data stream . . . . . . . . . . . 183
6.2.2 number of occurrences of a given element.
. . . . . . . . . . . . . 186
. . . . . . . . . . . . . . . . . . . . . . . . . . . 187
6.2.3 frequent elements
6.2.4 the second moment . . . . . . . . . . . . . . . . . . . . . . . . . . 189
6.3 matrix algorithms using sampling . . . . . . . . . . . . . . . . . . . . . . 192
6.3.1 id127 using sampling . . . . . . . . . . . . . . . . . 193
6.3.2
implementing length squared sampling in two passes . . . . . . . 197
sketch of a large matrix . . . . . . . . . . . . . . . . . . . . . . . . 197
6.3.3
6.4 sketches of documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
6.5 bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
6.6 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204

7 id91

7.1

structural properties of the id116 objective

208
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
7.1.1 preliminaries
7.1.2 two general assumptions on the form of clusters
. . . . . . . . . 209
spectral id91 . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
7.1.3
7.2 id116 id91 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
7.2.1 a maximum-likelihood motivation . . . . . . . . . . . . . . . . . . 211
7.2.2
. . . . . . . . . . . 212
7.2.3 lloyd   s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
7.2.4 ward   s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
7.2.5
id116 id91 on the line . . . . . . . . . . . . . . . . . . . . 215
7.3 k-center id91 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
7.4 finding low-error id91s . . . . . . . . . . . . . . . . . . . . . . . . . 216
7.5 spectral id91 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
7.5.1 why project? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
7.5.2 the algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
7.5.3 means separated by    (1) standard deviations . . . . . . . . . . . . 219
7.5.4 laplacians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
7.5.5 local spectral id91 . . . . . . . . . . . . . . . . . . . . . . . . 221
7.6 approximation stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
7.6.1 the conceptual idea . . . . . . . . . . . . . . . . . . . . . . . . . . 224
7.6.2 making this formal . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
7.6.3 algorithm and analysis
. . . . . . . . . . . . . . . . . . . . . . . . 225
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
single linkage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227

7.7 high-density clusters

7.7.1

4

7.7.2 robust linkage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
7.8 kernel methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
7.9 recursive id91 based on sparse cuts . . . . . . . . . . . . . . . . . . 229
7.10 dense submatrices and communities . . . . . . . . . . . . . . . . . . . . . 230
7.11 community finding and graph partitioning . . . . . . . . . . . . . . . . . 233
7.12 spectral id91 applied to social networks . . . . . . . . . . . . . . . . . 236
7.13 bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
7.14 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240

8 random graphs

8.1 the g(n, p) model

245
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
8.1.1 degree distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
8.1.2 existence of triangles in g(n, d/n)
. . . . . . . . . . . . . . . . . . 250
8.2 phase transitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
8.3 giant component . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
8.3.1 existence of a giant component . . . . . . . . . . . . . . . . . . . . 261
8.3.2 no other large components . . . . . . . . . . . . . . . . . . . . . . . 263
8.3.3 the case of p < 1/n . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
8.4 cycles and full connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . 265
8.4.1 emergence of cycles
. . . . . . . . . . . . . . . . . . . . . . . . . . 265
8.4.2 full connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
8.4.3 threshold for o(ln n) diameter . . . . . . . . . . . . . . . . . . . . 268
8.5 phase transitions for increasing properties . . . . . . . . . . . . . . . . . . 270
8.6 branching processes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
8.7 cnf-sat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
8.7.1
sat-solvers in practice . . . . . . . . . . . . . . . . . . . . . . . . . 278
8.7.2 phase transitions for cnf-sat . . . . . . . . . . . . . . . . . . . . 279
8.8 nonuniform models of random graphs . . . . . . . . . . . . . . . . . . . . 284
8.8.1 giant component in graphs with given degree distribution . . . . 285
8.9 growth models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
8.9.1 growth model without preferential attachment . . . . . . . . . . . 287
8.9.2 growth model with preferential attachment
. . . . . . . . . . . . 293
8.10 small world graphs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
8.11 bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
8.12 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301

9 topic models, nonnegative id105, hidden markov mod-

310
els, and id114
9.1 topic models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
9.2 an idealized model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
9.3 nonnegative id105 - nmf . . . . . . . . . . . . . . . . . . . 315
9.4 nmf with anchor terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
9.5 hard and soft id91 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318

5

9.6 the id44 model for id96 . . . . . . . . . 320
9.7 the dominant admixture model
. . . . . . . . . . . . . . . . . . . . . . . 322
9.8 formal assumptions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
9.9 finding the term-topic matrix . . . . . . . . . . . . . . . . . . . . . . . . 327
9.10 id48 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
9.11 id114 and belief propagation . . . . . . . . . . . . . . . . . . . 337
9.12 bayesian or belief networks . . . . . . . . . . . . . . . . . . . . . . . . . . 338
9.13 markov random fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
9.14 factor graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
9.15 tree algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
9.16 message passing in general graphs . . . . . . . . . . . . . . . . . . . . . . 342
9.17 graphs with a single cycle
. . . . . . . . . . . . . . . . . . . . . . . . . . 344
9.18 belief update in networks with a single loop . . . . . . . . . . . . . . . . 346
9.19 maximum weight matching . . . . . . . . . . . . . . . . . . . . . . . . . . 347
9.20 warning propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
9.21 correlation between variables . . . . . . . . . . . . . . . . . . . . . . . . . 351
9.22 bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
9.23 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357

10 other topics

10.2 compressed sensing and sparse vectors

360
10.1 ranking and social choice . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
10.1.1 randomization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
10.1.2 examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
. . . . . . . . . . . . . . . . . . . 364
10.2.1 unique reconstruction of a sparse vector
. . . . . . . . . . . . . . 365
10.2.2 e   ciently finding the unique sparse solution . . . . . . . . . . . . 366
10.3 applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
10.3.1 biological
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
10.3.2 low rank matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
10.4 an uncertainty principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
. . . . . . . . . . . . . . . 370

10.4.1 sparse vector in some coordinate basis
10.4.2 a representation cannot be sparse in both time and frequency

domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
10.5 gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
10.6 id135 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
10.6.1 the ellipsoid algorithm . . . . . . . . . . . . . . . . . . . . . . . . 375
10.7 integer optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
10.8 semi-de   nite programming . . . . . . . . . . . . . . . . . . . . . . . . . . 378
10.9 bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
10.10exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381

6

11 wavelets

385
11.1 dilation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
11.2 the haar wavelet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
11.3 wavelet systems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
11.4 solving the dilation equation . . . . . . . . . . . . . . . . . . . . . . . . . 390
11.5 conditions on the dilation equation . . . . . . . . . . . . . . . . . . . . . 392
11.6 derivation of the wavelets from the scaling function . . . . . . . . . . . . 394
11.7 su   cient conditions for the wavelets to be orthogonal . . . . . . . . . . . 398
11.8 expressing a function in terms of wavelets
. . . . . . . . . . . . . . . . . 401
11.9 designing a wavelet system . . . . . . . . . . . . . . . . . . . . . . . . . . 402
11.10applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
11.11 bibliographic notes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
11.12 exercises

12 appendix

406
12.1 de   nitions and notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
12.2 asymptotic notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
12.3 useful relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
12.4 useful inequalities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
12.5 id203 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
12.5.1 sample space, events, and independence . . . . . . . . . . . . . . . 420
12.5.2 linearity of expectation . . . . . . . . . . . . . . . . . . . . . . . . 421
12.5.3 union bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
12.5.4 indicator variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
12.5.5 variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
12.5.6 variance of the sum of independent random variables . . . . . . . 423
12.5.7 median . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
12.5.8 the central limit theorem . . . . . . . . . . . . . . . . . . . . . . 423
12.5.9 id203 distributions . . . . . . . . . . . . . . . . . . . . . . . . 424
12.5.10 bayes rule and estimators . . . . . . . . . . . . . . . . . . . . . . . 428
12.6 bounds on tail id203 . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
12.6.1 cherno    bounds
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
12.6.2 more general tail bounds . . . . . . . . . . . . . . . . . . . . . . . 433
12.7 applications of the tail bound . . . . . . . . . . . . . . . . . . . . . . . . 436
. . . . . . . . . . . . . . . . . . . . . . . . . 437
12.8 eigenvalues and eigenvectors
12.8.1 symmetric matrices
. . . . . . . . . . . . . . . . . . . . . . . . . . 439
12.8.2 relationship between svd and eigen decomposition . . . . . . . . 441
12.8.3 extremal properties of eigenvalues . . . . . . . . . . . . . . . . . . 441
12.8.4 eigenvalues of the sum of two symmetric matrices . . . . . . . . . 443
12.8.5 norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
12.8.6 important norms and their properties . . . . . . . . . . . . . . . . 446
12.8.7 additional id202 . . . . . . . . . . . . . . . . . . . . . . . 448
12.8.8 distance between subspaces . . . . . . . . . . . . . . . . . . . . . . 450

7

12.8.9 positive semide   nite matrix . . . . . . . . . . . . . . . . . . . . . . 451
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451

12.9 generating functions

12.9.1 generating functions for sequences de   ned by recurrence rela-

tionships . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452

12.9.2 the exponential generating function and the moment generating

function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
12.10miscellaneous . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
12.10.1 lagrange multipliers
. . . . . . . . . . . . . . . . . . . . . . . . . . 456
12.10.2 finite fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
12.10.3 application of mean value theorem . . . . . . . . . . . . . . . . . 457
12.10.4 sperner   s lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
12.10.5 pr  ufer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
12.11exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460

index

466

8

1

introduction

computer science as an academic discipline began in the 1960   s. emphasis was on
programming languages, compilers, operating systems, and the mathematical theory that
supported these areas. courses in theoretical computer science covered    nite automata,
id157, context-free languages, and computability. in the 1970   s, the study
of algorithms was added as an important component of theory. the emphasis was on
making computers useful. today, a fundamental change is taking place and the focus is
more on a wealth of applications. there are many reasons for this change. the merging
of computing and communications has played an important role. the enhanced ability
to observe, collect, and store data in the natural sciences, in commerce, and in other
   elds calls for a change in our understanding of data and how to handle it in the modern
setting. the emergence of the web and social networks as central aspects of daily life
presents both opportunities and challenges for theory.

while traditional areas of computer science remain highly important, increasingly re-
searchers of the future will be involved with using computers to understand and extract
usable information from massive data arising in applications, not just how to make com-
puters useful on speci   c well-de   ned problems. with this in mind we have written this
book to cover the theory we expect to be useful in the next 40 years, just as an under-
standing of automata theory, algorithms, and related topics gave students an advantage
in the last 40 years. one of the major changes is an increase in emphasis on id203,
statistics, and numerical methods.

early drafts of the book have been used for both undergraduate and graduate courses.
background material needed for an undergraduate course has been put in the appendix.
for this reason, the appendix has homework problems.

modern data in diverse    elds such as information processing, search, and machine
learning is often advantageously represented as vectors with a large number of compo-
nents. the vector representation is not just a book-keeping device to store many    elds
of a record. indeed, the two salient aspects of vectors: geometric (length, dot products,
orthogonality etc.) and id202ic (independence, rank, singular values etc.) turn
out to be relevant and useful. chapters 2 and 3 lay the foundations of geometry and
id202 respectively. more speci   cally, our intuition from two or three dimensional
space can be surprisingly o    the mark when it comes to high dimensions. chapter 2
works out the fundamentals needed to understand the di   erences. the emphasis of the
chapter, as well as the book in general, is to get across the intellectual ideas and the
mathematical foundations rather than focus on particular applications, some of which are
brie   y described. chapter 3 focuses on singular value decomposition (svd) a central tool
to deal with matrix data. we give a from-   rst-principles description of the mathematics
and algorithms for svd. applications of singular value decomposition include principal
component analysis, a widely used technique which we touch upon, as well as modern

9

applications to statistical mixtures of id203 densities, discrete optimization, etc.,
which are described in more detail.

exploring large structures like the web or the space of con   gurations of a large system
with deterministic methods can be prohibitively expensive. id93 (also called
markov chains) turn out often to be more e   cient as well as illuminative. the station-
ary distributions of such walks are important for applications ranging from web search to
the simulation of physical systems. the underlying mathematical theory of such random
walks, as well as connections to electrical networks, forms the core of chapter 4 on markov
chains.

one of the surprises of computer science over the last two decades is that some domain-
independent methods have been immensely successful in tackling problems from diverse
areas. machine learning is a striking example. chapter 5 describes the foundations
of machine learning, both algorithms for optimizing over given training examples, as
well as the theory for understanding when such optimization can be expected to lead to
good performance on new, unseen data. this includes important measures such as the
vapnik-chervonenkis dimension, important algorithms such as the id88 algorithm,
stochastic id119, boosting, and deep learning, and important notions such as
id173 and over   tting.

the    eld of algorithms has traditionally assumed that the input data to a problem is
presented in random access memory, which the algorithm can repeatedly access. this is
not feasible for problems involving enormous amounts of data. the streaming model and
other models have been formulated to re   ect this. in this setting, sampling plays a crucial
role and, indeed, we have to sample on the    y. in chapter 6 we study how to draw good
samples e   ciently and how to estimate statistical and id202 quantities, with such
samples.

while chapter 5 focuses on supervised learning, where one learns from labeled training
data, the problem of unsupervised learning, or learning from unlabeled data, is equally
important. a central topic in unsupervised learning is id91, discussed in chapter
7. id91 refers to the problem of partitioning data into groups of similar objects.
after describing some of the basic methods for id91, such as the id116 algorithm,
chapter 7 focuses on modern developments in understanding these, as well as newer al-
gorithms and general frameworks for analyzing di   erent kinds of id91 problems.

central to our understanding of large structures, like the web and social networks, is
building models to capture essential properties of these structures. the simplest model
is that of a random graph formulated by erd  os and renyi, which we study in detail in
chapter 8, proving that certain global phenomena, like a giant connected component,
arise in such structures with only local choices. we also describe other models of random
graphs.

10

chapter 9 focuses on linear-algebraic problems of making sense from data, in par-
ticular id96 and non-negative id105. in addition to discussing
well-known models, we also describe some current research on models and algorithms with
provable guarantees on learning error and time. this is followed by id114 and
belief propagation.

chapter 10 discusses ranking and social choice as well as problems of sparse represen-
tations such as compressed sensing. additionally, chapter 10 includes a brief discussion
of id135 and semide   nite programming. wavelets, which are an impor-
tant method for representing signals across a wide range of applications, are discussed in
chapter 11 along with some of their fundamental mathematical properties. the appendix
includes a range of background material.

a word about notation in the book. to help the student, we have adopted certain
notations, and with a few exceptions, adhered to them. we use lower case letters for
scalar variables and functions, bold face lower case for vectors, and upper case letters
for matrices. lower case near the beginning of the alphabet tend to be constants, in the
middle of the alphabet, such as i, j, and k, are indices in summations, n and m for integer
sizes, and x, y and z for variables. if a is a matrix its elements are aij and its rows are ai.
if ai is a vector its coordinates are aij. where the literature traditionally uses a symbol
for a quantity, we also used that symbol, even if it meant abandoning our convention. if
we have a set of points in some vector space, and work with a subspace, we use n for the
number of points, d for the dimension of the space, and k for the dimension of the subspace.

the term    almost surely    means with id203 tending to one. we use ln n for the
natural logarithm and log n for the base two logarithm. if we want base ten, we will use

log10 . to simplify notation and to make it easier to read we use e2(1    x) for(cid:0)e(1     x)(cid:1)2
and e(1     x)2 for e(cid:0)(1     x)2(cid:1) . when we say    randomly select    some number of points

from a given id203 distribution, independence is always assumed unless otherwise
stated.

11

2 high-dimensional space

2.1 introduction

high dimensional data has become very important. however, high dimensional space
is very di   erent from the two and three dimensional spaces we are familiar with. generate
n points at random in d-dimensions where each coordinate is a zero mean, unit variance
gaussian. for su   ciently large d, with high id203 the distances between all pairs
of points will be essentially the same. also the volume of the unit ball in d-dimensions,
the set of all points x such that |x|     1, goes to zero as the dimension goes to in   nity.
the volume of a high dimensional unit ball is concentrated near its surface and is also
concentrated at its equator. these properties have important consequences which we will
consider.

2.2 the law of large numbers

if one generates random points in d-dimensional space using a gaussian to generate
coordinates, the distance between all pairs of points will be essentially the same when d
is large. the reason is that the square of the distance between two points y and z,

d(cid:88)

i=1

|y     z|2 =

(yi     zi)2,

can be viewed as the sum of d independent samples of a random variable x that is dis-
tributed as the squared di   erence of two gaussians. in particular, we are summing inde-
pendent samples xi = (yi     zi)2 of a random variable x of bounded variance. in such a
case, a general bound known as the law of large numbers states that with high proba-
bility, the average of the samples will be close to the expectation of the random variable.
this in turn implies that with high id203, the sum is close to the sum   s expectation.

speci   cally, the law of large numbers states that
    e(x)

prob

(cid:18)(cid:12)(cid:12)(cid:12)(cid:12)x1 + x2 +        + xn

n

(cid:12)(cid:12)(cid:12)(cid:12)      
(cid:19)

    v ar(x)

n 2

.

(2.1)

the larger the variance of the random variable, the greater the id203 that the error
will exceed  . thus the variance of x is in the numerator. the number of samples n is in
the denominator since the more values that are averaged, the smaller the id203 that
the di   erence will exceed  . similarly the larger   is, the smaller the id203 that the
di   erence will exceed   and hence   is in the denominator. notice that squaring   makes
the fraction a dimensionless quantity.

we use two inequalities to prove the law of large numbers. the    rst is markov   s
inequality that states that the id203 that a nonnegative random variable exceeds a
is bounded by the expected value of the variable divided by a.

12

theorem 2.1 (markov   s inequality) let x be a nonnegative random variable. then
for a > 0,

prob(x     a)     e(x)
a

.

proof: for a continuous nonnegative random variable x with id203 density p,

   (cid:90)

a(cid:90)
   (cid:90)

0

e (x) =

   

xp(x)dx =

xp(x)dx +

xp(x)dx

xp(x)dx     a

a

p(x)dx = aprob(x     a).

   (cid:90)
   (cid:90)

0

thus, prob(x     a)     e(x)
a .

a

a

the same proof works for discrete random variables with sums instead of integrals.

corollary 2.2 prob(cid:0)x     be(x)(cid:1)     1

b

markov   s inequality bounds the tail of a distribution using only information about the
mean. a tighter bound can be obtained by also using the variance of the random variable.

theorem 2.3 (chebyshev   s inequality) let x be a random variable. then for c > 0,

proof: prob(cid:0)|x     e(x)|     c(cid:1) = prob(cid:0)|x     e(x)|2     c2(cid:1). let y = |x     e(x)|2. note that

prob

c2

.

y is a nonnegative random variable and e(y) = v ar(x), so markov   s inequality can be
applied giving:

prob(|x     e(x)|     c) = prob(cid:0)|x     e(x)|2     c2(cid:1)     e(|x     e(x)|2)

=

v ar(x)

c2

.

c2

(cid:16)|x     e(x)|     c

(cid:17)     v ar(x)

the law of large numbers follows from chebyshev   s inequality together with facts

about independent random variables. recall that:

e(x + y) = e(x) + e(y),
v ar(x     c) = v ar(x),
v ar(cx) = c2v ar(x).

13

also, if x and y are independent, then e(xy) = e(x)e(y). these facts imply that if x
and y are independent then v ar(x + y) = v ar(x) + v ar(y), which is seen as follows:

v ar(x + y) = e(x + y)2     e2(x + y)

= e(x2 + 2xy + y2)    (cid:0)e2(x) + 2e(x)e(y) + e2(y)(cid:1)

= e(x2)     e2(x) + e(y2)     e2(y) = v ar(x) + v ar(y),

where we used independence to replace e(2xy) with 2e(x)e(y).

theorem 2.4 (law of large numbers) let x1, x2, . . . , xn be n independent samples
of a random variable x. then

prob

(cid:18)(cid:12)(cid:12)(cid:12)(cid:12) x1 + x2 +        + xn
(cid:12)(cid:12)(cid:12)(cid:12)      
(cid:19)

    e(x)

    e(x)

(cid:12)(cid:12)(cid:12)(cid:12)      
(cid:19)
    v ar(cid:0) x1+x2+      +xn

 2

n

n

(cid:1)

proof: by chebychev   s inequality

(cid:18)(cid:12)(cid:12)(cid:12)(cid:12) x1 + x2 +        + xn

n

prob

    v ar(x)

n 2

=

=

=

1

(cid:0)v ar(x1) + v ar(x2) +        + v ar(xn)(cid:1)
n2 2 v ar(x1 + x2 +        + xn)

1

n2 2
v ar(x)

.

n 2

the law of large numbers is quite general, applying to any random variable x of
   nite variance. later we will look at tighter concentration bounds for spherical gaussians
and sums of 0-1 valued random variables.

one observation worth making about the law of large numbers is that the size of the
universe does not enter into the bound. for instance, if you want to know what fraction
of the population of a country prefers tea to co   ee, then the number n of people you need
to sample in order to have at most a    chance that your estimate is o    by more than  
depends only on   and    and not on the population of the country.

as an application of the law of large numbers, let z be a d-dimensional random point
whose coordinates are each selected from a zero mean, 1
2   variance gaussian. we set the
variance to 1
2   so the gaussian id203 density equals one at the origin and is bounded
below throughout the unit ball by a constant.1 by the law of large numbers, the square
of the distance of z to the origin will be   (d) with high id203. in particular, there is

1if we instead used variance 1, then the density at the origin would be a decreasing function of d,

namely ( 1

2   )d/2, making this argument more complicated.

14

vanishingly small id203 that such a random point z would lie in the unit ball. this
implies that the integral of the id203 density over the unit ball must be vanishingly
small. on the other hand, the id203 density in the unit ball is bounded below by a
constant. we thus conclude that the unit ball must have vanishingly small volume.

similarly if we draw two points y and z from a d-dimensional gaussian with unit

variance in each direction, then |y|2     d and |z|2     d. since for all i,

e(yi     zi)2 = e(y2

i ) + e(z2

i )     2e(yizi) = v ar(yi) + v ar(zi) + 2e(yi)e(zi) = 2,

i=1

(yi   zi)2     2d. thus by the pythagorean theorem, the random d-dimensional
|y   z|2 =
y and z must be approximately orthogonal. this implies that if we scale these random
points to be unit length and call y the north pole, much of the surface area of the unit ball
must lie near the equator. we will formalize these and related arguments in subsequent
sections.

d(cid:80)

we now state a general theorem on id203 tail bounds for a sum of indepen-
dent random variables. tail bounds for sums of bernoulli, squared gaussian and power
law distributed random variables can all be derived from this. the table in figure 2.1
summarizes some of the results.
theorem 2.5 (master tail bounds theorem) let x = x1 + x2 +        + xn, where
most   2. let 0     a        
x1, x2, . . . , xn are mutually independent random variables with zero mean and variance at
i )|       2s! for s = 3, 4, . . . ,(cid:98)(a2/4n  2)(cid:99).

2n  2. assume that |e(xs

then,

prob (|x|     a)     3e   a2/(12n  2).

the proof of theorem 2.5 is elementary. a slightly more general version, theorem 12.5,
is given in the appendix. for a brief intuition of the proof, consider applying markov   s
inequality to the random variable xr where r is a large even number. since r is even, xr
is nonnegative, and thus prob(|x|     a) = prob(xr     ar)     e(xr)/ar. if e(xr) is not
too large, we will get a good bound. to compute e(xr), write e(x) as e(x1 + . . . + xn)r
and expand the polynomial into a sum of terms. use the fact that by independence
e(xri
j ) to get a collection of simpler expectations that can be bounded
using our assumption that |e(xs

i )|       2s!. for the full proof, see the appendix.

j ) = e(xri

i )e(xrj

i xrj

2.3 the geometry of high dimensions

an important property of high-dimensional objects is that most of their volume is
near the surface. consider any object a in rd. now shrink a by a small amount   to
produce a new object (1      )a = {(1      )x|x     a}. then the following equality holds:

volume(cid:0)(1      )a(cid:1) = (1      )dvolume(a).

15

markov

condition

x     0

chebychev

any x

tail bound

prob(x     a)     e(x)

prob(cid:0)|x     e(x)|     a(cid:1)     var(x)

a

a2

cherno   

x = x1 + x2 +        + xn
xi     [0, 1] i.i.d. bernoulli;

prob(|x     e(x)|       e(x))

    3e   c  2e(x)

higher moments

gaussian
annulus

r positive even integer

x =(cid:112)x2
xi     n (0, 1);           

1 + x2

2 +        + x2
n indep.

n

power law

for xi; order k     4

x = x1 + x2 + . . . + xn

xi i.i.d ;        1/k2

prob(|x|     a)     e(xr)/ar
prob(|x        
prob(cid:0)|x     e(x)|       e(x)(cid:1)

n|       )     3e   c  2

    (4/  2kn)(k   3)/2

figure 2.1: table of tail bounds. the higher moments bound is obtained by apply-
ing markov to xr. the cherno   , gaussian annulus, and power law bounds follow from
theorem 2.5 which is proved in the appendix.

to see that this is true, partition a into in   nitesimal cubes. then, (1       )a is the union
of a set of cubes obtained by shrinking the cubes in a by a factor of 1       . when we
shrink each of the 2d sides of a d-dimensional cube by a factor f , its volume shrinks by a
factor of f d. using the fact that 1     x     e   x, for any object a in rd we have:

volume(cid:0)(1      )a(cid:1)

volume(a)

= (1      )d     e    d.

fixing   and letting d        , the above quantity rapidly approaches zero. this means
that nearly all of the volume of a must be in the portion of a that does not belong to
the region (1      )a.

let s denote the unit ball in d dimensions, that is, the set of points within distance
one of the origin. an immediate implication of the above observation is that at least a
1     e    d fraction of the volume of the unit ball is concentrated in s \ (1      )s, namely
in a small annulus of width   at the boundary. in particular, most of the volume of the
d-dimensional unit ball is contained in an annulus of width o(1/d) near the boundary. if

the ball is of radius r, then the annulus width is o(cid:0) r

(cid:1) .

d

16

1
1     1

d

annulus of
width 1
d

figure 2.2: most of the volume of the d-dimensional ball of radius r is contained in an
annulus of width o(r/d) near the boundary.

2.4 properties of the unit ball

we now focus more speci   cally on properties of the unit ball in d-dimensional space.
we just saw that most of its volume is concentrated in a small annulus of width o(1/d)
near the boundary. next we will show that in the limit as d goes to in   nity, the volume of
the ball goes to zero. this result can be proven in several ways. here we use integration.

2.4.1 volume of the unit ball

to calculate the volume v (d) of the unit ball in rd, one can integrate in either cartesian
or polar coordinates. in cartesian coordinates the volume is given by

x1=1(cid:90)

   
1   x2

1

(cid:90)

x2=

x1=   1

x2=   

   
1   x2

1

   

(cid:90)

xd=

1   x2

1            x2

d   1

      
xd=      

1   x2

1            x2

d   1

v (d) =

dxd        dx2dx1.

since the limits of the integrals are complicated, it is easier to integrate using polar
coordinates. in polar coordinates, v (d) is given by

(cid:90)

1(cid:90)

sd

r=0

v (d) =

rd   1drd   .

since the variables     and r do not interact,

(cid:90)

1(cid:90)

sd

r=0

(cid:90)

sd

1
d

d    =

a(d)

d

v (d) =

d   

rd   1dr =

where a(d) is the surface area of the d-dimensional unit ball. for instance, for d = 3 the
surface area is 4   and the volume is 4
3  . the question remains, how to determine the

17

surface area a (d) =(cid:82)

sd

d    for general d.

consider a di   erent integral

   (cid:90)

   (cid:90)

   (cid:90)

e

      

   (x2

1+x2

2+      x2

d)dxd        dx2dx1.

i (d) =

      

      

      

including the exponential allows integration to in   nity rather than stopping at the surface
of the sphere. thus, i(d) can be computed by integrating in both cartesian and polar
coordinates. integrating in polar coordinates will relate i(d) to the surface area a(d).
equating the two results for i(d) allows one to solve for a(d).

first, calculate i(d) by integration in cartesian coordinates.

      d

=(cid:0)   

  (cid:1)d =   

d
2 .

          (cid:90)
here, we have used the fact that(cid:82)    

i (d) =

   

e   x2dx

      
       e   x2 dx =
(cid:90)
   (cid:90)

sd

0

   (cid:82)

  . for a proof of this, see section 12.3
of the appendix. next, calculate i(d) by integrating in polar coordinates. the volume of
the di   erential element is rd   1d   dr. thus,

i (d) =

d   

e   r2rd   1dr.

the integral (cid:82)

sd

integral gives

   (cid:90)

0

and   (cid:0) 1

(cid:1) =

2

d    is the integral over the entire solid angle and gives the surface area,

a(d), of a unit sphere. thus, i (d) = a (d)

e   r2rd   1dr. evaluating the remaining

e   r2rd   1dr =

0

(cid:17)

   (cid:90)

(cid:16) 1

   (cid:90)
(cid:1) where the gamma function    (x) is a generalization of the
2  (cid:0) d

2     1dt =

(cid:18) d

2t    1

(cid:19)

e   tt

e   tt

d   1
2

2 dt

1
2

1
2

=

  

2

d

0

0

and hence, i(d) = a(d) 1
factorial function for noninteger values of x.    (x) = (x     1)    (x     1),    (1) =    (2) = 1,

2

   
  . for integer x,    (x) = (x     1)!.

combining i (d) =   

d
2 with i (d) = a (d) 1

a (d) =

establishing the following lemma.

(cid:1) yields
2  (cid:0) d
(cid:1)
2  (cid:0) d

  

d
2

2

1

2

18

lemma 2.6 the surface area a(d) and the volume v (d) of a unit-radius ball in d di-
mensions are given by

a (d) =

2

2   d
  ( d
2 )

and

v (d) =

2

2   d
d   ( d
2 )

.

3
2

= 4

  
  ( 3
2)

to check the formula for the volume of a unit ball, note that v (2) =    and v (3) =
2
3  , which are the correct volumes for the unit balls in two and three dimen-
3
sions. to check the formula for the surface area of a unit ball, note that a(2) = 2   and
   
a(3) = 2  
dimensions. note that    d
this implies that lim

(cid:1) grows as the factorial of d

= 4  , which are the correct surface areas for the unit ball in two and three

2 and   (cid:0) d

2 is an exponential in d

3
2
  

2 .

1
2

2

d       v (d) = 0, as claimed.

2.4.2 volume near the equator

an interesting fact about the unit ball in high dimensions is that most of its volume
is concentrated near its    equator   . in particular, for any unit-length vector v de   ning
   
   north   , most of the volume of the unit ball lies in the thin slab of points whose dot-
d). to show this fact, it su   ces by symmetry to    x
product with v has magnitude o(1/
   
v to be the    rst coordinate vector. that is, we will show that most of the volume of the
unit ball has |x1| = o(1/
d). using this fact, we will show that two random points in the
unit ball are with high id203 nearly orthogonal, and also give an alternative proof
from the one in section 2.4.1 that the volume of the unit ball goes to zero as d        .
theorem 2.7 for c     1 and d     3, at least a 1     2
d-dimensional unit ball has |x1|     c   
d   1

c e   c2/2 fraction of the volume of the

.

c e   c2/2 fraction of the half of
proof: by symmetry we just need to prove that at most a 2
the ball with x1     0 has x1     c   
. let a denote the portion of the ball with x1     c   
d   1
d   1
and let h denote the upper hemisphere. we will then show that the ratio of the volume
of a to the volume of h goes to zero by calculating an upper bound on volume(a) and
a lower bound on volume(h) and proving that

volume(a)
volume(h)

    upper bound volume(a)
lower bound volume(h)

=

e    c2
2 .

2
c

dx1 and whose face is a ball of dimension d     1 and radius(cid:112)1     x2

to calculate the volume of a, integrate an incremental volume that is a disk of width
1. the surface area of

the disk is (1     x2

1) d   1

2 v (d     1) and the volume above the slice is
2 v (d     1)dx1

volume(a) =

(1     x2
1)

d   1

(cid:90) 1

c   

d   1

19

x1

   
d     1
c

(cid:90)    
(cid:90)    

d   1

c   

         

h

x1

a

c   

d   1

figure 2.3: most of the volume of the upper hemisphere of the d-dimensional ball is
below the plane x1 = c   

.

d   1

to get an upper bound on the above integral, use 1     x     e   x and integrate to in   nity.
to integrate, insert x1
, which is greater than one in the range of integration, into the
integral. then

d   1
c

   

volume(a)    

e    d   1
2 x2

1v (d     1)dx1 = v (d     1)

x1e    d   1
2 x2

1dx1

   
d     1
c

(cid:90)    

c   

d   1

now

c   

d   1

x1e    d   1
2 x2

1dx1 =     1
d     1
thus, an upper bound on volume(a) is v (d   1)
d   1
the volume of the hemisphere below the plane x1 = 1   
volume of the upper hemisphere and this volume is at least that of a cylinder of height

1
d     1

e    d   1
2 x2
1

e    c2
2 .

e    c2

   
c

d   1

c   

(d   1)

=

2

is a lower bound on the entire

1     1

1   
and radius
d   1
fact that (1   x)a     1   ax for a     1, the volume of the cylinder is at least v (d   1)
   
d   1
2

d   1. the volume of the cylinder is v (d     1)(1     1

d   1) d   1

2

1   

d   1
. using the
for d     3.

(cid:113)

thus,

ratio     upper bound above plane

lower bound total hemisphere

=

e    c2

2

v (d   1)
   
d   1
c
v (d   1)
   
d   1
2

=

e    c2

2

2
c

one might ask why we computed a lower bound on the total hemisphere since it is one
half of the volume of the unit ball which we already know. the reason is that the volume
of the upper hemisphere is 1
v (d     1) in the numerator.

2v (d) and we need a formula with v (d    1) in it to cancel the

20

(cid:12)(cid:12)(cid:12)   

near orthogonality. one immediate implication of the above analysis is that if we
draw two points at random from the unit ball, with high id203 their vectors will be
nearly orthogonal to each other. speci   cally, from our previous analysis in section 2.3,
with high id203 both will be close to the surface and will have length 1     o(1/d).
   
from our analysis above, if we de   ne the vector in the direction of the    rst point as
   north   , with high id203 the second will have a projection of only   o(1/
   
d) in
this direction, and thus their dot-product will be   o(1/
   
d). this implies that with high
id203, the angle between the two vectors will be   /2    o(1/
d). in particular, we
have the following theorem that states that if we draw n points at random in the unit
ball, with high id203 all points will be close to unit length and each pair of points
will be almost orthogonal.

theorem 2.8 consider drawing n points x1, x2, . . . , xn at random from the unit ball.
with id203 1     o(1/n)

1. |xi|     1     2 ln n
2. |xi    xj|        
6 ln n   
d   1

d

for all i, and
for all i (cid:54)= j.

proof: for the    rst part, for any    xed i by the analysis of section 2.3, the id203
that |xi| < 1       is less than e    d. thus

prob(cid:0)|xi| < 1     2 ln n

(cid:1)     e   ( 2 ln n

d

d

)d = 1/n2.

by the union bound, the id203 there exists an i such that |xi| < 1     2 ln n
1/n.

d

is at most

2

for the second part, theorem 2.7 states that the id203 |xi| > c   
d   1

(cid:1) pairs i and j and for each such pair if we de   ne xi as    north   , the

c e    c2
id203 that the projection of xj onto the    north    direction is more than
most o(e    6 ln n

is at
2 ) = o(n   3). thus, the dot-product condition is violated with id203

2 . there are(cid:0)n
at most o(cid:0)(cid:0)n

(cid:1)n   3(cid:1) = o(1/n) as well.

   
6 ln n   
d   1

is at most

2

2

alternative proof that volume goes to zero. another immediate implication of
theorem 2.7 is that as d        , the volume of the ball approaches zero. speci   cally, con-
sider a small box centered at the origin of side length 2c   
   
. using theorem 2.7, we show
d   1
that for c = 2
ln d, this box contains over half of the volume of the ball. on the other
hand, the volume of this box clearly goes to zero as d goes to in   nity, since its volume is
o(( ln d

d   1)d/2). thus the volume of the ball goes to zero as well.

   
ln d, the fraction of the volume of the ball with |x1|     c   
by theorem 2.7 with c = 2
d   1

is at most:

e    c2

2 =

2
c

1   
ln d

e   2 ln d =

   
1
ln d

d2

<

1
d2 .

21

1

   

2
2

1

1

1
2

1
2

1

1
2

   

d
2

    unit radius sphere
       nearly all the volume
    vertex of hypercube

figure 2.4: illustration of the relationship between the sphere and the cube in 2, 4, and
d-dimensions.

since this is true for each of the d dimensions, by a union bound at most a o( 1
fraction of the volume of the ball lies outside the cube, completing the proof.

d)     1

2

side-length o(cid:0) ln d

discussion. one might wonder how it can be that nearly all the points in the unit ball
are very close to the surface and yet at the same time nearly all points are in a box of

(cid:1). the answer is to remember that points on the surface of the ball
(cid:16) 1   
(cid:17)

satisfy x2
.
in fact, it is often helpful to think of picking a random point on the sphere as very similar
to picking a random point of the form

d = 1, so for each coordinate i, a typical value will be   o

(cid:16)   1   

2 + . . . + x2

, . . .    1   

,   1   

,   1   

1 + x2

(cid:17)

d   1

.

d

d

d

d

d

2.5 generating points uniformly at random from a ball

consider generating points uniformly at random on the surface of the unit ball. for
the 2-dimensional version of generating points on the circumference of a unit-radius cir-
cle, independently generate each coordinate uniformly at random from the interval [   1, 1].
this produces points distributed over a square that is large enough to completely contain
the unit circle. project each point onto the unit circle. the distribution is not uniform
since more points fall on a line from the origin to a vertex of the square than fall on a line
from the origin to the midpoint of an edge of the square due to the di   erence in length.
to solve this problem, discard all points outside the unit circle and project the remaining
points onto the circle.

in higher dimensions, this method does not work since the fraction of points that fall
inside the ball drops to zero and all of the points would be thrown away. the solution is to
generate a point each of whose coordinates is an independent gaussian variable. generate
exp(   x2/2) on the
x1, x2, . . . , xd, using a zero mean, unit variance gaussian, namely,

1   
2  

22

real line.2 thus, the id203 density of x is

p (x) =

1

d
2

(2  )

e

1+x2

    x2

2+      +x2
2

d

and is spherically symmetric. normalizing the vector x = (x1, x2, . . . , xd) to a unit vector,
namely x|x|, gives a distribution that is uniform over the surface of the sphere. note that
once the vector is normalized, its coordinates are no longer statistically independent.

to generate a point y uniformly over the ball (surface and interior), scale the point

x|x| generated on the surface by a scalar        [0, 1]. what should the distribution of    be
as a function of r? it is certainly not uniform, even in 2 dimensions. indeed, the density
(cid:82) r=1
of    at r is proportional to r for d = 2. for d = 3, it is proportional to r2. by similar
reasoning, the density of    at distance r is proportional to rd   1 in d dimensions. solving
r=0 crd   1dr = 1 (the integral of density must equal 1) one should set c = d. another
way to see this formally is that the volume of the radius r ball in d dimensions is rdv (d).
dr (rdvd) = drd   1vd. so, pick   (r) with density equal to
the density at radius r is exactly d
drd   1 for r over [0, 1].

we have succeeded in generating a point

y =   

x
|x|

uniformly at random from the unit ball by using the convenient spherical gaussian dis-
tribution. in the next sections, we will analyze the spherical gaussian in more detail.

2.6 gaussians in high dimension

a 1-dimensional gaussian has its mass close to the origin. however, as the dimension
is increased something di   erent happens. the d-dimensional spherical gaussian with zero
mean and variance   2 in each coordinate has density function

(cid:17)

(cid:16)   |x|2

2  2

.

p(x) =

1

(2  )d/2   d

exp

the value of the density is maximum at the origin, but there is very little volume there.
when   2 = 1, integrating the id203 density over a unit ball centered at the origin
yields almost zero mass since the volume of such a ball is negligible. in fact, one needs

2one might naturally ask:    how do you generate a random number from a 1-dimensional gaussian?   
to generate a number from any distribution given its cumulative distribution function p,    rst select a
uniform random number u     [0, 1] and then choose x = p    1(u). for any a < b, the id203 that x is
between a and b is equal to the id203 that u is between p (a) and p (b) which equals p (b)     p (a)
as desired. for the 2-dimensional gaussian, one can generate a point in polar coordinates by choosing

angle    uniform in [0, 2  ] and radius r =(cid:112)   2 ln(u) where u is uniform random in [0, 1]. this is called

the box-muller transform.

23

   

d before there is a signi   cant volume and
to increase the radius of the ball to nearly
d, the
hence signi   cant id203 mass.
integral barely increases even though the volume increases since the id203 density
is dropping o    at a much higher rate. the following theorem formally states that nearly
all the id203 is concentrated in a thin annulus of width o(1) at radius

if one increases the radius much beyond

   

d.

   

   

theorem 2.9 (gaussian annulus theorem) for a d-dimensional spherical gaussian
d, all but at most 3e   c  2 of the prob-
d +   , where c is a    xed positive

with unit variance in each direction, for any           
d            |x|        
d(cid:80)

ability mass lies within the annulus
constant.
for a high-level intuition, note that e(|x|2) =
1) = d, so the mean
squared distance of a point from the center is d. the gaussian annulus theorem says
that the points are tightly concentrated. we call the square root of the mean squared
distance, namely

   
d, the radius of the gaussian.

i ) = de(x2

e(x2

i=1

to prove the gaussian annulus theorem we make use of a tail inequality for sums of

independent random variables of bounded moments (theorem 12.5).

proof (gaussian annulus theorem): let x = (x1, x2, . . . , xd) be a point selected
from a unit variance gaussian centered at the origin, and let r = |x|.
d            |y|    
d|       . if |r        
d +    is equivalent to |r        
   
d|       , then multiplying both sides by
   
   
d gives |r2     d|       (r +
d)       
d. so, it su   ces to bound the id203 that
r +
|r2     d|       

   

   

d.

   

rewrite r2     d = (x2

of variables: yi = x2
notice that e(yi) = e(x2
moments of yi.

d)     d = (x2

i     1. we want to bound the id203 that |y1 + . . . + yd|       

d     1) and perform a change
   
1 + . . . + x2
d.
i )     1 = 0. to apply theorem 12.5, we need to bound the sth

1     1) + . . . + (x2

for |xi|     1, |yi|s     1 and for |xi|     1, |yi|s     |xi|2s. thus

|e(ys

i )| = e(|yi|s)     e(1 + x2s

i ) = 1 + e(x2s
i )

= 1 +

x2se   x2/2dx

(cid:114) 2

  

(cid:90)    
(cid:90)    

0

using the substitution 2z = x2,
|e(ys

1   
  

i )| = 1 +
    2ss!.

0

2szs   (1/2)e   zdz

the last inequality is from the gamma integral.

24

since e(yi) = 0, v ar(yi) = e(y2

i )     222 = 8. unfortunately, we do not have |e(ys

i )|    
8s! as required in theorem 12.5. to    x this problem, perform one more change of variables,
using wi = yi/2. then, v ar(wi)     2 and |e(ws
i )|     2s!, and our goal is now to bound the
   
id203 that |w1 + . . . + wd|       
2 . applying theorem 12.5 where   2 = 2 and n = d,
this occurs with id203 less than or equal to 3e      2
96 .

d

in the next sections we will see several uses of the gaussian annulus theorem.

2.7 random projection and johnson-lindenstrauss lemma

one of the most frequently used subroutines in tasks involving high dimensional data
is nearest neighbor search. in nearest neighbor search we are given a database of n points
in rd where n and d are usually large. the database can be preprocessed and stored in
an e   cient data structure. thereafter, we are presented    query    points in rd and are
asked to    nd the nearest or approximately nearest database point to the query point.
since the number of queries is often large, the time to answer each query should be very
small, ideally a small function of log n and log d, whereas preprocessing time could be
larger, namely a polynomial function of n and d. for this and other problems, dimension
reduction, where one projects the database points to a k-dimensional space with k (cid:28) d
(usually dependent on log d) can be very useful so long as the relative distances between
points are approximately preserved. we will see using the gaussian annulus theorem
that such a projection indeed exists and is simple.

the projection f : rd     rk that we will examine (many related projections are
known to work as well) is the following. pick k gaussian vectors u1, u2, . . . , uk in rd
with unit-variance coordinates. for any vector v, de   ne the projection f (v) by:

f (v) = (u1    v, u2    v, . . . , uk    v).

with high id203, |f (v)|        
the projection f (v) is the vector of dot products of v with the ui. we will show that
k|v|. for any two vectors v1 and v2, f (v1     v2) =
f (v1)    f (v2). thus, to estimate the distance |v1     v2| between two vectors v1 and v2 in
rd, it su   ces to compute |f (v1)    f (v2)| = |f (v1     v2)| in the k-dimensional space since
the factor of
k is known and one can divide by it. the reason distances increase when
we project to a lower dimensional space is that the vectors ui are not unit length. also
notice that the vectors ui are not orthogonal. if we had required them to be orthogonal,
we would have lost statistical independence.

   

theorem 2.10 (the random projection theorem) let v be a    xed vector in rd
and let f be de   ned as above. there exists constant c > 0 such that for        (0, 1),

(cid:16)(cid:12)(cid:12)(cid:12)|f (v)|    

k|v|(cid:12)(cid:12)(cid:12)       

   

k|v|(cid:17)     3e   ck  2,

   

prob

where the id203 is taken over the random draws of vectors ui used to construct f .

25

proof: by scaling both sides of the inner inequality by |v|, we may assume that |v| = 1.
the sum of independent normally distributed real variables is also normally distributed
where the mean and variance are the sums of the individual means and variances. since
j=1 uijvj, the random variable ui    v has gaussian density with zero mean and

ui    v =(cid:80)d

unit variance, in particular,

(cid:32) d(cid:88)

(cid:33)

d(cid:88)

d(cid:88)

v ar(ui    v) = v ar

vijvj

=

v2
j v ar(uij) =

v2
j = 1

j=1

j=1

j=1

since u1  v, u2  v, . . . , uk  v are independent gaussian random variables, f (v) is a random
vector from a k-dimensional spherical gaussian with unit variance in each coordinate, and
so the theorem follows from the gaussian annulus theorem (theorem 2.9) with d replaced
by k.

the random projection theorem establishes that the id203 of the length of the
projection of a single vector di   ering signi   cantly from its expected value is exponentially
small in k, the dimension of the target subspace. by a union bound, the id203 that
any of o(n2) pairwise di   erences |vi     vj| among n vectors v1, . . . , vn di   ers signi   cantly
from their expected values is small, provided k     3
c  2 ln n. thus, this random projection
preserves all relative pairwise distances between points in a set of n points with high
id203. this is the content of the johnson-lindenstrauss lemma.

theorem 2.11 (johnson-lindenstrauss lemma) for any 0 <    < 1 and any integer
n, let k     3
c  2 ln n with c as in theorem 2.9. for any set of n points in rd, the random
projection f : rd     rk de   ned above has the property that for all pairs of points vi and
vj, with id203 at least 1     3/2n,

   
(1       )

k |vi     vj|     |f (vi)     f (vj)|     (1 +   )

   
k |vi     vj| .

proof: applying the random projection theorem (theorem 2.10), for any    xed vi and
vj, the id203 that |f (vi     vj)| is outside the range

(cid:104)

(1       )

   

   
k|vi     vj|, (1 +   )

c  2 . since there are(cid:0)n

k|vi     vj|(cid:105)
(cid:1) < n2/2 pairs of points, by the

is at most 3e   ck  2     3/n3 for k     3 ln n
union bound, the id203 that any pair has a large distortion is less than 3
2n.

2

remark: it is important to note that the conclusion of theorem 2.11 asserts for all vi
and vj, not just for most of them. the weaker assertion for most vi and vj is typically less
useful, since our algorithm for a problem such as nearest-neighbor search might return
one of the bad pairs of points. a remarkable aspect of the theorem is that the number
of dimensions in the projection is only dependent logarithmically on n. since k is often
much less than d, this is called a dimension reduction technique.
in applications, the
dominant term is typically the 1/  2 term.

26

for the nearest neighbor problem, if the database has n1 points and n2 queries are
expected during the lifetime of the algorithm, take n = n1 + n2 and project the database
to a random k-dimensional space, for k as in theorem 2.11. on receiving a query, project
the query to the same subspace and compute nearby database points. the johnson
lindenstrauss lemma says that with high id203 this will yield the right answer
whatever the query. note that the exponentially small in k id203 was useful here in
making k only dependent on ln n, rather than n.

2.8 separating gaussians

mixtures of gaussians are often used to model heterogeneous data coming from multiple
sources. for example, suppose we are recording the heights of individuals age 20-30 in a
city. we know that on average, men tend to be taller than women, so a natural model
would be a gaussian mixture model p(x) = w1p1(x) + w2p2(x), where p1(x) is a gaussian
density representing the typical heights of women, p2(x) is a gaussian density represent-
ing the typical heights of men, and w1 and w2 are the mixture weights representing the
proportion of women and men in the city. the parameter estimation problem for a mixture
model is the problem: given access to samples from the overall density p (e.g., heights of
people in the city, but without being told whether the person with that height is male
or female), reconstruct the parameters for the distribution (e.g., good approximations to
the means and variances of p1 and p2, as well as the mixture weights).

there are taller women and shorter men, so even if one solved the parameter estima-
tion problem for heights perfectly, given a data point, one couldn   t necessarily tell which
population it came from. that is, given a height, one couldn   t necessarily tell if it came
from a man or a woman. in this section, we will look at a problem that is in some ways
easier and some ways harder than this problem of heights. it will be harder in that we
will be interested in a mixture of two gaussians in high-dimensions as opposed to the
d = 1 case of heights. but it will be easier in that we will assume the means are quite
well-separated compared to the variances. speci   cally, our focus will be on a mixture of
two spherical unit-variance gaussians whose means are separated by a distance    (d1/4).
we will show that at this level of separation, we can with high id203 uniquely de-
termine which gaussian each data point came from. the algorithm to do so will actually
be quite simple. calculate the distance between all pairs of points. points whose distance
apart is smaller are from the same gaussian, whereas points whose distance is larger are
from di   erent gaussians. later, we will see that with more sophisticated algorithms, even
a separation of    (1) su   ces.

also e   |x|2/2 =(cid:81)

   
first, consider just one spherical unit-variance gaussian centered at the origin. from
d.
theorem 2.9, most of its id203 mass lies on an annulus of width o(1) at radius
i /2 and almost all of the mass is within the slab { x |    c     x1     c },
for c     o(1). pick a point x from this gaussian. after picking x, rotate the coordinate
system to make the    rst axis align with x. independently pick a second point y from

i e   x2

27

   

d

   
2d

   

d

(a)

   
d

x

p

   

   

   2 + 2d

   

(b)

   
2d
y

z

q

figure 2.5: (a) indicates that two randomly chosen points in high dimension are surely
almost nearly orthogonal. (b) indicates the distance between a pair of random points
from two di   erent unit balls approximating the annuli of two gaussians.

|x     y|    (cid:112)|x|2 + |y|2. see figure 2.5(a). more precisely, since the coordinate system

this gaussian. the fact that almost all of the id203 mass of the gaussian is within
the slab {x |     c     x1     c, c     o(1)} at the equator implies that y   s component along
x   s direction is o(1) with high id203. thus, y is nearly perpendicular to x. so,
   
d    o(1), 0, . . . , 0). since y is
has been rotated so that x is at the north pole, x = (
almost on the equator, further rotate the coordinate system so that the component of
y that is perpendicular to the axis of the north pole is in the second coordinate. then
y = (o(1),

   
d    o(1), 0, . . . , 0). thus,
   
(x     y)2 = d    o(

d) + d    o(
2d    o(1) with high id203.

   

and |x     y| =

   
d) = 2d    o(

   
d)

consider two spherical unit variance gaussians with centers p and q separated by a
   
distance    . the distance between a randomly chosen point x from the    rst gaussian
   2 + 2d, since x     p, p     q,
and a randomly chosen point y from the second is close to
and q     y are nearly mutually perpendicular. pick x and rotate the coordinate system
so that x is at the north pole. let z be the north pole of the ball approximating the
second gaussian. now pick y. most of the mass of the second gaussian is within o(1)
of the equator perpendicular to z     q. also, most of the mass of each gaussian is within
distance o(1) of the respective equators perpendicular to the line q     p. see figure 2.5
(b). thus,

|x     y|2        2 + |z     q|2 + |q     y|2

   
=    2 + 2d    o(

d)).

to ensure that the distance between two points picked from the same gaussian are
closer to each other than two points picked from di   erent gaussians requires that the
upper limit of the distance between a pair of points from the same gaussian is at most

28

   
2d +    2    o(1) or 2d + o(

   
2d + o(1)        
the lower limit of distance between points from di   erent gaussians. this requires that
d)     2d +    2, which holds when           (d1/4).
thus, mixtures of spherical gaussians can be separated in this way, provided their centers
are separated by   (d1/4).
if we have n points and want to correctly separate all of
them with high id203, we need our individual high-id203 statements to hold
with id203 1     1/poly(n),3 which means our o(1) terms from theorem 2.9 become
   
o(

   
log n). so we need to include an extra o(

log n) term in the separation distance.

algorithm for separating points from two gaussians: calculate all
pairwise distances between points. the cluster of smallest pairwise distances
must come from a single gaussian. remove these points. the remaining
points come from the second gaussian.

one can actually separate gaussians where the centers are much closer.
in the next
chapter we will use singular value decomposition to separate points from a mixture of two
gaussians when their centers are separated by a distance o(1).

2.9 fitting a spherical gaussian to data

given a set of sample points, x1, x2, . . . , xn, in a d-dimensional space, we wish to    nd
the spherical gaussian that best    ts the points. let f be the unknown gaussian with
mean    and variance   2 in each direction. the id203 density for picking these points
when sampling according to f is given by

(cid:33)

(cid:32)

c exp

    (x1       )2 + (x2       )2 +        + (xn       )2
(cid:21)n

(cid:20)(cid:90)

2  2

where the normalizing constant c is the reciprocal of

. in integrating from

    |x     |2
e

2  2 dx

(cid:20)(cid:90)

    |x|2

2  2 dx

e

(cid:21)   n

= 1
(2  )

n
2

and is

       to    , one can shift the origin to    and thus c is
independent of   .

the maximum likelihood estimator (id113) of f, given the samples x1, x2, . . . , xn, is

the f that maximizes the above id203 density.
lemma 2.12 let {x1, x2, . . . , xn} be a set of n d-dimensional points. then (x1       )2 +
(x2       )2+      +(xn       )2 is minimized when    is the centroid of the points x1, x2, . . . , xn,
namely    = 1
proof: setting the gradient of (x1       )2 + (x2       )2 +       + (xn       )2 with respect to   
to zero yields

n(x1 + x2 +        + xn).

   2 (x1       )     2 (x2       )                2 (xn       ) = 0.

solving for    gives    = 1

n(x1 + x2 +        + xn).

3poly(n) means bounded by a polynomial in n.

29

to determine the maximum likelihood estimate of   2 for f , set    to the true centroid.
next, show that    is set to the standard deviation of the sample. substitute    = 1
2  2 and
a = (x1       )2 + (x2       )2 +       + (xn       )2 into the formula for the id203 of picking
the points x1, x2, . . . , xn. this gives

now, a is    xed and    is to be determined. taking logs, the expression to maximize is

to    nd the maximum, di   erentiate with respect to   , set the derivative to zero, and solve
for   . the derivative is

   a       n ln

e     x2dx

(cid:21)n .

e   a  
e   x2  dx

(cid:20)(cid:82)

x

       .

(cid:82)

      (cid:90)

x

x

(cid:82)
(cid:82)
(cid:82)

y

|x|2e     x2dx
e     x2dx

.

y2e   y2dy
e   y2dy

.

   a + n

x

setting y = |   

  x| in the derivative, yields

   a +

n
  

y

since the ratio of the two integrals is the expected distance squared of a d-dimensional
spherical gaussian of standard deviation 1   
2 , we
2   gives    a + nd  2. setting    a + nd  2 = 0 shows that
get    a + nd
2   . substituting   2 for 1
   
a   
the maximum occurs when    =
. note that this quantity is the square root of the
nd
average coordinate distance squared of the samples to their mean, which is the standard
deviation of the sample. thus, we get the following lemma.

to its center, and this is known to be d

2

lemma 2.13 the maximum likelihood spherical gaussian for a set of samples is the
gaussian with center equal to the sample mean and standard deviation equal to the stan-
dard deviation of the sample from the true mean.

let x1, x2, . . . , xn be a sample of points generated by a gaussian id203 distri-
n(x1 + x2 +        + xn) is an unbiased estimator of the expected value

bution. then    = 1
of the distribution. however, if in estimating the variance from the sample set, we use
the estimate of the expected value rather than the true expected value, we will not get
an unbiased estimate of the variance, since the sample mean is not independent of the
sample set. one should use      = 1
see section 12.5.10 of the appendix.

n   1(x1 + x2 +        + xn) when estimating the variance.

30

2.10 bibliographic notes

the word vector model was introduced by salton [swy75]. there is vast literature on
the gaussian distribution, its properties, drawing samples according to it, etc. the reader
can choose the level and depth according to his/her background. the master tail bounds
theorem and the derivation of cherno    and other inequalities from it are from [kan09].
the original proof of the random projection theorem by johnson and lindenstrauss was
complicated. several authors used gaussians to simplify the proof. the proof here is due
to dasgupta and gupta [dg99]. see [vem04] for details and applications of the theorem.
[mu05] and [mr95b] are text books covering much of the material touched upon here.

31

2.11 exercises

exercise 2.1

1. let x and y be independent random variables with uniform distribution in [0, 1].

what is the expected value e(x), e(x2), e(x     y), e(xy), and e(x     y)2?

2. let x and y be independent random variables with uniform distribution in [    1

what is the expected value e(x), e(x2), e(x     y), e(xy), and e(x     y)2?

2, 1
2].

3. what is the expected squared distance between two points generated at random inside

a unit d-dimensional cube?

exercise 2.2 randomly generate 30 points inside the cube [    1
2]100 and plot distance
between points and the angle between the vectors from the origin to the points for all pairs
of points.
exercise 2.3 show that for any a     1 there exist distributions for which markov   s in-
equality is tight by showing the following:

2, 1

1. for each a = 2, 3, and 4 give a id203 distribution p(x) for a nonnegative random

variable x where prob(cid:0)x     a(cid:1) = e(x)
x where prob(cid:0)x     a(cid:1) = e(x)

a .

a .

2. for arbitrary a     1 give a id203 distribution for a nonnegative random variable

exercise 2.4 show that for any c     1 there exist distributions for which chebyshev   s
inequality is tight, in other words, p rob(|x     e(x)|     c) = v ar(x)/c2.

exercise 2.5 let x be a random variable with id203 density 1
zero elsewhere.

4 for 0     x     4 and

1. use markov   s inequality to bound the id203 that x     3.
2. make use of prob(|x|     a) = prob(x2     a2) to get a tighter bound.
3. what is the bound using prob(|x|     a) = prob(xr     ar)?

exercise 2.6 consider the id203 distribution p(x = 0) = 1     1
a and p(x = a) = 1
a.
plot the id203 that x is greater than or equal to a as a function of a for the bound
given by markov   s inequality and by markov   s inequality applied to x2 and x4.

exercise 2.7 consider the id203 density function p(x) = 0 for x < 1 and p(x) = c 1
for x     1.
x4

1. what should c be to make p a legal id203 density function?

2. generate 100 random samples from this distribution. how close is the average of

the samples to the expected value of x?

32

exercise 2.8 let g be a d-dimensional spherical gaussian with variance 1
rection, centered at the origin. derive the expected squared distance to the origin.

2 in each di-

exercise 2.9 consider drawing a random point x on the surface of the unit sphere in rd.
what is the variance of x1 (the    rst coordinate of x)? see if you can give an argument
without doing any integrals.

exercise 2.10 how large must    be for 99% of the volume of a 1000-dimensional unit-
radius ball to lie in the shell of   -thickness at the surface of the ball?
exercise 2.11 prove that 1 + x     ex for all real x. for what values of x is the approxi-
mation 1 + x     ex within 0.01?
exercise 2.12 for what value of d does the volume, v (d), of a d-dimensional unit ball
take on its maximum? hint: consider the ratio v (d)

v (d   1).

exercise 2.13 a 3-dimensional cube has vertices, edges, and faces. in a d-dimensional
cube, these components are called faces. a vertex is a 0-dimensional face, an edge a
1-dimensional face, etc.

1. for 0     k     d, how many k-dimensional faces does a d-dimensional cube have?

2. what is the total number of faces of all dimensions? the d-dimensional face is the

cube itself which you can include in your count.

3. what is the surface area of a unit cube in d-dimensions (a unit cube has side-length

one in each dimension)?

4. what is the surface area of the cube if the length of each side was 2?

5. prove that the volume of a unit cube is close to its surface.

exercise 2.14 consider the portion of the surface area of a unit radius, 3-dimensional
ball with center at the origin that lies within a circular cone whose vertex is at the origin.
what is the formula for the incremental unit of area when using polar coordinates to
integrate the portion of the surface area of the ball that is lying inside the circular cone?
what is the formula for the integral? what is the value of the integral if the angle of the
cone is 36   ? the angle of the cone is measured from the axis of the cone to a ray on the
surface of the cone.

exercise 2.15 consider a unit radius, circular cylinder in 3-dimensions of height one.
the top of the cylinder could be an horizontal plane or half of a circular ball. consider
these two possibilities for a unit radius, circular cylinder in 4-dimensions. in 4-dimensions
the horizontal plane is 3-dimensional and the half circular ball is 4-dimensional. in each
of the two cases, what is the surface area of the top face of the cylinder? you can use
v (d) for the volume of a unit radius, d-dimension ball and a(d) for the surface area of
a unit radius, d-dimensional ball. an in   nite length, unit radius, circular cylinder in 4-
dimensions would be the set {(x1, x2, x3, x4)|x2
4     1} where the coordinate x1 is
the axis.

2 + x2

3 + x2

33

exercise 2.16 given a d-dimensional circular cylinder of radius r and height h

1. what is the surface area in terms of v (d) and a(d)?

2. what is the volume?

exercise 2.17 how does the volume of a ball of radius two behave as the dimension of
the space increases? what if the radius was larger than two but a constant independent
of d? what function of d would the radius need to be for a ball of radius r to have
approximately constant volume as the dimension increases? hint: you may want to use

stirling   s approximation, n!    (cid:0) n

(cid:1)n , for factorial.

e

exercise 2.18 if lim

d       v (d) = 0, the volume of a d-dimensional ball for su   ciently large
d must be less than v (3). how can this be if the d-dimensional ball contains the three
dimensional ball?

exercise 2.19

1. write a recurrence relation for v (d) in terms of v (d     1) by integrating over x1.
hint: at x1 = t, the (d     1)-dimensional volume of the slice is the volume of a
   
1     t2. express this in terms of v (d    1) and
(d    1)-dimensional sphere of radius
write down the integral. you need not evaluate the integral.

2. verify the formula for d = 2 and d = 3 by integrating and comparing with v (2) =   

and v (3) = 4

3  

exercise 2.20 consider a unit ball a centered at the origin and a unit ball b whose
center is at distance s from the origin. suppose that a random point x is drawn from
   
the mixture distribution:    with id203 1/2, draw at random from a; with id203
d     1 is su   cient so that
1/2, draw at random from b   . show that a separation s (cid:29) 1/
   
prob(x     a     b) = o(1); i.e., for any   > 0 there exists c such that if s     c/
d     1, then
prob(x     a     b) <  . in other words, this extent of separation means that nearly all of
the mixture distribution is identi   able.

exercise 2.21 consider the upper hemisphere of a unit-radius ball in d-dimensions.
what is the height of the maximum volume cylinder that can be placed entirely inside
the hemisphere? as you increase the height of the cylinder, you need to reduce the cylin-
der   s radius so that it will lie entirely within the hemisphere.

exercise 2.22 what is the volume of the maximum size d-dimensional hypercube that
can be placed entirely inside a unit radius d-dimensional ball?

exercise 2.23 calculate the ratio of area above the plane x1 =   to the area of the upper
hemisphere of a unit radius ball in d-dimensions for   = 0.001, 0.01, 0.02, 0.03, 0.04, 0.05
and for d = 100 and d = 1, 000.

34

exercise 2.24 almost all of the volume of a ball in high dimensions lies in a narrow
slice of the ball at the equator. however, the narrow slice is determined by the point on
the surface of the ball that is designated the north pole. explain how this can be true
if several di   erent locations are selected for the location of the north pole giving rise to
di   erent equators.

exercise 2.25 explain how the volume of a ball in high dimensions can simultaneously
be in a narrow slice at the equator and also be concentrated in a narrow annulus at the
surface of the ball.

exercise 2.26 generate 500 points uniformly at random on the surface of a unit-radius
ball in 50 dimensions. then randomly generate    ve additional points. for each of the    ve
new points, calculate a narrow band of width 2   
at the equator, assuming the point was
the north pole. how many of the 500 points are in each band corresponding to one of the
   ve equators? how many of the points are in all    ve bands? how wide do the bands need
to be for all points to be in all    ve bands?

50

exercise 2.27 place 100 points at random on a d-dimensional unit-radius ball. assume
d is large. pick a random vector and let it de   ne two parallel hyperplanes on opposite
sides of the origin that are equal distance from the origin. how close can the hyperplanes
be moved and still have at least a .99 id203 that all of the 100 points land between
them?

exercise 2.28 let x and y be d-dimensional zero mean, unit variance gaussian vectors.
prove that x and y are almost orthogonal by considering their dot product.

exercise 2.29 prove that with high id203, the angle between two random vectors in
a high-dimensional space is at least 45   . hint: use theorem 2.8.

   
d onto a line
exercise 2.30 project the volume of a d-dimensional ball of radius
through the center. for large d, give an intuitive argument that the projected volume
should behave like a gaussian.

exercise 2.31

1. write a computer program that generates n points uniformly distributed over the

surface of a unit-radius d-dimensional ball.

2. generate 200 points on the surface of a sphere in 50 dimensions.

3. create several random lines through the origin and project the points onto each line.

plot the distribution of points on each line.

4. what does your result from (3) say about the surface area of the sphere in relation

to the lines, i.e., where is the surface area concentrated relative to each line?

35

   
exercise 2.32 if one generates points in d-dimensions with each coordinate a unit vari-
ance gaussian, the points will approximately lie on the surface of a sphere of radius
d.

1. what is the distribution when the points are projected onto a random line through

the origin?

2. if one uses a gaussian with variance four, where in d-space will the points lie?

exercise 2.33 randomly generate a 100 points on the surface of a sphere in 3-dimensions
and in 100-dimensions. create a histogram of all distances between the pairs of points in
both cases.

exercise 2.34 we have claimed that a randomly generated point on a ball lies near the
equator of the ball, independent of the point picked to be the north pole. is the same claim
true for a randomly generated point on a cube? to test this claim, randomly generate ten
  1 valued vectors in 128 dimensions. think of these ten vectors as ten choices for the
north pole. then generate some additional   1 valued vectors. to how many of the
original vectors is each of the new vectors close to being perpendicular; that is, how many
of the equators is each new vector close to?

exercise 2.35 de   ne the equator of a d-dimensional unit cube to be the hyperplane

(cid:26)

(cid:12)(cid:12)(cid:12) d(cid:80)

i=1

x

(cid:27)

xi = d
2

.

1. are the vertices of a unit cube concentrated close to the equator?

2. is the volume of a unit cube concentrated close to the equator?

3. is the surface area of a unit cube concentrated close to the equator?

exercise 2.36 consider a nonorthogonal basis e1, e2, . . . , ed. the ei are a set of linearly
independent unit vectors that span the space.

1. prove that the representation of any vector in this basis is unique.

   
2
2 , 1)e where z is expressed in the basis e1 =

   
2
2 )

2. calculate the squared length of z = (

i aiei and z = (cid:80)

(1, 0) and e2 = (      
3. if y = (cid:80)
2
2 ,
4. consider the basis e1 = (1, 0) and e2 = (      
2
2 ,

length of z is greater than the length of y? why or why not?

i biei, with 0 < ai < bi, is it necessarily true that the

(a) what is the representation of the vector (0,1) in the basis (e1, e2).

   
2
2 ).

   
2
2 ,

   
2
2 )?

(b) what is the representation of the vector (

(c) what is the representation of the vector (1, 2)?

36

e2

e2

e2

e1

e1

e1

exercise 2.37 generate 20 points uniformly at random on a 900-dimensional sphere of
radius 30. calculate the distance between each pair of points. then, select a method of
projection and project the data onto subspaces of dimension k=100, 50, 10, 5, 4, 3, 2, 1
k times the original distances and the new pair-wise
and calculate the di   erence between
distances. for each value of k what is the maximum di   erence as a percent of

   
k.

   

exercise 2.38 in d-dimensions there are exactly d-unit vectors that are pairwise orthog-
onal. however, if you wanted a set of vectors that were almost orthogonal you might
squeeze in a few more. for example, in 2-dimensions if almost orthogonal meant at least
45 degrees apart, you could    t in three almost orthogonal vectors. suppose you wanted to
   nd 1000 almost orthogonal vectors in 100 dimensions. here are two ways you could do
it:

1. begin with 1,000 orthonormal 1,000-dimensional vectors, and then project them to

a random 100-dimensional space.

2. generate 1000 100-dimensional random gaussian vectors.

implement both ideas and compare them to see which does a better job.

exercise 2.39 suppose there is an object moving at constant velocity along a straight
line. you receive the gps coordinates corrupted by gaussian noise every minute. how do
you estimate the current position?

exercise 2.40

1. what is the maximum size rectangle that can be    tted under a unit variance gaus-

sian?

2. what unit area rectangle best approximates a unit variance gaussian if one measure

goodness of    t by the symmetric di   erence of the gaussian and the rectangle.

37

exercise 2.41 let x1, x2, . . . , xn be independent samples of a random variable x with
mean    and variance   2. let ms = 1
xi be the sample mean. suppose one estimates
n

the variance using the sample mean rather than the true mean, that is,

n(cid:80)
n(cid:88)

i=1

1
n

i=1

  2
s =

(xi     ms)2

s ) = n   1

prove that e(  2

n   2 and thus one should have divided by n     1 rather than n.

hint: first calculate the variance of the sample mean and show that var(ms) = 1
then calculate e(  2

(cid:80)n
nvar(x).
i=1(xi   ms)2] by replacing xi   ms with (xi   m)   (ms   m).

s ) = e[ 1
n

exercise 2.42 generate ten values by a gaussian id203 distribution with zero mean
and variance one. what is the center determined by averaging the points? what is the
variance? in estimating the variance, use both the real center and the estimated center.
when using the estimated center to estimate the variance, use both n = 10 and n = 9.
how do the three estimates compare?

exercise 2.43 suppose you want to estimate the unknown center of a gaussian in d-
space which has variance one in each direction. show that o(log d/  2) random samples
from the gaussian are su   cient to get an estimate ms of the true center   , so that with
id203 at least 99%,

(cid:107)       ms(cid:107)          .

how many samples are su   cient to ensure that with id203 at least 99%

(cid:107)       ms(cid:107)2       ?

exercise 2.44 use the id203 distribution

e    1

2

(x   5)2

9

   
1
2  
3

to generate ten points.

(a) from the ten points estimate   . how close is the estimate of    to the true mean of

5?

(b) using the true mean of 5, estimate   2 by the formula   2 = 1
10

is the estimate of   2 to the true variance of 9?

10(cid:80)

i=1

(c) using your estimate m of the mean, estimate   2 by the formula   2 = 1
10

how close is the estimate of   2 to the true variance of 9?

(d) using your estimate m of the mean, estimate   2 by the formula   2 = 1
9

how close is the estimate of   2 to the true variance of 9?

38

(xi     5)2. how close

10(cid:80)
10(cid:80)

i=1

i=1

(xi     m)2.

(xi     m)2.

exercise 2.45 create a list of the    ve most important things that you learned about high
dimensions.

exercise 2.46 write a short essay whose purpose is to excite a college freshman to learn
about high dimensions.

39

3 best-fit subspaces and singular value decompo-

sition (svd)

3.1 introduction

in this chapter, we examine the singular value decomposition (svd) of a matrix.
consider each row of an n    d matrix a as a point in d-dimensional space. the singular
value decomposition    nds the best-   tting k-dimensional subspace for k = 1, 2, 3, . . . , for
the set of n data points. here,    best    means minimizing the sum of the squares of the
perpendicular distances of the points to the subspace, or equivalently, maximizing the
sum of squares of the lengths of the projections of the points onto this subspace.4 we
begin with a special case where the subspace is 1-dimensional, namely a line through the
origin. we then show that the best-   tting k-dimensional subspace can be found by k
applications of the best    tting line algorithm, where on the ith iteration we    nd the best
   t line perpendicular to the previous i     1 lines. when k reaches the rank of the matrix,
from these operations we get an exact decomposition of the matrix called the singular
value decomposition.

in matrix notation, the singular value decomposition of a matrix a with real entries
(we assume all our matrices have real entries) is the factorization of a into the product
of three matrices, a = u dv t , where the columns of u and v are orthonormal5 and the
matrix d is diagonal with positive real entries. the columns of v are the unit length vec-
tors de   ning the best    tting lines described above (the ith column being the unit-length
vector in the direction of the ith line). the coordinates of a row of u will be the fractions
of the corresponding row of a along the direction of each of the lines.

the svd is useful in many tasks. often a data matrix a is close to a low rank ma-
trix and it is useful to    nd a good low rank approximation to a. for any k, the singular
value decomposition of a gives the best rank-k approximation to a in a well-de   ned sense.

if ui and vi are columns of u and v respectively, then the matrix equation a = u dv t

can be rewritten as

a =

diiuivi

t .

(cid:88)

t is an n    d matrix with the
since ui is a n    1 matrix and vi is a d    1 matrix, uivi
same dimensions as a. the ith term in the above sum can be viewed as giving the compo-
nents of the rows of a along direction vi. when the terms are summed, they reconstruct a.

i

4this equivalence is due to the pythagorean theorem. for each point, its squared length (its distance
to the origin squared) is exactly equal to the squared length of its projection onto the subspace plus the
squared distance of the point to its projection; therefore, maximizing the sum of the former is equivalent
to minimizing the sum of the latter. for further discussion see section 3.2.

5a set of vectors is orthonormal if each is of length one and they are pairwise orthogonal.

40

this decomposition of a can be viewed as analogous to writing a vector x in some
orthonormal basis v1, v2, . . . , vd. the coordinates of x = (x    v1, x    v2 . . . , x    vd) are the
projections of x onto the vi   s. for svd, this basis has the property that for any k, the
   rst k vectors of this basis produce the least possible total sum of squares error for that
value of k.

in addition to the singular value decomposition, there is an eigenvalue decomposition.
let a be a square matrix. a vector v such that av =   v is called an eigenvector and
   the eigenvalue. when a is symmetric, the eigenvectors are orthogonal and a can be
expressed as a = v dv t where the eigenvectors are the columns of v and d is a diagonal
matrix with the corresponding eigenvalues on its diagonal. for a symmetric matrix a
the singular values and eigenvalues are identical. if the singular values are distinct, then
a   s singular vectors and eigenvectors are identical. if a singular value has multiplicity d
greater than one, the corresponding singular vectors span a subspace of dimension d and
any orthogonal basis of the subspace can be used as the eigenvectors or singular vectors.6

the singular value decomposition is de   ned for all matrices, whereas the more fa-
miliar eigenvector decomposition requires that the matrix a be square and certain other
conditions on the matrix to ensure orthogonality of the eigenvectors.
in contrast, the
columns of v in the singular value decomposition, called the right-singular vectors of a,
always form an orthogonal set with no assumptions on a. the columns of u are called
the left-singular vectors and they also form an orthogonal set (see section 3.6). a simple
consequence of the orthonormality is that for a square and invertible matrix a, the inverse
of a is v d   1u t .

eigenvalues and eignevectors satisfy av =   v. we will show that singular values and
vectors satisfy a somewhat analogous relationship. since avi is a n    1 matrix (vector),
the matrix a cannot act on it from the left. but at , which is a d    n matrix, can act on
this vector. indeed, we will show that

avi = diiui

and at ui = diivi.

in words, a acting on vi produces a scalar multiple of ui and at acting on ui produces
the same scalar multiple of vi. note that at avi = d2
iivi. the ith singular vector of a is
the ith eigenvector of the square symmetric matrix at a.

3.2 preliminaries

consider projecting a point ai = (ai1, ai2, . . . , aid) onto a line through the origin. then

a2
i1 + a2

i2 +        + a2

id = (length of projection)2 + (distance of point to line)2 .

6when d = 1 there are actually two possible singular vectors, one the negative of the other. the

subspace spanned is unique.

41

minimizing(cid:80)
alent to maximizing(cid:80)

dist2

i

i is equiv-
proj2
i

i

ai

disti

v

proji

figure 3.1: the projection of the point ai onto the line through the origin in the direction
of v.

this holds by the pythagorean theorem (see figure 3.1). thus

(distance of point to line)2 = a2

i1 + a2

i2 +        + a2

id     (length of projection)2 .

i2 +        + a2

(a2

i1 + a2

i=1

since

id) is a constant independent of the line, minimizing the sum
of the squares of the distances to the line is equivalent to maximizing the sum of the
squares of the lengths of the projections onto the line. similarly for best-   t subspaces,
maximizing the sum of the squared lengths of the projections onto the subspace minimizes
the sum of squared distances to the subspace.

n(cid:80)

thus we have two interpretations of the best-   t subspace. the    rst is that it minimizes
the sum of squared distances of the data points to it. this    rst interpretation and its use
are akin to the notion of least-squares    t from calculus.7 the second interpretation of
best-   t-subspace is that it maximizes the sum of projections squared of the data points
on it. this says that the subspace contains the maximum content of data among all
subspaces of the same dimension. the choice of the objective function as the sum of
squared distances seems a bit arbitrary and in a way it is. but the square has many nice
mathematical properties. the    rst of these, as we have just seen, is that minimizing the
sum of squared distances is equivalent to maximizing the sum of squared projections.

3.3 singular vectors

we now de   ne the singular vectors of an n    d matrix a. consider the rows of a as
n points in a d-dimensional space. consider the best    t line through the origin. let v
be a unit vector along this line. the length of the projection of ai, the ith row of a, onto
v is |ai    v|. from this we see that the sum of the squared lengths of the projections is
minimizing the vertical squared distances of the points to it, namely,(cid:80)n
7but there is a di   erence: here we take the perpendicular distance to the line or subspace, whereas,
in the calculus notion, given n pairs, (x1, y1), (x2, y2), . . . , (xn, yn), we    nd a line l = {(x, y)|y = mx + b}

i=1(yi     mxi     b)2.

42

|av|2. the best    t line is the one maximizing |av|2 and hence minimizing the sum of the
squared distances of the points to the line.

with this in mind, de   ne the    rst singular vector v1 of a as

v1 = arg max
|v|=1

|av|.

technically, there may be a tie for the vector attaining the maximum and so we should
not use the article    the   ; in fact,    v1 is always as good as v1. in this case, we arbitrarily
pick one of the vectors achieving the maximum and refer to it as    the    rst singular vector   
avoiding the more cumbersome    one of the vectors achieving the maximum   . we adopt
this terminology for all uses of arg max .

n(cid:80)

the value   1 (a) = |av1| is called the    rst singular value of a. note that   2
1 =
(ai    v1)2 is the sum of the squared lengths of the projections of the points onto the line

i=1
determined by v1.

if the data points were all either on a line or close to a line, intuitively, v1 should
give us the direction of that line.
it is possible that data points are not close to one
line, but lie close to a 2-dimensional subspace or more generally a low dimensional space.
suppose we have an algorithm for    nding v1 (we will describe one such algorithm later).
how do we use this to    nd the best-   t 2-dimensional plane or more generally the best    t
k-dimensional space?

the greedy approach begins by    nding v1 and then    nds the best 2-dimensional
subspace containing v1. the sum of squared distances helps. for every 2-dimensional
subspace containing v1, the sum of squared lengths of the projections onto the subspace
equals the sum of squared projections onto v1 plus the sum of squared projections along
a vector perpendicular to v1 in the subspace. thus, instead of looking for the best 2-
dimensional subspace containing v1, look for a unit vector v2 perpendicular to v1 that
maximizes |av|2 among all such unit vectors. using the same greedy strategy to    nd the
best three and higher dimensional subspaces, de   nes v3, v4, . . . in a similar manner. this
is captured in the following de   nitions. there is no apriori guarantee that the greedy
algorithm gives the best    t. but, in fact, the greedy algorithm does work and yields the
best-   t subspaces of every dimension as we will show.

the second singular vector , v2, is de   ned by the best    t line perpendicular to v1.

the value   2 (a) = |av2| is called the second singular value of a. the third singular

v2 = arg max

|av|

v   v1
|v|=1

43

vector v3 and the third singular value are de   ned similarly by

|av|

v3 = arg max
v   v1,v2
|v|=1

and

  3(a) = |av3|,

and so on. the process stops when we have found singular vectors v1, v2, . . . , vr, singular
values   1,   2, . . . ,   r, and

|av| = 0.

v   v1,v2,...,vr

max
|v|=1

the greedy algorithm found the v1 that maximized |av| and then the best    t 2-
dimensional subspace containing v1. is this necessarily the best-   t 2-dimensional sub-
space overall? the following theorem establishes that the greedy algorithm    nds the best
subspaces of every dimension.
theorem 3.1 (the greedy algorithm works) let a be an n  d matrix with singu-
lar vectors v1, v2, . . . , vr. for 1     k     r, let vk be the subspace spanned by v1, v2, . . . , vk.
for each k, vk is the best-   t k-dimensional subspace for a.

proof: the statement is obviously true for k = 1. for k = 2, let w be a best-   t 2-
dimensional subspace for a. for any orthonormal basis (w1, w2) of w , |aw1|2 + |aw2|2
is the sum of squared lengths of the projections of the rows of a onto w . choose an
orthonormal basis (w1, w2) of w so that w2 is perpendicular to v1. if v1 is perpendicular
to w , any unit vector in w will do as w2. if not, choose w2 to be the unit vector in w
perpendicular to the projection of v1 onto w. this makes w2 perpendicular to v1.8 since
v1 maximizes |av|2, it follows that |aw1|2     |av1|2. since v2 maximizes |av|2 over all
v perpendicular to v1, |aw2|2     |av2|2. thus

|aw1|2 + |aw2|2     |av1|2 + |av2|2.

hence, v2 is at least as good as w and so is a best-   t 2-dimensional subspace.

for general k, proceed by induction. by the induction hypothesis, vk   1 is a best-   t
k-1 dimensional subspace. suppose w is a best-   t k-dimensional subspace. choose an
orthonormal basis w1, w2, . . . , wk of w so that wk is perpendicular to v1, v2, . . . , vk   1.
then

|aw1|2 + |aw2|2 +        + |awk   1|2     |av1|2 + |av2|2 +        + |avk   1|2

since vk   1 is an optimal k     1 dimensional subspace. since wk is perpendicular to
v1, v2, . . . , vk   1, by the de   nition of vk, |awk|2     |avk|2. thus
|aw1|2 + |aw2|2 +        + |awk   1|2 + |awk|2     |av1|2 + |av2|2 +        + |avk   1|2 + |avk|2,
proving that vk is at least as good as w and hence is optimal.

8this can be seen by noting that v1 is the sum of two vectors that each are individually perpendicular

to w2, namely the projection of v1 to w and the portion of v1 orthogonal to w .

44

note that the n-dimensional vector avi is a list of lengths (with signs) of the projec-
tions of the rows of a onto vi. think of |avi| =   i(a) as the component of the matrix
a along vi. for this interpretation to make sense, it should be true that adding up the
squares of the components of a along each of the vi gives the square of the    whole content
of a   . this is indeed the case and is the matrix analogy of decomposing a vector into its
components along orthogonal directions.

r(cid:80)

consider one row, say aj, of a. since v1, v2, . . . , vr span the space of all rows of a,
(aj    vi)2 =

aj    v = 0 for all v perpendicular to v1, v2, . . . , vr. thus, for each row aj,
|aj|2. summing over all rows j,

i=1

r(cid:88)

n(cid:88)

r(cid:88)

r(cid:88)

(aj    vi)2 =

(aj    vi)2 =

|avi|2 =

  2
i (a).

i=1

i=1

j=1

i=1

i=1

r(cid:88)

n(cid:88)

|aj|2 =

j=1

|aj|2 =

n(cid:80)

n(cid:88)
d(cid:80)

j=1

j=1

k=1

n(cid:80)

j=1

but

a2
jk, the sum of squares of all the entries of a. thus, the sum of

squares of the singular values of a is indeed the square of the    whole content of a   , i.e.,
the sum of squares of all the entries. there is an important norm associated with this
quantity, the frobenius norm of a, denoted ||a||f de   ned as

(cid:115)(cid:88)

j,k

||a||f =

a2
jk.

of the frobenius norm. that is,(cid:80)   2

i (a) = ||a||2
f .

lemma 3.2 for any matrix a, the sum of squares of the singular values equals the square

proof: by the preceding discussion.

the vectors v1, v2, . . . , vr are called the right-singular vectors. the vectors avi form

a fundamental set of vectors and we normalize them to length one by

ui =

1

  i(a)

avi.

later we will show that ui similarly maximizes |ut a| over all u perpendicular to u1, . . . , ui   1.
these ui are called the left-singular vectors. clearly, the right-singular vectors are orthog-
onal by de   nition. we will show later that the left-singular vectors are also orthogonal.

3.4 singular value decomposition (svd)

let a be an n    d matrix with singular vectors v1, v2, . . . , vr and corresponding
avi where   iui is

singular values   1,   2, . . . ,   r. the left-singular vectors of a are ui = 1
  i

45

a vector whose coordinates correspond to the projections of the rows of a onto vi. each
  iuivt
is a rank one matrix whose rows are the    vi components    of the rows of a, i.e., the
i
projections of the rows of a in the vi direction. we will prove that a can be decomposed
into a sum of rank one matrices as

r(cid:88)

a =

  iuivt
i .

i=1

geometrically, each point is decomposed in a into its components along each of the r
orthogonal directions given by the vi. we will also prove this algebraically. we begin
with a simple lemma that two matrices a and b are identical if av = bv for all v.

lemma 3.3 matrices a and b are identical if and only if for all vectors v, av = bv.

proof: clearly, if a = b then av = bv for all v. for the converse, suppose that
av = bv for all v. let ei be the vector that is all zeros except for the ith component
which has value one. now aei is the ith column of a and thus a = b if for each i,
aei = bei.
theorem 3.4 let a be an n    d matrix with right-singular vectors v1, v2, . . . , vr, left-
singular vectors u1, u2, . . . , ur, and corresponding singular values   1,   2, . . . ,   r. then

proof: we    rst show that multiplying both a and

  iuivt

i by vj results in equality.

r(cid:88)

i=1

a =

  iuivt
i .

r(cid:80)

i=1

  iuivt

i vj =   juj = avj

r(cid:88)

i=1

r(cid:80)

i=1

since any vector v can be expressed as a linear combination of the singular vectors

plus a vector perpendicular to the vi, av =

  iuivt

i v for all v and by lemma 3.3,

a =

r(cid:80)
the decomposition a =(cid:80)

  iuivt
i .

i=1

i   iuivt
i

is called the singular value decomposition, svd,
of a. we can rewrite this equation in matrix notation as a = u dv t where ui is the ith
column of u , vt
is the ith row of v t , and d is a diagonal matrix with   i as the ith entry
i
on its diagonal. for any matrix a, the sequence of singular values is unique and if the
singular values are all distinct, then the sequence of singular vectors is unique up to signs.
however, when some set of singular values are equal, the corresponding singular vectors
span some subspace. any set of orthonormal vectors spanning this subspace can be used
as the singular vectors.

46

d
r    r

v t
r    d

a
n    d

u
n    r

=

figure 3.2: the svd decomposition of an n    d matrix.

3.5 best rank-k approximations

let a be an n    d matrix and think of the rows of a as n points in d-dimensional

space. let

a =

  iuivt
i

be the svd of a. for k     {1, 2, . . . , r}, let

r(cid:88)

i=1

k(cid:88)

ak =

  iuivt
i

i=1

be the sum truncated after k terms. it is clear that ak has rank k. we show that ak
is the best rank k approximation to a, where error is measured in the frobenius norm.
geometrically, this says that v1, . . . , vk de   ne the k-dimensional space minimizing the
sum of squared distances of the points to the space. to see why, we need the following
lemma.

lemma 3.5 the rows of ak are the projections of the rows of a onto the subspace vk
spanned by the    rst k singular vectors of a.

of the vector a onto vk is given by (cid:80)k
the projections of the rows of a onto vk is given by (cid:80)k

proof: let a be an arbitrary row vector. since the vi are orthonormal, the projection
t . thus, the matrix whose rows are
i . this last expression

i=1 avivt

simpli   es to

i=1 (a    vi)vi
k(cid:88)

k(cid:88)

avivi

t =

  iuivi

t = ak.

i=1

i=1

47

theorem 3.6 for any matrix b of rank at most k

(cid:107)a     ak(cid:107)f     (cid:107)a     b(cid:107)f
proof: let b minimize (cid:107)a     b(cid:107)2
f among all rank k or less matrices. let v be the space
spanned by the rows of b. the dimension of v is at most k. since b minimizes (cid:107)a     b(cid:107)2
f ,
it must be that each row of b is the projection of the corresponding row of a onto v :
otherwise replace the row of b with the projection of the corresponding row of a onto
v . this still keeps the row space of b contained in v and hence the rank of b is still at
most k. but it reduces (cid:107)a     b(cid:107)2

f , contradicting the minimality of ||a     b||f .

since each row of b is the projection of the corresponding row of a, it follows that
(cid:107)a     b(cid:107)2
f is the sum of squared distances of rows of a to v . since ak minimizes the
sum of squared distance of rows of a to any k-dimensional subspace, from theorem 3.1,
it follows that (cid:107)a     ak(cid:107)f     (cid:107)a     b(cid:107)f .

i=1   iuivi

a by ak = (cid:80)k

in addition to the frobenius norm, there is another matrix norm of interest. consider
an n    d matrix a and a large number of vectors where for each vector x we wish to
compute ax. it takes time o(nd) to compute each product ax but if we approximate
t and approximate ax by akx it requires only k dot products
of d-dimensional vectors, followed by a sum of k n-dimensional vectors, and takes time
o(kd + kn), which is a win provided k (cid:28) min(d, n). how is the error measured? since x
is unknown, the approximation needs to be good for every x. so we take the maximum
over all x of |(ak     a)x|. since this would be in   nite if |x| could grow without bound,
we restrict the maximum to |x|     1. formally, we de   ne a new norm of a matrix a by

||a||2 = max
|x|   1

|ax|.

this is called the 2-norm or the spectral norm. note that it equals   1(a).

as an application consider a large database of documents that form rows of an n    d
matrix a. there are d terms and each document is a d-dimensional vector with one
component for each term, which is the number of occurrences of the term in the document.
we are allowed to    preprocess    a. after the preprocessing, we receive queries. each
query x is an d-dimensional vector which speci   es how important each term is to the
query. the desired answer is an n-dimensional vector which gives the similarity (dot
product) of the query to each document in the database, namely ax, the    matrix-vector   
product. query time is to be much less than preprocessing time, since the idea is that we
need to answer many queries for the same database. there are many other applications
where one performs many matrix vector products with the same matrix. this technique
is applicable to these situations as well.

3.6 left singular vectors

the left singular vectors are also pairwise orthogonal. intuitively if ui and uj, i < j, were
not orthogonal, one would suspect that the right singular vector vj had a component of vi

48

which would contradict that vi and vj were orthogonal. let i be the smallest integer such
that ui is not orthogonal to all other uj. then to prove that ui and uj are orthogonal,
we add a small component of vj to vi, normalize the result to be a unit vector

v(cid:48)
i =

vi +  vj
|vi +  vj|

and show that |av(cid:48)
theorem 3.7 the left singular vectors are pairwise orthogonal.

i| > |avi|, a contradiction.

proof: let i be the smallest integer such that ui is not orthogonal to some other uj.
without loss of generality assume that ut
t uj < 0 then just replace ui
with    ui. clearly j > i since i was selected to be the smallest such index. for    > 0, let

i uj =    > 0. if ui

notice that v(cid:48)

i is a unit-length vector.
av(cid:48)

(cid:18)   iui +     juj

(cid:19)

   

1 +   2

ut
i

v(cid:48)
i =

vi +  vj
|vi +  vj|.

i =

  iui +     juj

   

1 +   2

(cid:16)

(cid:17)

has length at least as large as its component along ui which is

> (  i +     j  )

1       2

2

>   i       2

2   i +     j         3

2   j   >   i,

for su   ciently small  , a contradiction since vi +   vj is orthogonal to v1, v2, . . . , vi   1 since
j > i and   i is de   ned to be the maximum of |av| over such vectors.

next we prove that ak is the best rank k, 2-norm approximation to a. we    rst show
that the square of the 2-norm of a     ak is the square of the (k + 1)st singular value of a.
this is essentially by de   nition of ak; that is, ak represents the projections of the rows in
a onto the space spanned by the top k singular vectors, and so a     ak is the remaining
portion of those rows, whose top singular value will be   k+1.
lemma 3.8 (cid:107)a     ak(cid:107)2

2 =   2

k+1.

k(cid:80)

proof: let a =
and a     ak =

  iuivi

  iuivi

t be the singular value decomposition of a. then ak =
  iuivi
t . let v be the top singular vector of a     ak. express v as a

i=1

t

linear combination of v1, v2, . . . , vr. that is, write v =

cjvj. then

r(cid:80)
r(cid:80)

i=1

i=k+1

r(cid:80)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
r(cid:88)

j=1

i=k+1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ci  iuivi

t vi

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

r(cid:88)
r(cid:88)

i=k+1

i=k+1

t

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =
r(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) r(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =

cjvj

i=k+1

j=1

  iuivi

ci  iui

i   2
c2
i ,

|(a     ak)v| =

=

49

r(cid:80)

since the ui are orthonormal. the v maximizing this last quantity, subject to the con-
straint that |v|2 =
c2
i = 1, occurs when ck+1 = 1 and the rest of the ci are zero. thus,
(cid:107)a     ak(cid:107)2

k+1 proving the lemma.

2 =   2

i=1

finally, we prove that ak is the best rank k, 2-norm approximation to a:
theorem 3.9 let a be an n    d matrix. for any matrix b of rank at most k

(cid:107)a     ak(cid:107)2     (cid:107)a     b(cid:107)2 .

proof: if a is of rank k or less, the theorem is obviously true since (cid:107)a     ak(cid:107)2 = 0.
assume that a is of rank greater than k. by lemma 3.8, (cid:107)a     ak(cid:107)2
k+1. the null
space of b, the set of vectors v such that bv = 0, has dimension at least d     k. let
v1, v2, . . . , vk+1 be the    rst k + 1 singular vectors of a. by a dimension argument, it
follows that there exists a z (cid:54)= 0 in

2 =   2

null (b)     span{v1, v2, . . . , vk+1} .

scale z to be of length one.

since bz = 0,

(cid:107)a     b(cid:107)2

2     |(a     b) z|2 .

(cid:107)a     b(cid:107)2

n(cid:88)

(cid:0)vi

t z(cid:1)2

2     |az|2 .
k+1(cid:88)

(cid:0)vi

  2
i

=

i=1

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

since z is in the span{v1, v2, . . . , vk+1}

|az|2 =

  iuivi

t z

=

  2
i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

t z(cid:1)2       2

k+1

(cid:0)vi

t z(cid:1)2

k+1(cid:88)

i=1

=   2

k+1.

it follows that (cid:107)a     b(cid:107)2

2       2

k+1 proving the theorem.

for a square symmetric matrix a and eigenvector v, av =   v. we now prove the

analog for singular values and vectors we discussed in the introduction.

lemma 3.10 (analog of eigenvalues and eigenvectors)

second, note that from the svd, we get at ui = (cid:80)

proof: the    rst equation follows from the de   nition of left singular vectors. for the
t ui, where since the uj are

j   jvjuj

avi =   iui and at ui =   ivi.

orthonormal, all terms in the summation are zero except for j = i.

50

3.7 power method for singular value decomposition

computing the singular value decomposition is an important branch of numerical
analysis in which there have been many sophisticated developments over a long period of
time. the reader is referred to numerical analysis texts for more details. here we present
an    in-principle    method to establish that the approximate svd of a matrix a can be
computed in polynomial time. the method we present, called the power method, is simple
and is in fact the conceptual starting point for many algorithms. let a be a matrix whose
t . we wish to work with a matrix that is square and symmetric. let
b = at a. by direct multiplication, using the orthogonality of the ui   s that was proved
in theorem 3.7,

svd is (cid:80)

i   iuivi

(cid:32)(cid:88)

(cid:33)

(cid:33)(cid:32)(cid:88)
(cid:88)

j

j =

  jujvt
j

i vivt
  2
i .

b = at a =

(cid:88)

=

  i  jvi(ut

i

  iviut
i
i    uj)vt

i,j

i

in particular, bvj = ((cid:80)
tors, namely a =(cid:80)

the matrix b is square and symmetric, and has the same left and right-singular vectors.
j vj, so vj is an eigenvector of b with eigenvalue
  2
j . if a is itself square and symmetric, it will have the same right and left-singular vec-

i )vj =   2

i vivt

i   2

  ivivi

t and computing b is unnecessary.

now consider computing b2.

i

(cid:32)(cid:88)

(cid:33)(cid:32)(cid:88)

(cid:33)

  2
j vjvt
j

=

(cid:88)

ij

  2
i   2

j vi(vi

t vj)vj

t

b2 =

  2
i vivt
i

i

j

when i (cid:54)= j, the dot product vi
computing the kth power of b, all the cross product terms are zero and

t vj is zero by orthogonality.9 thus, b2 =

r(cid:88)

bk =

  2k
i vivi

t .

r(cid:80)

i=1

  4
i vivi

t . in

if   1 >   2, then the    rst term in the summation dominates, so bk       2k
t . this
means a close estimate to v1 can be computed by simply taking the    rst column of bk
and normalizing it to a unit vector.

1 v1v1

i=1

3.7.1 a faster method

a problem with the above method is that a may be a very large, sparse matrix, say a
108    108 matrix with 109 nonzero entries. sparse matrices are often represented by just

9the    outer product    vivj

t is a matrix and is not zero even for i (cid:54)= j.

51

a list of nonzero entries, say a list of triples of the form (i, j, aij). though a is sparse, b
need not be and in the worse case may have all 1016 entries nonzero10 and it is then impos-
sible to even write down b, let alone compute the product b2. even if a is moderate in
size, computing matrix products is costly in time. thus, a more e   cient method is needed.

instead of computing bk, select a random vector x and compute the product bkx.
the vector x can be expressed in terms of the singular vectors of b augmented to a full

d(cid:80)

orthonormal basis as x =

civi. then

i=1

bkx     (  2k

1 v1v1

t )

(cid:17)

civi

(cid:16) d(cid:88)

i=1

=   2k

1 c1v1.

normalizing the resulting vector yields v1, the    rst singular vector of a. the way bkx
is computed is by a series of matrix vector products, instead of matrix products. bkx =
at a . . . at ax, which can be computed right-to-left. this consists of 2k vector times
sparse id127s.

to compute k singular vectors, one selects a random vector r and    nds an orthonormal
basis for the space spanned by r, ar, . . . , ak   1r. then compute a times each of the basis
vectors, and    nd an orthonormal basis for the space spanned by the resulting vectors.
intuitively, one has applied a to a subspace rather than a single vector. one repeat-
edly applies a to the subspace, calculating an orthonormal basis after each application
to prevent the subspace collapsing to the one dimensional subspace spanned by the    rst
singular vector. the process quickly converges to the    rst k singular vectors.

an issue occurs if there is no signi   cant gap between the    rst and second singular
values of a matrix. take for example the case when there is a tie for the    rst singular
vector and   1 =   2. then, the above argument fails. we will overcome this hurdle.
theorem 3.11 below states that even with ties, the power method converges to some
vector in the span of those singular vectors corresponding to the    nearly highest    singular
values. the theorem assumes it is given a vector x which has a component of magnitude
at least    along the    rst right singular vector v1 of a. we will see in lemma 3.12 that a
random vector satis   es this condition with fairly high id203.
theorem 3.11 let a be an n  d matrix and x a unit length vector in rd with |xt v1|       ,
where    > 0. let v be the space spanned by the right singular vectors of a corresponding
to singular values greater than (1       )   1. let w be the unit vector after k = ln(1/    )
iterations of the power method, namely,

2  

(cid:0)at a(cid:1)k x
(cid:12)(cid:12)(cid:12)(at a)k x
(cid:12)(cid:12)(cid:12).

w =

10e.g., suppose each entry in the    rst row of a is nonzero and the rest of a is zero.

52

r(cid:88)

i=1

d(cid:88)

then w has a component of at most    perpendicular to v .

proof: let

a =

  iuivt
i

be the svd of a.
if the rank of a is less than d, then for convenience complete
{v1, v2, . . . vr} into an orthonormal basis {v1, v2, . . . vd} of d-space. write x in the basis
of the vi   s as

d(cid:80)

i=1

since (at a)k =
|c1|       .

x =

civi.

i=1

i vivt
  2k
i ,

it follows that (at a)kx =

d(cid:80)

i=1

  2k
i civi. by hypothesis,

suppose that   1,   2, . . . ,   m are the singular values of a that are greater than or equal
to (1       )   1 and that   m+1, . . . ,   d are the singular values that are less than (1       )   1.
now

|(at a)kx|2 =

i       4k
1 c2
the component of |(at a)kx|2 perpendicular to the space v is

  2k
i civi

  4k
i c2

i=1

i=1

=

1       4k

1   2.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

d(cid:88)

d(cid:88)

  4k
i c2

i     (1       )4k   4k

1

i     (1       )4k   4k
c2

1

i=m+1

i=m+1

i = |x| = 1. thus, the component of w perpendicular to v has squared

i=1 c2

d(cid:88)

since (cid:80)d

length at most (1     )4k  4k

1

  4k
1   2

and so its length is at most
(1       )2k  2k

(1       )2k

1

=

    2k
1

  

    e   2k  

  

=   

.

2 

since k = ln(1/   
lemma 3.12 let y     rn be a random vector with the unit variance spherical gaussian
as its id203 density. normalize y to be a unit length vector by setting x = y/|y|. let
v be any unit length vector. then

(cid:18)

prob

   
|xt v|     1
20

d

    1
10

+ 3e   d/96.

(cid:19)

53

(cid:16)|xt v|     1

(cid:17)     1

10 + 3e   d/96 is
   
proof: proving for the unit length vector x that prob
   
equivalent to proving for the unnormalized vector y that prob(|y|     2
d)     3e   d/96 and
   
10)     1/10. that prob(|y|     2
prob(|yt v|     1
d) is at most 3e   d/96 follows from theorem
d substituted for   . the id203 that |yt v|     1
10 is at most 1/10 follows
(2.9) with
   
from the fact that yt v is a random, zero mean, unit variance gaussian with density is at
2       1/2 in the interval [   1/10, 1/10], so the integral of the gaussian over the
most 1/
interval is at most 1/10.

   

20

d

3.8 singular vectors and eigenvectors

for a square matrix b, if bx =   x, then x is an eigenvector of b and    is the corre-
sponding eigenvalue. we saw in section 3.7, if b = at a, then the right singular vectors
vj of a are eigenvectors of b with eigenvalues   2
j . the same argument shows that the left
singular vectors uj of a are eigenvectors of aat with eigenvalues   2
j .

because b = (cid:80)

i   2

i vivi

t and for any x, xt vivi

the matrix b = at a has the property that for any vector x, xt bx     0. this is
t x = (xt vi)2     0. a matrix b with
the property that xt bx     0 for all x is called positive semi-de   nite. every matrix of
the form at a is positive semi-de   nite. in the other direction, any positive semi-de   nite
matrix b can be decomposed into a product at a, and so its eigenvalue decomposition
can be obtained from the singular value decomposition of a. the interested reader should
consult a id202 book.

3.9 applications of singular value decomposition

3.9.1 centering data

singular value decomposition is used in many applications and for some of these ap-
plications it is essential to    rst center the data by subtracting the centroid of the data
from each data point.11 if you are interested in the statistics of the data and how it varies
in relationship to its mean, then you would center the data. on the other hand, if you
are interested in    nding the best low rank approximation to a matrix, then you do not
center the data. the issue is whether you are    nding the best    tting subspace or the best
   tting a   ne space. in the latter case you    rst center the data and then    nd the best
   tting subspace. see figure 3.3.

we    rst show that the line minimizing the sum of squared distances to a set of points,
if not restricted to go through the origin, must pass through the centroid of the points.
this implies that if the centroid is subtracted from each data point, such a line will pass
through the origin. the best    t line can be generalized to k dimensional    planes   . the
operation of subtracting the centroid from all data points is useful in other contexts as
well. we give it the name    centering data   .

11the centroid of a set of points is the coordinate-wise average of the points.

54

figure 3.3: if one wants statistical information relative to the mean of the data, one
needs to center the data. if one wants the best low rank approximation, one would not
center the data.

lemma 3.13 the best-   t line (minimizing the sum of perpendicular distances squared)
of a set of data points must pass through the centroid of the points.

proof: subtract the centroid from each data point so that the centroid is 0. after
centering the data let (cid:96) be the best-   t line and assume for contradiction that (cid:96) does
not pass through the origin. the line (cid:96) can be written as {a +   v|       r}, where a is
the closest point to 0 on (cid:96) and v is a unit length vector in the direction of (cid:96), which is
perpendicular to a. for a data point ai, let dist(ai, (cid:96)) denote its perpendicular distance to
(cid:96). by the pythagorean theorem, we have |ai     a|2 = dist(ai, (cid:96))2 + (v   ai)2, or equivalently,
dist(ai, (cid:96))2 = |ai     a|2     (v    ai)2. summing over all data points:

(cid:0)|ai|2 + |a|2     2ai    a     (v    ai)2(cid:1)
(cid:88)
where we used the fact that since the centroid is 0,(cid:80)

(cid:0)|ai     a|2     (v    ai)2(cid:1) =
(cid:32)(cid:88)
    n(cid:88)

i ai = 0. the above expression is
minimized when a = 0, so the line (cid:96)(cid:48) = {  v :        r} through the origin is a better    t
than (cid:96), contradicting (cid:96) being the best-   t line.

|ai|2 + n|a|2    (cid:88)

n(cid:88)
n(cid:88)

|ai|2 + n|a|2     2a   

(v    ai)2 =

dist(ai, (cid:96))2 =

(v    ai)2,

n(cid:88)

n(cid:88)

(cid:33)

ai

i=1

i=1

i=1

i=1

i=1

=

i

i

i

a statement analogous to lemma 3.13 holds for higher dimensional objects. de   ne
an a   ne space as a subspace translated by a vector. so an a   ne space is a set of the
form

k(cid:88)

{v0 +

civi|c1, c2, . . . , ck     r}.

i=1

here, v0 is the translation and v1, v2, . . . , vk form an orthonormal basis for the subspace.

lemma 3.14 the k dimensional a   ne space which minimizes the sum of squared per-
pendicular distances to the data points must pass through the centroid of the points.

55

instead of (v    ai)2, we will now have(cid:80)k

proof: we only give a brief idea of the proof, which is similar to the previous lemma.
j=1(vj    ai)2, where the vj, j = 1, 2, . . . , k are an

orthonormal basis of the subspace through the origin parallel to the a   ne space.

3.9.2 principal component analysis

the traditional use of svd is in principal component analysis (pca). pca is il-
lustrated by a movie recommendation setting where there are n customers and d movies.
let matrix a with elements aij represent the amount that customer i likes movie j. one
hypothesizes that there are only k underlying basic factors that determine how much a
given customer will like a given movie, where k is much smaller than n or d. for example,
these could be the amount of comedy, drama, and action, the novelty of the story, etc.
each movie can be described as a k-dimensional vector indicating how much of these ba-
sic factors the movie has, and each customer can be described as a k-dimensional vector
indicating how important each of these basic factors is to that customer. the dot-product
of these two vectors is hypothesized to determine how much that customer will like that
movie. in particular, this means that the n   d matrix a can be expressed as the product
of an n    k matrix u describing the customers and a k    d matrix v describing the
movies. finding the best rank k approximation ak by svd gives such a u and v . one
twist is that a may not be exactly equal to u v , in which case a     u v is treated as
noise. another issue is that svd gives a factorization with negative entries. nonnegative
id105 (nmf) is more appropriate in some contexts where we want to keep
entries nonnegative. nmf is discussed in chapter 9

in the above setting, a was available fully and we wished to    nd u and v to identify
the basic factors. however, in a case such as movie recommendations, each customer may
have seen only a small fraction of the movies, so it may be more natural to assume that we
are given just a few elements of a and wish to estimate a. if a was an arbitrary matrix
of size n    d, this would require    (nd) pieces of information and cannot be done with a
few entries. but again hypothesize that a was a small rank matrix with added noise. if
now we also assume that the given entries are randomly drawn according to some known
distribution, then there is a possibility that svd can be used to estimate the whole of a.
this area is called collaborative    ltering and one of its uses is to recommend movies or to
target an ad to a customer based on one or two purchases. we do not describe it here.

3.9.3 id91 a mixture of spherical gaussians

id91 is the task of partitioning a set of points into k subsets or clusters where
each cluster consists of nearby points. di   erent de   nitions of the quality of a id91
lead to di   erent solutions. id91 is an important area which we will study in detail
in chapter 7. here we will see how to solve a particular id91 problem using singular
value decomposition.

56

                                          

factors

u

                                          

=

                                          

                                          

      

a

customers

      

movies

v

figure 3.4: customer-movie data

mathematical formulations of id91 tend to have the property that    nding the
highest quality solution to a given set of data is np-hard. one way around this is to
assume stochastic models of input data and devise algorithms to cluster data generated by
such models. mixture models are a very important class of stochastic models. a mixture
is a id203 density or distribution that is the weighted sum of simple component
id203 densities. it is of the form

f = w1p1 + w2p2 +        + wkpk,

where p1, p2, . . . , pk are the basic id203 densities and w1, w2, . . . , wk are positive real
numbers called mixture weights that add up to one. clearly, f is a id203 density
and integrates to one.

the model    tting problem is to    t a mixture of k basic densities to n independent,
identically distributed samples, each sample drawn according to the same mixture dis-
tribution f . the class of basic densities is known, but various parameters such as their
means and the component weights of the mixture are not. here, we deal with the case
where the basic densities are all spherical gaussians. there are two equivalent ways of
thinking of the hidden sample generation process when only the samples are given:

1. pick each sample according to the density f on rd.
2. pick a random i from {1, 2, . . . , k} where id203 of picking i is wi. then, pick

a sample according to the density pi.

one approach to the model-   tting problem is to break it into two subproblems:

1. first, cluster the set of samples into k clusters c1, c2, . . . , ck, where ci is the set of
samples generated according to pi (see (2) above) by the hidden generation process.

2. then    t a single gaussian distribution to each cluster of sample points.

57

the second problem is relatively easier and indeed we saw the solution in chapter
2, where we showed that taking the empirical mean (the mean of the sample) and the
empirical standard deviation gives us the best-   t gaussian. the    rst problem is harder
and this is what we discuss here.

if the component gaussians in the mixture have their centers very close together, then
the id91 problem is unresolvable. in the limiting case where a pair of component
densities are the same, there is no way to distinguish between them. what condition on
the inter-center separation will guarantee unambiguous id91? first, by looking at
1-dimensional examples, it is clear that this separation should be measured in units of the
standard deviation, since the density is a function of the number of standard deviation
from the mean. in one dimension, if two gaussians have inter-center separation at least
six times the maximum of their standard deviations, then they hardly overlap. this is
summarized in the question: how many standard deviations apart are the means? in one
dimension, if the answer is at least six, we can easily tell the gaussians apart. what is
the analog of this in higher dimensions?

we discussed in chapter 2 distances between two sample points from the same gaus-
sian as well the distance between two sample points from two di   erent gaussians. recall
from that discussion that if

    if x and y are two independent samples from the same spherical gaussian with

standard deviation12    then

    if x and y are samples from di   erent spherical gaussians each of standard deviation

   and means separated by distance    , then

|x     y|2     2(cid:0)   
|x     y|2     2(cid:0)   
2(cid:0)   
d     o(1)(cid:1)2  2 +    2 > 2(cid:0)   

d    o(1)(cid:1)2  2.
d    o(1)(cid:1)2  2 +    2.
d + o(1)(cid:1)2  2.

to ensure that points from the same gaussian are closer to each other than points from
di   erent gaussians, we need

expanding the squares, the high order term 2d cancels and we need that

    > cd1/4,

for some constant c. while this was not a completely rigorous argument, it can be used to
show that a distance based id91 approach (see chapter 2 for an example) requires an

12since a spherical gaussian has the same standard deviation in every direction, we call it the standard

deviation of the gaussian.

58

inter-mean separation of at least cd1/4 standard deviations to succeed, thus unfortunately
not keeping with mnemonic of a constant number of standard deviations separation of
the means. here, indeed, we will show that    (1) standard deviations su   ce provided the
number k of gaussians is o(1).

the central idea is the following. suppose we can    nd the subspace spanned by the
k centers and project the sample points to this subspace. the projection of a spherical
gaussian with standard deviation    remains a spherical gaussian with standard deviation
   (lemma 3.15). in the projection, the inter-center separation remains the same. so in
the projection, the gaussians are distinct provided the inter-center separation in the whole
space is at least ck1/4    which is less than cd1/4    for k (cid:28) d. interestingly, we will see that
the subspace spanned by the k-centers is essentially the best-   t k-dimensional subspace
that can be found by singular value decomposition.

lemma 3.15 suppose p is a d-dimensional spherical gaussian with center    and stan-
dard deviation   . the density of p projected onto a k-dimensional subspace v is a spherical
gaussian with the same standard deviation.

proof: rotate the coordinate system so v is spanned by the    rst k coordinate vectors.
the gaussian remains spherical with standard deviation    although the coordinates of
its center have changed. for a point x = (x1, x2, . . . , xd), we will use the notation x(cid:48) =
(x1, x2, . . . xk) and x(cid:48)(cid:48) = (xk+1, xk+2, . . . , xn). the density of the projected gaussian at
the point (x1, x2, . . . , xk) is

(cid:90)

   |x(cid:48)     (cid:48)|2

2  2

ce

   |x(cid:48)(cid:48)     (cid:48)(cid:48)|2
e

2  2

dx(cid:48)(cid:48) = c(cid:48)e

   |x(cid:48)     (cid:48)|2

2  2

.

this implies the lemma.

x(cid:48)(cid:48)

we now show that the top k singular vectors produced by the svd span the space of
the k centers. first, we extend the notion of best    t to id203 distributions. then
we show that for a single spherical gaussian whose center is not the origin, the best    t
1-dimensional subspace is the line though the center of the gaussian and the origin. next,
we show that the best    t k-dimensional subspace for a single gaussian whose center is not
the origin is any k-dimensional subspace containing the line through the gaussian   s center
and the origin. finally, for k spherical gaussians, the best    t k-dimensional subspace is
the subspace containing their centers. thus, the svd    nds the subspace that contains
the centers.

recall that for a set of points, the best-   t line is the line passing through the origin
that maximizes the sum of squared lengths of the projections of the points onto the line.
we extend this de   nition to id203 densities instead of a set of points.

59

1. the best    t 1-dimension subspace
to a spherical gaussian is the line
through its center and the origin.

2. any k-dimensional subspace contain-
ing the line is a best    t k-dimensional
subspace for the gaussian.

3. the best    t k-dimensional subspace
for k spherical gaussians is the sub-
space containing their centers.

figure 3.5: best    t subspace to a spherical gaussian.

de   nition 3.1 if p is a id203 density in d space, the best    t line for p is the line in
the v1 direction where

(cid:2)(vt x)2(cid:3) .

v1 = arg max
|v|=1

e
x   p

for a spherical gaussian centered at the origin, it is easy to see that any line passing
through the origin is a best    t line. our next lemma shows that the best    t line for a
spherical gaussian centered at    (cid:54)= 0 is the line passing through    and the origin.
lemma 3.16 let the id203 density p be a spherical gaussian with center    (cid:54)= 0.
the unique best    t 1-dimensional subspace is the line passing through    and the origin.
if    = 0, then any line through the origin is a best-   t line.

proof: for a randomly chosen x (according to p) and a    xed unit length vector v,

(cid:2)(vt x)2(cid:3) = e

e
x   p

(cid:104)(cid:0)vt (x       ) + vt   (cid:1)2(cid:105)
(cid:104)(cid:0)vt (x       )(cid:1)2
(cid:104)(cid:0)vt (x       )(cid:1)2(cid:105)
(cid:104)(cid:0)vt (x       )(cid:1)2(cid:105)

+ 2(cid:0)vt   (cid:1)(cid:0)vt (x       )(cid:1) +(cid:0)vt   (cid:1)2(cid:105)
+ 2(cid:0)vt   (cid:1) e(cid:2)vt (x       )(cid:3) +(cid:0)vt   (cid:1)2
+(cid:0)vt   (cid:1)2

x   p

= e
x   p

= e
x   p

= e
x   p

=   2 +(cid:0)vt   (cid:1)2

60

line v maximizes ex   p[(vt x)2] and therefore maximizes(cid:0)vt   (cid:1)2. this is maximized when

where the fourth line follows from the fact that e[vt (x       )] = 0, and the    fth line
follows from the fact that e[(vt (x       ))2] is the variance in the direction v. the best    t
v is aligned with the center   . to see uniqueness, just note that if    (cid:54)= 0, then vt    is
strictly less when v is not aligned with the center.

we now extend de   nition 3.1 to k-dimensional subspaces.

de   nition 3.2 if p is a id203 density in d-space then the best-   t k-dimensional
subspace vk is

(cid:0)|proj(x, v )|2(cid:1) ,

vk = argmax

v

dim(v )=k

e
x   p

where proj(x, v ) is the orthogonal projection of x onto v .

lemma 3.17 for a spherical gaussian with center   , a k-dimensional subspace is a best
   t subspace if and only if it contains   .

proof: if    = 0, then by symmetry any k-dimensional subspace is a best-   t subspace. if
   (cid:54)= 0, then, the best-   t line must pass through    by lemma 3.16. now, as in the greedy
algorithm for    nding subsequent singular vectors, we would project perpendicular to the
   rst singular vector. but after the projection, the mean of the gaussian becomes 0 and
any vectors will do as subsequent best-   t directions.

this leads to the following theorem.

theorem 3.18 if p is a mixture of k spherical gaussians, then the best    t k-dimensional
subspace contains the centers. in particular, if the means of the gaussians are linearly
independent, the space spanned by them is the unique best-   t k dimensional subspace.
proof: let p be the mixture w1p1+w2p2+      +wkpk. let v be any subspace of dimension
k or less. then,

(cid:0)|proj(x, v )|2(cid:1) =

e
x   p

k(cid:88)

i=1

wi e
x   pi

(cid:0)|proj(x, v )|2(cid:1)

if v contains the centers of the densities pi, by lemma 3.17, each term in the summation
is individually maximized, which implies the entire summation is maximized, proving the
theorem.

for an in   nite set of points drawn according to the mixture, the k-dimensional svd
subspace gives exactly the space of the centers. in reality, we have only a large number
of samples drawn according to the mixture. however, it is intuitively clear that as the
number of samples increases, the set of sample points will approximate the id203
density and so the svd subspace of the sample will be close to the space spanned by
the centers. the details of how close it gets as a function of the number of samples are
technical and we do not carry this out here.

61

3.9.4 ranking documents and web pages

an important task for a document collection is to rank the documents according to
their intrinsic relevance to the collection. a good candidate de   nition of    intrinsic rele-
vance    is a document   s projection onto the best-   t direction for that collection, namely the
top left-singular vector of the term-document matrix. an intuitive reason for this is that
this direction has the maximum sum of squared projections of the collection and so can be
thought of as a synthetic term-document vector best representing the document collection.

ranking in order of the projection of each document   s term vector along the best    t
direction has a nice interpretation in terms of the power method. for this, we consider
a di   erent example, that of the web with hypertext links. the world wide web can
be represented by a directed graph whose nodes correspond to web pages and directed
edges to hypertext links between pages. some web pages, called authorities, are the most
prominent sources for information on a given topic. other pages called hubs, are ones
that identify the authorities on a topic. authority pages are pointed to by many hub
pages and hub pages point to many authorities. one is led to what seems like a circular
de   nition: a hub is a page that points to many authorities and an authority is a page
that is pointed to by many hubs.

one would like to assign hub weights and authority weights to each node of the web.
if there are n nodes, the hub weights form an n-dimensional vector u and the authority
weights form an n-dimensional vector v. suppose a is the adjacency matrix representing
the directed graph. here aij is 1 if there is a hypertext link from page i to page j and 0
otherwise. given hub vector u, the authority vector v could be computed by the formula

vj     d(cid:88)

uiaij

i=1

since the right hand side is the sum of the hub weights of all the nodes that point to node
j. in matrix terms,

v = at u/|at u|.

similarly, given an authority vector v, the hub vector u could be computed by
u = av/|av|. of course, at the start, we have neither vector. but the above discus-
sion suggests a power iteration. start with any v. set u = av, then set v = at u, then
renormalize and repeat the process. we know from the power method that this converges
to the left and right-singular vectors. so after su   ciently many iterations, we may use the
left vector u as the hub weights vector and project each column of a onto this direction
and rank columns (authorities) in order of this projection. but the projections just form
the vector at u which equals a multiple of v. so we can just rank by order of the vj.
this is the basis of an algorithm called the hits algorithm, which was one of the early
proposals for ranking web pages.

62

a di   erent ranking called id95 is widely used. it is based on a random walk on

the graph described above. we will study id93 in detail in chapter 4.

3.9.5 an application of svd to a discrete optimization problem

in id91 a mixture of gaussians, svd was used as a dimension reduction tech-
nique. it found a k-dimensional subspace (the space of centers) of a d-dimensional space
and made the gaussian id91 problem easier by projecting the data to the subspace.
here, instead of    tting a model to data, we consider an optimization problem where ap-
plying dimension reduction makes the problem easier. the use of svd to solve discrete
optimization problems is a relatively new subject with many applications. we start with
an important np-hard problem, the maximum cut problem for a directed graph g(v, e).

the maximum cut problem is to partition the nodes of an n-node directed graph into
two subsets s and   s so that the number of edges from s to   s is maximized. let a be
the adjacency matrix of the graph. with each vertex i, associate an indicator variable xi.
the variable xi will be set to 1 for i     s and 0 for i       s. the vector x = (x1, x2, . . . , xn)
is unknown and we are trying to    nd it or equivalently the cut, so as to maximize the
number of edges across the cut. the number of edges across the cut is precisely

(cid:88)

i,j

xi(1     xj)aij.

thus, the maximum cut problem can be posed as the optimization problem

maximize(cid:80)
(cid:88)

i,j

xi(1     xj)aij

subject to xi     {0, 1}.

xi(1     xj)aij = xt a(1     x),

in matrix notation,

i,j

where 1 denotes the vector of all 1   s . so, the problem can be restated as

maximize xt a(1     x)

subject to xi     {0, 1}.

(3.1)

this problem is np-hard. however we will see that for dense graphs, that is, graphs
with    (n2) edges and therefore whose optimal solution has size    (n2),13 we can use the
svd to    nd a near optimal solution in polynomial time. to do so we will begin by

computing the svd of a and replacing a by ak =(cid:80)k

t in (3.1) to get

i=1   iuivi

maximize xt ak(1     x)

subject to xi     {0, 1}.

(3.2)

note that the matrix ak is no longer a 0-1 adjacency matrix.

we will show that:

13any graph of m edges has a cut of size at least m/2. this can be seen by noting that the expected

size of the cut for a random x     {0, 1}n is exactly m/2.

63

1. for each 0-1 vector x, xt ak(1     x) and xt a(1     x) di   er by at most

the maxima in (3.1) and (3.2) di   er by at most this amount.

n2   

k+1

. thus,

2. a near optimal x for (3.2) can be found in time no(k) by exploiting the low rank
of ak, which is polynomial time for constant k. by item 1 this is near optimal for
(3.1) where near optimal means with additive error of at most

n2   

.

k+1

first, we prove item 1. since x and 1     x are 0-1 n-vectors, each has length at most
n. by the de   nition of the 2-norm, |(a     ak)(1     x)|        
   
n||a     ak||2. now since
xt (a     ak)(1     x) is the dot product of the vector x with the vector (a     ak)(1     x),

|xt (a     ak)(1     x)|     n||a     ak||2.

by lemma 3.8, ||a     ak||2 =   k+1(a). the inequalities,
k+1     ||a||2

2 +          2

k+1       2

(k + 1)  2

1 +   2

f =

(cid:88)

i,j

ij     n2
a2

imply that   2

k+1     n2

k+1 and hence ||a     ak||2     n   

k+1

proving item 1.

next we focus on item 2. it is instructive to look at the special case when k=1 and a
is approximated by the rank one matrix a1. an even more special case when the left and
right-singular vectors u and v are identical is already np-hard to solve exactly because
it subsumes the problem of whether for a set of n integers, {a1, a2, . . . , an}, there is a
partition into two subsets whose sums are equal. however, for that problem, there is
an e   cient id145 algorithm that    nds a near-optimal solution. we will
build on that idea for the general rank k problem.

for item 2, we want to maximize (cid:80)k
dinates of the vector ui corresponding to elements in the set s, that is, ui(s) =(cid:80)
and similarly for vi. we will    nd s to maximize(cid:80)k

t (1     x)) over 0-1 vectors x. a
piece of notation will be useful. for any s     {1, 2, . . . n}, write ui(s) for the sum of coor-
j   s uij,
i=1   iui(s)vi(   s) using dynamic pro-

i=1   i(xt ui)(vi

gramming.

for a subset s of {1, 2, . . . , n}, de   ne the 2k-dimensional vector

w(s) =(cid:0)u1(s), v1(   s), u2(s), v2(   s), . . . , uk(s), vk(   s)(cid:1).

if we had the list of all such vectors, we could    nd (cid:80)k

i=1   iui(s)vi(   s) for each of them
and take the maximum. there are 2n subsets s, but several s could have the same w(s)
and in that case it su   ces to list just one of them. round each coordinate of each ui to
1
nk2 . call the rounded vector   ui. similarly obtain   vi. let
the nearest integer multiple of
  w(s) denote the vector (  u1(s),   v1(   s),   u2(s),   v2(   s), . . . ,   uk(s),   vk(   s)). we will construct
a list of all possible values of the vector   w(s). again, if several di   erent s   s lead to the

64

same vector   w(s), we will keep only one copy on the list. the list will be constructed by
id145. for the recursive step, assume we already have a list of all such
vectors for s     {1, 2, . . . , i} and wish to construct the list for s     {1, 2, . . . , i + 1}. each
s     {1, 2, . . . , i} leads to two possible s(cid:48)     {1, 2, . . . , i + 1}, namely, s and s     {i + 1}.
in the    rst case, the vector   w(s(cid:48)) = (  u1(s),   v1(   s) +   v1,i+1,   u2(s),   v2(   s) +   v2,i+1, . . . , ...).
in the second case, it is   w(s(cid:48)) = (  u1(s) +   u1,i+1,   v1(   s),   u2(s) +   u2,i+1,   v2(   s), . . . , ...). we
put in these two vectors for each vector in the previous list. then, crucially, we prune -
i.e., eliminate duplicates.

n

assume that k is constant. now, we show that the error is at most

since ui and vi are unit length vectors, |ui(s)|,|vi(   s)|        
as claimed.
n. also |  ui(s)     ui(s)|    
nk2 = 1
k2 and similarly for vi. to bound the error, we use an elementary fact: if a and b are
reals with |a|,|b|     m and we estimate a by a(cid:48) and b by b(cid:48) so that |a   a(cid:48)|,|b   b(cid:48)|            m ,
then a(cid:48)b(cid:48) is an estimate of ab in the sense

k+1

n2   

|ab     a(cid:48)b(cid:48)| = |a(b     b(cid:48)) + b(cid:48)(a     a(cid:48))|     |a||b     b(cid:48)| + (|b| + |b     b(cid:48)|)|a     a(cid:48)|     3m   .

using this,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) k(cid:88)

i=1

  i  ui(s)  vi(   s)    

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)     3k  1

  iui(s)vi(s)

k(cid:88)

i=1

   

n/k2     3n3/2/k     n2/k,

and this meets the claimed error bound.

next, we show that the running time is polynomially bounded. first, |  ui(s)|,|  vi(s)|    
   
n. since   ui(s) and   vi(s) are all integer multiples of 1/(nk2), there are at most 2n3/2k2
2
possible values of   ui(s) and   vi(s) from which it follows that the list of   w(s) never gets
larger than (2n3/2k2)2k which for    xed k is polynomially bounded.

we summarize what we have accomplished.

(cid:17)

(cid:16) n2   

k

theorem 3.19 given a directed graph g(v, e), a cut of size at least the maximum cut
minus o

can be computed in time polynomial in n for any    xed k.

note that achieving the same accuracy in time polynomial in n and k would give an

exact max cut in polynomial time.

3.10 bibliographic notes

singular value decomposition is fundamental to numerical analysis and id202.
there are many texts on these subjects and the interested reader may want to study
these. a good reference is [gvl96]. the material on id91 a mixture of gaussians
in section 3.9.3 is from [vw02]. modeling data with a mixture of gaussians is a stan-
dard tool in statistics. several well-known heuristics like the expectation-minimization

65

algorithm are used to learn (   t) the mixture model to data. recently, in theoretical com-
puter science, there has been modest progress on provable polynomial-time algorithms
for learning mixtures. some references are [ds07], [ak05], [am05], and [mv10]. the
application to the discrete optimization problem is from [fk99]. the section on rank-
ing documents/webpages is from two in   uential papers, one on hubs and authorities by
jon kleinberg [kle99] and the other on id95 by page, brin, motwani and winograd
[bmpw98].

66

3.11 exercises

exercise 3.1 (least squares vertical error) in many experiments one collects the
value of a parameter at various instances of time. let yi be the value of the parameter y
at time xi. suppose we wish to construct the best linear approximation to the data in the
sense that we wish to minimize the mean square error. here error is measured vertically
rather than perpendicular to the line. develop formulas for m and b to minimize the mean
square error of the points {(xi, yi)|1     i     n} to the line y = mx + b.

exercise 3.2 given    ve observed variables, height, weight, age, income, and blood pres-
sure of n people, how would one    nd the best least squares    t a   ne subspace of the form

a1 (height) + a2 (weight) + a3 (age) + a4 (income) + a5 (blood pressure) = a6

here a1, a2, . . . , a6 are the unknown parameters. if there is a good best    t 4-dimensional
a   ne subspace, then one can think of the points as lying close to a 4-dimensional sheet
rather than points lying in 5-dimensions. why might it be better to use the perpendicular
distance to the a   ne subspace rather than vertical distance where vertical distance is
measured along the coordinate axis corresponding to one of the variables?

exercise 3.3 manually    nd the best    t lines (not subspaces which must contain the ori-
gin) through the points in the sets below. subtract the center of gravity of the points in
the set from each of the points in the set and    nd the best    t line for the resulting points.
does the best    t line for the original data go through the origin?

1. (4,4) (6,2)

2. (4,2) (4,4) (6,2) (6,4)

3. (3,2.5) (3,5) (5,1) (5,3.5)

exercise 3.4 manually determine the best    t line through the origin for each of the
following sets of points. is the best    t line unique? justify your answer for each of the
subproblems.

1. {(0, 1) , (1, 0)}
2. {(0, 1) , (2, 0)}

exercise 3.5 manually    nd the left and right-singular vectors, the singular values, and
the svd decomposition of the matrices in figure 3.6.

exercise 3.6 consider the matrix

             1

2
   1
2
1    2
   1    2

            

a =

67

       1 1

0 3
3 0

      

m =

(0,3)

(1,1)

(3,0)

(1,3)

(0,2)

m =

(3,1)

             0 2

2 0
1 3
3 1

            

figure 3.6 a

(2,0)

figure 3.6 b

1. run the power method starting from x =(cid:0)1

(cid:1) for k = 3 steps. what does this give

figure 3.6: svd problem

1

as an estimate of v1?

2. what actually are the vi   s,   i   s, and ui   s? it may be easiest to do this by computing

the eigenvectors of b = at a.

3. suppose matrix a is a database of restaurant ratings: each row is a person, each
column is a restaurant, and aij represents how much person i likes restaurant j.
what might v1 represent? what about u1? how about the gap   1       2?

exercise 3.7 let a be a square n    n matrix whose rows are orthonormal. prove that
the columns of a are orthonormal.
exercise 3.8 suppose a is a n   n matrix with block diagonal structure with k equal size
blocks where all entries of the ith block are ai with a1 > a2 >        > ak > 0. show that a
has exactly k nonzero singular vectors v1, v2, . . . , vk where vi has the value ( k
n)1/2 in the
coordinates corresponding to the ith block and 0 elsewhere. in other words, the singular
vectors exactly identify the blocks of the diagonal. what happens if a1 = a2 =        = ak?
in the case where the ai are equal, what is the structure of the set of all possible singular
vectors?
hint: by symmetry, the top singular vector   s components must be constant in each block.

exercise 3.9 interpret the    rst right and left-singular vectors for the document term
matrix.

exercise 3.10 verify that the sum of r-rank one matrices

cixiyi

t can be written as

xcy t , where the xi are the columns of x, the yi are the columns of y, and c is a
diagonal matrix with the constants ci on the diagonal.

t be the svd of a. show that (cid:12)(cid:12)ut

1 a(cid:12)(cid:12) =   1 and that

exercise 3.11 let (cid:80)r
(cid:12)(cid:12)ut
1 a(cid:12)(cid:12) = max
(cid:12)(cid:12)uta(cid:12)(cid:12).

i=1   iuivi

|u|=1

r(cid:80)

i=1

68

exercise 3.12 if   1,   2, . . . ,   r are the singular values of a and v1, v2, . . . , vr are the
corresponding right-singular vectors, show that

r(cid:80)

i=1

1. at a =

  2
i vivi

t

2. v1, v2, . . . vr are eigenvectors of at a.

3. assuming that the eigenvectors of at a are unique up to multiplicative constants,
conclude that the singular vectors of a (which by de   nition must be unit length) are
unique up to sign.

  iuivt

i be the singular value decomposition of a rank r matrix a.

exercise 3.13 let (cid:80)

i

k(cid:80)

  iuivt

let ak =
quantities in terms of the singular values {  i, 1     i     r}.

i=1

i be a rank k approximation to a for some k < r. express the following

f

1. ||ak||2
2. ||ak||2
3. ||a     ak||2
4. ||a     ak||2

2

2

f

exercise 3.14 if a is a symmetric matrix with distinct singular values, show that the
left and right singular vectors are the same and that a = v dv t .

exercise 3.15 let a be a matrix. how would you compute

v1 = arg max

|v|=1

|av|?

how would you use or modify your algorithm for    nding v1 to compute the    rst few
singular vectors of a.

exercise 3.16 use the power method to compute the singular value decomposition of the
matrix

(cid:19)

(cid:18) 1 2

3 4

a =

exercise 3.17

1. write a program to implement the power method for computing the

   rst singular vector of a matrix. apply your program to the matrix

                     

a =

                      .

3       
2
4       
3
...
...
10 0       
0       
0

1
2
...
9
10

9
10

0
0

10
0
...
0
0

69

2. modify the power method to    nd the    rst four singular vectors of a matrix a as
follows. randomly select four vectors and    nd an orthonormal basis for the space
spanned by the four vectors. then multiply each of the basis vectors times a and
   nd a new orthonormal basis for the space spanned by the resulting four vectors.
apply your method to    nd the    rst four singular vectors of matrix a from part 1.
in matlab the command orth    nds an orthonormal basis for the space spanned by a
set of vectors.

exercise 3.18 a matrix a is positive semi-de   nite if for all x, xt ax     0.

1. let a be a real valued matrix. prove that b = aat is positive semi-de   nite.
2. let a be the adjacency matrix of a graph. the laplacian of a is l = d     a where
d is a diagonal matrix whose diagonal entries are the row sums of a. prove that
l is positive semi de   nite by showing that l = bt b where b is an m-by-n matrix
with a row for each edge in the graph, a column for each vertex, and we de   ne

             1 if i is the endpoint of e with lesser index

1 if i is the endpoint of e with greater index
0 if i is not an endpoint of e

bei =

exercise 3.19 prove that the eigenvalues of a symmetric real valued matrix are real.

exercise 3.20 suppose a is a square invertible matrix and the svd of a is a =(cid:80)
prove that the inverse of a is(cid:80)
r(cid:80)

exercise 3.21 suppose a is square, but not necessarily invertible and has svd a =

i . show that bax = x for all x in the span of the right-
i=1
singular vectors of a. for this reason b is sometimes called the pseudo inverse of a and
can play the role of a   1 in many applications.

i . let b =

r(cid:80)

  iuivt

viut
i .

viut

1
  i

1
  i

i=1

i

i

  iuivt
i .

exercise 3.22

1. for any matrix a, show that   k     ||a||f   

k

.

2. prove that there exists a matrix b of rank at most k such that ||a     b||2     ||a||f   

k

.

3. can the 2-norm on the left hand side in (2) be replaced by frobenius norm?

exercise 3.23 suppose an n    d matrix a is given and you are allowed to preprocess
a. then you are given a number of d-dimensional vectors x1, x2, . . . , xm and for each of
these vectors you must    nd the vector axj approximately, in the sense that you must    nd a
|yj     axj|       ||a||f|xj|. here    >0 is a given error bound. describe
vector yj satisfying

an algorithm that accomplishes this in time o(cid:0) d+n

(cid:1) per xj not counting the preprocessing

  2

time. hint: use exercise 3.22.

70

exercise 3.24 find the values of ci to maximize

r(cid:80)

i=1

c2
i = 1.

r(cid:80)

i=1

i where   1       2     . . . and
i   2
c2

exercise 3.25 (document-term matrices): suppose we have an m    n document-
term matrix a where each row corresponds to a document and has been normalized to
length one. de   ne the    similarity    between two such documents by their dot product.

1. consider a    synthetic    document whose sum of squared similarities with all docu-
ments in the matrix is as high as possible. what is this synthetic document and how
would you    nd it?

2. how does the synthetic document in (1) di   er from the center of gravity?

3. building on (1), given a positive integer k,    nd a set of k synthetic documents such
that the sum of squares of the mk similarities between each document in the matrix
and each synthetic document is maximized. to avoid the trivial solution of selecting
k copies of the document in (1), require the k synthetic documents to be orthogonal
to each other. relate these synthetic documents to singular vectors.

4. suppose that the documents can be partitioned into k subsets (often called clusters),
where documents in the same cluster are similar and documents in di   erent clusters
are not very similar. consider the computational problem of isolating the clusters.
this is a hard problem in general. but assume that the terms can also be partitioned
into k clusters so that for i (cid:54)= j, no term in the ith cluster occurs in a document
in the jth cluster. if we knew the clusters and arranged the rows and columns in
them to be contiguous, then the matrix would be a block-diagonal matrix. of course
the clusters are not known. by a    block    of the document-term matrix, we mean
a submatrix with rows corresponding to the ithcluster of documents and columns
corresponding to the ithcluster of terms . we can also partition any n vector into
blocks. show that any right-singular vector of the matrix must have the property
that each of its blocks is a right-singular vector of the corresponding block of the
document-term matrix.

5. suppose now that the k singular values are all distinct. show how to solve the

id91 problem.

hint: (4) use the fact that the right-singular vectors must be eigenvectors of at a. show
that at a is also block-diagonal and use properties of eigenvectors.
exercise 3.26 let u be a    xed vector. show that maximizing xt uut (1     x) subject to
xi     {0, 1} is equivalent to partitioning the coordinates of u into two subsets where the
sum of the elements in both subsets are as equal as possible.

71

exercise 3.27 read in a photo and convert to a matrix. perform a singular value de-
composition of the matrix. reconstruct the photo using only 5%, 10%, 25%, 50% of the
singular values.

1. print the reconstructed photo. how good is the quality of the reconstructed photo?

2. what percent of the forbenius norm is captured in each case?

hint: if you use matlab, the command to read a photo is imread. the types of    les that
can be read are given by imformats. to print the    le use imwrite. print using jpeg format.
to access the    le afterwards you may need to add the    le extension .jpg. the command
imread will read the    le in uint8 and you will need to convert to double for the svd code.
afterwards you will need to convert back to uint8 to write the    le. if the photo is a color
photo you will get three matrices for the three colors used.

exercise 3.28

1. create a 100   100 matrix of random numbers between 0 and 1 such
that each entry is highly correlated with the adjacent entries. find the svd of a.
what fraction of the frobenius norm of a is captured by the top 10 singular vectors?
how many singular vectors are required to capture 95% of the frobenius norm?

2. repeat (1) with a 100    100 matrix of statistically independent random numbers

between 0 and 1.

exercise 3.29 show that the running time for the maximum cut algorithm in section
3.9.5 can be carried out in time o(n3 + poly(n)kk), where poly is some polynomial.

exercise 3.30 let x1, x2, . . . , xn be n points in d-dimensional space and let x be the
n   d matrix whose rows are the n points. suppose we know only the matrix d of pairwise
distances between points and not the coordinates of the points themselves. the set of points
x1, x2, . . . , xn giving rise to the distance matrix d is not unique since any translation,
rotation, or re   ection of the coordinate system leaves the distances invariant. fix the
origin of the coordinate system so that the centroid of the set of points is at the origin.

that is,(cid:80)n

i=1 xi = 0.

1. show that the elements of xx t are given by

(cid:34)

n(cid:88)

k=1

n(cid:88)

k=1

d2
kj +

1
n2

(cid:35)

d2
kl

.

n(cid:88)

n(cid:88)

k=1

l=1

xixt

j =    1
2

ij     1
d2
n

ik     1
d2
n

2. describe an algorithm for determining the matrix x whose rows are the xi.

exercise 3.31

72

1. consider the pairwise distance matrix for twenty us cities given below. use the
algorithm of exercise 3.30 to place the cities on a map of the us. the algorithm is
called classical multidimensional scaling, cmdscale, in matlab. alternatively use the
pairwise distance matrix of 12 chinese cities to place the cities on a map of china.

note: any rotation or a mirror image of the map will have the same pairwise
distances.

2. suppose you had airline distances for 50 cities around the world. could you use

these distances to construct a 3-dimensional world model?

b
o
s
-
400
851
1551
1769
1605
2596
1137
1255
1123
188
1282
271
2300
483
1038
2099
2699
2493
393

b
u
f
400
-
454
1198
1370
1286
2198
803
1181
731
292
883
279
1906
178
662
1699
2300
2117
292

c
h
i
851
454
-
803
920
940
1745
482
1188
355
713
432
666
1453
410
262
1260
1858
1737
597

d
a
l
1551
1198
803
-
663
225
1240
420
1111
862
1374
586
1299
887
1070
547
999
1483
1681
1185

d
e
n
1769
1370
920
663
-
879
831
879
1726
700
1631
488
1579
586
1320
796
371
949
1021
1494

h
o
u
1605
1286
940
225
879
-
1374
484
968
1056
1420
794
1341
1017
1137
679
1200
1645
1891
1220

l
a

2596
2198
1745
1240
831
1374
-
1603
2339
1524
2451
1315
2394
357
2136
1589
579
347
959
2300

m
e
m
1137
803
482
420
879
484
1603
-
872
699
957
529
881
1263
660
240
1250
1802
1867
765

m
i
a
1255
1181
1188
1111
1726
968
2339
872
-
1511
1092
1397
1019
1982
1010
1061
2089
2594
2734
923

m
i
m
1123
731
355
862
700
1056
1524
699
1511
-
1018
290
985
1280
743
466
987
1584
1395
934

boston
bu   alo
chicago
dallas
denver
houston
los angeles
memphis
miami
minneapolis
new york
omaha
philadelphia
phoenix
pittsburgh
saint louis
salt lake city
san francisco
seattle
washington d.c.

73

n
y

188
292
713
1374
1631
1420
2451
957
1092
1018
-
1144
83
2145
317
875
1972
2571
2408
230

o
m
a
1282
883
432
586
488
794
1315
529
1397
290
1144
-
1094
1036
836
354
833
1429
1369
1014

p
h
i
271
279
666
1299
1579
1341
2394
881
1019
985
83
1094
-
2083
259
811
1925
2523
2380
123

p
h
o
2300
1906
1453
887
586
1017
357
1263
1982
1280
2145
1036
2083
-
1828
1272
504
653
1114
1973

p
i
t
483
178
410
1070
1320
1137
2136
660
1010
743
317
836
259
1828
-
559
1668
2264
2138
192

s
t
l
1038
662
262
547
796
679
1589
240
1061
466
875
354
811
1272
559
-
1162
1744
1724
712

s
l
c
2099
1699
1260
999
371
1200
579
1250
2089
987
1972
833
1925
504
1668
1162
-
600
701
1848

s
f

2699
2300
1858
1483
949
1645
347
1802
2594
1584
2571
1429
2523
653
2264
1744
600
-
678
2442

s
e
a
2493
2117
1737
1681
1021
1891
959
1867
2734
1395
2408
1369
2380
1114
2138
1724
701
678
-
2329

d
c

393
292
597
1185
1494
1220
2300
765
923
934
230
1014
123
1973
192
712
1848
2442
2329
-

boston
bu   alo
chicago
dallas
denver
houston
los angeles
memphis
miami
minneapolis
new york
omaha
philadelphia
phoenix
pittsburgh
saint louis
salt lake city
san francisco
seattle
washington d.c.

city

beijing
tianjin
shanghai
chongqing

hohhot
urumqi
lhasa

yinchuan
nanning
harbin

changchun
shenyang

bei-
jing

0

125
1239
3026
480
3300
3736
1192
2373
1230
979
684

tian-

shang- chong- hoh- urum-

lha-

jin
125

0

1150
1954
604
3330
3740
1316
2389
1207
955
661

hai
1239
1150

0

1945
1717
3929
4157
2092
1892
2342
2090
1796

qing
3026
1954
1945

0

1847
3202
2457
1570
993
3156
2905
2610

hot
480
604
1717
1847

0

2825
3260
716
2657
1710
1458
1164

qi

3300
3330
3929
3202
2825

0

2668
2111
4279
4531
4279
3985

sa

3736
3740
4157
2457
3260
2668

0

2547
3431
4967
4715
4421

yin-
chuan
1192
1316
2092
1570
716
2111
2547

0

2673
2422
2170
1876

nan- har- chang-
ning
2373
2389
1892
993
2657
4279
3431
2673

bin
1230
1207
2342
3156
1710
4531
4967
2422
3592

chun
979
955
2090
2905
1458
4279
4715
2170
3340
256

0

3592
3340
3046

0

256
546

0

294

shen-
yang
684
661
1796
2610
1164
3985
4421
1876
3046
546
294

0

exercise 3.32 one   s data in a high dimensional space may lie on a lower dimensional
sheath. to test for this one might for each data point    nd the set of closest data points
and calculate the vector distance from the data point to each of the close points. if the set
of these distance vectors is a lower dimensional space than the number of distance points,
then it is likely that the data is on a low dimensional sheath. to test the dimension of
the space of the distance vectors one might use the singular value decomposition to    nd
the singular values. the dimension of the space is the number of large singular values.
the low singular values correspond to noise or slight curvature of the sheath. to test
this concept generate a data set of points that lie on a one dimensional curve in three
space. for each point    nd maybe ten nearest points, form the matrix of distance, and do
a singular value decomposition on the matrix. report what happens.

using code such as the following to create the data.

function [ data, distance ] = create_sheath( n )

74

%creates n data points on a one dimensional sheath in three dimensional
%space
%
if nargin==0

n=100;

end
data=zeros(3,n);
for i=1:n

x=sin((pi/100)*i);
y=sqrt(1-x^2);
z=0.003*i;
data(:,i)=[x;y;z];

end
%subtract adjacent vertices
distance=zeros(3,10);
for i=1:5

distance(:,i)=data(:,i)-data(:,6);
distance(:,i+5)=data(:,i+6)-data(:,6);

end
end

75

4 id93 and markov chains

a random walk on a directed graph consists of a sequence of vertices generated from
a start vertex by probabilistically selecting an incident edge, traversing the edge to a new
vertex, and repeating the process.

we generally assume the graph is strongly connected, meaning that for any pair of
vertices x and y, the graph contains a path of directed edges starting at x and ending
at y. if the graph is strongly connected, then, as we will see, no matter where the walk
begins the fraction of time the walk spends at the di   erent vertices of the graph converges
to a stationary id203 distribution.

start a random walk at a vertex x and think of the starting id203 distribution as
putting a mass of one on x and zero on every other vertex. more generally, one could start
with any id203 distribution p, where p is a row vector with nonnegative components
summing to one, with px being the id203 of starting at vertex x. the id203
of being at vertex x at time t + 1 is the sum over each adjacent vertex y of being at y at
time t and taking the transition from y to x. let p(t) be a row vector with a component
for each vertex specifying the id203 mass of the vertex at time t and let p(t + 1) be
the row vector of probabilities at time t + 1. in matrix notation14

p(t)p = p(t + 1)

where the ijth entry of the matrix p is the id203 of the walk at vertex i selecting
the edge to vertex j.

a fundamental property of a random walk is that in the limit, the long-term average
id203 of being at a particular vertex is independent of the start vertex, or an initial
id203 distribution over vertices, provided only that the underlying graph is strongly
connected. the limiting probabilities are called the stationary probabilities. this funda-
mental theorem is proved in the next section.

a special case of id93, namely id93 on undirected graphs, has
important connections to electrical networks. here, each edge has a parameter called
conductance, like electrical conductance. if the walk is at vertex x, it chooses an edge to
traverse next from among all edges incident to x with id203 proportional to its con-
ductance. certain basic quantities associated with id93 are hitting time, which
is the expected time to reach vertex y starting at vertex x, and cover time, which is the
expected time to visit every vertex. qualitatively, for undirected graphs these quantities
are all bounded above by polynomials in the number of vertices. the proofs of these facts
will rely on the analogy between id93 and electrical networks.

14id203 vectors are represented by row vectors to simplify notation in equations like the one here.

76

random walk

markov chain

graph
vertex
strongly connected
aperiodic
strongly connected

and aperiodic
undirected graph

stochastic process
state
persistent
aperiodic

ergodic

time reversible

table 5.1: correspondence between terminology of id93 and markov chains

pxy of going from state x to state y where for each x, (cid:80)

aspects of the theory of id93 were developed in computer science with a
number of applications. among others, these include de   ning the id95 of pages on
the world wide web by their stationary id203. an equivalent concept called a
markov chain had previously been developed in the statistical literature. a markov chain
has a    nite set of states. for each pair of states x and y, there is a transition id203
y pxy = 1. a random walk in
the markov chain starts at some state. at a given time step, if it is in state x, the next
state y is selected randomly with id203 pxy. a markov chain can be represented by
a directed graph with a vertex representing each state and an edge with weight pxy from
vertex x to vertex y. we say that the markov chain is connected if the underlying directed
graph is strongly connected. that is, if there is a directed path from every vertex to every
other vertex. the matrix p consisting of the pxy is called the transition id203 matrix
of the chain. the terms    random walk    and    markov chain    are used interchangeably.
the correspondence between the terminologies of id93 and markov chains is
given in table 5.1.

a state of a markov chain is persistent if it has the property that should the state ever
be reached, the random process will return to it with id203 one. this is equivalent
to the property that the state is in a strongly connected component with no out edges.
for most of the chapter, we assume that the underlying directed graph is strongly con-
nected. we discuss here brie   y what might happen if we do not have strong connectivity.
consider the directed graph in figure 4.1b with three strongly connected components,
a, b, and c. starting from any vertex in a, there is a nonzero id203 of eventually
reaching any vertex in a. however, the id203 of returning to a vertex in a is less
than one and thus vertices in a, and similarly vertices in b, are not persistent. from
any vertex in c, the walk eventually will return with id203 one to the vertex, since
there is no way of leaving component c. thus, vertices in c are persistent.

a connected markov chain is said to be aperiodic if the greatest common divisor of
the lengths of directed cycles is one. it is known that for connected aperiodic chains, the

77

a

a

b

(a)

b

(b)

c

c

figure 4.1: (a) a directed graph with vertices having no out out edges and a strongly
connected component a with no in edges.
(b) a directed graph with three strongly connected components.

id203 distribution of the random walk converges to a unique stationary distribution.
aperiodicity is a technical condition needed in this proof. here, we do not prove this
theorem and do not worry about aperiodicity at all. it turns out that if we take the av-
erage id203 distribution of the random walk over the    rst t steps, then this average
converges to a limiting distribution for connected chains (without assuming aperiodicity)
and this average is what one uses in practice. we prove this limit theorem and explain
its uses in what is called the id115 (mcmc) method.

markov chains are used to model situations where all the information of the system
necessary to predict the future can be encoded in the current state. a typical example
is speech, where for a small k the current state encodes the last k syllables uttered by
the speaker. given the current state, there is a certain id203 of each syllable being
uttered next and these can be used to calculate the transition probabilities. another
example is a gambler   s assets, which can be modeled as a markov chain where the current
state is the amount of money the gambler has on hand. the model would only be valid
if the gambler   s bets depend only on current assets, not the past history.

later in the chapter, we study the widely used id115 method
(mcmc). here, the objective is to sample a large space according to some id203
distribution p. the number of elements in the space may be very large, say 10100. one de-
signs a markov chain where states correspond to the elements of the space. the transition
probabilities of the chain are designed so that the stationary id203 of the chain is the

78

id203 distribution p with which we want to sample. one chooses samples by taking
a random walk until the id203 distribution is close to the stationary distribution of
the chain and then selects the current state of the walk. the walk continues a number of
steps until the id203 distribution is nearly independent of where the walk was when
the    rst element was selected. a second point is then selected, and so on. although it
is impossible to store the graph in a computer since it has 10100 vertices, to do the walk
one needs only store the current vertex of the walk and be able to generate the adjacent
vertices by some algorithm. what is critical is that the id203 distribution of the
walk converges to the stationary distribution in time logarithmic in the number of states.

we mention two motivating examples. the    rst is to select a point at random in
d-space according to a id203 density such as a gaussian. put down a grid and let
each grid point be a state of the markov chain. given a id203 density p, design
transition probabilities of a markov chain so that the stationary distribution is p.
in
general, the number of states grows exponentially in the dimension d, but if the time
to converge to the stationary distribution grows polynomially in d, then one can do a
random walk on the graph until convergence to the stationary id203. once the sta-
tionary id203 has been reached, one selects a point. to select a set of points, one
must walk a number of steps between each selection so that the id203 of the current
point is independent of the previous point. by selecting a number of points one can es-
timate the id203 of a region by observing the number of selected points in the region.
a second example is from physics. consider an n   n grid in the plane with a particle
at each grid point. each particle has a spin of   1. a con   guration is a n2 dimensional
vector v = (v1, v2, . . . , vn2), where vi is the spin of the ith particle. there are 2n2 spin con-
   gurations. the energy of a con   guration is a function f (v) of the con   guration, not of
any single spin. a central problem in statistical mechanics is to sample spin con   gurations
according to their id203. it is easy to design a markov chain with one state per spin
con   guration so that the stationary id203 of a state is proportional to the state   s
energy. if a random walk gets close to the stationary id203 in time polynomial in n
rather than 2n2, then one can sample spin con   gurations according to their id203.

the markov chain has 2n2 states, one per con   guration. two states in the markov
chain are adjacent if and only if the corresponding con   gurations v and u di   er in just one
coordinate (ui = vi for all but one i). the metropilis-hastings random walk, described
in more detail in section 4.2, has a transition id203 from a con   guration v to an
adjacent con   guration u of

(cid:18)

(cid:19)

.

1
n2 min

1,

f (u)
f (v)

as we will see, the markov chain has a stationary id203 proportional to the energy.
there are two more crucial facts about this chain. the    rst is that to execute a step in
the chain, we do not need the whole chain, just the ratio f (u)
f (v) . the second is that under
suitable assumptions, the chain approaches stationarity in time polynomial in n.

79

a quantity called the mixing time, loosely de   ned as the time needed to get close to
the stationary distribution, is often much smaller than the number of states. in section
4.4, we relate the mixing time to a combinatorial notion called normalized conductance
and derive upper bounds on the mixing time in several cases.

4.1 stationary distribution

let p(t) be the id203 distribution after t steps of a random walk. de   ne the

long-term average id203 distribution a(t) by

(cid:0)p(0) + p(1) +        + p(t     1)(cid:1).

a(t) =

1
t

the fundamental theorem of markov chains asserts that for a connected markov chain,
a(t) converges to a limit id203 vector x that satis   es the equations xp = x. before
proving the fundamental theorem of markov chains, we    rst prove a technical lemma.

lemma 4.1 let p be the transition id203 matrix for a connected markov chain.
the n    (n + 1) matrix a = [p     i , 1] obtained by augmenting the matrix p     i with an
additional column of ones has rank n.
proof: if the rank of a = [p     i, 1] was less than n there would be a subspace of solu-
tions to ax = 0 of at least two-dimensions. each row in p sums to one, so each row in
p     i sums to zero. thus x = (1, 0), where all but the last coordinate of x is 1, is one
solution to ax = 0. assume there was a second solution (x,   ) perpendicular to (1, 0).
j pijxj +  . each xi is a convex combination
of some xj plus   . let s be the set of i for which xi attains its maximum value. since
x is perpendicular to 1, some xi is negative and thus   s is not empty. connectedness
implies that some xk of maximum value is adjacent to some xl of lower value. thus,

then (p    i)x+  1 = 0 and for each i, xi =(cid:80)
xk >(cid:80)

j pkjxj. therefore    must be greater than 0 in xk =(cid:80)

j pkjxj +   ..

on the other hand, the same argument with t the set of i with xi taking its minimum
value implies    < 0. this contradiction falsi   es the assumption of a second solution,
thereby proving the lemma.

theorem 4.2 (fundamental theorem of markov chains) for a connected markov
chain there is a unique id203 vector    satisfying   p =   . moreover, for any starting
distribution, lim

t       a(t) exists and equals   .

proof: note that a(t) is itself a id203 vector, since its components are nonnegative
and sum to 1. run one step of the markov chain starting with distribution a(t); the

80

distribution after the step is a(t)p . calculate the change in probabilities due to this step.

a(t)p     a(t) =

=

=

1
t
1
t
1
t

[p(0)p + p(1)p +        + p(t     1)p ]     1
t
[p(1) + p(2) +        + p(t)]     1
t
(p(t)     p(0)) .

[p(0) + p(1) +        + p(t     1)]

[p(0) + p(1) +        + p(t     1)]

thus, b(t) = a(t)p     a(t) satis   es |b(t)|     2

t     0, as t        .

by lemma 4.1 above, a = [p     i, 1] has rank n. the n    n submatrix b of a
consisting of all its columns except the    rst is invertible. let c(t) be obtained from
b(t) by removing the    rst entry. since a(t)p     a(t) = b(t) and b is obtained by
deleting the    rst column of p     i and adding a column of 1   s, a(t)b = [c(t), 1]. then
a(t) = [c(t) , 1]b   1     [0 , 1]b   1 establishing the theorem with    = [0 , 1]b   1.

we    nish this section with the following lemma useful in establishing that a id203
distribution is the stationary id203 distribution for a random walk on a connected
graph with edge probabilities.

lemma 4.3 for a random walk on a strongly connected graph with probabilities on the
x   x = 1, then    is

edges, if the vector    satis   es   xpxy =   ypyx for all x and y and (cid:80)
proof: since    satis   es   xpxy =   ypyx, summing both sides,   x =(cid:80)

the stationary distribution of the walk.

  ypyx and hence   

satis   es    =   p. by theorem 4.2,    is the unique stationary id203.

y

4.2 id115

the id115 (mcmc) method is a technique for sampling a mul-
tivariate id203 distribution p(x), where x = (x1, x2, . . . , xd). the mcmc method is
used to estimate the expected value of a function f (x)

(cid:88)

e(f ) =

f (x)p(x).

x

if each xi can take on two or more values, then there are at least 2d values for x, so an
explicit summation requires exponential time. instead, one could draw a set of samples,
where each sample x is selected with id203 p(x). averaging f over these samples
provides an estimate of the sum.

to sample according to p(x), design a markov chain whose states correspond to the
possible values of x and whose stationary id203 distribution is p(x). there are two
general techniques to design such a markov chain: the metropolis-hastings algorithm

81

and id150, which we will describe in the next two subsections. the fundamen-
tal theorem of markov chains, theorem 4.2, states that the average of the function f
over states seen in a su   ciently long run is a good estimate of e(f ). the harder task
is to show that the number of steps needed before the long-run average probabilities are
close to the stationary distribution grows polynomially in d, though the total number of
states may grow exponentially in d. this phenomenon known as rapid mixing happens for
a number of interesting examples. section 4.4 presents a crucial tool used to show rapid
mixing.

we used x     rd to emphasize that distributions are multi-variate. from a markov
chain perspective, each value x can take on is a state, i.e., a vertex of the graph on which
the random walk takes place. henceforth, we will use the subscripts i, j, k, . . . to denote
states and will use pi instead of p(x1, x2, . . . , xd) to denote the id203 of the state
corresponding to a given set of values for the variables. recall that in the markov chain
terminology, vertices of the graph are called states.

recall the notation that p(t) is the row vector of probabilities of the random walk
being at each state (vertex of the graph) at time t. so, p(t) has as many components
as there are states and its ith component is the id203 of being in state i at time t.
recall the long-term t-step average is

(cid:80)

a(t) =

1
t

[p(0) + p(1) +        + p(t     1)] .

(4.1)

the expected value of the function f under the id203 distribution p is e(f ) =
i fipi where fi is the value of f at state i. our estimate of this quantity will be the
average value of f at the states seen in a t step walk. call this estimate   . clearly, the
expected value of    is

(cid:88)

i

(cid:18)1

t(cid:88)

t

j=1

e(  ) =

fi

prob(cid:0)walk is in state i at time j(cid:1)(cid:19)

(cid:88)

i

=

fiai(t).

the expectation here is with respect to the    coin tosses    of the algorithm, not with respect
to the underlying distribution p. let fmax denote the maximum absolute value of f . it is

easy to see that(cid:12)(cid:12)(cid:12)(cid:88)

i

(cid:12)(cid:12)(cid:12)     fmax

(cid:88)

i

fipi     e(  )

|pi     ai(t)| = fmax||p     a(t)||1

(4.2)

where the quantity ||p     a(t)||1 is the l1 distance between the id203 distributions p
and a(t), often called the    total variation distance    between the distributions. we will
build tools to upper bound ||p     a(t)||1. since p is the stationary distribution, the t for
which ||p    a(t)||1 becomes small is determined by the rate of convergence of the markov
chain to its steady state.

the following proposition is often useful.

82

proposition 4.4 for two id203 distributions p and q,

(cid:88)

(cid:88)

||p     q||1 = 2

(pi     qi)+ = 2

(qi     pi)+

i

i

where x+ = x if x     0 and x+ = 0 if x < 0.

the proof is left as an exercise.

4.2.1 metropolis-hasting algorithm

the metropolis-hasting algorithm is a general method to design a markov chain whose
stationary distribution is a given target distribution p. start with a connected undirected
graph g on the set of states. if the states are the lattice points (x1, x2, . . . , xd) in rd
with xi     {0, 1, 2, , . . . , n}, then g could be the lattice graph with 2d coordinate edges at
each interior vertex. in general, let r be the maximum degree of any vertex of g. the
transitions of the markov chain are de   ned as follows. at state i select neighbor j with
id203 1
r . since the degree of i may be less than r, with some id203 no edge
is selected and the walk remains at i. if a neighbor j is selected and pj     pi, go to j. if
pj < pi, go to j with id203 pj/pi and stay at i with id203 1     pj
. intuitively,
pi
this favors    heavier    states with higher pi values. for i adjacent to j in g,

(cid:17)

1,

pj
pi

pij =

(cid:16)
pii = 1    (cid:88)

min

1
r

pij.

j(cid:54)=i

and

thus,

(cid:17)

(cid:16)

1,

pj
pi

pipij =

pi
r

min

=

1
r

min(pi, pj) =

pj
r

min

(cid:17)

(cid:16)

1,

pi
pj

= pjpji.

by lemma 4.3, the stationary probabilities are indeed pi as desired.

2, p(b) = 1

4, p(c) = 1

8, and p(d) = 1

example: consider the graph in figure 4.2. using the metropolis-hasting algorithm,
assign transition probabilities so that the stationary id203 of a random walk is
p(a) = 1
8. the maximum degree of any vertex is three,
so at a, the id203 of taking the edge (a, b) is 1
6. the id203 of taking the
3
edge (a, c) is 1
2
1
1 or 1
12. thus, the id203
3
8
of staying at a is 2
3. the id203
of taking the edge from c to a is 1
3 and the id203 of taking the edge from d to a is
3. thus, the stationary id203 of a is 1
1
2, which is the desired
id203.

1 or 1
3. the id203 of taking the edge from b to a is 1

1 or 1
12 and of taking the edge (a, d) is 1

3 = 1

3 + 1

3 + 1

3 + 1

1
4

1
8

2

2

3

4

1

8

1

8

1

2

2

83

2

p(a) = 1
p(b) = 1
p(c) = 1
p(d) = 1

4

8

8

1
2

a

d

1
8

1
4

b

c

1
8

3

1
3
1
3

1
4
1
8
1
8

a     b
a     c
a     d 1
a     a 1    1
b     a
b     c
b     b

1    1

1
3
1
3

1
8

2

6

2

12

1 = 1
1 = 1
1 = 1
6    1
12    1

12

2

12 = 2

3

6

4

1 = 1
3    1

6 = 1

2

3
1
3
1
3

c     a 1
c     b
c     d
c     c
d     a 1
d     c
d     d 1    1

1    1

3
1
3

3    1

3    1

3 = 0

3    1

3 = 1

3

p(a) = p(a)p(a     a) + p(b)p(b     a) + p(c)p(c     a) + p(d)p(d     a)

= 1
2

2

3 + 1

4

1

3 + 1

8

1

3 + 1

8

1

3 = 1

2

p(b) = p(a)p(a     b) + p(b)p(b     b) + p(c)p(c     b)

= 1
2

1

6 + 1

4

1

2 + 1

8

1

3 = 1

4

p(c) = p(a)p(a     c) + p(b)p(b     c) + p(c)p(c     c) + p(d)p(d     c)

= 1
2

1

12 + 1

4

1

6 + 1

8 0 + 1

8

1

3 = 1

8

p(d) = p(a)p(a     d) + p(c)p(c     d) + p(d)p(d     d)

= 1
2

1

12 + 1

8

1

3 + 1

8

1

3 = 1

8

figure 4.2: using the metropolis-hasting algorithm to set probabilities for a random
walk so that the stationary id203 will be the desired id203.

4.2.2 id150

id150 is another id115 method to sample from a
multivariate id203 distribution. let p (x) be the target distribution where x =
(x1, . . . , xd). id150 consists of a random walk on an undirectd graph whose
vertices correspond to the values of x = (x1, . . . , xd) and in which there is an edge from
x to y if x and y di   er in only one coordinate. thus, the underlying graph is like a
d-dimensional lattice except that the vertices in the same coordinate line form a clique.

to generate samples of x = (x1, . . . , xd) with a target distribution p (x), the gibbs
sampling algorithm repeats the following steps. one of the variables xi is chosen to be
updated. its new value is chosen based on the marginal id203 of xi with the other
variables    xed. there are two commonly used schemes to determine which xi to update.
one scheme is to choose xi randomly, the other is to choose xi by sequentially scanning
from x1 to xd.

suppose that x and y are two states that di   er in only one coordinate. without loss

84

5
8

3,1

2,1

1,1

1
6

1
8

1
3

7
12

3,2

2,2

1,2

1
6

1
6

1
4

1
3

3,3

1
12

2,3

1
12

1,3

1
6

5
12

3
8

3
4

p(1, 1) = 1
3
p(1, 2) = 1
4
p(1, 3) = 1
6
p(2, 1) = 1
8
p(2, 2) = 1
6
p(2, 3) = 1
12
p(3, 1) = 1
6
p(3, 2) = 1
6
p(3, 3) = 1
12

p(11)(12) = 1
2 ( 1
calculation of edge id203 p(11)(12)

d p12/(p11 + p12 + p13) = 1

4)/( 1

3

1
4

1

6) = 1

8/ 9

12 = 1

8

4

3 = 1

6

4

6

4

p(11)(12) = 1
3 = 1
2
3 = 1
p(11)(13) = 1
2
5 = 1
p(11)(21) = 1
2
5 = 2
p(11)(31) = 1
2
edge probabilities.

1
4
1
6
1
8
1
6

9

8

8

10

15

p(12)(11) = 1
2
p(12)(13) = 1
2
p(12)(22) = 1
2
p(12)(32) = 1
2

1
3
1
6
1
6
1
6

9

4

9

4

3 = 2
3 = 1
7 = 1
7 = 1

12

12

7

7

p(13)(11) = 1
2
p(13)(12) = 1
2
p(13)(23) = 1
2
p(13)(33) = 1
2

1
3
1
4
1
12
1
12

6

4

9

4

3 = 2
3 = 1
1 = 1
1 = 1

3

3

8

8

p(21)(22) = 1
2
p(21)(23) = 1
2
p(21)(11) = 1
2
p(21)(31) = 1
2

8

9

8

1
6
1
12
1
8
3
1
6

3 = 2
3 = 1
5 = 4
5 = 2

15

15

9

8

4

1

p11p(11)(12) = 1
3
p11p(11)(13) = 1
3
p11p(11)(21) = 1
3
veri   cation of a few edges, pipij = pjpji.

2
9 = p12p(12)(11)
2
9 = p13p(13)(11)
4
15 = p21p(21)(11)

6 = 1
9 = 1
10 = 1

6

1

1

8

note that the edge probabilities out of a state such as (1,1) do not add up to one.
that is, with some id203 the walk stays at the state that it is in. for example,
p(11)(11) = 1     (p(11)(12) + p(11)(13) + p(11)(21) + p(11)(31)) = 1     1

24     1

32     1

6     1

24 = 9
32.

figure 4.3: using the gibbs algorithm to set probabilities for a random walk so that
the stationary id203 will be a desired id203.

85

of generality let that coordinate be the    rst. then, in the scheme where a coordinate is
randomly chosen to modify, the id203 pxy of going from x to y is

the normalizing constant is 1/d since(cid:80)

pxy =

1
d

p(y1|x2, x3, . . . , xd).

p(y1|x2, x3, . . . , xd) equals 1 and summing over

d coordinates

d(cid:88)

(cid:88)

y1

p(yi|x1, x2, . . . , xi   1, xi+1 . . . xd) = d

gives a value of d. similarly,

i=1

yi

pyx =

=

1
d
1
d

p(x1|y2, y3, . . . , yd)
p(x1|x2, x3, . . . , xd).

here use was made of the fact that for j (cid:54)= 1, xj = yj.

it is simple to see that this chain has stationary id203 proportional to p (x).

rewrite pxy as

p(y1|x2, x3, . . . , xd)p(x2, x3, . . . , xd)

p(x2, x3, . . . , xd)

p(y1, x2, x3, . . . , xd)

p(x2, x3, . . . , xd)

p(y)

pxy =

=

=

1
d
1
d
1
d

p(x2, x3, . . . , xd)
again using xj = yj for j (cid:54)= 1. similarly write

pyx =

1
d

p(x)

p(x2, x3, . . . , xd)

from which it follows that p(x)pxy = p(y)pyx. by lemma 4.3 the stationary id203
of the random walk is p(x).

4.3 areas and volumes

computing areas and volumes is a classical problem. for many regular    gures in
two and three dimensions there are closed form formulae. in chapter 2, we saw how to
compute volume of a high dimensional sphere by integration. for general convex sets in
d-space, there are no closed form formulae. can we estimate volumes of d-dimensional
convex sets in time that grows as a polynomial function of d? the mcmc method answes

86

this question in the a   rmative.

one way to estimate the area of the region is to enclose it in a rectangle and estimate
the ratio of the area of the region to the area of the rectangle by picking random points
in the rectangle and seeing what proportion land in the region. such methods fail in high
dimensions. even for a sphere in high dimension, a cube enclosing the sphere has expo-
nentially larger area, so exponentially many samples are required to estimate the volume
of the sphere.

it turns out, however, that the problem of estimating volumes of sets can be reduced
to the problem of drawing uniform random samples from sets. suppose one wants to
estimate the volume of a convex set r. create a concentric series of larger and larger
spheres15 s1, s2, . . . , sk such that s1 is contained in r and sk contains r. then

vol(r) = vol(sk     r) =

vol(sk     r)
vol(sk   1     r)

vol(sk   1     r)
vol(sk   2     r)

       vol(s2     r)
vol(s1     r)

vol(s1)

if the radius of the sphere si is 1 + 1

d times the radius of the sphere si   1, then we have:

because vol(si)/vol(si   1) = (cid:0)1 + 1

d

1     vol(si     r)
(cid:1)d < e, and the fraction of si occupied by r is less
vol(si   1     r)

    e

than or equal to the fraction of si   1 occupied by r (due to the convexity of r and the
fact that the center of the spheres lies in r). this implies that the ratio v ol(si   r)
v ol(si   1   r) can
be estimated by rejection sampling, i.e., selecting points in si     r uniformly at random
and computing the fraction in si   1     r, provided one can select points at random from a
d-dimensional convex region.

the number of spheres is at most

o(log1+(1/d) r) = o(rd)

where r is the ratio of the radius of sk to the radius of s1. this means that it su   ces
to estimate each ratio to a factor of (1     
erd) in order to estimate the overall volume to
error 1     .

it remains to show how to draw a uniform random sample from a d-dimensional convex
set. here we will use the convexity of the set r and thus the sets si   r so that the markov
chain technique will converge quickly to its stationary id203. to select a random
sample from a d-dimensional convex set, impose a grid on the region and do a random
walk on the grid points. at each time, pick one of the 2d coordinate neighbors of the
current grid point, each with id203 1/(2d) and go to the neighbor if it is still in the

15one could also use rectangles instead of spheres.

87

si+1

si

r

figure 4.4: by sampling the area inside the dark line and determining the fraction of
points in the shaded region we compute v ol(si+1   r)
v ol(si   r) .
to sample we create a grid and assign a id203 of one to each grid point inside the
dark lines and zero outside. using metropolis-hasting edge probabilities the stationary
id203 will be uniform for each point inside the region and we can sample points
uniformly and determine the fraction within the shaded region.

set; otherwise, stay put and repeat. if the grid length in each of the d coordinate directions
is at most some a, the total number of grid points in the set is at most ad. although this
is exponential in d, the markov chain turns out to be rapidly mixing (the proof is beyond
our scope here) and leads to polynomial time bounded algorithm to estimate the volume
of any convex set in rd.

4.4 convergence of id93 on undirected graphs

the metropolis-hasting algorithm and id150 both involve id93
on edge-weighted undirected graphs. given an edge-weighted undirected graph, let wxy
denote the weight of the edge between nodes x and y, with wxy = 0 if no such edge exists.
y wxy. the markov chain has transition probabilities pxy = wxy/wx. we

let wx = (cid:80)
wx, i.e.,   x = wx/wtotal for wtotal =(cid:80)

assume the chain is connected.

we now claim that the stationary distribution    of this walk has   x proportional to

x(cid:48) wx(cid:48). speci   cally, notice that

wxpxy = wx

wxy
wx

= wxy = wyx = wy

wyx
wy

= wypyx.

therefore (wx/wtotal)pxy = (wy/wtotal)pyx and lemma 4.3 implies that the values   x =
wx/wtotal are the stationary probabilities.

an important question is how fast the walk starts to re   ect the stationary id203
of the markov process. if the convergence time was proportional to the number of states,
algorithms such as metropolis-hasting and id150 would not be very useful since

88

figure 4.5: a network with a constriction. all edges have weight 1.

the number of states can be exponentially large.

there are clear examples of connected chains that take a long time to converge. a
chain with a constriction, see figure 4.5, takes a long time to converge since the walk is
unlikely to cross the narrow passage between the two halves, both of which are reasonably
big. we will show in theorem 4.5 that the time to converge is quantitatively related to
the tightest constriction.

we de   ne below a combinatorial measure of constriction for a markov chain, called the
normalized conductance. we will relate normalized conductance to the time by which the
average id203 distribution of the chain is guaranteed to be close to the stationary
id203 distribution. we call this   -mixing time:

de   nition 4.1 fix    > 0. the   -mixing time of a markov chain is the minimum integer t
such that for any starting distribution p, the 1-norm di   erence between the t-step running
average id203 distribution16 and the stationary distribution is at most   .

de   nition 4.2 for a subset s of vertices, let   (s) denote (cid:80)

x   s   x. the normalized

conductance   (s) of s is

  (s) =

there is a simple interpretation of   (s). suppose without loss of generality that   (s)    
  (   s). then, we may write   (s) as

  (s) =

(cid:0)p(0) + p(1) +        + p(t     1)(cid:1) is called the running average distribution.

a

b

16recall that a(t) = 1
t

  xpxy

(x,y)   (s,   s)

(cid:80)
min(cid:0)  (s),   (   s)(cid:1) .
(cid:88)
(cid:88)
(cid:124) (cid:123)(cid:122) (cid:125)

(cid:124)(cid:123)(cid:122)(cid:125)

  x
  (s)

y      s

x   s

pxy

.

89

here, a is the id203 of being in x if we were in the stationary distribution restricted
to s and b is the id203 of stepping from x to   s in a single step. thus,   (s) is the
id203 of moving from s to   s in one step if we are in the stationary distribution
restricted to s.

it is easy to show that if we started in the distribution p0,x =   s/  (s) for x     s and

p0,x = 0 for x       s, the expected number of steps before we step into   s is

1  (s) + 2(1       (s))  (s) + 3(1       (s))2  (s) +        =

1

  (s)

.

clearly, to be close to the stationary distribution, we must at least get to   s once. so,
mixing time is lower bounded by 1/  (s). since we could have taken any s, mixing time
is lower bounded by the minimum over all s of   (s). we de   ne this quantity to be the
normalized conductance of the markov chain.

de   nition 4.3 the normalized conductance of the markov chain, denoted   , is de   ned
by

   = min

s   v,s(cid:54)={}   (s).

as we just argued, normalized conductance being high is a necessary condition for
rapid mixing. the theorem below proves the converse that normalized conductance being
high is su   cient for mixing. intuitively, if    is large, the walk rapidly leaves any subset
of states. but the proof of the theorem is quite di   cult. after we prove it, we will see
examples where the mixing time is much smaller than the cover time. that is, the number
of steps before a random walk reaches a random state independent of its starting state is
much smaller than the average number of steps needed to reach every state. in fact for
some graphs, called expanders, the mixing time is logarithmic in the number of states.

theorem 4.5 the   -mixing time of a random walk on an undirected graph is

(cid:18)ln(1/  min)

(cid:19)

o

  2  3

where   min is the minimum stationary id203 of any state.

proof: let t = c ln(1/  min)

  2  3

, for a suitable constant c. let

(cid:0)p(0) + p(1) +        + p(t     1)(cid:1)

a = a(t) =

1
t

be the running average distribution. we need to show that ||a       ||1       . let

vi =

ai
  i

,

90

f (x)

g(x)
g1 = {1}; g2 = {2, 3, 4}; g3 = {5}.

  1

  2

  3

  4

  5

x

figure 4.6: bounding l1 distance.

and renumber states so that v1     v2     v3            . thus, early indices i for which vi > 1
are states that currently have too much id203, and late indices i for which vi < 1
are states that currently have too little id203.

intuitively, to show that ||a       ||1        it is enough to show that the values vi are
relatively    at and do not drop too fast as we increase i. we begin by reducing our goal
to a formal statement of that form. then, in the second part of the proof, we prove that
vi do not fall fast using the concept of    id203    ows   .

we call a state i for which vi > 1    heavy    since it has more id203 according to
a than its stationary id203. let i0 be the maximum i such that vi > 1; it is the last
heavy state. by proposition (4.4):

||a       ||1 = 2

(vi     1)  i = 2

(1     vi)  i.

(4.3)

let

de   ne a function f : [0,   i0]     (cid:60) by f (x) = vi     1 for x     [  i   1,   i). see figure 4.6. now,

i0(cid:88)

i=1

(cid:88)

i   i0+1

  i =   1 +   2 +        +   i.

i0(cid:88)

(vi     1)  i =

(cid:90)   i0

f (x) dx.

(4.4)

i=1

0

we make one more technical modi   cation. we divide {1, 2, . . . , i0} into groups g1, g2, g3, . . . , gr,
of contiguous subsets. we specify the groups later. let ut = maxi   gtvi be the maximum

91

value of vi within gt. de   ne a new function g(x) by g(x) = ut     1 for x        i   gt[  i   1,   i);

see figure 4.6. since g(x)     f (x)(cid:90)   i0

(cid:90)   i0

f (x) dx    

0

0

g(x) dx.

we now assert (with ur+1 = 1):

(cid:90)   i0

0

r(cid:88)

t=1

  (g1     g2     . . .     gt)(ut     ut+1).

g(x) dx =

(4.5)

(4.6)

this is just the statement that the area under g(x) in the    gure is exactly covered by the
rectangles whose bottom sides are the dotted lines. we leave the formal proof of this to
the reader. we now focus on proving that

r(cid:88)

  (g1     g2     . . .     gt)(ut     ut+1)       /2,

(4.7)

t=1

for a sub-division into groups we specify which su   ces by 4.3, 4.4, 4.5 and 4.6. while we
start the proof of (4.7) with a technical observation (4.8), its proof will involve two nice
ideas: the notion of id203    ow and reckoning id203    ow in two di   erent ways.

i   i0+1(1     vi)  i >   /2 from which it follows that(cid:80)

i   i0+1(1   vi)  i        then we would be done by (4.3).
i   i0+1   i       /2

first, the technical observation: if 2(cid:80)
so assume now that(cid:80)

and so, for any subset a of heavy nodes,

min(  (a),   (   a))       
2

(4.8)
we now de   ne the subsets. g1 will be just {1}. in general, suppose g1, g2, . . . , gt   1 have
already been de   ned. we start gt at it = 1+ (end of gt   1). let it = k. we will de   ne l,
the last element of gt to be the largest integer greater than or equal to k and at most i0
so that

  (a).

l(cid:88)

j=k+1

  j           k
4

.

in lemma 4.6 which follows this theorem prove that for groups g1, g2, . . . , gr, u1.u2, . . . , ur, ur+1
as above

  (g1     g2     . . . gr)(ut     ut+1)     8
t    

.

now to prove (4.7), we only need an upper bound on r, the number of groups. if gt =
{k, k + 1, . . . , l}, with l < i0, then by de   nition of l, we have   l+1     (1 +     
2 )  k. so,
r     ln1+(    /2)(1/  1) + 2     ln(1/  1)/(    /2) + 2. this completes the proof of (4.7) and the
theorem.

we complete the proof of theorem 4.5 with the proof of lemma 4.6. the notation in the
lemma is that from the theorem.

92

lemma 4.6 suppose groups g1, g2, . . . , gr, u1.u2, . . . , ur, ur+1 are as above. then,

  (g1     g2     . . . gr)(ut     ut+1)     8
t    

.

proof: this is the main lemma. the proof of the lemma uses a crucial idea of id203
   ows. we will use two ways of calculating the id203    ow from heavy states to light
states when we execute one step of the markov chain starting at probabilities a. the
id203 vector after that step is ap . now, a     ap is the net loss of id203 for
each state due to the step.

one step is(cid:80)k

consider a particular group gt = {k, k + 1, . . . , l}, say. first consider the case when
k < i0. let a = {1, 2, . . . , k}. the net loss of id203 for each state from the set a in

i=1(ai     (ap )i) which is at most 2

t by the proof of theorem 4.2.

another way to reckon the net loss of id203 from a is to take the di   erence of

the id203    ow from a to   a and the    ow from   a to a. for any i < j,

net-   ow(i, j) =    ow(i, j)        ow(j, i) =   ipijvi       jpjivj =   jpji(vi     vj)     0,

thus, for any two states i and j, with i heavier than j, i.e., i < j, there is a non-negative
net    ow from i to j. (this is intuitively reasonable since it says that id203 is    owing
from heavy to light states.) since l     k, the    ow from a to {k + 1, k + 2, . . . , l} minus
the    ow from {k + 1, k + 2, . . . , l} to a is nonnegative. since for i     k and j > l, we have
vi     vk and vj     vl+1, the net loss from a is at least

  jpji(vi     vj)     (vk     vl+1)

  jpji.

(cid:88)

i   k
j>l

thus,

since

  jpji     2
t

.

(4.9)

  j           (a)/4

(cid:88)

i   k
j>l

(vk     vl+1)

(cid:88)
  jpji     l(cid:88)

i   k
j>l

j=k+1

i=1

j=k+1

l(cid:88)

k(cid:88)
(cid:88)
i   k<j   jpji    (cid:80)

i   k<j

and by the de   nition of   , using (4.8)

  jpji       min(  (a),   (   a))           k/2,

we have, (cid:80)

i   k
j>l

  jpji = (cid:80)

i   k;j   l   jpji           k/4. substituting this into the

inequality (4.9) gives

vk     vl+1     8

t      k

,

(4.10)

proving the lemma provided k < i0. if k = i0, the proof is similar but simpler.

93

4.4.1 using normalized conductance to prove convergence

we now apply theorem 4.5 to some examples to illustrate how the normalized con-
ductance bounds the rate of convergence. in each case we compute the mixing time for
the uniform id203 function on the vertices. our    rst examples will be simple graphs.
the graphs do not have rapid converge, but their simplicity helps illustrate how to bound
the normalized conductance and hence the rate of convergence.

a 1-dimensional lattice

consider a random walk on an undirected graph consisting of an n-vertex path with
self-loops at the both ends. with the self loops, we have pxy = 1/2 on all edges (x, y),
and so the stationary distribution is a uniform 1
n over all vertices by lemma 4.3. the set
with minimum normalized conductance is the set s with id203   (s)     1
2 having the
(x,y)   (s,   s)   xpxy, to id203 mass inside
2n and

smallest ratio of id203 mass exiting it,(cid:80)

it,   (s). this set consists of the    rst n/2 vertices, for which the numerator is 1
denominator is 1

2. thus,

  (s) =

1
n

.

by theorem 4.5, for    a constant such as 1/100, after o(n2 log n/ 3) steps, ||at       ||1    
1/100. this graph does not have rapid convergence. the hitting time and the cover time
are o(n2). in many interesting cases, the mixing time may be much smaller than the
cover time. we will see such an example later.

a 2-dimensional lattice

consider the n    n lattice in the plane where from each point there is a transition to
each of the coordinate neighbors with id203 1/4. at the boundary there are self-loops
with id203 1-(number of neighbors)/4. it is easy to see that the chain is connected.
since pij = pji, the function fi = 1/n2 satis   es fipij = fjpji and by lemma 4.3, f is the
stationary distribution. consider any subset s consisting of at most half the states. if
|s|     n2
4 , then the subset with the fewest edges leaving it consists of some number of
columns plus perhaps one additional partial column. the number of edges leaving s is at
least n. thus

since |s|     n2

4 , in this case

i   s

j      s

(cid:88)

(cid:88)

  (s)        

1
n2

  ipij        (cid:0)n
(cid:32)

min(cid:0) s

1/n
n2 ,   s

n2

(cid:1) =    
(cid:1)(cid:33)

=    

(cid:19)

(cid:18) 1
(cid:18) 1

n

n

.

(cid:19)

.

if |s| < n2
4 , the subset s of a given size that has the minimum number of edges leaving
consists of a square located at the lower left hand corner of the grid (exercise 4.21). if

94

|s| is not a perfect square then the right most column of s is short. thus at least 2(cid:112)|s|

points in s are adjacent to points in   s. each of these points contributes   ipij =    ( 1
the    ow(s,   s). thus,

n2 ) to

(cid:88)

(cid:88)

  ipij     c(cid:112)|s|
(cid:80)
(cid:80)
min(cid:0)  (s),   (   s)(cid:1)     c(cid:112)|s|/n2

j      s   ipij

|s|/n2 =

j      s

i   s

i   s

n2

(cid:18) 1

(cid:19)

n

.

c(cid:112)|s| =    

and

  (s) =

thus, in either case, after o(n2 ln n/ 3) steps, |a(t)       |1      .

a lattice in d-dimensions

next consider the n    n              n lattice in d-dimensions with a self-loop at each
boundary point with id203 1     (number of neighbors)/2d. the self loops make all
  i equal to n   d. view the lattice as an undirected graph and consider the random walk
on this undirected graph. since there are nd states, the cover time is at least nd and
thus exponentially dependent on d. it is possible to show (exercise 4.22) that    is    ( 1
dn).
since all   i are equal to n   d, the mixing time is o(d3n2 ln n/  3), which is polynomially
bounded in n and d.

the d-dimensional lattice is related to the metropolis-hastings algorithm and gibbs
sampling although in those constructions there is a nonuniform id203 distribution at
the vertices. however, the d-dimension lattice case suggests why the metropolis-hastings
and id150 constructions might converge fast.

a clique

consider an n vertex clique with a self loop at each vertex. for each edge, pxy = 1
n

and thus for each vertex,   x = 1

n . let s be a subset of the vertices. then

(x,y)   (s,   s)

(cid:88)
x   s   x,(cid:80)

(x,y)   (s,   s)   xpxy

(cid:80)
min((cid:80)

and

  (s) =

this gives a bound on the   -mixing time of

(cid:88)

x   s

|s|
n

.

  x =

  xpxy =   xpxy|s||s| =

1

n2|s||s|

=

min( 1

(cid:33)

x      s   x)

(cid:32)ln 1

o

  min
  2 3

1

n2|s||s|
n|s|, 1

n|s|)
(cid:19)

(cid:18)ln n

.

 3

= o

95

max(|s|,|s|) =

=

1
n

1
2

.

however, a walker on the clique starting from any id203 distribution will in one step
be exactly at the stationary id203 distribution.

a connected undirected graph

next consider a random walk on a connected n vertex undirected graph where at each
vertex all edges are equally likely. the stationary id203 of a vertex equals the degree
of the vertex divided by the sum of degrees. that is, if the degree of vertex x is dx and
the number of edges in the graph is m, then   x = dx
2m. notice that for any edge (x, y) we
have

(cid:18) dx

(cid:19)(cid:18) 1

(cid:19)

2m

dx

=

1
2m

.

  xpxy =

therefore, for any s, the total conductance of edges out of s is at least
   is at least 1
o(m2 ln n/  3) = o(n4 ln n/  3).

m. since   min     1

1
2m, and so
= o(ln n). thus, the mixing time is

2m     1

n2 , ln 1
  min

the gaussian distribution on the interval [-1,1]

consider the interval [   1, 1]. let    be a    grid size    speci   ed later and let g be the
   + 1 vertices {   1,   1 +   ,   1 + 2  , . . . , 1       , 1} having
graph consisting of a path on the 2
self loops at the two ends. let   x = ce     x2 for x     {   1,   1 +   ,   1 + 2  , . . . , 1       , 1}

where    > 1 and c has been adjusted so that(cid:80)

x   x = 1.

we now describe a simple markov chain with the   x as its stationary id203 and
argue its fast convergence. with the metropolis-hastings    construction, the transition
probabilities are

px,x+   =

1
2

min

1,

e     (x+  )2
e     x2

and px,x      =

1
2

min

1,

e     (x     )2
e     x2

let s be any subset of states with   (s)     1
[k  , 1] for k     2. it is easy to see that

2. first consider the case when s is an interval

(cid:32)

(cid:33)

.

(cid:32)

(cid:33)

(cid:90)    
(cid:90)    
(cid:32)

  (s)    

   

ce     x2 dx

x=(k   1)  

(k   1)  

x

(k     1)  
ce     ((k   1)  )2
  (k     1)  

(cid:33)

.

= o

ce     x2 dx

(cid:88)

(cid:88)

i   s

j /   s

now there is only one edge from s to   s and total conductance of edges out of s is

  ipij =   k  pk  ,(k   1)   = min(ce     k2  2, ce     (k   1)2  2) = ce     k2  2.

96

using 2     k     1/  ,        1, and   (   s)     1,

  (s) =

   ow(s,   s)

    ce     k2  2   (k     1)  
ce     ((k   1)  )2
       (  (k     1)  e       2(2k   1))        (    e   o(    )).

min(  (s),   (   s))

for the grid size less than the variance of the gaussian distribution,    < 1
  , we have      < 1,
so e   o(    ) =    (1), thus,   (s)        (    ). now,   min     ce          e   1/  , so ln(1/  min)     1/  .
if s is not an interval of the form [k, 1] or [   1, k], then the situation is only better
since there is more than one    boundary    point which contributes to    ow(s,   s). we do
not present this argument here. by theorem 4.5 in    (1/  2  3  3) steps, a walk gets within
   of the steady state distribution.

in the uniform id203 case the  -mixing time is bounded by n2 log n. for compar-
ison, in the gaussian case set    = 1/n and    = 1/3. this gives an  -mixing time bound
of n3. in the gaussian case with the entire initial id203 on the    rst vertex, the chain
begins to converge faster to the stationary id203 than the uniform distribution case
since the chain favors higher degree vertices. however, ultimately the distribution must
reach the lower id203 vertices on the other side of the gaussian   s maximum and
here the chain is slower since it favors not leaving the higher id203 vertices.

in these examples, we have chosen simple id203 distributions. the methods ex-

tend to more complex situations.

4.5 electrical networks and id93

in the next few sections, we study the relationship between electrical networks and
id93 on undirected graphs. the graphs have nonnegative weights on each edge.
a step is executed by picking a random edge from the current vertex with id203
proportional to the edge   s weight and traversing the edge.

an electrical network is a connected, undirected graph in which each edge (x, y) has
a resistance rxy > 0. in what follows, it is easier to deal with conductance de   ned as the
reciprocal of resistance, cxy = 1
, rather than resistance. associated with an electrical
rxy
network is a random walk on the underlying graph de   ned by assigning a id203
pxy = cxy/cx to the edge (x, y) incident to the vertex x, where the normalizing constant cx
cxy. note that although cxy equals cyx, the probabilities pxy and pyx may not be

equals(cid:80)

y

equal due to the id172 required to make the probabilities at each vertex sum to
one. we shall soon see that there is a relationship between current    owing in an electrical

97

network and a random walk on the underlying graph.

since we assume that the undirected graph is connected, by theorem 4.2 there is
a unique stationary id203 distribution.the stationary id203 distribution is   
where   x = cx
c0

cx. to see this, for all x and y

with c0 =(cid:80)

x

  xpxy =

cx
c0

cxy
cx

=

cy
c0

cyx
cy

=   ypyx

and hence by lemma 4.3,    is the unique stationary id203.

id94

id94 are useful in developing the relationship between electrical net-
works and id93 on undirected graphs. given an undirected graph, designate
a nonempty set of vertices as boundary vertices and the remaining vertices as interior
vertices. a harmonic function g on the vertices is a function whose value at the boundary
vertices is    xed to some boundary condition, and whose value at any interior vertex x is
a weighted average of its values at all the adjacent vertices y, with weights pxy satisfying
y pxy = 1 for each x. thus, if at every interior vertex x for some set of weights pxy

(cid:80)
satisfying(cid:80)

gypxy, then g is an harmonic function.

y pxy = 1, gx =(cid:80)

y

example: convert an electrical network with conductances cxy to a weighted, undirected
graph with probabilities pxy. let f be a function satisfying f p = f where p is the matrix
of probabilities. it follows that the function gx = fx
cx

is harmonic.

(cid:80)

y

gx = fx
cx

= 1
cx

fypyx = 1
cx

= 1
cx

(cid:80)

y

fy

cxy
cy

=(cid:80)

y

fy
cy

cxy
cx

(cid:80)
=(cid:80)

fy

y

y

cyx
cy

gypxy

a harmonic function on a connected graph takes on its maximum and minimum on
the boundary. this is easy to see for the following reason. suppose the maximum does
not occur on the boundary. let s be the set of vertices at which the maximum value is
attained. since s contains no boundary vertices,   s is nonempty. connectedness implies
that there is at least one edge (x, y) with x     s and y       s. the value of the function at
x is the weighted average of the value at its neighbors, all of which are less than or equal
to the value at x and the value at y is strictly less, a contradiction. the proof for the
minimum value is identical.

there is at most one harmonic function satisfying a given set of equations and bound-
ary conditions. for suppose there were two solutions, f (x) and g(x). the di   erence of two

98

6

1

8

1

6

5

4

5

8

5

3

5

3

graph with boundary vertices
dark and boundary conditions
speci   ed.

values of harmonic function
satisfying boundary conditions
where the edge weights at
each vertex are equal

figure 4.7: graph illustrating an harmonic function.

solutions is itself harmonic. since h(x) = f (x)   g(x) is harmonic and has value zero on the
boundary, by the min and max principles it has value zero everywhere. thus f (x) = g(x).

the analogy between electrical networks and id93

there are important connections between electrical networks and id93 on
undirected graphs. choose two vertices a and b. attach a voltage source between a and b
so that the voltage va equals one volt and the voltage vb equals zero. fixing the voltages
at va and vb induces voltages at all other vertices, along with a current    ow through the
edges of the network. what we will show below is the following. having    xed the voltages
at the vertices a and b, the voltage at an arbitrary vertex x equals the id203 that a
random walk that starts at x will reach a before it reaches b. we will also show there is
a related probabilistic interpretation of current as well.

probabilistic interpretation of voltages

before relating voltages and probabilities, we    rst show that the voltages form a har-
monic function. let x and y be adjacent vertices and let ixy be the current    owing through
the edge from x to y. by ohm   s law,

by kirchho      s law the currents    owing out of each vertex sum to zero.

ixy =

= (vx     vy)cxy.

vx     vy
rxy

(cid:88)

ixy = 0

y

99

(cid:88)
(cid:88)

y

(vx     vy)cxy = 0

(cid:88)

replacing currents in the above sum by the voltage di   erence times the conductance
yields

or

observing that (cid:80)
vx = (cid:80)

y

vx

cxy =

y

y

cxy = cx and that pxy = cxy
cx

vycxy.

, yields vxcx = (cid:80)

y

vypxycx. hence,

y

vypxy. thus, the voltage at each vertex x is a weighted average of the volt-
ages at the adjacent vertices. hence the voltages form a harmonic function with {a, b} as
the boundary.

let px be the id203 that a random walk starting at vertex x reaches a before b.
clearly pa = 1 and pb = 0. since va = 1 and vb = 0, it follows that pa = va and pb = vb.
furthermore, the id203 of the walk reaching a from x before reaching b is the sum
over all y adjacent to x of the id203 of the walk going from x to y in the    rst step
and then reaching a from y before reaching b. that is

(cid:88)

px =

pxypy.

y

hence, px is the same harmonic function as the voltage function vx and v and p satisfy the
same boundary conditions at a and b.. thus, they are identical functions. the id203
of a walk starting at x reaching a before reaching b is the voltage vx.

probabilistic interpretation of current

in a moment, we will set the current into the network at a to have a value which we will
equate with one random walk. we will then show that the current ixy is the net frequency
with which a random walk from a to b goes through the edge xy before reaching b. let
ux be the expected number of visits to vertex x on a walk from a to b before reaching b.
clearly ub = 0. consider a node x not equal to a or b. every time the walk visits x, it
must have come from some neighbor y. thus, the expected number of visits to x before
reaching b is the sum over all neighbors y of the expected number of visits uy to y before
reaching b times the id203 pyx of going from y to x. that is,

since cxpxy = cypyx

ux =

uy

cxpxy

cy

ux =

uypyx.

(cid:88)
(cid:88)

y

y

100

= (cid:80)

y

and hence ux
cx

uy
cy

pxy. it follows that ux
cx

is harmonic with a and b as the boundary

. now ux
cx

where the boundary conditions are ub = 0 and ua equals some    xed value. now, ub
= 0.
cb
setting the current into a to one,    xed the value of va. adjust the current into a so that
va equals ua
and vx satisfy the same boundary conditions and thus are the same
ca
harmonic function. let the current into a correspond to one walk. note that if the walk
starts at a and ends at b, the expected value of the di   erence between the number of times
the walk leaves a and enters a must be one. this implies that the amount of current into
a corresponds to one walk.

next we need to show that the current ixy is the net frequency with which a random

walk traverses edge xy.

ixy = (vx     vy)cxy =

(cid:19)

(cid:18) ux

cx

    uy
cy

cxy = ux

cxy
cx

    uy

cxy
cy

= uxpxy     uypyx

the quantity uxpxy is the expected number of times the edge xy is traversed from x to y
and the quantity uypyx is the expected number of times the edge xy is traversed from y to
x. thus, the current ixy is the expected net number of traversals of the edge xy from x to y.

e   ective resistance and escape id203

set va = 1 and vb = 0. let ia be the current    owing into the network at vertex a and
out at vertex b. de   ne the e   ective resistance re    between a and b to be re    = va
and
ia
the e   ective conductance ce    to be ce    = 1
. de   ne the escape id203, pescape, to
re   
be the id203 that a random walk starting at a reaches b before returning to a. we
now show that the escape id203 is ce   
. for convenience, assume that a and b are
ca
not adjacent. a slight modi   cation of the argument su   ces for the case when a and b are
adjacent.

since va = 1,

ia =

ia =

(va     vy)cay

y

(cid:88)
(cid:88)
(cid:34)

y

= ca

(cid:88)

y

cay
ca

vy

(cid:35)

payvy

.

cay     ca

1    (cid:88)

y

for each y adjacent to the vertex a, pay is the id203 of the walk going from vertex
a to vertex y. earlier we showed that vy is the id203 of a walk starting at y going
payvy is the id203 of a walk starting at a returning

to a before reaching b. thus,(cid:80)
to a before reaching b and 1    (cid:80)

y

payvy is the id203 of a walk starting at a reaching

y

101

b before returning to a. thus, ia = capescape. since va = 1 and ce    = ia
va
ce    = ia . thus, ce    = capescape and hence pescape = ce   
ca

.

, it follows that

for a    nite connected graph, the escape id203 will always be nonzero. consider
an in   nite graph such as a lattice and a random walk starting at some vertex a. form a
series of    nite graphs by merging all vertices at distance d or greater from a into a single
vertex b for larger and larger values of d. the limit of pescape as d goes to in   nity is the
id203 that the random walk will never return to a. if pescape     0, then eventually
any random walk will return to a. if pescape     q where q > 0, then a fraction of the walks
never return. thus, the escape id203 terminology.

4.6 id93 on undirected graphs with unit edge weights

we now focus our discussion on id93 on undirected graphs with uniform
edge weights. at each vertex, the random walk is equally likely to take any edge. this
corresponds to an electrical network in which all edge resistances are one. assume the
graph is connected. we consider questions such as what is the expected time for a random
walk starting at a vertex x to reach a target vertex y, what is the expected time until the
random walk returns to the vertex it started at, and what is the expected time to reach
every vertex?

hitting time

the hitting time hxy, sometimes called discovery time, is the expected time of a ran-
dom walk starting at vertex x to reach vertex y. sometimes a more general de   nition is
given where the hitting time is the expected time to reach a vertex y from a given starting
id203 distribution.

one interesting fact is that adding edges to a graph may either increase or decrease
hxy depending on the particular situation. adding an edge can shorten the distance from
x to y thereby decreasing hxy or the edge could increase the id203 of a random walk
going to some far o    portion of the graph thereby increasing hxy. another interesting
fact is that hitting time is not symmetric. the expected time to reach a vertex y from a
vertex x in an undirected graph may be radically di   erent from the time to reach x from y.

we start with two technical lemmas. the    rst lemma states that the expected time

to traverse a path of n vertices is    (n2).

lemma 4.7 the expected time for a random walk starting at one end of a path of n
vertices to reach the other end is    (n2).

proof: consider walking from vertex 1 to vertex n in a graph consisting of a single path
of n vertices. let hij, i < j, be the hitting time of reaching j starting from i. now h12 = 1

102

and

hi,i+1 = 1

2 + 1

2(1 + hi   1,i+1) = 1 + 1

2 (hi   1,i + hi,i+1)

2     i     n     1.

solving for hi,i+1 yields the recurrence

solving the recurrence yields

hi,i+1 = 2 + hi   1,i.

hi,i+1 = 2i     1.

to get from 1 to n, you need to    rst reach 2, then from 2 (eventually) reach 3, then from
3 (eventually) reach 4, and so on. thus by linearity of expectation,

h1,n =

n   1(cid:88)
n   1(cid:88)

i=1

hi,i+1 =

n   1(cid:88)
i     n   1(cid:88)

i=1

1

= 2

(2i     1)

i=1

i=1

n (n     1)

= 2
= (n     1)2 .

2

    (n     1)

the next lemma shows that the expected time spent at vertex i by a random walk

from vertex 1 to vertex n in a chain of n vertices is 2(i     1) for 2     i     n     1.

lemma 4.8 consider a random walk from vertex 1 to vertex n in a chain of n vertices.
let t(i) be the expected time spent at vertex i. then

          n     1

t (i) =

i = 1

2 (n     i) 2     i     n     1
1

i = n.

proof: now t (n) = 1 since the walk stops when it reaches vertex n. half of the time when
the walk is at vertex n     1 it goes to vertex n. thus t (n     1) = 2. for 3     i < n     1,
t (i) = 1
2t (2) + 1 and t (2) =
t (1) + 1

2 [t (i     1) + t (i + 1)] and t (1) and t (2) satisfy t (1) = 1
2t (3). solving for t(i + 1) for 3     i < n     1 yields
t(i + 1) = 2t(i)     t(i     1)

which has solution t(i) = 2(n     i) for 3     i < n     1. then solving for t(2) and t(1) yields
t (2) = 2 (n     2) and t (1) = n     1. thus, the total time spent at vertices is

n     1 + 2 (1 + 2 +        + n     2) + 1 = (n     1) + 2

(n     1)(n     2)

2

+ 1 = (n     1)2 + 1

which is one more than h1n and thus is correct.

103

clique of
size n/2

x

(cid:124)

(cid:123)(cid:122)

n/2

y

(cid:125)

figure 4.8:
hitting time.

illustration that adding edges to a graph can either increase or decrease

adding edges to a graph might either increase or decrease the hitting time hxy. con-
sider the graph consisting of a single path of n vertices. add edges to this graph to get the
graph in figure 4.8 consisting of a clique of size n/2 connected to a path of n/2 vertices.
then add still more edges to get a clique of size n. let x be the vertex at the midpoint of
the original path and let y be the other endpoint of the path consisting of n/2 vertices as
shown in the    gure. in the    rst graph consisting of a single path of length n, hxy =    (n2).
in the second graph consisting of a clique of size n/2 along with a path of length n/2,
hxy =    (n3).to see this latter statement, note that starting at x, the walk will go down
the path towards y and return to x for n/2    1 times on average before reaching y for the
   rst time, by lemma 4.8. each time the walk in the path returns to x, with id203
(n/2     1)/(n/2) it enters the clique and thus on average enters the clique   (n) times
before starting down the path again. each time it enters the clique, it spends   (n) time
in the clique before returning to x. it then reenters the clique   (n) times before starting
down the path to y. thus, each time the walk returns to x from the path it spends   (n2)
time in the clique before starting down the path towards y for a total expected time that
is   (n3) before reaching y. in the third graph, which is the clique of size n, hxy =    (n).
thus, adding edges    rst increased hxy from n2 to n3 and then decreased it to n.

hitting time is not symmetric even in the case of undirected graphs. in the graph of
figure 4.8, the expected time, hxy, of a random walk from x to y, where x is the vertex of
attachment and y is the other end vertex of the chain, is   (n3). however, hyx is   (n2).

commute time

the commute time, commute(x, y), is the expected time of a random walk starting at
x reaching y and then returning to x. so commute(x, y) = hxy + hyx. think of going
from home to o   ce and returning home. note that commute time is symmetric. we now
relate the commute time to an electrical quantity, the e   ective resistance. the e   ective
resistance between two vertices x and y in an electrical network is the voltage di   erence

104

between x and y when one unit of current is inserted at vertex x and withdrawn from
vertex y.

theorem 4.9 given a connected, undirected graph, consider the electrical network where
each edge of the graph is replaced by a one ohm resistor. given vertices x and y, the
commute time, commute(x, y), equals 2mrxy where rxy is the e   ective resistance from x
to y and m is the number of edges in the graph.

proof: insert at each vertex i a current equal to the degree di of vertex i. the total
current inserted is 2m where m is the number of edges. extract from a speci   c vertex j
all of this 2m current (note: for this to be legal, the graph must be connected). let vij
be the voltage di   erence from i to j. the current into i divides into the di resistors at
vertex i. the current in each resistor is proportional to the voltage across it. let k be a
vertex adjacent to i. then the current through the resistor between i and k is vij     vkj,
the voltage drop across the resistor. the sum of the currents out of i through the resistors
must equal di, the current injected into i.

(cid:88)

(vij     vkj) = divij    (cid:88)

vkj.

di =

k adj
to i

k adj
to i

solving for vij

(cid:88)

k adj
to i

(cid:88)

k adj
to i

1
di

vkj =

vij = 1 +

1
di

(1 + vkj).

(4.11)

now the hitting time from i to j is the average time over all paths from i to k adjacent

to i and then on from k to j. this is given by

hij =

1
di

(1 + hkj).

(4.12)

k adj
to i

subtracting (4.12) from (4.11), gives vij     hij = (cid:80)

(vkj     hkj). thus, the function
vij     hij is harmonic. designate vertex j as the only boundary vertex. the value of
vij     hij at i = j, namely vjj     hjj, is zero, since both vjj and hjj are zero. so the function
vij     hij must be zero everywhere. thus, the voltage vij equals the expected time hij from
i to j.

k adj
to i

1
di

to complete the proof of theorem 4.9, note that hij = vij is the voltage from i to j
when currents are inserted at all vertices in the graph and extracted at vertex j. if the
current is extracted from i instead of j, then the voltages change and vji = hji in the new

105

(cid:88)

i
   

   

   

   

   

j

    =   

   =

i
   

   

   

   

   

j
   

insert current at each vertex
equal to degree of the vertex.
extract 2m at vertex j, vij = hij.

(a)

   

   

   

   

j
   

=   

i
   

reverse currents in (b).
for new voltages    vji = hji.
since    vji = vij, hji = vij.

(c)

extract current from i instead of j.
for new voltages vji = hji.

(b)

i

2m
=   

j

2m
=   

superpose currents in (a) and (c).
2mrij = vij = hij + hji = commute(i, j).

(d)

figure 4.9: illustration of proof that commute(x, y) = 2mrxy where m is the number of
edges in the undirected graph and rxy is the e   ective resistance between x and y.

setup. finally, reverse all currents in this latter step. the voltages change again and for
the new voltages    vji = hji. since    vji = vij, we get hji = vij.

thus, when a current is inserted at each vertex equal to the degree of the vertex and
the current is extracted from j, the voltage vij in this set up equals hij. when we extract
the current from i instead of j and then reverse all currents, the voltage vij in this new set
up equals hji. now, superpose both situations, i.e., add all the currents and voltages. by
linearity, for the resulting vij, which is the sum of the other two vij   s, is vij = hij + hji. all
currents into or out of the network cancel except the 2m amps injected at i and withdrawn
at j. thus, 2mrij = vij = hij + hji = commute(i, j) or commute(i, j) = 2mrij where rij
is the e   ective resistance from i to j.

the following corollary follows from theorem 4.9 since the e   ective resistance ruv is

less than or equal to one when u and v are connected by an edge.
corollary 4.10 if vertices x and y are connected by an edge, then hxy + hyx     2m where
m is the number of edges in the graph.

proof: if x and y are connected by an edge, then the e   ective resistance rxy is less than
or equal to one.

106

corollary 4.11 for vertices x and y in an n vertex graph, the commute time, commute(x, y),
is less than or equal to n3.

proof: by theorem 4.9 the commute time is given by the formula commute(x, y) =
2mrxy where m is the number of edges. in an n vertex graph there exists a path from x
to y of length at most n. since the resistance can not be greater than that of any path

from x to y, rxy     n. since the number of edges is at most(cid:0)n
(cid:1)
(cid:19)
n    = n3.

commute(x, y) = 2mrxy     2

(cid:18)n

2

2

while adding edges into a graph can never increase the e   ective resistance between
two given nodes x and y, it may increase or decrease the commute time. to see this
consider three graphs: the graph consisting of a chain of n vertices, the graph of figure
4.8, and the clique on n vertices.

cover time

the cover time, cover(x, g) , is the expected time of a random walk starting at vertex x
in the graph g to reach each vertex at least once. we write cover(x) when g is understood.
the cover time of an undirected graph g, denoted cover(g), is

cover(g) = max

x

cover(x, g).

for cover time of an undirected graph, increasing the number of edges in the graph
may increase or decrease the cover time depending on the situation. again consider three
graphs, a chain of length n which has cover time   (n2), the graph in figure 4.8 which has
cover time   (n3), and the complete graph on n vertices which has cover time   (n log n).
adding edges to the chain of length n to create the graph in figure 4.8 increases the
cover time from n2 to n3 and then adding even more edges to obtain the complete graph
reduces the cover time to n log n.

note: the cover time of a clique is   (n log n) since this is the time to select every
integer out of n integers with high id203, drawing integers at random. this is called
the coupon collector problem. the cover time for a straight line is   (n2) since it is the
same as the hitting time. for the graph in figure 4.8, the cover time is   (n3) since one
takes the maximum over all start states and cover(x, g) =    (n3) where x is the vertex
of attachment.

theorem 4.12 let g be a connected graph with n vertices and m edges. the time for a
random walk to cover all vertices of the graph g is bounded above by 4m(n     1).

107

proof: consider a depth    rst search of the graph g starting from some vertex z and let
t be the resulting depth    rst search spanning tree of g. the depth    rst search covers
every vertex. consider the expected time to cover every vertex in the order visited by the
depth    rst search. clearly this bounds the cover time of g starting from vertex z. note
that each edge in t is traversed twice, once in each direction.

cover (z, g)     (cid:88)

hxy.

(x,y)   t
(y,x)   t

if (x, y) is an edge in t , then x and y are adjacent and thus corollary 4.10 implies
hxy     2m. since there are n     1 edges in the dfs tree and each edge is traversed twice,
once in each direction, cover(z)     4m(n     1). this holds for all starting vertices z. thus,
cover(g)     4m(n     1).

the theorem gives the correct answer of n3 for the n/2 clique with the n/2 tail. it

gives an upper bound of n3 for the n-clique where the actual cover time is n log n.

let rxy be the e   ective resistance from x to y. de   ne the resistance re    (g) of a graph

g by re    (g) = max
x,y

(rxy).

theorem 4.13 let g be an undirected graph with m edges. then the cover time for g
is bounded by the following inequality

m re    (g)     cover(g)     6e m re    (g) ln n + n

where e     2.718 is euler   s constant and re    (g) is the resistance of g.

proof: by de   nition re    (g) = max
x,y

(rxy). let u and v be the vertices of g for which

2commute(u, v). note that 1

rxy is maximum. then re    (g) = ruv. by theorem 4.9, commute(u, v) = 2mruv. hence
mruv = 1
2commute(u, v) is the average of huv and hvu, which
is clearly less than or equal to max(huv, hvu). finally, max(huv, hvu) is less than or equal
to max(cover(u, g), cover(v, g)) which is clearly less than the cover time of g. putting
these facts together gives the    rst inequality in the theorem.

m re    (g) = mruv = 1

2commute(u, v)     max(huv, hvu)     cover(g)

for the second inequality in the theorem, by theorem 4.9, for any x and y, commute(x, y)
equals 2mrxy which is less than or equal to 2m re    (g), implying hxy     2m re    (g). by
the markov inequality, since the expected time to reach y starting at any x is less than
2m re    (g), the id203 that y is not reached from x in 2m re    (g)e steps is at most
1
e . thus, the id203 that a vertex y has not been reached in 6e m re    (g) log n steps
is at most 1
n3 because a random walk of length 6e mre    (g) log n is a sequence of
e
3 log n id93, each of length 2emre    (g) and each possibly starting from di   erent

3 ln n = 1

108

vertices. suppose after a walk of 6em re    (g) log n steps, vertices v1, v2, . . . , vl had not
been reached. walk until v1 is reached, then v2, etc. by corollary 4.11 the expected time
for each of these is n3, but since each happens only with id203 1/n3, we e   ectively
take o(1) time per vi, for a total time at most n. more precisely,
cover(g)     6em re    (g) log n +

prob (v was not visited in the    rst 6em re    (g) steps) n3
n3 n3     6em re    (g) + n.

1

    6em re    (g) log n +

(cid:88)
(cid:88)

v

v

4.7 id93 in euclidean space

many physical processes such as brownian motion are modeled by id93.

id93 in euclidean d-space consisting of    xed length steps parallel to the co-
ordinate axes are really id93 on a d-dimensional lattice and are a special case
of id93 on graphs. in a random walk on a graph, at each time unit an edge
from the current vertex is selected at random and the walk proceeds to the adjacent vertex.

id93 on lattices

we now apply the analogy between id93 and current to lattices. consider
a random walk on a    nite segment    n, . . . ,   1, 0, 1, 2, . . . , n of a one dimensional lattice
starting from the origin. is the walk certain to return to the origin or is there some prob-
ability that it will escape, i.e., reach the boundary before returning? the id203 of
reaching the boundary before returning to the origin is called the escape id203. we
shall be interested in this quantity as n goes to in   nity.

convert the lattice to an electrical network by replacing each edge with a one ohm
resistor. then the id203 of a walk starting at the origin reaching n or    n before
returning to the origin is the escape id203 given by

pescape =

ce   
ca

where ce    is the e   ective conductance between the origin and the boundary points and ca
is the sum of the conductances at the origin. in a d-dimensional lattice, ca = 2d assuming
that the resistors have value one. for the d-dimensional lattice

pescape =

1

2d re   

in one dimension, the electrical network is just two series connections of n one-ohm re-
sistors connected in parallel. so as n goes to in   nity, re    goes to in   nity and the escape
id203 goes to zero as n goes to in   nity. thus, the walk in the unbounded one

109

0

1

2

3

12

4
20
number of resistors

in parallel

      

(a)

(b)

figure 4.10: 2-dimensional lattice along with the linear network resulting from shorting
resistors on the concentric squares about the origin.

dimensional lattice will return to the origin with id203 one. note, however, that
the expected time to return to the origin having taken one step away, which is equal to
commute(1, 0), is in   nite (theorem 4.9.

two dimensions

for the 2-dimensional lattice, consider a larger and larger square about the origin for
the boundary as shown in figure 4.10a and consider the limit of re    as the squares get
larger. shorting the resistors on each square can only reduce re    . shorting the resistors
results in the linear network shown in figure 4.10b. as the paths get longer, the number
of resistors in parallel also increases. the resistance between vertex i and i + 1 is really
4(2i + 1) unit resistors in parallel. the e   ective resistance of 4(2i + 1) resistors in parallel
is 1/4(2i + 1). thus,

re        1

4 + 1

12 + 1

20 +        = 1

4(1 + 1

3 + 1

5 +        ) =   (ln n).

since the lower bound on the e   ective resistance and hence the e   ective resistance goes
to in   nity, the escape id203 goes to zero for the 2-dimensional lattice.

three dimensions

in three dimensions, the resistance along any path to in   nity grows to in   nity but
the number of paths in parallel also grows to in   nity. it turns out there are a su   cient
number of paths that re    remains    nite and thus there is a nonzero escape id203.
we will prove this now. first note that shorting any edge decreases the resistance, so

110

y

7

3

1

1

3

7

x

figure 4.11: paths in a 2-dimensional lattice obtained from the 3-dimensional construc-
tion applied in 2-dimensions.

we do not use shorting in this proof, since we seek to prove an upper bound on the
resistance. instead we remove some edges, which increases their resistance to in   nity and
hence increases the e   ective resistance, giving an upper bound. to simplify things we
consider walks on a quadrant rather than the full grid. the resistance to in   nity derived
from only the quadrant is an upper bound on the resistance of the full grid.

the construction used in three dimensions is easier to explain    rst in two dimensions,
see figure 4.11. draw dotted diagonal lines at x + y = 2n     1. consider two paths
that start at the origin. one goes up and the other goes to the right. each time a path
encounters a dotted diagonal line, split the path into two, one which goes right and the
other up. where two paths cross, split the vertex into two, keeping the paths separate. by
a symmetry argument, splitting the vertex does not change the resistance of the network.
remove all resistors except those on these paths. the resistance of the original network is
less than that of the tree produced by this process since removing a resistor is equivalent
to increasing its resistance to in   nity.

the distances between splits increase and are 1, 2, 4, etc. at each split the number

111

1

2

4

figure 4.12: paths obtained from 2-dimensional lattice. distances between splits double
as do the number of parallel paths.

of paths in parallel doubles. see figure 4.12. thus, the resistance to in   nity in this two
dimensional example is

1
2

+

1
4

2 +

1
8

4 +        =

1
2

+

1
2

+

1
2

+        =    .

in the analogous three dimensional construction, paths go up, to the right, and out of
the plane of the paper. the paths split three ways at planes given by x + y + z = 2n     1.
each time the paths split the number of parallel segments triple. segments of the paths
between splits are of length 1, 2, 4, etc. and the resistance of the segments are equal to
the lengths. the resistance out to in   nity for the tree is

1

3 + 1

92 + 1

274 +        = 1

3

(cid:0)1 + 2

9 +       (cid:1) = 1

3

3 + 4

= 1

1
1    2
3

the resistance of the three dimensional lattice is less. it is important to check that the
paths are edge-disjoint and so the tree is a subgraph of the lattice. going to a subgraph is
equivalent to deleting edges which increases the resistance. that is why the resistance of
the lattice is less than that of the tree. thus, in three dimensions the escape id203
is nonzero. the upper bound on re    gives the lower bound

pescape = 1
2d

1
re   

    1
6.

a lower bound on re    gives an upper bound on pescape. to get the upper bound on

pescape, short all resistors on surfaces of boxes at distances 1, 2, 3,, etc. then

(cid:2)1 + 1

25 +       (cid:3)     1.23

9 + 1

6     0.2

re        1

6

this gives

pescape = 1
2d

1
re   

    5
6.

4.8 the web as a markov chain

a modern application of id93 on directed graphs comes from trying to estab-
lish the importance of pages on the world wide web. search engines output an ordered

112

1
20.85  i

pji

j

i

1
20.85  i

0.15  j

0.15  i

  i = 0.85  jpji + 0.85

2   i

  i = 1.48  jpji

figure 4.13: impact on id95 of adding a self loop

list of webpages in response to each search query. to do this, they have to solve two
problems at query time: (i)    nd the set of all webpages containing the query term(s) and
(ii) rank the webpages and display them (or the top subset of them) in ranked order. (i)
is done by maintaining a    reverse index    which we do not discuss here. (ii) cannot be
done at query time since this would make the response too slow. so search engines rank
the entire set of webpages (in the billions)    o   -line    and use that single ranking for all
queries. at query time, the webpages containing the query terms(s) are displayed in this
ranked order.

one way to do this ranking would be to take a random walk on the web viewed as a
directed graph (which we call the web graph) with an edge corresponding to each hyper-
text link and rank pages according to their stationary id203. hypertext links are
one-way and the web graph may not be strongly connected. indeed, for a node at the
   bottom    level there may be no out-edges. when the walk encounters this vertex the
walk disappears. another di   culty is that a vertex or a strongly connected component
with no in edges is never reached. one way to resolve these di   culties is to introduce
a random restart condition. at each step, with some id203 r, jump to a vertex se-
lected uniformly at random in the entire graph; with id203 1     r select an out-edge
at random from the current node and follow it. if a vertex has no out edges, the value
of r for that vertex is set to one. this makes the graph strongly connected so that the
stationary probabilities exist.

id95

the id95 of a vertex in a directed graph is the stationary id203 of the vertex,
where we assume a positive restart id203 of say r = 0.15. the restart ensures that
the graph is strongly connected. the id95 of a page is the frequency with which the
page will be visited over a long period of time. if the id95 is p, then the expected
time between visits or return time is 1/p. notice that one can increase the id95 of a
page by reducing the return time and this can be done by creating short cycles.

consider a vertex i with a single edge in from vertex j and a single edge out. the

113

stationary id203    satis   es   p =   , and thus

adding a self-loop at i, results in a new equation

  i =   jpji.

or

  i =   jpji +

1
2

  i

  i = 2   jpji.

of course,   j would have changed too, but ignoring this for now, id95 is doubled by
the addition of a self-loop. adding k self loops, results in the equation

  i =   jpji +

k

k + 1

  i,

and again ignoring the change in   j, we now have   i = (k + 1)  jpji. what prevents
one from increasing the id95 of a page arbitrarily? the answer is the restart. we
neglected the 0.15 id203 that is taken o    for the random restart. with the restart
taken into account, the equation for   i when there is no self-loop is

whereas, with k self-loops, the equation is

  i = 0.85  jpji

  i = 0.85  jpji + 0.85

k

k + 1

  i.

  jpji

solving for   i yields

  i =

0.85k + 0.85

0.15k + 1

which for k = 1 is   i = 1.48  jpji and in the limit as k         is   i = 5.67  jpji. adding a
single loop only increases id95 by a factor of 1.74.

relation to hitting time

recall the de   nition of hitting time hxy, which for two states x and y is the expected
time to reach y starting from x. here, we deal with hy, the average time to hit y, starting
at a random node. namely, hy = 1
x hxy, where the sum is taken over all n nodes x.
n
hitting time hy is closely related to return time and thus to the reciprocal of page rank.
return time is clearly less than the expected time until a restart plus hitting time. with
r as the restart value, this gives:

(cid:80)

return time to y     1
r

+ hy.

114

in the other direction, the fastest one could return would be if there were only paths of
length two (assume we remove all self-loops). a path of length two would be traversed
with at most id203 (1     r)2. with id203 r + (1     r) r = (2     r) r one restarts
and then hits v. thus, the return time is at least 2 (1     r)2 + (2     r) r    (hitting time).
combining these two bounds yields

2 (1     r)2 + (2     r) r(hitting time)     (return time)     1
r

+ (hitting time) .

the relationship between return time and hitting time can be used to see if a vertex has
unusually high id203 of short loops. however, there is no e   cient way to compute
hitting time for all vertices as there is for return time. for a single vertex v, one can
compute hitting time by removing the edges out of the vertex v for which one is com-
puting hitting time and then run the id95 algorithm for the new graph. the hitting
time for v is the reciprocal of the id95 in the graph with the edges out of v removed.
since computing hitting time for each vertex requires removal of a di   erent set of edges,
the algorithm only gives the hitting time for one vertex at a time. since one is probably
only interested in the hitting time of vertices with low hitting time, an alternative would
be to use a random walk to estimate the hitting time of low hitting time vertices.

spam

suppose one has a web page and would like to increase its id95 by creating other
web pages with pointers to the original page. the abstract problem is the following. we
are given a directed graph g and a vertex v whose id95 we want to increase. we may
add new vertices to the graph and edges from them to any vertices we want. we can also
add or delete edges from v. however, we cannot add or delete edges out of other vertices.

the id95 of v is the stationary id203 for vertex v with random restarts. if
we delete all existing edges out of v, create a new vertex u and edges (v, u) and (u, v),
then the id95 will be increased since any time the random walk reaches v it will be
captured in the loop v     u     v. a search engine can counter this strategy by more
frequent random restarts.

a second method to increase id95 would be to create a star consisting of the
vertex v at its center along with a large set of new vertices each with a directed edge to
v. these new vertices will sometimes be chosen as the target of the random restart and
hence the vertices increase the id203 of the random walk reaching v. this second
method is countered by reducing the frequency of random restarts.

notice that the    rst technique of capturing the random walk increases id95 but
does not e   ect hitting time. one can negate the impact on id95 of someone capturing
the random walk by increasing the frequency of random restarts. the second technique
of creating a star increases id95 due to random restarts and decreases hitting time.

115

one can check if the id95 is high and hitting time is low in which case the id95
is likely to have been arti   cially in   ated by the page capturing the walk with short cycles.

personalized id95

in computing id95, one uses a restart id203, typically 0.15, in which at each
step, instead of taking a step in the graph, the walk goes to a vertex selected uniformly
at random. in personalized id95, instead of selecting a vertex uniformly at random,
one selects a vertex according to a personalized id203 distribution. often the distri-
bution has id203 one for a single vertex and whenever the walk restarts it restarts
at that vertex. note that this may make the graph disconnected.

algorithm for computing personalized id95

first, consider the normal id95. let    be the restart id203 with which the
random walk jumps to an arbitrary vertex. with id203 1        the random walk
selects a vertex uniformly at random from the set of adjacent vertices. let p be a row
vector denoting the id95 and let a be the adjacency matrix with rows normalized to
sum to one. then

p =   

n (1, 1, . . . , 1) + (1       ) pa

p[i     (1       )a] =

  
n

(1, 1, . . . , 1)

or

p =   

n (1, 1, . . . , 1) [i     (1       ) a]   1.

thus, in principle, p can be found by computing the inverse of [i     (1       )a]   1. but
this is far from practical since for the whole web one would be dealing with matrices with
billions of rows and columns. a more practical procedure is to run the random walk and
observe using the basics of the power method in chapter 3 that the process converges to
the solution p.

for the personalized id95, instead of restarting at an arbitrary vertex, the walk
restarts at a designated vertex. more generally, it may restart in some speci   ed neighbor-
hood. suppose the restart selects a vertex using the id203 distribution s. then, in
the above calculation replace the vector 1
n (1, 1, . . . , 1) by the vector s. again, the compu-
tation could be done by a random walk. but, we wish to do the random walk calculation
for personalized id95 quickly since it is to be performed repeatedly. with more care
this can be done, though we do not describe it here.

4.9 bibliographic notes

the material on the analogy between id93 on undirected graphs and electrical
networks is from [ds84] as is the material on id93 in euclidean space. addi-

116

tional material on markov chains can be found in [mr95b], [mu05], and [per10]. for
material on id115 methods see [jer98] and [liu01].

the use of normalized conductance to prove convergence of markov chains is by
sinclair and jerrum, [sj89] and alon [alo86]. a polynomial time bounded markov chain
based method for estimating the volume of convex sets was developed by dyer, frieze and
kannan [dfk91].

117

4.10 exercises

exercise 4.1 the fundamental theorem of markov chains says that for a connected
markov chain, the long-term average distribution a(t) converges to a stationary distribu-
tion. does the t step distribution p(t) also converge for every connected markov chain?
consider the following examples: (i) a two-state chain with p12 = p21 = 1. (ii) a three
state chain with p12 = p23 = p31 = 1 and the other pij = 0. generalize these examples to
produce markov chains with many states.
exercise 4.2 does limt       a(t)     a(t + 1) = 0 imply that a(t) converges to some value?
hint: consider the average cumulative sum of the digits in the sequence 1021408116       

exercise 4.3 what is the stationary id203 for the following networks.

0.6

0.4 0.6

0.4 0.6

0.4 0.6

0.4

a

0.5

0

0.5

0

0.5

0

0.5

0

0.4

0.5

0.6

1

0.5

0.5
b

0.5

exercise 4.4 a markov chain is said to be symmetric if for all i and j, pij = pji. what
is the stationary distribution of a connected symmetric chain? prove your answer.

exercise 4.5 prove |p     q|1 = 2(cid:80)

i(pi     qi)+ for id203 distributions p and q,

(proposition 4.4).
exercise 4.6 let p(x), where x = (x1, x2, . . . , xd) xi     {0, 1}, be a multivariate probabil-
ity distribution. for d = 100, how would you estimate the marginal distribution

(cid:88)

p(x1) =

p(x1, x2, . . . , xd) ?

x2,...,xd

exercise 4.7 using the metropolis-hasting algorithm create a markov chain whose sta-
tionary id203 is that given in the following table. use the 3   3 lattice for the under-
lying graph.

x1x2
prob

00
1/16

01
1/8

02
1/16

10
1/8

11
1/4

12
1/8

20
1/16

21
1/8

22
1/16

exercise 4.8 using id150 create a 4    4 lattice where vertices in rows and
columns are cliques whose stationary id203 is that given in the following table.

118

x/y

1

2

3

4

1

1
16
1
32
1
32
1
16

2

1
32
1
8
1
8
1
32

3

1
32
1
8
1
8
1
32

4

1
16
1
32
1
32
1
16

note by symmetry there are only three types of vertices and only two types of rows or
columns.

exercise 4.9 how would you integrate a high dimensional multivariate polynomial dis-
tribution over some convex region?

exercise 4.10 given a time-reversible markov chain, modify the chain as follows. at
the current state, stay put (no move) with id203 1/2. with the other id203 1/2,
move as in the old chain. show that the new chain has the same stationary distribution.
what happens to the convergence time in this modi   cation?

exercise 4.11 let p be a id203 vector (nonnegative components adding up to 1) on
the vertices of a connected graph which is su   ciently large that it cannot be stored in a
computer. set pij (the transition id203 from i to j) to pj for all i (cid:54)= j which are
adjacent in the graph. show that the stationary id203 vector is p. is a random walk
an e   cient way to sample according to a id203 distribution that is close to p? think,
for example, of the graph g being the n-dimensional hypercube with 2n vertices, and p as
the uniform distribution over those vertices.

(cid:1). repeat adding a self loop with id203 1

exercise 4.12 construct the edge id203 for a three state markov chain where each
pair of states is connected by an undirected edge so that the stationary id203 is
2, 1

(cid:0) 1
exercise 4.13 consider a three state markov chain with stationary id203(cid:0) 1

2 to the vertex with id203 1
2.

(cid:1).

3, 1

6

2, 1

3, 1

6

consider the metropolis-hastings algorithm with g the complete graph on these three
vertices. for each edge and each direction what is the expected id203 that we would
actually make a move along the edge?
exercise 4.14 consider a distribution p over {0, 1}2 with p(00) = p(11) = 1
2 and p(01) =
p(10) = 0. give a connected graph on {0, 1}2 that would be bad for running metropolis-
hastings and a graph that would be good for running metropolis-hastings. what would be
the problem with id150?
exercise 4.15 consider p(x) where x     {0, 1}100 such that p (0) = 1
for x (cid:54)= 0. how does id150 behave?

2 and p (x) = 1/2

(2100   1)

119

exercise 4.16 given a connected graph g and an integer k how would you generate
connected subgraphs of g with k vertices with id203 proportional to the number of
edges in the subgraph? a subgraph of g does not need to have all edges of g that join
vertices of the subgraph. the probabilities need not be exactly proportional to the number
of edges and you are not expected to prove your algorithm for this problem.

exercise 4.17 suppose one wishes to generate uniformly at random a regular, degree
three, undirected, not necessarily connected multi-graph with 1,000 vertices. a multi-
graph may have multiple edges between a pair of vertices and self loops. one decides to
do this by a id115 technique. in particular, consider a (very large)
network where each vertex corresponds to a regular degree three, 1,000 vertex multi-graph.
for edges, say that the vertices corresponding to two graphs are connected by an edge if
one graph can be obtained from the other by a    ip of a pair of edges. in a    ip, a pair of
edges (a, b) and (c, d) are replaced by (a, c) and (b, d).

1. prove that the network whose vertices correspond to the desired graphs is connected.
that is, for any two 1000-vertex degree-3 multigraphs, it is possible to walk from
one to the other in this network.

2. prove that the stationary id203 of the random walk is uniform over all vertices.

3. give an upper bound on the diameter of the network.

4. how would you modify the process if you wanted to uniformly generate connected

degree three multi-graphs?

in order to use a random walk to generate the graphs in a reasonable amount of time, the
random walk must rapidly converge to the stationary id203. proving this is beyond
the material in this book.

exercise 4.18 construct, program, and execute an algorithm to estimate the volume of
a unit radius sphere in 20 dimensions by carrying out a random walk on a 20 dimensional
grid with 0.1 spacing.

exercise 4.19 what is the mixing time for the undirected graphs

1. two cliques connected by a single edge?

2. a graph consisting of an n vertex clique plus one additional vertex connected to one

vertex in the clique.

exercise 4.20 what is the mixing time for

1. g(n, p) with p = log n
n ?

2. a circle with n vertices where at each vertex an edge has been added to another
vertex chosen at random. on average each vertex will have degree four, two circle
edges, and an edge from that vertex to a vertex chosen at random, and possible some
edges that are the ends of the random edges from other vertices.

120

exercise 4.21 find the  -mixing time for a 2-dimensional lattice with n vertices in each
coordinate direction with a uniform id203 distribution. to do this solve the following
problems.

1. the minimum number of edges leaving a set s of size greater than or equal to n2/4

is n.

2. the minimum number of edges leaving a set s of size less than or equal to n2/4 is

(cid:98)   

s(cid:99).

3. compute   (s)

4. compute   

5. computer the  -mixing time

exercise 4.22 find the  -mixing time for a d-dimensional lattice with n vertices in each
coordinate direction with a uniform id203 distribution. to do this, solve the following
problems.

1. select a direction say x1 and push all elements of s in each column perpendicular
to x1 = 0 as close to x1 = 0 as possible. prove that the number of edges leaving s
is at least as large as the number leaving the modi   ed version of s.

2. repeat step one for each direction. argue that for a direction say x1, as x1 gets

larger a set in the perpendicular plane is contained in the previous set.

3. optimize the arrangements of elements in the plane x1 = 0 and move elements from
farthest out plane in to make all planes the same shape as x1 = 0 except for some
leftover elements of s in the last plane. argue that this does not increase the number
of edges out.

4. what con   gurations might we end up with?

5. argue that for a given size, s has at least as many edges as the modi   ed version of

s.

6. what is   (s) for a modi   ed form s?

7. what is    for a d-dimensional lattice?

8. what is the  -mixing time?

exercise 4.23

1. what is the set of possible id94 on a connected graph if there are only

interior vertices and no boundary vertices that supply the boundary condition?

121

2. let qx be the stationary id203 of vertex x in a random walk on an undirected
graph where all edges at a vertex are equally likely and let dx be the degree of vertex
x. show that qx
dx

is a harmonic function.

3. if there are multiple id94 when there are no boundary conditions, why

is the stationary id203 of a random walk on an undirected graph unique?

4. what is the stationary id203 of a random walk on an undirected graph?

exercise 4.24 in section 4.5, given an electrical network, we de   ne an associated markov
chain such that voltages and currents in the electrical network corresponded to properties
of the markov chain. can we go in the reverse order and for any markov chain construct
the equivalent electrical network?

exercise 4.25 what is the id203 of reaching vertex 1 before vertex 5 when starting
a random walk at vertex 4 in each of the following graphs.

1.

2.

1

2

3

4

5

1

2

3

4

6

5

exercise 4.26 consider the electrical resistive network in figure 4.14 consisting of ver-
tices connected by resistors. kircho       s law states that the currents at each vertex sum to
zero. ohm   s law states that the voltage across a resistor equals the product of the resis-
tance times the current through it. using these laws calculate the e   ective resistance of
the network.

exercise 4.27 consider the electrical network of figure 4.15.

1. set the voltage at a to one and at b to zero. what are the voltages at c and d?

2. what is the current in the edges a to c, a to d, c to d. c to b and d to b?

3. what is the e   ective resistance between a and b?

4. convert the electrical network to a graph. what are the edge probabilities at each
vertex so that the id203 of a walk starting at c (d) reaches a before b equals the
voltage at c (the voltage at d).?

122

r1

r3

i1

r2

i2

figure 4.14: an electrical network of resistors.

a

r=1

r=2

c

d

r=2

r=1

b

r=1

figure 4.15: an electrical network of resistors.

5. what is the id203 of a walk starting at c reaching a before b? a walk starting

at d reaching a before b?

6. what is the net frequency that a walk from a to b goes through the edge from c to

d?

7. what is the id203 that a random walk starting at a will return to a before

reaching b?

exercise 4.28 consider a graph corresponding to an electrical network with vertices a
and b. prove directly that ce   
must be less than or equal to one. we know that this is the
ca
escape id203 and must be at most 1. but, for this exercise, do not use that fact.

(cid:80)

exercise 4.29 (thomson   s principle) the energy dissipated by the resistance of edge xy
in an electrical network is given by i2
xyrxy. the total energy dissipation in the network
is e = 1
2 accounts for the fact that the dissipation in each edge is
2
counted twice in the summation. show that the actual current distribution is the distribu-
tion satisfying ohm   s law that minimizes energy dissipation.

xyrxy where the 1
i2

x,y

123

u

v

u

v

u

v

figure 4.16: three graphs

(a)

(b)

(c)

1

1

1

2

2

2

3

3

3

4

4

4

figure 4.17: three graph

exercise 4.30 (rayleigh   s law) prove that reducing the value of a resistor in a network
cannot increase the e   ective resistance. prove that increasing the value of a resistor cannot
decrease the e   ective resistance. you may use thomson   s principle exercise 4.29.

exercise 4.31 what is the hitting time huv for two adjacent vertices on a cycle of length
n? what is the hitting time if the edge (u, v) is removed?

exercise 4.32 what is the hitting time huv for the three graphs if figure 4.16.

exercise 4.33 show that adding an edge can either increase or decrease hitting time by
calculating h24 for the three graphs in figure 4.17.

exercise 4.34 consider the n vertex connected graph shown in figure 4.18 consisting
of an edge (u, v) plus a connected graph on n     1 vertices and m edges. prove that
huv = 2m + 1 where m is the number of edges in the n     1 vertex subgraph.

exercise 4.35 consider a random walk on a clique of size n. what is the expected
number of steps before a given vertex is reached?

124

n     1
vertices
m edges

u

v

figure 4.18: a connected graph consisting of n     1 vertices and m edges along with a
single edge (u, v).

exercise 4.36 what is the most general solution to the di   erence equation t(i + 2)    
5t(i + 1) + 6t(i) = 0. how many boundary conditions do you need to make the solution
unique?
exercise 4.37 given the di   erence equation akt(i + k) + ak   1t(i + k     1) +        + a1t(i +
1)+a0t(i) = 0 the polynomial aktk +ak   itk   1 +      +a1t+a0 = 0 is called the characteristic
polynomial.

1. if the equation has a set of r distinct roots, what is the most general form of the

solution?

2. if the roots of the characteristic polynomial are not distinct what is the most general

form of the solution?

3. what is the dimension of the solution space?

4. if the di   erence equation is not homogeneous (i.e., the right hand side is not 0) and
f(i) is a speci   c solution to the nonhomogeneous di   erence equation, what is the full
set of solutions to the nonhomogeneous di   erence equation?

exercise 4.38 show that adding an edge to a graph can either increase or decrease com-
mute time.
exercise 4.39 consider the set of integers {1, 2, . . . , n}.

1. what is the expected number of draws with replacement until the integer 1 is drawn.

2. what is the expected number of draws with replacement so that every integer is

drawn?

exercise 4.40 for each of the three graphs below what is the return time starting at
vertex a? express your answer as a function of the number of vertices, n, and then
express it as a function of the number of edges m.

125

a b

n vertices

a

a

b

    n     2    

b

a

b
n     1
clique

c

exercise 4.41 suppose that the clique in exercise 4.40 was replaced by an arbitrary graph
with m    1 edges. what would be the return time to a in terms of m, the total number of
edges.

exercise 4.42 suppose that the clique in exercise 4.40 was replaed by an arbitrary graph
with m    d edges and there were d edges from a to the graph. what would be the expected
length of a random path starting at a and ending at a after returning to a exactly d
times.

exercise 4.43 given an undirected graph with a component consisting of a single edge
   nd two eigenvalues of the laplacian l = d    a where d is a diagonal matrix with vertex
degrees on the diagonal and a is the adjacency matrix of the graph.

exercise 4.44 a researcher was interested in determining the importance of various
edges in an undirected graph. he computed the stationary id203 for a random walk
on the graph and let pi be the id203 of being at vertex i. if vertex i was of degree
di, the frequency that edge (i, j) was traversed from i to j would be 1
pi and the frequency
di
that the edge was traversed in the opposite direction would be 1
pj. thus, he assigned an
dj

(cid:12)(cid:12)(cid:12) to the edge. what is wrong with his idea?

importance of

pi     1

(cid:12)(cid:12)(cid:12) 1

pj

dj

di

exercise 4.45 prove that two independent id93 starting at the origin on a two
dimensional lattice will eventually meet with id203 one.

exercise 4.46 suppose two individuals are    ipping balanced coins and each is keeping
tract of the number of heads minus the number of tails. at some time will both individual   s
counts be the same?

exercise 4.47 consider the lattice in 2-dimensions. in each square add the two diagonal
edges. what is the escape id203 for the resulting graph?

exercise 4.48 determine by simulation the escape id203 for the 3-dimensional lat-
tice.

exercise 4.49 what is the escape id203 for a random walk starting at the root of
an in   nite binary tree?

126

e

d

a

c

b

d

a

c

b

figure 4.19: an undirected and a directed graph.

exercise 4.50 consider a random walk on the positive half line, that is the integers
0, 1, 2, . . .. at the origin, always move right one step. at all other integers move right
with id203 2/3 and left with id203 1/3. what is the escape id203?

exercise 4.51 consider the graphs in figure 4.19. calculate the stationary distribution
for a random walk on each graph and the    ow through each edge. what condition holds
on the    ow through edges in the undirected graph? in the directed graph?

exercise 4.52 create a random directed graph with 200 vertices and roughly eight edges
per vertex. add k new vertices and calculate the id95 with and without directed edges
from the k added vertices to vertex 1. how much does adding the k edges change the
id95 of vertices for various values of k and restart frequency? how much does adding
a loop at vertex 1 change the id95? to do the experiment carefully one needs to
consider the id95 of a vertex to which the star is attached. if it has low id95 its
page rank is likely to increase a lot.

exercise 4.53 repeat the experiment in exercise 4.52 for hitting time.

exercise 4.54 search engines ignore self loops in calculating id95. thus, to increase
id95 one needs to resort to loops of length two. by how much can you increase the
page rank of a page by adding a number of loops of length two?
exercise 4.55 number the vertices of a graph {1, 2, . . . , n}. de   ne hitting time to be the
expected time from vertex 1. in (2) assume that the vertices in the cycle are sequentially
numbered.

1. what is the hitting time for a vertex in a complete directed graph with self loops?

2. what is the hitting time for a vertex in a directed cycle with n vertices?

127

create exercise relating strongly connected and full rank
full rank implies strongly connected.
strongly connected does not necessarily imply full rank

       0 0 1

0 0 1
1 1 0

      

is graph aperiodic i      1 >   2?

exercise 4.56 using a web browser bring up a web page and look at the source html.
how would you extract the url   s of all hyperlinks on the page if you were doing a crawl
of the web? with internet explorer click on    source    under    view    to access the html
representation of the web page. with firefox click on    page source    under    view   .

exercise 4.57 sketch an algorithm to crawl the world wide web. there is a time delay
between the time you seek a page and the time you get it. thus, you cannot wait until the
page arrives before starting another fetch. there are conventions that must be obeyed if
one were to actually do a search. sites specify information as to how long or which    les
can be searched. do not attempt an actual search without guidance from a knowledgeable
person.

128

5 machine learning

5.1 introduction

machine learning algorithms are general purpose tools for generalizing from data.
they have proven to be able to solve problems from many disciplines without detailed
domain-speci   c knowledge. to date they have been highly successful for a wide range of
tasks including id161, id103, document classi   cation, automated
driving, computational science, and decision support.

the core problem. a core problem underlying many machine learning applications
is learning a good classi   cation rule from labeled data. this problem consists of a do-
main of interest x , called the instance space, such as the set of email messages or patient
records, and a classi   cation task, such as classifying email messages into spam versus
non-spam or determining which patients will respond well to a given medical treatment.
we will typically assume our instance space x = {0, 1}d or x = rd, corresponding to
data that is described by d boolean or real-valued features. features for email messages
could be the presence or absence of various types of words, and features for patient records
could be the results of various medical tests. to perform the learning task, our learning
algorithm is given a set s of labeled training examples, which are points in x along with
their correct classi   cation. this training data could be a collection of email messages,
each labeled as spam or not spam, or a collection of patients, each labeled by whether
or not they responded well to the given medical treatment. our algorithm then aims to
use the training examples to produce a classi   cation rule that will perform well over new
data, i.e., new points in x . a key feature of machine learning, which distinguishes it
from other algorithmic tasks, is that our goal is generalization: to use one set of data in
order to perform well on new data we have not seen yet. we focus on binary classi   cation
where items in the domain of interest are classi   ed into two categories (called the positive
class and the negative class), as in the medical and spam-detection examples above, but
nearly all the techniques described here will also apply to multi-way classi   cation.

how to learn. a high-level approach that many algorithms we discuss will follow is
to try to    nd a    simple    rule with good performance on the training data. for instance,
in the case of classifying email messages, we might    nd a set of highly indicative words
such that every spam email in the training data has at least one of these words and none
of the non-spam emails has any of them; in this case, the rule    if the message has any of
these words then it is spam, else it is not    would be a simple rule that performs well on
the training data. or, we might    nd a way of weighting words with positive and negative
weights such that the total weighted sum of words in the email message is positive on the
spam emails in the training data, and negative on the non-spam emails. we will then
argue that so long as the training data is representative of what future data will look
like, we can be con   dent that any su   ciently    simple    rule that performs well on the
training data will also perform well on future data. to make this into a formal math-

129

margin

figure 5.1: margin of a linear separator.

ematical statement, we need to be precise about what we mean by    simple    as well as
what it means for training data to be    representative    of future data. in fact, we will see
several notions of complexity, including bit-counting and vc-dimension, that will allow
us to make mathematical statements of this form. these statements can be viewed as
formalizing the intuitive philosophical notion of occam   s razor.

5.2 the id88 algorithm

to help ground our discussion, we begin by describing a speci   c interesting learning
algorithm, the id88 algorithm, for the problem of assigning positive and negative
weights to features (such as words) so that each positive example has a positive sum of
feature weights and each negative example has a negative sum of feature weights.

more speci   cally, the id88 algorithm is an e   cient algorithm for    nding a lin-
ear separator in d-dimensional space, with a running time that depends on the margin
of separation of the data. we are given as input a set s of training examples (points in
d-dimensional space), each labeled as positive or negative, and our assumption is that
there exists a vector w    such that for each positive example x     s we have xt w        1
and for each negative example x     s we have xt w           1. note that the quantity
xt w   /|w   | is the distance of the point x to the hyperplane xt w    = 0. thus, we can view
our assumption as stating that there exists a linear separator through the origin with all
positive examples on one side, all negative examples on the other side, and all examples
at distance at least    = 1/|w   | from the separator. this quantity    is called the margin
of separation (see figure 5.1).

the goal of the id88 algorithm is to    nd a vector w such that xt w > 0 for all
positive examples x     s, and xt w < 0 for all negative examples x     s. it does so via

130

the following update rule:

the id88 algorithm: start with the all-zeroes weight vector w = 0. then repeat
the following until xt w has the correct sign for all x     s (positive for positive examples and
negative for negative examples):

1. let x     s be an example for which xt w does not have the correct sign.

2. update as follows:

(a) if x is a positive example, let w     w + x.
(b) if x is a negative example, let w     w     x.

while simple, the id88 algorithm indeed will    nd a linear separator whenever
one exists, making at most (r/  )2 updates where r = maxx   s |x|. thus, if there exists
a hyperplane through the origin that correctly separates the positive examples from the
negative examples by a large margin relative to the radius of the smallest ball enclosing
the data, then the total number of updates will be small.
theorem 5.1 if there exists a vector w    such that xt w        1 for all positive examples
x     s and xt w           1 for all negative examples x     s (i.e., a linear separator of margin
   = 1/|w   |), then the number of updates made by the id88 algorithm is at most
r2|w   |2, where r = maxx   s |x|.

to get a feel for this bound, notice that if we multiply all entries in all the x     s by
100, we can divide all entries in w    by 100 and it will still satisfy the    if   condition. so
the bound is invariant to this kind of scaling, i.e., to our    units of measurement   .
proof of theorem 5.1: fix some w    satisfying the    if    condition of the theorem. we
will keep track of two quantities, wt w    and |w|2. first of all, each time we make an
update, wt w    increases by at least 1. that is because if x is a positive example, then

(w + x)t w    = wt w    + xt w        wt w    + 1,

by de   nition of w   . similarly, if x is a negative example, then

(w     x)t w    = wt w        xt w        wt w    + 1.

next, on each update, we claim that |w|2 increases by at most r2. let us    rst consider
updates on positive examples. if we update on a positive example x then we have

(w + x)t (w + x) = |w|2 + 2xt w + |x|2     |w|2 + |x|2     |w|2 + r2,

where the middle inequality comes from the fact that we only perform an update on a
positive example when xt w     0. similarly, if we update on a negative example x then
we have

(w     x)t (w     x) = |w|2     2xt w + |x|2     |w|2 + |x|2     |w|2 + r2.

131

note that it is important here that we only update on examples for which xt w has the
incorrect sign.

   

so, if we make m updates, then wt w        m , and |w|2     m r2, or equivalently,
m . finally, we use the fact that wt w   /|w   |     |w| which is just saying that
|w|     r
the projection of w in the direction of w    cannot be larger than the length of w. this
gives us:

   

m/|w   |     r

   

m
m     r|w   |
m     r2|w   |2

as desired.

what if there is no w    that perfectly separates the positive and negative examples? in
section 5.8 we will address this in the context of an online learning model, and we will see
that the id88 algorithm enjoys strong guarantees even if the best w    is not quite
perfect, as a function of a quantity called the    hinge loss    of w   .

in the next section, we consider a related issue. suppose the positive and negative
examples are not linearly separable (there is no hyperplane with the positives on one side
and the negatives on the other) but they are separable by some other simple curve such
as a circle. in that case, we can use a technique known as id81s.

5.3 id81s

suppose that instead of a linear separator decision boundary, the boundary between im-
portant emails and unimportant emails looks more like a circle, for example as in figure
5.2.

a powerful idea for addressing situations like this is to use what are called kernel
functions, or sometimes the    kernel trick   . here is the idea. suppose you have a function
k, called a    kernel   , over pairs of data points such that for some function    : rd     rn ,
where perhaps n (cid:29) d, we have k(x, x(cid:48)) =   (x)t   (x(cid:48)).
in that case, if we can write
the id88 algorithm so that it only interacts with the data via dot-products, and
then replace every dot-product with an invocation of k, then we can act as if we had
performed the function    explicitly without having to actually compute   .

for example, consider k(x, x(cid:48)) = (1 + xt x(cid:48))k for some integer k     1. it turns out this
corresponds to a mapping    into a space of dimension n     dk. for example, in the case
d = 2, k = 2 we have (using xi to denote the ith coordinate of x):

k(x, x(cid:48)) = (1 + x1x(cid:48)
= 1 + 2x1x(cid:48)
=   (x)t   (x(cid:48))

1 + x2x(cid:48)
2)2
1 + 2x2x(cid:48)
2 + x2

1x(cid:48)2

1 + 2x1x2x(cid:48)

1x(cid:48)

2 + x2

2x(cid:48)2

2

132

y

  

x

figure 5.2: data that is not linearly separable in the input space r2 but that is linearly
2), corresponding to the
separable in the      -space,      (x) = (1,
id81 k(x, x(cid:48)) = (1 + x1x(cid:48)

   
2x2, x2
1,

2x1,
1 + x2x(cid:48)
2)2.

2x1x2, x2

   

   

   

   

   

2x2, x2
1,

2x1,

for   (x) = (1,
2). notice also that a linear separator in this
space could correspond to a more complicated decision boundary such as an ellipse in
the original space. for instance, the hyperplane   (x)t w    = 0 for w    = (   4, 0, 0, 1, 0, 1)
corresponds to the circle x2

2 = 4 in the original space, such as in figure 5.2.

1 + x2

2x1x2, x2

the point of this is that if in the higher-dimensional      -space    there is a w    such
that the bound of theorem 5.1 is small, then the algorithm will halt after not too many
updates (and later we will see that under reasonable assumptions on the data, this implies
we can be con   dent in its ability to perform well on new data as well). but the nice thing
is we didn   t have to computationally perform the mapping   !

so, how can we view the id88 algorithm as only interacting with data via dot-
products? notice that w is always a linear combination of data points. for example, if
we made updates on examples x1, x2, and x5, and these examples were positive, positive,
and negative respectively, we would have w = x1 + x2     x5. so, if we keep track of w
this way, then to classify a new example x, we can write xt w = xt x1 + xt x2     xt x5.
so if we just replace each of these dot-products with    k   , we are running the algorithm
as if we had explicitly performed the    mapping. this is called    kernelizing    the algorithm.

many di   erent pairwise functions on examples are legal id81s. one easy
way to create a id81 is by combining other id81s together, via the
following theorem.

theorem 5.2 suppose k1 and k2 are id81s. then

1. for any constant c     0, ck1 is a legal kernel. in fact, for any scalar function f ,

the function k3(x, x(cid:48)) = f (x)f (x(cid:48))k1(x, x(cid:48)) is a legal kernel.

2. the sum k1 + k2, is a legal kernel.

133

3. the product, k1k2, is a legal kernel.

you will prove theorem 5.2 in exercise 5.9. notice that this immediately implies that the
function k(x, x(cid:48)) = (1 + xt x(cid:48))k is a legal kernel by using the fact that k1(x, x(cid:48)) = 1 is a
legal kernel, k2(x, x(cid:48)) = xt x(cid:48) is a legal kernel, then adding them, and then multiplying
that by itself k times. another popular kernel is the gaussian kernel, de   ned as:

k(x, x(cid:48)) = e   c|x   x(cid:48)|2.

if we think of a kernel as a measure of similarity, then this kernel de   nes the similarity
between two data objects as a quantity that decreases exponentially with the squared
distance between them. the gaussian kernel can be shown to be a true kernel func-
tion by    rst writing it as f (x)f (x(cid:48))e2cxt x(cid:48)
for f (x) = e   c|x|2 and then taking the taylor
expansion of e2cxt x(cid:48)
, applying the rules in theorem 5.2. technically, this last step re-
quires considering countably in   nitely many applications of the rules and allowing for
in   nite-dimensional vector spaces.

5.4 generalizing to new data

so far, we have focused on the problem of    nding a classi   cation rule that performs well
on a given set s of training data. but what we really want our classi   cation rule to do
is to perform well on new data we have not seen yet. to make guarantees of this form,
we need some assumption that our training data is somehow representative of what new
data will look like; formally, we will assume they are drawn from the same id203
distribution. additionally, we will see that we will want our algorithm   s classi   cation
rule to be    simple    in some way. together, these two conditions will allow us to make
generalization guarantees: guarantees on the ability of our learned classi   cation rule to
perform well on new unseen data.

formalizing the problem. to formalize the learning problem, assume there is some
id203 distribution d over the instance space x , such that (a) our training set s con-
sists of points drawn independently at random from d, and (b) our objective is to predict
well on new points that are also drawn from d. this is the sense in which we assume that
our training data is representative of future data. let c   , called the target concept, denote
the subset of x corresponding to the positive class for the binary classi   cation we are
aiming to make. for example, c    would correspond to the set of all patients who respond
well to a given treatment in a medical scenario, or it could correspond to the set of all
spam emails in a spam-detection scenario. so, each point in our training set s is labeled
according to whether or not it belongs to c    and our goal is to produce a set h     x , called
our hypothesis, which is close to c    with respect to distribution d. the true error of h is
errd(h) = prob(h(cid:52)c   ) where    (cid:52)    denotes symmetric di   erence, and id203 mass is
according to d. in other words, the true error of h is the id203 it incorrectly clas-
si   es a data point drawn at random from d. our goal is to produce h of low true error.
the training error of h, denoted errs(h), is the fraction of points in s on which h and

134

c    disagree. that is, errs(h) = |s     (h(cid:52)c   )|/|s|. training error is also called empirical
error. note that even though s is assumed to consist of points randomly drawn from d,
it is possible for a hypothesis h to have low training error or even to completely agree with
c    over the training sample, and yet have high true error. this is called over   tting the
training data. for instance, a hypothesis h that simply consists of listing the positive ex-
amples in s, which is equivalent to a rule that memorizes the training sample and predicts
positive on an example if and only if it already appeared positively in the training sample,
would have zero training error. however, this hypothesis likely would have high true error
and therefore would be highly over   tting the training data. more generally, over   tting is
a concern because algorithms will typically be optimizing over the training sample. to
design and analyze algorithms for learning, we will have to address the issue of over   tting.

to analyze over   tting, we introduce the notion of an hypothesis class, also called a
concept class or set system. an hypothesis class h over x is a collection of subsets of
x , called hypotheses. for instance, the class of intervals over x = r is the collection
{[a, b]|a     b}. the class of linear separators over x = rd is the collection

{{x     rd|w    x     w0}|w     rd, w0     r};

that is, it is the collection of all sets in rd that are linearly separable from their comple-
ment. in the case that x is the set of 4 points in the plane {(   1,   1), (   1, 1), (1,   1), (1, 1)},
the class of linear separators contains 14 of the 24 = 16 possible subsets of x .17 given
an hypothesis class h and training set s, what we typically aim to do algorithmically
is to    nd the hypothesis in h that most closely agrees with c    over s. for example, we
saw that the id88 algorithm will    nd a linear separator that agrees with the target
function over s so long as s is linearly separable. to address over   tting, we argue that if
s is large enough compared to some property of h, then with high id203 all h     h
have their training error close to their true error, so that if we    nd a hypothesis whose
training error is low, we can be con   dent its true error will be low as well.

before giving our    rst result of this form, we note that it will often be convenient to

associate each hypotheses with its {   1, 1}-valued indicator function

(cid:26) 1 x     h

   1 x (cid:54)    h

h(x) =

in this notation the true error of h is errd(h) = probx   d[h(x) (cid:54)= c   (x)] and the training
error is errs(h) = probx   s[h(x) (cid:54)= c   (x)].

5.5 over   tting and uniform convergence

we now present two generalization guarantees that explain how one can guard against
over   tting. to keep things simple, we assume our hypothesis class h is    nite. later, we
17the only two subsets that are not in the class are the sets {(   1,   1), (1, 1)} and {(   1, 1), (1,   1)}.

135

will see how to extend these results to in   nite classes as well. given a class of hypotheses
h, the    rst result states that for any given   greater than zero, so long as the training
  ln(|h|), it is unlikely any hypothesis h     h will have zero
data set is large compared to 1
training error but have true error greater than  . this means that with high id203,
any hypothesis that our algorithms    nds that agrees with the target hypothesis on the
training data will have low true error. the second result states that if the training data
 2 ln(|h|), then it is unlikely that the training error and true
set is large compared to 1
error will di   er by more than   for any hypothesis in h. this means that if we    nd an
hypothesis in h whose training error is low, we can be con   dent its true error will be low
as well, even if its training error is not zero.

the basic idea is the following. if we consider some h with large true error, and we
select an element x     x at random according to d, there is a reasonable chance that
x will belong to the symmetric di   erence h(cid:52)c   .
if we select a large enough training
sample s with each point drawn independently from x according to d, the chance that
s is completely disjoint from h(cid:52)c    will be incredibly small. this is just for a single
hypothesis h but we can now apply the union bound over all h     h of large true error,
when h is    nite. we formalize this below.
theorem 5.3 let h be an hypothesis class and let   and    be greater than zero. if a
training set s of size

(cid:0) ln|h| + ln(1/  )(cid:1),

n     1
 

is drawn from distribution d, then with id203 greater than or equal to 1        every
h in h with true error errd(h)       has training error errs(h) > 0. equivalently, with
id203 greater than or equal to 1       , every h     h with training error zero has true
error less than  .
proof: let h1, h2, . . . be the hypotheses in h with true error greater than or equal to  .
these are the hypotheses that we don   t want to output. consider drawing the sample s
of size n and let ai be the event that hi is consistent with s. since every hi has true error
greater than or equal to  

prob(ai)     (1      )n.

in other words, if we    x hi and draw a sample s of size n, the chance that hi makes no
mistakes on s is at most the id203 that a coin of bias   comes up tails n times in a
row, which is (1      )n. by the union bound over all i we have

prob (   iai)     |h|(1      )n.

using the fact that (1     )     e    , the id203 that any hypothesis in h with true error
greater than or equal to   has training error zero is at most |h|e    n. replacing n by the
sample size bound from the theorem statement, this is at most |h|e    ln |h|   ln(1/  ) =    as
desired.

136

(cid:122)

(cid:125)(cid:124)

not spam

(cid:123)

(cid:122)

(cid:123)

x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16

emails

0

0

0

1

0

0

0

0

0

0

0

1

0

0

1

1

1

1

1

1

1

0

1

1

1

1

target concept

hypothesis hi

(cid:125)(cid:124)

spam

   
1
(cid:108)
0
   

   
1
(cid:108)
1
   

   
0
(cid:108)
0
   

figure 5.3: the hypothesis hi disagrees with the truth in one quarter of the emails. thus
with a training set |s|, the id203 that the hypothesis will survive is (1     0.25)|s|

the conclusion of theorem 5.3 is sometimes called a    pac-learning guarantee    since
it states that if we can    nd an h     h consistent with the sample, then this h is probably
approximately correct.

theorem 5.3 addressed the case where there exists a hypothesis in h with zero train-
ing error. what if the best hi in h has 5% error on s? can we still be con   dent that its
true error is low, say at most 10%? for this, we want an analog of theorem 5.3 that says
for a su   ciently large training set s, every hi     h has training error within     of the
true error with high id203. such a statement is called uniform convergence because
we are asking that the training set errors converge to their true errors uniformly over all
sets in h. to see intuitively why such a statement should be true for su   ciently large
s and a single hypothesis hi, consider two strings that di   er in 10% of the positions and
randomly select a large sample of positions. the number of positions that di   er in the
sample will be close to 10%.

dom variables with prob(xi = 1) = p. let s =(cid:80)

to prove uniform convergence bounds, we use a tail inequality for sums of independent
bernoulli random variables (i.e., coin tosses). the following is particularly convenient and
is a variation on the cherno    bounds in section 12.6.1 of the appendix.
theorem 5.4 (hoe   ding bounds) let x1, x2, . . . , xn be independent {0, 1}-valued ran-
i xi (equivalently,    ip n coins of bias p
and let s be the total number of heads). for any 0            1,
prob(s/n > p +   )     e   2n  2
prob(s/n < p       )     e   2n  2.

theorem 5.4 implies the following uniform convergence analog of theorem 5.3.
theorem 5.5 (uniform convergence) let h be a hypothesis class and let   and    be
greater than zero. if a training set s of size

(cid:0) ln|h| + ln(2/  )(cid:1),

n     1
2 2

137

is drawn from distribution d, then with id203 greater than or equal to 1      , every h
in h satis   es |errs(h)     errd(h)|      .
proof: first,    x some h     h and let xj be the indicator random variable for the event
that h makes a mistake on the jth example in s. the xj are independent {0, 1} random
variables and the id203 that xi equals 1 is the true error of h, and the fraction of the
xj   s equal to 1 is exactly the training error of h. therefore, hoe   ding bounds guarantee
that the id203 of the event ah that |errd(h)     errs(h)| >   is less than or equal to
2e   2n 2. applying the union bound to the events ah over all h     h, the id203 that
there exists an h     h with the di   erence between true error and empirical error greater
than   is less than or equal to 2|h|e   2n 2. using the value of n from the theorem statement,
the right-hand-side of the above inequality is at most    as desired.

theorem 5.5 justi   es the approach of optimizing over our training sample s even if we
are not able to    nd a rule of zero training error. if our training set s is su   ciently large,
with high id203, good performance on s will translate to good performance on d.
note that theorems 5.3 and 5.5 require |h| to be    nite in order to be meaningful.
the notion of growth functions and vc-dimension in section 5.11 extend theorem 5.5 to
certain in   nite hypothesis classes.

5.6 illustrative examples and occam   s razor

we now present some examples to illustrate the use of theorem 5.3 and 5.5 and also

use these theorems to give a formal connection to the notion of occam   s razor.

5.6.1 learning disjunctions
consider the instance space x = {0, 1}d and suppose we believe that the target concept
can be represented by a disjunction (an or) over features, such as c    = {x|x1 = 1     x4 =
1    x8 = 1}, or more succinctly, c    = x1     x4     x8. for example, if we are trying to predict
whether an email message is spam or not, and our features correspond to the presence
or absence of di   erent possible indicators of spam-ness, then this would correspond to
the belief that there is some subset of these indicators such that every spam email has at
least one of them and every non-spam email has none of them. formally, let h denote
the class of disjunctions, and notice that |h| = 2d. so, by theorem 5.3, it su   ces to    nd
a consistent disjunction over a sample s of size

(cid:0)d ln(2) + ln(1/  )(cid:1).

|s| =

1
 

how can we e   ciently    nd a consistent disjunction when one exists? here is a simple
algorithm.

138

simple disjunction learner: given sample s, discard all features that are set to 1 in
any negative example in s. output the concept h that is the or of all features that remain.

lemma 5.6 the simple disjunction learner produces a disjunction h that is consis-
tent with the sample s (i.e., with errs(h) = 0) whenever the target concept is indeed a
disjunction.
proof: suppose target concept c    is a disjunction. then for any xi that is listed in c   ,
xi will not be set to 1 in any negative example by de   nition of an or. therefore, h will
include xi as well. since h contains all variables listed in c   , this ensures that h will
correctly predict positive on all positive examples in s. furthermore, h will correctly
predict negative on all negative examples in s since by design all features set to 1 in any
negative example were discarded. therefore, h is correct on all examples in s.

thus, combining lemma 5.6 with theorem 5.3, we have an e   cient algorithm for

pac-learning the class of disjunctions.

5.6.2 occam   s razor

occam   s razor is the notion, stated by william of occam around ad 1320, that in general
one should prefer simpler explanations over more complicated ones.18 why should one
do this, and can we make a formal claim about why this is a good idea? what if each of
us disagrees about precisely which explanations are simpler than others? it turns out we
can use theorem 5.3 to make a mathematical statement of occam   s razor that addresses
these issues.

first, what do we mean by a rule being    simple   ? let   s assume that each of us has
some way of describing rules, using bits (since we are computer scientists). the methods,
also called description languages, used by each of us may be di   erent, but one fact we can
say for certain is that in any given description language, there are at most 2b rules that
can be described using fewer than b bits (because 1 + 2 + 4 + . . . + 2b   1 < 2b). therefore,
by setting h to be the set of all rules that can be described in fewer than b bits and
plugging into theorem 5.3, we have the following:

theorem 5.7 (occam   s razor) fix any description language, and consider a training
sample s drawn from distribution d. with id203 at least 1       , any rule h with
errs(h) = 0 that can be described using fewer than b bits will have errd(h)       for
  [b ln(2) + ln(1/  )]. equivalently, with id203 at least 1       , all rules with
|s| = 1
errs(h) = 0 that can be described in fewer than b bits will have errd(h)     b ln(2)+ln(1/  )

.

|s|

for example, using the fact that ln(2) < 1 and ignoring the low-order ln(1/  ) term, this
means that if the number of bits it takes to write down a rule consistent with the training
data is at most 10% of the number of data points in our sample, then we can be con   dent

18the statement more explicitly was that    entities should not be multiplied unnecessarily.   

139

figure 5.4: a decision tree with three internal nodes and four leaves. this tree corre-
sponds to the boolean function   x1   x2     x1x2x3     x2   x3.

it will have error at most 10% with respect to d. what is perhaps surprising about this
theorem is that it means that we can each have di   erent ways of describing rules and yet
all use occam   s razor. note that the theorem does not say that complicated rules are
necessarily bad, or even that given two rules consistent with the data that the complicated
rule is necessarily worse. what it does say is that occam   s razor is a good policy in that
simple rules are unlikely to fool us since there are just not that many simple rules.

5.6.3 application: learning id90

one popular practical method for machine learning is to learn a decision tree; see figure
5.4. while    nding the smallest decision tree that    ts a given training sample s is np-
hard, there are a number of heuristics that are used in practice.19 suppose we run such
a heuristic on a training set s and it outputs a tree with k nodes. such a tree can be
described using o(k log d) bits: log2(d) bits to give the index of the feature in the root,
o(1) bits to indicate for each child if it is a leaf and if so what label it should have, and
then o(kl log d) and o(kr log d) bits respectively to describe the left and right subtrees,
where kl is the number of nodes in the left subtree and kr is the number of nodes in the
right subtree. so, by theorem 5.7, we can be con   dent the true error is low if we can
produce a consistent tree with fewer than  |s|/ log(d) nodes.

19for instance, one popular heuristic, called  , selects the feature to put inside any given node v
by choosing the feature of largest information gain, a measure of how much it is directly improving
prediction. formally, using sv to denote the set of examples in s that reach node v, and supposing that
feature xi partitions sv into s0
v (the examples in sv with xi = 0 and xi = 1, respectively), the
information gain of xi is de   ned as: ent(sv)     [
v )]. here, ent(s(cid:48)) is the binary
id178 of the label proportions in set s(cid:48); that is, if a p fraction of the examples in s(cid:48) are positive, then
ent(s(cid:48)) = p log2(1/p) + (1    p) log2(1/(1    p)), de   ning 0 log2(0) = 0. this then continues until all leaves
are pure   they have only positive or only negative examples.

|s1
v|
|sv| ent(s1

|s0
v|
|sv| ent(s0

v and s1

v ) +

140

5.7 id173: penalizing complexity

theorems 5.5 and 5.7 suggest the following idea. suppose that there is no simple rule
that is perfectly consistent with the training data, but we notice there are very simple
rules with training error 20%, say, and then some more complex rules with training error
10%, and so on. in this case, perhaps we should optimize some combination of training er-
ror and simplicity. this is the notion of id173, also called complexity penalization.

speci   cally, a regularizer is a penalty term that penalizes more complex hypotheses.
given our theorems so far, a natural measure of complexity of a hypothesis is the number
of bits we need to write it down.20 consider now    xing some description language, and let
hi denote those hypotheses that can be described in i bits in this language, so |hi|     2i.
let   i =   /2i. rearranging the bound of theorem 5.5, we know that with id203 at
least 1       i, all h     hi satisfy errd(h)     errs(h) +
. now, applying the
union bound over all i, using the fact that   1 +   2 +   3 + . . . =   , and also the fact that
ln(|hi|) + ln(2/  i)     i ln(4) + ln(2/  ), gives the following corollary.

(cid:113) ln(|hi|)+ln(2/  i)

2|s|

corollary 5.8 fix any description language, and consider a training sample s drawn
from distribution d. with id203 greater than or equal to 1       , all hypotheses h
satisfy

(cid:115)

errd(h)     errs(h) +

size(h) ln(4) + ln(2/  )

2|s|

where size(h) denotes the number of bits needed to describe h in the given language.

it tells us that rather than
corollary 5.8 gives us the tradeo    we were looking for.
searching for a rule of low training error, we instead may want to search for a rule with
a low right-hand-side in the displayed formula. if we can    nd one for which this quantity
is small, we can be con   dent true error will be low as well.

5.8 online learning

so far we have been considering what is often called the batch learning scenario. you are
given a    batch    of data   the training sample s   and your goal is to use it to produce
a hypothesis h that will have low error on new data, under the assumption that both s
and the new data are sampled from some    xed distribution d. we now switch to the
more challenging online learning scenario where we remove the assumption that data is
sampled from a    xed id203 distribution, or from any probabilistic process at all.

speci   cally, the online learning scenario proceeds as follows. at each time t = 1, 2, . . .,

two events occur:

20later we will see support vector machines that use a regularizer for linear separators based on the

margin of separation of data.

141

1. the algorithm is presented with an arbitrary example xt     x and is asked to make

a prediction (cid:96)t of its label.

2. the algorithm is told the true label of the example c   (xt) and is charged for a

mistake if c   (xt) (cid:54)= (cid:96)t.

the goal of the learning algorithm is to make as few mistakes as possible in total. for
example, consider an email classi   er that when a new email message arrives must classify
it as    important    or    it can wait   . the user then looks at the email and informs the
algorithm if it was incorrect. we might not want to model email messages as independent
random objects from a    xed id203 distribution, because they often are replies to
previous emails and build on each other. thus, the online learning model would be more
appropriate than the batch model for this setting.

intuitively, the online learning model is harder than the batch model because we have
removed the requirement that our data consists of independent draws from a    xed proba-
bility distribution. indeed, we will see shortly that any algorithm with good performance
in the online model can be converted to an algorithm with good performance in the batch
model. nonetheless, the online model can sometimes be a cleaner model for design and
analysis of algorithms.

5.8.1 an example: learning disjunctions

as a simple example, let   s revisit the problem of learning disjunctions in the online model.
we can solve this problem by starting with a hypothesis h = x1     x2     . . .     xd and using
it for prediction. we will maintain the invariant that every variable in the target disjunc-
tion is also in our hypothesis, which is clearly true at the start. this ensures that the
only mistakes possible are on examples x for which h(x) is positive but c   (x) is negative.
when such a mistake occurs, we simply remove from h any variable set to 1 in x. since
such variables cannot be in the target function (since x was negative), we maintain our
invariant and remove at least one variable from h. this implies that the algorithm makes
at most d mistakes total on any series of examples consistent with a disjunction.

in fact, we can show this bound is tight by showing that no deterministic algorithm

can guarantee to make fewer than d mistakes.

theorem 5.9 for any deterministic algorithm a there exists a sequence of examples   
and disjunction c    such that a makes at least d mistakes on sequence    labeled by c   .

proof: let    be the sequence e1, e2, . . . , ed where ej is the example that is zero everywhere
except for a 1 in the jth position. imagine running a on sequence    and telling a it made
a mistake on every example; that is, if a predicts positive on ej we set c   (ej) =    1 and if
a predicts negative on ej we set c   (ej) = +1. this target corresponds to the disjunction
of all xj such that a predicted negative on ej, so it is a legal disjunction. since a is

142

deterministic, the fact that we constructed c    by running a is not a problem:
it would
make the same mistakes if re-run from scratch on the same sequence and same target.
therefore, a makes d mistakes on this    and c   .

5.8.2 the halving algorithm

if we are not concerned with running time, a simple algorithm that guarantees to make at
most log2(|h|) mistakes for a target belonging to any given class h is called the halving
algorithm. this algorithm simply maintains the version space v     h consisting of all
h     h consistent with the labels on every example seen so far, and predicts based on
majority vote over these functions. each mistake is guaranteed to reduce the size of the
version space v by at least half (hence the name), thus the total number of mistakes is
at most log2(|h|). note that this can be viewed as the number of bits needed to write a
function in h down.

5.8.3 the id88 algorithm

earlier we described the id88 algorithm as a method for    nding a linear separator
consistent with a given training set s. however, the id88 algorithm also operates
naturally in the online setting as well.

recall that the basic assumption of the id88 algorithm is that the target func-
tion can be described by a vector w    such that for each positive example x we have
xt w        1 and for each negative example x we have xt w           1. recall also that we can
interpret xt w   /|w   | as the distance of x to the hyperplane xt w    = 0. thus, we can view
our assumption as stating that there exists a linear separator through the origin with all
positive examples on one side, all negative examples on the other side, and all examples at
distance at least    = 1/|w   | from the separator, where    is called the margin of separation.

the guarantee of the id88 algorithm will be that the total number of mistakes is
at most (r/  )2 where r = maxt |xt| over all examples xt seen so far. thus, if there exists
a hyperplane through the origin that correctly separates the positive examples from the
negative examples by a large margin relative to the radius of the smallest ball enclosing
the data, then the total number of mistakes will be small. the algorithm, restated in the

143

online setting, is as follows.

the id88 algorithm: start with the all-zeroes weight vector w = 0. then, for
t = 1, 2, . . . do:

1. given example xt, predict sgn(xt

t w).

2. if the prediction was a mistake, then update:

(a) if xt was a positive example, let w     w + xt.
(b) if xt was a negative example, let w     w     xt.

the id88 algorithm enjoys the following guarantee on its total number of mis-

t w        1 for the positive examples and xt

takes.
theorem 5.10 on any sequence of examples x1, x2, . . ., if there exists a vector w    such
t w           1 for the negative examples (i.e.,
that xt
a linear separator of margin    = 1/|w   |), then the id88 algorithm makes at most
r2|w   |2 mistakes, where r = maxt |xt|.
proof: fix some consistent w   . we will keep track of two quantities, wt w    and |w|2.
first of all, each time we make a mistake, wt w    increases by at least 1. that is because
if xt is a positive example, then

(w + xt)t w    = wt w    + xt

t w        wt w    + 1,

by de   nition of w   . similarly, if xt is a negative example, then

(w     xt)t w    = wt w        xt

t w        wt w    + 1.

next, on each mistake, we claim that |w|2 increases by at most r2. let us    rst consider
mistakes on positive examples. if we make a mistake on a positive example xt then we
have

(w + xt)t (w + xt) = |w|2 + 2xt

t w + |xt|2     |w|2 + |xt|2     |w|2 + r2,

where the middle inequality comes from the fact that we made a mistake, which means
that xt

t w     0. similarly, if we make a mistake on a negative example xt then we have

(w     xt)t (w     xt) = |w|2     2xt

t w + |xt|2     |w|2 + |xt|2     |w|2 + r2.

note that it is important here that we only update on a mistake.

so, if we make m mistakes, then wt w        m , and |w|2     m r2, or equivalently,
m . finally, we use the fact that wt w   /|w   |     |w| which is just saying that

|w|     r

   

144

the projection of w in the direction of w    cannot be larger than the length of w. this
gives us:

   

m/|w   |     r

   

m
m     r|w   |
m     r2|w   |2

as desired.

5.8.4 extensions: inseparable data and hinge loss
we assumed above that there exists a perfect w    that correctly classi   es all the exam-
ples, e.g., correctly classi   es all the emails into important versus non-important. this
is rarely the case in real-life data. what if even the best w    isn   t quite perfect? we
if there is an example that w    doesn   t cor-
can see what this does to the above proof:
rectly classify, then while the second part of the proof still holds, the    rst part (the dot
product of w with w    increasing) breaks down. however, if this doesn   t happen too of-
t w    is just a    little bit wrong    then we will only make a few more mistakes.
ten, and also xt
to make this formal, de   ne the hinge-loss of w    on a positive example xt as max(0, 1   
t w        1 as desired then the hinge-loss is zero; else, the hinge-
t w   ). in other words, if xt
xt
loss is the amount the lhs is less than the rhs.21 similarly, the hinge-loss of w    on a
t w   ). given a sequence of labeled examples s, de   ne
negative example xt is max(0, 1 + xt
the total hinge-loss lhinge(w   , s) as the sum of hinge-losses of w    on all examples in s.
we now get the following extended theorem.

(cid:0)r2|w   |2 + 2lhinge(w   , s)(cid:1)

theorem 5.11 on any sequence of examples s = x1, x2, . . ., the id88 algorithm
makes at most

min
w   
mistakes, where r = maxt |xt|.
proof: as before, each update of the id88 algorithm increases |w|2 by at most r2,
so if the algorithm makes m mistakes, we have |w|2     m r2.
what we can no longer say is that each update of the algorithm increases wt w    by
at least 1. instead, on a positive example we are    increasing    wt w    by xt
t w    (it could
be negative), which is at least 1     lhinge(w   , xt). similarly, on a negative example we
   increase    wt w    by    xt
t w   , which is also at least 1     lhinge(w   , xt). if we sum this up
over all mistakes, we get that at the end we have wt w        m     lhinge(w   , s), where we
are using here the fact that hinge-loss is never negative so summing over all of s is only
larger than summing over the mistakes that w made.

21this is called    hinge-loss    because as a function of xt

t w    it looks like a hinge.

145

finally, we just do some algebra. let l = lhinge(w   , s). so we have:

wt w   /|w   |     |w|

(wt w   )2     |w|2|w   |2
(m     l)2     m r2|w   |2
m 2     2m l + l2     m r2|w   |2
m     2l + l2/m     r2|w   |2

m     r2|w   |2 + 2l     l2/m     r2|w   |2 + 2l

as desired.

5.9 online to batch conversion

suppose we have an online algorithm with a good mistake bound, such as the id88
algorithm. can we use it to get a guarantee in the distributional (batch) learning setting?
intuitively, the answer should be yes since the online setting is only harder. indeed, this
intuition is correct. we present here two natural approaches for such online to batch
conversion.

example xt. since(cid:80)|s|

conversion procedure 1: random stopping. suppose we have an online algorithm
a with mistake-bound m . say we run the algorithm in a single pass on a sample s of size
m/ . let xt be the indicator random variable for the event that a makes a mistake on
t=1 xt]     m
where the expectation is taken over the random draw of s from d|s|. by linearity of
expectation, and dividing both sides by |s| we therefore have:

t=1 xt     m for any set s, we certainly have that e[(cid:80)|s|

e[xt]     m/|s| =  .

(5.1)

let ht denote the hypothesis used by algorithm a to predict on the tth example. since
the tth example was randomly drawn from d, we have e[errd(ht)] = e[xt]. this means
that if we choose t at random from 1 to |s|, i.e., stop the algorithm at a random time, the
expected error of the resulting prediction rule, taken over the randomness in the draw of
s and the choice of t, is at most   as given by equation (5.1). thus we have:
theorem 5.12 (online to batch via random stopping) if an online algorithm a
with mistake-bound m is run on a sample s of size m/  and stopped at a random time
between 1 and |s|, the expected error of the hypothesis h produced satis   es e[errd(h)]      .

conversion procedure 2: controlled testing. a second natural approach to us-
ing an online learning algorithm a in the distributional setting is to just run a series of
controlled tests. speci   cally, suppose that the initial hypothesis produced by algorithm
6     1)         . we draw a set of

a is h1. de   ne   i =   /(i + 2)2 so we have (cid:80)   

i=0   i = (   2

146

|s|(cid:88)

t=1

1
|s|

  log( 1
  1

n1 = 1
) random examples and test to see whether h1 gets all of them correct. note
that if errd(h1)       then the chance h1 would get them all correct is at most (1    )n1       1.
so, if h1 indeed gets them all correct, we output h1 as our hypothesis and halt. if not,
we choose some example x1 in the sample on which h1 made a mistake and give it to
algorithm a. algorithm a then produces some new hypothesis h2 and we again repeat,
testing h2 on a fresh set of n2 = 1

) random examples, and so on.

  log( 1
  2

  log( 1
  t

in general, given ht we draw a fresh set of nt = 1

) random examples and test
to see whether ht gets all of them correct. if so, we output ht and halt; if not, we choose
some xt on which ht(xt) was incorrect and give it to algorithm a. by choice of nt, if ht
had error rate   or larger, the chance we would mistakenly output it is at most   t. by
choice of the values   t, the chance we ever halt with a hypothesis of error   or larger is at
most   1 +   2 + . . .       . thus, we have the following theorem.
theorem 5.13 (online to batch via controlled testing) let a be an online learn-
  log( m
ing algorithm with mistake-bound m . then this procedure will halt after o( m
   ))
examples and with id203 at least 1        will produce a hypothesis of error at most  .

note that in this conversion we cannot re-use our samples: since the hypothesis ht depends
on the previous data, we need to draw a fresh set of nt examples to use for testing it.

5.10 support-vector machines

in a batch setting, rather than running the id88 algorithm and adapting it via
one of the methods above, another natural idea would be just to solve for the vector w
that minimizes the right-hand-side in theorem 5.11 on the given dataset s. this turns
out to have good guarantees as well, though they are beyond the scope of this book. in
fact, this is the support vector machine (id166) algorithm. speci   cally, id166s solve the
following id76 problem over a sample s = {x1, x2, . . . xn} where c is a
constant that is determined empirically.

(cid:88)

minimize

subject to

i

si

c|w|2 +
w    xi     1     si for all positive examples xi
w    xi        1 + si for all negative examples xi
si     0 for all i.

the variables si are called slack variables, and notice that the sum of the slack variables
is the total hinge loss of w. so, this id76 is minimizing a weighted sum
of 1/  2, where    is the margin, and the total hinge loss. if we were to add the constraint
that all si = 0 then this would be solving for the maximum margin linear separator for the
data. however, in practice, optimizing a weighted combination generally performs better.
id166s can also be kernelized, by using the dual of the above optimization problem (the

147

key idea is that the optimal w will be a weighted combination of data points, just as in the
id88 algorithm, and these weights can be variables in the optimization problem);
details are beyond the scope of this book.

5.11 vc-dimension

in section 5.5 we presented several theorems showing that so long as the training set

  log(|h|), we can be con   dent that every h     h with errd(h)      
s is large compared to 1
 2 log(|h|), then we can be con   dent
will have errs(h) > 0, and if s is large compared to 1
that every h     h will have |errd(h)    errs(h)|      . in essence, these results used log(|h|)
as a measure of complexity of class h. vc-dimension is a di   erent, tighter measure of
complexity for a concept class, and as we will see, is also su   cient to yield con   dence
bounds. for any class h, vcdim(h)     log2(|h|) but it can also be quite a bit smaller.
let   s introduce and motivate it through an example.

consider a database consisting of the salary and age for a random sample of the adult
population in the united states. suppose we are interested in using the database to an-
swer questions of the form:    what fraction of the adult population in the united states
has age between 35 and 45 and salary between $50,000 and $70,000?    that is, we are
interested in queries that ask about the fraction of the adult population within some axis-
parallel rectangle. what we can do is calculate the fraction of the database satisfying
this condition and return this as our answer. this brings up the following question: how
large does our database need to be so that with id203 greater than or equal to 1      ,
our answer will be within     of the truth for every possible rectangle query of this form?

if we assume our values are discretized such as 100 possible ages and 1,000 possible
salaries, then there are at most (100    1, 000)2 = 1010 possible rectangles. this means we
can apply theorem 5.5 with |h|     1010. speci   cally, we can think of the target concept
c    as the empty set so that errs(h) is exactly the fraction of the sample inside rectangle
h and errd(h) is exactly the fraction of the whole population inside h.22 this would tell
us that a sample size of

1
2 2 (10 ln 10 + ln(2/  )) would be su   cient.

however, what if we do not wish to discretize our concept class? another approach
would be to say that if there are only n adults total in the united states, then there
are at most n 4 rectangles that are truly di   erent with respect to d and so we could use
|h|     n 4. still, this suggests that s needs to grow with n , albeit logarithmically, and
one might wonder if that is really necessary. vc-dimension, and the notion of the growth
function of concept class h, will give us a way to avoid such discretization and avoid any
dependence on the size of the support of the underlying distribution d.

22technically d is the uniform distribution over the adult population of the united states, and we

want to think of s as an independent identically distributed sample from this d.

148

b

a

d

(b)

c

(a)

figure 5.5: (a) shows a set of four points that can be shattered by rectangles along with
some of the rectangles that shatter the set. not every set of four points can be shattered
as seen in (b). any rectangle containing points a, b, and c must contain d. no set of    ve
points can be shattered by rectangles with axis-parallel edges. no set of three collinear
points can be shattered, since any rectangle that contains the two end points must also
contain the middle point. more generally, since rectangles are convex, a set with one point
inside the convex hull of the others cannot be shattered.

5.11.1 de   nitions and key theorems
de   nition 5.1 given a set s of examples and a concept class h, we say that s is
shattered by h if for every a     s there exists some h     h that labels all examples in a
as positive and all examples in s \ a as negative.
de   nition 5.2 the vc-dimension of h is the size of the largest set shattered by h.

for example, there exist sets of four points in the plane that can be shattered by rect-
angles with axis-parallel edges, e.g., four points at the vertices of a diamond (see figure
5.5). given such a set s, for any a     s, there exists a rectangle with the points in a
inside the rectangle and the points in s \ a outside the rectangle. however, rectangles
with axis-parallel edges cannot shatter any set of    ve points. to see this, assume for
contradiction that there is a set of    ve points shattered by the family of axis-parallel
rectangles. find the minimum enclosing rectangle for the    ve points. for each edge there
is at least one point that has stopped its movement. identify one such point for each edge.
the same point may be identi   ed as stopping two edges if it is at a corner of the minimum
enclosing rectangle. if two or more points have stopped an edge, designate only one as
having stopped the edge. now, at most four points have been designated. any rectangle
enclosing the designated points must include the undesignated points. thus, the subset
of designated points cannot be expressed as the intersection of a rectangle with the    ve
points. therefore, the vc-dimension of axis-parallel rectangles is four.

we now need one more de   nition, which is the growth function of a concept class h.
de   nition 5.3 given a set s of examples and a concept class h, let h[s] = {h     s :

149

h     h}. that is, h[s] is the concept class h restricted to the set of points s. for integer
n and class h, let h[n] = max|s|=n |h[s]|; this is called the growth function of h.

for example, we could have de   ned shattering by saying that s is shattered by h
if |h[s]| = 2|s|, and then the vc-dimension of h is the largest n such that h[n] = 2n.
notice also that for axis-parallel rectangles, h[n] = o(n4). the growth function of a class
is sometimes called the shatter function or shatter coe   cient.

what connects these to learnability are the following three remarkable theorems. the
   rst two are analogs of theorem 5.3 and theorem 5.5 respectively, showing that one can
replace |h| with its growth function. this is like replacing the number of concepts in h
with the number of concepts    after the fact   , i.e., after s is drawn, and is subtle because
we cannot just use a union bound after we have already drawn our set s. the third
theorem relates the growth function of a class to its vc-dimension. we now present the
theorems, give examples of vc-dimension and growth function of various concept classes,
and then prove the theorems.
theorem 5.14 (growth function sample bound) for any class h and distribution
d, if a training sample s is drawn from d of size

n     2
 

[log2(2h[2n]) + log2(1/  )]

then with id203     1     , every h     h with errd(h)       has errs(h) > 0 (equivalently,
every h     h with errs(h) = 0 has errd(h) <  ).
theorem 5.15 (growth function uniform convergence) for any class h and dis-
tribution d, if a training sample s is drawn from d of size
 2 [ln(2h[2n]) + ln(1/  )]

n     8

then with id203     1       , every h     h will have |errs(h)     errd(h)|      .

theorem 5.16 (sauer   s lemma) if vcdim(h) = d then h[n]    (cid:80)d

(cid:0)n
(cid:1)     ( en

i=0

i

d )d.

notice that sauer   s lemma was fairly tight in the case of axis-parallel rectangles,
though in some cases it can be a bit loose. e.g., we will see that for linear separators
in the plane, their vc-dimension is 3 but h[n] = o(n2). an interesting feature about
sauer   s lemma is that it implies the growth function switches from taking the form 2n to
taking the form of roughly nvcdim(h) when n exceeds the vc-dimension of the class h.
putting theorems 5.14 and 5.16 together, with a little algebra we get the following

corollary (a similar corollary results by combining theorems 5.15 and 5.16):

150

corollary 5.17 (vc-dimension sample bound) for any class h and distribution d,
a training sample s of size

o

[vcdim(h) log(1/ ) + log(1/  )]

(cid:19)

(cid:18)1

 

is su   cient to ensure that with id203     1       , every h     h with errd(h)       has
errs(h) > 0 (equivalently, every h     h with errs(h) = 0 has errd(h) <  ).

for any class h, vcdim(h)     log2(|h|) since h must have at least 2k concepts in
order to shatter k points. thus corollary 5.17 is never too much worse than theorem 5.3
and can be much better.

5.11.2 examples: vc-dimension and growth function

rectangles with axis-parallel edges

as we saw above, the class of axis-parallel rectangles in the plane has vc-dimension

4 and growth function h[n] = o(n4).

intervals of the reals

intervals on the real line can shatter any set of two points but no set of three points
since the subset of the    rst and last points cannot be isolated. thus, the vc-dimension
of intervals is two. also, h[n] = o(n2) since we have o(n2) choices for the left and right
endpoints.

pairs of intervals of the reals

consider the family of pairs of intervals, where a pair of intervals is viewed as the set
of points that are in at least one of the intervals, in other words, their set union. there
exists a set of size four that can be shattered but no set of size    ve since the subset of    rst,
third, and last point cannot be isolated. thus, the vc-dimension of pairs of intervals is
four. also we have h[n] = o(n4).

convex polygons

consider the set system of all convex polygons in the plane. for any positive integer
n, place n points on the unit circle. any subset of the points are the vertices of a convex
polygon. clearly that polygon will not contain any of the points not in the subset. this
shows that convex polygons can shatter arbitrarily large sets, so the vc-dimension is
in   nite. notice that this also implies that h[n] = 2n.

halfspaces in d-dimensions

151

de   ne a halfspace to be the set of all points on one side of a linear separator, i.e.,
a set of the form {x|wt x     w0}. the vc-dimension of halfspaces in d-dimensions is d+1.

there exists a set of size d + 1 that can be shattered by halfspaces. select the d unit-
coordinate vectors plus the origin to be the d + 1 points. suppose a is any subset of these
d + 1 points. without loss of generality assume that the origin is in a. take a 0-1 vector
w which has 1   s precisely in the coordinates corresponding to vectors not in a. clearly
a lies in the half-space wt x     0 and the complement of a lies in the complementary
halfspace.

we now show that no set of d + 2 points in d-dimensions can be shattered by halfs-
paces. this is done by proving that any set of d + 2 points can be partitioned into two
disjoint subsets a and b of points whose convex hulls intersect. this establishes the claim
since any linear separator with a on one side must have its entire convex hull on that
side,23 so it is not possible to have a linear separator with a on one side and b on the other.

let convex(s) denote the convex hull of point set s.

s(cid:80)

theorem 5.18 (radon): any set s     rd with |s|     d + 2, can be partitioned into two
disjoint subsets a and b such that convex(a)     convex(b) (cid:54)=   .
proof: without loss of generality, assume |s| = d + 2. form a d   (d + 2) matrix with one
column for each point of s. call the matrix a. add an extra row of all 1   s to construct a
(d+1)  (d+2) matrix b. clearly the rank of this matrix is at most d+1 and the columns
are linearly dependent. say x = (x1, x2, . . . , xd+2) is a nonzero vector with bx = 0.
reorder the columns so that x1, x2, . . . , xs     0 and xs+1, xs+2, . . . , xd+2 < 0. normalize
|xi| = 1. let bi (respectively ai) be the ith column of b (respectively a). then,
x so
|xi| =
|xi|ai is a convex
i=s+1
combination of columns of a which proves the theorem. thus, s can be partitioned into
two sets, the    rst consisting of the    rst s points after the rearrangement and the second
consisting of points s + 1 through d + 2 . their convex hulls intersect as required.

|xi|bi from which it follows that
|xi| = 1 and

|xi|bi =
|xi|. since

|xi| = 1 each side of

d+2(cid:80)
s(cid:80)

s(cid:80)
d+2(cid:80)

|xi|ai and

|xi|ai =

|xi|ai =

d+2(cid:80)

d+2(cid:80)

d+2(cid:80)

s(cid:80)

s(cid:80)

s(cid:80)

i=s+1

i=s+1

i=s+1

i=s+1

i=1

i=1

i=1

i=1

i=1

i=1

radon   s theorem immediately implies that half-spaces in d-dimensions do not shatter

any set of d + 2 points.

spheres in d-dimensions

23if any two points x1 and x2 lie on the same side of a linear separator, so must any convex combination:

if w    x1     b and w    x2     b then w    (ax1 + (1     a)x2)     b.

152

a sphere in d-dimensions is a set of points of the form {x| |x     x0|     r}. the vc-
dimension of spheres is d + 1. it is the same as that of halfspaces. first, we prove that no
set of d + 2 points can be shattered by spheres. suppose some set s with d + 2 points can
be shattered. then for any partition a1 and a2 of s, there are spheres b1 and b2 such
that b1     s = a1 and b2     s = a2. now b1 and b2 may intersect, but there is no point
of s in their intersection. it is easy to see that there is a hyperplane perpendicular to
the line joining the centers of the two spheres with all of a1 on one side and all of a2 on
the other and this implies that halfspaces shatter s, a contradiction. therefore no d + 2
points can be shattered by hyperspheres.

it is also not di   cult to see that the set of d+1 points consisting of the unit-coordinate
vectors and the origin can be shattered by spheres. suppose a is a subset of the d + 1
points. let a be the number of unit vectors in a. the center a0 of our sphere will be
   
the sum of the vectors in a. for every unit vector in a, its distance to this center will
   
a + 1.
be
a. thus, we can choose the radius so that
the distance of the origin to the center is
precisely the points in a are in the hypersphere.

a     1 and for every unit vector outside a, its distance to this center will be

   

finite sets

the system of    nite sets of real numbers can shatter any    nite set of real numbers

and thus the vc-dimension of    nite sets is in   nite.

5.11.3 proof of main theorems
we begin with a technical lemma. consider drawing a set s of n examples from d and
let a denote the event that there exists h     h with zero training error on s but true
error greater than or equal to  . now draw a second set s(cid:48) of n examples from d and let
b denote the event that there exists h     h with zero error on s but error greater than
or equal to  /2 on s(cid:48). we claim that prob(b)     prob(a)/2.
lemma 5.19 let h be a concept class over some domain x and let s and s(cid:48) be sets of
n elements drawn from some distribution d on x , where n     8/ . let a be the event that
there exists h     h with zero error on s but true error greater than or equal to  . let b
be the event that there exists h     h with zero error on s but error greater than or equal
to  
proof: clearly, prob(b)     prob(a, b) = prob(a)prob(b|a). consider drawing set s
and suppose event a occurs. let h be in h with errd(h)       but errs(h) = 0. now,
draw set s(cid:48). e(error of h on s(cid:48)) = errd(h)      . so, by cherno    bounds, since n     8/ ,
prob(errs(cid:48)(h)      /2)     1/2. thus, prob(b|a)     1/2 and prob(b)     prob(a)/2 as
desired.

2 on s(cid:48). then prob(b)     prob(a)/2.

we now prove theorem 5.14, restated here for convenience.

153

theorem 5.14 (growth function sample bound) for any class h and distribution
d, if a training sample s is drawn from d of size

n     2
 

[log2(2h[2n]) + log2(1/  )]

then with id203     1     , every h     h with errd(h)       has errs(h) > 0 (equivalently,
every h     h with errs(h) = 0 has errd(h) <  ).
proof: consider drawing a set s of n examples from d and let a denote the event that
there exists h     h with true error greater than   but training error zero. our goal is to
prove that prob(a)       .

by lemma 5.19 it su   ces to prove that prob(b)       /2. consider a third experiment.
draw a set s(cid:48)(cid:48) of 2n points from d and then randomly partition s(cid:48)(cid:48) into two sets s and
s(cid:48) of n points each. let b    denote the event that there exists h     h with errs(h) = 0
but errs(cid:48)(h)      /2. prob(b   ) = prob(b) since drawing 2n points from d and randomly
partitioning them into two sets of size n produces the same distribution on (s, s(cid:48)) as does
drawing s and s(cid:48) directly. the advantage of this new experiment is that we can now
argue that prob(b   ) is low by arguing that for any set s(cid:48)(cid:48) of size 2n, prob(b   |s(cid:48)(cid:48)) is low,
with id203 now taken over just the random partition of s(cid:48)(cid:48) into s and s(cid:48). the key
point is that since s(cid:48)(cid:48) is    xed, there are at most |h[s(cid:48)(cid:48)]|     h[2n] events to worry about.
speci   cally, it su   ces to prove that for any    xed h     h[s(cid:48)(cid:48)], the id203 over the
partition of s(cid:48)(cid:48) that h makes zero mistakes on s but more than  n/2 mistakes on s(cid:48) is at
most   /(2h[2n]). we can then apply the union bound over h[s(cid:48)(cid:48)] = {h     s(cid:48)(cid:48)|h     h}.

to make the calculations easier, consider the following speci   c method for partitioning
s(cid:48)(cid:48) into s and s(cid:48). randomly put the points in s(cid:48)(cid:48) into pairs: (a1, b1), (a2, b2), . . ., (an, bn).
for each index i,    ip a fair coin. if heads put ai into s and bi into s(cid:48), else if tails put ai
into s(cid:48) and bi into s. now,    x some partition h     h[s(cid:48)(cid:48)] and consider the id203 over
these n fair coin    ips that h makes zero mistakes on s but more than  n/2 mistakes on s(cid:48).
first of all, if for any index i, h makes a mistake on both ai and bi then the id203 is
zero (because it cannot possibly make zero mistakes on s). second, if there are fewer than
 n/2 indices i such that h makes a mistake on either ai or bi then again the id203 is
zero because it cannot possibly make more than  n/2 mistakes on s(cid:48). so, assume there
are r      n/2 indices i such that h makes a mistake on exactly one of ai or bi. in this case,
the chance that all of those mistakes land in s(cid:48) is exactly 1/2r. this quantity is at most
1/2 n/2       /(2h[2n]) as desired for n as given in the theorem statement.

we now prove theorem 5.15, restated here for convenience.

theorem 5.15 (growth function uniform convergence) for any class h and dis-
tribution d, if a training sample s is drawn from d of size
 2 [ln(2h[2n]) + ln(1/  )]

n     8

154

then with id203     1       , every h     h will have |errs(h)     errd(h)|      .
proof: this proof is identical to the proof of theorem 5.14 except b    is now the event
that there exists a set h     h[s(cid:48)(cid:48)] such that the error of h on s di   ers from the error of h on
s(cid:48) by more than  /2. we again consider the experiment where we randomly put the points
in s(cid:48)(cid:48) into pairs (ai, bi) and then    ip a fair coin for each index i, if heads placing ai into s
and bi into s(cid:48), else placing ai into s(cid:48) and bi into s. consider the di   erence between the
number of mistakes h makes on s and the number of mistakes h makes on s(cid:48) and observe
how this di   erence changes as we    ip coins for i = 1, 2, . . . , n. initially, the di   erence
is zero. if h makes a mistake on both or neither of (ai, bi) then the di   erence does not
change. else, if h makes a mistake on exactly one of ai or bi, then with id203 1/2
the di   erence increases by one and with id203 1/2 the di   erence decreases by one.
if there are r     n such pairs, then if we take a random walk of r     n steps, what is the
id203 that we end up more than  n/2 steps away from the origin? this is equivalent
to asking: if we    ip r     n fair coins, what is the id203 the number of heads di   ers
from its expectation by more than  n/4. by hoe   ding bounds, this is at most 2e    2n/8.
this quantity is at most   /(2h[2n]) as desired for n as given in the theorem statement.

finally, we prove sauer   s lemma, relating the growth function to the vc-dimension.

theorem 5.16 (sauer   s lemma) if vcdim(h) = d then h[n]    (cid:80)d
|h[s]|     (cid:0) n   d

(cid:1), where we are de   ning (cid:0) n   d

(cid:1) = (cid:80)d

proof: let d = vcdim(h). our goal is to prove for any set s of n points that

(cid:0)n
(cid:1); this is the number of distinct

(cid:0)n
(cid:1)     ( en

ways of choosing d or fewer elements out of n. we will do so by induction on n. as a
base case, our theorem is trivially true if n     d.

d )d.

i=0

i=0

i

i

as a    rst step in the proof, notice that:

(cid:18) n

(cid:19)

    d

(cid:18)n     1
(cid:19)

    d

(cid:19)

(cid:18) n     1

    d     1

+

(5.2)

=

because we can partition the ways of choosing d or fewer items into those that do not
include the    rst item (leaving     d to be chosen from the remainder) and those that do
include the    rst item (leaving     d     1 to be chosen from the remainder).

now, consider any set s of n points and pick some arbitrary point x     s. by induc-

tion, we may assume that |h[s \{x}]|    (cid:0)n   1   d
is that |h[s]|     |h[s \ {x}]|    (cid:0) n   1   d   1

(cid:1). so, by equation (5.2) all we need to show
(cid:1). thus, our problem has reduced to analyzing how

many more partitions there are of s than there are of s \ {x} using sets in h.

if h[s] is larger than h[s \ {x}], it is because of pairs of sets in h[s] that di   er only
on point x and therefore collapse to the same set when x is removed. for set h     h[s]

155

containing point x, de   ne twin(h) = h \ {x}; this may or may not belong to h[s]. let
t = {h     h[s] : x     h and twin(h)     h[s]}. notice |h[s]|     |h[s \ {x}]| = |t |.

now, what is the vc-dimension of t ? if d(cid:48) = vcdim(t ), this means there is some set
r of d(cid:48) points in s \ {x} that are shattered by t . by de   nition of t , all 2d(cid:48)
subsets of r
can be extended to either include x, or not include x and still be a set in h[s]. in other
words, r     {x} is shattered by h. this means, d(cid:48) + 1     d. since vcdim(t )     d     1, by

induction we have |t |    (cid:0) n   1   d   1

(cid:1) as desired.

5.11.4 vc-dimension of combinations of concepts

often one wants to create concepts out of other concepts. for example, given several
linear separators, one could take their intersection to create a convex polytope. or given
several disjunctions, one might want to take their majority vote. we can use sauer   s
lemma to show that such combinations do not increase the vc-dimension of the class by
too much.

speci   cally, given k concepts h1, h2, . . . , hk and a booelan function f de   ne the set
combf (h1, . . . , hk) = {x     x : f (h1(x), . . . , hk(x)) = 1}, where here we are using hi(x) to
denote the indicator for whether or not x     hi. for example, f might be the and function
to take the intersection of the sets hi, or f might be the majority-vote function. this can
be viewed as a depth-two neural network. given a concept class h, a boolean function f ,
and an integer k, de   ne the new concept class com bf,k(h) = {combf (h1, . . . , hk) : hi    
h}. we can now use sauer   s lemma to produce the following corollary.
corollary 5.20 if the concept class h has vc-dimension d, then for any combination

function f , the class combf,k(h) has vc-dimension o(cid:0)kd log(kd)(cid:1).

proof: let n be the vc-dimension of combf,k(h), so by de   nition, there must exist
a set s of n points shattered by combf,k(h). we know by sauer   s lemma that there
are at most nd ways of partitioning the points in s using sets in h. since each set in
combf,k(h) is determined by k sets in h, and there are at most (nd)k = nkd di   erent
k-tuples of such sets, this means there are at most nkd ways of partitioning the points
using sets in combf,k(h). since s is shattered, we must have 2n     nkd, or equivalently
n     kd log2(n). we solve this as follows. first, assuming n     16 we have log2(n)        
   
n so
kd log2(n)     kd
n which implies that n     (kd)2. to get the better bound, plug back into
the original inequality. since n     (kd)2, it must be that log2(n)     2 log2(kd). substituting
log n     2 log2(kd) into n     kd log2 n gives n     2kd log2(kd).

this result will be useful for our discussion of boosting in section 5.12.

5.11.5 other measures of complexity

vc-dimension and number of bits needed to describe a set are not the only measures
of complexity one can use to derive generalization guarantees. there has been signi   cant

156

1
n

work on a variety of measures. one measure called rademacher complexity measures
the extent to which a given concept class h can    t random noise. given a set of n
(cid:80)n
examples s = {x1, . . . , xn}, the empirical rademacher complexity of h is de   ned as
i=1   ih(xi), where   i     {   1, 1} are independent random labels
rs(h) = e  1,...,  n max
h   h
2. e.g., if you assign random   1 labels to the points in s and the
with prob[  i = 1] = 1
best classi   er in h on average gets error 0.45 then rs(h) = 0.55     0.45 = 0.1. one can
prove that with id203 greater than or equal to 1      , every h     h satis   es true error
less than or equal to training error plus rs(h) + 3
2n . for more on results such as
this, see, e.g., [bm02].

(cid:113) ln(2/  )

5.12 strong and weak learning - boosting

we now describe boosting, which is important both as a theoretical result and as a

practical and easy-to-use learning method.

2        for some 0 <        1

a strong learner for a problem is an algorithm that with high id203 is able to
achieve any desired error rate   using a number of samples that may depend polynomially
on 1/ . a weak learner for a problem is an algorithm that does just a little bit better than
random guessing. it is only required to get with high id203 an error rate less than
or equal to 1
2. we show here that a weak-learner for a problem
that achieves the weak-learning guarantee for any distribution of data can be boosted to a
strong learner, using the technique of boosting. at the high level, the idea will be to take
our training sample s, and then to run the weak-learner on di   erent data distributions
produced by weighting the points in the training sample in di   erent ways. running the
weak learner on these di   erent weightings of the training sample will produce a series of
hypotheses h1, h2, . . ., and the idea of our reweighting procedure will be to focus attention
on the parts of the sample that previous hypotheses have performed poorly on. at the
end we will combine the hypotheses together by a majority vote.

assume the weak learning algorithm a outputs hypotheses from some class h. our
boosting algorithm will produce hypotheses that will be majority votes over t0 hypotheses
from h, for t0 de   ned below. this means that we can apply corollary 5.20 to bound the
vc-dimension of the class of hypotheses our boosting algorithm can produce in terms of
the vc-dimension of h. in particular, the class of rules that can be produced by the
booster running for t0 rounds has vc-dimension o(t0vcdim(h) log(t0vcdim(h))). this
in turn gives a bound on the number of samples needed, via corollary 5.17, to ensure that
high accuracy on the sample will translate to high accuracy on new data.

to make the discussion simpler, we will assume that the weak learning algorithm a,
when presented with a weighting of the points in our training sample, always (rather than
with high id203) produces a hypothesis that performs slightly better than random
guessing with respect to the distribution induced by weighting. speci   cally:

157

boosting algorithm

given a sample s of n labeled examples x1, . . . , xn, initialize each
example xi to have a weight wi = 1. let w = (w1, . . . , wn).

for t = 1, 2, . . . , t0 do

call the weak learner on the weighted sample (s, w), receiving
hypothesis ht.
multiply the weight of each example that was misclassi   ed by
ht by    =

. leave the other weights as they are.

1
2 +  
2     
1

end

output the classi   er maj(h1, . . . , ht0) which takes the majority vote
of the hypotheses returned by the weak learner. assume t0 is odd so
there is no tie.

figure 5.6: the boosting algorithm

de   nition 5.4 (  -weak learner on sample) a   -weak learner is an algorithm that
given examples, their labels, and a nonnegative real weight wi on each example xi, produces
a classi   er that correctly labels a subset of examples with total weight at least ( 1
wi.

2 +  )

n(cid:80)

i=1

at the high level, boosting makes use of the intuitive notion that if an example was
misclassi   ed, one needs to pay more attention to it. the boosting procedure is in figure
5.6.

theorem 5.21 let a be a   -weak learner for sample s. then t0 = o( 1
  2 log n) is su   -
cient so that the classi   er maj(h1, . . . , ht0) produced by the boosting procedure has training
error zero.

proof: suppose m is the number of examples the    nal classi   er gets wrong. each of
these m examples was misclassi   ed at least t0/2 times so each has weight at least   t0/2.
thus the total weight is at least m  t0/2. on the other hand, at time t+1, only the weights
of examples misclassi   ed at time t were increased. by the property of weak learning, the
2       ) of the total weight at time t. let
total weight of misclassi   ed examples is at most ( 1
weight(t) be the total weight at time t. then

weight(t + 1)    (cid:16)

  (cid:0) 1
2       (cid:1) +(cid:0) 1

2 +   (cid:1)(cid:17)    weight(t)

= (1 + 2  )    weight(t).

158

since weight(0) = n, the total weight at the end is at most n(1 + 2  )t0. thus

m  t0/2     total weight at end     n(1 + 2  )t0.

substituting    = 1/2+  

1/2      = 1+2  

1   2   and rearranging terms

m     n(1     2  )t0/2(1 + 2  )t0/2 = n[1     4  2]t0/2.

using 1     x     e   x, m     ne   2t0  2. for t0 > ln n
items must be zero.

2  2 , m < 1, so the number of misclassi   ed

having completed the proof of the boosting result, here are two interesting observa-

tions:

connection to hoe   ding bounds: the boosting result applies even if our weak learn-
ing algorithm is    adversarial   , giving us the least helpful classi   er possible subject
to de   nition 5.4. this is why we don   t want the    in the boosting algorithm to be
too large, otherwise the weak learner could return the negation of the classi   er it
gave the last time. suppose that the weak learning algorithm gave a classi   er each
time that for each example,    ipped a coin and produced the correct answer with
2       , so it is a   -weak
id203 1
learner in expectation. in that case, if we called the weak learner t0 times, for any
   xed xi, hoe   ding bounds imply the chance the majority vote of those classi   ers is
incorrect on xi is at most e   2t0  2. so, the expected total number of mistakes m is
at most ne   2t0  2. what is interesting is that this is the exact bound we get from
boosting without the expectation for an adversarial weak-learner.

2 +    and the wrong answer with id203 1

a minimax view: consider a 2-player zero-sum game 24 with one row for each example
xi and one column for each hypothesis hj that the weak-learning algorithm might
output. if the row player chooses row i and the column player chooses column j,
then the column player gets a payo    of one if hj(xi) is correct and gets a payo   
of zero if hj(xi) is incorrect. the   -weak learning assumption implies that for any
randomized strategy for the row player (any    mixed strategy    in the language of
game theory), there exists a response hj that gives the column player an expected
payo    of at least 1
2 +   . the von neumann minimax theorem 25 states that this
implies there exists a id203 distribution on the columns (a mixed strategy for
the column player) such that for any xi, at least a 1
2 +    id203 mass of the

24a two person zero sum game consists of a matrix whose columns correspond to moves for player 1
and whose rows correspond to moves for player 2. the ijth entry of the matrix is the payo    for player
1 if player 1 choose the jth column and player 2 choose the ith row. player 2   s payo    is the negative of
player1   s.

25the von neumann minimax theorem states that there exists a mixed strategy for each player so that
given player 2   s strategy the best payo    possible for player 1 is the negative of given player 1   s strategy
the best possible payo    for player 2. a mixed strategy is one in which a id203 is assigned to every
possible move for each situation a player could be in.

159

columns under this distribution is correct on xi. we can think of boosting as a
fast way of    nding a very simple id203 distribution on the columns (just an
average over o(log n) columns, possibly with repetitions) that is nearly as good (for
any xi, more than half are correct) that moreover works even if our only access to
the columns is by running the weak learner and observing its outputs.

we argued above that t0 = o( 1
  2 log n) rounds of boosting are su   cient to produce a
majority-vote rule h that will classify all of s correctly. using our vc-dimension bounds,
this implies that if the weak learner is choosing its hypotheses from concept class h, then
a sample size

(cid:18)1

(cid:18)vcdim(h)

(cid:19)(cid:19)

n =   o

 

  2

is su   cient to conclude that with id203 1        the error is less than or equal to  ,
where we are using the   o notation to hide logarithmic factors. it turns out that running
the boosting procedure for larger values of t0 i.e., continuing past the point where s is
classi   ed correctly by the    nal majority vote, does not actually lead to greater over   tting.
the reason is that using the same type of analysis used to prove theorem 5.21, one can
show that as t0 increases, not only will the majority vote be correct on each x     s, but
2 +   (cid:48) fraction of the classi   ers,
in fact each example will be correctly classi   ed by a 1
where   (cid:48)        as t0        . i.e., the vote is approaching the minimax optimal strategy for
the column player in the minimax view given above. this in turn implies that h can be
well-approximated over s by a vote of a random sample of o(1/  2) of its component weak
hypotheses hj. since these small random majority votes are not over   tting by much, our
generalization theorems imply that h cannot be over   tting by much either.

5.13 stochastic id119

we now describe a widely-used algorithm in machine learning, called stochastic gradi-
ent descent (sgd). the id88 algorithm we examined in section 5.8.3 can be viewed
as a special case of this algorithm, as can methods for deep learning.

let f be a class of real-valued functions fw : rd     r where w = (w1, w2, . . . , wn) is a
vector of parameters. for example, we could think of the class of linear functions where
n = d and fw(x) = wt x, or we could have more complicated functions where n > d. for
each such function fw we can de   ne an associated set hw = {x : fw(x)     0}, and let
hf = {hw : fw     f}. for example, if f is the class of linear functions then hf is the
class of linear separators.

to apply stochastic id119, we also need a id168 l(fw(x), c   (x)) that
describes the real-valued penalty we will associate with function fw for its prediction on
an example x whose true label is c   (x). the algorithm is then the following:

160

stochastic id119:

given: starting point w = winit and learning rates   1,   2,   3, . . .

   
t).
(e.g., winit = 0 and   t = 1 for all t, or   t = 1/

consider a sequence of random examples (x1, c   (x1)), (x2, c   (x2)), . . ..
1. given example (xt, c   (xt)), compute the gradient    l(fw(xt), c   (xt)) of the loss of
fw(xt) with respect to the weights w. this is a vector in rn whose ith component is
   l(fw(xt),c   (xt))

   wi

.

2. update: w     w       t   l(fw(xt), c   (xt)).
let   s now try to understand the algorithm better by seeing a few examples of instan-

tiating the class of functions f and id168 l.

first, consider n = d and fw(x) = wt x, so f is the class of linear predictors. consider
the id168 l(fw(x), c   (x)) = max(0,   c   (x)fw(x)), and recall that c   (x)     {   1, 1}.
in other words, if fw(x) has the correct sign, then we have a loss of 0, otherwise we have
a loss equal to the magnitude of fw(x). in this case, if fw(x) has the correct sign and is
non-zero, then the gradient will be zero since an in   nitesimal change in any of the weights
will not change the sign. so, when hw(x) is correct, the algorithm will leave w alone.
=    c   (x)xi. so,
on the other hand, if fw(x) has the wrong sign, then    l
using   t = 1, the algorithm will update w     w + c   (x)x. note that this is exactly the
   wi
id88 algorithm. (technically we must address the case that fw(x) = 0; in this case,
we should view fw as having the wrong sign just barely.)

=    c   (x)    w  x

   wi

bound on error rate: for any sample s, the training error is at most(cid:80)

as a small modi   cation to the above example, consider the same class of linear predic-
tors f but now modify the id168 to the hinge-loss l(fw(x), c   (x)) = max(0, 1    
c   (x)fw(x)). this id168 now requires fw(x) to have the correct sign and have mag-
nitude at least 1 in order to be zero. hinge loss has the useful property that it is an upper
x   s l(fw(x), c   (x)).
with this id168, stochastic id119 is called the margin id88 algo-
rithm.

more generally, we could have a much more complex class f. for example, consider
a layered circuit of soft threshold gates. each node in the circuit computes a linear func-
tion of its inputs and then passes this value through an    activation function    such as
a(z) = tanh(z) = (ez     e   z)/(ez + e   z). this circuit could have multiple layers with
the output of layer i being used as the input to layer i + 1. the vector w would be the
concatenation of all the weight vectors in the network. this is the idea of deep neural
networks discussed further in section 5.15.

while it is di   cult to give general guarantees on when stochastic id119 will
succeed in    nding a hypothesis of low error on its training set s, theorems 5.7 and 5.5

161

imply that if it does and if s is su   ciently large, we can be con   dent that its true error
will be low as well. suppose that stochastic id119 is run on a machine where
each weight is a 64-bit    oating point number. this means that its hypotheses can each
be described using 64n bits. if s has size at least 1
  [64n ln(2) + ln(1/  )], by theorem 5.7
it is unlikely any such hypothesis of true error greater than   will be consistent with the
sample, and so if it    nds a hypothesis consistent with s, we can be con   dent its true error
is at most  . or, by theorem 5.5, if |s|     1
   nal hypothesis h produced by stochastic id119 satis   es true error leas than
or equal to training error plus  .

(cid:0)64n ln(2) + ln(2/  )(cid:1) then almost surely the

2 2

5.14 combining (sleeping) expert advice

imagine you have access to a large collection of rules-of-thumb that specify what to
predict in di   erent situations. for example, in classifying news articles, you might have
one that says    if the article has the word    football   , then classify it as sports    and another
that says    if the article contains a dollar    gure, then classify it as business   . in predicting
the stock market, these could be di   erent economic indicators. these predictors might
at times contradict each other, e.g., a news article that has both the word    football    and
a dollar    gure, or a day in which two economic indicators are pointing in di   erent direc-
tions. it also may be that no predictor is perfectly accurate with some much better than
others. we present here an algorithm for combining a large number of such predictors
with the guarantee that if any of them are good, the algorithm will perform nearly as well
as each good predictor on the examples on which that predictor    res.

formally, de   ne a    sleeping expert    to be a predictor h that on any given example x
either makes a prediction on its label or chooses to stay silent (asleep). we will think of
them as black boxes. now, suppose we have access to n such sleeping experts h1, . . . , hn,
and let si denote the subset of examples on which hi makes a prediction (e.g., this could
be articles with the word    football    in them). we consider the online learning model,
and let mistakes(a, s) denote the number of mistakes of an algorithm a on a sequence
of examples s. then the guarantee of our algorithm a will be that for all i

e(cid:0)mistakes(a, si)(cid:1)     (1 +  )    mistakes(hi, si) + o(cid:0) log n

(cid:1)

 

where   is a parameter of the algorithm and the expectation is over internal randomness
in the randomized algorithm a.

as a special case, if h1, . . . , hn are concepts from a concept class h, and so they all
make predictions on every example, then a performs nearly as well as the best concept
in h. this can be viewed as a noise-tolerant version of the halving algorithm of section
5.8.2 for the case that no concept in h is perfect. the case of predictors that make
predictions on every example is called the problem of combining expert advice, and the
more general case of predictors that sometimes    re and sometimes are silent is called the

162

sleeping experts problem.

combining sleeping experts algorithm:
initialize each expert hi with a weight wi = 1. let       (0, 1). for each example x, do the

following:

let wx = (cid:80)

hj   hx

1. [make prediction] let hx denote the set of experts hi that make a prediction on x, and

wj. choose hi     hx with id203 pix = wi/wx and predict hi(x).

2. [receive feedback] given the correct label, for each hi     hx let mix = 1 if hi(x) was

3. [update weights] for each hi     hx, update its weight as follows:

incorrect, else let mix = 0.

(cid:16)(cid:80)
note that(cid:80)

    let rix =
pjxmjx
    update wi     wi(1 +  )rix.

hj   hx

(cid:17)

/(1 +  )     mix.

hj   hx

pjxmjx represents the algorithm   s id203 of making a mis-
take on example x. so, hi is rewarded for predicting correctly (mix = 0) especially
when the algorithm had a high id203 of making a mistake, and hi is penal-
ized for predicting incorrectly (mix = 1) especially when the algorithm had a low
id203 of making a mistake.

for each hi (cid:54)    hx, leave wi alone.

theorem 5.22 for any set of n sleeping experts h1, . . . , hn, and for any sequence of
examples s, the combining sleeping experts algorithm a satis   es for all i:

e(cid:0)mistakes(a, si)(cid:1)     (1 +  )    mistakes(hi, si) + o(cid:0) log n

(cid:1)

 

where si = {x     s : hi     hx}.

proof: consider sleeping expert hi. the weight of hi after the sequence of examples s
is exactly:

(cid:80)

(cid:104)(cid:16)(cid:80)

(cid:105)
= (1 +  )e[mistakes(a,si)]/(1+ )   mistakes(hi,si).

/(1+ )   mix

hj   hx

pjxmjx

x   si

(cid:17)

wi = (1 +  )

let w =(cid:80)

j wj. clearly wi     w. therefore, taking logs, we have:

e(cid:0)mistakes(a, si)(cid:1)/(1 +  )     mistakes(hi, si)     log1+  w.
(cid:1) .
e(cid:0)mistakes(a, si)(cid:1)     (1 +  )    mistakes(hi, si) + o(cid:0) log w

),

 

 

so, using the fact that log1+  w = o( log w

163

(cid:88)

i

do so, we need to show that for each x,(cid:80)
wj that (cid:80)
dividing both sides by (cid:80)

initially, w = n. to prove the theorem, it is enough to prove that w never increases. to
wi, or equivalently
hi   hx
i pix(1 +  )rix     1, where for convenience we

wi(1 +  )rix    (cid:80)

hi   hx

hj   hx

de   ne pix = 0 for hi (cid:54)    hx.

for this we will use the inequalities that for   , z     [0, 1],   z     1     (1       )z and

     z     1 + (1       )z/  . speci   cally, we will use    = (1 +  )   1. we now have:

pix(1 +  )rix =

j pjxmjx)  

(cid:88)
pix  mix   ((cid:80)
(cid:16)
    (cid:88)
(cid:33)
(cid:32)(cid:88)

pix

i

i

   

pix

(cid:88)

1     (1       )mix

1 + (1       )

pjxmjx

(cid:17)(cid:32)
(cid:88)

(cid:32)(cid:88)
(cid:88)

j

(cid:33)(cid:33)
(cid:88)

    (1       )

pixmix + (1       )

pix

pjxmjx

i

= 1     (1       )

i

pixmix + (1       )

i

j

pjxmjx

(cid:88)

j

where the second-to-last line follows from using (cid:80)

= 1,

i

increases and the bound follows as desired.

5.15 deep learning

i pix = 1 in two places. so w never

deep learning, or deep neural networks, refers to training many-layered networks of

nonlinear computational units.

each computational unit or gate works as follows: there are a set of    wires    bringing
inputs to the gate. each wire has a    weight   ; the gate   s output is a real number obtained
by applying a non-linear    activation function    g : r     r to the the weighted sum of the
input values. the activation function g is generally the same for all gates in the network,
though, the number of inputs to individual gates may di   er.
the input to the network is an example x     rd. the    rst layer of the network
transforms the example into a new vector f1(x). then the second layer transforms f1(x)
into a new vector f2(f1(x)), and so on. finally, the kth layer outputs the    nal prediction
f (x) = fk(fk   1(. . . (f1(x)))).
in supervised learning, we are given training examples x1, x2, . . . , and corresponding
labels c   (x1), c   (x2), . . .. the training process    nds a set of weights of all wires so as to
minimize the error: (f0(x1     c   (x1))2 + (f0(x2)     c   (x2))2 +       . (one could alternatively
aim to minimize other quantities besides the sum of squared errors of training examples.)
often training is carried out by running stochastic gradient descient, i.e., doing stochastic
id119 in the weights space.

164

the motivation for deep learning is that often we are interested in data, such as images,
that are given to us in terms of very low-level features, such as pixel intensity values. our
goal is to achieve some higher-level understanding of each image, such as what objects
are in the image and what they are doing. to do so, it is natural to    rst convert the given
low-level representation into one of higher-level features. that is what the layers of the
network aim to do. deep learning is also motivated by id72, with the idea
that a good higher-level representation of data should be useful for a wide range of tasks.
indeed, a common use of deep learning for id72 is to share initial levels of
the network across tasks.

a typical architecture of a deep neural network consists of layers of logic units. in a
fully connected layer, the output of each gate in the layer is connected to the input of
every gate in the next layer. however, if the input is an image one might like to recognize
features independent of where they are located in the image. to achieve this one often
uses a number of convolution layers. in a convolution layer, each gate gets inputs from a
small k    k grid where k may be 5 to 10. there is a gate for each k    k square array of
the image. the weights on each gate are tied together so that each gate recognizes the
same feature. there will be several such collections of gates, so several di   erent features
can be learned. such a level is called a convolution level and the fully connected layers
are called autoencoder levels. a technique called pooling is used to keep the number of
gates reasonable. a small k    k grid with k typically set to two is used to scan a layer.
the stride is set so the grid will provide a non overlapping cover of the layer. each k    k
input grid will be reduced to a single cell by selecting the maximum input value or the
average of the inputs. for k = 2 this reduces the number of cells by a factor of four.

deep learning networks are trained by stochastic id119 (section 5.13), some-
times called back propagation in the network context. an error function is constructed
and the weights are adjusted using the derivative of the error function. this requires that
the error function be di   erentiable. a smooth threshold is used such as

tanh(x) =

ex     e   x
ex + e   x

where

or sigmod(x) = 1

1+e   x where
e   x

    sigmod(x)

   x

=

(1 + e   x)2 = sigmod(x)

in fact the function

relu (x) =

(cid:26) x x     0

0 otherwise

(cid:18)ex     e   x

(cid:19)2

   
   x

ex + e   x

ee     e   e
ex + e   x = 1    
1 + e   x = sigmoid(x)(cid:0)1     sigmoid(x)(cid:1).

e   x

(cid:26) 1 x     0

0 otherwise

where

   relu(x)

   x

=

seems to work well even though its derivative at x = 0 is unde   ned. an advantage of
relu over sigmoid is that relu does not saturate far from the origin.

165

                                                   
                                                   

each gate is connected to a
k    k grid. weights are tied
together.

second set of gates each
connected to a k    k grid.
weights are tied together.

figure 5.7: convolution layers

w1

w2

w3

w4

w5

w6

figure 5.8: a deep learning fully connected network.

166

w1

w2

w1

w2

w3

(a)

(b)

figure 5.9: autoencoder technique used to train one level at a time. in the figure 5.9
(a) train w1 and w2. then in figure 5.9 (b), freeze w1 and train w2 and w3. in this
way one trains one set of weights at a time.

training a deep learning network of 7 or 8 levels using id119 can be compu-
tationally expensive.26 to address this issue one can train one level at a time on unlabeled
data using an idea called autoencoding. there are three levels, the input, a middle level
called the hidden level, and an output level as shown in figure 5.9a. there are two sets
of weights. w1 is the weights of the hidden level gates and w2 is w t
1 . let x be the input
pattern and y be the output. the error is |x     y|2. one uses id119 to reduce
the error. once the weights w1 are determined they are frozen and a second hidden level
of gates is added as in figure 5.9 b. in this network w3 = w t
2 and stochastic gradient
descent is again used this time to determine w2. in this way one level of weights is trained
at a time.

the output of the hidden gates is an encoding of the input. an image might be a
108 dimensional input and there may only be 105 hidden gates. however, the number of
images might be 107 so even though the dimension of the hidden layer is smaller than the
dimension of the input, the number of possible codes far exceeds the number of inputs
and thus the hidden layer is a compressed representation of the input. if the hidden layer
were the same dimension as the input layer one might get the identity mapping. this
does not happen for id119 starting with random weights.

the output layer of a deep network typically uses a softmax procedure. softmax is
a generalization of id28 where given a set of vectors {x1, x2, . . . xn} with
labels l1, l2, . . . ln, li     {0, 1} and with a weight vector w we de   ne the id203 that
26in the image recognition community, researchers work with networks of 150 levels. the levels tend

to be convolution rather than fully connected.

167

(cid:17)

(cid:17)

(cid:19)

the label l given x equals 0 or 1 by

and

prob(l = 1|x) =

1

1 + e   wtx

=   (wtx)

prob(l = 0|x) = 1     prob(l = 1/x)

where    is the sigmoid function.

de   ne a cost function

j(w) =

li log(prob(l = 1|x)) + (1     li) log(1     prob(l = 1|x))

and compute w to minimize j(x). then

j(w) =

li log(  (wtx)) + (1     li) log(1       (wtx))

(cid:88)

(cid:16)

i

(cid:88)

(cid:16)

i

=   (wtx)(1       (wtx))xj, it follows that     log(  (wtx))

   wj

=   (wtx)(1     (wtx))xj

,

  (wtx)

since      (wtx)
thus

   wj

   j
   wj

=

=

=

=

(cid:18)
(cid:16)
(cid:16)
(cid:16)

i

(cid:88)
(cid:88)
(cid:88)
(cid:88)

i

i

i

  (wtx)(1       (wtx))

  (wtx)

xj     (1     li)

li

(1       (wtx))  (wtx)

1       (wtx)

xj

(cid:17)

(cid:17)

li(1       (wtx))xj     (1     li)  (wtx)xj

(lixj     li  (wtx)xj       (wtx)xj + li  (wtx)xj

(cid:17)

li       (wtx)

xj.

softmax is a generalization of id28 to multiple classes. thus, the labels
li take on values {1, 2, . . . , k}. for an input x, softmax estimates the id203 of each
label. the hypothesis is of the form

                prob(l = 1|x, w1)

prob(l = 2|x, w2)

prob(l = k|x, wk)

...

                =

                ewt

1 x
ewt
2 x
...
ewt
k x

               

1(cid:80)k

i=1 ewt
i x

hw(x) =

where the matrix formed by the weight vectors is

w = (w1, w2, . . . , wk)t

168

convolution

pooling

image

convolution levels

fully connected levels

softmax

figure 5.10: a convolution network

169

w is a matrix since for each label li, there is a vector wi of weights.

consider a set of n inputs {x1, x2, . . . , xn}. de   ne

(cid:26) 1 if l = k
(cid:80)k

0 otherwise

  (l = k) =

j(w ) =

n(cid:88)
   wij(w ) =     n(cid:88)

i=1

  (li = j) log

k(cid:88)
(cid:0)  (lj = k)     prob(lj = k)|xj, w(cid:1).

ewt
j xi
h=1 ewt
h xi

j=1

.

xj

the derivative of the cost function with respect to the weights is

and

note    wij(w ) is a vector. since wi is a vector, each component of    wij(w ) is the

derivative with respect to one component of the vector wi.

j=1

over    tting is a major concern in deep learning since large networks can have hun-
in image recognition, the number of training images can
dreds of millions of weights.
be signi   cantly increased by random jittering of the images. another technique called
dropout randomly deletes a fraction of the weights at each training iteration. regulariza-
tion is used to assign a cost to the size of weights and many other ideas are being explored.

deep learning is an active research area. some of the ideas being explored are what
do individual gates or sets of gates learn. if one trains a network twice from starting with
random sets of weights, do gates learn the same features? in image recognition, the early
convolution layers seem to learn features of images rather than features of the speci   c set
of images they are being trained with. once a network is trained on say a set of images
one of which is a cat one can freeze the weights and then    nd images that will map to
the activation vector generated by the cat image. one can take an artwork image and
separate the style from the content and then create an image using the content but a
di   erent style [geb15]. this is done by taking the activation of the original image and
moving it to the manifold of activation vectors of images of a given style. one can do
many things of this type. for example one can change the age of a child in an image
or change some other feature [gkl+15]. for more information about deep learning, see
[ben09].27

5.15.1 id3 (gans)

a method that is promising in trying to generate images that look real is to create code
that tries to discern between real images and synthetic images.

27see

also

the

tutorials:

http://deeplearning.net/tutorial/deeplearning.pdf

and

http://deeplearning.stanford.edu/tutorial/.

170

image

generator

real
image

synthetic

image

discriminator

one    rst trains the synthetic image discriminator to distinguish between real images and
synthetic ones. then one trains the image generator to generate images that the discrim-
inator believes are real images. alternating the training between the two units ends up
forcing the image generator to produce real looking images. this is the idea of generative
adversarial networks.

there are many possible applications for this technique. suppose you wanted to train
a network to translate from english to german. first train a discriminator to determine
if a sentence is a real sentence in german as opposed to a synthetic sentence. then train
a translator for english to german and a translator from german to english.

translate
to german

translate
to english

discriminator

5.16 further current directions

we now brie   y discuss a few additional current directions in machine learning, focusing
on semi-supervised learning, active learning, and id72.

5.16.1 semi-supervised learning

semi-supervised learning refers to the idea of trying to use a large unlabeled data set u to
augment a given labeled data set l in order to produce more accurate rules than would
have been achieved using just l alone. the motivation is that in many settings (e.g.,
document classi   cation, image classi   cation, id103), unlabeled data is much
more plentiful than labeled data, so one would like to make use of it if possible. of course,
unlabeled data is missing the labels! nonetheless it often contains information that an

171

algorithm can take advantage of.

as an example, suppose one believes the target function is a linear separator that
separates most of the data by a large margin. by observing enough unlabeled data to es-
timate the id203 mass near to any given linear separator, one could in principle then
discard separators in advance that slice through dense regions and instead focus attention
on just those that indeed separate most of the distribution by a large margin. this is the
high level idea behind a technique known as semi-supervised id166s. alternatively, sup-
pose data objects can be described by two di   erent    kinds    of features (e.g., a webpage
could be described using words on the page itself or using words on links pointing to the
page), and one believes that each kind should be su   cient to produce an accurate classi-
   er. then one might want to train a pair of classi   ers (one on each type of feature) and
use unlabeled data for which one is con   dent but the other is not to bootstrap, labeling
such examples with the con   dent classi   er and then feeding them as training data to the
less-con   dent one. this is the high-level idea behind a technique known as co-training.
or, if one believes    similar examples should generally have the same label   , one might
construct a graph with an edge between examples that are su   ciently similar, and aim for
a classi   er that is correct on the labeled data and has a small cut value on the unlabeled
data; this is the high-level idea behind graph-based methods.

a formal model: the batch learning model introduced in sections 5.1 and 5.6 in essence
assumes that one   s prior beliefs about the target function be described in terms of a class
of functions h. in order to capture the reasoning used in semi-supervised learning, we
need to also describe beliefs about the relation between the target function and the data
distribution. a clean way to do this is via a notion of compatibility    between a hypoth-
esis h and a distribution d. formally,    maps pairs (h,d) to [0, 1] with   (h,d) = 1
meaning that h is highly compatible with d and   (h,d) = 0 meaning that h is very
incompatible with d. the quantity 1      (h,d) is called the unlabeled error rate of h, and
denoted errunl(h). note that for    to be useful, it must be estimatable from a    nite sam-
ple; to this end, let us further require that    is an expectation over individual examples.
that is, overloading notation for convenience, we require   (h,d) = ex   d[  (h, x)], where
   : h    x     [0, 1].

for instance, suppose we believe the target should separate most data by margin   .
we can represent this belief by de   ning   (h, x) = 0 if x is within distance    of the de-
cision boundary of h, and   (h, x) = 1 otherwise. in this case, errunl(h) will denote the
id203 mass of d within distance    of h   s decision boundary. as a di   erent exam-
ple, in co-training, we assume each example can be described using two    views    that
each are su   cient for classi   cation; that is, there exist c   
2 such that for each example
x = (cid:104)x1, x2(cid:105) we have c   
2(x2). we can represent this belief by de   ning a hypothesis
h = (cid:104)h1, h2(cid:105) to be compatible with an example (cid:104)x1, x2(cid:105) if h1(x1) = h2(x2) and incompatible
otherwise; errunl(h) is then the id203 mass of examples on which h1 and h2 disagree.

1(x1) = c   

1, c   

172

as with the class h, one can either assume that the target is fully compatible (i.e.,
errunl(c   ) = 0) or instead aim to do well as a function of how compatible the target is.
the case that we assume c        h and errunl(c   ) = 0 is termed the    doubly realizable
case   . the concept class h and compatibility notion    are both viewed as known.

intuition: in this framework, the way that unlabeled data helps in learning can be in-
tuitively described as follows. suppose one is given a concept class h (such as linear
separators) and a compatibility notion    (such as penalizing h for points within distance
   of the decision boundary). suppose also that one believes c        h (or at least is close)
and that errunl(c   ) = 0 (or at least is small). then, unlabeled data can help by allowing
one to estimate the unlabeled error rate of all h     h, thereby in principle reducing the
search space from h (all linear separators) down to just the subset of h that is highly
compatible with d. the key challenge is how this can be done e   ciently (in theory,
in practice, or both) for natural notions of compatibility, as well as identifying types of
compatibility that data in important problems can be expected to satisfy.

a theorem: the following is a semi-supervised analog of our basic sample complexity
theorem, theorem 5.3. first,    x some set of functions h and compatibility notion   .

given a labeled sample l, de   ne (cid:99)err(h) to be the fraction of mistakes of h on l. given
an unlabeled sample u , de   ne   (h, u ) = ex   u [  (h, x)] and de   ne (cid:99)errunl(h) = 1     (h, u ).
that is, (cid:99)err(h) and (cid:99)errunl(h) are the empirical error rate and unlabeled error rate of h,
unlabeled set u drawn from d, the h     h that optimizes (cid:99)errunl(h) subject to (cid:99)err(h) = 0

respectively. finally, given    > 0, de   ne hd,  (  ) to be the set of functions f     h such
that errunl(f )       .
theorem 5.23 if c        h then with id203 at least 1       , for labeled set l and
will have errd(h)       for

equivalently, for |u| satisfying this bound, for any |l|, whp the h     h that minimizes

, and |l|     1
 

ln|hd,  (errunl(c   ) + 2 )| + ln

(cid:20)

|u|     2
 2

(cid:21)
(cid:99)errunl(h) subject to (cid:99)err(h) = 0 has
(cid:20)

ln|h| + ln

4
  

errd(h)     1
|l|

ln|hd,  (errunl(c   ) + 2 )| + ln

proof: by hoe   ding bounds, |u| is su   ciently large so that with id203 at least

{f     h : (cid:99)errunl(f )     errunl(c   ) +  }     hd,  (errunl(c   ) + 2 ).

1       /2, all h     h have |(cid:99)errunl(h)     errunl(h)|      . thus we have:
(cid:99)err(h) = 0 and (cid:99)errunl(h)     errunl(c   ) +   have errd(h)      ; furthermore, (cid:99)errunl(c   )    
h     h that optimizes (cid:99)errunl(h) subject to (cid:99)err(h) = 0 has errd(h)      , as desired.

the given bound on |l| is su   cient so that with id203 at least 1      , all h     h with
errunl(c   ) +  , so such a function h exists. therefore, with id203 at least 1       , the

(cid:21)

2
  

.

(cid:21)

2
  

.

(cid:20)

173

one can view theorem 5.23 as bounding the number of labeled examples needed to learn
well as a function of the    helpfulness    of the distribution d with respect to   . namely,
a helpful distribution is one in which hd,  (  ) is small for    slightly larger than the
compatibility of the true target function, so we do not need much labeled data to identify a
good function among those in hd,  (  ). for more information on semi-supervised learning,
see [bb10, bm98, csz06, joa99, zhu06, zgl03].

5.16.2 active learning

active learning refers to algorithms that take an active role in the selection of which ex-
amples are labeled. the algorithm is given an initial unlabeled set u of data points drawn
from distribution d and then interactively requests for the labels of a small number of
these examples. the aim is to reach a desired error rate   using much fewer labels than
would be needed by just labeling random examples (i.e., passive learning).

  log( 1

as a simple example, suppose that data consists of points on the real line and h =
{fa : fa(x) = 1 i    x     a} for a     r. that is, h is the set of all threshold functions on
the line. it is not hard to show (see exercise 5.2) that a random labeled sample of size
   )) is su   cient to ensure that with id203     1       , any consistent threshold
o( 1
a(cid:48) has error at most  . moreover, it is not hard to show that    ( 1
  ) random examples are
necessary for passive learning. however, with active learning we can achieve error   using
only o(log( 1
   )) labels. speci   cally,    rst draw an unlabeled sample u of size
o( 1
if these are both negative
then output a(cid:48) =    , and if these are both positive then output a(cid:48) =       . otherwise (the
leftmost is negative and the rightmost is positive), perform binary search to    nd two ad-
jacent examples x, x(cid:48) such that x is negative and x(cid:48) is positive, and output a(cid:48) = (x + x(cid:48))/2.
this threshold a(cid:48) is consistent with the labels on the entire set u , and so by the above
argument, has error       with id203     1       .

   )). then query the leftmost and rightmost points:

  ) + log log( 1

  log( 1

the agnostic case, where the target need not belong in the given class h is quite a bit
more subtle, and addressed in a quite general way in the    a2    agnostic active learning
algorithm [bbl09]. for more information on active learning, see [das11, bu14].

5.16.3 id72

in this chapter we have focused on scenarios where our goal is to learn a single target
function c   . however, there are also scenarios where one would like to learn multiple target
functions c   
n. if these functions are related in some way, then one could hope to
do so with less data per function than one would need to learn each function separately.
this is the idea of id72.

2, . . . , c   

1, c   

one natural example is object recognition. given an image x, c   

1(x) might be 1 if x is
a co   ee cup and 0 otherwise; c   
3(x) might
be 1 if x is a laptop and 0 otherwise. these recognition tasks are related in that image

2(x) might be 1 if x is a pencil and 0 otherwise; c   

174

features that are good for one task are likely to be helpful for the others as well. thus,
one approach to id72 is to try to learn a common representation under
which each of the target functions can be described as a simple function. another natural
example is personalization. consider a id103 system with n di   erent users.
in this case there are n target tasks (recognizing the speech of each user) that are clearly
related to each other. some good references for id72 are [tm95, thr96].

5.17 bibliographic notes

the basic theory underlying learning in the distributional setting was developed by vap-
nik [vap82], vapnik and chervonenkis [vc71], and valiant [val84]. the connection of
this to the notion of occam   s razor is due to [behw87]. for more information on uniform
convergence, id173 and complexity penalization, see [vap98]. the id88 al-
gorithm for online learning of linear separators was    rst analyzed by block [blo62] and
noviko    [nov62]; the proof given here is from [mp69]. a formal description of the on-
line learning model and its connections to learning in the distributional setting is given
in [lit87]. support vector machines and their connections to id81s were    rst
introduced by [bgv92], and extended by [cv95], with analysis in terms of margins given
by [stbwa98]. for further reading on id166s, learning with id81s, and regu-
larization, see [ss01]. vc dimension is due to vapnik and chervonenkis [vc71] with the
results presented here given in blumer, ehrenfeucht, haussler and warmuth [behw89].
boosting was    rst introduced by schapire [sch90], and adaboost and its guarantees are
due to freund and schapire [fs97] . analysis of the problem of combining expert advice
was given by littlestone and warmuth [lw94] and cesa-bianchi et al. [cbfh+97]; the
analysis of the sleeping experts problem given here is from [bm07].

175

5.18 exercises
exercise 5.1 (section 5.5 and 5.6) consider the instance space x = {0, 1}d and let
h be the class of 3-cnf formulas. that is, h is the set of concepts that can be described
as a conjunction of clauses where each clause is an or of up to 3 literals. (these are also
called 3-sat formulas). for example c    might be (x1     x2   x3)(x2   x4)(  x1   x3)(x2   x3   x4).
assume we are in the pac learning setting, so examples are drawn from some underlying
distribution d and labeled by some 3-cnf formula c   .

1. give a number of samples m that would be su   cient to ensure that with id203
    1       , all 3-cnf formulas consistent with the sample have error at most   with
respect to d.

2. give a polynomial-time algorithm for pac-learning the class of 3-cnf formulas.
exercise 5.2 (section 5.5) consider the instance space x = r, and the class of func-
tions h = {fa : fa(x) = 1 i    x     a} for a     r. that is, h is the set of all threshold
functions on the line. prove that for any distribution d, a sample s of size o( 1
  log( 1
   ))
is su   cient to ensure that with id203     1       , any fa(cid:48) such that errs(fa(cid:48)) = 0 has
errd(fa(cid:48))      . note that you can answer this question from    rst principles, without using
the concept of vc-dimension.

exercise 5.3 (id88; section 5.8.3) consider running the id88 algorithm
in the online model on some sequence of examples s. let s(cid:48) be the same set of examples
as s but presented in a di   erent order. does the id88 algorithm necessarily make
the same number of mistakes on s as it does on s(cid:48)? if so, why? if not, show such an s
and s(cid:48) (consisting of the same set of examples in a di   erent order) where the id88
algorithm makes a di   erent number of mistakes on s(cid:48) than it does on s.

exercise 5.4 (representation and linear separators) show that any disjunction (see
section 5.6.1) over {0, 1}d can be represented as a linear separator. show that moreover
   
d).
the margin of separation is    (1/
exercise 5.5 (linear separators; easy) show that the parity function on d     2
boolean variables cannot be represented by a linear threshold function. the parity function
is 1 if and only if an odd number of inputs is 1.

exercise 5.6 (id88; section 5.8.3) we know the id88 algorithm makes
at most 1/  2 mistakes on any sequence of examples that is separable by margin    (we
assume all examples are normalized to have length 1). however, it need not    nd a sep-
arator of large margin. if we also want to    nd a separator of large margin, a natural
alternative is to update on any example x such that f   (x)(w    x) < 1; this is called the
margin id88 algorithm.

1. argue why margin id88 is equivalent to running stochastic id119 on
the class of linear predictors (fw(x) = w    x) using hinge loss as the id168
and using   t = 1.

176

2. prove that on any sequence of examples that are separable by margin   , this algorithm

will make at most 3/  2 updates.

3. in part 2 you probably proved that each update increases |w|2 by at most 3. use
this (and your result from part 2) to conclude that if you have a dataset s that
is separable by margin   , and cycle through the data until the margin id88
algorithm makes no more updates, that it will    nd a separator of margin at least
  /3.

exercise 5.7 (id90, id173; section 5.6) pruning a decision tree:
let s be a labeled sample drawn iid from some distribution d over {0, 1}n, and suppose
we have used s to create some decision tree t . however, the tree t is large, and we are
concerned we might be over   tting. give a polynomial-time algorithm for pruning t that
   nds the pruning h of t that optimizes the right-hand-side of corollary 5.8, i.e., that for
a given    > 0 minimizes:

(cid:115)

errs(h) +

size(h) ln(4) + ln(2/  )

2|s|

.

to discuss this, we need to de   ne what we mean by a    pruning    of t and what we mean
by the    size    of h. a pruning h of t is a tree in which some internal nodes of t have been
turned into leaves, labeled    +    or           depending on whether the majority of examples in
s that reach that node are positive or negative. let size(h) = l(h) log(n) where l(h) is
the number of leaves in h.

hint #1: it is su   cient, for each integer l = 1, 2, . . . , l(t ), to    nd the pruning of t
with l leaves of lowest empirical error on s, that is, hl = argminh:l(h)=lerrs(h). then
you can just plug them all into the displayed formula above and pick the best one.

hint #2: use id145.

exercise 5.8 (id90, sleeping experts; sections 5.6, 5.14)    pruning    a
decision tree online via sleeping experts: suppose that, as in the above problem, we are
given a decision tree t , but now we are faced with a sequence of examples that arrive
online. one interesting way we can make predictions is as follows. for each node v of
t (internal node or leaf ) create two sleeping experts: one that predicts positive on any
example that reaches v and one that predicts negative on any example that reaches v. so,
the total number of sleeping experts is o(l(t )).

1. say why any pruning h of t , and any assignment of {+,   } labels to the leaves of h,
corresponds to a subset of sleeping experts with the property that exactly one sleeping
expert in the subset makes a prediction on any given example.

2. prove that for any sequence s of examples, and any given number of leaves l, if
, then the expected error
we run the sleeping-experts algorithm using   =
rate of the algorithm on s (the total number of mistakes of the algorithm divided by

|s|

(cid:113) l log(l(t ))

177

(cid:113) l log(l(t ))
(cid:104)

|s|) will be at most errs(hl) + o(
is the pruning of t with l leaves of lowest error on s.

|s|

), where hl = argminh:l(h)=lerrs(h)

3. in the above question, we assumed l was given. explain how we can remove this as-
by instantiating
sumption and achieve a bound of minl
l(t ) copies of the above algorithm (one for each value of l) and then combining
these algorithms using the experts algorithm (in this case, none of them will be
sleeping).

errs(hl) + o(

|s|

)

(cid:113) l log(l(t ))

(cid:105)

exercise 5.9 kernels; (section 5.3) prove theorem 5.2.

exercise 5.10 what is the vc-dimension of right corners with axis aligned edges that
are oriented with one edge going to the right and the other edge going up?

exercise 5.11 (vc-dimension; section 5.11) what is the vc-dimension v of the
class h of axis-parallel boxes in rd? that is, h = {ha,b : a, b     rd} where ha,b(x) = 1
if ai     xi     bi for all i = 1, . . . , d and ha,b(x) =    1 otherwise.

1. prove that the vc-dimension is at least your chosen v by giving a set of v points

that is shattered by the class (and explaining why it is shattered).

2. prove that the vc-dimension is at most your chosen v by proving that no set of

v + 1 points can be shattered.

exercise 5.12 (vc-dimension, id88, and margins; sections 5.8.3, 5.11)
say that a set of points s is shattered by linear separators of margin    if every labeling
of the points in s is achievable by a linear separator of margin at least   . prove that no
set of 1/  2 + 1 points in the unit ball is shattered by linear separators of margin   .

hint: think about the id88 algorithm and try a proof by contradiction.

exercise 5.13 (linear separators) suppose the instance space x is {0, 1}d and con-
sider the target function c    that labels an example x as positive if the least index i for
which xi = 1 is odd, else labels x as negative. in other words, c   (x) =    if x1 = 1 then
positive else if x2 = 1 then negative else if x3 = 1 then positive else ... else negative   .
show that the rule can be represented by a linear threshold function.

exercise 5.14 (linear separators; harder) prove that for the problem of exercise
5.13, we cannot have a linear separator with margin at least 1/f (d) where f (d) is bounded
above by a polynomial function of d.

exercise 5.15 vc-dimension prove that the vc-dimension of circles in the plane is
three.

exercise 5.16 vc-dimension show that the vc-dimension of arbitrary right triangles
in the plane is seven.

178

exercise 5.17 vc-dimension prove that the vc-dimension of triangles in the plane
is seven.

exercise 5.18 vc-dimension prove that the vc dimension of convex polygons in the
plane is in   nite.

exercise 5.19 at present there are many interesting research directions in deep learning
that are being explored. this exercise focuses on whether gates in networks learn the same
thing independent of the architecture or how the network is trained. on the web there
are several copies of alexnet that have been trained starting from di   erent random initial
weights. select two copies and form a matrix where the columns of the matrix correspond
to gates in the    rst copy of alexnet and the rows of the matrix correspond to gates of
the same level in the second copy. the ijth entry of the matrix is the covariance of the
activation of the jth gate in the    rst copy of alexnet with the ith gate in the second copy.
the covariance is the expected value over all images in the data set.

1. match the gates in the two copies of the network using a bipartite graph matching

algorithm. what is the fraction of matches that have a high covariance?

2. it is possible that there is no good one to one matching of gates but that some small
set of gates in the    rst copy of the network learn what some small set of gates in the
second copy learn. explore a id91 technique to match sets of gates and carry
out an experiment to do this.

exercise 5.20

1. input an image to a deep learning network. reproduce the image from the activation
vector, aimage, it produced by inputting a random image and producing an activation
vector arandom. then by id119 modify the pixels in the random image to
minimize the error function |aimage     arandom|2.

2. train a deep learning network to produce an image from an activation network.

exercise 5.21

1. create and train a simple deep learning network consisting of a convolution level with
pooling, a fully connected level, and then softmax. keep the network small. for input
data use the mnist data set http://yann.lecun.com/exdb/mnist/ with 28  28
images of digits. use maybe 20 channels for the convolution level and 100 gates for
the fully connected level.

2. create and train a second network with two fully connected levels, the    rst level with
200 gates and the second level with 100 gates. how does the accuracy of the second
network compare to the    rst?

179

3. train the second network again but this time use the activation vector of the 100
gate level and train the second network to produce that activation vector and only
then train the softmax. how does the accuracy compare to direct training of the
second network and the    rst network?

convolution

   rst network

second network

180

6 algorithms for massive data problems: stream-

ing, sketching, and sampling

6.1 introduction

this chapter deals with massive data problems where the input data is too large to be
stored in random access memory. one model for such problems is the streaming model,
where n data items a1, a2, . . . , an arrive one at a time. for example, the ai might be
ip addresses being observed by a router on the internet. the goal is for our algorithm
to compute some statistics, property, or summary of these data items without using too
much memory, much less than n. more speci   cally, we assume each ai itself is a b-bit
quantity where b is not too large. for example, each ai might be an integer in {1, . . . , m}
where m = 2b. our goal will be to produce some desired output using space polynomial
in b and log n; see figure 6.1.

for example, a very easy problem to solve in the streaming model is to compute the
sum of all the ai. if each ai is an integer between 1 and m = 2b, then the sum of all the ai
is an integer between 1 and mn and so the number of bits of memory needed to maintain
the sum is o(b + log n). a harder problem, which we discuss shortly, is computing the
number of distinct numbers in the input sequence.

one natural approach for tackling a range of problems in the streaming model is to
perform random sampling of the input    on the    y   . to introduce the basic    avor of
sampling on the    y, consider a stream a1, a2, . . . , an from which we are to select an index
i with id203 proportional to the value of ai. when we see an element, we do not
know the id203 with which to select it since the normalizing constant depends on
all of the elements including those we have not yet seen. however, the following method
works. let s be the sum of the ai   s seen so far. maintain s and an index i selected
with id203 ai
s . initially i = 1 and s = a1. having seen symbols a1, a2, . . . , aj, s will
equal a1 + a2 +        + aj and for each i in {1, . . . , j}, the selected index will be i with
s . on seeing aj+1, change the selected index to j + 1 with id203 aj+1
id203 ai
s+aj+1
and otherwise keep the same index as before with id203 1     aj+1
. if we change
the index to j + 1, clearly it was selected with the correct id203. if we keep i as our
selection, then it will have been selected with id203

s+aj+1

(cid:18)

(cid:19) ai

s

1     aj+1
s + aj+1

=

s

s + aj+1

ai
s

=

ai

s + aj+1

which is the correct id203 for selecting index i. finally s is updated by adding aj+1
to s. this problem comes up in many areas such as sleeping experts where there is a
sequence of weights and we want to pick an expert with id203 proportional to its
weight. the ai   s are the weights and the subscript i denotes the expert.

181

stream a1, a2, . . . , an

algorithm
(low space)

some output

figure 6.1: high-level representation of the streaming model

6.2 frequency moments of data streams

an important class of problems concerns the frequency moments of data streams. as
mentioned above, a data stream a1, a2, . . . , an of length n consists of symbols ai from
an alphabet of m possible symbols which for convenience we denote as {1, 2, . . . , m}.
throughout this section, n, m, and ai will have these meanings and s (for symbol) will
denote a generic element of {1, 2, . . . , m}. the frequency fs of the symbol s is the number
of occurrences of s in the stream. for a nonnegative integer p, the pth frequency moment
of the stream is

m(cid:88)

s=1

f p
s .

(cid:18)
m(cid:88)
(cid:18) m(cid:80)

s=1

n
m

s     2
f 2
(cid:19)1/p

f p
s

note that the p = 0 frequency moment corresponds to the number of distinct symbols
occurring in the stream using the convention 00 = 0. the    rst frequency moment is just
s , is useful in computing
the variance of the stream, i.e., the average squared di   erence from the average frequency.

n, the length of the string. the second frequency moment,(cid:80)
(cid:32)
(cid:17)2(cid:19)

(cid:16) n

(cid:17)2

(cid:33)

(cid:16)

s f 2

m(cid:88)

m(cid:88)

1
m

s=1

fs     n
m

=

1
m

fs +

m

=

1
m

f 2
s

s=1

    n2
m2

in the limit as p becomes large,

ment(s).

s=1

is the frequency of the most frequent ele-

we will describe sampling based algorithms to compute these quantities for streaming
data shortly. first a note on the motivation for these problems. the identity and fre-
quency of the the most frequent item, or more generally, items whose frequency exceeds a
given fraction of n, is clearly important in many applications. if the items are packets on
a network with source and/or destination addresses, the high frequency items identify the
heavy bandwidth users. if the data consists of purchase records in a supermarket, the high
frequency items are the best-selling items. determining the number of distinct symbols
is the abstract version of determining such things as the number of accounts, web users,
or credit card holders. the second moment and variance are useful in networking as well
as in database and other applications. large amounts of network log data are generated
by routers that can record the source address, destination address, and the number of
packets for all the messages passing through them. this massive data cannot be easily
sorted or aggregated into totals for each source/destination. but it is important to know

182

if some popular source-destination pairs have a lot of tra   c for which the variance is one
natural measure.

6.2.1 number of distinct elements in a data stream

consider a sequence a1, a2, . . . , an of n elements, each ai an integer in the range 1 to m
where n and m are very large. suppose we wish to determine the number of distinct ai in
the sequence. each ai might represent a credit card number extracted from a sequence of
credit card transactions and we wish to determine how many distinct credit card accounts
there are. note that this is easy to do in o(m) space by just storing a bit-vector that
records which elements have been seen so far and which have not. it is also easy to do in
o(n log m) space by storing a list of all distinct elements that have been seen. however,
our goal is to use space logarithmic in m and n. we    rst show that this is impossible
using an exact deterministic algorithm. any deterministic algorithm that determines the
number of distinct elements exactly must use at least m bits of memory on some input
sequence of length o(m). we then will show how to get around this problem using ran-
domization and approximation.

lower bound on memory for exact deterministic algorithm

we show that any exact deterministic algorithm must use at least m bits of memory
on some sequence of length m + 1. suppose we have seen a1, . . . , am, and suppose for sake
of contradiction that our algorithm uses less than m bits of memory on all such sequences.
there are 2m     1 possible subsets of {1, 2, . . . , m} that the sequence could contain and
yet only 2m   1 possible states of our algorithm   s memory. therefore there must be two
di   erent subsets s1 and s2 that lead to the same memory state.
if s1 and s2 are of
di   erent sizes, then clearly this implies an error for one of the input sequences. on the
other hand, if they are the same size, then if the next element is in s1 but not s2, the
algorithm will give the same answer in both cases and therefore must give an incorrect
answer on at least one of them.

algorithm for the number of distinct elements

to beat the above lower bound, consider approximating the number of distinct el-
ements. our algorithm will produce a number that is within a constant factor of the
correct answer using randomization and thus a small id203 of failure. first, the
idea: suppose the set s of distinct elements was itself chosen uniformly at random from
{1, . . . , m}. let min denote the minimum element in s. what is the expected value of
min? if there was one distinct element, then its expected value would be roughly m
2 . if
there were two distinct elements, the expected value of the minimum would be roughly
m
3 . more generally, for a random set s, the expected value of the minimum is approxi-
mately m|s|+1. see figure 6.2. solving min = m|s|+1 yields |s| = m
min     1. this suggests

keeping track of the minimum element in o(log m) space and using this equation to give

183

|s| + 1 subsets

(cid:125)(cid:124)

(cid:123)

(cid:122)

m|s|+1

figure 6.2: estimating the size of s from the minimum element in s which has value

approximately m|s|+1. the elements of s partition the set {1, 2, . . . , m} into |s| + 1 subsets
each of size approximately m|s|+1.

an estimate of |s|.

converting the intuition into an algorithm via hashing

in general, the set s might not have been chosen uniformly at random.

if the el-
ements of s were obtained by selecting the |s| smallest elements of {1, 2, . . . , m}, the
above technique would give a very bad answer. however, we can convert our intuition
into an algorithm that works well with high id203 on every sequence via hashing.
speci   cally, we will use a hash function h where

h : {1, 2, . . . , m}     {0, 1, 2, . . . , m     1} ,

and then instead of keeping track of the minimum element ai     s, we will keep track of
the minimum hash value. the question now is: what properties of a hash function do
we need? since we need to store h, we cannot use a totally random mapping since that
would take too many bits. luckily, a pairwise independent hash function, which can be
stored compactly is su   cient.

we recall the formal de   nition of pairwise independence below. but    rst recall that
a hash function is always chosen at random from a family of hash functions and phrases
like    id203 of collision    refer to the id203 in the choice of hash function.

2-universal (pairwise independent) hash functions

a set of hash functions

h =(cid:8)h | h : {1, 2, . . . , m}     {0, 1, 2, . . . , m     1}(cid:9)

is 2-universal or pairwise independent if for all x and y in {1, 2, . . . , m} with x (cid:54)= y,
h(x) and h(y) are each equally likely to be any element of {0, 1, 2, . . . , m     1} and are
statistically independent. it follows that a set of hash functions h is 2-universal if and
only if for all x and y in {1, 2, . . . , m}, x (cid:54)= y, h(x) and h(y) are each equally likely to be
any element of {0, 1, 2, . . . , m     1}, and for all w, z we have:

(cid:0)h (x) = w and h (y) = z(cid:1) = 1

m 2 .

prob
h   h

184

we now give an example of a 2-universal family of hash functions. let m be a prime
greater than m. for each pair of integers a and b in the range [0, m     1], de   ne a hash
function

hab (x) = ax + b

(mod m )

to store the hash function hab, store the two integers a and b. this requires only o(log m )
space. to see that the family is 2-universal note that h(x) = w and h(y) = z if and only
if

(cid:19)(cid:18) a

(cid:19)

=

b

(cid:19)

(cid:18) w

z

(mod m )

y 1

(cid:18) x 1
(cid:19)
(cid:18) x 1
(cid:19)
(cid:18)a

if x (cid:54)= y, the matrix

is invertible modulo m .28 thus

y 1

(cid:19)   1(cid:18)w
(cid:18)x 1
(cid:1) there is a unique(cid:0)a
(cid:1). hence

y 1

=

z

b

(cid:19)

and for each(cid:0)w

z

(mod m )

b

prob(cid:0)h(x) = w and h(y) = z(cid:1) =

1
m 2

and h is 2-universal.

analysis of distinct element counting algorithm

let b1, b2, . . . , bd be the distinct values that appear in the input. select h from the
2-universal family of hash functions h. then the set s = {h(b1), h(b2), . . . , h(bd)} is a
set of d random and pairwise independent values from the set {0, 1, 2, . . . , m     1}. we
now show that m
min is a good estimate for d, the number of distinct elements in the input,
where min = min(s).

lemma 6.1 with id203 at least 2
smallest element of s.

proof: first, we show that prob(cid:0) m

independence.

(cid:18) m

prob

> 6d

min

(cid:19)

3     d

min > 6d(cid:1) < 1
(cid:19)

(cid:19)

m
6d

(cid:18)

min <

= prob

(cid:18)

prob

h(bi) <

    d

m
6d

= prob

    d(cid:88)

i=1

m , we have d

6     m

min     6d, where min is the

6 + d

m . this part does not require pairwise

   k, h (bk) <

(cid:18)
(cid:32)(cid:100) m

6d(cid:101)

m

(cid:33)

    d

(cid:19)
(cid:18) 1

m
6d

6d

(cid:19)

+

1
m

    1
6

+

d
m

.

28the primality of m ensures that inverses of elements exist in z   

m and m > m ensures that if x (cid:54)= y,

then x and y are not equal mod m .

185

t

(cid:124)

(cid:123)(cid:122)

m
6d

(cid:125)

min here if
there exists
i, h(bi)     m

6d

m
d

6m
d

(cid:124)

(cid:125)

(cid:123)(cid:122)

min here
if for all i,
h(bi)     6m

d

figure 6.3: location of the minimum in the distinct counting algorithm.

next, we show that prob(cid:0) m
first, prob(cid:0) m

(cid:1) < 1
(cid:1) = prob(cid:0)   k, h (bk) > 6m
(cid:1) = prob(cid:0)min > 6m
(cid:26) 0 if h (bi) > 6m

min < d

de   ne the indicator variable

min < d

d

d

6

6

yi =

1 otherwise

d

6. this part will use pairwise independence.

(cid:1). for i = 1, 2, . . . , d,

and let

y =

d(cid:88)

i=1

yi.

var (y) = dvar (y1). further, since y1 is 0 or 1, var(y1) = e(cid:2)(y1     e(y1))2(cid:3) = e(y2

we want to show that with good id203, we will see a hash value in [0, 6m
d ], i.e.,
d, and e (y)     6. for 2-way
that prob(y = 0) is small. now prob (yi = 1)     6
independent random variables, the variance of their sum is the sum of their variances. so
1)    
e2(y1) = e(y1)     e2(y1)     e (y1) . thus var(y)     e (y). by the chebyshev inequality,

d, e (yi)     6
(cid:18)

(cid:1) = prob

(cid:19)

   k h (bk) >

6m
d

(cid:18) m

prob

<

d
6

min

(cid:19)

= prob(cid:0)min > 6m

d

= prob (y = 0)
    prob (|y     e (y)|     e (y))
    var(y)
e2 (y)

    1

    1
6

e (y)

since m
6     m
d

min > 6d with id203 at most 1
min     6d with id203 at least 2

6 + d
3     d
m .

m and m

min < d

6 with id203 at most 1
6,

6.2.2 number of occurrences of a given element.

to count the number of occurrences of a given element in a stream requires at most
log n space where n is the length of the stream. clearly, for any length stream that occurs
in practice, one can a   ord log n space. for this reason, the following material may never
be used in practice, but the technique is interesting and may give insight into how to solve

186

some other problem.

consider a string of 0   s and 1   s of length n in which we wish to count the number of
occurrences of 1   s. clearly with log n bits of memory we could keep track of the exact
number of 1   s. however, the number can be approximated with only log log n bits.

let m be the number of 1   s that occur in the sequence. keep a value k such that 2k
is approximately the number m of occurrences. storing k requires only log log n bits of
memory. the algorithm works as follows. start with k=0. for each occurrence of a 1,
add one to k with id203 1/2k. at the end of the string, the quantity 2k     1 is the
estimate of m. to obtain a coin that comes down heads with id203 1/2k,    ip a fair
coin, one that comes down heads with id203 1/2, k times and report heads if the fair
coin comes down heads in all k    ips.

given k, on average it will take 2k ones before k is incremented. thus, the expected

number of 1   s to produce the current value of k is 1 + 2 + 4 +        + 2k   1 = 2k     1.

6.2.3 frequent elements

the majority and frequent algorithms

first consider the very simple problem of n people voting. there are m candidates,
{1, 2, . . . , m}. we want to determine if one candidate gets a majority vote and if so
who. formally, we are given a stream of integers a1, a2, . . . , an, each ai belonging to
{1, 2, . . . , m}, and want to determine whether there is some s     {1, 2, . . . , m} which oc-
curs more than n/2 times and if so which s. it is easy to see that to solve the problem
exactly on read-once streaming data with a deterministic algorithm, requires    (min(n, m))
space. suppose n is even and the last n/2 items are identical. suppose also that after
reading the    rst n/2 items, there are two di   erent sets of elements that result in the same
content of our memory.
in that case, a mistake would occur if the second half of the
stream consists solely of an element that is in one set, but not in the other. if n/2     m
then there are at least 2m     1 possible subsets of the    rst n/2 elements.
if n/2     m

(cid:0)m
(cid:1) subsets. by the above argument, the number of bits of mem-

then there are (cid:80)n/2

ory must be at least the base 2 logarithm of the number of subsets, which is    (min(m, n)).

i=1

i

surprisingly, we can bypass the above lower bound by slightly weakening our goal.
again let   s require that if some element appears more than n/2 times, then we must
output it. but now, let us say that if no element appears more than n/2 times, then our
algorithm may output whatever it wants, rather than requiring that it output    no   . that
is, there may be    false positives   , but no    false negatives   .

majority algorithm

store a1 and initialize a counter to one. for each subsequent ai, if ai is the

187

same as the currently stored item, increment the counter by one. if it di   ers,
decrement the counter by one provided the counter is nonzero. if the counter
is zero, then store ai and set the counter to one.

to analyze the algorithm, it is convenient to view the decrement counter step as    elim-
inating    two items, the new one and the one that caused the last increment in the counter.
it is easy to see that if there is a majority element s, it must be stored at the end. if
not, each occurrence of s was eliminated; but each such elimination also causes another
item to be eliminated. thus for a majority item not to be stored at the end, more than
n items must have eliminated, a contradiction.

next we modify the above algorithm so that not just the majority, but also items
with frequency above some threshold are detected. more speci   cally, the algorithm below
   nds the frequency (number of occurrences) of each element of {1, 2, . . . , m} to within an
k+1. that is, for each symbol s, the algorithm produces a value   fs in
additive term of
[fs     n
k+1, fs], where fs is the true number of occurrences of symbol s in the sequence.
it will do so using o(k log n + k log m) space by keeping k counters instead of just one
counter.

n

algorithm frequent

maintain a list of items being counted. initially the list is empty. for each
item, if it is the same as some item on the list, increment its counter by one.
if it di   ers from all the items on the list, then if there are less than k items
on the list, add the item to the list with its counter set to one. if there are
already k items on the list, decrement each of the current counters by one.
delete an element from the list if its count becomes zero.

theorem 6.2 at the end of algorithm frequent, for each s     {1, 2, . . . , m}, its counter
on the list   fs satis   es   fs     [fs     n
k+1, fs]. if some s does not occur on the list, its counter
is zero and the theorem asserts that fs     n
k+1.
proof: the fact that   fs     fs is immediate. to show   fs     fs     n
k+1, view each decrement
counter step as eliminating some items. an item is eliminated if the current ai being read
is not on the list and there are already k symbols di   erent from it on the list; in this case, ai
and k other distinct symbols are simultaneously eliminated. thus, the elimination of each
occurrence of an s     {1, 2, . . . , m} is really the elimination of k + 1 items corresponding
to distinct symbols. thus, no more than n/(k + 1) occurrences of any symbol can be
eliminated. it is clear that if an item is not eliminated, then it must still be on the list at
the end. this proves the theorem.

theorem 6.2 implies that we can compute the true frequency of every s     {1, 2, . . . , m}

to within an additive term of n

k+1.

188

6.2.4 the second moment

and recall that the second moment of the stream is given by(cid:80)m
s occurs in the stream. at the end of the stream, the sum will equal (cid:80)m

this section focuses on computing the second moment of a stream with symbols from
{1, 2, . . . , m}. let fs denote the number of occurrences of the symbol s in the stream,
s . to calculate the
second moment, for each symbol s, 1     s     m, independently set a random variable xs
to   1 with id203 1/2. in particular, think of xs as the output of a random hash
function h(s) whose range is just the two buckets {   1, 1}. for now, think of h as a fully
independent hash function. maintain a sum by adding xs to the sum each time the symbol
s=1 xsfs. the
expected value of the sum will be zero where the expectation is over the choice of the   1
value for the xs.

s=1 f 2

(cid:33)

(cid:32) m(cid:88)

s=1

e

xsfs

= 0

although the expected value of the sum is zero, its actual value is a random variable and
the expected value of the square of the sum is given by

(cid:32) m(cid:88)

(cid:33)2

(cid:32) m(cid:88)

(cid:33)

s=1

s=1

(cid:32)(cid:88)

s(cid:54)=t

(cid:33)

m(cid:88)

s=1

e

xsfs

= e

x2
sf 2
s

+ 2e

xsxtfsft

=

f 2
s ,

the last equality follows since e (xsxt) = e(xs)e(xt) = 0 for s (cid:54)= t, using pairwise
independence of the random variables. thus

(cid:32) m(cid:88)

(cid:33)2

a =

xsfs

is an unbiased estimator of(cid:80)m
this point we could use markov   s inequality to state that prob(a     3(cid:80)m
(cid:33)
(cid:32) m(cid:88)

(cid:32) (cid:88)

we want to get a tighter guarantee. to do so, consider the second moment of a:

s in that it has the correct expectation. note that at
s )     1/3, but

(cid:33)4

s=1 f 2

s=1 f 2

s=1

e(a2) = e

xsfs

= e

xsxtxuxvfsftfufv

.

s=1

1   s,t,u,v   m

the last equality is by expansion. assume that the random variables xs are 4-wise inde-
pendent, or equivalently that they are produced by a 4-wise independent hash function.
then, since the xs are independent in the last sum, if any one of s, u, t, or v is distinct
from the others, then the expectation of the term is zero. thus, we need to deal only
with terms of the form x2

each term in the above sum has four indices, s, t, u, v, and there are (cid:0)4

t for t (cid:54)= s and terms of the form x4
sx2
s.

(cid:1) ways of

2

189

choosing two indices that have the same x value. thus,

(cid:33)

(cid:33)

x4
sf 4
s

(cid:32) m(cid:88)

s=1

x2
sx2

t f 2

s f 2
t

+ e

e(a2)    

2

e

(cid:32) m(cid:88)
(cid:18)4
(cid:19)
m(cid:88)
m(cid:88)
(cid:32) m(cid:88)

(cid:33)2

t=s+1

s=1

s=1

f 2
s

= 6

    3

m(cid:88)

t=s+1

m(cid:88)

s f 2
f 2

t +

f 4
s

s=1

= 3e2(a).

therefore, v ar(a) = e(a2)     e2(a)     2e2(a).

s=1

since the variance is comparable to the square of the expectation, repeating the process

several times and taking the average, gives high accuracy with high id203.

theorem 6.3 the average x of r = 2
4-way independent random variables is

  2   estimates a1, . . . , ar using independent sets of

prob (|x     e(x)| >   e(x)) <

v ar(x)
 2e2(x)

      .

proof: the proof follows from the fact that taking the average of r independent repe-
titions reduces variance by a factor of r, so that v ar(x)         2e2(x), and then applying
chebyshev   s inequality.

it remains to show that we can implement the desired 4-way independent random vari-
ables using o(log m) space. we earlier gave a construction for a pairwise-independent set
of hash functions; now we need 4-wise independence, though only into a range of {   1, 1}.
below we present one such construction.

error-correcting codes, polynomial interpolation and limited-way indepen-
dence

consider the problem of generating a random m-dimensional vector x of   1   s so that
any four coordinates are mutually independent. such an m-dimensional vector may be
generated from a truly random    seed    of only o(log m) mutually independent bits. thus,
we need only store the o(log m) bits and can generate any of the m coordinates when
needed. for any k, there is a    nite    eld f with exactly 2k elements, each of which can
be represented with k bits and arithmetic operations in the    eld can be carried out in
o(k2) time. here, k is the ceiling of log2 m. a basic fact about polynomial interpolation
is that a polynomial of degree at most three is uniquely determined by its value over
any    eld f at four points. more precisely, for any four distinct points a1, a2, a3, a4 in f
and any four possibly not distinct values b1, b2, b3, b4 in f , there is a unique polynomial
f (x) = f0 + f1x + f2x2 + f3x3 of degree at most three, so that with computations done

190

over f , f (ai) = bi 1     i     4.

the de   nition of the pseudo-random   1 vector x with 4-way independence is simple.
choose four elements f0, f1, f2, f3 at random from f and form the polynomial f (s) =
f0 + f1s + f2s2 + f3s3. this polynomial represents x as follows. for s = 1, 2, . . . , m, xs
is the leading bit of the k-bit representation of f (s).29 thus, the m-dimensional vector x
requires only o(k) bits where k = (cid:100)log m(cid:101).

lemma 6.4 the x de   ned above has 4-way independence.
proof: assume that the elements of f are represented in binary using   1 instead of the
traditional 0 and 1. let s, t, u, and v be any four coordinates of x and let   ,   ,   , and
   have values in   1. there are exactly 2k   1 elements of f whose leading bit is    and
similarly for   ,   , and   . so, there are exactly 24(k   1) 4-tuples of elements b1, b2, b3, and
b4 in f so that the leading bit of b1 is   , the leading bit of b2 is   , the leading bit of b3
is   , and the leading bit of b4 is   . for each such b1, b2, b3, and b4, there is precisely one
polynomial f so that f (s) = b1, f (t) = b2, f (u) = b3, and f (v) = b4. the id203
that xs =   , xt =   , xu =   , and xv =    is precisely

24(k   1)

total number of f

24(k   1)
24k =

1
16

.

=

four way independence follows since prob(xs =   ) = prob(xt =   ) = prob(xu =   ) =
prob(xv =   ) = 1/2 and thus

prob(xs =   )prob(xt =   )prob(xu =   )prob(xv =   )
= prob(xs =   , xt =   , xu =    and xs =   )

lemma 6.4 describes how to get one vector x with 4-way independence. however, we
need r = o(1/  2) mutually independent vectors. choose r independent polynomials at
the outset.

to implement the algorithm with low space, store only the polynomials in memory.
this requires 4k = o(log m) bits per polynomial for a total of o( log m
  2 ) bits. when a
symbol s in the stream is read, compute each polynomial at s to obtain the value for the
corresponding value of the xs and update the running sums. xs is just the leading bit of
the value of the polynomial evaluated at s. this calculation requires o(log m) time. thus,
we repeatedly compute the xs from the    seeds   , namely the coe   cients of the polynomials.

this idea of polynomial interpolation is also used in other contexts. error-correcting
codes is an important example. to transmit n bits over a channel which may introduce
noise, one can introduce redundancy into the transmission so that some channel errors

29here we have numbered the elements of the    eld f s = 1, 2, . . . , m.

191

can be corrected. a simple way to do this is to view the n bits to be transmitted as
coe   cients of a polynomial f (x) of degree n     1. now transmit f evaluated at points
1, 2, 3, . . . , n + m. at the receiving end, any n correct values will su   ce to reconstruct
the polynomial and the true message. so up to m errors can be tolerated. but even if
the number of errors is at most m, it is not a simple matter to know which values are
corrupted. we do not elaborate on this here.

6.3 matrix algorithms using sampling

we now move from the streaming model to a model where the input is stored in
memory, but because the input is so large, one would like to produce a much smaller
approximation to it, or perform an approximate computation on it in low space. for
instance, the input might be stored in a large slow memory and we would like a small
   sketch    that can be stored in smaller fast memory and yet retains the important prop-
erties of the original input. in fact, one can view a number of results from the chapter on
machine learning in this way: we have a large population, and we want to take a small
sample, perform some optimization on the sample, and then argue that the optimum
solution on the sample will be approximately optimal over the whole population. in the
chapter on machine learning, our sample consisted of independent random draws from
the overall population or data distribution. here we will be looking at matrix algorithms
and to achieve errors that are small compared to the frobenius norm of the matrix rather
than compared to the total number of entries, we will perform non-uniform sampling.

algorithms for matrix problems like id127, low-rank approximations,
singular value decomposition, compressed representations of matrices, id75
etc. are widely used but some require o(n3) time for n    n matrices.

the natural alternative to working on the whole input matrix is to pick a random
sub-matrix and compute with that. here, we will pick a subset of columns or rows of the
input matrix. if the sample size s is the number of columns we are willing to work with,
we will do s independent identical trials. in each trial, we select a column of the matrix.
all that we have to decide is what the id203 of picking each column is. sampling
uniformly at random is one option, but it is not always good if we want our error to be
a small fraction of the frobenius norm of the matrix. for example, suppose the input
matrix has all entries in the range [   1, 1] but most columns are close to the zero vector
with only a few signi   cant columns. then, uniformly sampling a small number of columns
is unlikely to pick up any of the signi   cant columns and essentially will approximate the
original matrix with the all-zeroes matrix.30

30there are, on the other hand, many positive statements one can make about uniform sampling.
for example, suppose the columns of a are data points in an m-dimensional space (one dimension per
row). fix any k-dimensional subspace, such as the subspace spanned by the k top singular vectors. if
we randomly sample   o(k/ 2) columns uniformly, by the vc-dimension bounds given in chapter 6, with
high id203 for every vector v in the k-dimensional space and every threshold    , the fraction of the

192

we will see that the    optimal    probabilities are proportional to the squared length of
columns. this is referred to as length squared sampling and since its    rst discovery in the
mid-90   s, has been proved to have several desirable properties which we will see. note
that all sampling we will discuss here is done with replacement.

two general notes on this approach:
(i) we will prove error bounds which hold for all input matrices. our algorithms
are randomized, i.e., use a random number generator, so the error bounds are random
variables. the bounds are on the expected error or tail id203 bounds on large errors
and apply to any matrix. note that this contrasts with the situation where we have a
stochastic model of the input matrix and only assert error bounds for    most    matrices
drawn from the id203 distribution of the stochastic model. a mnemonic is - our
algorithms can toss coins, but our data does not toss coins. a reason for proving error
bounds for any matrix is that in real problems, like the analysis of the web hypertext link
matrix or the patient-genome expression matrix, it is the one matrix the user is interested
in, not a random matrix. in general, we focus on general algorithms and theorems, not
speci   c applications, so the reader need not be aware of what the two matrices above
mean.

(ii) there is    no free lunch   . since we only work on a small random sample and not
on the whole input matrix, our error bounds will not be good for certain matrices. for
example, if the input matrix is the identity, it is intuitively clear that picking a few ran-
dom columns will miss the other directions.

to the reader: why aren   t (i) and (ii) mutually contradictory?

6.3.1 id127 using sampling

suppose a is an m   n matrix and b is an n   p matrix and the product ab is desired.
we show how to use sampling to get an approximate product faster than the traditional
multiplication. let a (:, k) denote the kth column of a. a (:, k) is a m    1 matrix. let
b (k, :) be the kth row of b. b (k, :) is a 1    n matrix. it is easy to see that

n(cid:88)

ab =

a (:, k)b (k, :) .

k=1

note that for each value of k, a(:, k)b(k, :) is an m   p matrix each element of which is a
single product of elements of a and b. an obvious use of sampling suggests itself. sample
some values for k and compute a (:, k) b (k, :) for the sampled k   s and use their suitably
scaled sum as the estimate of ab. it turns out that nonuniform sampling probabilities
are useful. de   ne a random variable z that takes on values in {1, 2, . . . , n}. let pk denote
the id203 that z assumes the value k. we will solve for a good choice of probabilities
sampled columns a that satisfy vt a        will be within     of the fraction of the columns a in the overall
matrix a satisfying vt a        .

193

later, but for now just consider the pk as nonnegative numbers that sum to one. de   ne
an associated random matrix variable that has value

x =

1
pk

a (:, k) b (k, :)

(6.1)

with id203 pk. let e (x) denote the entry-wise expectation.

e (x) =

prob(z = k)

1
pk

a (:, k) b (k, :) =

a (:, k)b (k, :) = ab.

n(cid:88)

k=1

n(cid:88)

k=1

this explains the scaling by 1
pk
each of whose components is correct in expectation. we will be interested in

in x. in particular, x is a matrix-valued random variable

e(cid:0)||ab     x||2

(cid:1) .

f

this can be viewed as the variance of x, de   ned as the sum of the variances of all its
entries.

(cid:88)

e(cid:0)x2

ij

(cid:1)     e (xij)2 =

(cid:32)(cid:88)

(cid:88)

(cid:33)

var(x) =

var (xij) =

m(cid:88)

p(cid:88)

pk

1
p2
k

a2
ikb2
kj

    ||ab||2
f .

i=1

j=1

ij

ij

k

we want to choose pk to minimize this quantity, and notice that we can ignore the ||ab||2
f
term since it doesn   t depend on the pk   s at all. we can now simplify by exchanging the
order of summations to get

(cid:88)

(cid:88)

ij

k

pk

1
p2
k

a2
ikb2

kj =

(cid:88)

k

1
pk

(cid:32)(cid:88)

(cid:33)(cid:32)(cid:88)

(cid:33)

a2
ik

b2
kj

=

i

j

k

(cid:88)

1
pk

(cid:12)(cid:12)a (:, k)(cid:12)(cid:12)2(cid:12)(cid:12)b (k, :)(cid:12)(cid:12)2.

what is the best choice of pk to minimize this sum? it can be seen by calculus31 that the
minimizing pk are proportional to |a(:, k)||b(k, :)|. in the important special case when
b = at , pick columns of a with probabilities proportional to the squared length of the
columns. even in the general case when b is not at , doing so simpli   es the bounds.
this sampling is called    length squared sampling   . if pk is proportional to |a (:, k)|2, i.e,
pk =

, then

|a(:,k)|2
||a||2

f

e(cid:0)||ab     x||2

f

(cid:1) = var(x)     ||a||2
(cid:80)s

f

(cid:88)

k

|b (k, :)|2 = ||a||2

f||b||2
f .

to reduce the variance, we can do s independent trials. each trial i, i = 1, 2, . . . , s
yields a matrix xi as in (6.1). we take 1
i=1 xi as our estimate of ab. since the
s
variance of a sum of independent random variables is the sum of variances, the variance

31by taking derivatives, for any set of nonnegative numbers ck,(cid:80)

ck
pk

is minimized with pk proportional

k

   

to

ck.

194

                                    

a

m    n

                                    

                                    

b

n    p

                                    

   

                                          

                                          

sampled
scaled
columns

of
a

m    s

       corresponding

scaled rows of b

s    p

      

figure 6.4: approximate id127 using sampling

(cid:80)s
s(cid:88)

i=1

1
s

i=1 xi is 1

of 1
s
in each trial. expanding this, gives:

s var(x) and so is at most 1

(cid:18) a (:, k1) b (k1, :)

pk1

xi =

1
s

s||a||2

f||b||2

f . let k1, . . . , ks be the k   s chosen

(cid:19)

+

a (:, k2) b (k2, :)

pk2

+        +

a (:, ks) b (ks, :)

pks

.

(6.2)

we will    nd it convieneint to write this as the product of an m    s matrix with a s    p
matrix as follows: let c be the m    s matrix consisting of the following columns which
are scaled versions of the chosen columns of a:
a(:, k2)   
spk2

a(:, k1)   
spk1

a(:, ks)   
spks

, . . .

,

.

note that the scaling has a nice property, which the reader can verify:

(6.3)
de   ne r to be the s  p matrix with the corresponding rows of b similarly scaled, namely,
r has rows

e(cid:0)cc t(cid:1) = aat .

b(k1, :)   
spk1

b(ks, :)   
spks

,

, . . .

b(k2, :)   
spk2

e(cid:0)rt r(cid:1) = bt b.

.

(6.4)

the reader may verify that

(cid:80)s

i=1 xi = cr. this is represented in figure 6.4. we summarize

from (6.2), we see that 1
s
our discussion in theorem 6.3.1.
theorem 6.5 suppose a is an m    n matrix and b is an n    p matrix. the product
ab can be estimated by cr, where c is an m    s matrix consisting of s columns of a
picked according to length-squared distribution and scaled to satisfy (6.3) and r is the

195

s    p matrix consisting of the corresponding rows of b scaled to satisfy (6.4). the error
is bounded by:

e(cid:0)||ab     cr||2

(cid:1)     ||a||2

f

f ||b||2
s

f

.

thus, to ensure e (||ab     cr||2
f , it su   ces to make s greater than or
equal to 1/  2. if    is    (1), so s     o(1), then the multiplication cr can be carried out in
time o(mp).

f )       2||a||2

f||b||2

when is this error bound good and when is it not? let   s focus on the case that b = at
so we have just one matrix to consider. if a is the identity matrix, then the guarantee is
not very good. in this case, ||aat||2
f = n, but the right-hand-side of the inequality is n2
s .
so we would need s > n for the bound to be any better than approximating the product
with the zero matrix.

more generally, the trivial estimate of the all zero matrix for aat makes an error in
frobenius norm of ||aat||f . what s do we need to ensure that the error is at most this?
if   1,   2, . . . are the singular values of a, then the singular values of aat are   2
2, . . .
and

1,   2

(cid:88)

||aat||2

f =

so from theorem 6.3.1, e(||aat     cr||2

(cid:88)

t

f =

  2
t .

f provided

t

  4
t

and

||a||2
f )     ||aat||2
2 + . . .)2
s     (  2
1 +   2
  4
1 +   4
2 + . . .

.

if rank(a) = r, then there are r non-zero   t and the best general upper bound on the
ratio (  2
1+  2
is r, so in general, s needs to be at least r. if a is full rank, this means
  4
1+  4
sampling will not gain us anything over taking the whole matrix!

2+...)2
2+...

however, if there is a constant c and a small integer p such that

1 +   2
  2

2 + . . . +   2

p     c(  2

1 +   2

2 +        +   2
r ),

(6.5)

then,

(  2
1 +   2
  4
1 +   4

2 + . . .)2
2 + . . .

    c2 (  2
1 +   2
  4
1 +   4

2 + . . . +   2
p)2
2 + . . . +   2
p

    c2p,

and so s     c2p gives us a better estimate than the zero matrix. increasing s by a factor
decreases the error by the same factor. condition 6.5 is indeed the hypothesis of the
subject of principal component analysis (pca) and there are many situations when the
data matrix does satisfy the condition and sampling algorithms are useful.

196

6.3.2

implementing length squared sampling in two passes

traditional matrix algorithms often assume that the input matrix is in random access
memory (ram) and so any particular entry of the matrix can be accessed in unit time.
for massive matrices, ram may be too small to hold the entire matrix, but may be able
to hold and compute with the sampled columns and rows.

consider a high-level model where the input matrix or matrices have to be read from
external memory using one pass in which one can read sequentially all entries of the ma-
trix and sample.

it is easy to see that two passes su   ce to draw a sample of columns of a according
to length squared probabilities, even if the matrix is not in row-order or column-order
and entries are presented as a linked list. in the    rst pass, compute the length squared of
each column and store this information in ram. the lengths squared can be computed as
running sums. then, use a random number generator in ram to determine according to
length squared id203 the columns to be sampled. then, make a second pass picking
the columns to be sampled.

if the matrix is already presented in external memory in column-order, then one pass
will do. the idea is to use the primitive in section 6.1: given a read-once stream of
positive numbers a1, a2, . . . , an, at the end have an i     {1, 2, . . . , n} such that the proba-
. filling in the speci   cs is left as an exercise for the reader.
bility that i was chosen is

ai(cid:80)n

j=1 aj

6.3.3 sketch of a large matrix

the main result of this section is that for any matrix, a sample of columns and rows,
each picked according to length squared distribution provides a good sketch of the matrix.
let a be an m   n matrix. pick s columns of a according to length squared distribution.
let c be the m   s matrix containing the picked columns scaled so as to satisy (6.3), i.e.,
   
spk. similarly, pick r rows of a according to length
if a(:, k) is picked, it is scaled by 1/
squared distribution on the rows of a. let r be the r  n matrix of the picked rows, scaled
   
rpk. we then have e(rt r) = at a.
as follows: if row k of a is picked, it is scaled by 1/
from c and r, one can    nd a matrix u so that a     cu r. the schematic diagram is
given in figure 6.5.

one may recall that the top k singular vectors of the svd of a give a similar picture;
however, the svd takes more time to compute, requires all of a to be stored in ram,
and does not have the property that the rows and columns are directly from a. the last
property, that the approximation involves actual rows/columns of the matrix rather than
linear combinations, is called an interpolative approximation and is useful in many con-
texts. on the other hand, the svd yields the best 2-norm approximation. error bounds
for the approximation cu r are weaker.

197

                                    

                                    

                                    

   

                                    

sample
columns

n    s

a

n    m

(cid:21)(cid:20) sample rows

r    m

(cid:21)

(cid:20) multi

plier
s    r

figure 6.5: schematic diagram of the approximation of a by a sample of s columns and
r rows.

we brie   y touch upon two motivations for such a sketch. suppose a is the document-
term matrix of a large collection of documents. we are to    read    the collection at the
outset and store a sketch so that later, when a query represented by a vector with one
entry per term arrives, we can    nd its similarity to each document in the collection.
similarity is de   ned by the dot product. in figure 6.5 it is clear that the matrix-vector
product of a query with the right hand side can be done in time o(ns + sr + rm) which
would be linear in n and m if s and r are o(1). to bound errors for this process, we
need to show that the di   erence between a and the sketch of a has small 2-norm. re-
call that the 2-norm ||a||2 of a matrix a is max
|ax|. the fact that the sketch is an
|x|=1
interpolative approximation means that our approximation essentially consists a subset
of documents and a subset of terms, which may be thought of as a representative set of
documents and terms. additionally, if a is sparse in its rows and columns, each document
contains only a small fraction of the terms and each term is in only a small fraction of
the documents, then this sparsity property will be preserved in c and r, unlike with svd.

a second motivation comes from analyzing gene microarray data. here, a is a matrix
in which each row is a gene and each column is a condition. entry (i, j) indicates the
extent to which gene i is expressed in condition j. in this context, a cu r decomposition
provides a sketch of the matrix a in which rows and columns correspond to actual genes
and conditions, respectively. this can often be easier for biologists to interpret than a
singular value decomposition in which rows and columns would be linear combinations of
the genes and conditions.

it remains now to describe how to    nd u from c and r. there is a n    n matrix p
of the form p = qr that acts as the identity on the space spanned by the rows of r and
zeros out all vectors orthogonal to this space. we state this now and postpone the proof.
lemma 6.6 if rrt is invertible, then p = rt (rrt )   1r has the following properties:

198

(i) it acts as the identity matrix on the row space of r. i.e., p x = x for every vector x

of the form x = rt y (this de   nes the row space of r). furthermore,

(ii) if x is orthogonal to the row space of r, then p x = 0.

if rrt is not invertible, let rank (rrt ) = r and rrt = (cid:80)r

rrt . then,

p = rt

satis   es (i) and (ii).

t=1

(cid:32) r(cid:88)

(cid:33)

1
  2
t

t

utvt

r

t=1   tutvt

t be the svd of

we begin with some intuition. in particular, we    rst present a simpler idea that does
not work, but that motivates an idea that does. write a as ai, where i is the n    n
identity matrix. approximate the product ai using the algorithm of theorem 6.3.1, i.e.,
by sampling s columns of a according to a length-squared distribution. then, as in the
last section, write ai     cw , where w consists of a scaled version of the s rows of i
corresponding to the s columns of a that were picked. theorem 6.3.1 bounds the error
||a   cw||2
f . but we would like the error to be a small fraction
of ||a||2
f which would require s     n, which clearly is of no use since this would pick as
many or more columns than the whole of a.

f by ||a||2

s||a||2

f||i||2

f /s = n

let   s use the identity-like matrix p instead of i in the above discussion. using the
fact that r is picked according to length squared sampling, we will show the following
proposition later.
proposition 6.7 a     ap and the error e (||a     ap||2

2) is at most

r||a||2
1   
f .

we then use theorem 6.3.1 to argue that instead of doing the multiplication ap , we can
use the sampled columns of a and the corresponding rows of p . the s sampled columns
of a form c. we have to take the corresponding s rows of p = rt (rrt )   1r, which is
the same as taking the corresponding s rows of rt , and multiplying this by (rrt )   1r. it
is easy to check that this leads to an expression of the form cu r. further, by theorem
6.3.1, the error is bounded by

e(cid:0)||ap     cu r||2

(cid:1)     e(cid:0)||ap     cu r||2

(cid:1)     ||a||2

f

2

f||p||2
s

f

    r
s

||a||2
f ,

(6.6)

since we will show later that:
f     r.
proposition 6.8 ||p||2

putting (6.6) and proposition 6.7 together, and using the fact that by triangle inequality
||a    cu r||2     ||a    ap||2 +||ap     cu r||2, which in turn implies that ||a    cu r||2
2    
2||a     ap||2

2 + 2||ap     cu r||2

2, the main result below follows.

199

theorem 6.9 let a be an m    n matrix and r and s be positive integers. let c be an
m    s matrix of s columns of a picked according to length squared sampling and let r be
a matrix of r rows of a picked according to length squared sampling. then, we can    nd
from c and r an s    r matrix u so that

e(cid:0)||a     cu r||2

(cid:1)     ||a||2

2

f

(cid:18) 2   

(cid:19)

.

+

2r
s

r

if s is    xed, the error is minimized when r = s2/3. choosing s = 1/  3 and r = 1/  2,
the bound becomes o(  )||a||2
f . when is this bound meaningful? we discuss this further
after    rst proving all the claims used in the discussion above.

proof of lemma 6.6: first consider the case that rrt is invertible. for x = rt y,
rt (rrt )   1rx = rt (rrt )   1rrt y = rt y = x. if x is orthogonal to every row of r,
r =

then rx = 0, so p x = 0. more generally, if rrt = (cid:80)
(cid:80)

t , then, rt(cid:80)

t   tutvt

t and clearly satis   es (i) and (ii).

1
  2
t

t

t vtvt

next we prove proposition 6.7. first, recall that

||a     ap||2

2 = max

{x:|x|=1}

|(a     ap )x|2.

now suppose x is in the row space v of r. from lemma 6.6, p x = x, so for x     v ,
(a    ap )x = 0. since every vector can be written as a sum of a vector in v plus a vector
orthogonal to v , this implies that the maximum must therefore occur at some x     v    .
for such x, by lemma 6.6, (a   ap )x = ax. thus, the question becomes: for unit-length
x     v    , how large can |ax|2 be? to analyze this, write:

|ax|2 = xt at ax = xt (at a     rt r)x     ||at a     rt r||2|x|2     ||at a     rt r||2.

2     ||a||4

2     ||at a     rt r||2. so, it su   ces to prove that ||at a    
this implies that ||a     ap||2
rt r||2
f /r which follows directly from theorem 6.3.1, since we can think of rt r
as a way of estimating at a by picking according to length-squared distribution columns
of at , i.e., rows of a. this proves proposition 6.7.

proposition 6.8 is easy to see. by lemma 6.6, p is the identity on the space v spanned
f is the

by the rows of r, and p x = 0 for x perpendicular to the rows of r. thus ||p||2
sum of its singular values squared which is at most r as claimed.

we now brie   y look at the time needed to compute u . the only involved step in
computing u is to    nd (rrt )   1 or do the svd of rrt . but note that rrt is an r    r
matrix and since r is much smaller than n and m, this is fast.

200

i   2

1(a) = 1. then

1(a) = ||a||2
  2

f = (cid:80)

2 = 1 and e(||a     cu r||2

understanding the bound in theorem 6.9: to better understand the bound in
theorem 6.9 consider when it is meaningful and when it is not. first, choose parameters
s =   (1/  3) and r =   (1/  2) so that the bound becomes e(||a     cu r||2
2)       ||a||2
f .
recall that ||a||2
i (a), i.e., the sum of squares of all the singular values of a.
also, for convenience scale a so that   2

(cid:88)
top k singular values of a are all    (1) for k (cid:29) m1/3, so that (cid:80)
to a low-dimensional subspace, and in particular if (cid:80)

this, gives an intuitive sense of when the guarantee is good and when it is not. if the
i (a) (cid:29) m1/3, then
the guarantee is only meaningful when    = o(m   1/3), which is not interesting because it
requires s > m. on the other hand, if just the    rst few singular values of a are large
and the rest are quite small, e.g, a represents a collection of points that lie very close
i (a) is a constant, then to be
meaningful the bound requires    to be a small constant. in this case, the guarantee is
indeed meaningful because it implies that a constant number of rows and columns provides
a good 2-norm approximation to a.

2)       

  2
i (a).

i   2

i   2

i

6.4 sketches of documents

suppose one wished to store all the web pages from the world wide web. since there
are billions of web pages, one might want to store just a sketch of each page where a sketch
is some type of compact description that captures su   cient information to do whatever
task one has in mind. for the current discussion, we will think of a web page as a string
of characters, and the task at hand will be one of estimating similarities between pairs of
web pages.

we begin this section by showing how to estimate similarities between sets via sam-
pling, and then how to convert the problem of estimating similarities between strings into
a problem of estimating similarities between sets.

consider subsets of size 1000 of the integers from 1 to 106. suppose one wished to

compute the resemblance of two subsets a and b by the formula

resemblance (a, b) =

|a   b|
|a   b|

suppose that instead of using the sets a and b, one sampled the sets and compared
random subsets of size ten. how accurate would the estimate be? one way to sample
would be to select ten elements uniformly at random from a and b. suppose a and b
were each of size 1000, over lapped by 500, and both were represented by six samples.
even though half of the six samples of a were in b they would not likely be among the
samples representing b. see figure 6.6. this method is unlikely to produce overlapping
samples. another way would be to select the ten smallest elements from each of a and
b. if the sets a and b overlapped signi   cantly one might expect the sets of ten smallest

201

a

b

figure 6.6: samples of overlapping sets a and b.

elements from each of a and b to also overlap. one di   culty that might arise is that the
small integers might be used for some special purpose and appear in essentially all sets
and thus distort the results. to overcome this potential problem, rename all elements
using a random permutation.

suppose two subsets of size 1000 overlapped by 900 elements. what might one expect
the overlap of the 10 smallest elements from each subset to be? one would expect the
nine smallest elements from the 900 common elements to be in each of the two sampled
subsets for an overlap of 90%. the expected resemblance(a, b) for the size ten sample
would be 9/11=0.81.

another method would be to select the elements equal to zero mod m for some inte-
ger m. if one samples mod m the size of the sample becomes a function of n. sampling
mod m allows one to handle containment.

in another version of the problem one has a string of characters rather than a set.
here one converts the string into a set by replacing it by the set of all of its substrings
of some small length k. corresponding to each string is a set of length k substrings. if
k is modestly large, then two strings are highly unlikely to give rise to the same set of
substrings. thus, we have converted the problem of sampling a string to that of sampling
a set. instead of storing all the substrings of length k, we need only store a small subset
of the length k substrings.

suppose you wish to be able to determine if two web pages are minor modi   cations
of one another or to determine if one is a fragment of the other. extract the sequence of
words occurring on the page, viewing each word as a character. then de   ne the set of
substrings of k consecutive words from the sequence. let s(d) be the set of all substrings
of k consecutive words occurring in document d. de   ne resemblance of a and b by

and de   ne containment as

resemblance (a, b) =

|s(a)   s(b)|
|s(a)   s(b)|

containment (a, b) =

|s(a)   s(b)|

|s(a)|

let    be a random permutation of all length k substrings. de   ne f (a) to be the s
smallest elements of a and v (a) to be the set mod m in the ordering de   ned by the

202

permutation.

then

and

f (a)     f (b)
f (a)     f (b)

|v (a)   v (b)|
|v (a)   v (b)|

are unbiased estimates of the resemblance of a and b. the value

|v (a)   v (b)|

|v (a)|

is an unbiased estimate of the containment of a in b.

6.5 bibliographic notes

the hashing-based algorithm for counting the number of distrinct elements in a data
stream described in section 6.2.1 is due to flajolet and martin [fm85]. algorithm fre-
quent for identifying the most frequent elements is due to misra and gries [mg82]. the
algorithm for estimating the second moment of a data stream described in section 6.2.4
is due to alon, matias and szegedy [ams96], who also gave algorithms and lower bounds
for other kth moments. these early algorithms for streaming data signi   cantly in   uenced
further research in the area. improvements and generalizations of algorithm frequent
were made in [mm02].

length-squared sampling was introduced by frieze, kannan and vempala [fkv04];
the algorithms of section 6.3 are from [dkm06a, dkm06b]. the material in section 6.4
on sketches of documents is from broder et al. [bgmz97].

203

6.6 exercises
exercise 6.1 let a1, a2, . . . , an, be a stream of symbols each an integer in {1, . . . , m}.

1. give an algorithm that will select a symbol uniformly at random from the stream.

how much memory does your algorithm require?

2. give an algorithm that will select a symbol with id203 proportional to a2
i .

exercise 6.2 how would one pick a random word from a very large book where the prob-
ability of picking a word is proportional to the number of occurrences of the word in the
book?

exercise 6.3 consider a matrix where each element has a id203 of being selected.
can you select a row according to the sum of probabilities of elements in that row by just
selecting an element according to its id203 and selecting the row that the element is
in?

exercise 6.4 for the streaming model give an algorithm to draw t independent samples
of indices i, each with id203 proportional to the value of ai. some images may be
drawn multiple times. what is its memory usage?
exercise 6.5 for some constant c > 0, it is possible to create 2cm subsets of {1, . . . , m},
each with m/2 elements, such that no two of the subsets share more than 3m/8 elements
in common.32 use this fact to argue that any deterministic algorithm that even guarantees
to approximate the number of distinct elements in a data stream with error less than m
must use    (m) bits of memory on some input sequence of length n     2m.
16

exercise 6.6 consider an algorithm that uses a random hash function and gives an
estimate   x of the true value x of some variable. suppose that x
at least 0.6. the id203 of the estimate is with respect to choice of the hash function.
how would you improve the id203 that x
know the variance taking average may not help and we need to use some other function
of multiple runs.

4       x     4x with id203
4       x     4x to 0.8? hint: since we do not

exercise 6.7 give an example of a set h of hash functions such that h(x) is equally
likely to be any element of {0, . . . , m     1} (h is 1-universal) but h is not 2-universal.

exercise 6.8 let p be a prime. a set of hash functions

h = {h|{0, 1, . . . , p     1}     {0, 1, . . . , p     1}}

is 3-universal if for all x,y,z,u,v,w in {0, 1, . . . , p     1} , where x, y, z are distinct we have

(cid:16)

prob

h (x) = u, h (y) = v, h (z) = w

(cid:17)

=

1
p3 .

32for example, choosing them randomly will work with high id203. you expect two subsets of size
m/2 to share m/4 elements in common, and with high id203 they will share no more than 3m/8.

204

(a) is the set {hab(x) = ax + b mod p | 0     a, b < p} of hash functions 3-universal?

(b) give a 3-universal set of hash functions.

exercise 6.9 select a value for k and create a set

h =(cid:8)x|x = (x1, x2, . . . , xk), xi     {0, 1, . . . , k     1}(cid:9)

where the set of vectors h is pairwise independent and |h| < kk. we say that a set of vec-
tors is pairwise independent if for any subset of two of the coordinates, all of the k2 possible
pairs of values that could appear in those coordinates such as (0, 0), (0, 1), . . . , (1, 0), (1, 1), . . .
occur the exact same number of times.

exercise 6.10 consider a coin that comes down heads with id203 p. prove that the
expected number of    ips needed to see a heads is 1/p.
exercise 6.11 randomly generate a string x1x2        xn of 106 0   s and 1   s with id203
1/2 of xi being a 1. count the number of ones in the string and also estimate the number of
ones by the coin-   ip approximate counting algorithm, in section 6.2.2. repeat the process
for p=1/4, 1/8, and 1/16. how close is the approximation?

counting frequent elements
the majority and frequent algorithms
the second moment

exercise 6.12

1. construct an example in which the majority algorithm gives a false positive, i.e.,

stores a non majority element at the end.

2. construct an example in which the frequent algorithm in fact does as badly as in the

theorem, i.e., it under counts some item by n/(k+1).

exercise 6.13 let p be a prime and n     2 be an integer. what representation do you
use to do arithmetic in the    nite    eld with pn elements? how do you do addition? how
do you do multiplication?

error-correcting codes, polynomial interpolation and limited-way indepen-
dence

exercise 6.14 let f be a    eld. prove that for any four distinct points a1, a2, a3, and a4
in f and any four possibly not distinct values b1, b2, b3, and b4 in f , there is a unique
polynomial f (x) = f0 + f1x + f2x2 + f3x3 of degree at most three so that f (ai) = bi,
1     i     4 with all computations done over f . if you use the vandermonde matrix you
can use the fact that the matrix is nonsingular.

205

sketch of a large matrix

exercise 6.15 suppose we want to pick a row of a matrix at random where the id203
of picking row i is proportional to the sum of squares of the entries of that row. how would
we do this in the streaming model?

(a) do the problem when the matrix is given in column order.

(b) do the problem when the matrix is represented in sparse notation: it is just presented

as a list of triples (i, j, aij), in arbitrary order.

id127 using sampling

exercise 6.16 suppose a and b are two matrices. prove that ab =

n(cid:80)

k=1

a (:, k)b (k, :).

exercise 6.17 generate two 100 by 100 matrices a and b with integer values between
1 and 100. compute the product ab both directly and by sampling. plot the di   erence
in l2 norm between the results as a function of the number of samples. in generating
the matrices make sure that they are skewed. one method would be the following. first
generate two 100 dimensional vectors a and b with integer values between 1 and 100. next
generate the ith row of a with integer values between 1 and ai and the ith column of b
with integer values between 1 and bi.

approximating a matrix with a sample of rows and columns

subject to the constraints xk     0 and (cid:80)

exercise 6.18 suppose a1, a2, . . . , am are nonnegative reals. show that the minimum
xk = 1 is attained when the xk are
of

m(cid:80)

ak
xk

k=1

   

proportional to

ak.

k

sketches of documents

exercise 6.19 construct two di   erent strings of 0   s and 1   s having the same set of sub-
strings of length k = 3.

exercise 6.20 (random strings, empirical analysis). consider random strings of length
n composed of the integers 0 through 9, where we represent a string x by its set sk(x)
of length k-substrings. perform the following experiment: choose two random strings x
|sk(x)   sk(y)|
and y of length n = 10, 000 and compute their resemblance
|sk(x)   sk(y)| for k = 1, 2, 3 . . ..
what does the graph of resemblance as a function of k look like?

exercise 6.21 (random strings, theoretical analysis). consider random strings of length
n composed of the integers 0 through 9, where we represent a string x by its set sk(x) of
length k-substrings. consider now drawing two random strings x and y of length n and
computing their resemblance

|sk(x)   sk(y)|
|sk(x)   sk(y)|.

206

1. prove that for k     1

strings have resemblance equal to 1.

2 log10(n), with high id203 as n goes to in   nity the two

2. prove that for k     3 log10(n), with high id203 as n goes to in   nity the two

strings have resemblance equal to 0.

exercise 6.22 discuss how you might go about detecting plagiarism in term papers.

exercise 6.23 suppose you had one billion web pages and you wished to remove dupli-
cates. how might you do this?

exercise 6.24 consider the following lyrics:

when you walk through the storm hold your head up high and don   t be afraid
of the dark. at the end of the storm there   s a golden sky and the sweet silver
song of the lark.
walk on, through the wind, walk on through the rain though your dreams be
tossed and blown. walk on, walk on, with hope in your heart and you   ll never
walk alone, you   ll never walk alone.

how large must k be to uniquely recover the lyric from the set of all subsequences of
symbols of length k? treat the blank as a symbol.

exercise 6.25 blast: given a long string a, say of length 109 and a shorter string b,
say 105, how do we    nd a position in a which is the start of a substring b(cid:48) that is close
to b? this problem can be solved by id145 in polynomial time, but    nd
a faster algorithm to solve this problem.
hint: (shingling approach) one possible approach would be to    x a small length, say
seven, and consider the shingles of a and b of length seven. if a close approximation to
b is a substring of a, then a number of shingles of b must be shingles of a. this should
allows us to    nd the approximate location in a of the approximation of b. some    nal
algorithm should then be able to    nd the best match.

207

7 id91

7.1 introduction

id91 refers to partitioning a set of objects into subsets according to some de-
sired criterion. often it is an important step in making sense of large amounts of data.
id91 comes up in many contexts. one might want to partition a set of news articles
into clusters based on the topics of the articles. given a set of pictures of people, one
might want to group them into clusters based on who is in the image. or one might want
to cluster a set of protein sequences according to the protein function. a related problem
is not    nding a full partitioning but rather just identifying natural clusters that exist.
for example, given a collection of friendship relations among people, one might want to
identify any tight-knit groups that exist. in some cases we have a well-de   ned correct
answer, e.g., in id91 photographs of individuals by who is in them, but in other cases
the notion of a good id91 may be more subjective.

before running a id91 algorithm, one    rst needs to choose an appropriate repre-
sentation for the data. one common representation is as vectors in rd. this corresponds
to identifying d real-valued features that are then computed for each data object. for ex-
ample, to represent documents one might use a    bag of words    representation, where each
feature corresponds to a word in the english language and the value of the feature is how
many times that word appears in the document. another common representation is as
vertices in a graph, with edges weighted by some measure of how similar or dissimilar the
two endpoints are. for example, given a set of protein sequences, one might weight edges
based on an edit-distance measure that essentially computes the cost of transforming one
sequence into the other. this measure is typically symmetric and satis   es the triangle
inequality, and so can be thought of as a    nite metric. a point worth noting up front
is that often the    correct    id91 of a given set of data depends on your goals. for
instance, given a set of photographs of individuals, we might want to cluster the images by
who is in them, or we might want to cluster them by facial expression. when representing
the images as points in space or as nodes in a weighted graph, it is important that the
features we use be relevant to the criterion we care about. in any event, the issue of how
best to represent data to highlight the relevant information for a given task is generally
addressed using knowledge of the speci   c domain. from our perspective, the job of the
id91 algorithm begins after the data has been represented in some appropriate way.

in this chapter, our goals are to discuss (a) some commonly used id91 algorithms
and what one can prove about them, and (b) models and assumptions on data under which
we can    nd a id91 close to the correct id91.

7.1.1 preliminaries

we will follow the standard notation of using n to denote the number of data points
and k to denote the number of desired clusters. we will primarily focus on the case that

208

k is known up front, but will also discuss algorithms that produce a sequence of solutions,
one for each value of k, as well as algorithms that produce a cluster tree that can encode
multiple id91s at each value of k. we will generally use a = {a1, . . . , an} to denote
the n data points. we also think of a as a matrix with rows a1, . . . , an.

7.1.2 two general assumptions on the form of clusters

before choosing a id91 algorithm, it is useful to have some general idea of what
a good id91 should look like. in general, there are two types of assumptions often
made that in turn lead to di   erent classes of id91 algorithms.

center-based clusters: one assumption commonly made is that clusters are center-
based. this means that the id91 can be de   ned by k centers c1, . . . , ck, with each
data point assigned to whichever center is closest to it. note that this assumption does
not yet tell whether one choice of centers is better than another. for this, one needs
an objective, or optimization criterion. three standard criteria often used are k-center,
k-median, and id116 id91, de   ned as follows.
k-center id91: find a partition c = {c1, . . . , ck} of a into k clusters, with corre-
sponding centers c1, . . . , ck, to minimize the maximum distance between any data
point and the center of its cluster. that is, we want to minimize

  kcenter(c) =

k

max
j=1

max
ai   cj

d(ai, cj).

k-center id91 makes sense when we believe clusters should be local regions in
space. it is also often thought of as the       rehouse location problem    since one can
think of it as the problem of locating k    re-stations in a city so as to minimize the
maximum distance a    re-truck might need to travel to put out a    re.

k-median id91: find a partition c = {c1, . . . , ck} of a into k clusters, with corre-
sponding centers c1, . . . , ck, to minimize the sum of distances between data points
and the centers of their clusters. that is, we want to minimize

  kmedian(c) =

d(ai, cj).

k(cid:88)

(cid:88)

j=1

ai   cj

k-median id91 is more noise-tolerant than k-center id91 because we are
taking a sum rather than a max. a small number of outliers will typically not
change the optimal solution by much, unless they are very far away or there are
several quite di   erent near-optimal solutions.

id116 id91: find a partition c = {c1, . . . , ck} of a into k clusters, with cor-
responding centers c1, . . . , ck, to minimize the sum of squares of distances between

209

data points and the centers of their clusters. that is, we want to minimize

  kmeans(c) =

d2(ai, cj).

k(cid:88)

(cid:88)

j=1

ai   cj

id116 id91 puts more weight on outliers than k-median id91, because
we are squaring the distances, which magni   es large values. this puts it somewhat
in between k-median and k-center id91 in that regard. using distance squared
has some mathematical advantages over using pure distances when data are points in
rd. for example, corollary 7.2 that asserts that with the distance squared criterion,
the optimal center for a given group of data points is its centroid.

the id116 criterion is more often used when data consists of points in rd, whereas
k-median is more commonly used when we have a    nite metric, that is, data are nodes in
a graph with distances on edges.

when data are points in rd, there are in general two variations of the id91 prob-
lem for each of the criteria. we could require that each cluster center be a data point or
allow a cluster center to be any point in space. if we require each center to be a data

point, the optimal id91 of n data points into k clusters can be solved in time (cid:0)n

k
times a polynomial in the length of the data. first, exhaustively enumerate all sets of k
data points as the possible sets of k cluster centers, then associate each point to its nearest
center and select the best id91. no such naive enumeration procedure is available
when cluster centers can be any point in space. but, for the id116 problem, corol-
lary 7.2 shows that once we have identi   ed the data points that belong to a cluster, the
best choice of cluster center is the centroid of that cluster, which might not be a data point.

(cid:1)

when k is part of the input or may be a function of n, the above optimization prob-
lems are all np-hard.33 so, guarantees on algorithms will typically involve either some
form of approximation or some additional assumptions, or both.

high-density clusters: if we do not believe our desired clusters will be center-based,
an alternative assumption often made is that clusters consist of high-density regions sur-
rounded by low-density    moats    between them. for example, in the id91 of figure
7.1 we have one natural cluster a that looks center-based but the other cluster b consists
of a ring around cluster a. as seen in the    gure, this assumption does not require clus-
ters to correspond to convex regions and it can allow them to be long and stringy. we
describe a non-center-based id91 method in section 7.7. in section 7.9 we prove the
e   ectiveness of an algorithm which    nds a    moat   , cuts up data    inside    the moat and
   outside    into two pieces and recursively applies the same procedure to each piece.

33if k is a constant, then as noted above, the version where the centers must be data points can be

solved in polynomial time.

210

b

a

figure 7.1: example where the natural id91 is not center-based.

7.1.3 spectral id91

an important part of a id91 toolkit when data lies in rd is singular value de-
composition. spectral id91 refers to the following algorithm: find the space v
spanned by the top k right singular vectors of the matrix a whose rows are the data
points. project data points to v and cluster in the projection.

an obvious reason to do this is dimension reduction, id91 in the d dimensional
space where data lies is reduced to id91 in a k dimensional space (usually, k << d).
a more important point is that under certain assumptions one can prove that spectral
id91 gives a id91 close to the true id91. we already saw this in the case
when data is from a mixture of spherical gaussians, section 3.9.3. the assumption used is
   the means separated by a constant number of standard deviations   . in section 7.5, we
will see that in a much more general setting, which includes common stochastic models,
the same assumption, in spirit, yields similar conclusions. section 7.4, has another setting
with a similar result.

7.2 id116 id91

we assume in this section that data points lie in rd and focus on the id116 criterion.

7.2.1 a maximum-likelihood motivation

we now consider a maximum-likelihood motivation for using the id116 criterion.
suppose that the data was generated according to an equal weight mixture of k spherical
well-separated gaussian densities centered at   1,   2, . . . ,   k, each with variance one in
every direction. then the density of the mixture is

prob(x) =

1

(2  )d/2

1
k

e   |x     i|2.

k(cid:88)

i=1

denote by   (x) the center nearest to x. since the exponential function falls o    fast,
assuming x is noticeably closer to its nearest center than to any other center, we can

211

approximate(cid:80)k

thus

i=1 e   |x     i|2 by e   |x     (x)|2 since the sum is dominated by its largest term.

prob(x)    

1

(2  )d/2k

e   |x     (x)|2.

the likelihood of drawing the sample of points x1, x2, . . . , xn from the mixture, if the
centers were   1,   2, . . . ,   k, is approximately

n(cid:89)

1

1
kn

(2  )nd/2

i=1

e   |x(i)     (x(i))|2 = ce   (cid:80)n

i=1 |x(i)     (x(i))|2.

minimizing the sum of squared distances to cluster centers    nds the maximum likelihood
  1,   2, . . . ,   k. this motivates using the sum of distance squared to the cluster centers.

7.2.2 structural properties of the id116 objective

suppose we have already determined the id91 or the partitioning into c1, c2, . . . , ck.

what are the best centers for the clusters? the following lemma shows that the answer
is the centroids, the coordinate means, of the clusters.
lemma 7.1 let {a1, a2, . . . , an} be a set of points. the sum of the squared distances of
the ai to any point x equals the sum of the squared distances to the centroid of the ai plus
n times the squared distance from x to the centroid. that is,

(cid:88)

(cid:88)

|ai     x|2 =

|ai     c|2 + n|c     x|2

i

i

where c = 1
n

n(cid:80)
proof: (cid:88)

i=1

|ai     x|2 =

i

since c is the centroid,(cid:80)
since the    rst term,(cid:80)

i

ai is the centroid of the set of points.

|ai     c + c     x|2

(cid:88)
(cid:88)
(ai     c) = 0. thus,(cid:80)

|ai     c|2 + 2(c     x)   (cid:88)

i

i

i

=

|ai     x|2 =(cid:80)

i

i

(ai     c) + n|c     x|2

|ai     c|2 + n|c     x|2

a corollary of lemma 7.1 is that the centroid minimizes the sum of squared distances
|ai     c|2, is a constant independent of x and setting x = c sets the

i

second term, n(cid:107)c     x(cid:107)2, to zero.
corollary 7.2 let {a1, a2, . . . , an} be a set of points. the sum of squared distances of
the ai to a point x is minimized when x is the centroid, namely x = 1
n

(cid:80)

ai.

i

212

7.2.3 lloyd   s algorithm

corollary 7.2 suggests the following natural strategy for id116 id91, known as
lloyd   s algorithm. lloyd   s algorithm does not necessarily    nd a globally optimal solution
but will    nd a locally-optimal one. an important but unspeci   ed step in the algorithm is
its initialization: how the starting k centers are chosen. we discuss this after discussing
the main algorithm.

lloyd   s algorithm:

start with k centers.

cluster each point with the center nearest to it.

find the centroid of each cluster and replace the set of old centers with the centroids.

repeat the above two steps until the centers converge according to some criterion, such
as the id116 score no longer improving.

this algorithm always converges to a local minimum of the objective. to show conver-
gence, we argue that the sum of the squares of the distances of each point to its cluster
center always improves. each iteration consists of two steps. first, consider the step
that    nds the centroid of each cluster and replaces the old centers with the new centers.
by corollary 7.2, this step improves the sum of internal cluster distances squared. the
second step reclusters by assigning each point to its nearest cluster center, which also
improves the internal cluster distances.

a problem that arises with some implementations of the id116 id91 algorithm
is that one or more of the clusters becomes empty and there is no center from which to
measure distance. a simple case where this occurs is illustrated in the following example.
you might think how you would modify the code to resolve this issue.

example: consider running the id116 id91 algorithm to    nd three clusters on
the following 1-dimension data set: {2,3,7,8} starting with centers {0,5,10}.

0

0

0

1

1

1

2

2

2

3

3

3

4

4

4

5

5

5

6

6

6

7

7

7

8

8

8

9

9

9

10

10

10

the center at 5 ends up with no items and there are only two clusters instead of the
desired three.

213

figure 7.2: a locally-optimal but globally-suboptimal id116 id91.

as noted above, lloyd   s algorithm only    nds a local optimum to the id116 objective
that might not be globally optimal. consider, for example, figure 7.2. here data lies in
three dense clusters in r2: one centered at (0, 1), one centered at (0,   1) and one centered
at (3, 0). if we initialize with one center at (0, 1) and two centers near (3, 0), then the
center at (0, 1) will move to near (0, 0) and capture the points near (0, 1) and (0,   1),
whereas the centers near (3, 0) will just stay there, splitting that cluster.

because the initial centers can substantially in   uence the quality of the result, there
has been signi   cant work on initialization strategies for lloyd   s algorithm. one popular
strategy is called    farthest traversal   . here, we begin by choosing one data point as initial
center c1 (say, randomly), then pick the farthest data point from c1 to use as c2, then
pick the farthest data point from {c1, c2} to use as c3, and so on. these are then used
as the initial centers. notice that this will produce the correct solution in the example in
figure 7.2.

farthest traversal can unfortunately get fooled by a small number of outliers. to ad-
dress this, a smoother, probabilistic variation known as id116++ instead weights data
points based on their distance squared from the previously chosen centers. then it selects
the next center probabilistically according to these weights. this approach has the nice
property that a small number of outliers will not overly in   uence the algorithm so long as
they are not too far away, in which case perhaps they should be their own clusters anyway.

another approach is to run some other approximation algorithm for the id116
problem, and then use its output as the starting point for lloyd   s algorithm. note that
applying lloyd   s algorithm to the output of any other algorithm can only improve its
score. an alternative svd-based method for initialization is described and analyzed in
section 7.5.

214

(0,1) (0,-1) (3,0) 7.2.4 ward   s algorithm

cost(c) =(cid:80)

another popular heuristic for id116 id91 is ward   s algorithm. ward   s algo-
rithm begins with each data point in its own cluster, and then repeatedly merges pairs of
clusters until only k clusters remain. speci   cally, ward   s algorithm merges the two clus-
ters that minimize the immediate increase in id116 cost. that is, for a cluster c, de   ne
ai   c d2(ai, c), where c is the centroid of c. then ward   s algorithm merges
the pair (c, c(cid:48)) minimizing cost(c     c(cid:48))     cost(c)     cost(c(cid:48)). thus, ward   s algorithm
can be viewed as a greedy id116 algorithm.

7.2.5

id116 id91 on the line

one case where the optimal id116 id91 can be found in polynomial time is
when points lie in r1, i.e., on the line. this can be done using id145, as
follows.

first, assume without loss of generality that the data points a1, . . . , an have been
sorted, so a1     a2     . . .     an. now, suppose that for some i     1 we have already
computed the optimal k(cid:48)-means id91 for points a1, . . . , ai for all k(cid:48)     k; note that
this is trivial to do for the base case of i = 1. our goal is to extend this solution to points
a1, . . . , ai+1. to do so, observe that each cluster will contain a consecutive sequence of
data points. so, given k(cid:48), for each j     i + 1, compute the cost of using a single center
for points aj, . . . , ai+1, which is the sum of distances of each of these points to their mean
value. then add to that the cost of the optimal k(cid:48)     1 id91 of points a1, . . . , aj   1
which we computed earlier. store the minimum of these sums, over choices of j, as our
optimal k(cid:48)-means id91 of points a1, . . . , ai+1. this has running time of o(kn) for a
given value of i. so overall our running time is o(kn2).

7.3 k-center id91

in this section, instead of using the id116 id91 criterion, we use the k-center
criterion. recall that the k-center criterion partitions the points into k clusters so as to
minimize the maximum distance of any point to its cluster center. call the maximum dis-
tance of any point to its cluster center the radius of the id91. there is a k-id91
of radius r if and only if there are k spheres, each of radius r, which together cover all
the points. below, we give a simple algorithm to    nd k spheres covering a set of points.
the following lemma shows that this algorithm only needs to use a radius that is at most
twice that of the optimal k-center solution. note that this algorithm is equivalent to the
farthest traversal strategy for initializing lloyd   s algorithm.

the farthest traversal k-id91 algorithm

pick any data point to be the    rst cluster center. at time t, for t = 2, 3, . . . , k,
pick the farthest data point from any existing cluster center; make it the tth cluster
center.

215

theorem 7.3 if there is a k-id91 of radius r
k-id91 with radius at most r.

2, then the above algorithm    nds a

proof: suppose for contradiction that there is some data point x that is distance greater
than r from all centers chosen. this means that each new center chosen was distance
greater than r from all previous centers, because we could always have chosen x. this
implies that we have k +1 data points, namely the centers chosen plus x, that are pairwise
more than distance r apart. clearly, no two such points can belong to the same cluster
in any k-id91 of radius r

2, contradicting the hypothesis.

7.4 finding low-error id91s

in the previous sections we saw algorithms for    nding a local optimum to the id116
id91 objective, for    nding a global optimum to the id116 objective on the line, and
for    nding a factor 2 approximation to the k-center objective. but what about    nding
a id91 that is close to the correct answer, such as the true id91 of proteins
by function or a correct id91 of news articles by topic? for this we need some
assumption about the data and what the correct answer looks like. the next few sections
consider algorithms based on di   erent such assumptions.

7.5 spectral id91
let a be a n  d data matrix with each row a data point and suppose we want to partition
the data points into k clusters. spectral id91 refers to a class of id91 algorithms
which share the following outline:

    find the space v spanned by the top k (right) singular vectors of a.
    project data points into v .
    cluster the projected points.

7.5.1 why project?

the reader may want to read section 3.9.3, which shows the e   cacy of spectral id91
for data stochastically generated from a mixture of spherical gaussians. here, we look at
general data which may not have a stochastic generation model.

we will later describe the last step in more detail. first, lets understand the central
advantage of doing the projection to v . it is simply that for any reasonable (unknown)
id91 of data points, the projection brings data points closer to their cluster centers.
this statement sounds mysterious and likely false, since the assertion is for any rea-
sonable unknown id91. we quantify it in theorem 7.4. first some notation: we
represent a k-id91 by a n   d matrix c (same dimensions as a), where row i of c is

216

figure 7.3: clusters in the full space and their projections

the center of the cluster to which data point i belongs. so, there are only k distinct rows
of c and each other row is a copy of one of these rows. the id116 objective function,
namely, the sum of squares of the distances of data points to their cluster centers is

n(cid:88)

|ai     ci|2 = ||a     c||2
f .

i=1

the projection reduces the sum of distance squares to cluster centers from ||a     c||2
2 in the projection. recall that ||a     c||2 is the spectral norm,
to at most 8k||a     c||2
which is the top singular value of a     c. now, ||a     c||2
t (a) and often,
||a     c||f >>
k||a     c||2 and so the projection substantially reduces the sum of
squared distances to cluster centers.

f = (cid:80)

t   2

   

f

we will see later that in many id91 problems, including models like mixtures of
gaussians and stochastic block models of communities, there is a desired id91 c
where the regions overlap in the whole space, but are separated in the projection. figure
7.3 is a schematic illustration. now we state the theorem and give its surprisingly simple
proof.

theorem 7.4 let a be an n    d matrix with ak the projection of the rows of a to the
subspace of the    rst k right singular vectors of a. for any matrix c of rank less than or
equal to k

||ak     c||2

f     8k||a     c||2
2.

217

ak is a matrix that is close to every c, in the sense ||ak     c||2

2. while
this seems contradictory, another way to state this is that for c far away from ak in
frobenius norm, ||a     c||2 will also be high.
proof: since the rank of (ak     c) is less than or equal to 2k,

f     8k||a    c||2

||ak     c||2

f     2k||ak     c||2

2

and

||ak     c||2     ||ak     a||2 + ||a     c||2     2||a     c||2.

the last inequality follows since ak is the best rank k approximation in spectral norm
(theorem 3.9) and c has rank at most k. the theorem follows.

   
suppose now in the id91 c we would like to    nd, the cluster centers that are pairwise
k||a     c||2) apart. this holds for many id91 problems including data
at least    (
generated by stochastic models. then, it will be easy to see that in the projection,
most data points are a constant factor farther from centers of other clusters than their
own cluster center and this makes it very easy for the following algorithm to    nd the
id91 c modulo a small fraction of errors.

7.5.2 the algorithm
   
n by   (c). in the next section, we give an interpretation of ||a    c||2
denote ||a    c||2/
indicating that   (c) is akin to the standard deviation of id91 c and hence the
notation   (c). we assume for now that   (c) is known to us for the desired id91 c.
this assumption can be removed by essentially doing a binary search.

spectral id91 - the algorithm

1. find the top k right singular vectors of data matrix a and project rows of a to the

space spanned by them to get ak.(cf. section 3.5).

2. select a random row from ak and form a cluster with all rows of ak at distance less

than 6k  (c)/   from it.

3. repeat step 2 k times.

theorem 7.5 if in a k-id91 c, every pair of centers is separated by at least 15k  (c)/  
and every cluster has at least   n points in it, then with id203 at least 1       , spectral
id91    nds a id91 c(cid:48) that di   ers from c on at most   2n points.

proof: let vi denote row i of ak. we    rst show that for most data points, the projection
of data point is within distance 3k  (c)/   of its cluster center. i.e., we show that |m| is
small, where,

m = {i : |vi     ci|     3k  (c)/  }.

218

f =(cid:80)

i |vi     ci|2    (cid:80)

i   m |vi     ci|2     |m| 9k2  2(c)

  2

. so, using theorem 7.4,

now, ||ak     c||2
we get:

.

  2

|m|9k2  2(c)

    ||ak     c||2

f     8kn  2(c) =    |m|     8  2n
9k

(7.1)
call a data point i    good    if i /    m . for any two good data points i and j belong-
ing to the same cluster, since, their projections are within 3k  (c)/   of the center of the
cluster, projections of the two data points are within 6k  (c)/   of each other. on the
other hand, if two good data points i and k are in di   erent clusters, since, the centers
of the two clusters are at least 15k  (c)/   apart, their projections must be greater than
15k  (c)/       6k  (c)/   = 9k  (c)/   apart. so, if we picked a good data point (say point
i) in step 2, the set of good points we put in its cluster is exactly the set of good points
in the same cluster as i. thus, if in each of the k executions of step 2, we picked a good
point, all good points are correctly clustered and since |m|       2n, the theorem would hold.

to complete the proof, we must argue that the id203 of any pick in step 2 being
bad is small. the id203 that the    rst pick in step 2 is bad is at most |m|/n       2/k.
for each subsequent execution of step 2, all the good points in at least one cluster are
remaining candidates. so there are at least (       2)n good points left and so the id203
that we pick a bad point is at most |m|/(        2)n which is at most   /k. the union bound
over the k executions yields the desired result.

7.5.3 means separated by    (1) standard deviations

for id203 distribution on the real line, the mnemonic    means separated by six
standard deviations    su   ces to distinguish di   erent distributions. spectral id91
enables us to do the same thing in higher dimensions provided k     o(1) and six is
replaced by some constant. first we de   ne standard deviation for general not necessarily
stochastically generated data: it is just the maximum over all unit vectors v of the square
root of the mean squared distance of data points from their cluster centers in the direction
v, namely, the standard deviation   (c) of id91 c is de   ned as:

  (c)2 =

1
n

maxv:|v|=1

[(ai     ci)    v]2 =

1
n

maxv:|v|=1|(a     c)v|2 =

||a     c||2
2.

1
n

this coincides with the de   nition of   (c) we made earlier. assuming k     o(1), it is easy
to see that the theorem 7.5 can be restated as

if cluster centers in c are separated by    (  (c)), then the spectral id91
algorithm    nds c(cid:48) which di   ers from c only in a small fraction of data points.

it can be seen that the    means separated by    (1) standard deviations    condition holds
for many stochastic models. we illustrate with two examples here. first, suppose we have
a mixture of k     o(1) spherical gaussians, each with standard deviation one. the data

219

n(cid:88)

i=1

is generated according to this mixture. if the means of the gaussians are    (1) apart,
then the condition - means separated by    (1) standard deviations- is satis   ed and so if
we project to the svd subspace and cluster, we will get (nearly) the correct id91.
this was already discussed in detail in chapter ??.

we discuss a second example. stochastic block models are models of communities.
suppose there are k communities c1, c2, . . . , ck among a population of n people. sup-
pose the id203 of two people in the same community knowing each other is p and
if they are in di   erent communities, the id203 is q, where, q < p.34 we assume the
events that person i knows person j are independent across all i and j.

speci   cally, we are given an n    n data matrix a, where aij = 1 if and only if i and
j know each other. we assume the aij are independent random variables, and use ai to
denote the ith row of a. it is useful to think of a as the adjacency matrix of a graph, such
as the friendship network in facebook. we will also think of the rows ai as data points.
the id91 problem is to classify the data points into the communities they belong to.
in practice, the graph is fairly sparse, i.e., p and q are small, namely, o(1/n) or o(ln n/n).

consider the simple case of two communities with n/2 people in each and with

p =

  
n

q =

  
n

where   ,        o(ln n).

let u and v be the centroids of the data points in community one and community two
respectively; so ui     p for i     c1 and uj     q for j     c2 and vi     q for i     c1 and vj     p
for j     c2. we have

|u     v|2 =

(uj     vj)2     (         )2

n(cid:88)
inter-centroid distance                 

n =

n2

j=1

(         )2

n

.

(7.2)
we need to upper bound ||a     c||2. this is non-trivial since we have to prove a
uniform upper bound on |(a     c)v| for all unit vectors v. fortunately, the subject to
random matrix theory (rmt) already does this for us. rmt tells that

n

.

   
||a     c||2     o   (

   
np) = o   (

  ),

where, the o    hides logarithmic factors. so as long as                     (
means separated by    (1) standard deviations and spectral id91 works.

   
  ), we have the

34more generally, for each pair of communities a and b, there could be a id203 pab that a person
from community a knows a person from community b. but for the discussion here, we take paa = p for
all a and pab = q, for all a (cid:54)= b.

220

one important observation is that in these examples as well as many others, the k-
means objective function in the whole space is too high and so the projection is essential
before we can cluster.

7.5.4 laplacians

an important special case of spectral id91 is when k = 2 and a spectral algorithm

is applied to the laplacian matrix l of a graph, which is de   ned as

l = d     a

where a is the adjacency matrix and d is a diagonal matrix of degrees. since a has a
negative sign, we look at the lowest two singular values and corresponding vectors rather
than the highest,

l is a symmetric matrix and is easily seen to be posiitve semi-de   nite: for any vector

x, we have

xt lx =

(cid:88)

i

i     (cid:88)

(i,j)   e

diix2

(cid:88)

(i,j)   e

xixj =

1
2

(xi     xj)2.

also since all row sums of l (and l is symmetric) are zero, its lowest eignvalue is 0 with
the eigenvector 1 of all 1   s. this is also the lowest singular vector of l. the projection
of all data points (rows) to this vector is just the origin and so gives no information. if
we take the second lowest singular vector and project to it which is essentially projecting
to the space of the bottom two singular vectors, we get the very simple problem of n real
numbers which we need to cluster into two clusters.

7.5.5 local spectral id91

so far our emphasis has been on partitioning the data into disjoint clusters. however,
the structure of many data sets consists of over lapping communities. in this case using
id116 with spectral id91, the overlap of two communities shows up as a commu-
nity. this is illustrated in figure 7.4.

(cid:19)

(cid:18) x

an alternative to using id116 with spectral id91 is to    nd the minimum 1-
norm vector in the space spanned by the top singular vectors. let a be the matrix whose
columns are the singular vectors. to    nd a vector y in the space spanned by the columns
of a solve the linear system ax = y. this is a slightly di   erent looking problem then
ax = c where c is a constant vector. to convert ax = y to the more usual form write it
as [a,   i]
= 0. however, if we want to minimize ||y||1 the solution is x = y = 0.
thus we add the row 1, 1, . . . 1, 0, 0, . . . 0 to [a,   i] and a 1 to the top of the vector [x, y]
to force the coordinates of x to add up to one. minimizing ||y||1 does not appear to be a
linear program but we can write y = ya     yb and require ya     0 and yb     0. now    nding

y

221

the minimum one norm vector in the span of the columns of a is the linear program

(cid:32)(cid:88)

(cid:33)

(cid:88)

min

yai +

ybi

i

i

subject to (a,   i, i)

       = 0 ya     0 yb     0.

       x

ya
yb

local communities
in large social networks with a billion vertices, global id91 is likely to result in
communities of several hundred million vertices. what you may actually want is a local
community containing several individuals with only 50 vertices. to do this if one starts a
random walk at a vertex v and computes the frequency of visiting vertices, it will converge
to the    rst singular vector. however, the distribution after a small number of steps will
be primarily in the small community containing v and will be proportional to the    rst
singular vector distribution restricted to the vertices in the small community containing v,
only will be higher by some constant value. if one wants to determine the local communi-
ties containing vertices v1, v2, and v3, start with three id203 distributions, one with
id203 one at v1, one with id203 one at v2, and one with id203 one at v3
and    nd early approximation to the    rst three singular vectors. then    nd the minimum
1-norm vector in the space spanned by the early approximations.

hidden structure
in the previous section we discussed overlapping communities. another issue is hidden
structure. suppose the vertices of a social network could be partitioned into a number of
strongly connected communities. by strongly connected we mean the id203 of an
edge between two vertices in a community is much higher than the id203 of an edge
between two vertices in di   erent communities. suppose the vertices of the graph could
be partitioned in another way which was incoherent35 with the    rst partitioning and the
id203 of an edge between two vertices in one of these communities is higher than an
edge between two vertices in di   erent communities. if the id203 of an edge between
two vertices in a community of this second partitioning is less than that in the    rst, then
a id91 algorithm is likely to    nd the    rst partitioning rather than the second. how-
ever, the second partitioning, which we refer to as hidden structure, may be the structure
that we want to    nd. the way to do this is to use your favorite id91 algorithm to
produce the dominant structure and then stochastically weaken the dominant structure
by removing some community edges in the graph. now if you apply the id91 algo-
rithm to the modi   ed graph, it can    nd the hidden community structure. having done
this, go back to the original graph, weaken the hidden structure and reapply the cluster-
ing algorithm. it will now    nd a better approximation to the dominant structure. this
technology can be used to    nd a number of hidden levels in several types of social networks.

35incoherent, give de   nition

222

                                          

1 1 1 1 1 1 0 0 0
1 1 1 1 1 1 0 0 0
1 1 1 1 1 1 0 0 0
1 1 1 1 1 1 0 0 0
1 1 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1 1
0 0 0 0 1 1 0 0 0
0 0 0 0 1 1 0 0 0
0 0 0 0 1 1 0 0 0

(a)

                                          

                                          

                                          

0.33
0.31
0.33
0.31
0.33
0.31
0.33
0.31
0.44    0.09
0.44    0.09
0.24    0.49
0.24    0.49
0.24    0.49

(b)

(c);

figure 7.4: (a) illustrates the adjacency matrix of a graph with a six vertex clique that
overlaps a    ve vertex clique in two vertices. (b) illustrates the matrix where columns
consist of the top two singular vectors, and (c) illustrates the mapping of rows in the
singular vector matrix to three points in two dimensional space. instead of two cliques
we get the non overlapping portion of each of the two clique plus their intersection as
communities instead of the two cliques as communities.

block model

one technique for generating graphs with communities is to use the block model where
the vertices are partitioned into blocks and each block is a gnp graph generated with
some edge id203. the edges in o    diagonal blocks are generated with a lower prob-
ability. one can also generate graphs with hidden structure. for example, the vertices in
an n vertex graph might be partitioned into two communities, the    rst community having
vertices 1 to n/2 and the second community having vertices n/2 + 1 to n. the dominant
structure is generated with id203 p1 for edges within communities and id203 q1
for edges between communities. the the vertices are randomly permuted and the hidden
structure is generated using the    rst n/2 vertices in the permuted order for one commu-
nity and the remaining vertices for the second community with probabilities p2 and q2
which ar lower than p1 and q1..

an interesting question is how to determine the quality of a community found. many
researchers use an existing standard of what the communities are. however, if you want
to using id91 techniques to    nd communities there probably is no external standard
or you would just use that instead of id91. a way to determine if you have found a
real community structure is to ask if the graph is more likely generated by a model of the
structure found than by a completely random model. suppose you found a partition of
two communities each with n/2 vertices. using the number of edges in each community
and the number of inter community edges ask what is the id203 of the graph being
generated by a bloc model where p and q are the probabilities determined by the edge
density within communities and the edge density between communities. one can compare
this id203 with the id203 that the graph was generated by a gnp model with
id203 (p + q)/2.

223

7.6 approximation stability

7.6.1 the conceptual idea

we now consider another condition that will allow us to produce accurate clusters
from data. to think about this condition, imagine that we are given a few thousand news
articles that we want to cluster by topic. these articles could be represented as points in
a high-dimensional space (e.g., axes could correspond to di   erent meaningful words, with
coordinate i indicating the frequency of that word in a given article). or, alternatively, it
could be that we have developed some text-processing program that given two articles x
and y computes some measure of distance d(x, y) between them. we assume there exists
some correct id91 ct of our news articles into k topics; of course, we do not know
what ct is, that is what we want our algorithm to    nd.

if we are id91 with an algorithm that aims to minimize the id116 score of its
solution, then implicitly this means we believe that the id91 cop t
kmeans of minimum k-
means score is either equal to, or very similar to, the id91 ct . unfortunately,    nding
the id91 of minimum id116 score is np-hard. so, let us broaden our belief a bit
and assume that any id91 c whose id116 score is within 10% of the minimum
is also very similar to ct . this should give us a little bit more slack. unfortunately,
   nding a id91 of score within 10% of the minimum is also an np-hard problem.
nonetheless, we will be able to use this assumption to e   ciently    nd a id91 that is
close to ct . the trick is that np-hardness is a worst-case notion, whereas in contrast,
this assumption implies structure on our data.in particular, it implies that all id91s
that have score within 10% of the minimum have to be similar to each other. we will
then be able to utilize this structure in a natural    ball-growing    id91 algorithm.

7.6.2 making this formal

1, . . . , c(cid:48)

to make this discussion formal, we    rst specify what we mean when we say that two
di   erent ways of id91 some data are    similar    to each other. let c = {c1, . . . , ck}
and c(cid:48) = {c(cid:48)
k} be two di   erent k-id91s of some dataset a. for example, c
could be the id91 that our algorithm produces, and c(cid:48) could be the id91 ct .
let us de   ne the distance between these two id91s to be the fraction of points that
would have to be re-clustered in c to make c match c(cid:48), where by    match    we mean that
there should be a bijection between the clusters of c and the clusters of c(cid:48). we can write
this distance mathematically as:

dist(c,c(cid:48)) = min

  

1
n

|ci \ c(cid:48)

  (i)|,

k(cid:88)

i=1

where the minimum is over all permutations    of {1, . . . , k}.

for c > 1 and   > 0 we say that a data set satis   es (c,  )-approximation-stability
with respect to a given objective (such as id116 or k-median) if every id91 c

224

whose cost is within a factor c of the minimum-cost id91 for that objective satis   es
dist(c,ct ) <  . that is, it is su   cient to be within a factor c of optimal to the our
objective in order for the fraction of points clustered incorrectly to be less than  . we will
speci   cally focus in this discussion on the k-median objective rather than the id116
objective, since it is a bit easier to work with.

what we will now show is that under this condition, even though it may be np-hard
in general to    nd a id91 that is within a factor c of optimal, we can nonetheless
e   ciently    nd a id91 c(cid:48) such that dist(c(cid:48),ct )      , so long as all clusters in ct are
reasonably large. to simplify notation, let c    denote the id91 of minimum k-median
cost, and to keep the discussion simpler, let us also assume that ct = c   ; that is, the
target id91 is also the id91 with the minimum k-median score.

7.6.3 algorithm and analysis

of its cluster in c   . notice that the k-median cost of c    is op t =(cid:80)n

before presenting an algorithm, we begin with a helpful lemma that will guide our
design. for a given data point ai, de   ne its weight w(ai) to be its distance to the center
i=1 w(ai). de   ne
wavg = op t /n to be the average weight of the points in a. finally, de   ne w2(ai) to be
the distance of ai to its second-closest center in c   .

lemma 7.6 assume dataset a satis   es (c,  ) approximation-stability with respect to the
k-median objective, each cluster in ct has size at least 2 n, and ct = c   . then,

1. fewer than  n points ai have w2(ai)     w(ai)     (c     1)wavg/ .
2. at most 5 n/(c     1) points ai have w(ai)     (c     1)wavg/(5 ).

proof: for part (1), suppose that  n points ai have w2(ai)     w(ai)     (c     1)wavg/ .
consider modifying ct to a new id91 c(cid:48) by moving each of these points ai into
the cluster containing its second-closest center. by assumption, the id116 cost of the
id91 has increased by at most  n(c     1)wavg/  = (c     1)    op t. this means that
the cost of the new id91 is at most c   op t . however, dist(c(cid:48),ct ) =   because (a) we
moved  n points to di   erent clusters, and (b) each cluster in ct has size at least 2 n so the
optimal permutation    in the de   nition of dist remains the identity. so, this contradicts
approximation stability. part (2) follows from the de   nition of    average   ; if it did not

hold then(cid:80)n

i=1 w(ai) > nwavg, a contradiction.

a datapoint ai is bad if it satis   es either item (1) or (2) of lemma 7.6 and good if it
satis   es neither one. so, there are at most b =  n + 5 n
c   1 bad points and the rest are good.
de   ne    critical distance    dcrit = (c   1)wavg
. lemma 7.6 implies that the good points have
distance at most dcrit to the center of their own cluster in c    and distance at least 5dcrit
to the center of any other cluster in c   .

5 

225

this suggests the following algorithm. suppose we create a graph g with the points
ai as vertices, and edges between any two points ai and aj with d(ai, aj) < 2dcrit. notice
that by triangle inequality, the good points within the same cluster in c    have distance
less than 2dcrit from each other so they will be fully connected and form a clique. also,
again by triangle inequality, any edge that goes between di   erent clusters must be be-
tween two bad points. in particular, if ai is a good point in one cluster, and it has an edge
to some other point aj, then aj must have distance less than 3dcrit to the center of ai   s
cluster. this means that if aj had a di   erent closest center, which obviously would also
be at distance less than 3dcrit, then ai would have distance less than 2dcrit + 3dcrit = 5dcrit
to that center, violating its goodness. so, bridges in g between di   erent clusters can only
occur between bad points.

assume now that each cluster in ct has size at least 2b+1; this is the sense in which we
are requiring that  n be small compared to the smallest cluster in ct . in this case, create
a new graph h by connecting any two points ai and aj that share at least b + 1 neighbors
in common in g, themselves included. since every cluster has at least 2b + 1     b = b + 1
good points, and these points are fully connected in g, this means that h will contain an
edge between every pair of good points in the same cluster. on the other hand, since the
only edges in g between di   erent clusters are between bad points, and there are at most
b bad points, this means that h will not have any edges between di   erent clusters in ct .
thus, if we take the k largest connected components in h, these will all correspond to
subsets of di   erent clusters in ct , with at most b points remaining.

at this point we have a correct id91 of all but at most b points in a. call these
clusters c1, . . . , ck, where cj     c   
j . to cluster the remaining points ai, we assign them
to the cluster cj that minimizes the median distance between ai and points in cj. since
each cj has more good points than bad points, and each good point in cj has distance at
most dcrit to center c   
j , by triangle inequality the median of these distances must lie in the
range [d(ai, c   
i ) + dcrit]. this means that this second step will correctly
cluster all points ai for which w2(ai)     w(ai) > 2dcrit. in particular, we correctly cluster
all points except possibly for some of the at most  n satisfying item (1) of lemma 7.6.

i )     dcrit, d(ai, c   

the above discussion assumes the value dcrit is known to our algorithm; we leave it as
an exercise to the reader to modify the algorithm to remove this assumption. summariz-
ing, we have the following algorithm and theorem.

algorithm k-median stability (given c,  , dcrit)

1. create a graph g with a vertex for each datapoint in a, and an edge between

vertices i and j if d(ai, aj)     2dcrit.

2. create a graph h with a vertex for each vertex in g and an edge between vertices i
and j if i and j share at least b + 1 neighbors in common, themselves included, for
b =  n + 5 n

c   1. let c1, . . . , ck denote the k largest connected components in h.

226

3. assign each point not in c1     . . .    ck to the cluster cj of smallest median distance.

theorem 7.7 assume a satis   es (c,  ) approximation-stability with respect to the k-
median objective, that each cluster in ct has size at least 10 
then algorithm k-median stability will    nd a id91 c such that dist(c,ct )      .

c   1n+2 n+1, and that ct = c   .

7.7 high-density clusters

we now turn from the assumption that clusters are center-based to the assumption
that clusters consist of high-density regions, separated by low-density moats such as in
figure 7.1.

7.7.1 single linkage

one natural algorithm for id91 under the high-density assumption is called single
linkage. this algorithm begins with each point in its own cluster and then repeatedly
merges the two    closest    clusters into one, where the distance between two clusters is
de   ned as the minimum distance between points in each cluster. that is, dmin(c, c(cid:48)) =
minx   c,y   c(cid:48) d(x, y), and the algorithm merges the two clusters c and c(cid:48) whose dmin value
is smallest over all pairs of clusters breaking ties arbitrarily. it then continues until there
are only k clusters. this is called an agglomerative id91 algorithm because it begins
with many clusters and then starts merging, or agglomerating them together.36 single-
linkage is equivalent to running kruskal   s minimum-spanning-tree algorithm, but halting
when there are k trees remaining. the following theorem is fairly immediate.
theorem 7.8 suppose the desired id91 c   
exists some distance    such that

k satis   es the property that there

1 , . . . , c   

1. any two data points in di   erent clusters have distance at least   , and
2. for any cluster c   

i and any partition of c   

i into two non-empty sets a and c   

i \ a,

there exist points on each side of the partition of distance less than   .

then, single-linkage will correctly recover the id91 c   
proof: consider running the algorithm until all pairs of clusters c and c(cid:48) have dmin(c, c(cid:48))       .
at that point, by (2), each target cluster c   
i will be fully contained within some cluster
of the single-linkage algorithm. on the other hand, by (1) and by induction, each cluster
c of the single-linkage algorithm will be fully contained within some c   
i of the target
id91, since any merger of subsets of distinct target clusters would require dmin       .
therefore, the single-linkage clusters are indeed the target clusters.

1 , . . . , c   
k .

36other agglomerative algorithms include complete linkage which merges the two clusters whose max-
imum distance between points is smallest, and ward   s algorithm described earlier that merges the two
clusters that cause the id116 cost to increase by the least.

227

7.7.2 robust linkage

the single-linkage algorithm is fairly brittle. a few points bridging the gap between
two di   erent clusters can cause it to do the wrong thing. as a result, there has been
signi   cant work developing more robust versions of the algorithm.

one commonly used robust version of single linkage is wishart   s algorithm. a ball
of radius r is created for each point with the point as center. the radius r is gradually
increased starting from r = 0. the algorithm has a parameter t. when a ball has t or
more points the center point becomes active. when two balls with active centers intersect
the two center points are connected by an edge. the parameter t prevents a thin string
of points between two clusters from causing a spurious merger. note that wishart   s al-
gorithm with t = 1 is the same as single linkage.

in fact, if one slightly modi   es the algorithm to de   ne a point to be live if its ball
of radius r/2 contains at least t points, then it is known [cd10] that a value of t =
o(d log n) is su   cient to recover a nearly correct solution under a natural distributional
formulation of the id91 problem. speci   cally, suppose data points are drawn from
some id203 distribution d over rd, and that the clusters correspond to high-density
regions surrounded by lower-density moats. more speci   cally, the assumption is that

1. for some distance    > 0, the   -interior of each target cluster c   

i has density at least
some quantity    (the   -interior is the set of all points at distance at least    from
the boundary of the cluster),

2. the region between target clusters has density less than   (1      ) for some   > 0,

3. the clusters should be separated by distance greater than 2  , and

4. the   -interior of the clusters contains most of their id203 mass.

then, for su   ciently large n, the algorithm will with high id203    nd nearly correct
clusters. in this formulation, we allow points in low-density regions that are not in any
target clusters at all. for details, see [cd10].

robust median neighborhood linkage robusti   es single linkage in a di   erent way.
this algorithm guarantees that if it is possible to delete a small fraction of the data such
that for all remaining points x, most of their |c   (x)| nearest neighbors indeed belong to
their own cluster c   (x), then the hierarchy on clusters produced by the algorithm will
include a close approximation to the true id91. we refer the reader to [blg14] for
the algorithm and proof.

7.8 kernel methods

kernel methods combine aspects of both center-based and density-based id91.
in center-based approaches like id116 or k-center, once the cluster centers are    xed, the

228

voronoi diagram of the cluster centers determines which cluster each data point belongs
to. this implies that clusters are pairwise linearly separable.

if we believe that the true desired clusters may not be linearly separable, and yet we
wish to use a center-based method, then one approach, as in the chapter on learning,
is to use a kernel. recall that a id81 k(x, y) can be viewed as performing
an implicit mapping    of the data into a possibly much higher dimensional space, and
then taking a dot-product in that space. that is, k(x, y) =   (x)      (y). this is then
viewed as the a   nity between points x and y. we can extract distances in this new
space using the equation |z1     z2|2 = z1    z1 + z2    z2     2z1    z2, so in particular we have
|  (x)      (y)|2 = k(x, x) + k(y, y)    2k(x, y). we can then run a center-based id91
algorithm on these new distances.

one popular id81 to use is the gaussian kernel. the gaussian kernel uses
an a   nity measure that emphasizes closeness of points and drops o    exponentially as the
points get farther apart. speci   cally, we de   ne the a   nity between points x and y by

k(x, y) = e

2  2 (cid:107)x   y(cid:107)2
    1

.

another way to use a   nities is to put them in an a   nity matrix, or weighted graph.
this graph can then be separated into clusters using a graph partitioning procedure such
as the one in following section.

7.9 recursive id91 based on sparse cuts

we now consider the case that data are nodes in an undirected connected graph
g(v, e) where an edge indicates that the end point vertices are similar. recursive clus-
tering starts with all vertices in one cluster and recursively splits a cluster into two parts
whenever there is a small number of edges from one part to the other part of the cluster.
formally, for two disjoint sets s and t of vertices, de   ne

  (s, t ) =

number of edges from s to t

.

total number of edges incident to s in g

degree of vertex i and for a subset s of vertices, let d(s) =(cid:80)

  (s, t ) measures the relative strength of similarities between s and t . let d(i) be the
i   s d(i). let m be the total
number of edges in the graph. the following algorithm aims to cut only a small fraction
of the edges and to produce clusters that are internally consistent in that no subset of the
cluster has low similarity to the rest of the cluster.

recursive id91: select an appropriate value for  . if a current cluster
w has a subset s with d(s)     1
into two clusters s and w \ s. repeat until no such split is possible.

2d(w ) and   (s, s     w )       , then split w

theorem 7.9 at termination of recursive id91, the total number of edges between
vertices in di   erent clusters is at most o(  m ln n).

229

proof: each edge between two di   erent clusters at the end was deleted at some stage
by the algorithm. we will    charge    edge deletes to vertices and bound the total charge.
when the algorithm partitions a cluster w into s and w \s with d(s)     (1/2)d(w ), each
k     s is charged d(k)
d(w ) times the number of edges being deleted. since   (s, w \ s)       ,
the charge added to each k     w is a most   d(k). a vertex is charged only when it is
in the smaller part, d(s)     d(w )/2, of the cut. so between any two times it is charged,
d(w ) is reduced by a factor of at least two and so a vertex can be charged at most
log2 m     o(ln n) times, proving the theorem.

implementing the algorithm requires computing mins   w   (s, w \ s) which is an np-
hard problem. so the theorem cannot be implemented right away. luckily, eigenvalues and
eigenvectors, which can be computed fast, give an approximate answer. the connection
between eigenvalues and sparsity, known as cheeger   s inequality, is deep with applications
to markov chains among others. we do not discuss this here.

7.10 dense submatrices and communities

represent n data points in d-space by the rows of an n    d matrix a. assume that a
has all nonnegative entries. examples to keep in mind for this section are the document-
term matrix and the customer-product matrix. we address the question of how to de   ne
and    nd e   ciently a coherent large subset of rows. to this end, the matrix a can be
represented by a bipartite graph figure 7.5. one side has a vertex for each row and the
other side a vertex for each column. between the vertex for row i and the vertex for
column j, there is an edge with weight aij.

we want a subset s of row vertices and a subset t of column vertices so that

(cid:88)

a(s, t ) =

aij

i   s,j   t

is high. this simple de   nition is not good since a(s, t ) will be maximized by taking
all rows and columns. we need a balancing criterion that ensures that a(s, t ) is high
relative to the sizes of s and t . one possibility is to maximize a(s,t )
|s||t| . this is not a good
measure either, since it is maximized by the single edge of highest weight. the de   nition
we use is the following. let a be a matrix with nonnegative entries. for a subset s of
rows and a subset t of columns, the density d(s, t ) of s and t is d(s, t ) = a(s,t )   
|s||t| . the
density d(a) of a is de   ned as the maximum value of d(s, t ) over all subsets of rows and
columns. this de   nition applies to bipartite as well as non bipartite graphs.

one important case is when a   s rows and columns both represent the same set and
. if a is an n    n
aij is the similarity between object i and object j. here d(s, s) = a(s,s)
0-1 matrix, it can be thought of as the adjacency matrix of an undirected graph, and
d(s, s) is the average degree of a vertex in s. the subgraph of maximum average degree
in a graph can be found exactly by network    ow techniques, as we will show in the next

|s|

230

figure 7.5: example of a bipartite graph.

section. we do not know an e   cient (polynomial-time) algorithm for    nding d(a) exactly
in general. however, we show that d(a) is within a o(log2 n) factor of the top singular
value of a assuming |aij|     1 for all i and j. this is a theoretical result. the gap may be
much less than o(log2 n) for many problems, making singular values and singular vectors
quite useful. also, s and t with d(s, t )        (d(a)/ log2 n) can be found algorithmically.
theorem 7.10 let a be an n    d matrix with entries between 0 and 1. then

  1(a)     d(a)       1(a)

.

4 log n log d
furthermore, subsets s and t satisfying d(s, t )       1(a)
singular vector of a.

4 log n log d may be found from the top

proof: let s and t be the subsets of rows and columns that achieve d(a) = d(s, t ).
1   
consider an n-vector u that is
|t|

1   
|s| on s and 0 elsewhere and a d-vector v that is

on t and 0 elsewhere. then,

  1 (a)     ut av =(cid:80)

establishing the    rst inequality.

ij

uivjaij = d(s, t ) = d(a)

to prove the second inequality, express   1 (a) in terms of the    rst left and right

singular vectors x and y.

  1(a) = xt ay =

(cid:88)

xiaijyj,

|x| = |y| = 1.

i,j

(cid:80)

i,j

since the entries of a are nonnegative, the components of the    rst left and right singular
vectors must all be nonnegative, that is, xi     0 and yj     0 for all i and j. to bound
xiaijyj, break the summation into o (log n log d) parts. each part corresponds to a
given    and    and consists of all i such that        xi < 2   and all j such that        yi < 2  .
the log n log d parts are de   ned by breaking the rows into log n blocks with    equal to
1
n , . . . , 1 and by breaking the columns into log d blocks with    equal
2

1   
n , 2 1   

n , 4 1   

1   
n ,

231

,

1   
d

1   
d

to 1
2
be ignored at a loss of at most 1

   
, . . . , 1. the i such that xi < 1
2

will
4  1(a). exercise 7.27 proves the loss is at most this amount.

   
n and the j such that yj < 1
2

2   
d

4   
d

,

,

d

i = 1, the set s = {i|       xi < 2  } has |s|     1
x2

  2 and similarly,

since(cid:80)

t = {j|       yj     2  } has |t|     1

i

(cid:88)

  2 . thus

(cid:88)

i

     xi   2  

j

     yj   2  

xiyjaij     4    a(s, t )

    4    d(s, t )(cid:112)|s||t|

    4d(s, t )
    4d(a).

from this it follows that

or

proving the second inequality.

  1 (a)     4d (a) log n log d

d (a)       1(a)

4 log n log d

it is clear that for each of the values of (  ,   ), we can compute a(s, t ) and d(s, t )
as above and taking the best of these d(s, t )    s gives us an algorithm as claimed in the
theorem.

note that in many cases, the nonzero values of xi and yj after zeroing out the low
entries will only go from 1
for yj, since the singular vectors
2
are likely to be balanced given that aij are all between 0 and 1. in this case, there will
be o(1) groups only and the log factors disappear.

n for xi and 1

n to c   
1   

to c   

1   
d

d

2

another measure of density is based on similarities. recall that the similarity between
objects represented by vectors (rows of a) is de   ned by their dot products. thus, simi-
larities are entries of the matrix aat . de   ne the average cohesion f (s) of a set s of rows
of a to be the sum of all pairwise dot products of rows in s divided by |s|. the average
cohesion of a is the maximum over all subsets of rows of the average cohesion of the subset.

since the singular values of aat are squares of singular values of a, we expect f (a)

to be related to   1(a)2 and d(a)2. indeed it is. we state the following without proof.
lemma 7.11 d(a)2     f (a)     d(a) log n. also,   1(a)2     f (a)     c  1(a)2
log n .

f (a) can be found exactly using    ow techniques as we will see later.

232

7.11 community finding and graph partitioning

assume that data are nodes in a possibly weighted graph where edges represent some
notion of a   nity between their endpoints. in particular, let g = (v, e) be a weighted
graph. given two sets of nodes s and t , de   ne

e(s, t ) =

eij.

(cid:88)

i   s
j   t

we then de   ne the density of a set s to be

d(s, s) =

e(s, s)

|s|

.

if g is an undirected graph, then d(s, s) can be viewed as the average degree in the
vertex-induced subgraph over s. the set s of maximum density is therefore the subgraph
of maximum average degree. finding such a set can be viewed as    nding a tight-knit
community inside some network. in the next section, we describe an algorithm for    nding
such a set using network    ow techniques.

flow methods

here we consider dense induced subgraphs of a graph. an
induced subgraph of a graph consisting of a subset of the vertices of the graph along with
all edges of the graph that connect pairs of vertices in the subset of vertices. we show
that    nding an induced subgraph with maximum average degree can be done by network
   ow techniques. this is simply maximizing the density d(s, s) over all subsets s of the
graph. first consider the problem of    nding a subset of vertices such that the induced
subgraph has average degree at least    for some parameter   . then do a binary search
on the value of    until the maximum    for which there exists a subgraph with average
degree at least    is found.

given a graph g in which one wants to    nd a dense subgraph, construct a directed
graph h from the given graph and then carry out a    ow computation on h. h has a
node for each edge of the original graph, a node for each vertex of the original graph,
plus two additional nodes s and t. there is a directed edge with capacity one from s to
each node corresponding to an edge of the original graph and a directed edge with in   nite
capacity from each node corresponding to an edge of the original graph to the two nodes
corresponding to the vertices the edge connects. finally, there is a directed edge with
capacity    from each node corresponding to a vertex of the original graph to t.

notice there are three types of cut sets of the directed graph that have    nite capacity,
figure 7.6. the    rst cuts all arcs from the source. it has capacity e, the number of edges
of the original graph. the second cuts all edges into the sink. it has capacity   v, where v
is the number of vertices of the original graph. the third cuts some arcs from s and some
arcs into t. it partitions the set of vertices and the set of edges of the original graph into
two blocks. the    rst block contains the source node s, a subset of the edges es, and a

233

1
1

1

s

   
   
   
   

   
   

[u,v]

[v,w]

[w,x]

edges

u

v

w

x

vertices

  

  

  

  

t

figure 7.6: the directed graph h used by the    ow technique to    nd a dense subgraph

subset of the vertices vs de   ned by the subset of edges. the    rst block must contain both
end points of each edge in es; otherwise an in   nite arc will be in the cut. the second block
contains t and the remaining edges and vertices. the edges in this second block either
connect vertices in the second block or have one endpoint in each block. the cut set will
cut some in   nite arcs from edges not in es coming into vertices in vs. however, these
arcs are directed from nodes in the block containing t to nodes in the block containing s.
note that any    nite capacity cut that leaves an edge node connected to s must cut the
two related vertex nodes from t, figure 7.6. thus, there is a cut of capacity e     es +   vs
where vs and es are the vertices and edges of a subgraph. for this cut to be the minimal
cut, the quantity e     es +   vs must be minimal over all subsets of vertices of the original
graph and the capcity must be less than e and also less than   v.

> e

if there is a subgraph with vs vertices and es edges where the ratio es
vs

is su   ciently
v , es       vs > 0 and e     es +   vs < e.
large so that es
similarly e <   v and thus e     es +   vs <   v. this implies that the cut e     es +   vs is less
vs
than either e or   v and the    ow algorithm will    nd a nontrivial cut and hence a proper
subset. for di   erent values of    in the above range there maybe di   erent nontrivial cuts.

v , then for    such that es
vs

>    > e

note that for a given density of edges, the number of edges grows as the square of the
v if vs is small. thus, the    ow method
. to    nd small communities one

number of vertices and es
vs
works well in    nding large subsets since it works with es
vs
would need to use a method that worked with es
v2
s

as the following example illustrates.

is less likely to exceed e

example: consider    nding a dense subgraph of 1,000 vertices and 2,000 internal edges in
a graph of 106 vertices and 6  106 edges. for concreteness, assume the graph was generated
by the following process. first, a 1,000-vertex graph with 2,000 edges was generated as a

234

   

   

  

t

s

1

   

cut

edges and vertices
in the community

figure 7.7: cut in    ow graph

random regular degree four graph. the 1,000-vertex graph was then augmented to have
106 vertices and edges were added at random until all vertices were of degree 12. note
that each vertex among the    rst 1,000 has four edges to other vertices among the    rst
1,000 and eight edges to other vertices. the graph on the 1,000 vertices is much denser
than the whole graph in some sense. although the subgraph induced by the 1,000 vertices
has four edges per vertex and the full graph has twelve edges per vertex, the id203
of two vertices of the 1,000 being connected by an edge is much higher than for the graph
as a whole. the id203 is given by the ratio of the actual number of edges connecting
vertices among the 1,000 to the number of possible edges if the vertices formed a complete
graph.

e(cid:0)(cid:0)v
(cid:1)(cid:1) =

2

p =

2e

v(v     1)

for the 1,000 vertices, this number is p = 2  2,000
1,000  999
number is p = 2  6  106
connected should allow us to    nd the dense subgraph.

   = 4    10   3. for the entire graph this
106  106 = 12    10   6. this di   erence in id203 of two vertices being

in our example, the cut of all arcs out of s is of capacity 6    106, the total number
of edges in the graph, and the cut of all arcs into t is of capacity    times the number
of vertices or       106. a cut separating the 1,000 vertices and 2,000 edges would have
capacity 6    106     2, 000 +       1, 000. this cut cannot be the minimum cut for any value
of    since es
v . the point is that to    nd the 1,000 vertices, we
have to maximize a(s, s)/|s|2 rather than a(s, s)/|s|. note that a(s, s)/|s|2 penalizes
vs
large |s| much more and therefore can    nd the 1,000 node    dense    subgraph.

v = 6, hence es
vs

= 2 and e

< e

235

                  

a =

                   v =

                  

                  

1 0
1 0
1 0
0 1
0 1

1 1 1 0 0
1 1 1 0 0
1 1 1 0 0
0 0 0 1 1
0 0 0 1 1

figure 7.8: illustration of spectral id91.

7.12 spectral id91 applied to social networks

finding communities in social networks is di   erent from other id91 for several
reasons. first we often want to    nd communities of size say 20 to 50 in networks with
100 million vertices. second a person is in a number of overlapping communities and thus
we are not    nding disjoint clusters. third there often are a number of levels of structure
and a set of dominant communities may be hiding a set of weaker communities that are
of interest. spectral id91 is one approach to these issues.

in spectral id91 of the vertices of a graph, one creates a matrix v whose columns
correspond to the    rst k singular vectors of the adjacency matrix. each row of v is
the projection of a row of the adjacency matrix to the space spanned by the k singular
vectors. in the example below, the graph has    ve vertices divided into two cliques, one
consisting of the    rst three vertices and the other the last two vertices. the top two right
singular vectors of the adjacency matrix, not normalized to length one, are (1, 1, 1, 0, 0)t
and (0, 0, 0, 1, 1)t . the    ve rows of the adjacency matrix projected to these vectors form
the 5    2 matrix in figure 7.8. here, there are two ideal clusters with all edges inside a
cluster being present including self-loops and all edges between clusters being absent. the
   ve rows project to just two points, depending on which cluster the rows are in. if the
clusters were not so ideal and instead of the graph consisting of two disconnected cliques,
the graph consisted of two dense subsets of vertices where the two sets were connected by
only a few edges, then the singular vectors would not be indicator vectors for the clusters
but close to indicator vectors. the rows would be mapped to two clusters of points instead
of two points. a id116 id91 algorithm would    nd the clusters.

if the clusters were overlapping, then instead of two clusters of points, there would be
three clusters of points where the third cluster corresponds to the overlapping vertices of
the two clusters. instead of using id116 id91, we might instead    nd the minimum
1-norm vector in the space spanned by the two singular vectors. the minimum 1-norm
vector will not be an indicator vector, so we would threshold its values to create an
indicator vector for a cluster. instead of    nding the minimum 1-norm vector in the space
spanned by the singular vectors in v, we might look for a small 1-norm vector close to
the subspace.

(1     |x|1 +    cos(  ))

min

x

236

here    is the cosine of the angle between x and the space spanned by the two singular
vectors.    is a control parameter that determines how close we want the vector to be to
the subspace. when    is large, x must be close to the subspace. when    is zero, x can
be anywhere.

finding the minimum 1-norm vector in the space spanned by a set of vectors can be
formulated as a id135 problem. to    nd the minimum 1-norm vector in v,
write v x = y where we want to solve for both x and y. note that the format is di   erent
from the usual format for a set of linear equations ax = b where b is a known vector.

finding the minimum 1-norm vector looks like a nonlinear problem.

min|y|1 subject to v x = y

to remove the absolute value sign, write y = y1     y2 with y1     0 and y2     0. then solve

(cid:32) n(cid:88)

(cid:33)

n(cid:88)

min

y1i +

y2i

i=1

i=1

subject to v x = y, y1     0, and y2     0.

write v x = y1     y2 as v x     y1 + y2 = 0. then we have the linear equations in a format
we are accustomed to.

       =

       x

y1
y2

               

                0

0
...
0

[v,   i, i]

this is a id135 problem. the solution, however, happens to be x = 0,
y1 = 0, and y2 = 0. to resolve this, add the equation y1i = 1 to get a community con-
taining the vertex i.

often we are looking for communities of 50 or 100 vertices in graphs with hundreds of
million of vertices. we want a method to    nd such communities in time proportional to
the size of the community and not the size of the entire graph. here spectral id91
can be used but instead of calculating singular vectors of the entire graph, we do some-
thing else. consider a random walk on a graph. if we walk long enough the id203
distribution converges to the    rst eigenvector. however, if we take only a few steps from a
start vertex or small group of vertices that we believe de   ne a cluster, the id203 will
distribute over the cluster with some of the id203 leaking out to the remainder of
the graph. to get the early convergence of several vectors that ultimately converge to the
   rst few singular vectors, take a subspace [x, ax, a2x, a3x] and propagate the subspace.
at each iteration    nd an orthonormal basis and then multiply each basis vector by a.
then take the resulting basis vectors after a few steps, say    ve, and    nd a minimum
1-norm vector in the subspace.

237

a third issue that arises is when a dominant structure hides an important weaker
structure. one can run their algorithm to    nd the dominant structure and then weaken the
dominant structure by randomly removing edges in the clusters so that the edge density is
similar to the remainder of the network. then reapplying the algorithm often will uncover
weaker structure. real networks often have several levels of structure. the technique
can also be used to improve state of the art id91 algorithms. after weakening the
dominant structure to    nd the weaker hidden structure one can go back to the original data
and weaken the hidden structure and reapply the algorithm to again    nd the dominant
structure. this improves most state of the art id91 algorithms.

238

7.13 bibliographic notes

id91 has a long history. for a good general survey, see [jai10]. for a collection
of surveys giving an in-depth treatment of many di   erent approaches to, applications of,
and perspectives on id91, see [id48r15]; e.g., see [ab15] for a good discussion of
center-based id91. lloyd   s algorithm for id116 id91 is from [llo82], and
the id116++ initialization method is due to arthur and vassilvitskii [av07]. ward   s
algorithm, from 1963, appears in [war63]. analysis of the farthest-traversal algorithm for
the k-center problem is due to gonzalez [gon85].

theorem 7.4 is from [kv09]. analysis of spectral id91 in stochastic models is
given in [mcs01], and the analysis of spectral id91 without a stochastic model and
7.5.2 is due to [kk10].

the de   nition of approximation-stability is from [bbg13] and [bbv08], and the anal-

ysis given in section 7.6 is due to [bbg13].

single-linkage id91 goes back at least to florek et al. [f(cid:32)lp+51], and wishart   s
robust version is from [wis69]. extensions of theorem 7.8 are given in [bbv08], and the-
oretical guarantees for di   erent forms of robust linkage are given in [cd10] and [blg14].
a good survey of kernel methods in id91 appears in [fcmr08].

section 7.9 is a simpli   ed version of [kvv04]. section 7.10 is from [rv99].

239

7.14 exercises

exercise 7.1 construct examples where using distances instead of distance squared gives
bad results for gaussian densities. for example, pick samples from two 1-dimensional
unit variance gaussians, with their centers 10 units apart. cluster these samples by trial
and error into two clusters,    rst according to id116 and then according to the k-median
criteria. the id116 id91 should essentially yield the centers of the gaussians as
cluster centers. what cluster centers do you get when you use the k-median criterion?

exercise 7.2 let v = (1, 3). what is the l1 norm of v? the l2 norm? the square of
the l1 norm?

exercise 7.3 show that in 1-dimension, the center of a cluster that minimizes the sum
of distances of data points to the center is in general not unique. suppose we now require
the center also to be a data point; then show that it is the median element (not the mean).
further in 1-dimension, show that if the center minimizes the sum of squared distances
to the data points, then it is unique.

exercise 7.4 construct a block diagonal matrix a with three blocks of size 50. each
matrix element in a block has value p = 0.7 and each matrix element not in a block has
value q = 0.3. generate a 150    150 matrix b of random numbers in the range [0,1]. if
bij     aij replace aij with the value one. otherwise replace aij with value zero. the rows
of a have three natural clusters. generate a random permutation and use it to permute
the rows and columns of the matrix a so that the rows and columns of each cluster are
randomly distributed.

1. apply the id116 algorithm to a with k = 3. do you    nd the correct clusters?
2. apply the id116 algorithm to a for 1     k     10. plot the value of the sum of

squares to the cluster centers versus k. was three the correct value for k?

exercise 7.5 let m be a k    k matrix whose elements are numbers in the range [0,1].
a matrix entry close to one indicates that the row and column of the entry correspond to
closely related items and an entry close to zero indicates unrelated entities. develop an
algorithm to match each row with a closely related column where a column can be matched
with only one row.

exercise 7.6 the simple greedy algorithm of section 7.3 assumes that we know the clus-
tering radius r. suppose we do not. describe how we might arrive at the correct r?

exercise 7.7 for the k-median problem, show that there is at most a factor of two ratio
between the optimal value when we either require all cluster centers to be data points or
allow arbitrary points to be centers.

exercise 7.8 for the id116 problem, show that there is at most a factor of four ratio
between the optimal value when we either require all cluster centers to be data points or
allow arbitrary points to be centers.

240

n(cid:80)

exercise 7.9 consider id91 points in the plane according to the k-median criterion,
where cluster centers are required to be data points. enumerate all possible id91   s
and select the one with the minimum cost. the number of possible ways of labeling n
points, each with a label from {1, 2, . . . , k} is kn which is prohibitive. show that we can

   nd the optimal id91 in time at most a constant times(cid:0)n

(cid:1) + k2. note that(cid:0)n

(cid:1)     nk

k

k

which is much smaller than kn when k << n.

exercise 7.10 suppose in the previous exercise, we allow any point in space (not neces-
sarily data points) to be cluster centers. show that the optimal id91 may be found
in time at most a constant times n2k2.
exercise 7.11 corollary 7.2 shows that for a set of points {a1, a2, . . . , an}, there is a
|ai     x|2. show examples
unique point x, namely their centroid, which minimizes
|ai     x| is not unique. (consider just points on the real line.)

where the x minimizing

n(cid:80)

n(cid:80)

i=1

show examples where the x de   ned as above are far apart from each other.

i=1

exercise 7.12 let {a1, a2, . . . , an} be a set of unit vectors in a cluster. let c = 1
be the cluster centroid. the centroid c is not in general a unit vector. de   ne the similarity
between two points ai and aj as their dot product. show that the average cluster similarity
1
t is the same whether it is computed by averaging all pairs or computing the
n2

(cid:80)

aiaj

ai

i=1

n

i,j

average similarity of each point with the centroid of the cluster.

exercise 7.13 for some synthetic data estimate the number of local minima for id116
by using the birthday estimate. is your estimate an unbaised estimate of the number? an
upper bound? a lower bound? why?

exercise 7.14 examine the example in figure 7.9 and discuss how to    x it. optimizing
according to the k-center or k-median criteria would seem to produce id91 b while
id91 a seems more desirable.

exercise 7.15 prove that for any two vectors a and b, |a     b|2     1
exercise 7.16 let a be an n  d data matrix, b its best rank k approximation, and c the
optimal centers for id116 id91 of rows of a. how is it possible that (cid:107)a     b(cid:107)2
f <
(cid:107)a     c(cid:107)2
f ?

2|a|2     |b|2.

exercise 7.17 suppose s is a    nite set of points in space with centroid   (s). if a set t
of points is added to s, show that the centroid   (s     t ) of s     t is at distance at most
|s|+|t||  (t )       (s)| from   (s).
|t|

241

a

b

figure 7.9: insert caption

exercise 7.18 what happens if we relax this restriction, for example, if we allow for s,
the entire set?

exercise 7.19 given the graph g = (v, e) of a social network where vertices represent
individuals and edges represent relationships of some kind, one would like to de   ne the
concept of a community. a number of di   erent de   nitions are possible.

1. a subgraph s = (vs, es) whose density es
v 2
s

is greater than that of the graph e
v 2 .

2. a subgraph s with a low conductance like property such as the number of graph edges
leaving the subgraph normalized by the minimum size of s or v     s where size is
measured by the sum of degrees of vertices in s or in v     s.

3. a subgraph that has more internal edges than in a random graph with the same

degree distribution.

which would you use and why?

exercise 7.20 a stochastic matrix is a matrix with non negative entries in which each
row sums to one. show that for a stochastic matrix, the largest eigenvalue is one. show
that the eigenvalue has multiplicity one if and only if the corresponding markov chain is
connected.

exercise 7.21 show that if p is a stochastic matrix and    satis   es   ipij =   jpji, then
for any left eigenvector v of p , the vector u with components ui = vi
is a right eigenvector
  i
with the same eigenvalue.

exercise 7.22 give an example of a id91 problem where the clusters are not linearly
separable in the original space, but are separable in a higher dimensional space.
hint: look at the example for gaussian kernels in the chapter on learning.

exercise 7.23 the gaussian kernel maps points to a higher dimensional space. what is
this mapping?

242

exercise 7.24 agglomerative id91 requires that one calculate the distances between
all pairs of points. if the number of points is a million or more, then this is impractical.
one might try speeding up the agglomerative id91 algorithm by maintaining a 100
clusters at each unit of time. start by randomly selecting a hundred points and place each
point in a cluster by itself. each time a pair of clusters is merged randomly select one of
the remaining data points and create a new cluster containing that point. suggest some
other alternatives.

exercise 7.25 let a be the adjacency matrix of an undirected graph. let d(s, s) = a(s,s)
be the density of the subgraph induced by the set of vertices s. prove that d (s, s) is the

average degree of a vertex in s. recall that a(s, t ) = (cid:80)
is maximized by the single edge with highest aij. recall that a(s, t ) = (cid:80)

exercise 7.26 suppose a is a matrix with non negative entries. show that a(s, t )/(|s||t|)

i   s,j   t

aij

aij

|s|

i   s,j   t

(cid:88)

exercise 7.27 suppose a is a matrix with non negative entries and
|x| = |y| = 1.
   
d. show that the loss is no more
n and all yj less than 1/2

  1(a) = xt ay =

xiaijyj,

   
zero out all xi less than 1/2
than 1/4

th of   1(a).

i,j

exercise 7.28 consider other measures of density such as a(s,t )
  . discuss the signi   cance of the densest subgraph according to these measures.

|s|  |t|   for di   erent values of

exercise 7.29 let a be the adjacency matrix of an undirected graph. let m be the
matrix whose ijth element is aij     didj
2m . partition the vertices into two groups s and   s.
let s be the indicator vector for the set s and let   s be the indicator variable for   s. then
st m s is the number of edges in s above the expected number given the degree distribution
and st m   s is the number of edges from s to   s above the expected number given the degree
distribution. prove that if st m s is positive st m   s must be negative.

exercise 7.30 which of the three axioms, scale invariance, richness, and consistency
are satis   ed by the following id91 algorithms.

1. id116

2. spectral id91.

exercise 7.31 (research problem): what are good measures of density that are also
e   ectively computable? is there empirical/theoretical evidence that some are better than
others?

243

exercise 7.32 create a graph with a small community and start a random walk in the
community. calculate the frequency distribution over the vertices of the graph and normal-
ize the frequency distribution by the stationary id203. plot the ratio of the normalized
frequency for the vertices of the graph. what is the shape of the plot for vertices in the
small community?

exercise 7.33

1. create a random graph with the following two structures imbedded in it. the    rst
structure has three equal size communities with no edges between communities and
the second structure has    ve equal size communities with no edges between commu-
nities.

2. apply a id91 algorithm to    nd the dominate structure. which structure did

you get?

3. weaken the dominant structure by removing a fraction of its edges and see if you

can    nd the hidden structure.

exercise 7.34 experiment with    nding hidden communities.

exercise 7.35 generate a bloc model with two equal size communities where p and q
are the probabilities for the edge density within communities and the edge density between
communities. then generated a gnp model with id203 (p + q)/2. which of two
models most likely generates a community with half of the vertices?

244

8 random graphs

large graphs appear in many contexts such as the world wide web, the internet,
social networks, journal citations, and other places. what is di   erent about the modern
study of large graphs from traditional id207 and graph algorithms is that here
one seeks statistical properties of these very large graphs rather than an exact answer
to questions on speci   c graphs. this is akin to the switch physics made in the late 19th
century in going from mechanics to statistical mechanics. just as the physicists did, one
formulates abstract models of graphs that are not completely realistic in every situation,
but admit a nice mathematical development that can guide what happens in practical
situations. perhaps the most basic model is the g (n, p) model of a random graph. in
this chapter, we study properties of the g(n, p) model as well as other models.

8.1 the g(n, p) model

the g (n, p) model, due to erd  os and r  enyi, has two parameters, n and p. here n is
the number of vertices of the graph and p is the edge id203. for each pair of distinct
vertices, v and w, p is the id203 that the edge (v,w) is present. the presence of each
edge is statistically independent of all other edges. the graph-valued random variable
with these parameters is denoted by g (n, p). when we refer to    the graph g (n, p)   , we
mean one realization of the random variable. in many cases, p will be a function of n
such as p = d/n for some constant d. for example, if p = d/n then the expected degree
n     d. in order to simplify calculations in this chapter,
of a vertex of the graph is (n     1) d
n     1. in fact, conceptually it is helpful to
we will often use the approximation that n   1
think of n as both the total number of vertices and as the number of potential neighbors
of any given node, even though the latter is really n     1; for all our calculations, when n
is large, the correction is just a low-order term.

the interesting thing about the g(n, p) model is that even though edges are chosen
independently with no    collusion   , certain global properties of the graph emerge from the
independent choices. for small p, with p = d/n, d < 1, each connected component in the
graph is small. for d > 1, there is a giant component consisting of a constant fraction of
the vertices. in addition, there is a rapid transition at the threshold d = 1. below the
threshold, the id203 of a giant component is very small, and above the threshold,
the id203 is almost one.

the phase transition at the threshold d = 1 from very small o(n) size components to a
giant    (n) sized component is illustrated by the following example. suppose the vertices
represent people and an edge means the two people it connects know each other. given a
chain of connections, such as a knows b, b knows c, c knows d, ..., and y knows z, we
say that a indirectly knows z. thus, all people belonging to a connected component of
the graph indirectly know each other. suppose each pair of people, independent of other
pairs, tosses a coin that comes up heads with id203 p = d/n. if it is heads, they

245

1     o(1)

id203
of a giant
component

o(1)

1       

expected number of friends per person

1 +   

figure 8.1: id203 of a giant component as a function of the expected number of
people each person knows directly.

know each other; if it comes up tails, they don   t. the value of d can be interpreted as the
expected number of people a single person directly knows. the question arises as to how
large are sets of people who indirectly know each other?

if the expected number of people each person knows is more than one, then a giant
component of people, all of whom indirectly know each other, will be present consisting
of a constant fraction of all the people. on the other hand, if in expectation, each person
knows less than one person, the largest set of people who know each other indirectly is a
vanishingly small fraction of the whole. furthermore, the transition from the vanishing
fraction to a constant fraction of the whole, happens abruptly between d slightly less than
one to d slightly more than one. see figure 8.1. note that there is no global coordination
of who knows whom. each pair of individuals decides independently. indeed, many large
real-world graphs, with constant average degree, have a giant component. this is perhaps
the most important global property of the g(n, p) model.

8.1.1 degree distribution

one of the simplest quantities to observe in a real graph is the number of vertices
of given degree, called the vertex degree distribution.
it is also very simple to study
these distributions in g (n, p) since the degree of each vertex is the sum of n independent
random variables, which results in a binomial distribution.

example: in g(n, 1
2), each vertex is of degree close to n/2. in fact, for any    > 0, the
degree of each vertex almost surely is within 1       times n/2. to see this, note that the
degree of a vertex is the sum of n     1     n indicator variables that take on value one or

246

35

34

33

32

31

40

39

38

37

36

5

4

3

2

1

10

9

8

7

6

15

14

13

12

11

20

19

18

17

16

25

24

23

22

21

a graph with 40 vertices and 24 edges

17

22

1

30

34

3

2

18

23

31

35

19

7

9

6

8

4

5

10

11

12

13

14

15

16

36

37

38

20

24

26

28

32

39

30

29

28

27

26

21

25

27

29

33

40

a randomly generated g(n, p) graph with 40 vertices and 24 edges

figure 8.2: two graphs, each with 40 vertices and 24 edges. the second graph was
randomly generated using the g(n, p) model with p = 1.2/n. a graph similar to the top
graph is almost surely not going to be randomly generated in the g(n, p) model, whereas
a graph similar to the lower graph will almost surely occur. note that the lower graph
consists of a giant component along with a number of small components that are trees.

247

binomial distribution

power law distribution

figure 8.3: illustration of the binomial and the power law distributions.

zero depending on whether the edge is present or not, each of mean 1
4. the
expected value of the sum is the sum of the expected values and the variance of the sum
is the sum of the variances, and hence the degree has mean     n
   
4 . thus,
the id203 mass is within an additive term of   c
n of the mean for some constant
c and thus within a multiplicative factor of 1      of n

2 and variance 1
2 and variance     n

2 for su   ciently large n.

the degree distribution of g (n, p) for general p is also binomial. since p is the prob-
ability of an edge being present, the expected degree of a vertex is p(n     1)     pn. the
degree distribution is given by

prob(vertex has degree k) =(cid:0)n   1

(cid:1)pk(1     p)n   k   1    (cid:0)n

(cid:1) is the number of ways of choosing k edges, out of the possible n edges,

k

k

the quantity(cid:0)n

(cid:1)pk(1     p)n   k.

and pk(1    p)n   k is the id203 that the k selected edges are present and the remaining
n     k are not.

k

the binomial distribution falls o    exponentially fast as one moves away from the mean.
however, the degree distributions of graphs that appear in many applications do not ex-
hibit such sharp drops. rather, the degree distributions are much broader. this is often
referred to as having a    heavy tail   . the term tail refers to values of a random variable
far away from its mean, usually measured in number of standard deviations. thus, al-
though the g (n, p) model is important mathematically, more complex models are needed
to represent real world graphs.

consider an airline route graph. the graph has a wide range of degrees from degree
one or two for a small city to degree 100 or more, for a major hub. the degree distribution
is not binomial. many large graphs that arise in various applications appear to have power
law degree distributions. a power law degree distribution is one in which the number of
vertices having a given degree decreases as a power of the degree, as in

248

number(degree k vertices) = c n
kr ,

for some small positive real r, often just slightly less than three. later, we will consider
a random graph model giving rise to such degree distributions.

the following theorem states that the degree distribution of the random graph g (n, p)
is tightly concentrated about its expected value. that is, the id203 that the degree
np, drops o    exponentially
of a vertex di   ers from its expected degree by more than   
fast with   .

   

   

theorem 8.1 let v be a vertex of the random graph g(n, p). let    be a real number in
(0,

np).

prob(|np     deg(v)|       

   
np)     3e     2/8.

proof: the degree deg(v) is the sum of n     1 independent bernoulli random variables,
x1, x2, . . . , xn   1, where, xi is the indicator variable that the ith edge from v is present. so,
approximating n     1 with n, the theorem follows from theorem 12.6 in the appendix.

   

although the id203 that the degree of a single vertex di   ers signi   cantly from
its expected value drops exponentially, the statement that the degree of every vertex is
close to its expected value requires that p is    ( log n
n ). that is, the expected degree grows
at least logarithmically with the number of vertices.
corollary 8.2 suppose    is a positive constant. if p     9 ln n
vertex has degree in the range (1       )np to (1 +   )np.

n  2 , then almost surely every

np to get that the id203 that an individual
proof: apply theorem 8.1 with    =   
vertex has degree outside the range [(1       )np, (1 +   )np] is at most 3e     2np/8. by the
union bound, the id203 that some vertex has degree outside this range is at most
3ne     2np/8. for this to be o(1), it su   ces for p     9 ln n
n  2 .

n ) is necessary.

note that the assumption p is    ( log n

if p = d/n for d a constant,
then some vertices may well have degrees outside the range [(1       )d, (1 +   )d]. indeed,
shortly we will see that it is highly likely that for p = 1
n there is a vertex of degree
   (log n/ log log n). moreover, for p = 1
n it is easy to see that with high id203 there
will be at least one vertex of degree zero.

(cid:1) the expected degree of a vertex is approximately n/2. in many real applications,

when p is a constant, the expected degree of vertices in g (n, p) increases with n. in

g(cid:0)n, 1

we will be concerned with g (n, p) where p = d/n, for d a constant, i.e., graphs whose
expected degree is a constant d independent of n. as n goes to in   nity, the binomial
distribution with p = d
n

2

prob(k) =

(cid:18)n
(cid:19)(cid:18) d

(cid:19)k(cid:18)

(cid:19)n   k

1     d
n

k

n

249

approaches the poisson distribution

to see this, assume k = o(n) and use the approximations (cid:0)n
(cid:0)1     d

(cid:1)n   k    (cid:0)1     d

prob(k) =

k

e   d.

dk
k!

(cid:1)     nk

n

(cid:1)n     e   d. then
(cid:18)n
(cid:19)(cid:18) d
(cid:19)k(cid:18)

n

lim
n      

k

n

(cid:19)n   k

1     d
n

=

nk
k!

dk

nk e   d =

dk
k!

e   d.

k! , n     k     n, and

note that for p = d
n , where d is a constant independent of n, the id203 of the
binomial distribution falls o    rapidly for k > d, and is essentially zero once k! dominates
dk. this justi   es the k = o(n) assumption. thus, the poisson distribution is a good
approximation.

example: in g(n, 1
n) many vertices are of degree one, but not all. some are of degree
zero and some are of degree greater than one. in fact, it is highly likely that there is a
vertex of degree    (log n/ log log n). the id203 that a given vertex is of degree k is

(cid:18)n     1

(cid:19)(cid:18) 1

(cid:19)k(cid:18)

k

n

1     1
n

(cid:19)n   1   k    

(cid:18)n
(cid:19)(cid:18) 1

(cid:19)k(cid:18)

k

n

1     1
n

(cid:19)n   k     e   1

.

k!

prob (k) =

if k = log n/ log log n,

log kk = k log k =

log n

log log n

(log log n     log log log n)     log n

and thus kk     n. since k!     kk     n, the id203 that a vertex has degree k =
log n/ log log n is at least 1
en. if the degrees of vertices were independent random
variables, then this would be enough to argue that there would be a vertex of degree
e    = 0.31. but the degrees
    1

log n/ log log n with id203 at least 1    (cid:0)1     1

(cid:1)n = 1     e

k! e   1     1

are not quite independent since when an edge is added to the graph it a   ects the degree
of two vertices. this is a minor technical point, which one can get around.

en

8.1.2 existence of triangles in g(n, d/n)

what is the expected number of triangles in g(cid:0)n, d

(cid:1), when d is a constant? as the

number of vertices increases one might expect the number of triangles to increase, but this
is not the case. although the number of triples of vertices grows as n3, the id203
of an edge between two speci   c vertices decreases linearly with n. thus, the id203
of all three edges between the pairs of vertices in a triple of vertices being present goes
down as n   3, exactly canceling the rate of growth of triples.

n

250

a random graph with n vertices and edge id203 d/n, has an expected number

of triangles that is independent of n, namely d3/6. there are (cid:0)n
each triple has id203 (cid:0) d
(j, k), and (i, k) being present. then the number of triangles is x = (cid:80)

(cid:1) triples of vertices.
(cid:1)3 of being a triangle. let    ijk be the indicator variable

for the triangle with vertices i, j, and k being present. that is, all three edges (i, j),
ijk    ijk. even
though the existence of the triangles are not statistically independent events, by linearity
of expectation, which does not assume independence of the variables, the expected value
of a sum of random variables is the sum of the expected values. thus, the expected
number of triangles is

n

3

(cid:16)(cid:88)

(cid:17)

(cid:88)

ijk

ijk

e(x) = e

   ijk

=

e(   ijk) =

(cid:18)n
(cid:19)(cid:18) d

3

n

(cid:19)3     d3

.

6

even though on average there are d3

6 triangles per graph, this does not mean that with
high id203 a graph has a triangle. maybe half of the graphs have d3
3 triangles and
the other half have none for an average of d3
6 triangles. then, with id203 1/2, a
graph selected at random would have no triangle. if 1/n of the graphs had d3
6 n triangles
and the remaining graphs had no triangles, then as n goes to in   nity, the id203 that
a graph selected at random would have a triangle would go to zero.

we wish to assert that with some nonzero id203 there is at least one triangle
in g(n, p) when p = d
n. if all the triangles were on a small number of graphs, then the
number of triangles in those graphs would far exceed the expected value and hence the
variance would be high. a second moment argument rules out this scenario where a small
fraction of graphs have a large number of triangles and the remaining graphs have none.

let   s calculate e(x2) where x is the number of triangles. write x as x =(cid:80)

ijk    ijk,
where    ijk is the indicator variable of the triangle with vertices i, j, and k being present.
expanding the squared term

e(x2) = e

   ijk

= e

   ijk   i(cid:48)j(cid:48)k(cid:48)

.

(cid:16)(cid:88)

(cid:17)2

(cid:16) (cid:88)

i,j,k

i, j, k
i(cid:48),j(cid:48),k(cid:48)

(cid:17)

split the above sum into three parts. in part 1, let s1 be the set of i, j, k and i(cid:48), j(cid:48), k(cid:48)
which share at most one vertex and hence the two triangles share no edge. in this case,
   ijk and    i(cid:48)j(cid:48)k(cid:48) are independent and

(cid:17)

(cid:88)

e(   ijk)e(   i(cid:48)j(cid:48)k(cid:48))    (cid:16)(cid:88)

(cid:17)(cid:16)(cid:88)

(cid:17)

e

   ijk   i(cid:48)j(cid:48)k(cid:48)

=

e(   ijk)

e(   i(cid:48)j(cid:48)k(cid:48))

= e2(x).

(cid:16)(cid:88)

s1

s1

all
ijk

all
i(cid:48)j(cid:48)k(cid:48)

251

or

the two triangles of part 1 are either
disjoint or share at most one vertex

the two triangles
of part 2 share an
edge

the two triangles in
part 3 are the same tri-
angle

figure 8.4: the triangles in part 1, part 2, and part 3 of the second moment argument
for the existence of triangles in g(n, d

in part 2, i, j, k and i(cid:48), j(cid:48), k(cid:48) share two vertices and hence one edge. see figure 8.4.

four vertices and    ve edges are involved overall. there are at most(cid:0)n
subsets and(cid:0)4

(cid:1)     o(n4), 4-vertex
(cid:1) ways to partition the four vertices into two triangles with a common edge.

n).

4

the id203 of all    ve edges in the two triangles being present is p5, so this part sums
to o(n4p5) = o(d5/n) and is o(1). there are so few triangles in the graph, the id203
of two triangles sharing an edge is extremely unlikely.

2

in part 3, i, j, k and i(cid:48), j(cid:48), k(cid:48) are the same sets. the contribution of this part of the

summation to e(x2) is(cid:0)n

(cid:1)p3 = d3

3

6 . thus, putting all three parts together, we have:
e(x2)     e2(x) +

+ o(1),

d3
6

which implies

value. thus,

var(x) = e(x2)     e2(x)     d3
6

prob(x = 0)     prob(cid:0)|x     e(x)|     e(x)(cid:1) .

+ o(1).

for x to be equal to zero, it must di   er from its expected value by at least its expected

by chebychev inequality,

    d3/6 + o(1)

prob(x = 0)     var(x)
e2(x)
6    = 1.8, prob(x = 0) < 1 and g(n, p) has a triangle with nonzero
6 < 1 and there simply are not enough edges in the

d3 + o(1).

6, e(x) = d3

    6

d6/36

(8.1)

   
   
thus, for d > 3
id203. for d < 3
graph for there to be a triangle.

8.2 phase transitions

many properties of random graphs undergo structural changes as the edge id203
passes some threshold value. this phenomenon is similar to the abrupt phase transitions in

252

physics, as the temperature or pressure increases. some examples of this are the abrupt
appearance of cycles in g(n, p) when p reaches 1/n and the disappearance of isolated
vertices when p reaches ln n
n . the most important of these transitions is the emergence of
a giant component, a connected component of size   (n), which happens at d = 1. recall
figure 8.1.

id203 transition

p = o( 1
n)

p = d

n , d < 1

p = d

p = d

n , d = 1
n , d > 1

ln n

(cid:113) 2 ln n

n

n

p = 1
2

p =

p = ln n
n

p = 1
2

forest of trees, no component
of size greater than o(log n)
cycles appear, no component
of size greater than o(log n)
components of size o(n 2
3 )
giant component plus o(log n)
components
giant component plus isolated
vertices

diameter two
disappearance of isolated vertices
appearance of hamilton circuit
diameter o(log n)
clique of size (2      ) ln n

table 1: phase transitions

for these and many other properties of random graphs, a threshold exists where an
abrupt transition from not having the property to having the property occurs. if there
p1(n)
p(n) = 0, g (n, p1 (n)) almost surely does not
exists a function p (n) such that when lim
n      
p(n) =    , g (n, p2 (n)) almost surely has the property,

have the property, and when lim
n      

p2(n)

then we say that a phase transition occurs, and p (n) is the threshold. recall that g(n, p)
   almost surely does not have the property    means that the id203 that it has the
property goes to zero in the limit, as n goes to in   nity. we shall soon see that every
increasing property has a threshold. this is true not only for increasing properties of
g (n, p), but for increasing properties of any combinatorial structure. if for cp (n), c < 1,
the graph almost surely does not have the property and for cp (n) , c > 1, the graph
almost surely has the property, then p (n) is a sharp threshold. the existence of a giant
component has a sharp threshold at 1/n. we will prove this later.

in establishing phase transitions, we often use a variable x(n) to denote the number
of occurrences of an item in a random graph. if the expected value of x(n) goes to zero as
n goes to in   nity, then a graph picked at random almost surely has no occurrence of the

253

1

prob(x > 0)

0

1

n1+ 

1

n log n

1
n

log n

n

1
2

0.6
n

0.8
n

1
n

1.2
n

1.4
n

1   o(1)

n

1
n

1+o(1)

n

(a)

(b)

(c)

figure 8.5: figure 8.5(a) shows a phase transition at p = 1
n . the dotted line shows
an abrupt transition in prob(x) from 0 to 1. for any function asymptotically less than
1
n, prob(x)>0 is zero and for any function asymptotically greater than 1
n, prob(x)>0 is
one. figure 8.5(b) expands the scale and shows a less abrupt change in id203 unless
the phase transition is sharp as illustrated by the dotted line. figure 8.5(c) is a further
expansion and the sharp transition is now more smooth.

a e(x), which implies that the id203 of x(n)     1 is at most e(x(n)).

item. this follows from markov   s inequality. since x is a nonnegative random variable
prob(x     a)     1
that is, if the expected number of occurrences of an item in a graph goes to zero, the
id203 that there are one or more occurrences of the item in a randomly selected
graph goes to zero. this is called the    rst moment method.

the previous section showed that the property of having a triangle has a threshold at
p(n) = 1/n. if the edge id203 p1(n) is o(1/n), then the expected number of triangles
goes to zero and by the    rst moment method, the graph almost surely has no triangle.
1/n        , then from (8.1), the id203
however, if the edge id203 p2(n) satis   es p2(n)
of having no triangle is at most 6/d3 + o(1) = 6/(np2(n))3 + o(1), which goes to zero. this
latter case uses what we call the second moment method. the    rst and second moment
methods are broadly used. we describe the second moment method in some generality
now.

when the expected value of x(n), the number of occurrences of an item, goes to
in   nity, we cannot conclude that a graph picked at random will likely have a copy since
the items may all appear on a vanishingly small fraction of the graphs. we resort to a
technique called the second moment method. it is a simple idea based on chebyshev   s
inequality.

theorem 8.3 (second moment method) let x(n) be a random variable with e(x) > 0.
if

var(x) = o(cid:0)e2(x)(cid:1),

then x is almost surely greater than zero.

254

no items

e(x)     0.1

at least one
occurrence
of item in
10% of the
graphs

for 10% of the
graphs, x     1

figure 8.6: if the expected fraction of the number of graphs in which an item occurs
did not go to zero, then e (x), the expected number of items per graph, could not be
zero. suppose 10% of the graphs had at least one occurrence of the item. then the
expected number of occurrences per graph must be at least 0.1. thus, e (x)     0 implies
the id203 that a graph has an occurrence of the item goes to zero. however, the
other direction needs more work. if e (x) is large, a second moment argument is needed
to conclude that the id203 that a graph picked at random has an occurrence of the
item is nonnegligible, since there could be a large number of occurrences concentrated on
a vanishingly small fraction of all graphs. the second moment argument claims that for
a nonnegative random variable x with e (x) > 0, if var(x) is o(e2 (x)) or alternatively if
e (x2)     e2 (x) (1 + o(1)), then almost surely x > 0.

proof: if e(x) > 0, then for x to be less than or equal to zero, it must di   er from its
expected value by at least its expected value. thus,

prob(x     0)     prob

by chebyshev inequality

prob

(cid:16)|x     e(x)|     e(x)

(cid:16)|x     e(x)|     e(x)
(cid:17)
(cid:17)     var(x)

    0.

.

e2(x)

thus, prob(x     0) goes to zero if var(x) is o (e2(x)) .

corollary 8.4 let x be a random variable with e(x) > 0. if

e(x2)     e2(x)(cid:0)1 + o(1)(cid:1),

then x is almost surely greater than zero.
proof: if e(x2)     e2(x)(1 + o(1)), then

v ar(x) = e(x2)     e2(x)     e2(x)o(1) = o(cid:0)e2(x)(cid:1).

255

second moment arguments are more di   cult than    rst moment arguments since they
deal with variance and without independence we do not have e(xy) = e(x)e(y). in the
triangle example, dependence occurs when two triangles share a common edge. however,
if p = d
n , there are so few triangles that almost surely no two triangles share a common
edge and the lack of statistical independence does not a   ect the answer. in looking for
a phase transition, almost always the transition in id203 of an item being present
occurs when the expected number of items transitions.

threshold for graph diameter two (two degrees of separation)

we now present the    rst example of a sharp phase transition for a property. this
means that slightly increasing the edge id203 p near the threshold takes us from
almost surely not having the property to almost surely having it. the property is that
of a random graph having diameter less than or equal to two. the diameter of a graph
is the maximum length of the shortest path between a pair of nodes. in other words, the
property is that every pair of nodes has    at most two degrees of separation   .

the following technique for deriving the threshold for a graph having diameter two
is a standard method often used to determine the threshold for many other objects. let
x be a random variable for the number of objects such as triangles, isolated vertices, or
hamiltonian circuits, for which we wish to determine a threshold. then we determine
the value of p, say p0, where the expected value of x goes from vanishingly small to un-
boundedly large. for p < p0 almost surely a graph selected at random will not have a
copy of the item. for p > p0, a second moment argument is needed to establish that the
items are not concentrated on a vanishingly small fraction of the graphs and that a graph
picked at random will almost surely have a copy.

our    rst task is to    gure out what to count to determine the threshold for a graph
having diameter two. a graph has diameter two if and only if for each pair of vertices i
and j, either there is an edge between them or there is another vertex k to which both i
and j have an edge. so, what we will count is the number of pairs i and j that fail, i.e.,
the number of pairs i and j that have more than two degrees of separation. the set of
neighbors of i and the set of neighbors of j are random subsets of expected cardinality
n. such statements often
   
   
go under the general name of    birthday paradox    though it is not a paradox. in what
follows, we will prove a threshold of o(
ln n/
n) for a graph to have diameter two. the
extra factor of
   

np. for these two sets to intersect requires np        
ln n ensures that every one of the (cid:0)n

(cid:1) pairs of i and j has a common

n or p     1   

neighbor. when p = c
   
2, the graph almost surely has diameter less than or equal to two.

2, the graph almost surely has diameter greater

than two and for c >

(cid:113) ln n

n , for c <

   

2

theorem 8.5 the property that g (n, p) has diameter two has a sharp threshold at

(cid:113) ln n

n .

   

p =

2

256

proof: if g has diameter greater than two, then there exists a pair of nonadjacent ver-
tices i and j such that no other vertex of g is adjacent to both i and j. this motivates
calling such a pair bad .

introduce a set of indicator variables iij, one for each pair of vertices (i, j) with i < j,

where iij is 1 if and only if the pair (i, j) is bad. let

(cid:88)

x =

iij

i<j

be the number of bad pairs of vertices. putting i < j in the sum ensures each pair (i, j)
is counted only once. a graph has diameter at most two if and only if it has no bad pair,
i.e., x = 0. thus, if
lim
n       e (x) = 0, then for large n, almost surely, a graph has no bad
pair and hence has diameter at most two.

the id203 that a given vertex is adjacent to both vertices in a pair of vertices
(i, j) is p2. hence, the id203 that the vertex is not adjacent to both vertices is
1     p2. the id203 that no vertex is adjacent to the pair (i, j) is (1     p2)n   2 and the

id203 that i and j are not adjacent is 1     p. since there are (cid:0)n

(cid:1) pairs of vertices,

2

the expected number of bad pairs is

(cid:113) ln n

n ,

setting p = c

e (x) =

(cid:19)

(cid:18)n
(cid:16)

2

(1     p)(cid:0)1     p2(cid:1)n   2 .
(cid:113) ln n
(cid:17)(cid:0)1     c2 ln n
(cid:1)n

n

n

2

e (x)    = n2
   = n2
   = 1

1     c
2 e   c2 ln n
2n2   c2.

(cid:113) ln n

n with c >

   
2,

   

for c >

2, lim

n       e (x) = 0. by the    rst moment method, for p = c

g (n, p) almost surely has no bad pair and hence has diameter at most two.

next, consider the case c <

2 where lim

   

argument to claim that almost surely a graph has a bad pair and thus has diameter greater
than two.

(cid:32)(cid:88)

(cid:33)2

(cid:32)(cid:88)

n       e (x) =    . we appeal to a second moment
(cid:88)

(cid:88)

(cid:33)

         (cid:88)

          =

e(x2) = e

iij

= e

iij

ikl

= e

iijikl

e (iijikl).

i<j

i<j

k<l

i<j
k<l

i<j
k<l

the summation can be partitioned into three summations depending on the number of
distinct indices among i, j, k, and l. call this number a.

257

e(cid:0)x2(cid:1) =

(cid:88)

i < j
k < l

a = 4

e (iijikl) +

e (iijiik) +

(cid:88)

{i, j, k}
i < j

(cid:88)

e(cid:0)i 2

ij

(cid:1).

i < j

(8.2)

a = 3

a = 2

consider the case a = 4 where i, j, k, and l are all distinct. if iijikl = 1, then both
pairs (i, j) and (k, l) are bad and so for each u not in {i, j, k, l}, at least one of the edges
(i, u) or (j, u) is absent and, in addition, at least one of the edges (k, u) or (l, u) is absent.
the id203 of this for one u not in {i, j, k, l} is (1     p2)2. as u ranges over all the
n     4 vertices not in {i, j, k, l}, these events are all independent. thus,

e(iijikl)    (cid:0)1     p2(cid:1)2(n   4)    (cid:16)

(cid:17)2n(cid:0)1 + o(1)(cid:1)     n   2c2(cid:0)1 + o(1)(cid:1)
n4   2c2(cid:0)1 + o(1)(cid:1),

and the    rst sum is

1     c2 ln n
n

e(iijikl)     1
4

(cid:88)

i < j
k < l

where, the 1

4 is because only a fourth of the 4-tupples (i, j, k, l) have i < j and k < l.

for the second summation, observe that if iijiik = 1, then for every vertex u not equal
to i, j, or k, either there is no edge between i and u or there is an edge (i, u) and both
edges (j, u) and (k, u) are absent. the id203 of this event for one u is

1     p + p(1     p)2 = 1     2p2 + p3     1     2p2.
thus, the id203 for all such u is (1     2p2)n   3. substituting c

(cid:16)

1     2c2 ln n

n

(cid:17)n   3    = e   2c2 ln n = n   2c2,

(cid:113) ln n

n for p yields

which is an upper bound on e(iijikl) for one i, j, k, and l with a = 3. summing over all
distinct triples yields n3   2c2 for the second summation in (8.2).

for the third summation, since the value of iij is zero or one, e(cid:0)i 2

(cid:1) = e (iij). thus,

ij

(cid:88)

e(cid:0)i 2

(cid:1) = e (x) .
4n4   2c2 + n3   2c2 + n2   c2 and e (x)    = 1

ij

ij

hence, e (x2)     1
2n2   c2, from which it follows that
   
2, e (x2)     e2 (x) (1 + o(1)). by a second moment argument, corollary 8.4, a
for c <
graph almost surely has at least one bad pair of vertices and thus has diameter greater
than two. therefore, the property that the diameter of g(n, p) is less than or equal to

two has a sharp threshold at p =

2

(cid:113) ln n

   

n

258

disappearance of isolated vertices

the disappearance of isolated vertices

n . at
this point the giant component has absorbed all the small components and with the
disappearance of isolated vertices, the graph becomes connected.

in g (n, p) has a sharp threshold at ln n

theorem 8.6 the disappearance of isolated vertices in g (n, p) has a sharp threshold of
ln n
n .

proof: let x be the number of isolated vertices in g (n, p). then,

since we believe the threshold to be ln n

e (x) = n (1     p)n   1 .

n       n(cid:0)1     c ln n

n

(cid:1)n = lim

n , consider p = c ln n

n . then,
n       ne   c ln n = lim

n       n1   c.

lim
n       e (x) = lim

if c >1, the expected number of isolated vertices, goes to zero. if c < 1, the expected
number of isolated vertices goes to in   nity. if the expected number of isolated vertices
goes to zero, it follows that almost all graphs have no isolated vertices. on the other
hand, if the expected number of isolated vertices goes to in   nity, a second moment ar-
gument is needed to show that almost all graphs have an isolated vertex and that the
isolated vertices are not concentrated on some vanishingly small set of graphs with almost
all graphs not having isolated vertices.

assume c < 1. write x = i1 + i2 +       + in where ii is the indicator variable indicating
e (iiij). since ii

whether vertex i is an isolated vertex. then e (x2) =

e (i 2

i ) + 2(cid:80)

i<j

i = ii and the    rst sum has value e (x). since all elements in the second

n(cid:80)
e(cid:0)x2(cid:1) = e (x) + n (n     1) e (i1i2)

i=1

equals 0 or 1, i 2
sum are equal

= e (x) + n (n     1) (1     p)2(n   1)   1 .

the minus one in the exponent 2(n     1)     1 avoids counting the edge from vertex 1 to
vertex 2 twice. now,

n (1     p)n   1 + n (n     1) (1     p)2(n   1)   1

e (x2)
e2 (x)

=

(cid:16)

n2 (1     p)2(n   1)

1     1
n

1     p

.

(cid:17) 1
(cid:35)

1

1     c ln n

n

= lim
n      

1

=

n (1     p)n   1 +
(cid:34)
n       e (x) =     and
(cid:17)

(cid:16)

lim

1
n1   c +

1     1
n

(cid:16)

1 + c

ln n
n

(cid:17)

= o(1) + 1.

for p = c ln n

n with c < 1,

lim
n      

e (x2)
e2 (x)

= lim
n      

259

figure 8.7: a degree three vertex with three adjacent degree two vertices. graph cannot
have a hamilton circuit.

by the second moment argument, corollary 8.4, the id203 that x = 0 goes to zero
implying that almost all graphs have an isolated vertex. thus, ln n
n is a sharp threshold
for the disappearance of isolated vertices. for p = c ln n
n , when c > 1 there almost surely
are no isolated vertices, and when c < 1 there almost surely are isolated vertices.

hamilton circuits

so far in establishing phase transitions in the g(n, p) model for an item such as the
disappearance of isolated vertices, we introduced a random variable x that was the number
of occurrences of the item. we then determined the id203 p for which the expected
value of x went from zero to in   nity. for values of p for which e(x)     0, we argued that
with high id203, a graph generated at random had no occurrences of x. for values of
x for which e(x)        , we used the second moment argument to conclude that with high
id203, a graph generated at random had occurrences of x. that is, the occurrences
that forced e(x) to in   nity were not all concentrated on a vanishingly small fraction of
the graphs. one might raise the question for the g(n, p) graph model, do there exist
items that are so concentrated on a small fraction of the graphs that the value of p where
e(x) goes from zero to in   nity is not the threshold? an example where this happens is
hamilton circuits.

a hamilton circuit is a simple cycle that includes all the vertices. for example, in a
graph of 4 vertices, there are three possible hamilton circuits: (1, 2, 3, 4), (1, 2, 4, 3), and
(1, 3, 2, 4). note that our graphs are undirected, so the circuit (1, 2, 3, 4) is the same as
the circuit (1, 4, 3, 2).

let x be the number of hamilton circuits in g(n, p) and let p = d

n for some constant
2(n     1)! potential hamilton circuits in a graph and each has id203

d. there are 1

260

( d
n)n of actually being a hamilton circuit. thus,

e(x) =

(cid:19)n

(cid:18) d
(cid:19)n

n

1
2

(n     1)!

(cid:17)n(cid:18) d
(cid:39)(cid:16)n
(cid:26) 0

e

   

n
d < e
    d > e

.

this suggests that the threshold for hamilton circuits occurs when d equals euler   s con-
stant e. this is not possible since the graph still has isolated vertices and is not even
connected for p = e

n . thus, the second moment argument is indeed necessary.

the actual threshold for hamilton circuits is 1

n log n. for any p(n) asymptotically
greater, g(n, p) will have a hamilton circuit with id203 one. this is the same
threshold as for the disappearance of degree one vertices. clearly a graph with a degree
one vertex cannot have a hamilton circuit. but it may seem surprising that hamilton
circuits appear as soon as degree one vertices disappear. you may ask why at the moment
degree one vertices disappear there cannot be a subgraph consisting of a degree three
vertex adjacent to three degree two vertices as shown in figure 8.7. the reason is that the
frequency of degree two and three vertices in the graph is very small and the id203
that four such vertices would occur together in such a subgraph is too small for it to
happen with nonnegligible id203.

8.3 giant component

consider g(n, p) for p = 1+ 
n where   is a constant greater than zero. we now show that
with high id203, such a graph contains a giant component, namely a component of
size    (n). moreover, with high id203, the graph contains only one such component,
and all other components are much smaller, of size only o(log n). we begin by arguing
existence of a giant component.

8.3.1 existence of a giant component

to see that with high id203 the graph has a giant component, do a depth    rst search
(dfs) on g(n, p) where p = (1 +  )/n with 0 <   < 1/8. note that it su   ces to consider
this range of   since increasing the value of p only increases the id203 that the graph
has a giant component.

(cid:1) bernoulli(p) independent random bits and answer

to perform the dfs, generate (cid:0)n

2

261

u

e

unvisited vertices

frontier

f

small connected
components
already found

figure 8.8: picture after  n2/2 edge queries. the potential edges from the small con-
nected components to unvisited vertices do not exist in the graph. however, since many
edges must have been found the frontier must be big and hence there is a giant component.

the tth edge query according to the tth bit. as the dfs proceeds, let

e = set of fully explored vertices whose exploration is complete
u = set of unvisited vertices
f = frontier of visited and still being explored vertices .

initially the set of fully explored vertices, e, and the frontier, f are empty and the
set of unvisited vertices, u equals {1, 2, . . . , n}. if the frontier is not empty and u is the
active vertex of the dfs, the dfs queries each unvisited vertex in u until it    nds a vertex
v for which there is an edge (u, v) and moves v from u to the frontier and v becomes the
active vertex. if no edge is found from u to an unvisited vertex in u, then u is moved from
the frontier to the set of fully explored vertices e. if frontier is empty, the dfs moves an
unvisited vertex from u to frontier and starts a new component. if both frontier and u
are empty all connected components of g have been found. at any time all edges between
the current fully explored vertices, e, and the current unvisited vertices, u, have been
queried since a vertex is moved from the frontier to e only when there is no edge from
the vertex to u.

intuitively, after  n2/2 edge queries a large number of edges must have been found since
p = 1+ 
n . none of these can connect components already found with the set of unvisited
vertices, and we will use this to show that with high id203 the frontier must be large.
since the frontier will be in a connected component, a giant component exists with high
id203. we    rst prove that after  n2/2 edge queries the set of fully explored vertices
is of size less than n/3.
lemma 8.7 after  n2/2 edge queries, with high id203 |e| < n/3.
proof: if not, at some t       n2/2,
|e| = n/3. a vertex is added to frontier only when
an edge query is answered yes. so at time t, |f| is less than or equal to the sum of   n2/2
bernoulli(p) random variables, which with high id203 is at most   n2p     n/3. so,

262

at t, |u| = n     |e|     |f|     n/3. since there are no edges between fully explored vertices
and unvisited vertices, |e| |u|     n2/9 edge queries must have already been answered in
the negative. but t > n2/9 contradicts t      n2/2     n2/16. thus |e|     n/3.

the frontier vertices in the search of a connected component are all in the component
being searched. thus if at any time the frontier set has    (n) vertices there is a giant
component.

lemma 8.8 after  n2/2 edge queries, with high id203 the set f consists of at least
 2n/30 vertices.
proof: after   n2/2 queries, say, |f| <   2n/30. thus

|u| = n     |e|     |f| = n     n
3

     2n
30

    1

and so the dfs is still active. each positive answer to an edge query so far resulted in some
vertex moving from u to f, which possibly later moved to e. the expected number of
yes answers so far is p  n2/2 = (1 +   )  n/2 and with high id203, the number of yes
answers is at least (  n/2) + (  2n/3). so,

|e| + |f|       n
2

+

  2n
3

=    |e|       n
2

+

3  2n
10

.

we must have |e| |u|       n2/2. now, |e||u| = |e|(n   |e|   |f|) increases as |e| increases
from   n

to n/3, so we have

2 + 3  2n

10

(cid:18)   n

2

(cid:19)(cid:18)

+

3  2n
10

n       n
2

    3  2n
10

      2n
30

>

  n2
2

,

(cid:19)

|e||u|    

a contradiction.

8.3.2 no other large components

we now argue that for p = (1 +  )/n for constant   > 0, with high id203 there is
only one giant component, and in fact all other components have size o(log n).

we begin with a preliminary observation. suppose that a g(n, p) graph had at least
a    id203 of having two (or more) components of size   (log n), i.e., asymptotically
greater than log n. then, there would be at least a   /2 id203 of the graph having
two (or more) components with   (log n) vertices inside the subset a = {1, 2, . . . ,  n/2}.
the reason is that an equivalent way to construct a graph g(n, p) is to    rst create it in the
usual way and then to randomly permute the vertices. any component of size   (log n)
will with high id203 after permutation have at least an  /4 fraction of its vertices
within the    rst  n/2. thus, it su   ces to prove that with high id203 at most one
component has   (log n) vertices within the set a to conclude that with high id203

263

the graph has only one component with   (log n) vertices overall.

we now prove that with high id203, a g(n, p) graph for p = (1 +  )/n has at
most one component with   (log n) vertices inside the set a. to do so, let b be the set
of (1      /2)n vertices not in a. now, construct the graph as follows. first, randomly
   ip coins of bias p to generate the edges within set a and the edges within set b. at
this point, with high id203, b has at least one giant component, by the argument
from section 8.3.1, since p = (1 +  )/n     (1 +  /4)/|b| for 0 <       1/2. let c    be
a giant component inside b. now,    ip coins of bias p to generate the edges between a
and b except for those incident to c   . at this point, let us name all components with
  (log n) vertices inside a as c1, c2, c3, . . .. finally,    ip coins of bias p to generate the
edges between a and c   .

in the    nal step above, notice that with high id203, each ci is connected to c   .
in particular, there are   (n log n) possible edges between any given ci and c   , each one
of which is present with id203 p. thus the id203 that this particular ci is not
connected to c    is at most (1     p)  (n log n) = 1/n  (1). thus, by the union bound, with
high id203 all such ci are connected to c   , and there is only one component with
  (log n) vertices within a as desired.

8.3.3 the case of p < 1/n

when p < 1/n, then with high id203 all components in g(n, p) are of size o(log n).
this is easiest to see by considering a variation on the above dfs that (a) begins with
f containing a speci   c start vertex ustart, and then (b) when a vertex u is taken from
f to explore, it pops u o    of f , explores u fully by querying to    nd all edges between
u and u , and then pushes the endpoints v of those edges onto f . thus, this is like an
explicit-stack version of dfs, compared to the previous recursive-call version of dfs. let us
call the exploration of such a vertex u a step. to make this process easier to analyze, let
us say that if f ever becomes empty, we create a brand-new, fake    red vertex   , connect it
to each vertex in u with id203 p, place the new red vertex into f , and then continue
the dfs from there.

let zk denote the number of real (non-red) vertices discovered after k steps, not in-
cluding ustart. for any given real vertex u (cid:54)= ustart, the id203 that u is not discovered
in k steps is (1     p)k, and notice that these events are independent over the di   erent ver-

tices u (cid:54)= ustart. therefore, the distribution of zk is binomial(cid:0)n     1, 1     (1     p)k(cid:1). note

that if zk < k then the process must have required creating a fake red vertex by step k,
meaning that ustart is in a component of size at most k. thus, it su   ces to prove that
prob(zk     k) < 1/n2, for k = c ln n for a suitably large constant c, to then conclude by
union bound over choices of ustart that with high id203 all vertices are in components
of size at most c ln n.

264

to prove that prob(zk     k) < 1/n2 for k = c ln n, we use the fact that (1   p)k     1   pk
so 1     (1     p)k     pk. so, the id203 that zk is greater than or equal to k is at most
the id203 that a coin of bias pk    ipped n     1 times will have at least k heads. but
since pk(n     1)     (1      )k for some constant   > 0, by cherno    bounds this id203
is at most e   c0k for some constant c0 > 0. when k = c ln n for a suitably large constant
c, this id203 is at most 1/n2, as desired.

8.4 cycles and full connectivity

this section considers when cycles form and when the graph becomes fully connected.
for both of these problems, we look at each subset of k vertices and see when they form
either a cycle or when they form a connected component.

8.4.1 emergence of cycles

the emergence of cycles in g (n, p) has a threshold when p equals to 1/n. however,

the threshold is not sharp.

theorem 8.9 the threshold for the existence of cycles in g (n, p) is p = 1/n.

proof: let x be the number of cycles in g (n, p). to form a cycle of length k, the vertices

can be selected in (cid:0)n
(cid:1) ways. given the k vertices of the cycle, they can be ordered by
(cid:1) (k   1)!
there are(cid:0)n
(cid:19)
(cid:18)n
n(cid:88)

arbitrarily selecting a    rst vertex, then a second vertex in one of k-1 ways, a third in one
of k     2 ways, etc. since a cycle and its reversal are the same cycle, divide by 2. thus,

possible cycles of length k and

(k   1)!

k

k

2

2 pk     n(cid:80)

2k pk     n(cid:80)

nk

(np)k = (np)3 1   (np)n   2

1   np     2(np)3,

e (x) =

k

k=3

k=3

k=3

n(cid:80)

provided that np < 1/2. when p is asymptotically less than 1/n, then lim

n       np = 0 and
(np)k = 0. so, as n goes to in   nity, e(x) goes to zero. thus, the graph almost

lim
n      
surely has no cycles by the    rst moment method. a second moment argument can be
used to show that for p = d/n, d > 1, a graph will have a cycle with id203 tending
to one.

k=3

the argument above does not yield a sharp threshold since we argued that e(x)     0
n . a sharp threshold requires

only under the assumption that p is asymptotically less than 1
e(x)     0 for p = d/n, d < 1.

265

property

threshold

cycles
giant component
giant component
+ isolated vertices
connectivity, disappearance
of isolated vertices

diameter two

1/n
1/n

1
2

ln n

n

ln n

(cid:113) 2 ln n

n

n

table 2: thresholds for various properties

consider what happens in more detail when p = d/n, d a constant.

(cid:18)n
(cid:19)(k     1)!
n(cid:88)
n(cid:88)
n(cid:88)

k=3
1
2

k=3

2

pk

k!

k=3

nk

1
2

k
n(n     1)       (n     k + 1)

n(n     1)       (n     k + 1)

e (x) =

=

=

(k     1)! pk

dk
k

.

n(cid:80)

e (x) converges if d < 1, and diverges if d     1. if d < 1, e (x)     1

equals a constant greater than zero. if d = 1, e (x) = 1
2

only the    rst log n terms of the sum. since
n(n   1)      (n   k+1)

    1/2. thus,

nk

n

n   i = 1 + i

k=3
then, in the limit as n goes to in   nity

e (x)     1

2

n(n   1)      (n   k+1)

nk

1

k     1

4

1
k .

k=3

2

nk

k=3

k=3

n       e (x)

n(n   1)      (n   k+1)

dk
k and lim

n(cid:80)
1
k . consider
n   i     ei/n   i, it follows that
log n(cid:80)

n       e (x)     lim
n      

lim

1
4

1

k     lim

n       (log log n) =    .

log n(cid:80)

log n(cid:80)

k=3

for p = d/n, d < 1, e (x) converges to a nonzero constant. for d > 1, e(x) converges
to in   nity and a second moment argument shows that graphs will have an unbounded
number of cycles increasing with n.

8.4.2 full connectivity

as p increases from p = 0, small components form. at p = 1/n a giant component
emerges and swallows up smaller components, starting with the larger components and

266

ending up swallowing isolated vertices forming a single connected component at p = ln n
n ,
at which point the graph becomes connected. we begin our development with a technical
lemma.

lemma 8.10 the expected number of connected components of size k in g(n, p) is at
most

(cid:18)n
(cid:19)

k

kk   2pk   1(1     p)kn   k2.

proof: the id203 that k vertices form a connected component consists of the prod-
uct of two probabilities. the    rst is the id203 that the k vertices are connected,
and the second is the id203 that there are no edges out of the component to the
remainder of the graph. the    rst id203 is at most the sum over all spanning trees
of the k vertices, that the edges of the spanning tree are present. the    at most    in the
lemma statement is because g (n, p) may contain more than one spanning tree on these
nodes and, in this case, the union bound is higher than the actual id203. there are
kk   2 spanning trees on k nodes. see section 12.10.5 in the appendix. the id203 of
all the k     1 edges of one spanning tree being present is pk   1 and the id203 that
there are no edges connecting the k vertices to the remainder of the graph is (1     p)k(n   k).
thus, the id203 of one particular set of k vertices forming a connected component
is at most kk   2pk   1 (1     p)kn   k2
. thus, the expected number of connected components of

size k is at most(cid:0)n

(cid:1)kk   2pk   1(1     p)kn   k2.

k

we now prove that for p = 1
2
nents except for isolated vertices.

ln n
n , the giant component has absorbed all small compo-

theorem 8.11 for p = c ln n
and a giant component. for c > 1, almost surely the graph is connected.

n with c > 1/2, almost surely there are only isolated vertices

proof: we prove that almost surely for c > 1/2, there is no connected component with
k vertices for any k, 2     k     n/2. this proves the    rst statement of the theorem since, if
there were two or more components that are not isolated vertices, both of them could not
be of size greater than n/2. the second statement that for c > 1 the graph is connected
then follows from theorem 8.6 which states that isolated vertices disappear at c = 1.

we now show that for p = c ln n

n , the expected number of components of size k,
2     k     n/2, is less than n1   2c and thus for c > 1/2 there are no components, except
for isolated vertices and the giant component. let xk be the number of connected com-
ponents of size k. substitute p = c ln n
and simplify using

(cid:0)n
(cid:1)     (en/k)k, 1     p     e   p, k     1 < k, and x = eln x to get

(cid:1)kk   2pk   1 (1     p)kn   k2

n into (cid:0)n

k

k

(cid:18)

e(xk)     exp

(cid:19)

.

ln n + k + k ln ln n     2 ln k + k ln c     ck ln n + ck2 ln n
n

267

keep in mind that the leading terms here for large k are the last two and, in fact, at k = n,
they cancel each other so that our argument does not prove the fallacious statement for
c     1 that there is no connected component of size n, since there is. let

f (k) = ln n + k + k ln ln n     2 ln k + k ln c     ck ln n + ck2 ln n
n

.

di   erentiating with respect to k,

f(cid:48)(k) = 1 + ln ln n     2
k

+ ln c     c ln n +

2ck ln n

n

and

f(cid:48)(cid:48) (k) =

2
k2 +

2c ln n

n

> 0.

thus, the function f (k) attains its maximum over the range [2, n/2] at one of the extreme
points 2 or n/2. at k = 2, f (2)     (1     2c) ln n and at k = n/2, f (n/2)        c n
4 ln n. so
f (k) is maximum at k = 2. for k = 2, e(xk) = ef (k) is approximately e(1   2c) ln n = n1   2c
and is geometrically falling as k increases from 2. at some point e(xk) starts to increase
but never gets above n    c
4 n. thus, the expected sum of the number of components of size
k, for 2     k     n/2 is

       n/2(cid:88)

       = o(n1   2c).

e

xk

k=2

this expected number goes to zero for c > 1/2 and the    rst-moment method implies that,
almost surely, there are no components of size between 2 and n/2. this completes the
proof of theorem 8.11.

8.4.3 threshold for o(ln n) diameter

we now show that within a constant factor of the threshold for graph connectivity, not
n for su   ciently

only is the graph connected, but its diameter is o(ln n). that is, if p > c ln n
large constant c, the diameter of g(n, p) is o(ln n) with high id203.

consider a particular vertex v. let si be the set of vertices at distance i from v. we
argue that as i increases, with high id203 |s1| + |s2| +        + |si| grows by at least a
factor of two, up to a size of n/1000. this implies that in o(ln n) steps, at least n/1000
vertices are connected to v. then, there is a simple argument at the end of the proof of
theorem 8.13 that a pair of n/1000 sized subsets, connected to two di   erent vertices v
and w, have an edge between them with high id203.

lemma 8.12 consider g(n, p) for su   ciently large n with p = c ln n
n for any c > 0. let
si be the set of vertices at distance i from some    xed vertex v. if |s1| + |s2| +        + |si|    
n/1000, then

prob(cid:0)|si+1| < 2(|s1| + |s2| +        + |si|)(cid:1)     e   10|si|.

268

proof: let |si| = k. for each vertex u not in s1     s2     . . .     si, the id203 that
u is not in si+1 is (1     p)k and these events are independent. so, |si+1| is the sum of
n    (|s1| +|s2| +       +|si|) independent bernoulli random variables, each with id203
of

1     (1     p)k     1     e   ck ln n/n

of being one. note that n     (|s1| + |s2| +        + |si|)     999n/1000. so,

e(|si+1|)     999n
1000

(1     e   ck ln n
n ).

subtracting 200k from each side

e(|si+1|)     200k     n
2

1     e   ck ln n

n     400

(cid:18)

(cid:19)

.

k
n

n and f (  ) = 1     e   c   ln n     400  . by di   erentiation f(cid:48)(cid:48)(  )     0, so f is concave

let    = k
and the minimum value of f over the interval [0, 1/1000] is attained at one of the end
points. it is easy to check that both f (0) and f (1/1000) are greater than or equal to
zero for su   ciently large n. thus, f is nonnegative throughout the interval proving that
e(|si+1|)     200|si|. the lemma follows from cherno    bounds.
theorem 8.13 for p     c ln n/n, where c is a su   ciently large constant, almost surely,
g(n, p) has diameter o(ln n).

proof: by corollary 8.2, almost surely, the degree of every vertex is    (np) =    (ln n),
which is at least 20 ln n for c su   ciently large. assume that this holds. so, for a    xed
vertex v, s1 as de   ned in lemma 8.12 satis   es |s1|     20 ln n.

is at most(cid:80)n/1000

let i0 be the least i such that |s1|+|s2|+      +|si| > n/1000. from lemma 8.12 and the
union bound, the id203 that for some i, 1     i     i0   1, |si+1| < 2(|s1|+|s2|+      +|si|)
k=20 ln n e   10k     1/n4. so, with id203 at least 1     (1/n4), each si+1 is
at least double the sum of the previous sj    s, which implies that in o(ln n) steps, i0 + 1
is reached.

consider any other vertex w. we wish to    nd a short o(ln n) length path between
v and w. by the same argument as above, the number of vertices at distance o(ln n)
from w is at least n/1000. to complete the argument, either these two sets intersect in
which case we have found a path from v to w of length o(ln n) or they do not intersect.
in the latter case, with high id203 there is some edge between them. for a pair of
disjoint sets of size at least n/1000, the id203 that none of the possible n2/106 or
more edges between them is present is at most (1   p)n2/106 = e      (n ln n). there are at most
22n pairs of such sets and so the id203 that there is some such pair with no edges
is e      (n ln n)+o(n)     0. note that there is no conditioning problem since we are arguing
this for every pair of such sets. think of whether such an argument made for just the n
subsets of vertices, which are vertices at distance at most o(ln n) from a speci   c vertex,
would work.

269

8.5 phase transitions for increasing properties

for many graph properties such as connectivity, having no isolated vertices, having a
cycle, etc., the id203 of a graph having the property increases as edges are added to
the graph. such a property is called an increasing property. q is an increasing property
of graphs if when a graph g has the property, any graph obtained by adding edges to g
must also have the property. in this section we show that any increasing property has a
threshold, although not necessarily a sharp one.

the notion of increasing property is de   ned in terms of adding edges. the following
intuitive lemma proves that if q is an increasing property, then increasing p in g (n, p)
increases the id203 of the property q.
lemma 8.14 if q is an increasing property of graphs and 0     p     q     1, then the
id203 that g (n, q) has property q is greater than or equal to the id203 that
g (n, p) has property q.

proof: this proof uses an interesting relationship between g (n, p) and g (n, q). generate
g (n, q) as follows. first generate g (n, p). this means generating a graph on n vertices
and
with edge probabilities p. then, independently generate another graph g
take the union by including an edge if either of the two graphs has the edge. call the
resulting graph h. the graph h has the same distribution as g (n, q). this follows since
the id203 that an edge is in h is p + (1    p) q   p
1   p = q, and, clearly, the edges of h are
independent. the lemma follows since whenever g (n, p) has the property q, h also has
the property q.

n, q   p
1   p

(cid:16)

(cid:17)

we now introduce a notion called replication. an m-fold replication of g(n, p) is a
random graph obtained as follows. generate m independent copies of g(n, p) on the
same set of vertices. include an edge in the m-fold replication if the edge is in any one
of the m copies of g(n, p). the resulting random graph has the same distribution as
g(n, q) where q = 1     (1     p)m since the id203 that a particular edge is not in the
m-fold replication is the product of probabilities that it is not in any of the m copies
of g(n, p). if the m-fold replication of g(n, p) does not have an increasing property q,
then none of the m copies of g(n, p) has the property. the converse is not true. if no
copy has the property, their union may have it. since q is an increasing property and
q = 1     (1     p)m     1     (1     mp) = mp

prob(cid:0)g(n, mp) has q(cid:1)     prob(cid:0)g(n, q) has q(cid:1)

(8.3)

we now show that every increasing property q has a phase transition. the transition
occurs at the point p(n) at which the id203 that g(n, p(n)) has property q is 1
2.
we will prove that for any function asymptotically less then p(n) that the id203 of
having property q goes to zero as n goes to in   nity.

270

(cid:124)

(cid:123)(cid:122)

copies of g

(cid:125)

if any graph has three or more edges, then the
m-fold replication has three or more edges.

the m-fold
replication h

(cid:124)

(cid:123)(cid:122)

copies of g

(cid:125)

even if no graph has three or more edges, the
m-fold replication might have three or more edges.

the m-fold
replication h

figure 8.9: the property that g has three or more edges is an increasing property. let
h be the m-fold replication of g. if any copy of g has three or more edges, h has three
or more edges. however, h can have three or more edges even if no copy of g has three
or more edges.

theorem 8.15 each increasing property q of g(n, p) has a phase transition at p(n),
where for each n, p(n) is the minimum real number an for which the id203 that
g(n, an) has property q is 1/2.

proof: let p0(n) be any function such that

lim
n      

p0(n)
p(n)

= 0.

we assert that almost surely g(n, p0) does not have the property q. suppose for con-
tradiction, that this is not true. that is, the id203 that g(n, p0) has the property
q does not converge to zero. by the de   nition of a limit, there exists    > 0 for which
the id203 that g(n, p0) has property q is at least    on an in   nite set i of n. let
m = (cid:100)(1/  )(cid:101). let g(n, q) be the m-fold replication of g(n, p0). the id203 that
g(n, q) does not have q is at most (1       )m     e   1     1/2 for all n     i. for these n, by
(11.4)

prob(g(n, mp0) has q)     prob(g(n, q) has q)     1/2.

since p(n) is the minimum real number an for which the id203 that g(n, an) has
property q is 1/2, it must be that mp0(n)     p(n). this implies that p0(n)
is at least 1/m
in   nitely often, contradicting the hypothesis that lim
n      

p0(n)
p(n) = 0.

p(n)

271

a symmetric argument shows that for any p1(n) such that lim
n      

almost surely has property q.

p(n)

p1(n) = 0, g(n, p1)

8.6 branching processes

a branching process is a method for creating a random tree. starting with the root
node, each node has a id203 distribution for the number of its children. the root of
the tree is a parent and its descendants are the children with their descendants being the
grandchildren. the children of the root are the    rst generation, their children the second
generation, and so on. branching processes have obvious applications in population stud-
ies.

we analyze a simple case of a branching process where the distribution of the number
of children at each node in the tree is the same. the basic question asked is what is the
id203 that the tree is    nite, i.e., the id203 that the branching process dies out?
this is called the extinction id203.

our analysis of the branching process will give the id203 of extinction, as well

as the expected size of the components conditioned on extinction.

an important tool in our analysis of branching processes is the generating func-
tion. the generating function for a nonnegative integer valued random variable y is
pixi where pi is the id203 that y equals i. the reader not familiar with

f (x) =

   (cid:80)

i=0

generating functions should consult section 12.9 of the appendix.

let the random variable zj be the number of children in the jth generation and let
fj (x) be the generating function for zj. then f1 (x) = f (x) is the generating function for
the    rst generation where f (x) is the generating function for the number of children at a
node in the tree. the generating function for the 2nd generation is f2(x) = f (f (x)). in
general, the generating function for the j + 1st generation is given by fj+1 (x) = fj (f (x)).
to see this, observe two things.

first, the generating function for the sum of two identically distributed integer valued

random variables x1 and x2 is the square of their generating function

f 2 (x) = p2

0 + (p0p1 + p1p0) x + (p0p2 + p1p1 + p2p0) x2 +        .

for x1 + x2 to have value zero, both x1 and x2 must have value zero, for x1 + x2 to have
value one, exactly one of x1 or x2 must have value zero and the other have value one, and
so on. in general, the generating function for the sum of i independent random variables,
each with generating function f (x), is f i (x).

272

m < 1

f (x)

m > 1

p0

m = 1 and p1 < 1

q

x

figure 8.10: illustration of the root of equation f (x) = x in the interval [0,1).

   (cid:88)

the second observation is that the coe   cient of xi in fj (x) is the id203 of
there being i children in the jth generation. if there are i children in the jth generation,
the number of children in the j + 1st generation is the sum of i independent random
variables each with generating function f (x). thus, the generating function for the j + 1st
generation, given i children in the jth generation, is f i(x). the generating function for
the j + 1st generation is given by

   (cid:80)

i=0

if fj(x) =

fj+1(x) =

prob(zj = i)f i(x).

i=0

aixi, then fj+1 is obtained by substituting f (x) for x in fj(x).

since f (x) and its iterates, f2, f3, . . ., are all polynomials in x with nonnegative co-
e   cients, f (x) and its iterates are all monotonically increasing and convex on the unit
interval. since the probabilities of the number of children of a node sum to one, if p0 < 1,
some coe   cient of x to a power other than zero in f (x) is nonzero and f (x) is strictly
increasing.

let q be the id203 that the branching process dies out. if there are i children
in the    rst generation, then each of the i subtrees must die out and this occurs with
id203 qi. thus, q equals the summation over all values of i of the product of the
id203 of i children times the id203 that i subtrees will die out. this gives

i=0 piqi. thus, q is the root of x =(cid:80)   

q =(cid:80)   

i=0 pixi, that is x = f (x).

this suggests focusing on roots of the equation f (x) = x in the interval [0,1]. the value

x = 1 is always a root of the equation f (x) = x since f (1) =
pi = 1. when is there a
smaller nonnegative root? the derivative of f (x) at x = 1 is f(cid:48)(1) = p1 + 2p2 + 3p3 +       .
let m = f(cid:48)(1). thus, m is the expected number of children of a node. if m > 1, one
might expect the tree to grow forever, since each node at time j is expected to have more

i=0

   (cid:80)

273

f (x)

q

p0

f (f (x))

f (x)

x

figure 8.11: illustration of convergence of the sequence of iterations f1(x), f2(x), . . . to
q.

than one child. but this does not imply that the id203 of extinction is zero. in fact,
if p0 > 0, then with positive id203, the root will have no children and the process
will become extinct right away. recall that for g(n, d
n), the expected number of children
is d, so the parameter m plays the role of d.

if m < 1, then the slope of f (x) at x = 1 is less than one. this fact along with
convexity of f (x) implies that f (x) > x for x in [0, 1) and there is no root of f (x) = x in
the interval [0, 1).
if m = 1 and p1 < 1, then once again convexity implies that f (x) > x for x     [0, 1)
and there is no root of f (x) = x in the interval [0, 1). if m = 1 and p1 = 1, then f (x) is
the straight line f (x) = x.

if m >1, then the slope of f (x) is greater than the slope of x at x = 1. this fact,
along with convexity of f (x), implies f (x) = x has a unique root in [0,1). when p0 = 0,
the root is at x = 0.

let q be the smallest nonnegative root of the equation f (x) = x. for m < 1 and for
m=1 and p0 < 1, q equals one and for m >1, q is strictly less than one. we shall see
that the value of q is the extinction id203 of the branching process and that 1     q is
the immortality id203. that is, q is the id203 that for some j, the number of
children in the jth generation is zero. to see this, note that for m > 1, lim
j       fj (x) = q for
0     x < 1. figure 8.11 illustrates the proof which is given in lemma 8.16. similarly note
that when m < 1 or m = 1 with p0 < 1, fj (x) approaches one as j approaches in   nity.

274

   (cid:80)

lemma 8.16 assume m > 1. let q be the unique root of f(x)=x in [0,1). in the limit as
j goes to in   nity, fj (x) = q for x in [0, 1).
proof: if 0     x     q, then x < f (x)     f (q) and iterating this inequality

x < f1 (x) < f2 (x) <        < fj (x) < f (q) = q.

clearly, the sequence converges and it must converge to a    xed point where f (x) = x.
similarly, if q     x < 1, then f (q)     f (x) < x and iterating this inequality

x > f1 (x) > f2 (x) >        > fj (x) > f (q) = q.

in the limit as j goes to in   nity fj (x) = q for all x, 0     x < 1. that is

j       fj(x) = q + 0x + 0x2 +       

lim

and there are no children with id203 q and no    nite number of children with prob-
ability zero.

recall that fj (x) is the generating function

prob (zj = i) xi. the fact that in the

limit the generating function equals the constant q, and is not a function of x, says that
prob (zj = 0) = q and prob (zj = i) = 0 for all    nite nonzero values of i. the remaining
id203 is the id203 of a non   nite component. thus, when m >1, q is the
extinction id203 and 1-q is the id203 that zj grows without bound.

i=0

theorem 8.17 consider a tree generated by a branching process. let f (x) be the gener-
ating function for the number of children at each node.

1. if the expected number of children at each node is less than or equal to one, then the

id203 of extinction is one unless the id203 of exactly one child is one.

2. if the expected number of children of each node is greater than one, then the proba-

bility of extinction is the unique solution to f (x) = x in [0, 1).

proof: let pi be the id203 of i children at each node. then f (x) = p0 + p1x +
p2x2 +        is the generating function for the number of children at each node and f(cid:48)(1) =
p1 + 2p2 + 3p3 +       
is the slope of f (x) at x = 1. observe that f(cid:48)(1) is the expected
number of children at each node.

since the expected number of children at each node is the slope of f (x) at x = 1, if
the expected number of children is less than or equal to one, the slope of f (x) at x = 1
is less than or equal to one and the unique root of f (x) = x in (0, 1] is at x = 1 and the
id203 of extinction is one unless f(cid:48)(1) = 1 and p1 = 1. if f(cid:48)(1) = 1 and p1 = 1,
f (x) = x and the tree is an in   nite degree one chain. if the slope of f (x) at x = 1 is
greater than one, then the id203 of extinction is the unique solution to f (x) = x in
[0, 1).

275

a branching process can be viewed as the process of creating a component in an in   -
nite graph. in a    nite graph, the id203 distribution of descendants is not a constant
as more and more vertices of the graph get discovered.

the simple branching process de   ned here either dies out or goes to in   nity. in bio-
logical systems there are other factors, since processes often go to stable populations. one
possibility is that the id203 distribution for the number of descendants of a child
depends on the total population of the current generation.

expected size of extinct families

we now show that the expected size of an extinct family is    nite, provided that m (cid:54)= 1.
note that at extinction, the size must be    nite. however, the expected size at extinction
could conceivably be in   nite, if the id203 of dying out did not decay fast enough. for
example, suppose that with id203 1
2 it became extinct with size 3, with id203
1
4 it became extinct with size 9, with id203 1
8 it became extinct with size 27, etc. in
such a case the expected size at extinction would be in   nite even though the process dies
out with id203 one. we now show this does not happen.
lemma 8.18 if the slope m = f(cid:48) (1) does not equal one, then the expected size of an
extinct family is    nite. if the slope m equals one and p1 = 1, then the tree is an in   nite
degree one chain and there are no extinct families. if m=1 and p1 < 1, then the expected
size of the extinct family is in   nite.

proof: let zi be the random variable denoting the size of the ith generation and let q be
the id203 of extinction. the id203 of extinction for a tree with k children in
the    rst generation is qk since each of the k children has an extinction id203 of q.
note that the expected size of z1, the    rst generation, over extinct trees will be smaller
than the expected size of z1 over all trees since when the root node has a larger number
of children than average, the tree is more likely to be in   nite.

by bayes rule
prob (z1 = k|extinction) = prob (z1 = k)

prob (extinction|z1 = k)

prob (extinction)

= pk

qk
q

= pkqk   1.

knowing the id203 distribution of z1 given extinction, allows us to calculate the
expected size of z1 given extinction.

   (cid:88)

e (z1|extinction) =

kpkqk   1 = f(cid:48) (q) .

we now prove, using independence, that the expected size of the ith generation given
extinction is

k=0

276

(cid:16)

(cid:17)i

f(cid:48) (q)

.

e (zi|extinction) =

for i = 2, z2 is the sum of z1 independent random variables, each independent of the ran-
dom variable z1. so, e(z2|z1 = j and extinction) = e( sum of j copies of z1|extinction) =
je(z1|extinction). summing over all values of j

   (cid:88)
   (cid:88)

j=1

e(z2|extinction) =

=

e(z2|z1 = j and extinction)prob(z1 = j|extinction)

je(z1|extinction)prob(z1 = j|extinction)

j=1

= e(z1|extinction)

   (cid:88)

j=1

jprob(z1 = j|extinction) = e2(z1|extinction).

since e(z1|extinction) = f(cid:48)(q), e (z2|extinction) = (f(cid:48) (q))2. similarly, e (zi|extinction) =
(f(cid:48) (q))i . the expected size of the tree is the sum of the expected sizes of each generation.
that is,

   (cid:88)

   (cid:88)

expected size of
tree given extinction

=

e (zi|extinction) =

i=0

i=0

(f(cid:48) (q))i =

1

1     f(cid:48) (q)

.

thus, the expected size of an extinct family is    nite since f(cid:48) (q) < 1 provided m (cid:54)= 1.

the fact that f(cid:48)(q) < 1 is illustrated in figure 8.10. if m <1, then q=1 and f(cid:48)(q) = m
is less than one. if m >1, then q     [0, 1) and again f(cid:48)(q) <1 since q is the solution to
f (x) = x and f(cid:48)(q) must be less than one for the curve f (x) to cross the line x. thus,
for m <1 or m >1, f(cid:48)(q) <1 and the expected tree size of
1   f(cid:48)(q) is    nite. for m=1 and
p1 < 1, one has q=1 and thus f(cid:48)(q) = 1 and the formula for the expected size of the tree
diverges.

1

8.7 cnf-sat

phase transitions occur not only in random graphs, but in other random structures
as well. an important example is that of satis   ability of boolean formulas in conjunctive
normal form. a conjunctive normal form (cnf) formula over n variables x1, . . . , xn is
an and of ors of literals, where a literal is a variable or its negation. for example, the
following is a cnf formula over the variables {x1, x2, x3, x4}:

(x1       x2     x3)(x2       x4)(x1     x4)(x3     x4)(x2       x3     x4).

each or of literals is called a clause; for example, the above formula has    ve clauses. a
k-cnf formula is a cnf formula in which each clause has size at most k, so the above
formula is a 3-cnf formula. an assignment of true/false values to variables is said to

277

satisfy a cnf formula if it satis   es every clause in it. setting all variables to true satis   es
the above cnf formula, and in fact this formula has multiple satisfying assignments. a
formula is said to be satis   able it there exists at least one assignment of truth values to
variables that satis   es it.

many important problems can be converted into questions of    nding satisfying as-
signments of cnf formulas.
indeed, the cnf-sat problem of whether a given cnf
formula is satis   able is np-complete, meaning that any problem in the class np can be
converted into it. as a result, it is believed to be highly unlikely that there will ever
exist an e   cient algorithm for worst-case instances. however, there are solvers that turn
out to work very well in practice on instances arising from a wide range of applications.
there is also substantial structure and understanding of the satis   ability of random cnf
formulas. the next two sections discuss each in turn.

8.7.1 sat-solvers in practice

while the sat problem is np-complete, a number of algorithms have been developed
that perform extremely well in practice on sat formulas arising in a range of applica-
tions. such applications include hardware and software veri   cation, creating action plans
for robots and robot teams, solving combinatorial puzzles, and even proving mathematical
theorems.

broadly, there are two classes of solvers: complete solvers and incomplete solvers. com-
plete solvers are guaranteed to    nd a satisfying assignment whenever one exists; if they
do not return a solution, then you know the formula is not satis   able. complete solvers
are often based on some form of recursive tree search. incomplete solvers instead make a
   best e   ort   ; they are typically based on some local-search heuristic, and they may fail
to output a solution even when a formula is satis   able. however, they are typically much
faster than complete solvers.

an example of a complete solver is the following dpll (davis-putnam-logemann-
loveland) style procedure. first, if there are any variables xi that never appear in negated
form in any clause, then set those variables to true and delete clauses where the literal xi
appears. similarly, if there are any xi that only appear in negated form, then set those
variables to false and delete clauses where the literal   xi appears. second, if there are
any clauses that have only one literal in them (such clauses are called unit clauses), then
set that literal as needed to satisfy the clause. e.g., if the clause was    (  x3)    then one
would set x3 to false. then remove that clause along with any other clause containing
that literal, and shrink any clause containing the negation of that literal (e.g., a clause
such as (x3     x4) would now become just (x4), and one would then run this rule again
on this clause). finally, if neither of the above two cases applies, then one chooses some
literal and recursively tries both settings for it. speci   cally, choose some literal (cid:96) and re-
cursively check if the formula is satis   able conditioned on setting (cid:96) to true; if the answer

278

is    yes    then we are done, but if the answer is    no    then recursively check if the formula
is satis   able conditioned on setting (cid:96) to false. notice that this procedure is guaranteed to
   nd a satisfying assignment whenever one exists.

an example of an incomplete solver is the following local-search procedure called
walksat. walksat begins with a random assignment of truth-values to variables. if this
happens to satisfy the formula, then it outputs success.
if not, then it chooses some
unsatis   ed clause c at random. if c contains some variable xi whose truth-value can
be    ipped (causing c to be satis   ed) without causing any other clause to be unsatis   ed,
then xi   s truth-value is    ipped. otherwise, walksat either (a)    ips the truth-value of the
variable in c that causes the fewest other clauses to become unsatis   ed, or else (b)    ips
the truth-value of a random xi in c; the choice of whether to perform (a) or (b) is deter-
mined by    ipping a coin of bias p. thus, walksat is performing a kind of random walk
in the space of truth-assignments, hence the name. walksat also has two time-thresholds
tf lips and trestarts. if the above procedure has not found a satisfying assignment after
tf lips    ips, it then restarts with a fresh initial random assignment and tries again; if that
entire process has not found a satisfying assignment after trestarts restarts, then it outputs
   no assignment found   .

the above solvers are just two simple examples. due to the importance of the cnf-
sat problem, development of faster sat-solvers is an active area of computer science
research. sat-solving competitions are held each year, and solvers are routinely being
used to solve challenging veri   cation, planning, and scheduling problems.

8.7.2 phase transitions for cnf-sat

we now consider the question of phase transitions in the satis   ability of random k-

cnf formulas.

generate a random cnf formula f with n variables, m clauses, and k literals per
clause, where recall that a literal is a variable or its negation. speci   cally, each clause

in f is selected independently at random from the set of all(cid:0)n

(cid:1)2k possible clauses of size

k. equivalently, to generate a clause, choose a random set of k distinct variables, and
then for each of those variables choose to either negate it or not with equal probabil-
ity. here, the number of variables n is going to in   nity, m is a function of n, and k is
a    xed constant. a reasonable value to think of for k is k = 3. unsatis   ability is an
increasing property since adding more clauses preserves unsatis   ability. by arguments
similar to section 8.5, there is a phase transition, i.e., a function m(n) such that if m1(n)
is o(m(n)), a random formula with m1(n) clauses is, almost surely, satis   able and for
m2(n) with m2(n)/m(n)        , a random formula with m2(n) clauses is, almost surely,
unsatis   able. it has been conjectured that there is a constant rk independent of n such
that rkn is a sharp threshold.

k

279

assignments, the expected number of satisfying assignments for a formula with cn clauses

here we derive upper and lower bounds on rk. it is relatively easy to get an upper
bound on rk. a    xed truth assignment satis   es a random k clause with id203
1     1
2k because of the 2k truth assignments to the k variables in the clause, only one
fails to satisfy the clause. thus, with id203 1
2k , the clause is not satis   ed, and with
id203 1     1
2k , the clause is satis   ed. let m = cn. now, cn independent clauses are

(cid:1)cn. if c = 2k ln 2, the expected number of satisfying assignments is

all satis   ed by the    xed assignment with id203(cid:0)1     1
is 2n(cid:0)1     1
(cid:1)2k
(cid:0)1     1

2n(cid:0)1     1
(cid:1)n2k ln 2     2ne   n ln 2 = 2n2   n = 1.

(cid:1)cn. since there are 2n truth

is at most 1/e and approaches 1/e in the limit. thus,

2n(cid:0)1     1

(cid:1)n2k ln 2 .

2k

2k

2k

2k

for c > 2k ln 2, the expected number of satisfying assignments goes to zero as n        .
here the expectation is over the choice of clauses which is random, not the choice of a
truth assignment. from the    rst moment method, it follows that a random formula with
cn clauses is almost surely not satis   able. thus, rk     2k ln 2.

2k

the other direction, showing a lower bound for rk, is not that easy. from now on, we
focus only on the case k = 3. the statements and algorithms given here can be extended
to k     4, but with di   erent constants.
it turns out that the second moment method
cannot be directly applied to get a lower bound on r3 because the variance is too high. a
simple algorithm, called the smallest clause heuristic (abbreviated sc), yields a satisfy-
ing assignment with id203 tending to one if c < 2
3. other more
di   cult to analyze algorithms, push the lower bound on r3 higher.

3, proving that r3     2

the smallest clause heuristic repeatedly executes the following. assign true to a
random literal in a random shortest clause and delete the clause since it is now satis   ed.
in more detail, pick at random a 1-literal clause, if one exists, and set that literal to
true. if there is no 1-literal clause, pick a 2-literal clause, select one of its two literals and
set the literal to true. otherwise, pick a 3-literal clause and a literal in it and set the
literal to true. if we encounter a 0-length clause, then we have failed to    nd a satisfying
assignment; otherwise, we have found one.

a related heuristic, called the unit clause heuristic, selects a random clause with one
literal, if there is one, and sets the literal in it to true. otherwise, it picks a random as
yet unset literal and sets it to true. another variation is the    pure literal    heuristic. it
sets a random    pure literal   , a literal whose negation does not occur in any clause, to
true, if there are any pure literals; otherwise, it sets a random literal to true.

when a literal w is set to true, all clauses containing w are deleted, since they are
satis   ed, and   w is deleted from any clause containing   w. if a clause is reduced to length

280

zero (no literals), then the algorithm has failed to    nd a satisfying assignment to the
formula. the formula may, in fact, be satis   able, but the algorithm has failed.

example: consider a 3-cnf formula with n variables and cn clauses. with n variables
there are 2n literals, since a variable and its complement are distinct literals. the expected
number of times a literal occurs is calculated as follows. each clause has three literals.
thus, each of the 2n di   erent literals occurs (3cn)
2c times on average. suppose c = 5.
then each literal appears 7.5 times on average. if one sets a literal to true, one would
expect to satisfy 7.5 clauses. however, this process is not repeatable since after setting a
literal to true there is conditioning so that the formula is no longer random.

2n = 3

theorem 8.19 if the number of clauses in a random 3-cnf formula grows as cn where
c is a constant less than 2/3, then with id203 1     o(1), the shortest clause (sc)
heuristic    nds a satisfying assignment.

the proof of this theorem will take the rest of the section. a general impediment to
proving that simple algorithms work for random instances of many problems is condition-
ing. at the start, the input is random and has properties enjoyed by random instances.
but, as the algorithm is executed, the data is no longer random; it is conditioned on the
steps of the algorithm so far. in the case of sc and other heuristics for    nding a satisfying
assignment for a boolean formula, the argument to deal with conditioning is relatively
simple.

we supply some intuition before giving the proof. imagine maintaining a queue of 1
and 2-clauses. a 3-clause enters the queue when one of its literals is set to false and it
becomes a 2-clause. sc always picks a 1 or 2-clause if there is one and sets one of its
literals to true. at any step when the total number of 1 and 2-clauses is positive, one of
the clauses is removed from the queue. consider the arrival rate, that is, the expected
number of arrivals into the queue at a given time t. for a particular clause to arrive into
the queue at time t to become a 2-clause, it must contain the negation of the literal being
set to true at time t. it can contain any two other literals not yet set. the number of

(cid:1)22. so, the id203 that a particular clause arrives in the queue at

such clauses is(cid:0)n   t

time t is at most

2

(cid:0)n   t
(cid:1)22
(cid:1)23
(cid:0)n

2

3

   

3

2(n     2)

.

since there are cn clauses in total, the arrival rate is 3c
2 , which for c < 2/3 is a constant
strictly less than one. the arrivals into the queue of di   erent clauses occur independently
(lemma 8.20), the queue has arrival rate strictly less than one, and the queue loses one
or more clauses whenever it is nonempty. this implies that the queue never has too many
clauses in it. a slightly more complicated argument will show that no clause remains as
a 1 or 2-clause for   (ln n) steps (lemma 8.21). this implies that the id203 of two
contradictory 1-length clauses, which is a precursor to a 0-length clause, is very small.

281

lemma 8.20 let ti be the    rst time that clause i turns into a 2-clause. ti is     if clause
i gets satis   ed before turning into a 2-clause. the ti are mutually independent over the
randomness in constructing the formula and the randomness in sc, and for any t,

prob(ti = t)    

3

2(n     2)

.

proof: for the proof, generate the clauses in a di   erent way. the important thing is
that the new method of generation, called the method of    deferred decisions   , results in
the same distribution of input formulae as the original. the method of deferred decisions
is tied in with the sc algorithm and works as follows. at any time, the length of each
clause (number of literals) is all that we know; we have not yet picked which literals are
in each clause. at the start, every clause has length three and sc picks one of the clauses
uniformly at random. now, sc wants to pick one of the three literals in that clause to
set to true, but we do not know which literals are in the clause. at this point, we pick
uniformly at random one of the 2n possible literals. say for illustration, we picked   x102.
the literal   x102 is placed in the clause and set to true. the literal x102 is set to false. we
must also deal with occurrences of the literal or its negation in all other clauses, but again,
we do not know which clauses have such an occurrence. we decide that now. for each
clause, independently, with id203 3/n include either the literal   x102 or its negation
x102, each with id203 1/2. in the case that we included   x102 (the literal we had set
to true), the clause is now deleted, and if we included x102 (the literal we had set to false),
we decrease the residual length of the clause by one.

at a general stage, suppose the fates of i variables have already been decided and
n     i remain. the residual length of each clause is known. among the clauses that are
not yet satis   ed, choose a random shortest length clause. among the n     i variables
remaining, pick one uniformly at random, then pick it or its negation as the new literal.
include this literal in the clause thereby satisfying it. since the clause is satis   ed, the
algorithm deletes it. for each other clause, do the following.
if its residual length is
l, decide with id203 l/(n     i) to include the new variable in the clause and if so
with id203 1/2 each, include it or its negation. if the literal that was set to true is
included in a clause, delete the clause as it is now satis   ed. if its negation is included
in a clause, then just delete the literal and decrease the residual length of the clause by one.

why does this yield the same distribution as the original one? first, observe that the
order in which the variables are picked by the method of deferred decisions is independent
of the clauses; it is just a random permutation of the n variables. look at any one clause.
for a clause, we decide in order whether each variable or its negation is in the clause. so
for a particular clause and a particular triple i, j, and k with i < j < k, the id203
that the clause contains the ith, the jth, and kth literal (or their negations) in the order

282

determined by deferred decisions is:

(cid:0)1     3
(cid:0)1     2
(cid:16)

(cid:1)(cid:0)1     3
(cid:1)(cid:0)1     2
(cid:17)(cid:16)

n   i
1     1
n   j

n   i   1
1     1

(cid:1)      (cid:0)1     3
(cid:1)
(cid:1)      (cid:16)
(cid:17)      (cid:0)1     1

n   i+2
1     2

n   j   1

n   1

n

n   j+2

(cid:17)

3

n   i+1

(cid:1)

2

n   j+1

n   k+2

1

n   k+1 =

3

n(n   1)(n   2),

where the (1            ) factors are for not picking the current variable or negation to be in-
cluded and the others are for including the current variable or its negation. independence
among clauses follows from the fact that we have never let the occurrence or nonoccur-
rence of any variable in any clause in   uence our decisions on other clauses.

now, we prove the lemma by appealing to the method of deferred decisions to generate
the formula. ti = t if and only if the method of deferred decisions does not put the current
literal at steps 1, 2, . . . , t     1 into the ith clause, but puts the negation of the literal at
step t into it. thus, the id203 is precisely

(cid:0)1     3

(cid:1)(cid:0)1     3

n

n   1

(cid:1)      (cid:0)1     3

n   t+2

(cid:1)

1
2

n   t+1     3

2(n   2),

3

as claimed. clearly the ti are independent since again deferred decisions deal with di   er-
ent clauses independently.
lemma 8.21 there exists a constant c2 such that with id203 1     o(1), no clause
remains a 2 or 1-clause for more than c2 ln n steps.
i.e., once a 3-clause becomes a
2-clause, it is either satis   ed or reduced to a 0-clause in o(ln n) steps.

proof: say that t is a    busy time    if there exists at least one 2-clause or 1-clause at time
t, and de   ne a time-window [r + 1, s] to be a    busy window    if time r is not busy but
then each t     [r + 1, s] is a busy time. we will prove that for some constant c2, with
id203 1     o(1), all busy windows have length at most c2 ln n.
fix some r and s and consider the event that [r + 1, s] is a busy window. since sc always
decreases the total number of 1 and 2-clauses by one whenever it is positive, we must have
generated at least s     r new 2-clauses between r and s. now, de   ne an indicator variable
for each 3-clause which has value one if the clause turns into a 2-clause between r and
s. by lemma 8.20 these variables are independent and the id203 that a particular
3-clause turns into a 2-clause at a time t is at most 3/(2(n    2)). summing over t between
r and s,

prob(cid:0)a 3-clause turns into a 2-clause during [r, s](cid:1)     3(s     r)

.

2(n     2)

2(n   2)    
since there are cn clauses in all, the expected sum of the indicator variables is cn 3(s   r)
3c(s   r)
. note that 3c/2 < 1, which implies the arrival rate into the queue of 2 and 1-
clauses is a constant strictly less than one. using cherno    bounds, if s     r     c2 ln n for

2

283

appropriate constant c2, the id203 that more than s     r clauses turn into 2-clauses
between r and s is at most 1/n3. applying the union bound over all o(n2) possible choices
of r and s, we get that the id203 that any clause remains a 2 or 1-clause for more
than c2 ln n steps is o(1).

now, assume the 1     o(1) id203 event of lemma 8.21 that no clause remains a
2 or 1-clause for more than c2 ln n steps. we will show that this implies it is unlikely the
sc algorithm terminates in failure.

suppose sc terminates in failure. this means that at some time t, the algorithm
generates a 0-clause. at time t     1, this clause must have been a 1-clause. suppose the
clause consists of the literal w. since at time t     1, there is at least one 1-clause, the
shortest clause rule of sc selects a 1-clause and sets the literal in that clause to true.
this other clause must have been   w. let t1 be the    rst time either of these two clauses,
w or   w, became a 2-clause. we have t    t1     c2 ln n. clearly, until time t, neither of these
two clauses is picked by sc. so, the literals which are set to true during this period are
chosen independent of these clauses. say the two clauses were w + x + y and   w + u + v
at the start. x, y, u, and v must all be negations of literals set to true during steps t1 to

t. so, there are only o(cid:0)(ln n)4(cid:1) choices for x, y, u, and v for a given value of t. there are
w and   w, and n choices for t. thus, there are o(cid:0)n4(ln n)4(cid:1) choices for what these clauses
id203 that these choices are actually realized is therefore o(cid:0)n4(ln n)4/n6(cid:1) = o(1),

contain and which clauses they are in the input. on the other hand, for any given i and j,
the id203 that clauses i and j both match a given set of literals is o(1/n6). thus the

o(n) choices of w, o(n2) choices of which two clauses i and j of the input become these

as required.

8.8 nonuniform models of random graphs

so far we have considered the g(n, p) random graph model in which all vertices have
the same expected degree, and moreover degrees are concentrated close to their expecta-
tion. however, large graphs occurring in the real world tend to have power law degree
distributions. for a power law degree distribution, the number f (d) of vertices of degree
d scales as 1/d   for some constant    > 0.

one way to generate such graphs is to stipulate that there are f (d) vertices of degree
d and choose uniformly at random from the set of graphs with this degree distribution.
clearly, in this model the graph edges are not independent and this makes these random
graphs harder to analyze. but the question of when phase transitions occur in random
graphs with arbitrary degree distributions is still of interest. in this section, we consider
when a random graph with a nonuniform degree distribution has a giant component. our
treatment in this section, and subsequent ones, will be more intuitive without providing
rigorous proofs.

284

consider a graph in which half of the vertices are degree one and half
are degree two. if a vertex is selected at random, it is equally likely to be
degree one or degree two. however, if we select an edge at random and
walk to a random endpoint, the vertex is twice as likely to be degree
two as degree one.
in many graph algorithms, a vertex is reached
by randomly selecting an edge and traversing the edge to reach an
endpoint. in this case, the id203 of reaching a degree i vertex is
proportional to i  i where   i is the fraction of vertices that are degree
i.

figure 8.12: id203 of encountering a degree d vertex when following a path in a
graph.

8.8.1 giant component in graphs with given degree distribution

molloy and reed address the issue of when a random graph with a nonuniform degree
distribution has a giant component. let   i be the fraction of vertices of degree i. there
will be a giant component if and only if

i(i     2)  i > 0.

   (cid:80)

i=0

to see intuitively that this is the correct formula, consider exploring a component
of a graph starting from a given seed vertex. degree zero vertices do not occur except
in the case where the vertex is the seed.
if a degree one vertex is encountered, then
that terminates the expansion along the edge into the vertex. thus, we do not want to
encounter too many degree one vertices. a degree two vertex is neutral in that the vertex
is entered by one edge and left by the other. there is no net increase in the size of the
frontier. vertices of degree i greater than two increase the frontier by i     2 vertices. the
vertex is entered by one of its edges and thus there are i     1 edges to new vertices in the
frontier for a net gain of i     2. the i  i in (i     2) i  i is proportional to the id203 of
reaching a degree i vertex and the i     2 accounts for the increase or decrease in size of
the frontier when a degree i vertex is reached.

that the summation (cid:80)n
example: consider applying the molloy reed conditions to the g(n, p) model, and use
pi to denote the id203 that a vertex has degree i, i.e., in analog to   i. it turns out
i=0 i(i     2)pi gives value zero precisely when p = 1/n, the point
binomial, where the id203 that a vertex is of degree i is given by pi =(cid:0)n
at which the phase transition occurs. at p = 1/n, the average degree of each vertex is
(cid:1)pi(1     p)n   i when p = 1/n.
n(cid:80)
one and there are n/2 edges. however, the actual degree distribution of the vertices is

i(i     2)pi = 0 for pi =(cid:0)n

(cid:1)pi(1   p)n   i.

i

we now show that lim
n      

i

i=0

285

(cid:19)i(cid:18)

(cid:19)n   i

1     1
n
n(n     1)       (n     i + 1)

(cid:18)

i! ni

n(n     1)       (n     i + 1)

i! ni

(cid:19)n(cid:18)
(cid:19)i

1     1
n

(cid:18) n

n     1

(cid:19)   i

1     1
n

n(cid:88)

i=0

lim
n      

i(i     2)

i

(cid:18)n
(cid:19)(cid:18) 1
n(cid:88)
n(cid:88)

i=0

n

= lim
n      

i(i     2)

i(i     2)

=

   

1
e

   (cid:88)

lim
n      
i(i     2)

i=0

i!

i=0

.

i(i   2)

i! = 0, note that

   (cid:80)

i=0

to see that

   (cid:88)

i=0

i2
i!

=

and

thus,

   (cid:88)
   (cid:88)

i=0

i=1

   (cid:88)

i=1

=

i=1

i
i!

   (cid:88)
   (cid:88)
   (cid:80)

i=0

=

i! =

i=0

i
i!

=

   (cid:80)

i=0

1

(i     1)!

=

   (cid:88)
   (cid:80)

i=0

i=0

i

(i     1)!

i + 1

i!

=

i
i!

+

i(i   2)

i2

i!     2

i
i! = 0.

   (cid:88)
   (cid:88)

i=0

i=0

1
i!

1
i!

= 2

   (cid:88)

i=0

1
i!

.

8.9 growth models

many graphs that arise in the outside world started as small graphs that grew over
time. in a model for such graphs, vertices and edges are added to the graph over time.
in such a model there are many ways in which to select the vertices for attaching a new
edge. one is to select two vertices uniformly at random from the set of existing vertices.
another is to select two vertices with id203 proportional to their degree. this latter
method is referred to as preferential attachment. a variant of this method would be to
add a new vertex at each unit of time and with id203    add an edge where one
end of the edge is the new vertex and the other end is a vertex selected with id203
proportional to its degree. the graph generated by this latter method is a tree with a
power law degree distribution.

286

8.9.1 growth model without preferential attachment

consider a growth model for a random graph without preferential attachment. start
with zero vertices. at each unit of time a new vertex is created and with id203   
two vertices chosen at random are joined by an edge. the two vertices may already have
an edge between them. in this case, we add another edge. so, the resulting structure is a
multi-graph, rather then a graph. since at time t, there are t vertices and in expectation
only o(  t) edges where there are t2 pairs of vertices, it is very unlikely that there will be
many multiple edges.

the degree distribution for this growth model is calculated as follows. the number of
vertices of degree k at time t is a random variable. let dk(t) be the expectation of the
number of vertices of degree k at time t. the number of isolated vertices increases by one
at each unit of time and decreases by the number of isolated vertices, b(t), that are picked
to be end points of the new edge. b(t) can take on values 0,1, or 2. taking expectations,

d0(t + 1) = d0(t) + 1     e(b(t)).

now b(t) is the sum of two 0-1 valued random variables whose values are the number
of degree zero vertices picked for each end point of the new edge. even though the
two random variables are not independent, the expectation of b(t) is the sum of the
expectations of the two variables and is 2   d0(t)

. thus,

t

d0(t + 1) = d0(t) + 1     2  

d0(t)

.

t

the number of degree k vertices increases whenever a new edge is added to a degree k    1
vertex and decreases when a new edge is added to a degree k vertex. reasoning as above,

dk (t + 1) = dk(t) + 2  

dk   1(t)

t

    2  

dk(t)

t

.

(8.4)

note that this formula, as others in this section, is not quite precise. for example, the
same vertex may be picked twice, so that the new edge is a self-loop. for k << t, this
problem contributes a minuscule error. restricting k to be a    xed constant and letting
t         in this section avoids these problems.

assume that the above equations are exactly valid. clearly, d0(1) = 1 and d1(1) =
d2(1) =        = 0. by induction on t, there is a unique solution to (8.4), since given dk(t)
for all k, the equation determines dk(t + 1) for all k. there is a solution of the form
dk(t) = pkt, where pk depends only on k and not on t, provided k is    xed and t        .
again, this is not precisely true since d1(1) = 0 and d1(2) > 0 clearly contradict the
existence of a solution of the form d1(t) = p1t.

set dk(t) = pkt. then,

(t + 1) p0 = p0t + 1     2  

p0t
t

287

figure 8.13: in selecting a component at random, each of the two components is equally
likely to be selected. in selecting the component containing a random vertex, the larger
component is twice as likely to be selected.

p0 = 1     2  p0

p0 =

1

1 + 2  

and

(t + 1) pk = pkt + 2  

pk   1t
pk = 2  pk   1     2  pk

t

    2  

pkt
t

pk =

=

=

2  

(cid:18) 2  

1 + 2  

1 + 2  

1

pk   1

(cid:19)k
(cid:18) 2  

p0

1 + 2  

1 + 2  

(cid:19)k

.

(8.5)

thus, the model gives rise to a graph with a degree distribution that falls o    exponentially
fast with the degree.

the generating function for component size

let nk(t) be the expected number of components of size k at time t. then nk(t) is
proportional to the id203 that a randomly picked component is of size k. this is
not the same as picking the component containing a randomly selected vertex (see figure
8.13). indeed, the id203 that the size of the component containing a randomly se-
lected vertex is k is proportional to knk(t). we will show that there is a solution for nk(t)
of the form akt where ak is a constant independent of t. after showing this, we focus on
the generating function g(x) for the numbers kak(t) and use g(x) to    nd the threshold
for giant components.

consider n1(t), the expected number of isolated vertices at time t. at each unit of
t many isolated

time, an isolated vertex is added to the graph and an expected 2  n1(t)

288

vertices are chosen for attachment and thereby leave the set of isolated vertices. thus,

n1(t + 1) = n1(t) + 1     2  

n1(t)

t

.

for k >1, nk(t) increases when two smaller components whose sizes sum to k are joined
by an edge and decreases when a vertex in a component of size k is chosen for attachment.
the id203 that a vertex selected at random will be in a size k component is knk(t)
.
thus,

t

nk(t + 1) = nk(t) +   

jnj(t)

t

(k     j)nk   j(t)

t

    2  

knk(t)

t

.

k   1(cid:88)

j=1

to be precise, one needs to consider the actual number of components of various sizes,
rather than the expected numbers. also, if both vertices at the end of the edge are in the
same k-vertex component, then nk(t) does not go down as claimed. these small inaccu-
racies can be ignored.

(cid:80)   

consider solutions of the form nk(t) = akt. note that nk(t) = akt implies the num-
ber of vertices in a connected component of size k is kakt. since the total number of
vertices at time t is t, kak is the id203 that a random vertex is in a connected
component of size k. the recurrences here are valid only for k    xed as t        . so
k=0 kak may be less than 1, in which case, there are non   nite size components whose
j(k     j)ajak   j.

sizes are growing with t. solving for ak yields a1 = 1

k   1(cid:80)

1+2   and ak =   

1+2k  

j=1

   (cid:88)

consider the generating function g(x) for the distribution of component sizes where
the coe   cient of xk is the id203 that a vertex chosen at random is in a component
of size k.

k=1

g(x) =

kakxk.

now, g(1) =(cid:80)   
log n (since its expected degree increases by 2/t and(cid:80)n

k=0 kak is the id203 that a randomly chosen vertex is in a    nite sized
component. for    = 0, this is clearly one, since all vertices are in components of size
one. on the other hand, for    = 1, the vertex created at time one has expected degree
t=1(2/t) =   (log n)); so, it is in a
non   nite size component. this implies that for    = 1, g(1) < 1 and there is a non   nite
size component. assuming continuity, there is a   critical above which g(1) < 1. from the
formula for the a(cid:48)

is, we will derive the di   erential equation
g =    2  xg(cid:48) + 2  xgg(cid:48) + x

and then use the equation for g to determine the value of    at which the phase transition
for the appearance of a non   nite sized component occurs.

derivation of g(x)

289

from

and

derive the equations

and

ak =

a1 =

1

1 + 2  

k   1(cid:88)

  

1 + 2k  

j=1

j(k     j)ajak   j

a1 (1 + 2  )     1 = 0

ak (1 + 2k  ) =   

j(k     j)ajak   j

k   1(cid:88)

j=1

k   1(cid:88)

j=1

akk2xk   1.

k=1

   (cid:88)
   (cid:80)
k   1(cid:88)

k=1

for k     2. the generating function is formed by multiplying the kth equation by kxk and
summing over all k. this gives

akk2xk   1 =   

kxk

j(k     j)ajak   j.

   (cid:88)

   x +

k=1

note that

thus,

   (cid:88)

k=1

kakxk + 2  x

   (cid:80)

k=1

g(x) =

kakxk and g(cid:48)(x) =

   (cid:88)
k   1(cid:88)
   (cid:88)

k=1

   x + g(x) + 2  xg(cid:48)(x) =   

kxk

j(k     j)ajak   j.

working with the right hand side

   (cid:88)

k   1(cid:88)

  

kxk

j(k     j)ajak   j =   x

k=1

j=1

k=1

j=1

now breaking the j + k     j into two sums gives

j2ajxj   1(k     j)ak   jxk   j +   x

   (cid:88)

k   1(cid:88)

k=1

j=1

  x

j=1

j(k     j)(j + k     j)xk   1ajak   j.

   (cid:88)

k   1(cid:88)

k=1

j=1

jajxj(k     j)2ak   jxk   j   1.

notice that the second sum is obtained from the    rst by substituting k     j for j and that
both terms are   xg(cid:48)g. thus,

   x + g(x) + 2  xg(cid:48)(x) = 2  xg(cid:48)(x)g(x).

hence,

g(cid:48) =

1     g
1     g

x

.

1
2  

290

phase transition for non   nite components

   (cid:80)

the generating function g(x) contains information about the    nite components of the
graph. a    nite component is a component of size 1, 2, . . ., which does not depend on t.

observe that g(1) =

kak and hence g(1) is the id203 that a randomly chosen

k=0

vertex will belong to a component of    nite size. if g(1) = 1 there are no in   nite compo-
nents. when g(1) (cid:54)= 1, then 1     g(1) is the expected fraction of the vertices that are in
non   nite components. potentially, there could be many such non   nite components. but
an argument similar to part 3 of theorem ?? concludes that two fairly large components
would merge into one. suppose there are two connected components at time t, each of
size at least t4/5. consider the earliest created 1
2t4/5 vertices in each part. these vertices
must have lived for at least 1
2t4/5 time after creation. at each time, the id203 of an
edge forming between two such vertices, one in each component, is at least      (t   2/5) and

so the id203 that no such edge formed is at most(cid:0)1       t   2/5(cid:1)t4/5/2     e      (  t2/5)     0.

so with high id203, such components would have merged into one. but this still
leaves open the possibility of many components of size t  , (ln t)2, or some other slowly
growing function of t.

we now calculate the value of    at which the phase transition for a non   nite component

occurs. recall that the generating function for g (x) satis   es

g(cid:48) (x) =

1     g(x)
1     g (x)

x

.

1
2  

if    is greater than some   critical, then g(1) (cid:54)= 1. in this case the above formula at x = 1
simpli   es with 1     g(1) canceling from the numerator and denominator, leaving just 1
2   .
since kak is the id203 that a randomly chosen vertex is in a component of size k,
the average size of the    nite components is g(cid:48)(1) =

k2ak. now, g(cid:48)(1) is given by

   (cid:80)

k=1

g(cid:48)(1) =

1
2  

(8.6)

for all    greater than   critical. if    is less than   critical, then all vertices are in    nite compo-
nents. in this case g(1) = 1 and both the numerator and the denominator approach zero.
appling l   hopital   s rule

or

lim
x   1

g(cid:48)(x) = 1

2  

xg(cid:48)(x)   g(x)

(cid:12)(cid:12)(cid:12)(cid:12)x=1
(cid:0)g(cid:48)(1)     g(1)(cid:1).

x2
g(cid:48)(x)

(g(cid:48)(1))2 = 1

2  

291

the quadratic (g(cid:48)(1))2     1

2   g(cid:48)(1) + 1

2     (cid:113) 1

2   g(1) = 0 has solutions
1       

4  2     4

2  

1

1     8  
4  

g(cid:48)(1) =

2

=

.

(8.7)

the two solutions given by (8.7) become complex for    > 1/8 and thus can be valid only
for 0            1/8. for    > 1/8, the only solution is g(cid:48)(1) = 1
2   and an in   nite component
exists. as    is decreased, at    = 1/8 there is a singular point where for    < 1/8 there are
three possible solutions, one from (8.6) which implies a giant component and two from
(8.7) which imply no giant component. to determine which one of the three solutions is
valid, consider the limit as        0. in the limit all components are of size one since there
are no edges. only (8.7) with the minus sign gives the correct solution

g(cid:48) (1) =

1        

1     8  
4  

=

1    (cid:0)1     1

464  2 +       (cid:1)

28       1

4  

= 1 + 4   +        = 1.

in the absence of any nonanalytic behavior in the equation for g(cid:48) (x) in the region
0        < 1/8, we conclude that (8.7) with the minus sign is the correct solution for
0        < 1/8 and hence the critical value of    for the phase transition is 1/8. as we shall
see, this is di   erent from the static case.

as the value of    is increased, the average size of the    nite components increase from

one to

1        

(cid:12)(cid:12)(cid:12)(cid:12)  =1/8
(cid:12)(cid:12)  =1/8 = 4 and then decreases as 1

1     8  
4  

= 2

when    reaches the critical value of 1/8. at    = 1/8, the average size of the    nite com-
ponents jumps to 1
2   as the giant component swallows
2  
up the    nite components starting with the larger components.

comparison to static random graph

consider a static random graph with the same degree distribution as the graph in the
growth model. again let pk be the id203 of a vertex being of degree k. from (8.5)

recall the molloy reed analysis of random graphs with given degree distributions which
i(i     2)pi = 0. using this, it is easy to see

asserts that there is a phase transition at

that a phase transition occurs for    = 1/4. for    = 1/4,

(2  )k

pk =

(1 + 2  )k+1 .

   (cid:80)

i=0

292

pk = (2  )k

(1+2  )k+1 =

(cid:16) 1

(cid:17)k
(cid:17)k+1 =

(cid:16)

2
1
2

1+

(cid:17)k
(cid:16) 1
(cid:0) 3
(cid:1)k = 2

2

3

3
2

2

(cid:0) 1

(cid:1)k

3

fractional
size of
in   nite
component

1

0

grown

static

1/8 1/4

  

figure 8.14: comparison of the static random graph model and the growth model. the
curve for the growth model is obtained by integrating g(cid:48).

and

(cid:0) 1

3

i(i     2) 2

3

   (cid:88)

i=0

recall that 1 + a + a2 +       = 1

(cid:1)i = 2

   (cid:80)

i(cid:0) 1

i2(cid:0) 1

(cid:1)i     4

   (cid:80)
1   a, a + 2a2 + 3a3        = a

i=0

i=0

3

3

3

3

(cid:1)i = 2

3    3

2     4

3    3

4 = 0.

(1   a)2 , and a + 4a2 + 9a3        = a(1+a)
(1   a)3 .

see references at end of the chapter for calculating the fractional size sstatic of the

giant component in the static graph. the result is

(cid:26) 0

sstatic =

   

  +

1
  2+2  

       1
4
   > 1
4

1    

8.9.2 growth model with preferential attachment

consider a growth model with preferential attachment. at each time unit, a vertex is
added to the graph. then with id203   , an edge is attached to the new vertex and
to a vertex selected at random with id203 proportional to its degree. this model
generates a tree with a power law distribution.

let di(t) be the expected degree of the ith vertex at time t. the sum of the expected
degrees of all vertices at time t is 2  t and thus the id203 that an edge is connected
to vertex i at time t is di(t)

2  t . the degree of vertex i is governed by the equation

   
   t

di(t) =   

di (t)
2  t

=

di(t)
2t

where    is the id203 that an edge is added at time t and di(t)
2  t
the vertex i is selected for the end point of the edge.

is the id203 that

293

expected
degree

d

(cid:122)

i = 1, 2, 3

(cid:113) t

i

  

di(t) > d

(cid:125)(cid:124)

(cid:123) (cid:122)

di(t) < d

(cid:125)(cid:124)

i =   2
d2 t
       vertex number       

(cid:123)

i = t

figure 8.15: illustration of degree of ith vertex at time t. at time t, vertices numbered
1 to   2

d2 t have degrees greater than d.

the two in the denominator governs the solution, which is of the form at
1
2 or a =   i

of a is determined by the initial condition di (t) =    at t = i. thus,    = ai
hence, di(t) =   

1
2 . the value
    1
2 .

(cid:113) t

i .

next, we determine the id203 distribution of vertex degrees. now, di(t) is less
d2 t and thus
d2 . hence, the id203 that a vertex has degree

d2 t. the fraction of the t vertices at time t for which i >   2

than d provided i >   2
that the degree is less than d is 1       2
less than d is 1       2

d2 . the id203 density p(d) satis   es

and can be obtained from the derivative of prob(degree < d).

(cid:90) d

0

p(d)   d = prob(degree < d) = 1       2
d2

(cid:18)

(cid:19)

1       2
d2

= 2

  2
d3 ,

p(d) =

   
   d

a power law distribution.

8.10 small world graphs

in the 1960   s, stanley milgram carried out an experiment that indicated that most
pairs of individuals in the united states were connected by a short sequence of acquain-
tances. milgram would ask a source individual, say in nebraska, to start a letter on its
journey to a target individual in massachusetts. the nebraska individual would be given
basic information about the target including his address and occupation and asked to
send the letter to someone he knew on a    rst name basis, who was closer to the target
individual, in order to transmit the letter to the target in as few steps as possible. each

294

person receiving the letter would be given the same instructions. in successful experi-
ments, it would take on average    ve to six steps for a letter to reach its target. this
research generated the phrase    six degrees of separation    along with substantial research
in social science on the interconnections between people. surprisingly, there was no work
on how to    nd the short paths using only local information.

in many situations, phenomena are modeled by graphs whose edges can be partitioned
into local and long-distance. we adopt a simple model of a directed graph due to klein-
berg, having local and long-distance edges. consider a 2-dimensional n  n grid where each
vertex is connected to its four adjacent vertices via bidirectional local edges. in addition
to these local edges, there is one long-distance edge out of each vertex. the id203
that the long-distance edge from vertex u terminates at v, v (cid:54)= u, is a function of the
distance d(u, v) from u to v. here distance is measured by the shortest path consisting
only of local grid edges. the id203 is proportional to 1/dr(u, v) for some constant r.
this gives a one parameter family of random graphs. for r equal zero, 1/d0(u, v) = 1 for
all u and v and thus the end of the long-distance edge at u is uniformly distributed over all
vertices independent of distance. as r increases the expected length of the long-distance
edge decreases. as r approaches in   nity, there are no long-distance edges and thus no
paths shorter than that of the lattice path. what is interesting is that for r less than two,
there are always short paths, but no local algorithm to    nd them. a local algorithm is an
algorithm that is only allowed to remember the source, the destination, and its current
location and can query the graph to    nd the long-distance edge at the current location.
based on this information, it decides the next vertex on the path.

the di   culty is that for r < 2, the end points of the long-distance edges are too-
uniformly distributed over the vertices of the grid. although short paths exist, it is
unlikely on a short path to encounter a long-distance edge whose end point is close to
the destination. when r equals two, there are short paths and the simple algorithm that
always selects the edge that ends closest to the destination will    nd a short path. for r
greater than two, again there is no local algorithm to    nd a short path. indeed, with high
id203, there are no short paths at all.

the id203 that the long-distance edge from u goes to v is proportional to
d   r(u, v). note that the constant of proportionality will vary with the vertex u depend-
ing on where u is relative to the border of the n    n grid. however, the number of
vertices at distance exactly k from u is at most 4k and for k     n/2 is at least k. let
it is the inverse of the constant of

v d   r(u, v) be the normalizing constant.

cr(u) = (cid:80)

proportionality.

for r > 2, cr(u) is lower bounded by

cr(u) =

(cid:88)

d   r(u, v)     n/2(cid:88)

v

k=1

295

n/2(cid:88)

k=1

k1   r     1.

(k)k   r =

destination

figure 8.16: for r < 2, on a short path you are unlikely to encounter a long-distance
edge that takes you close to the destination.

r > 2 the lengths of long distance edges tend to be short so the
id203 of encountering a su   ciently long, long-distance edge is
too low.

r = 2 selecting the edge with end point closest to the destina-
tion    nds a short path.

r < 2 the ends of long distance edges tend to be uniformly dis-
tributed.
short paths exist but a polylog length path is unlikely
to encounter a long distance edge whose end point is close to the
destination.

figure 8.17: e   ects of di   erent values of r on the expected length of long-distance edges
and the ability to    nd short paths.

296

k=1 k1   r is at least one.

(cid:88)

for r = 2 the normalizing constant cr(u) is upper bounded by

no matter how large r is the    rst term of(cid:80)n/2
d   r(u, v)     2n(cid:88)
d   r(u, v)     n/2(cid:88)
(cid:88)
(k)k   r     n/2(cid:88)
4 terms, the smallest of which is(cid:0) n

for r < 2, the normalizing constant cr(u) is lower bounded by

(4k)k   2     4

k1   r has n

the summation

2n(cid:88)

n/2(cid:80)

cr(u) =

cr(u) =

k=n/4

1
k

k=1

k=1

k=1

4

v

v

k1   r.

k=n/4

=   (ln n).

(cid:1)1   r or(cid:0) n

2

(cid:1)1   r depending

on whether r is greater or less than one. this gives the following lower bound on cr(u).

cr(u)     n
4

  (n1   r) =   (n2   r).

no short paths exist for the r > 2 case.

for r > 2, we    rst show that for at least half of the pairs of vertices, there is no short
path between them. we begin by showing that the expected number of edges of length
2r goes to zero. the id203 of an edge from u to v is d   r(u, v)/cr(u)
greater than n r+2
where cr(u) is lower bounded by a constant. the id203 that a particular edge of
2 ) for some
length greater than or equal to n r+2
constant c. since there are n2 long edges, the expected number of edges of length at least
n r+2
2 , which for r > 2 goes to zero. thus, by the    rst
moment method, almost surely, there are no such edges.

is chosen is upper bounded by cn

is at most cn2n    (r+2)

or cn 2   r

   ( r+2

2r

2r

2

for at least half of the pairs of vertices, the grid distance, measured by grid edges
between the vertices, is greater than or equal to n/4. any path between them must have
4n r   2
at least 1
2r and so there is
no polylog length path.

2r edges since there are no edges longer than n r+2

4n/n r+2

2r = 1

an algorithm for the r = 2 case

for r = 2, the local algorithm that selects the edge that ends closest to the destination
t    nds a path of expected length o(ln n)3. suppose the algorithm is at a vertex u which
is at distance k from t. then within an expected o(ln n)2 steps, the algorithm reaches a
point at distance at most k/2. the reason is that there are    (k2) vertices at distance at
most k/2 from t. each of these vertices is at distance at most k + k/2 = o(k) from u. see
figure 8.18. recall that the normalizing constant cr is upper bounded by o(ln n), and

297

< 3k/2

k/2

u

k

t

   (k2) vertices at
distance k/2 from t

figure 8.18: small worlds.

hence, the constant of proportionality is lower bounded by some constant times 1/ ln n.
the id203 that the long-distance edge from u goes to one of these vertices is at least

   (k2k   2/ ln n) =    (1/ ln n).

consider    (ln n)2 steps of the path from u. the long-distance edges from the points
visited at these steps are chosen independently and each has id203    (1/ ln n) of
reaching within k/2 of t. the id203 that none of them does is

(cid:0)1        (1/ ln n)(cid:1)c(ln n)2

= c1e    ln n =

c1
n

for a suitable choice of constants. thus, the distance to t is halved every o(ln n)2 steps
and the algorithm reaches t in an expected o(ln n)3 steps.

a local algorithm cannot    nd short paths for the r < 2 case

for r < 2 no local polylog time algorithm exists for    nding a short path. to illustrate
the proof, we    rst give the proof for the special case r = 0, and then give the proof for
r < 2.

when r = 0, all vertices are equally likely to be the end point of a long-distance edge.
thus, the id203 of a long-distance edge hitting one of the n vertices that are within
distance
n, the id203 that
   
the path does not encounter such an edge is (1     1/n)

   
n of the destination is 1/n. along a path of length

   
n . now,

(cid:18)

(cid:19)   

(cid:18)

(cid:19)n 1   

n

lim
n      

1     1
n

n

= lim
n      

1     1
n

    1   

n = 1.

= lim

n       e

298

   

since with id203 1/2 the starting point is at distance at least n/4 from the desti-
nation and in
n steps, the path will not encounter a long-distance edge ending within
   
n of the destination, for at least half of the starting points the path length will
distance
n and hence not in polylog time.
be at least

n. thus, the expected time is at least 1
2

   

   

for the general r < 2 case, we show that a local algorithm cannot    nd paths of length
o(n(2   r)/4). let    = (2     r)/4 and suppose the algorithm    nds a path with at most n  
edges. there must be a long-distance edge on the path which terminates within distance
n   of t; otherwise, the path would end in n   grid edges and would be too long. there are
o(n2  ) vertices within distance n   of t and the id203 that the long-distance edge from

one vertex of the path ends at one of these vertices is at most n2  (cid:0) 1
upper bound on the id203 of a long-distance edge hitting v is   (cid:0) 1

see this, recall that the lower bound on the normalizing constant is   (n2   r) and hence an

(cid:1) = n(r   2)/2. to
(cid:1) independent

n2   r

of where v is. thus, the id203 that the long-distance edge from one of the n   vertices
n2   r = n r   2
on the path hits any one of the n2   vertices within distance n   of t is n2  
2 .
the id203 that this happens for any one of the n   vertices on the path is at most
n r   2

4 = n(r   2)/4 = o(1) as claimed.

2 n   = n r   2

2 n 2   r

1

n2   r

short paths exist for r < 2

finally we show for r < 2 that there are o(ln n) length paths between s and t. the
proof is similar to the proof of theorem 8.13 showing o(ln n) diameter for g(n, p) when
p is    (ln n/n), so we do not give all the details here. we give the proof only for the case
when r = 0.

   
for a particular vertex v, let si denote the set of vertices at distance i from v. using
ln n), then |si| is    (ln n). for later i, we argue a constant
only local edges, if i is o(
factor growth in the size of si as in theorem 8.13. as long as |s1|+|s2|+      +|si|     n2/2,
for each of the n2/2 or more vertices outside, the id203 that the vertex is not in
si+1 is (1     1
2n2 since the long-distance edge from each vertex of si chooses
a long-distance neighbor at random. so, the expected size of si+1 is at least |si|/4 and
using cherno   , we get constant factor growth up to n2/2. thus, for any two vertices v
and w, the number of vertices at distance o(ln n) from each is at least n2/2. any two
sets of cardinality at least n2/2 must intersect giving us a o(ln n) length path from v to
w.

n2 )|si|     1     |si|

8.11 bibliographic notes

the g(n, p) random graph model is from erd  os r  enyi [er60]. among the books
written on properties of random graphs a reader may wish to consult frieze and karonski
[fk15], jansen, luczak and ruci  nski [jlr00],or bollob  as [bol01]. material on phase
transitions can be found in [bt87]. the argument for existence of a giant component is

299

from krivelevich and sudakov [ks13]. for additional material on the giant component
consult [kar90] or [jklp93].

for further description of ideas used in practical cnf-sat solvers, see [gkss08]. a
discussion of solvers used in the 2015 sat race appears in [bbis16]. the work on phase
transitions for cnf was started by chao and franco [cf86]. further work was done in
[fs96], [ap03], [fri99], and others. the proof here that the sc algorithm produces a
solution when the number of clauses is cn for c < 2

3 is from [chv92].

material on branching process can be found in [an72]. the phase transition for giant
components in random graphs with given degree distributions is from molloy and reed
[mr95a].

there are numerous papers on growth models. the material in this chapter was based
primarily on [chk+01] and [ba]. the material on small world is based on kleinberg,
[kle00] which follows earlier work by watts and strogatz [ws98].

300

8.12 exercises

exercise 8.1 search the world wide web to    nd some real world graphs in machine
readable form or data bases that could automatically be converted to graphs.

1. plot the degree distribution of each graph.

2. count the number of connected components of each size in each graph.

3. count the number of cycles in each graph.

4. describe what you    nd.

5. what is the average vertex degree in each graph? if the graph were a g(n, p) graph,

what would the value of p be?

6. spot di   erences between your graphs and g(n, p) for p from item 5. look at sizes

of connected components, cycles, size of giant component.

exercise 8.2 in g(n, p) the id203 of a vertex having degree k is(cid:0)n

(cid:1)pk(1     p)n   k.

k

1. show by direct calculation that the expected degree is np.

2. compute directly the variance of the degree distribution.

3. where is the mode of the binomial distribution for a given value of p? the mode is

the point at which the id203 is maximum.

exercise 8.3

1. plot the degree distribution for g(1000, 0.003).

2. plot the degree distribution for g(1000, 0.030).

exercise 8.4 to better understand the binomial distribution plot (cid:0)n
exercise 8.5 in g(cid:0)n, 1

(cid:1)pk(1     p)n   k as a
(cid:1) , argue that with high id203 there is no vertex of degree

function of k for n = 50 and k = 0.05, 0.5, 0.95. for each value of p check the sum over
all k to ensure that the sum is one.

k

greater than 6 log n
log log n (i.e.,the id203 that such a vertex exists goes to zero as n goes
to in   nity). you may use the poisson approximation and may wish to use the fact that
k!     ( k

n

e )k.

exercise 8.6 the example of section 8.1.1 showed that if the degrees in g(n, 1
n) were
independent there would almost surely be a vertex of degree    (log n/ log log n). however,
the degrees are not independent. show how to overcome this di   culty.

301

exercise 8.7 let f (n) be a function that is asymptotically less than n. some such func-

tions are 1/n, a constant d, log n or n

(cid:16)

1 + f (n)
n

1
3 . show that

(cid:17)n (cid:39) ef (n)(1  o(1)).
(cid:17)n(cid:105)
(cid:104)(cid:16)

1 + f (n)
n
f (n)

= 1.

for large n. that is

ln

lim
n      

exercise 8.8

1. in the limit as n goes to in   nity, how does(cid:0)1     1

(cid:1)n ln n behave.

n

(cid:0) n+1

n

(cid:1)n?

2. what is lim
n      

number of triangles in each graph. try the experiment with n=100.

exercise 8.9 consider a random permutation of the integers 1 to n. the integer i is
said to be a    xed point of the permutation if i is the integer in the ith position of the
permutation. use indicator variables to determine the expected number of    xed points in
a random permutation.

(cid:1) with n = 1000 and d=2, 3, and 6. count the
(cid:1)? what is
(cid:1)? a 4-clique consists of four vertices with all

exercise 8.10 generate a graph g(cid:0)n, d
exercise 8.11 what is the expected number of squares (4-cycles) in g(cid:0)n, d
the expected number of 4-cliques in g(cid:0)n, d
(cid:0)4
(cid:1) edges present.
with all(cid:0)4

exercise 8.12 carry out an argument, similar to the one used for triangles, to show that
p = 1
n2/3 is a threshold for the existence of a 4-clique. a 4-clique consists of four vertices

(cid:1) edges present.

n

n

n

2

2

n, and
exercise 8.13 what is the expected number of simple paths of length 3, log n,
n     1 in g(n, d
n)? a simple path is a path where no vertex appears twice as in a cycle.
the expected number of simple paths of a given length being in   nite does not imply that a
graph selected at random has such a path.
exercise 8.14 let x be an integer chosen uniformly at random from {1, 2, . . . , n}. count
the number of distinct prime factors of n. the exercise is to show that the number of prime
factors almost surely is   (ln ln n). let p stand for a prime number between 2 and n.

   

1. for each    xed prime p, let ip be the indicator function of the event that p divides x.

p + o(cid:0) 1

n

(cid:1).

show that e(ip) = 1

302

2. the random variable of interest, y = (cid:80)
(cid:80)

p

picked at random. show that the variance of y is o(ln ln n). for this, assume the
known result that the number of primes p between 2 and n is o(n/ ln n) and that

p = ln ln n. to bound the variance of y, think of what e(ipiq) is for p (cid:54)= q, both

1

ip, is the number of prime divisors of x

p
primes.

3. use (1) and (2) to prove that the number of prime factors is almost surely   (ln ln n).

exercise 8.15 suppose one hides a clique of size k in a random graph g(cid:0)n, 1

(cid:1). i.e., in

the random graph, choose some subset s of k vertices and put in the missing edges to make
s a clique. presented with the modi   ed graph, the goal is to    nd s. the larger s is, the
easier it should be to    nd. in fact, if k is more than c
n ln n, then with high id203
the clique leaves a telltale sign identifying s as the k vertices of largest degree. prove this
statement by appealing to theorem 8.1. it remains a puzzling open problem to    nd such
hidden cliques when k is smaller, say, o(n1/3).

   

2

we can ask the corresponding question about random graphs. for example, in g(cid:0)n, 1

exercise 8.16 the clique problem in a graph is to    nd the maximal size clique. this
problem is known to be np-hard and so a polynomial time algorithm is thought unlikely.
there almost surely is a clique of size (2       ) log n for any    > 0. but it is not known how
to    nd one in polynomial time.

(cid:1)

2

1. show that in g(n, 1

2) there almost surely are no cliques of size greater than or equal

to 2 log2 n.

time no(ln n) if one exists.

2), almost surely there are

cliques of size (2       ) log2 n.

2. use the second moment method to show that in g(n, 1

4. give an o (n2) algorithm that    nds a clique of size     (log n) in g(n, 1

(cid:1) in
3. show that for any    > 0, a clique of size (2       ) log n can be found in g(cid:0)n, 1
(cid:1).
id203. hint: use a greedy algorithm. apply your algorithm to g(cid:0)1000, 1
set in g(cid:0)n, 1

5. an independent set in a graph is a set of vertices such that no two of them are
connected by an edge. give a polynomial time algorithm for    nding an independent

(cid:1)of size     (log n) with high id203.

what size clique do you    nd?

2) with high

2

2

2

exercise 8.17 suppose h is a    xed graph on cn vertices with 1
4c2(log n)2 edges. show
that if c     2, with high id203, h does not occur as a vertex-induced subgraph of
g(n, 1/4). in other words, there is no subset of cn vertices of g such that the graph g
restricted to these vertices is isomorphic to h. or, equivalently, for any subset s of cn
vertices of g and any 1-1 mapping f between these vertices and the vertices of h, there is
either an edge (i, j) within s such that the edge (f (i), f (j)) does not exist in h or there
is a non-edge i, j in s such that (f (i), f (j)) does exist in h.

303

exercise 8.18 given two instances, g1 and g2 of g(n, 1
2), consider the size of the largest
vertex-induced subgraph common to both g1 and g2. in other words, consider the largest
k such that for some subset s1 of k vertices of g1 and some subset s2 of k vertices of g2,
the graph g1 restricted to s1 is isomorphic to the graph g2 restricted to s2. prove that
with high id203, k < 4 log2 n.

exercise 8.19 (birthday problem) what is the number of integers that must be drawn
with replacement from a set of n integers so that some integer, almost surely, will be
selected twice?

exercise 8.20 suppose you have an algorithm for    nding communities in a social net-
work. assume that the way the algorithm works is that given the graph g for the social
network, it    nds a subset of vertices satisfying a desired property p. the speci   cs of prop-
erty p are unimportant for this question. if there are multiple subsets s of vertices that
satisfy property p , assume that the algorithm    nds one such set s at random.

in running the algorithm you    nd thousands of communities and wonder how many
communities there are in the graph. finally, when you    nd the 10, 000th community, it is
a duplicate. it is the same community as one found earlier. use the birthday problem to
derive an estimate of the total number of communities in g.

n

pi = 1    (cid:0)1     d

(cid:1)i . if the connected component containing the start vertex has i vertices,

exercise 8.21 do a breadth    rst search in g(n, d
n) with d > 1 starting from some vertex.
the number of discovered vertices, zi, after i steps has distribution binomial(n, pi) where
then zi = i. show that as n         (and d is a    xed constant), prob(zi = i) is o(1/n) unless
i     c1 ln n or i     c2n for some constants c1, c2.
exercise 8.22 for f (x) = 1    e   dx     x, what is the value of xmax = arg max f (x)? what
is the value of f (xmax)? recall from the text that in a breadth    rst search of g(n, d
n), f (x)
is the expected normalized size of the frontier (size of frontier divided by n) at normalized
time x (x = t/n). where does the maximum expected value of the frontier of a breadth
search in g(n, d

n) occur as a function of n?

exercise 8.23 generate a random graph on 50 vertices by starting with the empty graph
and then adding random edges one at a time. how many edges do you need to add until
cycles    rst appear (repeat the experiment a few times and take the average)? how many
edges do you need to add until the graph becomes connected (repeat the experiment a few
times and take the average)?

exercise 8.24 consider g(n, p) with p = 1
3n.

1. use the second moment method to show that with high id203 there exists a

simple path of length 10. in a simple path no vertex appears twice.

2. argue that on the other hand, it is unlikely there exists any cycle of length 10.

304

exercise 8.25 complete the second moment argument of theorem 8.9 to show that for
p = d
n, d > 1, g(n, p) almost surely has a cycle. hint: if two cycles share one or more
edges, then the union of the two cycles is at least one greater than the union of the vertices.

exercise 8.26 what is the expected number of isolated vertices in g(n, p) for p = 1
2
as a function of n?

ln n

n

exercise 8.27 theorem 8.13 shows that for some c > 0 and p = c ln n/n, g(n, p) has
diameter o (ln n). tighten the argument to pin down as low a value as possible for c.

exercise 8.28 what is diameter of g(n,p) for various values of p? remember that the
graph becomes fully connected at ln n

n and has diameter two at

2 /lnn
n .

   

exercise 8.29

1. list    ve increasing properties of g (n, p).

2. list    ve non increasing properties .

exercise 8.30 if y and z are independent, nonnegative, integer valued random variables,
then the generating function of the sum y + z is the product of the generating function of
y and z. show that this follows from e(xy+z) = e(xyxz) = e(xy)e(xz).

exercise 8.31 let fj(x) be the jth iterate of the generating function f (x) of a branch-
ing process. when m > 1, limj      fj(x) = q for 0     x < 1. in the limit this implies
prob (zj = 0) = q and prob (zj = i) = 0 for all nonzero    nite values of i. shouldn   t the
probabilities add up to 1? why is this not a contradiction?

exercise 8.32 try to create a id203 distribution for a branching process which
varies with the current population in which future generations neither die out, nor grow
to in   nity.

exercise 8.33 consider generating the edges of a random graph by    ipping two coins,
one with id203 p1 of heads and the other with id203 p2 of heads. add the edge
to the graph if either coin comes down heads. what is the value of p for the generated
g(n, p) graph?

exercise 8.34 in the proof of theorem 8.15 that every increasing property has a thresh-
p0(n)
p(n) = 0 that g(n, p0) almost surely did not have
old, we proved for p0(n) such that lim
n      
p1(n) = 0,

property q. give the symmetric argument that for any p1(n) such that
g(n, p1) almost surely has property q.
exercise 8.35 consider a model of a random subset n (n, p) of integers {1, 2, . . . n} de-
   ned by independently at random including each of {1, 2, . . . n} into the set with id203
p. de   ne what an    increasing property    of n (n, p) means. prove that every increasing
property of n (n, p) has a threshold.

lim
n      

p(n)

305

exercise 8.36 n (n, p) is a model of a random subset of integers {1, 2, . . . n} de   ned by
independently at random including each of {1, 2, . . . n} into the set with id203 p.
what is the threshold for n (n, p) to contain

1. a perfect square,

2. a perfect cube,

3. an even number,

4. three numbers such that x + y = z ?

exercise 8.37 explain why the property that n (n, p) contains the integer 1 has a thresh-
old. what is the threshold?
exercise 8.38 the sudoku game consists of a 9    9 array of squares. the array is
partitioned into nine 3    3 squares. each small square should be    lled with an integer
between 1 and 9 so that each row, each column, and each 3    3 square contains exactly
one copy of each integer. initially the board has some of the small squares    lled in in such
a way that there is exactly one way to complete the assignments of integers to squares.
some simple rules can be developed to    ll in the remaining squares such as if a row does
not contain a given integer and if every column except one in which the square in the row
is blank contains the integer, then place the integer in the remaining blank square in the
row. explore phase transitions for the sudoku game. some possibilities are:

1. start with a 9    9 array of squares with each square containing a number between
1 and 9 such that no row, column, or 3    3 square has two copies of any integer.
develop a set of simple rules for    lling in squares such as if a row does not contain
a given integer and if every column except one in which the square in the row is
blank contains the integer, then place the integer in the remaining blank entry in the
row. how many integers can you randomly erase and your rules will still completely
   ll in the board?

2. generalize the sudoku game for arrays of size n2    n2. develop a simple set of
rules for completing the game. start with a legitimate completed array and erase k
entries at random. experimentally determine the threshold for the integer k such
that if only k entries of the array are erased, your set of rules will    nd a solution?
exercise 8.39 in a square n    n grid, each of the o(n2) edges is randomly chosen to
be present with id203 p and absent with id203 1     p. consider the increasing
property that there is a path from the bottom left corner to the top right corner which
always goes to the right or up. show that p = 1/2 is a threshold for the property. is it a
sharp threshold?

exercise 8.40 the threshold property seems to be related to uniform distributions. what
if we considered other distributions? consider a model where i is selected from the set
{1, 2, . . . , n} with id203 proportional to 1
i . is there a threshold for perfect squares?
is there a threshold for arithmetic progressions?

306

exercise 8.41 modify the proof that every increasing property of g(n, p) has a threshold
to apply to the 3-cnf satis   ability problem.

exercise 8.42 evaluate(cid:0)1     1

(cid:1)2k

2k

for k=3, 5, and 7. how close is it to 1/e?

exercise 8.43 for a random 3-cnf formula with n variables and cn clauses for some
constant c, what is the expected number of satisfying assignments?

exercise 8.44 which of the following variants of the sc algorithm admit a theorem like
theorem 8.20?

1. among all clauses of least length, pick the    rst one in the order in which they appear

in the formula.

2. set the literal appearing in most clauses independent of length to 1.

exercise 8.45 suppose we have a queue of jobs serviced by one server. there is a total
of n jobs in the system. at time t, each remaining job independently decides to join the
queue to be serviced with id203 p = d/n, where d < 1 is a constant. each job has a
processing time of 1 and at each time the server services one job, if the queue is nonempty.
show that with high id203, no job waits more than    (ln n) time to be serviced once
it joins the queue.

exercise 8.46 consider g (n, p). show that there is a threshold (not necessarily sharp)
for 2-colorability at p = 1/n. in particular,    rst show that for p = d/n with d < 1, with
high id203 g(n, p) is acyclic, so it is bipartite and hence 2-colorable. next, when
pn        , the expected number of triangles goes to in   nity. show that in that case, there
is a triangle almost surely and therefore almost surely the graph is not 2-colorable.

exercise 8.47 a vertex cover of size k for a graph is a set of k vertices such that one end
of each edge is in the set. experimentally play with the following problem. for g(20, 1
2),
for what value of k is there a vertex cover of size k?

exercise 8.48 construct an example of a formula which is satis   able, but the sc heuris-
tic fails to    nd a satisfying assignment.

exercise 8.49 in g(n, p), let xk be the number of connected components of size k. using
xk, write down the id203 that a randomly chosen vertex is in a connected component
of size k. also write down the expected size of the connected component containing a
randomly chosen vertex.

exercise 8.50 describe several methods of generating a random graph with a given degree
distribution. describe di   erences in the graphs generated by the di   erent methods.

307

exercise 8.51 consider generating a random graph adding one edge at a time. let n(i,t)
be the number of components of size i at time t.

n(1, 1) = n
n(1, t) = 0
n(i, t) = n(i, t     1) +

t > 1

(cid:88) j(i     j)

n2

n (j, t     1) n (i     j, t     1)     2i
n

n (i)

compute n(i,t) for a number of values of i and t. what is the behavior? what is the

sum of n(i,t) for    xed t and all i? can you write a generating function for n(i,t)?

exercise 8.52 the global id91 coe   cient of a graph is de   ned as follows. let dv be
the degree of vertex v and let ev be the number of edges connecting pairs of vertices that
are adjacent to vertex v. the global id91 coe   cient c is given by

(cid:88)

c =

2ev

dv(dv   1).

v

in a social network, for example, it measures what fraction of pairs of friends of each
person are themselves friends. if many are, the id91 coe   cient is high. what is c
for a random graph with p = d
n in the limit as n goes to in   nity? for a denser graph?
compare this value to that for some social network.

exercise 8.53 consider a structured graph, such as a grid or cycle, and gradually add
edges or reroute edges at random. let l be the average distance between all pairs of
vertices in a graph and let c be the ratio of triangles to connected sets of three vertices.
plot l and c as a function of the randomness introduced.
exercise 8.54 consider an n    n grid in the plane.

1. prove that for any vertex u, there are at least k vertices at distance k for 1     k    

n/2.

2. prove that for any vertex u, there are at most 4k vertices at distance k.

3. prove that for one half of the pairs of points, the distance between them is at least

n/4.

exercise 8.55 recall the de   nition of a small-world graph in section 8.10. show that
in a small-world graph with r     2, that there exist short paths with high id203. the
proof for r = 0 is in the text.
exercise 8.56 change the small worlds graph as follows. start with a n    n grid where
each vertex has one long-distance edge to a vertex chosen uniformly at random. these are
exactly like the long-distance edges for r = 0. instead of having grid edges, we have some
other graph with the property that for each vertex, there are   (t2) vertices at distance t
from the vertex for t     n. show that, almost surely, the diameter is o(ln n).

308

exercise 8.57 consider an n-node directed graph with two random out-edges from each
node. for two vertices s and t chosen at random, prove that with high id203 there
exists a path of length at most o(ln n) from s to t.

exercise 8.58 explore the concept of small world by experimentally determining the an-
swers to the following questions:

1. how many edges are needed to disconnect a small world graph? by disconnect we
mean at least two pieces each of reasonable size. is this connected to the emergence
of a giant component?

2. how does the diameter of a graph consisting of a cycle change as one adds a few

random long-distance edges?

exercise 8.59 in the small world model with r < 2, would it help if the algorithm could
look at edges at any node at a cost of one for each node looked at?

exercise 8.60 make a list of the ten most interesting things you learned about random
graphs.

309

9 topic models, nonnegative id105,

id48, and id114

in the chapter on machine learning, we saw many algorithms for    tting functions to
data. for example, suppose we want to learn a rule to distinguish spam from nonspam
email and we were able to represent email messages as points in rd such that the two
categories are linearly separable. then, we could run the id88 algorithm to    nd a
linear separator that correctly partitions our training data. furthermore, we could argue
that if our training sample was large enough, then with high id203, this translates
to high accuracy on future data coming from the same id203 distribution. an inter-
esting point to note here is that these algorithms did not aim to explicitly learn a model
of the distribution d+ of spam emails or the distribution d    of nonspam emails. instead,
they aimed to learn a separator to distinguish spam from nonspam. in this chapter, we
look at algorithms that, in contrast, aim to explicitly learn a probabilistic model of the
process used to generate the observed data. this is a more challenging problem, and
typically requires making additional assumptions about the generative process. for ex-
ample, in the chapter on high-dimensional space, we assumed data came from a gaussian
distribution and we learned the parameters of the distribution. in the chapter on svd,
we considered the more challenging case that data comes from a mixture of k gaussian
distributions. for k = 2, this is similar to the spam detection problem, but harder in that
we are not told which training emails are spam and which are nonspam, but easier in that
we assume d+ and d    are gaussian distributions. in this chapter, we examine other
important model-   tting problems, where we assume a speci   c type of process is used to
generate data, and then aim to learn the parameters of this process from observations.

9.1 topic models

id96 is the problem of    tting a certain type of stochastic model to a given
collection of documents. the model assumes there exist r    topics   , that each document is
a mixture of these topics, and that the topic mixture of a given document determines the
probabilities of di   erent words appearing in the document. for a collection of news arti-
cles, the topics may be politics, sports, science, etc. a topic is a set of word frequencies.
for the topic of politics, words like    president    and    election    may have high frequencies,
whereas for the topic of sports, words like    pitcher    and    goal    may have high frequencies.
a document (news item) may be 60% politics and 40% sports. in that case, the word
frequencies in the document are assumed to be convex combinations of word frequencies
for each of these topics with weights 0.6 and 0.4 respectively.

each document is viewed as a    bag of words    or terms.37. namely, we disregard the
order and context in which each word occurs in the document and instead only list the
frequency of occurrences of each word. frequency is the number of occurrences of the

37in practice, terms are typically words or phrases, and not all words are chosen as terms. for example,

articles and simple verbs, pronouns etc. may not be considered terms.

310

word divided by the total count of all words in the document. throwing away context
information may seem wasteful, but this approach works fairly well in practice. each doc-
ument is a vector with d components where d is the total number of di   erent terms that
exist; each component of the vector is the frequency of a particular term in the document.
we can represent a collection of n documents by a d    n matrix a called the term-
document matrix, with one column per document and one row per term. the topic model
hypothesizes that there exist r topics (r is typically small) such that each document is
a mixture of these topics. in particular, each document has an associated vector with r
nonnegative components summing to one, telling us the fraction of the document that is
on each of the topics. in the example above, this vector would have 0.6 in the component
for politics and 0.4 in the component for sports. these can be arranged vectors as the
columns of a r    n matrix c, called the topic-document matrix. finally, there is a third
d    r matrix b for the topics. each column of b is a vector corresponding to one topic;
it is the vector of expected frequencies of terms in that topic. the vector of expected
frequencies for a document is a convex combination of the expected frequencies for topics,
with the topic weights given by the vector in c for that document. in matrix notation,
let p be a n    d matrix with column p (:, j) denoting the expected frequencies of terms
in document j. then,

p = bc.

(9.1)

pictorially, we can represent this as:

                                          

t

e

r

m

d o c u m e n t

p

t o p i c

b

                                          

                                          

t

e

=

r

m

                                          

                  

t
o
p
i
c

d o c u m e n t

c

                  

topic models are stochastic models that generate documents according to the fre-
quency matrix p above. pij is viewed as the id203 that a random term of document
j is the ith term in the dictionary. we make the assumption that terms in a document are
drawn independently. in general, b is assumed to be a    xed matrix, whereas c is random.
so, the process to generate n documents, each containing m terms, is the following:
de   nition 9.1 (document generation process) let d be a distribution over a mix-
ture of topics. let b be the term-topic matrix. create a d    n term-document matrix a
as follows:

    intialize aij = 0 for i = 1, 2, . . . , d; j = 1, 2, . . . , n.38
38we will use i to index into the set of all terms, j to index documents and l to index topics.

311

    for j = 1, 2, . . . , n

    pick column j of c from distribution d. this will be the topic mixture for

document j, and induces p (:, j) = bc(:, j).

    for t = 1, 2, . . . , m, do:

    generate the tth term xt of document j from the multinomial distribution
    add 1/m to axt,j.

over {1, 2, . . . , d} with id203 vector p (:, j) i.e., prob(xt = i) = pij.

the id96 problem is to infer b and c from a. the id203 distribution
d, of the columns of c is not yet speci   ed. the most commonly used distribution is the
dirichlet distribution that we study in detail in section 9.6.

often we are given fewer terms of each document than the number of terms or the

number of documents. even though

e(aij|p ) = pij,

(9.2)

and in expectation a equals p , the variance is high. for example, for the case when
d, a(:, j) is likely to have 1/m in a random
pij = 1/d for all i with m much less than
subset of m coordinates since no term is likely to be picked more than once. thus

   

(cid:18) 1

(cid:19)

(cid:18) 1

(cid:19)

d

||a(:, j)     p (:, j)||1 = m

    1
d

m

+ (d     m)

    2,

the maximum possible. this says that in l1 norm, which is the right norm when dealing
with id203 vectors, the    noise    a  ,j     p  ,j is likely to be larger than p  ,j. this is one
of the reasons why the model id136 problem is hard. write

(9.3)
where, a is the d    n term-document matrix, b is a d    r term-topic matrix and c is
a r    n topic-document matrix. n stands for noise, which can have high norm. the l1
norm of each column of n could be as high as that of bc.

a = bc + n,

there are two main ways of tackling the computational di   culty of    nding b and c
from a. one is to make assumptions on the matrices b and c that are both realistic and
also admit e   cient computation of b and c. the trade-o    between these two desirable
properties is not easy to strike and we will see several approaches beginning with the
strongest assumptions on b and c in section 9.2. the other way is to restrict n . here
again, an idealized way would be to assume n = 0 which leads to what is called the non-
negative id105 (nmf) (section 9.3) problem of factoring the given matrix
a into the product of two nonnegative matrices b and c. with a further restriction on b,
called anchor terms, (section 9.4), there is a polynomial time algorithm to do nmf. the

312

strong restriction of n = 0 can be relaxed (section ??), but at the cost of computational
e   ciency.

the most common approach to id96 makes an assumption on the probabil-
ity distribution of c, namely, that the columns of c are independent dirichlet distributed
random vectors. this is called the id44 model (section 9.6), which
does not admit an e   cient computational procedure. we show that the dirichlet distri-
bution leads to many documents having a    primary topic,    whose weight is much larger
than average in the document. this motivates a model called the    dominant admixture
model    (section 9.7) which admits an e   cient algorithm.

on top of whatever other assumptions are made, we assume that in each document,
the m terms in it are drawn independently as in de   nition 9.1. this is perhaps the biggest
assumption of all.

9.2 an idealized model

the topic model id136 problem is in general computationally hard. but under
certain reasonable assumptions, it can be solved in polynomial time as we will see in this
chapter. we start here with a highly idealized model that was historically the    rst for
which a polynomial time algorithm was devised. in this model, we make two assumptions:

the pure topic assumption: each document is purely on a single topic. i.e., each

column j of c has a single entry equal to 1, and the rest of the entries are 0.

separability assumption: the sets of terms occurring in di   erent topics are disjoint.

i.e., for each row i of b, there is a unique column l with bil (cid:54)= 0.

under these assumptions, the data matrix a has a block structure. let tl denote the
set of documents on topic l and sl the set of terms occurring in topic l. after rearranging
columns and rows so that the rows in each sl occur consecutively and the columns of each
tl occur consecutively, the matrix a looks like:

                                                   

a =

s1

s2

s3

                                                   

t

e

r

m

t o p i c

t3
0
0
0
0
0
0
   
   
   

0
0
0
0
0
0
   
   
   

0
0
0
   
   
   
0
0
0

0
0
0
0
0
0
   
   
   

   
   
   
0
0
0
0
0
0.

t1
   
   
   
0
0
0
0
0
0

   
   
   
0
0
0
0
0
0

t2
0
0
0
   
   
   
0
0
0

0.
0
0
   
   
   
0
0
0

313

if we can partition the documents into r clusters, t1, t2, . . . , tr, one for each topic,
we can take the average of each cluster and that should be a good approximation to the
corresponding column of b. it would also su   ce to    nd the sets sl of terms, since from
them we could read o    the sets tl of topics. we now formally state the document gen-
eration process under the pure topic assumption and the associated id91 problem.
note that under the pure topics assumption, the distribution d over columns of c is
speci   ed by the id203 that we pick each topic to be the only topic of a document.
let   1,   2, . . . ,   r be these probabilities.

document generation process under pure topics assumption:

    intialize all aij to zero.
    for each document do

    select a topic from the distribution given by {  1,   2, . . . ,   r}.
    select m words according to the distribution for the selected topic.
    for each selected word add 1/m to the document-term entry of the matrix a.

de   nition 9.2 (id91 problem) given a generated as above and the number of
topics r, partition the documents {1, 2, . . . , n} into r clusters t1, t2, . . . , tr, each speci   ed
by a topic.
approximate version: partition the documents into r clusters, where at most   n of
the j     {1, 2, . . . , n} are misclustered.

the approximate version of de   nition 9.2 su   ces since we are taking the average of
the document vectors in each cluster j and returning the result as our approximation to
column j of b. note that even if we clustered perfectly, the average will only approximate
the column of b. we now show how we can    nd the term clusters sl, which then can be
used to solve the id91 problem.

construct a graph g on d vertices, with one vertex per term, and put an edge between
two vertices if they co-occur in any document. by the separability assumption, we know
that there are no edges between vertices belonging to di   erent sl. this means that if each
sl is a connected component in this graph, then we will be done. note that we need to
assume m     2 (each document has at least two words) since if all documents have just
one word, there will be no edges in the graph at all and the task is hopeless.

let us now focus on a speci   c topic l and ask how many documents nl we need so that
with high id203, sl is a connected component. one annoyance here is that some
words may have very low id203 and not become connected to the rest of sl. on the
other hand, words of low id203 can   t cause much harm since they are unlikely to be
the only words in a document, and so it doesn   t matter that much if we fail to cluster
them. we make this argument formal here.

314

let    < 1/3 and de   ne    =   m. consider a partition of sl into two subsets of terms w
and w that each have id203 mass at least    in the distribution of terms in topic l.
suppose that for every such partition, there is at least one edge between w and w . this
would imply that the largest connected component   sl in sl must have id203 mass
at least 1       . if   sl had id203 mass between    and 1        then using w =   sl would
violate the assumption about partitions with mass greater than    having an edge between
them. if the largest partition   sl had id203 mass less than   , then one could create a
union of connected components w that violates the assumption. since prob(   sl)     1       ,
the id203 that a new random document of topic l contains only words not in   sl is
at most   m =   . thus, if we can prove the statement about partitions, we will be able to
correctly cluster nearly all new random documents.

to prove the statement about partitions,    x some partition of sl into w and w that
each have id203 mass at least   . the id203 that m words are all in w or w is
at most prob(w )m + prob(w )m. thus the id203 that none of nl documents creates
an edge between w and w is

(cid:0)prob(w )m + prob(w )m(cid:1)nl     (  m + (1       )m)nl

    ((1       /2)m)nl
    e     mnl/2

where the    rst inequality is due to convexity and the second is a calculation. since there
are at most 2d di   erent possible partitions of sl into w and w , the union bound ensures
at most a    id203 of failure by having

2de     mnl/2       .

this in turn is satis   ed for

(cid:19)

d ln 2 + ln

(cid:18)
(cid:1), then with id203 at least 1       , the largest

1
  

.

mnl     2
  

(cid:0)d ln 2 + ln 1

this proves the following result.
lemma 9.1 if nlm     2
connected component in sl has id203 mass at least 1       . this in turn implies that
the id203 to fail to correctly cluster a new random document of topic l is at most
   =   1/m.

  

  

9.3 nonnegative id105 - nmf

we saw in section 9.1, while the expected value e(a|b, c) equals bc, the variance

can be high. write

a = bc + n,

where, n stands for noise. in id96, n can be high. but it will be useful to    rst
look at the problem when there is no noise. this can be thought of as the limiting case

315

as the number of words per document goes to in   nity.

suppose we have the exact equations a = bc where a is the given matrix with non-
negative entries and all column sums equal to 1. given a and the number of topics r,
can we    nd b and c such that a = bc where b and c have nonnegative entries? this
is called the nonnegative id105 (nmf) problem and has applications be-
sides id96. if b and c are allowed to have negative entries, we can use singular
value decomposition on a using the top r singular vectors of a.

before discussing nmf, we will take care of one technical issue. in id96,
besides requiring b and c to be nonnegative, we have additional constraints id30
from the fact that frequencies of terms in one particular topic are nonnegative reals
summing to one, and that the fractions of each topic that a particular document is on are
also nonnegative reals summing to one. all together, the constraints are:

1. a = bc.

2. the entries of b and c are all nonnegative.

3. columns of both b and c sums to one.

it will su   ce to ensure the    rst two conditions.

lemma 9.2 let a be a matrix with nonnegaitve elements and columns summing to one.
the problem of    nding a factorization bc of a satisfying the three conditions above is
reducible to the nmf problem of    nding a factorization bc satisfying conditions (1) and
(2).

proof: suppose we have a factorization bc that satis   es (1) and (2) of a matrix a whose
columns each sum to one. we can multiply the lth column of b by a positive real number
and divide the lth row of c by the same real number without violating a = bc. by doing
l bilclj
i aij, is 1.

this, we may assume that each column of b sums to one. now we have aij = (cid:80)
l clj, the sum of the jth column of c,(cid:80)
which implies(cid:80)

i,l bilclj =(cid:80)

i aij =(cid:80)

thus the columns of c sum to one giving (3).

given an d    n matrix a and an integer r, the exact nmf problem is to determine
whether there exists a factorization of a into bc where b is an d    r matrix with non-
negative entries and c is r    n matrix with nonnegative entries and if so,    nd such a
factorization.39

nonnegative id105 is a general problem and there are many heuristic
algorithms to solve the problem. in general, they su   er from one of two problems. they
could get stuck at local optima which are not solutions or take exponential time. in fact,

39b   s columns form a    basis    in which a   s columns can be expressed as nonnegative linear combinations,

the    coe   cients    being given by matrix c.

316

the nmf problem is np-hard. in practice, often r is much smaller than n and d. we show
   rst that while the nmf problem as formulated above is a nonlinear problem in r(n + d)
unknowns (the entries of b and c), it can be reformulated as a nonlinear problem with
just 2r2 unknowns under the simple nondegeneracy assumption that a has rank r. this,
in turn, allows for an algorithm that runs in polynomial time when r is a constant.

lemma 9.3 if a has rank r, then the nmf problem can be formulated as a problem with
2r2 unknowns. using this, the exact nmf problem can be solved in polynomial time if r
is constant.

proof: if a = bc, then each row of a is a linear combination of the rows of c. so the
space spanned by the rows of a is contained in the space spanned by the rows of the r   n
matrix c. the latter space has dimension at most r, while the former has dimension r by
assumption. so they must be equal. thus every row of c must be a linear combination
of the rows of a. choose any set of r independent rows of a to form a r    m matrix a1.
then c = sa1 for some r    r matrix s. by analogous reasoning, if a2 is a n    r matrix
of r independent columns of a, there is a r    r matrix t such that b = a2t . now we
can easily cast nmf in terms of unknowns s and t :

a = a2t sa1

;

(sa1)ij     0 ;

(a2t )kl     0    i, j, k, l.

it remains to solve the nonlinear problem in 2r2 variables. there is a classical algo-
rithm which solves such problems in time exponential only in r2 (polynomial in the other
parameters). in fact, there is a logical theory, called the theory of reals, of which this
is a special case and any problem in this theory can be solved in time exponential in the
number of variables. we do not give details here.

9.4 nmf with anchor terms

an important case of nmf, which can be solved e   ciently, is the case where there are
anchor terms. an anchor term for a topic is a term that occurs in the topic and does not
occur in any other topic. for example, the term    batter    may be an anchor term for the
topic baseball and    election    for the topic politics. consider the case that each topic has
an anchor term. this assumption is weaker than the separability assumption of section
9.2, which says that all terms are anchor terms.

in matrix notation, the assumption that each topic has an anchor term implies that
for each column of the term-topic matrix b, there is a row whose sole nonzero entry is in
that column.

de   nition 9.3 (anchor term) for each l = 1, 2, . . . r, there is an index il such that

bil,l (cid:54)= 0 and    l(cid:48) (cid:54)= l bil,l(cid:48) = 0 .

317

in this case, it is easy to see that each row of the topic-document matrix c has a

scalar multiple of it occurring as a row of the given term-document matrix a.

                              

0.3    c4

a

0.2    c2

                               =

                              

election

batter

0

0

0

0.3

b

0

0.2

0

0

                              

                        

                         .

    c1    
    c2    
    c4    

if there is a nmf of a, there is one in which no row of c is a nonnegative linear
combination of other rows of c. if some row of c is a nonnegative linear combination of
the other rows of c, then eliminate that row of c as well as the corresponding column of
b and suitably modify the other columns of b maintaining a = bc. for example, if

c5 = 4    c3 + 3    c6,

delete row 5 of c, add 4 times column 5 of b to column 3 of b, add 3 times column 5
of b to column 6 of b, and delete column 5 of b. after repeating this, each row of c is
positively independent of the other rows of c, i.e., it cannot be expressed as a nonnegative
linear combination of the other rows.

if a = bc is a nmf of a and there are rows in a that are positive linear combinations
of other rows, the rows can be remove and the corresponding rows of b remove to give a
nmf   a =   bc where   a and   b are the matrices a and b with the removed rows. since
there are no rows in   a that are linear combinations of other rows of   a,   b is a diagonal
matrix and the rows of   a are scalar multiples of rows of c. now set c =   a and   b = i
and restore the rows to   b to get b such that a = bc.

to remove rows of a that are scalar multiples of previous rows in polynomial time

check if there are real numbers x1, x2, . . . xi   1, xi+1, . . . xn such that

(cid:88)

j(cid:54)=i

xjaj = ai xj     0.

this is a linear program and can be solved in polynomial time. while the algorithm
runs in polynomial time, it requires solving one linear program per term. an improved
method, not presented here, solves just one linear program.

9.5 hard and soft id91

in section 9.2, we saw that under the assumptions that each document is purely on
one topic and each term occurs in only one topic, approximately    nding b was reducible

318

figure 9.1: geometry of id96. the corners of the triangle are the columns
of b. the columns of a for topic 1 are represented by circles, for topic 2 by squares, and
for topic 3 by dark circles. columns of bc (not shown) are always inside the big triangle,
but not necessarily the columns of a.

to id91 documents according to their topic. id91 here has the usual meaning
of partitioning the set of documents into clusters. we call this hard id91, meaning
each data point is to be assigned to a single cluster.

the more general situation is that each document has a mixture of several topics. we
may still view each topic as a cluster and each topic vector, i.e., each column of b, as
a    cluster center    (figure 9.1). but now, each document belongs fractionally to several
clusters, the fractions being given by the column of c corresponding to the document. we
may then view p (:, j) = bc(:, j) as the    cluster center    for document j. the document
vector a(:, j) is its cluster center plus an o   set or noise n (:, j).

barring ties, each column of c has a largest entry. this entry is the primary topic
of document j in id96. identifying the primary topic of each document is a
   hard id91    problem, which intuitively is a useful step in solving the    soft cluster-
ing    problem of    nding the fraction of each cluster each data point belongs to.    soft
id91    just refers to    nding b and c so that n = a     bc is small. in this sense,
soft id91 is equivalent to nmf.

we will see in sections 9.8 and 9.9 that doing hard id91 to identify the primary
topic and using that to solve the soft id91 problem can be carried out under some
assumptions. the primary topic of each document is used to    nd the    catchwords    of
each topic, the important words in a weaker sense than anchor words, and then using
the catchwords to    nd the term-topic matrix b and then c. but as stated earlier, the

319

general nmf problem is np-hard. so, we make some assumptions before solving the
problem. for this, we    rst look at id44 (lda), which guides us
towards reasonable assumptions.

9.6 the id44 model for id96

the most widely used model for id96 is the id44
(lda) model. in this model, the topic weight vectors of the documents, the columns of
c, are picked independently from what is known as a dirichlet distribution. the term-
topic matrix b is    xed. it is not random. the dirichlet distribution has a parameter   
called the    concentration parameter   , which is a real number in (0, 1), typically set to
1/r. for each vector v with r nonnegative components summing to one,

prob density ( column j of c = v) =

1

g(  )

v     1

l

,

r(cid:89)

l=1

where, g(  ) is the normalizing constant so that the total id203 mass is one. since
   < 1, if any vl = 0, then the id203 density is in   nite.

once c is generated, the id44 model hypothesizes that the

matrix

acts as the id203 matrix for the data matrix a, namely,

p = bc

e(a|p ) = p.

assume the model picks m terms from each document. each trial is according to the
multinomial distribution with id203 vector p (:, j); so the id203 that the    rst
term we pick to include in the document j is the ith term in the dictionary is pij. then,
aij is set equal to the fraction out of m of the number of times term i occurs in document j.

the dirichlet density favors low vl, but since the vl have to sum to one, there is at
least one component that is high. we show that if    is small, then with high id203,
the highest entry of the column is typically much larger than the average. so, in each
document, one topic, which may be thought as the    primary topic    of the document, gets
disproportionately high weight. to prove this, we have to work out some properties of
the dirichlet distribution. the    rst lemma describes the marginal id203 density of
each coordinate of a dirichlet distributed random variable:

lemma 9.4 suppose the joint distribution of y = (y1, y2, . . . , yr) is the dirichlet distri-
bution with concentration parameter   . then, the marginal id203 density q(y) of y1
is given by

q(y) =

  (r   + 1)

  (  )  ((r     1)   + 1)

y     1(1     y)(r   1)   ,        (0, 1],

where,    is the gamma function (see appendix for the de   nition).

320

(cid:90)

(cid:18)(cid:90)

proof: by de   nition of the marginal,

q(y) =

1

g(  )

y     1

y2,y3,...,yr
y2+y3+      +yr=1   y

(y2 y3        yr)     1 dy2 dy3 . . . dyr.

put zl = yl/(1     y). with this change of variables,

q(y) =

1

g(  )

y     1(1     y)(r   1)  

z2,z3,...,zr
z2+z3+      +zr=1   y

(z2z3        zr)     1 dz2 dz3 . . . dzr

(cid:19)

.

the quantity inside the parentheses is independent of y, so for some c we have

q(y) = cy     1(1     y)(r   1)  .

since(cid:82) 1

0 q(y) dy = 1, we must have

(cid:82) 1
0 y     1(1     y)(r   1)  

1

c =

=

  (r   + 1)

  (  )  ((r     1)   + 1)

.

lemma 9.5 suppose the joint distribution of y = (y1, y2, . . . , yr) is the dirichlet distri-
bution with parameter        (0, 1). for        (0, 1),

prob (y1     1       )     0.85     (r   1)  +1
(r     1)   + 1

.

hence for    = 1/r, we have prob(y1     1       )     0.4   2/r. if also,    < 0.5, then,

prob (maxr

l=1yl     1       )     0.4   2.

proof: since    < 1, we have y     1 > 1 for y < 1 and so q(y)     c(1     y)(r   1)  , so

(cid:90) 1

1     

q(y) dy    

c

(r     1)   + 1

   (r   1)  +1.

to lower bound c, note that   (  )     1/   for        (0, 1). also,   (x) is an increasing function
for x     1.5, so if (r     1)   + 1     1.5, then,   (r   + 1)       ((r     1)   + 1) and in this case,
the    rst assertion of the lemma follows. if (r    1)   + 1     [1, 1.5], then,   ((r    1)   + 1)     1
and   (r   + 1)     min
z   [1,2]

  (z)     0.85, so again, the    rst assertion follows.

if now,    = 1/r, then (r     1)   + 1 < 2 and so    (r   1)  +1/((r     1)   + 1)        2/2. so the
second assertion of the lemma follows easily. for the third assertion, note that yl > 1      ,
l = 1, 2, . . . , r are mutually exclusive events for    < 0.5 (since at most one yl can be
l=1 prob(yl > 1       ) = rprob(y1    
greater than 1/2), so prob
1       )     0.4   2.

= (cid:80)r

yl     1       

(cid:16) r

max
l=1

(cid:17)

321

for example, from the last lemma, it follows that

1. with high probabilty, a constant fraction of the documents have a primary topic of
weight at least 0.6. in expectation, the fraction of documents for which this holds
is at least 0.4(0.6)2.

2. also with high id203, a smaller constant fraction of the documents are nearly

pure (weight at least 0.95 on a single topic). take    = 0.05.

if the total number of documents, n, is large, there will be many nearly pure doc-
uments. since for nearly pure documents, cl,j     0.95, bc:,j = b(:, j) +    , where,
||   ||1     0.05.
if we could    nd the nearly pure documents for a given topic l, then
the average of the a columns corresponding to these documents will be close to the aver-
age of those columns in the matrix bc (though this is not true for individual columns)
and it is intuitively clear that we would be done.

we pursue (1) and (2) in the next section, where we see that under these assumptions,

plus one more assumption, we can indeed    nd b.
more generally, the concentration parameter may be di   erent for di   erent topics. we
then have   1,   2, . . . ,   r so that

prob density ( column j of c = v)     r(cid:89)

v  l   1

l

,

the model    tting problem for id44 given a,    nd the b, the
term-topic matrix, is in general np-hard. there are heuristics, however, which are widely
used. id44 is known to work well in several application areas.

l=1

9.7 the dominant admixture model

in this section, we formulate a model with three key assumptions. the    rst two are mo-
tivated by id44, respectively by (1) and (2) of the last section. the
third assumption is also natural; it is more realistic than the anchor words assumptions
discussed earlier. this section is self-contained and no familiarity with latent dirichlet
allocation is needed.

we    rst recall the notation. a is a d    n data matrix with one document per column,
which is the frequency vector of the d terms in that document. m is the number of words
in each document. r is the    inner dimension   , i.e., b is d    r and c is r    n. we always
index topics by l and l(cid:48), terms by i, and documents by j.

we give an intuitive description of the model assumptions    rst and then make formal

statements.

1. primary topic each document has a primary topic. the weight of the primary

topic in the document is high and the weight of each non-primary topic is low.

322

2. pure document each topic has at least one pure document that is mostly on that

topic.

3. catchword each topic has at least one catchword, which has high frequency in

that topic and low frequency in other topics.

in the next section, we state quantitative versions of the assumptions and show that
these assumptions su   ce to yield a simple polynomial time algorithm to    nd the primary
topic of each document. the primary topic classi   cation can then be used to    nd b
approximately, but this requires a further assumption (4) in section (9.9) below, which is
a robust version of the pure document assumption.

let   s provide some intuition for how we are able to do the primary topic classi   cation.
by using the primary topic and catchword assumptions, we can show (quantitative version
in claim 9.1 below) that if i is a catchword for topic l, then there is a threshold   i, which
we can compute for each catchword, so that for each document j with primary topic l,
pij is above   i and for each document j whose primary topic is not l, pij is substantially
below   i. so, if

1. we were given p , and

2. knew a catchword for each topic and the threshold, we can    nd the primary topic

of each document.

we illustrate the situation in equation 9.4, where rows 1, 2, . . . , r of p correspond
to catchwords for topics 1, 2, . . . , r and we have rearranged columns in order of primary
topic. h stands for a high entry and l for a low entry.

                                    

p =

h h h l l l l l l l l l
l l l h h h l l l l l l
l l l l l l h h h l l l
l l l l l l l l l h h h
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

                                    

(9.4)

we are given a, but not p . while e(a|p ) = p , a could be far o    from p . in fact,
if in column j of p , there are many entries smaller than c/m in a (since we are doing
only m multinomial trials), they could all be zeros and so are not a good approxima-
tion to the entries of p (:, j). however, if pij > c/m, for a large value c, then aij     pij.
think of tossing a coin m times whose id203 of heads is pij. if pij     c/m, then the
number of heads one gets is close to pijm. we will assume c larger than    (log(nd)) for
catchwords so that for every such i and j we have pij     aij. see the formal catchwords
assumption in the next section. this addresses (1), namely, a     b, at least in these rows.

323

one can ask if this is a reasonable assumption. if m is in the hundreds, the assump-
tion is arguably reasonable. but a weaker and more reasonable assumption would be that
there is a set of catchwords, not just one, with total frequency higher than c/m. however,
here we use the stronger assumption of a single high frequency catchword.

(2) is more di   cult to address. let l(i) = arg maxr

l(cid:48)=1 bil(cid:48). let tl be the set of j with
primary topic l. whether or not i is a catchword, the primary topic assumption will imply
that pij does not drop by more than a certain factor    among j     tl(i). we prove this
formally in claim 9.1 of section 9.8. that claim also proves that if i is a catchword for
topic l, that there is a sharp drop in pij between j     tl and j /    tl.

but for noncatchwords, there is no guarantee of a sharp fall in pij between j     tl(i)
and j /    tl(i). however, we can identify for each i, where the    rst fall of roughly    factor
from the maximum occurs in row i of a. for catchwords, we show below (9.8) that this
happens precisely between tl(i) and [n] \ tl(i). for noncatchwords, we show that the fall
does not occur among j     tl(i). so, the minimal sets where the fall occurs are the tl and
we use this to identify them. we call this process pruning.

9.8 formal assumptions

parameters   ,   ,    and    are real numbers in (0, 0.4] satisfying

(1) primary topic there is a partition of [n] into t1, t2, . . . , tk with:

   +        (1     3  )  .

(cid:40)      

      .

clj

for j     tl
for j /    tl.

.

(2) pure document for each l, there is some j with

clj     1       .

(3) catchwords for each l, there is at least one catchword i satisfying:

bil(cid:48)       bil
bil        , where,    =

c log(10nd/  )

m  2  2

, c constant.

for l(cid:48) (cid:54)= l

(9.5)

(9.6)

(9.7)

(9.8)

let

r
max
l(cid:48)=1

l(i) = arg

(9.9)
another way of stating the assumption bil        is that the expected number of times term
i occurs in topic l among m independent trials is at least c log(10nd/  )/  2  2 which grows
only logarithmically in n and d. as stated at the end of the last section, the point of
requiring bil        for catchwords is so that using the hoe   ding-cherno    inequality, we can
assert that aij     pij. we state the hoe   ding-cherno    inequality in the form we use it:

bil(cid:48).

324

lemma 9.6

prob(|aij     pij|         max(pij,   )/4)       
10nd

.

so, with id203 at least 1     (  /10),

|aij     pij|         max(  , pij)/4    i, j

simultaneously. after paying the failure id203 of   /10, we henceforth assume that
the above holds.

proof: since aij is the average of m independent bernoulli trials, each with expectation
pij, the hoe   ding-cherno    inequality asserts that

prob (|aij     pij|        )     2 exp

   cmmin

(cid:18)

(cid:19)(cid:19)

,    

.

(cid:18)   2

pij

plugging in     =     max(pij,   )/4, the    rst statement of the lemma follows with some
calculation. the second statement is proved by a union bound over the nd possible (i, j)
values.

algorithm

1. compute thresholds:   i =   (1       ) max
2. do thresholding: de   ne a matrix   a by

j

aij.

(cid:40)

1
0

if aij       i and   i         (cid:0)1     5  

2

(cid:1) .

.

otherwise .

  aij =

3. pruning: let ri = {j|  aij = 1}. if any ri strictly contains another, set all

entries of row i of   a to zero.

theorem 9.7 for i = 1, 2, . . . , d, let ri = {j|  aij = 1} at the end of the algorithm. then,
each nonempty ri = tl(i), with l(i) as in (9.9).

proof: we start with a lemma which proves the theorem for catchwords. this is the
bulk of the work in the proof of the theorem.

lemma 9.8 if i is a catchword for topic l, then ri = tl.

proof: assume throughout this proof that i is a catchword for topic l. the proof consists
of three claims. the    rst argues that for j     tl, pij is high and for j /    tl, pij is low. the
second claim argues the same for aij instead of pij. it follows from the hoe   ding-cherno   
inequality since aij is just the average of m bernoulli trials, each with id203 pij.
the third claim shows that the threshold computed in the    rst step of the algorithm falls
between the high and the low.

325

claim 9.1 for i, a catchword for topic l,

bil     pij     bil  
pij     bil  (1     3  )

for j     tl
for j /    tl.

proof: for j     tl, using (9.6)

r(cid:88)

l(cid:48)=1

pij =

bil(cid:48)cl(cid:48),j     [bil  , bil]

since bil = max

l(cid:48)

bil(cid:48). for j /    tl,

(cid:88)

l(cid:48)(cid:54)=l

pij = bilclj +

bil(cid:48)cl(cid:48)j     bilclj +   bil(1     clj)     bil(   +   )     bil  (1     3  ),

(9.10)

where, the    rst inequality is from (9.7) and the second inequality is because subject to the
constraint clj        imposed by the primary topic assumption (9.6), bilclj +   bil(1     clj) is
maximized when clj =   . we have also used (9.5).
claim 9.2 with id203 at least 1       /10, for every l and every catchword i of l:

(cid:40)    bil  (1       /4)

aij

    bil  (1     (11/4)  ),

for j     tl
for j /    tl

proof: suppose for some j     tl, aij < bil  (1       /4). then, since pij     bil   by claim
(9.1), |aij     pij|         bil/4 by claim (9.1). since i is a catchword, bil        and so
|aij     pij|         bil/4     (    max(pij,   )/4) and we get the    rst inequality of the current
claim using lemma 9.6.

for the second inequality: for j /    tl, pij     bil  (1     3  ) by claim 9.1 and so if this
inequality is violated, |aij     pij|     bil    /4 and we get a contradiction to lemma (9.6).

claim 9.3 with id203 at least 1     , for every topic l and every catchword i of topic l,

the   i computed in step 1 of the algorithm satis   es:   i    (cid:0)(1   (5/2)  )bil   , bil  (1     /2)(cid:1).

proof: if i is a catchword for topic l and j0 a pure document for l, then

k(cid:88)

l(cid:48)=1

pij0 =

bil(cid:48)cl(cid:48)j0     bilclj0     (1       )bil.

applying lemma 9.6, aij0 > (1    (3/2)  )bil. thus,   i computed in step 1 of the algorithm
satis   es   i > (1     (3  /2))(1       )bil       (1     (5/2)  )  bil. hence,   aij is not set to zero for
all j. now, since pij     bil for all j, aij     (1 +   /4)bil by lemma 9.6 implying
  i = maxjaij(1       )       bil(1 + (  /4))(1       )       bil  (1       /2).

326

claims 9.2 and 9.3, complete the proof of lemma 9.8.

the lemma proves theorem 9.7 for catchwords. note that since each topic has at least

one catchword, for each l, there is some i with ri = tl.

suppose i is a noncatchword. let a = maxj aij. if a <   (1     (5  /2)) , then   i <
    (1    (5  /2)) and the entire row of   a will be set to all zeros by the algorithm, so ri =    
and there is nothing to prove. assume that a       (1     (5  /2)). let j0 = arg maxj aij.
then a = aij0       (1     (5  /2)). we claim pi,j0     a(1       /2). if not, pij0 < a(1       /2) and

(cid:18) pij0  

,

      

4

4

(cid:19)

|aij0     pij0| > max

which contradicts lemma 9.6. so,

let l = l(i). then

a(1       /2)     pij0 =

bil(cid:48)cl(cid:48)j0     bil.

bil     pij0       (1     3  ).

r(cid:88)

l(cid:48)=1

,

(9.11)

also, if j1 is a pure document for topic l, cl,j1     (1       ) so, pi,j1     bilcl,j1     bil(1       ).
now, we claim that

ai,j1     bil(1     (3  /2)).

if not,

pij1     aij1 > bil(1       )     bil(1     (3  /2)) = bil(  /2)     max

(9.12)

(cid:18)     

4

,

pij1  

4

(cid:19)

,

contradicting lemma 9.6. so (9.12) holds and thus,
a     bil(1     (3  /2))

(9.13)
now, for all j     tl, pij     bilclj     a(1       /2)  . so, by applying lemma 9.6 again, for all
j     tl,

aij     a(1       )  .

by step 1 of the algorithm,   i = a(1       )  , so aij       i for all j     tl. so, either ri = tl
or tl (cid:40) ri. in the latter case, the pruning step will set   aij = 0 for all j, since topic l has
some catchword i0 for which ri0 = tl by lemma 9.8.

9.9 finding the term-topic matrix

for this, we need an extra assumption, which we    rst motivate. suppose as in section
9.8, we assume that there is a single pure document for each topic. in terms of the figure

327

9.1 of three topics, this says that there is a column of p close to each vertex of the tri-
angle. but the corresponding column of a can be very far from this. so, even if we were
told which document is pure for each topic, we cannot    nd the column of b. however, if
we had a large number of nearly pure documents for each topic, since the corresponding
columns of a are independent even conditioned on p , the average of these columns gives
us a good estimate of the column of b. we also note that there is a justi   cation for
assuming the existence of a number of documents which are nearly pure for each topic
based on the id44 model, (see (2) of section 9.6). the assumption
is:

assumption (4): set of pure documents for each l, there is a set wl of at least

  n documents with

clj     1       
4

   j     wl.

if we could    nd the set of pure documents for each topic with possibly a small fraction
of errors, we could average them. the major task of this section is to state and prove an
algorithm that does this. for this, we use the primary topic classi   cation, t1, t2, . . . , tr
from the last section. we know that a for catchword i of topic l, the maximum value of
pij, j = 1, 2, . . . , n occurs for a pure document and indeed if the assumption above holds,
the set of   n/4 documents with the top   n/4 values of pij should be all pure documents.
but to make use of this, we need to know the catchword, which we are not given. to
discover them, we use another property of catchwords. if i is a catchword for topic l,
(cid:54)= l, the values of pij are (substantially) lower. so we know that if i is a
then on tl(cid:48), l(cid:48)
catchword of topic l, then it has the property:

property:

  n/4 th maximum value among pij, j     tl is substantially higher than

than the   n/4 th maximum value among pij, j     tl(cid:48) for any l(cid:48) (cid:54)= l.

we can computationally recognize the property for a (not p ) and on the lines of

lemma 9.6, we can show that it holds essentially for a if and only if it holds for p .

but then, we need to prove a converse of the statement above, namely we need to
show that if the property holds for i and l, then i is a catchword for topic l. since catch-
words are not necessarily unique, this is not quite true. but we will prove that any i
satisfying the property for topic l does have bil(cid:48) <   bil    l(cid:48) (cid:54)= l (lemma 9.11) and so acts
essentially like a catchword. using this, we will show that the   n/4 documents among all
documents with the highest values of aij for an i satisfying the property, will be nearly
pure documents on topic l in lemma 9.12 and use this to argue that their average gives
a good approximation to column l of b (theorem 9.13).

the extra steps in the algorithm: (by the theorem, the tl, l = 1, 2, . . . , r are now

known.)

1. for l = 1, 2, . . . , r, and for i = 1, 2, . . . , d, let g(i, l) be the (1    (  /4))/;/;th fractile of

328

{aij : j     tl}. 40

2. for each l, choose an i(l), (we will prove there is at least 1) such that
g(i(l), l)     (1     (  /2))   ; g(i(l), l(cid:48))     (1     2  )    g(i(l), l)    l(cid:48) (cid:54)= l.

(9.14)

3. let rl be the set of   n/4 j   s among j = 1, 2, . . . , n with the highest ai(l),j.

4. return   b  ,l = 1|rl|

j   rl

a  ,j as our approximation to b  ,l.

(cid:80)

lemma 9.9 i(l) satisfying (9.14) exists for each l.
proof: let i be a catchword for l. then, since,    j     wl, pij     bilclj     bil(1     (  /4)) and
bil       , we have aij     bil(1     (  /2)) and so g(i, l)     (1     (  /2))bil     (1     (  /2))  , by
lemma 9.6. for j /    tl,

aij     bil  (1     (5  /2))

by claim 1.2 and so g(i, l(cid:48))     bil  (1     (5  /2)). so g(i, l) satis   es both the requirements
of step 2 of the algorithm.

fix attention on one l. let i = i(l). let

4  (cid:1)   .
lemma 9.10   i    (cid:0)1     3
proof: we have pij =(cid:80)r

  i =

r

max
k=1

bik.

k=1 bikckj       i for all i. so, aij       i +     

9.6. so, either,   i        whence the lemma clearly holds or   i <    and
      j =    g(i, l)       i + (    /4)  .

   j, aij       i +

    
4

4 max(  ,   i), from lemma

by de   nition of i(l), g(i, l)     (1     (  /2))  , so

  i +

    
4

       (1     (  /2))  ,

from which the lemma follows.
lemma 9.11 bik       bil for all k (cid:54)= l.
40the   th fractile of a set s of real numbers is the largest real number a so that at least   |s| elements

of s are each greter than or equal to a.

329

proof: suppose not. let

we have bil(cid:48) >   bil and

l(cid:48) = arg max
k:k(cid:54)=l

bik.

  i = max(bil, bil(cid:48)) <

since for j     wl(cid:48), at most   /4 weight is put on topics other than l(cid:48),

bil(cid:48)
  

.

(cid:18)

   j     wl(cid:48) , pij     bil(cid:48)(1       
4

)) +

  
4

  i < bil(cid:48)

1       
4

+

  
4  

also, for j     wl(cid:48),

by lemma 9.6,

pij     bil(cid:48)cl(cid:48)j     bil(cid:48)(1       
4

).

(cid:19)

.

(9.15)

(9.16)

(9.17)

max(  , pij)     bil(cid:48)(1       
4

)         
4

  i

1     (3  /4)

by lemma 9.10

(cid:18)

   j     wl(cid:48), aij     pij         
4
    bil(cid:48)

  

,

(cid:19)

1     3
4

using (9.15) and        0.4. from (9.18), it follows that

g(i, l(cid:48))     bil(cid:48)
since pij       i for all j, using lemma 9.10,

   j, aij       i + (    /4)max(  , pij)       i

by (9.15); this implies

(cid:19)

1     3
4

  

.

(cid:18)
(cid:18)

(cid:19)

    
4

1

1     (3  /4)

< bil(cid:48)

1 + (5  /6)

  

,

1 +

g(i, l)     bil(cid:48)(1 + (5  /6))

  

.

now, the de   ntion of i(l) implies

g(i, l(cid:48))     g(i, l)  (1     2  )     bil(cid:48)(1 + (5  /6))  (1     2  )/       bil(cid:48)(1       )

contradicting (9.19) and proving lemma 9.11.
lemma 9.12 for each j     rl of step 3 of the algorithm, we have

clj     1     2  .

330

(9.18)

(9.19)

(9.20)

proof: let j = {j : clj < 1     2  }. take a j     j. we argue that j /    rl.

pij     bil(1     2  ) + 2    bil     bil(1     1.2  ),

by lemma 9.11 using        0.4. so for j     j, we have

aij     bil(1     1.2  ) +

    
4

max(  , bil) < bil(1       ).

but

   j     wl, pij     bil(1       
4

) =    aij     bil(1       ) =    g(i, l)     bil(1       ).

so for no j     j is aij     g(i, l) and hence no j     j belong sto rl.

theorem 9.13 assume

for all l, 1     l     r, the   b  ,l returned by step 4 of the algorithm satis   es

n     cd

m  3 ; m     c
  2 .

||b  ,l       b  ,l||1     6  .

proof: recall that bc = p . let v = a     p . from lemma 9.12, we know that for each
j     rl, clj     1     2  . so

p  ,j = (1       )b  ,l + v,

where,        2   and v is a combination of other columns of b with ||v||1            2  . thus,
we have that

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
so it su   ces now to show that(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

|rl|

|rl|

(cid:88)
(cid:88)

j   rl

j   rl

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

p  ,j     b  ,l

p  ,j     a  ,l

    2  .

    2  .

(9.21)

(9.22)

note that for an individual j     rl, ||a  ,j     p  ,j||1 can be almost two. for example, if each
pi,j = 1/d, then, aij would be 1/m for a random subset of m j    s and zero for the rest.
what we exploit is that when we average over    (n) j    s in rl, the error is small. for this,
the independence of a  ,j, j     rl would be useful. but they are not necessarily independent,
there being conditioning on the fact that they all belong to rl. but there is a simple way
around this conditioning. namely, we prove (9.22) with very high id203 for each

r     [n],|r| =   n/4 and then just take the union bound over all(cid:0) n

(cid:1) such subsets.

(  n/4)

we know that e (a  ,j) = p  ,j. now consider the random variable x de   ned by

x =

1
|r|

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1

v  ,j

.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

j   r

331

e

=

=

i=1

i=1

i=1

vij

vij

j   r

j   r

j   r

j   r

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

e(v2

(9.23)

1
|r|

e(x) =

v  ,j||1

1
|r|e

    1
|r|

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

we also have to bound e(x).

x is a function of m|r| independent random variables, namely, the choice of m|r| terms
in the |r| documents. changing any one, changes x by at most 1/m|r|. so the bounded
di   erence inequality from id203 (??? ref ???) implies that

d(cid:88)
(cid:33)2      jensen   s inequality:e(y)    (cid:112)e(y2)
ij) since {vij, j     r}are indep. and var adds up
(cid:33)1/2

prob (|x     ex| >   )     2 exp(cid:0)     2  mn/8(cid:1) .
(cid:33)
(cid:32)
||(cid:88)
(cid:118)(cid:117)(cid:117)(cid:117)(cid:116)e
      (cid:32)(cid:88)
d(cid:88)
(cid:115)(cid:88)
d(cid:88)
(cid:32) d(cid:88)
(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116)(cid:88)
d(cid:88)
ij) = pij/m and (cid:80)
i pij = 1 and by hypothesis, n     cd/m  3. using this along
           2 exp(cid:0)   c  3mn(cid:1) ,
      (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:88)
           2 exp(cid:0)   c  3mn + c  n(cid:1)       ,
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:88)
(cid:1)     (cn/  n)  n/4     exp(c  n) and m     c/  2 by hypothesis.
because the number of r is(cid:0) n

since e(v2
with (9.23), we see that for a single r     {1, 2, . . . , n} with |r| =   n/4,

chauchy-schwartz
   
d   
m  n

         r,|r| =

which implies using the union bound that

1
|r|
   
d
|r|
   
d
|r|

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1

ij)    

    2  

    2  

      ,

  n
4

e(v2

|r|

|r|

e(v2

prob

prob

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

j   r

j   r

j   r

v  ,j

v  ,j

   

ij)

i=1

i=1

=

:

j

this completes the proof of the theorem.

(  n/4)

9.10 id48

a hidden markov model (id48) consists of a    nite set of states with a transition
between each pair of states. there is an initial id203 distribution    on the states

332

and a transition id203 aij associated with the transition from state i to state j. each
state also has a id203 distribution p(o, i) giving the id203 of outputting the
symbol o in state i. a transition consists of two components. a state transition to a
new state followed by the output of a symbol. the id48 starts by selecting a start state
according to the distribution    and outputting a symbol.

example: an example of an id48 with two states q and p and two output symbols h
and t is illustrated below.

1
2

q
2h 1
2t

1

3
4

1
2

p
3h 1
3t

2

1
4

the initial distribution is   (q) = 1 and   (p) = 0. at each step a change of state occurs
followed by the output of heads or tails with id203 determined by the new state.

we consider three problems in increasing order of di   culty. first, given an id48
what is the id203 of a given output sequence? second, given an id48 and an out-
put sequence, what is the most likely sequence of states? and third, knowing that the
id48 has at most n states and given an output sequence, what is the most likely id48?
only the third problem concerns a    hidden    markov model. in the other two problems,
the model is known and the questions can be answered in polynomial time using dynamic
programming. there is no known polynomial time algorithm for the third question.

how probable is an output sequence

given an id48, how probable is the output sequence o = o0o1o2        ot of length
t +1? to determine this, calculate for each state i and each initial segment of the sequence
of observations, o0o1o2        ot of length t + 1, the id203 of observing o0o1o2        ot
ending in state i. this is done by a id145 algorithm starting with t = 0
and increasing t. for t = 0 there have been no transitions. thus, the id203 of
observing o0 ending in state i is the initial id203 of starting in state i times the
id203 of observing o0 in state i. the id203 of observing o0o1o2        ot ending
in state i is the sum of the probabilities over all states j of observing o0o1o2        ot   1
ending in state j times the id203 of going from state j to state i and observing ot.
the time to compute the id203 of a sequence of length t when there are n states is
o(n2t ). the factor n2 comes from the calculation for each time unit of the contribution
from each possible previous state to the id203 of each possible current state. the
space complexity is o(n) since one only needs to remember the id203 of reaching

333

each state for the most recent value of t.

algorithm to calculate the id203 of the output sequence

the id203, prob(o0o1        ot , i) of the output sequence o0o1        ot ending in

state i is given by

prob(o0, i) =   (i)p(o0, i)

and for t = 1 to t

prob(o0o1        ot, i) =(cid:80)

j

prob(o0o1        ot   1, j)aijp(ot+1, i).

example: what is the id203 of the sequence hhht by the id48 in the above two
state example?

1
2

72

1

1

2 + 5
2 + 1
2 = 1

3
4

6

1

8

t = 3

t = 2

t = 1

t = 0

3
32

1
2

1
2

1
8

1
2

1
2

3
4

1

2 = 19

384

3
32

1
2

1

2 = 3

32

1
8

1
2

1
2

1
2

0

72

2

1

3 + 5
3 + 1
3 = 1

1
4

6

2

6

1
4

1

3 = 37
64  27

2

3 = 5

72

q

p

for t = 0, the q entry is 1/2 since the id203 of being in state q is one and the proba-
bility of outputting heads is 1
2. the entry for p is zero since the id203 of starting in
state p is zero. for t = 1, the q entry is 1
2 and in state q
the id48 goes to state q with id203 1
2. the
p entry is 1
2 and in state q the id48 goes to state p with
id203 1
32 which
consists of two terms. the    rst term is the id203 of ending in state q at t = 1 times
the id203 of staying in q and outputting h. the second is the id203 of ending
in state p at t = 1 times the id203 of going from state p to state q and outputting h.

6 since for t = 0 the q entry is 1
2 and outputs heads with id203 2

2 and outputs heads with id203 1

8 since for t = 0 the q entry is 1

3. for t = 2, the q entry is 3

from the table, the id203 of producing the sequence hhht is 19

384 + 37

1728 = 0.0709.

the most likely sequence of states - the viterbi algorithm

given an id48 and an observation o = o0o1        ot , what is the most likely sequence
of states? the solution is given by the viterbi algorithm, which is a slight modi   cation
to the id145 algorithm just given for determining the id203 of an
output sequence. for t = 0, 1, 2, . . . , t and for each state i, we calculate the id203

334

of the most likely sequence of states to produce the output o0o1o2        ot ending in state
i as follows. for each state j, we have already computed the id203 of the most
likely sequence producing o0o1o2        ot   1 ending in state j, and we multiply this by the
id203 of the transition from j to i producing ot. we then select the j for which this
product is largest. note that in the previous example, we added the probabilities of each
possibility together. now we take the maximum and also record where the maximum
came from. the time complexity is o(n2t ) and the space complexity is o(nt ). the
space complexity bound is argued as follows. in calculating the id203 of the most
likely sequence of states that produces o0o1 . . . ot ending in state i, we remember the
previous state j by putting an arrow with edge label t from i to j. at the end, can    nd
the most likely sequence by tracing backwards as is standard for id145
algorithms.

t = 3

example: for the earlier example what is the most likely sequence of states to produce
the output hhht?
max{ 1
max{ 1
2 = 1
q

max{ 3
max{ 1
3 = 1

2} = 1
2} = 3

3} = 1
3} = 1

2, 1
2, 1
q

3, 1
3, 1
q

q or p

t = 2

t = 1

t = 0

0 p

3
4

1
2

1
2

1
2

1
2

1
2

1
4

1
2

1
2

3
4

1
2

1
4

p

48

24

64

48

48

24

96

24

q

q

1

1

8

1

6

1

1

8

1

1

8

2

6

2

2

6

1
2

q

p

note that the two sequences of states, qqpq and qpqq, are tied for the most likely se-
quences of states.

determining the underlying hidden markov model

given an n-state id48, how do we adjust the transition probabilities and output prob-
abilities to maximize the id203 of an output sequence o1o2        ot ? the assumption
is that t is much larger than n.41 there is no known computationally e   cient method
for solving this problem. however, there are iterative techniques that converge to a local
optimum.

let aij be the transition id203 from state i to state j and let bj(ok) be the
id203 of output ok given that the id48 is in state j. given estimates for the id48
parameters, aij and bj, and the output sequence o, we can improve the estimates by
calculating for each time step the id203 that the id48 goes from state i to state j
and outputs the symbol ok, conditioned on o being the output sequence.

41if t     n then one can just have the id48 be a linear sequence that outputs o1o2 . . . ot with

id203 1.

335

aij

transition id203 from state i to state j

bj(ot+1) id203 of ot+1 given that the id48 is in state j at time t + 1

  t(i)

  t+1(j)

id203 of seeing o0o1        ot and ending in state i at time t
id203 of seeing the tail of the sequence ot+2ot+3        ot given state j
at time t + 1

  (i, j)

id203 of going from state i to state j at time t given the sequence
of outputs o

st(i)

id203 of being in state i at time t given the sequence of outputs o

p(o)

id203 of output sequence o

given estimates for the id48 parameters, aij and bj, and the output sequence o, the
id203   t(i, j) of going from state i to state j at time t is given by the id203 of
producing the output sequence o and going from state i to state j at time t divided by
the id203 of producing the output sequence o.

  t(i, j) =

  t(i)aijbj(ot+1)  t+1(j)

p(o)

the id203 p(o) is the sum over all pairs of states i and j of the numerator in the
above formula for   t(i, j). that is,

(cid:88)

(cid:88)

i

j

p(o) =

  t(j)aijbj(ot+1)  t+1(j).

the id203 of being in state i at time t is given by

st(i) =

  t(i, j).

summing st(i) over all time periods gives the expected number of times state i is visited
and the sum of   t(i, j) over all time periods gives the expected number of times edge i to
j is traversed.

j=1

given estimates of the id48 parameters ai,j and bj(ok), we can calculate by the above

formulas estimates for

1. (cid:80)t   1

i=1 st(i), the expected number of times state i is visited and departed from

n(cid:88)

336

2. (cid:80)t   1

i=1   t(i, j), the expected number of transitions from state i to state j

using these estimates we can obtain new estimates of the id48 parameters

aij =

expected number of transitions from state i to state j

expected number of transitions out of state i

=

t=1   t(i, j)
t=1 st(i)

(cid:80)t   1
(cid:80)t   1
t   1(cid:80)
(cid:80)t   1

ot=ok

t=1

=

subject to

st(j)

t=1 st(j)

bj(ok) =

expected number of times in state j observing symbol ok

expected number of times in state j

by iterating the above formulas we can arrive at a local optimum for the id48 parameters
ai,j and bj(ok).

9.11 id114 and belief propagation

a graphical model is a compact representation of a id203 distribution over n
variables x1, x2, . . . , xn. it consists of a graph, directed or undirected, whose vertices cor-
respond to variables that take on values from some set. in this chapter, we consider the
case where the set of values the variables take on is    nite, although id114 are
often used to represent id203 distributions with continuous variables. the edges of
the graph represent relationships or constraints between the variables.

in the directed model, it is assumed that the directed graph is acyclic. this model
represents a joint id203 distribution that factors into a product of conditional prob-
abilities.

p (x1, x2, . . . , xn) =

p (xi|parents of xi)

n(cid:89)

the directed graphical model is called a bayesian or belief network and appears frequently
in the arti   cial intelligence and the statistics literature.

i=1

the undirected graphical model, called a markov random    eld, can also represent a
joint id203 distribution of the random variables at its vertices. in many applications
the markov random    eld represents a function of the variables at the vertices which is to
be optimized by choosing values for the variables.

a third model called the factor model is akin to the markov random    eld, but here
the dependency sets have a di   erent structure. in the following sections we describe all
these models in more detail.

337

c1

d1

s1

c2

causes

d2

diseases

s2

symptoms

figure 9.2: a id110

9.12 bayesian or belief networks

a id110 is a directed acyclic graph where vertices correspond to variables
and a directed edge from y to x represents a id155 p(x|y). if a vertex x
has edges into it from y1, y2, . . . , yk, then the id155 is p (x | y1, y2, . . . , yk).
the variable at a vertex with no in edges has an unid155 distribution.
if the value of a variable at some vertex is known, then the variable is called evidence.
an important property of a id110 is that the joint id203 is given by the
product over all nodes of the id155 of the node conditioned on all its
immediate predecessors.

in the example of fig. 9.1, a patient is ill and sees a doctor. the doctor ascertains
the symptoms of the patient and the possible causes such as whether the patient was in
contact with farm animals, whether he had eaten certain foods, or whether the patient
has an hereditary predisposition to any diseases. using the above id110 where
the variables are true or false, the doctor may wish to determine one of two things. what
is the marginal id203 of a given disease or what is the most likely set of diseases. in
determining the most likely set of diseases, we are given a t or f assignment to the causes
and symptoms and ask what assignment of t or f to the diseases maximizes the joint
id203. this latter problem is called the maximum a posteriori id203 (map).

given the conditional probabilities and the probabilities p (c1) and p (c2) in figure
9.1, the joint id203 p (c1, c2, d1, . . .) can be computed easily for any combination
of values of c1, c2, d1, . . .. however, we might wish to    nd the value of the variables of
highest id203 (map) or we might want one of the marginal probabilities p (d1) or
p (d2). the obvious algorithms for these two problems require evaluating the probabil-
ity p (c1, c2, d1, . . .) over exponentially many input values or summing the id203
p (c1, c2, d1, . . .) over exponentially many values of the variables other than those for

338

which we want the marginal id203. in certain situations, when the joint id203
distribution can be expressed as a product of factors, a belief propagation algorithm can
solve the maximum a posteriori problem or compute all marginal probabilities quickly.

9.13 markov random fields

the markov random    eld model arose    rst in statistical mechanics where it was called
the ising model. it is instructive to start with a description of it. the simplest version
n grid. each
of the ising model consists of n particles arranged in a rectangular
particle can have a spin that is denoted   1. the energy of the whole system depends
on interactions between pairs of neighboring particles. let xi be the spin,   1, of the ith
particle. denote by i     j the relation that i and j are adjacent in the grid. in the ising
model, the energy of the system is given by

n       
   

f (x1, x2, . . . , xn) = exp

c

|xi     xj|

.

(cid:33)

(cid:32)

(cid:88)

i   j

the constant c can be positive or negative. if c < 0, then energy is lower if many adjacent
pairs have opposite spins and if c > 0 the reverse holds. the model was    rst used to
model probabilities of spin con   gurations in physical materials.

minimizing (cid:80)

in most computer science settings, such functions are mainly used as objective func-
tions that are to be optimized subject to some constraints. the problem is to    nd the
minimum energy set of spins under some constraints on the spins. usually the constraints
just specify the spins of some particles. note that when c > 0, this is the problem of
|xi     xj| subject to the constraints. the objective function is convex and
so this can be done e   ciently. if c < 0, however, we need to minimize a concave function
for which there is no known e   cient algorithm. the minimization of a concave func-
tion in general is np-hard. intuitively, this is because the set of inputs for which f (x) is
less than some given value can be nonconvex or even consist of many disconnected regions.

i   j

a second important motivation comes from the area of vision. it has to to do with
reconstructing images. suppose we are given noisy observations of the intensity of light at
individual pixels, x1, x2, . . . , xn, and wish to compute the true values, the true intensities,
of these variables y1, y2, . . . , yn. there may be two sets of constraints, the    rst stipulating
that the yi should generally be close to the corresponding xi and the second, a term
correcting possible observation errors, stipulating that yi should generally be close to the
values of yj for j     i. this can be formulated as

(cid:32)(cid:88)

i

min

y

(cid:33)

(cid:88)

i   j

|xi     yi| +

|yi     yj|

,

where the values of xi are constrained to be the observed values. the objective function
is convex and polynomial time minimization algorithms exist. other objective functions

339

x1 + x2 + x3

x1 + x2

x1 + x3

x2 + x3

x1

x2

x3

figure 9.3: the factor graph for the function

f (x1, x2, x3) = (x1 + x2 + x3)(x1 +   x2)(x1 +   x3)(  x2 +   x3).

using say sum of squares instead of sum of absolute values can be used and there are
polynomial time algorithms as long as the function to be minimized is convex.

more generally, the correction term may depend on all grid points within distance
two of each point rather than just immediate neighbors. even more generally, we may
have n variables y1, y2, . . . yn with the value of some of them already speci   ed and subsets
s1, s2, . . . sm of these variables constrained in some way. the constraints are accumulated
into one objective function which is a product of functions f1, f2, . . . , fm, where function fi
i=1 fi(yj, j     si)
subject to constrained values. note that the vision example had a sum instead of a prod-
uct, but by taking exponentials we can turn the sum into a product as in the ising model.

is evaluated on the variables in subset si. the problem is to minimize(cid:81)m

in general, the fi are not convex; indeed they may be discrete. so the minimization
cannot be carried out by a known polynomial time algorithm. the most used forms of the
markov random    eld involve si which are cliques of a graph. so we make the following
de   nition.

a markov random field consists of an undirected graph and an associated function
that factorizes into functions associated with the cliques of the graph. the special case
when all the factors correspond to cliques of size one or two is of interest.

9.14 factor graphs

can be expressed as f (x) = (cid:81)

factor graphs arise when we have a function f of a variables x = (x1, x2, . . . , xn) that
f   (x  ) where each factor depends only on some small

number of variables x  . the di   erence from markov random    elds is that the variables
corresponding to factors do not necessarily form a clique. associate a bipartite graph
where one set of vertices correspond to the factors and the other set to the variables.

  

340

place an edge between a variable and a factor if the factor contains that variable. see
figure 9.3.

9.15 tree algorithms

let f (x) be a function that is a product of factors. when the factor graph is a tree
there are e   cient algorithms for solving certain problems. with slight modi   cations, the
algorithms presented can also solve problems where the function is the sum of terms rather
than a product of factors.

the    rst problem is called marginalization and involves evaluating the sum of f over
all variables except one. in the case where f is a id203 distribution the algorithm
computes the marginal probabilities and thus the word marginalization. the second prob-
lem involves computing the assignment to the variables that maximizes the function f .
when f is a id203 distribution, this problem is the maximum a posteriori probabil-
ity or map problem.

if the factor graph is a tree (such as in figure 9.4), then there exists an e   cient al-
gorithm for solving these problems. note that there are four problems: the function f
is either a product or a sum and we are either marginalizing or    nding the maximizing
assignment to the variables. all four problems are solved by essentially the same algo-
rithm and we present the algorithm for the marginalization problem when f is a product.
assume we want to    sum out    all the variables except x1, leaving a function of x1.

call the variable node associated with some variable xi node xi. first, make the node
x1 the root of the tree.
it will be useful to think of the algorithm    rst as a recursive
algorithm and then unravel the recursion. we want to compute the product of all factors
occurring in the sub-tree rooted at the root with all variables except the root-variable
summed out. let gi be the product of all factors occurring in the sub-tree rooted at
node xi with all variables occurring in the subtree except xi summed out. since this is a
tree, x1 will not reoccur anywhere except the root. now, the grandchildren of the root
are variable nodes and suppose inductively, each grandchild xi of the root, has already
computed its gi. it is easy to see that we can compute g1 as follows.

each grandchild xi of the root passes its gi to its parent, which is a factor node. each
child of x1 collects all its children   s gi, multiplies them together with its own factor and
sends the product to the root. the root multiplies all the products it gets from its children
and sums out all variables except its own variable, namely here x1.

unraveling the recursion is also simple, with the convention that a leaf node just re-
ceives 1, product of an empty set of factors, from its children. each node waits until it
receives a message from each of its children. after that, if the node is a variable node,
it computes the product of all incoming messages, and sums this product function over

341

x1

x1

x1 + x2 + x3

x3 + x4 + x5

x2

x3

x4

x4

x5

x5

figure 9.4: the factor graph for the function f = x1 (x1 + x2 + x3) (x3 + x4 + x5) x4x5.

all assignments to the variables except for the variable of the node. then, it sends the
resulting function of one variable out along the edge to its parent. if the node is a factor
node, it computes the product of its factor function along with incoming messages from
all the children and sends the resulting function out along the edge to its parent.

the reader should prove that the following invariant holds assuming the graph is a tree:

invariant the message passed by each variable node to its parent is the product of
all factors in the subtree under the node with all variables in the subtree except its own
summed out.

consider the following example where

f = x1 (x1 + x2 + x3) (x3 + x4 + x5) x4x5

and the variables take on values 0 or 1. consider marginalizing f by computing

f (x1) =

x1 (x1 + x2 + x3) (x3 + x4 + x5) x4x5,

(cid:88)

x2x3x4x5

in this case the factor graph is a tree as shown in figure 9.4. the factor graph as a
rooted tree and the messages passed by each node to its parent are shown in figure 9.5.
if instead of computing marginals, one wanted the variable assignment that maximizes
the function f , one would modify the above procedure by replacing the summation by a
maximization operation. obvious modi   cations handle the situation where f (x) is a sum
of products.

(cid:88)

f (x) =

g (x)

x1,...,xn

9.16 message passing in general graphs

the simple message passing algorithm in the last section gives us the one variable
function of x1 when we sum out all the other variables. for a general graph that is not

342

(cid:80)

x2,x3

x1(x1 + x2 + x3)(2 + x3) = 10x2

1 + 11x1

x1

x1    

x1

x1 + x2 + x3

1    

x2

x3

(x1 + x2 + x3)(2 + x3)    

(cid:80)

x4,x5

(x3 + x4 + x5)x4x5
= 2 + x3    

(x3 + x4 + x5)x4x5    

x3 + x4 + x5

x4    

x4    

x4

x4

x5    

x5    

x5

x5

figure 9.5: messages.

a tree, we formulate an extension of that algorithm. but unlike the case of trees, there
is no proof that the algorithm will converge and even if it does, there is no guarantee
that the limit is the marginal id203. this has not prevented its usefulness in some
applications.

first, lets ask a more general question, just for trees. suppose we want to compute
for each i the one-variable function of xi when we sum out all variables xj, j (cid:54)= i. do we
have to repeat what we did for x1 once for each xi? luckily, the answer is no. it will
su   ce to do a second pass from the root to the leaves of essentially the same message
passing algorithm to get all the answers. recall that in the    rst pass, each edge of the
tree has sent a message    up   , from the child to the parent. in the second pass, each edge
will send a message down from the parent to the child. we start with the root and work
downwards for this pass. each node waits until its parent has sent it a message before
sending messages to each of its children. the rules for messages are:

343

rule 1 the message from a factor node v to a child xi, which is the variable node xi,
is the product of all messages received by v in both passes from all nodes other than xi
times the factor at v itself.

rule 2 the message from a variable node xi to a factor node child, v, is the product
of all messages received by xi in both passes from all nodes except v, with all variables
except xi summed out. the message is a function of xi alone.

at termination, when the graph is a tree, if we take the product of all messages re-
ceived in both passes by a variable node xi and sum out all variables except xi in this
product, what we get is precisely the entire function marginalized to xi. we do not give
the proof here. but the idea is simple. we know from the    rst pass that the product of
the messages coming to a variable node xi from its children is the product of all factors in
the sub-tree rooted at xi. in the second pass, we claim that the message from the parent
v to xi is the product of all factors which are not in the sub-tree rooted at xi which one
can show either directly or by induction working from the root downwards.

we can apply the same rules 1 and 2 to any general graph. we do not have child and
parent relationships and it is not possible to have the two synchronous passes as before.
the messages keep    owing and one hopes that after some time, the messages will stabilize,
but nothing like that is proven. we state the algorithm for general graphs now:

rule 1 at each unit of time, each factor node v sends a message to each adjacent
node xi. the message is the product of all messages received by v at the previous step
except for the one from xi multiplied by the factor at v itself.

rule 2 at each time, each variable node xi sends a message to each adjacent node v.
the message is the product of all messages received by xi at the previous step except the
one from v, with all variables except xi summed out.

9.17 graphs with a single cycle

the message passing algorithm gives the correct answers on trees and on certain other
graphs. one such situation is graphs with a single cycle which we treat here. we switch
from the marginalization problem to the map problem as the proof of correctness is
simpler for the map problem. consider the network in figure 9.6a with a single cycle.
the message passing scheme will count some evidence multiply. the local evidence at a
will get passed around the loop and will come back to a. thus, a will count the local
evidence multiple times. if all evidence is multiply counted in equal amounts, then there
is a possibility that though the numerical values of the marginal probabilities (beliefs) are
wrong, the algorithm still converges to the correct maximum a posteriori assignment.

consider the unwrapped version of the graph in figure 9.6b. the messages that the

344

a

b

c

(a) a graph with a single cycle

a

b

c

a

b

c

a

b

c

(b) segment of unrolled graph

figure 9.6: unwrapping a graph with a single cycle

loopy version will eventually converge to, assuming convergence, are the same messages
that occur in the unwrapped version provided that the nodes are su   ciently far in from
the ends. the beliefs in the unwrapped version are correct for the unwrapped graph since
it is a tree. the only question is, how similar are they to the true beliefs in the original
network.

write p (a, b, c) = elog p(a,b,c) = ej(a,b,c) where j (a, b, c) = log p (a, b, c). then
where the j(cid:48) is
the id203 for the unwrapped network is of the form ekj(a,b,c)+j(cid:48)
associated with vertices at the ends of the network where the beliefs have not yet stabi-
lized and the kj (a, b, c) comes from k inner copies of the cycle where the beliefs have
stabilized. note that the last copy of j in the unwrapped network shares an edge with j(cid:48)
and that edge has an associated   . thus, changing a variable in j has an impact on the
value of j(cid:48) through that   . since the algorithm maximizes jk = kj (a, b, c) + j(cid:48) in the
unwrapped network for all k, it must maximize j (a, b, c). to see this, set the variables
a, b, c, so that jk is maximized. if j (a, b, c) is not maximized, then change a, b, and
c to maximize j (a, b, c). this increases jk by some quantity that is proportional to
k. however, two of the variables that appear in copies of j (a, b, c) also appear in j(cid:48)

345

y2

x2

y3

x3

x4

y4

y1

yn

x1

xn

figure 9.7: a markov random    eld with a single loop.

and thus j(cid:48) might decrease in value. as long as j(cid:48) decreases by some    nite amount, we
can increase jk by increasing k su   ciently. as long as all      s are nonzero, j(cid:48) which is
proportional to log   , can change by at most some    nite amount. hence, for a network
with a single loop, assuming that the message passing algorithm converges, it converges
to the maximum a posteriori assignment.

9.18 belief update in networks with a single loop

in the previous section, we showed that when the message passing algorithm converges,
it correctly solves the map problem for graphs with a single loop. the message passing
algorithm can also be used to obtain the correct answer for the marginalization problem.
consider a network consisting of a single loop with variables x1, x2, . . . , xn and evidence
y1, y2, . . . , yn as shown in figure 9.7. the xi and yi can be represented by vectors having
a component for each value xi can take on. to simplify the discussion assume the xi take
on values 1, 2, . . . , m.

let mi be the message sent from vertex i to vertex i + 1 mod n. at vertex i + 1
each component of the message mi is multiplied by the evidence yi+1 and the constraint
function   . this is done by forming a diagonal matrix di+1 where the diagonal elements
are the evidence and then forming a matrix mi whose jkth element is    (xi+1 = j, xi = k).
the message mi+1 is midi+1mi. multiplication by the diagonal matrix di+1 multiplies
the components of the message mi by the associated evidence. multiplication by the
matrix mi multiplies each component of the vector by the appropriate value of    and
sums over the values producing the vector which is the message mi+1. once the message

346

has travelled around the loop, the new message m(cid:48)

1 is given by
1 = mnd1mn   1dn        m2d3m1d2m1
m(cid:48)

let m = mnd1mn   1dn        m2d3m1d2m1. assuming that m    s principal eigenvalue is
unique, the message passing will converge to the principal vector of m . the rate of con-
vergences depends on the ratio of the    rst and second eigenvalues.

an argument analogous to the above concerning the messages going clockwise around
the loop applies to messages moving counter-clockwise around the loop. to obtain the es-
timate of the marginal id203 p (x1), one multiples component-wise the two messages
arriving at x1 along with the evidence y1. this estimate does not give the true marginal
id203 but the true marginal id203 can be computed from the estimate and the
rate of convergences by id202.

9.19 maximum weight matching

we have seen that the belief propagation algorithm converges to the correct solution
in trees and graphs with a single cycle. it also correctly converges for a number of prob-
lems. here we give one example, the maximum weight matching problem where there is
a unique solution.

we apply the belief propagation algorithm to    nd the maximal weight matching
(mwm) in a complete bipartite graph. if the mwm in the bipartite graph is unique,
then the belief propagation algorithm will converge to it.

{b1, . . . , bn} , and (ai, bj)     e,

let g = (v1, v2, e) be a complete bipartite graph where v1 = {a1, . . . , an} , v2 =
1     i, j     n. let    = {   (1) , . . . ,    (n)} be a per-

mutation of {1, . . . , n}. the collection of edges (cid:8)(cid:0)a1, b  (1)

(cid:1) , . . . ,(cid:0)an, b  (n)

(cid:1)(cid:9)is called a

matching which is denoted by   . let wij be the weight associated with the edge (ai, bj).
wi  (i). the maximum weight matching       is

n(cid:80)

i=1

the weight of the matching    is w   =
      = arg max

w  

  

the    rst step is to create a factor graph corresponding to the mwm problem. each
edge of the bipartite graph is represented by a variable cij which takes on the value zero or
one. the value one means that the edge is present in the matching, the value zero means
that the edge is not present in the matching. a set of constraints is used to force the set
cij = 1. any

of edges to be a matching. the constraints are of the form(cid:80)

cij = 1 and(cid:80)

0,1 assignment to the variables cij that satis   es all of the constraints de   nes a matching.
in addition, we have constraints for the weights of the edges.

j

i

we now construct a factor graph, a portion of which is shown in figure 9.8. associated
with the factor graph is a function f (c11, c12, . . .) consisting of a set of terms for each cij

347

enforcing the constraints and summing the weights of the edges of the matching. the
terms for c12 are

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:32)(cid:88)

i

     

    1

ci2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)       

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:32)(cid:88)

j

(cid:33)

    1

c1j

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) + w12c12

where    is a large positive number used to enforce the constraints when we maximize the
function. finding the values of c11, c12, . . . that maximize f    nds the maximum weighted
matching for the bipartite graph.

if the factor graph was a tree, then the message from a variable node x to its parent
is a message g(x) that gives the maximum value for the subtree for each value of x. to
compute g(x), one sums all messages into the node x. for a constraint node, one sums
all messages from subtrees and maximizes the sum over all variables except the variable
of the parent node subject to the constraint. the message from a variable x consists of
two pieces of information, the value p (x = 0) and the value p (x = 1). this information
can be encoded into a linear function of x:

[p (x = 1)     p (x = 0)] x + p (x = 0) .

thus, the messages are of the form ax + b. to determine the map value of x once the
algorithm converges, sum all messages into x and take the maximum over x=1 and x=0
to determine the value for x. since the arg maximum of a linear form ax +b depends
only on whether a is positive or negative and since maximizing the output of a constraint
depends only on the coe   cient of the variable, we can send messages consisting of just
the variable coe   cient.

to calculate the message to c12 from the constraint that node b2 has exactly one
neighbor, add all the messages that    ow into the constraint node from the ci2, i (cid:54)= 1
nodes and maximize subject to the constraint that exactly one variable has value one. if
c12 = 0, then one of ci2, i (cid:54)= 1, will have value one and the message is max
   (i, 2). if
i(cid:54)=1
c12 = 1, then the message is zero. thus, we get

    max
i(cid:54)=1

   (i, 2) x + max
i(cid:54)=1

   (i, 2)

and send the coe   cient     max
i(cid:54)=1
constraint node is   (1, 2) = w12     max
i(cid:54)=1

   (i, 2).

   (i, 2). this means that the message from c12 to the other

the alpha message is calculated in a similar fashion. if c12 = 0, then one of c1j will
   (1, j). if c12 = 1, then the message is zero. thus,

have value one and the message is max
j(cid:54)=1
the coe   cient     max
j(cid:54)=1

   (1, j) is sent. this means that   (1, 2) = w12     max
j(cid:54)=1

   (1, j).

to prove convergence, we unroll the constraint graph to form a tree with a constraint
node as the root. in the unrolled graph a variable node such as c12 will appear a number

348

(cid:80)

j c1j = 1

w12c12

c12

   

   

  (1, 2)

  (1, 2)

(cid:80)

i ci2 = 1

constraint forcing
b2 to have exactly
one neighbor

constraint forcing
a1to have exactly
one neighbor

c32

c42

cn2

figure 9.8: portion of factor graph for the maximum weight matching problem.

of times which depends on how deep a tree is built. each occurrence of a variable such
as c12 is deemed to be a distinct variable.

lemma 9.14 if the tree obtained by unrolling the graph is of depth k, then the messages
to the root are the same as the messages in the constraint graph after k-iterations.

proof: straightforward.

de   ne a matching in the tree to be a set of vertices so that there is exactly one variable
node of the match adjacent to each constraint. let    denote the vertices of the matching.
heavy circles in figure 9.9 represent the nodes of the above tree that are in the matching   .

let    be the vertices corresponding to maximum weight matching edges in the bi-
partite graph. recall that vertices in the above tree correspond to edges in the bipartite
graph. the vertices of    are denoted by dotted circles in the above tree.

consider a set of trees where each tree has a root that corresponds to one of the con-
straints. if the constraint at each root is satis   ed by the edge of the mwm, then we have
found the mwm. suppose that the matching at the root in one of the trees disagrees
with the mwm. then there is an alternating path of vertices of length 2k consisting of
vertices corresponding to edges in    and edges in   . map this path onto the bipartite
graph. in the bipartite graph the path will consist of a number of cycles plus a simple
path. if k is large enough there will be a large number of cycles since no cycle can be of
length more than 2n. let m be the number of cycles. then m     2k

2n = k
n.

let       be the mwm in the bipartite graph. take one of the cycles and use it as an
alternating path to convert the mwm to another matching. assuming that the mwm
is unique and that the next closest matching is    less, w          w   >    where    is the new

349

(cid:80)

j c1j = 1

c11

c13

c1n

(cid:80)

j ci2 = 1

(cid:80)

i ci2 = 1

c12

c22

c32

cn2

figure 9.9: tree for mwm problem.

matching.

consider the tree matching. modify the tree matching by using the alternating path
of all cycles and the left over simple path. the simple path is converted to a cycle by
adding two edges. the cost of the two edges is at most 2w* where w* is the weight of the
maximum weight edge. each time we modify    by an alternating cycle, we increase the
cost of the matching by at least   . when we modify    by the left over simple path, we
increase the cost of the tree matching by        2w    since the two edges that were used to
create a cycle in the bipartite graph are not used. thus
weight of    - weight of   (cid:48)     k

n        2w   

which must be negative since   (cid:48) is optimal for the tree. however, if k is large enough this
becomes positive, an impossibility since   (cid:48) is the best possible. since we have a tree, there
can be no cycles, as messages are passed up the tree, each subtree is optimal and hence the
total tree is optimal. thus the message passing algorithm must    nd the maximum weight
matching in the weighted complete bipartite graph assuming that the maximum weight
matching is unique. note that applying one of the cycles that makes up the alternating
path decreased the bipartite graph match but increases the value of the tree. however,
it does not give a higher tree matching, which is not possible since we already have the
maximum tree matching. the reason for this is that the application of a single cycle does
not result in a valid tree matching. one must apply the entire alternating path to go from
one matching to another.

350

a

b

c

i

j

figure 9.10: warning propagation

9.20 warning propagation

signi   cant progress has been made using methods similar to belief propagation in
   nding satisfying assignments for 3-cnf formulas. thus, we include a section on a
version of belief propagation, called warning propagation, that is quite e   ective in    nding
assignments. consider a factor graph for a sat problem (figure 9.10). index the variables
by i, j, and k and the factors by a, b, and c. factor a sends a message mai to each variable i
that appears in the factor a called a warning. the warning is 0 or 1 depending on whether
or not factor a believes that the value assigned to i is required for a to be satis   ed. a
factor a determines the warning to send to variable i by examining all warnings received
by other variables in factor a from factors containing them.

for each variable j, sum the warnings from factors containing j that warn j to take
value t and subtract the warnings that warn j to take value f. if the di   erence says that
j should take value t or f and this value for variable j does not satisfy a, and this is
true for all j, then a sends a warning to i that the value of variable i is critical for factor a.

start the warning propagation algorithm by assigning 1 to a warning with id203
1/2. iteratively update the warnings. if the warning propagation algorithm converges,
then compute for each variable i the local    eld hi and the contradiction number ci. the
local    eld hi is the number of clauses containing the variable i that sent messages that
i should take value t minus the number that sent messages that i should take value f.
the contradiction number ci is 1 if variable i gets con   icting warnings and 0 otherwise.
if the factor graph is a tree, the warning propagation algorithm converges. if one of the
warning messages is one, the problem is unsatis   able; otherwise it is satis   able.

9.21 correlation between variables

in many situations one is interested in how the correlation between variables drops o   
with some measure of distance. consider a factor graph for a 3-cnf formula. measure
the distance between two variables by the shortest path in the factor graph. one might
ask if one variable is assigned the value true, what is the percentage of satisfying assign-
ments of the 3-cnf formula in which the second variable also is true. if the percentage
is the same as when the    rst variable is assigned false, then we say that the two variables

351

are uncorrelated. how di   cult it is to solve a problem is likely to be related to how fast
the correlation decreases with distance.

another illustration of this concept is in counting the number of perfect matchings
in a graph. one might ask what is the percentage of matching in which some edge is
present and ask how correlated this percentage is with the presences or absence of edges
at some distance d. one is interested in whether the correlation drops o    with distance.
to explore this concept we consider the ising model studied in physics.

as mentioned earlier, the ising or ferromagnetic model is a pairwise random markov
   eld. the underlying graph, usually a lattice, assigns a value of   1, called spin, to the
variable at each vertex. the id203 (gibbs measure) of a given con   guration of spins
e  xixj where xi =   1 is the value associated

is proportional to exp(   (cid:80)

(i,j)   e

xixj) = (cid:81)
(cid:81)

(i,j)   e

p (x1, x2, . . . , xn) = 1
z

(i,j)   e

   (cid:80)

xixj

(i,j)   e

exp(  xixj) = 1

z e

with vertex i. thus

where z is a id172 constant.

the value of the summation is simply the di   erence in the number of edges whose
vertices have the same spin minus the number of edges whose vertices have opposite spin.
the constant    is viewed as inverse temperature. high temperature corresponds to a low
value of    and low temperature corresponds to a high value of   . at high temperature,
low   , the spins of adjacent vertices are uncorrelated whereas at low temperature adjacent
vertices have identical spins. the reason for this is that the id203 of a con   guration

is proportional to e

. as    is increased, for con   gurations with a large number of

edges whose vertices have identical spins, e
increases more than for con   gurations
whose edges have vertices with non identical spins. when the id172 constant 1
z
is adjusted for the new value of   , the highest id203 con   gurations are those where
adjacent vertices have identical spins.

   (cid:80)

i   j

xixj

   (cid:80)

i   j

xixj

given the above id203 distribution, what is the correlation between two variables
xi and xj? to answer this question, consider the id203 that xi = +1 as a function
of the id203 that xj = +1. if the id203 that xi = +1 is 1
2 independent of the
value of the id203 that xj = +1, we say the values are uncorrelated.

2 ln d+1

consider the special case where the graph g is a tree. in this case a phase transition
occurs at   0 = 1
d   1 where d is the degree of the tree. for a su   ciently tall tree and for
   >   0, the id203 that the root has value +1 is bounded away from 1/2 and depends
on whether the majority of leaves have value +1 or -1. for    <   0 the id203 that
the root has value +1 is 1/2 independent of the values at the leaves of the tree.

352

consider a height one tree of degree d. if i of the leaves have spin +1 and d     i have

spin -1, then the id203 of the root having spin +1 is proportional to

ei     (d   i)   = e(2i   d)  .

if the id203 of a leaf being +1 is p, then the id203 of i leaves being +1 and
d     i being -1 is

thus, the id203 of the root being +1 is proportional to

a =

pi(1     p)d   ie(2i   d)   = e   d  

(cid:18)d

(cid:19)(cid:0)pe2  (cid:1)i

(1     p)d   i = e   d  (cid:2)pe2   + 1     p(cid:3)d

(cid:18)d

(cid:19)

d(cid:88)

i

i=1

and the id203 of the root being    1 is proportional to

pi (1     p)d   i

(cid:18)d

i

(cid:19)
d(cid:88)

i

i=1

b =

d(cid:88)

i=1

= e   d  

i

(cid:18)d
(cid:19)
d(cid:88)
d(cid:88)

i=1

(cid:18)d
(cid:18)d

i

pi(1     p)d   ie   (2i   d)  

(cid:19)
pi(cid:2)(1     p)e   2(i   d)  (cid:3)
(cid:19)
pi(cid:2)(1     p)e2  (cid:3)d   i

= e   d  

= e   d  (cid:2)p + (1     p)e2  (cid:3)d

i=1

i

.

the id203 of the root being +1 is

where

and

q = a

a+b =

[pe2   +1   p]d

+[p+(1   p)e2  ]d = c

d

[pe2   +1   p]d

c =(cid:2)pe2   + 1     p(cid:3)d

d =(cid:2)pe2   + 1     p(cid:3)d +(cid:2)p + (1     p) e2  (cid:3)d.

at high temperature, low   , the id203 q of the root of the height one tree being

+1 in the limit as    goes to zero is

p + 1     p

q =

[p + 1     p] + [p + 1     p]

=

independent of p. at low temperature, high   ,

q    

pde2  d

pde2  d + (1     p)de2  d =

pd

pd + (1     p)d =

353

1
2

(cid:26) 0 p = 0

1 p = 1

.

q goes from a low id203 of +1 for p below 1/2 to high id203 of +1 for p above
1/2.

now consider a very tall tree. if the p is the id203 that a root has value +1,
we can iterate the formula for the height one tree and observe that at low temperature
the id203 of the root being one converges to some value. at high temperature, the
id203 of the root being one is 1/2 independent of p. at the phase transition, the slope
of q at p=1/2 is one. see figure 9.11.

now the slope of the id203 of the root being 1 with respect to the id203 of

a leaf being 1 in this height one tree is

d    c

   p     c    d
d2

   p

   q
   p

=

since the slope of the function q(p) at p=1/2 when the phase transition occurs is one, we
can solve    q
   p = 1 for the value of    where the phase transition occurs. first, we show that
   d
= 0.
   p

(cid:12)(cid:12)(cid:12)p=

1
2

   d

d =(cid:2)pe2   + 1     p(cid:3)d
   p = d(cid:2)pe2   + 1     p(cid:3)d   1(cid:0)e2       1(cid:1) + d(cid:2)p + (1     p) e2  (cid:3)d   1(cid:0)1     e2  (cid:1)
(cid:12)(cid:12)(cid:12)p=
(cid:2)1 + e2  (cid:3)d   1(cid:0)1     e2  (cid:1) = 0
d(cid:2)pe2   + 1     p(cid:3)d   1(cid:0)e2       1(cid:1)

= d
2d   1

d    c

1
2

[pe2   + 1     p]d + [p + (1     p) e2  ]d

2d   1

+(cid:2)p + (1     p) e2  (cid:3)d
(cid:2)e2   + 1(cid:3)d   1(cid:0)e2       1(cid:1) + d
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)p=
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)p=
(cid:3)d   1(cid:0)e2       1(cid:1)
(cid:3)d +(cid:2) 1
2e2  (cid:3)d =
d(cid:0)e2       1(cid:1)
d(cid:0)e2       1(cid:1) = 1 + e2  

d(cid:0)e2       1(cid:1)

1 + e2   = 1

2 + 1

1 + e2  

   c
   p
d

=

=

1
2

1
2

   p     c    d
d2

   p

=

d(cid:2) 1
(cid:2) 1

2e2   + 1

2

2e2   + 1

2

   d
   p

(cid:12)(cid:12)(cid:12)(cid:12)p=

1
2

then

   q
   p

=

setting

and solving for    yields

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)p=

1
2

e2   = d+1
d   1
2 ln d+1
d   1

   = 1

to complete the argument, we need to show that q is a monotonic function of p. to see

354

1

high
temperature

id203 q(p) of
the root being 1
as a function of p

1/2

at phase transition
slope of q(p) equals 1
at p = 1/2

low
temperature

1/2

1

0

0

id203 p of a leaf being 1

figure 9.11: shape of q as a function of p for the height one tree and three values of   
corresponding to low temperature, the phase transition temperature, and high tempera-
ture.

this, write q = 1
1+

b
a

. a is a monotonically increasing function of p and b is monotonically

decreasing. from this it follows that q is monotonically increasing.

in the iteration going from p to q, we do not get the true marginal probabilities at
each level since we ignored the e   ect of the portion of the tree above. however, when we
get to the root, we do get the true marginal for the root. to get the true marginal   s for
the interior nodes we need to send messages down from the root.

note: the joint id203 distribution for the tree is of the form e

suppose x1 has value 1 with id203 p. then de   ne a function   , called evidence, such
that

(cid:26) p
=(cid:0)p     1

2

for x1 = 1
1     p for x1 =    1

(cid:1) x1 + 1

2

   (x1) =

and multiply the joint id203 function by   . note, however, that the marginal prob-
ability of x1 is not p. in fact, it may be further from p after multiplying the conditional
id203 function by the function   .

9.22 bibliographic notes

a formal de   nition of topic models described in this chapter as well as the lda model
are from blei, ng and jordan [bnj03]; see also [ble12]. non-negative matrix factoriza-
tion has been used in several contexts, for example [ds03]. anchor terms were de   ned

355

   (cid:80)

xixj

(ij)   e)

= (cid:81)

(i,j)   e

e  xixj .

and used in [agkm16]. sections 9.7 - 9.9 are simpli   ed versions of results from [bbk14].

good introductions to id48, id114, id110s,
and belief propagation appear in [gha01, bis06]. the use of markov random fields
for id161 originated in the work of boykov, veksler, and zabih [bvz98], and
further discussion appears in [bis06]. factor graphs and message-passing algorithms on
them were formalized as a general approach (incorporating a range of existing algorithms)
in [kfl01]. message-passing in graphs with loops is discussed in [wei97, wf01].

the use of belief propagation for maximum weighted matching is from [bss08]. sur-
vey propagation and warning propagation for    nding satisfying assignments to k-cnf
formulas are described and analyzed in [mpz02, bmz05, acort11]. for additional
relevant papers and surveys, see [fd07, yfw01, yfw03, fk00].

356

9.23 exercises

exercise 9.1 find a nonnegative factorization of the matrix

                  

                  

5
6
4
3
1
2
7
7 10
6
4
8
6 10 11

a =

indicate the steps in your method and show the intermediate results.

exercise 9.2 find a nonnegative factorization of each of the following matrices.

                              
                              

(1)

(3)

                              

3

3

10 9 15 14 13
1
2
1
7 13 11 11
8
7
5 11 10
7
11
6
5 11
5
3
3
1
1
1
2
2
2
2

4

3

1

3

3

4

4
3
13 16 13 10 5 13 14 10
15 24 21 12 9 21 18 12
6
7
1
1
4
5
3
3

7 15 10
2
4
2
6
3
7
6 12
6

16 15
4
4
8
7
12 12

6
1
4
3

            12 22 41 35
             =

19 20 13 48
11 14 16 29
14 16 14 36

                              

4
2
2
6

5 5 10 14 17
6
2 2
4
1 1
1 1
3
3 3
10
5 5 10 16 18
2 2
7

4
4
2
8

4

6

(2)

                              
                               (4)
            1 1
            (cid:18)1 2 4 3
            10 1
(cid:19)

2 2 1 5

3
9

1
3
2

9
4
6

4
9

4
12

1
3
9 9
6 6 12 16 15 15 4
3 3
1

4
9

3

4

3

3

            

exercise 9.3 consider the matrix a that is the product of nonnegative matrices b and
c.

which rows of a are approximate positive linear combinations of other rows of a?
find an approxiamte nonnegative factorization of a

exercise 9.4 consider a set of vectors s in which no vector is a positive linear combi-
nation of the other vectors in the set. given a set t containing s along with a number of
elements from the convex hull of s    nd the vectors in s. develop an e   cient method to
   nd s from t which does not require id135.
hint: the points of s are vertices of the convex hull of t . the euclidean length of a
vector is a convex function and so its maximum over a polytope is attained at one of the
vertices. center the set and    nd the the maximum length vector in t . this will be one
element of s.

357

exercise 9.5 de   ne the nonnegative column rank of a m  n matrix a to be the minimum
number of vectors of in m space with the property that every column of a can be expressed
as a nonnegative linear combination of these vectors.

1. show that the nonnegative column rank of a is at least the rank of a.
2. construct a 3    n matrix whose nonnegative column rank is n.

[hint: take the
plane x + y = z = 1 in 3    space; draw a circle in the plane and take n points on
the circle.]

3. show that the nonnegative column rank need not be the same as the nonnegative row

rank.

4. read/look up a paper of vavasis showing that the computation of nonnegative rank

is np-hard.

exercise 9.6 what happens to the id96 problem, when m the number of words
in a document goes to in   nity? argue that the idealized id96 problem of section
9.2 is easy to solve when m goes to in   nity.

exercise 9.7 suppose y = (y1, y2, . . . , yr) is jointly distributed according to the dirichlet
distribution with parameter    = 1/r. show that the expected value of maxr
l=1yl is greater
than 0.1. [hint: lemma 9.6]

exercise 9.8 suppose there are s documents in a collection which are all nearly pure
for a particular topic.
i.e., in each of these documents, that topic has weight at least
1       . suppose someone    nds and hands to you these documents. then their average is
an approximation to the topic vector. in terms of s, m and    compute an upper bound on
the error of approximation.

we could suggest at the start of section 9.7 that they do the following exercise before
reading the section, so they get the intuition.

exercise 9.9 two topics and two words. toy case of the    dominant admixture model   .
suppose in a topic model, there are just two words and two topics; word 1 is a    key word   
of topic 1 and word 2 is a key word of topic 2 in the sense:

b11     2b12

;

b21     1
2

b22.

suppose each document has one of the two topics as a dominant topic in the sense:

also suppose

max(c1j, c2j)     0.75.

|(a     bc)ij|     0.1    i, j.

show that there are two real numbers   1 and   2 such that each document j with dominant
topic 1 has a1j       1 and a2j <   2 and each document j(cid:48) with dominant topic 2 has
a2j(cid:48)       2 and a1j(cid:48) <        1.

358

exercise 9.10 what is the id203 of heads occurring after a su   ciently long sequence
of transitions in viterbi algorithm example of the most likely sequence of states?

exercise 9.11 find optimum parameters for a three state id48 and given output se-
quence. note the id48 must have a strong signature in the output sequence or we prob-
ably will not be able to    nd it. the following example may not be good for that
reason.

1
1
2

1
4

1
3

2
1
4

1
4

1
3

3
1
4

1
2

1
3

1

2

3

a b
1
3
4
4

1
4

1
3

3
4

2
3

1

2

3

exercise 9.12 in the ising model for a tree of degree one, a chain of vertices, is there a
phase transition where the correlation between the value at the root and the value at the
leaves becomes independent? work out mathematical what happens.

exercise 9.13 for a boolean function in cnf the marginal id203 gives the number
of satis   able assignments with x1.

how does one obtain the number of satisfying assignments for a 2-cnf formula? not

completely related to    rst sentence.

359

10 other topics

10.1 ranking and social choice

combining feedback from multiple users to rank a collection of items is an important
task. we rank movies, restaurants, web pages, and many other items. ranking has be-
come a multi-billion dollar industry as organizations try to raise the position of their web
pages in the results returned by search engines to relevant queries. developing a method
of ranking that cannot be easily gamed by those involved is an important task.

a ranking of a collection of items is de   ned as a complete ordering. for every pair of
items a and b, either a is preferred to b or b is preferred to a. furthermore, a ranking is
transitive in that a > b and b > c implies a > c.

one problem of interest in ranking is that of combining many individual rankings into
one global ranking. however, merging ranked lists in a meaningful way is nontrivial as
the following example illustrates.

example: suppose there are three individuals who rank items a, b, and c as illustrated
in the following table.

individual    rst item second item third item

1
2
3

a
b
c

b
c
a

c
a
b

suppose our algorithm tried to rank the items by    rst comparing a to b and then
comparing b to c. in comparing a to b, two of the three individuals prefer a to b and thus
we conclude a is preferable to b. in comparing b to c, again two of the three individuals
prefer b to c and we conclude that b is preferable to c. now by transitivity one would
expect that the individuals would prefer a to c, but such is not the case, only one of the
individuals prefers a to c and thus c is preferable to a. we come to the illogical conclusion
that a is preferable to b, b is preferable to c, and c is preferable to a.

suppose there are a number of individuals or voters and a set of candidates to be
ranked. each voter produces a ranked list of the candidates. from the set of ranked lists
can one construct a reasonable single ranking of the candidates? assume the method of
producing a global ranking is required to satisfy the following three axioms.

nondictatorship     the algorithm cannot always simply select one individual   s ranking

to use as the global ranking.

unanimity     if every individual prefers a to b, then the global ranking must prefer a to

b.

360

independent of irrelevant alternatives     if individuals modify their rankings but
keep the order of a and b unchanged, then the global order of a and b should
not change.

arrow showed that it is not possible to satisfy all three of the above axioms. we begin
with a technical lemma.

lemma 10.1 for a set of rankings in which each individual ranks an item b either    rst
or last (some individuals may rank b    rst and others may rank b last), a global ranking
satisfying the above axioms must put b    rst or last.

proof: let a, b, and c be distinct items. suppose to the contrary that b is not    rst or
last in the global ranking. then there exist a and c where the global ranking puts a > b
and b > c. by transitivity, the global ranking puts a > c. note that all individuals can
move c above a without a   ecting the order of b and a or the order of b and c since b
was    rst or last on each list. thus, by independence of irrelevant alternatives, the global
ranking would continue to rank a > b and b > c even if all individuals moved c above a
since that would not change the individuals relative order of a and b or the individuals
relative order of b and c. but then by unanimity, the global ranking would need to put
c > a, a contradiction. we conclude that the global ranking puts b    rst or last.

theorem 10.2 (arrow) any deterministic algorithm for creating a global ranking from
individual rankings of three or more elements in which the global ranking satis   es una-
nimity and independence of irrelevant alternatives is a dictatorship.

proof: let a, b, and c be distinct items. consider a set of rankings in which every in-
dividual ranks b last. by unanimity, the global ranking must also rank b last. let the
individuals, one by one, move b from bottom to top leaving the other rankings in place.
by unanimity, the global ranking must eventually move b from the bottom all the way to
the top. when b    rst moves, it must move all the way to the top by lemma 10.1.

let v be the    rst individual whose change causes the global ranking of b to change.
we argue that v is a dictator. first, we argue that v is a dictator for any pair ac not
involving b. we will refer to the three rankings of v in figure 10.1. the    rst ranking
of v is the ranking prior to v moving b from the bottom to the top and the second is
the ranking just after v has moved b to the top. choose any pair ac where a is above
c in v   s ranking. the third ranking of v is obtained by moving a above b in the second
ranking so that a > b > c in v   s ranking. by independence of irrelevant alternatives,
the global ranking after v has switched to the third ranking puts a > b since all indi-
vidual ab votes are the same as in the    rst ranking, where the global ranking placed
a > b. similarly b > c in the global ranking since all individual bc votes are the same
as in the second ranking, in which b was at the top of the global ranking. by transitiv-
ity the global ranking must put a > c and thus the global ranking of a and c agrees with v.

361

b

b

...
a
...
c
...
b
v

...
a
...
...
...
b

global

b

b

b

b

b
...
a
...
c
...
v

b

b

b
...
...
...
c
...

global

b

b a
b
...
c
...
...
v

b

b

a
b
...
c
...
...

global

   rst ranking

second ranking

third ranking

figure 10.1: the three rankings that are used in the proof of theorem 10.2.

now all individuals except v can modify their rankings arbitrarily while leaving b in its
extreme position and by independence of irrelevant alternatives, this does not a   ect the
global ranking of a > b or of b > c. thus, by transitivity this does not a   ect the global
ranking of a and c. next, all individuals except v can move b to any position without
a   ecting the global ranking of a and c.

at this point we have argued that independent of other individuals    rankings, the
global ranking of a and c will agree with v   s ranking. now v can change its ranking
arbitrarily, provided it maintains the order of a and c, and by independence of irrelevant
alternatives the global ranking of a and c will not change and hence will agree with v.
thus, we conclude that for all a and c, the global ranking agrees with v independent of
the other rankings except for the placement of b. but other rankings can move b without
changing the global order of other elements. thus, v is a dictator for the ranking of any
pair of elements not involving b.

note that v changed the relative order of a and b in the global ranking when it moved

b from the bottom to the top in the previous argument. we will use this in a moment.

the individual v is also a dictator over every pair ab. repeat the construction showing
that v is a dictator for every pair ac not involving b only this time place c at the bottom.
there must be an individual vc who is a dictator for any pair such as ab not involving c.
since both v and vc can a   ect the global ranking of a and b independent of each other, it
must be that vc is actually v. thus, the global ranking agrees with v no matter how the
other voters modify their rankings.

10.1.1 randomization

an interesting randomized algorithm that satis   es unanimity and independence of irrel-
evant alternatives is to pick a random individual and use that individual   s ranking as

362

the output. this is called the    random dictator    rule because it is a randomization over
dictatorships. an analogous scheme in the context of voting would be to select a winner
with id203 proportional to the number of votes for that candidate, because this is
the same as selecting a random voter and telling that voter to determine the winner. note
that this method has the appealing property that as a voter, there is never any reason
to strategize, e.g., voting for candidate a rather than your preferred candidate b because
you think b is unlikely to win and you don   t want to throw away your vote. with this
method, you should always vote for your preferred candidate.

10.1.2 examples

borda count: suppose we view each individual   s ranking as giving each item a score:
putting an item in last place gives it one point, putting it in second-to-last place gives it
two points, third-to-last place is three points, and so on. in this case, one simple way to
combine rankings is to sum up the total number of points received by each item and then
sort by total points. this is called the extended borda count method.

let   s examine which axioms are satis   ed by this approach. it is easy to see that it
is a nondictatorship. it also satis   es unanimity: if every individual prefers a to b, then
every individual gives more points to a than to b, and so a will receive a higher total than
b. by arrow   s theorem, the approach must fail independence of irrelevant alternatives,
and indeed this is the case. here is a simple example with three voters and four items
{a, b, c, d} where the independence of irrelevant alternatives axiom fails:

individual

ranking

1
2
3

abcd
abcd
bacd

in this example, a receives 11 points and is ranked    rst, b receives 10 points and is ranked
second, c receives 6 points and is ranked third, and d receives 3 points and is ranked
fourth. however, if individual 3 changes his ranking to bcda, then this reduces the total
number of points received by a to 9, and so b is now ranked    rst overall. thus, even
though individual 3   s relative order of b and a did not change, and indeed no individual   s
relative order of b and a changed, the global order of b and a did change.

hare voting: an interesting system for voting is to have everyone vote for their fa-
vorite candidate. if some candidate receives a majority of the votes, he or she is declared
the winner. if no candidate receives a majority of votes, the candidate with the fewest
votes is dropped from the slate and the process is repeated.

the hare system implements this method by asking each voter to rank all the can-
didates. then one counts how many voters ranked each candidate as number one. if no
candidate receives a majority, the candidate with the fewest number one votes is dropped

363

from each voters ranking. if the dropped candidate was number one on some voters list,
then the number two candidate becomes that voter   s number one choice. the process of
counting the number one rankings is then repeated.

we can convert the hare voting system into a ranking method in the following way.
whichever candidate is dropped    rst is put in last place, whichever is dropped second is
put in second-to-last place, and so on, until the system selects a winner, which is put in
   rst place. the candidates remaining, if any, are placed between the    rst-place candidate
and the candidates who were dropped, in an order determined by running this procedure
recursively on just those remaining candidates.

as with borda count, the hare system also fails to satisfy independence of irrelevant
alternatives. consider the following situation in which there are 21 voters that fall into
four categories. voters within a category rank individuals in the same order.

category

number of voters
in category

preference order

1
2
3
4

7
6
5
3

abcd
bacd
cbad
dcba

the hare system would    rst eliminate d since d gets only three rank one votes. then
it would eliminate b since b gets only six rank one votes whereas a gets seven and c gets
eight. at this point a is declared the winner since a has thirteen votes to c   s eight votes.
so, the    nal ranking is acbd.

now assume that category 4 voters who prefer b to a move b up to    rst place. this
keeps their order of a and b unchanged, but it reverses the global order of a and b. in
particular, d is    rst eliminated since it gets no rank one votes. then c with    ve votes is
eliminated. finally, b is declared the winner with 14 votes, so the    nal ranking is bacd.

interestingly, category 4 voters who dislike a and have ranked a last could prevent a
from winning by moving a up to    rst. ironically this results in eliminating d, then c, with
   ve votes and declaring b the winner with 11 votes. note that by moving a up, category
4 voters were able to deny a the election and get b to win, whom they prefer over a.

10.2 compressed sensing and sparse vectors

de   ne a signal to be a vector x of length d, and de   ne a measurement of x to be a dot-
product of x with some known vector ai. if we wish to uniquely reconstruct x without
any assumptions, then d linearly-independent measurements are necessary and su   cient.

364

d

a

n

=

b

x

figure 10.2: ax = b has a vector space of solutions but possibly only one sparse
solution. if the columns of a are unit length vectors that are pairwise nearly orthogonal,
then the system has a unique sparse solution.

given b = ax where a is known and invertible, we can reconstruct x as x = a   1b. in
the case where there are fewer than d independent measurements and the rank of a is less
than d, there will be multiple solutions. however, if we knew that x is sparse with s (cid:28) d
nonzero elements, then we might be able to reconstruct x with far fewer measurements
using a matrix a with n (cid:28) d rows. see figure 10.2.
in particular, it turns out that
a matrix a whose columns are nearly orthogonal, such as a matrix of random gaussian
entries, will be especially well-suited to this task. this is the idea of compressed sensing.
note that we cannot make the columns of a be completely orthogonal since a has more
columns than rows.

compressed sensing has found many applications, including reducing the number of
sensors needed in photography, using the fact that images tend to be sparse in the wavelet
domain, and in speeding up magnetic resonance imaging in medicine.

10.2.1 unique reconstruction of a sparse vector

a vector is said to be s-sparse if it has at most s nonzero elements. let x be a d-
dimensional, s-sparse vector with s (cid:28) d. consider solving ax = b for x where a is an
n    d matrix with n < d. the set of solutions to ax = b is a subspace. however, if we
restrict ourselves to sparse solutions, under certain conditions on a there is a unique s-
sparse solution. suppose that there were two s-sparse solutions, x1 and x2. then x1     x2
would be a 2s-sparse solution to the homogeneous system ax = 0. a 2s-sparse solution to
the homogeneous equation ax = 0 requires that some 2s columns of a be linearly depen-
dent. unless a has 2s linearly dependent columns there can be only one s-sparse solution.

the solution to the reconstruction problem is simple. if the matrix a has at least 2s

365

1-norm solution

    2-norm solution

figure 10.3: illustration of minimum 1-norm and 2-norm solutions.

rows and the entries of a were selected at random from a standard gaussian, then with
id203 one, no set of 2s columns will be linearly dependent. we can see this by not-
ing that if we    rst    x a subset of 2s columns and then choose the entries at random, the
id203 that this speci   c subset is linearly dependent is the same as the id203
that 2s random gaussian vectors in a 2s-dimensional space are linearly dependent, which

is zero.42 so, taking the union bound over all(cid:0) d
to solve for x we could try all (cid:0)d

(cid:1) subsets, the id203 that any one
(cid:1) possible locations for the nonzero elements in x and

the above argument shows that if we choose n = 2s and pick entries of a randomly
from a gaussian, with id203 one there will be a unique s-sparse solution. thus,

of them is linearly dependent is zero.

2s

aim to solve ax = b over just those s columns of a: any one of these that gives a
solution will be the correct answer. however, this takes time    (ds) which is exponential
in s. we turn next to the topic of e   cient algorithms, describing a polynomial-time
optimization procedure that will    nd the desired solution when n is su   ciently large and
a is constructed appropriately.

s

10.2.2 e   ciently finding the unique sparse solution

to    nd a sparse solution to ax = b, one would like to minimize the zero norm (cid:107)x(cid:107)0
over {x|ax = b}, i.e., minimize the number of nonzero entries. this is a computationally
hard problem. there are techniques to minimize a convex function over a convex set, but
||x||0 is not a convex function, and with no further assumptions, it is np-hard. with this
in mind, we use the one-norm as a proxy for the zero-norm and minimize the one-norm
i |xi| over {x|ax = b}. although this problem appears to be nonlinear, it can
be solved by id135 by writing x = u    v, u     0, and v     0, and minimizing

(cid:107)x(cid:107)1 =(cid:80)
the linear function(cid:80)

vi subject to au-av=b, u     0, and v     0.

ui +(cid:80)

i

i

42this can be seen by selecting the vectors one at a time. the id203 that the ith new vector lies
fully in the lower dimensional subspace spanned by the previous i    1 vectors is zero, and so by the union
bound the overall id203 is zero.

366

2s, 1

we now show if the columns of the n by d matrix a are unit length almost orthogo-
2s) that minimizing (cid:107)x(cid:107)1 over
nal vectors with pairwise dot products in the range (    1
{x|ax = b} recovers the unique s-sparse solution to ax=b. the ijth element of the ma-
trix at a is the cosine of the angle between the ith and jth columns of a. if the columns
of a are unit length and almost orthogonal, at a will have ones on its diagonal and all
o    diagonal elements will be small. by theorem 2.8, if a has n = s2 log d rows and each
column is a random unit-length n-dimensional vector, with high id203 all pairwise
dot-products will have magnitude less than 1
2s as desired.43 here, we use s2 log d, a larger
value of n compared to the existence argument in section 10.2.1, but now the algorithm
is computationally e   cient.

2s , 1

2s , 1

let x0 denote the unique s-sparse solution to ax = b and let x1 be a solution of
smallest possible one-norm. let z = x1     x0. we now prove that z = 0 implying that
x1 = x0. first, az = ax1     ax0 = b     b = 0. this implies that at az = 0. since each
column of a is unit length, the matrix at a has ones on its diagonal. since every pair of
distinct columns of a has dot-product in the range (    1
2s), each o   -diagonal entry in
at a is in the range (    1
2s). these two facts imply that unless z = 0, every entry in z
2s||z||1. if the jth entry in z had absolute value greater
must have absolute value less than 1
2s||z||1, it would not be possible for the jth entry of at az to equal 0
than or equal to 1
unless ||z||1 = 0.

have at least half of its (cid:96)1 norm inside of s, i.e.,(cid:80)

finally let s denote the support of x0, where |s|     s. we now argue that z must
2||z||1. this will complete
the argument because it implies that the average value of |zj| for j     s is at least 1
2s||z||1,
which as shown above is only possible if ||z||1 = 0. let tin denote the sum of the absolute
values of the entries of x1 in the set s, and let tout denote the sum of the absolute values
of the entries of x1 outside of s. so, tin + tout = ||x1||1. let t0 be the one-norm of x0.
since x1 is the minimum one norm solution, t0     tin + tout, or equivalently t0     tin     tout.
j(cid:54)   s |zj|, or
2||z||1, which as noted above implies that ||z||1 = 0, as desired.

j(cid:54)   s |zj| = tout. this implies that(cid:80)

j   s |zj|     t0     tin and(cid:80)

but(cid:80)
equivalently,(cid:80)

j   s |zj|    (cid:80)

j   s |zj|     1

j   s |zj|     1

to summarize, we have shown the following theorem and corollary.

theorem 10.3 if matrix a has unit-length columns a1, . . . , ad and the property that
|ai    aj| < 1
2s for all i (cid:54)= j, then if the equation ax = b has a solution with at most s
nonzero coordinates, this solution is the unique minimum 1-norm solution to ax = b.
corollary 10.4 for some absolute constant c, if a has n rows for n     cs2 log d and each
column of a is chosen to be a random unit-length n-dimensional vector, then with high
id203 a satis   es the conditions of theorem 10.3 and therefore if the equation ax = b
has a solution with at most s nonzero coordinates, this solution is the unique minimum
1-norm solution to ax = b.

43note that the roles of    n    and    d    are reversed here compared to theorem 2.8.

367

position on genome

trees

=

phenotype; outward
manifestation, observables

genotype:
internal code

figure 10.4: the system of linear equations used to    nd the internal code for some
observable phenomenon.

the condition of theorem 10.3 is often called incoherence of the matrix a. other more
involved arguments show that it is possible to recover the sparse solution using one-norm
minimization for a number of rows n as small as o(s log(ds)).

10.3 applications

10.3.1 biological

there are many areas where linear systems arise in which a sparse solution is unique.
one is in plant breeding. consider a breeder who has a number of apple trees and for
each tree observes the strength of some desirable feature. he wishes to determine which
genes are responsible for the feature so he can crossbreed to obtain a tree that better
expresses the desirable feature. this gives rise to a set of equations ax = b where each
row of the matrix a corresponds to a tree and each column to a position on the genone.
see figure 10.4. the vector b corresponds to the strength of the desired feature in each
tree. the solution x tells us the position on the genone corresponding to the genes that
account for the feature. it would be surprising if there were two small independent sets
of genes that accounted for the desired feature. thus, the matrix should have a property
that allows only one sparse solution.

368

10.3.2 low rank matrices

suppose l is a low rank matrix that has been corrupted by noise. that is, a = l + r.
if the r is gaussian, then principal component analysis will recover l from a. however,
if l has been corrupted by several missing entries or several entries have a large noise
added to them and they become outliers, then principal component analysis may be far
o   . however, if l is low rank and r is sparse, then l can be recovered e   ectively from
l+r. to do this,    nd the l and r that minimize (cid:107)l(cid:107)    +  (cid:107)r(cid:107)1.44 here the nuclear norm
(cid:80)
(cid:107)l(cid:107)    is the 1-norm of the vector of singular values of l and ||r||1 is the entrywise 1-norm
ij |rij|. a small value of (cid:107)l(cid:107)    indicates a sparse vector of singular values and hence a
low rank matrix. minimizing (cid:107)l(cid:107)    +   (cid:107)r(cid:107)1 subject to l + r = a is a complex problem
and there has been much work on it. the reader is referred to add references
notice that we do not need to know the rank of l or the elements that were corrupted.
all we need is that the low rank matrix l is not sparse and that the sparse matrix r is
not low rank. we leave the proof as an exercise.

if a is a small matrix one method to    nd l and r by minimizing ||l||    + ||r||1 is to
   nd the singular value decomposition a = u   v t and minimize ||  ||1 + ||r||1 subject to
a = l + r and u   v t being the singular value decomposition of a. this can be done
using lagrange multipliers (??). write r = r+ + r    where r+     0 and r        0. let

n(cid:88)

(cid:88)

(cid:88)

r+
ij +

r   
ij.

f (  i, rij) =

  i +

write the lagrange formula

i=1

ij

ij

l = f (  i, rij) +   i  igi

where the gi are the required constraints

ij     0
1. r+
ij     0
2. r   
3.   i     0

5. ut

i uj =

4. aij = lij + rij

(cid:26) 1 i = j
(cid:26) 1 i = j
7. li =(cid:80) ui  ivt

0 i (cid:54)= j

0 i (cid:54)= j

i vj =

6. vt

j

44to minimize the absolute value of x write x = u     v and using id135 minimize u + v

subject to u     0 and v     0.

369

conditions (5) and (6) insure that u   v t is the svd of some matrix. the solution is
obtained when    (l) = 0 which can be found by id119 using    2(l).

an example where low rank matrices that have been corrupted might occur is aerial
photographs of an intersection. given a long sequence of such photographs, they will be
the same except for cars and people. if each photo is converted to a vector and the vector
used to make a column of a matrix, then the matrix will be low rank corrupted by the
tra   c. finding the original low rank matrix will separate the cars and people from the
back ground.

10.4 an uncertainty principle

given a function x(t), one can represent the function by the composition of sinusoidal
functions. basically one is representing the time function by its frequency components.
the transformation from the time representation of a function to it frequency represen-
tation is accomplished by a fourier transform. the fourier transform of a function x(t)
is given by

converting the frequency representation back to the time representation is done by the
inverse fourier transformation

(cid:90)

(cid:90)

f (  ) =

x(t)e   2    tdt

x(t) =

f (  )e2    td  

in the discrete case, x = [x0, x1, . . . , xn   1] and f = [f0, f1, . . . , fn   1]. the fourier trans-
n   ij where    is the principal nth root of unity. the inverse

form is f = ax with aij = 1   
transform is x = bf where b = a   1 has the simple form bij = 1   

n      ij.

there are many other transforms such as the laplace, wavelets, chirplets, etc. in fact,

any nonsingular n    n matrix can be used as a transform.

10.4.1 sparse vector in some coordinate basis

consider ax = b where a is a square n    n matrix. the vectors x and b can be con-
sidered as two representations of the same quantity. for example, x might be a discrete
time sequence, b the frequency spectrum of x, and the matrix a the fourier transform.
the quantity x can be represented in the time domain by x and in the frequency domain
by its fourier transform b.

any orthonormal matrix can be thought of as a transformation and there are many
important transformations other than the fourier transformation. consider a transfor-
mation a and a signal x in some standard representation. then y = ax transforms
the signal x to another representation y. if a spreads any sparse signal x out so that

370

the information contained in each coordinate in the standard basis is spread out to all
coordinates in the second basis, then the two representations are said to be incoherent.
a signal and its fourier transform are one example of incoherent vectors. this suggests
that if x is sparse, only a few randomly selected coordinates of its fourier transform are
needed to reconstruct x. below, we show that a signal cannot be too sparse in both its
time domain and its frequency domain.

10.4.2 a representation cannot be sparse in both time and frequency

domains

there is an uncertainty principle that states that a time signal cannot be sparse in
both the time domain and the frequency domain. if the signal is of length n, then the
product of the number of nonzero coordinates in the time domain and the number of
nonzero coordinates in the frequency domain must be at least n. this is the mathemati-
cal version of heisenberg   s uncertainty principle. before proving the uncertainty principle
we    rst prove a technical lemma.

in dealing with the fourier transform it is convenient for indices to run from 0 to n    1
rather than from 1 to n. let x0, x1, . . . , xn   1 be a sequence and let f0, f1, . . . , fn   1 be its
j = 0, . . . , n    1.

      1. then fj = 1   

discrete fourier transform. let i =

n   1(cid:80)

n jk,

    2  i

xke

n

k=0

in matrix form f = zx where zjk = e

    2  i

n jk.

                f0

f1
...
fn   1

                =

                  

1   
n

e

1
    2  i
n
...

1

1
...
1 e

1
    2  i
n 2
e
...

n (n     1) e
    2  i

n 2 (n     1)
    2  i

      
      

      

1

n (n     1)
    2  i

e

...

n (n     1)2
    2  i

e

                  

                x0

x1
...
xn   1

               

if some of the elements of x are zero, delete the zero elements of x and the corresponding
columns of the matrix. to maintain a square matrix, let nx be the number of nonzero
elements in x and select nx consecutive rows of the matrix. normalize the columns of the
resulting submatrix by dividing each element in a column by the column element in the
   rst row. the resulting submatrix is a vandermonde matrix that looks like

             1

a
a2
a3

            

1
b
b2
b3

1
1
c
d
c2 d2
c3 d3

and is nonsingular.

lemma 10.5 if x0, x1, . . . , xn   1 has nx nonzero elements, then f0, f1, . . . , fn   1 cannot
have nx consecutive zeros.

371

                                          

1
3

1

1

1

1

1

z3 z6 1

1
1
1 1
z2 z3 z4 z5 z6 z7 z8
1 z
z3 z5 z7
1 z2 z4 z6 z8 z
z3 z6
1 z3 z6 1
z5
1 z4 z8 z3 z7 z2 z6 z
z6 z2 z7 z3 z8 z4
1 z5 z
z6 z3
1 z6 z3 1
z8 z6 z4 z2
1 z7 z5 z3 z
1 z8 z7 z6 z5 z4 z3 z2 z

z6 z3 1

                                          

                                          

                                          

1
0
0
1
0
0
1
0
0

                                          

=

1
3

3

1 + z3 + z6
1 + z6 + z3

3

1 + z3 + z6
1 + z6 + z3

3

1 + z3 + z6
1 + z6 + z3

                                          

=

                                          

                                          

1
0
0
1
0
0
1
0
0

figure 10.5: the transform of the sequence 100100100.

proof: let i1, i2, . . . , inx be the indices of the nonzero elements of x. then the elements
of the fourier transform in the range k = m + 1, m + 2, . . . , m + nx are

nx(cid:80)

j=1

fk = 1   

n

   2  i
n kij

xij e

      1 and the multiplication of the exponent by ij to account for the

note the use of i as
actual location of the element in the sequence. normally, if every element in the sequence
was included, we would just multiply by the index of summation.

convert the equation to matrix form by de   ning zkj = 1   

n kij) and write
f = zx where now x is the vector consisting of the nonzero elements of the original x. by
its de   nition, x (cid:54)= 0. to prove the lemma we need to show that f is nonzero. this will be
true provided z is nonsingular since x = z   1f . if we rescale z by dividing each column
by its leading entry we get the vandermonde determinant which is nonsingular.

n exp(    2  i

theorem 10.6 let nx be the number of nonzero elements in x and let nf be the number
of nonzero elements in the fourier transform of x. let nx divide n. then nxnf     n.

proof: if x has nx nonzero elements, f cannot have a consecutive block of nx zeros. since
nx divides n there are n
blocks each containing at least one nonzero element. thus, the
nx
product of nonzero elements in x and f is at least n.

the fourier transform of spikes proves that above bound is tight

   
to show that the bound in theorem 10.6 is tight we show that the fourier transform
n     1 zeros,
of the sequence of length n consisting of
is the sequence itself. for example, the fourier transform of the sequence 100100100 is
100100100. thus, for this class of sequences, nxnf = n.
   
n

n) be the sequence of 1   s and 0   s with

n ones, each one separated by

n 1   s spaced

   
n,

   

   

   

   
theorem 10.7 let s (
apart. the fourier transform of s (

n,

   

n) is itself.

372

   

   
   
n, . . . , (
n, 2

n     1)

   

n) equals

   
n and the 1/

   
   
proof: consider the columns 0,
   
which s (
n,
n, 0     k <
   
   
k
s (
n,

   
n. these are the columns for
   
n of column
n) has value 1. the element of the matrix z in the row j
   
n is znkj = 1. thus, the product of these rows of z times the vector

n = 1.
n id172 yields fj
   
   
   
n, j     {0,
n, the row b, b (cid:54)= j
n     1},
   
   
   
   
for rows whose index is not of the form j
n, . . . ,
   
n     1)
n   1)b
n are 1, zb, z2b, . . . , z(
n, . . . , (
n, 2
the elements in row b in the columns 0,
   
   
n = 1 and zb (cid:54)= 1.
1 + zb + z2b        + z(
nb   1
and thus fb = 1   
= 1   
z
zb   1 = 0 since zb

n   1)b(cid:17)

(cid:16)

   

   

   

n

n

ms better suited to perhaps a homework question

10.5 gradient

   x1

,    f (x0)
   x2

the gradient of a function f (x) of d variables, x = (x1, x2, . . . , xd), at a point x0 is
denoted (cid:53)f (x0). it is a d-dimensional vector with components    f (x0)
, . . . ,    f (x0)
,
   xd
where    f
are partial derivatives. without explicitly stating, we assume that the deriva-
   xi
tives referred to exist. the rate of increase of the function f as we move from x0 in a
direction u is (cid:53)f (x0)    u. so the direction of steepest descent is    (cid:53)f (x0); this is a nat-
ural direction to move to minimize f . but by how much should we move? a large move
may overshoot the minimum. see figure 10.6. a simple    x is to minimize f on the line
from x0 in the direction of steepest descent by solving a one dimensional minimization
problem. this gives us the next iterate x1 and we repeat. we do not discuss the issue
of step-size any further. instead, we focus on in   nitesimal id119, where, the
algorithm makes in   nitesimal moves in the    (cid:53)f (x0) direction. whenever (cid:53)f is not the
zero vector, we strictly decrease the function in the direction    (cid:53)f , so the current point
is not a minimum of the function f . conversely, a point x where (cid:53)f = 0 is called a
   rst-order local optimum of f . a    rst-order local optimum may be a local minimum, local
maximum, or a saddle point. we ignore saddle points since numerical error is likely to
prevent id119 from stoping at a saddle point.
in general, local minima do not
have to be global minima, see figure 10.6, and id119 may converge to a local
minimum that is not a global minimum. when the function f is convex, this is not the
case.

a function f of a single variable x is said to be convex if for any two points a and b,
the line joining f (a) and f (b) is above the curve f (  ). a function of many variables is
convex if on any line segment in its domain, it acts as a convex function of one variable
on the line segment.

de   nition 10.1 a function f over a convex domain is a convex function if for any two
points x and y in the domain, and any    in [0, 1] we have

f (  x + (1       )y)       f (x) + (1       )f (y).

the function is concave if the inequality is satis   ed with     instead of    .

373

figure 10.6: id119 overshooting minimum

theorem 10.8 suppose f is a convex, di   erentiable function de   ned on a closed bounded
convex domain. then any    rst-order local minimum is also a global minimum. in   nites-
imal id119 always reaches the global minimum.

proof: we will prove that if x is a local minimum, then it must be a global minimum.
if not, consider a global minimum point y (cid:54)= x. on the line joining x and y, the function
must not go above the line joining f (x) and f (y). this means for an in   nitesimal    > 0,
moving distance    from x towards y, the function must decrease, so (cid:53)f (x) is not 0,
contradicting the assumption that x is a local minimum.

   2

   xi   xj

the second derivatives

form a matrix, called the hessian, denoted h(f (x)).

the hessian of f at x is a symmetric d    d matrix with ijth entry    2f
(x). the second
derivative of f at x in the direction u is the rate of change of the    rst derivative in the
direction u from x. it is easy to see that it equals

   xi   xj

ut h(f (x))u.

to see this, note that the second derivative of f along the unit vector u is

(cid:88)

(cid:88)

uj

j

i

(cid:18)

(cid:19)

   
   xj

ui

   f (x)

   xi

(cid:88)
(cid:88)

uj

j

=

j,i

((cid:53)f (x)    u) =

   
   xj

ujui

   2f (x)
   xj   xi

.

theorem 10.9 suppose f is a function from a closed convex domain d in rd to the
reals and the hessian of f exists everywhere in d. then f is convex (concave) on d if
and only if the hessian of f is positive (negative) semi-de   nite everywhere on d.

id119 requires the gradient to exist. but, even if the gradient is not always
de   ned, one can minimize a convex function over a convex domain e   ciently, i.e., in
polynomial time. technically, one can only    nd an approximate minimum and the time

374

depends on the error parameter as well as the presentation of the convex set. we do not
go into these details. but, in principle we can minimize a convex function over a convex
domain. we can also maximize a concave function over a concave domain. however, in
general, we do not have e   cient procedures to maximize a convex function over a convex
domain. it is easy to see that at a    rst-order local minimum of a possibly non-convex
function, the gradient vanishes. but second-order local decrease of the function may be
possible. the steepest second-order decrease is in the direction of   v, where, v is the
eigenvector of the hessian corresponding to the largest absolute valued eigenvalue.

10.6 id135

id135 is an optimization problem that has been carefully studied and is
immensely useful. we consider id135 problem in the following form where
a is an m    n matrix, m     n, of rank m, c is 1    n, b is m    1, and x is n    1 :

max c    x

subject to ax = b, x     0.

inequality constraints can be converted to this form by adding slack variables. also, we
can do gaussian elimination on a and if it does not have rank m, we either    nd that
the system of equations has no solution, whence we may stop or we can    nd and discard
redundant equations. after this preprocessing, we may assume that a    s rows are inde-
pendent.

the simplex algorithm is a classical method to solve id135 problems. it
is a vast subject and is well discussed in many texts. here, we will discuss the ellipsoid
algorithm which is in a sense based more on continuous mathematics and is closer to the
spirit of this book.

10.6.1 the ellipsoid algorithm

the    rst polynomial time algorithm for id13545 was developed by khachiyan

based on work of iudin, nemirovsky and shor and is called the ellipsoid algorithm. the
algorithm is best stated for the seemingly simpler problem of determining whether there
is a solution to ax     b and if so    nding one. the ellipsoid algorithm starts with a large
ball in d-space which is guaranteed to contain the polyhedron ax     b. even though we
do not yet know if the polyhedron is empty or non-empty, such a ball can be found. the
algorithm checks if the center of the ball is in the polyhedron, if it is, we have achieved our
objective. if not, we know from the separating hyperplane theorem of convex geometry
that there is a hyperplane called the separating hyperplane through the center of the ball

45although there are examples where the simplex algorithm requires exponential time, it was shown
by shanghua teng and dan spielman that the expected running time of the simplex algorithm on an
instance produced by taking an arbitrary instance and then adding small gaussian perturbations to it is
polynomial.

375

figure 10.7: ellipsoid algorithm

such that the whole polytope lies in one of the half spaces.

we then    nd an ellipsoid which contains the ball intersected with this half-space. see
figure 10.7. the ellipsoid is guaranteed to contain ax     b as was the ball earlier. if the
center of the ellipsoid does not satisfy the inequalities, then again there is a separating
hyper plane and we repeat the process. after a suitable number of steps, either we    nd a
solution to the original ax     b or we end up with a very small ellipsoid. if the original a
and b had integer entries, one can ensure that the set ax     b, after a slight perturbation
which preserves its emptiness/non-emptiness, has a volume of at least some   > 0. if our
ellipsoid has shrunk to a volume of less than this  , then there is no solution. clearly
this must happen within log   v0/  = o(v0d/ ) steps, where v0 is an upper bound on the
initial volume and    is the factor by which the volume shrinks in each step. we do not
go into details of how to get a value for v0, but the important points are that (i) only the
logarithm of v0 appears in the bound on the number of steps, and (ii) the dependence on
d is linear. these features ensure a polynomial time algorithm.

the main di   culty in proving fast convergence is to show that the volume of the
ellipsoid shrinks by a certain factor in each step. thus, the question can be phrased as
suppose e is an ellipsoid with center x0 and consider the half-ellipsoid e(cid:48) de   ned by

e(cid:48) = {x|x     e, a    (x     x0)     0}

where a is some unit length vector. let   e be the smallest volume ellipsoid containing e(cid:48).

376

ellipsoid containing half-sphereseparating hyperplanepolytopeshow that

vol(   e)
vol(e)

    1       

for some    > 0. a sequence of geometric reductions transforms this into a simple problem.
translate and then rotate the coordinate system so that x0 = 0 and a = (1, 0, 0, . . . , 0).
finally, apply a nonsingular linear transformation    so that    e = b = {x| |x| = 1}, the
unit sphere. the important point is that a nonsingular linear transformation    multiplies
the volumes of all sets by |det(   )|, so that vol(   e)
vol(   (e)). the following lemma answers
the question raised.
lemma 10.10 consider the half-sphere b(cid:48) = {x|x1     0,
ellipsoid   e contains b(cid:48):

|x|     1}. the following

vol(e) = vol(   (   e))

(cid:40)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:18) d + 1

(cid:19)2(cid:18)

d

+

(cid:18) d2     1
(cid:19)2
(cid:19)(cid:18) d2

(cid:19)(cid:18)
(cid:19)(d   1)/2     1     1

x2
2 + x2

d2

d2     1

.

4d

x1     1
d + 1

(cid:18) d

d + 1

vol(   e)
vol(b)

=

(cid:19)

(cid:41)

3 + . . . + x2
d

    1

.

  e =

x

further,

the proof is left as an exercise (exercise 10.27).

10.7 integer optimization

the problem of maximizing a linear function subject to linear inequality constraints,

but with the variables constrained to be integers is called integer programming.

max c    x

subject to ax     b with xi integers

(cid:88)

this problem is np-hard. one way to handle the hardness is to relax the integer con-
straints, solve the linear program in polynomial time, and round the fractional values to
integers. the simplest rounding, round each variable which is 1/2 or more to 1, the rest
to 0, yields sensible results in some cases. the vertex cover problem is one of them. the
problem is to choose a subset of vertices so that each edge is covered with at least one of
its end points in the subset. the integer program is:

min

subject to xi + xj     1     edges (i, j); xi integers .

xi

i

solve the linear program. at least one variable for each edge must be at least 1/2 and
the simple rounding converts it to one. the integer solution is still feasible. it clearly
at most doubles the objective function from the id135 solution and since
the lp solution value is at most the optimal integer programming solution value, we are
within a factor of two of the optimal.

377

10.8 semi-de   nite programming

semi-de   nite programs are special cases of convex programs. recall that an n   n ma-
trix a is positive semi-de   nite if and only if a is symmetric and for all x     rn, xt ax     0.
there are many equivalent characterizations of positive semi-de   nite matrices. we men-
tion one. a symmetric matrix a is positive semi-de   nite if and only if it can be expressed
as a = bbt for a possibly rectangular matrix b.

a semi-de   nite program (sdp) is the problem of minimizing a linear function ct x
subject to a constraint that f = f0 + f1x1 + f2x2 +        + fdxd is positive semi-de   nite.
here f0, f1, . . . , fd are given symmetric matrices.

f0 + f1y1 + f2y2 +        + fdyd are positive semi-de   nite, then so is f(cid:0)  x + (1       )y(cid:1)

this is a convex program since the set of x satisfying the constraint is a convex
set. to see this, note that if f (x) = f0 + f1x1 + f2x2 +        + fdxd and f (y) =
for 0            1.
it turns out
that there are more e   cient algorithms for sdp   s than general convex programs and that
many interesting problems can be formulated as sdp   s. we discuss the latter aspect here.

in principle, sdp   s can be solved in polynomial time.

linear programs are special cases of sdp   s. for any vector v, let diag(v) denote a
diagonal matrix with the components of v on the diagonal. then it is easy to see that
the constraints v     0 are equivalent to the constraint diag(v) is positive semi-de   nite.
consider the linear program:

minimize ct x subject to ax = b; x     0.

rewrite ax = b as ax    b     0 and b    ax     0 and use the idea of diagonal matrices

above to formulate this as an sdp.

a second interesting example is that of quadratic programs of the form:

minimize

(ct x)2
dt x

subject to ax + b     0.

this is equivalent to

minimize t subject to ax + b     0 and t     (ct x)2
dt x .

this is in turn equivalent to the sdp

minimize t subject to the following matrix being positive semi-de   nite:

       diag(ax + b)

0
0

378

       .

0

0
t

ct x
ct x dt x

application to approximation algorithms.

an exciting area of application of sdp is in    nding near-optimal solutions to some
integer problems. the central idea is best illustrated by its early application in a break-
through due to goemans and williamson [gw95] for the maximum cut problem which
given a graph g(v, e) asks for the cut s,   s maximizing the number of edges going across
the cut from s to   s. for each i     v , let xi be an integer variable assuming values   1
depending on whether i     s or i       s respectively. then the max-cut problem can be
posed as

(1     xixj) subject to the constraints xi     {   1, +1}.

maximize (cid:80)

(i,j)   e

the integrality constraint on the xi makes the problem np-hard. instead replace the
integer constraints by allowing the xi to be unit length vectors. this enlarges the set of
feasible solutions since   1 are just 1-dimensional vectors of length 1. the relaxed problem
is an sdp and can be solved in polynomial time. to see that it is an sdp, consider xi as
the rows of a matrix x. the variables of our sdp are not x, but actually y = xx t ,
which is a positive semi-de   nite matrix. the sdp is

(1     yij) subject to y positive semi-de   nite,

maximize (cid:80)

(i,j)   e

which can be solved in polynomial time. from the solution y ,    nd x satisfying y = xx t .
now, instead of a   1 label on each vertex, we have vector labels, namely the rows of x.
we need to round the vectors to   1 to get an s. one natural way to do this is to pick
a random vector v and if for vertex i, xi    v is positive, put i in s, otherwise put it in
  s. goemans and wiiliamson showed that this method produces a cut guaranteed to be
at least 0.878 times the maximum. the .878 factor is a big improvement on the previous
best factor of 0.5 which is easy to get by putting each vertex into s with id203 1/2.

application to machine learning.

as discussed in chapter 5, id81s are a powerful tool in machine learning.
they allow one to apply algorithms that learn linear classi   ers, such as id88 and
support vector machines, to problems where the positive and negative examples might
have a more complicated separating curve.

more speci   cally, a kernel k is a function from pairs of examples to reals such that
for some implicit function    from examples to (cid:60)n , we have k(a, a(cid:48)) =   (a)t   (a(cid:48)). (we
are using    a    and    a(cid:48)    to refer to examples, rather than x and x(cid:48), in order to not con   ict
with the notation used earlier in this chapter.) notice that this means that for any set of

379

examples {a1, a2, . . . , an}, the matrix a whose ij entry equals k(ai, aj) is positive semi-
de   nite. speci   cally, a = bbt where the ith row of b equals   (ai).

given that a kernel corresponds to a positive semi-de   nite matrix, it is not surprising
that there is a related use of semi-de   nite programming in machine learning. in particular,
suppose that one does not want to specify up-front exactly which kernel an algorithm
should use. in that case, a natural idea is instead to specify a space of id81s and
allow the algorithm to select the best one from that space for the given data. speci   cally,
given some labeled training data and some unlabeled test data, one could solve for the
matrix a over the combined data set that is positive semi-de   nite (so that it is a legal
id81) and optimizes some given objective. this objective might correspond
to separating the positive and negative examples in the labeled data while keeping the
kernel simple so that it does not over-   t. if this objective is linear in the coe   cients of
a along with possibly additional linear constraints on a, then this is an sdp. this is the
high-level idea of kernel learning,    rst proposed in [lcb+04].

10.9 bibliographic notes

arrow   s impossibility theorem, stating that any ranking of three or more items satisfying
unanimity and independence of irrelevant alternatives must be a dictatorship, is from
[arr50]. for extensions to arrow   s theorem on the manipulability of voting rules, see
gibbard [gib73] and satterthwaite [sat75]. a good discussion of issues in social choice
appears in [lis13]. the results presented in section 10.2.2 on compressed sensing are due
to donoho and elad [de03] and gribonval and nielsen [gn03]. see [don06] for more
details on issues in compressed sensing. the ellipsoid algorithm for id135 is
due to khachiyan [kha79] based on work of shor [sho70] and iudin and nemirovski [in77].
for more information on the ellipsoid algorithm and on semi-de   nite programming, see
the book of gr  otschel, lov  asz, and schrijver [gls12]. the use of sdps for approximating
the max-cut problem is due to goemans and williamson[gw95], and the use of sdps for
learning a id81 is due to [lcb+04].

380

10.10 exercises

exercise 10.1 select a method that you believe is good for combining individual rankings
into a global ranking. consider a set of rankings where each individual ranks b last. one
by one move b from the bottom to the top leaving the other rankings in place. does there
exist a v as in theorem 10.2 where v is the ranking that causes b to move from the bottom
to the top in the global ranking. if not, does your method of combing individual rankings
satisfy the axioms of unanimity and independence of irrelevant alternatives.

exercise 10.2 show that for the three axioms: non dictator, unanimity, and indepen-
dence of irrelevant alternatives, it is possible to satisfy any two of the three.

exercise 10.3 does the axiom of independence of irrelevant alternatives make sense?
what if there were three rankings of    ve items. in the    rst two rankings, a is number one
and b is number two. in the third ranking, b is number one and a is number    ve. one
might compute an average score where a low score is good. a gets a score of 1+1+5=7
and b gets a score of 2+2+1=5 and b is ranked number one in the global ranking. now if
the third ranker moves a up to the second position, a   s score becomes 1+1+2=4 and the
global ranking of a and b changes even though no individual ranking of a and b changed.
is there some alternative axiom to replace independence of irrelevant alternatives? write
a paragraph on your thoughts on this issue.

exercise 10.4 prove that in the proof of theorem 10.2, the global ranking agrees with
column v even if item b is moved down through the column.

exercise 10.5 let a be an m by n matrix with elements from a zero mean, unit variance
gaussian. how large must n be for there to be two or more sparse solutions to ax = b
with high id203. you will need to de   ne how small s should be for a solution with at
most s nonzero elements to be sparse.
exercise 10.6 section 10.2.1 showed that if a is an n    d matrix with entries selected
at random from a standard gaussian, and n     2s, then with id203 one there will be
a unique s-sparse solution to ax = b. show that if n     s, then with id203 one there
will not be a unique s-sparse solution. assume d > s.

exercise 10.7 section 10.2.2 used the fact that n = o(s2 log d) rows is su   cient so
that if each column of a is a random unit-length n-dimensional vector, then with high
id203 all pairwise dot-products of columns will have magnitude less than 1
2s. here,
we show that n =    (log d) rows is necessary as well. to make the notation less confusing
for this argument, we will use    m    instead of    d   .

speci   cally, prove that for m > 3n, it is not possible to have m unit-length n-dimensional

vectors such that all pairwise dot-products of those vectors are less than 1
2.

2 then |u     v|     1 (if their dot-product is equal to 1

some hints: (1) note that if two unit-length vectors u and v have dot-product greater
than or equal to 1
2 then u, v, and the
origin form an equilateral triangle). so, it is enough to prove that m > 3n unit-length
vectors in (cid:60)n cannot all have distance at least 1 from each other. (2) use the fact that the
volume of a ball of radius r in (cid:60)n is proportional to rn.

381

exercise 10.8 create a random 100 by 100 orthonormal matrix a and a sparse 100-
dimensional vector x. compute ax = b. randomly select a few coordinates of b and
reconstruct x from the samples of b using the minimization of 1-norm technique of section
10.2.2. did you get x back?
exercise 10.9 let a be a low rank n    m matrix. let r be the rank of a. let   a be a
corrupted by gaussian noise. prove that the rank r svd approximation to   a minimizes

(cid:12)(cid:12)(cid:12)a       a
(cid:12)(cid:12)(cid:12)2

f

.

exercise 10.10 prove that minimizing ||x||0 subject to ax = b is np-complete.
exercise 10.11 when one wants to minimize ||x||0 subject to some constraint the prob-
lem is often np-hard and one uses the 1-norm as a proxy for the 0-norm. to get an
insite into this issue consider minimizing ||x||0 subject to the constraint that x lies in a
convex region. for simplicity assume the convex region is a sphere with center more than
the radius of the circle from the origin. explore sparsity of solution when minimizing the
1-norm for values of x in the circular region with regards to location of the center.

exercise 10.12 express the matrix

2
2
2
2
13

17 2 2 2
2 2 2
2
2
2 9 2
2 2 2
2
2
2 2 2

as the sum of a low rank matrix plus a sparse matrix. to simplify the computation assume
you want the low rank matrix to be symmetric so that its singular valued decomposition
will be v   v t .
exercise 10.13 generate 100    100 matrices of rank 20, 40, 60 80, and 100. in each
matrix randomly delete 50, 100, 200, or 400 entries.
in each case try to recover the
original matrix. how well do you do?

exercise 10.14 repeat the previous exercise but instead of deleting elements, corrupt the
elements by adding a reasonable size corruption to the randomly selected matrix entries.

end of sparse solutions, start of uncertainty principle

exercise 10.15 compute the fourier transform of the sequence 1000010000.

exercise 10.16 what is the fourier transform of a gaussian?

exercise 10.17 what is the fourier transform of a cyclic shift of a sequence?

382

exercise 10.18 let s(i, j) be the sequence of i blocks each of length j where each block
of symbols is a 1 followed by j     1 0   s. the number n=6 is factorable but not a perfect
square. what is fourier transform of s (2, 3)= 100100?

exercise 10.19 let z be the n root of unity. prove that(cid:8)zbi|0     i < n(cid:9) = {zi|0     i < n}

provide that b does not divide n.
exercise 10.20 show that if the elements in the second row of the n    n vandermonde
matrix

                     

1
a
a2
...
an   1

1
b
b2
...
bn   1

      
      
      

      

1
c
c2
...
cn   1

                     

are distinct, then the vandermonde matrix is nonsingular by expressing the determinant
of the matrix as an n     1 degree polynomial in a.

exercise 10.21 show that the following two statements are equivalent.
1. if the elements in the second row of the n    n vandermonde matrix

                     

1
a
a2
...
an   1

1
b
b2
...
bn   1

      
      
      

      

1
c
c2
...
cn   1

                     

are distinct, then the vandermonde matrix is nonsingular.

2. specifying the value of an nth degree polynomial at n + 1 points uniquely determines

the polynomial.

exercise 10.22 many problems can be formulated as    nding x satisfying ax = b where
a has more columns than rows and there is a subspace of solutions. if one knows that the
solution is sparse but some error in the measurement b may prevent    nding the sparse
solution, they might add some residual error to b and reformulate the problem as solving
for x and r subject to ax = b + r where r is the residual error. discuss the advantages
and disadvantages of each of the following three versions of the problem.

1. set r=0 and    nd x= argmin(cid:107)x(cid:107)1 satisfying ax = b

2. lasso:    nd x= argmin(cid:0)(cid:107)x(cid:107)1 +   (cid:107)r(cid:107)2

(cid:1) satisfying ax = b + r

2

3.    nd x
  

=argmin(cid:107)x(cid:107)1 such that (cid:107)r(cid:107)2

2 <   

383

exercise 10.23 let m = l+r where l is a low rank matrix corrupted by a sparse noise
matrix r. why can we not recover l from m if r is low rank or if l is sparse?

exercise 10.24

1. suppose for a univariate convex function f and a    nite interval d, |f(cid:48)(cid:48)(x)|       |f(cid:48)(x)|
for every x. then, what is a good step size to choose for id119? derive a
bound on the number of steps needed to get an approximate minimum of f in terms
of as few parameters as possible.

2. generalize the statement and proof to convex functions of d variables.

exercise 10.25 prove that the maximum of a convex function over a polytope is attained
at one of its vertices.

exercise 10.26 create a convex function and a convex region where the maximization
problem has local maximuns.

exercise 10.27 prove lemma 10.10.

exercise 10.28 consider the following symmetric matrix a:

             1

0
1
1

1
1
1    1
0
0
1
2
1    1 0
2

            

find four vectors v1, v2, v3, v4 such that aij = vi
matrix b such that a = bbt .

t vj for all 1     i, j     4. also,    nd a

exercise 10.29 prove that if a1 and a2 are positive semi-de   nite matrices, then so is
a1 + a2.

7. smoothed analysis of algorithms: the simplex algorithm usually takes a polyno-
mial number of steps, journal of the association for computing machinery (jacm), 51
(3) pp: 385463, may 2004. conference version: the annual acm symposium on theory
of computing, pages 296-305, 2001 (with dan spielman).

384

11 wavelets

given a vector space of functions, one would like an orthonormal set of basis functions
that span the space. the fourier transform provides a set of basis functions based on
sines and cosines. often we are dealing with functions that have local structure in which
case we would like the basis vectors to have    nite support. also we would like to have
an e   cient algorithm for computing the coe   cients of the expansion of a function in the
basis.

11.1 dilation

we begin our development of wavelets by    rst introducing dilation. a dilation is a

mapping that scales all distances by the same factor.

   

a dilation equation is an equation where a function is de   ned in terms of a linear

combination of scaled, shifted versions of itself. for instance,

d   1(cid:88)

f (x) =

ckf (2x     k).

an example of this is f (x) = f (2x) + f (2x     1) which has a solution f (x) equal to one
for 0     x < 1 and is zero elsewhere. the equation is illustrated in the    gure below. the
solid rectangle is f (x) and the dotted rectangles are f (2x) and f (2x     1).

k=0

f (x)

f (2x     1)

f (2x)

0

1
2

1

another example is f (x) = 1
in the    gure below. the function f (x) is indicated by solid lines. the functions 1
f (2x + 1), and 1

2f (2x     2) are indicated by dotted lines.

2f (2x     2). a solution is illustrated
2f (2x),

2f (2x) + f (2x     1) + 1

1
f (x)

1
2f (2x)

f (2x     1)
2f (2x     2)

1

0

1

2

385

k=1 ckf (2x    k) then we say that all dilations in the

equation are factor of two reductions.

if a dilation equation is of the form(cid:80)d   1
two or the integral(cid:82)    

lemma 11.1 if a dilation equation in which all the dilations are a factor of two reduction
has a solution, then either the coe   cients on the right hand side of the equation sum to

       f (x)dx of the solution is zero.

proof: integrate both sides of the dilation equation from        to +   .

(cid:90)    

      

f (x)dx =

=

(cid:90)    
d   1(cid:88)

      

k=0

d   1(cid:88)
(cid:90)    

k=0

      

ckf (2x     k)dx =

f (2x     k)dx

ck

f (2x)dx =

f (x)dx

d   1(cid:88)
d   1(cid:88)

k=0

ck

k=0

(cid:90)    
(cid:90)    

      

ck

      

1
2

   (cid:82)

      

d   1(cid:80)

k=0

       f (x)dx (cid:54)= 0, then dividing both sides by

f (x)dx gives

ck = 2

if(cid:82)    

the above proof interchanged the order of the summation and the integral. this is valid
provided the 1-norm of the function is    nite. also note that there are nonzero solutions to
dilation equations in which all dilations are a factor of two reduction where the coe   cients
do not sum to two such as

f (x) = f (2x) + f (2x     1) + f (2x     2) + f (2x     3)

or

in these examples f (x) takes on both positive and negative values and(cid:82)    

f (x) = f (2x) + 2f (2x     1) + 2f (2x     2) + 2f (2x     3) + f (2x     4).

       f (x)dx = 0.

11.2 the haar wavelet

j

2 so that the 2-norm,(cid:82)    

let   (x) be a solution to the dilation equation f (x) = f (2x) + f (2x    1). the function
   is called a scale function or scale vector and is used to generate the two dimensional
family of functions,   jk(x) =   (2jx     k), where j and k are non-negative integers. other
authors scale   jk =   (2jx     k) by 2
jk(t)dt, is 1. however, for
educational purposes, simplifying the notation for ease of understanding was preferred.
for a given value of j, the shifted versions, {  jk|k     0}, span a space vj. the spaces
v0, v1, v2, . . . are larger and larger spaces and allow better and better approximations to
a function. the fact that   (x) is the solution of a dilation equation implies that for any
   xed j,   jk is a linear combination of the {  j+1,k(cid:48)|k(cid:48)     0} and this ensures that vj     vj+1.
it is for this reason that it is desirable in designing a wavelet system for the scale function
to satisfy a dilation equation. for a given value of j, the shifted   jk are orthogonal in the

         2

386

1

1

1

1

3
  00(x) =   (x)

2

1

1

1

1

3
  01(x) =   (x     1)

2

1

3
  02(x) =   (x     2)

2

1

3
  03(x) =   (x     3)

2

4

1

1

1

1

3
  10(x) =   (2x)

2

1

3
  20(x) =   (4x)

2

1

2

  11(x) =   (2x     1)

3

1

2

  12(x) =   (2x     2)

3

1

2

  13(x) =   (2x     3)

3

1

1

1

1

2

  21(x) =   (4x     1)

3

1

2

  22(x) =   (4x     2)

3

1

2

  23(x) =   (4x     3)

3

figure 11.1: set of scale functions associated with the haar wavelet.

sense that(cid:82)

x   jk(x)  jl(x)dx = 0 for k (cid:54)= l.

note that for each j, the set of functions   jk, k = 0, 1, 2 . . . , form a basis for a vector
space vj and are orthogonal. the set of basis vectors   jk, for all j and k, form an over-
complete basis and for di   erent values of j are not orthogonal. since   jk,   j+1,2k, and
  j+1,2k+1 are linearly dependent, for each value of j delete   j+1,k for odd values of k to
get a linearly independent set of basis vectors. to get an orthogonal set of basis vectors,
de   ne

                                 

2k

1
   1 2k+1

2j     x < 2k+1
2j     x < 2k+2

2j

2j

0

otherwise

  jk(x) =

and replace   j,2k with   j+1,2k. basically, replace the three functions

1

1

1

1

  (x)

11
2

  (2x)

11
2
  (2x     1)

by the two functions

387

the haar wavelet

0 otherwise

          1 0     x < 1
                                 

0     x < 1
2     x < 1

1
   1 1

otherwise

0

2

  (x) =

  (x) =

1

1

1
  (x)

  (x)

  (x)

1

  (x)

1

x

1

1

x

-1

1

the basis set becomes

  00   10
  20   22
  30   32   34   36
  40   42   44   46   48   4,10   4,12   4,14

to approximate a function that has only    nite support, select a scale vector   (x)
whose scale is that of the support of the function to be represented. next approximate
the function by the set of scale functions   (2jx     k), k = 0, 1, . . . , for some    xed value of
j. the value of j is determined by the desired accuracy of the approximation. basically
the x axis has been divided into intervals of size 2   j and in each interval the function is
approximated by a    xed value. it is this approximation of the function that is expressed
as a linear combination of the basis functions.

tion is(cid:80)2j   1

once the value of j has been selected, the function is sampled at 2j points, one in each
interval of width 2   j. let the sample values be s0, s1, . . . . the approximation to the func-
k=0 sk  (2jx    k) and is represented by the vector (s0, s1 . . . , s2j   1). the problem
now is to represent the approximation to the function using the basis vectors rather than
the nonorthogonal set of scale functions   jk(x). this is illustrated in the following example.

388

to represent the function corresponding to a vector such as ( 3 1 4 8 3 5 7 9 ),

one needs to    nd the ci such that

                                     =

                                    

                                    

3
1
4
8
3
5
7
9

1
1
1
1
1    1
1    1
1    1
1    1

0
1
1
0
0
1
0
0    1
0
1
1
0
0
1    1
0
1
0
0
0
1    1
0    1
0
0
0
0
0
1
0
0
1
0    1
0
0
0
1
0    1
1
0
0
0
0    1
0    1
0
0

                                    

                                    

                                     .

c1
c2
c3
c4
c5
c6
c7
c8

the    rst column represents the scale function   (x) and subsequent columns the      s.
the tree in figure 11.2 illustrates an e   cient way to    nd the coe   cients representing
the vector ( 3 1 4 8 3 5 7 9 ) in the basis. each vertex in the tree contains the
average of the quantities of its two children. the root gives the average of the elements in
the vector, which is 5 in this example. this average is the coe   cient of the basis vector
in the    rst column of the above matrix. the second basis vector converts the average
of the eight elements into the average of the    rst four elements, which is 4, and the last
four elements, which is 6, with a coe   cient of -1. working up the tree determines the
coe   cients for each basis vector.

1

8

4

6

9

5

7

8

3

4

6

3

2

4

5

figure 11.2: tree of function averages

                                     = 5

                                    

                                    

3
1
4
8
3
5
7
9

1
1
1
1
1
1
1
1

                                    

   1

                                    

1
1
1

1   1   1   1   1

                                    

                                    

                                    

1

1   1   1

0
0
0
0

   2

   2

                                    +1

                                    

                                    

1   1

0
0
0
0
0
0

   2

                                    

                                    

   1

                                    

0
0
0
0

1   1

0
0

0
0

1   1

0
0
0
0

                                    

   1

                                    

0
0
0
0
0
0

1   1

                                    

                                    

0
0
0
0
1

1   1   1

389

11.3 wavelet systems

so far we have explained wavelets using the simple-to-understand haar wavelet. we
now consider general wavelet systems. a wavelet system is built from a basic scaling
function   (x), which comes from a dilation equation. scaling and shifting of the basic
scaling function gives a two dimensional set of scaling functions   jk where

  jk(x) =   (2jx     k).

for a    xed value of j, the   jk span a space vj. if   (x) satis   es a dilation equation

d   1(cid:88)

  (x) =

ck  (2x     k),

then   jk is a linear combination of the   j+1,k   s and this implies that v0     v1     v2     v3        .

k=0

11.4 solving the dilation equation

consider solving a dilation equation

  (x) =

d   1(cid:88)

k=0

ck  (2x     k)

to obtain the scale function for a wavelet system. perhaps the easiest way is to assume
a solution and then calculate the scale function by successive approximation as in the
following program for the daubechies scale function:

  (x) = 1+

   
   
4   (2x) + 3+

4   (2x     1) + 3      

4   (2x     2) + 1      

3

3

3

4   (2x     3),

3

the solution will actually be samples of   (x) at some desired resolution.

program compute-daubechies:

set the initial approximation to   (x) by generating a vector whose components
approximate the samples of   (x) at equally spaced values of x.

begin with the coe   cients of the dilation equation.

   
c1 = 1+
4

3

   
c2 = 3+
4

3

c3 = 3      

4

3

c4 = 1      

4

3

execute the following loop until the values for   (x) converge.

begin

calculate   (2x) by averaging successive values of   (x) together. fill
out the remaining half of the vector representing   (2x) with zeros.

390

calculate   (2x   1),   (2x   2), and   (2x   3) by shifting the contents
of   (2x) the appropriate distance, discarding the zeros that move
o    the right end and adding zeros at the left end.
calculate the new approximation for   (x) using the above values
for   (2x     1),   (2x     2), and   (2x     3) in the dilation equation for
  (2x).

end

figure 11.3: daubechies scale function and associated wavelet

the convergence of the iterative procedure for computing is fast if the eigenvectors of

a certain matrix are unity.

another approach to solving the dilation equation

consider the dilation equation   (x) = 1

continuous solutions with support in 0     x < 2.

2f (2x) + f (2x     1) + 1

2f (2x     2) and consider

  (0) = 1
  (2) = 1
  (1) = 1

2  (0) +   (   1) +   (   2) = 1
2  (4) +   (3) +   (2) = 1
2  (2) + 0 + 0
2  (2) +   (1) +   (0) = 0 +   (1) + 0

2  (0) + 0 + 0

  (0) = 0
  (2) = 0
  (1) arbitrary

set   (1) = 1. then

  ( 1

2) = 1

2  (1) +   (0) + 1

2  (   1) = 1

2

  ( 3

2) = 1

2  (3) +   (2) + 1

2  (1) = 1
2  (    3
2) + 1
one can continue this process and compute   ( i
2j ) for larger values of j until   (x) is
approximated to a desired accuracy. if   (x) is a simple equation as in this example, one
could conjecture its form and verify that the form satis   es the dilation equation.

2) +   (    1

4) = 1

2) = 1

2  ( 1

  ( 1

.

2

4

391

11.5 conditions on the dilation equation

we would like a basis for a vector space of functions where each basis vector has
   nite support and the basis vectors are orthogonal. this is achieved by a wavelet system
consisting of a shifted version of a scale function that satis   es a dilation equation along
with a set of wavelets of various scales and shifts. for the scale function to have a nonzero
integral, lemma 11.1 requires that the coe   cients of the dilation equation sum to two.
although the scale function   (x) for the haar system has the property that   (x) and
  (x     k), k > 0, are orthogonal, this is not true for the scale function for the dilation
2  (2x    2). the conditions that integer shifts of the
equation   (x) = 1
scale function be orthogonal and that the scale function has    nite support puts additional
conditions on the coe   cients of the dilation equation. these conditions are developed in
the next two lemmas.

2  (2x) +   (2x    1) + 1

if   (x) and   (x     k) are orthogonal for k (cid:54)= 0 and   (x) has been normalized so that

d   1(cid:88)

lemma 11.2 let

  (x) =

ck  (2x     k).

k=0

i=0 cici   2k = 2  (k).

(cid:82)    
         (x)  (x     k)dx =   (k), then(cid:80)d   1
proof: assume   (x) has been normalized so that(cid:82)    
(cid:90)    
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)

  (x)  (x     k)dx =

(cid:90)    

(cid:90)    

x=      

x=      

ci  (2x     i)

i=0

=

cicj

d   1(cid:88)

j=0

since

(cid:90)    

x=      

i=0

j=0

  (2x     i)  (2x     2k     j)dx =

         (x)  (x     k)dx =   (k). then

cj  (2x     2k     j)dx

  (2x     i)  (2x     2k     j)dx

  (y     i)  (y     2k     j)dy

  (y)  (y + i     2k     j)dy

  (x)  (x     k)dx =

  (2k + j     i) =

cici   2k. since   (x) was nor-

(cid:90)    
(cid:90)    

      

x=      
malized so that

d   1(cid:88)

d   1(cid:88)

i=0

j=0

cicj

1
2

  (x)  (x     k)dx =   (k), it follows that

cici   2k = 2  (k).

x=      

(cid:90)    
(cid:90)    

x=      

x=      
  (2k + j     i),

d   1(cid:88)

i=0

1
2

1
2
1
2
1
2

=

=

d   1(cid:88)

i=0

392

scale and wavelet coe   cients equations

k=0 ck  (2x     k)
  (x)  (x     k)dx =   (k)

  (x) =(cid:80)d   1
   (cid:82)
d   1(cid:80)
d   1(cid:80)

cj = 2

j=0

      

cjcj   2k = 2  (k)

j=0

ck = 0 unless 0     k     d     1

d even

d   1(cid:80)

j=0

c2j =

d   1(cid:80)

j=0

c2j+1

d   1(cid:80)

  (x) =

bk  (x     k)
  (x)  (x     k) = 0

k=0

  (x)dx = 0

  (x)  (x     k)dx =   (k)

(   1)kbibi   2k = 2  (k)

x=      

x=      

x=      

   (cid:82)
   (cid:82)
   (cid:82)
d   1(cid:80)
d   1(cid:80)
d   1(cid:80)

j=0

i=0

cjbj   2k = 0

bj = 0

j=0

bk = (   1)kcd   1   k

one designs wavelet systems so the above conditions are satis   ed.

lemma 11.2 provides a necessary but not su   cient condition on the coe   cients of
the dilation equation for shifts of the scale function to be orthogonal. one should note
that the conditions of lemma 11.2 are not true for the triangular or piecewise quadratic
solutions to

  (x) =

  (2x) +   (2x     1) +

  (2x     2)

1
2

1
2

and

  (x) =

1
4

  (2x) +

  (2x     1) +

3
4

  (2x     2) +

3
4

  (2x     3)

1
4

which overlap and are not orthogonal.

for   (x) to have    nite support the dilation equation can have only a    nite number of

terms. this is proved in the following lemma.
lemma 11.3 if 0     x < d is the support of   (x), and the set of integer shifts, {  (x    
k)|k     0}, are linearly independent, then ck = 0 unless 0     k     d     1.
proof: if the support of   (x) is 0     x < d, then the support of   (2x) is 0     x < d

2 . if

  (x) =

ck  (2x     k)

   (cid:88)

k=      

393

d   1(cid:80)
d   1(cid:88)

i=0

i=0

d   1(cid:88)

d   1(cid:88)

i=0

the support of both sides of the equation must be the same. since the   (x   k) are linearly
independent the limits of the summation are actually k = 0 to d     1 and

  (x) =

ck  (2x     k).

it follows that ck = 0 unless 0     k     d     1.

k=0

the condition that the integer shifts are linearly independent is essential to the proof

and the lemma is not true without this condition.

one should also note that
and k = d   1

2

cici   2k = 0 for k (cid:54)= 0 implies that d is even since for d odd

cici   2k =

cici   d+1 = cd   1c0.

equation has d terms and the coe   cients satisfy the linear equation(cid:80)d   1
2 quadratic equations(cid:80)d   1

for cd   1c0 to be zero either cd   1 or c0 must be zero. since either c0 = 0 or cd   1 = 0, there
are only d    1 nonzero coe   cients. from here on we assume that d is even. if the dilation
k=0 ck = 2 and the
2     1

i=0 cici   2k = 2  (k) for 1     k     d   1

2 , then for d > 2 there are d

coe   cients that can be used to design the wavelet system to achieve desired properties.

d

11.6 derivation of the wavelets from the scaling function

d   1(cid:80)

by   (x) =

in a wavelet system one develops a mother wavelet as a linear combination of integer
shifts of a scaled version of the scale function   (x). let the mother wavelet   (x) be given
bk  (2x     k). one wants integer shifts of the mother wavelet   (x     k) to
be orthogonal and also for integer shifts of the mother wavelet to be orthogonal to the
scaling function   (x). these conditions place restrictions on the coe   cients bk which are
the subject matter of the next two lemmas.

k=0

lemma 11.4 (orthogonality of   (x) and   (x     k)) let   (x) =

and   (x   k) are orthogonal for k (cid:54)= 0 and   (x) has been normalized so that(cid:82)    

bk  (2x     k). if   (x)
         (x)  (x   

k=0

d   1(cid:80)

k)dx =   (k), then

proof: analogous to lemma 11.2.

i=0

d   1(cid:88)

(   1)kbibi   2k = 2  (k).

394

d   1(cid:80)

d   1(cid:80)

i=0

  (x) =

k=0

all k, then

proof:(cid:90)    

x=      

lemma 11.5 (orthogonality of   (x) and   (x     k)) let   (x) =

bk  (2x     k). if

  (x)  (x     k)dx =   (k) and

   (cid:82)

x=      

cibi   2k = 0 for all k.

d   1(cid:80)

   (cid:82)

ck  (2x     k) and
  (x)  (x     k)dx = 0 for

k=0

x=      

  (x)  (x     k)dx =

(cid:90)    

x=      

d   1(cid:88)

i=0

ci  (2x     i)

d   1(cid:88)

j=1

bj  (2x     2k     j)dx = 0.

interchanging the order of integration and summation

d   1(cid:88)

d   1(cid:88)

i=0

j=0

(cid:90)    

cibj

x=      

  (2x     i)  (2x     2k     j)dx = 0

substituting y = 2x     i yields

d   1(cid:88)

d   1(cid:88)

i=0

j=0

(cid:90)    

y=      

  (y)  (y     2k     j + i)dy = 0

1
2

thus,

summing over j gives

cibj

d   1(cid:88)

i=0

d   1(cid:88)
d   1(cid:88)

j=0

i=0

cibj  (2k + j     i) = 0

cibi   2k = 0

lemma 11.5 gave a condition on the coe   cients in the equations for   (x) and   (x) if
integer shifts of the mother wavelet are to be orthogonal to the scale function. in addition,
for integer shifts of the mother wavelet to be orthogonal to the scale function requires
that bk = (   1)kcd   1   k.

lemma 11.6 let the scale function   (x) equal

ck  (2x   k) and let the wavelet function

d   1(cid:80)

k=0

  (x) equal

bk  (2x     k). if the scale functions are orthogonal

d   1(cid:80)

k=0

(cid:90)    

      

  (x)  (x     k)dx =   (k)

395

d   1(cid:80)
2   1(cid:88)

j=0

d

d   1(cid:80)

and the wavelet functions are orthogonal with the scale function

   (cid:90)

x=      

  (x)  (x     k)dx = 0

for all k, then bk = (   1)kcd   1   k.

proof: by lemma 11.5,

even indices gives

cjbj   2k = 0 for all k. separating

2   1(cid:88)

d

for all k.

j=0

j=0

d   1(cid:80)

j=0

cjbj   2k = 0 into odd and

c2jb2j   2k +

c2j+1b2j+1   2k = 0

(11.1)

c0b0 + c2b2+c4b4 +        + c1b1 + c3b3 + c5b5 +        = 0
+ c3b1 + c5b3 +        = 0
+ c5b1 +        = 0

c2b0+c4b2 +       
c4b0 +       

k = 0
k = 1
k = 2

d   1(cid:80)

by lemmas 11.2 and 11.4,

j=0
separating odd and even terms,

cjcj   2k = 2  (k) and

bjbj   2k = 2  (k) and for all k.

j=0

c2j+1c2j+1   2k = 2  (k)

(11.2)

c2jc2j   2k +

d

2   1(cid:88)
2   1(cid:88)

j=0

d

d

2   1(cid:88)
2   1(cid:88)

j=0

d

b2jb2j   2k +

(   1)jb2j+1b2j+1   2k = 2  (k)

(11.3)

and

for all k.

j=0

j=0

c0c0 + c2c2+c4c4 +        + c1c1 + c3c3 + c5c5 +        = 2
+ c3c1 + c5c3 +        = 0
+ c5c1 +        = 0

c2c0+c4c2 +       
c4c0 +       

b0b0 + b2b2+b4b4 +        + b1b1     b3b3 + b5b5            = 2
    b3b1 + b5b3            = 0
+ b5b1            = 0

b2b0+b4b2 +       
b4b0 +       

k = 0
k = 1
k = 2

k = 0
k = 1
k = 2

396

let ce = (c0, c2, . . . , cd   2), co = (c1, c3, . . . , cd   1), be = (b0, b2, . . . , bd   2), and bo =
(b1, b3, . . . , bd   1). equations 12.1, 12.2, and 11.3 can be expressed as convolutions46 of
these sequences. equation 12.1 is ce    br
o =   (k),
and 11.3 is be     br
o =   (k), where the superscript r stands for reversal of the
sequence. these equations can be written in matrix format as

e + co    br
o = 0, 12.2 is ce    c r
(cid:19)

e + bo     br

e + co    c r

   

c r

be bo

(cid:18) c r
(cid:18) ce co
(cid:19)
(cid:19)(cid:18) f (c r
(cid:18) f (ce) f (co)
(cid:17)(cid:16)

f (be) f (bo)

f (c r

(cid:16)

(cid:18) 2  
(cid:19)

0

0
2  

(cid:19)
(cid:18) 2 0

0 2

e br
e
o br
o

=

e ) f (br
e )
o ) f (br
o )

=

(cid:19)

.

(cid:17)

where f denotes the transform. taking the determinant yields

f (ce)f (bo)     f (be)f (co)

f (ce)f (bo)     f (co)f (be)
thus f (ce)f (bo)     f (co)f (be) = 2 and the inverse transform yields

= 4

taking the fourier or z-transform yields

ce     bo     co     be = 2  (k).

convolution by c r

d   1(cid:80)

j=0

now

e yields
e     ce     bo     c r
c r
e     be = c r
cjbj   2k = 0 so    c r
e     ce     bo + c r
c r
e     ce + c r
(c r

e     be     co = c r

e     2  (k)

o     bo. thus

o     bo     co = 2c r
o     co)     bo = 2c r
2  (k)     bo = 2c r
ce = br
o

e       (k)
e       (k)
e       (k)

thus, ci = 2bd   1   i for even i. by a similar argument, convolution by c r

0 yields

since c r

)     b0 =    c r

0     ce     b0     c r
c r
0     be
   c r

e     c r
   (ce     c r

e     be     c r
e + c r

0     c0     be = 2c r

0   (k)

0     c0     be = 2c r
0     c0)     be = 2c r
   2  (k)be = 2c r
   be = c r

0

0   (k)
0   (k)
0   (k)

thus, ci =    2bd   1   i for all odd i and hence ci = (   1)i2bd   1   i for all i.
46the convolution of (a0, a1, . . . , ad   1) and (b0, b1, . . . , bd   1) denoted

(a0, a1, . . . , ad   1)     (b0, b1, . . . , bd   1) is the sequence
(a0bd   1, a0bd   2 + a1bd   1, a0bd   3 + a1bd   2 + a3bd   1 . . . , ad   1b0).

397

11.7 su   cient conditions for the wavelets to be orthogonal

section 11.6 gave necessary conditions on the bk and ck in the de   nitions of the scale
function and wavelets for certain orthogonality properties. in this section we show that
these conditions are also su   cient for certain orthogonality conditions. one would like a
wavelet system to satisfy certain conditions.

1. wavelets,   j(2jx     k), at all scales and shifts to be orthogonal to the scale function

  (x).

2. all wavelets to be orthogonal. that is

(cid:90)    

      

  j(2jx     k)  l(2lx     m)dx =   (j     l)  (k     m)

3.   (x) and   jk, j     l and all k, to span vl, the space spanned by   (2lx     k) for all k.
these items are proved in the following lemmas. the    rst lemma gives su   cient conditions
on the wavelet coe   cients bk in the de   nition

(cid:88)

  (x) =

bk  (2x     k)

k

for the mother wavelet so that the wavelets will be orthogonal to the scale function. the
lemma shows that if the wavelet coe   cients equal the scale coe   cients in reverse order
with alternating negative signs, then the wavelets will be orthogonal to the scale function.

lemma 11.7 if bk = (   1)kcd   1   k, then(cid:82)    

         (x)  (2jx     l)dx = 0 for all j and l.

proof: assume that bk = (   1)kcd   1   k. we    rst show that   (x) and   (x    k) are orthog-
onal for all values of k. then we modify the proof to show that   (x) and   (2jx     k) are
orthogonal for all j and k.

assume bk = (   1)kcd   1   k. then

(cid:90)    

      

  (x)  (x     k) =

=

=

i=0

      

(cid:90)    
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)

j=0

j=0

i=0

i=0

d   1(cid:88)
(cid:90)    

j=0

      

ci  (2x     i)

bj  (2x     2k     j)dx

ci(   1)jcd   1   j

  (2x     i)  (2x     2k     j)dx

(   1)jcicd   1   j  (i     2k     j)

(   1)jc2k+jcd   1   j

j=0

=
= c2kcd   1     c2k+1cd   2 +        + cd   2c2k   1     cd   1c2k
= 0

398

(cid:90)    

      

the last step requires that d be even which we have assumed for all scale functions.

for the case where the wavelet is   (2j     l),    rst express   (x) as a linear combination

of   (2j   1x     n). now for each these terms

  (2j   1x     m)  (2jx     k)dx = 0

to see this, substitute y = 2j   1x. then

  (2jx     m)  (2jx     k)dx =

(cid:90)    

      

(cid:90)    

      

1
2j   1

  (y     m)  (2y     k)dy

which by the previous argument is zero.

the next lemma gives conditions on the coe   cients bk that are su   cient for the

wavelets to be orthogonal.
lemma 11.8 if bk = (   1)kcd   1   k, then

proof: the    rst level wavelets are orthogonal.

(cid:90)    

      

(cid:90)    

      

  (x)  (x     k)dx =

bj  (2x     2k     j)dx

  (2x     i)  (2x     2k     j)dx

1

1

bi

=

bj

i=0

j=0

      

d   1(cid:88)

(cid:90)    

bi  (2x     i)

2k   l(2lx     m)dx =   (j     l)  (k     m).
2j   j(2jx     k)
(cid:90)    
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)

(   1)icd   1   i(   1)i   2kcd   1   i+2k

bibj  (i     2k     j)

(   1)2i   2kcd   1   icd   1   i+2k

bibi   2k

      

j=0

j=0

i=0

i=0

i=0

i=0

=

=

=

=

substituting j for d     1     i yields

i=0

d   1(cid:88)

j=0

cjcj+2k = 2  (k)

399

(cid:90)    

      

(cid:90)    

      

j=0

  (4x     i)  (2x     2k     j)dx

  (4x     i)  (4x     4k     2j     l)dx

example of orthogonality when wavelets are of di   erent scale.

  (2x)  (x     k)dx =

bi  (4x     i)

bj  (2x     2k     j)dx

d   1(cid:88)

(cid:90)    
d   1(cid:88)
d   1(cid:88)
d   1(cid:88)

      

i=0

i=0

i=0

=

bibj

since   (2x     2k     j) =

cl  (4x     4k     2j     l)

  (2x)  (x     k)dx =

bibjcl

d   1(cid:88)
d   1(cid:88)

l=0

i=0

d   1(cid:88)
d   1(cid:88)
d   1(cid:88)

i=0

j=0

d   1(cid:88)
d   1(cid:88)
d   1(cid:88)

j=0

i=0

j=0

=

=

l=0

bibjci   4k   2j

d   1(cid:80)

j=0

since

cjbj   2k = 0,

bici   4k   2j =   (j     2k) thus

bibjcl  (i     4k     2j     l)

  (2x)  (x     k)dx =

bj  (j     2k) = 0.

d   1(cid:80)

l=0

d   1(cid:80)
(cid:90)    

i=0

      

(cid:90)    

      

(cid:90)    

      

d   1(cid:88)

j=0

orthogonality of scale function with wavelet of di   erent scale.

(cid:90)    

      

  (x)  (2x     k)dx =

=

=

      

(cid:90)    
d   1(cid:88)
d   1(cid:88)

j=0

cj

j=0

d   1(cid:88)
(cid:90)    
(cid:90)    

      

cj

1
2

      

j=0

cj  (2x     j)  (2x     k)dx

  (2x     j)  (2x     k)dx

  (y     j)  (y     k)dy

= 0

if    was of scale 2j,    would be expanded as a linear combination of    of scale 2j all of
which would be orthogonal to   .

400

11.8 expressing a function in terms of wavelets

given a wavelet system with scale function    and mother wavelet    we wish to express
a function f (x) in terms of an orthonormal basis of the wavelet system. first we will ex-
press f (x) in terms of scale functions   jk(x) =   (2jx     k). to do this we will build a tree
similar to that in figure 11.2 for the haar system, except that computing the coe   cients
will be much more complex. recall that the coe   cients at a level in the tree are the
coe   cients to represent f (x) using scale functions with the precision of the level.

f (x) using level j scale functions. since the   j(x     k) are orthogonal

k=0 ajk  j(x     k) where the ajk are the coe   cients in the expansion of

let f (x) =(cid:80)   

(cid:90)    

ajk =

x=      

f (x)  j(x     k)dx.

expanding   j in terms of   j+1 yields

d   1(cid:88)

m=0

cm  j+1(2x     2k     m)dx

f (x)  j+1(2x     2k     m)dx

(cid:90)    
d   1(cid:88)
d   1(cid:88)

m=0

m=0

ajk =

=

=

x=      

f (x)

(cid:90)    

cm

x=      

cmaj+1,2k+m

let n = 2k + m. now m = n     2k. then

d   1(cid:88)

ajk =

cn   2kaj+1,n

(11.4)

n=2k

in construction the tree similar to that in figure 11.2, the values at the leaves are
the values of the function sampled in the intervals of size 2   j. equation 11.4 is used to
compute values as one moves up the tree. the coe   cients in the tree could be used if we
wanted to represent f (x) using scale functions. however, we want to represent f (x) using
one scale function whose scale is the support of f (x) along with wavelets which gives us
an orthogonal set of basis functions. to do this we need to calculate the coe   cients for
the wavelets. the value at the root of the tree is the coe   cient for the scale function. we
then move down the tree calculating the coe   cients for the wavelets.

finish by calculating wavelet coe   cients

maybe add material on jpeg

example: add example using d4.
maybe example using sinc

401

11.9 designing a wavelet system

in designing a wavelet system there are a number of parameters in the dilation equa-
tion. if one uses d terms in the dilation equation, one degree of freedom can be used to
satisfy

d   1(cid:88)

i=0

ci = 2

which insures the existence of a solution with a nonzero mean. another d
freedom are used to satisfy

2 degrees of

d   1(cid:88)

cici   2k =   (k)

i=0

which insures the orthogonal properties. the remaining d
used to obtain some desirable properties such as smoothness. smoothness appears to be
related to vanishing moments of the scaling function. material on the design of systems
is beyond the scope of this book and can be found in the literature.

2     1 degrees of freedom can be

11.10 applications

wavelets are widely used in data compression for images and speech, as well as in
id161 for representing images. unlike the sines and cosines of the fourier
transform, wavelets have spatial locality in addition to frequency information, which can
be useful for better understanding the contents of an image and for relating pieces of
di   erent images to each other. wavelets are also being used in power line communication
protocols that send data over highly noisy channels.

11.11 bibliographic notes

in 1909 alfred haar presented an orthonormal basis for functions with    nite support.
ingrid daubechies

402

11.12 exercises
exercise 11.1 give a solution to the dilation equation f (x) = f (2x)+f (2x   k) satisfying
f (0) = 1. assume k is an integer.
exercise 11.2 are there solutions to f (x) = f (2x) + f (2x     1) other than a constant
multiple of

(cid:26) 1 0     x < 1

0 otherwise

?

f (x) =

exercise 11.3 is there a solution to f (x) = 1
f (0) = f (1) = 1 and f (2) = 0?

2f (2x) + f (2x     1) + 1

2f (2x     2) with

exercise 11.4 what is the solution to the dilation equation

f (x) = f (2x) + f (2x     1) + f (2x     2) + f (2x     3).

exercise 11.5 consider the dilation equation

f (x) = f (2x) + 2f (2x     1) + 2f (2x     2) + 2f (2x     3) + f (2x     4)

1. what is the solution to the dilation equation?

2. what is the value of(cid:82)    

       f (x)dx?

exercise 11.6 what are the solutions to the following families of dilation equations.

f (x) =f (2x) + f (2x     1)

f (x) =

f (x) =

1
2
1
4

f (2x) +

f (2x) +

f (2x     1) +
f (2x     1) +

f (2x     2) +
f (2x     2) +

1
2
1
4

f (2x     3)
f (2x     3) +

1
2
1
4

f (2x     4) +

1
4

f (2x     5)

1
4

1
2
1
4

1
k

f (2x     6) +

+

1
4

1
4

f (2x     7)
1
k

f (2x) +        +

f (x) =

1
k

f (2x) +

f (2x)

1.

2.

f (x) =

f (x) =

f (x) =

f (x) =

1
3
1
4
1
5
1
k

f (2x) +

f (2x) +

f (2x) +

f (2x) +

f (2x     1) +
2
3
f (2x     1) +
3
4
f (2x     1) +
4
5
k     1
k

f (2x     2) +
f (2x     2) +
f (2x     2) +

2
3
3
4
4
5

k     1
k

1
3
1
4
1
5

f (2x     3)
f (2x     3)
f (2x     3)
1
k

f (2x     1) +

f (2x     2) +

f (2x     3)

403

3.

4.

f (x) =

f (x) =

f (x) =

f (x) =

f (x) =

f (x) =

f (x) =

f (x) =

exercise 11.7

2

3

1
1
f (2x) +
2
2
f (2x)     1
3
2
2
f (2x)     3
5
2
2
1 + 2k

f (2x     1) +
f (2x     1) +
f (2x     1) +
f (2x)     2k     1

1
2
3
2
5
2

f (2x     2) +
1
2
f (2x     2)     1
2
f (2x     2)     3
2
1 + 2k

f (2x     3)
f (2x     3)
f (2x     3)
f (2x     2)     2k     1

f (2x     1) +

1
2
f (2x) +
3
3
f (2x)     1
4
3
3
f (2x)     4
7
3
3
1 + 3k

f (2x     1) +
f (2x     1) +
f (2x     1) +
f (2x)     2     3k

2
3
5
3
8
3

f (2x     2) +
1
3
f (2x     2)     2
3
f (2x     2)     5
3
2 + 3k

f (2x     3)
f (2x     3)
f (2x     3)
f (2x     2)     1     3k

f (2x     1) +

2

3

f (2x     3)

f (2x     3)

2

3

1. what is the solution to the dilation equation f (x) = 1

write a program to see what the solution looks like.

2f (2x) + 3

2f (2x     1)? hint:

2. how does the solution change when the equation is changed to f (x) = 1

3f (2x) +

5

3f (2x     1)?

3. how does the solution change if the coe   cients no longer sum to two as in f (x) =

f (2x) + 3f (2x     1)?

exercise 11.8 if f (x) is frequency limited by 2  , prove that

f (x) =

sin(  (x     k))

  (x     k)

.

f (k)

hint: use the nyquist sampling theorem which states that a function frequency limited by
2   is completely determined by samples spaced one unit apart. note that this result means
that

f (k) =

f (x)

sin(  (x     k))

  (x     k)

dx

exercise 11.9 compute an approximation to the scaling function that comes from the
dilation equation

   

  (x) =

1 +
4

3

  (2x) +

3 +
4

3        

4

   
3

  (2x     1) +

404

3

  (2x     2) +

3

  (2x     3).

   
1

4

2

3

   (cid:88)

k=0

(cid:90)    

      

exercise 11.10 consider f (x) to consist of the semi circle (x     1
for 0     x     1 and 0 otherwise.

2)2 + y2 = 1

4 and y     0

1. using precision j = 4    nd the coe   cients for the scale functions and the wavelets

for d4 de   ned by the dilation equation

   

   
3

  (x) =

1 +
4

3

  (2x) +

3 +
4

  (2x     1) +

3        

4

3

  (2x     2) +

   
1

4

3

  (2x     3)

2. graph the approximation to the semi circle for precision j = 4.

exercise 11.11 what is the set of all solutions to the dilation equation
   
3
1

3        

   

   

3

3

3

  (2x     1) +

  (2x     2) +

  (2x     3)

  (x) =

  (2x) +

1 +
4

3 +
4

4

4

exercise 11.12 prove that if scale functions de   ned by a dilation equation are orthogo-
nal, then the sum of the even coe   cients must equal the sum of the odd coe   cients in the

dilation equation. that is,(cid:80)

c2k =(cid:80)

c2k+1.

k

k

function = wavelets

acc=32; %accuracy of computation
phit=[1:acc zeros(1,3*acc)];

c1=(1+3^0.5)/4; c2=(3+3^0.5)/4; c3=(3-3^0.5)/4; c4=(1-3^0.5)/4;

for i=1:10

temp=(phit(1:2:4*acc)+phit(2:2:4*acc))/2;
phi2t=[temp zeros(1,3*acc)];

phi2tshift1=[ zeros(1,acc) temp zeros(1,2*acc)];
phi2tshift2=[ zeros(1,2*acc) temp zeros(1,acc)];
phi2tshift3=[ zeros(1,3*acc) temp ];

phit=c1*phi2t+c2*phi2tshift1+c3*phi2tshift2+c4*phi2tshift3;

plot(phit)
figure(gcf)
pause

end plot(phit) figure(gcf) end

405

12 appendix

12.1 de   nitions and notation

. . . ,   3,   2,   1,

0, 1, 2, 3, . . .

nonnegative integers

(cid:125)(cid:124)
(cid:123)(cid:122)

(cid:124)

(cid:122)

(cid:123)(cid:122)

integers

(cid:125)

(cid:123)
(cid:125)

positive integers

(cid:124)

substructures
a substring of a string is a continuous string of symbols from the original string. a
subsequence of a sequence is a sequence of elements from the original sequence in order
but not necessarily continuous. with subgraphs there are two possible de   nitions. we
de   ne a subgraph of a graph to be a subset of the vertices and a subset of the edges of
the graph induced by the vertices. an induced subgraph is a subset of the vertices and
all the edges of the graph induced by the subset of vertices.

12.2 asymptotic notation

we introduce the big o notation here. a motivating example is analyzing the running
time of an algorithm. the running time may be a complicated function of the input length
n such as 5n3 + 25n2 ln n     6n + 22. asymptotic analysis is concerned with the behavior
as n         where the higher order term 5n3 dominates. further, the coe   cient 5 of 5n3
is not of interest since its value varies depending on the machine model. so we say that
the function is o(n3). the big o notation applies to functions on the positive integers
taking on positive real values.

de   nition 12.1 for functions f and g from the natural numbers to the positive reals,
f (n) is o(g(n)) if there exists a constant c >0 such that for all n, f (n)     cg(n).

thus, f (n) = 5n3 + 25n2 ln n     6n + 22 is o(n3). the upper bound need not be tight.
for example, in this case f (n) is also o(n4). note in our de   nition we require g(n) to be
strictly greater than 0 for all n.

to say that the function f (n) grows at least as fast as g(n), one uses a notation omega.
for positive real valued f and g, f (n) is    (g(n)) if there exists a constant c > 0 such that
for all n, f (n)     cg(n). if f (n) is both o(g(n)) and    (g(n)), then f (n) is   (g(n)). theta
is used when the two functions have the same asymptotic growth rate.

little o is used. we say f (n) is o(g(n)) if

many times one wishes to bound the low order terms. to do this, a notation called
f (n)
g(n) = 0. note that f (n) being o(g(n))
means that asymptotically f (n) does not grow faster than g(n), whereas f (n) being
o(g(n)) means that asymptotically f (n)/g(n) goes to zero. if f (n) = 2n +
n, then f (n)

lim
n      

   

406

is o(n) but in bounding the lower order term, we write f (n) = 2n + o(n). finally, we
write f (n)     g(n) if lim
g(n) =    . the di   erence
n      
between f (n) being   (g(n)) and f (n)     g(n) is that in the    rst case f (n) and g(n) may
di   er by a multiplicative constant factor. we also note here that formally, o(g(n)) is a
set of functions, namely the set of functions f such that f (n) is o(g(n)); that is,    f (n) is
o(g(n))    formally means that f (n)     o(g(n)).

f (n)
g(n) = 1 and say f (n) is   (g(n)) if lim
n      

f (n)

   

   

=

<

=

>

asymptotic upper bound
f (n) is o(g(n)) if for all n, f (n)     cg(n) for some constant c > 0.

asymptotic lower bound
f (n) is    (g(n)) if for all n, f (n)     cg(n) for some constant c > 0.

asymptotic equality
f (n) is   (g(n)) if it is both o(g(n)) and    (g(n)).

f (n) is o(g(n)) if lim
n      
f (n)     g(n) if lim
n      

f (n)
g(n) = 1.

f (n)
g(n) = 0 .

f (n) is    (g (n)) if

f (n)

g(n) =    .

lim
n      

407

ai = 1 + a + a2 +        =

ai = 1 + a + a2 +        =

iai = a + 2a2 + 3a3        =

i2ai = a + 4a2 + 9a3        =

a (cid:54)= 1

,

|a| < 1

1     an+1
1     a
1

,

1     a
a

(1     a)2 ,
a(1 + a)
(1     a)3 ,

|a| < 1

|a| < 1

i =

n(n + 1)

2

i2 =

n(n + 1)(2n + 1)

6

1
i2 =

  2
6

12.3 useful relations

summations

i=0

i=0

i=0

n(cid:88)
   (cid:88)
   (cid:88)
   (cid:88)
n(cid:88)
n(cid:88)
   (cid:88)

i=0

i=1

i=1

i=1

we prove one equality:

   (cid:88)
   (cid:80)

i=0

i=0

iai = a + 2a2 + 3a3        =

a

(1     a)2 , provided |a| < 1.

proof: write s =

iai. so,

   (cid:88)

as =

iai+1 =

   (cid:88)

i=1

(i     1)ai.

i=0

thus,

   (cid:88)
from which the equality follows. the sum(cid:80)

s     as =

   (cid:88)

iai    

i=1

i=1

(i     1)ai =

   (cid:88)

i=1

ai =

a
1     a

,

i2ai can also be done by an extension of this

i

method (left to the reader). using generating functions, we will see another proof of both
these equalities by derivatives.

   (cid:88)

i=1

1
i

2 +(cid:0) 1

= 1 + 1

3 + 1

4

(cid:1) +(cid:0) 1

5 + 1

6 + 1

7 + 1

8

(cid:1) +            1 + 1

2 +        and thus diverges.

2 + 1

408

n(cid:80)

n(cid:80)
   where       = 0.5772 is euler   s constant. thus,

1
i grows as ln n since

the summation

i    (cid:82) n
n(cid:80)

i=1

i=1

1

truncated taylor series

i=1

x=1

1
i

(cid:18) n(cid:80)

1
x dx. in fact, lim
n      
   = ln(n) +    for large n.

i=1

(cid:19)
i     ln(n)

1

=

if all the derivatives of a function f (x) exist, then we can write

f (x) = f (0) + f(cid:48)(0)x + f(cid:48)(cid:48)(0)

+        .

x2
2

the series can be truncated. in fact, there exists some y between 0 and x such that

f (x) = f (0) + f(cid:48)(y)x.

also, there exists some z between 0 and x such that

f (x) = f (0) + f(cid:48)(0)x + f(cid:48)(cid:48)(z)

x2
2

and so on for higher derivatives. this can be used to derive inequalities. for example, if
f (x) = ln(1 + x), then its derivatives are

f(cid:48)(x) =

1

1 + x

; f(cid:48)(cid:48)(x) =    

1

(1 + x)2 ; f(cid:48)(cid:48)(cid:48)(x) =

2

(1 + x)3 .

for any z, f(cid:48)(cid:48)(z) < 0 and thus for any x, f (x)     f (0) + f(cid:48)(0)x, hence ln(1 + x)     x, which
also follows from the inequality 1 + x     ex. also using

f (x) = f (0) + f(cid:48)(0)x + f(cid:48)(cid:48)(0)

x2
2

+ f(cid:48)(cid:48)(cid:48)(z)

x3
3!

for z >    1, f(cid:48)(cid:48)(cid:48)(z) > 0, and so for x >    1,

ln(1 + x) > x     x2
2

.

exponentials and logs

ex = 1 + x +

x2
2!

+

x3
3!

alog b = blog a
+       

setting x = 1 in the equation ex = 1 + x + x2

1

e     2.718
3! +        yields e =

e     0.3679.
   (cid:80)

1
i!.

2! + x3

i=0

409

(cid:0)1 + a

n

(cid:1)n = ea

lim
n      
ln(1 + x) = x     1
2

x2 +

1
3

x3     1
4

x4       

|x| < 1

the above expression with    x substituted for x gives rise to the approximations

ln(1     x) <    x

which also follows from 1     x     e   x, since ln(1     x) is a monotone function for x     (0, 1).
for 0 < x < 0.69, ln(1     x) >    x     x2.

trigonometric identities

eix = cos(x) + i sin(x)
2 (eix + e   ix)
cos(x) = 1
2i (eix     e   ix)
sin(x) = 1
sin(x    y) = sin(x) cos(y)    cos(x) sin(y)
cos(x    y) = cos(x) cos(y)     sin(x) sin(y)
cos (2  ) = cos2        sin2    = 1     2 sin2   
sin (2  ) = 2 sin    cos   
sin2   
cos2   

2 (1     cos   )
2 (1 + cos   )

2 = 1
2 = 1

410

gaussian and related integrals

1

a2+x2 dx = 1

a thus

1

a2+x2 dx =   

a

e    a2x2

2 dx =

thus

    a2x2
e

2 dx = 1

   (cid:90)
   (cid:90)

      

a   
2  

      

eax2

1
2a
a tan   1 x
   
2  
a

(cid:114)   

1
4a
   
  

(cid:90)
(cid:90)
   (cid:90)
   (cid:90)
   (cid:90)
(cid:90)    
   (cid:90)

0

0

0

      

xeax2dx =

x2e   ax2dx =

    x2

x2ne

a2 dx =

    x2

x2n+1e

e   x2dx =

a
1    3    5          (2n     1)

2n+1

a2n   1 =

   

  

(cid:16) a

(cid:17)2n+1

(2n)!

n!

2

n!
2

a2n+2

   (x2+y2)dxdy. let x =
e

a2 dx =
   
  

(cid:18)    (cid:82)
(cid:12)(cid:12)(cid:12)(cid:12) =

      

      

   (cid:82)
(cid:12)(cid:12)(cid:12)(cid:12) = r

(cid:19)2

=

      

e   x2dx

   (cid:82)
(cid:12)(cid:12)(cid:12)(cid:12) cos        r sin   
2  (cid:90)
   (cid:90)

r cos   

sin   

   (x2+y2)dxdy =
e

e   r2j (r,   ) drd  

2  (cid:90)
(cid:105)   

0

e   r2rdr

(cid:104) e   r2

2

0

0

0

d  

=   

   (cid:82)

      

e   x2dx =

to verify

      
   

  , consider

   x
     
   y
     

   r
   y
   r

(cid:12)(cid:12)(cid:12)(cid:12)    x
   (cid:90)
   (cid:90)
   (cid:90)

      

      

j (r,   ) =

thus,

          (cid:90)

      

e   x2dx

      2

=

=

0

=    2  

   (cid:82)

      

thus,

e   x2dx =

   

  .

411

r cos    and y = r sin   . the jacobian of this transformation of variables is

the integral(cid:82)    
1 xrdx converges if r        1       and diverges if r        1 +  
(cid:90)    
r = 1      
1 = 1
i1+  converges since(cid:80)   
1 =        r = 1 +  

(cid:12)(cid:12)   
(cid:40)     1
(cid:12)(cid:12)(cid:12)(cid:12)   
  x (cid:12)(cid:12)   
i <(cid:82)    
1 xrdx and(cid:80)   

thus(cid:80)   
(cid:82)    

xrdx =

r + 1

    1

xr+1

1
x 

i=2

i=1

i=1

=

1

1

1

1

1

 

 

1 xrdx.

i1     diverges since(cid:80)   

1

i=1

1
i >

miscellaneous integrals(cid:90) 1
the binomial coe   cient(cid:0)n

for de   nition of the gamma function see section 12.4 binomial coe   cients

(n   k)!k! is the number of ways of choosing k items from n.
the number of ways of choosing d + 1 items from n + 1 items equals the number of ways
of choosing the d + 1 items from the    rst n items plus the number of ways of choosing d
of the items from the    rst n items with the other item being the last of the n + 1 items.

k

x     1(1     x)     1dx =

  (  )  (  )
  (   +   )

x=0

(cid:1) = n!
(cid:19)
(cid:18)n

(cid:18) n

(cid:19)

(cid:18)n + 1

(cid:19)

.

=

d + 1

d + 1

+

d

the observation that the number of ways of choosing k items from 2n equals the
number of ways of choosing i items from the    rst n and choosing k     i items from the
second n summed over all i, 0     i     k yields the identity

(cid:18)n

k(cid:88)

(cid:19)(cid:18) n

(cid:18)2n
(cid:19)
setting k = n in the above formula and observing that(cid:0)n
(cid:19)

k     i

(cid:19)

i=0

=

k

i

i

.

(cid:1) =(cid:0) n

n   i

(cid:1) yields

(cid:0)n

i

(cid:1)(cid:0) m

k   i

k(cid:80)

i=0

more generally

(cid:1) =(cid:0)n+m

k

(cid:19)2

(cid:18)2n

(cid:18)n
n(cid:88)
(cid:1) by a similar derivation.

i=0

=

n

i

.

412

12.4 useful inequalities
1 + x     ex for all real x.

one often establishes an inequality such as 1 + x     ex by showing that the dif-
ference of the two sides, namely ex     (1 + x), is always positive. this can be done
by taking derivatives. the    rst and second derivatives are ex     1 and ex. since ex
is always positive, ex     1 is monotonic and ex     (1 + x) is convex. since ex     1 is
monotonic, it can be zero only once and is zero at x = 0. thus, ex     (1 + x) takes
on its minimum at x = 0 where it is zero establishing the inequality.

(1     x)n     1     nx for 0     x     1

let g(x) = (1     x)n     (1     nx). we establish g(x)     0 for x in [0, 1] by taking
the derivative.

g(cid:48)(x) =    n(1     x)n   1 + n = n(cid:0)1     (1     x)n   1(cid:1)     0

for 0     x     1. thus, g takes on its minimum for x in [0, 1] at x = 0 where g(0) = 0
proving the inequality.

(x + y)2     2x2 + 2y2

the inequality follows from (x + y)2 + (x     y)2 = 2x2 + 2y2.

lemma 12.1 for any nonnegative reals a1, a2, . . . , an and any        [0, 1], (cid:0)(cid:80)n
(cid:80)n

i=1 ai

i=1 a  
i .

(cid:1)      

proof: we will see that we can reduce the proof of the lemma to the case when only one
of the ai is nonzero and the rest are zero. to this end, suppose a1 and a2 are both positive
and without loss of generality, assume a1     a2. add an in   nitesimal positive amount  
to a1 and subtract the same amount from a2. this does not alter the left hand side. we
claim it does not increase the right hand side. to see this, note that

2 =   (a     1

1     a     1

(a1 +  )   + (a2      )       a  
1     a     1

1     a  
2     0, proving the claim. now by repeating this
and since        1     0, we have a     1
process, we can make a2 = 0 (at that time a1 will equal the sum of the original a1 and
a2). now repeating on all pairs of ai, we can make all but one of them zero and in the
process, we have left the left hand side the same, but have not increased the right hand
side. so it su   ces to prove the inequality at the end which clearly holds. this method of
proof is called the variational method.

)  + o( 2),

2

413

1 + x     ex for all real x
(1     x)n     1     nx for 0     x     1
(x + y)2     2x2 + 2y2

triangle inequality

|x + y|     |x| + |y|.

cauchy-schwartz inequality

|x||y|     xt y

young   s inequality for positive real numbers p and q where 1
positive reals x and y,

p + 1

q = 1 and

xy     1
p

xp +

yq.

1
q

h  older   s inequality for positive real numbers p and q with 1

p + 1

q = 1,

n(cid:88)

i=1

|xiyi|    

|xi|p

(cid:32) n(cid:88)
(cid:33)

i=1

  ixi

(cid:32) n(cid:88)

f

(cid:33)1/p(cid:32) n(cid:88)

|yi|q

i=1

(cid:33)1/q

.

    n(cid:88)

  if (xi),

jensen   s inequality for a convex function f , for   1 + . . . +   n = 1,   i     0,

i=1

i=1

the triangle inequality
for any two vectors x and y, |x + y|     |x| + |y|. this can be seen by viewing x
and y as two sides of a triangle; equality holds i    the angle    between x and y is 180
degrees. formally, by the law of cosines we have |x + y|2 = |x|2 + |y|2     2|x||y| cos(  )    
|x|2 + |y|2 + 2|x||y| = (|x| + |y|)2. the inequality follows by taking square roots.

stirling approximation

22n

1   
  n

(cid:19)

(cid:16)n

(cid:17)n    

2  n

n!    =
   
2  n

e

nn
en < n! <

(cid:16) n

(cid:17)n    

1.4

e

(cid:19)    =

(cid:18)2n

n

1 +

1

12n     1

(cid:16) n

(cid:17)n    

e

n.

(cid:18)

2  n

nn
en

   

n     n!     e

414

we prove the inequalities, except for constant factors. namely, we prove that

write ln(n!) = ln 1 + ln 2 +        + ln n. this sum is approximately (cid:82) n
inde   nite integral (cid:82) ln x dx = (x ln x     x) gives an approximation, but without the

   
x=1 ln x dx. the
   
n
n, di   erentiate twice and note that ln x is a concave function. this

term. to get the
means that for any positive x0,

ln x0 + ln(x0 + 1)

2

   

ln x dx,

(cid:90) x0+1

x=x0

since for x     [x0, x0 + 1], the curve ln x is always above the spline joining (x0, ln x0) and
(x0 + 1, ln(x0 + 1)). thus,

ln(n!) =
   

ln 1
2

(cid:90) n

x=1

+

ln 1 + ln 2

2

+

ln 2 + ln 3

+        +

2

ln x dx +

ln n

= [x ln x     x]n

1 +

ln n

2

= n ln n     n + 1 +

2
ln n

.

2

ln(n     1) + ln n

2

+

ln n

2

ne. for the lower bound on n!, start with the fact that for any

thus, n!     nne   n   
x0     1/2 and any real   

(cid:90) x0+.5

ln x dx.

x=x0   0.5

ln x0     1
2

(ln(x0 +   ) + ln(x0       ))

implies

thus,

ln(n!) = ln 2 + ln 3 +        + ln n    

ln x0    

(cid:90) n+.5

x=1.5
from which one can derive a lower bound with a calculation.

ln x dx,

stirling approximation for the binomial coe   cient

(cid:18)n

(cid:19)

k

   (cid:16) en

(cid:17)k

k

using the stirling approximation for k!,

(cid:18)n

(cid:19)

=

n!

(n     k)!k!

    nk
k!

   =

(cid:16) en

(cid:17)k

k

.

the gamma function

k

for a > 0

   (a) =

  (cid:0) 1

2

(cid:1) =

   (cid:90)

xa   1e   xdx

0

   

  ,    (1) =    (2) = 1, and for n     2,    (a) = (a     1)   (a     1) .

415

to prove    (a) = (a     1)   (a     1) use integration by parts.

f (x) g(cid:48) (x) dx = f (x) g (x)    

f(cid:48) (x) g (x) dx

write   (a) =(cid:82)    

x=0 f (x)g(cid:48)(x) dx, where, f (x) = xa   1 and g(cid:48)(x) = e   x. thus,

  (a) =

xa   1e   xdx = [f (x)g(x)]   

x=0 +

(a     1)xa   2e   x dx

= lim

x       xa   1e   x + (a     1)  (a     1) = (a     1)  (a     1),

(cid:90)

(cid:90)    

x=0

(cid:90)

   (cid:90)

0

as claimed.

cauchy-schwartz inequality(cid:32) n(cid:88)

(cid:33)(cid:32) n(cid:88)

(cid:33)

y2
i

x2
i

   

(cid:32) n(cid:88)

xiyi

(cid:33)2

i=1

i=1

i=1

in vector form, |x||y|     xt y, the inequality states that the dot product of two vectors
is at most the product of their lengths. the cauchy-schwartz inequality is a special case
of h  older   s inequality with p = q = 2.

young   s inequality

for positive real numbers p and q where 1

q = 1 and positive reals x and y,

p + 1
yq     xy.

1
p

xp +

1
q

the left hand side of young   s inequality, 1
q yq, is a convex combination of xp and yq
since 1
q sum to 1. ln(x) is a concave function for x > 0 and so the ln of the convex
combination of the two elements is greater than or equal to the convex combination of
the ln of the two elements

p and 1

p xp + 1

ln(

1
p

xp +

1
q

yp)     1
p

ln(xp) +

1
q

ln(yq) = ln(xy).

since for x     0, ln x is a monotone increasing function, 1

p xp + 1

q yq     xy..

h  older   s inequality

416

p + 1

q = 1,

n(cid:88)

|xiyi|    

(cid:33)1/q

for positive real numbers p and q with 1

(cid:32) n(cid:88)
i = yi / ((cid:80)n
i = xi / ((cid:80)n
i does not change the inequality. now(cid:80)n
let x(cid:48)
(cid:80)n
i and yi by
i|q = 1, so it su   ces to prove
i=1 |y(cid:48)
y(cid:48)
|y(cid:48)
i|q
i|     |x(cid:48)
i|p
i|     1. apply young   s inequality to get |x(cid:48)
i=1 |x(cid:48)
iy(cid:48)
iy(cid:48)
q . summing over i, the
p +
q = 1    nishing the proof.

(cid:33)1/p(cid:32) n(cid:88)
i|p =(cid:80)n

i=1 |yi|q)1/q. replacing xi by x(cid:48)

i=1 |xi|p)1/p and y(cid:48)

right hand side sums to 1

i=1 |x(cid:48)

|xi|p

p + 1

|yi|q

i=1

i=1

i=1

.

for a1, a2, . . . , an real and k a positive integer,

(a1 + a2 +        + an)k     nk   1(|a1|k + |a2|k +        + |an|k).

using h  older   s inequality with p = k and q = k/(k     1),

|a1 + a2 +        + an|     |a1    1| + |a2    1| +        + |an    1|

(cid:32) n(cid:88)

(cid:33)1/k

   

|ai|k

(1 + 1 +        + 1)(k   1)/k ,

i=1

from which the current inequality follows.

arithmetic and geometric means

the arithmetic mean of a set of nonnegative reals is at least their geometric mean.

for a1, a2, . . . , an > 0,

1
n

   
ai     n
a1a2        an.

n(cid:88)

i=1

assume that a1     a2     . . .     an. we reduce the proof to the case when all the ai are equal
using the variational method. in this case the inequality holds with equality. suppose
a1 > a2. let    be a positive in   nitesimal. add    to a2 and subtract    from a1 to get closer
to the case when they are equal. the left hand side 1
n

i=1 ai does not change.
(a1       )(a2 +   )a3a4        an = a1a2        an +   (a1     a2)a3a4        an + o(  2)

(cid:80)n

   
a1a2        an. so if the inequality
for small enough    > 0. thus, the change has increased n
holds after the change, it must hold before. by continuing this process, one can make all
the ai equal.

> a1a2        an

approximating sums by integrals

417

n+1(cid:82)

f (x)dx     n(cid:80)

x=m

i=m

f (i)     n(cid:82)

x=m   1

f (x)dx

m     1 m

n n + 1

figure 12.1: approximating sums by integrals

for monotonic decreasing f (x),

n(cid:90)

x=m   1

f (i)    

f (x)dx.

9 +        + 1

4 + 1

1
x2 dx

n2     n(cid:82)

x=1

n+1(cid:90)

f (x)dx     n(cid:88)
x2 dx     n(cid:88)

i2 = 1

x=m

i=m

1

1

i=2

1

i2     2     1
n.

n+1(cid:90)
n+1     n(cid:80)

x=2

i=1

see fig. 12.1. thus,

and hence 3

2     1

jensen   s inequality

for a convex function f ,

(cid:18)1

2

f

(cid:19)

(x1 + x2)

more generally for any convex function f ,

(f (x1) + f (x2)) .

    1
2

(cid:33)

(cid:32) n(cid:88)

i=1

f

  ixi

    n(cid:88)

i=1

  if (xi),

n(cid:80)

i=1

where 0       i     1 and
random variable x,

  i = 1. from this, it follows that for any convex function f and

e (f (x))     f (e (x)) .

418

we prove this for a discrete random variable x taking on values a1, a2, . . . with prob(x =
ai) =   i:

(cid:88)

(cid:32)(cid:88)

(cid:33)

e(f (x)) =

  if (ai)     f

  iai

= f (e(x)).

i

i

f (x1)

f (x2)

x1

figure 12.2: for a convex function f , f(cid:0) x1+x2

x2

2

(cid:1)     1

2 (f (x1) + f (x2)) .

example: let f (x) = xk for k an even positive integer. then, f(cid:48)(cid:48)(x) = k(k     1)xk   2
which since k     2 is even is nonnegative for all x implying that f is convex. thus,

e (x)     k(cid:112)e (xk),

k is a monotone function of t, t > 0. it is easy to see that this inequality does not

since t 1
necessarily hold when k is odd; indeed for odd k, xk is not a convex function.

tails of gaussians

for bounding the tails of gaussian densities, the following inequality is useful. the

proof uses a technique useful in many contexts. for t > 0,

(cid:90)    
x=t e   x2 dx    (cid:82)    

x=t

in proof,    rst write: (cid:82)    

e   x2 dx     e   t2

2t

.

t e   x2 dx, using the fact that x     t in the range of
integration. the latter expression is integrable in closed form since d(e   x2) = (   2x)e   x2
yielding the claimed bound.

x=t

x

a similar technique yields an upper bound on

(cid:90) 1

(1     x2)   dx,

x=  

419

for        [0, 1] and    > 0. just use (1    x2)       x

   (1    x2)   over the range and integrate the

last expression.(cid:90) 1

(cid:90) 1

x=  

(1     x2)  dx    

=

x=  

(1       2)  +1
2  (   + 1)

(1     x2)  dx =

x
  

   1

2  (   + 1)

(1     x2)  +1

(cid:12)(cid:12)(cid:12)(cid:12)1

x=  

12.5 id203

consider an experiment such as    ipping a coin whose outcome is determined by chance.
to talk about the outcome of a particular experiment, we introduce the notion of a ran-
dom variable whose value is the outcome of the experiment. the set of possible outcomes
is called the sample space. if the sample space is    nite, we can assign a id203 of
occurrence to each outcome. in some situations where the sample space is in   nite, we can
1
assign a id203 of occurrence. the id203 p (i) = 6
i2 for i an integer greater
  2
than or equal to one is such an example. the function assigning the probabilities is called
a id203 distribution function.

in many situations, a id203 distribution function does not exist. for example,
for the uniform id203 on the interval [0,1], the id203 of any speci   c value is
zero. what we can do is de   ne a id203 density function p(x) such that

prob(a < x < b) =

p(x)dx

b(cid:90)

a

(cid:90) a

      

p(x)dx

if x is a continuous random variable for which a density function exists, then the cumu-
lative distribution function f (a) is de   ned by

which gives the id203 that x     a.

f (a) =

12.5.1 sample space, events, and independence

there may be more than one relevant random variable in a situation. for example, if
one tosses n coins, there are n random variables, x1, x2, . . . , xn, taking on values 0 and 1,
a 1 for heads and a 0 for tails. the set of possible outcomes, the sample space, is {0, 1}n.
an event is a subset of the sample space. the event of an odd number of heads, consists
of all elements of {0, 1}n with an odd number of 1   s.

420

let a and b be two events. the joint occurrence of the two events is denoted by
(a   b). the id155 of event a given that event b has occurred is denoted
by prob(a|b)and is given by

prob(a|b) =

prob(a     b)

prob(b)

.

events a and b are independent if the occurrence of one event has no in   uence on the
id203 of the other. that is, prob(a|b) = prob(a) or equivalently, prob(a     b) =
prob(a)prob(b). two random variables x and y are independent if for every possible set
a of values for x and every possible set b of values for y, the events x in a and y in b
are independent.

a collection of n random variables x1, x2, . . . , xn is mutually independent if for all

possible sets a1, a2, . . . , an of values of x1, x2, . . . , xn,
prob(x1     a1, x2     a2, . . . , xn     an) = prob(x1     a1)prob(x2     a2)       prob(xn     an).

if the random variables are discrete, it would su   ce to say that for any real numbers
a1, a2, . . . , an
prob(x1 = a1, x2 = a2, . . . , xn = an) = prob(x1 = a1)prob(x2 = a2)       prob(xn = an).
random variables x1, x2, . . . , xn are pairwise independent if for any ai and aj, i (cid:54)= j,
prob(xi = ai, xj = aj) = prob(xi = ai)prob(xj = aj). mutual independence is much
stronger than requiring that the variables are pairwise independent. consider the exam-
ple of 2-universal hash functions discussed in chapter ??.

if (x, y) is a random vector and one normalizes it to a unit vector

the coordinates are no longer independent since knowing the value of one coordinate
uniquely determines the absolute value of the other.

12.5.2 linearity of expectation

value, e(x), of a random variable x is e(x) =(cid:80)
   (cid:82)

x

an important concept is that of the expectation of a random variable. the expected
xp(x) in the discrete case and e(x) =

xp(x)dx in the continuous case. the expectation of a sum of random variables is equal

      
to the sum of their expectations. the linearity of expectation follows directly from the
de   nition and does not require independence.

421

(cid:18)

x   

x2+y2

,

y   

x2+y2

(cid:19)

12.5.3 union bound

by boole   s formula.

prob(a1     a2           an) =

let a1, a2, . . . , an be events. the actual id203 of the union of events is given

often we only need an upper bound on the id203 of the union and use

prob(ai     aj) +

prob(ai     aj     ak)         

prob(ai)   (cid:88)
n(cid:88)
prob(a1     a2            an)     n(cid:88)

i=1

ij

(cid:88)

ijk

prob(ai)

this upper bound is called the union bound.

12.5.4 indicator variables

i=1

a useful tool is that of an indicator variable that takes on value 0 or 1 to indicate
whether some quantity is present or not. the indicator variable is useful in determining
the expected size of a subset. given a random subset of the integers {1, 2, . . . , n}, the
expected size of the subset is the expected value of x1 + x2 +        + xn where xi is the
indicator variable that takes on value 1 if i is in the subset.

example: consider a random permutation of n integers. de   ne the indicator function
xi = 1 if the ith integer in the permutation is i. the expected number of    xed points is
given by

(cid:32) n(cid:88)

(cid:33)

n(cid:88)

e

xi

=

i=1

i=1

e(xi) = n

1
n

= 1.

note that the xi are not independent. but, linearity of expectation still applies.

example: consider the expected number of vertices of degree d in a random graph
g(n, p). the number of vertices of degree d is the sum of n indicator random variables, one
for each vertex, with value one if the vertex has degree d. the expectation is the sum of the
expectations of the n indicator random variables and this is just n times the expectation

of one of them. thus, the expected number of degree d vertices is n(cid:0)n

(cid:1)pd(1     p)n   d.

d

12.5.5 variance

in addition to the expected value of a random variable, another important parameter
is the variance. the variance of a random variable x, denoted var(x) or often   2(x) is
e (x     e (x))2 and measures how close to the expected value the random variable is likely
to be. the standard deviation    is the square root of the variance. the units of    are the
same as those of x.

422

by linearity of expectation

  2 = e (x     e (x))2 = e(x2)     2e(x)e(x) + e2(x) = e(cid:0)x2(cid:1)     e2 (x) .

12.5.6 variance of the sum of independent random variables

in general, the variance of the sum is not equal to the sum of the variances. however,

if x and y are independent, then e (xy) = e (x) e (y) and

var(x + y) = var (x) + var (y) .

to see this

var(x + y) = e(cid:0)(x + y)2(cid:1)     e2(x + y)

= e(x2) + 2e(xy) + e(y2)     e2(x)     2e(x)e(y)     e2(y).

from independence, 2e(xy)     2e(x)e(y) = 0 and

var(x + y) = e(x2)     e2(x) + e(y2)     e2(y)

= var(x) + var(y).

more generally, if x1, x2, . . . , xn are pairwise independent random variables, then

var(x1 + x2 +        + xn) = var(x1) + var(x2) +        + var(xn).

for the variance of the sum to be the sum of the variances only requires pairwise inde-
pendence not full independence.

12.5.7 median

one often calculates the average value of a random variable to get a feeling for the
magnitude of the variable. this is reasonable when the id203 distribution of the
variable is gaussian, or has a small variance. however, if there are outliers, then the
average may be distorted by outliers. an alternative to calculating the expected value is
to calculate the median, the value for which half of the id203 is above and half is
below.

12.5.8 the central limit theorem

let s = x1 + x2 +        + xn be a sum of n independent random variables where each xi

has id203 distribution

the expected value of each xi is 1/2 with variance

xi =

(cid:26) 0 with id203 0.5
(cid:19)2 1
(cid:19)2 1
(cid:18)1

1 with id203 0.5

(cid:18)1

    0

2

    1

+

2

2

2

.

=

1
4

.

  2
i =

423

the expected value of s is n/2 and since the variables are independent, the variance of
the sum is the sum of the variances and hence is n/4. how concentrated s is around its
   
n
mean depends on the standard deviation of s which is
2 . for n equal 100 the expected
value of s is 50 with a standard deviation of 5 which is 10% of the mean. for n = 10, 000
the expected value of s is 5,000 with a standard deviation of 50 which is 1% of the
mean. note that as n increases, the standard deviation increases, but the ratio of the
standard deviation to the mean goes to zero. more generally, if xi are independent and
identically distributed, each with standard deviation   , then the standard deviation of
x1 + x2 +        + xn is
has standard deviation   . the central limit
theorem makes a stronger assertion that in fact x1+x2+      +xn
has gaussian distribution
with standard deviation   .

n  . so, x1+x2+      +xn

   

   

   

n

n

theorem 12.2 suppose x1, x2, . . . , xn is a sequence of identically distributed independent
random variables, each with mean    and variance   2. the distribution of the random
variable

(x1 + x2 +        + xn     n  )

1   
n

converges to the distribution of the gaussian with mean 0 and variance   2.

12.5.9 id203 distributions

the gaussian or normal distribution

the normal distribution is

1   
2    

    1
e

2

(x   m)2

  2

where m is the mean and   2 is the variance. the coe   cient
makes the integral of
the distribution be one. if we measure distance in units of the standard deviation    from
the mean, then

2    

1   

  (x) =

1   
2  

e    1

2 x2.

standard tables give values of the integral

t(cid:90)

  (x)dx

0

and from these values one can compute id203 integrals for a normal distribution
with mean m and variance   2.

general gaussians

so far we have seen spherical gaussian densities in rd. the word spherical indicates
that the level curves of the density are spheres. if a random vector y in rd has a spherical

424

gaussian density with zero mean, then yi and yj, i (cid:54)= j, are independent. however, in
many situations the variables are correlated. to model these gaussians, level curves that
are ellipsoids rather than spheres are used.

for a random vector x, the covariance of xi and xj is e((xi       i)(xj       j)). we list
the covariances in a matrix called the covariance matrix, denoted   .47 since x and    are
column vectors, (x       )(x       )t is a d    d matrix. expectation of a matrix or vector
means componentwise expectation.

   = e(cid:0)(x       )(x       )t(cid:1).

the general gaussian density with mean    and positive de   nite covariance matrix    is

1(cid:112)(2  )d det(  )

f (x) =

exp

(x       )t      1(x       )

   1
2

(cid:18)

(cid:19)

.

to compute the covariance matrix of the gaussian, substitute y =      1/2(x       ). noting
that a positive de   nite symmetric matrix has a square root:
e((x       )(x       )t = e(  1/2yyt   1/2)

=   1/2(cid:0)e(yyt )(cid:1)   1/2 =   .

the density of y is the unit variance, zero mean gaussian, thus e(yyt ) = i.

bernoulli trials and the binomial distribution

a bernoulli trial has two possible outcomes, called success or failure, with probabilities
p and 1     p, respectively. if there are n independent bernoulli trials, the id203 of
exactly k successes is given by the binomial distribution

(cid:18)n
(cid:19)

k

b (n, p) =

pk(1     p)n   k

the mean and variance of the binomial distribution b(n, p) are np and np(1    p), respec-
tively. the mean of the binomial distribution is np, by linearity of expectations. the
variance is np(1     p) since the variance of a sum of independent random variables is the
sum of their variances.

let x1 be the number of successes in n1 trials and let x2 be the number of successes
in n2 trials. the id203 distribution of the sum of the successes, x1 + x2, is the same
as the distribution of x1 + x2 successes in n1 + n2 trials. thus, b (n1, p) + b (n2, p) =
b (n1 + n2, p).

47   is the standard notation for the covariance matrix. we will use it sparingly so as not to confuse

with the summation sign.

425

example, in g(cid:0)n, 1

when p is a constant, the expected degree of vertices in g (n, p) increases with n. for

(cid:1), the expected degree of a vertex is (n     1)/2. in many applications,

we will be concerned with g (n, p) where p = d/n, for d a constant; i.e., graphs whose
expected degree is a constant d independent of n. holding d = np constant as n goes to
in   nity, the binomial distribution

2

prob (k) =

approaches the poisson distribution

(cid:18)n
(cid:19)

k

pk (1     p)n   k

prob(k) =

to see this, assume k = o(n) and use the approximations n     k    = n, (cid:0)n
(cid:0)1     1

(cid:1)n   k    = e   1 to approximate the binomial distribution by

k!

k

(np)k

e   np =

e   d.

dk
k!

n

(cid:1)    = nk

k! , and

(cid:18)n
(cid:19)

(cid:18) d

(cid:19)k

n

lim
n      

k

pk(1     p)n   k =

nk
k!

(1     d
n

)n =

e   d.

dk
k!

note that for p = d
n, where d is a constant independent of n, the id203 of the bi-
nomial distribution falls o    rapidly for k > d, and is essentially zero for all but some
   nite number of values of k. this justi   es the k = o(n) assumption. thus, the poisson
distribution is a good approximation.

poisson distribution

the poisson distribution describes the id203 of k events happening in a unit of
time when the average rate per unit of time is   . divide the unit of time into n segments.
when n is large enough, each segment is su   ciently small so that the id203 of two
events happening in the same segment is negligible. the poisson distribution gives the
id203 of k events happening in a unit of time and can be derived from the binomial
distribution by taking the limit as n        .

(cid:18)n
(cid:19)(cid:18)   
(cid:19)k(cid:18)
(cid:18)   

k

(cid:19)k(cid:18)

n
1       
n

(cid:19)n   k
(cid:19)   k

(cid:19)n(cid:18)

1       
n
1       
n

let p =   

n. then

prob(k successes in a unit of time) = lim
n      
n (n     1)       (n     k + 1)

= lim
n      

= lim
n      

e     

  k
k!

k!

n

426

in the limit as n goes to in   nity the binomial distribution p (k) =(cid:0)n

(cid:1)pk (1     p)n   k be-

comes the poisson distribution p (k) = e        k
k! . the mean and the variance of the poisson
distribution have value   . if x and y are both poisson random variables from distributions
with means   1 and   2 respectively, then x + y is poisson with mean m1 + m2. for large
n and small p the binomial distribution can be approximated with the poisson distribution.
the binomial distribution with mean np and variance np(1     p) can be approximated
by the normal distribution with mean np and variance np(1   p). the central limit theorem
tells us that there is such an approximation in the limit. the approximation is good if
both np and n(1     p) are greater than 10 provided k is not extreme. thus,

k

this approximation is excellent provided k is   (n). the poisson approximation

is o    for central values and tail values even for p = 1/2. the approximation

(cid:18)n

k

2

2

(cid:19)k(cid:18)1
(cid:19)(cid:18)1
(cid:18)n
(cid:19)
(cid:19)

k

(cid:18) n

k

(cid:19)n   k    =

1(cid:112)  n/2

    (n/2   k)2

1
2 n

e

.

pk (1     p)k    = e   np (np)k

k!

pk (1     p)n   k    =

    (pn   k)2

pn

1   
  pn

e

is good for p = 1/2 but is o    for other values of p.

generation of random numbers according to a given id203 distribution

suppose one wanted to generate a random variable with id203 density p(x) where
p(x) is continuous. let p (x) be the cumulative distribution function for x and let u be
a random variable with uniform id203 density over the interval [0,1]. then the ran-
dom variable x = p    1 (u) has id203 density p(x).

x(cid:90)

example: for a cauchy density function the cumulative distribution function is

1

1
  

1
2

1
  

tan   1 (x) .

p (x) =

setting u = p (x) and solving for x yields x = tan(cid:0)  (cid:0)u     1
and calculate x = tan(cid:0)  (cid:0)u     1

(cid:1)(cid:1). thus, to generate a
(cid:1)(cid:1) . the value of x varies from        to     with p (0) = 1/2.

random number x     0 using the cauchy distribution, generate u, 0     u     1, uniformly

1 + t2 dt =

t=      

+

2

2

for the id203 distribution prob(x = i) = 6
  2
butions prob(x = i) = c 1
variance.

i3 and prob(xi = 2i) = 1

1

i2 e(x) =    . the id203 distri-
4i have    nite expectation but in   nite

427

12.5.10 bayes rule and estimators

bayes rule

bayes rule relates the id155 of a given b to the conditional proba-

bility of b given a.

prob (a|b) =

prob (b|a) prob (a)

prob (b)

suppose one knows the id203 of a and wants to know how this id203 changes
if we know that b has occurred. prob(a) is called the prior id203. the conditional
id203 prob(a|b) is called the posterior id203 because it is the id203 of
a after we know that b has occurred.

the example below illustrates that if a situation is rare, a highly accurate test will

often give the wrong answer.

example: let a be the event that a product is defective and let b be the event that a
test says a product is defective. let prob(b|a) be the id203 that the test says a

product is defective assuming the product is defective and let prob(cid:0)b|   a(cid:1) be the proba-
defective? suppose prob(a) = 0.001, prob(b|a) = 0.99, and prob(cid:0)b|   a(cid:1) = 0.02. then

what is the id203 prob(a|b) that the product is defective if the test says it is

bility that the test says a product is defective if it is not actually defective.

prob (b) = prob (b|a) prob (a) + prob(cid:0)b|   a(cid:1) prob(cid:0)   a(cid:1)

= 0.99    0.001 + 0.02    0.999
= 0.02087

and

prob (a|b) =

prob (b|a) prob (a)

prob (b)

    0.99    0.001

0.0210

= 0.0471

even though the test fails to detect a defective product only 1% of the time when it
is defective and claims that it is defective when it is not only 2% of the time, the test
is correct only 4.7% of the time when it says a product is defective. this comes about
because of the low frequencies of defective products.

the words prior, a posteriori, and likelihood come from id47.

a posteriori =

prob (a|b) =

likelihood    prior
normalizing constant
prob (b|a) prob (a)

prob (b)

the a posteriori id203 is the id155 of a given b. the likelihood
is the id155 prob(b|a).

428

unbiased estimators

  2. for this distribution, m = x1+x2+      +xn
that e(m) =    and 1
n

consider n samples x1, x2, . . . , xn from a gaussian distribution of mean    and variance
is an unbiased estimator of   , which means
(xi       )2 is an unbiased estimator of   2. however, if    is not
(xi     m)2 is an unbiased estimator of   2.

known and is approximated by m, then 1
n   1

n(cid:80)

i=1

n

n(cid:80)

i=1

id113 id113

suppose the id203 distribution of a random variable x depends on a parameter
r. with slight abuse of notation, since r is a parameter rather than a random variable, we
denote the id203 distribution of x as p (x|r) . this is the likelihood of observing x if
r was in fact the parameter value. the job of the maximum likelihood estimator, id113,
is to    nd the best r after observing values of the random variable x. the likelihood of r
being the parameter value given that we have observed x is denoted l(r|x). this is again
not a id203 since r is a parameter, not a random variable. however, if we were to
apply bayes    rule as if this was a id155, we get

l(r|x) =

prob(x|r)prob(r)

prob(x)

.

now, assume prob(r) is the same for all r. the denominator prob(x) is the absolute
id203 of observing x and is independent of r. so to maximize l(r|x), we just maxi-
mize prob(x|r). in some situations, one has a prior guess as to the distribution prob(r).
this is then called the    prior    and in that case, we call prob(x|r) the posterior which we
try to maximize.

example: consider    ipping a coin 100 times. suppose 62 heads and 38 tails occur.
what is the maximum likelihood value of the id203 of the coin to come down heads
when the coin is    ipped? in this case, it is r = 0.62. the id203 that we get 62 heads
if the unknown id203 of heads in one trial is r is

prob (62 heads|r) =

r62(1     r)38.

(cid:18)100
(cid:19)

62

this quantity is maximized when r = 0.62. to see this take the derivative with respect
to r of r62(1    r)38 the derivative is zero at r = 0.62 and the second derivative is negative
indicating a maximum. thus, r = 0.62 is the maximum likelihood estimator of the
id203 of heads in a trial.

429

12.6 bounds on tail id203

12.6.1 cherno    bounds

markov   s inequality bounds the id203 that a nonnegative random variable exceeds
a value a.

or

p(x     a)     e(x)
a

p(cid:0)x     ae(x)(cid:1)     1

.

a

if one also knows the variance,   2, then using chebyshev   s inequality one can bound the
id203 that a random variable di   ers from its expected value by more than a standard
deviations. let m = e(x). then chebyshev   s inequality states that

p(|x     m|     a  )     1
a2

if a random variable s is the sum of n independent random variables x1, x2, . . . , xn of
   nite variance, then better bounds are possible. here we focus on the case where the
n independent variables are binomial. in the next section we consider the more general
case where we have independent random variables from any distribution that has a    nite
variance.

let x1, x2, . . . , xn be independent random variables where

(cid:26) 0 prob 1     p

1 prob

p

.

xi =

n(cid:80)

consider the sum s =

xi. here the expected value of each xi is p and by linearity

i=1

of expectation, the expected value of the sum is m=np. cherno    bounds bound the
id203 that the sum s exceeds (1 +   ) m or is less than (1       ) m. we state these
bounds as theorems 12.3 and 12.4 below and give their proofs.

(cid:16)
theorem 12.3 for any    > 0, prob(cid:0)s > (1 +   )m(cid:1) <
theorem 12.4 let 0 <        1, then prob(cid:0)s < (1       )m(cid:1) <

e  

(cid:16)

(cid:17)m

< e      2m

2

.

(cid:17)m

(1+  )(1+  )

e     

(1+  )(1+  )

proof (theorem 12.3): for any    > 0, the function e  x is monotone. thus,

e  x is nonnegative for all x, so we can apply markov   s inequality to get

prob(cid:0)s > (1 +   )m(cid:1) = prob(cid:0)e  s > e  (1+  )m(cid:1) .
prob(cid:0)e  s > e  (1+  )m(cid:1)     e     (1+  )me(cid:0)e  s(cid:1) .

430

since the xi are independent,

(cid:33)
n(cid:89)
e(cid:0)e  xi(cid:1)
(cid:0)p(e       1) + 1(cid:1).

e  xi

i=1

=

  

e

xi

i=1

= e

(cid:33)

(cid:32)
(cid:32) n(cid:89)
e(cid:0)e  s(cid:1) = e
n(cid:80)
n(cid:89)
n(cid:89)
(cid:0)e  p + 1     p(cid:1) =
n(cid:89)
e(cid:0)e  s(cid:1) <

i=1

i=1

i=1

=

ep(e     1).

using the inequality 1 + x < ex with x = p(e       1) yields

thus, for all    > 0

i=1

prob(cid:0)s > (1 +   )m(cid:1)     prob(cid:0)e  s > e  (1+  )m(cid:1)
    e     (1+  )me(cid:0)e  s(cid:1)
n(cid:89)

    e     (1+  )m

ep(e     1).

i=1

setting    = ln(1 +   )

ep(eln(1+  )   1)

n(cid:89)
prob(cid:0)s > (1 +   )m(cid:1)    (cid:0)e    ln(1+  )(cid:1)(1+  )m
(cid:19)(1+  )m n(cid:89)
(cid:19)(1+  )m
(cid:33)m

(cid:18) 1
(cid:18) 1
(cid:32)

(1 +   )

1 +   

   

   

i=1

i=1

enp  

ep  

   

e  

(1 +   )(1+  )

.

to simplify the bound of theorem 12.3, observe that

(1 +   ) ln (1 +   ) =    +

  2
2

      3
6

+

  4
12

           .

therefore

and hence

(1 +   )(1+  ) = e  +   2

2       3

6 +   4

12          

431

thus, the bound simpli   es to

e  

(1+  )(1+  ) = e      2

2 +   3

6          .

prob(cid:0)s < (1 +   ) m(cid:1)     e      2

2 m+   3

6 m         .

for small    the id203 drops exponentially with   2.

when    is large another simpli   cation is possible. first

prob(cid:0)s > (1 +   ) m(cid:1)    

(cid:32)

e  

(cid:33)m

   

(cid:18) e

(cid:19)(1+  )m

1 +   
if    > 2e     1, substituting 2e     1 for    in the denominator yields

(1 +   )(1+  )

prob(s > (1 +   ) m)     2   (1+  )m.

theorem 12.3 gives a bound on the id203 of the sum being signi   cantly greater
than the mean. we now prove theorem 12.4, bounding the id203 that the sum will
be signi   cantly less than its mean.

proof (theorem 12.4): for any    > 0

prob(cid:0)s < (1       )m(cid:1) = prob(cid:0)     s >    (1       )m(cid:1) = prob(cid:0)e     s > e     (1     )m(cid:1) .

applying markov   s inequality

prob(cid:0)s < (1       )m(cid:1) <

e(e     x)
e     (1     )m <

n(cid:81)

i=1

e(e     xi)
e     (1     )m .

now

thus,

since 1 + x < ex

setting    = ln 1
1     

e(e     xi) = pe      + 1     p = 1 + p(e          1) + 1.

i=1

prob(s < (1       )m) <

n(cid:81)
prob(cid:0)s < (1       )m(cid:1) <
prob(cid:0)s < (1       )m(cid:1) <

.

[1 + p(e          1)]
e     (1     )m
enp(e        1)
e     (1     )m .

enp(1        1)
(1       )(1     )m

(cid:18)

e     

(1       )(1     )

(cid:19)m

.

<

432

but for 0 <        1, (1       )(1     ) > e     +   2

2 . to see this note that

(1       ) ln (1       ) = (1       )

(cid:19)
(cid:19)

+       

  4
+
3
+       

(cid:18)
(cid:18)

          

      3
3

            2
2
           +   2 +

(cid:18)   3

2

  3
2
      3
3

(cid:19)

+

+       

=             2
      3
2
3
  2       2
=       +
2
  3
6

+

=       +
          +

  2
2
  2
2

.

(cid:18)

(cid:19)m

e     

(1       )(1     )

< e    m  2

2

.

it then follows that

prob(cid:0)s < (1       )m(cid:1) <

12.6.2 more general tail bounds

the main purpose of this section is to state the master tail bounds theorem of chapter
2 (with more detail), give a proof of it, and derive the other tail inequalities mentioned
in the table in that chapter. recall that markov   s inequality bounds the tail id203
of a nonnegative random variable x based only on its expectation. for a > 0,

pr(x > a)     e(x)
a

.

as a grows, the bound drops o    as 1/a. given the second moment of x, recall that
chebyshev   s inequality, which does not assume x is a nonnegative random variable, gives
a tail bound falling o    as 1/a2

pr(|x     e(x)|     a)     e

(cid:16)(cid:0)x     e(x)(cid:1)2(cid:17)

a2

.

higher moments yield bounds by applying either of these two theorems. for example,
if r is a nonnegative even integer, then xr is a nonnegative random variable even if x takes
on negative values. applying markov   s inequality to xr,

pr(|x|     a) = pr(xr     ar)     e(xr)
ar

,

433

a bound that falls o    as 1/ar. the larger the r, the greater the rate of fall, but a bound
on e(xr) is needed to apply this technique.

for a random variable x that is the sum of a large number of independent random
variables, x1, x2, . . . , xn, one can derive bounds on e(xr) for high even r. there are many
situations where the sum of a large number of independent random variables arises. for
example, xi may be the amount of a good that the ith consumer buys, the length of the ith
message sent over a network, or the indicator random variable of whether the ith record
in a large database has a certain property. each xi is modeled by a simple id203
distribution. gaussian, exponential id203 density (at any t > 0 is e   t), or binomial
distributions are typically used, in fact, respectively in the three examples here. if the
xi have 0-1 distributions, then the cherno    bounds described in section 12.6.1 can be
used to bound the tails of x = x1 + x2 +        + xn. but exponential and gaussian random
variables are not bounded so the proof technique used in section 12.6.1 does not apply.
however, good bounds on the moments of these two distributions are known. indeed, for
any integer s > 0, the sth moment for the unit variance gaussian and the exponential are
both at most s!.

given bounds on the moments of individual xi the following theorem proves moment
bounds on their sum. we use this theorem to derive tail bounds not only for sums of 0-1
random variables, but also gaussians, exponentials, poisson, etc.

the gold standard for tail bounds is the central limit theorem for independent, iden-
tically distributed random variables x1, x2,       , xn with zero mean and var(xi) =   2 that
   
states as n         the distribution of x = (x1 + x2 +        + xn)/
n tends to the gaus-
   
sian density with zero mean and variance   2. loosely, this says that in the limit, the
tails of x = (x1 + x2 +        + xn)/
n are bounded by that of a gaussian with variance
  2. but this theorem is only in the limit, whereas, we prove a bound that applies for all n.

in the following theorem, x is the sum of n independent, not necessarily identically
distributed, random variables x1, x2, . . . , xn, each of zero mean and variance at most   2.
by the central limit theorem, in the limit the id203 density of x goes to that of
the gaussian with variance at most n  2. in a limit sense, this implies an upper bound
of ce   a2/(2n  2) for the tail id203 pr(|x| > a) for some constant c. the following
theorem assumes bounds on higher moments, and asserts a quantitative upper bound of
3e   a2/(12n  2) on the tail id203, not just in the limit, but for every n. we will apply
this theorem to get tail bounds on sums of gaussian, binomial, and power law distributed
random variables.
theorem 12.5 let x = x1 + x2 +       + xn, where x1, x2, . . . , xn are mutually independent
random variables with zero mean and variance at most   2. suppose a     [0,
2n  2] and

   

434

s     n  2/2 is a positive even integer and |e(xr

i )|       2r!, for r = 3, 4, . . . , s. then,

pr (|x1 + x2 +        xn|     a)    

if further, s     a2/(4n  2), then we also have:

pr (|x1 + x2 +        xn|     a)     3e   a2/(12n  2).

(cid:18)2sn  2

(cid:19)s/2

.

a2

proof: we    rst prove an upper bound on e(xr) for any even positive integer r and then
use markov   s inequality as discussed earlier. expand (x1 + x2 +        + xn)r.

(cid:19)

(cid:88)(cid:18)
(cid:88) r!

r

r1, r2, . . . , rn
r1!r2!       rn!

xr1
1 xr2

2        xrn

n

1 xr2
xr1
2        xrn

n

(x1 + x2 +        + xn)r =

=

(cid:88) r!

where the ri range over all nonnegative integers summing to r. by independence

e(xr) =

r1!r2!       rn!

e(xr1

1 )e(xr2

2 )       e(xrn
n ).

(cid:88)

if in a term, any ri = 1, the term is zero since e(xi) = 0. assume henceforth that
(r1, r2, . . . , rn) runs over sets of nonzero ri summing to r where each nonzero ri is at least
two. there are at most r/2 nonzero ri in each set. since |e(xri

i )|       2ri!,

e(xr)     r!

(r1,r2,...,rn)

  2( number of nonzero ri in set).

collect terms of the summation with t nonzero ri for t = 1, 2, . . . , r/2. there are (cid:0)n
(cid:1)
(cid:0)r   2t+t   1

subsets of {1, 2, . . . , n} of cardinality t. once a subset is    xed as the set of t values of i
with nonzero ri, set each of the ri     2. that is, allocate two to each of the ri and then
allocate the remaining r    2t to the t ri arbitrarily. the number of such allocations is just

(cid:1) =(cid:0)r   t   1

(cid:1). so,

t

t   1

t   1

e(xr)     r!

f (t), where

f (t) =

r/2(cid:88)

t=1

(cid:18)n

(cid:19)(cid:18)r     t     1
(cid:19)

t

t     1

  2t.

thus f (t)     h(t), where h(t) = (n  2)t

t! 2r   t   1. since t     r/2     n  2/4, we have

h(t)
h(t     1)

=

n  2
2t

    2.

so, we get

e(xr) = r!

r/2(cid:88)

t=1

f (t)     r!h(r/2)(1 +

1
2

+

1
4

+        )     r!

(r/2)!

2r/2(n  2)r/2.

435

applying markov inequality,

pr(|x| > a) = pr(|x|r > ar)     r!(n  2)r/22r/2

(r/2)!ar

= g(r)    

(cid:18)2rn  2

(cid:19)r/2

a2

.

this holds for all r     s, r even and applying it with r = s, we get the    rst inequality of
the theorem.

we now prove the second inequality. for even r, g(r)/g(r     2) = 4(r   1)n  2

and so
g(r) decreases as long as r     1     a2/(4n  2). taking r to be the largest even integer
less than or equal to a2/(6n  2), the tail id203 is at most e   r/2, which is at most
e    e   a2/(12n  2)     3    e   a2/(12n  2), proving the theorem.

a2

12.7 applications of the tail bound

cherno    bounds

cherno    bounds deal with sums of bernoulli random variables. here we apply theo-

rem 12.5 to derive these.

theorem 12.6 suppose y1, y2, . . . , yn are independent 0-1 random variables with e(yi) =
p for all i. let y = y1 + y2 +        + yn. then for any c     [0, 1],

prob(cid:0)|y     e(y)|     cnp(cid:1)     3e   npc2/8.

proof: let xi = yi     p. then, e(xi) = 0 and e(x2

i ) = e(y     p)2 = p. for s     3,

|e(xs

i )| = |e(yi     p)s|

= |p(1     p)s + (1     p)(0     p)s|

=(cid:12)(cid:12)p(1     p)(cid:0)(1     p)s   1 + (   p)s   1(cid:1)(cid:12)(cid:12)

    p.

apply theorem 12.5 with a = cnp. noting that a <

   

2 np, completes the proof.

section (12.6.1) contains a di   erent proof that uses a standard method based on

moment-generating functions and gives a better constant in the exponent.

power law distributions

the power law distribution of order k where k is a positive integer is

for x     1.
if a random variable x has this distribution for k     4, then

f (x) =

k     1
xk

   = e(x) =

k     1
k     2

k     1

(k     2)2(k     3)

.

and var(x) =

436

pr (|x     e(x)|       e(x))    

theorem 12.7 suppose x1, x2, . . . , xn are i.i.d, each distributed according to the power
law of order k     4 (with n > 10k2). then, for x = x1 + x2 +        + xn, and any
   
       (1/(2

nk), 1/k2), we have

(cid:19)(k   3)/2

.

(cid:18)

4

  2(k     1)n

(cid:90)    

(y       )s

dy

1

yk

zk   s
  k   s (1     z)s
y2 dy. thus

proof: for integer s, the sth moment of xi     e(xi), namely, e((xi       )s), exists if and
only if s     k     2. for s     k     2,

e((xi       )s) = (k     1)

using the substitution of variable z =   /y

(y       )s

= ys   k(1     z)s =

as y goes from 1 to    , z goes from    to 0, and dz =       

e((xi       )s) =(k     1)

yk

(cid:90)    
(cid:90) 1

1

k     1
  k   s   1

=

0

(y       )s

dy

yk
(1     z)szk   s   2dz +

(cid:90)   

1

k     1
  k   s   1

(1     z)szk   s   2dz.

the    rst integral is just the standard integral of the beta function and its value is s!(k   2   s)!
(k   1)!
to bound the second integral, note that for z     [1,   ], |z     1|     1

.

zk   s   2    (cid:0)1 +(cid:0)1/(k     2)(cid:1)(cid:1)k   s   2     e(k   s   2)/(k   2)     e.
(cid:18) 1

k   2 and

so, |e((xi       )s)|     (k     1)s!(k     2     s)!
now, apply the    rst inequality of theorem 12.5 with s of that theorem set to k     2 or
k     3 whichever is even. note that a =   e(x)        
2n  2 (since        1/k2). the present

e(k     1)
(k     2)s+1     s!var(y)

(k     1)!

k     4

e
3!

+

+

    s!var(x).

(cid:19)

theorem follows by a calculation.

12.8 eigenvalues and eigenvectors

let a be an n  n real matrix. the scalar    is called an eigenvalue of a if there exists a
nonzero vector x satisfying the equation ax =   x. the vector x is called the eigenvector
of a associated with   . the set of all eigenvectors associated with a given eigenvalue form
a subspace as seen from the fact that if ax =   x and ay =   y, then for any scalars c
and d, a(cx + dy) =   (cx + dy). the equation ax =   x has a nontrivial solution only if
det(a       i) = 0. the equation det(a       i) = 0 is called the characteristic equation and
has n not necessarily distinct roots.

matrices a and b are similar if there is an invertible matrix p such that a = p    1bp .

437

theorem 12.8 if a and b are similar, then they have the same eigenvalues.

proof: let a and b be similar matrices. then there exists an invertible matrix p
such that a = p    1bp . for an eigenvector x of a with eigenvalue   , ax =   x, which
implies p    1bp x =   x or b(p x) =   (p x). so, p x is an eigenvector of b with the same
eigenvalue   . since the reverse also holds, the theorem follows.

even though two similar matrices, a and b, have the same eigenvalues, their eigen-

vectors are in general di   erent.

the matrix a is diagonalizable if a is similar to a diagonal matrix.

theorem 12.9 a is diagonalizable if and only if a has n linearly independent eigenvec-
tors.

proof:

(only if ) assume a is diagonalizable. then there exists an invertible matrix p
and a diagonal matrix d such that d = p    1ap . thus, p d = ap . let the diago-
nal elements of d be   1,   2, . . . ,   n and let p1, p2, . . . , pn be the columns of p . then
ap = [ap1, ap2, . . . , apn] and p d = [  1p1,   2p2, . . . ,   npn] . hence api =   ipi. that
is, the   i are the eigenvalues of a and the pi are the corresponding eigenvectors. since p
is invertible, the pi are linearly independent.

(if ) assume that a has n linearly independent eigenvectors p1, p2, . . . , pn with cor-

responding eigenvalues   1,   2, . . . ,   n. then api =   ipi and reversing the above steps

ap = [ap1, ap2, . . . , apn] = [  1p1,   2p2, . . .   npn] = p d.

thus, ap = p d. since the pi are linearly independent, p is invertible and hence a =
p dp    1. thus, a is diagonalizable.

it follows from the proof of the theorem that if a is diagonalizable and has eigenvalue
   with multiplicity k, then there are k linearly independent eigenvectors associated with   .

a matrix p is orthogonal if it is invertible and p    1 = p t . a matrix a is orthogonally
diagonalizable if there exists an orthogonal matrix p such that p    1ap = d is diagonal.
if a is orthogonally diagonalizable, then a = p dp t and ap = p d. thus, the columns
of p are the eigenvectors of a and the diagonal elements of d are the corresponding
eigenvalues.

if p is an orthogonal matrix, then p t ap and a are both representations of the same
linear transformation with respect to di   erent bases. to see this, note that if e1, e2, . . . , en
is the standard basis, then aij is the component of aej along the direction ei, namely,
t aej. thus, a de   nes a linear transformation by specifying the image under the
aij = ei

438

transformation of each basis vector. denote by pj the jth column of p . it is easy to see that
(p t ap )ij is the component of apj along the direction pi, namely, (p t ap )ij = pi
t apj.
since p is orthogonal, the pj form a basis of the space and so p t ap represents the same
linear transformation as a, but in the basis p1, p2, . . . , pn.

another remark is in order. check that

a = p dp t =

n(cid:88)

i=1

diipipi

t .

compare this with the singular value decomposition where

n(cid:88)

a =

  iuivi

t ,

the only di   erence being that ui and vi can be di   erent and indeed if a is not square,
they will certainly be.

i=1

12.8.1 symmetric matrices

for an arbitrary matrix, some of the eigenvalues may be complex. however, for a
symmetric matrix with real entries, all eigenvalues are real. the number of eigenvalues
of a symmetric matrix, counting multiplicities, equals the dimension of the matrix. the
set of eigenvectors associated with a given eigenvalue form a vector space. for a non-
symmetric matrix, the dimension of this space may be less than the multiplicity of the
eigenvalue. thus, a nonsymmetric matrix may not be diagonalizable. however, for a
symmetric matrix the eigenvectors associated with a given eigenvalue form a vector space
of dimension equal to the multiplicity of the eigenvalue. thus, all symmetric matrices are
diagonalizable. the above facts for symmetric matrices are summarized in the following
theorem.

theorem 12.10 (real spectral theorem) let a be a real symmetric matrix. then

1. the eigenvalues,   1,   2, . . . ,   n, are real, as are the components of the corresponding

eigenvectors, v1, v2, . . . , vn.

2. (spectral decomposition) a is orthogonally diagonalizable and indeed

n(cid:88)

a = v dv t =

  ivivi

t ,

where v is the matrix with columns v1, v2, . . . , vn, |vi| = 1 and d is a diagonal
matrix with entries   1,   2, . . . ,   n.

i=1

439

(cid:18)   1 0

(cid:19)

proof: avi =   ivi and vi
pose. then

cavi =   ivi

cvi. here the c superscript means conjugate trans-

  i = vi

cavi = (vi

cavi)cc = (vi

cacvi)c = (vi

cavi)c =   c
i

and hence   i is real.

since   i is real, a nontrivial solution to (a       ii) x = 0 has real components.

let p be a real symmetric matrix such that p v1 = e1 where e1 = (1, 0, 0, . . . , 0)t and

p    1 = p t . we will construct such a p shortly. since av1 =   1v1,

p ap t e1 = p av1 =   p v1 =   1e1.

the condition p ap t e1 =   1e1 plus symmetry implies that p ap t =
where
a(cid:48) is n     1 by n     1 and symmetric. by induction, a(cid:48) is orthogonally diagonalizable. let
q be the orthogonal matrix with qa(cid:48)qt = d(cid:48), a diagonal matrix. q is (n     1)    (n     1).
augment q to an n    n matrix by putting 1 in the (1, 1) position and 0 elsewhere in the
   rst row and column. call the resulting matrix r. r is orthogonal too.

0 a(cid:48)

(cid:19)

(cid:18)   1 0

0 a(cid:48)

r

rt =

(cid:19)

(cid:18)   1 0

0 d(cid:48)

=    rp ap t rt =

(cid:18)   1 0

0 d(cid:48)

(cid:19)

.

since the product of two orthogonal matrices is orthogonal, this    nishes the proof of (2)
except it remains to construct p . for this, take an orthonormal basis of space containing
v1. suppose the basis is {v1, w2, w3, . . .} and v is the matrix with these basis vectors as
its columns. then p = v t will do.

theorem 12.11 (the fundamental theorem of symmetric matrices) a real ma-
trix a is orthogonally diagonalizable if and only if a is symmetric.

proof: (if ) assume a is orthogonally diagonalizable. then there exists p such that
d = p    1ap . since p    1 = p t , we get

a = p dp    1 = p dp t

at = (p dp t )t = p dp t = a

which implies

and hence a is symmetric.
(only if ) already proved.

note that a nonsymmetric matrix may not be diagonalizable, it may have eigenvalues
that are not real, and the number of linearly independent eigenvectors corresponding to
an eigenvalue may be less than its multiplicity. for example, the matrix

       1 1 0

0 1 1
1 0 1

      

440

(cid:18) 1 2

(cid:19)

   
2 , and 1

3

2     i

   
3
2 . the matrix

2 + i

has eigenvalues 2, 1
has characteristic equation
(1       )2 = 0 and thus has eigenvalue 1 with multiplicity 2 but has only one linearly
c (cid:54)= 0.
independent eigenvector associated with the eigenvalue 1, namely x = c

(cid:18) 1

(cid:19)

0 1

0

neither of these situations is possible for a symmetric matrix.

12.8.2 relationship between svd and eigen decomposition
the singular value decomposition exists for any n    d matrix whereas the eigenvalue
decomposition exists only for certain square matrices. for symmetric matrices the de-
compositions are essentially the same.

e and singular value decomposition a = usdsv t

the singular values of a matrix are always positive since they are the sum of squares
of the projection of a row of a matrix onto a singular vector. given a symmetric matrix,
the eigenvalues can be positive or negative. if a is a symmetric matrix with eigenvalue
decomposition a = vedev t
s , what is
the relationship between de and ds, and between ve and vs, and between us and ve?
observe that if a can be expressed as qdqt where q is orthonormal and d is diagonal,
then aq = qd. that is, each column of q is an eigenvector and the elements of d
are the eigenvalues. thus, if the eigenvalues of a are distinct, then q is unique up to
a permutation of columns. if an eigenvalue has multiplicity k, then the space spanned
the k columns is unique.
in the following we will use the term essentially unique to
capture this situation. now aat = usd2
s . by an argument
similar to the one above, us and vs are essentially unique and are the eigenvectors or
negatives of the eigenvectors of a and at . the eigenvalues of aat or at a are the squares
of the eigenvalues of a. if a is not positive semi de   nite and has negative eigenvalues,
then in the singular value decomposition a = usdsvs, some of the left singular vectors
are the negatives of the eigenvectors. let s be a diagonal matrix with   1(cid:48)s on the
diagonal depending on whether the corresponding eigenvalue is positive or negative. then
a = (uss)(sds)vs where uss = ve and sds = de.

s and at a = vsd2

sv t

su t

12.8.3 extremal properties of eigenvalues

in this section we derive a min max characterization of eigenvalues that implies that
the largest eigenvalue of a symmetric matrix a has a value equal to the maximum of
xt ax over all vectors x of unit length. that is, the largest eigenvalue of a equals the
2-norm of a. if a is a real symmetric matrix there exists an orthogonal matrix p that
diagonalizes a. thus

where d is a diagonal matrix with the eigenvalues of a,   1       2                  n, on its
diagonal. rather than working with a, it is easier to work with the diagonal matrix d.
this will be an important technique that will simplify many proofs.

p t ap = d

441

consider maximizing xt ax subject to the conditions

n(cid:80)

i=1

1.

x2
i = 1

2. rt

i x = 0,

1     i     s

where the ri are any set of nonzero vectors. we ask over all possible sets {ri|1     i     s}
of s vectors, what is the minimum value assumed by this maximum.

(xtax) =
theorem 12.12 (min max theorem) for a symmetric matrix a, min
r1,...,rs
  s+1 where the minimum is over all sets {r1, r2, . . . , rs} of s nonzero vectors and the
maximum is over all unit vectors x orthogonal to the s nonzero vectors.

max
ri   x

x

proof: a is orthogonally diagonalizable. let p satisfy p t p = i and p t ap = d, d
diagonal. let y = p t x. then x = p y and

xt ax = yt p t ap y = yt dy =

  iy2
i

i subject to(cid:80) y2

xt ax subject to(cid:80) x2

since there is a one-to-one correspondence between unit vectors x and y, maximizing

n(cid:80)

i = 1 is equivalent to maximizing

  iy2

i = 1. since

i=1
  1       i, 2     i     n, y = (1, 0, . . . , 0) maximizes
  iy2
i at   1. then x = p y is the    rst
column of p and is the    rst eigenvector of a. similarly   n is the minimum value of xt ax
subject to the same conditions.

i=1

now consider maximizing xt ax subject to the conditions

1. (cid:80) x2

i = 1

2. rt

i x = 0

where the ri are any set of nonzero vectors. we ask over all possible choices of s vectors
what is the minimum value assumed by this maximum.

n(cid:88)
n(cid:80)

i=1

min
r1,...,rs

max
rt
i x=0

x

xt ax

as above, we may work with y. the conditions are

1. (cid:80) y2

i = 1

2. qt

i y = 0 where, qt

i = rt

i p

442

consider any choice for the vectors r1, r2, . . . , rs. this gives a corresponding set of qi. the
yi therefore satisfy s linear homogeneous equations. if we add ys+2 = ys+3 =        yn = 0
we have n     1 homogeneous equations in n unknowns y1, . . . , yn. there is at least one

solution that can be normalized so that(cid:80) y2
(cid:88)

i = 1. with this choice of y

yt dy =

i      s+1

  iy2

since coe   cients greater than or equal to s + 1 are zero. thus, for any choice of ri there
will be a y such that

(yt p t ap y)       s+1

max
rt
i y=0

y

and hence

min

r1,r2,...,rs

(yt p t ap y)       s+1.

max
rt
i y=0

y

however, there is a set of s constraints for which the minimum is less than or equal to
  s+1. fix the relations to be yi = 0, 1     i     s. there are s equations in n unknowns
and for any y subject to these relations

n(cid:88)

yt dy =

i       s+1.

  iy2

combining the two inequalities, min max yt dy =   s+1.

s+1

the above theorem tells us that the maximum of xt ax subject to the constraint that
|x|2 = 1 is   1. consider the problem of maximizing xt ax subject to the additional re-
striction that x is orthogonal to the    rst eigenvector. this is equivalent to maximizing
ytp tap y subject to y being orthogonal to (1,0,. . . ,0), i.e. the    rst component of y being
0. this maximum is clearly   2 and occurs for y = (0, 1, 0, . . . , 0). the corresponding x is
the second column of p or the second eigenvector of a.

similarly the maximum of xt ax for p1

t x = p2

t x =        ps

t x = 0 is   s+1 and is

obtained for x = ps+1.

12.8.4 eigenvalues of the sum of two symmetric matrices

the min max theorem is useful in proving many other results. the following theorem
shows how adding a matrix b to a matrix a changes the eigenvalues of a. the theorem
is useful for determining the e   ect of a small perturbation on the eigenvalues of a.
theorem 12.13 let a and b be n    n symmetric matrices. let c=a+b. let   i,   i,
and   i denote the eigenvalues of a, b, and c respectively, where   1       2     . . .   n and
similarly for   i,   i. then   s +   1       s       s +   n.

443

proof: by the min max theorem we have

  s = min

r1,...,rs   1

(xt ax).

max
ri   x

x

suppose r1, r2, . . . , rs   1 attain the minimum in the expression. then using the min max
theorem on c,

(cid:0)xt (a + b)x(cid:1)

max

x   r1,r2,...rs   1

  s    
   
max
x   r1,r2,...rs   1
      s + max

x

(xt ax) +
max
(xt bx)       s +   1.

x   r1,r2,...rs   1

(xt bx)

therefore,   s       s +   1.

an application of the result to a = c + (   b), gives   s       s       n. the eigenvalues
of -b are minus the eigenvalues of b and thus      n is the largest eigenvalue. hence
  s       s +   n and combining inequalities yields   s +   1       s       s +   n.
lemma 12.14 let a and b be n    n symmetric matrices. let c=a+b. let   i,   i,
and   i denote the eigenvalues of a, b, and c respectively, where   1       2     . . .   n and
similarly for   i,   i. then   r+s   1       r +   s.
proof: there is a set of r   1 relations such that over all x satisfying the r   1 relationships

and a set of s     1 relations such that over all x satisfying the s     1 relationships

max(xt ax) =   r.

consider x satisfying all these r + s     2 relations. for any such x

max(xt bx) =   s.

xt cx = xt ax + xt bxx       r +   s

and hence over all the x

max(xt cx)       s +   r
taking the minimum over all sets of r + s     2 relations

  r+s   1 = min max(xt cx)       r +   s

444

12.8.5 norms
a set of vectors {x1, . . . , xn} is orthogonal if xi
t xj = 0 for i (cid:54)= j and is orthonormal if
in addition |xi| = 1 for all i. a matrix a is orthonormal if at a = i. if a is a square
orthonormal matrix, then rows as well as columns are orthogonal. in other words, if a
is square orthonormal, then at is also. in the case of matrices over the complexes, the
concept of an orthonormal matrix is replaced by that of a unitary matrix. a    is the con-
jugate transpose of a if a   
ij is the complex
conjugate of the ijth element of a. a matrix a over the    eld of complex numbers is
unitary if aa    = i.

ij is the ijth entry of a    and   a   

ij =   aji where a   

norms

a norm on rn is a function f : rn     r satisfying the following three axioms:
1. f (x)     0,
2. f (x + y)     f (x) + f (y), and
3. f (  x) = |  |f (x).

a norm on a vector space provides a distance function where

distance(x, y) = norm(x     y).

an important class of norms for vectors is the p-norms de   ned for p > 0 by

|x|p = (|x1|p +        + |xn|p)

1

p .

important special cases are

|x|0 = the number of non zero entries (not quite a norm: fails #3)
|x|1 = |x1| +        + |xn|

|x|2 =(cid:112)|x1|2 +        + |xn|2

|x|    = max|xi|.

lemma 12.15 for any 1     p < q, |x|q     |x|p.

proof:

(cid:88)

|xi|q.

|x|q

q =

nonnegative reals a1, a2, . . . , an and any        (0, 1), we have ((cid:80)n

let ai = |xi|q and    = p/q. using jensen   s inequality (see section 12.4) that for any
i , the

i=1 ai)       (cid:80)n

i=1 a  

i

lemma is proved.

445

there are two important matrix norms, the matrix p-norm

and the frobenius norm

||a||p = max
|x|=1

(cid:107)ax(cid:107)p

||a||f =

(cid:115)(cid:88)
t ai = tr(cid:0)at a(cid:1). a similar argument
f =(cid:80)
f = tr(cid:0)at a(cid:1) = tr(cid:0)aat(cid:1).
f = tr(cid:0)aat(cid:1). thus, (cid:107)a(cid:107)2

a2
ij.

ai

ij

i

let ai be the ith column of a. then (cid:107)a(cid:107)2
on the rows yields (cid:107)a(cid:107)2
if a is symmetric and rank k

||a||2

2     ||a||2

f     k ||a||2
2 .

|abx|. let y be the value of x that achieves the maximum and

12.8.6 important norms and their properties
lemma 12.16 ||ab||2     ||a||2 ||b||2
proof: ||ab||2 = max
|x|=1
let z = by. then

||ab||2 = |aby| = |az| =

(cid:12)(cid:12)(cid:12)(cid:12)a

(cid:12)(cid:12)(cid:12)(cid:12)|z|

z
|z|

(cid:12)(cid:12)(cid:12)a z|z|

(cid:12)(cid:12)(cid:12)     max

|x|=1

but

|ax| = ||a||2 and |z|     max
|x|=1

|bx| = ||b||2. thus ||ab||2     ||a||2 ||b||2.

let q be an orthonormal matrix.
|qx| = |x|.
2 = xt qt qx = xt x = |x|2
2.

lemma 12.17 for all x,
proof: |qx|2
lemma 12.18 ||qa||2 = ||a||2
proof: for all x,

f     ||a||2

f ||b||2

|ax|

max
|x|=1
lemma 12.19 ||ab||2

cauchy-schwartz inequality (cid:13)(cid:13)ai
(cid:80)
(cid:80)
(cid:107)ai(cid:107)2(cid:80)

(cid:107)ai(cid:107)2 (cid:107)bj(cid:107)2 =(cid:80)

i

j

i

j

446

|qx| = |x|. replacing x by ax, |qax| = |ax| and thus max
|x|=1

|qax| =

proof: let ai be the ith column of a and let bj be the jth column of b. by the

f

(cid:13)(cid:13)     (cid:107)ai(cid:107)(cid:107)bj(cid:107). thus ||ab||2

f = (cid:80)

(cid:80)

(cid:12)(cid:12)ai

(cid:12)(cid:12)2    

t bj

i

j

t bj
(cid:107)bj(cid:107)2 = ||a||2

f ||b||2

f

lemma 12.20 ||qa||f = ||a||f
proof: ||qa||2
lemma 12.21 for real, symmetric matrix a with eigenvalues   1       2     . . ., (cid:107)a(cid:107)2
max(  2

f = tr(at qt qa) = tr(at a) = ||a||2
f .

n) and (cid:107)a(cid:107)2

2 +        +   2

1 +   2

1,   2

f =   2

n

2 =

proof: suppose the spectral decomposition of a is p dp t , where p is an orthogo-
nal matrix and d is diagonal. we saw that ||p t a||2 = ||a||2. applying this again,
||p t ap||2 = ||a||2. but, p t ap = d and clearly for a diagonal matrix d, ||d||2 is the
largest absolute value diagonal entry from which the    rst equation follows. the proof of
the second is analogous.

if a is real and symmetric and of rank k then ||a||2

2     ||a||2

f     k ||a||2

2

2     ||a||2

theorem 12.22 ||a||2
proof: it is obvious for diagonal matrices that ||d||2
2. let d =
qtaq where q is orthonormal. the result follows immediately since for q orthonormal,
||qa||2 = ||a||2 and ||qa||f = ||a||f .

f     k ||d||2

f     k ||a||2

2     ||d||2

2

real and symmetric are necessary for some of these theorems. this condition was
needed to express    = qt aq. for example, in theorem 12.22 suppose a is the n    n
matrix

                1 1

1 1
...
...
1 1

                .

a =

0

||a||2 = 2 and ||a||f =

   
2n. but a is rank 2 and ||a||f > 2||a||2 for n > 8.

lemma 12.23 let a be a symmetric matrix. then (cid:107)a(cid:107)2 = max
|x|=1
|ax|. thus,
proof: by de   nition, the 2-norm of a is (cid:107)a(cid:107)2 = max
|x|=1

xt at ax =(cid:112)  2

   

(cid:107)a(cid:107)2 = max
|x|=1

|ax| = max
|x|=1

1 =   1 = max
|x|=1

(cid:12)(cid:12)xt ax(cid:12)(cid:12).
(cid:12)(cid:12)xt ax(cid:12)(cid:12)

the two norm of a matrix a is greater than or equal to the 2-norm of any of its

columns. let au be a column of a.
lemma 12.24 |au|     (cid:107)a(cid:107)2
proof: let eu be the unit vector with a 1 in position u and all other entries zero. note
   = max
|x|=1

|ax|. let x = eu where au is row u. then |au| = |aeu|     max
|x|=1

|ax| =   

447

12.8.7 additional id202
lemma 12.25 let a be an n    n symmetric matrix. then det(a) =   1  2          n.
proof: the det (a       i) is a polynomial in    of degree n. the coe   cient of   n will be   1
depending on whether n is odd or even. let the roots of this polynomial be   1,   2, . . . ,   n.
then det(a       i) = (   1)n

(         i). thus

n(cid:81)

i=1

det(a) = det(a       i)|  =0 = (   1)n

n(cid:89)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  =0

(         i)

=   1  2          n

the trace of a matrix is de   ned to be the sum of its diagonal elements. that is,

tr (a) = a11 + a22 +        + ann.
lemma 12.26 tr(a) =   1 +   2 +        +   n.
proof: consider the coe   cient of   n   1 in det(a       i) = (   1)n

          a11        a12

a21
...

      
a22              
...
...

          .

a       i =

n(cid:81)

i=1

(         i). write

calculate det(a       i) by expanding along the    rst row. each term in the expansion
involves a determinant of size n     1 which is a polynomial in    of deg n     2 except for
the principal minor which is of deg n     1. thus the term of deg n     1 comes from

(a11       ) (a22       )       (ann       )

and has coe   cient (   1)n   1 (a11 + a22 +        + ann). now

n(cid:89)

i=1

(   1)n

(         i) = (   1)n (         1)(         2)       (         n)

= (   1)n(cid:16)

  n     (  1 +   2 +        +   n)  n   1 +       (cid:17)

therefore equating coe   cients   1 +   2 +        +   n = a11 + a22 +        + ann = tr(a)

(cid:18) 1 0

(cid:19)

(cid:18) 1 0

(cid:19)

note that (tr(a))2 (cid:54)= tr(a2). for example a =

has trace 3, a2 =

0 2
has trace 5 (cid:54)=9. however tr(a2) =   2
n. to see this, observe that a2 =
(v t dv )2 = v t d2v . thus, the eigenvalues of a2 are the squares of the eigenvalues for
a.

2 +        +   2

1 +   2

0 4

448

of a is a = p dp t . we have

alternative proof that tr(a) =   1+   2+       +   n : suppose the spectral decomposition

tr (a) = tr(cid:0)p dp t(cid:1) = tr(cid:0)dp t p(cid:1) = tr (d) =   1 +   2 +        +   n.

lemma 12.27 if a is n    m and b is a m    n matrix, then tr(ab)=tr(ba).

n(cid:88)

m(cid:88)

m(cid:88)

n(cid:88)

i=1

j=1

j=1

i=1

tr(ab) =

aijbji =

bjiaij = tr (ba)

pseudo inverse

(cid:16) 1

(cid:17)

let a be an n    m rank r matrix and let a = u   v t be the singular value decompo-
where   1, . . . ,   r are the nonzero singular
it is the unique x that

sition of a. let   (cid:48) = diag
values of a. then a(cid:48) = v   (cid:48)u t is the pseudo inverse of a.
minimizes (cid:107)ax     i(cid:107)f .

, . . . , 1
  r

, 0, . . . , 0

  1

second eigenvector

suppose the eigenvalues of a matrix are   1       2            . the second eigenvalue,
  2, plays an important role for matrices representing graphs. it may be the case that
|  n| > |  2|.

why is the second eigenvalue so important? consider partitioning the vertices of a
regular degree d graph g = (v, e) into two blocks of equal size so as to minimize the
number of edges between the two blocks. assign value +1 to the vertices in one block and
-1 to the vertices in the other block. let x be the vector whose components are the   1
values assigned to the vertices. if two vertices, i and j, are in the same block, then xi and
xj are both +1 or both    1 and (xi   xj)2 = 0. if vertices i and j are in di   erent blocks then
(xi     xj)2 = 4. thus, partitioning the vertices into two blocks so as to minimize the edges
between vertices in di   erent blocks is equivalent to    nding a vector x with coordinates
  1 of which half of its coordinates are +1 and half of which are    1 that minimizes

(cid:88)

(i,j)   e

ecut =

1
4

(xi     xj)2

let a be the adjacency matrix of g. then

xt ax =(cid:80)

aijxixj = 2 (cid:80)
(cid:18) number of edges
(cid:19)
(cid:18) total number

within components

edges

ij

= 2   
= 2   

of edges

    4   

xixj

(cid:19)

(cid:18) number of edges
(cid:19)

(cid:18) number of edges

    2   

between components

(cid:19)

between components

449

maximizing xt ax over all x whose coordinates are   1 and half of whose coordinates are
+1 is equivalent to minimizing the number of edges between components.

since    nding such an x is computationally di   cult, one thing we can try to do is
replace the integer condition on the components of x and the condition that half of the
components are positive and half of the components are negative with the conditions

n(cid:80)

n(cid:80)

x2
i = 1 and

xi = 0. then    nding the optimal x gives us the second eigenvalue since

i=1
it is easy to see that the    rst eigenvector is along 1

i=1

n(cid:80)

  2 = max
x   v1

xt ax(cid:80) x2
n(cid:80)
(cid:18) number of edges

i=1

i

(cid:19)

(cid:18) total number

(cid:19)

actually we should use
2   
a larger set of x. the fact that   2 gives us a bound on the minimum number of cross
edges is what makes it so important.

x2
i = n not
    4   

x2
i = 1. thus n  2 must be greater than

since the maximum is taken over

between components

of edges

i=1

12.8.8 distance between subspaces

suppose s1 and s2 are two subspaces. choose a basis of s1 and arrange the basis
vectors as the columns of a matrix x1; similarly choose a basis of s2 and arrange the
basis vectors as the columns of a matrix x2. note that s1 and s2 can have di   erent
dimensions. de   ne the square of the distance between two subspaces by

dist2(s1, s2) = dist2(x1, x2) = ||x1     x2x t

2 x1||2

f

since x1     x2x t

2 x1 and x2x t

2 x1 are orthogonal

f =(cid:13)(cid:13)x1     x2x t

2 x1

(cid:107)x1(cid:107)2

and hence

dist2 (x1, x2) = (cid:107)x1(cid:107)2

(cid:13)(cid:13)2

f

(cid:13)(cid:13)2
f +(cid:13)(cid:13)x2x t
f    (cid:13)(cid:13)x2x t

2 x1

2 x1

(cid:13)(cid:13)2

f .

intuitively, the distance between x1 and x2 is the frobenius norm of the component of
x1 not in the space spanned by the columns of x2.

if x1 and x2 are 1-dimensional unit length vectors, dist2 (x1, x2) is the sine squared

of the angle between the spaces.

example: consider two subspaces in four dimensions

               

1   
2
0
1   
2
0

0
1   
3
1   
3
1   
3

               

450

x1 =

x2 =

             1 0

0 1
0 0
0 0

            

here

dist2 (x1, x2) =

=

1   
2
0
1   
2
0

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
               
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
             0

0
1   
2
0

             1 0

0 1
0 0
0 0

            (cid:18) 1 0 0 0

0 1 0 0

(cid:19)               

               

1   
2
0
1   
2
0

0
1   
3
1   
3
1   
3

=

7
6

                   
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
            

2

f

0
1   
3
1   
3
1   
3

0
0
1   
3
1   
3

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2

f

in essence, we projected each column vector of x1 onto x2 and computed the frobenius
norm of x1 minus the projection. the frobenius norm of each column is the sin squared
of the angle between the original column of x1 and the space spanned by the columns of
x2.

12.8.9 positive semide   nite matrix
a square symmetric matrix is positive semide   nite if for all x, xtax     0. there are
actually three equivalent de   nitions of positive semide   nite.

1. for all x, xtax     0

2. all eigenvalues are nonnegative

3. a = bt b

we will prove (1) implies (2), (2) implies (3), and (3) implies (1).

1. (1) implies (2) if   i were negative, select x = vi. let ei be the vector of all zeros

except for a one in position i. then xt ax = vi

t v dv t vi = ei

t dei =   i < 0.

2 d 1
2. (2) implies (3) a = v dv t = v d 1
3. (3) implies (1) xtax = (xb)t bx     0

2 v t = bt b

12.9 generating functions

a sequence a0, a1, . . ., can be represented by a generating function g(x) =

   (cid:80)

aixi. the

advantage of the generating function is that it captures the entire sequence in a closed
form that can be manipulated as an entity. for example, if g(x) is the generating func-
tion for the sequence a0, a1, . . ., then x d
dx g(x) is the generating function for the sequence
0, a1, 2a2, 3a3, . . . and x2g(cid:48)(cid:48)(x) + xg(cid:48)(x) is the generating function for the sequence for
0, a1, 4a2, 9a3, . . .

i=0

451

example: the generating function for the sequence 1, 1, . . . is

ating function for the sequence 0, 1, 2, 3, . . . is

   (cid:80)

   (cid:80)

   (cid:80)

i=0

ixi =

x d
dx xi = x d

dx

xi = x d
dx

1

1   x = x

(1   x)2 .

i=0

i=0

   (cid:80)

i=0

xi = 1

1   x. the gener-

example: if a can be selected 0 or 1 times and b can be selected 0, 1, or 2 times and c
can be selected 0, 1, 2, or 3 times, in how many ways can    ve objects be selected. consider
the generating function for the number of ways to select objects. the generating function
for the number of ways of selecting objects, selecting only a   s is 1+x, only b   s is 1+x+x2,
and only c   s is 1 + x + x2 + x3. the generating function when selecting a   s, b   s, and c   s
is the product.

(1 + x)(1 + x + x2)(1 + x + x2 + x3) = 1 + 3x + 5x2 + 6x3 + 5x4 + 3x5 + x6

the coe   cient of x5 is 3 and hence we can select    ve objects in three ways: abbcc,
abccc, or bbccc.

   (cid:80)

the generating functions for the sum of random variables

   (cid:80)

let f (x) =

pixi be the generating function for an integer valued random variable

i=0

where pi is the id203 that the random variable takes on value i. let g(x) =

qixi

xi in the product f (x)g(x) is(cid:80)i

be the generating function of an independent integer valued random variable where qi
is the id203 that the random variable takes on the value i. the sum of these two
random variables has the generating function f (x)g(x). this is because the coe   cient of
k=0 pkqk   i and this is also the id203 that the sum of
the random variables is i. repeating this, the generating function of a sum of independent
nonnegative integer valued random variables is the product of their generating functions.

i=0

12.9.1 generating functions for sequences de   ned by recurrence relation-

ships

consider the fibonacci sequence

0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, . . .

de   ned by the recurrence relationship

f0 = 0

f1 = 1

fi = fi   1 + fi   2

i     2

452

multiply each side of the recurrence by xi and sum i from two to in   nity.

   (cid:88)

i=2

fixi =

   (cid:88)

   (cid:88)
= x(cid:0)f1x + f2x2 +       (cid:1) + x2 (f0 + f1x +       )

fi   1xi +

fi   2xi

i=2

i=2

f2x2 + f3x3 +        = f1x2 + f2x3 +        + f0x2 + f1x3 +       

let

f (x) =

   (cid:88)

fixi.

substituting (12.2) into (12.1) yields

i=0

f (x)     f0     f1x = x (f (x)     f0) + x2f (x)
f (x)     x = xf (x) + x2f (x)
f (x)(1     x     x2) = x

thus, f (x) = x

1   x   x2 is the generating function for the fibonacci sequence.

(12.1)

(12.2)

note that generating functions are formal manipulations and do not necessarily con-
verge outside some region of convergence. consider the generating function f (x) =

   (cid:80)

   (cid:80)

i=0

fixi = x

1   x   x2 for the fibonacci sequence. using

fixi,
f (1) = f0 + f1 + f2 +        =    

i=0

and using f (x) = x

1   x   x2

asymptotic behavior

f (1) =

1

1     1     1

=    1.

5

are the reciprocals of the two roots of the quadratic

to determine the asymptotic behavior of the fibonacci sequence write

x

f (x) =

1     x     x2 =

1       1x

+

   

5
5

      
1       2x

5
5

   
where   1 = 1+
1     x     x2 = 0.
2

5

and   1 = 1      

2

   

5
5

f (x) =

then

thus,

(cid:16)
1 +   1x + (  1x)2 +           (cid:0)1 +   2x + (  2x)2 +       (cid:1)(cid:17)

.

   

5
5

fn =

1       n
2 ) .

(  n

453

   
5   n
since   2 < 1 and   1 > 1, for large n, fn
integer and   2 < 1, it must be the case that fn =
all n.
means and standard deviations of sequences

   =

5

(cid:106)

1 . in fact, since fn =

   
2   n

5

2

fn +

(cid:107)

   
5 (  n
. hence fn =

5

(cid:106)   
1       n
2 ) is an
5
5   n
for

(cid:107)

1

generating functions are useful for calculating the mean and standard deviation of a
sequence. let z be an integral valued random variable where pi is the id203 that
pixi be the
z equals i. the expected value of z is given by m =

ipi. let p(x) =

generating function for the sequence p1, p2, . . .. the generating function for the sequence
p1, 2p2, 3p3, . . . is

   (cid:80)

i=0

   (cid:80)

i=0

   (cid:88)

i=0

x

d
dx

p(x) =

ipixi.

thus, the expected value of the random variable z is m = xp(cid:48)(x)|x=1 = p(cid:48)(1). if p was not
a id203 function, its average value would be p(cid:48)(1)
p(1) since we would need to normalize
the area under p to one.

the variance of z is e(z2)     e2(z) and can be obtained as follows.

x2 d
dx

p(x)

=

i(i     1)xip(x)

(cid:12)(cid:12)(cid:12)(cid:12)x=1

   (cid:88)
   (cid:88)

i=0

i2xip(x)

=
= e(z2)     e(z).

i=0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x=1
   (cid:88)

i=0

   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x=1

ixip(x)

thus,   2 = e(z2)     e2(z) = e(z2)     e(z) + e(z)     e2(z) = p(cid:48)(cid:48)(1) + p(cid:48)(1)    (cid:0)p(cid:48)(1)(cid:1)2.

12.9.2 the exponential generating function and the moment generating

function

besides the ordinary generating function there are a number of other types of gener-
ating functions. one of these is the exponential generating function. given a sequence

a0, a1, . . . , the associated exponential generating function is g(x) =

   (cid:80)

i=0

ai

xi
i! .

moment generating functions

the kth moment of a random variable x around the point b is given by e((x     b)k).
usually the word moment is used to denote the moment around the value 0 or around
the mean. in the following, we use moment to mean the moment about the origin.

454

the moment generating function of a random variable x is de   ned by

  (t) = e(etx) =

etxp(x)dx

   (cid:90)

      

replacing etx by its power series expansion 1 + tx + (tx)2

(cid:32)

   (cid:90)

      

  (t) =

1 + tx +

(tx)2

2!

+       

       gives

2!

(cid:33)

p(x)dx

thus, the kth moment of x about the origin is k! times the coe   cient of tk in the power
series expansion of the moment generating function. hence, the moment generating func-
tion is the exponential generating function for the sequence of moments about the origin.

the moment generating function transforms the id203 distribution p(x) into a
function    (t) of t. note   (0) = 1 and is the area or integral of p(x). the moment
generating function is closely related to the characteristic function which is obtained by
replacing etx by eitx in the above integral where i =
transform which is obtained by replacing etx by e   itx.

      1 and is related to the fourier

  (t) is closely related to the fourier transform and its properties are essentially the
same. in particular, p(x) can be uniquely recovered by an inverse transform from   (t).
mi
i! ti converges abso-

more speci   cally, if all the moments mi are    nite and the sum

   (cid:80)

i=0

lutely in a region around the origin, then p(x) is uniquely determined.

the gaussian id203 distribution with zero mean and unit variance is given by

p (x) = 1   

2  

e    x2

2 . its moments are given by

   (cid:90)
(cid:40) n!

1   
2  

      

un =

=

xne    x2

2 dx

2 )!

n

2 ( n
2
0

n even

n odd

to derive the above, use integration by parts to get un = (n     1) un   2 and combine
2 and v = xn   1. then

this with u0 = 1 and u1 = 0. the steps are as follows. let u = e    x2
u(cid:48) =    xe    x2

2 and v(cid:48) = (n     1) xn   2. now uv =(cid:82) u(cid:48)v+(cid:82) uv(cid:48) or

e    x2

2 xn   1 =

(n     1) xn   2e    x2

2 dx.

(cid:90)

(cid:90)

xne    x2

2 dx +

455

2 dx = (n     1)(cid:82) xn   2e    x2

(cid:82) xne    x2
   (cid:82)

xne    x2

2 dx = (n     1)

xn   2e    x2

2 dx

   (cid:82)

      

2 dx     e    x2

2 xn   1

from which

      
thus, un = (n     1) un   2.

the moment generating function is given by

   (cid:88)

n=0

   (cid:88)

n=0

n even

g (s) =

unsn
n!

=

n!
2 n
n
2 !

2

sn
n!

=

   (cid:88)

i=0

(cid:18) s2

(cid:19)i

2

   (cid:88)

i=0

1
i!

s2i
2ii!

=

= e

s2
2 .

for the general gaussian, the moment generating function is

(cid:16)   2

(cid:17)

2

s2

g (s) = esu+

thus, given two independent gaussians with mean u1 and u2 and variances   2
the product of their moment generating functions is

1 and   2
2,

es(u1+u2)+(  2

1+  2

2)s2

,

the moment generating function for a gaussian with mean u1 + u2 and variance   2
1 +   2
2.
thus, the convolution of two gaussians is a gaussian and the sum of two random vari-
ables that are both gaussian is a gaussian random variable.

12.10 miscellaneous

12.10.1 lagrange multipliers

lagrange multipliers are used to convert a constrained optimization problem into an un-
constrained optimization. suppose we wished to maximize a function f (x) subject to a
constraint g(x) = c. the value of f (x) along the constraint g(x) = c might increase for
a while and then start to decrease. at the point where f (x) stops increasing and starts
to decrease, the contour line for f (x) is tangent to the curve of the constraint g(x) = c.
stated another way the gradient of f (x) and the gradient of g(x) are parallel.

by introducing a new variable    we can express the condition by    xf =      xg and

g = c. these two conditions hold if and only if

(cid:0)f (x) +    (g (x)     c)(cid:1) = 0

   x  

the partial with respect to    establishes that g(x) = c. we have converted the constrained
optimization problem in x to an unconstrained problem with variables x and   .

456

g(x, y) = c

f (x, y)

figure 12.3: in    nding the minimum of f (x, y) within the ellipse, the path head towards
the minimum of f (x, y) until it hits the boundary of the ellipse and then follows the
boundary of the ellipse until the tangent of the boundary is in the same direction as the
contour line of f (x, y).

12.10.2 finite fields

for a prime p and integer n there is a unique    nite    eld with pn elements. in section
8.6 we used the    eld gf(2n), which consists of polynomials of degree less than n with
coe   cients over the    eld gf(2). in gf(28)

(x7 + x5 + x) + (x6 + x5 + x4) = x7 + x6 + x4 + x

multiplication is modulo an irreducible polynomial. thus

(x7 + x5 + x)(x6 + x5 + x4) = x13 + x12 + x11 + x11 + x10 + x9 + x7 + x6 + x5

= x13 + x12 + x10 + x9 + x7 + x6 + x5
= x6 + x4 + x3 + x2

mod x8 + x4 + x3 + x + 1

division of x13 + x12 + x10 + x9 + x7 + x6 + x5 by x6 + x4 + x3 + x2 is illustrated below.

   x5(x8 + x4 + x3 + x2 + 1) = x13
   x4(x8 + x4 + x3 + x2 + 1) =
   x2(x8 + x4 + x3 + x2 + 1) =

x12 +x10
x12

x10
x10

x13 +x12 +x10 +x9
+x9

+x7 +x6 +x5
+x6 +x5

+x8
+x8 +x7
+x8 +x7

+x5 +x4
+x5
x4
x6 +x5
x6

x3

x2
+x4 +x3 +x2

12.10.3 application of mean value theorem

the mean value theorem states that if f (x) is continuous and di   erentiable on the
. that is, at some

interval [a, b], then there exists c, a     c     b such that f(cid:48)(c) = f (b)   f (a)

b   a

457

f (x)

a

c

b

figure 12.4: illustration of the mean value theorem.

point between a and b the derivative of f equals the slope of the line from f (a) to f (b).
see figure 12.10.3.

one application of the mean value theorem is with the taylor expansion of a function.

the taylor expansion about the origin of f (x) is

f (x) = f (0) + f(cid:48)(0)x +

f(cid:48)(cid:48)(0)x2 +

1
2!

1
3!

f(cid:48)(cid:48)(cid:48)(0)x3 +       

(12.3)

by the mean value theorem there exists c, 0     c     x, such that f(cid:48)(c) = f (x)   f (0)
f (x)     f (0) = xf(cid:48)(c). thus

x

or

xf(cid:48)(c) = f(cid:48)(0)x +

f(cid:48)(cid:48)(0)x2 +

1
2!

1
3!

f(cid:48)(cid:48)(cid:48)(0)x3 +       

and

f (x) = f (0) + xf(cid:48)(c).

one could apply the mean value theorem to f(cid:48)(x) in

f(cid:48)(x) = f(cid:48)(0) + f(cid:48)(cid:48)(0)x +

then there exists d, 0     d     x such that

f(cid:48)(cid:48)(cid:48)(0)x2 +       

1
2!

integrating

xf(cid:48)(cid:48)(d) = f(cid:48)(cid:48)(0)x +

f(cid:48)(cid:48)(cid:48)(0)x2 +       

1
2!

x2f(cid:48)(cid:48)(d) =

1
2

f(cid:48)(cid:48)(0)x +

1
2!

1
3!

f(cid:48)(cid:48)(cid:48)(0)x3 +       

458

substituting into eq(12.3)

f (x) = f (0) + f(cid:48)(0)x +

x2f(cid:48)(cid:48)(d).

1
2

12.10.4 sperner   s lemma

consider a triangulation of a 2-dimensional simplex. let the vertices of the simplex
be colored r, b, and g. if the vertices on each edge of the simplex are colored only with
the two colors at the endpoints then the triangulation must have a triangle whose ver-
tices are three di   erent colors. in fact, it must have an odd number of such vertices. a
generalization of the lemma to higher dimensions also holds.

create a graph whose vertices correspond to the triangles of the triangulation plus an
additional vertex corresponding to the outside region. connect two vertices of the graph
by an edge if the triangles corresponding to the two vertices share a common edge that
is color r and b. the edge of the original simplex must have an odd number of such
triangular edges. thus, the outside vertex of the graph must be of odd degree. the graph
must have an even number of odd degree vertices. each odd vertex is of degree 0, 1, or 2.
the vertices of odd degree, i.e. degree one, correspond to triangles which have all three
colors.

12.10.5 pr  ufer

here we prove that the number of labeled trees with n vertices is nn   2. by a labeled
tree we mean a tree with n vertices and n distinct labels, each label assigned to one vertex.
theorem 12.28 the number of labeled trees with n vertices is nn   2.

proof: (pr  ufer sequence) there is a one-to-one correspondence between labeled trees
and sequences of length n     2 of integers between 1 and n. an integer may repeat in the
sequence. the number of such sequences is clearly nn   2. although each vertex of the tree
has a unique integer label the corresponding sequence has repeating labels. the reason for
this is that the labels in the sequence refer to interior vertices of the tree and the number
of times the integer corresponding to an interior vertex occurs in the sequence is related
to the degree of the vertex. integers corresponding to leaves do not appear in the sequence.

to see the one-to-one correspondence,    rst convert a tree to a sequence by deleting
the lowest numbered leaf. if the lowest numbered leaf is i and its parent is j, append j to
the tail of the sequence. repeating the process until only two vertices remain yields the
sequence. clearly a labeled tree gives rise to only one sequence.

it remains to show how to construct a unique tree from a sequence. the proof is
by induction on n. for n = 1 or 2 the induction hypothesis is trivially true. assume
the induction hypothesis true for n     1. certain numbers from 1 to n do not appear

459

in the sequence and these numbers correspond to vertices that are leaves. let i be
the lowest number not appearing in the sequence and let j be the    rst integer in the
sequence. then i corresponds to a leaf connected to vertex j. delete the integer j from
the sequence. by the induction hypothesis there is a unique labeled tree with integer
labels 1, . . . , i     1, i + 1, . . . , n. add the leaf i by connecting the leaf to vertex j. we
need to argue that no other sequence can give rise to the same tree. suppose some other
sequence did. then the ith integer in the sequence must be j. by the induction hypothesis
the sequence with j removed is unique.

algorithm

create leaf list - the list of labels not appearing in the pr  ufer sequence. n is the

length of the pr  ufer list plus two.
while pr  ufer sequence is non empty do
begin

p =   rst integer in pr  ufer sequence
e =smallest label in leaf list
add edge (p, e)
delete e from leaf list
delete p from pr  ufer sequence
if p no longer appears in pr  ufer sequence add p to leaf list

end
there are two vertices e and f on leaf list, add edge (e, f )

12.11 exercises

exercise 12.1 what is the di   erence between saying f (n) is o (n3) and f (n) is o (n3)?
exercise 12.2 if f (n)     g (n) what can we say about f (n) + g(n) and f (n)     g(n)?
exercise 12.3 what is the di   erence between     and   ?

exercise 12.4 if f (n) is o (g (n)) does this imply that g (n) is     (f (n))?

exercise 12.5 what is lim
k      

(cid:0) k   1

k   2

(cid:1)k   2.

exercise 12.6 select a, b, and c uniformly at random from [0, 1]. the id203 that
b < a is 1/2. the id203 that c<a is 1/2. however, the id203 that both b and c are
less than a is 1
3 not 1/4. why is this? note that the six possible permutations abc, acb,
bac, cab, bca, and cba, are all equally likely. assume that a, b, and c are drawn from the
interval (0,1]. given that b < a, what is the id203 that c < a?

exercise 12.7 let a1, a2, . . . , an be events. prove that prob(a1   a2          an)     n(cid:80)

prob(ai)

460

i=1

exercise 12.8 give an example of three random variables that are pairwise independent
but not fully independent.

exercise 12.9 give examples of nonnegative valued random variables with median >>
mean. can we have median << mean?

exercise 12.10 consider n samples x1, x2, . . . , xn from a gaussian distribution of mean
   and variance   . for this distribution m = x1+x2+      +xn
is an unbiased estimator of
(xi       )2 is an unbiased estimator of   2. prove that if we
  . if    is known then 1
n

n

n(cid:80)

i=1
approximate    by m, then 1
n   1

(xi     m)2 is an unbiased estimator of   2.

n(cid:80)

i=1

exercise 12.11 given the distribution

1   

2  3

    1
e

2( x

3 )2

what is the id203 that x >1?

exercise 12.12 e

2 has value 1 at x = 0 and drops o    very fast as x increases. suppose

we wished to approximate e

2 by a function f (x) where

    x2

    x2

(cid:26) 1

0

f (x) =

|x|     a
|x| > a

.

what value of a should we use? what is the integral of the error between f (x) and e

    x2

2 ?

exercise 12.13 given two sets of red and black balls with the number of red and black
balls in each set shown in the table below.

set 1
set 2

red black
40
50

60
50

randomly draw a ball from one of the sets. suppose that it turns out to be red. what is
the id203 that it was drawn from set 1?
exercise 12.14 why cannot one prove an analogous type of theorem that states p (x     a)    
e(x)
a ?

exercise 12.15 compare the markov and chebyshev bounds for the following id203
distributions

(cid:26) 1 x = 1
(cid:26) 1/2

0

0 otherwise

0     x     2
otherwise

1. p(x) =

2. p(x) =

461

exercise 12.16 let s be the sum of n independent random variables x1, x2, . . . , xn where
for each i

(cid:26) 0 prob

xi =

1. how large must    be if we wish to have p rob(cid:0)s < (1       ) m(cid:1) <   ?
2. if we wish to have p rob(cid:0)s > (1 +   ) m(cid:1) <   ?

1 prob

p
1     p

exercise 12.17 what is the expected number of    ips of a coin until a head is reached?
assume p is id203 of a head on an individual    ip. what is value if p=1/2?

exercise 12.18 given the joint id203

p(a,b)
b=0
b=1

a=0
1/16
1/4

a=1
1/8
9/16

1. what is the marginal id203 of a? of b?

2. what is the id155 of b given a?

exercise 12.19 consider independent random variables x1, x2, and x3, each equal to
2. let s = x1 + x2 + x3 and let f be event that s     {1, 2}. condi-
zero with id203 1
tioning on f , the variables x1, x2, and x3 are still each zero with id203 1
2. are they
still independent?

exercise 12.20 consider rolling two dice a and b. what is the id203 that the sum
s will add to nine? what is the id203 that the sum will be 9 if the roll of a is 3?

exercise 12.21 write the generating function for the number of ways of producing chains
using only pennies, nickels, and dines. in how many ways can you produce 23 cents?

exercise 12.22 a dice has six faces, each face of the dice having one of the numbers 1
though 6. the result of a role of the dice is the integer on the top face. consider two roles
of the dice. in how many ways can an integer be the sum of two roles of the dice.

exercise 12.23 if a(x) is the generating function for the sequence a0, a1, a2, . . ., for what
sequence is a(x)(1-x) the generating function.
exercise 12.24 how many ways can one draw n a(cid:48)s and b(cid:48)s with an even number of a(cid:48)s.

exercise 12.25 find the generating function for the recurrence ai = 2ai   1 + i where
a0 = 1.

462

exercise 12.26 find a closed form for the generating function for the in   nite sequence
of prefect squares 1, 4, 9, 16, 25, . . .

exercise 12.27 given that
what sequence is

1

1   2x the generating function?

1
1   x is the generating function for the sequence 1, 1, . . ., for

exercise 12.28 find a closed form for the exponential generating function for the in   nite
sequence of prefect squares 1, 4, 9, 16, 25, . . .

exercise 12.29 prove that the l2 norm of (a1, a2, . . . , an) is less than or equal to the l1
norm of (a1, a2, . . . , an).
exercise 12.30 prove that there exists a y, 0     y     x, such that f (x) = f (0) + f(cid:48)(y)x.

exercise 12.31 show that the eigenvectors of a matrix a are not a continuous function
of changes to the matrix.

exercise 12.32 what are the eigenvalues of the two graphs shown below? what does
this say about using eigenvalues to determine if two graphs are isomorphic.

exercise 12.33 let a be the adjacency matrix of an undirected graph g. prove that
eigenvalue   1 of a is at least the average degree of g.

exercise 12.34 show that if a is a symmetric matrix and   1 and   2 are distinct eigen-
values then their corresponding eigenvectors x1 and x2 are orthogonal.
hint:

exercise 12.35 show that a matrix is rank k if and only if it has k nonzero eigenvalues
and eigenvalue 0 of rank n-k.

exercise 12.36 prove that maximizing xt ax
to the condition that x be of unit length.

xt x is equivalent to maximizing xt ax subject

exercise 12.37 let a be a symmetric matrix with smallest eigenvalue   min. give a
bound on the largest element of a   1.

exercise 12.38 let a be the adjacency matrix of an n vertex clique with no self loops.
thus, each row of a is all ones except for the diagonal entry which is zero. what is the
spectrum of a.

exercise 12.39 let a be the adjacency matrix of an undirect graph g. prove that the
eigenvalue   1 of a is at least the average degree of g.

463

exercise 12.40 we are given the id203 distribution for two random vectors x and
y and we wish to stretch space to maximize the expected distance between them. thus,

we will multiply each coordinate by some quantity ai. we restrict

a2
i = d. thus, if we
increase some coordinate by ai > 1, some other coordinate must shrink. given random
vectors x = (x1, x2, . . . , xd) and y = (y1, y2, . . . , yd) how should we select ai to maximize

e(cid:0)|x     y|2(cid:1)? the ai stretch di   erent coordinates. assume

i=1

d(cid:80)

and that xi has some arbitrary distribution.

e(cid:0)|x     y|2(cid:1) = e

d(cid:80)

i=1

i     2xiyi + y2
i )

1

1
2
1
2

yi =

(cid:26) 0
i (xi     yi)2(cid:3) =
(cid:2)a2
i e(cid:0)x2
d(cid:80)

a2

=

i=1

d(cid:80)
a2
i e (x2
i     xi + 1

(cid:1)

i=1

2

d(cid:80)

i=1

since e (x2

i ) = e (xi) we get . thus, weighting the coordinates has no e   ect assuming

i = 1. why is this? since e (yi) = 1
a2
2.

e(cid:0)|x     y|2(cid:1) is independent of the value of xi hence its distribution.
i e(cid:0)xi     1

(cid:26) 0
e(cid:0)|x     y|2(cid:1) =

and e (yi) = 1

what if yi =

4. then

a2
i e (x2

d(cid:80)

i ) =

a2

3
4
1
4

1

i=1

i     2xiyi + y2
d(cid:80)

(cid:0) 1

a2
i

2e (xi) + 1

4

=

i=1

d(cid:80)
(cid:1)

i=1

(cid:1)

.

2xi + 1

4

to maximize put all weight on the coordinate of x with highest id203 of one. what
if we used 1-norm instead of the two norm?

e (|x     y|) = e

ai |xi     yi| =

aie |xi     yi| =

aibi

i=1

i=1

i=1

d(cid:88)

d(cid:88)

where bi = e (xi     yi). if
of a and b is maximized when both are in the same direction.

i = 1, then to maximize let ai = bi
a2

i=1

b . taking the dot product

d(cid:88)

d(cid:80)

exercise 12.41 maximize x+y subject to the constraint that x2 + y2 = 1.

exercise 12.42 draw a tree with 10 vertices and label each vertex with a unique integer
from 1 to 10. construct the prfer sequence for the tree. given the prfer sequence recreate
the tree.

464

exercise 12.43 construct the tree corresponding to the following prfer sequences

1. 113663

2. 552833226

465

index

2-universal, 184
4-way independence, 191

a   nity matrix, 229
algorithm

greedy k-id91, 215
id116, 211
singular value decomposition, 51

almost surely, 253
anchor term, 317
aperiodic, 77
arithmetic mean, 417

bad pair, 257
bayes rule, 428
bayesian, 338
id110, 338
belief network, 338
belief propagation, 337
bernoulli trials, 425
best    t, 40
bigoh, 406
binomial distribution, 248

approximated by poisson, 426

boosting, 158
branching process, 272

cartesian coordinates, 17
cauchy-schwartz inequality, 414, 416
central limit theorem, 423
characteristic equation, 437
characteristic function, 455
chebyshev   s inequality, 13
cherno    bounds, 430
id91, 208

k-center criterion, 215
id116, 211
sparse cuts, 229

cnf

cnf-sat, 279

cohesion, 232

combining expert advice, 162
commute time, 104
id155, 421
conductance, 97
coordinates

cartesian, 17
polar, 17

coupon collector problem, 107
cumulative distribution function, 420
current

probabilistic interpretation, 100

cycles, 266

emergence, 265
number of, 265

data streams

counting frequent elements, 187
frequency moments, 182
frequent element, 188
majority element, 187
number of distinct elements, 183
number of occurrences of an element,

186

second moment, 189
degree distribution, 248

power law, 248

depth    rst search, 261
diagonalizable, 438
diameter of a graph, 256, 268
diameter two, 266
dilation, 385
disappearance of isolated vertices, 266
discovery time, 102
distance

total variation, 82

distribution

vertex degree, 246
document ranking, 62

e   ective resistance, 105
eigenvalue, 437

466

eigenvector, 54, 437
electrical network, 97
erd  os r  enyi, 245
error correcting codes, 190
escape id203, 101
euler   s constant, 108
event, 420
expected degree

vertex, 245

expected value, 421
exponential generating function, 454
extinct families

size, 276

extinction id203, 272, 274

finite    elds, 457
first moment method, 254
fourier transform, 370, 455
frequency domain, 371

g(n,p), 245
gamma function, 18
gamma function , 415
gaussian, 23, 424, 456
   tting to data, 29
tail, 419

gaussians

sparating, 27

general tail bounds, 433
generating function, 272
component size, 288
for sum of two variables, 272

generating functions, 451
generating points in the unit ball, 22
geometric mean, 417
giant component, 246, 253, 259, 261, 266
id150, 84
graph

connecntivity, 265
resistance, 108

graphical model, 337
greedy

k-id91, 215
growth models, 286

with preferential attachment, 293
without preferential attachment, 287

h  older   s inequality, 414, 416
haar wavelet, 386
harmonic function, 98
hash function

universal, 184

heavy tail, 248
hidden markov model, 332
hitting time, 102, 114

immortality id203, 274
incoherent, 368, 371
increasing property, 253, 270

unsatis   ability, 279

independence

limited way, 190

independent, 421
indicator random variable, 257

of triangle, 251

indicator variable, 422
ising model, 352
isolated vertices, 259, 266

number of, 259

jensen   s inequality, 418
johnson-lindenstrauss lemma, 25, 26

k-id91, 215
id116 id91 algorithm, 211
kernel methods, 228
kirchho      s law, 99
kleinberg, 295

lagrange, 456
laplacian, 70
law of large numbers, 12, 14
learning, 129
linearity of expectation, 251, 421
lloyd   s algorithm, 211
local algorithm, 295
long-term probabilities, 80

m-fold, 270

467

markov chain, 77

state, 82

id115, 78
markov random    eld, 340
markov   s inequality, 13
matrix

multiplication

by sampling, 193
diagonalizable, 438
similar, 437

maximum cut problem, 63
id113, 429
maximum likelihood estimator, 29
maximum principle, 98
mcmc, 78
mean value theorem, 457
median, 423
metropolis-hastings algorithm, 83
mixing time, 80
model

random graph, 245

molloy reed, 285
moment generating function, 455
mutually independent, 421

nearest neighbor problem, 27
nonuniform random graphs, 284
normalized conductance, 80, 89
number of triangles in g(n, p), 251

ohm   s law, 99
orthonormal, 445

page rank, 113

personalized , 116

persistent, 77
phase transition, 253

cnf-sat, 279
non   nite components, 291

poisson distribution, 426
polar coordinates, 17
polynomial interpolation, 190
positive semide   nite, 451
power iteration, 62

power law distribution, 248
power method, 51
power-law distribution, 284
pr  ufer, 459
principle component analysis, 56
id203 density function, 420
id203 distribution function, 420
psuedo random, 191
pure-literal heuristic, 280

queue, 281

arrival rate, 281

radon, 152
random graph, 245
random projection, 25

theorem, 25

random variable, 420
random walk

eucleadean space, 109
in three dimensions, 110
in two dimensions, 110
on lattice, 109
undirected graph, 102
web, 112

rapid mixing, 82
real spectral theorem, 439
replication, 270
resistance, 97, 108

e   fective, 101

restart, 113

value, 113

return time, 113

sample space, 420
sampling

length squared, 194
satisfying assignments

expected number of, 280

scale function, 386
scale vector, 386
second moment method, 251, 254
sharp threshold, 253
similar matrices, 437

468

variance, 422
variational method, 413
vc-dimension, 148

convex polygons, 151
   nite sets, 153
half spaces, 151
intervals, 151
pairs of intervals, 151
rectangles, 151
spheres, 152

viterbi algorithm, 334
voltage

probabilistic interpretation, 99

wavelet, 385
world wide web, 112

young   s inequality, 414, 416

singular value decomposition, 40
singular vector, 42

   rst, 43
left, 45
right, 45
second, 43

six-degrees separation, 295
sketch

matrix, 197

sketches

documents, 201

small world, 294
smallest-clause heuristic, 280
spam, 115
spectral id91, 216
sperner   s lemma, 459
stanley milgram, 294
state, 82
stirling approximation, 414
streaming model, 181
symmetric matrices, 439

tail bounds, 430, 433
tail of gaussian, 419
taylor series, 409
threshold, 252

cnf-sat, 277
diameter o(ln n), 269
disappearance of isolated vertices, 259
emergence of cycles, 265
emergence of diameter two, 256
giant component plus isolated vertices,

267

time domain, 371
total variation distance, 82
trace, 448
triangle inequality, 414
triangles, 250

union bound, 422
unit-clause heuristic, 280
unitary matrix, 445
unsatis   ability, 279

469

references

[ab15]

pranjal awasthi and maria-florina balcan. center based id91: a foun-
dational perspective. in christian hennig, marina meila, fionn murtagh,
and roberto rocci, editors, handbook of cluster analysis. crc press, 2015.

[acort11] dimitris achlioptas, amin coja-oghlan, and federico ricci-tersenghi. on
the solution-space geometry of random id124 problems.
random structures & algorithms, 38(3):251   268, 2011.

[agkm16] sanjeev arora, rong ge, ravi kannan, and ankur moitra. computing a
nonnegative id105 - provably. siam j. comput., 45(4):1582   
1611, 2016.

[ak05]

sanjeev arora and ravindran kannan. learning mixtures of separated non-
spherical gaussians. annals of applied id203, 15(1a):69   92, 2005. pre-
liminary version in stoc 2001.

[alo86]

noga alon. eigenvalues and expanders. combinatorica, 6:83   96, 1986.

[am05]

[ams96]

[an72]

[ap03]

[arr50]

[av07]

[ba]

[bb10]

dimitris achlioptas and frank mcsherry. on spectral learning of mixtures
of distributions. in colt, pages 458   469, 2005.

noga alon, yossi matias, and mario szegedy. the space complexity of
approximating the frequency moments. in proceedings of the twenty-eighth
annual acm symposium on theory of computing, pages 20   29. acm, 1996.

krishna athreya and p. e. ney. branching processes, volume 107. springer,
berlin, 1972.

dimitris achlioptas and yuval peres. the threshold for random k-sat is 2k
(ln 2 - o(k)). in stoc, pages 223   231, 2003.

kenneth j. arrow. a di   culty in the concept of social welfare. journal of
political economy, 58(4):328   346, 1950.

david arthur and sergei vassilvitskii. id116++: the advantages of care-
ful seeding. in proceedings of the eighteenth annual acm-siam symposium
on discrete algorithms, pages 1027   1035. society for industrial and applied
mathematics, 2007.

albert-lszl barabsi and rka albert. emergence of scaling in random net-
works. science, 286(5439).

m.-f. balcan and a. blum. a discriminative model for semi-supervised
learning. journal of the acm, 57(3):19:1   19:46, march 2010.

470

[bbg13]

[bbis16]

[bbk14]

[bbl09]

[bbv08]

maria-florina balcan, avrim blum, and anupam gupta. id91 under
approximation stability. journal of the acm (jacm), 60(2):8, 2013.
tom balyo, armin biere, markus iser, and carsten sinz. {sat} race 2015.
arti   cial intelligence, 241:45     65, 2016.

trapit bansal, chiranjib bhattacharyya, and ravindran kannan. a provable
svd-based algorithm for learning topics in dominant admixture corpus. in
advances in neural information processing systems 27 (nips), pages 1997   
2005, 2014.

m.-f. balcan, a. beygelzimer, and j. langford. agnostic active learning.
journal of computer and system sciences, 75(1):78     89, 2009. special issue
on learning theory. an earlier version appeared in international conference
on machine learning 2006.

maria-florina balcan, avrim blum, and santosh vempala. a discriminative
framework for id91 via similarity functions. in proceedings of the forti-
eth annual acm symposium on theory of computing, pages 671   680. acm,
2008.

[behw87] a. blumer, a. ehrenfeucht, d. haussler, and m. k. warmuth. occam   s

razor. information processing letters, 24:377   380, april 1987.

[behw89] a. blumer, a. ehrenfeucht, d. haussler, and m. k. warmuth. learnabil-
ity and the vapnik-chervonenkis dimension. journal of the association for
computing machinery, 36(4):929   865, 1989.

[ben09]

yoshua bengio. learning deep architectures for ai. foundations and trends
in machine learning, 2(1):1   127, 2009.

[bgmz97] andrei z broder, steven c glassman, mark s manasse, and geo   rey zweig.
syntactic id91 of the web. computer networks and isdn systems,
29(8-13):1157   1166, 1997.

[bgv92]

[bis06]

[ble12]

[blg14]

b. e. boser, i. m. guyon, and v. n. vapnik. a training algorithm for
optimal margin classi   ers. in proceedings of the fifth annual workshop on
computational learning theory, 1992.

christopher m bishop. pattern recognition and machine learning. springer,
2006.

david m. blei. probabilistic topic models. commun. acm, 55(4):77   84,
2012.

maria-florina balcan, yingyu liang, and pramod gupta. robust hierar-
chical id91. journal of machine learning research, 15(1):3831   3871,
2014.

471

[blo62]

[bm98]

[bm02]

h.d. block. the id88: a model for brain functioning. reviews of
modern physics, 34:123   135, 1962. reprinted in neurocomputing, anderson
and rosenfeld.

a. blum and t. mitchell. combining labeled and unlabeled data with co-
training.
in conference on learning theory (colt). morgan kaufmann
publishers, 1998.

p. l. bartlett and s. mendelson. rademacher and gaussian complexities:
risk bounds and structural results. journal of machine learning research,
3:463   482, 2002.

[bm07]

a. blum and y. mansour. from external to internal regret. journal of
machine learning research, 8:1307   1324, 2007.

[bmpw98] sergey brin, rajeev motwani, lawrence page, and terry winograd. what
can you do with a web in your pocket? data engineering bulletin, 21:37   47,
1998.

[bmz05]

alfredo braunstein, marc m  ezard, and riccardo zecchina. survey propa-
gation: an algorithm for satis   ability. random structures & algorithms,
27(2):201   226, 2005.

[bnj03]

david m. blei, andrew y. ng, and michael i. jordan. latent dirichlet
allocation. journal of machine learning research, 3:993   1022, 2003.

[bol01]

b  ela bollob  as. random graphs. cambridge university press, 2001.

[bss08]

[bt87]

[bu14]

[bvz98]

mohsen bayati, devavrat shah, and mayank sharma. max-product for max-
imum weight matching: convergence, correctness, and lp duality.
ieee
transactions on id205, 54(3):1241   1251, 2008.

b  ela bollob  as and andrew thomason. threshold functions. combinatorica,
7(1):35   38, 1987.

maria-florina balcan and ruth urner. active learning - modern learning
theory, pages 1   6. springer berlin heidelberg, berlin, heidelberg, 2014.

yuri boykov, olga veksler, and ramin zabih. markov random    elds with
e   cient approximations. in id161 and pattern recognition, 1998.
proceedings. 1998 ieee computer society conference on, pages 648   655.
ieee, 1998.

[cbfh+97] n. cesa-bianchi, y. freund, d.p. helmbold, d. haussler, r.e. schapire, and
m.k. warmuth. how to use expert advice. journal of the acm, 44(3):427   
485, 1997.

472

[cd10]

kamalika chaudhuri and sanjoy dasgupta. rates of convergence for the
cluster tree. in advances in neural information processing systems, pages
343   351, 2010.

[cf86]

ming-te chao and john v. franco. probabilistic analysis of two heuristics
for the 3-satis   ability problem. siam j. comput., 15(4):1106   1118, 1986.

[chk+01] duncan s. callaway, john e. hopcroft, jon m. kleinberg, m. e. j. newman,
and steven h. strogatz. are randomly grown graphs really random? phys.
rev. e, 64((issue 4)), 2001.

[chv92]

[csz06]

[cv95]

[das11]

[de03]

33rd annual symposium on foundations of computer science, 24-27 octo-
ber 1992, pittsburgh, pennsylvania, usa. ieee, 1992.

o. chapelle, b. sch  olkopf, and a. zien, editors. semi-supervised learning.
mit press, cambridge, ma, 2006.

c. cortes and v. vapnik. support-vector networks. machine learning,
20(3):273     297, 1995.

sanjoy dasgupta. two faces of active learning. theor. comput. sci.,
412(19):1767   1781, april 2011.

david l. donoho and michael elad. optimally sparse representation in
general (nonorthogonal) dictionaries via (cid:96)1 minimization. proceedings of the
national academy of sciences, 100(5):2197   2202, 2003.

[dfk91]

martin dyer, alan frieze, and ravindran kannan. a random polynomial
time algorithm for approximating the volume of convex bodies. journal of
the association for computing machinary, 38:1   17, 1991.

[dg99]

sanjoy dasgupta and anupam gupta. an elementary proof of the johnson-
lindenstrauss lemma. 99(006), 1999.

[dkm06a] petros drineas, ravi kannan, and michael w mahoney. fast monte carlo
algorithms for matrices i: approximating id127. siam jour-
nal on computing, 36(1):132   157, 2006.

[dkm06b] petros drineas, ravi kannan, and michael w mahoney. fast monte carlo
algorithms for matrices ii: computing a low-rank approximation to a matrix.
siam journal on computing, 36(1):158   183, 2006.

[don06]

[ds84]

david l donoho. compressed sensing. ieee transactions on information
theory, 52(4):1289   1306, 2006.

peter g. doyle and j. laurie snell. id93 and electric networks,
volume 22 of carus mathematical monographs. mathematical association
of america, washington, dc, 1984.

473

[ds03]

[ds07]

[er60]

david l. donoho and victoria stodden. when does non-negative matrix
factorization give a correct decomposition into parts? in advances in neural
information processing systems 16 (nips), pages 1141   1148, 2003.

sanjoy dasgupta and leonard j. schulman. a probabilistic analysis of em
for mixtures of separated, spherical gaussians. journal of machine learning
research, 8:203   226, 2007.

paul erd  os and alfred r  enyi. on the evolution of random graphs. publi-
cation of the mathematical institute of the hungarian academy of sciences,
5:17   61, 1960.

[fcmr08] maurizio filippone, francesco camastra, francesco masulli, and stefano
rovetta. a survey of kernel and id106 for id91. pattern
recognition, 41(1):176   190, 2008.

[fd07]

[fk99]

[fk00]

[fk15]

[fkv04]

[f(cid:32)lp+51]

[fm85]

[fri99]

[fs96]

brendan j frey and delbert dueck. id91 by passing messages between
data points. science, 315(5814):972   976, 2007.

alan m. frieze and ravindan kannan. quick approximation to matrices
and applications. combinatorica, 19(2):175   220, 1999.

brendan j frey and ralf koetter. exact id136 using the attenuated
max-product algorithm. advanced mean    eld methods: theory and practice,
2000.

a. frieze and m. karo  nski. introduction to random graphs. cambridge
university press, 2015.

alan frieze, ravi kannan, and santosh vempala. fast monte-carlo algo-
rithms for    nding low-rank approximations. journal of the acm (jacm),
51(6):1025   1041, 2004.

k florek, j (cid:32)lukaszewicz, j perkal, hugo steinhaus, and s zubrzycki. sur
la liaison et la division des points d   un ensemble    ni. in colloquium mathe-
maticae, volume 2, pages 282   285, 1951.

philippe flajolet and g nigel martin. probabilistic counting algorithms for
data base applications. journal of computer and system sciences, 31(2):182   
209, 1985.

friedgut. sharp thresholds of graph properties and the k-sat problem. jour-
nal of the american math. soc., 12, no 4:1017   1054, 1999.

alan m. frieze and stephen suen. analysis of two simple heuristics on a
random instance of k-sat. j. algorithms, 20(2):312   355, 1996.

474

[fs97]

[geb15]

[gha01]

y. freund and r. schapire. a decision-theoretic generalization of on-line
learning and an application to boosting. journal of computer and system
sciences, 55(1):119   139, 1997.

leon a. gatys, alexander s. ecker, and matthias bethge. a neural algo-
rithm of artistic style. corr, abs/1508.06576, 2015.

zoubin ghahramani. an introduction to id48 and bayesian
networks. international journal of pattern recognition and arti   cial intelli-
gence, 15(01):9   42, 2001.

[gib73]

a. gibbard. manipulation of voting schemes: a general result. econometrica,
41:587   601, 1973.

[gkl+15]

[gkss08]

[gls12]

[gn03]

[gon85]

[gvl96]

[gw95]

jacob r. gardner, matt j. kusner, yixuan li, paul upchurch, kilian q.
weinberger, and john e. hopcroft. deep manifold traversal: changing labels
with convolutional features. corr, abs/1511.06421, 2015.

carla p. gomes, henry a. kautz, ashish sabharwal, and bart selman. sat-
is   ability solvers. handbook of id99, pages 89   134,
2008.

martin gr  otschel, l  aszl  o lov  asz, and alexander schrijver. geometric algo-
rithms and combinatorial optimization, volume 2. springer science & busi-
ness media, 2012.

r  emi gribonval and morten nielsen. sparse decompositions in    incoherent   
dictionaries. in proceedings of the 2003 international conference on image
processing, icip 2003, barcelona, catalonia, spain, september 14-18, 2003,
pages 33   36, 2003.

teo   lo f gonzalez. id91 to minimize the maximum intercluster dis-
tance. theoretical computer science, 38:293   306, 1985.

gene h. golub and charles f. van loan. matrix computations (3. ed.).
johns hopkins university press, 1996.

michel x goemans and david p williamson. improved approximation al-
gorithms for maximum cut and satis   ability problems using semide   nite
programming. journal of the acm (jacm), 42(6):1115   1145, 1995.

[id48r15] christian hennig, marina meila, fionn murtagh, and roberto rocci. hand-

book of cluster analysis. crc press, 2015.

[in77]

db iudin and arkadi s nemirovskii. informational complexity and e   cient
methods for solving complex extremal problems. matekon, 13(3):25   45, 1977.

475

[jai10]

[jer98]

anil k jain. data id91: 50 years beyond id116. pattern recognition
letters, 31(8):651   666, 2010.

mark jerrum. mathematical foundations of the id115
method. in dorit hochbaum, editor, approximation algorithms for np-hard
problems, 1998.

[jklp93]

svante janson, donald e. knuth, tomasz luczak, and boris pittel. the
birth of the giant component. random struct. algorithms, 4(3):233   359,
1993.

[jlr00]

[joa99]

[kan09]

[kar90]

[kfl01]

[kha79]

[kk10]

[kle99]

[kle00]

[ks13]

[kv09]

svante janson, tomasz   luczak, and andrzej ruci  nski. random graphs.
john wiley and sons, inc, 2000.

t. joachims. transductive id136 for text classi   cation using support
vector machines. in international conference on machine learning, pages
200   209, 1999.

ravindran kannan. a new id203 inequality using typical moments and
concentration results. in focs, pages 211   220, 2009.

richard m. karp. the transitive closure of a random digraph. random
structures and algorithms, 1(1):73   94, 1990.

frank r kschischang, brendan j frey, and h-a loeliger. factor graphs
and the sum-product algorithm. ieee transactions on id205,
47(2):498   519, 2001.

leonid g khachiyan. a polynomial algorithm in id135.
akademiia nauk sssr, doklady, 244:1093   1096, 1979.

amit kumar and ravindran kannan. id91 with spectral norm and
the id116 algorithm. in foundations of computer science (focs), 2010
51st annual ieee symposium on, pages 299   308. ieee, 2010.

jon m. kleinberg. authoritative sources in a hyperlinked environment.
journal of the acm, 46(5):604   632, 1999.

jon m. kleinberg. the small-world phenomenon: an algorithm perspective.
in stoc, pages 163   170, 2000.

michael krivelevich and benny sudakov. the phase transition in random
graphs: a simple proof. random struct. algorithms, 43(2):131   138, 2013.

ravi kannan and santosh vempala. spectral algorithms. foundations and
trends in theoretical computer science, 4(3-4):157   288, 2009.

476

[kvv04]

ravi kannan, santosh vempala, and adrian vetta. on id91s: good,
bad and spectral. j. acm, 51(3):497   515, may 2004.

[lcb+04] gert rg lanckriet, nello cristianini, peter bartlett, laurent el ghaoui,
and michael i jordan. learning the kernel matrix with semide   nite pro-
gramming. journal of machine learning research, 5(jan):27   72, 2004.

[lis13]

[lit87]

christian list. social choice theory. in edward n. zalta, editor, the stanford
encyclopedia of philosophy. metaphysics research lab, stanford university,
winter 2013 edition, 2013.

nick littlestone. learning quickly when irrelevant attributes abound: a
new linear-threshold algorithm. in 28th annual symposium on foundations
of computer science, pages 68   77. ieee, 1987.

[liu01]

jun liu. monte carlo strategies in scienti   c computing. springer, 2001.

[llo82]

[lw94]

[mcs01]

[mg82]

[mm02]

[mp69]

[mpz02]

[mr95a]

[mr95b]

[mu05]

stuart lloyd. least squares quantization in pcm.
id205, 28(2):129   137, 1982.

ieee transactions on

n. littlestone and m. k. warmuth. the weighted majority algorithm. in-
formation and computation, 108(2):212   261, 1994.

frank mcsherry. spectral partitioning of random graphs. in focs, pages
529   537, 2001.

jayadev misra and david gries. finding repeated elements. science of
computer programming, 2(2):143   152, 1982.

gurmeet singh manku and rajeev motwani. approximate frequency counts
over data streams. in proceedings of the 28th international conference on
very large data bases, pages 346   357. vldb endowment, 2002.

m. minsky and s. papert. id88s: an introduction to computational
geometry. the mit press, 1969.

marc m  ezard, giorgio parisi, and riccardo zecchina. analytic and algorith-
mic solution of random satis   ability problems. science, 297(5582):812   815,
2002.

michael molloy and bruce a. reed. a critical point for random graphs with
a given degree sequence. random struct. algorithms, 6(2/3):161   180, 1995.

rajeev motwani and prabhakar raghavan. randomized algorithms. cam-
bridge university press, 1995.

michael mitzenmacher and eli upfal. id203 and computing - random-
ized algorithms and probabilistic analysis. cambridge university press, 2005.

477

[mv10]

[nov62]

ankur moitra and gregory valiant. settling the polynomial learnability of
mixtures of gaussians. in focs, pages 93   102, 2010.

a.b.j. noviko   . on convergence proofs on id88s. in proceedings of
the symposium on the mathematical theory of automata, vol. xii, pages
615   622, 1962.

[per10]

markov chains and mixing times. american mathematical society, 2010.

[rv99]

kannan ravi and vinay v. analyzing the structure of large graphs. 1999.

[sat75]

[sch90]

[sho70]

[sj89]

[ss01]

m.a. satterthwaite. strategy-proofness and arrows conditions: existence and
correspondence theorems for voting procedures and social welfare functions.
journal of economic theory, 10:187   217, 1975.

rob schapire. strength of weak learnability. machine learning, 5:197   227,
1990.

naum z shor. convergence rate of the id119 method with dilata-
tion of the space. cybernetics and systems analysis, 6(2):102   108, 1970.

alistair sinclair and mark jerrum. approximate counting, uniform gen-
eration and rapidly mixing markov chains. information and computation,
82:93   133, 1989.

bernhard scholkopf and alexander j. smola. learning with kernels: support
vector machines, id173, optimization, and beyond. mit press,
cambridge, ma, usa, 2001.

[stbwa98] john shawe-taylor, peter l bartlett, robert c williamson, and martin an-
thony. structural risk minimization over data-dependent hierarchies. ieee
transactions on id205, 44(5):1926   1940, 1998.

[swy75]

g. salton, a. wong, and c. s. yang. a vector space model for automatic
indexing. commun. acm, 18:613   620, november 1975.

[thr96]

[tm95]

s. thrun. explanation-based neural network learning: a lifelong learning
approach. kluwer academic publishers, boston, ma, 1996.

sebastian thrun and tom m. mitchell. lifelong robot learning. robotics
and autonomous systems, 15(1-2):25   46, 1995.

[val84]

leslie g. valiant. a theory of the learnable. in stoc, pages 436   445, 1984.

[vap82]

[vap98]

v. n. vapnik. estimation of dependences based on empirical data.
springer-verlag, new york, 1982.

v. n. vapnik. statistical learning theory. john wiley and sons inc., new
york, 1998.

478

[vc71]

v. vapnik and a. chervonenkis. on the uniform convergence of relative
frequencies of events to their probabilities. theory of id203 and its
applications, 16(2):264   280, 1971.

[vem04]

santosh vempala. the random projection method. dimacs, 2004.

[vw02]

[war63]

[wei97]

[wf01]

[wis69]

[ws98]

[yfw01]

[yfw03]

[zgl03]

santosh vempala and grant wang. a spectral algorithm for learning mix-
tures of distributions. journal of computer and system sciences, pages
113   123, 2002.

j.h. ward. hierarchical grouping to optimize an objective function. journal
of the american statistical association, 58(301):236   244, 1963.

yair weiss. belief propagation and revision in networks with loops. technical
report a.i. memo no. 1616, mit, 1997.

yair weiss and william t freeman. on the optimality of solutions of the
max-product belief-propagation algorithm in arbitrary graphs. ieee trans-
actions on id205, 47(2):736   744, 2001.

david wishart. mode analysis: a generalization of nearest neighbor which
reduces chaining e   ects. numerical taxonomy, 76(282-311):17, 1969.

d. j. watts and s. h. strogatz. collective dynamics of    small-world    net-
works. nature, 393 (6684), 1998.

jonathan s yedidia, william t freeman, and yair weiss. bethe free energy,
kikuchi approximations, and belief propagation algorithms. advances in
neural information processing systems, 13, 2001.

jonathan s yedidia, william t freeman, and yair weiss. understanding
belief propagation and its generalizations. exploring arti   cial intelligence in
the new millennium, 8:236   239, 2003.

x. zhu, z. ghahramani, and j. la   erty. semi-supervised learning using
gaussian    elds and id94. in proc. 20th international confer-
ence on machine learning, pages 912   912, 2003.

[zhu06]

x. zhu. semi-supervised learning literature survey. 2006. computer sciences
tr 1530 university of wisconsin - madison.

479

