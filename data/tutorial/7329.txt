2 id114 in a nutshell

daphne koller, nir friedman, lise getoor and ben taskar

probabilistic id114 are an elegant framework which combines uncer-
tainty (probabilities) and logical structure (independence constraints) to compactly
represent complex, real-world phenomena. the framework is quite general in that
many of the commonly proposed statistical models (kalman    lters, hidden markov
models, ising models) can be described as id114. id114 have
enjoyed a surge of interest in the last two decades, due both to the    exibility and
power of the representation and to the increased ability to e   ectively learn and
perform id136 in large networks.

2.1 introduction

id114 [11, 3, 5, 9, 7] have become an extremely popular tool for mod-
eling uncertainty. they provide a principled approach to dealing with uncertainty
through the use of id203 theory, and an e   ective approach to coping with
complexity through the use of id207. the two most common types of graph-
ical models are id110s (also called belief networks or causal networks)
and markov networks (also called markov random    elds (mrfs)).
at a high level, our goal is to e   ciently represent a joint distribution p over
some set of random variables x = {x1, . . . , xn}. even in the simplest case where
these variables are binary-valued, a joint distribution requires the speci   cation of
2n numbers     the probabilities of the 2n di   erent assignments of values x1, . . . , xn.
however, it is often the case that there is some structure in the distribution that
allows us to factor the representation of the distribution into modular components.
the structure that id114 exploit is the independence properties that
exist in many real-world phenomena.

the independence properties in the distribution can be used to represent such
high-dimensional distributions much more compactly. probabilistic graphical mod-
els provide a general-purpose modeling language for exploiting this type of structure
in our representation. id136 in probabilistic id114 provides us with

14

id114 in a nutshell

the mechanisms for gluing all these components back together in a probabilistically
coherent manner. e   ective learning, both parameter estimation and model selec-
tion, in probabilistic id114 is enabled by the compact parameterization.
this chapter provides a compact id114 tutorial based on [8]. we cover
representation, id136, and learning. our tutorial is not comprehensive; for more
details see [8, 11, 3, 5, 9, 4, 6].

2.2 representation

the two most common classes of id114 are id110s and
markov networks. the underlying semantics of id110s are based on
directed graphs and hence they are also called directed id114. the
underlying semantics of markov networks are based on undirected graphs; markov
networks are also called undirected id114. it is possible, though less
common, to use a mixed directed and undirected representation (see, for example,
the work on chain graphs [10, 2]); however, we will not cover them here.
basic to our representation is the notion of conditional independence:

de   nition 2.1
let x, y , and z be sets of random variables. x is conditionally independent of
y given z in a distribution p if

p (x = x, y = y | z = z) = p (x = x | z = z)p (y = y | z = z)

for all values x     v al(x), y     v al(y ) and z     v al(z).
in the case where p is understood, we use the notation (x     y | z) to say that x
is conditionally independent of y given z. if it is clear from the context, sometimes
we say    independent    when we really mean    conditionally independent   .

2.2.1 id110s

the core of the id110 representation is a directed acyclic graph (dag)
g. the nodes of g are the random variables in our domain and the edges correspond,
intuitively, to direct in   uence of one node on another. one way to view this graph is
as a data structure that provides the skeleton for representing the joint distribution
compactly in a factorized way.
let g be a bn graph over the variables x1, . . . , xn. each random variable xi
in the network has an associated id155 distribution (cpd) or local
probabilistic model. the cpd for xi, given its parents in the graph (denoted paxi),
is p (xi | paxi). it captures the id155 of the random variable,
given its parents in the graph. cpds can be described in a variety of ways. a
common, but not necessarily compact, representation for a cpd is a table which
contains a row for each possible set of values for the parents of the node describing

2.2 representation

15

pneumonia
pneumonia

tuberculosis
tuberculosis

lung infiltrates
lung infiltrates

xray
xray

sputum smear
sputum smear

tp

p(i |p, t )

p
p
p
p

t
t
t
t

i
i

i
i
i
i

0.8

0.6

0.2

0.01

p(x|i )
p(x|i )

0.8
0.8

0.6
0.6

p(p)

0.05

p 

p(t)

0.02

t

i 

s

s
s

p(s|t )

0.8

0.6

x 

s 

(a)

(b)

figure 2.1 (a) a simple id110 showing two potential diseases, pneu-
monia and tuberculosis, either of which may cause a patient to have lung in   ltrates.
the lung in   ltrates may show up on an xray; there is also a separate sputum
smear test for tuberculosis. all of the random variables are boolean. (b) the same
id110, together with the id155 tables. the probabili-
ties shown are the id203 that the random variable takes the value true (given
the values of its parents); the id155 that the random variable is
false is simply 1 minus the id203 that it is true.

the id203 of di   erent values for xi. these are often referred to as table cpds,
and are tables of multinomial distributions. other possibilities are to represent
the distributions via a tree structure (called, appropriately enough, tree-structured
cpds), or using an even more compact representation such as a noisy-or or noisy-
max.
example 2.1
consider the simple id110 shown in    gure 2.1. this is a toy example
indicating the interactions between two potential diseases, pneumonia and tuber-
culosis. both of them may cause a patient to have lung in   ltrates. there are two
tests that can be performed. an x-ray can be taken, which may indicate whether
the patient has lung in   ltrates. there is a separate sputum smear test for tubercu-
losis.    gure 2.1(a) shows the dependency structure among the variables. all of the
variables are assumed to be boolean.    gure 2.1(b) shows the id155
distributions for each of the random variables. we use initials p , t , i, x, and s
for shorthand. at the roots, we have the prior id203 of the patient having
each disease. the id203 that the patient does not have the disease a priori
is simply 1 minus the id203 he or she has the disease; for simplicity only the
probabilities for the true case are shown. similarly, the conditional probabilities
for the non-root nodes give the id203 that the random variable is true, for
di   erent possible instantiations of the parents.

16

id114 in a nutshell

n(cid:2)

de   nition 2.2
let g be a bayesinan network graph over the variables x1, . . . , xn. we say that a
distribution pb over the same space factorizes according to g if pb can be expressed
as a product

pb(x1, . . . , xn) =

p (xi | paxi).

(2.1)

i=1

a id110 is a pair (g,   g) where pb factorizes over g, and where pb is
speci   ed as set of cpds associated with g   s nodes, denoted   g.
the equation above is called the chain rule for id110s. it gives us a
method for determining the id203 of any complete assignment to the set of
random variables: any entry in the joint can be computed as a product of factors,
one for each variable. each factor represents a id155 of the variable
given its parents in the network.
example 2.2
the id110 in    gure 2.1(a) describes the following factorization:

p (p, t, i, x, s) = p (p )p (t )p (i | p, t )p (x | i)p (s | t ).

sometimes it is useful to think of the id110 as describing a generative
process. we can view the graph as encoding a generative sampling process executed
by nature, where the value for each variable is selected by nature using a distribution
that depends only on its parents. in other words, each variable is a stochastic
function of its parents.

2.2.2 conditional independence assumptions in id110s

another way to view a id110 is as a compact representation for a set
of conditional independence assumptions about a distribution. these conditional
independence assumptions are called the local markov assumptions. while we won   t
go into the full details here, this view is, in a strong sense, equivalent to the view
of the id110 as providing a factorization of the distribution.

de   nition 2.3

given a bn network structure g over random variables x1, . . . , xn, let nondescendantsxi
denote the variables in the graph that are not descendants of xi. then g encodes
the following set of conditional independence assumptions, called the local markov
assumptions:

for each variable xi, we have that

(xi     nondescendantsxi

| paxi),

in other words, the local markov assumptions state that each node xi is inde-
pendent of its nondescendants given its parents.

2.2 representation

17

x

z

y

y

z

x

z

x

y

x

y

(a)

(b)

(c)

z

(d)

figure 2.2 (a) an indirect causal e   ect; (b) an indirect evidential e   ect; (c) a
common cause; (d) a common e   ect.

example 2.3
the bn in    gure 2.1(a) describes the following local markov assumptions: (p    
t |    ), (t     p |    ), (x     {p, t, s} | i), and (s     {p, i, x} | t ).

these are not the only independence assertions that are encoded by a network.
a general procedure called d-separation (which stands for directed separation) can
answer whether an independence assertion must hold in any distribution consistent
with the graph g. however, note that other independencies may hold in some
distributions consistent with g; these are due to    ukes in the particular choice of
parameters of the network (and this is why they hold in some of the distributions).
returning to our de   nition of d-separation, it is useful to view probabilistic
in   uence as a    ow in the graph. our analysis here tells us when in   uence from
x can       ow    through z to a   ect our beliefs about y . we will consider    ow allows
(undirected) paths in the graph.

consider a simple three-node path x   y    z if in   uence can    ow from x to y

via z, we say that the path x   z   y is active. there are four cases:
causal path x     z     y : active if and only if z is not observed.
evidential path x     z     y : active if and only if z is not observed.
common cause x     z     y : active if and only if z is not observed.
common e   ect x     z     y : active if and only if either z or one of z   s
descendants is observed.
a structure where x     z     y (as in    gure 2.2(d)) is also called a v-structure.
example 2.4
in the bn from    gure 2.1(a), the path from p     i     x is active if i is not
observed. on the other hand, the path from p     i     t is active if i is observed.
now consider a longer path x1             xn. intuitively, for in   uence to       ow   
from x1 to xn, it needs to    ow through every single node on the trail. in other
words, x1 can in   uence xn if every two-edge path xi   1   xi   xi+1 along the trail
allows in   uence to    ow. we can summarize this intuition in the following de   nition:

18

id114 in a nutshell

de   nition 2.4

let g be a bn structure, and x1    . . .    xn a path in g. let e be a subset of
nodes of g. the path x1    . . .    xn is active given evidence e if
whenever we have a v-structure xi   1     xi     xi+1, then xi or one of its
descendants is in e;
no other node along the path is in e.

our    ow intuition carries through to graphs in which there is more than one
path between two nodes: one node can in   uence another if there is any path along
which in   uence can    ow. putting these intuitions together, we obtain the notion
of d-separation, which provides us with a notion of separation between nodes in a
directed graph (hence the term d-separation, for directed separation):

de   nition 2.5
let x, y , z be three sets of nodes in g. we say that x and y are d-separated
given z, denoted d-sepg(x; y | z), if there is no active path between any node
x     x and y     y given z.

finally, an important theorem which relates the independencies which hold in a

distribution to the factorization of a distribution is the following:

theorem 2.6
let g be a bn graph over a set of random variables x and let p be a joint
distribution over the same space. if all the local markov properties associated with
g hold in p , then p factorizes according to g.

theorem 2.7
let g be a bn graph over a set of random variables x and let p be a joint
distribution over the same space. if p factorizes according to g, then all the local
markov properties associated with g hold in p .

2.2.3 markov networks

the second common class of probabilistic id114 is called a markov net-
work or a markov random    eld. the models are based on undirected graphical
models. these models are useful in modeling a variety of phenomena where one
cannot naturally ascribe a directionality to the interaction between variables. fur-
thermore, the undirected models also o   er a di   erent and often simpler perspective
on directed models, both in terms of the independence structure and the id136
task.

a representation that implements this intuition is that of an undirected graph.
as in a id110, the nodes in the graph of a markov network graph
h represent the variables, and the edges correspond to some notion of direct
probabilistic interaction between the neighboring variables.

the remaining question is how to parameterize this undirected graph. the graph
structure represents the qualitative properties of the distribution. to represent the

2.2 representation

19

distribution, we need to associate the graph structure with a set of parameters, in
the same way that cpds were used to parameterize the directed graph structure.
however, the parameterization of markov networks is not as intuitive as that of
id110s, as the factors do not correspond either to probabilities or to
conditional probabilities.

the most general parameterization is a factor:

de   nition 2.8
let d be a set of random variables. we de   ne a factor to be a function from
val(d) to ir+.

de   nition 2.9
let h be a markov network structure. a distribution ph factorizes over h if it is
associated with
a set of subsets d1, . . . , dm, where each di is a complete subgraph of h;
factors   1[d1], . . . ,   m[dm],

such that

where

ph(x1, . . . , xn) =

(cid:2)

p

1
z

(x1, . . . , xn),

h(x1, . . . , xn) =   i[d1]      2[d2]                m[dm]
(cid:2)

p

is an unnormalized measure and

z =

(cid:3)

(cid:2)
h(x1, . . . , xn)
p

x1,...,xn

is a normalizing constant called the partition function. a distribution p that
factorizes over h is also called a gibbs distribution over h. (the naming convention
has roots in statistical physics.)

note that this de   nition is quite similar to the factorization de   nition for
id110s: there, we decomposed the distribution as a product of cpds.
in the case of markov networks, the only constraint on the parameters in the factor
is non-negativity.

as every complete subgraph is a subset of some clique, we can simplify the
parameterization by introducing factors only for cliques, rather than for subcliques.
more precisely, let c 1, . . . , ck be the cliques in h. we can parameterize p using a
set of factors   1[c 1], . . . ,   k[ck]. these factors are called clique potentials (in the
context of the markov network h). it is tempting to think of the clique potentials
as representing the marginal probabilities of the variables in their scope. however,
this is incorrect. it is important to note that, although conceptually somewhat
simpler, the parameterization using clique potentials can obscure the structure that

20

id114 in a nutshell

is present in the original parameterization, and can possibly lead to an exponential
increase in the size of the representation.

it is often useful to consider a slightly di   erent way of specifying potentials, by
using a logarithmic transformation. in particular, we can rewrite a factor   [d] as

  [d] = exp(    [d]),

where  [d] =     ln   [d] is often called an energy function. the use of the word
   energy    derives from statistical physics, where the id203 of a physical state
(e.g., a con   guration of a set of electrons), depends inversely on its energy.

in this logarithmic representation, we have that

ph(x1, . . . , xn)     exp

 i[di]

.

(cid:4)
    m(cid:3)

(cid:5)

i=1

the logarithmic representation ensures that the id203 distribution is posi-

tive. moreover, the logarithmic parameters can take any real value.

a subclass of markov networks that arises in many contexts is that of pairwise
markov networks, representing distributions where all of the factors are over single
variables or pairs of variables. more precisely, a pairwise markov network over a
graph h is associated with a set of node potentials {  [xi] : i = 1, . . . , n} and a set of
edge potentials {  [xi, xj] : (xi, xj)     h}. the overall distribution is (as always)
the normalized product of all of the potentials (both node and edge). pairwise
mrfs are attractive because of their simplicity, and because interactions on edges
are an important special case that often arises in practice.
example 2.5
figure 2.3(a) shows a simple markov network. this toy example has random
variables describing the tuberculosis status of four patients. patients that have been
in contact are linked by undirected edges. the edges indicate the possibilities for the
disease transmission. for example, p atient 1 has been in contact with p atient 2
and p atient 3, but has not been in contact with p atient 4.    gure 2.3(b) shows the
same markov network, along with the node and edge potentials. we use p 1, p 2,
p 3, and p 4 for shorthand. in this case, all of the node and edge potentials are the
same, but this is not a requirement. the node potentials show that the patients
are much more likely to be uninfected. the edge potentials capture the intuition
that it is most likely for two people to have the same infection state     either both
infected, or both not. furthermore, it is more likely that they are both not infected.

2.2.4

independencies in markov networks

as in the case of id110s, the graph structure in a markov network can
be viewed as encoding a set of independence assumptions. intuitively, in markov
networks, probabilistic in   uence       ows    along the undirected paths in the graph,
but is blocked if we condition on the intervening nodes. we can de   ne two sets

2.2 representation

21

p1
p1
p1
p1
p1
p1

  (p1
  (p1

)
)

0.2
0.2

100
100

  (p1 , p3 )

1
0.5
0.5
2

p3
p3
p3

  (p3

)

0.2

100

p1
p1
p1
p1
p1

p3
p3
p3
p3
p3

p2

p4 

p2
p2
p2

  (p2

)

0.2

100

p2
p2
p2
p2
p2

p4
p4
p4
p4
p4

  (p2 , p4 )

1
0.5
0.5
2

p4
p4
p4

  (p4

)

0.2

100

p1
p1
p1
p1
p1

p2
p2
p2
p2
p2

  (p1 , p2 )

1
0.5
0.5
2

p1 

p3 

p3
p3
p3
p3
p3

p4
p4
p4
p4
p4

  (p3 , p4 )

1
0.5
0.5
2

(b)

tb patient 1

tb patient 2

tb patient 3

tb patient 4

(a)

figure 2.3 (a) a simple markov network describing the tuberculosis status of four
patients. the links between patients indicate which patients have been in contact
with each other. (b) the same markov network, together with the node and edge
potentials.

of independence assumptions, the local markov properties and the global markov
properties.

the local markov properties are associated with each node in the graph and are
based on the intuition that we can block all in   uences on a node by conditioning
on its immediate neighbors.

de   nition 2.10
let h be an undirected graph. then for each node x     x , the markov blanket of
x, denoted nh(x), is the set of neighbors of x in the graph (those that share an
edge with x). we de   ne the local markov independencies associated with h to be

i(cid:2)(h) = {(x     x     {x}     nh(x) | nh(x)) : x     x}.

in other words, the markov assumptions state that x is independent of the rest of
the nodes in the graph given its immediate neighbors.
example 2.6
the mn in    gure 2.3(a) describes the following local markov assumptions: (p1    
p4 | {p2, p3}), (p2     p3 | {p1, p4}), (p3     p2 | {p1, p4}), (p4     p1 | {p2, p3}).

to de   ne the global markov properties, we begin by de   ning active paths in

undirected graphs.

de   nition 2.11

let h be a markov network structure, and x1    . . .    xk be a path in h. let
e     x be a set of observed variables. the path x1    . . .    xk is active given e if
none of the xi   s, i = 1, . . . , k, is in e.

22

id114 in a nutshell

using this notion, we can de   ne a notion of separation in the undirected graph.

this is the analogue of d-separation; note how much simpler it is.

de   nition 2.12
we say that a set of nodes z separates x and y in h, denoted seph(x; y | z),
if there is no active path between any node x     x and y     y given z. we de   ne
the global markov assumptions associated with h to be

i(h) = {(x     y | z) : seph(x; y | z)}.

as in the case of id110s, we can make a connection between the local
markov properties and the global markov properties. the assumptions are in fact
equivalent, but only for positive distributions. (informally, a distribution is positive
if every possible joint instantiation has id203 > 0.)

we begin with the analogue to theorem 2.7, which asserts that a gibbs distribu-

tion satis   es the global independencies associated with the graph.

theorem 2.13
let p be a distribution over x , and h a markov network structure over x . if p is
a gibbs distribution over h, then all the local markov properties associated with
h hold in p .

the other direction, which goes from the global independence properties of a
distribution to its factorization, is known as the hammersley-cli   ord theorem.
unlike for id110s, this direction does not hold in general. it only holds
under the additional assumption that p is a positive distribution.

theorem 2.14
let p be a positive distribution over x , and h a markov network graph over x .
if all of the independence constraints implied by h hold in p , then p is a gibbs
distribution over h.

this result shows that, for positive distributions, the global markov property
implies that the distribution factorizes according to the network structure. thus,
for this class of distributions, we have that a distribution p factorizes over a markov
network h if and only if all of the independencies implied by h hold in p . the
positivity assumption is necessary for this result to hold.

2.3 id136

both directed and undirected id114 represent a full joint id203
distribution over x . we describe some of the main query types one might expect
to answer with a joint distribution, and discuss the computational complexity of
answering such queries using a graphical model.
the most common query type is the standard id155 query,
p (y | e = e). such a query consists of two parts: the evidence, a subset e of

2.3 id136

23

random variables in the network, and an instantiation e to these variables; and
the query, a subset y of random variables in the network. our task is to compute
p (y | e = e) = p (y ,e)
p (e) , i.e., the id203 distribution over the values y of y ,
conditioned on the fact that e = e.

another type of query that often arises is that of    nding the most probable
assignment to some subset of variables. as with id155 queries,
we have evidence e = e. in this case, however, we are trying to compute the most
likely assignment to some subset of the remaining variables. this problem has two
variants, where the    rst variant is an important special case of the second. the
simplest variant of this task is the most probable explanation (mpe) queries. an
mpe query tries to    nd the most likely assignment to all of the (non-evidence)
variables. more precisely, if we let w = x     e, our task is to    nd the most likely
assignment to the variables in w given the evidence e = e: argmaxwp (w, e),
where, in general, argmaxxf(x) represents the value of x for which f(x) is maximal.
note that there might be more than one assignment that has the highest posterior
id203. in this case, we can either decide that the mpe task is to return the
set of possible assignments, or to return an arbitrary member of that set.

(cid:6)

in the second variant, the maximum a posteriori (map) query, we have a
subset of variables y which forms our query. the task is to    nd the most likely
assignment to the variables in y given the evidence e = e: argmaxyp (y | e).
this class of queries is clearly more general than mpe queries, so it might not
be clear why the class of mpe queries is su   ciently interesting to consider as a
special case. the di   erence becomes clearer if we explicitly write out the expression
for a general map query. if we let z = x     y     e, the map task is to
z p (y , z | e). map queries contain both summations and
compute: argmaxy
maximizations; in a way, they contain elements of both a id155
query and an mpe query. this combination makes the map task harder than
either of these other tasks. in particular, there are techniques and analysis for the
mpe task that do not generalize to the map task. this observation, combined
with the fact that the mpe case is reasonably common, makes it worthwhile to
consider mpe as a separate task. note that in statistics literature, as well as in
some work on id114, the term map is often used to mean mpe, but
the distinction can be made clear from the context.

in principle, a graphical model can be used to answer all of the query types
described above. we simply generate the joint distribution, and exhaustively sum
out the joint (in the case of a id155 query), search for the most
likely entry (in the case of an mpe query), or both (in the case of an map query).
however, this approach to the id136 problem is not very satisfactory, as it
results in the exponential blowup of the joint distribution that the graphical model
representation was precisely designed to avoid.

24

id114 in a nutshell

we assume that we are dealing with a set of factors f over a set of variables x .

this set of factors de   nes a possibly unnormalized function

pf (x ) =

(cid:2)

  .

     f

(2.2)

for a id110 without evidence, the factors are simply the cpds, and the
distribution pf is a normalized distribution. for a id110 b with evi-
dence e = e, the factors are the cpds restricted to e, and pf (x ) = pb(x , e). for
a markov network h (with or without evidence), the factors are the (restricted)
(cid:2)
compatibility potentials, and pf is the unnormalized distribution p
h before di-
viding by the partition function. it is important to note, however, that most of
the operations that one can perform on a normalized distribution can also be per-
formed on an unnormalized one. thus, we can marginalize pf on a subset of the
variables by summing out the others. we can also consider a id155
pf (x | y ) = pf (x, y )/pf (y ). thus, for the purposes of this section, we treat
pf as a distribution, ignoring the fact that it may not be normalized.
in the worst case, the complexity of probabilistic id136 is unavoidable. below,
we assume that the set of factors {       f} of the graphical model de   ning the desired
distribution can be speci   ed in a polynomial number of bits (in terms of the number
of variables).

theorem 2.15
the following decision problems are np-complete:
given a distribution pf over x , a variable x     x , and a value x     val(x),
decide whether pf (x = x) > 0.
given a distribution pf over x and a number   , decide whether there exists an
assignment x to x such that pf (x) >   .
the following problem is #p-complete:
given a distribution pf over x , a variable x     x , and a value x     val(x),
compute pf (x = x).

these results seem like very bad news: every type of id136 in graphical
models is np-hard or harder. in fact, even the simple problem of computing
the distribution over a single binary variable is np-hard. assuming (as seems
increasingly likely) that the best computational performance we can achieve for
np-hard problems is exponential in the worst case, there seems to be no hope for
e   cient algorithms for even the simplest type of id136. however, as we discuss
below, the worst-case blowup can often be avoided. for all other models, we will
resort to approximate id136 techniques. note that the worst-case results for
approximate id136 are also negative:

2.3 id136

25

theorem 2.16
the following problem is np-hard for any       (0, 1/2): given a distribution pf
over x , a variable x     x , and a value x     val(x),    nd a number   , such that
|pf (x = x)       |      .

fortunately, many types of exact id136 can be performed e   ciently for a
very important class of id114 (low treewidth) we de   ne below. for a
large number of models, however, exact id136 is intractable and we resort to
approximations. broadly speaking, there are two major frameworks for probabilistic
id136: optimization-based and sampling-based. exact id136 algorithms have
been historically derived from the id145 perspective, by carefully
avoiding repeated computations. we take a somewhat unconventional approach here
by presenting exact and approximate id136 in a uni   ed optimization framework.
we thus start out by considering approximate id136 and then present conditions
under which it yields exact results.

2.3.1

id136 as optimization

the methods that fall into an optimization framework are based on a simple
conceptual principle: de   ne a target class of    easy    distributions q, and then search
for a particular instance q within that class which is the    best    approximation to
pf . queries can then be answered using id136 on q rather than on pf. the
speci   c algorithms that have been considered in the literature di   er in many details.
however, most of them can be viewed as optimizing a target function for measuring
the quality of approximation.

suppose that we want to approximate pf with another distribution q. intuitively,
we want to choose the approximation q to be close to pf . there are many
possible ways to measure the distance between two distributions, such as the
euclidean distance (l2), or the l1 distance. our main challenge, however, is that
our aim is to avoid performing id136 with the distribution pf ; in particular, we
cannot e   ectively compute marginal distributions in pf . hence, we need methods
that allow us to optimize the distance (technically, divergence) between q and
pf without answering hard queries in pf . a priori, this requirement may seem
impossible to satisfy. however, it turns out that there exists a distance measure    
the relative id178 (or kl-divergence)     that allows us to exploit the structure
(cid:7)
of pf without performing reasoning with it.
recall that the relative id178 between p1 and p2 is de   ned as id(p1||p2) =
. the relative id178 is always non-negative, and equal to 0 if and
iep1
only if p1 = p2. thus, we can use it as a distance measure, and choose to    nd an
approximation q to pf that minimizes the relative id178. however, the relative
id178 is not symmetric     id(p1||p2) (cid:10)= id(p2||p1). a priori, it might appear that
id(pf||q) is a more appropriate measure for approximate id136, as one of the
main information-theoretic justi   cations for relative id178 is the number of bits
lost when coding a true message distribution pf using an (approximate) estimate q.

ln p1(x )
p2(x )

(cid:8)

26

id114 in a nutshell

however, computing the so-called m-projection q of pf     the argminqid(pf||q)
    is actually equivalent to running id136 in pf . somewhat surprisingly, as we
show in the subsequent discussion, this does not apply to the so-called i-projection:
we can exploit the structure of pf to optimize argminqid(q||pf ) e   ciently, without
running id136 in pf .
an additional reason for using relative id178 as our distance measure is based
on the following result, which relates the relative id178 id(q||pf ) with the
partition function z:

proposition 2.17

where f [pf , q] is the energy functional f [pf , q] =

ln z = f [pf , q] + id(q||pf ),
(cid:6)

(2.3)

     f ieq[ln   ] + ihq(x ).

this proposition has several important rami   cations. note that the term ln z does
not depend on q. hence, minimizing the relative id178 id(q||pf ) is equivalent
to maximizing the energy functional f [pf , q]. this latter term relates to concepts
from statistical physics, and it is the negative of what is referred to in that    eld as
the helmholtz free energy. while explaining the physics-based motivation for this
term is out of the scope of this chapter, we continue to use the standard terminology
of energy functional.

in the remainder of this section, we pose the problem of    nding a good approxi-
mation q as one of maximizing the energy functional, or, equivalently, minimizing
the relative id178. importantly, the energy functional involves expectations in q.
as we show, by choosing approximations q that allow for e   cient id136, we can
both evaluate the energy functional and optimize it e   ectively.
moreover, as id(q||pf )     0, we have that ln z     f [pf , q]. that is, the energy
functional is a lower bound on the value of the logarithm of the partition function
z, for any choice of q. why is this fact signi   cant? recall that, in directed models,
the partition function z is the id203 of the evidence. computing the partition
function is often the hardest part of id136. and so this theorem shows that if
we have a good approximation (that is, id(q||pf ) is small), then we can get a good
lower bound approximation to z. the fact that this approximation is a lower bound
plays an important role in learning parameters of id114.

2.3.2 exact id136 as optimization

before considering approximate id136 methods, we illustrate the use of a varia-
tional approach to derive an exact id136 procedure. the concepts we introduce
here will serve in discussion of the following approximate id136 methods.

the goal of exact id136 here will be to compute marginals of the distribution.
to achieve this goal, we will need to make sure that the set of distributions q is
expressive enough to represent the target distribution pf . instead of approximating
pf , the solution of the optimization problem transforms the representation of the

2.3 id136

27

a

a
a

b

b
b

c

c
c

d

d
d

a1,1
a1,1

a1,2
a1,2

a1,3
a1,3

a1,4
a1,4

a2,1
a2,1

a2,2
a2,2

a2,3
a2,3

a2,4
a2,4

a3,1
a3,1

a3,2
a3,2

a3,3
a3,3

a3,4
a3,4

a4,1
a4,1

a4,2
a4,2

a4,3
a4,3

a4,4
a4,4

(a)

(b)

figure 2.4 (a) chain-structured id110 and equivalent markov net-
work (b) grid-structured markov network.

distribution from a product of factors into a more useful form q that directly yields
the desired marginals.

to accomplish this, we will need to optimize over the set of distributions q that
include pf . then, if we search over this set, we are guaranteed to    nd a distribution
   ||pf ) = 0, which is therefore the unique global optimum of our
    for which id(q
q
energy functional. we will represent this set using an undirected graphical model
called the clique tree, for reasons that will be clear below.
consider the undirected graph corresponding to the set of factors f. in this
graph, nodes are connected if they appear together in a factor. note that if a factor
is the cpd of a directed graphical model, then the family will be a clique in the
graph, so its connectivity is denser then the original directed graph since parents
have been connected (moralized). the key property for exact id136 in the graph
is chordality:

de   nition 2.18

let x1   x2             xk   x1 be a loop in the graph; a chord in the loop is an edge
connecting xi and xj for two nonconsecutive nodes xi, xj. an undirected graph
h is said to be chordal if any loop x1   x2             xk   x1 for k     4 has a chord.

in other words, the longest    minimal loop    (one that has no shortcut) is a triangle.
thus, chordal graphs are often also called triangulated.

the simplest (and most commonly used) chordal graphs are chain-structured
(see    gure 2.4(a)). what if the graph is not chordal? for example, grid-structured
graphs are commonly used in id161 for pixel-labeling problems (see    g-
ure 2.4(b)). to make a graph chordal (triangulate it),    ll-in edges are added to
short-circuit loops. there are generally many ways to do this and    nding the least
number of edges to    ll is np-hard. however, good heuristic algorithms for this
problem exist [12, 1].

we now de   ne a cluster graph     the backbone of the graphical data structure
needed to perform id136. each node in the cluster graph is a cluster, which

28

id114 in a nutshell

is associated with a subset of variables; the graph contains undirected edges that
connect clusters whose scopes have some nonempty intersection.

de   nition 2.19
a cluster graph k for a set of factors f over x is an undirected graph, each of
whose nodes i is associated with a subset c i     x . a cluster graph must be family-
preserving     each factor        f must be associated with a cluster c, denoted
  (  ), such that scope[  ]     c i. each edge between a pair of clusters c i and cj is
associated with a sepset si,j = c i     c j. a singly connected cluster graph (a tree)
is called a cluster tree.

de   nition 2.20
let t be a cluster tree over a set of factors f. we say that t has the running
intersection property if, whenever there is a variable x such that x     c i and
x     c j, then x is also in every cluster in the (unique) path in t between ci and
cj. a cluster tree that satis   es the running intersection property is called a clique
tree.

theorem 2.21
every chordal graph g has a clique tree t .

constructing the clique tree from a chordal graph is actually relatively easy:
(1)    nd maximal cliques of the graph (this is easy in chordal graphs) and (2)
run a maximum spanning tree algorithm on the appropriate clique graph. more
speci   cally, we build an undirected graph whose nodes are the maximal cliques,
and where every pair of nodes ci, cj is connected by an edge whose weight is
|ci     c j|.
because of this correspondence, we can de   ne a very important characteristic of

a graph, which is critical to the complexity of exact id136:

de   nition 2.22
the treewidth of a chordal graph is the size of the largest clique minus 1. the
treewidth of an untriangulated graph is the minimum treewidth of all of its trian-
gulations.

note that the treewidth of a chain in    gure 2.4(a) is 1 and the treewidth of the

grid in    gure 2.4(b) is 4.

2.3.2.1 the optimization problem
suppose we are given a clique tree t for pf . that is, t satis   es the running
intersection property and the family preservation property. moreover, suppose we
are given a set of potentials q = {  i}     {  i,j : (c i   c j)     t }, where ci denotes
clusters in t , si,j denote separators along edges in t ,   i is a potential over c i, and
  i,j is a potential over si,j. the set of potentials de   nes a distribution q according

2.3 id136

to t by the formula

(cid:9)

q(x ) =

(cid:9)

ci   t   i

(ci   c j )   t   i,j

.

29

(2.4)

note that by construction, q can represent pf by simply letting the appropriate
potentials   i equal factors   i and letting   i,j equal 1. however, we will consider a
di   erent, more useful, representation.

de   nition 2.23

the set of potentials q is calibrated when for each (c i   cj)     t the potential   i,j
on si,j is the marginal of   i (and   j).

proposition 2.24
let q be a set of calibrated potentials for t , and let q be the distribution de   ned
by (2.4). then   i[ci] = q(ci) and   i,j[si,j] = q(si,j).

in other words, the potentials correspond to marginals of the distribution q de   ned
by (2.4). now if q is a set of uncalibrated potentials for t , and q is the distribution
de   ned by (2.4), we can construct q(cid:2)
, a set of calibrated potentials which represent
q by simply using the appropriate marginals of q.

once we decide to focus our attention on calibrated clique trees, we can rewrite
the energy functional in a factored form, as a sum of terms each of which depends
directly only on one of the potentials in q. this form reveals the structure in the
distribution, and is therefore a much better starting point for further analysis. as
we shall see, this form will also be the basis for our approximations in subsequent
sections.

de   nition 2.25
given a clique tree t with a set of potentials, q, and an assignment    that maps
factors in pf to clusters in t , we de   ne the factored energy functional

(cid:3)

(cid:10)

(cid:11)

ln   0
i

+

ie  i

i

(cid:3)

ci   t

(cid:3)

ih  i(c i)    

(ci   cj )   t

ih  i,j (si,j),

(2.5)

  f [pf , q] =

(cid:9)

where   0

i =

  ,  (  )=i   .

before we prove that the energy functional is equivalent to its factored form, let
.
us    rst understand its form. the    rst term is a sum of terms of the form ie  i
recall that   0
i is a factor (not necessarily a distribution) over the scope ci, that is,
a function from val(c i) to ir+. its logarithm is therefore a function from val(c i)
to ir. the clique potential   i is a distribution over val(c i). we can therefore
i . the last two terms are entropies of the
compute the expectation,
distributions     the potentials and messages     associated with the clusters and
sepsets in the tree.

  i[ci] ln   0

(cid:6)

ln   0
i

ci

(cid:10)

(cid:11)

30

id114 in a nutshell

proposition 2.26
if q is a set of calibrated potentials for t , and q is de   ned by by (2.4), then

  f [pf , q] = f [pf , q].

using this form of the energy, we can now de   ne the optimization problem. we    rst
need to de   ne the space over which we are optimizing. if q is factorized according
to t , we can represent it by a set of calibrated potentials. calibration is essentially
a constraint on the potentials, as a clique tree is calibrated if neighboring potentials
agree on the marginal distribution on their joint subset. thus, we pose the following
constrained optimization procedure:

ctree-optimize

find
that maximize

subject to

q
  f [pf , q]

(cid:3)
(cid:3)

c i\si,j

  i =   i,j,

  i = 1,

   (c i   cj)     t ;
   ci     t .

(2.6)

(2.7)

c i

the constraints (2.6) and (2.7) ensure that the potentials in q are calibrated and
represent legal distributions. it can be shown that the objective function is strictly
concave in the variables   ,   . the constraints de   ne a convex set (linear subspace),
so this optimization problem has a unique maximum. since q can represent pf ,
this maximum is attained when id(q||pf ) = 0.

2.3.2.2 fixed-point characterization

we can now prove that the stationary points of this constrained optimization
function     the points at which the gradient is orthogonal to all the constraints
    can be characterized by a set of self-consistent equations.

recall that a stationary point of a function is either a local maximum, a local
minimum, or a saddle point. in this optimization problem, there is a single global
maximum. although we do not show it here, we can show that it is also the
single stationary point. we can therefore de   ne the global optimum declaratively,
as a set of equations, using standard methods based on lagrange multipliers.
as we now show, this declarative formulation gives rise to a set of equations
which precisely corresponds to message-passing steps in the clique tree, a standard
id136 procedure usually derived via id145.

2.3 id136

31

i

i

i

  0
i

   {j}

(2.8)

  j   i

  k   i

j   nc

k   nc

   
   

c i   si,j

  i   j    

   
    (cid:2)
   
   

theorem 2.27
a set of potentials q is a stationary point of ctree-optimize if and only if there
exists a set of factors {  i   j[si,j] : c i   cj     t } such that

(cid:3)
   
    (cid:2)
  i       0
  i,j =   j   i      i   j ,
where nci are the neighboring cliques of c i in t .
theorem 2.27 illustrates themes that appear in many approaches that turn
variational problems into message-passing schemes. it provides a characterization of
the solution of the optimization problem in terms of    xed-point equations that must
hold when we    nd a maximal q. these    xed-point equations de   ne the relationships
that must hold between the di   erent parameters involved in the optimization
problem. most importantly, (2.8) de   nes each   i   j in terms of   k   j other than
  i   j. the other parameters are all de   ned in a noncyclic way in terms of the
  i   j   s.

(2.10)

(2.9)

the form of the equations resulting from the theorem suggest an iterative
procedure for    nding a    xed point, in which we view the equations as assignments,
and iteratively apply equations to the current values of the left-hand side to de   ne
a new value for the right-hand side. we initialize all of the   i   j   s to 1, and then
iteratively apply (2.8), computing the left-hand side   i   j of each equality in terms
of the right-hand side (essentially converting each equality sign to an assignment).
clearly, a single iteration of this process does not usually su   ce to make the
equalities hold; however, under certain conditions (which hold in this particular
case) we can guarantee that this process converges to a solution satisfying all of the
equations in (2.8); the other equations are now easy to satisfy.

2.3.3 loopy belief propagation in pairwise markov networks

we focus on the class of pairwise markov networks. in these networks, we have
a univariate potential   i[xi] over each variable xi, and in addition a pairwise
potential   (i,j)[xi, xj] over some pairs of variables. these pairwise potentials
correspond to edges in the markov network. examples of such networks include
our simple tuberculosis example in    gure 2.3 and the grid networks we discussed
above.

the transformation of such a network into a cluster graph is fairly straight-
forward. for each potential, we introduce a corresponding cluster, and put edges
between the clusters that have overlapping scope. in other words, there is an edge

32

id114 in a nutshell

1: a, b, c
1: a, b, c

2: b, c, d
2: b, c, d
2: b, c, d

3: b,d,f
3: b,d,f

4: b, e
4: b, e
4: b, e

5: d, e
5: d, e

1: a, b, c
1: a, b, c

2: b, c, d
2: b, c, d
2: b, c, d

3: b,d,f
3: b,d,f

4: b, e
4: b, e
4: b, e

5: d, e
5: d, e

12: b, c
12: b, c

6: a
6: a

7: b
7: b

8: c
8: c

9: d
9: d

10: e
10: e

11: f
11: f

6: a
6: a

7: b
7: b

8: c
8: c

9: d
9: d

10: e
10: e

11: f
11: f

(a) k3

(b) k4

figure 2.5 two additional examples of generalized cluster graphs for a markov
network with potentials over {a, b, c}, {b, c, d}, {b, d, f}, {b, e}, and {d, e}. (a)
bethe factorization. (b) capturing interactions between {a, b, c} and {b, c, d}.

between the cluster c (i,j) that corresponds to the edge xi   xj and the clusters
ci and c j that correspond to the univariate factors over xi and xj.

as there is a direct correspondence between the clusters in the cluster graphs and
variables or edges in the original markov network, it is often convenient to think
of the propagation steps as operations on the original network. moreover, as each
pairwise cluster has only two neighbors, we consider two propagation steps along
the path c i   c(i,j)   c j as propagating information between xi and xj. indeed,
early versions of generalized belief propagation were stated in these terms. this
algorithm is known as loopy belief propagation, as it uses propagation steps used by
algorithms for markov trees, except that it was applied to networks with loops.

a natural question is how to extend this method to networks that are more
complex than pairwise markov networks. once we have larger potentials, they may
overlap in ways that result in complex interactions among them.
one simple construction creates a bipartite graph. the    rst layer consists of
   large    clusters, with one cluster for each factor    in f, whose scope is scope[  ].
these clusters ensure that we satisfy the family-preservation property. the second
layer consists of    small    univariate clusters, one for each random variable. finally,
we place an edge between each univariate cluster x on the second layer and each
cluster in the    rst layer that includes x; the scope of this edge is x itself. for a
concrete example, see    gure 2.5(a).

we can easily verify that this is a proper cluster graph. first, by construction it
satis   es the family-preserving property. second, the edges that mention a variable
x form a star-shaped subgraph with edges from the univariate cluster with scope
x to all the large clusters that contain x. we will call this construction the bethe
approximation (for reasons that will be clari   ed below). the construction of this
cluster graph is simple and can easily be automated.

so far, our discussion of belief propagation has been entirely procedural, and mo-
tivated purely by similarity to message-passing algorithms for cluster trees. is there
any formal justi   cation for this approach? is there a sense in which we can view
this algorithm as providing an approximation to the exact id136 task? in this
section, we show that belief propagation can be justi   ed using the energy function
formulation. speci   cally, the messages passed by generalized belief propagation can
be derived from    xed-point equations for the stationary points of an approximate

2.3 id136

33

version of the energy functional of (2.3). as we shall see, this formulation provides
signi   cant insight into the generalized belief propagation algorithm. it allows us to
better understand the convergence properties of generalized belief propagation, and
to characterize its convergence points. it also suggests generalizations of the algo-
rithm which have better convergence properties, or that optimize a more    accurate   
approximation to the energy functional.

our construction will be similar to the one in section 2.3.2 for exact id136.
however, there are some di   erences. as we saw, the calibrated cluster graph
maintains the information in pf . however, the resulting cluster potentials are
not, in general, the marginals of pf . in fact, these cluster potentials may not
represent the marginals of any single coherent joint distribution over x . thus, we
can think of generalized belief propagation as constructing a set of pseudo-marginal
distributions, each one over the variables in one cluster. these pseudo-marginals are
calibrated, and therefore locally consistent with each other, but are not necessarily
marginals of a single underlying joint distribution.

the energy functional f [pf , q] has terms involving the id178 of an entire joint
distribution; thus, it cannot be used to evaluate the quality of an approximation
de   ned in terms of (possibly incoherent) pseudo-marginals. however, the factored
free energy functional   f [pf , q] is de   ned in terms of entropies of clusters and
messages, and is therefore well-de   ned for pseudo-marginals q. thus, we can write
down an optimization problem as before:

cgraph-optimize

find
that maximize

subject to

q
  f [pf , q]

(cid:3)
(cid:3)

c i\si,j

  i =   i,j,

  i = 1,

   (c i   c j)     t ;
   c i     t .

(2.11)

(2.12)

ci

importantly, however, unlike for clique trees,   f [pf , q]
is no longer simply a
reformulation of the free energy, but rather an approximation of it. thus, our
optimization problem contains two approximations: we are using an approximation,
rather than an exact, energy functional; and we are optimizing it over the space of
pseudo-marginals, which is a relaxation (a superspace) of the space of all coherent
id203 distributions that factorize over the cluster graph. the approximate
energy functional in this case is a restricted form of an approximation known as
the kikuchi free energy in statistical physics.

we noted that the energy functional is a lower bound of the log-partition function;
thus, by maximizing it, we get better approximations of pf . unfortunately, the
factored energy functional, which is only an approximation to the true energy
functional, is not necessarily also a lower bound. nonetheless, it is still a reasonable
strategy to maximize the approximate energy functional.

34

id114 in a nutshell

our maximization problem is the natural analogue of ctree-optimize to the case
of cluster graphs. not surprisingly, we can derive a similar analogue to theorem 2.27.

ci   si,j

  0
i

(cid:2)

   {j}

k   nc
  j   i

i

theorem 2.28
a set of potentials q is a stationary point of cgraph-optimize if and only if for
every edge (c i   cj)     k there are auxiliary potentials   i   j(si,j) and   j   i(sj,i)
so that
  i   j    

(cid:3)

(cid:2)

(2.13)

  k   i

  

i

i

  

(2.15)

(2.14)

  i       0
j   nc
  i,j =   j   i      i   j.
this theorem shows that we can characterize convergence points of the energy
function in terms of the original potentials and messages between clusters. we
can, once again, de   ne a procedural variant, in which we initialize   i   j, and then
iteratively use (2.13) to rede   ne each   i   j in terms of the current values of other
  k   i. theorem 2.28 shows that convergence points of this procedure are related to
stationary points of   f [pf , q].

it is relatively easy to verify that   f [pf , q] is bounded from above. and thus,
this function must have a maximum. there are two cases. the maximum is either
an interior point or a boundary point (some of the probabilities in q are 0). in the
former case the maximum is also a stationary point, which implies that it satis   es
the condition of theorem 2.28. in the latter case, the maximum is not necessarily
a stationary point. this situation, however, is very rare in practice, and can be
guaranteed not to arise if we make some fairly benign assumptions.

it is important to understand what these results imply, and what they do not.
the results imply only that the convergence points of generalized belief propagation
are stationary points of the free energy function they do not imply that we can
reach these convergence points by applying belief propagation steps. in fact, there
is no guarantee that the message-passing steps of generalized belief propagation
necessarily improve the free energy objective: a message passing step may increase
or decrease the energy functional. (in fact, if generalized belief propagation was
guaranteed to monotonically improve the functional, then it would necessarily
always converge.)

what are the implications of this result? first, it provides us with a declarative
semantics for generalized belief propagation in terms of optimization of a target
functional. this declarative semantics opens the way to investigate other compu-
tational approaches for optimizing the same functional. we discuss some of these
approaches below.

this result also allows us to understand what properties are important for this
type of approximation, and subsequently to design other approximations that may
be more accurate, or better in some other way. as a concrete example, recall that,
in our discussion of generalized cluster graphs, we required the running intersection

2.3 id136

35

property. this property has two important implications. first, that the set of
clusters that contain some variable x are connected; hence, the marginal over x
will be the same in all of these clusters at the calibration point. second, that there
is no cycle of clusters and sepsets all of which contain x. we can motivate this
assumption intuitively, by noting that it prevents us from allowing information
about x to cycle endlessly through a loop. the free energy function analysis
provides a more formal justi   cation. to understand it, consider    rst the form of the
factored free energy functional when our cluster graph k has the form of the bethe
approximation recall that in the bethe approximation graph there are two layers:
one consisting of clusters that correspond to factors in f, and the other consisting
of univariate clusters. when the cluster graph is calibrated, these univariate clusters
have the same distribution as the separators between them and the factors in the
   rst layer. as such, we can combine together the id178 terms for all the separators
labeled by x and the associated univariate cluster and rewrite the free energy, as
follows:

proposition 2.29

if q = {     :        f}     {  i(xi)} is a calibrated set of potentials for k for a bethe
: xi     x}, then
approximation cluster graph with clusters {c   :        f}     {xi
(2.16)

(di     1)ih  i(xi),

ih    (c   )    

  f [pf , q] =

ie    [ln   ] +

(cid:3)

(cid:3)

(cid:3)

     f

     f

i

where di = |{   : xi     scope[  ]}| is the number of factors that contain xi.
note that (2.16) is equivalent to the factored free energy only when q is calibrated.
however, as we are interested only in such cases, we can freely alternate between
the two forms for the purpose of    nding    xed points of the factored free energy
functional. equation (2.16) is known as the bethe free energy, and again has a
history in statistical mechanics. the bethe approximation we discussed above is a
construction in terms of cluster graphs that is designed to match the bethe free
energy.

as we can see in this alternative form, if the variable xi appears in di clusters in
the cluster graph, then it appears in an id178 term with a positive sign exactly
di times. due to the running intersection property, the number of separators that
contain xi is di     1 (the number of edges in a tree with k vertices is k     1), so that
xi appears in an id178 term with a negative sign exactly di     1 times. in this
case, we say that the counting number of xi is 1. thus, our approximation does not
over- or undercount the id178 of xi. it is not di   cult to show that the counting
number result holds for any approximation that satis   es the running intersection
property. thus, one motivation for the running intersection property is that cluster
graphs satisfying it provide a better approximation to the free energy functional.
this intuition forms the basis for improved approximations. speci   cally, we
can construct energy functionals (called kikuchi free energy approximations) that
resemble (2.5), in which we introduce additional id178 terms, with both positive
and negative signs, in a way that ensures that the counting number for all variables

36

id114 in a nutshell

is 1. somewhat remarkably, the same analysis we performed in this section    
de   ning a set of    xed-point equations for stationary points of the approximate free
energy     also leads to message-passing algorithms for these richer approximations.
the propagation rules for these approximations, which also fall under the heading
of generalized belief propagation, are more elaborate, and we do not discuss them
here.

2.3.4 sampling-based approximate id136

as we discussed above, another approach to dealing with the worst-case combinato-
rial explosion of exact id136 in id114 is via sampling-based methods.
in these methods, we approximate the joint distribution as a set of instantiations
to all or some of the variables in the network. these instantiations, often called
samples, represent part of the id203 mass.
the general framework for most of the discussion is as follows. consider some
distribution p (x ), and assume we want to estimate the id203 of some event
y = y relative to p , for some y     x and y     val(y ). more generally, we might
want to estimate the expectation of some function f(x ) relative to p ; this task
is a generalization, as we can choose f(  ) = 11{  (cid:15)y (cid:16) = y}. we approximate this
expectation by generating a set of m samples, estimating the value of the function
or its expectation relative to each of the generated samples, and then aggregating
the results.

2.3.4.1 id115 methods

id115 (abbreviated mcmc ) is an approach for generating
samples from the posterior distribution. as we discussed, we cannot typically sample
from the posterior directly; however, we can construct a process which gradually
samples from distributions that are closer and closer to the posterior. intuitively,
we de   ne a state graph whose nodes are the states of the system, i.e., possible
instantiations val(x ). (this graph is very di   erent from the graphical model that
de   nes the distribution p (x ), whose nodes correspond to variables.) we then de   ne
a process that randomly traverses this graph, moving from one state to another.
this process is de   ned so that, ultimately (after enough steps), the id203 of
being in any particular state is the desired posterior distribution.

we begin by describing the general framework of markov chains, and then
describe their application to approximate id136 in id114. we note
that, unlike forward sampling methods (including likelihood weighting), markov
chain methods apply equally well to directed and to undirected models.

a markov chain is de   ned in terms of a set of states, and a transition model
from one state to another. the chain de   nes a process that evolves stochastically
from state to state.

2.3 id136

37

de   nition 2.30
a markov chain is de   ned via a state space val(x) and a transition id203
model, which de   nes, for every state x     val(x) a next-state distribution over
val(x). the transition id203 of going from x to x(cid:2) is denoted t (x     x(cid:2)).
this transition id203 applies whenever the chain is in state x.

we note that, in this de   nition and in the subsequent discussion, we restrict
attention to homogeneous markov chains, where the system dynamics do not change
over time.

we can imagine a random sampling process that de   nes a sequence of states
x(0), x(1), x(2), . . .. as the transition model is random, the state of the process at
step t can be viewed as a random variable x (t). we assume that the initial state
x (0) is distributed according to some initial state distribution p (0)(x (0)). we can
now de   ne distributions over the subsequent states p (1)(x (1)), p (2)(x (2)), . . . using
the chain dynamics:

p (t+1)(x (t+1) = x(cid:2)

) =

p (t)(x (t) = x)t (x     x(cid:2)

).

(2.17)

(cid:3)

x   val(x)

intuitively, the id203 of being at state x(cid:2) at time t + 1 is the sum over all
possible states x that the chain could have been in at time t of the id203
being in state x times the id203 that the chain took a transition from x to
x(cid:2).

as the process converges, we would expect p (t+1) to be close to p (t). using (2.17),

we obtain

p (t)(x(cid:2)

)     p (t+1)(x(cid:2)

) =

p (t)(x)t (x     x(cid:2)

).

(cid:3)

x   val(x)

at convergence, we would expect the resulting distribution   (x) to be an equi-
librium relative to the transition model; i.e., the id203 of being in a state
is the same as the id203 of transitioning into it from a randomly sampled
predecessor. formally:

de   nition 2.31
a distribution   (x) is a stationary distribution for a markov chain t if it satis   es

  (x = x(cid:2)

) =

  (x = x)t (x     x(cid:2)

).

(2.18)

(cid:3)

x   val(x)

we wish to restrict attention to markov chains that have a unique stationary
distribution, which is reached from any starting distribution p (0). there are various
conditions that su   ce to guarantee this property. the condition most commonly
used is a fairly technical one, that the chain be ergodic. in the context of markov
chains where the state space val(x) is    nite, the following condition is equivalent
to this requirement:

38

id114 in a nutshell

de   nition 2.32
a markov chain is said to be regular if there exists some number k such that, for
every x, x(cid:2)     val(x), the id203 of getting from x to x(cid:2) in exactly k steps is
greater than 0.

the following result can be shown to hold:

theorem 2.33
a    nite-state markov chain t has a unique stationary distribution if and only if it
is regular.

ensuring regularity is usually straightforward. two simple conditions that guar-

antee regularity in    nite-state markov chains are:

it is possible to get from any state to any state using a positive id203 path
in the state graph.
for each state x, there is a positive id203 of transitioning from x to x in
one step (a self-loop).

these two conditions together are su   cient but not necessary to guarantee regu-
larity. however, they often hold in the chains used in practice.

2.3.4.2 markov chains for id114

the theory of markov chains provides a general framework for generating samples
from a target distribution   . in this section, we discuss the application of this
framework to the sampling tasks encountered in probabilistic id114. in
this case, we typically wish to generate samples from the posterior distribution
p (x | e = e). thus, we wish to de   ne a chain for which p (x | e) is the stationary
distribution. clearly, there are many ways of de   ning such a chain. we focus on
the most common approaches.
in id114, we de   ne the states of the markov chain to be instantiations
   to x , which are compatible with e; i.e., all of the states    in the markov chain
satisfy   (cid:15)e(cid:16) = e. the states in our markov chain are therefore some subset of
the possible assignments to the variables x . in order to de   ne a markov chain, we
need to de   ne a process that transitions from one state to the other, converging to
a stationary distribution   (  ) which is the desired posterior distribution p (   | e).
in the case of id114, our state space has a factorized structure    
each state is an assignment to several variables. when de   ning a transition model
over this state space, we can consider a fully general case, where a transition can
go from any state to any state. however, it is often convenient to decompose the
transition model, considering transitions that only update a single component of
the state vector at a time, i.e., only a value for a single variable.
in this case,
as in several other settings, we often de   ne a set of transition models t1, . . . ,tk,
each with its own dynamics. in certain cases, the di   erent transition models are
necessary, because no single transition model on its own su   ces to ensure regularity.

2.3 id136

39

in other cases, having multiple transition models simply makes the state space more
   connected,    and therefore speeds the convergence to a stationary distribution.

there are several ways of combining these multiple transition models into a single
chain. one common approach is simply to randomly select between them at each
step, using any distribution. thus, for example, at each step, we might select one
of t1, . . . ,tk, each with id203 1/k. alternatively, we can simply cycle over the
di   erent transition models, taking each one in turn. clearly, this approach does not
de   ne a homogeneous chain, as the transition model used in step i is di   erent from
the one used in step i + 1. however, we can simply view the process as de   ning a
single transition model t each of whose steps is an aggregate step, consisting of
   rst taking t1, then t2, . . . , through tk.
in the case of id114, we de   ne x = x     e = {x1, . . . , xk}. we
de   ne a multiple transition chain, where we have a local transition model ti for
each variable xi     x. let u i = x     {xi}, and let ui denote an instantiation
to u i. the model ti takes a state (ui, xi) and transitions to a state of the form
(cid:2)
i). as we discussed above, we can combine the di   erent local transition models
(ui, x
into a single global model in various ways.

2.3.4.3 id150

id150 is one simple yet e   ective markov chain for factored state spaces,
which is particularly e   cient for id114. we de   ne the local transition
model ti as follows. intuitively, we simply    forget    the value of xi in the current
state, and sample a new value for xi from its posterior given the rest of the current
state. more precisely, let (ui, xi) be a state in the chain. we de   ne

t ((ui, xi)     (ui, x
(cid:2)
(cid:2)
i)) = p (x
i

| ui).

(2.19)

note that the transition id203 does not depend on the current value xi of xi,
but only on the remaining state ui.

the gibbs chain is de   ned via a set of local transition models; we use the
multistep transition model to combine them. note that the di   erent local transitions
are taken consecutively; i.e., having changed the value for a variable x1, the value
for x2 is sampled based on the new value. also note that we are only collecting a
single sample for every sequence where each local transition has been taken once.
this chain is guaranteed to be regular whenever the distribution is positive,
so that every value of xi has positive id203 given an assignment ui to the
remaining variables. in this case, we can get from any state to any state in at most
k local transition steps, where k = |x     e|. positivity is, however, not necessary;
there are many examples of nonpositive distributions where the gibbs chain is
regular. it is also easy to show that the posterior distribution p (x | e) is a
stationary distribution of this process.
id150 is particularly well suited to many id114, where we
can compute the transition id203 p (xi | ui) very e   ciently. in particular, as

40

id114 in a nutshell

we now show, this distribution can be done based only on the markov blanket of xi.
we show this analysis for a markov network; the extension to id110s is
straightforward. in general, we can decompose the id203 of an instantiation
as follows:
p (x1 | x2, . . . , xn) =
  j[cj].
for shorthand, let   j[xi, u] denote   j[xi, u(cid:15)c j(cid:16)]. we can now compute

j : xi   cj

j : xi(cid:6)   cj

  j[c j] =

(cid:2)

(cid:2)

(cid:2)

  j[c j]

1
z

1
z

j

(cid:2)
p (x
i

| ui) =

(cid:6)

(cid:2)
i, ui)
p (x
(cid:2)(cid:2)
i

(cid:2)(cid:2)
p (x
i , ui)

x

=

(cid:6)

(cid:9)
(cid:9)

(cid:2)(cid:2)
i

x

c j(cid:7)xi
cj(cid:7)xi

(cid:2)
i, ui]
  j[x
(cid:2)(cid:2)
i , ui)] .
  j[(x

(2.20)

this last expression uses only the clique potentials involving xi, and depends only
on the instantiation in ui of xi   s markov blanket. in the case of id110s,
this expression reduces to a formula involving only the cpds of xi and its children,
and its value, again, depends only on the assignment in ui to the markov blanket
of xi. it can thus be computed very e   ciently.

we note that the markov chain de   ned by a graphical model is not necessarily
it turns out
regular, and might not converge to a unique stationary distribution.
that this type of situation can only arise if the distribution de   ned by the graphical
model is nonpositive, i.e., if the cpds or clique potentials have entries with the
value 0.

theorem 2.34
let h be a markov network such that all of the clique potentials are strictly positive.
then the gibbs-sampling markov chain is regular.

2.3.4.4 building a markov chain

as we discussed, the use of mcmc methods relies on the construction of a
markov chain that has the desired properties: regularity, and the target stationary
distribution. above, we described the gibbs chain, a simple markov chain that is
guaranteed to have these properties under certain assumptions. however, gibbs
sampling is only applicable in certain circumstances; in particular, we must be able
to sample from the distribution p (xi | ui). although this sampling step is easy for
discrete id114, there are other types of models where this step is not
practical, and the gibbs chain is not applicable. unfortunately, it is beyond the
scope of this chapter to discuss the metropolis-hastings algorithm, a more general
method of constructing a markov chain that is guaranteed to converge to the desired
stationary distribution.

2.3.4.5 generating samples

the burn-in time for a large markov chain is often quite large. thus, the naive
algorithm described above has to execute a large number of sampling steps for

2.3 id136

41

every usable sample. however, a key observation is that, if x(t) is sampled from   ,
then x(t+1) is also sampled from   . thus, once we have run the chain long enough
that we are sampling from the stationary distribution (or a distribution close to it),
we can continue generating samples from the same trajectory, and obtain a large
number of samples from the stationary distribution.
more formally, assume that we use x(0), . . . , x(t ) as our burn-in phase, and then
collect m samples x(t +1), . . . , x(t +m ). thus, we have collected a data set d where
(cid:6)
xm = x(t +m), for m = 1, . . . , m. assume, for simplicity, that x(t +1) is sampled
from   , and hence so are all of the samples in d. it follows that for any function
f:

m=1 f(xm) is an unbiased estimator for ie  (x)[f(x)].

the key problem, of course, is that consecutive samples from the same trajectory
are correlated. thus, we cannot expect the same performance as we would from
m independent samples from   . in other words, the variance of the estimator is
signi   cantly higher than that of an estimator generated by m independent samples
from   , as discussed above.

m

one solution to this problem is not to collect consecutive samples from the chain.
rather, having collected a sample x(t ), we let the chain run for a while, and collect
a second sample x(t +d) for some appropriate choice of d. for d large enough, x(t )
and x(t +d) are only slightly correlated, and we can view them as independent
samples from   . however, the time d required for    forgetting    the correlation is
clearly related to the mixing time of the chain. thus, chains that are slow to mix
initially also require larger d in order to produce close-to-independent samples.
nevertheless, the samples do come from the correct distribution for any value of d,
and hence it is often better to compromise and use a shorter d than it is to use a
shorter burn-in time t . this method thus allows us to collect a larger number of
usable samples with fewer transitions of the markov chain.

in fact, we can often make even better use of the samples generated using this
single-chain approach. although the samples between x(t ) and x(t +d) are not
independent samples, there is no reason to discard them. that is, using all of the
samples x(t ), x(t +1), . . . , x(t +d) produces a provably better estimator than using
just the two samples x(t ) and x(t +d): our variance is always no higher if we use all
of the samples we generated rather than a subset. thus, the strategy of picking only
a subset of the samples is useful primarily in settings where there is a signi   cant
cost associated with using each sample (e.g., the evaluation of f is costly), so that
we might want to reduce the overall number of samples used.

2.3.4.6 discussion

this description of the use of markov chains is quite abstract: it contains no
speci   cation of the number of chains to run, the metrics for evaluating mixing,
techniques for determining the delay between samples that would allow them
to be considered independent, and more. unfortunately, at this point, there is
little theoretical analysis that can help answer these questions for the chains that
are of interest to us. thus, the application of markov chains is more of an art

42

id114 in a nutshell

than a science, and often requires signi   cant experimentation and hand-tuning of
parameters.

nevertheless, mcmc methods are, for many probabilistic models, the only
technique that can achieve reasonable performance. speci   cally, unlike forward
sampling methods, it does not degrade when the id203 of the evidence is low,
or when the posterior is very di   erent from the prior. furthermore, unlike forward
sampling, it applies to undirected models as well as to directed models. as such, it
is an important component in the suite of approximate id136 techniques.

2.4 learning

next, we turn our attention to learning id114 [4, 6]. there are two
variants of the learning task: parameter estimation and structure learning. in the
parameter estimation task, we assume that the qualitative dependency structure
of the graphical model is known; i.e., in the directed model case, g is given, and
in the undirected case, h is given. in this case, the learning task is simply to    ll
in the parameters that de   ne the cpds of the attributes or the parameters which
de   ne the potential functions of the markov network. in the structure learning task,
there is no additional required input (although the user can, if available, provide
prior knowledge about the structure, e.g., in the form of constraints). the goal is
to extract a id110 or markov network, structure as well as parameters,
from the training data alone. we discuss each of these problems in turn.

2.4.1 parameter estimation in id110s

we begin with learning the parameters for a id110 where the depen-
dency structure is known. in other words, we are given the structure g that de-
termines the set of parents for each random variable, and our task is to learn the
parameters   g that de   ne the cpds for this structure. our learning is based on a
particular training set d = {x1, . . . , xm}, which, for now, we will assume is complete
(i.e., each instance is fully observed, there are no missing values). while this task is
relatively straightforward, it is of interest in and of itself. in addition, it is a crucial
component in the structure learning algorithm described in section 2.4.3.

there are two approaches to parameter estimation: maximum likelihood estima-
tion (id113) and bayesian approaches. the key ingredient for both is the likelihood
function: the id203 of the data given the model. this function captures the
response of the id203 distribution to changes in the choice of parameters. the
likelihood of a parameter set is de   ned to be the id203 of the data given the
model. for a id110 structure g the likelihood of a parameter set   g is

l(  g : d) = p (d |   g).

2.4 learning

43

2.4.1.1 maximum likelihood parameter estimation

given the above, one approach to parameter estimation is maximum likelihood
parameter estimation. here, our goal is to    nd the parameter setting   g that
maximizes the likelihood l(  g : d). for id110s, the likelihood can
be decomposed as follows:
l(  g,d) =

p (xj :   g)

j=1

m(cid:2)
m(cid:2)
n(cid:2)

j=1

n(cid:2)
m(cid:2)

i=1

i=1

j=1

=

=

p (xj
i

p (xj
i

| paxj

i

| paxj

i

:   g)

:   g)

we will use   xi|pai to denote the subset of parameters that determine p (xi | pai).
in the case where the parameters are disjoint (each cpd is parameterized by a
separate set of parameters that do not overlap; this allows us to maximize each
parameter set independently. we can write the likelihood as follows:

l(  g : d) =

li(  xi|pai : d),

where the local likelihood function for xi is

li(  xi|pai : d) =

p (xj
i

| paj

i :   xi|pai).

n(cid:2)

i=1

m(cid:2)

j=1

the simplest parameterization for the cpds is as a table. suppose we have a
variable x with parents u. if we represent that cpd p (x | u) as a table, then we
will have a parameter   x|u for each combination of x     val(x) and u     val(u).
in this case, we can write the local likelihood function as follows:
lx(  x|u : d) =

  xj|uj

m(cid:2)
(cid:2)

j=1

   
    (cid:2)

   
    ,

=

(2.21)
where nu,x is the number of times x = x and pai = u in d. that is, we have
grouped together all the occurrences of   x|u in the product over all instances.

u   val(u )

x   val(x)

  nu,x
x|u

we need to maximize this term under the constraints that, for each choice of

value for the parents u, the id155 is legal:

(cid:3)

  x|u = 1

for all u.

44

id114 in a nutshell

these constraints imply that the choice of value for   x|u can impact the choice of
value for   x(cid:2)|u. however, the choice of parameters given di   erent values u of u
are independent of each other. thus, we can maximize each of the terms in square
brackets in (2.21) independently.

we can thus further decompose the local likelihood function for a tabular cpd
into a product of simple likelihood functions. it is easy to see that each of these
likelihood functions is a multinomial
likelihood. the counts in the data for the
di   erent outcomes x are simply {nu,x : x     val(x)}. we can then immediately
use the id113 parameters for a multinomial which are simply

where we use the fact that nu =

,

    x|u = nu,x
(cid:6)
nu

x nu,x.

2.4.1.2 bayesian parameter estimation

in many cases, maximum likelihood parameter estimation is not robust, as it over-
   ts the training data. the bayesian approach uses a prior distribution over the
parameters to smooth the irregularities in the training data, and is therefore sig-
ni   cantly more robust. as we will see in section 2.4.3, the bayesian framework also
gives us a good metric for evaluating the quality of di   erent candidate structures.
roughly speaking, the bayesian approach introduces a prior over the unknown
parameters, allowing us to specify a joint distribution over the unknown parameters
and the data instances, and performs bayesian conditioning, using the data as
evidence, to compute a posterior distribution over these parameters.

consider the following simple example: we want to estimate parameters for a
simple network with two variables x and y , where x is the parent of y . our
training data consists of observations xj , yj for j = 1, . . . , m. in addition, assume
that our cpds are represented as multinomials and we have unknown parameter
vectors   x,   y |x0, and   y |x1.

the dependencies between these variables are described in the network of    g-
ure 2.6. this is the meta-id110 that describes our learning setup. this
id110 structure immediately reveals several points. for example, the
instances are independent given the unknown parameters. in addition, a common
assumption made is that the individual parameter variables are a priori indepen-
dent. that is, we believe that knowing the value of one parameter tells us nothing
about another. this is called parameter independence. the suitability of this as-
sumption depends on the domain, and it should be considered with care.

if we accept parameter independence, we can draw an important conclusion.
complete data d-separates the parameters for di   erent cpds. given the data set
d, we can determine the posterior over   x independently of the posterior over
  y |x. once we solve each problem separately, we can combine the results. this
is the analogous result to the likelihood decomposition for id113 estimation of
section 2.4.1.1.

2.4 learning

45

  x
  y|x0   y|x1

x[1]

x[2]

y[1]

y[2]

. . .
. . .

x[m]

y[m]

figure 2.6 the id110 for parameter estimation for a simple two-node
id110.

consider, for example, the learning setting described in    gure 2.6, where we take
both x and y to be binary. we need to represent the posterior   x and   y |x given
the data. if we use a dirichlet prior over   x,   y |x0, and   y |x1, then the posterior
p (  x | x1, . . . , xm ) can also be represented as a dirichlet distribution.
suppose that p (  x) is a dirichlet prior with hyperparameters   x0 and   x1,
p (  y |x0) is a dirichlet prior with hyperparameters   y0|x0 and   y1|x0, and p (  y |x1)
is a dirichlet prior with hyperparameters   y0|x1 and   y1|x1.

as in decomposition for the likelihood function in section 2.4.1.1, the likelihood
terms that involve   y |x0 depend on all the data elements x j such that xj = x0 and
the terms that involve   y |x1 depend on all the data elements x j such that xj = x1
we can decompose the joint distribution over parameters and data as follows:
(cid:2)
p (  g,d) = p (  x)lx(  x : d)
(cid:2)

p (  y |x1)

j:xj =x1

p (yj | xn :   y |x1)
p (yj | xj :   y |x0)

p (  y |x0)

j:xj =x0

thus, this joint distribution is a product of three separate joint distributions
with a dirichlet prior for some multinomial parameter and data drawn from this
multinomial. we can conclude that the posterior for p (  x | d) is dirichlet with
hyperparameters   x0 +nx0 and   x1 +nx1; the posterior for p (  y |x0 | d) is dirichlet
with hyperparameters   y0|x0 + nx0,y0 and   y1|x0 + nx0,y1; and the posterior for
p (  y |x1 | d) is dirichlet with hyperparameters   y0|x1 + nx1,y0 and   y1|x1 + nx1,y1.
the same pattern of reasoning we discussed applied to the general case. let d
be a complete data set for x , and let g be a network structure over these variables
with table cpds. if the prior p (  g) satis   es parameter independence, then

p (  g | d) =

p (  xi|pa

i

| d).

(cid:2)

(cid:2)

i

pa
i

if p (  x|u) is a dirichlet prior with hyperparameters   x1|u, . . . ,   xk|u, then the
posterior p (  x|u | d) is a dirichlet distribution with hyperparameters   x1|u +
nu,x1, . . . ,   xk|u + nu,xk .

46

id114 in a nutshell

this induces a predictive model in which, for the next instance, we have that

p (xi[m + 1] = xi | u[m + 1] = u,d) =

(cid:6)

  xi|u + nxi,u
i   xi|u + nxi,u

.

(2.22)

putting this all together, we can see that for computing the id203 of a
new instance, we can use a single network parameterized as usual, via a set of
multinomials, but ones computed as in (2.22).

2.4.2 parameter estimation in markov networks

x exp [   (cid:6)
(cid:6)

n

i=1  i[xi]]

unfortunately, for general markov networks, the likelihood function cannot be
decomposed. a notable exception is chordal markov networks, but we will focus
on the general case here. for a network with a set of cliques d1, . . . , dn, the
likelihood function is given by

m(cid:2)

j=1

l( ,d) =

1
z

exp

(cid:4)
    n(cid:3)

i=1

(cid:5)

 i[xj
i ]

,

i is the value of the variables di in the instance xj and z =

where xj
is the id172 constant. this id172 constant is responsible for cou-
pling the estimation parameters and e   ectively ruling out a closed-form solution.
luckily, this objective function is concave in  , so we have an unconstrained con-
cave maximization problem, which can be solved by simple gradient ascent or
second-order methods.
more concretely, for each ui     val(di) we have a parameter  i,ui
    ir. this is
the simplest case of complete parameterization. often, however, parameters may
be tied or clamped to zero. this does not change the fundamental complexity or
method of estimation. the derivative of the log-likelihood with respect to  i,ui is
given by

    log l( ,d)

    i,ui

=

(cid:7)
p (ui |  )     11{xj

m(cid:3)

j=1

(cid:8)

i = ui}

= mp (ui |  )     nui.

note that the gradient is zero when the counts of the data correspond exactly
with the expected counts predicted by the model. in practice, a prior on the
parameters is used to help avoid over   tting. the standard prior is a diagonal
gaussian,       n(0,   2i), which adds an additional factor of  i,u
to the gradient.
to compute the id203 p (ui |  ) needed to evaluate the gradient, we need
to perform id136 in the markov network. unlike in id110s, where
parameters of intractable (large treewidth) graphs can be estimated by simple
counting because of local id172, the undirected case requires id136
even during the learning stage. this is one of the prices of the    exibility of global
id172 in markov networks. see further discussion in chapter 4. because
of this added complexity, maximum-likelihood learning of the markov network

  2

i

2.4 learning

47

structure is much more expensive and much less investigated; we will focus below
on id110s.

2.4.3 learning the id110 structure

next we consider the problem of learning the structure of a id110. there
are three broad classes of algorithms for bn structure learning:

constraint-based approaches these approaches view a id110 as
a representation of independencies. they try to test for conditional dependence
and independence in the data, and then    nd a network that best explains these
dependencies and independencies. constraint-based methods are quite intuitive;
they closely follow the de   nition of id110. a potential disadvantage
of these methods is they can be sensitive to failures in individual independence
tests.
score-based approaches these approaches view a id110 as specify-
ing a statistical model, and then address learning as a model selection problem.
these all operate on the same principle: we de   ne a hypothesis space of poten-
tial models     the set of possible network structures we are willing to consider
    and a scoring function that measures how well the model    ts the observed
data. our computational task is then to    nd the highest-scoring network struc-
ture. the space of id110s is a combinatorial space, consisting of a
2). therefore, even with a scoring
superexponential number of structures     2o(n
function, it is not clear how one can    nd the highest-scoring network. there are
very special cases where we can    nd the optimal network. in general, however,
the problem is np-hard, and we resort to heuristic search techniques. score-based
methods consider the whole structure at once, and are therefore less sensitive to
individual failures and are better at making compromises between the extent to
which variables are dependent in the data and the    cost    of adding the edge.
the disadvantage of the score-based approaches is that they are in general not
gauranteed to    nd the optimal solution.
bayesian model averaging approaches the third class of approaches do not
attempt to learn a single structure. they are based on a bayesian framework
describing a distribution over possible structures and try to average the prediction
of all possible structures. since the number of structures is immense, performing
this task seems impossible. for some classes of models this can be done e   ciently,
and for others we need to resort to approximations.

in this chapter, we focus on the second approach, score-based approaches to

structure selection. for details about the other approaches, see [8].

48

id114 in a nutshell

2.4.3.1 structure scores

as discussed above, score-based methods approach the problem of structure learn-
ing as an optimization problem. we de   ne a score function that can score each
candidate structure with respect to the training data, and then search for a high-
scoring structure. as can be expected, one of the most important decisions we
must make in this framework is the choice of scoring function. in this subsection,
we discuss two of the most obvious choices.

the likelihood score a natural choice for scoring function is the likelihood
function, which we used for parameter estimation. this measures the id203
of the data given a model; thus, it seems intuitive to    nd a model that would make
the data as probable as possible.
assume that we want to maximize the likelihood of the model. our goal is to
   nd both a graph g and parameters   g that maximize the likelihood. it is easy to
show that to    nd the maximum-likelihood (g,   g) pair, we should    nd the graph
structure g that achieves the highest likelihood when we use the id113 parameters
for g. we therefore de   ne

scorel(g : d) = (cid:12)((cid:15)g,     g(cid:16) : d),

where (cid:12)((cid:15)g,     g(cid:16) : d) is the logarithm of the likelihood function, and     g are the
maximum-likelihood parameters for g. (it is typically easier to deal with the
logarithm of the likelihood.)

the problem with the likelihood score is that it over   ts the training data. it
will learn a model that precisely    ts the speci   cs of the empirical distribution in
our training set. this model captures both dependencies that are    true    of the
underlying distribution, and dependencies that are artifacts of the speci   c set of
instances that were given as training data. it therefore fails to generalize well to
new data cases: these are sampled from the underlying distribution, which is not
identical to the empirical distribution in our training set.

however it is reasonable to use the maximum-likelihood score when there are ad-
ditional mechanisms that disallow overcomplicated structures. for example, learn-
ing networks with a    xed indegree. such a limitation can constrain the tendency
to over   t when using the maximum-likelihood score.

bayesian score an alternative scoring function is based on bayesian considera-
tions. recall that the main principle of the bayesian approach is that, whenever we
have uncertainty over anything, we should place a distribution over it. in this case,
we have uncertainty both over structure and over parameters. we therefore de   ne
a structure prior p (g) that puts a prior id203 on di   erent graph structures,
and a parameter prior p (  g | g) that puts a id203 on a di   erent choice of

2.4 learning

49

parameters once the graph is given. by bayes rule, we have

p (g | d) = p (d | g)p (g)

,

p (d)

where, as usual, the denominator is simply a normalizing factor that does not help
distinguish between di   erent structures. then, we de   ne the bayesian score as

scoreb(g : d) = log p (d | g) + log p (g),

(2.23)

the ability to ascribe a prior over structures gives us a way of preferring some
structures over others. for example, we can penalize dense structures more than
sparse ones. it turns out, however, that this term in the score is almost irrelevant
compared to the second term. this    rst term, p (d | g) takes into consideration
our uncertainty over the parameters:

(cid:20)

p (d | g) =

p (d |   g,g)p (  g | g)d  g,

(2.24)
where p (d |   g,g) is the likelihood of the data given the network (cid:15)g,   g(cid:16) and
p (  g | g) is our prior distribution over di   erent parameter values for the network
g. this term is the marginal likelihood of the data given the structure, since we
marginalize out the unknown parameters.

  g

note that the marginal likelihood is di   erent from the maximum-likelihood score.
both terms examine the likelihood of the data given the structure. the maximum-
likelihood score returns the maximum of this function. in contrast, the marginal
likelihood is the average value of this function, where we average based on the prior
measure p (  g | g).

i

that p (  xi|pa
that

instantiating this further, if we consider a network with dirichlet priors, such
: j = 1, . . . ,|xi|, then we have
   
(cid:2)
    ,

| g) has hyperparameters {  
g
xj
i
(cid:2)
(cid:6)

g
+ nxj
|ui
xj
g
i
  (  
)
|ui
xj
i

g
  (  
xi|ui
g
  (  
xi|ui

   
     (  

p (d | g) =

ui   val(pag

   val(xi)

+ nui)

(cid:2)

|ui

i ,ui

xj
i

xi

)

)

)

i

. in practice, we use the logarithm of this formula, which

g
where   
xi|ui
is more manageable to compute numerically.

g
j   
xj
i

|ui

=

the bayesian score is biased toward simpler structures, but as it gets more data,
it is willing to recognize that a more complex structure is necessary. in other words,
it trades o       t to data with model complexity. to understand behavior, it is useful to
consider an approximation to the bayesian score that better exposes its fundamental
properties.

50

id114 in a nutshell

theorem 2.35
if we use a dirichlet parameter prior for all parameters in our network, then, as
m        , we have that

log p (d | g) = (cid:12)(    g : d)     log m
2

dim[g] + o(1),

where dim[g] is the number of independent parameters in g.

from this we see that the bayesian score tends precisely to trade o    the likelihood

       t to data     on the one hand, and the model complexity on the other.

this approximation is called the bayesian information criterion (bic) score:

scorebic(g : d) = (cid:12)(    g : d)     log m
2

dim[g]

our next task is to de   ne the actual priors that are used in the bayesian score. in
the case of the prior of network structures, p (g), note that although this term seems
to describe our bias for a certain structure, in fact it plays a relatively minor role. as
we can see in theorem 2.35, the logarithm of the marginal likelihood grows linearly
with the number of examples, while the prior over structures remains constant.
thus, the structure prior does not play an important role in asymptotic analysis as
long as it does not rule out (i.e., assign id203 0) any structure.

in part because of this, it is common to use a uniform prior over structures.
nonetheless, the structure prior can make some di   erence when we consider small
samples. thus, we might want to encode some of our preferences in this prior. for
example, we might penalize edges in the graph, and use a prior

p (g)     c

|g|

,

where c is some constant smaller than 1, and |g| is the number of edges in the
graph. in both these choices (the uniform, and the penalty per edge) it su   ces to
use a value that is proportional to the prior, since the normalizing constant is the
same for all choices of g and hence can be ignored.
it is mathematically convenient to assume that the structure prior satis   es
structure modularity. this condition requires that the prior p (g) is proportional to
a product of terms, where each term relates to one family. formally,

(cid:2)

p (g)    

p (paxi = pa

g
xi

),

i

g
xi

where p (paxi = pa
) denotes the prior id203 assigned to choosing the
speci   c set of parents for xi. structure priors that satisfy this property do not
penalize for global properties of the graph (such as its depth), but only for local
properties (such as the number of indegrees).

next we need to represent our parameter priors. the number of possible struc-
tures is superexponential, which makes it di   cult to elicit separate parameters for
each one.

2.4 learning

51

a simple approach is simply to take some    xed dirichlet distribution, e.g.,
dirichlet(  ,   ,   , . . . ,   ), for every parameter, where    is a predetermined constant.
a typical choice is    = 1. this prior is often referred to as the k2 prior, referring
to the name of the system where it was    rst used.

a more sophisticated approach is called the bde prior. we elicit a prior distri-
(cid:2) for

(cid:2) over the entire id203 space and an equivalent sample size m

bution p
the set of imaginary samples. we then set the parameters as follows:

  xi|pa

i

= m

(cid:2)    p

(cid:2)

(xi, pai).

this choice avoids certain inconsistencies exhibited by the k2 prior. we can
(cid:2) as a id110, whose structure can represent our prior about
represent p
(cid:2)
the domain structure. most simply, when we have no prior knowledge, we set p
to be the uniform distribution, i.e., the empty id110 with a uniform
marginal distribution for each variable.

the bde score turns out to satisfy an important property. two networks are
said to be i-equivalent if they encode the same set of independence statements.
hence based on observed independencies we cannot distinguish between i-equivalent
networks. this suggests that based on observing data cases, we do not expect to
distinguish between equivalent networks. the bde score has the desirable property
that i-equivalent networks have the same score, or are score-equivalent.

2.4.3.2 search

we now have a well-de   ned optimization problem. our input is
training set d;
scoring function (including priors, if needed);
a set gg of possible network structures (incorporating any prior knowledge).
our desired output is a network structure (from the set of possible structures) that
maximizes the score.

it turns out that, for this discussion, we can ignore the speci   c choice of score.

our search algorithms will apply unchanged to all three of these scores.

an important property of the scores that a   ects the e   ciency of search is their
decomposability. a score is decomposable if we can write the score of a network
structure g:

(cid:3)

score(g : d) =

famscore(xi | pa

g
i

: d)

i

all of the scores we have considered are decomposable. another property that is
shared by all these scores is score equivalence; if g is independence-equivalent to
g(cid:2), then score(g : d) = score(g(cid:2)

: d).

52

id114 in a nutshell

there are several special cases where structure learning is tractable. we won   t go
into full details, but two important cases are: (1) learning tree-structured networks
and (2) learning networks with known ordering over the variables.

a network is tree-structured if each variable has at most one parent. in this case,
for decomposable, score-equivalent scores, we can construct an undirected graph,
where the weight on an edge xi     xj is the change in network score if we add
xi as the parent of xj (note that, because of score-equivalence, this is the same as
the change if we add xj as parent of xi). we can    nd a weighted spanning tree of
this graph in polynomial time. we can transform the undirected spanning tree into
a directed spanning tree by choosing an arbitrary root, and directing edges away
from the root.
another interesting tractable case is the problem of learning a bn structure
consistent with some known total order     over x and bounded indegree d. in other
words, we restrict attention to structures g where if xi     pa
then xi     xj
and
straightforward; for example, a temporal    ow over the order in which variables take
on their values. in this case, for each xi we can evaluate each possible parent-set
of size d from {x1, . . . , xi   1}. this is polynomial in n (but exponential in d).
unfortunately, the general case,    nding an optimally scoring g   , for bounded
degree d     2, is np-hard. instead of aiming for an algorithm that will always    nd
the highest-scoring network, we resort to heuristic algorithms that attempt to    nd
the best network, but are not guaranteed to do so.

(cid:21)(cid:21)(cid:21) < d. for some domains,    nding an ordering such as this is relatively

(cid:21)(cid:21)(cid:21)pa

g
xj

g
xj

to de   ne the heuristic search algorithm, we must de   ne the search space and
search procedure. we can think of a search space as a graph, where each vertex or
node is a candidate network structure to be considered, and edges denote possible
   moves    that the search procedure can perform. the search procedure de   nes an
algorithm that explores the search space, without necessarily seeing all of it . the
simplest search procedure is the greedy one that whenever it is a node chooses to
move the neighbor that has the highest score, until it reaches a node that has a
better score than all of its neighbors.
to elaborate further, in our case a node in the search space is a complete network
structure g over x . there is a tradeo    in how densely each node is connected with
how e   ective the search will be. if each node has few neighbors, then the search
procedure has to consider only few options at each point of the search. thus, it can
a   ord to evaluate each of these options. however, paths from the initial node to a
good one might be long and complex. on the other hand, if each node has many
neighbors, there are short paths from each point to another, but we might not be
able to pick it, because we don   t have time to evaluate all of the options at each
step.

a good tradeo    for this problem chooses reasonably few neighbors for each node,
but ensures that the    diameter    of the search space remains small. a natural choice
of neighbors of a network structure is a set of structures that are identical to it
except for small    local    modi   cations. the most commonly used operators which
de   ne the local modi   cations are

2.4 learning

53

)

// score

// initial network structure

// a set of search operators

procedure greedy-structure-search (
g   ,
d // fully observed dataset
score,
o,
gbest     g   
do
g     gbest
progress     false
for each operator o     o
go     o(g)
if go is legal structure then
if score(go : d) > score(gbest
gbest     go
progress     true

// result of applying o on g

: d) then

while progress
return gbest

1
2
3
4
5
6
7
8
9
10
11
12
13

figure 2.7 greedy structure search algorithm, with an arbitrary scoring function
score(g : d).

add an edge;
delete an edge;
reverse an edge.
in other words, if we consider the node g, then the neighboring nodes in the
search space are those where we change one edge, either by adding one, deleting
one, or reversing the orientation of one. we only consider operations that result
in legal networks (i.e., acyclic networks satisfying any constraints such as bounded
indegree).

this de   nition of search space is quite natural and has several desirable prop-
erties. first, notice that the diameter of the search space is at most n2. that is,
there is a relatively short path between any two networks we choose. to see this,
note that if we consider traversing a path from g1 to g2, we can start by deleting
all edges in g1 that do not appear in g2, and then we can add the edges that are
in g2 and not in g1. clearly, the number of steps we take is bounded by the total
number of edges we can have, n2.
second, recall that the score of a network g is a sum of local scores. the operations
we consider result in changing only one local score term (in the case of addition
or deletion of an edge) or two (in the case of edge reversal). thus, they result in a
local change in the score     the    main    mass of the score remains the same. this
implies that there is some sense of    continuity    in the score of neighboring nodes.
the search methods most commonly used are local search procedures. such
search procedures are characterized by the following design: they keep a    current   
candidate node. at each iteration they explore some of the neighboring nodes, and

54

id114 in a nutshell

then decide to make a    step    to one of the neighbors and make it the current
candidate. these iterations are repeated until some termination condition. in other
words, local search procedures can be thought of as keeping one pointer into the
search space and moving it around.

one of the simplest, and often used, search procedures is the greedy hill-climbing
procedure. the intuition is simple. as the name suggests, at each step we take the
step that leads to the largest improvement in the score. the actual details of the
procedure are shown in    gure 2.7. we pick an initial network structure g as a
starting point; this network can be the empty one, a random choice, the best tree,
or a network obtained from some prior knowledge. we compute its score. we then
consider all of the neighbors of g in the space     all of the legal networks obtained
by applying a single operator to g     and compute the score for each of them. we
then apply the change that leads to the best improvement in the score. we continue
this process until no modi   cation improves the score.

we can improve on the performance of greedy hill-climbing by using more clever

search algorithms. some common extensions are:

tabu search: keep a list of k most recently visited structures and avoid them,
i.e., apply the best move that leads to a structure not on the list. this approach
deals with local maxima whose    hill    has fewer than k structures.
random restarts: once stuck, apply some    xed number of random edge changes
and then restart the greedy search. at the end of the search, select the best
structure encountered anywhere on the trajectory. this approach can escape from
the basin of one local maximum to another.
simulated annealing: evaluate operators in random order. if the randomly
selected operator induces an uphill step, move to the resulting structure. (note:
it does not have to be the best of the current neighbors.) if the operator induces a
downhill step, apply it with id203 inversely proportional to the reduction in
score. a temperature parameter determines the id203 of taking downhill
steps. as the search progress, the temperature decreases, and the algorithm
becomes less likely to take a downhill step.

2.5 conclusion

this chapter presented a condensed description of id114, including their
representation, id136 algorithms, and learning algorithms. many topics have not
been covered; we refer the reader to [8] for a more complete description.

references

[1] a. becker and d. geiger. a su   ciently fast algorithm for    nding close to

optimal clique trees. arti   cial intelligence, 125(1-2):3   17, 2001.

references

55

[2] w. buntine. chain graphs for learning. in proceedings of the conference on

uncertainty in arti   cial intelligence, 1995.

[3] r. g. cowell, a. p. dawid, s. l. lauritzen, and d. j. spiegelhalter. probabilistic

networks and id109. springer-verlag, new york, 1999.

[4] d. heckerman. a tutorial on learning with id110s. technical

report msr-tr-95-06, microsoft research, seattle, wa, 1996.

[5] f. v. jensen. id110s and decision graphs. springer-verlag, new

york, 2001.

[6] m. i. jordan, editor. learning in graphics models. the mit press, cambridge,

ma, 1998.

[7] m. i. jordan. id114. statistical science (special issue on bayesian

statistics), 19:140   155, 2004.

[8] d. koller and n. friedman. bns and beyond, 2007. to appear.
[9] s. lauritzen. id114. oxford university press, new york, 1996.
[10] s. lauritzen and n. wermuth. id114 for association between
variables, some of which are qualitative and some quantitative. annals of
statistics, 17(1):31   57, 1989.

[11] j. pearl. probabilistic reasoning in intelligent systems. morgan kaufmann,

san mateo, ca, 1988.

[12] k. shoikhet and d. geiger. a practical algorithm for    nding optimal trian-
gulations. in proceedings of the national conference on arti   cial intelligence,
1997.

