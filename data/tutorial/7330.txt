arti   cal neural networks

stat 27725/cmsc 25400: machine learning

shubhendu trivedi

university of chicago

november 2015

arti   cal neural networks

stat 27725/cmsc 25400

things we will look at today

    biological neural networks as inspiration for arti   cial

neural networks

    model of a neuron (id88)
    multi-layer id88s
    training feed forward networks: the id26

algorithm

    deep learning: convolutional neural networks
    visualization of learned features

arti   cal neural networks

stat 27725/cmsc 25400

neural networks

the human brain has an estimated 1011 neurons, each
connected on average, to 104 others
inputs come from the dendrites, are aggregated in the soma.
if the neuron starts    ring, impulses are propagated to other
neurons via axons

neuron activity is typically excited or inhibited through
connections to other neurons

the fastest neuron switching times are known to be on the
order of 10   3 seconds - quite slow compared to computer
switching speeds of 10   10 seconds
yet, humans are surprisingly quick in making complex
decisions: example - takes roughly 10   1 seconds to visually
recognize your mother

arti   cal neural networks

stat 27725/cmsc 25400

neural networks

note that the sequence of neurons    rings that can take place
during this interval cannot be possibly more than a few
hundred steps (given the switching speed of the neurons)

thus, depth of the network can   t be great (clear layer by layer
organization in the visual system)

this observation has led many to speculate that the
information-processing abilities of biological neural systems
must follow from highly parallel processes, operating on
representations that are distributed over many neurons.

arti   cal neural networks

stat 27725/cmsc 25400

neural networks

neurons are simple. but their arrangement in multi-layered
networks is very powerful

they self organize. learning e   ectively is change in
organization (or connection strengths).

humans are very good at recognizing patterns. how does the
brain do it?

arti   cal neural networks

stat 27725/cmsc 25400

neural networks

in the perceptual system, neurons represent features of the
sensory input

the brain learns to extract many layers of features. features
in one layer represent more complex combinations of features
in the layer below. (e.g. hubel weisel (vision), 1959, 1962)

how can we imitate such a process on a computer?

arti   cal neural networks

stat 27725/cmsc 25400

neural networks

[slide credit: thomas serre]

arti   cal neural networks

stat 27725/cmsc 25400

first generation neural networks: mccullogh

pitts (1943)

arti   cal neural networks

stat 27725/cmsc 25400

a model adaptive neuron

this is just a id88 (seen earlier in class)
assumes data are linearly separable. simple stochastic
algorithm for learning the linear classi   er
theorem (noviko   , 1962) let w, w0 be a linear separator
with (cid:107)w(cid:107) = 1, and margin   . then id88 will converge
after

(cid:16) (maxi (cid:107)xi(cid:107))2

(cid:17)

o

  2

arti   cal neural networks

stat 27725/cmsc 25400

id88 as a model of the brain?

id88 developed in the 1950s

key publication: the id88: a probabilistic model for
information storage and organization in the brain, frank
rosenblatt, psychological review, 1958

goal: pattern classi   cation

from    mechanization of thought process    (1959):    the
navy revealed the embryo of an electronic computer today
that it expects will be able to walk, talk, see, write, reproduce
itself and be conscious of its existence. later id88s will
be able to recognize people and call out their names and
instantly translate speech in one language to speech and
writing in another language, it was predicted.   

another ancient milestone: hebbian learning rule (donald
hebb, 1949)

arti   cal neural networks

stat 27725/cmsc 25400

id88 as a model of the brain?

the mark i id88 machine was the    rst implementation
of the id88 algorithm.
the machine was connected to a camera that used 2020
cadmium sul   de photocells to produce a 400-pixel image. the
main visible feature is a patchboard that allowed
experimentation with di   erent combinations of input features.
to the right of that are arrays of potentiometers that
implemented the adaptive weights

arti   cal neural networks

stat 27725/cmsc 25400

adaptive neuron: id88

a id88 represents a decision surface in a d dimensional
space as a hyperplane
works only for those sets of examples that are linearly
separable
many boolean functions can be represented by a id88:
and, or, nand,nor

arti   cal neural networks

stat 27725/cmsc 25400

problems?

if features are complex enough, anything can be classi   ed

thus features are really hand coded. but it comes with a
clever algorithm for weight updates

if features are restricted, then some interesting tasks cannot
be learned and thus id88s are fundamentally limited in
what they can do. famous examples: xor, group invariance
theorems (minsky, papert, 1969)

arti   cal neural networks

stat 27725/cmsc 25400

coda

single neurons are not able to solve complex tasks (linear
decision boundaries). limited in the input-output mappings
they can learn to model.

more layers of linear units are not enough (still linear). a    xed
non-linearity at the output is not good enough either

we could have multiple layers of adaptive, non-linear hidden
units. these are called multi-layer id88s

were considered a solution to represent nonlinearly separable
function in the 70s

many local minima: id88 convergence theorem does
not apply.

intuitive conjecture in the 60s: there is no learning algorithm
for multilayer id88s

arti   cal neural networks

stat 27725/cmsc 25400

multi-layer id88s

digression: kernel methods

we have looked at how each neuron will look like

but did not mention id180. some common
choices:

how can we learn the weights?

arti   cal neural networks

stat 27725/cmsc 25400

learning multiple layers of features

[slide: g. e. hinton]

arti   cal neural networks

stat 27725/cmsc 25400

review: neural networks

w(2)
0

w(2)
1

w(1)
21

f

w(2)
2

h

w(1)
d1

. . .

w(2)
m

. . .

h

h0     1

x0     1 w(1)

h

01
w(1)
11

x1

feedforward operation, from input x to output   y:

x2

       m(cid:88)

xd

(cid:32) d(cid:88)

(cid:33)

      

+ w(2)
0

  y(x; w) = f

w(2)
j h

w(1)
ij xi + w(1)

0j

j=1

i=1

slide adapted from ttic 31020, gregory shakhnarovich

arti   cal neural networks

stat 27725/cmsc 25400

training the network

error of the network on a training set:

n(cid:88)

i=1

l(x; w) =

(yi       y(xi; w))2

1
2

generally, no closed-form solution;
resort to id119

let   s start with a simple linear model   y =(cid:80)

need to evaluate derivative of l on a single example
j wjxij:

   l(xi)

   wj

= (  yi     yi)

xij.

(cid:124)

(cid:123)(cid:122)

error

(cid:125)

arti   cal neural networks

stat 27725/cmsc 25400

id26

      (cid:88)

      

general unit activation in a multilayer network:

zt

h

zt = h

wjtzj

j

forward propagation: calculate for each unit at =(cid:80)

w2t. . .

wst

w1t

z1

z2

zs

j wjtzj

the loss l depends on wjt only through at:

   l
   wjt

=

   l
   at

   at
   wjt

=

   l
   at

zj

arti   cal neural networks

stat 27725/cmsc 25400

id26

   l
   wjt

=

   l
   at

zj

   l
   wjt

=

zj

   l

   at(cid:124)(cid:123)(cid:122)(cid:125)

  t

output unit with linear activation:   t =   y     y
hidden unit zt = h(at) which sends inputs to units s:

(cid:88)

  t =

   as
   at

   l
   as
= h(cid:48)(at)

s   s

(cid:88)

wts  s

zt

s   s

as =

wts

zs

. . .

(cid:88)

j:j   s

wjsh(aj)

arti   cal neural networks

stat 27725/cmsc 25400

id26: example

output: f (a) = a

hidden:

h(a) = tanh(a) =

ea     e   a
ea + e   a ,

w(2)
1

1h

f

w(2)
m

w(2)
2

. . .

2h

m h

h(cid:48)(a) = 1     h(a)2.

w(1)
11

w(1)
21

x0 x1

. . .

w(1)
d1

xd

given example x, feed-forward inputs:

input to hidden: aj =

d(cid:88)

i=0

w(1)

ij xi,

m(cid:88)

hidden output: zj = tanh(aj),

net output:   y = a =

w(2)

j zj.

arti   cal neural networks

stat 27725/cmsc 25400

j=0

id26: example

m(cid:88)

j=0

d(cid:88)

i=0

aj =

w(1)

ij xi,

zj = tanh(aj),

  y = a =

w(2)

j zj.

2 (y       y)2.

error on example x: l = 1
output unit:    =    l
next, compute   s for the hidden units:
  j = (1     zj)2w(2)
j   

   a = y       y.

derivatives w.r.t. weights:

   l
   w(1)
ij

=   jxi,

   l
   w(2)

j

=   zj.

update weights: wj     wj         zj and w(1)
is called the weight decay

ij     w(1)

ij         jxi.   

arti   cal neural networks

stat 27725/cmsc 25400

multidimensional output

loss on example (x, y):

k(cid:88)

k=1

1
2

(yk       yk)2

f

k

w(2)
mk

m h

f

f
. . .

k

w(2)
1k

w(2)
. . .
2k

2h

1h

w(1)
11

w(1)
21

x0 x1

. . .

w(1)
d1

xd

now, for each output unit   k = yk       yk;
for hidden unit j,

k(cid:88)

  j = (1     zj)2

w(2)

jk   k.

k=1

arti   cal neural networks

stat 27725/cmsc 25400

multilayer id88s

theoretical result [cybenko, 1989]: 2-layer net with linear
output can approximate any continuous function over compact
domain to arbitrary accuracy (given enough hidden units!)
the more number of hidden layers, the better...
.. in theory. large neural networks need a lot of labeled data,
and optimization is hard
neural networks went out of fashion due to this reason
roughly between 1990-2005
since 2006, they have made a comeback. mostly due to
availability of large datasets, more computational resources at
hand, and a number of tricks to make them work
return very competitive and state of the art performance in
tasks with perceptual input such as vision and speech (better
than human performance in some tasks), and dominating the
landscape in natural language processing

arti   cal neural networks

stat 27725/cmsc 25400

deep learning: convolutional neural networks

arti   cal neural networks

stat 27725/cmsc 25400

hierarchial representations

let   s elaborate a bit.

arti   cal neural networks

stat 27725/cmsc 25400

why use deep multi layered models?

argument 1: visual scenes are hierarchially organized (so is
language, neural nets for that next time)

arti   cal neural networks

stat 27725/cmsc 25400

why use deep multi layered models?

argument 2: biological vision is hierarchically organized, and we
want to glean some ideas from there

argument 3: shallow representations are ine   cient at representing
highly varying functions

arti   cal neural networks

stat 27725/cmsc 25400

why use deep multi layered models?

[figure: honglak lee]

arti   cal neural networks

stat 27725/cmsc 25400

motivation: vision

how can we produce good internal representations of visual
data to support recognition?

what do we mean by good? the learning machine should be
able to classify objects into classes, and not be a   ected by
things such as pose, scale, position of the object in the image,
lighting conditions, clutter, occlusion etc.

one way to attempt to do this has resulted in a breed of
feedforward neural networks with a very speci   c kind of
architecture - convolutional neural networks. this
architecture tries to capture some of the above invariances.

originally introduced in 1989 (id26 applied to
handwriting zip code recognition, y. lecun, 1989;
gradient-based learning applied to document recognition,
lecun et al, 1998)

arti   cal neural networks

stat 27725/cmsc 25400

convolutional neural networks

figure: yann lecun

arti   cal neural networks

stat 27725/cmsc 25400

convolutional neural networks

feedforward feature extraction: convolve input with learned
   lters     non-linearity     spatial pooling     id172.
training is done by backpropagating errors

arti   cal neural networks

stat 27725/cmsc 25400

convolutional layer

arti   cal neural networks

stat 27725/cmsc 25400

convolutional layer

arti   cal neural networks

stat 27725/cmsc 25400

convolutional layer

arti   cal neural networks

stat 27725/cmsc 25400

convolutional layer

[illustration by ranzato. mathieu et al. fast training of id98s
through ffts]

arti   cal neural networks

stat 27725/cmsc 25400

convolutional layer

learn multiple such    lters

if 100    lters are used, we get 100 feature maps

arti   cal neural networks

stat 27725/cmsc 25400

subsampling

pass the each    pixel    of the feature map through a
nonlinearity

subsample to reduce the size of feature map into half (need
not be half)

repeat convolutions on these reduced images, followed by
non-linearity and subsampling.

eventually we will have feature maps of size 1. these are fed
to a classi   er, such as id166 for the    nal classi   cation

arti   cal neural networks

stat 27725/cmsc 25400

visualizing features: id163 challenge

2012

14 million labeled images with 20,000 classes

images gathered from the internet and labeled by humans via
amazon turk

challenge: 1.2 million training images, 1000 classes.

arti   cal neural networks

stat 27725/cmsc 25400

visualizing features: id163 challenge

2012

winning model (   alexnet   ) was a convolutional network
similar to yann lecun, 1998
more data: 1.2 million versus a few thousand images
fast two gpu implementation trained for a week
better id173 (dropout, next time?)

[a. krizhevsky, i. sutskever, g. e. hinton: id163 classi   cation
with deep convolutional neural networks, nips 2012]

arti   cal neural networks

stat 27725/cmsc 25400

layer 1    lters

arti   cal neural networks

stat 27725/cmsc 25400

layer 2 patches

arti   cal neural networks

stat 27725/cmsc 25400

layer 2 patches

arti   cal neural networks

stat 27725/cmsc 25400

layer 3 patches

arti   cal neural networks

stat 27725/cmsc 25400

layer 3 patches

arti   cal neural networks

stat 27725/cmsc 25400

layer 4 patches

arti   cal neural networks

stat 27725/cmsc 25400

layer 4 patches

arti   cal neural networks

stat 27725/cmsc 25400

evolution of filters

arti   cal neural networks

stat 27725/cmsc 25400

evolution of filters

arti   cal neural networks

stat 27725/cmsc 25400

arti   cal neural networks

stat 27725/cmsc 25400

conv. net successes

very active area of research. best accuracies on google street
view, mnist, tra   c sign recognition, id164, face
recognition (deepface is used on fb) and detection, semantic
segmentation are by using convolutional networks

current networks are deeper (google lenet has 22 layers,
   id199    (schmidhuber et al) can be deeper still)

arti   cal neural networks

stat 27725/cmsc 25400

next time

id173 in training feedforward networks

basic ideas of neural generative models (restricted boltzmann
machines, deep belief nets), autoencoders. connections to
manifold learning

recurrent neural networks (modeling sequences)

time permitting: id56s and neural word
embeddings

arti   cal neural networks

stat 27725/cmsc 25400

