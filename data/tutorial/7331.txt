arti   cial neural networks ii

stat 27725/cmsc 25400: machine learning

shubhendu trivedi

university of chicago

november 2015

arti   cial neural networks ii

stat 27725/cmsc 25400

things we will look at today

arti   cial neural networks ii

stat 27725/cmsc 25400

things we will look at today

    id173 in neural networks

arti   cial neural networks ii

stat 27725/cmsc 25400

things we will look at today

    id173 in neural networks
    drop out

arti   cial neural networks ii

stat 27725/cmsc 25400

things we will look at today

    id173 in neural networks
    drop out
    sequence to sequence learning using recurrent neural

networks

arti   cial neural networks ii

stat 27725/cmsc 25400

things we will look at today

    id173 in neural networks
    drop out
    sequence to sequence learning using recurrent neural

networks

    generative neural methods

arti   cial neural networks ii

stat 27725/cmsc 25400

a short primer on id173: empirical

risk

assume that the data are sampled from an unknown
distribution p(x, y)

arti   cial neural networks ii

stat 27725/cmsc 25400

a short primer on id173: empirical

risk

assume that the data are sampled from an unknown
distribution p(x, y)
next we choose the id168 l, and a parametric model
family f (x; w)

arti   cial neural networks ii

stat 27725/cmsc 25400

a short primer on id173: empirical

risk

assume that the data are sampled from an unknown
distribution p(x, y)
next we choose the id168 l, and a parametric model
family f (x; w)
ideally, our goal is to minimize the expected loss, called the
risk

r(w) = e(x0,y0)   p(x,y)[l(f (x0; w), y0)]

arti   cial neural networks ii

stat 27725/cmsc 25400

a short primer on id173: empirical

risk

assume that the data are sampled from an unknown
distribution p(x, y)
next we choose the id168 l, and a parametric model
family f (x; w)
ideally, our goal is to minimize the expected loss, called the
risk

r(w) = e(x0,y0)   p(x,y)[l(f (x0; w), y0)]

the true distribution is unknown. so, we instead work with a
proxy that is measurable: empirical loss on the training set

l(w, x, y) =

1
n

l(f (xi; w), yi)

n(cid:88)

i=1

arti   cial neural networks ii

stat 27725/cmsc 25400

model complexity and over   tting

consider data drawn from a 3rd order model:

arti   cial neural networks ii

stat 27725/cmsc 25400

how to avoid over   tting?

if a model over   ts (is too sensitive to the data), it would be
unstable and will not generalize well.

arti   cial neural networks ii

stat 27725/cmsc 25400

how to avoid over   tting?

if a model over   ts (is too sensitive to the data), it would be
unstable and will not generalize well.

intuitively, the complexity of the model can be measured by
the number of    degrees of freedom    (independent
parameters) (previous example?)

arti   cial neural networks ii

stat 27725/cmsc 25400

how to avoid over   tting?

if a model over   ts (is too sensitive to the data), it would be
unstable and will not generalize well.

intuitively, the complexity of the model can be measured by
the number of    degrees of freedom    (independent
parameters) (previous example?)

idea: directly penalize by the number of parameters (called
the akaike information criterion): minimize

n(cid:88)

l(f (xi; w), yi) + #params

i=1

arti   cial neural networks ii

stat 27725/cmsc 25400

description length

intuition: should not penalize the parameters, but the number
of bits needed to encode the parameters

arti   cial neural networks ii

stat 27725/cmsc 25400

description length

intuition: should not penalize the parameters, but the number
of bits needed to encode the parameters

with a    nite set of parameter values, these are equivalent.
with an in   nite set, we can limit the e   ective number of
degrees of freedom by restricting the value of the parameters.

arti   cial neural networks ii

stat 27725/cmsc 25400

description length

intuition: should not penalize the parameters, but the number
of bits needed to encode the parameters

with a    nite set of parameter values, these are equivalent.
with an in   nite set, we can limit the e   ective number of
degrees of freedom by restricting the value of the parameters.

then we can have regularized risk minimization:

n(cid:88)

l(f (xi; w), yi) +    (w)

i=1

arti   cial neural networks ii

stat 27725/cmsc 25400

description length

intuition: should not penalize the parameters, but the number
of bits needed to encode the parameters

with a    nite set of parameter values, these are equivalent.
with an in   nite set, we can limit the e   ective number of
degrees of freedom by restricting the value of the parameters.

then we can have regularized risk minimization:

n(cid:88)

l(f (xi; w), yi) +    (w)

i=1

we can measure    size    in di   erent ways: l1, l2 norms

arti   cial neural networks ii

stat 27725/cmsc 25400

description length

intuition: should not penalize the parameters, but the number
of bits needed to encode the parameters

with a    nite set of parameter values, these are equivalent.
with an in   nite set, we can limit the e   ective number of
degrees of freedom by restricting the value of the parameters.

then we can have regularized risk minimization:

n(cid:88)

l(f (xi; w), yi) +    (w)

i=1

we can measure    size    in di   erent ways: l1, l2 norms

id173 is basically a way to implement occam   s razor

arti   cial neural networks ii

stat 27725/cmsc 25400

id173 in neural networks

we have infact already looked at one method (for vision tasks)

arti   cial neural networks ii

stat 27725/cmsc 25400

id173 in neural networks

we have infact already looked at one method (for vision tasks)

how is this a form of id173?

arti   cial neural networks ii

stat 27725/cmsc 25400

id173 in neural networks

weight decay: penalize (cid:107)w l(cid:107)2 or (cid:107)w l(cid:107)1 in every layer

arti   cial neural networks ii

stat 27725/cmsc 25400

id173 in neural networks

weight decay: penalize (cid:107)w l(cid:107)2 or (cid:107)w l(cid:107)1 in every layer
why is it called weight decay?

arti   cial neural networks ii

stat 27725/cmsc 25400

id173 in neural networks

weight decay: penalize (cid:107)w l(cid:107)2 or (cid:107)w l(cid:107)1 in every layer
why is it called weight decay?
parameter sharing (id98s, id56s)

arti   cial neural networks ii

stat 27725/cmsc 25400

id173 in neural networks

weight decay: penalize (cid:107)w l(cid:107)2 or (cid:107)w l(cid:107)1 in every layer
why is it called weight decay?
parameter sharing (id98s, id56s)
dataset augmentation id163 2012, discussed last time
was won by signi   cant dataset augmentation

arti   cial neural networks ii

stat 27725/cmsc 25400

id173 in neural networks

early stoppping:

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout

a more exotic id173 technique. introduced in 2012
and one of the factors in the recent neural net successes

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout

a more exotic id173 technique. introduced in 2012
and one of the factors in the recent neural net successes

every sample is processed by a decimated neural network

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout

a more exotic id173 technique. introduced in 2012
and one of the factors in the recent neural net successes

every sample is processed by a decimated neural network

but, they all do the same job, and share weights

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout

a more exotic id173 technique. introduced in 2012
and one of the factors in the recent neural net successes

every sample is processed by a decimated neural network

but, they all do the same job, and share weights

dropout: a simple way to prevent neural networks from over   tting, n srivastava, g hinton, a krizhevsky, i

sutskever, r salakhutdinov, jmlr 2014

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout: feedforward operation

, and yl+1

i = f (z(l+1)

i

)

= w(l+1)

i

yl + b(l+1)

i

i

without dropout: z(l+1)
with dropout:
r(l)
j = bernoulli(p)
  y(l) = r(l)     y(l)
z(l+1)
= w(l+1)
i
i = f (z(l+1)
yl+1

  yl + b(l+1)
)

i

i

i

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout: at test time

use a single neural net with weights scaled down

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout: at test time

use a single neural net with weights scaled down
by doing this scaling, 2n networks with shared weights can be
combined into a single neural network to be used at test time

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout: at test time

use a single neural net with weights scaled down
by doing this scaling, 2n networks with shared weights can be
combined into a single neural network to be used at test time

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout: at test time

use a single neural net with weights scaled down
by doing this scaling, 2n networks with shared weights can be
combined into a single neural network to be used at test time

extreme form of id112

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout: performance

these architectures have 2 to 4 hidden layers with 1024 to 2048
hidden units

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout: performance

dropout: a simple way to prevent neural networks from over   tting, n srivastava, g hinton, a krizhevsky, i

sutskever, r salakhutdinov, jmlr 2014

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout: e   ect on sparsity

dropout: a simple way to prevent neural networks from over   tting, n srivastava, g hinton, a krizhevsky, i

sutskever, r salakhutdinov, jmlr 2014

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout for id75

objective: (cid:107)y     xw(cid:107)2

2

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout for id75

2

objective: (cid:107)y     xw(cid:107)2
when input is dropped out such that any input dimension is
retained with id203 p. the input can be expressed as
r     x where r     {0, 1}n  d is a random matrix with
rij     bernoulli(p)

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout for id75

2

objective: (cid:107)y     xw(cid:107)2
when input is dropped out such that any input dimension is
retained with id203 p. the input can be expressed as
r     x where r     {0, 1}n  d is a random matrix with
rij     bernoulli(p)
marginalizing the noise, the objective becomes:
er    bernoulli(p)(cid:107)y     (r     x)w(cid:107)2

min

2

w

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout for id75

2

objective: (cid:107)y     xw(cid:107)2
when input is dropped out such that any input dimension is
retained with id203 p. the input can be expressed as
r     x where r     {0, 1}n  d is a random matrix with
rij     bernoulli(p)
marginalizing the noise, the objective becomes:
er    bernoulli(p)(cid:107)y     (r     x)w(cid:107)2

min

2

w

this is the same as:

(cid:107)y   pxw(cid:107)2

2 +p(1   p)(cid:107)  w(cid:107)2

2 where    = (diag(x t x))1/2

min

w

arti   cial neural networks ii

stat 27725/cmsc 25400

dropout for id75

2

objective: (cid:107)y     xw(cid:107)2
when input is dropped out such that any input dimension is
retained with id203 p. the input can be expressed as
r     x where r     {0, 1}n  d is a random matrix with
rij     bernoulli(p)
marginalizing the noise, the objective becomes:
er    bernoulli(p)(cid:107)y     (r     x)w(cid:107)2

min

2

w

this is the same as:

(cid:107)y   pxw(cid:107)2

2 +p(1   p)(cid:107)  w(cid:107)2

2 where    = (diag(x t x))1/2

min

w

thus, dropout with id75 is equivalent, in
expectation to ridge regression with a particular form of   

arti   cial neural networks ii

stat 27725/cmsc 25400

why does this make sense?

id112 is always good if models are diverse enough

arti   cial neural networks ii

stat 27725/cmsc 25400

why does this make sense?

id112 is always good if models are diverse enough
motivation 1: ten conspiracies each involving    ve people is
probably a better way to wreak havoc than a conspiracy
involving 50 people. if conditions don   t change (stationary)
and plenty of time for rehearsal, a big conspiracy can work
well, but otherwise will    over   t   

arti   cial neural networks ii

stat 27725/cmsc 25400

why does this make sense?

id112 is always good if models are diverse enough
motivation 1: ten conspiracies each involving    ve people is
probably a better way to wreak havoc than a conspiracy
involving 50 people. if conditions don   t change (stationary)
and plenty of time for rehearsal, a big conspiracy can work
well, but otherwise will    over   t   
motivation 2: comes from a theory for the superiority of
sexual reproduction in evolution (livnat, papadimitriou,
pnas, 2010).

arti   cial neural networks ii

stat 27725/cmsc 25400

why does this make sense?

id112 is always good if models are diverse enough
motivation 1: ten conspiracies each involving    ve people is
probably a better way to wreak havoc than a conspiracy
involving 50 people. if conditions don   t change (stationary)
and plenty of time for rehearsal, a big conspiracy can work
well, but otherwise will    over   t   
motivation 2: comes from a theory for the superiority of
sexual reproduction in evolution (livnat, papadimitriou,
pnas, 2010).
seems plausible that asexual reproduction should be a better
way to optimize for individual    tness (in sexual reproduction if
a good combination is found, it   s split again)

arti   cial neural networks ii

stat 27725/cmsc 25400

why does this make sense?

id112 is always good if models are diverse enough
motivation 1: ten conspiracies each involving    ve people is
probably a better way to wreak havoc than a conspiracy
involving 50 people. if conditions don   t change (stationary)
and plenty of time for rehearsal, a big conspiracy can work
well, but otherwise will    over   t   
motivation 2: comes from a theory for the superiority of
sexual reproduction in evolution (livnat, papadimitriou,
pnas, 2010).
seems plausible that asexual reproduction should be a better
way to optimize for individual    tness (in sexual reproduction if
a good combination is found, it   s split again)
criterion for natural selection may not be individual    tness
but mixability. thus role of sexual reproduction is not just to
allow useful new genes to propagate but also to ensure that
complex coadaptations between genes are broken

arti   cial neural networks ii

stat 27725/cmsc 25400

sequence learning with neural networks

arti   cial neural networks ii

stat 27725/cmsc 25400

problems with mlps for sequence tasks

the    api    is too limited. they only accept an input of a
   xed dimensionality and map it to an output that is again of a
   xed dimensionality

arti   cial neural networks ii

stat 27725/cmsc 25400

problems with mlps for sequence tasks

the    api    is too limited. they only accept an input of a
   xed dimensionality and map it to an output that is again of a
   xed dimensionality

this is great when working (for example) with images, and
the output is an encoding of the category

arti   cial neural networks ii

stat 27725/cmsc 25400

problems with mlps for sequence tasks

the    api    is too limited. they only accept an input of a
   xed dimensionality and map it to an output that is again of a
   xed dimensionality

this is great when working (for example) with images, and
the output is an encoding of the category

this is bad when if we are interested in machine translation
or id103

arti   cial neural networks ii

stat 27725/cmsc 25400

problems with mlps for sequence tasks

the    api    is too limited. they only accept an input of a
   xed dimensionality and map it to an output that is again of a
   xed dimensionality

this is great when working (for example) with images, and
the output is an encoding of the category

this is bad when if we are interested in machine translation
or id103

traditional neural networks treat every example
independently. imagine the task is to classify events at every
   xed point in the movie. a plain vanilla neural network would
not be able to use its knowledge about the previous events to
help in classifying the current.

arti   cial neural networks ii

stat 27725/cmsc 25400

problems with mlps for sequence tasks

the    api    is too limited. they only accept an input of a
   xed dimensionality and map it to an output that is again of a
   xed dimensionality

this is great when working (for example) with images, and
the output is an encoding of the category

this is bad when if we are interested in machine translation
or id103

traditional neural networks treat every example
independently. imagine the task is to classify events at every
   xed point in the movie. a plain vanilla neural network would
not be able to use its knowledge about the previous events to
help in classifying the current.

recurrent neural networks address this issue by having loops.

arti   cial neural networks ii

stat 27725/cmsc 25400

some sequence tasks

figure credit: andrej karpathy

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

the loops in them allow the information to persist

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

the loops in them allow the information to persist

for some input xi, we pass it through a hidden state a and
then output a value hi. the loop allows information to be
passed from one time step to another

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

the loops in them allow the information to persist

for some input xi, we pass it through a hidden state a and
then output a value hi. the loop allows information to be
passed from one time step to another

a id56 can be thought of as multiple copies of the same
network, each of which passes a message to its successor

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

more generally, a id56 can be thought of as arranging hidden
state vectors hl
and l = 1, . . . , l being the depth

t in a 2-d grid, with t = 1, . . . , t being time

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

t in a 2-d grid, with t = 1, . . . , t being time

more generally, a id56 can be thought of as arranging hidden
state vectors hl
and l = 1, . . . , l being the depth
h0
t = xt and hl
t
intermediate vectors hl
hl   1

is used to predict the output vector yt. all
t   1

t are computed as a function of hl

t

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

t in a 2-d grid, with t = 1, . . . , t being time

more generally, a id56 can be thought of as arranging hidden
state vectors hl
and l = 1, . . . , l being the depth
h0
t = xt and hl
t
intermediate vectors hl
hl   1
id56 is a recurrence of the form:

is used to predict the output vector yt. all
t   1

t are computed as a function of hl

t

(cid:19)

(cid:18) hl   1

t
hl
t   1

hl
t = tanhw l

illustration credit: chris olah

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

the chain like structure enables sequence modeling

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

the chain like structure enables sequence modeling

w varies between layers but is shared through time

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

the chain like structure enables sequence modeling

w varies between layers but is shared through time

basically the inputs from the layer below and before in time
are transformed by a non-linearity after an additive interaction
(weak coupling)

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

the chain like structure enables sequence modeling

w varies between layers but is shared through time

basically the inputs from the layer below and before in time
are transformed by a non-linearity after an additive interaction
(weak coupling)

the plain vanilla id56 described is infact turing complete
with the right size and weight matrix

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

the chain like structure enables sequence modeling

w varies between layers but is shared through time

basically the inputs from the layer below and before in time
are transformed by a non-linearity after an additive interaction
(weak coupling)

the plain vanilla id56 described is infact turing complete
with the right size and weight matrix

   if training vanilla neural nets is optimization over functions,
training recurrent nets is optimization over programs   

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

training id56s might seem daunting.

infact, we can simply adopt the id26 algorithm
after unrolling the id56

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

training id56s might seem daunting.

infact, we can simply adopt the id26 algorithm
after unrolling the id56

if we have to look at sequences of size s, we unroll each loop
into s steps, and treat it as a normal neural network to train
using id26

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

training id56s might seem daunting.

infact, we can simply adopt the id26 algorithm
after unrolling the id56

if we have to look at sequences of size s, we unroll each loop
into s steps, and treat it as a normal neural network to train
using id26

this is called id26 through time

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

training id56s might seem daunting.

infact, we can simply adopt the id26 algorithm
after unrolling the id56

if we have to look at sequences of size s, we unroll each loop
into s steps, and treat it as a normal neural network to train
using id26

this is called id26 through time

but weights are shared across di   erent time stamps? how is
this constraint enforced?

arti   cial neural networks ii

stat 27725/cmsc 25400

recurrent neural networks

training id56s might seem daunting.

infact, we can simply adopt the id26 algorithm
after unrolling the id56

if we have to look at sequences of size s, we unroll each loop
into s steps, and treat it as a normal neural network to train
using id26

this is called id26 through time

but weights are shared across di   erent time stamps? how is
this constraint enforced?

train the network as if there were no constraints, obtain
weights at di   erent time stamps, average them

arti   cial neural networks ii

stat 27725/cmsc 25400

problems

recurrent neural networks have trouble learning long term
dependencies (hochreiter and schmidhuber, 1991 and bengio
et al, 1994)
consider a language model in which the task is to predict the
next word based on the previous

arti   cial neural networks ii

stat 27725/cmsc 25400

problems

recurrent neural networks have trouble learning long term
dependencies (hochreiter and schmidhuber, 1991 and bengio
et al, 1994)
consider a language model in which the task is to predict the
next word based on the previous
sometimes the context can be clear immediately:    the clouds
are in the sky   

arti   cial neural networks ii

stat 27725/cmsc 25400

problems

recurrent neural networks have trouble learning long term
dependencies (hochreiter and schmidhuber, 1991 and bengio
et al, 1994)
consider a language model in which the task is to predict the
next word based on the previous
sometimes the context can be clear immediately:    the clouds
are in the sky   
sometimes the dependency is more long term:    we are
basically from transylvania, although i grew up in spain, but i
can still speak    uent romanian.   
in principle, id56s should be able to learn long term
dependencies with the right parameter choices, but learning
those parameters is hard.

arti   cial neural networks ii

stat 27725/cmsc 25400

problems

recurrent neural networks have trouble learning long term
dependencies (hochreiter and schmidhuber, 1991 and bengio
et al, 1994)
consider a language model in which the task is to predict the
next word based on the previous
sometimes the context can be clear immediately:    the clouds
are in the sky   
sometimes the dependency is more long term:    we are
basically from transylvania, although i grew up in spain, but i
can still speak    uent romanian.   
in principle, id56s should be able to learn long term
dependencies with the right parameter choices, but learning
those parameters is hard.
the long short term memory was proposed to solve this
problem (hochreiter and schmidhuber, 1997)

arti   cial neural networks ii

stat 27725/cmsc 25400

long short term memory networks

vanilla id56: error propagation is blocked by a non-linearity illustration

credit: chris olah

arti   cial neural networks ii

stat 27725/cmsc 25400

long short term memory networks

arti   cial neural networks ii

stat 27725/cmsc 25400

long short term memory

one of the main points about lstm is the cell state ct,
which runs across time and can travel unchanged only with
minor linear interactions

arti   cial neural networks ii

stat 27725/cmsc 25400

long short term memory

one of the main points about lstm is the cell state ct,
which runs across time and can travel unchanged only with
minor linear interactions

the lstm regulates the cell state by various gates, which
gives the ability to remove or add information to the cell state.

arti   cial neural networks ii

stat 27725/cmsc 25400

long short term memory

one of the main points about lstm is the cell state ct,
which runs across time and can travel unchanged only with
minor linear interactions

the lstm regulates the cell state by various gates, which
gives the ability to remove or add information to the cell state.

each of the gates are composed of a sigmoid non-linearity
followed by a pointwise multiplication

arti   cial neural networks ii

stat 27725/cmsc 25400

long short term memory

one of the main points about lstm is the cell state ct,
which runs across time and can travel unchanged only with
minor linear interactions

the lstm regulates the cell state by various gates, which
gives the ability to remove or add information to the cell state.

each of the gates are composed of a sigmoid non-linearity
followed by a pointwise multiplication

there are three types of gates in lstm (e.g. forget gate
helps the lstm to learn to forget)

arti   cial neural networks ii

stat 27725/cmsc 25400

long short term memory

precise form of the lstm update is:

             i

f
o
  ct
t = f (cid:12) cl
cl

             =

             sigm

sigm
sigm
tanh

             w l

(cid:19)

(cid:18) hl   1

t
hl
t   1

t   1 + i (cid:12)   ct, and hl

t = o (cid:12) tanh(cl
t)

arti   cial neural networks ii

stat 27725/cmsc 25400

some applications: id134

id134 (karpathy and li, 2014)

arti   cial neural networks ii

stat 27725/cmsc 25400

id56 shakespeare

using a character level language model trained on all of
shakespeare.
viola: why, salisbury must    nd his    esh and thought that which
i am not aps, not a man and in    re, to show the reining of the
raven and the wars to grace my hand reproach within, and not a
fair are hand, that caesar and my goodly father   s world; when i
was heaven of presence and our    eets, we spare with hours, but
cut thy council i am great, murdered and by thy master   s ready
there my power to give thee but so much as hell: some service in
the noble bondman here, would show him to her wine.
king lear: o, if you were a feeble sight, the courtesy of your
law, your sight and several breath, will wear the gods with his
heads, and my hands are wonder   d at the deeds, so drop upon your
lordship   s head, and your opinion shall be against your honour.

arti   cial neural networks ii

stat 27725/cmsc 25400

image generation

(also uses attention mechanism - not discussed) draw: a
recurrent neural network for image generation (gregor et al.,
2015)

arti   cial neural networks ii

stat 27725/cmsc 25400

applications

acoustic modeling

arti   cial neural networks ii

stat 27725/cmsc 25400

applications

acoustic modeling

natural language processing i.e. parsing etc

arti   cial neural networks ii

stat 27725/cmsc 25400

applications

acoustic modeling

natural language processing i.e. parsing etc

machine translation (e.g. google translate uses id56s)

arti   cial neural networks ii

stat 27725/cmsc 25400

applications

acoustic modeling

natural language processing i.e. parsing etc

machine translation (e.g. google translate uses id56s)

voice transcription

arti   cial neural networks ii

stat 27725/cmsc 25400

applications

acoustic modeling

natural language processing i.e. parsing etc

machine translation (e.g. google translate uses id56s)

voice transcription

video and image understanding

arti   cial neural networks ii

stat 27725/cmsc 25400

applications

acoustic modeling

natural language processing i.e. parsing etc

machine translation (e.g. google translate uses id56s)

voice transcription

video and image understanding

list goes on

arti   cial neural networks ii

stat 27725/cmsc 25400

generative neural models

arti   cial neural networks ii

stat 27725/cmsc 25400

recap: multilayered neural networks

let layer k compute an output vector hk using the output
hk   1 of the previous layer.

arti   cial neural networks ii

stat 27725/cmsc 25400

recap: multilayered neural networks

let layer k compute an output vector hk using the output
hk   1 of the previous layer. note that the input x = h0

arti   cial neural networks ii

stat 27725/cmsc 25400

recap: multilayered neural networks

let layer k compute an output vector hk using the output
hk   1 of the previous layer. note that the input x = h0

hk = tanh(bk + w khk   1)

arti   cial neural networks ii

stat 27725/cmsc 25400

recap: multilayered neural networks

let layer k compute an output vector hk using the output
hk   1 of the previous layer. note that the input x = h0

hk = tanh(bk + w khk   1)

top layer output hl is used for making a prediction. if the
target is given by y, then we de   ne a loss l(hl, y) (convex in
bl + w lhl   1)

arti   cial neural networks ii

stat 27725/cmsc 25400

recap: multilayered neural networks

let layer k compute an output vector hk using the output
hk   1 of the previous layer. note that the input x = h0

hk = tanh(bk + w khk   1)

top layer output hl is used for making a prediction. if the
target is given by y, then we de   ne a loss l(hl, y) (convex in
bl + w lhl   1)
we might have the output layer return the following
non-linearity

(cid:80)

i hl   1

ebl
i+w l
j ebl

j +w l

j hl   1

hl
i =

arti   cial neural networks ii

stat 27725/cmsc 25400

recap: multilayered neural networks

let layer k compute an output vector hk using the output
hk   1 of the previous layer. note that the input x = h0

hk = tanh(bk + w khk   1)

top layer output hl is used for making a prediction. if the
target is given by y, then we de   ne a loss l(hl, y) (convex in
bl + w lhl   1)
we might have the output layer return the following
non-linearity

(cid:80)

i hl   1

ebl
i+w l
j ebl

j +w l

j hl   1

hl
i =

this is called the softmax and can be used as an estimator of
p(y = i|x)

arti   cial neural networks ii

stat 27725/cmsc 25400

recap: multilayered neural networks

one loss to be considered: l(hl, y) =     log p (y = y|x)

arti   cial neural networks ii

stat 27725/cmsc 25400

the di   culty of training deep networks

until 2006, deep architectures were not used extensively in
machine learning

arti   cial neural networks ii

stat 27725/cmsc 25400

the di   culty of training deep networks

until 2006, deep architectures were not used extensively in
machine learning

poor training and generalization errors using the standard
random initialization (with the exception of convolutional
neural networks)

arti   cial neural networks ii

stat 27725/cmsc 25400

the di   culty of training deep networks

until 2006, deep architectures were not used extensively in
machine learning

poor training and generalization errors using the standard
random initialization (with the exception of convolutional
neural networks)

di   cult to propagate gradients to lower layers. too many
connections in a deep architecture

arti   cial neural networks ii

stat 27725/cmsc 25400

the di   culty of training deep networks

until 2006, deep architectures were not used extensively in
machine learning

poor training and generalization errors using the standard
random initialization (with the exception of convolutional
neural networks)

di   cult to propagate gradients to lower layers. too many
connections in a deep architecture

purely discriminative. no generative model for the raw input
features x (connections go upwards)

arti   cial neural networks ii

stat 27725/cmsc 25400

initial breakthrough: layer-wise training

unsupervised pre-training is possible in certain deep
generative models (hinton, 2006)

arti   cial neural networks ii

stat 27725/cmsc 25400

initial breakthrough: layer-wise training

unsupervised pre-training is possible in certain deep
generative models (hinton, 2006)

idea: greedily train one layer at a time using a simple model
(restricted id82)

arti   cial neural networks ii

stat 27725/cmsc 25400

initial breakthrough: layer-wise training

unsupervised pre-training is possible in certain deep
generative models (hinton, 2006)

idea: greedily train one layer at a time using a simple model
(restricted id82)

use the parameters learned to initialize a feedforward neural
network, and    ne tune for classi   cation

arti   cial neural networks ii

stat 27725/cmsc 25400

sigmoid belief networks, 1992

the generative model is decomposed as:

p (x, h1, . . . , hl) = p (hl)

p (hk|hk+1)

(cid:16) l   1(cid:89)

(cid:17)

p (x|h1)

k=1

arti   cial neural networks ii

stat 27725/cmsc 25400

sigmoid belief networks, 1992

the generative model is decomposed as:

p (x, h1, . . . , hl) = p (hl)

p (hk|hk+1)

(cid:16) l   1(cid:89)

(cid:17)

p (x|h1)

k=1

marginalization yields p (x). intractable in practice except for
tiny models

arti   cial neural networks ii

stat 27725/cmsc 25400

sigmoid belief networks, 1992

the generative model is decomposed as:

p (x, h1, . . . , hl) = p (hl)

p (hk|hk+1)

(cid:16) l   1(cid:89)

(cid:17)

p (x|h1)

k=1

marginalization yields p (x). intractable in practice except for
tiny models

r. neal, connectionist learning of belief networks, 1992

dayan, p., hinton, g. e., neal, r., and zemel, r. s. the helmholtz machine, 1995

l. saul, t. jaakkola, and m. jordan, mean    eld theory for sigmoid belief networks, 1996

arti   cial neural networks ii

stat 27725/cmsc 25400

id50, 2006

similar to sigmoid belief networks, except the top two layers
p (x|h1)

p (x, h1, . . . , hl) = p (hl   1, hl)

p (hk|hk+1)

(cid:16) l   2(cid:89)

(cid:17)

k=1

arti   cial neural networks ii

stat 27725/cmsc 25400

id50, 2006

similar to sigmoid belief networks, except the top two layers
p (x|h1)

p (x, h1, . . . , hl) = p (hl   1, hl)

p (hk|hk+1)

(cid:16) l   2(cid:89)

(cid:17)

k=1

arti   cial neural networks ii

stat 27725/cmsc 25400

id50, 2006

similar to sigmoid belief networks, except the top two layers
p (x|h1)

p (x, h1, . . . , hl) = p (hl   1, hl)

p (hk|hk+1)

(cid:16) l   2(cid:89)

(cid:17)

k=1

the joint distribution of the top two layers is a restricted
id82

arti   cial neural networks ii

stat 27725/cmsc 25400

energy based models

before looking at rbms, let   s look at the basics of energy
based models

arti   cial neural networks ii

stat 27725/cmsc 25400

energy based models

before looking at rbms, let   s look at the basics of energy
based models

such models assign a scalar energy to each con   guration of
the variables of interest. learning then corresponds to
modifying the energy function so that its shape has desirable
properties

p (x) =

e   energy(x)

z

where z =

e   energy(x)

(cid:88)

x

arti   cial neural networks ii

stat 27725/cmsc 25400

energy based models

before looking at rbms, let   s look at the basics of energy
based models

such models assign a scalar energy to each con   guration of
the variables of interest. learning then corresponds to
modifying the energy function so that its shape has desirable
properties

p (x) =

e   energy(x)

z

where z =

e   energy(x)

(cid:88)

x

we only care about the marginal (since only x is observed)

arti   cial neural networks ii

stat 27725/cmsc 25400

energy based models

with hidden variables p (x, h) = e   energy(x,h)

z

arti   cial neural networks ii

stat 27725/cmsc 25400

energy based models

with hidden variables p (x, h) = e   energy(x,h)
we only care about the marginal (since only x is observed)

z

p (x) =(cid:80)

e   energy(x,h)

h

z

arti   cial neural networks ii

stat 27725/cmsc 25400

energy based models

with hidden variables p (x, h) = e   energy(x,h)
we only care about the marginal (since only x is observed)

z

p (x) =(cid:80)

e   energy(x,h)

h

z

we can introduce the notion of free-energy

p (x) =

e   freeenergy(x)

z

, with z =

e   freeenergy(x)

(cid:88)

x

arti   cial neural networks ii

stat 27725/cmsc 25400

energy based models

with hidden variables p (x, h) = e   energy(x,h)
we only care about the marginal (since only x is observed)

z

p (x) =(cid:80)

e   energy(x,h)

h

z

we can introduce the notion of free-energy

p (x) =

e   freeenergy(x)

z

, with z =

where

freeenergy(x) =     log

(cid:88)

h

(cid:88)

x

e   freeenergy(x)

e   energy(x,h)

arti   cial neural networks ii

stat 27725/cmsc 25400

energy based models

with hidden variables p (x, h) = e   energy(x,h)
we only care about the marginal (since only x is observed)

z

p (x) =(cid:80)

e   energy(x,h)

h

z

we can introduce the notion of free-energy

p (x) =

e   freeenergy(x)

z

, with z =

where

freeenergy(x) =     log

(cid:88)

h

(cid:88)

x

e   freeenergy(x)

e   energy(x,h)

the data log-likelihood gradient has an interesting form
(details skipped)

arti   cial neural networks ii

stat 27725/cmsc 25400

restricted id82s

x1     h1     p (h|x1)     x2     p (x|h1)     h2     p (h|x2)     . . .

arti   cial neural networks ii

stat 27725/cmsc 25400

back to id50

arti   cial neural networks ii

stat 27725/cmsc 25400

back to id50

everything is completely unsupervised till now. we can treat
these weights learned as an initialization, treat the network as
a feedword network and    ne tune using id26

arti   cial neural networks ii

stat 27725/cmsc 25400

id50

g. e. hinton, r. r. salakhutdinov, reducing the dimensionality of data with neural networks, science, 2006

g. e. hinton, s osindero, yw teh, a fast learning algorithm for deep belief nets, neural computation, 2006

arti   cial neural networks ii

stat 27725/cmsc 25400

id50: object parts

convolutional id50 for scalable unsupervised learning of hierarchical representations. honglak lee,

roger grosse, rajesh ranganath, and andrew y. ng

arti   cial neural networks ii

stat 27725/cmsc 25400

e   ect of unsupervised pre-training

arti   cial neural networks ii

stat 27725/cmsc 25400

e   ect of unsupervised pre-training

arti   cial neural networks ii

stat 27725/cmsc 25400

why does unsupervised pre-training work?

id173. feature representations that are good for
p (x) are good for p (y|x)

arti   cial neural networks ii

stat 27725/cmsc 25400

why does unsupervised pre-training work?

id173. feature representations that are good for
p (x) are good for p (y|x)
optimization: unsupervised pre-training leads to better
regions of the space i.e. better than random initialization

arti   cial neural networks ii

stat 27725/cmsc 25400

why does unsupervised pre-training work?

id173. feature representations that are good for
p (x) are good for p (y|x)
optimization: unsupervised pre-training leads to better
regions of the space i.e. better than random initialization

arti   cial neural networks ii

stat 27725/cmsc 25400

autoencoders

main idea

sparse autoencoders

denoising autoencoders

pretraining using autoencoders

arti   cial neural networks ii

stat 27725/cmsc 25400

