   #[1]rss [2]slideshare search [3]alternate [4]alternate [5]alternate
   [6]alternate [7]alternate [8]alternate [9]slideshow json oembed profile
   [10]slideshow xml oembed profile [11]alternate [12]alternate
   [13]alternate

   (button)

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our [14]user
   agreement and [15]privacy policy.

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our
   [16]privacy policy and [17]user agreement for details.

   [18]slideshare [19]explore search [20]you

     * [21]linkedin slideshare

     * [22]upload
     * [23]login
     * [24]signup

     *
     * ____________________ (button) submit search

     * [25]home
     * [26]explore

     * [27]presentation courses
     * [28]powerpoint courses
     *
     * by [29]linkedin learning

   ____________________
   successfully reported this slideshow.

   we use your linkedin profile and activity data to personalize ads and
   to show you more relevant ads. [30]you can change your ad preferences
   anytime.
   optimization for deep learning

   optimization for deep learning sebastian ruder phd candidate, insight
   research centre, nuig research scientist, aylien @se...

   agenda 1 introduction 2 id119 variants 3 challenges 4
   id119 optimization algorithms 5 parallelizing ...

   introduction introduction id119 is a way to minimize an
   objective function j(  )        rd : model parameters   : le...

   id119 variants id119 variants 1 batch gradient
   descent 2 stochastic id119 3 mini-batch gr...

   id119 variants batch id119 batch id119
   computes gradient with the entire dataset. update ...

   id119 variants batch id119 pros: guaranteed to
   converge to global minimum for convex error surfaces ...

   id119 variants stochastic id119 stochastic
   id119 computes update for each example x(i)y(i...

   id119 variants stochastic id119 pros much faster
   than batch id119. allows online learning...

   id119 variants stochastic id119 batch gradient
   descent vs. sgd    uctuation figure: batch gradient des...

   id119 variants mini-batch id119 mini-batch
   id119 performs update for every mini-batch of ...

   id119 variants mini-batch id119 pros reduces
   variance of updates. can exploit id127 ...

   id119 variants mini-batch id119 method accuracy
   update speed memory usage online learning batch grad...

   challenges challenges choosing a learning rate. de   ning an annealing
   schedule. updating features to di   erent extent. avoid...

   id119 optimization algorithms id119 optimization
   algorithms 1 momentum 2 nesterov accelerated gradie...

   id119 optimization algorithms momentum momentum sgd has
   trouble navigating ravines. momentum [qian, 1999] helps...

   id119 optimization algorithms momentum reduces updates for
   dimensions whose gradients change directions. increa...

   id119 optimization algorithms nesterov accelerated gradient
   nesterov accelerated gradient momentum blindly acce...

   id119 optimization algorithms adagrad adagrad previous
   methods: same learning rate    for all parameters   . adag...

   id119 optimization algorithms adagrad pros well-suited for
   dealing with sparse data. signi   cantly improves robu...

   id119 optimization algorithms adadelta adadelta adadelta
   [zeiler, 2012] restricts the window of accumulated pas...

   id119 optimization algorithms adadelta      t =        e[g2]t + gt
   (8) denominator is just root mean squared (rms) er...

   id119 optimization algorithms rmsprop rmsprop developed
   independently from adadelta around the same time by geo...

   id119 optimization algorithms adam adam adaptive moment
   estimation (adam) [kingma and ba, 2015] also stores run...

   id119 optimization algorithms adam mt and vt are initialized
   as 0-vectors. for this reason, they are biased tow...

   id119 optimization algorithms adam extensions adam
   extensions 1 adamax [kingma and ba, 2015] adam with     norm 2...

   id119 optimization algorithms update equations update
   equations method update equation sgd gt =   t j(  t)      t = ...

   id119 optimization algorithms comparison of optimizers
   visualization of algorithms (a) sgd optimization on loss...

   id119 optimization algorithms comparison of optimizers which
   optimizer to choose? adaptive learning rate method...

   parallelizing and distributing sgd parallelizing and distributing sgd 1
   hogwild! [niu et al., 2011] parallel sgd updates o...

   additional strategies for optimizing sgd additional strategies for
   optimizing sgd 1 shu   ing and curriculum learning [bengi...

   outlook outlook 1 tuned sgd vs. adam 2 sgd with restarts 3 learning to
   optimize 4 understanding generalization in deep lea...

   outlook tuned sgd vs. adam tuned sgd vs. adam many recent papers use
   sgd with learning rate annealing. sgd with tuned lear...

   outlook sgd with restarts sgd with restarts at each restart, the
   learning rate is initialized to some value and decreases ...

   outlook sgd with restarts snapshot ensembles 1 train model until
   convergence with cosine annealing schedule. 2 save model ...

   outlook learning to optimize learning to optimize rather than manually
   de   ning an update rule, learn it. update rules outp...

   outlook learning to optimize discovered update rules powersign:   f
   (t)   sign(g)   sign(m)     g (16)   : often e or 2 f : either...

   outlook understanding generalization in deep learning understanding
   generalization in deep learning optimization is closel...

   outlook case studies case studies deep bia   ne attention for neural
   id33 [dozat and manning, 2017] adam with ...

   outlook case studies thank you for attention! for more details and
   derivations of the id119 optimization algori...

   bibliography bibliography i [abadi et al., 2015] abadi, m., agarwal,
   a., barham, p., brevdo, e., chen, z., citro, c., corr...

   bibliography bibliography ii [bengio et al., 2009] bengio, y.,
   louradour, j., collobert, r., and weston, j. (2009). curric...

   bibliography bibliography iii [dinh et al., 2017] dinh, l., pascanu,
   r., bengio, s., and bengio, y. (2017). sharp minima c...

   bibliography bibliography iv [duchi et al., 2011] duchi, j., hazan, e.,
   and singer, y. (2011). adaptive subgradient method...

   bibliography bibliography v [kawaguchi, 2016] kawaguchi, k. (2016).
   deep learning without poor local minima. in advances i...

   bibliography bibliography vi [loshchilov and hutter, 2017] loshchilov,
   i. and hutter, f. (2017). sgdr: stochastic gradient...

   bibliography bibliography vii [nesterov, 1983] nesterov, y. (1983). a
   method for unconstrained convex minimization problem...

   bibliography bibliography viii [ruder, 2016] ruder, s. (2016). an
   overview of id119 optimization algorithms. ar...

   bibliography bibliography ix [wu et al., 2016] wu, y., schuster, m.,
   chen, z., le, q. v., norouzi, m., macherey, w., kriku...

   bibliography bibliography x [zhang et al., 2017a] zhang, c., bengio,
   s., hardt, m., recht, b., and vinyals, o. (2017a). un...
   upcoming slideshare
   []
   loading in    5
     
   [] 1
   (button)
   1 of 49 (button)
   (button) (button)
   like this presentation? why not share!
     * share
     * email
     *
     *

     * [31]the ai rush the ai rush by jean-baptiste dumont 1045521 views
     * [32]ai and machine learning demystified... ai and machine learning
       demystified... by carol smith 3617964 views
     * [33]10 facts about jobs in the future 10 facts about jobs in the
       future by pew research cent... 660319 views
     * [34]2017 holiday survey: an annual anal... 2017 holiday survey: an
       annual anal... by deloitte united s... 1068303 views
     * [35]harry surden - artificial intellige... harry surden -
       artificial intellige... by harry surden 621975 views
     * [36]inside google's numbers in 2017 inside google's numbers in 2017
       by rand fishkin 1204964 views

   (button)

   share slideshare
     __________________________________________________________________

     * [37]facebook
     * [38]twitter
     * [39]linkedin

   embed
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   size (px)
   start on
   [x] show related slideshares at end
   wordpress shortcode ____________________
   link ____________________

optimization for deep learning

   13,070 views

     * (button) share
     * (button) like
     * (button) download
     * ...
          +

   [40]sebastian ruder

[41]sebastian ruder

   , research scientist, deepmind
   [42]follow

   (button) (button) (button)

   published on nov 26, 2017

   talk on optimization for deep learning, which gives an overview of
   id119 optimization algorithms and highlights some current
   research directions.
   (button) ...

   published in: [43]science

     * [44]5 comments
     * [45]64 likes
     * [46]statistics
     * [47]notes

     * full name
       full name
       comment goes here.
       12 hours ago   [48]delete [49]reply [50]block
       are you sure you want to [51]yes [52]no
       your message goes here

   no profile picture user
   ____________________
   [53](button) post
     * [54]snigdhakhanna2
       [55]snigdhakhanna2
       insightful and informative. thanks!!
       2 weeks ago    [56]reply
       are you sure you want to  [57]yes  [58]no
       your message goes here
     * [59]guyugmonkhlkhagvachu
       [60]guyugmonkhlkhagvachu
       thanks a lot!!!
       2 weeks ago    [61]reply
       are you sure you want to  [62]yes  [63]no
       your message goes here
     * [64]michellerobertsq923
       [65]helen serrano
       secrets to making $$$ with paid surveys...          
       https://tinyurl.com/make2793amonth
       3 weeks ago    [66]reply
       are you sure you want to  [67]yes  [68]no
       your message goes here
     * [69]johnmayer664
       [70]johnmayer664
       hello! get your professional job-winning resume here - check our
       website! https://vk.cc/818rfv
       10 months ago    [71]reply
       are you sure you want to  [72]yes  [73]no
       your message goes here
     * [74]igoraherne
       [75]igor aherne
       sebastian, do you know if powersign needs a simple moving average
       or an exponential moving average? i saw an implementation where 'm'
       was actually a momentum:
       https://medium.com/bigdatarepublic/custom-optimizer-in-tensorflow-d
       5b41f75644a would this be this correct?
       11 months ago    [76]reply
       are you sure you want to  [77]yes  [78]no
       your message goes here

     * [79]zaishengli
       [80]zaishengli
       1 week ago
     * [81]zixia
       [82]li zhuohuan , partner at preangel at preangel partners
       3 weeks ago
     * [83]andrewpanufnik
       [84]andrew panufnik , - at - at -
       1 month ago
     * [85]melaos
       [86]melaos
       1 month ago
     * [87]fatibadaoui
       [88]fati badaoui , master sciences de donn  es et big data ensias
       1 month ago

   [89]show more
   no downloads
   views
   total views
   13,070
   on slideshare
   0
   from embeds
   0
   number of embeds
   208
   actions
   shares
   0
   downloads
   1,055
   comments
   5
   likes
   64
   embeds 0
   no embeds
   no notes for slide

optimization for deep learning

    1. 1. optimization for deep learning sebastian ruder phd candidate,
       insight research centre, nuig research scientist, aylien @seb ruder
       advanced topics in computational intelligence dublin institute of
       technology 24.11.17 sebastian ruder optimization for deep learning
       24.11.17 1 / 49
    2. [90]2. agenda 1 introduction 2 id119 variants 3
       challenges 4 id119 optimization algorithms 5
       parallelizing and distributing sgd 6 additional strategies for
       optimizing sgd 7 outlook sebastian ruder optimization for deep
       learning 24.11.17 2 / 49
    3. [91]3. introduction introduction id119 is a way to
       minimize an objective function j(  )        rd : model parameters   :
       learning rate   j(  ): gradient of the objective function with regard
       to the parameters updates parameters in opposite direction of
       gradient. update equation:    =                j(  ) figure: optimization
       with id119 sebastian ruder optimization for deep
       learning 24.11.17 3 / 49
    4. [92]4. id119 variants id119 variants 1 batch
       id119 2 stochastic id119 3 mini-batch
       id119 di   erence: amount of data used per update
       sebastian ruder optimization for deep learning 24.11.17 4 / 49
    5. [93]5. id119 variants batch id119 batch
       id119 computes gradient with the entire dataset. update
       equation:    =                j(  ) for i in range(nb_epochs ): params_grad
       = evaluate_gradient ( loss_function , data , params) params =
       params - learning_rate * params_grad listing 1: code for batch
       id119 update sebastian ruder optimization for deep
       learning 24.11.17 5 / 49
    6. [94]6. id119 variants batch id119 pros:
       guaranteed to converge to global minimum for convex error surfaces
       and to a local minimum for non-convex surfaces. cons: very slow.
       intractable for datasets that do not    t in memory. no online
       learning. sebastian ruder optimization for deep learning 24.11.17 6
       / 49
    7. [95]7. id119 variants stochastic id119
       stochastic id119 computes update for each example
       x(i)y(i). update equation:    =                j(  ; x(i); y(i)) for i in
       range(nb_epochs ): np.random.shuffle(data) for example in data:
       params_grad = evaluate_gradient ( loss_function , example , params)
       params = params - learning_rate * params_grad listing 2: code for
       stochastic id119 update sebastian ruder optimization for
       deep learning 24.11.17 7 / 49
    8. [96]8. id119 variants stochastic id119 pros
       much faster than batch id119. allows online learning.
       cons high variance updates. figure: sgd    uctuation (source:
       wikipedia) sebastian ruder optimization for deep learning 24.11.17
       8 / 49
    9. [97]9. id119 variants stochastic id119 batch
       id119 vs. sgd    uctuation figure: batch id119
       vs. sgd    uctuation (source: wikidocs.net) sgd shows same
       convergence behaviour as batch id119 if learning rate is
       slowly decreased (annealed) over time. sebastian ruder optimization
       for deep learning 24.11.17 9 / 49
   10. [98]10. id119 variants mini-batch id119
       mini-batch id119 performs update for every mini-batch of
       n examples. update equation:    =                j(  ; x(i:i+n); y(i:i+n))
       for i in range(nb_epochs ): np.random.shuffle(data) for batch in
       get_batches(data , batch_size =50): params_grad = evaluate_gradient
       ( loss_function , batch , params) params = params - learning_rate *
       params_grad listing 3: code for mini-batch id119 update
       sebastian ruder optimization for deep learning 24.11.17 10 / 49
   11. [99]11. id119 variants mini-batch id119 pros
       reduces variance of updates. can exploit id127
       primitives. cons mini-batch size is a hyperparameter. common sizes
       are 50-256. typically the algorithm of choice. usually referred to
       as sgd even when mini-batches are used. sebastian ruder
       optimization for deep learning 24.11.17 11 / 49
   12. [100]12. id119 variants mini-batch id119
       method accuracy update speed memory usage online learning batch
       id119 good slow high no stochastic id119 good
       (with annealing) high low yes mini-batch id119 good
       medium medium yes table: comparison of trade-o   s of gradient
       descent variants sebastian ruder optimization for deep learning
       24.11.17 12 / 49
   13. [101]13. challenges challenges choosing a learning rate. de   ning an
       annealing schedule. updating features to di   erent extent. avoiding
       suboptimal minima. sebastian ruder optimization for deep learning
       24.11.17 13 / 49
   14. [102]14. id119 optimization algorithms id119
       optimization algorithms 1 momentum 2 nesterov accelerated gradient
       3 adagrad 4 adadelta 5 rmsprop 6 adam 7 adam extensions sebastian
       ruder optimization for deep learning 24.11.17 14 / 49
   15. [103]15. id119 optimization algorithms momentum momentum
       sgd has trouble navigating ravines. momentum [qian, 1999] helps sgd
       accelerate. adds a fraction    of the update vector of the past step
       vt   1 to current update vector vt. momentum term    is usually set to
       0.9. vt =   vt   1 +      j(  )    =        vt (1) (a) sgd without momentum
       (b) sgd with momentum figure: source: genevieve b. orr sebastian
       ruder optimization for deep learning 24.11.17 15 / 49
   16. [104]16. id119 optimization algorithms momentum reduces
       updates for dimensions whose gradients change directions. increases
       updates for dimensions whose gradients point in the same
       directions. figure: optimization with momentum (source:
       distill.pub) sebastian ruder optimization for deep learning
       24.11.17 16 / 49
   17. [105]17. id119 optimization algorithms nesterov
       accelerated gradient nesterov accelerated gradient momentum blindly
       accelerates down slopes: first computes gradient, then makes a big
       jump. nesterov accelerated gradient (nag) [nesterov, 1983]    rst
       makes a big jump in the direction of the previous accumulated
       gradient          vt   1. then measures where it ends up and makes a
       correction, resulting in the complete update vector. vt =    vt   1 +
            j(         vt   1)    =        vt (2) figure: nesterov update (source: g.
       hinton   s lecture 6c) sebastian ruder optimization for deep learning
       24.11.17 17 / 49
   18. [106]18. id119 optimization algorithms adagrad adagrad
       previous methods: same learning rate    for all parameters   .
       adagrad [duchi et al., 2011] adapts the learning rate to the
       parameters (large updates for infrequent parameters, small updates
       for frequent parameters). sgd update:   t+1 =   t           gt gt =   t
       j(  t) adagrad divides the learning rate by the square root of the
       sum of squares of historic gradients. adagrad update:   t+1 =   t       
           gt + gt (3) gt     rd  d : diagonal matrix where each diagonal
       element i, i is the sum of the squares of the gradients w.r.t.   i
       up to time step t : smoothing term to avoid division by zero :
       element-wise multiplication sebastian ruder optimization for deep
       learning 24.11.17 18 / 49
   19. [107]19. id119 optimization algorithms adagrad pros
       well-suited for dealing with sparse data. signi   cantly improves
       robustness of sgd. lesser need to manually tune learning rate. cons
       accumulates squared gradients in denominator. causes the learning
       rate to shrink and become in   nitesimally small. sebastian ruder
       optimization for deep learning 24.11.17 19 / 49
   20. [108]20. id119 optimization algorithms adadelta adadelta
       adadelta [zeiler, 2012] restricts the window of accumulated past
       gradients to a    xed size. sgd update:      t =          gt   t+1 =   t +      t
       (4) de   nes running average of squared gradients e[g2]t at time t:
       e[g2 ]t =   e[g2 ]t   1 + (1       )g2 t (5)   : fraction similarly to
       momentum term, around 0.9 adagrad update:      t =            gt + gt (6)
       preliminary adadelta update:      t =        e[g2]t + gt (7) sebastian
       ruder optimization for deep learning 24.11.17 20 / 49
   21. [109]21. id119 optimization algorithms adadelta      t =    
          e[g2]t + gt (8) denominator is just root mean squared (rms) error
       of gradient:      t =        rms[g]t gt (9) note: hypothetical units do
       not match. de   ne running average of squared parameter updates and
       rms: e[     2 ]t =   e[     2 ]t   1 + (1       )     2 t rms[     ]t = e[     2]t +
       (10) approximate with rms[     ]t   1, replace    for    nal adadelta
       update:      t =     rms[     ]t   1 rms[g]t gt   t+1 =   t +      t (11)
       sebastian ruder optimization for deep learning 24.11.17 21 / 49
   22. [110]22. id119 optimization algorithms rmsprop rmsprop
       developed independently from adadelta around the same time by geo   
       hinton. also divides learning rate by a running average of squared
       gradients. rmsprop update: e[g2 ]t =   e[g2 ]t   1 + (1       )g2 t   t+1
       =   t        e[g2]t + gt (12)   : decay parameter; typically set to 0.9
         : learning rate; a good default value is 0.001 sebastian ruder
       optimization for deep learning 24.11.17 22 / 49
   23. [111]23. id119 optimization algorithms adam adam
       adaptive moment estimation (adam) [kingma and ba, 2015] also stores
       running average of past squared gradients vt like adadelta and
       rmsprop. like momentum, stores running average of past gradients
       mt. mt =   1mt   1 + (1       1)gt vt =   2vt   1 + (1       2)g2 t (13) mt:
          rst moment (mean) of gradients vt: second moment (uncentered
       variance) of gradients   1,   2: decay rates sebastian ruder
       optimization for deep learning 24.11.17 23 / 49
   24. [112]24. id119 optimization algorithms adam mt and vt
       are initialized as 0-vectors. for this reason, they are biased
       towards 0. compute bias-corrected    rst and second moment estimates:
         mt = mt 1       t 1   vt = vt 1       t 2 (14) adam update rule:   t+1 =
         t              vt +   mt (15) sebastian ruder optimization for deep
       learning 24.11.17 24 / 49
   25. [113]25. id119 optimization algorithms adam extensions
       adam extensions 1 adamax [kingma and ba, 2015] adam with     norm 2
       nadam [dozat, 2016] adam with nesterov accelerated gradient
       sebastian ruder optimization for deep learning 24.11.17 25 / 49
   26. [114]26. id119 optimization algorithms update equations
       update equations method update equation sgd gt =   t j(  t)      t =      
          gt   t =   t +      t momentum      t =       vt   1       gt nag      t =       vt   1    
            j(         vt   1) adagrad      t =            gt + gt adadelta      t =    
       rms[     ]t   1 rms[g]t gt rmsprop      t =        e[g2]t + gt adam      t =       
             vt +   mt table: update equations for the id119
       optimization algorithms. sebastian ruder optimization for deep
       learning 24.11.17 26 / 49
   27. [115]27. id119 optimization algorithms comparison of
       optimizers visualization of algorithms (a) sgd optimization on loss
       surface contours (b) sgd optimization on saddle point figure:
       source and full animations: alec radford sebastian ruder
       optimization for deep learning 24.11.17 27 / 49
   28. [116]28. id119 optimization algorithms comparison of
       optimizers which optimizer to choose? adaptive learning rate
       methods (adagrad, adadelta, rmsprop, adam) are particularly useful
       for sparse features. adagrad, adadelta, rmsprop, and adam work well
       in similar circumstances. [kingma and ba, 2015] show that
       bias-correction helps adam slightly outperform rmsprop. sebastian
       ruder optimization for deep learning 24.11.17 28 / 49
   29. [117]29. parallelizing and distributing sgd parallelizing and
       distributing sgd 1 hogwild! [niu et al., 2011] parallel sgd updates
       on cpu shared memory access without parameter lock only works for
       sparse input data 2 downpour sgd [dean et al., 2012] multiple
       replicas of model on subsets of training data run in parallel
       updates sent to parameter server; updates fraction of model
       parameters 3 delay-tolerant algorithms for sgd [mcmahan and
       streeter, 2014] methods also adapt to update delays 4 tensorflow
       [abadi et al., 2015] computation graph is split into a subgraph for
       every device communication takes place using send/receive node
       pairs 5 elastic averaging sgd [zhang et al., 2015] links parameters
       elastically to a center variable stored by parameter server
       sebastian ruder optimization for deep learning 24.11.17 29 / 49
   30. [118]30. additional strategies for optimizing sgd additional
       strategies for optimizing sgd 1 shu   ing and curriculum learning
       [bengio et al., 2009] shu   e training data after every epoch to
       break biases order training examples to solve progressively harder
       problems; infrequently used in practice 2 batch id172 [io   e
       and szegedy, 2015] re-normalizes every mini-batch to zero mean,
       unit variance must-use for id161 3 early stopping    early
       stopping (is) beautiful free lunch    (geo    hinton) 4 gradient noise
       [neelakantan et al., 2015] add gaussian noise to gradient makes
       model more robust to poor initializations sebastian ruder
       optimization for deep learning 24.11.17 30 / 49
   31. [119]31. outlook outlook 1 tuned sgd vs. adam 2 sgd with restarts 3
       learning to optimize 4 understanding generalization in deep
       learning 5 case studies sebastian ruder optimization for deep
       learning 24.11.17 31 / 49
   32. [120]32. outlook tuned sgd vs. adam tuned sgd vs. adam many recent
       papers use sgd with learning rate annealing. sgd with tuned
       learning rate and momentum is competitive with adam [zhang et al.,
       2017b]. adam converges faster, but underperforms sgd on some tasks,
       e.g. machine translation [wu et al., 2016]. adam with 2 restarts
       and sgd-style annealing converges faster and outperforms sgd
       [denkowski and neubig, 2017]. increasing the batch size may have
       the same e   ect as decaying the learning rate [smith et al., 2017].
       sebastian ruder optimization for deep learning 24.11.17 32 / 49
   33. [121]33. outlook sgd with restarts sgd with restarts at each
       restart, the learning rate is initialized to some value and
       decreases with cosine annealing [loshchilov and hutter, 2017].
       converges 2   to 4   faster with comparable performance. figure:
       learning rate schedules with warm restarts [loshchilov and hutter,
       2017] sebastian ruder optimization for deep learning 24.11.17 33 /
       49
   34. [122]34. outlook sgd with restarts snapshot ensembles 1 train model
       until convergence with cosine annealing schedule. 2 save model
       parameters. 3 perform warm restart and repeat steps 1-3 m times. 4
       ensemble saved models. figure: sgd vs. snapshot ensemble [huang et
       al., 2017] sebastian ruder optimization for deep learning 24.11.17
       34 / 49
   35. [123]35. outlook learning to optimize learning to optimize rather
       than manually de   ning an update rule, learn it. update rules
       outperform existing optimizers and transfer across tasks. figure:
       neural optimizer search [bello et al., 2017] sebastian ruder
       optimization for deep learning 24.11.17 35 / 49
   36. [124]36. outlook learning to optimize discovered update rules
       powersign:   f (t)   sign(g)   sign(m)     g (16)   : often e or 2 f :
       either 1 or a decay function of the training step t m: moving
       average of gradients scales update by   f (t) or 1/  f (t) depending
       on whether the direction of the gradient and its running average
       agree. addsign: (   + f (t)     sign(g)     sign(m))     g (17)   : often 1
       or 2 scales update by    + f (t) or        f (t). sebastian ruder
       optimization for deep learning 24.11.17 36 / 49
   37. [125]37. outlook understanding generalization in deep learning
       understanding generalization in deep learning optimization is
       closely tied to generalization. the number of possible local minima
       grows exponentially with the number of parameters [kawaguchi,
       2016]. di   erent local minima generalize to di   erent extents. recent
       insights in understanding generalization: neural networks can
       completely memorize random inputs [zhang et al., 2017a]. sharp
       minima found by batch id119 have high generalization
       error [keskar et al., 2017]. local minima that generalize well can
       be made arbitrarily sharp [dinh et al., 2017]. several submissions
       at iclr 2018 on understanding generalization. sebastian ruder
       optimization for deep learning 24.11.17 37 / 49
   38. [126]38. outlook case studies case studies deep bia   ne attention
       for neural id33 [dozat and manning, 2017] adam with
         1 = 0.9,   2 = 0.9 report large positive impact on    nal performance
       of lowering   2 attention is all you need [vaswani et al., 2017]
       adam with   1 = 0.9,   2 = 0.98, = 10   9 , learning rate       = d   0.5
       model    min(step num   0.5 , step num    warmup steps   1.5 ) warmup
       steps = 4000 sebastian ruder optimization for deep learning
       24.11.17 38 / 49
   39. [127]39. outlook case studies thank you for attention! for more
       details and derivations of the id119 optimization
       algorithms, refer to [ruder, 2016]. sebastian ruder optimization
       for deep learning 24.11.17 39 / 49
   40. [128]40. bibliography bibliography i [abadi et al., 2015] abadi,
       m., agarwal, a., barham, p., brevdo, e., chen, z., citro, c.,
       corrado, g., davis, a., dean, j., devin, m., ghemawat, s.,
       goodfellow, i., harp, a., irving, g., isard, m., jia, y., kaiser,
       l., kudlur, m., levenberg, j., man, d., monga, r., moore, s.,
       murray, d., shlens, j., steiner, b., sutskever, i., tucker, p.,
       vanhoucke, v., vasudevan, v., vinyals, o., warden, p., wicke, m.,
       yu, y., and zheng, x. (2015). tensorflow: large-scale machine
       learning on heterogeneous distributed systems. [bello et al., 2017]
       bello, i., zoph, b., vasudevan, v., and le, q. v. (2017). neural
       optimizer search with id23. in proceedings of the
       34th international conference on machine learning. sebastian ruder
       optimization for deep learning 24.11.17 40 / 49
   41. [129]41. bibliography bibliography ii [bengio et al., 2009] bengio,
       y., louradour, j., collobert, r., and weston, j. (2009). curriculum
       learning. proceedings of the 26th annual international conference
       on machine learning, pages 41   48. [dean et al., 2012] dean, j.,
       corrado, g. s., monga, r., chen, k., devin, m., le, q. v., mao, m.
       z., ranzato, m. a., senior, a., tucker, p., yang, k., and ng, a. y.
       (2012). large scale distributed deep networks. nips 2012: neural
       information processing systems, pages 1   11. [denkowski and neubig,
       2017] denkowski, m. and neubig, g. (2017). stronger baselines for
       trustable results in id4. in workshop on
       id4 (wid4). sebastian ruder optimization for
       deep learning 24.11.17 41 / 49
   42. [130]42. bibliography bibliography iii [dinh et al., 2017] dinh,
       l., pascanu, r., bengio, s., and bengio, y. (2017). sharp minima
       can generalize for deep nets. in proceedings of the 34 th
       international conference on machine learning. [dozat, 2016] dozat,
       t. (2016). incorporating nesterov momentum into adam. iclr
       workshop, (1):2013   2016. [dozat and manning, 2017] dozat, t. and
       manning, c. d. (2017). deep bia   ne attention for neural dependency
       parsing. in iclr 2017. sebastian ruder optimization for deep
       learning 24.11.17 42 / 49
   43. [131]43. bibliography bibliography iv [duchi et al., 2011] duchi,
       j., hazan, e., and singer, y. (2011). adaptive subgradient methods
       for online learning and stochastic optimization. journal of machine
       learning research, 12:2121   2159. [huang et al., 2017] huang, g.,
       li, y., pleiss, g., liu, z., hopcroft, j. e., and weinberger, k. q.
       (2017). snapshot ensembles: train 1, get m for free. in proceedings
       of iclr 2017. [io   e and szegedy, 2015] io   e, s. and szegedy, c.
       (2015). batch id172: accelerating deep network training by
       reducing internal covariate shift. arxiv preprint
       arxiv:1502.03167v3. sebastian ruder optimization for deep learning
       24.11.17 43 / 49
   44. [132]44. bibliography bibliography v [kawaguchi, 2016] kawaguchi,
       k. (2016). deep learning without poor local minima. in advances in
       neural information processing systems 29 (nips 2016). [keskar et
       al., 2017] keskar, n. s., mudigere, d., nocedal, j., smelyanskiy,
       m., and tang, p. t. p. (2017). on large-batch training for deep
       learning: generalization gap and sharp minima. in proceedings of
       iclr 2017. [kingma and ba, 2015] kingma, d. p. and ba, j. l.
       (2015). adam: a method for stochastic optimization. international
       conference on learning representations, pages 1   13. sebastian ruder
       optimization for deep learning 24.11.17 44 / 49
   45. [133]45. bibliography bibliography vi [loshchilov and hutter, 2017]
       loshchilov, i. and hutter, f. (2017). sgdr: stochastic gradient
       descent with warm restarts. in proceedings of iclr 2017. [mcmahan
       and streeter, 2014] mcmahan, h. b. and streeter, m. (2014).
       delay-tolerant algorithms for asynchronous distributed online
       learning. advances in neural information processing systems
       (proceedings of nips), pages 1   9. [neelakantan et al., 2015]
       neelakantan, a., vilnis, l., le, q. v., sutskever, i., kaiser, l.,
       kurach, k., and martens, j. (2015). adding gradient noise improves
       learning for very deep networks. pages 1   11. sebastian ruder
       optimization for deep learning 24.11.17 45 / 49
   46. [134]46. bibliography bibliography vii [nesterov, 1983] nesterov,
       y. (1983). a method for unconstrained convex minimization problem
       with the rate of convergence o(1/k2). doklady ansssr (translated as
       soviet.math.docl.), 269:543   547. [niu et al., 2011] niu, f., recht,
       b., christopher, r., and wright, s. j. (2011). hogwild!: a
       lock-free approach to parallelizing stochastic id119.
       pages 1   22. [qian, 1999] qian, n. (1999). on the momentum term in
       id119 learning algorithms. neural networks : the o   cial
       journal of the international neural network society, 12(1):145   151.
       sebastian ruder optimization for deep learning 24.11.17 46 / 49
   47. [135]47. bibliography bibliography viii [ruder, 2016] ruder, s.
       (2016). an overview of id119 optimization algorithms.
       arxiv preprint arxiv:1609.04747. [smith et al., 2017] smith, s. l.,
       kindermans, p.-j., and le, q. v. (2017). don   t decay the learning
       rate, increase the batch size. in arxiv preprint arxiv:1711.00489.
       [vaswani et al., 2017] vaswani, a., shazeer, n., parmar, n.,
       uszkoreit, j., jones, l., gomez, a. n., kaiser, l., and polosukhin,
       i. (2017). attention is all you need. in advances in neural
       information processing systems. sebastian ruder optimization for
       deep learning 24.11.17 47 / 49
   48. [136]48. bibliography bibliography ix [wu et al., 2016] wu, y.,
       schuster, m., chen, z., le, q. v., norouzi, m., macherey, w.,
       krikun, m., cao, y., gao, q., macherey, k., klingner, j., shah, a.,
       johnson, m., liu, x., kaiser, l., gouws, s., kato, y., kudo, t.,
       kazawa, h., stevens, k., kurian, g., patil, n., wang, w., young,
       c., smith, j., riesa, j., rudnick, a., vinyals, o., corrado, g.,
       hughes, m., and dean, j. (2016). google   s neural machine
       translation system: bridging the gap between human and machine
       translation. arxiv preprint arxiv:1609.08144. [zeiler, 2012]
       zeiler, m. d. (2012). adadelta: an adaptive learning rate method.
       arxiv preprint arxiv:1212.5701. sebastian ruder optimization for
       deep learning 24.11.17 48 / 49
   49. [137]49. bibliography bibliography x [zhang et al., 2017a] zhang,
       c., bengio, s., hardt, m., recht, b., and vinyals, o. (2017a).
       understanding deep learning requires rethinking generalization. in
       proceedings of iclr 2017. [zhang et al., 2017b] zhang, j.,
       mitliagkas, i., and r  e, c. (2017b). yellowfin and the art of
       momentum tuning. in arxiv preprint arxiv:1706.03471. [zhang et al.,
       2015] zhang, s., choromanska, a., and lecun, y. (2015). deep
       learning with elastic averaging sgd. neural information processing
       systems conference (nips 2015), pages 1   24. sebastian ruder
       optimization for deep learning 24.11.17 49 / 49

          [138]recommended

     * teaching online: synchronous classes
       teaching online: synchronous classes
       online course - linkedin learning
     * gamification for interactive learning
       gamification for interactive learning
       online course - linkedin learning
     * teaching with technology
       teaching with technology
       online course - linkedin learning
     * frontiers of natural language processing
       frontiers of natural language processing
       sebastian ruder
     * strong baselines for neural semi-supervised learning under domain
       shift
       strong baselines for neural semi-supervised learning under domain
       shift
       sebastian ruder
     * on the limitations of unsupervised bilingual dictionary induction
       on the limitations of unsupervised bilingual dictionary induction
       sebastian ruder
     * neural semi-supervised learning under domain shift
       neural semi-supervised learning under domain shift
       sebastian ruder
     * successes and frontiers of deep learning
       successes and frontiers of deep learning
       sebastian ruder
     * human evaluation: why do we need it? - dr. sheila castilho
       human evaluation: why do we need it? - dr. sheila castilho
       sebastian ruder
     * machine intelligence in hr technology: resume analysis at scale -
       adrian mihai
       machine intelligence in hr technology: resume analysis at scale -
       adrian mihai
       sebastian ruder

     * [139]english
     * [140]espa  ol
     * [141]portugu  s
     * [142]fran  ais
     * [143]deutsch

     * [144]about
     * [145]dev & api
     * [146]blog
     * [147]terms
     * [148]privacy
     * [149]copyright
     * [150]support

     *
     *
     *
     *
     *

   linkedin corporation    2019

     

share clipboard
     __________________________________________________________________

   [151]  
     * facebook
     * twitter
     * linkedin

   link ____________________

public clipboards featuring this slide
     __________________________________________________________________

   (button)   
   no public clipboards found for this slide

select another clipboard
     __________________________________________________________________

   [152]  

   looks like you   ve clipped this slide to already.
   ____________________

   create a clipboard

you just clipped your first slide!

   clipping is a handy way to collect important slides you want to go back
   to later. now customize the name of a clipboard to store your clips.
     __________________________________________________________________

   name* ____________________
   description ____________________
   visibility
   others can see my clipboard [ ]
   (button) cancel (button) save

   bizographics tracking image

references

   visible links
   1. https://www.slideshare.net/rss/latest
   2. https://www.slideshare.net/opensearch.xml
   3. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
   4. https://es.slideshare.net/sebastianruder/optimization-for-deep-learning
   5. https://fr.slideshare.net/sebastianruder/optimization-for-deep-learning
   6. https://de.slideshare.net/sebastianruder/optimization-for-deep-learning
   7. https://pt.slideshare.net/sebastianruder/optimization-for-deep-learning
   8. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
   9. https://www.slideshare.net/api/oembed/2?format=json&url=http://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  10. https://www.slideshare.net/api/oembed/2?format=xml&url=http://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  11. https://www.slideshare.net/mobile/sebastianruder/optimization-for-deep-learning
  12. android-app://net.slideshare.mobile/slideshare-app/ss/82765626
  13. ios-app://917418728/slideshare-app/ss/82765626
  14. http://www.linkedin.com/legal/user-agreement
  15. http://www.linkedin.com/legal/privacy-policy
  16. http://www.linkedin.com/legal/privacy-policy
  17. http://www.linkedin.com/legal/user-agreement
  18. https://www.slideshare.net/
  19. https://www.slideshare.net/explore
  20. https://www.slideshare.net/login
  21. https://www.slideshare.net/
  22. https://www.slideshare.net/upload
  23. https://www.slideshare.net/login
  24. https://www.slideshare.net/w/signup
  25. https://www.slideshare.net/
  26. https://www.slideshare.net/explore
  27. https://www.linkedin.com/learning/topics/presentations?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  28. https://www.linkedin.com/learning/topics/powerpoint?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  29. https://www.linkedin.com/learning?trk=slideshare_subnav_learning
  30. https://www.linkedin.com/psettings/privacy
  31. https://public.slidesharecdn.com/jeanbaptiste.dumont/the-ai-rush-121047435
  32. https://public.slidesharecdn.com/carologic/ai-and-machine-learning-demystified-by-carol-smith-at-midwest-ux-2017
  33. https://public.slidesharecdn.com/pewinternet/10-facts-about-jobs-in-the-future
  34. https://public.slidesharecdn.com/deloitteus/2017-holiday-survey-an-annual-analysis-of-the-peak-shopping-season
  35. https://public.slidesharecdn.com/harrysurden/harry-surden-artificial-intelligence-and-law-overview
  36. https://public.slidesharecdn.com/randfish/inside-googles-numbers-in-2017
  37. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  38. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  39. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  40. https://www.slideshare.net/sebastianruder?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  41. https://www.slideshare.net/sebastianruder?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  42. https://www.slideshare.net/signup?login_source=slideview.popup.follow&from=addcontact&from_source=https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  43. https://www.slideshare.net/featured/category/science
  44. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning#comments-panel
  45. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning#likes-panel
  46. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning#stats-panel
  47. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning#notes-panel
  48. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  49. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  50. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  51. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  52. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  53. https://www.slideshare.net/signup?login_source=slideview.popup.comment&from=comments&from_source=https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  54. https://www.slideshare.net/snigdhakhanna2?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  55. https://www.slideshare.net/snigdhakhanna2?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  56. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  57. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  58. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  59. https://www.slideshare.net/guyugmonkhlkhagvachu?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  60. https://www.slideshare.net/guyugmonkhlkhagvachu?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  61. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  62. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  63. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  64. https://www.slideshare.net/michellerobertsq923?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  65. https://www.slideshare.net/michellerobertsq923?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  66. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  67. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  68. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  69. https://www.slideshare.net/johnmayer664?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  70. https://www.slideshare.net/johnmayer664?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  71. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  72. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  73. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  74. https://www.slideshare.net/igoraherne?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  75. https://www.slideshare.net/igoraherne?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  76. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  77. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  78. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  79. https://www.slideshare.net/zaishengli?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  80. https://www.slideshare.net/zaishengli?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  81. https://www.slideshare.net/zixia?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  82. https://www.slideshare.net/zixia?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  83. https://www.slideshare.net/andrewpanufnik?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  84. https://www.slideshare.net/andrewpanufnik?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  85. https://www.slideshare.net/melaos?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  86. https://www.slideshare.net/melaos?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  87. https://www.slideshare.net/fatibadaoui?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  88. https://www.slideshare.net/fatibadaoui?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  89. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  90. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-2-638.jpg?cb=1511702523
  91. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-3-638.jpg?cb=1511702523
  92. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-4-638.jpg?cb=1511702523
  93. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-5-638.jpg?cb=1511702523
  94. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-6-638.jpg?cb=1511702523
  95. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-7-638.jpg?cb=1511702523
  96. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-8-638.jpg?cb=1511702523
  97. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-9-638.jpg?cb=1511702523
  98. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-10-638.jpg?cb=1511702523
  99. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-11-638.jpg?cb=1511702523
 100. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-12-638.jpg?cb=1511702523
 101. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-13-638.jpg?cb=1511702523
 102. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-14-638.jpg?cb=1511702523
 103. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-15-638.jpg?cb=1511702523
 104. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-16-638.jpg?cb=1511702523
 105. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-17-638.jpg?cb=1511702523
 106. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-18-638.jpg?cb=1511702523
 107. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-19-638.jpg?cb=1511702523
 108. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-20-638.jpg?cb=1511702523
 109. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-21-638.jpg?cb=1511702523
 110. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-22-638.jpg?cb=1511702523
 111. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-23-638.jpg?cb=1511702523
 112. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-24-638.jpg?cb=1511702523
 113. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-25-638.jpg?cb=1511702523
 114. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-26-638.jpg?cb=1511702523
 115. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-27-638.jpg?cb=1511702523
 116. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-28-638.jpg?cb=1511702523
 117. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-29-638.jpg?cb=1511702523
 118. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-30-638.jpg?cb=1511702523
 119. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-31-638.jpg?cb=1511702523
 120. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-32-638.jpg?cb=1511702523
 121. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-33-638.jpg?cb=1511702523
 122. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-34-638.jpg?cb=1511702523
 123. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-35-638.jpg?cb=1511702523
 124. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-36-638.jpg?cb=1511702523
 125. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-37-638.jpg?cb=1511702523
 126. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-38-638.jpg?cb=1511702523
 127. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-39-638.jpg?cb=1511702523
 128. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-40-638.jpg?cb=1511702523
 129. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-41-638.jpg?cb=1511702523
 130. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-42-638.jpg?cb=1511702523
 131. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-43-638.jpg?cb=1511702523
 132. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-44-638.jpg?cb=1511702523
 133. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-45-638.jpg?cb=1511702523
 134. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-46-638.jpg?cb=1511702523
 135. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-47-638.jpg?cb=1511702523
 136. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-48-638.jpg?cb=1511702523
 137. https://image.slidesharecdn.com/optimizationtalk-171126132036/95/optimization-for-deep-learning-49-638.jpg?cb=1511702523
 138. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning#related-tab-content
 139. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
 140. https://es.slideshare.net/sebastianruder/optimization-for-deep-learning
 141. https://pt.slideshare.net/sebastianruder/optimization-for-deep-learning
 142. https://fr.slideshare.net/sebastianruder/optimization-for-deep-learning
 143. https://de.slideshare.net/sebastianruder/optimization-for-deep-learning
 144. https://www.slideshare.net/about
 145. https://www.slideshare.net/developers
 146. http://blog.slideshare.net/
 147. https://www.slideshare.net/terms
 148. https://www.slideshare.net/privacy
 149. http://www.linkedin.com/legal/copyright-policy
 150. https://www.linkedin.com/help/slideshare
 151. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
 152. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning

   hidden links:
 154. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
 155. https://www.slideshare.net/signup?login_source=slideview.clip.like&from=clip&layout=foundation&from_source=
 156. https://www.slideshare.net/login?from_source=%2fsebastianruder%2foptimization-for-deep-learning%3ffrom_action%3dsave&from=download&layout=foundation
 157. https://www.slideshare.net/signup?login_source=slideview.popup.flags&from=flagss&from_source=https%3a%2f%2fwww.slideshare.net%2fsebastianruder%2foptimization-for-deep-learning
 158. https://www.linkedin.com/learning/teaching-online-synchronous-classes?trk=slideshare_sv_learning
 159. https://www.linkedin.com/learning/gamification-for-interactive-learning?trk=slideshare_sv_learning
 160. https://www.linkedin.com/learning/teaching-with-technology?trk=slideshare_sv_learning
 161. https://www.slideshare.net/sebastianruder/frontiers-of-natural-language-processing
 162. https://www.slideshare.net/sebastianruder/strong-baselines-for-neural-semisupervised-learning-under-domain-shift
 163. https://www.slideshare.net/sebastianruder/on-the-limitations-of-unsupervised-bilingual-dictionary-induction
 164. https://www.slideshare.net/sebastianruder/neural-semisupervised-learning-under-domain-shift
 165. https://www.slideshare.net/sebastianruder/successes-and-frontiers-of-deep-learning
 166. https://www.slideshare.net/sebastianruder/human-evaluation-why-do-we-need-it-dr-sheila-castilho
 167. https://www.slideshare.net/sebastianruder/machine-intelligence-in-hr-technology-resume-analysis-at-scale-adrian-mihai
 168. http://www.linkedin.com/company/linkedin
 169. http://www.facebook.com/linkedin
 170. http://twitter.com/slideshare
 171. http://www.google.com/+linkedin
 172. https://www.slideshare.net/rss/latest
