6.825 id23

examples

tas: meg aycinena and emma brunskill

1 mini grid world

(a) transition model of 3x3 world.

(b) optimal policy    of 3x3
world.

(c) utility function of optimal
policy of 3x3 world.

figure 1: a 3x3 grid world.

in the mini grid world shown in figure 1, there are two terminal states: state (3,2) with a
negative reward of    1, and (3,3) with a positive reward of +1. according to the transition model
shown in figure 1(a), an action succeeds with id203 0.8, and goes to the left or right of the
intended direction with id203 0.1, respectively. the optimal policy    is shown in figure 1(b),
and the correct utility function the optimal policy is shown in figure 1(c).

2 passive learning

we conduct a series of three trials in this environment. we start in the start state (1,1), take actions
according to the    xed policy    given in figure 1(b), and end once a terminal state is reached. the
trials are as follows:

trial

1
2
3

hstate,rewardi series
h(1, 1), 0i e    h(1, 2), 0i e    h(1, 3), 0i s    h(2, 3), 0i s    h(3, 3), 1i
h(1, 1), 0i e    h(1, 2), 0i s    h(2, 2), 0i n    h(1, 2), 0i e    h(1, 3), 0i s    h(2, 3), 0i s    h(3, 3), 1i
h(1, 1), 0i s    h(2, 1), 0i n    h(1, 1), 0i e    h(1, 2), 0i e    h(1, 3), 0i s    h(2, 3), 0i s    h(3, 3), 1i

1

wesn0.10.10.8+1312312   11.0312312   1.00.2940.4970.5810.8490.7350.6390.5542.1 passive adp

2.1.1 learning the transition model
first, we must learn the transition model p r(s0 | s, a). this is easy, because the world is fully
observable. for simplicity, we use simple ml estimation (counting), but a more robust approach
would incorporate laplacian correction or another bayesian prior. the ml estimate of the transi-
tion function can be computed as:

p
p r(s0 | s, a) = n(s, a, s0)
s00 n(s, a, s00)

(1)

where n(s, a, s0) denotes the number of times the state s0 was reached when taking action a in
state s.

the ml estimate of the transition function, using the trials above, is:

(1, 1), e
(1, 2), e
(1, 3), s
(2, 1), n
(2, 2), n
(2, 3), s
(3, 1), e

(1, 1)

0
0
0
1
0
0
?

(1, 2)
0.75

0
0
0
1
0
?

(1, 3)

0

0.75

0
0
0
0
?

(2, 1)
0.25

0
0
0
0
0
?

(2, 2)

(2, 3)

(3, 1)

(3, 2)

(3, 3)

0

0.25

0
0
0
0
?

0
0
1
0
0
0
?

0
0
0
0
0
0
?

0
0
0
0
0
0
?

0
0
0
0
0
1
?

note that we only learn distributions over state-action pairs that are relevant, given the    xed policy.
also, we know nothing about state (3,1) because we have never seen it during our trials.

2.1.2 learning the utility function

now, we learn the utility function on states, by plugging in our learned transition model and the
observed rewards, and then solving the bellman equations:

v   (s) = r(s) +   

p r(s0 | s,   (s))v   (s0)

(2)

x

s0

where   (s) denotes the action determined by the policy    in state s. because there is no max
operator in this expression, we have a simple set of |s| linear equations in |s| unknowns, which can
be solved in closed form. let the discount factor    = 0.9.

v   ((1, 1)) = 0 + 0.9(0.75v   ((1, 2)) + 0.25v   ((2, 1)))
v   ((1, 2)) = 0 + 0.9(0.75v   ((1, 3)) + 0.25v   ((2, 2)))
v   ((1, 3)) = 0 + 0.9(v   ((2, 3)))
v   ((2, 1)) = 0 + 0.9(v   ((1, 1)))
v   ((2, 2)) = 0 + 0.9(v   ((1, 2)))
v   ((2, 3)) = 0 + 0.9(v   ((3, 3)))
v   ((3, 3)) = 1.0

2

figure 2: learned v   (s), using passive adp.

in this case, because we never saw states (3,1) or (3,2), we have two fewer equations and unknowns.
solving this system yields the values shown in figure 2. they are actually quite a good ap-

proximation to the true underlying utilities.

2.2 passive td learning

in passive td learning, we use each observed transition to adjust the utility values of observed
states to agree with the constraint equations given in equation (2). the update equation is:

v   (s)     v   (s) +   (n(s))(r(s) +   v   (s0)     v   (s))

(3)

where n(s) denotes the number of times we have been in state s, and   (n) is a function that
increases and n decreases. here we use   (n) = 1
n.
the update equation for each hs, a, s0i tuple. we also keep a table of n(s) counts.

initialize all v   (s) to 0. (this can also be done randomly.) then, for each trial, we perform

trial 1

the initial utilities are all zero, and the only observed reward is in the    nal state in the trial, state
(3, 3). thus, the only non-trivial update while performing the updates for trial 1 is for state (3, 3):

v   ((3, 3))     0 + 1(1 + 0.9(0)     0)

    1

the resulting v   (s) is shown in figure 3(b).

trial 2

the only non-zero update in this trial is for state (2, 3):

v   ((2, 3))     0 +
    0.45

1
2

(0 + 0.9(1)     0)

the resulting v   (s) is shown in figure 3(d).

3

1.0312312??0.480.590.90.810.660.54(a) n (s), after trial 1.

(b) v   (s), after trial 1.

(c) n (s), after trial 2.

(d) v   (s), after trial 2.

(e) n (s), after trial 3.

(f) v   (s), after trial 3.

figure 3: learned utilities, using passive td learning.

trial 3

now we have two non-zero updates during this trial;    rst state (1, 3), and then another update to
state (2, 3):

1
v   ((1, 3))     0 +
3
    0.135
v   ((2, 3))     0.45 +

    0.6

(0 + 0.9(0.45)     0)

(0 + 0.9(1)     0.45)

1
3

the resulting v   (s) is shown in figure 3(f).

from this example, we can see how slowly td(0) progresses (although it will eventually con-
verge!). the more general purpose td(  ) algorithm tries to be smarter about how it uses the
information from each trial.

4

1312312000011111.0312312000000002312312000122321.031231200000.450003312312001133441.031231200000.135000.63 active learning

3.1 id24

id24 is an alternate td method that learns values on state-action pairs, q(s, a), instead of
utilities on states. the td update equation for id24 is:
q(s, a)     q(s, a) +   (n(s, a))(r(s) +    max

(4)

a0 q(s0, a0)     q(s, a))

where n(s, a) denotes the number of times we have been in state s and taken action a, and   (n)
is a function that increases and n decreases. again we use   (n) = 1
n.
the update equation for each hs, a, s0i tuple. we also keep a table of n(s, a) counts.

initialize all q(s, a) to 0. (this can also be done randomly.) then, for each trial, we perform

we will use the same set of trials as for the previous two examples, for simplicity. however, in
general, since id24 is an active learning algorithm, each trial would have been produced using
an exploration function that trades o    between exploring the state-action space, and exploiting the
current learned model.

also, in the version of id24 presented in russell and norvig (page 776), a terminal state
cannot have a reward. however, this is just an artifact of their particular formulation, and not
inherent to id24. thus, for consistency with the previous examples, we will simply set the
value of q(s, a) for a terminal state s to its reward r(s), for all values of a.

trial 1

as we said above, we will learned in this trial that state (3, 3) is a terminal state with reward 1.
therefore, we will set the value of the q-function for (3, 3 to 1, for all a. and since all other utilities
are zero, this is the only non-trivial update while performing updates for trial 1.

q((3, 3), n)     1
q((3, 3), e)     1
q((3, 3), s)     1
q((3, 3), w )     1

the resulting q(s, a) is shown in figure 4(b).

trial 2
the only non-zero update in this trial is for state-action pair h(2, 3), si:

q((2, 3), s)     0 +
    0.45

1
2

(0 + 0.9(1)     0)

the resulting q(s, a) is shown in figure 4(d).

5

(a) n (s, a), after trial 1.

(b) q(s, a), after trial 1.

(c) n (s, a), after trial 2.

(d) q(s, a), after trial 2.

(e) n (s, a), after trial 3.

(f) q(s, a), after trial 3.

figure 4: learned utilities, using id24.

trial 3

as in passive td learning, now we have two non-zero updates during this trial;    rst state (1, 3),
and then another update to state (2, 3):

1
q((1, 3), s)     0 +
3
    0.135
q((2, 3), s)     0.45 +

    0.6

(0 + 0.9(0.45)     0)

(0 + 0.9(1)     0.45)

1
3

the resulting q(s, a) is shown in figure 4(f).

6

1312312111100000000000000000000000000003123120000000000000000000000000000000011112312312222210000000000000100000000000003123120000.4500000000000000000000000000001111331231233331001000000000011000000000000312312000.1350.600000000000000000000000000001111