    collect days of sensorimotor data
    test new algorithms in the real world
    can an agent learn to react to spontaneous 

r
l&
a i

introduction to 

id23

with function approximation

id23 and arti   cial intelligence laboratory

rich sutton

alberta centre for machine learning
department of computing science

university of alberta

canada

(with thanks to david silver and michael littman for some slides and ideas)

what is id23?

agent-oriented learning   learning by interacting with an 
environment to achieve a goal 

    more realistic and ambitious than other kinds of machine 

learning

learning by trial and error, with only delayed evaluative 
feedback (reward)

   

   

the kind of machine learning most like natural learning

learning that can tell for itself when it is right or wrong

the beginnings of a science of mind that is neither natural 
science nor applications technology

computer science

engineering

neuroscience

machine 
learning

optimal 
control

reward
system

reinforcement 

learning

operations 
research

classical/operant

conditioning

mathematics

bounded
rationality

psychology

economics

david silver 2015

example: hajime kimura   s rl robots

before

after

backward

new robot, same algorithm

the rl interface

agent

state, 
stimulus, 
situation

reward, 
gain, payoff, 
cost

action, 
response, 
control

environment

(world)

    environment may be unknown, nonlinear, stochastic and complex
    agent learns a policy mapping states to actions

    seeking to maximize its cumulative reward in the long run

signature challenges of rl

evaluative feedback (reward)

sequentiality, delayed consequences

need for trial and error, to explore as well as exploit

non-stationarity

the    eeting nature of time and online data

some rl successes

    learned the world   s best player of backgammon (tesauro 1995)
    learned acrobatic helicopter autopilots (ng, abbeel, coates et al 
2006+)
    widely used in the placement and selection of advertisements and 
pages on the web (e.g., a-b tests)
    used to make strategic decisions in jeopardy! (ibm   s watson 2011)
    achieved human-level performance on atari games from pixel-level 
visual input, in conjunction with deep learning (google deepmind 
2015)
    in all these cases, performance was better than could be obtained 
by any other method, and was obtained without human instruction

example: td-gammon

example: td gammon

tesauro, 1992-1995

 

b
b
a
r
 
2
5
2
4

 

 
 

 

2
3
2
2
2
1

 

 
 

2
0
1
9

 

 
 

 

1
8
1
7
 
1
6
 
 
1
5
 
1
4
 
1
3
 
 
1
2
 
1
1
 
 
1
0
 
 
 
9
 
 
 
8
 
 
 
7
 
 
 
 
6
 
 
 
5
 
 
 
4
 
 
 
 
3
 
 
 
2
 
 
 
 
1
 
 
 
0
 
 

v(s, w)

estimated state value
(    prob of winning)

w

action selection

by a shallow search

w
b
a
r
 

s

start with a random network
play millions of games against itself
learn a value function from this simulated experience
six weeks later it   s the best player of backgammon in the world
originally used expert handcrafted features, later repeated with raw board positions

rl + deep learing performance on atari games

space invaders

breakout

enduro

rl + deep learing performance on atari games

space invaders

breakout

enduro

rl + deep learning, applied to classic atari games   

google deepmind 2015, bowling et al. 2012

    learned to play 49 games for the atari 2600 game console,   
without labels or human input, from self-play and the score alone

research letter

convolution

convolution

fully connected

fully connected

no input

mapping raw
screen pixels

to predictions
of    nal score
for each of 18
joystick actions

    learned to play better than all previous algorithms   
and at human level for more than half the games   

figure 1 | schematic illustration of the convolutional neural network. the
details of the architecture are explained in the methods. the input to the neural
network consists of an 84 3 84 3 4 image produced by the preprocessing
map w, followed by three convolutional layers (note: snaking blue line

symbolizes sliding of each filter across input image) and two fully connected
layers with a single output for each valid action. each hidden layer is followed
by a rectifier nonlinearity (that is, max 0,x  

  ).

difficult and engaging for human players. we used the same network
architecture, hyperparameter values (see extended data table 1) and
learning procedure throughout   taking high-dimensional data (210|160
colour video at 60 hz) as input   to demonstrate that our approach
robustly learns successful policies over a variety of games based solely
on sensory inputs with only very minimal prior knowledge (that is, merely

we compared id25 with the best performing methods from the
id23 literature on the 49 games where results were
available12,15. in addition to the learned agents, we also report scores for
a professional human games tester playing under controlled conditions
and a policy that selects actions uniformly at random (extended data
table 2 and fig. 3, denoted by 100% (human) and 0% (random) on y

same learning 
algorithm applied 
to all 49 games! 
w/o human tuning

outline

introduction to rl successes and challenges

the formal problem: finite id100

part i: exact solution methods and core theoretical ideas

part ii: approximate solution methods
    semi-gradient methods 
    on-policy and off-policy methods 
    the deadly triad; how to evade or survive it 

miscellany and closing remarks

you are the reinforcement learner!
(interactive demo)

optimal policy
(deterministic)
state
action

a
b

2
1

true model of the world

2

-10,80%

1

+10,
100%

a

-10,20%
1
+40,80%

b

+20,
 20%

2

+40,20%

+20,80%

the environment: 
a finite markov decision process (mdp)

howard, 1960

discrete time

t = 1, 2, 3, . . .

a    nite set of states

a    nite set of actions

a    nite set of rewards

life is a trajectory:

1

+10,
100%

a

2

-10,80%

-10,20%
1
+40,80%

b

+20,
 20%

2

+40,20%

+20,80%

. . . st, at, rt+1, st+1, at+1, rt+2, st+2, . . .

with arbitrary markov (stochastic, state-dependent) dynamics:

p(r , s0|s, a) = probhrt+1 = r , st+1 = s0    st = s, at = ai

policies

deterministic policy

a =    (s)

an agent following a policy

at =    (st)

e.g.

state

action

a
b

2
1

the number of 

deterministic policies 
is exponential in the 
number of states

informally the agent   s goal is to choose each action so as to 
maximize the discounted sum of future rewards, 
to choose each at to maximize rt+1 +  rt+2 +  2rt+3 +       
we are searching for a policy

action-value functions

an action-value function says how good it is to be in a state, 
take an action, and thereafter follow a policy:

q   (s, a) = ehrt+1 +  rt+2 +  2rt+3 +           st = s, at = a, at+1:1       i

action-value function 

for the optimal policy and  =0.9

state action value
130.39
133.77
166.23
146.23

a
a
b
b

1
2
1
2

1

+10,
100%

a

2

-10,80%

-10,20%
1
+40,80%

b

+20,
 20%

2

+40,20%

+20,80%

optimal policies

a policy       is optimal if it maximizes the action-value function:

      

q      

(s, a) = max

   

q   (s, a) = q   (s, a)

thus all optimal policies share the same optimal value function

given the optimal value function, it is easy to act optimally:

      (s) = arg max

a

q   (s, a)

   greedi   cation   

we say that the optimal policy is greedy with respect to the 
optimal value function

there is always at least one deterministic optimal policy

part i 

exact solution methods 

(tabular methods) 

id24, the simplest rl algorithm

q(s, a)

1.initialize an array               arbitrarily
2.choose actions in any way, perhaps based on    , such that all 

actions are taken in all states (in   nitely often in the limit)

q

3.on each time step, change one element of the array:

 q(st, at) =       rt+1 +   max

a

q(st+1, a)   q(st, at)   

target

4.if desired, reduce the step-size parameter    over time
theorem: for appropriate choice of 4,     converges to     , and 
its greedy policy to an optimal policy      (watkins & dayan 1992)
this is kind of amazing     learning long-term optimal behavior 
without any model of the environment, for arbitrary mdps!

q
      

q   

demo

o   -policy learning gridworld

policy improvement theorem

given the value function for any policy    :
   

q   (s, a)

for all s, a

it can always be greedi   ed to obtain a better policy:

   0(s) = arg max

a

q   (s, a)

(     is not unique)
   0

where better means:

q   0(s, a)   q   (s, a)

for all s, a

with equality only if both policies are optimal

the dance of policy and value (policy iteration)

   1

   2

   3

evaluate

g r e e d i

f y

evaluate
f y

g r e e d i
evaluate
e

g r e

q   1

q   2

q   3

d if y

      

.
 
.
 
.

eval

greed
      

q   

any policy evaluates to a unique value 
function (soon we will see how to learn it) 
which can be greedi   ed to produce a 
better policy

that in turn evaluates to a value function 

which can in turn be greedi   ed   
each policy is strictly better than the 
previous, until eventually both are optimal
there are no local optima 
the dance converges in a    nite number 
of steps, usually very few

the dance is very robust

to initial conditions

to delayed and asynchronous updating, as in parallel and distributed 
implementations

to incomplete evaluation and greedi   cation
    updating only some states but not others
    updating only part of the way

to randomization and noise

in particular, it works if only a single state is updated at a time by a 
random amount that is only correct in expectation

the explore/exploit dilemma

you can   t do the action that you think is best all the time
    because you will miss out big   forever   if you are wrong
to    nd the real best action, you must explore them all   an 
in   nite number of times!

   

you also can   t explore all the time
    because then you would never get any advantage of your 

learning

thus you must both explore and exploit, but neither to excess. 
what is the right balance?

how did id24 escape the dilemma?

id24, the simplest rl algorithm

1.initialize an array               arbitrarily
2.choose actions in any way, perhaps based on    , such that all 

actions are taken in all states (in   nitely often in the limit)

q

3.on each time step, change one element of the array:

4.if desired, reduce the step-size parameter    over time

 q(st,at)=      rt+1+ maxaq(st+1,a) q(st,at)   q(s,a)id64

the key idea underlying both id145 (dp) and 
all temporal-difference (td) learning

updating an estimate from an estimate, a guess from a guess

based on the bellman expectation equation:

q   (s, a)= ehrt+1 +  rt+2 +  2rt+3 +           st = s, at = a, at+1:1       i
q   (s, a) = ehrt+1 +   max
{z

= ehrt+1 +  q   (st+1, at+1)    st = s, at = a, at+1       i
|

or the bellman optimality equation:
q   (st+1, a0)

    st = s, at = ai

id24   s target for q(st , at )

}

a0

id24 is off-policy learning

off-policy learning is learning about the value of a policy other 
than the policy being used to generate the trajectory

id24 learns about the value of its deterministic greedy 
policy   which gradually become optimal   from data while 
behaving in a more exploratory manner
   
    and this is essential to its strategy for escaping the explore/

thus id24 is off-policy

exploit dilemma

some terminology
   
   
    on-policy learning is when the two policies are the same

the target policy is the policy being learned about
the behavior policy is the policy generating the trajectory data

part ii 

approximate solution methods 

(function approximation)

so, rl    nds optimal policies for arbitrary environments, if the 
value functions and policies can be exactly represented in 
tables

but the real world is too large and complex for tables

will rl work with approximations?

will rl work with function approximators?

function approximation
represent the action-value function by a parameterized 
function approximator with parameter 

   

q(s, a,    )     q   (s, a) or     q   (s, a)

the approximator could be a deep neural network, with the 
weights being the parameter
    or simply a linear weighting of features (the most pressing 
theoretical problems are all best addressed in this setting)

function approximation is a powerful concept, e.g., subsuming 
much of the problem of hidden state

for large applications, it is important that all computations 
scale linearly with the number of parameters

does id24 work with function 
approximation?

yes, there is a obvious generalization of id24 to function 
approximation (watkins 1989)

often, it works well

but there are counterexamples
    simple examples where the parameters diverge to in   nity
    even for linear function approximation

we could get by, but something is not right, there is something, 
probably many things, that we are not understanding

semi-gradient id24 (watkins 1989)

consider the following objective function, based on the 
bellman optimality equation:

a

l(   ) = e2664
    t =       rt+1 +   max

 rt+1 +   max
{z
|

  q(st+1, a,    )

    q(st, at,    !23775
  q(st+1, a,    t)     q(st, at,    t)    @   q(st, at,    t)

the target here depends on the parameter, but if we ignore 
that dependence when taking the derivative, then we get a 
semi-gradient id24 update:

}

@   t

target

a

semi-gradient sarsa (rummery 1994, sutton 1988)

consider instead an objective function based on the bellman 
expectation equation:

target

|

{z

l(   ) = e264 rt+1 +     q(st+1, at+1,    )
    q(st, at,    !2375
}
    t =       rt+1 +     q(st+1, at+1,    t)     q(st, at,    t)    @   q(st, at,    t)

again the target depends on the parameter, and again we 
ignore that dependence when taking the derivative, this time to 
get the semi-gradient sarsa update:

this is an on-policy algorithm: it approximates      not      ;    
q   
thus     should be near greedy, typically it is  -greedy

@   t

q   

   

why is it called sarsa?

it is the only learning update that uses exactly these    ve things 
from the trajectory:

. . . st, at, rt+1, st+1, at+1, . . .

sarsa is equivalent to the td(0) algorithm (sutton 1988) when 
applied to state-action pairs rather than to states
what is an  -greedy policy?
an  -greedy policy is a stochastic policy that is usually greedy, 
but with small id203   instead selects an action at random

as an on-policy method, semi-gradient 
sarsa has good convergence properties

if the function approximator is linear, it is
    guaranteed convergent for prediction (   xed target policy)

(sutton 1988, dayan 1992, tsitsiklis & van roy 1997)

    guaranteed non-divergent for control, with bounded error 

(though may    chatter       gordon 1995)

for general non-linear function approximation, there is one 
known counterexample, but it is very arti   cial and contrived

on-policy methods typically perform better than off-policy 
methods, but    nd poorer policies

cliff-walking example (on-policy vs off-policy)

chapter 3. finite id100

r

r = !1

3.3 8.8 4.4 5.3 1.5

1.5 3.0 2.3 1.9 0.5

0.1 0.7 0.7 0.4 -0.4

safe path

optimal path

actions

-1.0 -0.4 -0.4 -0.6 -1.2
s
-1.9 -1.3 -1.2 -1.4 -2.0

t h e   c l

i f f

g

figure 3.5: grid example: exceptional reward dynamics (left) and state-value function for
the equiprobable random policy (right).

r

r = !100

(b)

except those that move the agent out of the special states a and b. from state a,
all four actions yield a reward of +10 and take the agent to a0. from state b, all
actions yield a reward of +5 and take the agent to b0.

sarsa

!25

suppose the agent selects all four actions with equal id203 in all states.

cliff-walking example (on-policy vs off-policy)

r

r

(on-policy)

(off-policy)

both algorithms 
are      greedy

   = 0.1

mountain car demo 

goal

gravity wins

    linear function 
approximation
    coarse-coded features 
of state (tile coding, 
cmac)

moore 1990, sutton 1996

acrobot demo, sarsa(  =0.9)   

episode 6

acrobot demo, sarsa(  =0.9)

episode 40+

robocup soccer keepaway 

stone, sutton & kuhlmann, 2005

random

learned

hand-coded

hold

stone, sutton & kuhlmann, 2005

13 continuous state variables 

how is the state encoded?
(for 3 vs 2) 
in 13 continuous state variables

11 distances among 
the players, ball, 
and the center of 
the field 

2 angles to takers 
along passing lanes 

the feature-construction pipeline

robocup feature vectors 

.
.
.

.......

.

full 
soccer 
state 

sparse, coarse, 

tile coding 

linear 
map    

action 
values 

13 continuous 
state variables 

r  
huge binary feature vector 
    s 
(about 400 1   s and 40,000 0   s) 

but let   s return to the bad news,  
the problem of instability with 

semi-gradient id24

what causes the problem of instability?

it has nothing to do with learning or sampling
    even id145, the classical solution method 

for known mdps, suffers from divergence with function 
approximation

it has nothing to do with exploration, greedi   cation, or control
    even policy evaluation alone can diverge

it has nothing to do with complex non-linear approximators
    even simple linear approximators can produce instability 

the deadly triad

the risk of divergence arises whenever we combine three things:
1. function approximation

signi   cantly generalizing from large numbers of examples

2. id64

learning value estimates from other value estimates,    
as in id145 and temporal-difference learning

3. off-policy learning

(why is id145 off-policy?)

learning about a policy from data not due to that policy,    
as in id24, where we learn about the greedy policy from 
data with a necessarily more exploratory policy

any two without the third is ok

can we do without id64?

id64 is critical to the computational ef   ciency of dp
id64 is critical to the data ef   ciency of td methods
on the other hand, id64 introduces bias, which harms 
the asymptotic performance of approximate methods
the degree of id64 can be    nely controlled via the    
parameter, from   =0 (full id64) to   =1 (no id64)
for the naive loss:   
    
    
semi-gradient sarsa(  ) converges to a    xpoint            where

l(   ) = e      q   (st, at)     q(st, at,       2 

   sarsa

l(   sarsa)    

(cid:15482)    =1 is best!?

1     
1    

l(   )

min

   

tsitsiklis & van roy 1997

4 examples of the effect of id64   
suggest that   =1 (no id64) is a very poor choice

in all cases, 
lower is better

red points are   
the cases of no 
id64

pure

id64

no

id64

other ways to survive the deadly triad
use high   .  use good features
recent results suggest that replay and more stable targets (e.g., double 
id24, van hasselt 2010) may help, but it is too soon to be sure
use least-squares methods like off-policy lstd(  ) (yu 2010, mahmood et 
al. 2015). such methods (bradtke & barto 1996, boyan 2000) easily 
survive the triad, but their computational costs scale with the square of the 
number of parameters
try the new true-gradient rl methods (gradient-td and proximal-gradient-
td) developed by maei (2011) and mahadevan (2015) et al. these seem 
to me to be the best attempts to make td methods with the robust 
convergence properties of stochastic id119. residual gradient 
methods (baird 1999) are also true gradient methods, but optimize a poor 
objective, or can   t learn purely from data (double sampling). these and 
other methods based on the bellman error/residual are not recommended
try the even newer emphatic-td methods (sutton, white & mahmood 
2015, yu 2015). these semi-gradient methods attain stability through an 
extension of the early on-policy theorems

outline

introduction to rl successes and challenges

the formal problem: finite id100

part i: exact solution methods and core theoretical ideas

part ii: approximate solution methods
    semi-gradient methods 
    on-policy and off-policy methods 
    the deadly triad; how to evade or survive it 

miscellany and closing remarks

the many dimensions of rl

problems
    prediction vs control 
    mdps vs bandits (one state, non-sequential)

methods
    tabular vs function approximation
    on-policy vs off-policy
    id64 vs monte carlo (uni   ed by eligibility traces)
    model-based vs model-free
    value-based vs policy-based

and yet there is an amazing unity and convergence of methods

policy-gradient actor-critic methods

policy is explicitly represented 
with its own parameters 
independent of any value 
function
policy parameters are updated 
by stochastic gradient ascent in 
a performance measure such as 
average reward per step
a state-value function (critic) is 
optional but can signi   cantly 
reduce variance
good convergence properties 
(on-policy)

why approximate policies rather than values?

in many problems, the policy is simpler than the value function

in many problems, the optimal policy is stochastic
    e.g., bluf   ng, pomdps

to enable smoother change in policies

to avoid a search on every step (the max)

to better relate to biology

we should never discount   
when optimizing approximate policies!

(cid:1)!

it breaks the de   nition of an optimal policy
with approximation, the optimal policy is no 
longer representable
there is no way to rank the remaining policies
different policies will be best in different states
instead, you must say what states you care 
about
or else use average reward    
(which you should probably do anyway)

model-based id23

learn a model of the environment   s transition dynamics

  p(r , s0|s, a)     p(r , s0|s, a)

use it to generate simulated trajectories

apply rl methods to the simulated trajectories, as if they had 
really happened, to learn an action-value function and policy

can be intermixed with direct rl

model-based rl: gridworld example

andrew ng, adam coates, pieter abbeel, et al., stanford university

eligibility traces

an elegant uni   cation of id64 and monte carlo (non-
id64) methods

a key algorithmic innovation that greatly reduces computational 
complexity in multi-step prediction learning; its most important 
advantages have nothing to do with id64 or control

necessary to extend rl beyond discrete time steps that just 
happen to be nicely aligned with the world   s causal dynamics

one of the most algorithmically intense topics in rl
   

interacts strongly with off-policy learning via importance 
sampling, and concomitant struggles to reduce variance 

temporal abstraction in rl

function approximation abstracts over state, but we need also 
to abstract over time

there are several approaches, including the framework of 
options   macro-actions of extended and variable duration that 
can nevertheless interoperate with primitive actions in dp 
planning methods and td learning methods

the problems of temporal abstraction can be divided into three 
classes, increasing in dif   culty:
   
   
    discovering temporal abstractions and selecting among them

representing temporal abstractions (e.g., by options)
learning temporal abstractions (e.g., by off-policy methods)

conclusion

id23 is a big topic, with a long history, an 
elegant theoretical core, novel algorithms, many open 
problems, and vast unexplored territories

rl can be viewed as a microcosm of the whole ai problem, 
including planning, acting, learning, perception, world 
modeling, even id99

yet, even so, it can be reduced to small steps on each of 
which measurable progress can be made

rl    ts well into the longest mega-trend in ai, that towards 
turning more of the work over to the machine

realistic, ambitious, pragmatic

thank you for your attention

the rl&ai group at 
the univ. of alberta

in 2011

