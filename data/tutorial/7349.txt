mllib: scalable machine learning on spark

xiangrui meng   

collaborators: ameet talwalkar, evan sparks, virginia smith, xinghao 
pan, shivaram venkataraman, matei zaharia, rean grif   th, john duchi, 
joseph gonzalez, michael franklin, michael i. jordan, tim kraska, etc.   

1

what is mllib?

2

what is mllib?

mllib is a spark subproject providing machine 
learning primitives: 

    initial contribution from amplab, uc berkeley 

    shipped with spark since version 0.8 

    33 contributors

3

what is mllib?

algorithms:!
    classi   cation: id28, linear support vector machine 

(id166), naive bayes 

    regression: generalized id75 (glm) 
    collaborative    ltering: alternating least squares (als) 
    id91: id116 
    decomposition: singular value decomposition (svd), principal 

component analysis (pca)

4

why mllib?

5

scikit-learn?

algorithms:!
    classi   cation: id166, nearest neighbors, id79,     

   

regression: support vector regression (svr), ridge regression, 
lasso, id28,    !

    id91: id116, spectral id91,     
    decomposition: pca, non-negative id105 (nmf), 

independent component analysis (ica),    

6

mahout?

algorithms:!
    classi   cation: id28, naive bayes, id79,     
    collaborative    ltering: als,     
    id91: id116, fuzzy id116,     
    decomposition: svd, randomized svd,    

7

mahout?

liblinear?

h2o?
matlab?

scikit-learn?

vowpal wabbit?

r?
weka?

8

why mllib?

9

why mllib?

    it is built on apache spark, a fast and general 

engine for large-scale data processing. 
    run programs up to 100x faster than    
hadoop mapreduce in memory, or    
10x faster on disk. 

    write applications quickly in java, scala, or python.

10

id119

w   w         

nxi=1

g(w; xi, yi)

val points = spark.textfile(...).map(parsepoint).cache() 
var w = vector.zeros(d) 
for (i <- 1 to numiterations) { 
    val gradient = points.map { p => 
        (1 / (1 + exp(-p.y * w.dot(p.x)) - 1) * p.y * p.x 
    ).reduce(_ + _) 
    w -= alpha * gradient 
}

11

id116 (scala)

// load and parse the data. 
val data = sc.textfile("kmeans_data.txt") 
val parseddata = data.map(_.split(    ').map(_.todouble)).cache() 
"
// cluster the data into two classes using kmeans. 
val clusters = kmeans.train(parseddata, 2, numiterations = 20) 
"
// compute the sum of squared errors. 
val cost = clusters.computecost(parseddata) 
println("sum of squared errors = " + cost)

12

id116 (python)

# load and parse the data 
data = sc.textfile("kmeans_data.txt") 
parseddata = data.map(lambda line:  
               array([float(x) for x in line.split('    )])).cache() 
"
# build the model (cluster the data) 
clusters = kmeans.train(parseddata, 2, maxiterations = 10, 
             runs = 1, initialization_mode = "kmeans||") 
"
# evaluate id91 by computing the sum of squared errors 
def error(point): 
    center = clusters.centers[clusters.predict(point)] 
    return sqrt(sum([x**2 for x in (point - center)])) 
"
cost = parseddata.map(lambda point: error(point)) 
         .reduce(lambda x, y: x + y) 
print("sum of squared error = " + str(cost))

13

dimension reduction   

+ id116

// compute principal components 
val points: rdd[vector] = ... 
val mat = rowrddmatrix(points) 
val pc = mat.computeprincipalcomponents(20) 
"
// project points to a low-dimensional space 
val projected = mat.multiply(pc).rows 
"
// train a id116 model on the projected data 
val model = kmeans.train(projected, 10)

collaborative    ltering

// load and parse the data 
val data = sc.textfile("mllib/data/als/test.data") 
val ratings = data.map(_.split(',') match { 
    case array(user, item, rate) =>  
      rating(user.toint, item.toint, rate.todouble) 
}) 
"
// build the recommendation model using als 
val model = als.train(ratings, 1, 20, 0.01) 
"
// evaluate the model on rating data 
val usersproducts = ratings.map { case rating(user, product, rate) =>   
  (user, product) 
} 
val predictions = model.predict(usersproducts)

15

why mllib?

    it ships with spark as    
a standard component.

16

out for dinner?!
    search for a restaurant and make a reservation. 
    start navigation. 
    food looks good? take a photo and share.

17

why smartphone?

out for dinner?!
    search for a restaurant and make a reservation. (yellow pages?) 
    start navigation. (gps?) 
    food looks good? take a photo and share. (camera?)

18

why mllib?

a special-purpose device may be better at one 
aspect than a general-purpose device. but the cost 
of context switching is high: 
    different languages or apis 
    different data formats 
    different tuning tricks

19

spark sql + mllib

// data can easily be extracted from existing sources, 
// such as apache hive. 
val trainingtable = sql(""" 
  select e.action, 
         u.age, 
         u.latitude, 
         u.longitude 
  from users u 
  join events e 
  on u.userid = e.userid""") 
"
// since `sql` returns an rdd, the results of the above 
// query can be easily used in mllib. 
val training = trainingtable.map { row => 
  val features = vectors.dense(row(1), row(2), row(3)) 
  labeledpoint(row(0), features) 

} "

val model = id166withsgd.train(training)

streaming + mllib

// collect tweets using streaming 
"
// train a id116 model 
val model: kmmeansmodel = ... 
"
// apply model to filter tweets 
val tweets = twitterutils.createstream(ssc, some(authorizations(0))) 
val statuses = tweets.map(_.gettext) 
val filteredtweets =  
  statuses.filter(t => model.predict(featurize(t)) == clusternumber) 
"
// print tweets within this particular cluster 
filteredtweets.print()

graphx + mllib

// assemble link graph 
val graph = graph(pages, links) 
val id95: rdd[(long, double)] = graph.staticid95(10).vertices 
"
// load page labels (spam or not) and content features 
val labelandfeatures: rdd[(long, (double, seq((int, double)))] = ... 
val training: rdd[labeledpoint] =  
  labelandfeatures.join(id95).map { 
    case (id, ((label, features), id95)) => 
      labeledpoint(label, vectors.sparse(features ++ (1000, id95)) 
} 
"
// train a spam detector using id28 
val model = logisticregressionwithsgd.train(training)

why mllib?

    spark is a general-purpose big data platform. 

    runs in standalone mode, on yarn, ec2, and mesos, also 

on hadoop v1 with simr. 

    reads from hdfs, s3, hbase, and any hadoop data source. 

    mllib is a standard component of spark providing 

machine learning primitives on top of spark. 

    mllib is also comparable to or even better than other 
libraries specialized in large-scale machine learning.

23

why mllib?

    spark is a general-purpose big data platform. 

    runs in standalone mode, on yarn, ec2, and mesos, also 

on hadoop v1 with simr. 

    reads from hdfs, s3, hbase, and any hadoop data source. 

    mllib is a standard component of spark providing 

machine learning primitives on top of spark. 

    mllib is also comparable to or even better than other 
libraries specialized in large-scale machine learning.

24

why mllib?

    scalability 

    performance 

    user-friendly apis 

    integration with spark and its other components

25

id28

26

id28 - weak scaling

e
m

i
t
l
l

a
w
 
e
v
i
t

l

a
e
r

10

8

6

4

2

0
 
0

mlbase
mllib
vw
ideal

5

10

15

# machines

20

 

4000

3000

2000

1000

)
s
(
 
e
m

i
t
l
l
a
w

n=6k, d=160k
n=12.5k, d=160k
n=25k, d=160k
n=50k, d=160k
n=100k, d=160k
n=200k, d=160k

25

30

0

 

mllib
mlbase

vw

matlab

fig. 6: weak scaling for id28
    full dataset: 200k images, 160k dense features. 
    similar weak scaling. 
    mllib within a factor of 2 of vw   s wall-clock time.

mlbase
vw
ideal

30

35

 

25

p
u
d
e
e
p
s

20

15

 

27

0
 
0

5

10

15

20

25

30

l

a
w

1000

id28 - strong scaling

fig. 6: weak scaling for id28

# machines

0

 

mlbase

vw

matlab

p
u
d
e
e
p
s

35

30

25

20

15

10

5

0
 
0

mllib
mlbase
vw
ideal

5

10

15

# machines

20

 

fig. 5: walltime for weak scaling for id28.

)
s
(
 
e
m

i
t
l
l

a
w

1400

1200

1000

800

600

400

200

0

 

1 machine
2 machines
4 machines
8 machines
16 machines
32 machines

mllib
mlbase

vw

matlab

fig. 7: walltime for strong scaling for id28.

25

30

fig. 8: strong scaling for id28

    fixed dataset: 50k images, 160k dense features. 
    mllib exhibits better scaling properties. 
    mllib is faster than vw with 16 and 32 machines.

with respect to computation. in practice, we see comparable
scaling results as more machines are added.

system
mlbase
graphlab
mahout

lines of code

32
383
865

in matlab, we implement id119 instead of
sgd, as id119 requires roughly the same number
of numeric operations as sgd but does not require an inner
loop to pass over the data. it can thus be implemented in a
   vectorized    fashion, which leads to a signi   cantly more favor-

28

collaborative    ltering

29

collaborative    ltering

    recover   a   ra-ng   matrix   from   a   

subset   of   its   entries.   

30

?????als - wall-clock time

    dataset: scaled version of net   ix data (9x in size). 
    cluster: 9 machines. 
    mllib is an order of magnitude faster than mahout. 
    mllib is within factor of 2 of graphlab.

31

systemwall-     clock   /me   (seconds)matlab15443mahout4206graphlab291mllib481implementation of id116

initialization: 

   

random 

    id116++ 

    id116||

implementation of id116

iterations: 
    for each point,    nd its closest center.   

    update cluster centers.

li = arg min

2

j kxi   cjk2
cj = pi,li=j xj
pi,li=j 1

   
implementation of id116

the points are usually sparse, but the centers are most likely to be 
dense. computing the distance takes o(d) time. so the time 
complexity is o(n d k) per iteration. we don   t take any advantage of 
sparsity on the running time. however, we have 

kx   ck2

2 = kxk2

2 + kck2

2   2hx, ci

computing the inner product only needs non-zero elements. so we 
can cache the norms of the points and of the centers, and then only 
need the inner products to obtain the distances. this reduce the 
running time to o(nnz k + d k) per iteration. 
"
however, is it accurate?

implementation of als

    broadcast everything 
    data parallel 
    fully parallel

35

alternating least squares (als)

36

broadcast everything
    master loads (small) 
data    le and initializes 
models. 

ratings

movie!
factors

user!
factors

master

workers

    master broadcasts 

data and initial 
models. 
    at each iteration, 
updated models are 
broadcast again. 
    works ok for small 
data. 
    lots of 

communication 
overhead - doesn   t 
scale well. 

data parallel

movie!
factors

user!
factors

master

ratings

ratings

ratings

ratings

workers

38

initial models 

    workers load data 
    master broadcasts 
    at each iteration, 
updated models are 
broadcast again 
    much better scaling 
    works on large 
    works well for smaller 

datasets 

models. (low k)

fully parallel

ratings
movie!
user!
factors
factors

ratings
movie!
user!
factors
factors

ratings
movie!
user!
factors
factors

ratings
movie!
user!
factors
factors

master

workers

39

    workers load data 
    models are 
instantiated at 
workers. 
    at each iteration, 

models are shared via 
join between workers. 

    much better 
scalability. 
    works on large 

datasets 

implementation of als

    broadcast everything 
    data parallel 
    fully parallel 
    block-wise parallel 

    users/products are partitioned into blocks and join is 

based on blocks instead of individual user/product.

40

new features for v1.x

    sparse data 
    classi   cation and regression tree (cart) 
    svd and pca 
    l-bfgs 
    model evaluation 
    discretization

41

contributors

ameet talwalkar, andrew tulloch, chen chao, nan zhu, db tsai, evan 
sparks, frank dai, ginger smith, henry saputra, holden karau, 
hossein falaki, jey kottalam, cheng lian, marek kolodziej, mark 
hamstra, martin jaggi, martin weindel, matei zaharia, nick pentreath, 
patrick wendell, prashant sharma, reynold xin, reza zadeh, sandy 
ryza, sean owen, shivaram venkataraman, tor myklebust, xiangrui 
meng, xinghao pan, xusen yin, jerry shao, ryan lecompte

42

interested?

    website: http://spark.apache.org 

    tutorials: http://ampcamp.berkeley.edu 

    spark summit: http://spark-summit.org 

    github: https://github.com/apache/spark 
    mailing lists: user@spark.apache.org   
                     dev@spark.apache.org

43

