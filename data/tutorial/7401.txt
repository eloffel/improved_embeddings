speech and language processing. daniel jurafsky & james h. martin.
rights reserved.

draft of september 23, 2018.

copyright c(cid:13) 2018.

all

chapter

10 formal grammars of english

syntax

the study of grammar has an ancient pedigree; panini   s grammar of sanskrit was
written over two thousand years ago and is still referenced today in teaching san-
skrit. despite this history, knowledge of grammar remains spotty at best. in this
chapter, we make a preliminary stab at addressing some of these gaps in our knowl-
edge of grammar and syntax, as well as introducing some of the formal mechanisms
that are available for capturing this knowledge in a computationally useful manner.
the word syntax comes from the greek s  yntaxis, meaning    setting out together
or arrangement   , and refers to the way words are arranged together. we have seen
various syntactic notions in previous chapters. the regular languages introduced
in chapter 2 offered a simple way to represent the ordering of strings of words, and
chapter 3 showed how to compute probabilities for these word sequences. chapter 8
showed that part-of-speech categories could act as a kind of equivalence class for
words. in this chapter and next few we introduce a variety of syntactic phenomena
and models for syntax and grammar that go well beyond these simpler approaches.

the bulk of this chapter is devoted to the topic of context-free grammars. context-
free grammars are the backbone of many formal models of the syntax of natural
language (and, for that matter, of computer languages). as such, they are integral to
many computational applications, including grammar checking, semantic interpreta-
tion, dialogue understanding, and machine translation. they are powerful enough to
express sophisticated relations among the words in a sentence, yet computationally
tractable enough that ef   cient algorithms exist for parsing sentences with them (as
we show in chapter 11). in chapter 12, we show that adding id203 to context-
free grammars gives us a powerful model of disambiguation. and in chapter 15 we
show how they provide a systematic framework for semantic interpretation.

in addition to an introduction to this grammar formalism, this chapter also pro-
vides a brief overview of the grammar of english. to illustrate our grammars, we
have chosen a domain that has relatively simple sentences, the air traf   c informa-
tion system (atis) domain (hemphill et al., 1990). atis systems were an early
example of spoken language systems for helping book airline reservations. users
try to book    ights by conversing with the system, specifying constraints like i   d like
to    y from atlanta to denver.

10.1 constituency

the fundamental notion underlying the idea of constituency is that of abstraction    
groups of words behaving as a single units, or constituents. a signi   cant part of
developing a grammar involves discovering the inventory of constituents present in
the language.

how do words group together in english? consider the noun phrase, a sequence
of words surrounding at least one noun. here are some examples of noun phrases

noun phrase

2 chapter 10

    formal grammars of english

(thanks to damon runyon):

harry the horse
the broadway coppers
they

a high-class spot such as mindy   s
the reason he comes into the hot box
three parties from brooklyn

what evidence do we have that these words group together (or    form constituents   )?

one piece of evidence is that they can all appear in similar syntactic environments,
for example, before a verb.

three parties from brooklyn arrive. . .
a high-class spot such as mindy   s attracts. . .
the broadway coppers love. . .
they sit

but while the whole noun phrase can occur before a verb, this is not true of each
of the individual words that make up a noun phrase. the following are not grammat-
ical sentences of english (recall that we use an asterisk (*) to mark fragments that
are not grammatical english sentences):

*from arrive. . . *as attracts. . .
*the is. . .

*spot sat. . .

preposed
postposed

thus, to correctly describe facts about the ordering of these words in english, we
must be able to say things like    noun phrases can occur before verbs   .

other kinds of evidence for constituency come from what are called preposed or
postposed constructions. for example, the prepositional phrase on september sev-
enteenth can be placed in a number of different locations in the following examples,
including at the beginning (preposed) or at the end (postposed):

on september seventeenth, i   d like to    y from atlanta to denver
i   d like to    y on september seventeenth from atlanta to denver
i   d like to    y from atlanta to denver on september seventeenth

but again, while the entire phrase can be placed differently, the individual words

making up the phrase cannot be

*on september, i   d like to    y seventeenth from atlanta to denver
*on i   d like to    y september seventeenth from atlanta to denver
*i   d like to    y on september from atlanta to denver seventeenth

see radford (1988) for further examples of groups of words behaving as a single

constituent.

10.2 context-free grammars

the most widely used formal system for modeling constituent structure in english
and other natural languages is the context-free grammar, or id18. context-
free grammars are also called phrase-structure grammars, and the formalism
is equivalent to backus-naur form, or bnf. the idea of basing a grammar on
constituent structure dates back to the psychologist wilhelm wundt (1900) but was
not formalized until chomsky (1956) and, independently, backus (1959).

a context-free grammar consists of a set of rules or productions, each of which

id18

rules

10.2

    context-free grammars

3

lexicon
np

expresses the ways that symbols of the language can be grouped and ordered to-
gether, and a lexicon of words and symbols. for example, the following productions
express that an np (or noun phrase) can be composed of either a propernoun or
a determiner (det) followed by a nominal; a nominal in turn can consist of one or
more nouns.

np     det nominal
np     propernoun

nominal     noun | nominal noun

context-free rules can be hierarchically embedded, so we can combine the pre-

vious rules with others, like the following, that express facts about the lexicon:

det     a
det     the
noun        ight

terminal

non-terminal

derivation
parse tree

dominates

start symbol

verb phrase

the symbols that are used in a id18 are divided into two classes. the symbols
that correspond to words in the language (   the   ,    nightclub   ) are called terminal
symbols; the lexicon is the set of rules that introduce these terminal symbols. the
symbols that express abstractions over these terminals are called non-terminals. in
each context-free rule, the item to the right of the arrow (   ) is an ordered list of one
or more terminals and non-terminals; to the left of the arrow is a single non-terminal
symbol expressing some cluster or generalization. notice that in the lexicon, the
non-terminal associated with each word is its lexical category, or part-of-speech,
which we de   ned in chapter 8.

a id18 can be thought of in two ways: as a device for generating sentences
and as a device for assigning a structure to a given sentence. viewing a id18 as a
generator, we can read the     arrow as    rewrite the symbol on the left with the string
of symbols on the right   .
so starting from the symbol:
we can use our    rst rule to rewrite np as:
and then rewrite nominal as:
and    nally rewrite these parts-of-speech as:

np
det nominal
det noun
a    ight

we say the string a    ight can be derived from the non-terminal np. thus, a id18
can be used to generate a set of strings. this sequence of rule expansions is called a
derivation of the string of words. it is common to represent a derivation by a parse
tree (commonly shown inverted with the root at the top). figure 10.1 shows the tree
representation of this derivation.

in the parse tree shown in fig. 10.1, we can say that the node np dominates
all the nodes in the tree (det, nom, noun, a,    ight). we can say further that it
immediately dominates the nodes det and nom.

the formal language de   ned by a id18 is the set of strings that are derivable
from the designated start symbol. each grammar must have one designated start
symbol, which is often called s. since context-free grammars are often used to de   ne
sentences, s is usually interpreted as the    sentence    node, and the set of strings that
are derivable from s is the set of sentences in some simpli   ed version of english.

let   s add a few additional rules to our inventory. the following rule expresses

the fact that a sentence can consist of a noun phrase followed by a verb phrase:

s     np vp i prefer a morning    ight

4 chapter 10

    formal grammars of english

np

det

nom

a

noun

   ight

figure 10.1 a parse tree for    a    ight   .

a verb phrase in english consists of a verb followed by assorted other things;
for example, one kind of verb phrase consists of a verb followed by a noun phrase:

vp     verb np prefer a morning    ight

or the verb may be followed by a noun phrase and a prepositional phrase:

vp     verb np pp leave boston in the morning

or the verb phrase may have a verb followed by a prepositional phrase alone:

vp     verb pp leaving on thursday

a prepositional phrase generally has a preposition followed by a noun phrase.
for example, a common type of prepositional phrase in the atis corpus is used to
indicate location or direction:

pp     preposition np from los angeles

the np inside a pp need not be a location; pps are often used with times and
dates, and with other nouns as well; they can be arbitrarily complex. here are ten
examples from the atis corpus:

to seattle
in minneapolis
on wednesday
in the evening
on the ninth of july
figure 10.2 gives a sample lexicon, and fig. 10.3 summarizes the grammar rules
we   ve seen so far, which we   ll call l0. note that we can use the or-symbol | to
indicate that a non-terminal has alternate possible expansions.

on these    ights
about the ground transportation in chicago
of the round trip    ight on united airlines
of the ap    fty seven    ight
with a stopover in nashville

we can use this grammar to generate sentences of this    atis-language   . we
start with s, expand it to np vp, then choose a random expansion of np (let   s say, to
i), and a random expansion of vp (let   s say, to verb np), and so on until we generate
the string i prefer a morning    ight. figure 10.4 shows a parse tree that represents a
complete derivation of i prefer a morning    ight.

it is sometimes convenient to represent a parse tree in a more compact format
called bracketed notation; here is the bracketed representation of the parse tree of
fig. 10.4:
(10.1)

[s [np [pro i]] [vp [v prefer] [np [det a] [nom [n morning] [nom [n    ight]]]]]]

bracketed
notation

10.2

    context-free grammars

5

noun        ights | breeze | trip | morning
verb     is | prefer | like | need | want |    y
adjective     cheapest | non-stop |    rst | latest
| other | direct
pronoun     me | i | you | it

proper-noun     alaska | baltimore | los angeles
| chicago | united | american
determiner     the | a | an | this | these | that
preposition     from | to | on | near
conjunction     and | or | but

figure 10.2 the lexicon for l0.

nominal     nominal noun morning +    ight

grammar rules
s     np vp
np     pronoun

| proper-noun
| det nominal
| noun
vp     verb

| verb np
| verb np pp
| verb pp

examples

i + want a morning    ight

i
los angeles
a +    ight

   ights

do
want + a    ight
leave + boston + in the morning
leaving + on thursday

pp     preposition np from + los angeles

figure 10.3 the grammar for l0, with example phrases for each rule.

a id18 like that of l0 de   nes a formal language. we saw in chapter 2 that a for-
mal language is a set of strings. sentences (strings of words) that can be derived by a
grammar are in the formal language de   ned by that grammar, and are called gram-
matical sentences. sentences that cannot be derived by a given formal grammar are
not in the language de   ned by that grammar and are referred to as ungrammatical.
this hard line between    in    and    out    characterizes all formal languages but is only
a very simpli   ed model of how natural languages really work. this is because de-
termining whether a given sentence is part of a given natural language (say, english)
often depends on the context. in linguistics, the use of formal languages to model
natural languages is called generative grammar since the language is de   ned by
the set of possible sentences    generated    by the grammar.

grammatical
ungrammatical

generative
grammar

10.2.1 formal de   nition of context-free grammar

we conclude this section with a quick, formal description of a context-free gram-
mar and the language it generates. a context-free grammar g is de   ned by four
parameters: n,   , r, s (technically this is a    4-tuple   ).

6 chapter 10

    formal grammars of english

s

np

vp

pro

verb

np

i

prefer

det

nom

a

nom

noun

noun

   ight

morning

figure 10.4 the parse tree for    i prefer a morning    ight    according to grammar l0.

n a set of non-terminal symbols (or variables)
   a set of terminal symbols (disjoint from n)
r a set of rules or productions, each of the form a        ,

where a is a non-terminal,
   is a string of symbols from the in   nite set of strings (      n)   

s a designated start symbol and a member of n

for the remainder of the book we adhere to the following conventions when dis-
cussing the formal properties of context-free grammars (as opposed to explaining
particular facts about english or other languages).

capital letters like a, b, and s
s
lower-case greek letters like   ,    , and   
lower-case roman letters like u, v, and w

non-terminals
the start symbol
strings drawn from (      n)   
strings of terminals

a language is de   ned through the concept of derivation. one string derives an-
other one if it can be rewritten as the second one by some series of rule applications.
more formally, following hopcroft and ullman (1979),

if a        is a production of r and    and    are any strings in the set
(      n)   , then we say that   a   directly derives       , or   a             .

derivation is then a generalization of direct derivation:

let   1,   2, . . . ,   m be strings in (      n)   ,m     1, such that

directly derives

derives

we say that   1 derives   m, or   1

         m.

  1       2,  2       3, . . . ,  m   1       m

we can then formally de   ne the language lg generated by a grammar g as the
set of strings composed of terminal symbols that can be derived from the designated

10.3

    some grammar rules for english

7

start symbol s.

lg = {w|w is in       and s

       w}

syntactic
parsing

the problem of mapping from a string of words to its parse tree is called syn-

tactic parsing; we de   ne algorithms for parsing in chapter 11.

10.3 some grammar rules for english

in this section, we introduce a few more aspects of the phrase structure of english;
for consistency we will continue to focus on sentences from the atis domain. be-
cause of space limitations, our discussion is necessarily limited to highlights. read-
ers are strongly advised to consult a good reference grammar of english, such as
huddleston and pullum (2002).

10.3.1 sentence-level constructions
in the small grammar l0, we provided only one sentence-level construction for
declarative sentences like i prefer a morning    ight. among the large number of
constructions for english sentences, four are particularly common and important:
declaratives, imperatives, yes-no questions, and wh-questions.

sentences with declarative structure have a subject noun phrase followed by
a verb phrase, like    i prefer a morning    ight   . sentences with this structure have
a great number of different uses that we follow up on in chapter 24. here are a
number of examples from the atis domain:
i want a    ight from ontario to chicago
the    ight should be eleven a.m. tomorrow
the return    ight should leave at around seven p.m.

sentences with imperative structure often begin with a verb phrase and have
no subject. they are called imperative because they are almost always used for
commands and suggestions; in the atis domain they are commands to the system.

show the lowest fare
give me sunday   s    ights arriving in las vegas from new york city
list all    ights between    ve and seven p.m.

we can model this sentence structure with another rule for the expansion of s:

s     vp

declarative

imperative

yes-no question

sentences with yes-no question structure are often (though not always) used to
ask questions; they begin with an auxiliary verb, followed by a subject np, followed
by a vp. here are some examples. note that the third example is not a question at
all but a request; chapter 24 discusses the uses of these question forms to perform
different pragmatic functions such as asking, requesting, or suggesting.

do any of these    ights have stops?
does american   s    ight eighteen twenty    ve serve dinner?
can you give me the same information for united?

here   s the rule:

s     aux np vp

8 chapter 10

    formal grammars of english

wh-phrase
wh-word

the most complex sentence-level structures we examine here are the various wh-
structures. these are so named because one of their constituents is a wh-phrase, that
is, one that includes a wh-word (who, whose, when, where, what, which, how, why).
these may be broadly grouped into two classes of sentence-level structures. the
wh-subject-question structure is identical to the declarative structure, except that
the    rst noun phrase contains some wh-word.

what airlines    y from burbank to denver?
which    ights depart burbank after noon and arrive in denver by six p.m?
whose    ights serve breakfast?

here is a rule. exercise 10.7 discusses rules for the constituents that make up the
wh-np.

s     wh-np vp

wh-non-subject-
question

in the wh-non-subject-question structure, the wh-phrase is not the subject of the
sentence, and so the sentence includes another subject. in these types of sentences
the auxiliary appears before the subject np, just as in the yes-no question structures.
here is an example followed by a sample rule:

what    ights do you have from burbank to tacoma washington?

long-distance
dependencies

clause

s     wh-np aux np vp

constructions like the wh-non-subject-question contain what are called long-
distance dependencies because the wh-np what    ights is far away from the predi-
cate that it is semantically related to, the main verb have in the vp. in some models
of parsing and understanding compatible with the grammar rule above, long-distance
dependencies like the relation between    ights and have are thought of as a semantic
relation. in such models, the job of    guring out that    ights is the argument of have
is done during semantic interpretation. in other models of parsing, the relationship
between    ights and have is considered to be a syntactic relation, and the grammar is
modi   ed to insert a small marker called a trace or empty category after the verb.
we return to such empty-category models when we introduce the id32 on
page 15.

10.3.2 clauses and sentences
before we move on, we should clarify the status of the s rules in the grammars we
just described. s rules are intended to account for entire sentences that stand alone
as fundamental units of discourse. however, s can also occur on the right-hand side
of grammar rules and hence can be embedded within larger sentences. clearly then,
there   s more to being an s than just standing alone as a unit of discourse.

what differentiates sentence constructions (i.e., the s rules) from the rest of the
grammar is the notion that they are in some sense complete. in this way they corre-
spond to the notion of a clause, which traditional grammars often describe as form-
ing a complete thought. one way of making this notion of    complete thought    more
precise is to say an s is a node of the parse tree below which the main verb of the s
has all of its arguments. we de   ne verbal arguments later, but for now let   s just see
an illustration from the tree for i prefer a morning    ight in fig. 10.4 on page 6. the
verb prefer has two arguments: the subject i and the object a morning    ight. one of
the arguments appears below the vp node, but the other one, the subject np, appears
only below the s node.

10.3

    some grammar rules for english

9

10.3.3 the noun phrase
our l0 grammar introduced three of the most frequent types of noun phrases that
occur in english: pronouns, proper nouns and the np     det nominal construction.
the central focus of this section is on the last type since that is where the bulk of
the syntactic complexity resides. these noun phrases consist of a head, the central
noun in the noun phrase, along with various modi   ers that can occur before or after
the head noun. let   s take a close look at the various parts.

the determiner

noun phrases can begin with simple lexical determiners, as in the following exam-
ples:

a stop
those    ights

the    ights
any    ights

this    ight
some    ights

the role of the determiner in english noun phrases can also be    lled by more

complex expressions, as follows:

united   s    ight
united   s pilot   s union
denver   s mayor   s mother   s canceled    ight

in these examples, the role of the determiner is    lled by a possessive expression
consisting of a noun phrase followed by an    s as a possessive marker, as in the
following rule.

det     np (cid:48)s

the fact that this rule is recursive (since an np can start with a det) helps us
model the last two examples above, in which a sequence of possessive expressions
serves as a determiner.

under some circumstances determiners are optional in english. for example,

determiners may be omitted if the noun they modify is plural:

(10.2) show me    ights from san francisco to denver on weekdays
as we saw in chapter 8, mass nouns also don   t require determination. recall that
mass nouns often (not always) involve something that is treated like a substance
(including e.g., water and snow), don   t take the inde   nite article    a   , and don   t tend
to pluralize. many abstract nouns are mass nouns (music, homework). mass nouns
in the atis domain include breakfast, lunch, and dinner:

(10.3) does this    ight serve dinner?

the nominal

the nominal construction follows the determiner and contains any pre- and post-
head noun modi   ers. as indicated in grammar l0, in its simplest form a nominal
can consist of a single noun.

nominal     noun

as we   ll see, this rule also provides the basis for the bottom of various recursive
rules used to capture more complex nominal constructions.

10 chapter 10

    formal grammars of english

cardinal
numbers

ordinal
numbers
quanti   ers

before the head noun
a number of different kinds of word classes can appear before the head noun (the
   postdeterminers   ) in a nominal. these include cardinal numbers, ordinal num-
bers, quanti   ers, and adjectives. examples of cardinal numbers:

two friends

one stop

ordinal numbers include    rst, second, third, and so on, but also words like next,

last, past, other, and another:

the    rst one
the last    ight

the next day
the other american    ight

the second leg

some quanti   ers (many, (a) few, several) occur only with plural count nouns:

many fares

adjectives occur after quanti   ers but before nouns.
a non-stop    ight
the earliest lunch    ight

a    rst-class fare
the longest layover
adjectives can also be grouped into a phrase called an adjective phrase or ap.
aps can have an adverb before the adjective (see chapter 8 for de   nitions of adjec-
tives and adverbs):

adjective
phrase

the least expensive fare

after the head noun
a head noun can be followed by postmodi   ers. three kinds of nominal postmodi-
   ers are common in english:

prepositional phrases
non-   nite clauses
relative clauses
common in the atis corpus since they are used to mark the origin and destina-

all    ights from cleveland
any    ights arriving after eleven a.m.
a    ight that serves breakfast

tion of    ights.

here are some examples of prepositional phrase postmodi   ers, with brackets
inserted to show the boundaries of each pp; note that two or more pps can be strung
together within a single np:

all    ights [from cleveland] [to newark]
arrival [in san jose] [before seven p.m.]
a reservation [on    ight six oh six] [from tampa] [to montreal]

here   s a new nominal rule to account for postnominal pps:

nominal     nominal pp

non-   nite

gerundive

the three most common kinds of non-   nite postmodi   ers are the gerundive (-

ing), -ed, and in   nitive forms.

gerundive postmodi   ers are so called because they consist of a verb phrase that

begins with the gerundive (-ing) form of the verb. here are some examples:

any of those [leaving on thursday]
any    ights [arriving after eleven a.m.]
   ights [arriving within thirty minutes of each other]

10.3

    some grammar rules for english

11

we can de   ne the nominals with gerundive modi   ers as follows, making use of

a new non-terminal gerundvp:

nominal     nominal gerundvp

we can make rules for gerundvp constituents by duplicating all of our vp pro-

ductions, substituting gerundv for v.

gerundvp     gerundv np

| gerundv pp | gerundv | gerundv np pp

gerundv can then be de   ned as

gerundv     being | arriving | leaving | . . .

the phrases in italics below are examples of the two other common kinds of

non-   nite clauses, in   nitives and -ed forms:

the last    ight to arrive in boston
i need to have dinner served
which is the aircraft used by this    ight?

relative
pronoun

a postnominal relative clause (more correctly a restrictive relative clause), is
a clause that often begins with a relative pronoun (that and who are the most com-
mon). the relative pronoun functions as the subject of the embedded verb in the
following examples:

a    ight that serves breakfast
   ights that leave in the morning
the one that leaves at ten thirty    ve

we might add rules like the following to deal with these:
nominal     nominal relclause
relclause     (who | that) vp

the relative pronoun may also function as the object of the embedded verb, as
in the following example; we leave for the reader the exercise of writing grammar
rules for more complex relative clauses of this kind.

the earliest american airlines    ight that i can get

various postnominal modi   ers can be combined, as the following examples

show:

a    ight [from phoenix to detroit] [leaving monday evening]
evening    ights [from nashville to houston] [that serve dinner]
a friend [living in denver] [that would like to visit me here in washington dc]

predeterminers

before the noun phrase
word classes that modify and appear before nps are called predeterminers. many
of these have to do with number or amount; a common predeterminer is all:

all the    ights
the example noun phrase given in fig. 10.5 illustrates some of the complexity

all non-stop    ights

all    ights

that arises when these rules are combined.

12 chapter 10

    formal grammars of english

np

predet

np

all

det

the

nom

nom

gerundivevp

nom

pp

leaving before 10

nom

pp

to tampa

nom

noun

from denver

noun

   ights

morning

figure 10.5 a parse tree for    all the morning    ights from denver to tampa leaving before 10   .

10.3.4 the verb phrase
in the
the verb phrase consists of the verb and a number of other constituents.
simple rules we have built so far, these other constituents include nps and pps and
combinations of the two:

disappear

vp     verb
vp     verb np prefer a morning    ight
vp     verb np pp leave boston in the morning
vp     verb pp leaving on thursday

verb phrases can be signi   cantly more complicated than this. many other kinds
of constituents, such as an entire embedded sentence, can follow the verb. these are
called sentential complements:

sentential
complements

you [vp [v said [s you had a two hundred sixty six dollar fare]]
[vp [v tell] [np me] [s how to get from the airport in philadelphia to down-
town]]
i [vp [v think [s i would like to take the nine thirty    ight]]

here   s a rule for these:

vp     verb s

similarly, another potential constituent of the vp is another vp. this is often the

case for verbs like want, would like, try, intend, need:
i want [vp to    y from milwaukee to orlando]
hi, i want [vp to arrange three    ights]

10.3

    some grammar rules for english

13

frame
/0
np
np np
ppfrom ppto
np ppwith
vpto
vpbrst
s

verb
eat, sleep
prefer,    nd, leave
show, give
   y, travel
help, load
prefer, want, need
can, would, might
mean

example
i ate
find [np the    ight from pittsburgh to boston]
show [np me] [np airlines with    ights from pittsburgh]
i would like to    y [pp from boston] [pp to philadelphia]
can you help [np me] [pp with a    ight]
i would prefer [vpto to go by united airlines]
i can [vpbrst go from boston]
does this mean [s aa has a hub in boston]

figure 10.6 subcategorization frames for a set of example verbs.

transitive
intransitive

subcategorize

subcategorizes
for

complements

subcategorization

frame

while a verb phrase can have many possible kinds of constituents, not every
verb is compatible with every verb phrase. for example, the verb want can be used
either with an np complement (i want a    ight . . . ) or with an in   nitive vp comple-
ment (i want to    y to . . . ). by contrast, a verb like    nd cannot take this sort of vp
complement (* i found to    y to dallas).

this idea that verbs are compatible with different kinds of complements is a very
old one; traditional grammar distinguishes between transitive verbs like    nd, which
take a direct object np (i found a    ight), and intransitive verbs like disappear,
which do not (*i disappeared a    ight).

where traditional grammars subcategorize verbs into these two categories (tran-
sitive and intransitive), modern grammars distinguish as many as 100 subcategories.
we say that a verb like    nd subcategorizes for an np, and a verb like want sub-
categorizes for either an np or a non-   nite vp. we also call these constituents the
complements of the verb (hence our use of the term sentential complement above).
so we say that want can take a vp complement. these possible sets of complements
are called the subcategorization frame for the verb. another way of talking about
the relation between the verb and these other constituents is to think of the verb as
a logical predicate and the constituents as logical arguments of the predicate. so we
can think of such predicate-argument relations as find(i, a flight) or want(i, to
fly). we talk more about this view of verbs and arguments in chapter 14 when we
talk about predicate calculus representations of verb semantics. subcategorization
frames for a set of example verbs are given in fig. 10.6.

we can capture the association between verbs and their complements by making
separate subtypes of the class verb (e.g., verb-with-np-complement, verb-with-inf-
vp-complement, verb-with-s-complement, and so on):

verb-with-np-complement        nd | leave | repeat | . . .
verb-with-s-complement     think | believe | say | . . .

verb-with-inf-vp-complement     want | try | need | . . .

each vp rule could then be modi   ed to require the appropriate verb subtype:

vp     verb-with-no-complement
vp     verb-with-np-comp np prefer a morning    ight
vp     verb-with-s-comp s said there were two    ights

disappear

a problem with this approach is the signi   cant increase in the number of rules

and the associated loss of generality.

14 chapter 10

    formal grammars of english

conjunctions
coordinate

10.3.5 coordination
the major phrase types discussed here can be conjoined with conjunctions like and,
or, and but to form larger constructions of the same type. for example, a coordinate
noun phrase can consist of two other noun phrases separated by a conjunction:

please repeat [np [np the    ights] and [np the costs]]
i need to know [np [np the aircraft] and [np the    ight number]]

here   s a rule that allows these structures:

np     np and np

note that the ability to form coordinate phrases through conjunctions is often
used as a test for constituency. consider the following examples, which differ from
the ones given above in that they lack the second determiner.

please repeat the [nom [nom    ights] and [nom costs]]
i need to know the [nom [nom aircraft] and [nom    ight number]]

the fact that these phrases can be conjoined is evidence for the presence of the
underlying nominal constituent we have been making use of. here   s a new rule for
this:

nominal     nominal and nominal

the following examples illustrate conjunctions involving vps and ss.

what    ights do you have [vp [vp leaving denver] and [vp arriving in
san francisco]]
[s [s i   m interested in a    ight from dallas to washington] and [s i   m
also interested in going to baltimore]]

the rules for vp and s conjunctions mirror the np one given above.

vp     vp and vp
s     s and s

metarules

since all the major phrase types can be conjoined in this fashion, it is also pos-
sible to represent this conjunction fact more generally; a number of grammar for-
malisms such as gpsg ((gazdar et al., 1985)) do this using metarules such as the
following:

x     x and x

this metarule simply states that any non-terminal can be conjoined with the same
non-terminal to yield a constituent of the same type. of course, the variable x
must be designated as a variable that stands for any non-terminal rather than a non-
terminal itself.

10.4 treebanks

suf   ciently robust grammars consisting of context-free grammar rules can be used
to assign a parse tree to any sentence. this means that it is possible to build a
corpus where every sentence in the collection is paired with a corresponding parse

treebank

id32

10.4

    treebanks

15

tree. such a syntactically annotated corpus is called a treebank. treebanks play
an important role in parsing, as we discuss in chapter 11, as well as in linguistic
investigations of syntactic phenomena.

a wide variety of treebanks have been created, generally through the use of
parsers (of the sort described in the next few chapters) to automatically parse each
sentence, followed by the use of humans (linguists) to hand-correct the parses. the
id32 project (whose pos tagset we introduced in chapter 8) has pro-
duced treebanks from the brown, switchboard, atis, and wall street journal cor-
pora of english, as well as treebanks in arabic and chinese. a number of treebanks
use the dependency representation we will introduce in chapter 13, including many
that are part of the universal dependencies project (nivre et al., 2016).

10.4.1 example: the id32 project
figure 10.7 shows sentences from the brown and atis portions of the penn tree-
bank.1 note the formatting differences for the part-of-speech tags; such small dif-
ferences are common and must be dealt with in processing treebanks. the penn
treebank part-of-speech tagset was de   ned in chapter 8. the use of lisp-style
parenthesized notation for trees is extremely common and resembles the bracketed
notation we saw earlier in (10.1). for those who are not familiar with it we show a
standard node-and-line tree representation in fig. 10.8.

((s

(np-sbj (dt that)
(jj cold) (, ,)
(jj empty) (nn sky) )

(vp (vbd was)

(adjp-prd (jj full)

(pp (in of)

(np (nn fire)

(cc and)
(nn light) ))))

(. .) ))

(a)

((s

(np-sbj the/dt flight/nn )
(vp should/md

(vp arrive/vb

(pp-tmp at/in

(np eleven/cd a.m/rb ))
(np-tmp tomorrow/nn )))))

(b)

traces
syntactic
movement

figure 10.7 parsed sentences from the ldc treebank3 version of the brown (a) and atis
(b) corpora.

figure 10.9 shows a tree from the wall street journal. this tree shows an-
the use of traces (-none- nodes) to mark
other feature of the id32s:
long-distance dependencies or syntactic movement. for example, quotations often
follow a quotative verb like say. but in this example, the quotation    we would have
to wait until we have collected on those assets    precedes the words he said. an
empty s containing only the node -none- marks the position after said where the
quotation sentence often occurs. this empty node is marked (in treebanks ii and
iii) with the index 2, as is the quotation s at the beginning of the sentence. such
co-indexing may make it easier for some parsers to recover the fact that this fronted
or topicalized quotation is the complement of the verb said. a similar -none- node

1 the id32 project released treebanks in multiple languages and in various stages; for ex-
ample, there were treebank i (marcus et al., 1993), treebank ii (marcus et al., 1994), and treebank iii
releases of english treebanks. we use treebank iii for our examples.

16 chapter 10

    formal grammars of english

np-sbj

s

vp

dt

jj

that

cold

,

,

jj

nn

vbd

adjp-prd

empty

sky

was

jj

pp

.

.

full

in

np

of

nn

cc

nn

   re

and

light

figure 10.8 the tree corresponding to the brown corpus sentence in the previous    gure.

marks the fact that there is no syntactic subject right before the verb to wait; instead,
the subject is the earlier np we. again, they are both co-indexed with the index 1.

( (s (             )
(s-tpc-2

(np-sbj-1 (prp we) )
(vp (md would)

(vp (vb have)

(s

(np-sbj (-none- *-1) )
(vp (to to)

(vp (vb wait)

(sbar-tmp (in until)

(s

(np-sbj (prp we) )
(vp (vbp have)

(vp (vbn collected)

(pp-clr (in on)

(np (dt those)(nns assets)))))))))))))

(, ,) (             )
(np-sbj (prp he) )
(vp (vbd said)

(s (-none- *t*-2) ))

(. .) ))

figure 10.9 a sentence from the wall street journal portion of the ldc id32.
note the use of the empty -none- nodes.

the id32 ii and treebank iii releases added further information to
make it easier to recover the relationships between predicates and arguments. cer-

10.4

    treebanks

17

lexicon
prp     we | he
dt     the | that | those
jj     cold | empty | full
nn     sky |    re | light |    ight | tomorrow
nns     assets
cc     and
in     of | at | until | on
cd     eleven
rb     a.m.
vb     arrive | have | wait
vbd     was | said
vbp     have
vbn     collected
md     should | would
to     to

grammar
s     np vp .
s     np vp
s         s     , np vp .
s     -none-
np     dt nn
np     dt nns
np     nn cc nn
np     cd rb
np     dt jj , jj nn
np     prp
np     -none-
vp     md vp
vp     vbd adjp
vp     vbd s
vp     vbn pp
vp     vb s
vp     vb sbar
vp     vbp vp
vp     vbn pp
vp     to vp
sbar     in s
adjp     jj pp
pp     in np

figure 10.10 a sample of the id18 grammar rules and lexical entries that would be ex-
tracted from the three treebank sentences in fig. 10.7 and fig. 10.9.

tain phrases were marked with tags indicating the grammatical function of the phrase
(as surface subject, logical topic, cleft, non-vp predicates) its presence in particular
text categories (headlines, titles), and its semantic function (temporal phrases, lo-
cations) (marcus et al. 1994, bies et al. 1995). figure 10.9 shows examples of the
-sbj (surface subject) and -tmp (temporal phrase) tags. figure 10.8 shows in addi-
tion the -prd tag, which is used for predicates that are not vps (the one in fig. 10.8
is an adjp). we   ll return to the topic of grammatical function when we consider
dependency grammars and parsing in chapter 13.

10.4.2 treebanks as grammars
the sentences in a treebank implicitly constitute a grammar of the language repre-
sented by the corpus being annotated. for example, from the three parsed sentences
in fig. 10.7 and fig. 10.9, we can extract each of the id18 rules in them. for sim-
plicity, let   s strip off the rule suf   xes (-sbj and so on). the resulting grammar is
shown in fig. 10.10.

the grammar used to parse the id32 is relatively    at, resulting in very
many and very long rules. for example, among the approximately 4,500 different
rules for expanding vps are separate rules for pp sequences of any length and every
possible arrangement of verb arguments:

vp     vbd pp
vp     vbd pp pp
vp     vbd pp pp pp
vp     vbd pp pp pp pp
vp     vb advp pp
vp     vb pp advp
vp     advp vb pp

18 chapter 10

    formal grammars of english

as well as even longer rules, such as

vp     vbp pp pp pp pp pp advp pp

which comes from the vp marked in italics:

this mostly happens because we go from football in the fall to lifting in the
winter to football again in the spring.

some of the many thousands of np rules include

np     dt jj nn
np     dt jj nns
np     dt jj nn nn
np     dt jj jj nn
np     dt jj cd nns
np     rb dt jj nn nn
np     rb dt jj jj nns
np     dt jj jj nnp nns
np     dt nnp nnp nnp nnp jj nn
np     dt jj nnp cc jj jj nn nns
np     rb dt jjs nn nn sbar
np     dt vbg jj nnp nnp cc nnp
np     dt jj nns , nns cc nn nns nn
np     dt jj jj vbg nn nnp nnp fw nnp
np     np jj , jj        sbar        nns

the last two of those rules, for example, come from the following two noun phrases:

[dt the] [jj state-owned] [jj industrial] [vbg holding] [nn company] [nnp instituto]
[nnp nacional] [fw de] [nnp industria]
[np shearson   s] [jj easy-to-   lm], [jj black-and-white]    [sbar where we stand]   
[nns commercials]

viewed as a large grammar in this way, the id32 iii wall street journal
corpus, which contains about 1 million words, also has about 1 million non-lexical
rule tokens, consisting of about 17,500 distinct rule types.

various facts about the treebank grammars, such as their large numbers of    at
rules, pose problems for probabilistic parsing algorithms. for this reason, it is com-
mon to make various modi   cations to a grammar extracted from a treebank. we
discuss these further in chapter 12.

10.4.3 heads and head finding
we suggested informally earlier that syntactic constituents could be associated with
a lexical head; n is the head of an np, v is the head of a vp. this idea of a head for
each constituent dates back to bloom   eld (1914). it is central to constituent-based
grammar formalisms such as head-driven phrase structure grammar (pollard and
sag, 1994), as well as the dependency-based approaches to grammar we   ll discuss
in chapter 13. heads and head-dependent relations have also come to play a central
role in computational linguistics with their use in probabilistic parsing (chapter 12)
and in id33 (chapter 13).

in one simple model of lexical heads, each context-free rule is associated with
a head (charniak 1997, collins 1999). the head is the word in the phrase that is
grammatically the most important. heads are passed up the parse tree; thus, each
non-terminal in a parse tree is annotated with a single word, which is its lexical head.

10.4

    treebanks

19

s(dumped)

np(workers)

vp(dumped)

nns(workers)

vbd(dumped)

np(sacks)

pp(into)

workers

dumped

nns(sacks)

p

np(bin)

sacks

into

dt(a)

nn(bin)

a

bin

figure 10.11 a lexicalized tree from collins (1999).

figure 10.11 shows an example of such a tree from collins (1999), in which each
non-terminal is annotated with its head.

for the generation of such a tree, each id18 rule must be augmented to identify
one right-side constituent to be the head daughter. the headword for a node is
then set to the headword of its head daughter. choosing these head daughters is
simple for textbook examples (nn is the head of np) but is complicated and indeed
controversial for most phrases. (should the complementizer to or the verb be the
head of an in   nite verb-phrase?) modern linguistic theories of syntax generally
include a component that de   nes heads (see, e.g., (pollard and sag, 1994)).

an alternative approach to    nding a head is used in most practical computational
systems. instead of specifying head rules in the grammar itself, heads are identi   ed
dynamically in the context of trees for speci   c sentences.
in other words, once
a sentence is parsed, the resulting tree is walked to decorate each node with the
appropriate head. most current systems rely on a simple set of hand-written rules,
such as a practical one for id32 grammars given in collins (1999) but
developed originally by magerman (1995). for example, the rule for    nding the
head of an np is as follows (collins, 1999, p. 238):

or jjr.

    if the last word is tagged pos, return last-word.
    else search from right to left for the    rst child which is an nn, nnp, nnps, nx, pos,
    else search from left to right for the    rst child which is an np.
    else search from right to left for the    rst child which is a $, adjp, or prn.
    else search from right to left for the    rst child which is a cd.
    else search from right to left for the    rst child which is a jj, jjs, rb or qp.
    else return the last word

selected other rules from this set are shown in fig. 10.12. for example, for vp
rules of the form vp     y1        yn, the algorithm would start from the left of y1       
yn looking for the    rst yi of type to; if no tos are found, it would search for the
   rst yi of type vbd; if no vbds are found, it would search for a vbn, and so on.
see collins (1999) for more details.

20 chapter 10

    formal grammars of english

parent direction
adjp

left

priority list
nns qp nn $ advp jj vbn vbg adjp jjr np jjs dt fw rbr rbs
sbar rb
rb rbr rbs fw advp to cd jjr jj in np jjs nn

right
left
right
left
left
left
left

advp
prn
prt
qp
s
sbar
vp
figure 10.12 selected head rules from collins (1999). the set of head rules is often called a head percola-
tion table.
10.5 grammar equivalence and normal form

rp
$ in nns nn jj rb dt cd ncd qp jjr jjs
to in vp s sbar adjp ucp np
whnp whpp whadvp whadjp in dt s sq sinv sbar frag
to vbd vbn md vbz vb vbg vbp vp adjp nn nns np

normal form

chomsky
normal form

binary
branching

a formal language is de   ned as a (possibly in   nite) set of strings of words. this
suggests that we could ask if two grammars are equivalent by asking if they gener-
ate the same set of strings. in fact, it is possible to have two distinct context-free
grammars generate the same language.

we usually distinguish two kinds of grammar equivalence: weak equivalence
and strong equivalence. two grammars are strongly equivalent if they generate the
same set of strings and if they assign the same phrase structure to each sentence
(allowing merely for renaming of the non-terminal symbols). two grammars are
weakly equivalent if they generate the same set of strings but do not assign the same
phrase structure to each sentence.

it is sometimes useful to have a normal form for grammars, in which each of
the productions takes a particular form. for example, a context-free grammar is in
chomsky normal form (cnf) (chomsky, 1963) if it is   -free and if in addition
each production is either of the form a     b c or a     a. that is, the right-hand side
of each rule either has two non-terminal symbols or one terminal symbol. chomsky
normal form grammars are binary branching, that is they have binary trees (down
to the prelexical nodes). we make use of this binary branching property in the cky
parsing algorithm in chapter 11.

any context-free grammar can be converted into a weakly equivalent chomsky

normal form grammar. for example, a rule of the form

a     b c d

can be converted into the following two cnf rules (exercise 10.8 asks the reader to
formulate the complete algorithm):

a     b x
x     c d

sometimes using binary branching can actually produce smaller grammars. for

example, the sentences that might be characterized as

vp -> vbd np pp*

are represented in the id32 by this series of rules:

vp     vbd np pp
vp     vbd np pp pp

10.6

    lexicalized grammars

21

vp     vbd np pp pp pp
vp     vbd np pp pp pp pp
...

but could also be generated by the following two-rule grammar:

vp     vbd np pp
vp     vp pp

chomsky-
adjunction

the generation of a symbol a with a potentially in   nite sequence of symbols b with
a rule of the form a     a b is known as chomsky-adjunction.

10.6 lexicalized grammars

the approach to grammar presented thus far emphasizes phrase-structure rules while
minimizing the role of the lexicon. however, as we saw in the discussions of
agreement, subcategorization, and long distance dependencies, this approach leads
to solutions that are cumbersome at best, yielding grammars that are redundant,
hard to manage, and brittle. to overcome these issues, numerous alternative ap-
proaches have been developed that all share the common theme of making bet-
ter use of the lexicon. among the more computationally relevant approaches are
lexical-functional grammar (lfg) (bresnan, 1982), head-driven phrase structure
grammar (hpsg) (pollard and sag, 1994), tree-adjoining grammar (tag) (joshi,
1985), and id35 (id35). these approaches differ with
respect to how lexicalized they are     the degree to which they rely on the lexicon
as opposed to phrase structure rules to capture facts about the language.

the following section provides an introduction to id35, a heavily lexicalized
approach motivated by both syntactic and semantic considerations, which we will
return to in chapter 14. chapter 13 discusses dependency grammars, an approach
that eliminates phrase-structure rules entirely.

categorial
grammar

10.6.1 id35
in this section, we provide an overview of categorial grammar (ajdukiewicz 1935,
bar-hillel 1953), an early lexicalized grammar model, as well as an important mod-
ern extension, id35, or id35 (steedman 1996,steed-

combinatory
categorial
grammar man 1989,steedman 2000).

the categorial approach consists of three major elements: a set of categories,
a lexicon that associates words with categories, and a set of rules that govern how
categories combine in context.

categories
categories are either atomic elements or single-argument functions that return a cat-
egory as a value when provided with a desired category as argument. more formally,
we can de   ne c , a set of categories for a grammar as follows:

    a     c , where a is a given set of atomic elements
    (x/y), (x\y)     c , if x, y     c
the slash notation shown here is used to de   ne the functions in the grammar.
it speci   es the type of the expected argument, the direction it is expected be found,
and the type of the result. thus, (x/y) is a function that seeks a constituent of type

22 chapter 10

    formal grammars of english

y to its right and returns a value of x; (x\y) is the same except it seeks its argument
to the left.

the set of atomic categories is typically very small and includes familiar el-
ements such as sentences and noun phrases. functional categories include verb
phrases and complex noun phrases among others.

the lexicon
the lexicon in a categorial approach consists of assignments of categories to words.
these assignments can either be to atomic or functional categories, and due to lexical
ambiguity words can be assigned to multiple categories. consider the following
sample lexical entries.

   ight :
miami :
cancel : (s\np)/np

n
np

nouns and proper nouns like    ight and miami are assigned to atomic categories,
re   ecting their typical role as arguments to functions. on the other hand, a transitive
verb like cancel is assigned the category (s\np)/np: a function that seeks an np on
its right and returns as its value a function with the type (s\np). this function can,
in turn, combine with an np on the left, yielding an s as the result. this captures the
kind of subcategorization information discussed in section 10.3.4, however here the
information has a rich, computationally useful, internal structure.
ditransitive verbs like give, which expect two arguments after the verb, would
have the category ((s\np)/np)/np: a function that combines with an np on its
right to yield yet another function corresponding to the transitive verb (s\np)/np
category such as the one given above for cancel.

rules
the rules of a categorial grammar specify how functions and their arguments com-
bine. the following two rule templates constitute the basis for all categorial gram-
mars.

x/y y     x
y x\y     x

(10.4)
(10.5)

the    rst rule applies a function to its argument on the right, while the second
looks to the left for its argument. we   ll refer to the    rst as forward function appli-
cation, and the second as backward function application. the result of applying
either of these rules is the category speci   ed as the value of the function being ap-
plied.

given these rules and a simple lexicon, let   s consider an analysis of the sentence
united serves miami. assume that serves is a transitive verb with the category
(s\np)/np and that united and miami are both simple nps. using both forward
and backward function application, the derivation would proceed as follows:

united

np

serves

miami
(s\np)/np np

s\np

s

>

<

10.6

    lexicalized grammars

23

categorial grammar derivations are illustrated growing down from the words,
rule applications are illustrated with a horizontal line that spans the elements in-
volved, with the type of the operation indicated at the right end of the line. in this
example, there are two function applications: one forward function application indi-
cated by the > that applies the verb serves to the np on its right, and one backward
function application indicated by the < that applies the result of the    rst to the np
united on its left.

with the addition of another rule, the categorial approach provides a straight-
forward way to implement the coordination metarule described earlier on page 14.
recall that english permits the coordination of two constituents of the same type,
resulting in a new constituent of the same type. the following rule provides the
mechanism to handle such examples.

x conj x     x

(10.6)

this rule states that when two constituents of the same category are separated by a
constituent of type conj they can be combined into a single larger constituent of
the same type. the following derivation illustrates the use of this rule.

flew

we
np (s\np)/pp pp/np

to

geneva

np

drove

and
conj (s\np)/pp pp/np

to

chamonix

np

pp

>

>

s\np

s\np
s

pp

s\np

>

>

<  >

<

here the two s\np constituents are combined via the conjunction operator <  >
to form a larger constituent of the same type, which can then be combined with the
subject np via backward function application.

these examples illustrate the lexical nature of the categorial grammar approach.
the grammatical facts about a language are largely encoded in the lexicon, while the
rules of the grammar are boiled down to a set of three rules. unfortunately, the basic
categorial approach does not give us any more expressive power than we had with
traditional id18 rules; it just moves information from the grammar to the lexicon. to
move beyond these limitations id35 includes operations that operate over functions.

the    rst pair of operators permit us to compose adjacent functions.

x/y y /z     x/z
y\z x\y     x\z

(10.7)
(10.8)

forward
composition

backward
composition

type raising

the    rst rule, called forward composition, can be applied to adjacent con-
stituents where the    rst is a function seeking an argument of type y to its right, and
the second is a function that providesy as a result. this rule allows us to compose
these two functions into a single one with the type of the    rst constituent and the
argument of the second. although the notation is a little awkward, the second rule,
backward composition is the same, except that we   re looking to the left instead of
to the right for the relevant arguments. both kinds of composition are signalled by a
b in id35 diagrams, accompanied by a < or > to indicate the direction.

the next operator is type raising. type raising elevates simple categories to the
status of functions. more speci   cally, type raising takes a category and converts
it to function that seeks as an argument a function that takes the original category

24 chapter 10

    formal grammars of english

as its argument. the following schema show two versions of type raising: one for
arguments to the right, and one for the left.

x     t /(t\x)
x     t\(t /x)

(10.9)
(10.10)

the category t in these rules can correspond to any of the atomic or functional
categories already present in the grammar.

a particularly useful example of type raising transforms a simple np argument
in subject position to a function that can compose with a following vp. to see how
this works, let   s revisit our earlier example of united serves miami. instead of clas-
sifying united as an np which can serve as an argument to the function attached to
serve, we can use type raising to reinvent it as a function in its own right as follows.

np     s/(s\np)

combining this type-raised constituent with the forward composition rule (10.7)
permits the following alternative to our previous derivation.

united

np

>t
s/(s\np)

serves

miami
(s\np)/np np

s/np

s

>b

>

by type raising united to s/(s\np), we can compose it with the transitive verb
serves to yield the (s/np) function needed to complete the derivation.

there are several interesting things to note about this derivation. first, is it
provides a left-to-right, word-by-word derivation that more closely mirrors the way
humans process language. this makes id35 a particularly apt framework for psy-
cholinguistic studies. second, this derivation involves the use of an intermediate
unit of analysis, united serves, that does not correspond to a traditional constituent
in english. this ability to make use of such non-constituent elements provides id35
with the ability to handle the coordination of phrases that are not proper constituents,
as in the following example.
(10.11) we    ew icelandair to geneva and swissair to london.

here, the segments that are being coordinated are icelandair to geneva and
swissair to london, phrases that would not normally be considered constituents, as
can be seen in the following standard derivation for the verb phrase    ew icelandair
to geneva.

   ew

icelandair

to

geneva

(vp/pp)/np

np

pp/np

np

vp/pp

>

vp

pp

>

>

in this derivation, there is no single constituent that corresponds to icelandair
to geneva, and hence no opportunity to make use of the <  > operator. note that
complex id35 categories can can get a little cumbersome, so we   ll use vp as a
shorthand for (s\np) in this and the following derivations.

the following alternative derivation provides the required element through the
use of both backward type raising (10.10) and backward function composition (10.8).

10.6

    lexicalized grammars

25

   ew

(v p/pp)/np

icelandair

np

<t
(v p/pp)\((v p/pp)/np)

to

geneva

pp/np

np

pp

<t
v p\(v p/pp)
<b

>

<

v p\((v p/pp)/np)
v p

applying the same analysis to swissair to london satis   es the requirements
for the <  > operator, yielding the following derivation for our original example
(10.11).

   ew

(v p/pp)/np

icelandair

np

<t
(v p/pp)\((v p/pp)/np)

to

geneva

pp/np

np

and
conj

>

pp

<t
v p\(v p/pp)

swissair

np

<t
(v p/pp)\((v p/pp)/np)

v p\((v p/pp)/np)

v p\((v p/pp)/np)

to

london

pp/np

np

>

pp

<t
v p\(v p/pp)

<

<  >

<

<

v p\((v p/pp)/np)
v p

finally, let   s examine how these advanced operators can be used to handle long-
distance dependencies (also referred to as syntactic movement or extraction). as
mentioned in section 10.3.1, long-distance dependencies arise from many english
constructions including wh-questions, relative clauses, and topicalization. what
these constructions have in common is a constituent that appears somewhere dis-
tant from its usual, or expected, location. consider the following relative clause as
an example.

the    ight that united diverted

here, divert is a transitive verb that expects two np arguments, a subject np to its
left and a direct object np to its right; its category is therefore (s\np)/np. however,
in this example the direct object the    ight has been    moved    to the beginning of the
clause, while the subject united remains in its normal position. what is needed is a
way to incorporate the subject argument, while dealing with the fact that the    ight is
not in its expected location.

the following derivation accomplishes this, again through the combined use of

type raising and function composition.
that

   ight

the
np/n n (np\np)/(s/np)

>

np

diverted
(s\np)/np

united

np

>t
s/(s\np)

s/np

np\np

np

>b

>

<

as we saw with our earlier examples, the    rst step of this derivation is type raising
united to the category s/(s\np) allowing it to combine with diverted via forward
composition. the result of this composition is s/np which preserves the fact that we
are still looking for an np to    ll the missing direct object. the second critical piece
is the lexical category assigned to the word that: (np\np)/(s/np). this function
seeks a verb phrase missing an argument to its right, and transforms it into an np
seeking a missing element to its left, precisely where we    nd the    ight.

26 chapter 10

    formal grammars of english

id35bank

as with phrase-structure approaches, treebanks play an important role in id35-
based approaches to parsing. id35bank (hockenmaier and steedman, 2007) is the
largest and most widely used id35 treebank. it was created by automatically trans-
lating phrase-structure trees from the id32 via a rule-based approach. the
method produced successful translations of over 99% of the trees in the penn tree-
bank resulting in 48,934 sentences paired with id35 derivations. it also provides
a lexicon of 44,000 words with over 1200 categories. chapter 12 will discuss how
these resources can be used to train id35 parsers.

10.7 summary

this chapter has introduced a number of fundamental concepts in syntax through
the use of context-free grammars.

    in many languages, groups of consecutive words act as a group or a con-
stituent, which can be modeled by context-free grammars (which are also
known as phrase-structure grammars).

    a context-free grammar consists of a set of rules or productions, expressed
over a set of non-terminal symbols and a set of terminal symbols. formally,
a particular context-free language is the set of strings that can be derived
from a particular context-free grammar.

    a generative grammar is a traditional name in linguistics for a formal lan-

guage that is used to model the grammar of a natural language.

    there are many sentence-level grammatical constructions in english; declar-
ative, imperative, yes-no question, and wh-question are four common types;
these can be modeled with context-free rules.

    an english noun phrase can have determiners, numbers, quanti   ers, and
adjective phrases preceding the head noun, which can be followed by a num-
ber of postmodi   ers; gerundive vps, in   nitives vps, and past participial
vps are common possibilities.

    subjects in english agree with the main verb in person and number.
    verbs can be subcategorized by the types of complements they expect. sim-
ple subcategories are transitive and intransitive; most grammars include
many more categories than these.

    treebanks of parsed sentences exist for many genres of english and for many

languages. treebanks can be searched with tree-search tools.

    any context-free grammar can be converted to chomsky normal form, in
which the right-hand side of each rule has either two non-terminals or a single
terminal.

    lexicalized grammars place more emphasis on the structure of the lexicon,

lessening the burden on pure phrase-structure rules.

    combinatorial categorial grammar (id35) is an important computationally

relevant lexicalized approach.

bibliographical and historical notes

27

bibliographical and historical notes

[the origin of the idea of phrasal constituency, cited in percival (1976)]:
den sprachlichen ausdruck f  ur die willk  urliche
gliederung einer gesammtvorstellung in ihre
in logische beziehung zueinander gesetzten bestandteile   
[the linguistic expression for the arbitrary division of a total idea
into its constituent parts placed in logical relations to one another]
w. wundt

according to percival (1976), the idea of breaking up a sentence into a hierar-
chy of constituents appeared in the v  olkerpsychologie of the groundbreaking psy-
chologist wilhelm wundt (wundt, 1900). wundt   s idea of constituency was taken
up into linguistics by leonard bloom   eld in his early book an introduction to the
study of language (bloom   eld, 1914). by the time of his later book, language
(bloom   eld, 1933), what was then called    immediate-constituent analysis    was a
well-established method of syntactic study in the united states. by contrast, tra-
ditional european grammar, dating from the classical period, de   ned relations be-
tween words rather than constituents, and european syntacticians retained this em-
phasis on such dependency grammars, the subject of chapter 13.

american structuralism saw a number of speci   c de   nitions of the immediate
constituent, couched in terms of their search for a    discovery procedure   : a method-
ological algorithm for describing the syntax of a language. in general, these attempt
to capture the intuition that    the primary criterion of the immediate constituent is the
degree in which combinations behave as simple units    (bazell, 1966, p. 284). the
most well known of the speci   c de   nitions is harris    idea of distributional similarity
to individual units, with the substitutability test. essentially, the method proceeded
by breaking up a construction into constituents by attempting to substitute simple
structures for possible constituents   if a substitution of a simple form, say, man,
was substitutable in a construction for a more complex set (like intense young man),
then the form intense young man was probably a constituent. harris   s test was the
beginning of the intuition that a constituent is a kind of equivalence class.

the    rst formalization of this idea of hierarchical constituency was the phrase-
structure grammar de   ned in chomsky (1956) and further expanded upon (and
argued against) in chomsky (1957) and chomsky (1975). from this time on, most
generative linguistic theories were based at least in part on context-free grammars or
generalizations of them (such as head-driven phrase structure grammar (pollard
and sag, 1994), lexical-functional grammar (bresnan, 1982), government and
binding (chomsky, 1981), and construction grammar (kay and fillmore, 1999),
inter alia); many of these theories used schematic context-free templates known as
x-bar schemata, which also relied on the notion of syntactic head.

shortly after chomsky   s initial work, the context-free grammar was reinvented
by backus (1959) and independently by naur et al. (1960) in their descriptions of
the algol programming language; backus (1996) noted that he was in   uenced by
the productions of emil post and that naur   s work was independent of his (backus   )
own. (recall the discussion on page ?? of multiple invention in science.) after this
early work, a great number of computational models of natural language processing
were based on context-free grammars because of the early development of ef   cient
algorithms to parse these grammars (see chapter 11).

x-bar
schemata

28 chapter 10

    formal grammars of english

as we have already noted, grammars based on context-free rules are not ubiqui-
tous. various classes of extensions to id18s are designed speci   cally to handle long-
distance dependencies. we noted earlier that some grammars treat long-distance-
dependent items as being related semantically but not syntactically; the surface syn-
tax does not represent the long-distance link (kay and fillmore 1999, culicover and
jackendoff 2005). but there are alternatives.

one extended formalism is id34 (tag)

(joshi, 1985).
the primary tag data structure is the tree, rather than the rule. trees come in two
kinds: initial trees and auxiliary trees. initial trees might, for example, represent
simple sentential structures, and auxiliary trees add recursion into a tree. trees are
combined by two operations called substitution and adjunction. the adjunction
operation handles long-distance dependencies. see joshi (1985) for more details.
an extension of id34, called lexicalized tree adjoining gram-
mars is discussed in chapter 12. id34 is a member of the family
of mildly context-sensitive languages.

we mentioned on page 15 another way of handling long-distance dependencies,
based on the use of empty categories and co-indexing. the id32 uses
this model, which draws (in various treebank corpora) from the extended standard
theory and minimalism (radford, 1997).

readers interested in the grammar of english should get one of the three large
reference grammars of english: huddleston and pullum (2002), biber et al. (1999),
and quirk et al. (1985). another useful reference is mccawley (1998).

there are many good introductory textbooks on syntax from different perspec-
tives. sag et al. (2003) is an introduction to syntax from a generative perspective,
focusing on the use of phrase-structure rules, uni   cation, and the type hierarchy in
head-driven phrase structure grammar. van valin, jr. and la polla (1997) is an
introduction from a functional perspective, focusing on cross-linguistic data and on
the functional motivation for syntactic structures.

generative

functional

exercises

10.1 draw tree structures for the following atis phrases:

1. dallas
2. from denver
3. after    ve p.m.
4. arriving in washington
5. early    ights
6. all redeye    ights
7. on thursday
8. a one-way fare
9. any delays in denver

10.2 draw tree structures for the following atis sentences:

1. does american airlines have a    ight between    ve a.m. and six a.m.?
2. i would like to    y on american airlines.
3. please repeat that.
4. does american 487 have a    rst-class section?
5. i need to    y between philadelphia and atlanta.
6. what is the fare from atlanta to denver?

exercises

29

7. is there an american airlines    ight from philadelphia to dallas?

10.3 assume a grammar that has many vp rules for different subcategorizations,
as expressed in section 10.3.4, and differently subcategorized verb rules like
verb-with-np-complement. how would the rule for postnominal relative clauses
(10.4) need to be modi   ed if we wanted to deal properly with examples like
the earliest    ight that you have? recall that in such examples the pronoun
that is the object of the verb get. your rules should allow this noun phrase but
should correctly rule out the ungrammatical s *i get.

10.4 does your solution to the previous problem correctly model the np the earliest
   ight that i can get? how about the earliest    ight that i think my mother
wants me to book for her? hint: this phenomenon is called long-distance
dependency.

10.5 write rules expressing the verbal subcategory of english auxiliaries; for ex-

ample, you might have a rule verb-with-bare-stem-vp-complement     can.

10.6 nps like fortune   s of   ce or my uncle   s marks are called possessive or genitive
noun phrases. we can model possessive noun phrases by treating the sub-np
like fortune   s or my uncle   s as a determiner of the following head noun. write
grammar rules for english possessives. you may treat    s as if it were a separate
word (i.e., as if there were always a space before    s).

10.7 page 8 discussed the need for a wh-np constituent. the simplest wh-np is
one of the wh-pronouns (who, whom, whose, which). the wh-words what
and which can be determiners: which four will you have?, what credit do you
have with the duke? write rules for the different types of wh-nps.

10.8 write an algorithm for converting an arbitrary context-free grammar into chom-

sky normal form.

possessive
genitive

30 chapter 10     formal grammars of english

ajdukiewicz, k. (1935). die syntaktische konnexit  at. stu-
dia philosophica, 1, 1   27. english translation    syntactic
connexion    by h. weber in mccall, s. (ed.) 1967. polish
logic, pp. 207   231, oxford university press.

backus, j. w. (1959). the syntax and semantics of the pro-
posed international algebraic language of the zurich acm-
gamm conference. in information processing: proceed-
ings of the international conference on information pro-
cessing, paris, pp. 125   132. unesco.

backus, j. w. (1996). transcript of question and answer ses-
sion. in wexelblat, r. l. (ed.), history of programming
languages, p. 162. academic press.

bar-hillel, y. (1953). a quasi-arithmetical notation for syn-
tactic description. language, 29, 47   58. reprinted in y.
bar-hillel. (1964). language and information: selected
essays on their theory and application, addison-wesley,
61   74.

bazell, c. e. (1952/1966). the correspondence fallacy in
structural linguistics. in hamp, e. p., householder, f. w.,
and austerlitz, r. (eds.), studies by members of the en-
glish department, istanbul university (3), reprinted in
readings in linguistics ii (1966), pp. 271   298. university
of chicago press.

biber, d., johansson, s., leech, g., conrad, s., and fine-
gan, e. (1999). longman grammar of spoken and written
english. pearson esl, harlow.

bies, a., ferguson, m., katz, k., and macintyre, r. (1995).
bracketing guidelines for treebank ii style id32
project..

bloom   eld, l. (1914). an introduction to the study of lan-

guage. henry holt and company.

bloom   eld, l. (1933). language. university of chicago

press.

bresnan, j. (ed.). (1982). the mental representation of

grammatical relations. mit press.

charniak, e. (1997). statistical parsing with a context-free
in aaai-97, pp. 598   603.

grammar and word statistics.
aaai press.

chomsky, n. (1956). three models for the description of
language. ire transactions on id205, 2(3),
113   124.

chomsky, n. (1956/1975). the logical structure of linguis-

tic theory. plenum.

chomsky, n. (1957). syntactic structures. mouton, the

hague.

chomsky, n. (1963). formal properties of grammars.

in
luce, r. d., bush, r., and galanter, e. (eds.), handbook
of mathematical psychology, vol. 2, pp. 323   418. wiley.
chomsky, n. (1981). lectures on government and binding.

foris.

collins, m. (1999). head-driven statistical models for nat-
ural language parsing. ph.d. thesis, university of penn-
sylvania, philadelphia.

culicover, p. w. and jackendoff, r. (2005). simpler syntax.

oxford university press.

gazdar, g., klein, e., pullum, g. k., and sag, i. a. (1985).

generalized phrase structure grammar. blackwell.

harris, z. s. (1946). from morpheme to utterance. lan-

guage, 22(3), 161   183.

hemphill, c. t., godfrey, j., and doddington, g. (1990).
the atis spoken language systems pilot corpus. in pro-
ceedings darpa speech and natural language workshop,
hidden valley, pa, pp. 96   101.

hockenmaier, j. and steedman, m. (2007). id35bank: a cor-
pus of id35 derivations and dependency structures extracted
from the id32. computational linguistics, 33(3),
355   396.

hopcroft, j. e. and ullman, j. d. (1979).

introduction to
automata theory, languages, and computation. addison-
wesley.

huddleston, r. and pullum, g. k. (2002). the cambridge
grammar of the english language. cambridge university
press.

joshi, a. k. (1985). id34s: how much
context-sensitivity is required to provide reasonable struc-
tural descriptions?.
in dowty, d. r., karttunen, l., and
zwicky, a. (eds.), natural language parsing, pp. 206   
250. cambridge university press.

kay, p. and fillmore, c. j. (1999). grammatical construc-
tions and linguistic generalizations: the what   s x doing
y? construction. language, 75(1), 1   33.

magerman, d. m. (1995). statistical decision-tree models

for parsing. in acl-95, pp. 276   283.

marcus, m. p., kim, g., marcinkiewicz, m. a., macintyre,
r., bies, a., ferguson, m., katz, k., and schasberger, b.
(1994). the id32: annotating predicate argu-
in arpa human language technology
ment structure.
workshop, plainsboro, nj, pp. 114   119. morgan kauf-
mann.

marcus, m. p., santorini, b., and marcinkiewicz, m. a.
(1993). building a large annotated corpus of english: the
id32. computational linguistics, 19(2), 313   
330.

mccawley, j. d. (1998). the syntactic phenomena of en-

glish. university of chicago press.

naur, p., backus, j. w., bauer, f. l., green, j., katz, c.,
mccarthy, j., perlis, a. j., rutishauser, h., samelson, k.,
vauquois, b., wegstein, j. h., van wijnagaarden, a., and
woodger, m. (1960). report on the algorithmic language
algol 60. communications of the acm, 3(5), 299   314.
revised in cacm 6:1, 1-17, 1963.

nivre, j., de marneffe, m.-c., ginter, f., goldberg, y., haji  c,
j., manning, c. d., mcdonald, r. t., petrov, s., pyysalo,
s., silveira, n., tsarfaty, r., and zeman, d. (2016). uni-
versal dependencies v1: a multilingual treebank collec-
tion. in lrec-16.

percival, w. k. (1976). on the historical source of immedi-
ate constituent analysis. in mccawley, j. d. (ed.), syntax
and semantics volume 7, notes from the linguistic under-
ground, pp. 229   242. academic press.

pollard, c. and sag, i. a. (1994). head-driven phrase struc-

ture grammar. university of chicago press.

quirk, r., greenbaum, s., leech, g., and svartvik, j.
(1985). a comprehensive grammar of the english lan-
guage. longman.

radford, a. (1988). transformational grammar: a first

course. cambridge university press.

radford, a. (1997). syntactic theory and the structure of
english: a minimalist approach. cambridge university
press.

exercises

31

sag, i. a., wasow, t., and bender, e. m. (eds.). (2003). syn-
tactic theory: a formal introduction. csli publications,
stanford, ca.

steedman, m. (1989). constituency and coordination in a
combinatory grammar. in baltin, m. r. and kroch, a. s.
(eds.), alternative conceptions of phrase structure, pp.
201   231. university of chicago.

steedman, m. (1996). surface structure and interpretation.

mit press. linguistic inquiry monograph, 30.

steedman, m. (2000). the syntactic process. the mit

press.

van valin, jr., r. d. and la polla, r. (1997). syntax: struc-
ture, meaning, and function. cambridge university press.
wundt, w. (1900). v  olkerpsychologie: eine untersuchung
der entwicklungsgesetze von sprache, mythus, und sitte.
w. engelmann, leipzig. band ii: die sprache, zweiter
teil.

