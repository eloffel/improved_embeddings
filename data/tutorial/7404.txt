speech and language processing. daniel jurafsky & james h. martin.
rights reserved.

draft of september 23, 2018.

copyright c(cid:13) 2018.

all

chapter

13 id33

dependency
grammars

typed
dependency

free word order

the focus of the three previous chapters has been on context-free grammars and
their use in automatically generating constituent-based representations. here we
present another family of grammar formalisms called dependency grammars that
are quite important in contemporary speech and language processing systems. in
these formalisms, phrasal constituents and phrase-structure rules do not play a direct
role. instead, the syntactic structure of a sentence is described solely in terms of the
words (or lemmas) in a sentence and an associated set of directed binary grammatical
relations that hold among the words.

the following diagram illustrates a dependency-style analysis using the standard

graphical method favored in the dependency-parsing community.

root

dobj

det

nsubj

nmod

nmod

case

i prefer the morning    ight through denver

(13.1)

relations among the words are illustrated above the sentence with directed, la-
beled arcs from heads to dependents. we call this a typed dependency structure
because the labels are drawn from a    xed inventory of grammatical relations. it also
includes a root node that explicitly marks the root of the tree, the head of the entire
structure.

figure 13.1 shows the same dependency analysis as a tree alongside its corre-
sponding phrase-structure analysis of the kind given in chapter 10. note the ab-
sence of nodes corresponding to phrasal constituents or lexical categories in the
dependency parse; the internal structure of the dependency parse consists solely
of directed relations between lexical items in the sentence. these relationships di-
rectly encode important information that is often buried in the more complex phrase-
structure parses. for example, the arguments to the verb prefer are directly linked to
it in the dependency structure, while their connection to the main verb is more dis-
tant in the phrase-structure tree. similarly, morning and denver, modi   ers of    ight,
are linked to it directly in the dependency structure.

a major advantage of dependency grammars is their ability to deal with lan-
guages that are morphologically rich and have a relatively free word order. for
example, word order in czech can be much more    exible than in english; a gram-
matical object might occur before or after a location adverbial. a phrase-structure
grammar would need a separate rule for each possible place in the parse tree where
such an adverbial phrase could occur. a dependency-based approach would just
have one link type representing this particular adverbial relation. thus, a depen-
dency grammar approach abstracts away from word-order information, representing
only the information that is necessary for the parse.

an additional practical motivation for a dependency-based approach is that the
head-dependent relations provide an approximation to the semantic relationship be-

2 chapter 13

    id33

prefer

s

i

   ight

np

vp

the

morning

denver

pro

verb

np

i

prefer

det

nom

through

the

nom

pp

nom

noun

p

np

noun

   ight

through

pro

morning

denver

figure 13.1 a dependency-style parse alongside the corresponding constituent-based analysis for i prefer the
morning    ight through denver.

tween predicates and their arguments that makes them directly useful for many ap-
plications such as coreference resolution, id53 and information ex-
traction. constituent-based approaches to parsing provide similar information, but it
often has to be distilled from the trees via techniques such as the head    nding rules
discussed in chapter 10.

in the following sections, we   ll discuss in more detail the inventory of relations
used in id33, as well as the formal basis for these dependency struc-
tures. we   ll then move on to discuss the dominant families of algorithms that are
used to automatically produce these structures. finally, we   ll discuss how to eval-
uate dependency parsers and point to some of the ways they are used in language
processing applications.

13.1 dependency relations

grammatical
relation

head
dependent

grammatical
function

the traditional linguistic notion of grammatical relation provides the basis for the
binary relations that comprise these dependency structures. the arguments to these
relations consist of a head and a dependent. we   ve already discussed the notion of
heads in chapter 10 and chapter 12 in the context of constituent structures. there,
the head word of a constituent was the central organizing word of a larger constituent
(e.g, the primary noun in a noun phrase, or verb in a verb phrase). the remaining
words in the constituent are either direct, or indirect, dependents of their head. in
dependency-based approaches, the head-dependent relationship is made explicit by
directly linking heads to the words that are immediately dependent on them, bypass-
ing the need for constituent structures.

in addition to specifying the head-dependent pairs, dependency grammars allow
us to further classify the kinds of grammatical relations, or grammatical function,

13.1

    dependency relations

3

nominal subject
direct object
indirect object
clausal complement
open clausal complement

clausal argument relations description
nsubj
dobj
iobj
ccomp
xcomp
nominal modi   er relations description
nmod
amod
nummod
appos
det
case
other notable relations
conj
cc
figure 13.2 selected dependency relations from the universal dependency set. (de marn-
effe et al., 2014)

nominal modi   er
adjectival modi   er
numeric modi   er
appositional modi   er
determiner
prepositions, postpositions and other case markers
description
conjunct
coordinating conjunction

in terms of the role that the dependent plays with respect to its head. familiar notions
such as subject, direct object and indirect object are among the kind of relations we
have in mind. in english these notions strongly correlate with, but by no means de-
termine, both position in a sentence and constituent type and are therefore somewhat
redundant with the kind of information found in phrase-structure trees. however, in
more    exible languages the information encoded directly in these grammatical rela-
tions is critical since phrase-based constituent syntax provides little help.

not surprisingly, linguists have developed taxonomies of relations that go well
beyond the familiar notions of subject and object. while there is considerable vari-
ation from theory to theory, there is enough commonality that efforts to develop a
computationally useful standard are now possible. the universal dependencies
project (nivre et al., 2016) provides an inventory of dependency relations that are
linguistically motivated, computationally useful, and cross-linguistically applicable.
fig. 13.2 shows a subset of the relations from this effort. fig. 13.3 provides some
example sentences illustrating selected relations.

the motivation for all of the relations in the universal dependency scheme is
beyond the scope of this chapter, but the core set of frequently used relations can be
broken into two sets: clausal relations that describe syntactic roles with respect to a
predicate (often a verb), and modi   er relations that categorize the ways that words
that can modify their heads.

consider the following example sentence:

root

dobj

det

nsubj

nmod

nmod

case

united canceled the morning    ights to houston

(13.2)

the clausal relations nsubj and dobj identify the subject and direct object of
the predicate cancel, while the nmod, det, and case relations denote modi   ers of
the nouns    ights and houston.

universal
dependencies

4 chapter 13

    id33

relation
nsubj
dobj

iobj
nmod
amod
nummod
appos
det

examples with head and dependent
united canceled the    ight.
united diverted the    ight to reno.
we booked her the    rst    ight to miami.
we booked her the    ight to miami.
we took the morning    ight.
book the cheapest    ight.
before the storm jetblue canceled 1000    ights.
united, a unit of ual, matched the fares.
the    ight was canceled.
which    ight was delayed?
we    ew to denver and drove to steamboat.
we    ew to denver and drove to steamboat.
book the    ight through houston.

conj
cc
case
figure 13.3 examples of core universal dependency relations.

13.2 dependency formalisms

dependency
tree

in their most general form, the dependency structures we   re discussing are simply
directed graphs. that is, structures g = (v,a) consisting of a set of vertices v , and
a set of ordered pairs of vertices a, which we   ll refer to as arcs.

for the most part we will assume that the set of vertices, v , corresponds exactly
to the set of words in a given sentence. however, they might also correspond to
punctuation, or when dealing with morphologically complex languages the set of
vertices might consist of stems and af   xes. the set of arcs, a, captures the head-
dependent and grammatical function relationships between the elements in v .

further constraints on these dependency structures are speci   c to the underlying
grammatical theory or formalism. among the more frequent restrictions are that the
structures must be connected, have a designated root node, and be acyclic or planar.
of most relevance to the parsing approaches discussed in this chapter is the common,
computationally-motivated, restriction to rooted trees. that is, a dependency tree
is a directed graph that satis   es the following constraints:

1. there is a single designated root node that has no incoming arcs.
2. with the exception of the root node, each vertex has exactly one incoming arc.
3. there is a unique path from the root node to each vertex in v .

taken together, these constraints ensure that each word has a single head, that the
dependency structure is connected, and that there is a single root node from which
one can follow a unique directed path to each of the words in the sentence.

13.2.1 projectivity
the notion of projectivity imposes an additional constraint that is derived from the
order of the words in the input, and is closely related to the context-free nature of
human languages discussed in chapter 10. an arc from a head to a dependent is
said to be projective if there is a path from the head to every word that lies between
the head and the dependent in the sentence. a dependency tree is then said to be
projective if all the arcs that make it up are projective. all the dependency trees
we   ve seen thus far have been projective. there are, however, many perfectly valid

13.3

    dependency treebanks

5

constructions which lead to non-projective trees, particularly in languages with a
relatively    exible word order.

consider the following example.

root

mod

nsubj

dobj

det

nmod

det

case

mod

adv

jetblue canceled our    ight this morning which was already late

(13.3)

in this example, the arc from    ight to its modi   er was is non-projective since
there is no path from    ight to the intervening words this and morning. as we can
see from this diagram, projectivity (and non-projectivity) can be detected in the way
we   ve been drawing our trees. a dependency tree is projective if it can be drawn
with no crossing edges. here there is no way to link    ight to its dependent was
without crossing the arc that links morning to its head.

our concern with projectivity arises from two related issues. first, the most
widely used english dependency treebanks were automatically derived from phrase-
structure treebanks through the use of head-   nding rules (chapter 10). the trees
generated in such a fashion are guaranteed to be projective since they   re generated
from context-free grammars.

second, there are computational limitations to the most widely used families of
parsing algorithms. the transition-based approaches discussed in section 13.4 can
only produce projective trees, hence any sentences with non-projective structures
will necessarily contain some errors. this limitation is one of the motivations for
the more    exible graph-based parsing approach described in section 13.5.

13.3 dependency treebanks

as with constituent-based methods, treebanks play a critical role in the development
and evaluation of dependency parsers. dependency treebanks have been created
using similar approaches to those discussed in chapter 10     having human annota-
tors directly generate dependency structures for a given corpus, or using automatic
parsers to provide an initial parse and then having annotators hand correct those
parsers. we can also use a deterministic process to translate existing constituent-
based treebanks into dependency trees through the use of head rules.

for the most part, directly annotated dependency treebanks have been created for
morphologically rich languages such as czech, hindi and finnish that lend them-
selves to dependency grammar approaches, with the prague dependency treebank
(bej  cek et al., 2013) for czech being the most well-known effort. the major english
dependency treebanks have largely been extracted from existing resources such as
the wall street journal sections of the id32(marcus et al., 1993). the
more recent ontonotes project (hovy et al. 2006,weischedel et al. 2011) extends
this approach going beyond traditional news text to include conversational telephone
speech, weblogs, usenet newsgroups, broadcast, and talk shows in english, chinese
and arabic.

the translation process from constituent to dependency structures has two sub-
tasks: identifying all the head-dependent relations in the structure and identifying
the correct dependency relations for these relations. the    rst task relies heavily on

6 chapter 13

    id33

the use of head rules discussed in chapter 10    rst developed for use in lexicalized
probabilistic parsers (magerman 1994,collins 1999,collins 2003). here   s a simple
and effective algorithm from xia and palmer (2001).

1. mark the head child of each node in a phrase structure, using the appropriate

head rules.

2. in the dependency structure, make the head of each non-head child depend on

the head of the head-child.

when a phrase-structure parse contains additional information in the form of
grammatical relations and function tags, as in the case of the id32, these
tags can be used to label the edges in the resulting tree. when applied to the parse
tree in fig. 13.4, this algorithm would produce the dependency structure in fig. 13.4.

root

sbj

aux

dobj

nmod

clr

tmp

case

nmod

amod

num

vinken will join the board as a nonexecutive director nov 29

(13.4)

the primary shortcoming of these extraction methods is that they are limited by
the information present in the original constituent trees. among the most impor-
tant issues are the failure to integrate morphological information with the phrase-
structure trees, the inability to easily represent non-projective structures, and the
lack of internal structure to most noun-phrases, as re   ected in the generally    at
rules used in most treebank grammars. for these reasons, outside of english, most
dependency treebanks are developed directly using human annotators.

13.4 transition-based id33

shift-reduce
parsing

con   guration

our    rst approach to id33 is motivated by a stack-based approach
called id132 originally developed for analyzing programming lan-
guages (aho and ullman, 1972). this classic approach is simple and elegant, em-
ploying a context-free grammar, a stack, and a list of tokens to be parsed. input
tokens are successively shifted onto the stack and the top two elements of the stack
are matched against the right-hand side of the rules in the grammar; when a match is
found the matched elements are replaced on the stack (reduced) by the non-terminal
from the left-hand side of the rule being matched. in adapting this approach for
id33, we forgo the explicit use of a grammar and alter the reduce
operation so that instead of adding a non-terminal to a parse tree, it introduces a
dependency relation between a word and its head. more speci   cally, the reduce ac-
tion is replaced with two possible actions: assert a head-dependent relation between
the word at the top of the stack and the word below it, or vice versa. figure 13.5
illustrates the basic operation of such a parser.

a key element in transition-based parsing is the notion of a con   guration which
consists of a stack, an input buffer of words, or tokens, and a set of relations rep-
resenting a dependency tree. given this framework, the parsing process consists of
a sequence of transitions through the space of possible con   gurations. the goal of

13.4

    transition-based id33

7

s

np-sbj

vp

nnp

md

vp

vinken

will

vb

np

pp-clr

np-tmp

join

dt

nn

in

np

nnp

cd

the

board

as

dt

jj

nn

nov

29

a

nonexecutive

director

s(join)

np-sbj(vinken)

vp(join)

nnp

md

vp(join)

vinken

will

vb

np(board)

pp-clr(director)

np-tmp(29)

join

dt

nn

in

np(director)

nnp

cd

the

board

as

dt

jj

nn

nov

29

a

nonexecutive

director

join

vinken

will

board

director

29

the

as

a

nonexecutive

nov

figure 13.4 a phrase-structure tree from the wall street journal component of the id32 3.

this process is to    nd a    nal con   guration where all the words have been accounted
for and an appropriate dependency tree has been synthesized.

to implement such a search, we   ll de   ne a set of transition operators, which
when applied to a con   guration produce new con   gurations. given this setup, we
can view the operation of a parser as a search through a space of con   gurations for
a sequence of transitions that leads from a start state to a desired goal state. at the
start of this process we create an initial con   guration in which the stack contains the

8 chapter 13

    id33

figure 13.5 basic transition-based parser. the parser examines the top two elements of the
stack and selects an action based on consulting an oracle that examines the current con   gura-
tion.

root node, the word list is initialized with the set of the words or lemmatized tokens
in the sentence, and an empty set of relations is created to represent the parse. in the
   nal goal state, the stack and the word list should be empty, and the set of relations
will represent the    nal parse.

in the standard approach to transition-based parsing, the operators used to pro-
duce new con   gurations are surprisingly simple and correspond to the intuitive ac-
tions one might take in creating a dependency tree by examining the words in a
single pass over the input from left to right (covington, 2001):

    assign the current word as the head of some previously seen word,
    assign some previously seen word as the head of the current word,
    or postpone doing anything with the current word, adding it to a store for later

processing.

to make these actions more precise, we   ll create three transition operators that

will operate on the top two elements of the stack:

    leftarc: assert a head-dependent relation between the word at the top of
stack and the word directly beneath it; remove the lower word from the stack.
    rightarc: assert a head-dependent relation between the second word on

the stack and the word at the top; remove the word at the top of the stack;

    shift: remove the word from the front of the input buffer and push it onto

the stack.

arc standard

this particular set of operators implements what is known as the arc standard
approach to transition-based parsing (covington 2001,nivre 2003). there are two
notable characteristics to this approach: the transition operators only assert relations
between elements at the top of the stack, and once an element has been assigned
its head it is removed from the stack and is not available for further processing.
as we   ll see, there are alternative transition systems which demonstrate different
parsing behaviors, but the arc standard approach is quite effective and is simple to
implement.

dependencyrelationswnw1w2s2...s1snparserinput bufferstackoracle13.4

    transition-based id33

9

to assure that these operators are used properly we   ll need to add some pre-
conditions to their use. first, since, by de   nition, the root node cannot have any
incoming arcs, we   ll add the restriction that the leftarc operator cannot be ap-
plied when root is the second element of the stack. second, both reduce operators
require two elements to be on the stack to be applied. given these transition opera-
tors and preconditions, the speci   cation of a transition-based parser is quite simple.
fig. 13.6 gives the basic algorithm.

function dependencyparse(words) returns dependency tree

state   {[root], [words], [] } ; initial con   guration
while state not    nal
t    oracle(state)
state    apply(t, state) ; apply it, creating a new state

; choose a transition operator to apply

return state

figure 13.6 a generic transition-based dependency parser

at each step, the parser consults an oracle (we   ll come back to this shortly) that
provides the correct transition operator to use given the current con   guration. it then
applies that operator to the current con   guration, producing a new con   guration.
the process ends when all the words in the sentence have been consumed and the
root node is the only element remaining on the stack.

the ef   ciency of transition-based parsers should be apparent from the algorithm.
the complexity is linear in the length of the sentence since it is based on a single left
to right pass through the words in the sentence. more speci   cally, each word must
   rst be shifted onto the stack and then later reduced.

note that unlike the id145 and search-based approaches dis-
cussed in chapters 12 and 13, this approach is a straightforward greedy algorithm
    the oracle provides a single choice at each step and the parser proceeds with that
choice, no other options are explored, no backtracking is employed, and a single
parse is returned in the end.

figure 13.7 illustrates the operation of the parser with the sequence of transitions

leading to a parse for the following example.

root

iobj

dobj

det

nmod

book me the morning    ight

(13.5)

let   s consider the state of the con   guration at step 2, after the word me has been

pushed onto the stack.

stack

word list

relations

[root, book, me] [the, morning,    ight]

the correct operator to apply here is rightarc which assigns book as the head of
me and pops me from the stack resulting in the following con   guration.

stack

relations
[root, book] [the, morning,    ight] (book     me)

word list

10 chapter 13

    id33

step

0
1
2
3
4
5
6
7
8
9
10

stack word list
[root]
[root, book]
[root, book, me]
[root, book]
[root, book, the]
[root, book, the, morning]
[root, book, the, morning,    ight]
[root, book, the,    ight]
[root, book,    ight]
[root, book]
[root]

[book, me, the, morning,    ight]
[me, the, morning,    ight]
[the, morning,    ight]
[the, morning,    ight]
[morning,    ight]
[   ight]
[]
[]
[]
[]
[]

action
shift
shift

rightarc

shift
shift
shift

leftarc
leftarc
rightarc
rightarc

done

relation added

(book     me)

(morning        ight)
(the        ight)
(book        ight)
(root     book)

figure 13.7 trace of a transition-based parse.

after several subsequent applications of the shift and leftarc operators, the con-
   guration in step 6 looks like the following:

stack

word list

[root, book, the, morning,    ight]

[]

relations
(book     me)

here, all the remaining words have been passed onto the stack and all that is left
to do is to apply the appropriate reduce operators. in the current con   guration, we
employ the leftarc operator resulting in the following state.

stack

word list

[root, book, the,    ight]

[]

relations
(book     me)

(morning        ight)

at this point, the parse for this sentence consists of the following structure.

dobj

nmod

book me the morning    ight

(13.6)

there are several important things to note when examining sequences such as
the one in figure 13.7. first, the sequence given is not the only one that might lead
to a reasonable parse. in general, there may be more than one path that leads to the
same result, and due to ambiguity, there may be other transition sequences that lead
to different equally valid parses.

second, we are assuming that the oracle always provides the correct operator
at each point in the parse     an assumption that is unlikely to be true in practice.
as a result, given the greedy nature of this algorithm, incorrect choices will lead to
incorrect parses since the parser has no opportunity to go back and pursue alternative
choices. section 13.4.2 will introduce several techniques that allow transition-based
approaches to explore the search space more fully.

finally, for simplicity, we have illustrated this example without the labels on
the dependency relations. to produce labeled trees, we can parameterize the left-
arc and rightarc operators with dependency labels, as in leftarc(nsubj) or
rightarc(dobj). this is equivalent to expanding the set of transition operators
from our original set of three to a set that includes leftarc and rightarc opera-
tors for each relation in the set of dependency relations being used, plus an additional
one for the shift operator. this, of course, makes the job of the oracle more dif   cult
since it now has a much larger set of operators from which to choose.

13.4

    transition-based id33

11

13.4.1 creating an oracle
state-of-the-art transition-based systems use supervised machine learning methods
to train classi   ers that play the role of the oracle. given appropriate training data,
these methods learn a function that maps from con   gurations to transition operators.
as with all supervised machine learning methods, we will need access to appro-
priate training data and we will need to extract features useful for characterizing the
decisions to be made. the source for this training data will be representative tree-
banks containing dependency trees. the features will consist of many of the same
features we encountered in chapter 8 for part-of-speech tagging, as well as those
used in chapter 12 for statistical parsing models.

generating training data
let   s revisit the oracle from the algorithm in fig. 13.6 to fully understand the learn-
ing problem. the oracle takes as input a con   guration and returns as output a tran-
sition operator. therefore, to train a classi   er, we will need con   gurations paired
with transition operators (i.e., leftarc, rightarc, or shift). unfortunately,
treebanks pair entire sentences with their corresponding trees, and therefore they
don   t directly provide what we need.

to generate the required training data, we will employ the oracle-based parsing
algorithm in a clever way. we will supply our oracle with the training sentences
to be parsed along with their corresponding reference parses from the treebank. to
produce training instances, we will then simulate the operation of the parser by run-
ning the algorithm and relying on a new training oracle to give us correct transition
operators for each successive con   guration.

to see how this works, let   s    rst review the operation of our parser. it begins with
a default initial con   guration where the stack contains the root, the input list is just
the list of words, and the set of relations is empty. the leftarc and rightarc
operators each add relations between the words at the top of the stack to the set of
relations being accumulated for a given sentence. since we have a gold-standard
reference parse for each training sentence, we know which dependency relations are
valid for a given sentence. therefore, we can use the reference parse to guide the
selection of operators as the parser steps through a sequence of con   gurations.

to be more precise, given a reference parse and a con   guration, the training

oracle proceeds as follows:

training oracle

    choose leftarc if it produces a correct head-dependent relation given the

reference parse and the current con   guration,

    otherwise, choose rightarc if (1) it produces a correct head-dependent re-
lation given the reference parse and (2) all of the dependents of the word at
the top of the stack have already been assigned,

    otherwise, choose shift.
the restriction on selecting the rightarc operator is needed to ensure that a
word is not popped from the stack, and thus lost to further processing, before all its
dependents have been assigned to it.

more formally, during training the oracle has access to the following informa-

tion:

    a current con   guration with a stack s and a set of dependency relations rc
    a reference parse consisting of a set of vertices v and a set of dependency

relations rp

12 chapter 13

    id33

step

0
1
2
3
4
5
6
7
8
9
10

stack
[root]
[root, book]
[root, book, the]
[root, book, the,    ight]
[root, book,    ight]
[root, book,    ight, through]
[root, book,    ight, through, houston]
[root, book,    ight, houston ]
[root, book,    ight]
[root, book]
[root]

word list
[book, the,    ight, through, houston]
[the,    ight, through, houston]
[   ight, through, houston]
[through, houston]
[through, houston]
[houston]
[]
[]
[]
[]
[]

predicted action

shift
shift
shift

leftarc

shift
shift

leftarc
rightarc
rightarc
rightarc

done

figure 13.8 generating training items consisting of con   guration/predicted action pairs by
simulating a parse with a given reference parse.

given this information, the oracle chooses transitions as follows:

leftarc(r): if (s1 r s2)     rp
rightarc(r): if (s2 r s1)     rp and    r(cid:48),w s.t.(s1 r(cid:48) w)     rp then (s1 r(cid:48) w)    
rc
shift: otherwise

let   s walk through some the steps of this process with the following example as

shown in fig. 13.8.

root

dobj

det

nmod

case

book the    ight through houston

(13.7)

at step 1, leftarc is not applicable in the initial con   guration since it asserts
a relation, (root     book), not in the reference answer; rightarc does assert a
relation contained in the    nal answer (root     book), however book has not been
attached to any of its dependents yet, so we have to defer, leaving shift as the only
possible action. the same conditions hold in the next two steps. in step 3, leftarc
is selected to link the to its head.

now consider the situation in step 4.

stack
relations
[root, book,    ight] [through, houston] (the        ight)

word buffer

here, we might be tempted to add a dependency relation between book and    ight,
which is present in the reference parse. but doing so now would prevent the later
attachment of houston since    ight would have been removed from the stack. for-
tunately, the precondition on choosing rightarc prevents this choice and we   re
again left with shift as the only viable option. the remaining choices complete the
set of operators needed for this example.

to recap, we derive appropriate training instances consisting of con   guration-
transition pairs from a treebank by simulating the operation of a parser in the con-
text of a reference dependency tree. we can deterministically record correct parser
actions at each step as we progress through each training example, thereby creating
the training set we require.

feature
template

13.4

    transition-based id33

13

features
having generated appropriate training instances (con   guration-transition pairs), we
need to extract useful features from the con   gurations so what we can train classi-
   ers. the features that are used to train transition-based systems vary by language,
genre, and the kind of classi   er being employed. for example, morphosyntactic
features such as case marking on subjects or direct objects may be more or less im-
portant depending on the language being processed. that said, the basic features that
we have already seen with part-of-speech tagging and partial parsing have proven to
be useful in training dependency parsers across a wide range of languages. word
forms, lemmas and parts of speech are all powerful features, as are the head, and
dependency relation to the head.

in the transition-based parsing framework, such features need to be extracted
from the con   gurations that make up the training data. recall that con   gurations
consist of three elements: the stack, the buffer and the current set of relations. in
principle, any property of any or all of these elements can be represented as features
in the usual way for training. however, to avoid sparsity and encourage generaliza-
tion, it is best to focus the learning algorithm on the most useful aspects of decision
making at each point in the parsing process. the focus of feature extraction for
transition-based parsing is, therefore, on the top levels of the stack, the words near
the front of the buffer, and the dependency relations already associated with any of
those elements.

by combining simple features, such as word forms or parts of speech, with spe-
ci   c locations in a con   guration, we can employ the notion of a feature template
that we   ve already encountered with id31 and part-of-speech tagging.
feature templates allow us to automatically generate large numbers of speci   c fea-
tures from a training set. as an example, consider the following feature templates
that are based on single positions in a con   guration.

(cid:104)s1.w,op(cid:105),(cid:104)s2.w,op(cid:105)(cid:104)s1.t,op(cid:105),(cid:104)s2.t,op(cid:105)
(cid:104)b1.w,op(cid:105),(cid:104)b1.t,op(cid:105)(cid:104)s1.wt,op(cid:105)

(13.8)

in these examples, individual features are denoted as location.property, where s
denotes the stack, b the word buffer, and r the set of relations. individual properties
of locations include w for word forms, l for lemmas, and t for part-of-speech. for
example, the feature corresponding to the word form at the top of the stack would be
denoted as s1.w, and the part of speech tag at the front of the buffer b1.t. we can also
combine individual features via concatenation into more speci   c features that may
prove useful. for example, the feature designated by s1.wt represents the word form
concatenated with the part of speech of the word at the top of the stack. finally, op
stands for the transition operator for the training example in question (i.e., the label
for the training instance).

let   s consider the simple set of single-element feature templates given above
in the context of the following intermediate con   guration derived from a training
oracle for example 13.2.

stack
[root, canceled,    ights] [to houston] (canceled     united)
(   ights     morning)

word buffer

relations

(   ights     the)

the correct transition here is shift (you should convince yourself of this before

14 chapter 13

    id33

proceeding). the application of our set of feature templates to this con   guration
would result in the following set of instantiated features.

(13.9)

(cid:104)s1.w =    ights,op = shift(cid:105)
(cid:104)s2.w = canceled,op = shift(cid:105)
(cid:104)s1.t = nns,op = shift(cid:105)
(cid:104)s2.t = vbd,op = shift(cid:105)
(cid:104)b1.w = to,op = shift(cid:105)
(cid:104)b1.t = to,op = shift(cid:105)
(cid:104)s1.wt =    ightsnns,op = shift(cid:105)

given that the left and right arc transitions operate on the top two elements of
the stack, features that combine properties from these positions are even more useful.
for example, a feature like s1.t     s2.t concatenates the part of speech tag of the word
at the top of the stack with the tag of the word beneath it.

(cid:104)s1.t     s2.t = nnsvbd,op = shift(cid:105)

(13.10)
not surprisingly, if two properties are useful then three or more should be even
better. figure 13.9 gives a baseline set of feature templates that have been employed
in various state-of-the-art systems (zhang and clark 2008,huang and sagae 2010,zhang
and nivre 2011).

note that some of these features make use of dynamic features     features such
as head words and dependency relations that have been predicted at earlier steps in
the parsing process, as opposed to features that are derived from static properties of
the input.

feature templates

source
one word s1.w
s2.w
b1.w
two word s1.w    s2.w
s1.t     s2.wt
s1.w    s1.t     s2.t

s1.wt
s2.wt
b0.wt
s1.t     b1.w

s1.t
s2.t
b1.w
s1.t     s2.t
s1.w    s2.w    s2.t s1.w    s1.t     s2.t
s1.w    s1.t

figure 13.9 standard feature templates for training transition-based dependency parsers.
in the template speci   cations sn refers to a location on the stack, bn refers to a location in the
word buffer, w refers to the wordform of the input, and t refers to the part of speech of the
input.

learning
over the years, the dominant approaches to training transition-based dependency
parsers have been multinomial id28 and support vector machines, both
of which can make effective use of large numbers of sparse features of the kind
described in the last section. more recently, neural network, or deep learning,
approaches of the kind described in chapter 8 have been applied successfully to
transition-based parsing (chen and manning, 2014). these approaches eliminate the
need for complex, hand-crafted features and have been particularly effective at over-
coming the data sparsity issues normally associated with training transition-based
parsers.

13.4

    transition-based id33

15

13.4.2 advanced methods in transition-based parsing
the basic transition-based approach can be elaborated in a number of ways to im-
prove performance by addressing some of the most obvious    aws in the approach.

alternative transition systems
the arc-standard transition system described above is only one of many possible sys-
tems. a frequently used alternative is the arc eager transition system. the arc eager
approach gets its name from its ability to assert rightward relations much sooner
than in the arc standard approach. to see this, let   s revisit the arc standard trace of
example 13.7, repeated here.

arc eager

root

dobj

det

nmod

case

book the    ight through houston

consider the dependency relation between book and    ight in this analysis. as
is shown in fig. 13.8, an arc-standard approach would assert this relation at step 8,
despite the fact that book and    ight    rst come together on the stack much earlier at
step 4. the reason this relation can   t be captured at this point is due to the presence
of the post-nominal modi   er through houston. in an arc-standard approach, depen-
dents are removed from the stack as soon as they are assigned their heads. if    ight
had been assigned book as its head in step 4, it would no longer be available to serve
as the head of houston.

the input buffer and the word at the top of the stack; pop the stack.

while this delay doesn   t cause any issues in this example, in general the longer
a word has to wait to get assigned its head the more opportunities there are for
something to go awry. the arc-eager system addresses this issue by allowing words
to be attached to their heads as early as possible, before all the subsequent words
dependent on them have been seen. this is accomplished through minor changes to
the leftarc and rightarc operators and the addition of a new reduce operator.
    leftarc: assert a head-dependent relation between the word at the front of
    rightarc: assert a head-dependent relation between the word on the top of
the stack and the word at front of the input buffer; shift the word at the front
of the input buffer to the stack.
    shift: remove the word from the front of the input buffer and push it onto
    reduce: pop the stack.
the leftarc and rightarc operators are applied to the top of the stack and
the front of the input buffer, instead of the top two elements of the stack as in the
arc-standard approach. the rightarc operator now moves the dependent to the
stack from the buffer rather than removing it, thus making it available to serve as the
head of following words. the new reduce operator removes the top element from
the stack. together these changes permit a word to be eagerly assigned its head and
still allow it to serve as the head for later dependents. the trace shown in fig. 13.10
illustrates the new decision sequence for this example.

the stack.

in addition to demonstrating the arc-eager transition system, this example demon-
strates the power and    exibility of the overall transition-based approach. we were
able to swap in a new transition system without having to make any changes to the

16 chapter 13

    id33

step

0
1
2
3
4
5
6
7
8
9
10

action

stack word list
[root]
[root, book]
[root, book, the]
[root, book]
[root, book,    ight]
[root, book,    ight, through]
[root, book,    ight]
[root, book,    ight, houston]
[root, book,    ight]
[root, book]
[root]

[book, the,    ight, through, houston] rightarc
[the,    ight, through, houston]
[   ight, through, houston]
[   ight, through, houston]
[through, houston]
[houston]
[houston]
[]
[]
[]
[]

leftarc
rightarc
reduce
reduce
reduce

leftarc
rightarc

done

shift

shift

relation added
(root     book)
(the        ight)
(book        ight)

(through     houston)
(   ight     houston)

figure 13.10 a processing trace of book the    ight through houston using the arc-eager
transition operators.

underlying parsing algorithm. this    exibility has led to the development of a di-
verse set of transition systems that address different aspects of syntax and semantics
including: assigning part of speech tags (choi and palmer, 2011a), allowing the
generation of non-projective dependency structures (nivre, 2009), assigning seman-
tic roles (choi and palmer, 2011b), and parsing texts containing multiple languages
(bhat et al., 2017).

id125

beam width

id125
the computational ef   ciency of the transition-based approach discussed earlier de-
rives from the fact that it makes a single pass through the sentence, greedily making
decisions without considering alternatives. of course, this is also the source of its
greatest weakness     once a decision has been made it can not be undone, even in
the face of overwhelming evidence arriving later in a sentence. another approach
is to systematically explore alternative decision sequences, selecting the best among
those alternatives. the key problem for such a search is to manage the large number
of potential sequences. id125 accomplishes this by combining a breadth-   rst
search strategy with a heuristic    lter that prunes the search frontier to stay within a
   xed-size beam width.

in applying id125 to transition-based parsing, we   ll elaborate on the al-
gorithm given in fig. 13.6. instead of choosing the single best transition operator
at each iteration, we   ll apply all applicable operators to each state on an agenda and
then score the resulting con   gurations. we then add each of these new con   gura-
tions to the frontier, subject to the constraint that there has to be room within the
beam. as long as the size of the agenda is within the speci   ed beam width, we can
add new con   gurations to the agenda. once the agenda reaches the limit, we only
add new con   gurations that are better than the worst con   guration on the agenda
(removing the worst element so that we stay within the limit). finally, to insure that
we retrieve the best possible state on the agenda, the while loop continues as long as
there are non-   nal states on the agenda.

the id125 approach requires a more elaborate notion of scoring than we
used with the greedy algorithm. there, we assumed that a classi   er trained using
supervised machine learning would serve as an oracle, selecting the best transition
operator based on features extracted from the current con   guration. regardless of
the speci   c learning approach, this choice can be viewed as assigning a score to all
the possible transitions and picking the best one.

  t (c) = argmaxscore(t,c)

13.5

    graph-based id33

17

with a id125 we are now searching through the space of decision se-
quences, so it makes sense to base the score for a con   guration on its entire history.
more speci   cally, we can de   ne the score for a new con   guration as the score of its
predecessor plus the score of the operator used to produce it.

con   gscore(c0) = 0.0
con   gscore(ci) = con   gscore(ci   1) + score(ti,ci   1)

this score is used both in    ltering the agenda and in selecting the    nal answer.

the new id125 version of transition-based parsing is given in fig. 13.11.

function dependencybeamparse(words, width) returns dependency tree
state   {[root], [words], [], 0.0}
agenda   (cid:104)state(cid:105);
while agenda contains non-   nal states

;initial con   guration

initial agenda

newagenda   (cid:104)(cid:105)
for each state     agenda do
child    apply(t, state)
newagenda    addtobeam(child, newagenda, width)

for all {t | t     validoperators(state)} do

agenda   newagenda
return bestof(agenda)

function addtobeam(state, agenda, width) returns updated agenda

if length(agenda) < width then

else if score(state) > score(worstof(agenda))

agenda    insert(state, agenda)
agenda    remove(worstof(agenda))
agenda    insert(state, agenda)

return agenda

figure 13.11 id125 applied to transition-based id33.

13.5 graph-based id33

graph-based approaches to id33 search through the space of possible
trees for a given sentence for a tree (or trees) that maximize some score. these
methods encode the search space as directed graphs and employ methods drawn
from id207 to search the space for optimal solutions. more formally, given a
sentence s we   re looking for the best dependency tree in gs, the space of all possible
trees for that sentence, that maximizes some score.

  t (s) = argmax

t   gs

score(t,s)

as with the probabilistic approaches to context-free parsing discussed in chap-
ter 12, the overall score for a tree can be viewed as a function of the scores of the
parts of the tree. the focus of this section is on edge-factored approaches where the

edge-factored

18 chapter 13

    id33

score for a tree is based on the scores of the edges that comprise the tree.

(cid:88)

e   t

score(t,s) =

score(e)

there are several motivations for the use of graph-based methods. first, unlike
transition-based approaches, these methods are capable of producing non-projective
trees. although projectivity is not a signi   cant issue for english, it is de   nitely a
problem for many of the world   s languages. a second motivation concerns parsing
accuracy, particularly with respect to longer dependencies. empirically, transition-
based methods have high accuracy on shorter dependency relations but accuracy de-
clines signi   cantly as the distance between the head and dependent increases (mc-
donald and nivre, 2011). graph-based methods avoid this dif   culty by scoring
entire trees, rather than relying on greedy local decisions.

the following section examines a widely-studied approach based on the use of a
maximum spanning tree (mst) algorithm for weighted, directed graphs. we then
discuss features that are typically used to score trees, as well as the methods used to
train the scoring models.

maximum
spanning tree

13.5.1 parsing
the approach described here uses an ef   cient greedy algorithm to search for optimal
spanning trees in directed graphs. given an input sentence, it begins by constructing
a fully-connected, weighted, directed graph where the vertices are the input words
and the directed edges represent all possible head-dependent assignments. an addi-
tional root node is included with outgoing edges directed at all of the other vertices.
the weights in the graph re   ect the score for each possible head-dependent relation
as provided by a model generated from training data. given these weights, a maxi-
mum spanning tree of this graph emanating from the root represents the preferred
dependency parse for the sentence. a directed graph for the example book that
   ight is shown in fig. 13.12, with the maximum spanning tree corresponding to the
desired parse shown in blue. for ease of exposition, we   ll focus here on unlabeled
id33. graph-based approaches to labeled parsing are discussed in
section 13.5.3.

before describing the algorithm it   s useful to consider two intuitions about di-
rected graphs and their spanning trees. the    rst intuition begins with the fact that
every vertex in a spanning tree has exactly one incoming edge. it follows from this
that every connected component of a spanning tree will also have one incoming edge.
the second intuition is that the absolute values of the edge scores are not critical to
determining its maximum spanning tree. instead, it is the relative weights of the
edges entering each vertex that matters. if we were to subtract a constant amount
from each edge entering a given vertex it would have no impact on the choice of
the maximum spanning tree since every possible spanning tree would decrease by
exactly the same amount.

the    rst step of the algorithm itself is quite straightforward. for each vertex
in the graph, an incoming edge (representing a possible head assignment) with the
highest score is chosen. if the resulting set of edges produces a spanning tree then
we   re done. more formally, given the original fully-connected graph g = (v,e), a
subgraph t = (v,f) is a spanning tree if it has no cycles and each vertex (other than
the root) has exactly one edge entering it. if the greedy selection process produces
such a tree then it is the best possible one.

13.5

    graph-based id33

19

figure 13.12

initial rooted, directed graph for book that    ight.

unfortunately, this approach doesn   t always lead to a tree since the set of edges
selected may contain cycles. fortunately, in yet another case of multiple discovery,
there is a straightforward way to eliminate cycles generated during the greedy se-
lection phase. chu and liu (1965) and edmonds (1967) independently developed
an approach that begins with greedy selection and follows with an elegant recursive
cleanup phase that eliminates cycles.

the cleanup phase begins by adjusting all the weights in the graph by subtracting
the score of the maximum edge entering each vertex from the score of all the edges
entering that vertex. this is where the intuitions mentioned earlier come into play.
we have scaled the values of the edges so that the weight of the edges in the cycle
have no bearing on the weight of any of the possible spanning trees. subtracting the
value of the edge with maximum weight from each edge entering a vertex results
in a weight of zero for all of the edges selected during the greedy selection phase,
including all of the edges involved in the cycle.

having adjusted the weights, the algorithm creates a new graph by selecting a
cycle and collapsing it into a single new node. edges that enter or leave the cycle
are altered so that they now enter or leave the newly collapsed node. edges that do
not touch the cycle are included and edges within the cycle are dropped.

now, if we knew the maximum spanning tree of this new graph, we would have
what we need to eliminate the cycle. the edge of the maximum spanning tree di-
rected towards the vertex representing the collapsed cycle tells us which edge to
delete to eliminate the cycle. how do we    nd the maximum spanning tree of this
new graph? we recursively apply the algorithm to the new graph. this will either
result in a spanning tree or a graph with a cycle. the recursions can continue as long
as cycles are encountered. when each recursion completes we expand the collapsed
vertex, restoring all the vertices and edges from the cycle with the exception of the
single edge to be deleted.

putting all this together, the maximum spanning tree algorithm consists of greedy
edge selection, re-scoring of edge costs and a recursive cleanup phase when needed.
the full algorithm is shown in fig. 13.13.

fig. 13.14 steps through the algorithm with our book that    ight example. the
   rst row of the    gure illustrates greedy edge selection with the edges chosen shown

rootbookthat   ight124456875720 chapter 13

    id33

function maxspanningtree(g=(v,e), root, score) returns spanning tree

f   []
t      []
score      []
for each v     v do

bestinedge   argmaxe=(u,v)    e score[e]
f   f     bestinedge
for each e=(u,v)     e do

score   [e]   score[e]     score[bestinedge]

if t=(v,f) is a spanning tree then return it
else

c   a cycle in f
g       contract(g, c)
t       maxspanningtree(g   , root, score   )
t    expand(t   , c)
return t

function contract(g, c) returns contracted graph

function expand(t, c) returns expanded graph

figure 13.13 the chu-liu edmonds algorithm for    nding a maximum spanning tree in a
weighted directed graph.

in blue (corresponding to the set f in the algorithm). this results in a cycle between
that and    ight. the scaled weights using the maximum value entering each node are
shown in the graph to the right.

collapsing the cycle between that and    ight to a single node (labelled tf) and
recursing with the newly scaled costs is shown in the second row. the greedy selec-
tion step in this recursion yields a spanning tree that links root to book, as well as an
edge that links book to the contracted node. expanding the contracted node, we can
see that this edge corresponds to the edge from book to    ight in the original graph.
this in turn tells us which edge to drop to eliminate the cycle

on arbitrary directed graphs, this version of the cle algorithm runs in o(mn)
time, where m is the number of edges and n is the number of nodes. since this par-
ticular application of the algorithm begins by constructing a fully connected graph
m = n2 yielding a running time of o(n3). gabow et al. (1986) present a more ef   -
cient implementation with a running time of o(m + nlogn).

13.5.2 features and training
given a sentence, s, and a candidate tree, t , edge-factored parsing models reduce
the score for the tree to a sum of the scores of the edges that comprise the tree.

score(s,t ) =

score(s,e)

each edge score can, in turn, be reduced to a weighted sum of features extracted

from it.

score(s,e) =

wi fi(s,e)

(cid:88)

e   t

n(cid:88)

i=1

13.5

    graph-based id33

21

figure 13.14 chu-liu-edmonds graph-based example for book that    ight

or more succinctly.

score(s,e) = w   f

given this formulation, we are faced with two problems in training our parser:

identifying relevant features and    nding the weights used to score those features.

the features used to train edge-factored models mirror those used in training
transition-based parsers (as shown in fig. 13.9). this is hardly surprising since in
both cases we   re trying to capture information about the relationship between heads
and their dependents in the context of a single relation. to summarize this earlier
discussion, commonly used features include:

the words.

    wordforms, lemmas, and parts of speech of the headword and its dependent.
    corresponding features derived from the contexts before, after and between
    id27s.
    the dependency relation itself.
    the direction of the relation (to the right or left).
    the distance from the head to the dependent.

as with transition-based approaches, pre-selected combinations of these features are
often used as well.

given a set of features, our next problem is to learn a set of weights correspond-
ing to each. unlike many of the learning problems discussed in earlier chapters,

rootbooktfrootbookthat   ight0-3-4-7-1-6-2rootbook12that7   ight8-4-30-2-6-1-700rootbook0tf-10-3-4-7-1-6-2rootbook12that7   ight81244568757deleted from cycle22 chapter 13

    id33

id136-based
learning

here we are not training a model to associate training items with class labels, or
parser actions. instead, we seek to train a model that assigns higher scores to cor-
rect trees than to incorrect ones. an effective framework for problems like this is to
use id136-based learning combined with the id88 learning rule. in this
framework, we parse a sentence (i.e, perform id136) from the training set using
some initially random set of initial weights. if the resulting parse matches the cor-
responding tree in the training data, we do nothing to the weights. otherwise, we
   nd those features in the incorrect parse that are not present in the reference parse
and we lower their weights by a small amount based on the learning rate. we do this
incrementally for each sentence in our training data until the weights converge.

more recently, recurrent neural network (id56) models have demonstrated state-

of-the-art performance in shared tasks on multilingual parsing (zeman et al. 2017,dozat
et al. 2017). these neural approaches rely solely on lexical information in the form
of id27s, eschewing the use of hand-crafted features such as those de-
scribed earlier.

13.5.3 advanced issues in graph-based parsing

13.6 evaluation

as with phrase structure-based parsing, the evaluation of dependency parsers pro-
ceeds by measuring how well they work on a test-set. an obvious metric would be
exact match (em)     how many sentences are parsed correctly. this metric is quite
pessimistic, with most sentences being marked wrong. such measures are not    ne-
grained enough to guide the development process. our metrics need to be sensitive
enough to tell if actual improvements are being made.

for these reasons, the most common method for evaluating dependency parsers
are labeled and unlabeled attachment accuracy. labeled attachment refers to the
proper assignment of a word to its head along with the correct dependency relation.
unlabeled attachment simply looks at the correctness of the assigned head, ignor-
ing the dependency relation. given a system output and a corresponding reference
parse, accuracy is simply the percentage of words in an input that are assigned the
correct head with the correct relation. this metrics are usually referred to as the
labeled attachment score (las) and unlabeled attachment score (uas). finally, we
can make use of a label accuracy score (ls), the percentage of tokens with correct
labels, ignoring where the relations are coming from.

as an example, consider the reference parse and system parse for the following

example shown in fig. 13.15.
(13.11) book me the    ight through houston.

the system correctly    nds 4 of the 6 dependency relations present in the refer-
ence parse and therefore receives an las of 2/3. however, one of the 2 incorrect
relations found by the system holds between book and    ight, which are in a head-
dependent relation in the reference parse; therefore the system therefore achieves an
uas of 5/6.

beyond attachment scores, we may also be interested in how well a system is
performing on a particular kind of dependency relation, for example nsubj, across
a development corpus. here we can make use of the notions of precision and recall
introduced in chapter 8, measuring the percentage of relations labeled nsubj by
the system that were correct (precision), and the percentage of the nsubj relations

13.7

    summary

23

root

iobj

obj

det

nmod

case

root

x-comp

nsubj

det

nmod

case

book me the

   ight

reference

through houston

book me the    ight
system

through houston

figure 13.15 reference and system parses for book me the    ight through houston, resulting in an las of
4/6 and an uas of 5/6.

present in the development set that were in fact discovered by the system (recall).
we can employ a confusion matrix to keep track of how often each dependency type
was confused for another.

13.7 summary

this chapter has introduced the concept of dependency grammars and dependency
parsing. here   s a summary of the main points that we covered:

ship among the words in a sentence.

    in dependency-based approaches to syntax, the structure of a sentence is de-
scribed in terms of a set of binary relations that hold between the words in a
sentence. larger notions of constituency are not directly encoded in depen-
dency analyses.
    the relations in a dependency structure capture the head-dependent relation-
    dependency-based analyses provides information directly useful in further
language processing tasks including information extraction, id29
and id53
    transition-based parsing systems employ a greedy stack-based algorithm to
    graph-based methods for creating dependency structures are based on the use
    both transition-based and graph-based approaches are developed using super-
    treebanks provide the data needed to train these systems. dependency tree-
banks can be created directly by human annotators or via automatic transfor-
mation from phrase-structure treebanks.
    evaluation of dependency parsers is based on labeled and unlabeled accuracy

of maximum spanning tree methods from id207.

vised machine learning techniques.

create dependency structures.

scores as measured against withheld development and test corpora.

bibliographical and historical notes

the dependency-based approach to grammar is much older than the relatively re-
cent phrase-structure or constituency grammars that have been the primary focus of
both theoretical and computational linguistics for years. it has its roots in the an-
cient greek and indian linguistic traditions. contemporary theories of dependency

24 chapter 13

    id33

grammar all draw heavily on the work of tesni`ere (1959). the most in   uential
dependency grammar frameworks include meaning-text theory (mtt) (mel     cuk,
1988), word grammar (hudson, 1984), functional generative description (fdg)
(sgall et al., 1986). these frameworks differ along a number of dimensions in-
cluding the degree and manner in which they deal with morphological, syntactic,
semantic and pragmatic factors, their use of multiple layers of representation, and
the set of relations used to categorize dependency relations.

automatic parsing using dependency grammars was    rst introduced into compu-
tational linguistics by early work on machine translation at the rand corporation
led by david hays. this work on id33 closely paralleled work on
constituent parsing and made explicit use of grammars to guide the parsing process.
after this early period, computational work on id33 remained inter-
mittent over the following decades. notable implementations of dependency parsers
for english during this period include link grammar (sleator and temperley, 1993),
constraint grammar (karlsson et al., 1995), and minipar (lin, 2003).

id33 saw a major resurgence in the late 1990   s with the appear-
ance of large dependency-based treebanks and the associated advent of data driven
approaches described in this chapter. eisner (1996) developed an ef   cient dynamic
programming approach to id33 based on bilexical grammars derived
from the id32. covington (2001) introduced the deterministic word by
word approach underlying current transition-based approaches. yamada and mat-
sumoto (2003) and kudo and matsumoto (2002) introduced both the shift-reduce
paradigm and the use of supervised machine learning in the form of support vector
machines to id33.

nivre (2003) de   ned the modern, deterministic, transition-based approach to de-
pendency parsing. subsequent work by nivre and his colleagues formalized and an-
alyzed the performance of numerous transition systems, training methods, and meth-
ods for dealing with non-projective language nivre and scholz 2004,nivre 2006,nivre
and nilsson 2005,nivre et al. 2007,nivre 2007.

the graph-based maximum spanning tree approach to id33 was

introduced by mcdonald et al. 2005,mcdonald et al. 2005.

the earliest source of data for training and evaluating dependency english parsers
came from the wsj id32 (marcus et al., 1993) described in chapter 10.
the use of head-   nding rules developed for use with probabilistic parsing facili-
tated the automatic extraction of dependency parses from phrase-based ones (xia
and palmer, 2001).

the long-running prague dependency treebank project (haji  c, 1998) is the most
signi   cant effort to directly annotate a corpus with multiple layers of morphological,
syntactic and semantic information. the current pdt 3.0 now contains over 1.5 m
tokens (bej  cek et al., 2013).

universal dependencies (ud) (nivre et al., 2016) is a project directed at creating
a consistent framework for dependency treebank annotation across languages with
the goal of advancing parser development across the worlds languages. under the
auspices of this effort, treebanks for over 30 languages have been annotated and
made available in a single consistent format. the ud annotation scheme evolved out
of several distinct efforts including stanford dependencies (de marneffe et al. 2006,
de marneffe and manning 2008, de marneffe et al. 2014), google   s universal part-
of-speech tags (petrov et al., 2012), and the interset interlingua for morphosyntactic
tagsets (zeman, 2008). driven in part by the ud framework, dependency treebanks
of a signi   cant size and quality are now available in over 30 languages (nivre et al.,

exercises

25

2016).

the conference on natural language learning (conll) has conducted an in-
   uential series of shared tasks related to id33 over the years (buch-
holz and marsi 2006, nilsson et al. 2007, surdeanu et al. 2008, haji  c et al. 2009).
more recent evaluations have focused on parser robustness with respect to morpho-
logically rich languages (seddah et al., 2013), and non-canonical language forms
such as social media, texts, and spoken language (petrov and mcdonald, 2012).
choi et al. (2015) presents a detailed performance analysis of 10 state-of-the-art de-
pendency parsers across an impressive range of metrics, as well as dependable, a
robust parser evaluation tool.

exercises

26 chapter 13     id33

aho, a. v. and ullman, j. d. (1972). the theory of parsing,

translation, and compiling, vol. 1. prentice hall.

bej  cek, e., haji  cov  a, e., haji  c, j., j    nov  a, p., kettnerov  a,
v., kol  a  rov  a, v., mikulov  a, m., m    rovsk  y, j., nedoluzhko,
a., panevov  a, j., pol  akov  a, l.,   sev  c    kov  a, m.,   st  ep  anek,
j., and zik  anov  a,   s. (2013). prague dependency treebank
3.0. tech. rep., institute of formal and applied linguis-
tics, charles university in prague. lindat/clarin dig-
ital library at institute of formal and applied linguistics,
charles university in prague.

bhat, i., bhat, r. a., shrivastava, m., and sharma, d.
(2017). joining hands: exploiting monolingual treebanks
for parsing of code-mixing data. in eacl-17, pp. 324   330.
buchholz, s. and marsi, e. (2006). conll-x shared task on
multilingual id33. in conll-06, pp. 149   
164.

chen, d. and manning, c. d. (2014). a fast and accurate de-
pendency parser using neural networks.. in emnlp 2014,
pp. 740   750.

choi, j. d. and palmer, m. (2011a). getting the most out
of transition-based id33. in acl 2011, pp.
687   692.

choi, j. d. and palmer, m. (2011b). transition-based se-
mantic role labeling using predicate argument id91.
in proceedings of the acl 2011 workshop on relational
models of semantics, pp. 37   45.

choi, j. d., tetreault, j., and stent, a. (2015). it depends:
dependency parser comparison using a web-based evalua-
tion tool. in acl 2015, pp. 26   31.

chu, y.-j. and liu, t.-h. (1965). on the shortest arbores-
cence of a directed graph. science sinica, 14, 1396   1400.
collins, m. (1999). head-driven statistical models for nat-
ural language parsing. ph.d. thesis, university of penn-
sylvania, philadelphia.

collins, m. (2003). head-driven statistical models for nat-
ural language parsing. computational linguistics, 29(4),
589   637.

covington, m. (2001). a fundamental algorithm for depen-
in proceedings of the 39th annual acm

dency parsing.
southeast conference, pp. 95   102.

de marneffe, m.-c., dozat, t., silveira, n., haverinen, k.,
ginter, f., nivre, j., and manning, c. d. (2014). univer-
sal stanford dependencies: a cross-linguistic typology.. in
lrec, vol. 14, pp. 4585   92.

de marneffe, m.-c., maccartney, b., and manning, c. d.
(2006). generating typed dependency parses from phrase
structure parses. in lrec-06.

de marneffe, m.-c. and manning, c. d. (2008). the stanford
typed dependencies representation. in coling 2008: pro-
ceedings of the workshop on cross-framework and cross-
domain parser evaluation, pp. 1   8.

dozat, t., qi, p., and manning, c. d. (2017). stanford   s
graph-based neural dependency parser at the conll 2017
in proceedings of the conll 2017 shared
shared task.
task, pp. 20   30.

edmonds, j. (1967). optimum branchings. journal of re-
search of the national bureau of standards b, 71(4), 233   
240.

eisner, j. (1996). three new probabilistic models for depen-
in coling-96, copen-

dency parsing: an exploration.
hagen, pp. 340   345.

gabow, h. n., galil, z., spencer, t., and tarjan, r. e.
(1986). ef   cient algorithms for    nding minimum spanning
trees in undirected and directed graphs. combinatorica,
6(2), 109   122.

haji  c, j. (1998). building a syntactically annotated cor-
pus: the prague dependency treebank, pp. 106   132.
karolinum.

haji  c, j., ciaramita, m., johansson, r., kawahara, d.,
mart    , m. a., m`arquez, l., meyers, a., nivre, j., pad  o,
s.,   st  ep  anek, j., stran  a  k, p., surdeanu, m., xue, n., and
zhang, y. (2009). the conll-2009 shared task: syntac-
tic and semantic dependencies in multiple languages.
in
conll-09, pp. 1   18.

hovy, e. h., marcus, m. p., palmer, m., ramshaw, l. a.,
and weischedel, r. (2006). ontonotes: the 90% solution.
in hlt-naacl-06.

huang, l. and sagae, k. (2010). id145 for
linear-time incremental parsing. in acl 2010, pp. 1077   
1086.

hudson, r. a. (1984). word grammar. blackwell.
karlsson, f., voutilainen, a., heikkil  a, j., and anttila,
a. (eds.). (1995). constraint grammar: a language-
independent system for parsing unrestricted text. mouton
de gruyter.

kudo, t. and matsumoto, y. (2002). japanese dependency
analysis using cascaded chunking. in conll-02, pp. 63   
69.

lin, d. (2003). dependency-based evaluation of minipar. in

workshop on the evaluation of parsing systems.

magerman, d. m. (1994). natural language parsing as sta-
tistical pattern recognition. ph.d. thesis, university of
pennsylvania.

marcus, m. p., santorini, b., and marcinkiewicz, m. a.
(1993). building a large annotated corpus of english: the
id32. computational linguistics, 19(2), 313   
330.

mcdonald, r., crammer, k., and pereira, f. c. n. (2005).
in

online large-margin training of dependency parsers.
acl-05, ann arbor, pp. 91   98.

mcdonald, r. and nivre, j. (2011). analyzing and integrat-
ing dependency parsers. computational linguistics, 37(1),
197   230.

mcdonald, r., pereira, f. c. n., ribarov, k., and haji  c, j.
(2005). non-projective id33 using spanning
tree algorithms. in hlt-emnlp-05.

mel     cuk, i. a. (1988). dependency syntax: theory and

practice. state university of new york press.

nilsson, j., riedel, s., and yuret, d. (2007). the conll 2007
shared task on id33. in proceedings of the
conll shared task session of emnlp-conll, pp. 915   
932. sn.

nivre, j. (2007).

incremental non-projective dependency

parsing. in naacl-hlt 07.

nivre, j. (2003). an ef   cient algorithm for projective de-
pendency parsing. in proceedings of the 8th international
workshop on parsing technologies (iwpt.

nivre, j. (2006). inductive id33. springer.

exercises

27

zeman, d., popel, m., straka, m., haji  c, j., nivre, j., gin-
ter, f., luotolahti, j., pyysalo, s., petrov, s., potthast, m.,
tyers, f. m., badmaeva, e., gokirmak, m., nedoluzhko,
a., cinkov  a, s., hajic jr., j., hlav  acov  a, j., kettnerov  a,
v., uresov  a, z., kanerva, j., ojala, s., missil  a, a., man-
ning, c. d., schuster, s., reddy, s., taji, d., habash,
n., leung, h., de marneffe, m.-c., sanguinetti, m., simi,
m., kanayama, h., de paiva, v., droganova, k., alonso,
h. m., c     oltekin, c   ., sulubacak, u., uszkoreit, h., macke-
tanz, v., burchardt, a., harris, k., marheinecke, k., rehm,
g., kayadelen, t., attia, m., el-kahky, a., yu, z., pitler,
e., lertpradit, s., mandl, m., kirchner, j., alcalde, h. f.,
strnadov  a, j., banerjee, e., manurung, r., stella, a., shi-
mada, a., kwak, s., mendonc  a, g., lando, t., nitisaroj,
r., and li, j. (2017). conll 2017 shared task: multilin-
gual parsing from raw text to universal dependencies. in
proceedings of the conll 2017 shared task: multilingual
parsing from raw text to universal dependencies, van-
couver, canada, august 3-4, 2017, pp. 1   19.

zhang, y. and clark, s. (2008). a tale of two parsers: inves-
tigating and combining graph-based and transition-based
id33 using beam-search. in emnlp-08, pp.
562   571.

zhang, y. and nivre, j. (2011). transition-based dependency
parsing with rich non-local features. in acl 2011, pp. 188   
193.

nivre, j. (2009). non-projective id33 in ex-

pected linear time. in acl ijcnlp 2009, pp. 351   359.

nivre, j., de marneffe, m.-c., ginter, f., goldberg, y., haji  c,
j., manning, c. d., mcdonald, r. t., petrov, s., pyysalo,
s., silveira, n., tsarfaty, r., and zeman, d. (2016). uni-
versal dependencies v1: a multilingual treebank collec-
tion. in lrec-16.

nivre, j., hall, j., nilsson, j., chanev, a., eryigit, g.,
k  ubler, s., marinov, s., and marsi, e. (2007). malt-
parser: a language-independent system for data-driven de-
pendency parsing. natural language engineering, 13(02),
95   135.

nivre, j. and nilsson, j. (2005). pseudo-projective depen-

dency parsing. in acl-05, pp. 99   106.

nivre, j. and scholz, m. (2004). deterministic dependency

parsing of english text. in coling-04, p. 64.

petrov, s., das, d., and mcdonald, r. (2012). a universal

part-of-speech tagset. in lrec-12.

petrov, s. and mcdonald, r. (2012). overview of the 2012
shared task on parsing the web. in notes of the first work-
shop on syntactic analysis of non-canonical language
(sancl), vol. 59.

seddah, d., tsarfaty, r., k  ubler, s., candito, m., choi, j. d.,
farkas, r., foster, j., goenaga, i., gojenola, k., goldberg,
y., green, s., habash, n., kuhlmann, m., maier, w., nivre,
j., przepi  orkowski, a., roth, r., seeker, w., versley, y.,
vincze, v., woli  nski, m., wr  oblewska, a., and villemonte
de la cl  ergerie, e. (2013). overview of the spmrl 2013
shared task: cross-framework evaluation of parsing mor-
in proceedings of the 4th
phologically rich languages.
workshop on statistical parsing of morphologically-rich
languages.

sgall, p., haji  cov  a, e., and panevova, j. (1986). the mean-

ing of the sentence in its pragmatic aspects. reidel.

sleator, d. and temperley, d. (1993). parsing english with

a link grammar. in iwpt-93.

surdeanu, m., johansson, r., meyers, a., m`arquez, l., and
nivre, j. (2008). the conll-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. in conll-08,
pp. 159   177.

tesni`ere, l. (1959).

  el  ements de syntaxe structurale. li-

brairie c. klincksieck, paris.

weischedel, r., hovy, e. h., marcus, m. p., palmer, m.,
belvin, r., pradhan, s., ramshaw, l. a., and xue, n.
(2011). ontonotes: a large training corpus for enhanced
processing.
in joseph olive, caitlin christianson, j. m.
(ed.), handbook of natural language processing and ma-
chine translation: darpa global automatic language
exploitation, pp. 54   63. springer.

xia, f. and palmer, m. (2001). converting dependency struc-
tures to phrase structures. in hlt-01, san diego, pp. 1   5.
yamada, h. and matsumoto, y. (2003). statistical depen-
in noord,

dency analysis with support vector machines.
g. v. (ed.), iwpt-03, pp. 195   206.

zeman, d. (2008). reusable tagset conversion using tagset

drivers.. in lrec-08.

