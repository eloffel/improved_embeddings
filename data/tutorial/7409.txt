speech and language processing. daniel jurafsky & james h. martin.
rights reserved.

draft of september 23, 2018.

copyright c(cid:13) 2018.

all

chapter

18 id14

sometime between the 7th and 4th centuries bce, the indian grammarian p  an. ini1
wrote a famous treatise on sanskrit grammar, the as.t.  adhy  ay     (   8 books   ), a treatise
that has been called    one of the greatest monuments of
human intelligence    (bloom   eld, 1933, 11). the work
describes the linguistics of the sanskrit language in the
form of 3959 sutras, each very ef   ciently (since it had to
be memorized!) expressing part of a formal rule system
that brilliantly pre   gured modern mechanisms of formal
language theory (penn and kiparsky, 2012). one set of
rules, relevant to our discussion in this chapter, describes
the k  arakas, semantic relationships between a verb and
noun arguments, roles like agent, instrument, or destina-
tion. p  an. ini   s work was the earliest we know of that tried
to understand the linguistic realization of events and their participants. this task
of understanding participants and their relationship to events   being able to answer
the question    who did what to whom    (and perhaps also    when and where   )   is a
central question of natural language understanding.

let   s move forward 2.5 millenia to the present and consider the very mundane
goal of understanding text about a purchase of stock by xyz corporation. this
purchasing event could take on a wide variety of surface forms. in the following
sentences we see that it could be described by a verb (sold, bought) or a noun (pur-
chase), and that xyz corp can be the syntactic subject (of bought), the indirect ob-
ject (of sold), or in a genitive or noun compound relation (with the noun purchase)
despite having notationally the same role in all of them:

    xyz corporation bought the stock.
    they sold the stock to xyz corporation.
    the stock was bought by xyz corporation.
    the purchase of the stock by xyz corporation...
    the stock purchase by xyz corporation...
in this chapter we introduce a level of representation that lets us capture the
commonality between these sentences. we will be able to represent the fact that
there was a purchase event, that the participants in this event were xyz corp and
some stock, and that xyz corp played a speci   c role, the role of acquiring the stock.
we call this shallow semantic representation level semantic roles. semantic
roles are representations that express the abstract role that arguments of a predicate
can take in the event; these can be very speci   c, like the buyer, abstract like the
agent, or super-abstract (the proto-agent). these roles can both represent gen-
eral semantic properties of the arguments and also express their likely relationship to
the syntactic role of the argument in the sentence. agents tend to be the subject of

1 figure shows a birch bark manuscript from kashmir of the rupavatra, a grammatical textbook based
on the sanskrit grammar of panini. image from the wellcome collection.

2 chapter 18

    id14

an active sentence, themes the direct object, and so on. these relations are codi   ed
in databases like propbank and framenet. we   ll introduce id14,
the task of assigning roles to the constituents or phrases in sentences. we   ll also
discuss selectional restrictions, the semantic sortal restrictions or preferences that
each individual predicate can express about its potential arguments, such as the fact
that the theme of the verb eat is generally something edible. along the way, we   ll
describe the various ways these representations can help in language understanding
tasks like id53 and machine translation.

18.1 semantic roles

consider how in chapter 14 we represented the meaning of arguments for sentences
like these:

(18.1) sasha broke the window.
(18.2) pat opened the door.

a neo-davidsonian event representation of these two sentences would be

   e,x,y breaking(e)    breaker(e,sasha)
   e,x,y opening(e)    opener(e,pat)

   brokent hing(e,y)   window(y)
   openedt hing(e,y)    door(y)

deep roles

thematic roles
agents

theme

semantic roles

in this representation, the roles of the subjects of the verbs break and open are
breaker and opener respectively. these deep roles are speci   c to each event; break-
ing events have breakers, opening events have openers, and so on.

if we are going to be able to answer questions, perform id136s, or do any
further kinds of natural language understanding of these events, we   ll need to know
a little more about the semantics of these arguments. breakers and openers have
something in common. they are both volitional actors, often animate, and they have
direct causal responsibility for their events.

thematic roles are a way to capture this semantic commonality between break-
ers and eaters. we say that the subjects of both these verbs are agents. thus, agent
is the thematic role that represents an abstract idea such as volitional causation. sim-
ilarly, the direct objects of both these verbs, the brokenthing and openedthing, are
both prototypically inanimate objects that are affected in some way by the action.
the semantic role for these participants is theme.

although thematic roles are one of the oldest linguistic models, as we saw above,
their modern formulation is due to fillmore (1968) and gruber (1965). although
there is no universally agreed-upon set of roles, figs. 18.1 and 18.2 list some the-
matic roles that have been used in various computational papers, together with rough
de   nitions and examples. most thematic role sets have about a dozen roles, but we   ll
see sets with smaller numbers of roles with even more abstract meanings, and sets
with very large numbers of roles that are speci   c to situations. we   ll use the general
term semantic roles for all sets of roles, whether small or large.

18.2

    diathesis alternations

3

thematic role
de   nition
the volitional causer of an event
agent
the experiencer of an event
experiencer
the non-volitional causer of the event
force
the participant most directly affected by an event
theme
the end product of an event
result
the proposition or content of a propositional event
content
an instrument used in an event
instrument
the bene   ciary of an event
beneficiary
the origin of the object of a transfer event
source
the destination of an object of a transfer event
goal
figure 18.1 some commonly used thematic roles with their de   nitions.

thematic role
agent
experiencer
force
theme
result
content
instrument
beneficiary
source
goal
figure 18.2 some prototypical examples of various thematic roles.

example
the waiter spilled the soup.
john has a headache.
the wind blows debris from the mall into our yards.
only after benjamin franklin broke the ice...
the city built a regulation-size baseball diamond...
mona asked    you met mary ann at a supermarket?   
he poached cat   sh, stunning them with a shocking device...
whenever ann callahan makes hotel reservations for her boss...
i    ew in from boston.
i drove to portland.

18.2 diathesis alternations

the main reason computational systems use semantic roles is to act as a shallow
meaning representation that can let us make simple id136s that aren   t possible
from the pure surface string of words, or even from the parse tree. to extend the
earlier examples, if a document says that company a acquired company b, we   d
like to know that this answers the query was company b acquired? despite the fact
that the two sentences have very different surface syntax. similarly, this shallow
semantics might act as a useful intermediate language in machine translation.

semantic roles thus help generalize over different surface realizations of pred-
icate arguments. for example, while the agent is often realized as the subject of
the sentence, in other cases the theme can be the subject. consider these possible
realizations of the thematic arguments of the verb break:
(18.3) john

broke the window.

agent

theme

(18.4) john

broke the window

with a rock.

agent
(18.5) the rock

theme
broke the window.

instrument

instrument

theme

(18.6) the window

broke.

theme

(18.7) the window

was broken by john.

theme

agent

4 chapter 18

thematic grid
case frame

    id14
these examples suggest that break has (at least) the possible arguments agent,
theme, and instrument. the set of thematic role arguments taken by a verb is
often called the thematic grid,   -grid, or case frame. we can see that there are
(among others) the following possibilities for the realization of these arguments of
break:

agent/subject, theme/object
agent/subject, theme/object,
instrument/subject, theme/object
theme/subject

instrument/ppwith

it turns out that many verbs allow their thematic roles to be realized in various
syntactic positions. for example, verbs like give can realize the theme and goal
arguments in two different ways:
(18.8)

gave the book

a. doris
agent
b. doris
agent

to cary.
goal

theme

gave cary
goal

the book.
theme

verb
alternation
dative
alternation

these multiple argument structure realizations (the fact that break can take agent,

instrument, or theme as subject, and give can realize its theme and goal in
either order) are called verb alternations or diathesis alternations. the alternation
we showed above for give, the dative alternation, seems to occur with particular se-
mantic classes of verbs, including    verbs of future having    (advance, allocate, offer,
owe),    send verbs    (forward, hand, mail),    verbs of throwing    (kick, pass, throw),
and so on. levin (1993) lists for 3100 english verbs the semantic classes to which
they belong (47 high-level classes, divided into 193 more speci   c classes) and the
various alternations in which they participate. these lists of verb classes have been
incorporated into the online resource verbnet (kipper et al., 2000), which links each
verb to both id138 and framenet entries.

18.3 semantic roles: problems with thematic roles

representing meaning at the thematic role level seems like it should be useful in
dealing with complications like diathesis alternations. yet it has proved quite dif   -
cult to come up with a standard set of roles, and equally dif   cult to produce a formal
de   nition of roles like agent, theme, or instrument.

for example, researchers attempting to de   ne role sets often    nd they need to
fragment a role like agent or theme into many speci   c roles. levin and rappa-
port hovav (2005) summarize a number of such cases, such as the fact there seem
to be at least two kinds of instruments, intermediary instruments that can appear
as subjects and enabling instruments that cannot:
(18.9)

a. the cook opened the jar with the new gadget.
b. the new gadget opened the jar.
a. shelly ate the sliced banana with a fork.
b. *the fork ate the sliced banana.

(18.10)

in addition to the fragmentation problem, there are cases in which we   d like to
reason about and generalize across semantic roles, but the    nite discrete lists of roles
don   t let us do this.

semantic role

proto-agent
proto-patient

18.4

    the proposition bank

5

finally, it has proved dif   cult to formally de   ne the thematic roles. consider the
agent role; most cases of agents are animate, volitional, sentient, causal, but any
individual noun phrase might not exhibit all of these properties.

these problems have led to alternative semantic role models that use either

many fewer or many more roles.

the    rst of these options is to de   ne generalized semantic roles that abstract
over the speci   c thematic roles. for example, proto-agent and proto-patient
are generalized roles that express roughly agent-like and roughly patient-like mean-
ings. these roles are de   ned, not by necessary and suf   cient conditions, but rather
by a set of heuristic features that accompany more agent-like or more patient-like
meanings. thus, the more an argument displays agent-like properties (being voli-
tionally involved in the event, causing an event or a change of state in another par-
ticipant, being sentient or intentionally involved, moving) the greater the likelihood
that the argument can be labeled a proto-agent. the more patient-like the proper-
ties (undergoing change of state, causally affected by another participant, stationary
relative to other participants, etc.), the greater the likelihood that the argument can
be labeled a proto-patient.

the second direction is instead to de   ne semantic roles that are speci   c to a

particular verb or a particular group of semantically related verbs or nouns.

in the next two sections we describe two commonly used lexical resources that
make use of these alternative versions of semantic roles. propbank uses both proto-
roles and verb-speci   c semantic roles. framenet uses semantic roles that are spe-
ci   c to a general semantic idea called a frame.

18.4 the proposition bank

propbank

the proposition bank, generally referred to as propbank, is a resource of sen-
tences annotated with semantic roles. the english propbank labels all the sentences
in the id32; the chinese propbank labels sentences in the penn chinese
treebank. because of the dif   culty of de   ning a universal set of thematic roles,
the semantic roles in propbank are de   ned with respect to an individual verb sense.
each sense of each verb thus has a speci   c set of roles, which are given only numbers
rather than names: arg0, arg1, arg2, and so on. in general, arg0 represents the
proto-agent, and arg1, the proto-patient. the semantics of the other roles
are less consistent, often being de   ned speci   cally for each verb. nonetheless there
are some generalization; the arg2 is often the benefactive, instrument, attribute, or
end state, the arg3 the start point, benefactive, instrument, or attribute, and the arg4
the end point.

here are some slightly simpli   ed propbank entries for one sense each of the
verbs agree and fall. such propbank entries are called frame    les; note that the
de   nitions in the frame    le for each role (   other entity agreeing   ,    extent, amount
fallen   ) are informal glosses intended to be read by humans, rather than being formal
de   nitions.

(18.11) agree.01

6 chapter 18

    id14

arg0: agreer
arg1: proposition
arg2: other entity agreeing

ex1:
ex2:

[arg0 the group] agreed [arg1 it wouldn   t make an offer].
[argm-tmp usually] [arg0 john] agrees [arg2 with mary]
[arg1 on everything].

(18.12) fall.01

arg1: logical subject, patient, thing falling
arg2: extent, amount fallen
arg3: start point
arg4: end point, end state of arg1
ex1:
ex2:

[arg1 sales] fell [arg4 to $25 million] [arg3 from $27 million].
[arg1 the average junk bond] fell [arg2 by 4.2%].

note that there is no arg0 role for fall, because the normal subject of fall is a

proto-patient.

the propbank semantic roles can be useful in recovering shallow semantic in-

formation about verbal arguments. consider the verb increase:
(18.13) increase.01    go up incrementally   

thing increasing

arg0: causer of increase
arg1:
arg2: amount increased by, ext, or mnr
arg3: start point
arg4: end point

a propbank id14 would allow us to infer the commonality in
the event structures of the following three examples, that is, that in each case big
fruit co. is the agent and the price of bananas is the theme, despite the differing
surface forms.
(18.14)
(18.15)
(18.16)

[arg0 big fruit co. ] increased [arg1 the price of bananas].
[arg1 the price of bananas] was increased again [arg0 by big fruit co. ]
[arg1 the price of bananas] increased [arg2 5%].

propbank also has a number of non-numbered arguments called argms, (argm-
tmp, argm-loc, etc) which represent modi   cation or adjunct meanings. these are
relatively stable across predicates, so aren   t listed with each frame    le. data labeled
with these modi   ers can be helpful in training systems to detect temporal, location,
or directional modi   cation across predicates. some of the argm   s include:

yesterday evening, now
at the museum, in san francisco
down, to bangkok
clearly, with much enthusiasm
because ... , in response to the ruling
themselves, each other

tmp
when?
loc
where?
dir
where to/from?
mnr
how?
prp/cau why?
rec
adv
prd
while propbank focuses on verbs, a related project, nombank (meyers et al.,
2004) adds annotations to noun predicates. for example the noun agreement in
apple   s agreement with ibm would be labeled with apple as the arg0 and ibm as

miscellaneous
secondary predication

...ate the meat raw

nombank

the arg2. this allows semantic role labelers to assign labels to arguments of both
verbal and nominal predicates.

18.5

    framenet

7

18.5 framenet

while making id136s about the semantic commonalities across different sen-
tences with increase is useful, it would be even more useful if we could make such
id136s in many more situations, across different verbs, and also between verbs
and nouns. for example, we   d like to extract the similarity among these three sen-
tences:
(18.17)
(18.18)
(18.19) there has been a [arg2 5%] rise [arg1 in the price of bananas].

[arg1 the price of bananas] increased [arg2 5%].
[arg1 the price of bananas] rose [arg2 5%].

framenet

frame

model
script

frame elements

note that the second example uses the different verb rise, and the third example
uses the noun rather than the verb rise. we   d like a system to recognize that the
price of bananas is what went up, and that 5% is the amount it went up, no matter
whether the 5% appears as the object of the verb increased or as a nominal modi   er
of the noun rise.

the framenet project is another semantic-role-labeling project that attempts
to address just these kinds of problems (baker et al. 1998, fillmore et al. 2003,
fillmore and baker 2009, ruppenhofer et al. 2016). whereas roles in the propbank
project are speci   c to an individual verb, roles in the framenet project are speci   c
to a frame.

what is a frame? consider the following set of words:

reservation,    ight, travel, buy, price, cost, fare, rates, meal, plane

there are many individual lexical relations of hyponymy, synonymy, and so on
between many of the words in this list. the resulting set of relations does not,
however, add up to a complete account of how these words are related. they are
clearly all de   ned with respect to a coherent chunk of common-sense background
information concerning air travel.

we call the holistic background knowledge that unites these words a frame (fill-
more, 1985). the idea that groups of words are de   ned with respect to some back-
ground information is widespread in arti   cial intelligence and cognitive science,
where besides frame we see related works like a model (johnson-laird, 1983), or
even script (schank and abelson, 1977).

a frame in framenet is a background knowledge structure that de   nes a set of
frame-speci   c semantic roles, called frame elements, and includes a set of predi-
cates that use these roles. each word evokes a frame and pro   les some aspect of the
frame and its elements. the framenet dataset includes a set of frames and frame
elements, the lexical units associated with each frame, and a set of labeled exam-
ple sentences. for example, the change position on a scale frame is de   ned as
follows:

this frame consists of words that indicate the change of an item   s posi-
tion on a scale (the attribute) from a starting point (initial value) to an
end point (final value).

core roles

some of the semantic roles (frame elements) in the frame are de   ned as in
fig. 18.3. note that these are separated into core roles, which are frame speci   c, and

8 chapter 18

    id14

non-core roles

non-core roles, which are more like the arg-m arguments in propbank, expressed
more general properties of time, location, and so on.

core roles

attribute
difference
final state

the attribute is a scalar property that the item possesses.
the distance by which an item changes its position on the scale.
a description that presents the item   s state after the change in the attribute   s
value as an independent predication.
the position on the scale where the item ends up.

final value
initial state a description that presents the item   s state before the change in the at-

tribute   s value as an independent predication.

initial value the initial position on the scale from which the item moves away.
item
value range a portion of the scale, typically identi   ed by its end points, along which the

the entity that has a position on the scale.

duration
speed
group

values of the attribute    uctuate.

some non-core roles

the length of time over which the change takes place.
the rate of change of the value.
the group in which an item changes the value of an
attribute in a speci   ed way.

figure 18.3 the frame elements in the change position on a scale frame from the framenet labelers
guide (ruppenhofer et al., 2016).

here are some example sentences:

(18.20)
(18.21)
(18.22)
(18.23)

[item oil] rose [attribute in price] [difference by 2%].
[item it] has increased [final state to having them 1 day a month].
[item microsoft shares] fell [final value to 7 5/8].
[item colon cancer incidence] fell [difference by 50%] [group among

men].

(18.24) a steady increase [initial value from 9.5] [final value to 14.3] [item

in dividends]

(18.25) a [difference 5%] [item dividend] increase...

note from these example sentences that the frame includes target words like rise,

fall, and increase. in fact, the complete frame consists of the following words:

soar
mushroom swell
swing
triple
tumble

edge
explode plummet
fall

verbs: dwindle move
advance
climb
decline
decrease    uctuate rise
diminish gain
grow
dip
increase skyrocket
double
drop
jump

rocket
shift

reach

slide

nouns: hike
decline
decrease

increase
rise

tumble

escalation shift
explosion
fall
   uctuation adverbs:
gain
increasingly
growth

framenet also codes relationships between frames, allowing frames to inherit
from each other, or representing relations between frames like causation (and gen-
eralizations among frame elements in different frames can be representing by inher-
itance as well). thus, there is a cause change of position on a scale frame that is
linked to the change of position on a scale frame by the cause relation, but that
adds an agent role and is used for causative examples such as the following:

18.6

    id14

9

(18.26)

[agent they] raised [item the price of their soda] [difference by 2%].
together, these two frames would allow an understanding system to extract the
common event semantics of all the verbal and nominal causative and non-causative
usages.

framenets have also been developed for many other languages including span-

ish, german, japanese, portuguese, italian, and chinese.

18.6 id14

semantic role
labeling

id14 (sometimes shortened as srl) is the task of automatically
   nding the semantic roles of each argument of each predicate in a sentence. cur-
rent approaches to id14 are based on supervised machine learning,
often using the framenet and propbank resources to specify what counts as a pred-
icate, de   ne the set of roles used in the task, and provide training and test sets.

recall that the difference between these two models of semantic roles is that
framenet (18.27) employs many frame-speci   c frame elements as roles, while prop-
bank (18.28) uses a smaller number of numbered argument labels that can be inter-
preted as verb-speci   c labels, along with the more general argm labels. some
examples:

(18.27)

[you]
cognizer

can   t

[blame]
target evaluee

[the program]

[for being unable to identify it]
reason

(18.28)

[the san francisco examiner]
arg0

issued
target arg1

[a special edition]

[yesterday]
argm-tmp

18.6.1 a feature-based algorithm for id14
a simpli   ed feature-based id14 algorithm is sketched in fig. 18.4.
feature-based algorithms   from the very earliest systems like (simmons, 1973)   
begin by parsing, using broad-coverage parsers to assign a parse to the input string.
figure 18.5 shows a parse of (18.28) above. the parse is then traversed to    nd all
words that are predicates.

for each of these predicates, the algorithm examines each node in the parse
tree and uses supervised classi   cation to decide the semantic role (if any) it plays
for this predicate. given a labeled training set such as propbank or framenet, a
feature vector is extracted for each node, using feature templates described in the
next subsection. a 1-of-n classi   er is then trained to predict a semantic role for
each constituent given these features, where n is the number of potential semantic
roles plus an extra none role for non-role constituents. any standard classi   cation
algorithms can be used. finally, for each test sentence to be labeled, the classi   er is
run on each relevant constituent.

instead of training a single-stage classi   er as in fig. 18.5, the node-level classi-

   cation task can be broken down into multiple steps:

1. pruning: since only a small number of the constituents in a sentence are
arguments of any given predicate, many systems use simple heuristics to prune
unlikely constituents.

2. identi   cation: a binary classi   cation of each node as an argument to be la-

beled or a none.

10 chapter 18

    id14

function semanticrolelabel(words) returns labeled tree

parse    parse(words)
for each predicate in parse do
for each node in parse do

featurevector    extractfeatures(node, predicate, parse)
classifynode(node, featurevector, parse)

figure 18.4 a generic semantic-role-labeling algorithm. classifynode is a 1-of-n clas-
si   er that assigns a semantic role (or none for non-role constituents), trained on labeled data
such as framenet or propbank.

figure 18.5 parse tree for a propbank sentence, showing the propbank argument labels. the dotted line
shows the path feature np   s   vp   vbd for arg0, the np-sbj constituent the san francisco examiner.

3. classi   cation: a 1-of-n classi   cation of all the constituents that were labeled

as arguments by the previous stage

the separation of identi   cation and classi   cation may lead to better use of fea-
tures (different features may be useful for the two tasks) or to computational ef   -
ciency.

global optimization
the classi   cation algorithm of fig. 18.5 classi   es each argument separately (   lo-
cally   ), making the simplifying assumption that each argument of a predicate can be
labeled independently. this assumption is false; there are interactions between argu-
ments that require a more    global    assignment of labels to constituents. for example,
constituents in framenet and propbank are required to be non-overlapping. more
signi   cantly, the semantic roles of constituents are not independent. for example
propbank does not allow multiple identical arguments; two constituents of the same
verb cannot both be labeled arg0 .

role labeling systems thus often add a fourth step to deal with global consistency
across the labels in a sentence. for example, the local classi   ers can return a list of
possible labels associated with probabilities for each constituent, and a second-pass
viterbi decoding or re-ranking approach can be used to choose the best consensus
label. integer id135 (ilp) is another common way to choose a solution
that conforms best to multiple constraints.

snp-sbj=arg0vpdtnnpnnpnnpthesanfranciscoexaminervbd=targetnp=arg1pp-tmp=argm-tmpissueddtjjnninnpaspecialeditionaroundnnnp-tmpnoonyesterday18.6

    id14

11

features for id14
most systems use some generalization of the core set of features introduced by
gildea and jurafsky (2000). common basic features templates (demonstrated on
the np-sbj constituent the san francisco examiner in fig. 18.5) include:

mantic roles tend to appear as nps, others as s or pp, and so on.

    the governing predicate, in this case the verb issued. the predicate is a cru-
cial feature since labels are de   ned only with respect to a particular predicate.
    the phrase type of the constituent, in this case, np (or np-sbj). some se-
    the headword of the constituent, examiner. the headword of a constituent
can be computed with standard head rules, such as those given in chapter 10
in fig. ??. certain headwords (e.g., pronouns) place strong constraints on the
possible semantic roles they are likely to    ll.

    the headword part of speech of the constituent, nnp.
    the path in the parse tree from the constituent to the predicate. this path is
marked by the dotted line in fig. 18.5. following gildea and jurafsky (2000),
we can use a simple linear representation of the path, np   s   vp   vbd.     and
    represent upward and downward movement in the tree, respectively. the
path is very useful as a compact representation of many kinds of grammatical
function relationships between the constituent and the predicate.

    the voice of the clause in which the constituent appears, in this case, active
(as contrasted with passive). passive sentences tend to have strongly different
linkings of semantic roles to surface form than do active ones.

either before or after.

    the binary linear position of the constituent with respect to the predicate,
    the subcategorization of the predicate, the set of expected arguments that
appear in the verb phrase. we can extract this information by using the phrase-
structure rule that expands the immediate parent of the predicate; vp     vbd
np pp for the predicate in fig. 18.5.

    the named entity type of the constituent.
    the    rst words and the last word of the constituent.
the following feature vector thus represents the    rst np in our example (recall
that most observations will have the value none rather than, for example, arg0,
since most constituents in the parse tree will not bear a semantic role):

arg0: [issued, np, examiner, nnp, np   s   vp   vbd, active, before, vp     np pp,
org, the, examiner]

other features are often used in addition, such as sets of id165s inside the
constituent, or more complex versions of the path features (the upward or downward
halves, or whether particular nodes occur in the path).

it   s also possible to use dependency parses instead of constituency parses as the
basis of features, for example using dependency parse paths instead of constituency
paths.

18.6.2 a neural algorithm for id14
the standard neural algorithm for id14 is based on the bi-lstm
iob tagger introduced in chapter 9, which we   ve seen applied to part-of-speech
tagging and named entity tagging, among other tasks. recall that with iob tagging,

12 chapter 18

    id14

figure 18.6 a bi-lstm approach to id14. most actual networks are
much deeper than shown in this    gure; 3 to 4 bi-lstm layers (6 to 8 total lstms) are
common. the input is a concatenation of an embedding for the input word and an embedding
of a binary variable which is 1 for the predicate to 0 for all other words. after he et al. (2017).

we have a begin and end tag for each possible role (b-arg0, i-arg0; b-arg1,
i-arg1, and so on), plus an outside tag o.

as with all the taggers, the goal is to compute the highest id203 tag se-

quence   y, given the input sequence of words w:

  y = argmax

y   t

p(y|w)

in algorithms like he et al. (2017), each input word is mapped to pre-trained em-
beddings, and also associated with an embedding for a    ag (0/1) variable indicating
whether that input word is the predicate. these concatenated embeddings are passed
through multiple layers of bi-directional lstm. state-of-the-art algorithms tend to
be deeper than for pos or ner tagging, using 3 to 4 layers (6 to 8 total lstms).
highway layers can be used to connect these layers as well.

output from the last bi-lstm can then be turned into an iob sequence as for
pos or ner tagging. tags can be locally optimized by taking the bi-lstm output,
passing it through a single layer into a softmax for each word that creates a proba-
bility distribution over all srl tags and the most likely tag for word xi is chosen as
ti, computing for each word essentially:

  yi = argmax
t   tags

p(t|wi)

however, just as feature-based srl tagging, this local approach to decoding doesn   t
exploit the global constraints between tags; a tag i-arg0, for example, must follow
another i-arg0 or b-arg0.

as we saw for pos and ner tagging, there are many ways to take advantage of
these global constraints. a crf layer can be used instead of a softmax layer on top
of the bi-lstm output, and the viterbi decoding algorithm can be used to decode
from the crf.

an even simpler viterbi decoding algorithm that may perform equally well and
doesn   t require adding crf complexity to the training process is to start with the
simple softmax. the softmax output (the entire id203 distribution over tags)
for each word is then treated it as a lattice and we can do viterbi decoding through
the lattice. the hard iob constraints can act as the transition probabilities in the

thecatslovehatsembeddingslstm1lstm1lstm1lstm1lstm2lstm2lstm2lstm2concatenationright-to-left lstid113ft-to-right lstmsoftmaxp(b-arg0)p(i-arg0)p(b-pred)p(b-arg1)0010word + is-predicate18.7

    selectional restrictions

13

viterbi decoding (thus the transition from state i-arg0 to i-arg1 would have
id203 0). alternatively, the training data can be used to learn bigram or trigram
tag transition probabilities as if doing id48 decoding. fig. 18.6 shows a sketch of
the algorithm.

18.6.3 evaluation of id14
the standard evaluation for id14 is to require that each argument
label must be assigned to the exactly correct word sequence or parse constituent, and
then compute precision, recall, and f-measure. identi   cation and classi   cation can
also be evaluated separately. two common datasets used for evaluation are conll-
2005 (carreras and m`arquez, 2005) and conll-2012 (pradhan et al., 2013).

18.7 selectional restrictions

selectional
restriction

we turn in this section to another way to represent facts about the relationship be-
tween predicates and arguments. a selectional restriction is a semantic type con-
straint that a verb imposes on the kind of concepts that are allowed to    ll its argument
roles. consider the two meanings associated with the following example:
(18.29) i want to eat someplace nearby.
there are two possible parses and semantic interpretations for this sentence.
in
the sensible interpretation, eat is intransitive and the phrase someplace nearby is
an adjunct that gives the location of the eating event. in the nonsensical speaker-as-
godzilla interpretation, eat is transitive and the phrase someplace nearby is the direct
object and the theme of the eating, like the np malaysian food in the following
sentences:
(18.30)

i want to eat malaysian food.

how do we know that someplace nearby isn   t the direct object in this sentence?
one useful cue is the semantic fact that the theme of eating events tends to be
something that is edible. this restriction placed by the verb eat on the    ller of its
theme argument is a selectional restriction.

selectional restrictions are associated with senses, not entire lexemes. we can

see this in the following examples of the lexeme serve:
(18.31) the restaurant serves green-lipped mussels.
(18.32) which airlines serve denver?
example (18.31) illustrates the offering-food sense of serve, which ordinarily re-
stricts its theme to be some kind of food example (18.32) illustrates the provides a
commercial service to sense of serve, which constrains its theme to be some type
of appropriate location.

selectional restrictions vary widely in their speci   city. the verb imagine, for
example, imposes strict requirements on its agent role (restricting it to humans
and other animate entities) but places very few semantic requirements on its theme
role. a verb like diagonalize, on the other hand, places a very speci   c constraint
on the    ller of its theme role: it has to be a matrix, while the arguments of the
adjectives odorless are restricted to concepts that could possess an odor:
(18.33) in rehearsal, i often ask the musicians to imagine a tennis game.

14 chapter 18

    id14

(18.34) radon is an odorless gas that can   t be detected by human senses.
(18.35) to diagonalize a matrix is to    nd its eigenvalues.

these examples illustrate that the set of concepts we need to represent selectional
restrictions (being a matrix, being able to possess an odor, etc) is quite open ended.
this distinguishes selectional restrictions from other features for representing lexical
knowledge, like parts-of-speech, which are quite limited in number.

18.7.1 representing selectional restrictions
one way to capture the semantics of selectional restrictions is to use and extend the
event representation of chapter 14. recall that the neo-davidsonian representation
of an event consists of a single variable that stands for the event, a predicate denoting
the kind of event, and variables and relations for the event roles. ignoring the issue of
the    -structures and using thematic roles rather than deep event roles, the semantic
contribution of a verb like eat might look like the following:

   e,x,y eating(e)    agent(e,x)    t heme(e,y)

with this representation, all we know about y, the    ller of the theme role, is that
it is associated with an eating event through the theme relation. to stipulate the
selectional restriction that y must be something edible, we simply add a new term to
that effect:

   e,x,y eating(e)    agent(e,x)    t heme(e,y)    ediblet hing(y)

when a phrase like ate a hamburger is encountered, a semantic analyzer can

form the following kind of representation:

   e,x,y eating(e)    eater(e,x)    t heme(e,y)    ediblet hing(y)    hamburger(y)
this representation is perfectly reasonable since the membership of y in the category
hamburger is consistent with its membership in the category ediblething, assuming
a reasonable set of facts in the knowledge base. correspondingly, the representation
for a phrase such as ate a takeoff would be ill-formed because membership in an
event-like category such as takeoff would be inconsistent with membership in the
category ediblething.

while this approach adequately captures the semantics of selectional restrictions,
there are two problems with its direct use. first, using fol to perform the simple
task of enforcing selectional restrictions is overkill. other, far simpler, formalisms
can do the job with far less computational cost. the second problem is that this
approach presupposes a large, logical knowledge base of facts about the concepts
that make up selectional restrictions. unfortunately, although such common-sense
knowledge bases are being developed, none currently have the kind of coverage
necessary to the task.

a more practical approach is to state selectional restrictions in terms of id138
synsets rather than as logical concepts. each predicate simply speci   es a id138
synset as the selectional restriction on each of its arguments. a meaning representa-
tion is well-formed if the role    ller word is a hyponym (subordinate) of this synset.
for our ate a hamburger example, for instance, we could set the selectional
restriction on the theme role of the verb eat to the synset {food, nutrient}, glossed
as any substance that can be metabolized by an animal to give energy and build

18.7

    selectional restrictions

15

sense 1
hamburger, beefburger --
(a fried cake of minced beef served on a bun)
=> sandwich

=> snack food

=> dish

=> nutriment, nourishment, nutrition...

=> food, nutrient

=> substance
=> matter

=> physical entity

=> entity

figure 18.7 evidence from id138 that hamburgers are edible.

tissue. luckily, the chain of hypernyms for hamburger shown in fig. 18.7 reveals
that hamburgers are indeed food. again, the    ller of a role need not match the
restriction synset exactly; it just needs to have the synset as one of its superordinates.
we can apply this approach to the theme roles of the verbs imagine, lift, and di-
agonalize, discussed earlier. let us restrict imagine   s theme to the synset {entity},
lift   s theme to {physical entity}, and diagonalize to {matrix}. this arrangement
correctly permits imagine a hamburger and lift a hamburger, while also correctly
ruling out diagonalize a hamburger.

18.7.2 selectional preferences
in the earliest implementations, selectional restrictions were considered strict con-
straints on the kind of arguments a predicate could take (katz and fodor 1963,
hirst 1987).
for example, the verb eat might require that its theme argument
be [+food]. early id51 systems used this idea to rule out
senses that violated the selectional restrictions of their governing predicates.

very quickly, however, it became clear that these selectional restrictions were

better represented as preferences rather than strict constraints (wilks 1975b, wilks 1975a).
for example, selectional restriction violations (like inedible arguments of eat) often
occur in well-formed sentences, for example because they are negated (18.36), or
because selectional restrictions are overstated (18.37):
(18.36) but it fell apart in 1931, perhaps because people realized you can   t eat

gold for lunch if you   re hungry.

(18.37) in his two championship trials, mr. kulkarni ate glass on an empty

stomach, accompanied only by water and tea.

modern systems for selectional preferences therefore specify the relation be-

tween a predicate and its possible arguments with soft constraints of some kind.

selectional association
one of the most in   uential has been the selectional association model of resnik
(1993). resnik de   nes the idea of selectional preference strength as the general
amount of information that a predicate tells us about the semantic class of its argu-
ments. for example, the verb eat tells us a lot about the semantic class of its direct
objects, since they tend to be edible. the verb be, by contrast, tells us less about
its direct objects. the selectional preference strength can be de   ned by the differ-
ence in information between two distributions: the distribution of expected semantic

selectional
preference
strength

16 chapter 18

    id14

relative id178
kl divergence

classes p(c) (how likely is it that a direct object will fall into class c) and the dis-
tribution of expected semantic classes for the particular verb p(c|v) (how likely is
it that the direct object of the speci   c verb v will fall into semantic class c). the
greater the difference between these distributions, the more information the verb is
giving us about possible objects. the difference between these two distributions can
be quanti   ed by relative id178, or the id181 (kullback and
leibler, 1951). the kullback-leibler or kl divergence d(p||q) expresses the dif-
ference between two id203 distributions p and q (we   ll return to this when we
discuss distributional models of meaning in chapter 6).

d(p||q) =

p(x)log

p(x)
q(x)

(18.38)

(cid:88)

x

the selectional preference sr(v) uses the kl divergence to express how much in-
formation, in bits, the verb v expresses about the possible semantic class of its argu-
ment.

sr(v) = d(p(c|v)||p(c))
p(c|v)log

=

p(c|v)
p(c)

(cid:88)

c

(18.39)

selectional
association

resnik then de   nes the selectional association of a particular class and verb as the
relative contribution of that class to the general selectional preference of the verb:

ar(v,c) =

1

sr(v)

p(c|v)log

p(c|v)
p(c)

(18.40)

the selectional association is thus a probabilistic measure of the strength of asso-
ciation between a predicate and a class dominating the argument to the predicate.
resnik estimates the probabilities for these associations by parsing a corpus, count-
ing all the times each predicate occurs with each argument word, and assuming
that each word is a partial observation of all the id138 concepts containing the
word. the following table from resnik (1996) shows some sample high and low
selectional associations for verbs and some id138 semantic classes of their direct
objects.

verb
read
write
see

direct object
semantic class assoc
writing
writing
entity

6.80
7.26
5.79

direct object
semantic class assoc
activity
commerce
method

-.20
0
-0.01

selectional preference via id155
an alternative to using selectional association between a verb and the id138 class
of its arguments, is to simply use the id155 of an argument word
given a predicate verb. this simple model of selectional preferences can be used
to directly model the strength of association of one verb (predicate) with one noun
(argument).

the id155 model can be computed by parsing a very large cor-
pus (billions of words), and computing co-occurrence counts: how often a given
verb occurs with a given noun in a given relation. the id155 of an

18.8

    primitive decomposition of predicates

17

argument noun given a verb for a particular relation p(n|v,r) can then be used as a
selectional preference metric for that pair of words (brockmann and lapata, 2003):

the inverse id203 p(v|n,r) was found to have better performance in some cases
(brockmann and lapata, 2003):

(cid:40) c(n,v,r)

c(v,r)

(cid:40) c(n,v,r)

c(n,r)

p(n|v,r) =

if c(n,v,r) > 0

0 otherwise

p(v|n,r) =

if c(n,v,r) > 0

0 otherwise

pseudowords

in cases where it   s not possible to get large amounts of parsed data, another option,
at least for direct objects, is to get the counts from simple part-of-speech based
approximations. for example pairs can be extracted using the pattern    v det n   ,
where v is any form of the verb, det is the   a      and n is the singular or plural
form of the noun (keller and lapata, 2003).

an even simpler approach is to use the simple log co-occurrence frequency of
the predicate with the argument logcount(v,n,r) instead of id155;
this seems to do better for extracting preferences for syntactic subjects rather than
objects (brockmann and lapata, 2003).

evaluating selectional preferences
one way to evaluate models of selectional preferences is to use pseudowords (gale
et al. 1992, sch  utze 1992). a pseudoword is an arti   cial word created by concate-
nating a test word in some context (say banana) with a confounder word (say door)
to create banana-door). the task of the system is to identify which of the two words
is the original word. to evaluate a selectional preference model (for example on the
relationship between a verb and a direct object) we take a test corpus and select all
verb tokens. for each verb token (say drive) we select the direct object (e.g., car),
concatenated with a confounder word that is its nearest neighbor, the noun with the
frequency closest to the original (say house), to make car/house). we then use the
selectional preference model to choose which of car and house are more preferred
objects of drive, and compute how often the model chooses the correct original ob-
ject (e.g., (car) (chambers and jurafsky, 2010).

another evaluation metric is to get human preferences for a test set of verb-
argument pairs, and have them rate their degree of plausibility. this is usually done
by using magnitude estimation, a technique from psychophysics, in which subjects
rate the plausibility of an argument proportional to a modulus item. a selectional
preference model can then be evaluated by its correlation with the human prefer-
ences (keller and lapata, 2003).

18.8 primitive decomposition of predicates

one way of thinking about the semantic roles we have discussed through the chapter
is that they help us de   ne the roles that arguments play in a decompositional way,
based on    nite lists of thematic roles (agent, patient, instrument, proto-agent, proto-
patient, etc.) this idea of decomposing meaning into sets of primitive semantics
elements or features, called primitive decomposition or componential analysis,

componential
analysis

18 chapter 18

    id14

has been taken even further, and focused particularly on predicates.

consider these examples of the verb kill:

(18.41) jim killed his philodendron.
(18.42) jim did something to cause his philodendron to become not alive.
there is a truth-conditional (   propositional semantics   ) perspective from which these
two sentences have the same meaning. assuming this equivalence, we could repre-
sent the meaning of kill as:
(18.43) kill(x,y)     cause(x, become(not(alive(y))))
thus using semantic primitives like do, cause, become not, and alive.

indeed, one such set of potential semantic primitives has been used to account for
some of the verbal alternations discussed in section 18.2 (lakoff 1965, dowty 1979).
consider the following examples.
(18.44) john opened the door.     cause(john, become(open(door)))
(18.45) the door opened.     become(open(door))
(18.46) the door is open.     open(door)

the decompositional approach asserts that a single state-like predicate associ-
ated with open underlies all of these examples. the differences among the meanings
of these examples arises from the combination of this single predicate with the prim-
itives cause and become.

while this approach to primitive decomposition can explain the similarity be-
tween states and actions or causative and non-causative predicates, it still relies on
having a large number of predicates like open. more radical approaches choose to
break down these predicates as well. one such approach to verbal predicate de-
composition that played a role in early natural language understanding systems is
conceptual dependency (cd), a set of ten primitive predicates, shown in fig. 18.8.

primitive
atrans

ptrans
mtrans

mbuild
propel
move
ingest
expel
speak
attend

de   nition
the abstract transfer of possession or control from one entity to
another
the physical transfer of an object from one location to another
the transfer of mental concepts between entities or within an
entity
the creation of new information within an entity
the application of physical force to move an object
the integral movement of a body part by an animal
the taking in of a substance by an animal
the expulsion of something from an animal
the action of producing a sound
the action of focusing a sense organ

figure 18.8 a set of conceptual dependency primitives.

below is an example sentence along with its cd representation. the verb brought
is translated into the two primitives atrans and ptrans to indicate that the waiter
both physically conveyed the check to mary and passed control of it to her. note
that cd also associates a    xed set of thematic roles with each primitive to represent
the various participants in the action.
(18.47) the waiter brought mary the check.

conceptual
dependency

18.9

    summary

19

   x,y atrans(x)    actor(x,waiter)    ob ject(x,check)    to(x,mary)
   ptrans(y)    actor(y,waiter)    ob ject(y,check)    to(y,mary)

18.9 summary

described by the predicate.

    semantic roles are abstract models of the role an argument plays in the event
    thematic roles are a model of semantic roles based on a single    nite list of
roles. other semantic role models include per-verb semantic role lists and
proto-agent/proto-patient, both of which are implemented in propbank,
and per-frame role lists, implemented in framenet.

    id14 is the task of assigning semantic role labels to the con-
stituents of a sentence. the task is generally treated as a supervised machine
learning task, with models trained on propbank or framenet. algorithms
generally start by parsing a sentence and then automatically tag each parse
tree node with a semantic role.

    semantic selectional restrictions allow words (particularly predicates) to post
constraints on the semantic properties of their argument words. selectional
preference models (like selectional association or simple conditional proba-
bility) allow a weight or id203 to be assigned to the association between
a predicate and an argument word or class.

bibliographical and historical notes

although the idea of semantic roles dates back to p  an. ini, they were re-introduced
into modern linguistics by gruber (1965), fillmore (1966) and fillmore (1968)).
fillmore, interestingly, had become interested in argument structure by studying
lucien tesni`ere   s groundbreaking   el  ements de syntaxe structurale (tesni`ere, 1959)
in which the term    dependency    was introduced and the foundations were laid for
dependency grammar. following tesni`ere   s terminology, fillmore    rst referred to
argument roles as actants (fillmore, 1966) but quickly switched to the term case,
(see fillmore (2003)) and proposed a universal list of semantic roles or cases (agent,
patient, instrument, etc.), that could be taken on by the arguments of predicates.
verbs would be listed in the lexicon with their case frame, the list of obligatory (or
optional) case arguments.

the idea that semantic roles could provide an intermediate level of semantic
representation that could help map from syntactic parse structures to deeper, more
fully-speci   ed representations of meaning was quickly adopted in natural language
processing, and systems for extracting case frames were created for machine trans-
lation (wilks, 1973), question-answering (hendrix et al., 1973), spoken-language
understanding (nash-webber, 1975), and dialogue systems (bobrow et al., 1977).
general-purpose semantic role labelers were developed. the earliest ones (sim-
mons, 1973)    rst parsed a sentence by means of an atn (augmented transition

20 chapter 18

    id14

network) parser. each verb then had a set of rules specifying how the parse should
be mapped to semantic roles. these rules mainly made reference to grammatical
functions (subject, object, complement of speci   c prepositions) but also checked
constituent internal features such as the animacy of head nouns. later systems as-
signed roles from pre-built parse trees, again by using dictionaries with verb-speci   c
case frames (levin 1977, marcus 1980).

by 1977 case representation was widely used and taught in ai and nlp courses,
and was described as a standard of natural language understanding in the    rst edition
of winston   s (1977) textbook arti   cial intelligence.

in the 1980s fillmore proposed his model of frame semantics, later describing

the intuition as follows:

   the idea behind frame semantics is that speakers are aware of possi-
bly quite complex situation types, packages of connected expectations,
that go by various names   frames, schemas, scenarios, scripts, cultural
narratives, memes   and the words in our language are understood with
such frames as their presupposed background.    (fillmore, 2012, p. 712)

the word frame seemed to be in the air for a suite of related notions proposed at
about the same time by minsky (1974), hymes (1974), and goffman (1974), as
well as related notions with other names like scripts (schank and abelson, 1975)
and schemata (bobrow and norman, 1975) (see tannen (1979) for a comparison).
fillmore was also in   uenced by the semantic    eld theorists and by a visit to the yale
ai lab where he took notice of the lists of slots and    llers used by early information
extraction systems like dejong (1982) and schank and abelson (1977). in the 1990s
fillmore drew on these insights to begin the framenet corpus annotation project.

at the same time, beth levin drew on her early case frame dictionaries (levin,
1977) to develop her book which summarized sets of verb classes de   ned by shared
argument realizations (levin, 1993). the verbnet project built on this work (kipper
et al., 2000), leading soon afterwards to the propbank semantic-role-labeled corpus
created by martha palmer and colleagues (palmer et al., 2005).

the combination of rich linguistic annotation and corpus-based approach in-
stantiated in framenet and propbank led to a revival of automatic approaches to
id14,    rst on framenet (gildea and jurafsky, 2000) and then on
propbank data (gildea and palmer, 2002, inter alia). the problem    rst addressed in
the 1970s by hand-written rules was thus now generally recast as one of supervised
machine learning enabled by large and consistent databases. many popular features
used for role labeling are de   ned in gildea and jurafsky (2002), surdeanu et al.
(2003), xue and palmer (2004), pradhan et al. (2005), che et al. (2009), and zhao
et al. (2009). the use of dependency rather than constituency parses was introduced
in the conll-2008 shared task (surdeanu et al., 2008). for surveys see palmer
et al. (2010) and m`arquez et al. (2008).

the use of neural approachess to id14 was pioneered by col-
lobert et al. (2011), who applied a crf on top of a convolutional net. early work
like foland, jr. and martin (2015) focused on using dependency features. later
work eschewed syntactic features altogether; (zhou and xu, 2015) introduced the
use of a stacked (6-8 layer) bi-lstm architecture, and (he et al., 2017) showed
how to augment the bi-lstm architecture with id199 and also replace
the crf with a* decoding that make it possible to apply a wide variety of global
constraints in srl decoding.

most id14 schemes only work within a single sentence, fo-
cusing on the object of the verbal (or nominal, in the case of nombank) predicate.

implicit
argument

isrl

bibliographical and historical notes

21

however, in many cases, a verbal or nominal predicate may have an implicit argu-
ment: one that appears only in a contextual sentence, or perhaps not at all and must
be inferred. in the two sentences this house has a new owner. the sale was    nalized
10 days ago. the sale in the second sentence has no arg1, but a reasonable reader
would infer that the arg1 should be the house mentioned in the prior sentence. find-
ing these arguments, implicit argument detection (sometimes shortened as isrl)
was introduced by gerber and chai (2010) and ruppenhofer et al. (2010). see do
et al. (2017) for more recent neural models.

to avoid the need for huge labeled training sets, unsupervised approaches for
id14 attempt to induce the set of semantic roles by id91 over
arguments. the task was pioneered by riloff and schmelzenbach (1998) and swier
and stevenson (2004); see grenager and manning (2006), titov and klementiev
(2012), lang and lapata (2014), woodsend and lapata (2015), and titov and khod-
dam (2014).

recent innovations in frame labeling include connotation frames, which mark
richer information about the argument of predicates. connotation frames mark the
sentiment of the writer or reader toward the arguements (for example using the verb
survive in he survived a bombing expresses the writer   s sympathy toward the subject
he and negative sentiment toward the bombing. connotation frames also mark effect
(something bad happened to x), value: (x is valuable), and mental state: (x is dis-
tressed by the event) (rashkin et al. 2016, rashkin et al. 2017). connotation frames
can also mark the power differential between the arguments (using the verb implore
means that the theme argument has greater power than the agent), and the agency
of each argument (waited is low agency). fig. 18.9 shows a visualization from sap
et al. (2017).

figure 18.9 the connotation frames of sap et al. (2017), showing that the verb implore
implies the agent has lower power than the theme (in contrast, say, with a verb like demanded,
and showing the low level of agency of the subject of waited. figure from sap et al. (2017).

selectional preference has been widely studied beyond the selectional associa-
tion models of resnik (1993) and resnik (1996). methods have included cluster-
ing (rooth et al., 1999), discriminative learning (bergsma et al., 2008), and topic
models (s  eaghdha 2010, ritter et al. 2010), and constraints can be expressed at the
level of words or classes (agirre and martinez, 2001). selectional preferences have
also been successfully integrated into id14 (erk 2007, zapirain
et al. 2013, do et al. 2017).

agentthemepower(ag < th)verbimplorehe implored the tribunal to show mercy.the princess waited for her prince.agentthemeagency(ag) = -verbwaitfigure2:theformalnotationoftheconnotationframesofpowerandagency.the   rstexampleshowstherelativepowerdifferentialimpliedbytheverb   implored   ,i.e.,theagent(   he   )isinapositionoflesspowerthanthetheme(   thetri-bunal   ).incontrast,   hedemandedthetribunalshowmercy   impliesthattheagenthasauthorityoverthetheme.thesecondexampleshowsthelowlevelofagencyimpliedbytheverb   waited   .interactivedemowebsiteofour   ndings(seefig-ure5intheappendixforascreenshot).2further-more,aswillbeseeninsection4.1,connotationframesoffernewinsightsthatcomplementandde-viatefromthewell-knownbechdeltest(bechdel,1986).inparticular,we   ndthathigh-agencywomenthroughthelensofconnotationframesarerareinmodern   lms.itis,inpart,becausesomemovies(e.g.,snowwhite)accidentallypassthebechdeltestandalsobecauseevenmovieswithstrongfemalecharactersarenotentirelyfreefromthedeeplyingrainedbiasesinsocialnorms.2connotationframesofpowerandagencywecreatetwonewconnotationrelations,powerandagency(examplesinfigure3),asanexpan-sionoftheexistingconnotationframelexicons.3threeamtcrowdworkersannotatedtheverbswithplaceholderstoavoidgenderbiasinthecon-text(e.g.,xrescuedy;anexampletaskisshownintheappendixinfigure7).wede   netheanno-tatedconstructsasfollows:powerdifferentialsmanyverbsimplytheau-thoritylevelsoftheagentandthemerelativeto2http://homes.cs.washington.edu/  msap/movie-bias/.3thelexiconsandademoareavailableathttp://homes.cs.washington.edu/  msap/movie-bias/.power(ag<th)power(ag>th)agency(ag)= agency(ag)=+figure3:sampleverbsintheconnotationframeswithhighannotatoragreement.sizeisindicativeofverbfrequencyinourcorpus(bigger=morefrequent),colordifferencesareonlyforlegibility.oneanother.forexample,iftheagent   dom-inates   thetheme(denotedaspower(ag>th)),thentheagentisimpliedtohavealevelofcontroloverthetheme.alternatively,iftheagent   hon-ors   thetheme(denotedaspower(ag<th)),thewriterimpliesthatthethemeismoreimportantorauthoritative.weusedamtid104tola-bel1700transitiveverbsforpowerdifferentials.withthreeannotatorsperverb,theinter-annotatoragreementis0.34(krippendorff   s   ).agencytheagencyattributedtotheagentoftheverbdenoteswhethertheactionbeingdescribedimpliesthattheagentispowerful,decisive,andcapableofpushingforwardtheirownstoryline.forexample,apersonwhoisdescribedas   ex-periencing   thingsdoesnotseemasactiveandde-cisiveassomeonewhoisdescribedas   determin-ing   things.amtworkerslabeled2000transi-tiveverbsforimplyinghigh/moderate/lowagency(inter-annotatoragreementof0.27).wedenotehighagencyasagency(ag)=+,andlowagencyasagency(ag)= .pairwiseagreementsonahardconstraintare56%and51%forpowerandagency,respec-tively.despitethis,agreementsreach96%and94%whenmoderatelabelsarecountedasagree-ingwitheitherhighorlowlabels,showingthatan-notatorsrarelystronglydisagreewithoneanother.somecontributingfactorsinthelowerkascoresincludethesubtletyofchoosingbetweenneutral22 chapter 18

    id14

exercises

agirre, e. and martinez, d. (2001). learning class-to-class

selectional preferences. in conll-01.

baker, c. f., fillmore, c. j., and lowe, j. b. (1998). the
in coling/acl-98, mon-

berkeley framenet project.
treal, canada, pp. 86   90.

bergsma, s., lin, d., and goebel, r. (2008). discriminative
learning of selectional preference from unlabeled text. in
emnlp-08, pp. 59   68.

bloom   eld, l. (1933). language. university of chicago

press.

bobrow, d. g., kaplan, r. m., kay, m., norman, d. a.,
thompson, h., and winograd, t. (1977). gus, a frame
driven dialog system. arti   cial intelligence, 8, 155   173.

bobrow, d. g. and norman, d. a. (1975). some princi-
ples of memory schemata. in bobrow, d. g. and collins,
a. (eds.), representation and understanding. academic
press.

brockmann, c. and lapata, m. (2003). evaluating and com-
bining approaches to selectional preference acquisition. in
eacl-03, pp. 27   34.

carreras, x. and m`arquez, l. (2005).

conll-2005 shared task: id14.
conll-05, pp. 152   164.

introduction to the
in

chambers, n. and jurafsky, d. (2010).

improving the use
of pseudo-words for evaluating selectional preferences. in
acl 2010, pp. 445   453.

che, w., li, z., li, y., guo, y., qin, b., and liu, t.
(2009). multilingual dependency-based syntactic and se-
mantic parsing. in conll-09, pp. 49   54.

collobert, r., weston,

j., bottou, l., karlen, m.,
kavukcuoglu, k., and kuksa, p. (2011). natural language
processing (almost) from scratch. the journal of machine
learning research, 12, 2493   2537.

dejong, g. f. (1982). an overview of the frump system.
in lehnert, w. g. and ringle, m. h. (eds.), strategies for
natural language processing, pp. 149   176. lawrence erl-
baum.

do, q. n. t., bethard, s., and moens, m.-f. (2017). improv-
ing implicit id14 by predicting semantic
frame arguments. in ijcnlp-17.

dowty, d. r. (1979). word meaning and montague gram-

mar. d. reidel.

erk, k. (2007). a simple, similarity-based model for selec-

tional preferences. in acl-07, pp. 216   223.

fillmore, c. j. (1966). a proposal concerning english prepo-
sitions. in dinneen, f. p. (ed.), 17th annual round table.,
vol. 17 of monograph series on language and linguistics,
pp. 19   34. georgetown university press, washington d.c.
fillmore, c. j. (1968). the case for case. in bach, e. w. and
harms, r. t. (eds.), universals in linguistic theory, pp.
1   88. holt, rinehart & winston.

fillmore, c. j. (1985). frames and the semantics of under-

standing. quaderni di semantica, vi(2), 222   254.

fillmore, c. j. (2003). valency and semantic roles: the con-
cept of deep structure case. in   agel, v., eichinger, l. m.,
eroms, h. w., hellwig, p., heringer, h. j., and lobin, h.
(eds.), dependenz und valenz: ein internationales hand-
buch der zeitgen  ossischen forschung, chap. 36, pp. 457   
475. walter de gruyter.

exercises

23

fillmore, c. j. (2012). encounters with language. computa-

tional linguistics, 38(4), 701   718.

fillmore, c. j. and baker, c. f. (2009). a frames approach
to semantic analysis. in heine, b. and narrog, h. (eds.),
the oxford handbook of linguistic analysis, pp. 313   340.
oxford university press.

fillmore, c. j., johnson, c. r., and petruck, m. r. l. (2003).
background to framenet. international journal of lexicog-
raphy, 16(3), 235   250.

foland, jr., w. r. and martin, j. h. (2015). dependency-
based id14 using convolutional neural
networks. in *sem 2015), pp. 279   289.

gale, w. a., church, k. w., and yarowsky, d. (1992). work
on statistical methods for id51. in
goldman, r. (ed.), proceedings of the 1992 aaai fall
symposium on probabilistic approaches to natural lan-
guage.

gerber, m. and chai, j. y. (2010). beyond nombank: a
study of implicit arguments for nominal predicates. in acl
2010, pp. 1583   1592.

gildea, d. and jurafsky, d. (2000). automatic labeling of

semantic roles. in acl-00, hong kong, pp. 512   520.

gildea, d. and jurafsky, d. (2002). automatic labeling of se-
mantic roles. computational linguistics, 28(3), 245   288.
gildea, d. and palmer, m. (2002). the necessity of syntac-
tic parsing for predicate argument recognition. in acl-02,
philadelphia, pa.

goffman, e. (1974). frame analysis: an essay on the orga-

nization of experience. harvard university press.

grenager, t. and manning, c. d. (2006). unsupervised dis-

covery of a statistical verb lexicon. in emnlp 2006.

gruber, j. s. (1965). studies in lexical relations. ph.d.

thesis, mit.

he, l., lee, k., lewis, m., and zettlemoyer, l. (2017). deep
in

id14: what works and what   s next.
acl 2017, pp. 473   483.

hendrix, g. g., thompson, c. w., and slocum, j. (1973).
language processing via canonical verbs and semantic
models. in proceedings of ijcai-73.

hirst, g. (1987). semantic interpretation and the resolution

of ambiguity. cambridge university press.

hymes, d. (1974). ways of speaking.

in bauman, r.
and sherzer, j. (eds.), explorations in the ethnography of
speaking, pp. 433   451. cambridge university press.

johnson-laird, p. n. (1983). mental models. harvard uni-

versity press, cambridge, ma.

katz, j. j. and fodor, j. a. (1963). the structure of a seman-

tic theory. language, 39, 170   210.

keller, f. and lapata, m. (2003). using the web to obtain fre-
quencies for unseen bigrams. computational linguistics,
29, 459   484.

kipper, k., dang, h. t., and palmer, m. (2000). class-based
construction of a verb lexicon. in aaai-00, austin, tx, pp.
691   696.

kullback, s. and leibler, r. a. (1951). on information and
suf   ciency. annals of mathematical statistics, 22, 79   86.
lakoff, g. (1965). on the nature of syntactic irregularity.
ph.d. thesis, indiana university. published as irregularity
in syntax. holt, rinehart, and winston, new york, 1970.

24 chapter 18     id14

lang, j. and lapata, m. (2014). similarity-driven semantic
role induction via graph partitioning. computational lin-
guistics, 40(3), 633   669.

ritter, a., etzioni, o., and mausam (2010). a latent dirich-
let allocation method for selectional preferences. in acl
2010, pp. 424   434.

levin, b. (1977). mapping sentences to case frames. tech.

rep. 167, mit ai laboratory. ai working paper 143.

levin, b. (1993). english verb classes and alternations: a

preliminary investigation. university of chicago press.

levin, b. and rappaport hovav, m. (2005). argument real-

ization. cambridge university press.

marcus, m. p. (1980). a theory of syntactic recognition for

natural language. mit press.

m`arquez, l., carreras, x., litkowski, k. c., and stevenson,
s. (2008). id14: an introduction to the
special issue. computational linguistics, 34(2), 145   159.
meyers, a., reeves, r., macleod, c., szekely, r., zielin-
ska, v., young, b., and grishman, r. (2004). the nom-
bank project: an interim report.
in proceedings of the
naacl/hlt workshop: frontiers in corpus annotation.

minsky, m. (1974). a framework for representing knowl-

edge. tech. rep. 306, mit ai laboratory. memo 306.

nash-webber, b. l. (1975). the role of semantics in auto-
matic speech understanding. in bobrow, d. g. and collins,
a. (eds.), representation and understanding, pp. 351   
382. academic press.

palmer, m., gildea, d., and xue, n. (2010). semantic role
labeling. synthesis lectures on human language tech-
nologies, 3(1), 1   103.

palmer, m., kingsbury, p., and gildea, d. (2005). the propo-
sition bank: an annotated corpus of semantic roles. com-
putational linguistics, 31(1), 71   106.
penn, g. and kiparsky, p. (2012). on p  an. ini and the gen-
erative capacity of contextualized replacement systems. in
coling-12, pp. 943   950.

pradhan, s., moschitti, a., xue, n., ng, h. t., bj  orkelund,
a., uryupina, o., zhang, y., and zhong, z. (2013). to-
wards robust linguistic analysis using ontonotes.
in
conll-13, pp. 143   152.

pradhan, s., ward, w., hacioglu, k., martin, j. h., and ju-
rafsky, d. (2005). id14 using different
syntactic views. in acl-05, ann arbor, mi.

rashkin, h., bell, e., choi, y., and volkova, s. (2017). mul-
tilingual connotation frames: a case study on social media
for targeted id31 and forecast. in acl 2017,
pp. 459   464.

rashkin, h., singh, s., and choi, y. (2016). connotation
frames: a data-driven investigation. in acl 2016, pp. 311   
321.

resnik, p. (1993). semantic classes and syntactic ambigu-
ity. in proceedings of the workshop on human language
technology, pp. 278   283.

resnik, p. (1996). selectional constraints: an information-
theoretic model and its computational realization. cogni-
tion, 61, 127   159.

riloff, e. and schmelzenbach, m. (1998). an empirical ap-
proach to conceptual case frame acquisition. in proceed-
ings of the sixth workshop on very large corpora, mon-
treal, canada, pp. 49   56.

rooth, m., riezler, s., prescher, d., carroll, g., and beil,
f. (1999). inducing a semantically annotated lexicon via
em-based id91. in acl-99, college park, ma, pp.
104   111.

ruppenhofer, j., ellsworth, m., petruck, m. r. l., johnson,
c. r., baker, c. f., and scheffczyk, j. (2016). framenet
ii: extended theory and practice..

ruppenhofer, j., sporleder, c., morante, r., baker, c., and
palmer, m. (2010). semeval-2010 task 10: linking events
and their participants in discourse. in proceedings of the
5th international workshop on semantic evaluation, pp.
45   50.

sap, m., prasettio, m. c., holtzman, a., rashkin, h., and
choi, y. (2017). connotation frames of power and agency
in modern    lms. in emnlp 2017, pp. 2329   2334.

schank, r. c. and abelson, r. p. (1975). scripts, plans, and

knowledge. in proceedings of ijcai-75, pp. 151   157.

schank, r. c. and abelson, r. p. (1977). scripts, plans,

goals and understanding. lawrence erlbaum.

sch  utze, h. (1992). context space. in goldman, r. (ed.),
proceedings of the 1992 aaai fall symposium on proba-
bilistic approaches to natural language.

s  eaghdha, d. o. (2010). latent variable models of selec-

tional preference. in acl 2010, pp. 435   444.

simmons, r. f. (1973). semantic networks: their com-
putation and use for understanding english sentences. in
schank, r. c. and colby, k. m. (eds.), computer models
of thought and language, pp. 61   113. w.h. freeman and
co.

surdeanu, m., harabagiu, s., williams, j., and aarseth, p.
(2003). using predicate-argument structures for informa-
tion extraction. in acl-03, pp. 8   15.

surdeanu, m., johansson, r., meyers, a., m`arquez, l., and
nivre, j. (2008). the conll-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. in conll-08,
pp. 159   177.

swier, r. and stevenson, s. (2004). unsupervised semantic

role labelling. in emnlp 2004, pp. 95   102.

tannen, d. (1979). what   s in a frame? surface evidence for
underlying expectations. in freedle, r. (ed.), new direc-
tions in discourse processing, pp. 137   181. ablex.

tesni`ere, l. (1959).

  el  ements de syntaxe structurale. li-

brairie c. klincksieck, paris.

titov, i. and khoddam, e. (2014). unsupervised induction of
semantic roles within a reconstruction-error minimization
framework. in naacl hlt 2015.

titov, i. and klementiev, a. (2012). a bayesian approach
to unsupervised semantic role induction. in eacl-12, pp.
12   22.

wilks, y. (1973). an arti   cial intelligence approach to ma-
chine translation. in schank, r. c. and colby, k. m. (eds.),
computer models of thought and language, pp. 114   151.
w.h. freeman.

wilks, y. (1975a). preference semantics. in keenan, e. l.
(ed.), the formal semantics of natural language, pp.
329   350. cambridge univ. press.

exercises

25

wilks, y. (1975b). a preferential, pattern-seeking, semantics
for natural language id136. arti   cial intelligence, 6(1),
53   74.

winston, p. h. (1977). arti   cial intelligence. addison wes-

ley.

woodsend, k. and lapata, m. (2015). distributed represen-
tations for unsupervised id14. in emnlp
2015, pp. 2482   2491.

xue, n. and palmer, m. (2004). calibrating features for se-

mantic role labeling. in emnlp 2004.

zapirain, b., agirre, e., m`arquez, l., and surdeanu, m.
(2013). selectional preferences for semantic role classi-
   cation. computational linguistics, 39(3), 631   663.

zhao, h., chen, w., kit, c., and zhou, g. (2009). mul-
tilingual dependency learning: a huge feature engineering
method to semantic id33. in conll-09, pp.
55   60.

zhou, j. and xu, w. (2015). end-to-end learning of seman-
tic role labeling using recurrent neural networks. in acl
2015, pp. 1127   1137.

