speech and language processing. daniel jurafsky & james h. martin.
rights reserved.

draft of september 23, 2018.

copyright c(cid:13) 2018.

all

chapter

2 id157, text

id172, id153

you are like my father in some ways.

user:
eliza: what resemblance do you see
user:

you are not very aggressive but i think you don   t want me to
notice that.

eliza

chatbots

text
id172

id121

you don   t argue with me.

eliza: what makes you think i am not very aggressive
user:
eliza: why do you think i don   t argue with you
user:
eliza: does it please you to believe i am afraid of you

you are afraid of me.

weizenbaum (1966)
the dialogue above is from eliza, an early natural language processing sys-
tem that could carry on a limited conversation with a user by imitating the responses
of a rogerian psychotherapist (weizenbaum, 1966). eliza is a surprisingly simple
program that uses pattern matching to recognize phrases like    you are x    and trans-
late them into suitable outputs like    what makes you think i am x?   . this simple
technique succeeds in this domain because eliza doesn   t actually need to know
anything to mimic a rogerian psychotherapist. as weizenbaum notes, this is one
of the few dialogue genres where listeners can act as if they know nothing of the
world. eliza   s mimicry of human conversation was remarkably successful: many
people who interacted with eliza came to believe that it really understood them
and their problems, many continued to believe in eliza   s abilities even after the
program   s operation was explained to them (weizenbaum, 1976), and even today
such chatbots are a fun diversion.

of course modern conversational agents are much more than a diversion; they
can answer questions, book    ights, or    nd restaurants, functions for which they rely
on a much more sophisticated understanding of the user   s intent, as we will see in
chapter 24. nonetheless, the simple pattern-based methods that powered eliza
and other chatbots play a crucial role in natural language processing.

we   ll begin with the most important tool for describing text patterns: the regular
expression. id157 can be used to specify strings we might want to
extract from a document, from transforming    you are x    in eliza above, to de   ning
strings like $199 or $24.99 for extracting tables of prices from a document.

we   ll then turn to a set of tasks collectively called text id172, in which
id157 play an important part. normalizing text means converting it
to a more convenient, standard form. for example, most of what we are going to
do with language relies on    rst separating out or tokenizing words from running
text, the task of id121. english words are often separated from each other
by whitespace, but whitespace is not always suf   cient. new york and rock    n    roll
are sometimes treated as large words despite the fact that they contain spaces, while
sometimes we   ll need to separate i   m into the two words i and am. for processing
tweets or texts we   ll need to tokenize emoticons like :) or hashtags like #nlproc.
some languages, like chinese, don   t have spaces between words, so word tokeniza-
tion becomes more dif   cult.

2 chapter 2

    id157, text id172, id153

lemmatization

id30

sentence
segmentation

another part of text id172 is lemmatization, the task of determining
that two words have the same root, despite their surface differences. for example,
the words sang, sung, and sings are forms of the verb sing. the word sing is the
common lemma of these words, and a lemmatizer maps from all of these to sing.
lemmatization is essential for processing morphologically complex languages like
arabic. id30 refers to a simpler version of lemmatization in which we mainly
just strip suf   xes from the end of the word. text id172 also includes sen-
tence segmentation: breaking up a text into individual sentences, using cues like
periods or exclamation points.

finally, we   ll need to compare words and other strings. we   ll introduce a metric
called id153 that measures how similar two strings are based on the number
of edits (insertions, deletions, substitutions) it takes to change one string into the
other. id153 is an algorithm with applications throughout language process-
ing, from id147 to id103 to coreference resolution.

2.1 id157

regular
expression

corpus

sir andrew: her c   s, her u   s and her t   s: why that?
shakespeare, twelfth night
one of the unsung successes in standardization in computer science has been the
regular expression (re), a language for specifying text search strings. this prac-
tical language is used in every computer language, word processor, and text pro-
cessing tools like the unix tools grep or emacs. formally, a regular expression is
an algebraic notation for characterizing a set of strings. they are particularly use-
ful for searching in texts, when we have a pattern to search for and a corpus of
texts to search through. a regular expression search function will search through the
corpus, returning all texts that match the pattern. the corpus can be a single docu-
ment or a collection. for example, the unix command-line tool grep takes a regular
expression and returns every line of the input document that matches the expression.
a search can be designed to return every match on a line, if there are more than
one, or just the    rst match. in the following examples we generally underline the
exact part of the pattern that matches the regular expression and show only the    rst
match. we   ll show id157 delimited by slashes but note that slashes are
not part of the id157.

id157 come in many variants. we   ll be describing extended regu-
lar expressions; different regular expression parsers may only recognize subsets of
these, or treat some expressions slightly differently. using an online regular expres-
sion tester is a handy way to test out your expressions and explore these variations.

2.1.1 basic regular expression patterns
the simplest kind of regular expression is a sequence of simple characters. to search
for woodchuck, we type /woodchuck/. the expression /buttercup/ matches any
string containing the substring buttercup; grep with that expression would return the
line i   m called little buttercup. the search string can consist of a single character
(like /!/) or a sequence of characters (like /urgl/).

id157 are case sensitive; lower case /s/ is distinct from upper
case /s/ (/s/ matches a lower case s but not an upper case s). this means that
the pattern /woodchucks/ will not match the string woodchucks. we can solve this

2.1

    id157

3

re
/woodchucks/
/a/
/!/
figure 2.1 some simple regex searches.

example patterns matched
   interesting links to woodchucks and lemurs   
   mary ann stopped by mona   s   
   you   ve left the burglar behind again!    said nori

problem with the use of the square braces [ and ]. the string of characters inside the
braces speci   es a disjunction of characters to match. for example, fig. 2.2 shows
that the pattern /[ww]/ matches patterns containing either w or w.

match

re
/[ww]oodchuck/ woodchuck or woodchuck
/[abc]/
/[1234567890]/

   a   ,    b   , or    c   
any digit

example patterns
   woodchuck   
   in uomini, in soldati   
   plenty of 7 to 5   

figure 2.2 the use of the brackets [] to specify a disjunction of characters.

the regular expression /[1234567890]/ speci   ed any single digit. while such
classes of characters as digits or letters are important building blocks in expressions,
they can get awkward (e.g., it   s inconvenient to specify

/[abcdefghijklmnopqrstuvwxyz]/

range

to mean    any capital letter   ). in cases where there is a well-de   ned sequence asso-
ciated with a set of characters, the brackets can be used with the dash (-) to specify
any one character in a range. the pattern /[2-5]/ speci   es any one of the charac-
ters 2, 3, 4, or 5. the pattern /[b-g]/ speci   es one of the characters b, c, d, e, f, or
g. some other examples are shown in fig. 2.3.

re
/[a-z]/
/[a-z]/
/[0-9]/

match
an upper case letter
a lower case letter
a single digit

example patterns matched
   we should call it    drenched blossoms       
   my beans were impatient to be hoed!   
   chapter 1: down the rabbit hole   

figure 2.3 the use of the brackets [] plus the dash - to specify a range.

the square braces can also be used to specify what a single character cannot be,
by use of the caret   . if the caret    is the    rst symbol after the open square brace [,
the resulting pattern is negated. for example, the pattern /[  a]/ matches any single
character (including special characters) except a. this is only true when the caret
is the    rst symbol after the open square brace. if it occurs anywhere else, it usually
stands for a caret; fig. 2.4 shows some examples.

re
/[  a-z]/
/[  ss]/
/[  \.]/
/[e  ]/
/a  b/

match (single characters)
not an upper case letter
neither    s    nor    s   
not a period
either    e    or         
the pattern    a  b   

example patterns matched
   oyfn pripetchik   
   i have no exquisite reason for   t   
   our resident djinn   
   look up    now   
   look up a   b now   

figure 2.4 the caret    for negation or just to mean   . see below re: the backslash for escaping the period.

how can we talk about optional elements, like an optional s in woodchuck and
woodchucks? we can   t use the square brackets, because while they allow us to say
   s or s   , they don   t allow us to say    s or nothing   . for this we use the question mark
/?/, which means    the preceding character or nothing   , as shown in fig. 2.5.

4 chapter 2

    id157, text id172, id153

re
/woodchucks?/
/colou?r/

example patterns matched
   woodchuck   
   colour   
figure 2.5 the question mark ? marks optionality of the previous expression.

match
woodchuck or woodchucks
color or colour

we can think of the question mark as meaning    zero or one instances of the
previous character   . that is, it   s a way of specifying how many of something that
we want, something that is very important in id157. for example,
consider the language of certain sheep, which consists of strings that look like the
following:

baa!
baaa!
baaaa!
baaaaa!
. . .

kleene *

kleene +

this language consists of strings with a b, followed by at least two a   s, followed
by an exclamation point. the set of operators that allows us to say things like    some
number of as    are based on the asterisk or *, commonly called the kleene * (gen-
erally pronounced    cleany star   ). the kleene star means    zero or more occurrences
of the immediately previous character or regular expression   . so /a*/ means    any
string of zero or more as   . this will match a or aaaaaa, but it will also match off
minor since the string off minor has zero a   s. so the regular expression for matching
one or more a is /aa*/, meaning one a followed by zero or more as. more complex
patterns can also be repeated. so /[ab]*/ means    zero or more a   s or b   s    (not
   zero or more right square braces   ). this will match strings like aaaa or ababab or
bbbb.

for specifying multiple digits (useful for    nding prices) we can extend /[0-9]/,
the regular expression for a single digit. an integer (a string of digits) is thus
/[0-9][0-9]*/. (why isn   t it just /[0-9]*/?)

sometimes it   s annoying to have to write the regular expression for digits twice,
so there is a shorter way to specify    at least one    of some character. this is the
kleene +, which means    one or more occurrences of the immediately preceding
character or regular expression   . thus, the expression /[0-9]+/ is the normal way
to specify    a sequence of digits   . there are thus two ways to specify the sheep
language: /baaa*!/ or /baa+!/.

one very important special character is the period (/./), a wildcard expression

that matches any single character (except a carriage return), as shown in fig. 2.6.

re
/beg.n/

match
any character between beg and n

example matches
begin, beg   n, begun

figure 2.6 the use of the period . to specify any character.

the wildcard is often used together with the kleene star to mean    any string of
characters   . for example, suppose we want to    nd any line in which a particular
word, for example, aardvark, appears twice. we can specify this with the regular
expression /aardvark.*aardvark/.

anchors are special characters that anchor id157 to particular places
in a string. the most common anchors are the caret    and the dollar sign $. the caret
   matches the start of a line. the pattern /  the/ matches the word the only at the

anchors

2.1

    id157

5

start of a line. thus, the caret    has three uses: to match the start of a line, to in-
dicate a negation inside of square brackets, and just to mean a caret. (what are the
contexts that allow grep or python to know which function a given caret is supposed
to have?) the dollar sign $ matches the end of a line. so the pattern (cid:32)$ is a useful
pattern for matching a space at the end of a line, and /  the dog\.$/ matches a
line that contains only the phrase the dog. (we have to use the backslash here since
we want the . to mean    period    and not the wildcard.)

there are also two other anchors: \b matches a word boundary, and \b matches
a non-boundary. thus, /\bthe\b/ matches the word the but not the word other.
more technically, a    word    for the purposes of a regular expression is de   ned as any
sequence of digits, underscores, or letters; this is based on the de   nition of    words   
in programming languages. for example, /\b99\b/ will match the string 99 in
there are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in
there are 299 bottles of beer on the wall (since 99 follows a number). but it will
match 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore,
or letter).

2.1.2 disjunction, grouping, and precedence

suppose we need to search for texts about pets; perhaps we are particularly interested
in cats and dogs. in such a case, we might want to search for either the string cat or
the string dog. since we can   t use the square brackets to search for    cat or dog    (why
can   t we say /[catdog]/?), we need a new operator, the disjunction operator, also
called the pipe symbol |. the pattern /cat|dog/ matches either the string cat or
the string dog.

sometimes we need to use this disjunction operator in the midst of a larger se-
quence. for example, suppose i want to search for information about pet    sh for
my cousin david. how can i specify both guppy and guppies? we cannot simply
say /guppy|ies/, because that would match only the strings guppy and ies. this
is because sequences like guppy take precedence over the disjunction operator |.
to make the disjunction operator apply only to a speci   c pattern, we need to use the
parenthesis operators ( and ). enclosing a pattern in parentheses makes it act like
a single character for the purposes of neighboring operators like the pipe | and the
kleene*. so the pattern /gupp(y|ies)/ would specify that we meant the disjunc-
tion only to apply to the suf   xes y and ies.

the parenthesis operator ( is also useful when we are using counters like the
kleene*. unlike the | operator, the kleene* operator applies by default only to
a single character, not to a whole sequence. suppose we want to match repeated
instances of a string. perhaps we have a line that has column labels of the form
column 1 column 2 column 3. the expression /column(cid:32)[0-9]+(cid:32)*/ will not
match any number of columns; instead, it will match a single column followed by
any number of spaces! the star here applies only to the space (cid:32) that precedes it,
not to the whole sequence. with the parentheses, we could write the expression
/(column(cid:32)[0-9]+(cid:32)*)*/ to match the word column, followed by a number and
optional spaces, the whole pattern repeated any number of times.

this idea that one operator may take precedence over another, requiring us to
sometimes use parentheses to specify what we mean, is formalized by the operator
precedence hierarchy for id157. the following table gives the order
of re operator precedence, from highest precedence to lowest precedence.

disjunction

precedence

operator
precedence

6 chapter 2

    id157, text id172, id153

parenthesis
counters
sequences and anchors
disjunction

()
* + ? {}

the   my end$

|

thus,

because

than sequences,
/the*/ matches theeeee but not thethe. because sequences have a higher prece-
dence than disjunction, /the|any/ matches the or any but not theny.

a higher precedence

counters have

patterns can be ambiguous in another way. consider the expression /[a-z]*/
when matching against the text once upon a time. since /[a-z]*/ matches zero or
more letters, this expression could match nothing, or just the    rst letter o, on, onc,
or once. in these cases id157 always match the largest string they can;
we say that patterns are greedy, expanding to cover as much of a string as they can.
there are, however, ways to enforce non-greedy matching, using another mean-
ing of the ? quali   er. the operator *? is a kleene star that matches as little text as
possible. the operator +? is a kleene plus that matches as little text as possible.

greedy
non-greedy
*?
+?

2.1.3 a simple example
suppose we wanted to write a re to    nd cases of the english article the. a simple
(but incorrect) pattern might be:

/the/

one problem is that this pattern will miss the word when it begins a sentence

and hence is capitalized (i.e., the). this might lead us to the following pattern:

/[tt]he/

but we will still incorrectly return texts with the embedded in other words (e.g.,
other or theology). so we need to specify that we want instances with a word bound-
ary on both sides:

/\b[tt]he\b/

suppose we wanted to do this without the use of /\b/. we might want this since
/\b/ won   t treat underscores and numbers as word boundaries; but we might want
to    nd the in some context where it might also have underlines or numbers nearby
(the or the25). we need to specify that we want instances in which there are no
alphabetic letters on either side of the the:

/[  a-za-z][tt]he[  a-za-z]/

but there is still one more problem with this pattern: it won   t    nd the word the
when it begins a line. this is because the regular expression [  a-za-z], which
we used to avoid embedded instances of the, implies that there must be some single
(although non-alphabetic) character before the the. we can avoid this by specify-
ing that before the the we require either the beginning-of-line or a non-alphabetic
character, and the same at the end of the line:

/(  |[  a-za-z])[tt]he([  a-za-z]|$)/

false positives
false negatives

the process we just went through was based on    xing two kinds of errors: false
positives, strings that we incorrectly matched like other or there, and false nega-
tives, strings that we incorrectly missed, like the. addressing these two kinds of

2.1

    id157

7

errors comes up again and again in implementing speech and language processing
systems. reducing the overall error rate for an application thus involves two antag-
onistic efforts:

    increasing precision (minimizing false positives)
    increasing recall (minimizing false negatives)

2.1.4 a more complex example
let   s try out a more signi   cant example of the power of res. suppose we want to
build an application to help a user buy a computer on the web. the user might want
   any machine with at least 6 ghz and 500 gb of disk space for less than $1000   .
to do this kind of retrieval, we    rst need to be able to look for expressions like 6
ghz or 500 gb or mac or $999.99. in the rest of this section we   ll work out some
simple id157 for this task.

first, let   s complete our regular expression for prices. here   s a regular expres-

sion for a dollar sign followed by a string of digits:

/$[0-9]+/

note that the $ character has a different function here than the end-of-line function
we discussed earlier. most regular expression parsers are smart enough to realize
that $ here doesn   t mean end-of-line. (as a thought experiment, think about how
regex parsers might    gure out the function of $ from the context.)

now we just need to deal with fractions of dollars. we   ll add a decimal point

and two digits afterwards:

/$[0-9]+\.[0-9][0-9]/

this pattern only allows $199.99 but not $199. we need to make the cents

optional and to make sure we   re at a word boundary:

/(  |\w)$[0-9]+(\.[0-9][0-9])?\b/

one last catch! this pattern allows prices like $199999.99 which would be far

too expensive! we need to limit the dollar

/(  |\w)$[0-9]{0,3}(\.[0-9][0-9])?\b/

how about speci   cations for > 6ghz processor speed? here   s a pattern for that:

/\b[6-9]+(cid:32)*(ghz|[gg]igahertz)\b/

note that we use /(cid:32)*/ to mean    zero or more spaces    since there might always
be extra spaces lying around. for disk space, we   ll need to allow for optional frac-
tions again (5.5 gb); note the use of ? for making the    nal s optional:

/\b[0-9]+(\.[0-9]+)?(cid:32)*(gb|[gg]igabytes?)\b/

modifying this regular expression so that it only matches more than 500 gb is

left as an exercise for the reader.

2.1.5 more operators
figure 2.7 shows some aliases for common ranges, which can be used mainly to
save typing. besides the kleene * and kleene + we can also use explicit numbers as

8 chapter 2

    id157, text id172, id153

counters, by enclosing them in curly brackets. the regular expression /{3}/ means
   exactly 3 occurrences of the previous character or expression   . so /a\.{24}z/
will match a followed by 24 dots followed by z (but not a followed by 23 or 25 dots
followed by a z).

re
\d
\d
\w
\w
\s
\s

match
any digit
any non-digit
any alphanumeric/underscore
a non-alphanumeric
whitespace (space, tab)
non-whitespace
figure 2.7 aliases for common sets of characters.

expansion
[0-9]
[  0-9]
[a-za-z0-9_]
[  \w]
[(cid:32)\r\t\n\f]
[  \s]

first matches
party(cid:32)of(cid:32)5
blue(cid:32)moon
daiyu
!!!!

in(cid:32)concord

a range of numbers can also be speci   ed. so /{n,m}/ speci   es from n to m
occurrences of the previous char or expression, and /{n,}/ means at least n occur-
rences of the previous expression. res for counting are summarized in fig. 2.8.

re
*
+
?
{n}
{n,m}
{n,}
{,m}

match
zero or more occurrences of the previous char or expression
one or more occurrences of the previous char or expression
exactly zero or one occurrence of the previous char or expression
n occurrences of the previous char or expression
from n to m occurrences of the previous char or expression
at least n occurrences of the previous char or expression
up to m occurrences of the previous char or expression

figure 2.8 regular expression operators for counting.

newline

finally, certain special characters are referred to by special notation based on the
backslash (\) (see fig. 2.9). the most common of these are the newline character
\n and the tab character \t. to refer to characters that are special themselves (like
., *, [, and \), precede them with a backslash, (i.e., /\./, /\*/, /\[/, and /\\/).

re
\*
\.
\?
\n
\t

match
an asterisk    *   
a period    .   
a question mark
a newline
a tab

first patterns matched
   k*a*p*l*a*n   
   dr. livingston, i presume   
   why don   t they come and lend a hand?   

figure 2.9 some characters that need to be backslashed.

substitution

2.1.6 regular expression substitution, capture groups, and eliza
an important use of id157 is in substitutions. for example, the substi-
tution operator s/regexp1/pattern/ used in python and in unix commands like
vim or sed allows a string characterized by a regular expression to be replaced by
another string:

s/colour/color/

it is often useful to be able to refer to a particular subpart of the string matching
the    rst pattern. for example, suppose we wanted to put angle brackets around all

2.1

    id157

9

integers in a text, for example, changing the 35 boxes to the <35> boxes. we   d
like a way to refer to the integer we   ve found so that we can easily add the brackets.
to do this, we put parentheses ( and ) around the    rst pattern and use the number
operator \1 in the second pattern to refer back. here   s how it looks:

s/([0-9]+)/<\1>/

the parenthesis and number operators can also specify that a certain string or
expression must occur twice in the text. for example, suppose we are looking for
the pattern    the xer they were, the xer they will be   , where we want to constrain
the two x   s to be the same string. we do this by surrounding the    rst x with the
parenthesis operator, and replacing the second x with the number operator \1, as
follows:

/the (.*)er they were, the \1er they will be/

here the \1 will be replaced by whatever string matched the    rst item in paren-
theses. so this will match the bigger they were, the bigger they will be but not the
bigger they were, the faster they will be.

this use of parentheses to store a pattern in memory is called a capture group.
every time a capture group is used (i.e., parentheses surround a pattern), the re-
sulting match is stored in a numbered register. if you match two different sets of
parentheses, \2 means whatever matched the second capture group. thus

/the (.*)er they (.*), the \1er we \2/
will match the faster they ran, the faster we ran but not the faster they ran, the faster
we ate. similarly, the third capture group is stored in \3, the fourth is \4, and so on.
parentheses thus have a double function in id157; they are used to
group terms for specifying the order in which operators should apply, and they are
used to capture something in a register. occasionally we might want to use parenthe-
ses for grouping, but don   t want to capture the resulting pattern in a register. in that
case we use a non-capturing group, which is speci   ed by putting the commands
?: after the open paren, in the form (?: pattern ).

/(?:some|a few) (people|cats) like some \1/
will match some cats like some cats but not some cats like some a few.

substitutions and capture groups are very useful in implementing simple chat-
bots like eliza (weizenbaum, 1966). recall that eliza simulates a rogerian
psychologist by carrying on conversations like the following:

capture group

register

non-capturing
group

they   re always bugging us about something or other.

user1: men are all alike.
eliza1: in what way
user2:
eliza2: can you think of a specific example
user3: well, my boyfriend made me come here.
eliza3: your boyfriend made you come here
user4:
eliza4: i am sorry to hear you are depressed

he says i   m depressed much of the time.

eliza works by having a series or cascade of regular expression substitutions
each of which matches and changes some part of the input lines. input lines are
   rst uppercased. the    rst substitutions then change all instances of my to your,
and i   m to you are, and so on. the next set of substitutions matches and replaces
other patterns in the input. here are some examples:

10 chapter 2

    id157, text id172, id153

s/.* i   m (depressed|sad) .*/i am sorry to hear you are \1/
s/.* i am (depressed|sad) .*/why do you think you are \1/
s/.* all .*/in what way/
s/.* always .*/can you think of a specific example/

since multiple substitutions can apply to a given input, substitutions are assigned
a rank and applied in order. creating patterns is the topic of exercise 2.3, and we
return to the details of the eliza architecture in chapter 24.

2.1.7 lookahead assertions
finally, there will be times when we need to predict the future: look ahead in the
text to see if some pattern matches, but not advance the match cursor, so that we can
then deal with the pattern if it occurs.

these lookahead assertions make use of the (? syntax that we saw in the previ-
ous section for non-capture groups. the operator (?= pattern) is true if pattern
occurs, but is zero-width, i.e.
the match pointer doesn   t advance. the operator
(?! pattern) only returns true if a pattern does not match, but again is zero-width
and doesn   t advance the cursor. negative lookahead is commonly used when we
are parsing some complex pattern but want to rule out a special case. for example
suppose we want to match, at the beginning of a line, any single word that doesn   t
start with    volcano   . we can use negative lookahead to do this:

/  (?!volcano)[a-za-z]+/

lookahead

zero-width

2.2 words

corpus
corpora

before we talk about processing words, we need to decide what counts as a word.
let   s start by looking at one particular corpus (plural corpora), a computer-readable
collection of text or speech. for example the brown corpus is a million-word col-
lection of samples from 500 written english texts from different genres (newspa-
per,    ction, non-   ction, academic, etc.), assembled at brown university in 1963   64
(ku  cera and francis, 1967). how many words are in the following brown sentence?

he stepped out into the hall, was delighted to encounter a water brother.

this sentence has 13 words if we don   t count punctuation marks as words, 15
if we count punctuation. whether we treat period (   .   ), comma (   ,   ), and so on as
words depends on the task. punctuation is critical for    nding boundaries of things
(commas, periods, colons) and for identifying some aspects of meaning (question
marks, exclamation marks, quotation marks). for some tasks, like part-of-speech
tagging or parsing or id133, we sometimes treat punctuation marks as if
they were separate words.

the switchboard corpus of american english telephone conversations between
strangers was collected in the early 1990s; it contains 2430 conversations averaging
6 minutes each, totaling 240 hours of speech and about 3 million words (godfrey
et al., 1992). such corpora of spoken language don   t have punctuation but do intro-
duce other complications with regard to de   ning words. let   s look at one utterance
from switchboard; an utterance is the spoken correlate of a sentence:

i do uh main- mainly business data processing

utterance

2.2

    words

11

dis   uency
fragment
   lled pause

lemma

wordform

word type

word token

this utterance has two kinds of dis   uencies. the broken-off word main- is
called a fragment. words like uh and um are called    llers or    lled pauses. should
we consider these to be words? again, it depends on the application. if we are
building a speech transcription system, we might want to eventually strip out the
dis   uencies.

but we also sometimes keep dis   uencies around. dis   uencies like uh or um
are actually helpful in id103 in predicting the upcoming word, because
they may signal that the speaker is restarting the clause or idea, and so for speech
recognition they are treated as regular words. because people use different dis   u-
encies they can also be a cue to speaker identi   cation. in fact clark and fox tree
(2002) showed that uh and um have different meanings. what do you think they are?
are capitalized tokens like they and uncapitalized tokens like they the same
word? these are lumped together in some tasks (id103), while for part-
of-speech or named-entity tagging, capitalization is a useful feature and is retained.
how about in   ected forms like cats versus cat? these two words have the same
lemma cat but are different wordforms. a lemma is a set of lexical forms having
the same stem, the same major part-of-speech, and the same word sense. the word-
form is the full in   ected or derived form of the word. for morphologically complex
languages like arabic, we often need to deal with lemmatization. for many tasks in
english, however, wordforms are suf   cient.

how many words are there in english? to answer this question we need to
distinguish two ways of talking about words. types are the number of distinct words
in a corpus; if the set of words in the vocabulary is v , the number of types is the
vocabulary size |v|. tokens are the total number n of running words. if we ignore
punctuation, the following brown sentence has 16 tokens and 14 types:

they picnicked by the pool, then lay back on the grass and looked at the stars.

when we speak about the number of words in the language, we are generally

referring to word types.

corpus
shakespeare
brown corpus
switchboard telephone conversations
coca
google id165s

tokens = n types = |v|
884 thousand 31 thousand
1 million 38 thousand
2.4 million 20 thousand
2 million
440 million
1 trillion
13 million

figure 2.10 rough numbers of types and tokens for some english language corpora. the
largest, the google id165s corpus, contains 13 million types, but this count only includes
types appearing 40 or more times, so the true number would be much larger.

fig. 2.10 shows the rough numbers of types and tokens computed from some
popular english corpora. the larger the corpora we look at, the more word types
we    nd, and in fact this relationship between the number of types |v| and number
of tokens n is called herdan   s law (herdan, 1960) or heaps    law (heaps, 1978)
after its discoverers (in linguistics and information retrieval respectively). it is shown
in eq. 2.1, where k and    are positive constants, and 0 <    < 1.

herdan   s law
heaps    law

|v| = kn  

(2.1)

the value of    depends on the corpus size and the genre, but at least for the
large corpora in fig. 2.10,    ranges from .67 to .75. roughly then we can say that

12 chapter 2

    id157, text id172, id153

the vocabulary size for a text goes up signi   cantly faster than the square root of its
length in words.

another measure of the number of words in the language is the number of lem-
mas instead of wordform types. dictionaries can help in giving lemma counts; dic-
tionary entries or boldface forms are a very rough upper bound on the number of
lemmas (since some lemmas have multiple boldface forms). the 1989 edition of the
oxford english dictionary had 615,000 entries.

2.3 corpora

words don   t appear out of nowhere. any particular piece of text that we study
is produced by one or more speci   c speakers or writers, in a speci   c dialect of a
speci   c language, at a speci   c time, in a speci   c place, for a speci   c function.

perhaps the most important dimension of variation is the language. nlp algo-
rithms are most useful when they apply across many languages. the world has 7097
languages at the time of this writing, according to the online ethnologue catalog
(simons and fennig, 2018). most nlp tools tend to be developed for the of   cial
languages of large industrialized nations (chinese, english, spanish, arabic, etc.),
but we don   t want to limit tools to just these few languages. furthermore, most lan-
guages also have multiple varieties, such as dialects spoken in different regions or
by different social groups. thus, for example, if we   re processing text in african
american vernacular english (aave), a dialect spoken by millions of people in the
united states, it   s important to make use of nlp tools that function with that dialect.
twitter posts written in aave make use of constructions like iont (i don   t in stan-
dard american english (sae)), or talmbout corresponding to sae talking about,
both examples that in   uence id40 (blodgett et al. 2016, jones 2015).
it   s also quite common for speakers or writers to use multiple languages in a
single communicative act, a phenomenon called code switching. code switch-
ing is enormously common across the world; here are examples showing spanish
and (transliterated) hindi code switching with english (solorio et al. 2014, jurgens
et al. 2017):
(2.2) por primera vez veo a @username actually being hateful! it was beautiful:)

[for the    rst time i get to see @username actually being hateful! it was
beautiful:]

(2.3) dost tha or ra- hega ... dont wory ... but dherya rakhe

[   he was and will remain a friend ... don   t worry ... but have faith   ]

aave

sae

code switching

another dimension of variation is the genre. the text that our algorithms must
process might come from newswire,    ction or non-   ction books, scienti   c articles,
wikipedia, or religious texts.
it might come from spoken genres like telephone
conversations, business meetings, police body-worn cameras, medical interviews,
or transcripts of television shows or movies. it might come from work situations
like doctors    notes, legal text, or parliamentary or congressional proceedings.

text also re   ects the demographic characteristics of the writer (or speaker): their
age, gender, race, socio-economic class can all in   uence the linguistic properties of
the text we are processing.

and    nally, time matters too. language changes over time, and for some lan-

guages we have good corpora of texts from different historical periods.

because language is so situated, when developing computational models for lan-

2.4

    text id172

13

guage processing, it   s important to consider who produced the language, in what
context, for what purpose, and make sure that the models are    t to the data.

2.4 text id172

before almost any natural language processing of a text, the text has to be normal-
ized. at least three tasks are commonly applied as part of any id172 process:

1. segmenting/tokenizing words from running text
2. normalizing word formats
3. segmenting sentences in running text.
in the next sections we walk through each of these tasks.

2.4.1 unix tools for crude id121 and id172
let   s begin with an easy, if somewhat naive version of word id121 and nor-
malization (and frequency computation) that can be accomplished for english solely
in a single unix command-line, inspired by church (1994). we   ll make use of some
unix commands: tr, used to systematically change particular characters in the in-
put; sort, which sorts input lines in alphabetical order; and uniq, which collapses
and counts adjacent identical lines.

for example let   s begin with the    complete words    of shakespeare in one text   le,
sh.txt. we can use tr to tokenize the words by changing every sequence of non-
alphabetic characters to a newline (   a-za-z    means alphabetic, the -c option com-
plements to non-alphabet, and the -s option squeezes all sequences into a single
character):

tr -sc    a-za-z       \n    < sh.txt

the output of this command will be:

the
sonnets
by
william
shakespeare
from
fairest
creatures
we
...

now that there is one word per line, we can sort the lines, and pass them to uniq

-c which will collapse and count them:

tr -sc    a-za-z       \n    < sh.txt | sort | uniq -c
with the following output:

1945 a
72 aaron
19 abbess
25 aaron

14 chapter 2

    id157, text id172, id153

6 abate
1 abates
5 abbess
6 abbey
3 abbot
...

alternatively, we can collapse all the upper case to lower case:

tr -sc    a-za-z       \n    < sh.txt | tr a-z a-z | sort | uniq -c
whose output is

14725 a

97 aaron

1 abaissiez

10 abandon

2 abandoned
2 abase
1 abash
14 abate

3 abated
3 abatement
...
now we can sort again to    nd the frequent words. the -n option to sort means
to sort numerically rather than alphabetically, and the -r option means to sort in
reverse order (highest-to-lowest):

tr -sc    a-za-z       \n    < sh.txt | tr a-z a-z | sort | uniq -c | sort -n -r
the results show that the most frequent words in shakespeare, as in any other

corpus, are the short function words like articles, pronouns, prepositions:
27378 the
26084 and
22538 i
19771 to
17481 of
14725 a
13826 you
12489 my
11318 that
11112 in

...
unix tools of this sort can be very handy in building quick word count statistics

for any corpus.

2.4.2 word id121 and id172
the simple unix tools above were    ne for getting rough word statistics but more
sophisticated algorithms are generally necessary for id121, the task of seg-
menting running text into words, and id172, the task of putting words/to-
kens in a standard format.

while the unix command sequence just removed all the numbers and punctu-
ation, for most nlp applications we   ll need to keep these in our id121. we

id121
id172

2.4

    text id172

15

often want to break off punctuation as a separate token; commas are a useful piece of
information for parsers, periods help indicate sentence boundaries. but we   ll often
want to keep the punctuation that occurs word internally, in examples like m.p.h,,
ph.d., at&t, cap   n. special characters and numbers will need to be kept in prices
($45.55) and dates (01/02/06); we don   t want to segment that price into separate to-
kens of    45    and    55   . and there are urls (http://www.stanford.edu), twitter
hashtags (#nlproc), or email addresses (someone@cs.colorado.edu).

number expressions introduce other complications as well; while commas nor-
mally appear at word boundaries, commas are used inside numbers in english, every
three digits: 555,500.50. languages, and hence id121 requirements, differ
on this; many continental european languages like spanish, french, and german, by
contrast, use a comma to mark the decimal point, and spaces (or sometimes periods)
where english puts commas, for example, 555 500,50.

a tokenizer can also be used to expand clitic contractions that are marked by
apostrophes, for example, converting what   re to the two tokens what are, and
we   re to we are. a clitic is a part of a word that can   t stand on its own, and can only
occur when it is attached to another word. some such contractions occur in other
alphabetic languages, including articles and pronouns in french (j   ai, l   homme).
depending on the application, id121 algorithms may also tokenize mul-
tiword expressions like new york or rock    n    roll as a single token, which re-
quires a multiword expression dictionary of some sort. id121 is thus inti-
mately tied up with named entity detection, the task of detecting names, dates, and
organizations (chapter 17).

one commonly used id121 standard is known as the id32 to-
kenization standard, used for the parsed corpora (treebanks) released by the lin-
guistic data consortium (ldc), the source of many useful datasets. this standard
separates out clitics (doesn   t becomes does plus n   t), keeps hyphenated words to-
gether, and separates out all punctuation:
input:
output:

   the san francisco-based restaurant,    they said,    doesn   t charge $10   .
   
said

francisco-based
charge

restaurant
10
$

san
   

the
,

does

they

n   t

   

   

,

.

tokens can also be normalized, in which a single normalized form is chosen for
words with multiple forms like usa and us or uh-huh and uhhuh. this standard-
ization may be valuable, despite the spelling information that is lost in the normal-
ization process. for information retrieval, we might want a query for us to match a
document that has usa; for information extraction we might want to extract coherent
information that is consistent across differently-spelled instances.

case folding is another kind of id172. for tasks like id103
and information retrieval, everything is mapped to lower case. for sentiment anal-
ysis and other text classi   cation tasks, information extraction, and machine transla-
tion, by contrast, case is quite helpful and case folding is generally not done (losing
the difference, for example, between us the country and us the pronoun can out-
weigh the advantage in generality that case folding provides).

in practice, since id121 needs to be run before any other language process-
ing, it is important for it to be very fast. the standard method for id121/nor-
malization is therefore to use deterministic algorithms based on id157
compiled into very ef   cient    nite state automata. carefully designed deterministic
algorithms can deal with the ambiguities that arise, such as the fact that the apos-
trophe needs to be tokenized differently when used as a genitive marker (as in the

clitic

id32
id121

case folding

16 chapter 2

    id157, text id172, id153

hanzi

maximum
matching

book   s cover), a quotative as in    the other class   , she said, or in clitics like they   re.

2.4.3 id40 in chinese: the maxmatch algorithm
some languages, including written chinese, japanese, and thai, do not use spaces to
mark potential word-boundaries, and so require alternative segmentation methods.
in chinese, for example, words are composed of characters known as hanzi. each
character generally represents a single morpheme and is pronounceable as a single
syllable. words are about 2.4 characters long on average. a simple algorithm that
does remarkably well for segmenting chinese, and often used as a baseline com-
parison for more advanced methods, is a version of greedy search called maximum
matching or sometimes maxmatch. the algorithm requires a dictionary (wordlist)
of the language.

the maximum matching algorithm starts by pointing at the beginning of a string.
it chooses the longest word in the dictionary that matches the input at the current
position. the pointer is then advanced to the end of that word in the string.
if
no word matches, the pointer is instead advanced one character (creating a one-
character word). the algorithm is then iteratively applied again starting from the
new pointer position. fig. 2.11 shows a version of the algorithm.

function maxmatch(sentence, dictionary) returns word sequence w

if sentence is empty
for i   length(sentence) downto 1

return empty list

   rstword =    rst i chars of sentence
remainder = rest of sentence
if indictionary(   rstword, dictionary)

return list(   rstword, maxmatch(remainder,dictionary) )

# no word was found, so make a one-character word
   rstword =    rst char of sentence
remainder = rest of sentence
return list(   rstword, maxmatch(remainder,dictionary) )
figure 2.11 the maxmatch algorithm for id40.

maxmatch works very well on chinese; the following example shows an appli-
cation to a simple chinese sentence using a simple chinese lexicon available from
the linguistic data consortium:

input:                            
output:           

                   

he especially likes peking duck

   he especially likes peking duck   

maxmatch doesn   t work as well on english. to make the intuition clear, we   ll
create an example by removing the spaces from the beginning of turing   s famous
quote    we can only see a short distance ahead   , producing    wecanonlyseeashortdis-
tanceahead   . the maxmatch results are shown below.
wecanonlyseeashortdistanceahead

input:
output: we canon l y see ash ort distance ahead
on english the algorithm incorrectly chose canon instead of stopping at can,
which left the algorithm confused and having to create single-character words l and

word error rate

morpheme
stem
af   x

2.4

    text id172

17

y and use the very rare word ort.

the algorithm works better in chinese than english, because chinese has much
shorter words than english. we can quantify how well a segmenter works using a
metric called word error rate. we compare our output segmentation with a perfect
hand-segmented (   gold   ) sentence, seeing how many words differ. the word error
rate is then the normalized minimum id153 in words between our output and
the gold: the number of word insertions, deletions, and substitutions divided by the
length of the gold sentence in words; we   ll see in section 2.5 how to compute edit
distance. even in chinese, however, maxmatch has problems, for example dealing
with unknown words (words not in the dictionary) or genres that differ a lot from
the assumptions made by the dictionary builder.

the most accurate chinese segmentation algorithms generally use statistical se-
quence models trained via supervised machine learning on hand-segmented training
sets; we   ll introduce sequence models in chapter 8.

2.4.4 collapsing words: lemmatization and id30
for many natural language processing situations we want two different forms of
a word to behave similarly. for example in web search, someone may type the
string woodchucks but a useful system might want to also return pages that mention
woodchuck with no s. this is especially common in morphologically complex lan-
guages like russian, where for example the word moscow has different endings in
the phrases moscow, of moscow, from moscow, and so on.

lemmatization is the task of determining that two words have the same root,
despite their surface differences. the words am, are, and is have the shared lemma
be; the words dinner and dinners both have the lemma dinner.

lemmatizing each of these forms to the same lemma will let us    nd all mentions
of words like moscow. the the lemmatized form of a sentence like he is reading
detective stories would thus be he be read detective story.

how is lemmatization done? the most sophisticated methods for lemmatization
involve complete morphological parsing of the word. morphology is the study of
the way words are built up from smaller meaning-bearing units called morphemes.
two broad classes of morphemes can be distinguished: stems   the central mor-
pheme of the word, supplying the main meaning    and af   xes   adding    additional   
meanings of various kinds. so, for example, the word fox consists of one morpheme
(the morpheme fox) and the word cats consists of two: the morpheme cat and the
morpheme -s. a morphological parser takes a word like cats and parses it into the
two morphemes cat and s, or a spanish word like amaren (   if in the future they
would love   ) into the morphemes amar    to love   , 3pl, and future subjunctive.

the porter stemmer

id30
porter stemmer

lemmatization algorithms can be complex. for this reason we sometimes make use
of a simpler but cruder method, which mainly consists of chopping off word-   nal
af   xes. this naive version of morphological analysis is called id30. one of
the most widely used id30 algorithms is the porter (1980). the porter stemmer
applied to the following paragraph:

this was not the map we found in billy bones   s chest, but
an accurate copy, complete in all things-names and heights
and soundings-with the single exception of the red crosses
and the written notes.

18 chapter 2

    id157, text id172, id153

produces the following stemmed output:

thi wa not the map we found in billi bone s chest but an
accur copi complet in all thing name and height and sound
with the singl except of the red cross and the written note
the algorithm is based on series of rewrite rules run in series, as a cascade, in
which the output of each pass is fed as input to the next pass; here is a sampling of
the rules:

cascade

ational     ate (e.g., relational     relate)

ing       
sses     ss (e.g., grasses     grass)

if stem contains vowel (e.g., motoring     motor)

detailed rule lists for the porter stemmer, as well as code (in java, python, etc.)
can be found on martin porter   s homepage; see also the original paper (porter, 1980).
simple stemmers can be useful in cases where we need to collapse across differ-
ent variants of the same lemma. nonetheless, they do tend to commit errors of both
over- and under-generalizing, as shown in the table below (krovetz, 1993):

errors of commission
organization organ
doing
numerical
policy

doe
numerous
police

errors of omission
european europe
analyzes
analysis
noisy
noise
sparse
sparsity

unknown
words

byte-pair
encoding
bpe

2.4.5 byte-pair encoding
id30 or lemmatizing has another side-bene   t. by treating two similar words
identically, these id172 methods help deal with the problem of unknown
words, words that a system has not seen before.

unknown words are particularly relevant for machine learning systems. as we
will see in the next chapter, machine learning systems often learn some facts about
words in one corpus (a training corpus) and then use these facts to make decisions
about a separate test corpus and its words. thus if our training corpus contains, say
the words low, and lowest, but not lower, but then the word lower appears in our
test corpus, our system will not know what to do with it. id30 or lemmatizing
everything to low can solve the problem, but has the disadvantage that sometimes
we don   t want words to be completely collapsed. for some purposes (for example
part-of-speech tagging) the words low and lower need to remain distinct.

a solution to this problem is to use a different kind of id121 in which
most tokens are words, but some tokens are frequent word parts like -er, so that an
unseen word can be represented by combining the parts.

the simplest such algorithm is byte-pair encoding, or bpe (sennrich et al.,
2016). byte-pair encoding is based on a method for text compression (gage, 1994),
but here we use it for id121 instead. the intuition of the algorithm is to
iteratively merge frequent pairs of characters,

the algorithm begins with the set of symbols equal to the set of characters. each
word is represented as a sequence of characters plus a special end-of-word symbol
  . at each step of the algorithm, we count the number of symbol pairs,    nd the
most frequent pair (   a   ,    b   ), and replace it with the new merged symbol (   ab   ). we
continue to count and merge, creating new longer and longer character strings, until

2.4

    text id172

19

we   ve done k merges; k is a parameter of the algorithm. the resulting symbol set
will consist of the original set of characters plus k new symbols.

the algorithm is run inside words (we don   t merge across word boundaries).
for this reason, the algorithm can take as input a dictionary of words together with
counts. for example, consider the following tiny input dictionary:

frequency
word
l o w   
5
l o w e s t    2
n e w e r   
6
w i d e r   
3
n e w   
2

we    rst count all pairs of symbols: the most frequent is the pair r    because it
occurs in newer (frequency of 6) and wider (frequency of 3) for a total of 9 occur-
rences. we then merge these symbols, treating r   as one symbol, and count again:

word
frequency
l o w   
5
l o w e s t    2
n e w e r  
6
w i d e r  
3
n e w   
2

now the most frequent pair is e r  , which we merge:
word
frequency
l o w   
5
l o w e s t    2
n e w er  
6
w i d er  
3
n e w   
2

our system has learned that there should be a token for word-   nal er, repre-

sented as er  . if we continue, the next merges are

(   e   ,    w   )
(   n   ,    ew   )
(   l   ,    o   )
(   lo   ,    w   )
(   new   ,    er     )
(   low   ,         )

the current set of symbols is thus {  , d, e, i, l, n, o, r, s, t, w,

r  , er  , ew, new, lo, low, newer  , low  }

when we need to tokenize a test sentence, we just run the merges we have
learned, greedily, in the order we learned them, on the test data.
(thus the fre-
quencies in the test data don   t play a role, just the frequencies in the training data).
so    rst we segment each test sentence word into characters. then we apply the    rst
rule: replace every instance of r    in the test corpus with r  , and then the second
rule: replace every instance of e r   in the test corpus with er  , and so on. by the
end, if the test corpus contained the word n e w e r   , it would be tokenized as a
full word. but a new (unknown) word like l o w e r    would be merged into the
two tokens low er  .

of course in real algorithms bpe is run with many thousands of merges on a
very large input dictionary. the result is that most words will be represented as

20 chapter 2

    id157, text id172, id153

full symbols, and only the very rare words (and unknown words) will have to be
represented by their parts.

the full bpe learning algorithm is given in fig. 2.12.

i m p o r t

re ,

c o l l e c t i o n s

d e f g e t s t a t s ( vocab ) :

p a i r s = c o l l e c t i o n s . d e f a u l t d i c t ( i n t )
f o r word ,

symbols = word . s p l i t ( )
f o r

f r e q i n vocab . i t e m s ( ) :
i n r a n g e ( l e n ( symbols )    1):

i
p a i r s [ symbols [ i ] , symbols [ i + 1 ] ] += f r e q

r e t u r n p a i r s

d e f merge vocab ( p a i r , v i n ) :

v o u t = {}
bigram = r e . e s c a p e (    
p = r e . c o m p i l e ( r     (? <!\s )     + bigram + r     ( ? !\ s )     )
f o r word i n v i n :

    . j o i n ( p a i r ) )

w out = p . sub (         . j o i n ( p a i r ) , word )
v o u t [ w out ] = v i n [ word ]

r e t u r n v o u t

vocab = {     l o w </w>   

: 5 ,

    l o w e s

t </w>   

: 2 ,

    n e w e r </w>    : 6 ,

   w i d e r </w>    : 3 ,

    n e w </w>    : 2}

num merges = 8

f o r

i n r a n g e ( num merges ) :

i
p a i r s = g e t s t a t s ( vocab )
b e s t = max ( p a i r s , key= p a i r s . g e t )
vocab = merge vocab ( b e s t , vocab )
p r i n t ( b e s t )

figure 2.12 python code for bpe learning algorithm from sennrich et al. (2016).

sentence
segmentation

2.4.6 sentence segmentation
sentence segmentation is another important step in text processing. the most use-
ful cues for segmenting a text into sentences are punctuation, like periods, question
marks, exclamation points. question marks and exclamation points are relatively
unambiguous markers of sentence boundaries. periods, on the other hand, are more
ambiguous. the period character    .    is ambiguous between a sentence boundary
marker and a marker of abbreviations like mr. or inc. the previous sentence that
you just read showed an even more complex case of this ambiguity, in which the    nal
period of inc. marked both an abbreviation and the sentence boundary marker. for
this reason, sentence id121 and word id121 may be addressed jointly.
in general, sentence id121 methods work by building a binary classi   er
(based on a sequence of rules or on machine learning) that decides if a period is part
of the word or is a sentence-boundary marker. in making this decision, it helps to
know if the period is attached to a commonly used abbreviation; thus, an abbrevia-
tion dictionary is useful.

state-of-the-art methods for sentence id121 are based on machine learning

and are introduced in later chapters.

2.5 minimum id153

2.5

    minimum id153

21

much of natural language processing is concerned with measuring how similar two
strings are. for example in id147, the user typed some erroneous
string   let   s say graffe   and we want to know what the user meant. the user prob-
ably intended a word that is similar to graffe. among candidate similar words,
the word giraffe, which differs by only one letter from graffe, seems intuitively
to be more similar than, say grail or graf, which differ in more letters. another
example comes from coreference, the task of deciding whether two strings such as
the following refer to the same entity:

stanford president john hennessy
stanford university president john hennessy

minimum edit
distance

alignment

again, the fact that these two strings are very similar (differing by only one word)
seems like useful evidence for deciding that they might be coreferent.

id153 gives us a way to quantify both of these intuitions about string sim-
ilarity. more formally, the minimum id153 between two strings is de   ned
as the minimum number of editing operations (operations like insertion, deletion,
substitution) needed to transform one string into another.

the gap between intention and execution, for example, is 5 (delete an i, substi-
tute e for n, substitute x for t, insert c, substitute u for n). it   s much easier to see
this by looking at the most important visualization for string distances, an alignment
between the two strings, shown in fig. 2.13. given two sequences, an alignment is
a correspondence between substrings of the two sequences. thus, we say i aligns
with the empty string, n with e, and so on. beneath the aligned strings is another
representation; a series of symbols expressing an operation list for converting the
top string into the bottom string: d for deletion, s for substitution, i for insertion.

i n t e * n t i o n
| | | | | | | | | |
* e x e c u t i o n
d s s

i s

figure 2.13 representing the minimum id153 between two strings as an alignment.
the    nal row gives the operation list for converting the top string into the bottom string: d for
deletion, s for substitution, i for insertion.

we can also assign a particular cost or weight to each of these operations. the
levenshtein distance between two sequences is the simplest weighting factor in
which each of the three operations has a cost of 1 (levenshtein, 1966)   we assume
that the substitution of a letter for itself, for example, t for t, has zero cost. the lev-
enshtein distance between intention and execution is 5. levenshtein also proposed
an alternative version of his metric in which each insertion or deletion has a cost of
1 and substitutions are not allowed. (this is equivalent to allowing substitution, but
giving each substitution a cost of 2 since any substitution can be represented by one
insertion and one deletion). using this version, the levenshtein distance between
intention and execution is 8.

22 chapter 2

    id157, text id172, id153

2.5.1 the minimum id153 algorithm
how do we    nd the minimum id153? we can think of this as a search task, in
which we are searching for the shortest path   a sequence of edits   from one string
to another.

dynamic
programming

figure 2.14 finding the id153 viewed as a search problem

the space of all possible edits is enormous, so we can   t search naively. however,
lots of distinct edit paths will end up in the same state (string), so rather than recom-
puting all those paths, we could just remember the shortest path to a state each time
we saw it. we can do this by using id145. id145
is the name for a class of algorithms,    rst introduced by bellman (1957), that apply
a table-driven method to solve problems by combining solutions to sub-problems.
some of the most commonly used algorithms in natural language processing make
use of id145, such as the viterbi algorithm (chapter 8) and the
cky algorithm for parsing (chapter 11).

the intuition of a id145 problem is that a large problem can
be solved by properly combining the solutions to various sub-problems. consider
the shortest path of transformed words that represents the minimum id153
between the strings intention and execution shown in fig. 2.15.

figure 2.15 path from intention to execution.

minimum edit
distance

imagine some string (perhaps it is exention) that is in this optimal path (whatever
it is). the intuition of id145 is that if exention is in the optimal
operation list, then the optimal sequence must also include the optimal path from
intention to exention. why? if there were a shorter path from intention to exention,
then we could use it instead, resulting in a shorter overall path, and the optimal
sequence wouldn   t be optimal, thus leading to a contradiction.

the minimum id153 algorithm was named by wagner and fischer (1974)
but independently discovered by many people (see the historical notes section of
chapter 8).

let   s    rst de   ne the minimum id153 between two strings. given two
strings, the source string x of length n, and target string y of length m, we   ll de   ne
d(i, j) as the id153 between x[1..i] and y [1.. j], i.e., the    rst i characters of x
and the    rst j characters of y . the id153 between x and y is thus d(n,m).

n t e n t i o ni n t e c n t i o ni n x e n t i o ndelinssubsti n t e n t i o nn t e n t i o ni n t e n t i o ne t e n t i o ne x e n t i o ne x e n u t i o ne x e c u t i o ndelete isubstitute n by esubstitute t by xinsert usubstitute n by c2.5

    minimum id153

23

we   ll use id145 to compute d(n,m) bottom up, combining so-
lutions to subproblems. in the base case, with a source substring of length i but an
empty target string, going from i characters to 0 requires i deletes. with a target
substring of length j but an empty source going from 0 characters to j characters
requires j inserts. having computed d(i, j) for small i, j we then compute larger
d(i, j) based on previously computed smaller values. the value of d(i, j) is com-
puted by taking the minimum of the three possible paths through the matrix which
arrive there:

          d[i    1, j] + del-cost(source[i])

d[i, j    1] + ins-cost(target[ j])
d[i    1, j    1] + sub-cost(source[i],target[ j])

d[i, j] = min

if we assume the version of levenshtein distance in which the insertions and
deletions each have a cost of 1 (ins-cost(  ) = del-cost(  ) = 1), and substitutions have
a cost of 2 (except substitution of identical letters have zero cost), the computation
for d(i, j) becomes:

                     

d[i, j] = min

d[i    1, j] + 1
d[i, j    1] + 1
d[i    1, j    1] +

(cid:26) 2;

0;

if source[i] (cid:54)= target[ j]
if source[i] = target[ j]

(2.4)

the algorithm is summarized in fig. 2.16; fig. 2.17 shows the results of applying
the algorithm to the distance between intention and execution with the version of
levenshtein in eq. 2.4.

knowing the minimum id153 is useful for algorithms like    nding poten-
tial spelling error corrections. but the id153 algorithm is important in another
way; with a small change, it can also provide the minimum cost alignment between
two strings. aligning two strings is useful throughout speech and language process-
ing. in id103, minimum id153 alignment is used to compute
the word error rate (chapter 26). alignment plays a role in machine translation, in
which sentences in a parallel corpus (a corpus with a text in two languages) need to
be matched to each other.

to extend the id153 algorithm to produce an alignment, we can start by
visualizing an alignment as a path through the id153 matrix. figure 2.18
shows this path with the boldfaced cell. each boldfaced cell represents an alignment
of a pair of letters in the two strings. if two boldfaced cells occur in the same row,
there will be an insertion in going from the source to the target; two boldfaced cells
in the same column indicate a deletion.

figure 2.18 also shows the intuition of how to compute this alignment path. the
computation proceeds in two steps. in the    rst step, we augment the minimum edit
distance algorithm to store backpointers in each cell. the backpointer from a cell
points to the previous cell (or cells) that we came from in entering the current cell.
we   ve shown a schematic of these backpointers in fig. 2.18. some cells have mul-
tiple backpointers because the minimum extension could have come from multiple
previous cells. in the second step, we perform a backtrace. in a backtrace, we start
from the last cell (at the    nal row and column), and follow the pointers back through
the id145 matrix. each complete path between the    nal cell and the
initial cell is a minimum distance alignment. exercise 2.7 asks you to modify the

backtrace

24 chapter 2

    id157, text id172, id153

function min-edit-distance(source, target) returns min-distance

n    length(source)
m    length(target)
create a distance matrix distance[n+1,m+1]

# initialization: the zeroth row and column is the distance from the empty string

d[0,0] = 0
for each row i from 1 to n do

d[i,0]   d[i-1,0] + del-cost(source[i])
d[0,j]   d[0, j-1] + ins-cost(target[j])

for each column j from 1 to m do

# recurrence relation:
for each row i from 1 to n do

for each column j from 1 to m do

d[i, j]    min( d[i   1, j] + del-cost(source[i]),
d[i   1, j   1] + sub-cost(source[i], target[j]),
d[i, j   1] + ins-cost(target[j]))

# termination
return d[n,m]

figure 2.16 the minimum id153 algorithm, an example of the class of dynamic
programming algorithms. the various costs can either be    xed (e.g.,    x,ins-cost(x) = 1)
or can be speci   c to the letter (to model the fact that some letters are more likely to be in-
serted than others). we assume that there is no cost for substituting a letter for itself (i.e.,
sub-cost(x,x) = 0).

src\tar
#
i
n
t
e
n
t
i
o
n

#
0
1
2
3
4
5
6
7
8
9

e
1
2
3
4
3
4
5
6
7
8

x
2
3
4
5
4
5
6
7
8
9

e
3
4
5
6
5
6
7
8
9
10

c
4
5
6
7
6
7
8
9
10
11

u
5
6
7
8
7
8
9
10
11
12

t
6
7
8
7
8
9
8
9
10
11

i
7
6
7
8
9
10
9
8
9
10

o
8
7
8
9
10
11
10
9
8
9

n
9
8
7
8
9
10
11
10
9
8

figure 2.17 computation of minimum id153 between intention and execution with
the algorithm of fig. 2.16, using levenshtein distance with cost of 1 for insertions or dele-
tions, 2 for substitutions.

minimum id153 algorithm to store the pointers and compute the backtrace to
output an alignment.

while we worked our example with simple levenshtein distance, the algorithm
in fig. 2.16 allows arbitrary weights on the operations. for id147, for
example, substitutions are more likely to happen between letters that are next to
each other on the keyboard. the viterbi algorithm is a probabilistic extension of
minimum id153. instead of computing the    minimum id153    between
two strings, viterbi computes the    maximum id203 alignment    of one string
with another. we   ll discuss this more in chapter 8.

2.6

    summary

25

o

    6

x
    2

e
    3

e
    1

c
    4

u
    5

    4 (cid:45)    5

t
#
    6
#
0
    1 (cid:45)       2 (cid:45)       3 (cid:45)       4 (cid:45)       5 (cid:45)       6 (cid:45)       7
i
n     2 (cid:45)       3 (cid:45)       4 (cid:45)       5 (cid:45)       6 (cid:45)       7 (cid:45)       8
    3 (cid:45)       4 (cid:45)       5 (cid:45)       6 (cid:45)       7 (cid:45)       8
(cid:45) 7
t
    4
e
n     5
    6
t
    7
i
o     8
n     9

n
i
    8     9
    7
(cid:45) 6
    7     8
    7 (cid:45)       8 (cid:45) 7
    8
       8 (cid:45)       9
    7        8 (cid:45)       9 (cid:45)       10
    9
(cid:45) 3
    4 (cid:45)       5 (cid:45)       6 (cid:45)       7 (cid:45)       8 (cid:45)       9 (cid:45)       10 (cid:45)       11 (cid:45)    10
    10        11
    5 (cid:45)       6 (cid:45)       7 (cid:45)       8 (cid:45)       9
    9     10
    6 (cid:45)       7 (cid:45)       8 (cid:45)       9 (cid:45)       10
    7 (cid:45)       8 (cid:45)       9 (cid:45)       10 (cid:45)       11
(cid:45) 8     9
    9 (cid:45) 8
    8 (cid:45)       9 (cid:45)       10 (cid:45)       11 (cid:45)       12
figure 2.18 when entering a value in each cell, we mark which of the three neighboring
cells we came from with up to three arrows. after the table is full we compute an alignment
(minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and
following the arrows back. the sequence of bold cells represents one possible minimum cost
alignment between the two strings. diagram design after gus   eld (1997).

(cid:45) 8
    9
    10
    11

    9
(cid:45) 8
    9
    10

2.6 summary

this chapter introduced a fundamental tool in language processing, the regular ex-
pression, and showed how to perform basic text id172 tasks including
id40 and id172, sentence segmentation, and id30.
we also introduce the important minimum id153 algorithm for comparing
strings. here   s a summary of the main points we covered about these ideas:

simple id157 substitutions or    nite automata.

    the regular expression language is a powerful tool for pattern-matching.
    basic operations in id157 include concatenation of symbols,
disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors
(  , $) and precedence operators ((,)).
    word id121 and id172 are generally done by cascades of
    the porter algorithm is a simple and ef   cient way to do id30, stripping
off af   xes. it does not have high accuracy but may be useful for some tasks.
    the minimum id153 between two strings is the minimum number of
operations it takes to edit one into the other. minimum id153 can be
computed by id145, which also results in an alignment of
the two strings.

bibliographical and historical notes

kleene (1951) and (1956)    rst de   ned id157 and the    nite automaton,
based on the mcculloch-pitts neuron. ken thompson was one of the    rst to build
id157 compilers into editors for text searching (thompson, 1968). his
editor ed included a command    g/regular expression/p   , or global regular expres-
sion print, which later became the unix grep utility.

text id172 algorithms has been applied since the beginning of the    eld.
one of the earliest widely-used stemmers was lovins (1968). id30 was also
applied early to the digital humanities, by packard (1973), who built an af   x-stripping
morphological parser for ancient greek. currently a wide variety of code for tok-

26 chapter 2

    id157, text id172, id153

enization and id172 is available, such as the stanford tokenizer (http://
nlp.stanford.edu/software/tokenizer.shtml) or specialized tokenizers for
twitter (o   connor et al., 2010), or for sentiment (http://sentiment.christopherpotts.
net/tokenizing.html). see palmer (2012) for a survey of text preprocessing.
while the max-match algorithm we describe is commonly used as a segmentation
baseline in languages like chinese, higher accuracy algorithms like the stanford
crf segmenter, are based on sequence models; see tseng et al. (2005) and chang
et al. (2008). nltk is an essential tool that offers both useful python libraries
(http://www.nltk.org) and textbook descriptions (bird et al., 2009) of many al-
gorithms including text id172 and corpus interfaces.

for more on herdan   s law and heaps    law, see herdan (1960, p. 28), heaps
(1978), egghe (2007) and baayen (2001); yasseri et al. (2012) discuss the relation-
ship with other measures of linguistic complexity. for more on id153, see the
excellent gus   eld (1997). our example measuring the id153 from    intention   
to    execution    was adapted from kruskal (1983). there are various publicly avail-
able packages to compute id153, including unix diff and the nist sclite
program (nist, 2005).

in his autobiography bellman (1984) explains how he originally came up with

the term id145:

i decided therefore to use the word,    programming   .

   ...the 1950s were not good years for mathematical research. [the]
secretary of defense ...had a pathological fear and hatred of the word,
research...
i
wanted to get across the idea that this was dynamic, this was multi-
stage... i thought, let   s ...
take a word that has an absolutely precise
meaning, namely dynamic... it   s impossible to use the word, dynamic,
in a pejorative sense. try thinking of some combination that will pos-
sibly give it a pejorative meaning.
it   s impossible. thus, i thought
id145 was a good name. it was something not even a
congressman could object to.   

exercises

2.1 write id157 for the following languages.

1. the set of all alphabetic strings;
2. the set of all lower case alphabetic strings ending in a b;
3. the set of all strings from the alphabet a,b such that each a is immedi-

ately preceded by and immediately followed by a b;

2.2 write id157 for the following languages. by    word   , we mean
an alphabetic string separated from other words by whitespace, any relevant
punctuation, line breaks, and so forth.

1. the set of all strings with two consecutive repeated words (e.g.,    hum-

bert humbert    and    the the    but not    the bug    or    the big bug   );

2. all strings that start at the beginning of the line with an integer and that

end at the end of the line with a word;

3. all strings that have both the word grotto and the word raven in them

(but not, e.g., words like grottos that merely contain the word grotto);

exercises

27

4. write a pattern that places the    rst word of an english sentence in a

register. deal with punctuation.

2.3

implement an eliza-like program, using substitutions such as those described
on page 9. you might want to choose a different domain than a rogerian psy-
chologist, although keep in mind that you would need a domain in which your
program can legitimately engage in a lot of simple repetition.

2.4 compute the id153 (using insertion cost 1, deletion cost 1, substitution

2.5

cost 1) of    leda    to    deal   . show your work (using the id153 grid).
figure out whether drive is closer to brief or to divers and what the edit dis-
tance is to each. you may use any version of distance that you like.

2.6 now implement a minimum id153 algorithm and use your hand-computed

results to check your code.

2.7 augment the minimum id153 algorithm to output an alignment; you

2.8
2.9

will need to store pointers and add a stage to compute the backtrace.
implement the maxmatch algorithm.
to test how well your maxmatch algorithm works, create a test set by remov-
ing spaces from a set of sentences. implement the word error rate metric (the
number of word insertions + deletions + substitutions, divided by the length
in words of the correct string) and compute the wer for your test set.

28 chapter 2     id157, text id172, id153

baayen, r. h. (2001). word frequency distributions.

springer.

bellman, r. (1957). id145. princeton uni-

versity press.

bellman, r. (1984). eye of the hurricane: an autobiogra-

phy. world scienti   c singapore.

bird, s., klein, e., and loper, e. (2009). natural language

processing with python. o   reilly.

blodgett, s. l., green, l., and o   connor, b. (2016). demo-
graphic dialectal variation in social media: a case study of
african-american english. in emnlp 2016.

chang, p.-c., galley, m., and manning, c. d. (2008). opti-
mizing chinese id40 for machine translation
performance. in proceedings of acl statistical mt work-
shop, pp. 224   232.

church, k. w. (1994). unix for poets. slides from 2nd el-

snet summer school and unpublished paper ms.

clark, h. h. and fox tree, j. e. (2002). using uh and um in

spontaneous speaking. cognition, 84, 73   111.

egghe, l. (2007). untangling herdan   s law and heaps    law:
mathematical and informetric arguments. jasist, 58(5),
702   709.

gage, p. (1994). a new algorithm for data compression. the

c users journal, 12(2), 23   38.

godfrey,

j., holliman, e., and mcdaniel,

(1992).
switchboard: telephone speech corpus for research
and development. in icassp-92, san francisco, pp. 517   
520.

j.

gus   eld, d. (1997). algorithms on strings, trees, and se-
quences: computer science and computational biology.
cambridge university press.

heaps, h. s. (1978). information retrieval. computational

and theoretical aspects. academic press.

herdan, g. (1960). type-token mathematics. the hague,

mouton.

jones, t. (2015). toward a description of african american
vernacular english dialect regions using    black twitter   .
american speech, 90(4), 403   440.

jurgens, d., tsvetkov, y., and jurafsky, d. (2017). incorpo-
rating dialectal variability for socially equitable language
identi   cation. in acl 2017, pp. 51   57.

kleene, s. c. (1951). representation of events in nerve nets
and    nite automata. tech. rep. rm-704, rand corpora-
tion. rand research memorandum.

kleene, s. c. (1956). representation of events in nerve
nets and    nite automata.
in shannon, c. and mccarthy,
j. (eds.), automata studies, pp. 3   41. princeton university
press.

krovetz, r. (1993). viewing morphology as an id136

process. in sigir-93, pp. 191   202.

kruskal, j. b. (1983). an overview of sequence compari-
son. in sankoff, d. and kruskal, j. b. (eds.), time warps,
string edits, and macromolecules: the theory and prac-
tice of sequence comparison, pp. 1   44. addison-wesley.
ku  cera, h. and francis, w. n. (1967). computational anal-
ysis of present-day american english. brown university
press, providence, ri.

levenshtein, v. i. (1966). binary codes capable of correcting
deletions, insertions, and reversals. cybernetics and con-
trol theory, 10(8), 707   710. original in doklady akademii
nauk sssr 163(4): 845   848 (1965).

lovins, j. b. (1968). development of a id30 algo-
rithm. mechanical translation and computational lin-
guistics, 11(1   2), 9   13.

nist (2005). id103 scoring toolkit (sctk) ver-

sion 2.1. http://www.nist.gov/speech/tools/.

o   connor, b., krieger, m., and ahn, d. (2010). tweetmotif:
exploratory search and topic summarization for twitter. in
icwsm.

packard, d. w. (1973). computer-assisted morphological
analysis of ancient greek. in zampolli, a. and calzolari,
n. (eds.), computational and mathematical linguistics:
proceedings of the international conference on computa-
tional linguistics, pisa, pp. 343   355. leo s. olschki.

palmer, d. (2012). text preprocessing.

in indurkhya, n.
and damerau, f. j. (eds.), handbook of natural language
processing, pp. 9   30. crc press.

porter, m. f. (1980). an algorithm for suf   x stripping. pro-

gram, 14(3), 130   127.

sennrich, r., haddow, b., and birch, a. (2016). neural ma-
chine translation of rare words with subword units. in acl
2016.

simons, g. f. and fennig, c. d. (2018). ethnologue: lan-
guages of the world, twenty-   rst edition.. dallas, texas.
sil international.

solorio, t., blair, e., maharjan, s., bethard, s., diab, m.,
ghoneim, m., hawwari, a., alghamdi, f., hirschberg, j.,
chang, a., and fung, p. (2014). overview for the    rst
shared task on language identi   cation in code-switched
data. in proceedings of the first workshop on computa-
tional approaches to code switching, pp. 62   72.

thompson, k. (1968). regular expression search algorithm.

communications of the acm, 11(6), 419   422.

tseng, h., chang, p.-c., andrew, g., jurafsky, d., and man-
ning, c. d. (2005). conditional random    eld word seg-
menter. in proceedings of the fourth sighan workshop
on chinese language processing.

wagner, r. a. and fischer, m. j. (1974). the string-to-string
correction problem. journal of the association for comput-
ing machinery, 21, 168   173.

weizenbaum, j. (1966). eliza     a computer program for
the study of natural language communication between man
and machine. communications of the acm, 9(1), 36   45.

weizenbaum, j. (1976). computer power and human rea-
son: from judgement to calculation. w.h. freeman and
company.

yasseri, t., kornai, a., and kert  esz, j. (2012). a practical
approach to language complexity: a wikipedia case study.
plos one, 7(11).

