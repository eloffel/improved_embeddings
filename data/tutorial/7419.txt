basic text processing
id157
id157
a formal language for specifying text strings
how can we search for any of these?
woodchuck
woodchucks
woodchuck
woodchucks


id157: disjunctions
letters inside square brackets []



ranges [a-z]

		

id157: negation in disjunction
negations [^ss]
carat means negation only when first in []

id157: more disjunction
woodchucks is another name for groundhog!
the pipe | for disjunction

id157: ?    *  +  .


stephen c kleene
kleene *,   kleene +   
id157: anchors  ^   $

example
find me all instances of the word    the    in a text.
the
                                                misses capitalized examples
[tt]he
                                                incorrectly returns other or theology
[^a-za-z][tt]he[^a-za-z]
                                          

errors
the process we just went through was based on fixing two kinds of errors
matching strings that we should not have matched (there, then, other)
false positives (type i)
not matching things that we should have matched (the)
false negatives (type ii)
errors cont.
in nlp we are always dealing with these kinds of errors.
reducing the error rate for an application often involves two antagonistic efforts: 
increasing accuracy or precision (minimizing false positives)
increasing coverage or recall (minimizing false negatives).
summary
id157 play a surprisingly large role
sophisticated sequences of id157 are often the first model for any text processing text
for many hard tasks, we use machine learning classifiers
but id157 are used as features in the classifiers
can be very useful in capturing generalizations

11
basic text processing
id157
basic text processing
word id121

text id172
every nlp task needs to do text id172: 
segmenting/tokenizing words in running text
normalizing word formats
segmenting sentences in running text



how many words?
i do uh main- mainly business data processing
fragments, filled pauses
seuss   s cat in the hat is different from other cats! 
lemma: same stem, part of speech, rough word sense
cat and cats = same lemma
wordform: the full inflected surface form
cat and cats = different wordforms
how many words?
they lay back on the san francisco grass and looked at the stars and their

type: an element of the vocabulary.
token: an instance of that type in running text.
how many?
15 tokens (or 14)
13 types (or 12) (or 11?)
how many words?
n = number of tokens
v = vocabulary = set of types
|v| is the size of the vocabulary






church and gale (1990): |v| > o(n  )

simple id121 in unix
(inspired by ken church   s unix for poets.)
given a text file, output the word tokens and their frequencies
tr -sc    a-za-z       \n    < shakes.txt 
     | sort 
     | uniq    c 

1945 a
  72 aaron
  19 abbess
   5 abbot
 ... ...
    
25 aaron
 6 abate
 1 abates
 5 abbess
 6 abbey
 3 abbot
....      
change all non-alpha to newlines
sort in alphabetical order
merge and count each type
the first step: tokenizing
tr -sc    a-za-z       \n    < shakes.txt | head

the
sonnets
by
william
shakespeare
from
fairest
creatures
we
...    
the second step: sorting
tr -sc    a-za-z       \n    < shakes.txt | sort | head

a
a
a
a
a
a
a
a
a
...   
more counting
merging upper and lower case
tr    a-z       a-z    < shakes.txt | tr    sc    a-za-z       \n    | sort | uniq    c 
sorting the counts
tr    a-z       a-z    < shakes.txt | tr    sc    a-za-z       \n    | sort | uniq    c | sort    n    r
23243 the
22225 i
18618 and
16339 to
15687 of
12780 a
12163 you
10839 my
10005 in
8954  d

what happened here?
issues in id121
finland   s capital         finland finlands finland   s  ?
what   re, i   m, isn   t       what are, i am, is not
hewlett-packard           hewlett packard ?
state-of-the-art          state of the art ?
lowercase		     lower-case lowercase lower case ?
san francisco	     one token or two?
m.p.h., phd.		     ??
id121: language issues
french
l'ensemble     one token or two?
l ? l    ? le ?
want l   ensemble to match with un ensemble

german noun compounds are not segmented
lebensversicherungsgesellschaftsangestellter
   life insurance company employee   
german information retrieval needs compound splitter
id121: language issues
chinese and japanese no spaces between words:
                                                            
                                                                                
sharapova now     lives in       us       southeastern     florida
further complicated in japanese, with multiple alphabets intermingled
dates/amounts in multiple formats
                  500                                       $500k(   6,000      )




end-user can express query entirely in hiragana!
word id121 in chinese
also called id40
chinese words are composed of characters
characters are generally 1 syllable and 1 morpheme.
average word is 2.4 characters long.
standard baseline segmentation algorithm: 
maximum matching  (also called greedy)
maximum matching
id40 algorithm
given a wordlist of chinese, and a string.
start a pointer at the beginning of the string
find the longest word in dictionary that matches the string starting at pointer
move the pointer over the word in string
go to 2
max-match segmentation illustration
thecatinthehat
thetabledownthere

doesn   t generally work in english!

but works astonishingly well in chinese
                                                            
                                                                             
modern probabilistic segmentation algorithms even better
the table down there
the cat in the hat
theta bled own there
basic text processing
word id121

basic text processing

word id172 and id30

id172
need to    normalize    terms 
information retrieval: indexed text & query terms must have same form.
we want to match u.s.a. and usa
we implicitly define equivalence classes of terms
e.g., deleting periods in a term
alternative: asymmetric expansion:
enter: window	search: window, windows
enter: windows	search: windows, windows, window
enter: windows	search: windows
potentially more powerful, but less efficient

case folding
applications like ir: reduce all letters to lower case
since users tend to use lower case
possible exception: upper case in mid-sentence?
e.g., general motors
fed vs. fed
sail vs. sail
for id31, mt, information extraction
case is helpful (us versus us is important)
lemmatization
reduce inflections or variant forms to base form
am, are, is     be
car, cars, car's, cars'     car
the boy's cars are different colors     the boy car be different color
lemmatization: have to find correct dictionary headword form
machine translation
spanish quiero (   i want   ), quieres (   you want   ) same lemma as querer    want   

morphology
morphemes:
the small meaningful units that make up words
stems: the core meaning-bearing units
affixes: bits and pieces that adhere to stems
often with grammatical functions
id30
reduce terms to their stems in information retrieval
id30 is crude chopping of affixes
language dependent
e.g., automate(s), automatic, automation all reduced to automat.

for example compressed 
and compression are both 
accepted as equivalent to 
compress.
for exampl compress and
compress ar both accept
as equival to compress

porter   s algorithm
the most common english stemmer
   step 1a
sses     ss	 caresses     caress
ies      i	 ponies       poni
ss       ss	 caress       caress
s                   cats          cat
  step 1b
(*v*)ing           walking       walk
              sing          sing
(*v*)ed            plastered     plaster
   

   step 2 (for long stems)
ational    ate relational    relate
izer    ize	  digitizer     digitize
ator    ate	  operator      operate
   
    step 3 (for longer stems)
al                revival        reviv
able              adjustable     adjust
ate           activate       activ
   

viewing morphology in a corpus
why only strip    ing if there is a vowel?
(*v*)ing           walking       walk
              sing          sing

36
viewing morphology in a corpus
why only strip    ing if there is a vowel?
(*v*)ing           walking       walk
              sing          sing

37
tr -sc 'a-za-z' '\n' < shakes.txt | grep    ing$' | sort | uniq -c | sort    nr 








tr -sc 'a-za-z' '\n' < shakes.txt | grep '[aeiou].*ing$' | sort | uniq -c | sort    nr
548 being
541 nothing
152 something
145 coming
130 morning
122 having
120 living
117 loving
116 being
102 going
1312 king
 548 being
 541 nothing
 388 king
 375 bring
 358 thing
 307 ring
 152 something
 145 coming
 130 morning 
dealing with complex morphology is sometimes necessary
some languages requires complex morpheme segmentation
turkish
uygarlastiramadiklarimizdanmissinizcasina
`(behaving) as if you are among those whom we could not civilize   
uygar `civilized    + las `become    
+ tir `cause    + ama `not able    
+ dik `past    + lar    plural   
+ imiz    p1pl    + dan    abl    
+ mis    past    + siniz    2pl    + casina    as if    

basic text processing

word id172 and id30

basic text processing

sentence segmentation and id90

sentence segmentation
!, ? are relatively unambiguous
period    .    is quite ambiguous
sentence boundary
abbreviations like inc. or dr.
numbers like .02% or 4.3
build a binary classifier
looks at a    .   
decides endofsentence/notendofsentence
classifiers: hand-written rules, id157, or machine-learning
determining if a word is end-of-sentence: a decision tree
more sophisticated decision tree features
case of word with    .   : upper, lower, cap, number
case of word after    .   : upper, lower, cap, number

numeric features
length of word with    .   
id203(word with    .    occurs at end-of-s)
id203(word after    .    occurs at beginning-of-s)
implementing id90
a decision tree is just an if-then-else statement
the interesting research is choosing the features
setting up the structure is often too hard to do by hand
hand-building only possible for very simple features, domains
for numeric features, it   s too hard to pick each threshold
instead, structure usually learned by machine learning from a training corpus

id90 and other classifiers
we can think of the questions in a decision tree
as features that could be exploited by any kind of classifier
id28
id166
neural nets
etc.

basic text processing

sentence segmentation and id90

