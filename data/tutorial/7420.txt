speech and language processing. daniel jurafsky & james h. martin.
rights reserved.

draft of september 23, 2018.

copyright c(cid:13) 2018.

all

chapter

4 naive bayes and sentiment

classi   cation

classi   cation lies at the heart of both human and machine intelligence. deciding
what letter, word, or image has been presented to our senses, recognizing faces
or voices, sorting mail, assigning grades to homeworks; these are all examples of
assigning a category to an input. the potential challenges of this task are highlighted
by the fabulist jorge luis borges (1964), who imagined classifying animals into:

(a) those that belong to the emperor, (b) embalmed ones, (c) those that
are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray
dogs, (h) those that are included in this classi   cation, (i) those that
tremble as if they were mad, (j) innumerable ones, (k) those drawn with
a very    ne camel   s hair brush, (l) others, (m) those that have just broken
a    ower vase, (n) those that resemble    ies from a distance.

text
categorization

sentiment
analysis

many language processing tasks involve classi   cation, although luckily our classes

are much easier to de   ne than those of borges. in this chapter we introduce the naive
bayes algorithm and apply it to text categorization, the task of assigning a label or
category to an entire text or document.

we focus on one common text categorization task, id31, the ex-
traction of sentiment, the positive or negative orientation that a writer expresses
toward some object. a review of a movie, book, or product on the web expresses the
author   s sentiment toward the product, while an editorial or political text expresses
sentiment toward a candidate or political action. extracting consumer or public sen-
timent is thus relevant for    elds from marketing to politics.

the simplest version of id31 is a binary classi   cation task, and the
words of the review provide excellent cues. consider, for example, the following
phrases extracted from positive and negative reviews of movies and restaurants,.
words like great, richly, awesome, and pathetic, and awful and ridiculously are very
informative cues:

+ ...zany characters and richly applied satire, and some great plot twists
    it was pathetic. the worst part about it was the boxing scenes...
+ ...awesome caramel sauce and sweet toasty almonds. i love this place!
    ...awful pizza and ridiculously overpriced...

spam detection is another important commercial application, the binary clas-
si   cation task of assigning an email to one of the two classes spam or not-spam.
many lexical and other features can be used to perform this classi   cation. for ex-
ample you might quite reasonably be suspicious of an email containing phrases like
   online pharmaceutical    or    without any cost    or    dear winner   .

another thing we might want to know about a text is the language it   s written
in. texts on social media, for example, can be in any number of languages and we   ll
need to apply different processing. the task of language id is thus the    rst step
in most language processing pipelines. related tasks like determining a text   s au-
thor, (authorship attribution), or author characteristics like gender, age, and native

spam detection

language id

authorship
attribution

2 chapter 4

    naive bayes and sentiment classification

supervised
machine
learning

language are text classi   cation tasks that are also relevant to the digital humanities,
social sciences, and forensic linguistics.

finally, one of the oldest tasks in text classi   cation is assigning a library sub-
ject category or topic label to a text. deciding whether a research paper concerns
epidemiology or instead, perhaps, embryology, is an important component of infor-
mation retrieval. various sets of subject categories exist, such as the mesh (medical
subject headings) thesaurus. in fact, as we will see, subject category classi   cation
is the task for which the naive bayes algorithm was invented in 1961.

classi   cation is essential for tasks below the level of the document as well.
we   ve already seen period disambiguation (deciding if a period is the end of a sen-
tence or part of a word), and word id121 (deciding if a character should be
a word boundary). even id38 can be viewed as classi   cation: each
word can be thought of as a class, and so predicting the next word is classifying the
context-so-far into a class for each next word. a part-of-speech tagger (chapter 8)
classi   es each occurrence of a word in a sentence as, e.g., a noun or a verb.

the goal of classi   cation is to take a single observation, extract some useful
features, and thereby classify the observation into one of a set of discrete classes.
one method for classifying text is to use hand-written rules. there are many areas
of language processing where hand-written rule-based classi   ers constitute a state-
of-the-art system, or at least part of it.

rules can be fragile, however, as situations or data change over time, and for
some tasks humans aren   t necessarily good at coming up with the rules. most cases
of classi   cation in language processing are instead done via supervised machine
learning, and this will be the subject of the remainder of this chapter. in supervised
learning, we have a data set of input observations, each associated with some correct
output (a    supervision signal   ). the goal of the algorithm is to learn how to map
from a new observation to a correct output.
formally, the task of supervised classi   cation is to take an input x and a    xed
set of output classes y = y1,y2, ...,ym and return a predicted class y     y . for text
classi   cation, we   ll sometimes talk about c (for    class   ) instead of y as our output
variable, and d (for    document   ) instead of x as our input variable. in the supervised
situation we have a training set of n documents that have each been hand-labeled
with a class: (d1,c1), ...., (dn,cn). our goal is to learn a classi   er that is capable of
mapping from a new document d to its correct class c     c. a probabilistic classi   er
additionally will tell us the id203 of the observation being in the class. this
full distribution over the classes can be useful information for downstream decisions;
avoiding making discrete decisions early on can be useful when combining systems.
many kinds of machine learning algorithms are used to build classi   ers. this
chapter introduces naive bayes; the following one introduces id28.
these exemplify two ways of doing classi   cation. generative classi   ers like naive
bayes build a model of how a class could generate some input data. given an ob-
servation, they return the class most likely to have generated the observation. dis-
criminative classi   ers like id28 instead learn what features from the
input are most useful to discriminate between the different possible classes. while
discriminative systems are often more accurate and hence more commonly used,
generative classi   ers still have a role.

4.1 naive bayes classi   ers

4.1

    naive bayes classifiers

3

naive bayes
classi   er

bag-of-words

in this section we introduce the multinomial naive bayes classi   er, so called be-
cause it is a bayesian classi   er that makes a simplifying (naive) assumption about
how the features interact.

the intuition of the classi   er is shown in fig. 4.1. we represent a text document
as if it were a bag-of-words, that is, an unordered set of words with their position
ignored, keeping only their frequency in the document. in the example in the    gure,
instead of representing the word order in all the phrases like    i love this movie    and
   i would recommend it   , we simply note that the word i occurred 5 times in the
entire excerpt, the word it 6 times, the words love, recommend, and movie once, and
so on.

figure 4.1
words is ignored (the bag of words assumption) and we make use of the frequency of each word.

intuition of the multinomial naive bayes classi   er applied to a movie review. the position of the

naive bayes is a probabilistic classi   er, meaning that for a document d, out of
all classes c     c the classi   er returns the class   c which has the maximum posterior
id203 given the document. in eq. 4.1 we use the hat notation    to mean    our
estimate of the correct class   .

  

  c = argmax

c   c

p(c|d)

(4.1)

bayesian
id136

this idea of bayesian id136 has been known since the work of bayes (1763),
and was    rst applied to text classi   cation by mosteller and wallace (1964). the in-
tuition of bayesian classi   cation is to use bayes    rule to transform eq. 4.1 into other
probabilities that have some useful properties. bayes    rule is presented in eq. 4.2;
it gives us a way to break down any id155 p(x|y) into three other

ititititititiiiiiloverecommendmoviethethethethetototoandandandseenseenyetwouldwithwhowhimsicalwhilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainabouti love this movie! it's sweet, but with satirical humor. the dialogue is great and the adventure scenes are fun... it manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. i would recommend it to just about anyone. i've seen it several times, and i'm always happy to see it again whenever i have a friend who hasn't seen it yet!it ithetoandseenyetwouldwhimsicaltimessweetsatiricaladventuregenrefairyhumorhavegreat   6 54332111111111111   4 chapter 4

    naive bayes and sentiment classification

probabilities:

p(x|y) =

p(y|x)p(x)

p(y)

we can then substitute eq. 4.2 into eq. 4.1 to get eq. 4.3:
p(d|c)p(c)

  c = argmax

c   c

p(c|d) = argmax
c   c

p(d)

(4.2)

(4.3)

we can conveniently simplify eq. 4.3 by dropping the denominator p(d). this
is possible because we will be computing p(d|c)p(c)
for each possible class. but p(d)
doesn   t change for each class; we are always asking about the most likely class for
the same document d, which must have the same id203 p(d). thus, we can
choose the class that maximizes this simpler formula:

p(d)

  c = argmax

c   c

p(c|d) = argmax
c   c

p(d|c)p(c)

(4.4)

prior
id203
likelihood

we thus compute the most probable class   c given some document d by choosing
the class which has the highest product of two probabilities: the prior id203
of the class p(c) and the likelihood of the document p(d|c):

(cid:122) (cid:125)(cid:124) (cid:123)

likelihood
p(d|c)

(cid:122)(cid:125)(cid:124)(cid:123)

prior
p(c)

  c = argmax

c   c

(4.5)

without loss of generalization, we can represent a document d as a set of features

f1, f2, ..., fn:

(cid:122)

(cid:125)(cid:124)

likelihood

p( f1, f2, ...., fn|c)

(cid:123)

(cid:122)(cid:125)(cid:124)(cid:123)

prior
p(c)

(4.6)

  c = argmax

c   c

unfortunately, eq. 4.6 is still too hard to compute directly: without some sim-
plifying assumptions, estimating the id203 of every possible combination of
features (for example, every possible set of words and positions) would require huge
numbers of parameters and impossibly large training sets. naive bayes classi   ers
therefore make two simplifying assumptions.

the    rst is the bag of words assumption discussed intuitively above: we assume
position doesn   t matter, and that the word    love    has the same effect on classi   cation
whether it occurs as the 1st, 20th, or last word in the document. thus we assume
that the features f1, f2, ..., fn only encode word identity and not position.
the second is commonly called the naive bayes assumption: this is the condi-
tional independence assumption that the probabilities p( fi|c) are independent given
the class c and hence can be    naively    multiplied as follows:

naive bayes
assumption

p( f1, f2, ...., fn|c) = p( f1|c)   p( f2|c)   ...   p( fn|c)

the    nal equation for the class chosen by a naive bayes classi   er is thus:

cnb = argmax

c   c

p(c)

p( f|c)

(cid:89)

f   f

(4.7)

(4.8)

to apply the naive bayes classi   er to text, we need to consider word positions,

by simply walking an index through every word position in the document:

4.2

    training the naive bayes classifier

5

positions     all word positions in test document

cnb = argmax

c   c

p(c)

i   positions

p(wi|c)

(cid:89)

(4.9)

naive bayes calculations, like calculations for id38, are done in
log space, to avoid under   ow and increase speed. thus eq. 4.9 is generally instead
expressed as

(cid:88)

cnb = argmax

c   c

logp(c) +

i   positions

logp(wi|c)

(4.10)

by considering features in log space eq. 4.10 computes the predicted class as
a linear function of input features. classi   ers that use a linear combination of
the inputs to make a classi   cation decision    like naive bayes and also logistic
regression    are called linear classi   ers.

linear
classi   ers

4.2 training the naive bayes classi   er

how can we learn the probabilities p(c) and p( fi|c)? let   s    rst consider the max-
imum likelihood estimate. we   ll simply use the frequencies in the data. for the
document prior p(c) we ask what percentage of the documents in our training set
are in each class c. let nc be the number of documents in our training data with
class c and ndoc be the total number of documents. then:

  p(c) =

nc
ndoc

(4.11)

to learn the id203 p( fi|c), we   ll assume a feature is just the existence of a
word in the document   s bag of words, and so we   ll want p(wi|c), which we compute
as the fraction of times the word wi appears among all words in all documents of
topic c. we    rst concatenate all documents with category c into one big    category
c    text. then we use the frequency of wi in this concatenated document to give a
maximum likelihood estimate of the id203:

  p(wi|c) =

(cid:80)

count(wi,c)
w   v count(w,c)

(4.12)

here the vocabulary v consists of the union of all the word types in all classes,

not just the words in one class c.

there is a problem, however, with maximum likelihood training. imagine we
are trying to estimate the likelihood of the word    fantastic    given class positive, but
suppose there are no training documents that both contain the word    fantastic    and
are classi   ed as positive. perhaps the word    fantastic    happens to occur (sarcasti-
cally?) in the class negative. in such a case the id203 for this feature will be
zero:

6 chapter 4

    naive bayes and sentiment classification

  p(   fantastic   |positive) =

(cid:80)

count(   fantastic   ,positive)
w   v count(w,positive)

= 0

(4.13)

but since naive bayes naively multiplies all the feature likelihoods together, zero
probabilities in the likelihood term for any class will cause the id203 of the
class to be zero, no matter the other evidence!

the simplest solution is the add-one (laplace) smoothing introduced in chap-
ter 3. while laplace smoothing is usually replaced by more sophisticated smoothing
algorithms in id38, it is commonly used in naive bayes text catego-
rization:

  p(wi|c) =

(cid:80)

count(wi,c) + 1
w   v (count(w,c) + 1)

(cid:0)(cid:80)
w   v count(w,c)(cid:1) +|v|

count(wi,c) + 1

=

(4.14)

unknown word

stop words

note once again that it is crucial that the vocabulary v consists of the union of
all the word types in all classes, not just the words in one class c (try to convince
yourself why this must be true; see the exercise at the end of the chapter).

what do we do about words that occur in our test data but are not in our vocab-
ulary at all because they did not occur in any training document in any class? the
solution for such unknown words is to ignore them   remove them from the test
document and not include any id203 for them at all.

finally, some systems choose to completely ignore another class of words: stop
words, very frequent words like the and a. this can be done by sorting the vocabu-
lary by frequency in the training set, and de   ning the top 10   100 vocabulary entries
as stop words, or alternatively by using one of the many pre-de   ned stop word list
available online. then every instance of these stop words are simply removed from
both training and test documents as if they had never occurred. in most text classi-
   cation applications, however, using a stop word list doesn   t improve performance,
and so it is more common to make use of the entire vocabulary and not use a stop
word list.

fig. 4.2 shows the    nal algorithm.

4.3 worked example

let   s walk through an example of training and testing naive bayes with add-one
smoothing. we   ll use a id31 domain with the two classes positive
(+) and negative (-), and take the following miniature training and test documents
simpli   ed from actual movie reviews.

cat

documents

training -
-
-
+
+
?

test

just plain boring
entirely predictable and lacks energy
no surprises and very few laughs
very powerful
the most fun    lm of the summer
predictable with no fun

the prior p(c) for the two classes is computed via eq. 4.11 as nc
ndoc

:

4.3

    worked example

7

function train naive bayes(d, c) returns log p(c) and log p(w|c)
for each class c     c

# calculate p(c) terms

ndoc = number of documents in d
nc = number of documents from d in class c
nc
logprior[c]    log
ndoc
v   vocabulary of d
bigdoc[c]   append(d) for d     d with class c
for each word w in v

# calculate p(w|c) terms

count(w,c)   # of occurrences of w in bigdoc[c]
count(w,c) + 1
loglikelihood[w,c]    log

(cid:80)
w(cid:48) in v (count (w(cid:48),c) + 1)

return logprior, loglikelihood, v

function test naive bayes(testdoc, logprior, loglikelihood, c, v) returns best c
for each class c     c

sum[c]    logprior[c]
for each position i in testdoc
word   testdoc[i]
if word     v

sum[c]   sum[c]+ loglikelihood[word,c]

return argmaxc sum[c]

figure 4.2 the naive bayes algorithm, using add-1 smoothing. to use add-   smoothing
instead, change the +1 to +   for loglikelihood counts in training.

p(   ) =

3
5

p(+) =

2
5

the word with doesn   t occur in the training set, so we drop it completely (as
mentioned above, we don   t use unknown word models for naive bayes). the like-
lihoods from the training set for the remaining three words    predictable   ,    no   , and
   fun   , are as follows, from eq. 4.14 (computing the probabilities for the remainder
of the words in the training set is left as exercise 4.?? (tbd)).

p(   predictable   |   ) =
p(   no   |   ) =
p(   fun   |   ) =

1 + 1
14 + 20
1 + 1
14 + 20
0 + 1
14 + 20

p(   predictable   |+) =
0 + 1
p(   no   |+) =
9 + 20
1 + 1
p(   fun   |+) =
9 + 20

0 + 1
9 + 20

for the test sentence s =    predictable with no fun   , after removing the word

   with   , the chosen class, via eq. 4.9, is therefore computed as follows:

p(   )p(s|   ) =

p(+)p(s|+) =

   2   2   1
   1   1   2

343 = 6.1   10   5
293 = 3.2   10   5

3
5
2
5

8 chapter 4

    naive bayes and sentiment classification

the model thus predicts the class negative for the test sentence.

4.4 optimizing for id31

binary nb

while standard naive bayes text classi   cation can work well for id31,
some small changes are generally employed that improve performance.

first, for sentiment classi   cation and a number of other text classi   cation tasks,
whether a word occurs or not seems to matter more than its frequency. thus it
often improves performance to clip the word counts in each document at 1 (see
the end of the chapter for pointers to these results). this variant is called binary
multinomial naive bayes or binary nb. the variant uses the same eq. 4.10 except
that for each document we remove all duplicate words before concatenating them
into the single big document. fig. 4.3 shows an example in which a set of four
documents (shortened and text-normalized for this example) are remapped to binary,
with the modi   ed counts shown in the table on the right. the example is worked
without add-1 smoothing to make the differences clearer. note that the results counts
need not be 1; the word great has a count of 2 even for binary nb, because it appears
in multiple documents.

four original documents:

boxing scenes

    it was pathetic the worst part was the
    no plot twists or great scenes
+ and satire and great plot twists
+ great scenes great    lm

after per-document binarization:

scenes

    it was pathetic the worst part boxing
    no plot twists or great scenes
+ and satire great plot twists
+ great scenes    lm

binary
nb
counts counts
+     +    
0
2
and
1
0
boxing
0
1
   lm
3
1
great
1
0
it
1
0
no
1
0
or
1
part
0
pathetic 0
1
1
1
plot
0
1
satire
2
1
scenes
1
0
the
twists
1
1
1
0
was
worst
0
1

0
1
0
1
1
1
1
1
1
1
0
2
2
1
2
1

1
0
1
2
0
0
0
0
0
1
1
1
0
1
0
0

figure 4.3 an example of binarization for the binary naive bayes algorithm.

a second important addition commonly made when doing text classi   cation for
sentiment is to deal with negation. consider the difference between i really like this
movie (positive) and i didn   t like this movie (negative). the negation expressed by
didn   t completely alters the id136s we draw from the predicate like. similarly,
negation can modify a negative word to produce a positive review (don   t dismiss this
   lm, doesn   t let us get bored).

a very simple baseline that is commonly used in sentiment to deal with negation
is during text id172 to prepend the pre   x not to every word after a token
of logical negation (n   t, not, no, never) until the next punctuation mark. thus the
phrase

didnt like this movie , but i

4.5

    naive bayes for other text classification tasks

9

becomes

didnt not_like not_this not_movie , but i

newly formed    words    like not like, not recommend will thus occur more of-
ten in negative document and act as cues for negative sentiment, while words like
not bored, not dismiss will acquire positive associations. we will return in chap-
ter 15 to the use of parsing to deal more accurately with the scope relationship be-
tween these negation words and the predicates they modify, but this simple baseline
works quite well in practice.

finally, in some situations we might have insuf   cient labeled training data to
train accurate naive bayes classi   ers using all words in the training set to estimate
positive and negative sentiment. in such cases we can instead derive the positive
and negative word features from sentiment lexicons, lists of words that are pre-
annotated with positive or negative sentiment. four popular lexicons are the general
inquirer (stone et al., 1966), liwc (pennebaker et al., 2007), the opinion lexicon
of hu and liu (2004) and the mpqa subjectivity lexicon (wilson et al., 2005).

for example the mpqa subjectivity lexicon has 6885 words, 2718 positive and
4912 negative, each marked for whether it is strongly or weakly biased. (chapter 19
will discuss how these lexicons can be learned automatically.) some samples of
positive and negative words from the mpqa lexicon include:

sentiment
lexicons

general
inquirer
liwc

+ : admirable, beautiful, con   dent, dazzling, ecstatic, favor, glee, great
    : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate

a common way to use lexicons in a naive bayes classi   er is to add a feature
that is counted whenever a word from that lexicon occurs. thus we might add a
feature called    this word occurs in the positive lexicon   , and treat all instances of
words in the lexicon as counts for that one feature, instead of counting each word
separately. similarly, we might add as a second feature    this word occurs in the
negative lexicon    of words in the negative lexicon. if we have lots of training data,
and if the test data matches the training data, using just two features won   t work as
well as using all the words. but when training data is sparse or not representative of
the test set, using dense lexicon features instead of sparse individual-word features
may generalize better.

4.5 naive bayes for other text classi   cation tasks

spam detection

in the previous section we pointed out that naive bayes doesn   t require that our
classi   er use all the words in the training data as features. in fact features in naive
bayes can express any property of the input text we want.

consider the task of spam detection, deciding if a particular piece of email is
an example of spam (unsolicited bulk email)     and one of the    rst applications of
naive bayes to text classi   cation (sahami et al., 1998).

a common solution here, rather than using all the words as individual features, is
to prede   ne likely sets of words or phrases as features, combined these with features
that are not purely linguistic. for example the open-source spamassassin tool1
prede   nes features like the phrase    one hundred percent guaranteed   , or the feature
mentions millions of dollars, which is a regular expression that matches suspiciously
large sums of money. but it also includes features like html has a low ratio of

1 https://spamassassin.apache.org

10 chapter 4

    naive bayes and sentiment classification

text to image area, that isn   t purely linguistic and might require some sophisticated
computation, or totally non-linguistic features about, say, the path that the email
took to arrive. more sample spamassassin features:

language id

    email subject line is all capital letters
    contains phrases of urgency like    urgent reply   
    email subject line contains    online pharmaceutical   
    html has unbalanced    head    tags
    claims you can be removed from the list
for other tasks, like language id   determining what language a given piece of
text is written in   the most effective naive bayes features are not words at all, but
byte id165s, 2-grams (   zw   ) 3-grams (   nya   ,     vo   ), or 4-grams (   ie z   ,    thei   ).
because spaces count as a byte, byte id165s can model statistics about the begin-
ning or ending of words. 2 a widely used naive bayes system, langid.py (lui
and baldwin, 2012) begins with all possible id165s of lengths 1-4, using feature
selection to winnow down to the most informative 7000    nal features.

language id systems are trained on multilingual text, such as wikipedia (wikipedia

text in 68 different languages were used in (lui and baldwin, 2011)), or newswire.
to make sure that this multilingual text correctly re   ects different regions, dialects,
and socio-economic classes, systems also add twitter text in many languages geo-
tagged to many regions (important for getting world english dialects from countries
with large anglophone populations like nigeria or india), bible and quran transla-
tions, slang websites like urban dictionary, corpora of african american vernacular
english (blodgett et al., 2016), and so on (jurgens et al., 2017).

4.6 naive bayes as a language model

as we saw in the previous section, naive bayes classi   ers can use any sort of fea-
ture: dictionaries, urls, email addresses, network features, phrases, and so on. but
if, as in the previous section, we use only individual word features, and we use all
of the words in the text (not a subset), then naive bayes has an important similar-
ity to id38. speci   cally, a naive bayes model can be viewed as a
set of class-speci   c unigram language models, in which the model for each class
instantiates a unigram language model.
each word p(word|c), the model also assigns a id203 to each sentence:

since the likelihood features from the naive bayes model assign a id203 to

(cid:89)

i   positions

p(s|c) =

p(wi|c)

(4.15)

thus consider a naive bayes model with the classes positive (+) and negative (-)

and the following model parameters:

it   s also possible to use codepoints, which are multi-byte unicode representations of characters in

2
character sets, but simply using bytes seems to work better.

4.7

    evaluation: precision, recall, f-measure

11

p(w|+) p(w|-)
w
0.1
i
love 0.1
this 0.01
fun 0.05
   lm 0.1
...
...

0.2
0.001
0.01
0.005
0.1
...

each of the two columns above instantiates a language model that can assign a

id203 to the sentence    i love this fun    lm   :

p(   i love this fun    lm   |+) = 0.1   0.1   0.01   0.05   0.1 = 0.0000005
p(   i love this fun    lm   |   ) = 0.2   0.001   0.01   0.005   0.1 = .0000000010

as it happens, the positive model assigns a higher id203 to the sentence:
p(s|pos) > p(s|neg). note that this is just the likelihood part of the naive bayes
model; once we multiply in the prior a full naive bayes model might well make a
different classi   cation decision.

4.7 evaluation: precision, recall, f-measure

gold labels

contingency
table

to introduce the methods for evaluating text classi   cation, let   s    rst consider some
simple binary detection tasks. for example, in spam detection, our goal is to label
every text as being in the spam category (   positive   ) or not in the spam category
(   negative   ). for each item (email document) we therefore need to know whether
our system called it spam or not. we also need to know whether the email is actually
spam or not, i.e. the human-de   ned labels for each document that we are trying to
match. we will refer to these human labels as the gold labels.

or imagine you   re the ceo of the delicious pie company and you need to know
what people are saying about your pies on social media, so you build a system that
detects tweets concerning delicious pie. here the positive class is tweets about
delicious pie and the negative class is all other tweets.

in both cases, we need a metric for knowing how well our spam detector (or
pie-tweet-detector) is doing. to evaluate any system for detecting things, we start
by building a contingency table like the one shown in fig. 4.4. each cell labels a
set of possible outcomes. in the spam detection case, for example, true positives are
documents that are indeed spam (indicated by human-created gold labels) and our
system said they were spam. false negatives are documents that are indeed spam
but our system labeled as non-spam.

to the bottom right of the table is the equation for accuracy, which asks what
percentage of all the observations (for the spam or pie examples that means all emails
or tweets) our system labeled correctly. although accuracy might seem a natural
metric, we generally don   t use it. that   s because accuracy doesn   t work well when
the classes are unbalanced (as indeed they are with spam, which is a large majority
of email, or with tweets, which are mainly not about pie).

to make this more explicit, imagine that we looked at a million tweets, and
let   s say that only 100 of them are discussing their love (or hatred) for our pie,
while the other 999,900 are tweets about something completely unrelated. imagine a

12 chapter 4

    naive bayes and sentiment classification

figure 4.4 contingency table

simple classi   er that stupidly classi   ed every tweet as    not about pie   . this classi   er
would have 999,900 true negatives and only 100 false negatives for an accuracy of
999,900/1,000,000 or 99.99%! what an amazing accuracy level! surely we should
be happy with this classi   er? but of course this fabulous    no pie    classi   er would
be completely useless, since it wouldn   t    nd a single one of the customer comments
we are looking for. in other words, accuracy is not a good metric when the goal is
to discover something that is rare, or at least not completely balanced in frequency,
which is a very common situation in the world.

that   s why instead of accuracy we generally turn to two other metrics: precision
and recall. precision measures the percentage of the items that the system detected
(i.e., the system labeled as positive) that are in fact positive (i.e., are positive accord-
ing to the human gold labels). precision is de   ned as

precision

precision =

true positives

true positives + false positives

recall

recall measures the percentage of items actually present in the input that were

correctly identi   ed by the system. recall is de   ned as

recall =

true positives

true positives + false negatives

precision and recall will help solve the problem with the useless    nothing is
pie    classi   er. this classi   er, despite having a fabulous accuracy of 99.99%, has
a terrible recall of 0 (since there are no true positives, and 100 false negatives, the
recall is 0/100). you should convince yourself that the precision at    nding relevant
tweets is equally problematic. thus precision and recall, unlike accuracy, emphasize
true positives:    nding the things that we are supposed to be looking for.

there are many ways to de   ne a single metric that incorporates aspects of both
precision and recall. the simplest of these combinations is the f-measure (van
rijsbergen, 1975) , de   ned as:

f-measure

f   =

(   2 + 1)pr
   2p + r

the    parameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. values of    > 1 favor recall, while
values of    < 1 favor precision. when    = 1, precision and recall are equally bal-
anced; this is the most frequently used metric, and is called f   =1 or just f1:

f1

true positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn4.7

    evaluation: precision, recall, f-measure

13

f1 =

2pr
p + r

(4.16)

f-measure comes from a weighted harmonic mean of precision and recall. the
harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-
rocals:

harmonicmean(a1,a2,a3,a4, ...,an) =

and hence f-measure is

f =

1

p + (1      ) 1
   1

r

(cid:18)

or

with    2 =

1      
  

1
a1

+ 1
a2

(cid:19)

n
+ 1
a3

+ ... + 1
an

(4.17)

f =

(   2 + 1)pr
   2p + r

(4.18)

harmonic mean is used because it is a conservative metric; the harmonic mean of
two values is closer to the minimum of the two values than the arithmetic mean is.
thus it weighs the lower of the two numbers more heavily.

4.7.1 more than two classes
up to now we have been assuming text classi   cation tasks with only two classes.
but lots of classi   cation tasks in language processing have more than two classes.
for id31 we generally have 3 classes (positive, negative, neutral) and
even more classes are common for tasks like part-of-speech tagging, word sense
disambiguation, id14, emotion detection, and so on.

there are two kinds of multi-class classi   cation tasks. in any-of or multi-label
classi   cation, each document or item can be assigned more than one label. we can
solve any-of classi   cation by building separate binary classi   ers for each class c,
trained on positive examples labeled c and negative examples not labeled c. given
a test document or item d, then each classi   er makes their decision independently,
and we may assign multiple labels to d.

more common in language processing is one-of or multinomial classi   cation,
in which the classes are mutually exclusive and each document or item appears in
exactly one class. here we again build a separate binary classi   er trained on positive
examples from c and negative examples from all other classes. now given a test
document or item d, we run all the classi   ers and choose the label from the classi   er
with the highest score. consider the sample confusion matrix for a hypothetical 3-
way one-of email categorization decision (urgent, normal, spam) shown in fig. 4.5.
the matrix shows, for example, that the system mistakenly labeled 1 spam doc-
ument as urgent, and we have shown how to compute a distinct precision and recall
value for each class. in order to derive a single metric that tells us how well the
system is doing, we can combine these values in two ways. in macroaveraging, we
compute the performance for each class, and then average over classes. in microav-
eraging, we collect the decisions for all classes into a single contingency table, and
then compute precision and recall from that table. fig. 4.6 shows the contingency
table for each class separately, and shows the computation of microaveraged and
macroaveraged precision.

as the    gure shows, a microaverage is dominated by the more frequent class (in
this case spam), since the counts are pooled. the macroaverage better re   ects the
statistics of the smaller classes, and so is more appropriate when performance on all
the classes is equally important.

any-of

one-of
multinomial
classi   cation

macroaveraging

microaveraging

14 chapter 4

    naive bayes and sentiment classification

figure 4.5 confusion matrix for a three-class categorization task, showing for each pair of
classes (c1,c2), how many documents from c1 were (in)correctly assigned to c2

figure 4.6 separate contingency tables for the 3 classes from the previous    gure, showing the pooled contin-
gency table and the microaveraged and macroaveraged precision.
4.8 test sets and cross-validation

development
test set
devset

cross-validation

10-fold
cross-validation

the training and testing procedure for text classi   cation follows what we saw with
id38 (section ??): we use the training set to train the model, then use
the development test set (also called a devset) to perhaps tune some parameters,
and in general decide what the best model is. once we come up with what we think
is the best model, we run it on the (hitherto unseen) test set to report its performance.
while the use of a devset avoids over   tting the test set, having a    xed training
set, devset, and test set creates another problem: in order to save lots of data for
training, the test set (or devset) might not be large enough to be representative. it
would be better if we could somehow use all our data both for training and test. we
do this by cross-validation: we randomly choose a training and test set division of
our data, train our classi   er, and then compute the error rate on the test set. then
we repeat with a different randomly selected training set and test set. we do this
sampling process 10 times and average these 10 runs to get an average error rate.
this is called 10-fold cross-validation.

the only problem with cross-validation is that because all the data is used for
testing, we need the whole corpus to be blind; we can   t examine any of the data
to suggest possible features and in general see what   s going on. but looking at the
corpus is often important for designing the system. for this reason, it is common

851060urgentnormalgold labelssystemoutputrecallu = 88+5+3precisionu= 88+10+115030200spamurgentnormalspam3recalln = recalls = precisionn= 605+60+50precisions= 2003+30+2006010+60+302001+50+2008811340trueurgenttruenotsystemurgentsystemnot604055212truenormaltruenotsystemnormalsystemnot200513383truespamtruenotsystemspamsystemnot2689999635trueyestruenosystemyessystemnoprecision =8+118= .42precision =200+33200= .86precision =60+5560= .52microaverageprecision268+99268= .73=macroaverageprecision3.42+.52+.86= .60=pooledclass 3: spamclass 2: normalclass 1: urgent4.9

    statistical significance testing

15

to create a    xed training set and test set, then do 10-fold cross-validation inside
the training set, but compute error rate the normal way in the test set, as shown in
fig. 4.7.

figure 4.7

10-fold cross-validation

4.9 statistical signi   cance testing

null hypothesis

in building systems we are constantly comparing the performance of systems. often
we have added some new bells and whistles to our algorithm and want to compare
the new version of the system to the unaugmented version. or we want to compare
our algorithm to a previously published one to know which is better.

we might imagine that to compare the performance of two classi   ers a and b
all we have to do is look at a and b   s score on the same test set   for example we
might choose to compare macro-averaged f1    and see whether it   s a or b that has
the higher score. but just looking at this one difference isn   t good enough, because
a might have a better performance than b on a particular test set just by chance.

let   s say we have a test set x of n observations x = x1,x2, ..,xn on which a   s
performance is better than b by    (x). how can we know if a is really better than b?
to do so we   d need to reject the null hypothesis that a isn   t really better than b and
this difference    (x) occurred purely by chance. if the null hypothesis was correct,
we would expect that if we had many test sets of size n and we measured a and b   s
performance on all of them, that on average a might accidentally still be better than
b by this amount    (x) just by chance.
more formally, if we had a random variable x ranging over test sets, the null
hypothesis h0 expects p(   (x) >    (x)|h0), the id203 that we   ll see similarly
big differences just by chance, to be high.
if we had all these test sets we could just measure all the    (x(cid:48)) for all the x(cid:48). if we
found that those deltas didn   t seem to be bigger than    (x), that is, that p-value(x) was
suf   ciently small, less than the standard thresholds of 0.05 or 0.01, then we might
reject the null hypothesis and agree that    (x) was a suf   ciently surprising difference
and a is really a better algorithm than b. following berg-kirkpatrick et al. (2012)
we   ll refer to p(   (x) >    (x)|h0) as p-value(x).

in language processing we don   t generally use traditional statistical approaches
like paired t-tests to compare system outputs because most metrics are not normally

training iterations13452678910devdevdevdevdevdevdevdevdevdevtrainingtrainingtrainingtrainingtrainingtrainingtrainingtrainingtrainingtrainingtrainingtest settesting16 chapter 4

    naive bayes and sentiment classification

bootstrap test
approximate
randomization

id64

distributed, violating the assumptions of the tests. the standard approach to comput-
ing p-value(x) in natural language processing is to use non-parametric tests like the
bootstrap test (efron and tibshirani, 1993)    which we will describe below   or a
similar test, approximate randomization (noreen, 1989). the advantage of these
tests is that they can apply to any metric; from precision, recall, or f1 to the id7
metric used in machine translation.

the word id64 refers to repeatedly drawing large numbers of smaller
samples with replacement (called bootstrap samples) from an original larger sam-
ple. the intuition of the bootstrap test is that we can create many virtual test sets
from an observed test set by repeatedly sampling from it. the method only makes
the assumption that the sample is representative of the population.

consider a tiny text classi   cation example with a test set x of 10 documents. the
   rst row of fig. 4.8 shows the results of two classi   ers (a and b) on this test set,
with each document labeled by one of the four possibilities: (a and b both right,
both wrong, a right and b wrong, a wrong and b right); a slash through a letter
( b) means that that classi   er got the answer wrong. on the    rst document both a
and b get the correct class (ab), while on the second document a got it right but b
got it wrong (a b). if we assume for simplicity that our metric is accuracy, a has an
accuracy of .70 and b of .50, so    (x) is .20. to create each virtual test set of size
n = 10, we repeatedly (10 times) select a cell from row x with replacement. fig. 4.8
shows a few examples.

9

8

7

6

5

4

3

2

10 a% b%    ()
1
x
ab a  b ab   ab a  b   ab a  b ab   a  b a  b .70 .50 .20
x   (1) a  b ab a  b   ab   ab a  b   ab ab   a  b ab .60 .60 .00
x   (2) a  b ab   a  b   ab   ab ab   ab a  b ab ab .60 .70 -.10
...
x   (b)
figure 4.8 the bootstrap: examples of b pseudo test sets being created from an initial true
test set x. each pseudo test set is created by sampling n = 10 times with replacement; thus an
individual sample is a single cell, a document with its gold label and the correct or incorrect
performance of classi   ers a and b.

now that we have a sampling distribution, we can do statistics on how how often
a has an accidental advantage. there are various ways to compute this advantage;
here we follow the version laid out in berg-kirkpatrick et al. (2012). we might
think that we should just ask, for each bootstrap sample x   (i), whether a beats b
by more than    (x). but there   s a problem: we didn   t draw these samples from a
distribution with 0 mean. the x   (i) were sampled from x, and so the expected value
of    (x   (i)) lies very close to    (x). that is, about half the time a will be better than
b, so we expect a to beat b by    (x). instead, we want to know how often a beats
these expectations by more than    (x). to correct for the expected success, we need
to zero-center, subtracting    (x) from each pseudo test set. thus we   ll be comparing
for each x   (i) whether    (x   (i)) > 2   (x). the full algorithm for the bootstrap is shown
in fig. 4.9. it is given a test set x, a number of samples b, and counts the percentage
of the b bootstrap test sets in which delta(x   (i)) > 2   (x). this percentage then
acts as a one-sided empirical p-value (more sophisticated ways to get p-values from
con   dence intervals also exist).

4.10

    advanced: feature selection

17

function bootstrap(test set x, num of samples b) returns p-value(x)

calculate    (x) # how much better does algorithm a do than b on x
for i = 1 to b do

# draw a bootstrap sample x   (i) of size n

select a member of x at random and add it to x   (i)

for j = 1 to n do
calculate    (x   (i)) # how much better does algorithm a do than b on x   (i)
s   s + 1 if    (x   (i)) > 2   (x)

# on what % of the b samples did algorithm a beat expectations?

for each x   (i)
p-value(x)     s
b
return p-value(x)

figure 4.9 a version of the bootstrap algorithm after berg-kirkpatrick et al. (2012).

4.10 advanced: feature selection

feature
selection

information
gain

the id173 technique introduced in the previous section is feature selection
is a method of removing features that are unlikely to generalize well. the basis
of feature selection is to assign some metric of goodness to each feature, rank the
features, and keep the best ones. the number of features to keep is a meta-parameter
that can be optimized on a dev set.

features are generally ranked by how informative they are about the classi   ca-
tion decision. a very common metric is information gain. information gain tells
us how many bits of information the presence of the word gives us for guessing the
class, and can be computed as follows (where ci is the ith class and   w means that a
document does not contain the word w):

g(w) =    

p(ci)logp(ci)

c(cid:88)

c(cid:88)
c(cid:88)

i=1

i=1

+p(w)

+p(   w)

p(ci|w)logp(ci|w)

p(ci|   w)logp(ci|   w)

(4.19)

4.11 summary

i=1

this chapter introduced the naive bayes model for classi   cation and applied it to
the text categorization task of id31.

learn to model the class given the observation.

    many language processing tasks can be viewed as tasks of classi   cation.
    text categorization, in which an entire text is assigned a class from a    nite set,
includes such tasks as id31, spam detection, language identi-
   cation, and authorship attribution.
    id31 classi   es a text as re   ecting the positive or negative orien-

tation (sentiment) that a writer expresses toward some object.

18 chapter 4

    naive bayes and sentiment classification

    naive bayes is a generative model that make the bag of words assumption
(position doesn   t matter) and the conditional independence assumption (words
are conditionally independent of each other given the class)

    naive bayes with binarized features seems to work better for many text clas-

si   cation tasks.

helpful.

    feature selection can be used to automatically remove features that aren   t

    classi   ers are evaluated based on precision and recall.
    classi   ers are trained using distinct training, dev, and test sets, including the

use of cross-validation in the training set.

bibliographical and historical notes

multinomial naive bayes text classi   cation was proposed by maron (1961) at the
rand corporation for the task of assigning subject categories to journal abstracts.
his model introduced most of the features of the modern form presented here, ap-
proximating the classi   cation task with one-of categorization, and implementing
add-   smoothing and information-based feature selection.

the conditional independence assumptions of naive bayes and the idea of bayes-
ian analysis of text seem to have been arisen multiple times. the same year as
maron   s paper, minsky (1961) proposed a naive bayes classi   er for vision and other
arti   cial intelligence problems, and bayesian techniques were also applied to the
text classi   cation task of authorship attribution by mosteller and wallace (1963). it
had long been known that alexander hamilton, john jay, and james madison wrote
the anonymously-published federalist papers. in 1787   1788 to persuade new york
to ratify the united states constitution. yet although some of the 85 essays were
clearly attributable to one author or another, the authorship of 12 were in dispute
between hamilton and madison. mosteller and wallace (1963) trained a bayesian
probabilistic model of the writing of hamilton and another model on the writings
of madison, then computed the maximum-likelihood author for each of the disputed
essays. naive bayes was    rst applied to spam detection in heckerman et al. (1998).
metsis et al. (2006), pang et al. (2002), and wang and manning (2012) show
that using boolean attributes with multinomial naive bayes works better than full
counts. binary multinomial naive bayes is sometimes confused with another variant
of naive bayes that also use a binary representation of whether a term occurs in
a document: multivariate bernoulli naive bayes. the bernoulli variant instead
estimates p(w|c) as the fraction of documents that contain a term, and includes a
id203 for whether a term is not in a document. mccallum and nigam (1998)
and wang and manning (2012) show that the multivariate bernoulli variant of naive
bayes doesn   t work as well as the multinomial algorithm for sentiment or other text
tasks.

there are a variety of sources covering the many kinds of text classi   cation
tasks. for id31 see pang and lee (2008), and liu and zhang (2012).
stamatatos (2009) surveys authorship attribute algorithms. on language identi   ca-
tion see jauhiainen et al. (2018); jaech et al. (2016) is an important early neural
system. the task of newswire indexing was often used as a test case for text classi-
   cation algorithms, based on the reuters-21578 collection of newswire articles.

see manning et al. (2008) and aggarwal and zhai (2012) on text classi   cation;
classi   cation in general is covered in machine learning textbooks (hastie et al. 2001,

exercises

19

witten and frank 2005, bishop 2006, murphy 2012).

non-parametric methods for computing statistical signi   cance were used    rst in
nlp in the muc competition (chinchor et al., 1993), and even earlier in speech
recognition (gillick and cox 1989, bisani and ney 2004). our description of the
bootstrap draws on the description in berg-kirkpatrick et al. (2012). recent work
has focused on issues including multiple test sets and multiple metrics (s  gaard
et al. 2014, dror et al. 2017).

metrics besides information gain for feature selection include   2, pointwise mu-
tual information, and gini index; see yang and pedersen (1997) for a comparison
and guyon and elisseeff (2003) for a broad introduction survey of feature selection.

exercises

4.1 assume the following likelihoods for each word being part of a positive or

negative movie review, and equal prior probabilities for each class.

pos neg
i
0.09 0.16
always 0.07 0.06
like
0.29 0.06
foreign 0.04 0.15
   lms
0.08 0.11

what class will naive bayes assign to the sentence    i always like foreign
   lms.   ?

4.2 given the following short movie reviews, each labeled with a genre, either

comedy or action:

comedy

1. fun, couple, love, love
2. fast, furious, shoot action
3. couple,    y, fast, fun, fun comedy
4. furious, shoot, shoot, fun action
5.    y, fast, shoot, love action

and a new document d:

fast, couple, shoot,    y

4.3

compute the most likely class for d. assume a naive bayes classi   er and use
add-1 smoothing for the likelihoods.
train two models, multinominal naive bayes and binarized naive bayes, both
with add-1 smoothing, on the following document counts for key sentiment
words, with positive or negative class assigned as noted.
doc    good       poor       great    (class)
d1. 3
d2. 0
d3. 1
d4. 1
d5. 0
use both naive bayes models to assign a class (pos or neg) to this sentence:

pos
pos
neg
neg
neg

0
1
3
5
2

3
2
0
2
0

a good, good plot and great characters, but poor acting.

do the two models agree or disagree?

20 chapter 4     naive bayes and sentiment classi   cation

aggarwal, c. c. and zhai, c. (2012). a survey of text classi-
   cation algorithms. in aggarwal, c. c. and zhai, c. (eds.),
mining text data, pp. 163   222. springer.

lui, m. and baldwin, t. (2011). cross-domain feature selec-
tion for language identi   cation. in ijcnlp-11, pp. 553   
561.

bayes, t. (1763). an essay toward solving a problem in the
doctrine of chances, vol. 53. reprinted in facsimiles of
two papers by bayes, hafner publishing, 1963.

berg-kirkpatrick, t., burkett, d., and klein, d. (2012). an
empirical investigation of statistical signi   cance in nlp. in
emnlp 2012, pp. 995   1005.

bisani, m. and ney, h. (2004). bootstrap estimates for
in

con   dence intervals in asr performance evaluation.
icassp-04, vol. i, pp. 409   412.

bishop, c. m. (2006). pattern recognition and machine

learning. springer.

blodgett, s. l., green, l., and o   connor, b. (2016). demo-
graphic dialectal variation in social media: a case study of
african-american english. in emnlp 2016.

borges, j. l. (1964).

the analytical language of john
wilkins. university of texas press. trans. ruth l. c.
simms.

chinchor, n., hirschman, l., and lewis, d. l. (1993). eval-
uating message understanding systems: an analysis of the
third message understanding conference. computational
linguistics, 19(3), 409   449.

dror, r., baumer, g., bogomolov, m., and reichart, r.
(2017). replicability analysis for natural language process-
ing: testing signi   cance with multiple datasets. tacl, 5,
471   486.

efron, b. and tibshirani, r. j. (1993). an introduction to the

bootstrap. crc press.

gillick, l. and cox, s. j. (1989). some statistical issues
in

in the comparison of id103 algorithms.
icassp-89, pp. 532   535.

guyon, i. and elisseeff, a. (2003). an introduction to vari-
able and feature selection. the journal of machine learn-
ing research, 3, 1157   1182.

hastie, t., tibshirani, r. j., and friedman, j. h. (2001). the

elements of statistical learning. springer.

heckerman, d., horvitz, e., sahami, m., and dumais, s. t.
(1998). a bayesian approach to    ltering junk e-mail. in
proceeding of aaai-98 workshop on learning for text
categorization, pp. 55   62.

hu, m. and liu, b. (2004). mining and summarizing cus-

tomer reviews. in kdd, pp. 168   177.

jaech, a., mulcaire, g., hathi, s., ostendorf, m., and smith,
n. a. (2016). hierarchical character-word models for lan-
guage identi   cation. in acl workshop on nlp for social
media, pp. 84   93.

jauhiainen, t., lui, m., zampieri, m., baldwin, t., and
lind  en, k. (2018). automatic language identi   cation in
texts: a survey. arxiv preprint arxiv:1804.08186.

jurgens, d., tsvetkov, y., and jurafsky, d. (2017). incorpo-
rating dialectal variability for socially equitable language
identi   cation. in acl 2017, pp. 51   57.

liu, b. and zhang, l. (2012). a survey of opinion mining
and id31. in aggarwal, c. c. and zhai, c.
(eds.), mining text data, pp. 415   464. springer.

lui, m. and baldwin, t. (2012). langid.py: an off-the-
shelf language identi   cation tool. in acl 2012, pp. 25   30.
manning, c. d., raghavan, p., and sch  utze, h. (2008). in-

troduction to information retrieval. cambridge.

maron, m. e. (1961). automatic indexing: an experimental

inquiry. journal of the acm (jacm), 8(3), 404   417.

mccallum, a. and nigam, k. (1998). a comparison of event
models for naive bayes text classi   cation. in aaai/icml-
98 workshop on learning for text categorization, pp. 41   
48.

metsis, v., androutsopoulos, i., and paliouras, g. (2006).
in

spam    ltering with naive bayes-which naive bayes?.
ceas, pp. 27   28.

minsky, m. (1961). steps toward arti   cial intelligence. pro-

ceedings of the ire, 49(1), 8   30.

mosteller, f. and wallace, d. l. (1963). id136 in an au-
thorship problem: a comparative study of discrimination
methods applied to the authorship of the disputed federal-
ist papers. journal of the american statistical association,
58(302), 275   309.

mosteller, f. and wallace, d. l. (1964). id136 and dis-
puted authorship: the federalist. springer-verlag. a
second edition appeared in 1984 as applied bayesian and
classical id136.

murphy, k. p. (2012). machine learning: a probabilistic

perspective. mit press.

noreen, e. w. (1989). computer intensive methods for test-

ing hypothesis. wiley.

pang, b. and lee, l. (2008). opinion mining and sentiment
analysis. foundations and trends in information retrieval,
2(1-2), 1   135.

pang, b., lee, l., and vaithyanathan, s. (2002). thumbs
up? sentiment classi   cation using machine learning tech-
niques. in emnlp 2002, pp. 79   86.

pennebaker, j. w., booth, r. j., and francis, m. e. (2007).
linguistic inquiry and word count: liwc 2007. austin,
tx.

sahami, m., dumais, s. t., heckerman, d., and horvitz, e.
(1998). a bayesian approach to    ltering junk e-mail. in
aaai workshop on learning for text categorization, pp.
98   105.

s  gaard, a., johannsen, a., plank, b., hovy, d., and alonso,
h. m. (2014). what   s in a p-value in nlp?. in conll-14.
stamatatos, e. (2009). a survey of modern authorship attri-

bution methods. jasist, 60(3), 538   556.

stone, p., dunphry, d., smith, m., and ogilvie, d. (1966).
the general inquirer: a computer approach to content
analysis. cambridge, ma: mit press.

van rijsbergen, c. j. (1975). information retrieval. butter-

worths.

wang, s. and manning, c. d. (2012). baselines and bigrams:
in acl

simple, good sentiment and topic classi   cation.
2012, pp. 90   94.

wilson, t., wiebe, j., and hoffmann, p. (2005). recogniz-
ing contextual polarity in phrase-level id31.
in hlt-emnlp-05, pp. 347   354.

witten, i. h. and frank, e. (2005). data mining: practical
machine learning tools and techniques (2nd ed.). mor-
gan kaufmann.

yang, y. and pedersen, j. (1997). a comparative study on
feature selection in text categorization. in icml, pp. 412   
420.

exercises

21

