speech and language processing. daniel jurafsky & james h. martin.
rights reserved.

draft of september 23, 2018.

copyright c(cid:13) 2018.

all

chapter

5 id28

logistic
regression

   and how do you know that these    ne begonias are not of equal importance?   
hercule poirot, in agatha christie   s the mysterious affair at styles
detective stories are as littered with clues as texts are with words. yet for the
poor reader it can be challenging to know how to weigh the author   s clues in order
to make the crucial classi   cation task: deciding whodunnit.

in this chapter we introduce an algorithm that is admirably suited for discovering
the link between features or cues and some particular outcome: id28.
indeed, id28 is one of the most important analytic tool in the social and
natural sciences. in natural language processing, id28 is the baseline
supervised machine learning algorithm for classi   cation, and also has a very close
relationship with neural networks. as we will see in chapter 7, a neural network can
be viewed as a series of id28 classi   ers stacked on top of each other.
thus the classi   cation and machine learning techniques introduced here will play
an important role throughout the book.

id28 can be used to classify an observation into one of two classes
(like    positive sentiment    and    negative sentiment   ), or into one of many classes.
because the mathematics for the two-class case is simpler, we   ll describe this special
case of id28    rst in the next few sections, and then brie   y summarize
the use of multinomial id28 for more than two classes in section 5.6.
we   ll introduce the mathematics of id28 in the next few sections.

but let   s begin with some high-level issues.
generative and discriminative classi   ers: the most important difference be-
tween naive bayes and id28 is that id28 is a discrimina-
tive classi   er while naive bayes is a generative classi   er.

these are two very different frameworks for how
to build a machine learning model. consider a visual
metaphor:
imagine we   re trying to distinguish dog
images from cat images. a generative model would
have the goal of understanding what dogs look like
and what cats look like. you might literally ask such
a model to    generate   , i.e. draw, a dog. given a test
image, the system then asks whether it   s the cat model or the dog model that better
   ts (is less surprised by) the image, and chooses that as its label.

a discriminative model, by contrast, is only try-
ing to learn to distinguish the classes (perhaps with-
out learning much about them). so maybe all the
dogs in the training data are wearing collars and the
cats aren   t. if that one feature neatly separates the
classes, the model is satis   ed.
if you ask such a
model what it knows about cats all it can say is that
they don   t wear collars.

2 chapter 5

    id28

more formally, recall that the naive bayes assigns a class c to a document d not

by directly computing p(c|d) but by computing a likelihood and a prior

(cid:122) (cid:125)(cid:124) (cid:123)

likelihood
p(d|c)

(cid:122)(cid:125)(cid:124)(cid:123)

prior
p(c)

  c = argmax

c   c

(5.1)

generative
model

discriminative
model

a generative model like naive bayes makes use of this likelihood term, which
expresses how to generate the features of a document if we knew it was of class c.
by contrast a discriminative model in this text categorization scenario attempts
to directly compute p(c|d). perhaps it will learn to assign high weight to document
features that directly improve its ability to discriminate between possible classes,
even if it couldn   t generate an example of one of the classes.
components of a probabilistic machine learning classi   er: like naive bayes,
id28 is a probabilistic classi   er that makes use of supervised machine
learning. machine learning classi   ers require a training corpus of m observations
input/output pairs (x(i),y(i)). (we   ll use superscripts in parentheses to refer to indi-
vidual instances in the training set   for sentiment classi   cation each instance might
be an individual document to be classi   ed). a machine learning system for classi   -
cation then has four components:

1. a feature representation of the input. for each input observation x(i), this
will be a vector of features [x1,x2, ...,xn]. we will generally refer to feature
i for input x( j) as x( j)
, sometimes simpli   ed as xi, but we will also see the
i
notation fi, fi(x), or, for multiclass classi   cation, fi(c,x).
2. a classi   cation function that computes   y, the estimated class, via p(y|x). in
the next section we will introduce the sigmoid and softmax tools for classi   -
cation.

3. an objective function for learning, usually involving minimizing error on

training examples. we will introduce the cross-id178 id168

4. an algorithm for optimizing the objective function. we introduce the stochas-

tic id119 algorithm.

id28 has two phases:
training: we train the system (speci   cally the weights w and b) using stochastic
test: given a test example x we compute p(y|x) and return the higher id203

id119 and the cross-id178 loss.

label y = 1 or y = 0.

5.1 classi   cation: the sigmoid

the goal of binary id28 is to train a classi   er that can make a binary
decision about the class of a new input observation. here we introduce the sigmoid
classi   er that will help us make this decision.

consider a single input observation x, which we will represent by a vector of
features [x1,x2, ...,xn] (we   ll show sample features in the next subsection). the clas-
si   er output y can be 1 (meaning the observation is a member of the class) or 0
(the observation is not a member of the class). we want to know the id203
p(y = 1|x) that this observation is a member of the class. so perhaps the decision

bias term
intercept

5.1

    classification: the sigmoid

3

is    positive sentiment    versus    negative sentiment   , the features represent counts
of words in a document, and p(y = 1|x) is the id203 that the document has
positive sentiment, while and p(y = 0|x) is the id203 that the document has
negative sentiment.

id28 solves this task by learning, from a training set, a vector of
weights and a bias term. each weight wi is a real number, and is associated with one
of the input features xi. the weight wi represents how important that input feature is
to the classi   cation decision, and can be positive (meaning the feature is associated
with the class) or negative (meaning the feature is not associated with the class).
thus we might expect in a sentiment task the word awesome to have a high positive
weight, and abysmal to have a very negative weight. the bias term, also called the
intercept, is another real number that   s added to the weighted inputs.

to make a decision on a test instance    after we   ve learned the weights in
training    the classi   er    rst multiplies each xi by its weight wi, sums up the weighted
features, and adds the bias term b. the resulting single number z expresses the
weighted sum of the evidence for the class.

(cid:32) n(cid:88)

(cid:33)

z =

wixi

+ b

(5.2)

i=1

dot product

in the rest of the book we   ll represent such sums using the dot product notation from
id202. the dot product of two vectors a and b, written as a   b is the sum of
the products of the corresponding elements of each vector. thus the following is an
equivalent formation to eq. 5.2:

z = w   x + b

(5.3)

but note that nothing in eq. 5.3 forces z to be a legal id203, that is, to lie
between 0 and 1. in fact, since weights are real-valued, the output might even be
negative; z ranges from        to    .

figure 5.1 the sigmoid function y = 1
1+e   z takes a real value and maps it to the range [0,1].
because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squash
outlier values toward 0 or 1.

sigmoid

logistic
function

to create a id203, we   ll pass z through the sigmoid function,    (z). the
sigmoid function (named because it looks like an s) is also called the logistic func-
tion, and gives id28 its name. the sigmoid has the following equation,
shown graphically in fig. 5.1:

y =    (z) =

1

1 + e   z

(5.4)

4 chapter 5

    id28

the sigmoid has a number of advantages; it take a real-valued number and maps
it into the range [0,1], which is just what we want for a id203. because it is
nearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier
values toward 0 or 1. and it   s differentiable, which as we   ll see in section 5.8 will
be handy for learning.

we   re almost there. if we apply the sigmoid to the sum of the weighted features,
we get a number between 0 and 1. to make it a id203, we just need to make
sure that the two cases, p(y = 1) and p(y = 0), sum to 1. we can do this as follows:

p(y = 1) =    (w   x + b)

=

1

1 + e   (w  x+b)

p(y = 0) = 1       (w   x + b)

= 1   

1

1 + e   (w  x+b)
e   (w  x+b)
1 + e   (w  x+b)

=

(5.5)

decision
boundary

now we have an algorithm that given an instance x computes the id203
p(y = 1|x). how do we make a decision? for a test instance x, we say yes if the
id203 p(y = 1|x) is more than .5, and no otherwise. we call .5 the decision
boundary:

(cid:26) 1 if p(y = 1|x) > 0.5

0 otherwise

  y =

5.1.1 example: sentiment classi   cation
let   s have an example. suppose we are doing binary sentiment classi   cation on
movie review text, and we would like to know whether to assign the sentiment class
+ or     to a review document doc. we   ll represent each input observation by the
following 6 features x1...x6 of the input; fig. 5.2 shows the features in a sample mini
test document.

count(positive lexicon)     doc)
count(negative lexicon)     doc)

(cid:26) 1 if    no        doc
(cid:26) 1 if    !        doc

var de   nition
x1
x2
x3
x4
x5
x6

0 otherwise

0 otherwise

log(word count of doc)

count(1st and 2nd pronouns     doc)

value in fig. 5.2
3
2

1
3

0
ln(64) = 4.15

let   s assume for the moment that we   ve already learned a real-valued weight
for each of these features, and that the 6 weights corresponding to the 6 features
are [2.5,   5.0,   1.2,0.5,2.0,0.7], while b = 0.1. (we   ll discuss in the next section
how the weights are learned.) the weight w1, for example indicates how important

5.1

    classification: the sigmoid

5

figure 5.2 a sample mini test document showing the extracted features in the vector x.

a feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to
a positive sentiment decision, while w2 tells us the importance of negative lexicon
words. note that w1 = 2.5 is positive, while w2 =    5.0, meaning that negative words
are negatively associated with a positive sentiment decision, and are about twice as
important as positive words.
given these 6 features and the input review x, p(+|x) and p(   |x) can be com-

puted using eq. 5.5:

p(+|x) = p(y = 1|x) =    (w   x + b)

=    ([2.5,   5.0,   1.2,0.5,2.0,0.7]   [3,2,1,3,0,4.15] + 0.1)
=    (1.805)
= 0.86

p(   |x) = p(y = 0|x) = 1       (w   x + b)

= 0.14

id28 is commonly applied to all sorts of nlp tasks, and any prop-
erty of the input can be a feature. consider the task of period disambiguation:
deciding if a period is the end of a sentence or part of a word, by classifying each
period into one of two classes eos (end-of-sentence) and not-eos. we might use
features like x1 below expressing that the current word is lower case and the class
is eos (perhaps with a positive weight), or that the current word is in our abbrevia-
tions dictionary (   prof.   ) and the class is eos (perhaps with a negative weight). a
feature can also express a quite complex combination of properties. for example a
period following a upper cased word is a likely to be an eos, but if the word itself is
st. and the previous word is capitalized, then the period is likely part of a shortening
of the word street.

0 otherwise

(cid:26) 1 if    case(wi) = lower   
(cid:26) 1 if    wi     acronymdict   
(cid:26) 1 if    wi = st. & case(wi   1) = cap   

0 otherwise

0 otherwise

x1 =

x2 =

x3 =

designing features: features are generally designed by examining the training
set with an eye to linguistic intuitions and the linguistic literature on the domain. a
careful error analysis on the training or dev set. of an early version of a system often
provides insights into features.

 it's hokey. there are virtually no surprises , and the writing is second-rate . so why was it so enjoyable? for one thing , the cast is great . another nice touch is the music . i was overcome with the urge to get off the couch and start dancing .  it sucked me in , and it'll do the same to you .x1=3x6=4.15x3=1x4=3x5=0x2=26 chapter 5

    id28

feature
interactions

feature
templates

for some tasks it is especially helpful to build complex features that are combi-
nations of more primitive features. we saw such a feature for period disambiguation
above, where a period on the word st. was less likely to be the end of sentence if
the previous word was capitalized. for id28 and naive bayes these
combination features or feature interactions have to be designed by hand.

for many tasks (especially when feature values can reference speci   c words)
we   ll need large numbers of features. often these are created automatically via fea-
ture templates, abstract speci   cations of features. for example a bigram template
for period disambiguation might create a feature for every pair of words that occurs
before a period in the training set. thus the feature space is sparse, since we only
have to create a feature if that id165 exists in that position in the training set. the
feature is generally created as a hash from the string descriptions. a user description
of a feature as,    bigram(american breakfast)    is hashed into a unique integer i that
becomes the feature number fi.

in order to avoid the extensive human effort of feature design, recent research in
nlp has focused on representation learning: ways to learn features automatically
in an unsupervised way from the input. we   ll introduce methods for representation
learning in chapter 6 and chapter 7.

choosing a classi   er id28 has a number of advantages over naive
bayes. naive bayes has overly strong conditional independence assumptions. con-
sider two features which are strongly correlated; in fact, imagine that we just add the
same feature f1 twice. naive bayes will treat both copies of f1 as if they were sep-
arate, multiplying them both in, overestimating the evidence. by contrast, logistic
regression is much more robust to correlated features; if two features f1 and f2 are
perfectly correlated, regression will simply assign part of the weight to w1 and part
to w2. thus when there are many correlated features, id28 will assign
a more accurate id203 than naive bayes. so id28 generally works
better on larger documents or datasets and is a common default.

despite the less accurate probabilities, naive bayes still often makes the correct
classi   cation decision. furthermore, naive bayes works extremely well (even bet-
ter than id28) on very small datasets (ng and jordan, 2002) or short
documents (wang and manning, 2012). furthermore, naive bayes is easy to imple-
ment and very fast to train (there   s no optimization step). so it   s still a reasonable
approach to use in some situations.

5.2 learning in id28

how are the parameters of the model, the weights w and bias b, learned?

id28 is an instance of supervised classi   cation in which we know
the correct label y (either 0 or 1) for each observation x. what the system produces,
via eq. 5.5 is   y, the system   s estimate of the true y. we want to learn parameters
(meaning w and b) that make   y for each training observation as close as possible to
the true y .

this requires 2 components that we foreshadowed in the introduction to the
chapter. the    rst is a metric for how close the current label (   y) is to the true gold
label y. rather than measure similarity, we usually talk about the opposite of this:
the distance between the system output and the gold output, and we call this distance
the id168 or the cost function. in the next section we   ll introduce the loss

loss

5.3

    the cross-id178 id168

7

function that is commonly used for id28 and also for neural networks,
the cross-id178 loss.

the second thing we need is an optimization algorithm for iteratively updating
the weights so as to minimize this id168. the standard algorithm for this is
id119; we   ll introduce the stochastic id119 algorithm in the
following section.

5.3 the cross-id178 id168

we need a id168 that expresses, for an observation x, how close the classi   er
output (   y =    (w   x + b)) is to the correct output (y, which is 0 or 1). we   ll call this:
(5.6)

l(   y,y) = how much   y differs from the true y

you could imagine using a simple id168 that just takes the mean squared
error between   y and y.

lmse(   y,y) =

(   y    y)2

1
2

(5.7)

it turns out that this mse loss, which is very useful for some algorithms like
id75, becomes harder to optimize (technically, non-convex), when it   s
applied to probabilistic classi   cation.

instead, we use a id168 that prefers the correct class labels of the training
example to be more likely. this is called conditional maximum likelihood estima-
tion: we choose the parameters w,b that maximize the log id203 of the true
y labels in the training data given the observations x. the resulting id168
is the negative log likelihood loss, generally called the cross id178 loss.
let   s derive this id168, applied to a single observation x. we   d like to
learn weights that maximize the id203 of the correct label p(y|x). since there
are only two discrete outcomes (1 or 0), this is a bernoulli distribution, and we can
express the id203 p(y|x) that our classi   er produces for one observation as
the following (keeping in mind that if y=1, eq. 5.8 simpli   es to   y; if y=0, eq. 5.8
simpli   es to 1      y):

p(y|x) =   y y (1      y)1   y

(5.8)

cross id178
loss

now we take the log of both sides. this will turn out to be handy mathematically,
and doesn   t hurt us; whatever values maximize a id203 will also maximize the
log of the id203:

log p(y|x) = log(cid:2)   y y (1      y)1   y(cid:3)

= ylog   y + (1    y)log(1      y)

(5.9)

eq. 5.9 describes a log likelihood that should be maximized. in order to turn this
into id168 (something that we need to minimize), we   ll just    ip the sign on
eq. 5.9. the result is the cross-id178 loss lce:

lce (   y,y) =    log p(y|x) =     [ylog   y + (1    y)log(1      y)]

(5.10)

finally, we can plug in the de   nition of   y =    (w   x) + b:

lce (w,b) =     [ylog   (w   x + b) + (1    y)log (1       (w   x + b))]

(5.11)

8 chapter 5

    id28

why does minimizing this negative log id203 do what we want? a perfect
classi   er would assign id203 1 to the correct outcome (y=1 or y=0) and prob-
ability 0 to the incorrect outcome. that means the higher   y (the closer it is to 1), the
better the classi   er; the lower   y is (the closer it is to 0), the worse the classi   er. the
negative log of this id203 is a convenient loss metric since it goes from 0 (neg-
ative log of 1, no loss) to in   nity (negative log of 0, in   nite loss). this id168
also insures that as id203 of the correct answer is maximized, the id203
of the incorrect answer is minimized; since the two sum to one, any increase in the
id203 of the correct answer is coming at the expense of the incorrect answer.
it   s called the cross-id178 loss, because eq. 5.9 is also the formula for the cross-
id178 between the true id203 distribution y and our estimated distribution
  y.

let   s now extend eq. 5.10 from one example to the whole training set: we   ll con-
tinue to use the notation that x(i) and y(i) mean the ith training features and training
label, respectively. we make the assumption that the training examples are indepen-
dent:

log p(training labels) = log

m(cid:89)
m(cid:88)
m(cid:88)

i=1

=

i=1
=    

p(y(i)|x(i))

i=1
log p(y(i)|x(i))

lce (   y(i),y(i))

(5.12)

(5.13)

(5.14)

we   ll de   ne the cost function for the whole dataset as the average loss for each

example:

cost(w,b) =

lce (   y(i),y(i))

m(cid:88)
m(cid:88)

i=1

i=1

1
m
=     1
m

y(i) log   (w   x(i) + b) + (1    y(i))log

(cid:16)

(cid:17)

1       (w   x(i) + b)

(5.15)

now we know what we want to minimize; in the next section, we   ll see how to

   nd the minimum.

5.4 id119

our goal with id119 is to    nd the optimal weights: minimize the loss
function we   ve de   ned for the model. in eq. 5.16 below, we   ll explicitly represent
the fact that the id168 l is parameterized by the weights, which we   ll refer to
in machine learning in general as    (in the case of id28    = w,b):

     = argmin

  

1
m

m(cid:88)

i=1

lce (y(i),x(i);   )

(5.16)

5.4

    id119

9

convex

how shall we    nd the minimum of this (or any) id168? id119
is a method that    nds a minimum of a function by    guring out in which direction
(in the space of the parameters   ) the function   s slope is rising the most steeply,
and moving in the opposite direction. the intuition is that if you are hiking in a
canyon and trying to descend most quickly down to the river at the bottom, you might
look around yourself 360 degrees,    nd the direction where the ground is sloping the
steepest, and walk downhill in that direction.

for id28, this id168 is conveniently convex. a convex func-
tion has just one minimum; there are no local minima to get stuck in, so gradient
descent starting from any point is guaranteed to    nd the minimum.

although the algorithm (and the concept of gradient) are designed for direction
vectors, let   s    rst consider a visualization of the the case where the parameter of our
system, is just a single scalar w, shown in fig. 5.3.

given a random initialization of w at some value w1, and assuming the loss
function l happened to have the shape in fig. 5.3, we need the algorithm to tell us
whether at the next iteration, we should move left (making w2 smaller than w1) or
right (making w2 bigger than w1) to reach the minimum.

gradient

learning rate

figure 5.3 the    rst step in iteratively    nding the minimum of this id168, by moving
w in the reverse direction from the slope of the function. since the slope is negative, we need
to move w in a positive direction, to the right. here superscripts are used for learning steps,
so w1 means the initial value of w (which is 0), w2 at the second step, and so on.

the id119 algorithm answers this question by    nding the gradient
of the id168 at the current point and moving in the opposite direction. the
gradient of a function of many variables is a vector pointing in the direction the
greatest increase in a function. the gradient is a multi-variable generalization of the
slope, so for a function of one variable like the one in fig. 5.3, we can informally
think of the gradient as the slope. the dotted line in fig. 5.3 shows the slope of this
hypothetical id168 at point w = w1. you can see that the slope of this dotted
line is negative. thus to    nd the minimum, id119 tells us to go in the
opposite direction: moving w in a positive direction.

the magnitude of the amount to move in id119 is the value of the slope
d
dw f (x;w) weighted by a learning rate   . a higher (faster) learning rate means that
we should move w more on each step. the change we make in our parameter is the
learning rate times the gradient (or the slope, in our single-variable example):

wt+1 = wt       

d
dw

f (x;w)

(5.17)

now let   s extend the intuition from a function of one scalar variable w to many

wloss0w1wminslope of loss at w1 is negative(goal)one stepof gradientdescent10 chapter 5

    id28

variables, because we don   t just want to move left or right, we want to know where
in the n-dimensional space (of the n parameters that make up   ) we should move.
the gradient is just such a vector; it expresses the directional components of the
sharpest slope along each of those n dimensions. if we   re just imagining two weight
dimension (say for one weight w and one bias b), the gradient might be a vector with
two orthogonal components, each of which tells us how much the ground slopes in
the w dimension and in the b dimension. fig. 5.4 shows a visualization:

figure 5.4 visualization of the gradient vector in two dimensions w and b.

in an actual id28, the parameter vector w is much longer than 1 or
2, since the input feature vector x can be quite long, and we need a weight wi for
each xi for each dimension/variable wi in w (plus the bias b), the gradient will have
a component that tells us the slope with respect to that variable. essentially we   re
asking:    how much would a small change in that variable wi in   uence the total loss
function l?   

in each dimension wi, we express the slope as a partial derivative    
    wi

of the loss
function. the gradient is then de   ned as a vector of these partials. we   ll represent   y
as f (x;   ) to make the dependence on    more obvious:

      l( f (x;   ),y)) =

                  

   
    w1
   
    w2

   
    wn

                  

l( f (x;   ),y)
l( f (x;   ),y)

...

l( f (x;   ),y)

(5.18)

(5.19)

the    nal equation for updating    based on the gradient is thus

  t+1 =   t          l( f (x;   ),y)

5.4.1 the gradient for id28
in order to update   , we need a de   nition for the gradient    l( f (x;   ),y). recall that
for id28, the cross-id178 id168 is:

lce (w,b) =     [ylog   (w   x + b) + (1    y)log (1       (w   x + b))]

(5.20)
it turns out that the derivative of this function for one observation vector x is
eq. 5.21 (the interested reader can see section 5.8 for the derivation of this equation):

    lce (w,b)

    w j

= [   (w   x + b)    y]x j

(5.21)

cost(w,b)wb5.4

    id119

11

note in eq. 5.21 that the gradient with respect to a single weight w j represents a
very intuitive value: the difference between the true y and our estimated   y =    (w  
x + b) for that observation, multiplied by the corresponding input value x j.

the loss for a batch of data or an entire dataset is just the average loss over the

m examples:

cost(w,b) =     1
m

m(cid:88)

i=1

y(i) log   (w   x(i) + b) + (1    y(i))log

(cid:16)

1       (w   x(i) + b)

(cid:17)

(5.22)

and the gradient for multiple data points is the sum of the individual gradients::

   cost(w,b)

    w j

=

(cid:104)

   (w   x(i) + b)    y(i)(cid:105)

m(cid:88)

i=1

x(i)
j

(5.23)

5.4.2 the stochastic id119 algorithm
stochastic id119 is an online algorithm that minimizes the id168
by computing its gradient after each training example, and nudging    in the right
direction (the opposite direction of the gradient). fig. 5.5 shows the algorithm.

function stochastic id119(l(), f (), x, y) returns   

f is a function parameterized by   
x is the set of training inputs x(1), x(2), ..., x(n)
y is the set of training outputs (labels) y(1), y(2), ..., y(n)

# where: l is the id168
#
#
#
      0
repeat t times

for each training tuple (x(i), y(i)) (in random order)
compute   y (i) = f (x(i);   )
compute the loss l(   y (i),y(i)) # how far off is   y(i)) from the true output y(i)?
g         l( f (x(i);   ),y(i))
# how should we move    to maximize loss ?
                g

# what is our estimated output   y?

# go the other way instead

return   

figure 5.5 the stochastic id119 algorithm

minibatch

stochastic id119 is called stochastic because it chooses a single ran-
dom example at a time, moving the weights so as to improve performance on that
single example. that can result in very choppy movements, so it   s also common to
do minibatch id119, which computes the gradient over batches of train-
ing instances rather than a single instance.

the learning rate    is a parameter that must be adjusted. if it   s too high, the
learner will take steps that are too large, overshooting the minimum of the loss func-
tion. if it   s too low, the learner will take steps that are too small, and take too long to
get to the minimum. it is most common to begin the learning rate at a higher value,
and then slowly decrease it, so that it is a function of the iteration k of training; you
will sometimes see the notation   k to mean the value of the learning rate at iteration
k.

12 chapter 5

    id28

5.4.3 working through an example
let   s walk though a single step of the id119 algorithm. we   ll use a sim-
pli   ed version of the example in fig. 5.2 as it sees a single observation x, whose
correct value is y = 1 (this is a positive review), and with only two features:

x1 = 3
x2 = 2

(count of positive lexicon words)
(count of negative lexicon words)

let   s assume the initial weights and bias in    0 are all set to 0, and the initial learning
rate    is 0.1:

w1 = w2 = b = 0

   = 0.1

the single update step requires that we compute the gradient, multiplied by the
learning rate

   t+1 =    t             l( f (x(i);   ),y(i))

in our mini example there are three parameters, so the gradient vector has 3 dimen-
sions, for w1, w2, and b. we can compute the    rst gradient as follows:

              lce (w,b)

    lce (w,b)
    lce (w,b)

    w1
    w2
    b

          =

   w,b =

       (   (w   x + b)    y)x1

(   (w   x + b)    y)x2
   (w   x + b)    y

       =

       (   (0)    1)x1

(   (0)    1)x2
   (0)    1

       =

          0.5x1

   0.5x2
   0.5

       =

          1.5

   1.0
   0.5

      

now that we have a gradient, we compute the new parameter vector    2 by mov-

ing    1 in the opposite direction from the gradient:

       w1

w2
b

            

          1.5

   1.0
   0.5

       =

      

       .15

.1
.05

   2 =

so after one step of id119, the weights have shifted to be: w1 = .15,

w2 = .1, and b = .05.

note that this observation x happened to be a positive example. we would expect
that after seeing more negative examples with high counts of negative words, that
the weight w2 would shift to have a negative value.

5.5 id173

numquam ponenda est pluralitas sine necessitate
   plurality should never be proposed unless needed   
william of occam

over   tting
generalize

id173

l2
id173

l1
id173

5.5

    id173

13

there is a problem with learning weights that make the model perfectly match
the training data. if a feature is perfectly predictive of the outcome because it hap-
pens to only occur in one class, it will be assigned a very high weight. the weights
for features will attempt to perfectly    t details of the training set, in fact too per-
fectly, modeling noisy factors that just accidentally correlate with the class. this
problem is called over   tting. a good model should be able to generalize well from
the training data to the unseen test set, but a model that over   ts will have poor gen-
eralization.

to avoid over   tting, a id173 term is added to the objective function in

eq. 5.16, resulting in the following objective:

  w = argmax

w

logp(y(i)|x(i))      r(w)

(5.24)

the new component, r(w) is called a id173 term, and is used to penalize
large weights. thus a setting of the weights that matches the training data perfectly,
but uses many weights with high values to do so, will be penalized more than a
setting that matches the data a little less well, but does so using smaller weights.

there are two common id173 terms r(w). l2 id173 is a quad-
ratic function of the weight values, named because it uses the (square of the) l2
norm of the weight values. the l2 norm, ||w||2, is the same as the euclidean
distance:

n(cid:88)

j=1

n(cid:88)

i=1

r(w ) = ||w||2

2 =

w2
j

the l2 regularized objective function becomes:

  w = argmax

w

logp(y(i)|x(i))

(cid:35)

n(cid:88)

j=1

w2
j

      

l1 id173 is a linear function of the weight values, named after the l1
norm ||w||1, the sum of the absolute values of the weights, or manhattan distance
(the manhattan distance is the distance you   d have to walk between two points in a
city with a street grid like new york):

r(w ) = ||w||1 =

|wi|

the l1 regularized objective function becomes:

  w = argmax

w

logp(y(i)|x(i))

(cid:35)

n(cid:88)

j=1

|w j|

      

these kinds of id173 come from statistics, where l1 id173 is
called the    lasso    or lasso regression (tibshirani, 1996) and l2 regression is called

(5.25)

(5.26)

(5.27)

(5.28)

m(cid:88)

1=1

(cid:34) m(cid:88)

1=i

(cid:34) m(cid:88)

1=i

14 chapter 5

    id28

ridge regression, and both are commonly used in language processing. l2 regu-
larization is easier to optimize because of its simple derivative (the derivative of w2
is just 2w), while l1 id173 is more complex (the derivative of |w| is non-
continuous at zero). but where l2 prefers weight vectors with many small weights,
l1 prefers sparse solutions with some larger weights but many more weights set to
zero. thus l1 id173 leads to much sparser weight vectors, that is, far fewer
features.

both l1 and l2 id173 have bayesian interpretations as constraints on
the prior of how weights should look. l1 id173 can be viewed as a laplace
prior on the weights. l2 id173 corresponds to assuming that weights are
distributed according to a gaussian distribution with mean    = 0.
in a gaussian
or normal distribution, the further away a value is from the mean, the lower its
id203 (scaled by the variance   ). by using a gaussian prior on the weights, we
are saying that weights prefer to have the value 0. a gaussian for a weight w j is

(cid:32)
    (w j        j)2

(cid:33)

2   2
j

1(cid:113)

2     2
j

exp

(5.29)

if we multiply each weight by a gaussian prior on the weight, we are thus maxi-

mizing the following constraint:

m(cid:89)

i=1

  w = argmax

w

p(y(i)|x(i))  

n(cid:89)

j=1

1(cid:113)

2     2
j

(cid:33)

(cid:32)
    (w j        j)2

2   2
j

exp

(5.30)

which in log space, with    = 0, and assuming 2   2 = 1, corresponds to

m(cid:88)

n(cid:88)

  w = argmax

w

logp(y(i)|x(i))      

i=1

j=1

w2
j

(5.31)

which is in the same form as eq. 5.26.

5.6 multinomial id28

multinominal
logistic
regression

softmax

sometimes we need more than two classes. perhaps we might want to do 3-way
sentiment classi   cation (positive, negative, or neutral). or we could be classifying
the part of speech of a word (choosing from 10, 30, or even 50 different parts of
speech), or assigning semantic labels like the named entities or semantic relations
we will introduce in chapter 17.

in such cases we use multinominal id28, also called softmax re-
gression (or, historically, the maxent classi   er). in multinominal id28
the target y is a variable that ranges over more than two classes; we want to know
the id203 of y being in each potential class c     c, p(y = c|x).
the multinominal logistic classi   er uses a generalization of the sigmoid, called
the softmax function, to compute the id203 p(y = c|x). the softmax function
takes a vector z = [z1,z2, ...,zk] of k arbitrary values and maps them to a id203
distribution, with each value in the range (0,1], and all the values summing to 1.
like the sigmoid, it is an exponential function;

5.6

    multinomial id28

15

for a vector z of dimensionality k, the softmax is de   ned as:

softmax(zi) =

ezi(cid:80)k

j=1 ez j

1     i     k

the softmax of an input vector z = [z1,z2, ...,zk] is thus a vector itself:

(cid:34)

ez1(cid:80)k

i=1 ezi

,

ez2(cid:80)k

i=1 ezi

, ...,

ezk(cid:80)k

i=1 ezi

(cid:35)

(5.32)

(5.33)

i=1 ezi is used to normalize all the values into probabilities.

softmax(z) =

the denominator(cid:80)k

thus for example given a vector:

z = [0.6,1.1,   1.5,1.2,3.2,   1.1]

the result softmax(z) is

[0.055,0.090,0.0067,0.10,0.74,0.010]

again like the sigmoid, the input to the softmax will be the dot product between
a weight vector w and an input vector x (plus a bias). but now we   ll need separate
weight vectors (and bias) for each of the k classes.

p(y = c|x) =

ewc    x + bc
k(cid:88)
ew j    x + b j

j=1

(5.34)

like the sigmoid, the softmax has the property of squashing values toward 0 or
1. thus if one of the inputs is larger than the others, will tend to push its id203
toward 1, and suppress the probabilities of the smaller inputs.

5.6.1 features in multinomial id28
for multiclass classi   cation the input features need to be a function of both the
observation x and the candidate output class c. thus instead of the notation xi, fi
or fi(x), when we   re discussing features we will use the notation fi(c,x), meaning
feature i for a particular class c for a given observation x.

in binary classi   cation, a positive weight on a feature pointed toward y=1 and
a negative weight toward y=0... but in multiclass a feature could be evidence for or
against an individual class.

let   s look at some sample features for a few nlp tasks to help understand this
perhaps unintuitive use of features that are functions of both the observation x and
the class c,
suppose we are doing text classi   cation, and instead of binary classi   cation our
task is to assign one of the 3 classes +,    , or 0 (neutral) to a document. now a
feature related to exclamation marks might have a negative weight for 0 documents,
and a positive weight for + or     documents:

16 chapter 5

    id28

var
f1(0,x)

f1(+,x)

f1(0,x)

de   nition

(cid:26) 1 if    !        doc
(cid:26) 1 if    !        doc
(cid:26) 1 if    !        doc

0 otherwise

0 otherwise

0 otherwise

wt
   4.5

2.6

1.3

5.6.2 learning in multinomial id28
multinomial id28 has a slightly different id168 than binary lo-
gistic regression because it uses the softmax rather than sigmoid classi   er, the loss
function for a single example x is the sum of the logs of the k output classes:

k(cid:88)
k(cid:88)

k=1

k=1

lce (   y,y) =    

=    

1{y = k}log p(y = k|x)

1{y = k}log

(cid:80)k
ewk  x+bk
j=1 ew j  x+b j

(5.35)

this makes use of the function 1{} which evaluates to 1 if the condition in the
brackets is true and to 0 otherwise.

the gradient for a single example turns out to be very similar to the gradient for
id28, although we don   t show the derivation here. it is the different
between the value for the true class k (which is 1) and the id203 the classi   er
outputs for class k, weighted by the value of the input xk:

    lce
    wk

5.7

interpreting models

(cid:32)
= (1{y = k}    p(y = k|x))xk
1{y = k}    ewk  x+bk

(cid:80)k
j=1 ew j  x+b j

=

(cid:33)

xk

(5.36)

interpretable

often we want to know more than just the correct classi   cation of an observation.
we want to know why the classi   er made the decision it did. that is, we want our
decision to be interpretable. interpretability can be hard to de   ne strictly, but the
core idea is that as humans we should know why our algorithms reach the conclu-
sions they do. because the features to id28 are often human-designed,
one way to understand a classi   er   s decision is to understand the role each feature it
plays in the decision. id28 can be combined with statistical tests (the
likelihood ratio test, or the wald test); investigating whether a particular feature is
signi   cant by one of these tests, or inspecting its magnitude (how large is the weight
w associated with the feature?) can help us interpret why the classi   er made the
decision it makes. this is enormously important for building transparent models.

furthermore, in addition to its use as a classi   er, id28 in nlp and
many other    elds is widely used as an analytic tool for testing hypotheses about the

5.8

    advanced: deriving the gradient equation

17

effect of various explanatory variables (features). in text classi   cation, perhaps we
want to know if logically negative words (no, not, never) are more likely to be asso-
ciated with negative sentiment, or if negative reviews of movies are more likely to
discuss the cinematography. however, in doing so it   s necessary to control for po-
tential confounds: other factors that might in   uence sentiment (the movie genre, the
year it was made, perhaps the length of the review in words). or we might be study-
ing the relationship between nlp-extracted linguistic features and non-linguistic
outcomes (hospital readmissions, political outcomes, or product sales), but need to
control for confounds (the age of the patient, the county of voting, the brand of the
product). in such cases, id28 allows us to test whether some feature is
associated with some outcome above and beyond the effect of other features.

5.8 advanced: deriving the gradient equation

in this section we give the derivation of the gradient of the cross-id178 loss func-
tion lce for id28. let   s start with some quick calculus refreshers.
first, the derivative of ln(x):

d
dx

ln(x) =

1
x

second, the (very elegant) derivative of the sigmoid:

d   (z)
dz =    (z)(1       (z))

(5.37)

(5.38)

chain rule

finally, the chain rule of derivatives. suppose we are computing the derivative
of a composite function f (x) = u(v(x)). the derivative of f (x) is the derivative of
u(x) with respect to v(x) times the derivative of v(x) with respect to x:

d f
dx =

du
dv

   dv
dx

(5.39)

first, we want to know the derivative of the id168 with respect to a single

weight w j (we   ll need to compute it for each weight, and for the bias):

    ll(w,b)

    w j

=

   
    w j

(cid:20)    

    [ylog   (w   x + b) + (1    y)log (1       (w   x + b))]

(cid:21)

=    

ylog   (w   x + b) +

   
    w j

    w j

(1    y)log [1       (w   x + b)]

next, using the chain rule, and relying on the derivative of log:
    ll(w,b)

1    y

y

=    

   (w   x + b)

   
    w j

   (w   x + b)   

1       (w   x + b)

    w j

rearranging terms:
    ll(w,b)

    w j

(cid:20)

=    

y

   (w   x + b)

   

1    y

1       (w   x + b)

(cid:21)    

    w j

(5.40)

   
    w j

1       (w   x + b)
(5.41)

   (w   x + b)

(5.42)

18 chapter 5

    id28

(cid:20)
(cid:20)

and now plugging in the derivative of the sigmoid, and using the chain rule one
more time, we end up with eq. 5.43:

    ll(w,b)

    w j

=    

y       (w   x + b)

   (w   x + b)[1       (w   x + b)]

y       (w   x + b)

   (w   x + b)[1       (w   x + b)]

=    
=    [y       (w   x + b)]x j
= [   (w   x + b)    y]x j

   (w   x + b)[1       (w   x + b)]

    (w   x + b)

    w j

   (w   x + b)[1       (w   x + b)]x j

(5.43)

(cid:21)
(cid:21)

5.9 summary

this chapter introduced the id28 model of classi   cation.

    id28 is a supervised machine learning classi   er that extracts
real-valued features from the input, multiplies each by a weight, sums them,
and passes the sum through a sigmoid function to generate a id203. a
threshold is used to make a decision.
    id28 can be used with two classes (e.g., positive and negative
sentiment) or with multiple classes (multinomial id28, for ex-
ample for n-ary text classi   cation, part-of-speech labeling, etc.).
    multinomial id28 uses the softmax function to compute proba-
    the weights (vector w and bias b) are learned from a labeled training set via a
    minimizing this id168 is a id76 problem, and iterative
    id173 is used to avoid over   tting.
    id28 is also one of the most useful analytic tools, because of its

algorithms like id119 are used to    nd the optimal weights.

id168, such as the cross-id178 loss, that must be minimized.

bilities.

ability to transparently study the importance of individual features.

bibliographical and historical notes

id28 was developed in the    eld of statistics, where it was used for
the analysis of binary data by the 1960s, and was particularly common in medicine
(cox, 1969). starting in the late 1970s it became widely used in linguistics as one
of the formal foundations of the study of linguistic variation (sankoff and labov,
1979).

nonetheless, id28 didn   t become common in natural language pro-
cessing until the 1990s, when it seems to have appeared simultaneously from two
directions. the    rst source was the neighboring    elds of information retrieval and
speech processing, both of which had made use of regression, and both of which
lent many other statistical techniques to nlp. indeed a very early use of logistic
regression for document routing was one of the    rst nlp applications to use (lsi)
embeddings as word representations (sch  utze et al., 1995).

at the same time in the early 1990s id28 was developed and ap-
plied to nlp at ibm research under the name maximum id178 modeling or

maximum
id178

exercises

19

maxent (berger et al., 1996), seemingly independent of the statistical literature. un-
der that name it was applied to id38 (rosenfeld, 1996), part-of-speech
tagging ((ratnaparkhi, 1996)), parsing (ratnaparkhi, 1997), and text classi   cation
(nigam et al., 1999).

more on classi   cation can be found in machine learning textbooks (hastie et al. 2001,

witten and frank 2005, bishop 2006, murphy 2012).

exercises

20 chapter 5     id28

berger, a., della pietra, s. a., and della pietra, v. j. (1996).
a maximum id178 approach to natural language process-
ing. computational linguistics, 22(1), 39   71.

bishop, c. m. (2006). pattern recognition and machine

learning. springer.

cox, d. (1969). analysis of binary data. chapman and hall,

london.

hastie, t., tibshirani, r. j., and friedman, j. h. (2001). the

elements of statistical learning. springer.

murphy, k. p. (2012). machine learning: a probabilistic

perspective. mit press.

ng, a. y. and jordan, m. i. (2002). on discriminative vs.
generative classi   ers: a comparison of id28
and naive bayes. in nips 14, pp. 841   848.

nigam, k., lafferty, j. d., and mccallum, a. (1999). using
maximum id178 for text classi   cation. in ijcai-99 work-
shop on machine learning for information    ltering, pp. 61   
67.

ratnaparkhi, a. (1996). a maximum id178 part-of-speech

tagger. in emnlp 1996, philadelphia, pa, pp. 133   142.

ratnaparkhi, a. (1997). a linear observed time statistical
in emnlp

parser based on maximum id178 models.
1997, providence, ri, pp. 1   10.

rosenfeld, r. (1996). a maximum id178 approach to
adaptive statistical id38. computer speech
and language, 10, 187   228.

sankoff, d. and labov, w. (1979). on the uses of variable

rules. language in society, 8(2-3), 189   222.

sch  utze, h., hull, d. a., and pedersen, j. (1995). a com-
parison of classi   ers and id194s for the
routing problem. in sigir-95, pp. 229   237.

tibshirani, r. j. (1996). regression shrinkage and selection
via the lasso. journal of the royal statistical society. series
b (methodological), 58(1), 267   288.

wang, s. and manning, c. d. (2012). baselines and bigrams:
in acl

simple, good sentiment and topic classi   cation.
2012, pp. 90   94.

witten, i. h. and frank, e. (2005). data mining: practical
machine learning tools and techniques (2nd ed.). mor-
gan kaufmann.

