speech and language processing. daniel jurafsky & james h. martin.
rights reserved.

draft of september 23, 2018.

copyright c(cid:13) 2018.

all

chapter

6 vector semantics

the asphalt that los angeles is famous for occurs mainly on its freeways. but in the
middle of the city is another patch of asphalt, the la brea tar pits, and this asphalt
preserves millions of fossil bones from the last of the ice ages of the pleistocene
epoch. one of these fossils is the smilodon, or sabre-toothed tiger, instantly rec-
ognizable by its long canines. five million years ago or so, a completely different
sabre-tooth tiger called thylacosmilus lived
in argentina and other parts of south amer-
ica. thylacosmilus was a marsupial whereas
smilodon was a placental mammal, but thy-
lacosmilus had the same long upper canines
and, like smilodon, had a protective bone
   ange on the lower jaw. the similarity of
these two mammals is one of many examples
of parallel or convergent evolution, in which particular contexts or environments
lead to the evolution of very similar structures in different species (gould, 1980).

the role of context is also important in the similarity of a less biological kind
of organism: the word. words that occur in similar contexts tend to have similar
meanings. this link between similarity in how words are distributed and similarity
in what they mean is called the distributional hypothesis. the hypothesis was
   rst formulated in the 1950s by linguists like joos (1950), harris (1954), and firth
(1957), who noticed that words which are synonyms (like oculist and eye-doctor)
tended to occur in the same environment (e.g., near words like eye or examined)
with the amount of meaning difference between two words    corresponding roughly
to the amount of difference in their environments    (harris, 1954, 157).

in this chapter we introduce a model known as vector semantics, which instan-
tiates this linguistic hypothesis by learning representations of the meaning of words
directly from their distributions in texts. these representations are used in every
natural language processing application that makes use of meaning. these word
representations are also the    rst example we will see in the book of representation
learning, automatically learning useful representations of the input text. finding
such unsupervised ways to learn representations of the input, instead of creating
representations by hand via feature engineering, is an important focus of recent
nlp research (bengio et al., 2013).

we   ll begin, however, by introducing some basic principles of word meaning,
which will motivate the vector semantic models of this chapter as well as extensions
that we   ll return to in appendix c, chapter 19, and chapter 18.

distributional
hypothesis

vector
semantics

representation
learning

2 chapter 6

    vector semantics

6.1 lexical semantics

how should we represent the meaning of a word? in the id165 models we saw in
chapter 3, and in many traditional nlp applications, our only representation of a
word is as a string of letters, or perhaps as an index in a vocabulary list. this repre-
sentation is not that different from a tradition in philosophy, perhaps you   ve seen it
in introductory logic classes, in which the meaning of words is often represented by
just spelling the word with small capital letters; representing the meaning of    dog   
as dog, and    cat    as cat).

representing the meaning of a word by capitalizing it is a pretty unsatisfactory

model. you might have seen the old philosophy joke:

q: what   s the meaning of life?
a: life

lexical
semantics

lemma
citation form

wordform

surely we can do better than this! after all, we   ll want a model of word meaning
to do all sorts of things for us. it should tell us that some words have similar mean-
ings (cat is similar to dog), other words are antonyms (cold is the opposite of hot). it
should know that some words have positive connotations (happy) while others have
negative connotations (sad). it should represent the fact that the meanings of buy,
sell, and pay offer differing perspectives on the same underlying purchasing event
(if i buy something from you, you   ve probably sold it to me, and i likely paid you).
more generally, a model of word meaning should allow us to draw useful infer-
ences that will help us solve meaning-related tasks like question-answering, sum-
marization, paraphrase or plagiarism detection, and dialogue.

in this section we summarize some of these desiderata, drawing on results in the

linguistic study of word meaning, which is called lexical semantics.

lemmas and senses let   s start by looking at how one word (we   ll choose mouse)
might be de   ned in a dictionary: 1

mouse (n)
1.
2.

any of numerous small rodents...
a hand-operated device that controls a cursor...

here the form mouse is the lemma, also called the citation form. the form
mouse would also be the lemma for the word mice; dictionaries don   t have separate
de   nitions for in   ected forms like mice. similarly sing is the lemma for sing, sang,
sung. in many languages the in   nitive form is used as the lemma for the verb, so
spanish dormir    to sleep    is the lemma for duermes    you sleep   . the speci   c forms
sung or carpets or sing or duermes are called wordforms.

as the example above shows, each lemma can have multiple meanings; the
lemma mouse can refer to the rodent or the cursor control device. we call each
of these aspects of the meaning of mouse a word sense. the fact that lemmas can be
homonymous (have multiple senses) can make interpretation dif   cult (is someone
who types    mouse info    to a search engine looking for a pet or a tool?). appendix c
will discuss the problem of homonymy, and introduce id51,
the task of determining which sense of a word is being used in a particular context.

relationships between words or senses one important component of word mean-
ing is the relationship between word senses. for example when one word has a sense

1 this example shortened from the online dictionary id138, discussed in appendix c.

6.1

    lexical semantics

3

synonym

whose meaning is identical to a sense of another word, or nearly identical, we say
the two senses of those two words are synonyms. synonyms include such pairs as

couch/sofa vomit/throw up    lbert/hazelnut car/automobile

propositional
meaning

principle of
contrast

antonym

reversives

similarity

a more formal de   nition of synonymy (between words rather than senses) is that
two words are synonymous if they are substitutable one for the other in any sentence
without changing the truth conditions of the sentence, the situations in which the
sentence would be true. we often say in this case that the two words have the same
propositional meaning.

while substitutions between some pairs of words like car / automobile or water /
h2o are truth preserving, the words are still not identical in meaning. indeed, proba-
bly no two words are absolutely identical in meaning. one of the fundamental tenets
of semantics, called the principle of contrast (br  eal 1897, ?, clark 1987), is the as-
sumption that a difference in linguistic form is always associated with at least some
difference in meaning. for example, the word h2o is used in scienti   c contexts and
would be inappropriate in a hiking guide   water would be more appropriate    and
this difference in genre is part of the meaning of the word. in practice, the word
synonym is therefore commonly used to describe a relationship of approximate or
rough synonymy.

where synonyms are words with identical or similar meanings, antonyms are

words with an opposite meaning, like:

long/short big/little fast/slow cold/hot dark/light
rise/fall

up/down in/out

two senses can be antonyms if they de   ne a binary opposition or are at opposite
ends of some scale. this is the case for long/short, fast/slow, or big/little, which are
at opposite ends of the length or size scale. another group of antonyms, reversives,
describe change or movement in opposite directions, such as rise/fall or up/down.

antonyms thus differ completely with respect to one aspect of their meaning   
their position on a scale or their direction   but are otherwise very similar, sharing
almost all other aspects of meaning. thus, automatically distinguishing synonyms
from antonyms can be dif   cult.

word similarity: while words don   t have many synonyms, most words do have
lots of similar words. cat is not a synonym of dog, but cats and dogs are certainly
similar words. in moving from synonymy to similarity, it will be useful to shift from
talking about relations between word senses (like synonymy) to relations between
words (like similarity). dealing with words avoids having to commit to a particular
representation of word senses, which will turn out to simplify our task.

the notion of word similarity is very useful in larger semantic tasks. for exam-
ple knowing how similar two words are is helpful if we are trying to decide if two
phrases or sentences mean similar things. phrase or sentence similarity is useful in
such natural language understanding tasks as id53, id141, and
summarization.

one way of getting values for word similarity is to ask humans to judge how
similar one word is to another. a number of datasets have resulted from such ex-
periments. for example the siid113x-999 dataset (hill et al., 2015) gives values on
a scale from 0 to 10, like the examples below, which range from near-synonyms
(vanish, disappear) to pairs that scarcely seem to have anything in common (hole,
agreement):

4 chapter 6

    vector semantics

vanish disappear
behave obey
belief
muscle bone
modest    exible
hole

9.8
7.3
impression 5.95
3.65
0.98
agreement 0.3

relatedness
association

semantic    eld

topic models

semantic frame

hyponym

hypernym

superordinate

word relatedness: the meaning of two words can be related in ways others than
similarity. one such class of connections is called word relatedness (budanitsky
and hirst, 2006), also traditionally called word association in psychology.

consider the meanings of the words coffee and cup; coffee is not similar to cup;
they share practically no features (coffee is a plant or a beverage, while a cup is an
manufactured object with a particular shape).

but coffee and cup are clearly related; they are associated in the world by com-
monly co-participating in a shared event (the event of drinking coffee out of a cup).
similarly the nouns scalpel and surgeon are not similar but are related eventively (a
surgeon tends to make use of a scalpel).

one common kind of relatedness between words is if they belong to the same
semantic    eld. a semantic    eld is a set of words which cover a particular semantic
domain and bear structured relations with each other.

for example, words might be related by being in the semantic    eld of hospitals
(surgeon, scalpel, nurse, anaesthetic, hospital), restaurants (waiter, menu, plate,
food, chef), or houses (door, roof, kitchen, family, bed).

semantic    elds are also related to topic models, like latent dirichlet alloca-
tion, lda, which apply unsupervised learning on large sets of texts to induce sets
of associated words from text. semantic    elds and topic models are a very useful
tool for discovering topical structure in documents.
semantic frames and roles: closely related to semantic    elds is the idea of a
semantic frame. a semantic frame is a set of words that denote perspectives or
participants in a particular type of event. a commercial transaction, for example,
is a kind of event in which one entity trades money to another entity in return for
some good or service, after which the good changes hands or perhaps the service
is performed. this event can be encoded lexically by using verbs like buy (the
event from the perspective of the buyer) sell (from the perspective of the seller), pay
(focusing on the monetary aspect), or nouns like buyer. frames have semantic roles
(like buyer, seller, goods, money), and words in a sentence can take on these roles.
knowing that buy and sell have this relation makes it possible for a system to
know that a sentence like sam bought the book from ling could be paraphrased as
ling sold the book to sam, and that sam has the role of the buyer in the frame and
ling the seller. being able to recognize such paraphrases is important for question
answering, and can help in shifting perspective for machine translation.
taxonomic relations: another way word senses can be related is taxonomically.
a word (or sense) is a hyponym of another word or sense if the    rst is more speci   c,
denoting a subclass of the other. for example, car is a hyponym of vehicle; dog is
a hyponym of animal, and mango is a hyponym of fruit. conversely, we say that
vehicle is a hypernym of car, and animal is a hypernym of dog. it is unfortunate that
the two words (hypernym and hyponym) are very similar and hence easily confused;
for this reason, the word superordinate is often used instead of hypernym.

superordinate vehicle fruit
subordinate

car

furniture mammal

mango chair

dog

is-a

connotations

sentiment

6.1

    lexical semantics

5

we can de   ne hypernymy more formally by saying that the class denoted by
the superordinate extensionally includes the class denoted by the hyponym. thus,
the class of animals includes as members all dogs, and the class of moving actions
includes all walking actions. hypernymy can also be de   ned in terms of entail-
ment. under this de   nition, a sense a is a hyponym of a sense b if everything
that is a is also b, and hence being an a entails being a b, or    x a(x)     b(x). hy-
ponymy/hypernymy is usually a transitive relation; if a is a hyponym of b and b is a
hyponym of c, then a is a hyponym of c. another name for the hypernym/hyponym
structure is the is-a hierarchy, in which we say a is-a b, or b subsumes a.

hypernymy is useful for tasks like id123 or id53;
knowing that leukemia is a type of cancer, for example, would certainly be useful in
answering questions about leukemia.

connotation: finally, words have affective meanings or connotations. the word
connotation has different meanings in different    elds, but here we use it to mean
the aspects of a word   s meaning that are related to a writer or reader   s emotions,
sentiment, opinions, or evaluations. for example some words have positive conno-
tations (happy) while others have negative connotations (sad). some words describe
positive evaluation (great, love) and others negative evaluation (terrible, hate). pos-
itive or negative evaluation expressed through language is called sentiment, as we
saw in chapter 4, and word sentiment plays a role in important tasks like sentiment
analysis, stance detection, and many aspects of natural language processing to the
language of politics and consumer reviews.

early work on affective meaning (osgood et al., 1957) found that words varied
along three important dimensions of affective meaning. these are now generally
called valence, arousal, and dominance, de   ned as follows:

valence: the pleasantness of the stimulus
arousal: the intensity of emotion provoked by the stimulus
dominance: the degree of control exerted by the stimulus

thus words like happy or satis   ed are high on valence, while unhappy or an-
noyed are low on valence. excited or frenzied are high on arousal, while relaxed
or calm are low on arousal. important or controlling are high on dominance, while
awed or in   uenced are low on dominance. each word is thus represented by three
numbers, corresponding to its value on each of the three dimensions, like the exam-
ples below:

valence arousal dominance

courageous 8.05
music
7.67
heartbreak 2.45
6.71
cub
life
6.68

5.5
5.57
5.65
3.95
5.59

7.38
6.5
3.58
4.24
5.89

osgood et al. (1957) noticed that in using these 3 numbers to represent the
meaning of a word, the model was representing each word as a point in a three-
dimensional space, a vector whose three dimensions corresponded to the word   s
rating on the three scales. this revolutionary idea that word meaning word could
be represented as a point in space (e.g., that part of the meaning of heartbreak can
be represented as the point [2.45,5.65,3.58]) was the    rst expression of the vector
semantics models that we introduce next.

6 chapter 6

    vector semantics

6.2 vector semantics

vector
semantics

how can we build a computational model that successfully deals with the different
aspects of word meaning we saw in the previous section (word senses, word simi-
larity and relatedness, lexical    elds and frames, connotation)?

a perfect model that completely deals with each of these aspects of word mean-
ing turns out to be elusive. but the current best model, called vector semantics,
draws its inspiration from linguistic and philosophical work of the 1950   s.

during that period, the philosopher ludwig wittgenstein, skeptical of the possi-
bility of building a completely formal theory of meaning de   nitions for each word,
suggested instead that    the meaning of a word is its use in the language    (wittgen-
stein, 1953, pi 43). that is, instead of using some logical language to de   ne each
word, we should de   ne words by some representation of how the word was used by
actual people in speaking and understanding.

linguists of the period like joos (1950), harris (1954), and firth (1957) (the
linguistic distributionalists), came up with a speci   c idea for realizing wittgenstein   s
intuition: de   ne a word by the environment or distribution it occurs in in language
use. a word   s distribution is the set of contexts in which it occurs, the neighboring
words or grammatical environments. the idea is that two words that occur in very
similar distributions (that occur together with very similar words) are likely to have
the same meaning.

let   s see an example illustrating this distributionalist approach. suppose you
didn   t know what the cantonese word ongchoi meant, but you do see it in the fol-
lowing sentences or contexts:
(6.1) ongchoi is delicious sauteed with garlic.
(6.2) ongchoi is superb over rice.
(6.3) ...ongchoi leaves with salty sauces...

and furthermore let   s suppose that you had seen many of these context words

occurring in contexts like:
(6.4) ...spinach sauteed with garlic over rice...
(6.5) ...chard stems and leaves are delicious...
(6.6) ...collard greens and other salty leafy greens

the fact that ongchoi occurs with words like rice and garlic and delicious and
salty, as do words like spinach, chard, and collard greens might suggest to the reader
that ongchoi is a leafy green similar to these other leafy greens.2

vector semantics thus combines two intuitions:

we can do the same thing computationally by just counting words in the context
of ongchoi; we   ll tend to see words like sauteed and eaten and garlic. the fact that
these words and other similar context words also occur around the word spinach or
collard greens can help us discover the similarity between these words and ongchoi.
the distributionalist intuition
(de   ning a word by counting what other words occur in its environment), and the
vector intuition of osgood et al. (1957) we saw in the last section on connotation:
de   ning the meaning of a word w as a vector, a list of numbers, a point in n-
dimensional space. there are various versions of vector semantics, each de   ning
the numbers in the vector somewhat differently, but in each case the numbers are
based in some way on counts of neighboring words.

2

it   s in fact ipomoea aquatica, a relative of morning glory sometimes called water spinach in english.

embeddings

6.2

    vector semantics

7

figure 6.1 a two-dimensional (id167) projection of embeddings for some words and
phrases, showing that words with similar meanings are nearby in space. the original 60-
dimensional embeddings were trained for a id31 task. simpli   ed from li et al.
(2015).

the idea of vector semantics is thus to represent a word as a point in some multi-
dimensional semantic space. vectors for representing words are generally called
embeddings, because the word is embedded in a particular vector space. fig. 6.1
displays a visualization of embeddings that were learned for a id31
task, showing the location of some selected words projected down from the original
60-dimensional space into a two dimensional space.

notice that positive and negative words seem to be located in distinct portions of
the space (and different also from the neutral function words). this suggests one of
the great advantages of vector semantics: it offers a    ne-grained model of meaning
that lets us also implement word similarity (and phrase similarity). for example,
the id31 classi   er we saw in chapter 4 only works if enough of the
important sentimental words that appear in the test set also appeared in the training
set. but if words were represented as embeddings, we could assign sentiment as
long as words with similar meanings as the test set words occurred in the training
set. vector semantic models are also extremely practical because they can be learned
automatically from text without any complex labeling or supervision.

as a result of these advantages, vector models of meaning are now the standard
way to represent the meaning of words in nlp. in this chapter we   ll introduce the
two most commonly used models. . first is the tf-idf model, often used a a baseline,
in which the meaning of a word is de   ned by a simple function of the counts of
nearby words. we will see that this method results in very long vectors that are
sparse, i.e. contain mostly zeros (since most words simply never occur in the context
of others).

then we   ll introduce the id97 model, one of a family of models that are

ways of constructing short, dense vectors that have useful semantic properties.

we   ll also introduce the cosine, the standard way to use embeddings (vectors)
to compute functions like semantic similarity, the similarity between two words,
two sentences, or two documents, an important tool in practical applications like
id53, summarization, or automatic essay grading.

goodnicebadworstnot goodwonderfulamazingterrificdislikeworsevery goodincredibly goodfantasticincredibly badnowyouithatwithbyto   sareisathan8 chapter 6

    vector semantics

6.3 words and vectors

term-document
matrix

vector space
model

vector

vector space
dimension

vector or distributional models of meaning are generally based on a co-occurrence
matrix, a way of representing how often words co-occur. this matrix can be con-
structed in various ways; let   s s begin by looking at one such co-occurrence matrix,
a term-document matrix.

6.3.1 vectors and documents
in a term-document matrix, each row represents a word in the vocabulary and each
column represents a document from some collection of documents. fig. 6.2 shows a
small selection from a term-document matrix showing the occurrence of four words
in four plays by shakespeare. each cell in this matrix represents the number of times
a particular word (de   ned by the row) occurs in a particular document (de   ned by
the column). thus fool appeared 58 times in twelfth night.

as you like it

twelfth night

julius caesar

henry v

battle
good
fool
wit
figure 6.2 the term-document matrix for four words in four shakespeare plays. each cell
contains the number of times the (row) word occurs in the (column) document.

1
114
36
20

0
80
58
15

13
89
4
3

7
62
1
2

the term-document matrix of fig. 6.2 was    rst de   ned as part of the vector
space model of information retrieval (salton, 1971). in this model, a document is
represented as a count vector, a column in fig. 6.3.

to review some basic id202, a vector is, at heart, just a list or array
of numbers. so as you like it is represented as the list [1,114,36,20] and julius
caesar is represented as the list [7,62,1,2]. a vector space is a collection of vectors,
characterized by their dimension.
in the example in fig. 6.3, the vectors are of
dimension 4, just so they    t on the page; in real term-document matrices, the vectors
representing each document would have dimensionality |v|, the vocabulary size.

the ordering of the numbers in a vector space is not arbitrary; each position
indicates a meaningful dimension on which the documents can vary. thus the    rst
dimension for both these vectors corresponds to the number of times the word battle
occurs, and we can compare each dimension, noting for example that the vectors for
as you like it and twelfth night have similar values (1 and 0, respectively) for the
   rst dimension.

as you like it

twelfth night

julius caesar

henry v

battle
good
fool
wit
figure 6.3 the term-document matrix for four words in four shakespeare plays. the red
boxes show that each document is represented as a column vector of length four.

1
114
36
20

0
80
58
15

13
89
4
3

7
62
1
2

we can think of the vector for a document as identifying a point in |v|-dimensional
space; thus the documents in fig. 6.3 are points in 4-dimensional space. since 4-
dimensional spaces are hard to draw in textbooks, fig. 6.4 shows a visualization in

6.3

    words and vectors

9

two dimensions; we   ve arbitrarily chosen the dimensions corresponding to the words
battle and fool.

figure 6.4 a spatial visualization of the document vectors for the four shakespeare play
documents, showing just two of the dimensions, corresponding to the words battle and fool.
the comedies have high values for the fool dimension and low values for the battle dimension.

term-document matrices were originally de   ned as a means of    nding similar
documents for the task of document information retrieval. two documents that are
similar will tend to have similar words, and if two documents have similar words
their column vectors will tend to be similar. the vectors for the comedies as you
like it [1,114,36,20] and twelfth night [0,80,58,15] look a lot more like each other
(more fools and wit than battles) than they do like julius caesar [7,62,1,2] or henry
v [13,89,4,3]. we can see the intuition with the raw numbers; in the    rst dimension
(battle) the comedies have low numbers and the others have high numbers, and we
can see it visually in fig. 6.4; we   ll see very shortly how to quantify this intuition
more formally.
a real term-document matrix, of course, wouldn   t just have 4 rows and columns,
let alone 2. more generally, the term-document matrix x has |v| rows (one for each
word type in the vocabulary) and d columns (one for each document in the collec-
tion); as we   ll see, vocabulary sizes are generally at least in the tens of thousands,
and the number of documents can be enormous (think about all the pages on the
web).

information retrieval (ir) is the task of    nding the document d from the d
documents in some collection that best matches a query q. for ir we   ll therefore also
represent a query by a vector, also of length |v|, and we   ll need a way to compare
two vectors to    nd how similar they are. (doing ir will also require ef   cient ways
to store and manipulate these vectors, which is accomplished by making use of the
convenient fact that these vectors are sparse, i.e., mostly zeros).

later in the chapter we   ll introduce some of the components of this vector com-

parison process: the tf-idf term weighting, and the cosine similarity metric.

information
retrieval

6.3.2 words as vectors
we   ve seen that documents can be represented as vectors in a vector space. but
vector semantics can also be used to represent the meaning of words, by associating
each word with a vector.

the word vector is now a row vector rather than a column vector, and hence the
dimensions of the vector are different. the four dimensions of the vector for fool,

row vector

51015202530510henry v [4,13]as you like it [36,1]julius caesar [1,7]battle fooltwelfth night [58,0]154035404550556010 chapter 6

    vector semantics

term-term
matrix
word-word
matrix

[36,58,1,4], correspond to the four shakespeare plays. the same four dimensions are
used to form the vectors for the other 3 words: wit, [20, 15, 2, 3]; battle, [1,0,7,13];
and good [114,80,62,89]. each entry in the vector thus represents the counts of the
word   s occurrence in the document corresponding to that dimension.

for documents, we saw that similar documents had similar vectors, because sim-
ilar documents tend to have similar words. this same principle applies to words:
similar words have similar vectors because they tend to occur in similar documents.
the term-document matrix thus lets us represent the meaning of a word by the doc-
uments it tends to occur in.

however, it is most common to use a different kind of context for the dimensions
of a word   s vector representation. rather than the term-document matrix we use the
term-term matrix, more commonly called the word-word matrix or the term-
context matrix, in which the columns are labeled by words rather than documents.
this matrix is thus of dimensionality |v|  |v| and each cell records the number of
times the row (target) word and the column (context) word co-occur in some context
in some training corpus. the context could be the document, in which case the cell
represents the number of times the two words appear in the same document. it is
most common, however, to use smaller contexts, generally a window around the
word, for example of 4 words to the left and 4 words to the right, in which case
the cell represents the number of times (in some training corpus) the column word
occurs in such a   4 word window around the row word.

for example here are 7-word windows surrounding four sample words from the

brown corpus (just one example of each word):

sugar, a sliced lemon, a tablespoonful of apricot

their enjoyment. cautiously she sampled her    rst pineapple
well suited to programming on the digital computer.

jam, a pinch each of,
and another fruit whose taste she likened
in    nding the optimal r-stage policy from

for the purpose of gathering data and information necessary for the study authorized in the

for each word we collect the counts (from the windows around each occurrence)
of the occurrences of context words. fig. 6.5 shows a selection from the word-word
co-occurrence matrix computed from the brown corpus for these four words.

aardvark

computer

data

pinch

result

sugar

...

apricot
pineapple

digital

...
...
...
...
...

0
0
0
0

0
0
2
1

0
0
1
6

1
1
0
0

0
0
1
4

1
1
0
0

information
figure 6.5 co-occurrence vectors for four words, computed from the brown corpus, show-
ing only six of the dimensions (hand-picked for pedagogical purposes). the vector for the
word digital is outlined in red. note that a real vector would have vastly more dimensions
and thus be much sparser.

note in fig. 6.5 that the two words apricot and pineapple are more similar to
each other (both pinch and sugar tend to occur in their window) than they are to
other words like digital; conversely, digital and information are more similar to each
other than, say, to apricot. fig. 6.6 shows a spatial visualization.
note that |v|, the length of the vector, is generally the size of the vocabulary,
usually between 10,000 and 50,000 words (using the most frequent words in the
training corpus; keeping words after about the most frequent 50,000 or so is gener-
ally not helpful). but of course since most of these numbers are zero these are sparse
vector representations, and there are ef   cient algorithms for storing and computing
with sparse matrices.

6.4

    cosine for measuring similarity

11

figure 6.6 a spatial visualization of word vectors for digital and information, showing just
two of the dimensions, corresponding to the words data and result.

now that we have some intuitions, let   s move on to examine the details of com-
puting word similarity. afterwards we   ll discuss the tf-idf method of weighting
cells.

6.4 cosine for measuring similarity

dot product
inner product

vector length

to de   ne similarity between two target words v and w, we need a measure for taking
two such vectors and giving a measure of vector similarity. by far the most common
similarity metric is the cosine of the angle between the vectors.

the cosine   like most measures for vector similarity used in nlp   is based on

the dot product operator from id202, also called the inner product:

dot-product((cid:126)v,(cid:126)w) =(cid:126)v   (cid:126)w =

viwi = v1w1 + v2w2 + ... + vnwn

(6.7)

as we will see, most metrics for similarity between vectors are based on the dot
product. the dot product acts as a similarity metric because it will tend to be high
just when the two vectors have large values in the same dimensions. alternatively,
vectors that have zeros in different dimensions   orthogonal vectors   will have a
dot product of 0, representing their strong dissimilarity.

this raw dot-product, however, has a problem as a similarity metric: it favors

long vectors. the vector length is de   ned as

n(cid:88)

i=1

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

|(cid:126)v| =

v2
i

(6.8)

i=1

the dot product is higher if a vector is longer, with higher values in each dimension.
more frequent words have longer vectors, since they tend to co-occur with more
words and have higher co-occurrence values with each of them. the raw dot product
thus will be higher for frequent words. but this is a problem; we   d like a similarity
metric that tells us how similar two words are regardless of their frequency.

the simplest way to modify the dot product to normalize for the vector length is
to divide the dot product by the lengths of each of the two vectors. this normalized
dot product turns out to be the same as the cosine of the angle between the two

12345612digital [1,1]result datainformation [6,4] 3412 chapter 6

    vector semantics

vectors, following from the de   nition of the dot product between two vectors (cid:126)a and
(cid:126)b:

(cid:126)a  (cid:126)b = |(cid:126)a||(cid:126)b|cos  
(cid:126)a  (cid:126)b
|(cid:126)a||(cid:126)b| = cos  

(6.9)

the cosine similarity metric between two vectors (cid:126)v and (cid:126)w thus can be computed as:

cosine((cid:126)v,(cid:126)w) =

(cid:126)v   (cid:126)w
|(cid:126)v||(cid:126)w| =

n(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

i=1

v2
i

i=1

viwi

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

i=1

w2
i

(6.10)

cosine

unit vector

for some applications we pre-normalize each vector, by dividing it by its length,
creating a unit vector of length 1. thus we could compute a unit vector from (cid:126)a by
dividing it by |(cid:126)a|. for unit vectors, the dot product is the same as the cosine.

the cosine value ranges from 1 for vectors pointing in the same direction, through
0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.
but raw frequency values are non-negative, so the cosine for these vectors ranges
from 0   1.

let   s see how the cosine computes which of the words apricot or digital is closer
in meaning to information, just using raw counts from the following simpli   ed table:

large data computer

0
2
1

2
0
1

apricot
digital

0
1
information
6
   
   
1 + 36 + 1
4 + 0 + 0
0 + 6 + 2
   
1 + 36 + 1

   
0 + 1 + 4

2 + 0 + 0

=

cos(apricot,information) =

cos(digital,information) =

2
   
=
2
38
8   
   
38
5

= .16

= .58

(6.11)

the model decides that information is closer to digital than it is to apricot, a

result that seems sensible. fig. 6.7 shows a visualization.

6.5 tf-idf: weighing terms in the vector

the co-occurrence matrix in fig. 6.5 represented each cell by the raw frequency of
the co-occurrence of two words.

it turns out, however, that simple frequency isn   t the best measure of association
between words. one problem is that raw frequency is very skewed and not very
discriminative. if we want to know what kinds of contexts are shared by apricot
and pineapple but not by digital and information, we   re not going to get good dis-
crimination from words like the, it, or they, which occur frequently with all sorts of
words and aren   t informative about any particular word. we saw this also in fig. 6.3
for the shakespeare corpus; the dimension for the word good is not very discrimina-
tive between plays; good is simply a frequent word and has roughly equivalent high
frequencies in each of the plays.

6.5

    tf-idf: weighing terms in the vector

13

figure 6.7 a graphical demonstration of cosine similarity, showing vectors for three words
(apricot, digital, and information) in the two dimensional space de   ned by counts of the
words data and large in the neighborhood. note that the angle between digital and informa-
tion is smaller than the angle between apricot and information. when two vectors are more
similar, the cosine is larger but the angle is smaller; the cosine has its maximum (1) when the
angle between two vectors is smallest (0   ); the cosine of all other angles is less than 1.

it   s a bit of a paradox. word that occur nearby frequently (maybe sugar appears
often in our corpus near apricot) are more important than words that only appear
once or twice. yet words that are too frequent   ubiquitous, like the or good    are
unimportant. how can we balance these two con   icting constraints?

the tf-idf algorithm (the    -    here is a hyphen, not a minus sign) algorithm is the

product of two terms, each term capturing one of these two intuitions:

1. the    rst is the term frequency (luhn, 1957): the frequency of the word in the
document. normally we want to downweight the raw frequency a bit, since
a word appearing 100 times in a document doesn   t make that word 100 times
more likely to be relevant to the meaning of the document. so we generally
use the log10 of the frequency, resulting in the following de   nition for the term
frequency weight:

(cid:26) 1 + log10 count(t,d)

tft,d =

0

if count(t,d) > 0
otherwise

thus terms which occur 10 times in a document would have a tf=2, 100 times
in a document tf=3, 1000 times tf=4, and so on.

2. the second factor is used to give a higher weight to words that occur only
in a few documents. terms that are limited to a few documents are useful
for discriminating those documents from the rest of the collection; terms that
occur frequently across the entire collection aren   t as helpful. the document
frequency dft of a term t is simply the number of documents it occurs in. by
contrast, the collection frequency of a term is the total number of times the
word appears in the whole collection in any document. consider in the col-
lection shakespeare   s 37 plays the two words romeo and action. the words
have identical collection frequencies of 113 (they both occur 113 times in all
the plays) but very different document frequencies, since romeo only occurs
in a single play. if our goal is    nd documents about the romantic tribulations
of romeo, the word romeo should be highly weighted:

term frequency

document
frequency

1234567123digitalapricotinformationdimension 1:    large   dimension 2:    data   14 chapter 6

    vector semantics

collection frequency document frequency

romeo 113
113
action

1
31

idf

we assign importance to these more discriminative words like romeo via
the inverse document frequency or idf term weight (sparck jones, 1972).
the idf is de   ned using the fraction n/dft, where n is the total number of
documents in the collection, and dft is the number of documents in which
term t occurs. the fewer documents in which a term occurs, the higher this
weight. the lowest weight of 1 is assigned to terms that occur in all the
documents.
in shakespeare
we would use a play; when processing a collection of encyclopedia articles
like wikipedia, the document is a wikipedia page; in processing newspaper
articles, the document is a single article. occasionally your corpus might
not have appropriate document divisions and you might need to break up the
corpus into documents yourself for the purposes of computing idf.

it   s usually clear what counts as a document:

because of the large number of documents in many collections, this mea-
sure is usually squashed with a log function. the resulting de   nition for in-
verse document frequency (idf) is thus

(cid:18) n

(cid:19)

dft

idft = log10

(6.12)

here are some idf values for some words in the shakespeare corpus, ranging
from extremely informative words which occur in only one play like romeo, to
those that occur in a few like salad or falstaff, to those which are very common like
fool or so common as to be completely non-discriminative since they occur in all 37
plays like good or sweet.3

word
romeo
salad
falstaff
forest
battle
fool
good
sweet

df
1
2
4
12
21
36
37
37

idf
1.57
1.27
0.967
0.489
0.074
0.012
0
0

tf-idf

the tf-idf weighting of the value for word t in document d, wt,d thus combines

term frequency with idf:

wt,d = tft,d    idft

(6.13)

fig. 6.8 applies tf-idf weighting to the shakespeare term-document matrix in fig. 6.2.
note that the tf-idf values for the dimension corresponding to the word good have
now all become 0; since this word appears in every document, the tf-idf algorithm
leads it to be ignored in any comparison of the plays. similarly, the word fool, which
appears in 36 out of the 37 plays, has a much lower weight.

the tf-idf weighting is by far the dominant way of weighting co-occurrence ma-
trices in information retrieval, but also plays a role in many other aspects of natural

3 sweet was one of shakespeare   s favorite adjectives, a fact probably related to the increased use of
sugar in european recipes around the turn of the 16th century (jurafsky, 2014, p. 175).

6.6

    applications of the tf-idf vector model

15

as you like it

twelfth night

julius caesar

henry v

0

0
0

0.22

0.074

0.019
0.049

battle
good
fool
wit
figure 6.8 a tf-idf weighted term-document matrix for four words in four shakespeare
plays, using the counts in fig. 6.2. note that the idf weighting has eliminated the importance
of the ubiquitous word good and vastly reduced the impact of the almost-ubiquitous word
fool.

0.0036
0.018

0.0083
0.022

0.021
0.044

0.28

0

0

language processing. it   s also a great baseline, the simple thing to try    rst. we   ll
look at other weightings like ppmi (positive pointwise mutual information) in sec-
tion 6.7.

6.6 applications of the tf-idf vector model

in summary, the vector semantics model we   ve described so far represents a target
word as a vector with dimensions corresponding to all the words in the vocabulary
(length |v|, with vocabularies of 20,000 to 50,000), which is also sparse (most values
are zero). the values in each dimension are the frequency with which the target
word co-occurs with each neighboring context word, weighted by tf-idf. the model
computes the similarity between two words x and y by taking the cosine of their
tf-idf vectors; high cosine, high similarity. this entire model is sometimes referred
to for short as the tf-idf model, after the weighting function.

one common use for a tf-idf model is to compute word similarity, a useful tool
for tasks like    nding word paraphrases, tracking changes in word meaning, or au-
tomatically discovering meanings of words in different corpora. for example, we
can    nd the 10 most similar words to any target word w by computing the cosines
between w and each of the v     1 other words, sorting, and looking at the top 10.

the tf-idf vector model can also be used to decide if two documents are similar.
we represent a document by taking the vectors of all the words in the document, and
computing the centroid of all those vectors. the centroid is the multidimensional
version of the mean; the centroid of a set of vectors is a single vector that has the
minimum sum of squared distances to each of the vectors in the set. given k word
vectors w1,w2, ...,wk, the centroid document vector d is:

d =

w1 + w2 + ... + wk

k

(6.14)

centroid

document
vector

given two documents, we can then compute their document vectors d1 and d2,

and estimate the similarity between the two documents by cos(d1,d2).

document similarity is also useful for all sorts of applications; information re-
trieval, plagiarism detection, news recommender systems, and even for digital hu-
manities tasks like comparing different versions of a text to see which are similar to
each other.

16 chapter 6

    vector semantics

6.7 optional: pointwise mutual information (pmi)

an alternative weighting function to tf-idf is called ppmi (positive pointwise mutual
information). ppmi draws on the intuition that best way to weigh the association
between two words is to ask how much more the two words co-occur in our corpus
than we would have a priori expected them to appear by chance.

pointwise mutual information (fano, 1961)4 is one of the most important con-
cepts in nlp. it is a measure of how often two events x and y occur, compared with
what we would expect if they were independent:

pointwise
mutual
information

i(x,y) = log2

p(x,y)
p(x)p(y)

(6.16)

the pointwise mutual information between a target word w and a context word

c (church and hanks 1989, church and hanks 1990) is then de   ned as:

pmi(w,c) = log2

p(w,c)
p(w)p(c)

(6.17)

the numerator tells us how often we observed the two words together (assuming
we compute id203 by using the id113). the denominator tells us how often
we would expect the two words to co-occur assuming they each occurred indepen-
dently; recall that the id203 of two independent events both occurring is just
the product of the probabilities of the two events. thus, the ratio gives us an esti-
mate of how much more the two words co-occur than we expect by chance. pmi is
a useful tool whenever we need to    nd words that are strongly associated.

pmi values range from negative to positive in   nity. but negative pmi values
(which imply things are co-occurring less often than we would expect by chance)
tend to be unreliable unless our corpora are enormous. to distinguish whether two
words whose individual id203 is each 10   6 occur together more often than
chance, we would need to be certain that the id203 of the two occurring to-
gether is signi   cantly different than 10   12, and this kind of granularity would require
an enormous corpus. furthermore it   s not clear whether it   s even possible to evalu-
ate such scores of    unrelatedness    with human judgments. for this reason it is more
common to use positive pmi (called ppmi) which replaces all negative pmi values
with zero (church and hanks 1989, dagan et al. 1993, niwa and nitta 1994)5:

ppmi(w,c) = max(log2

p(w,c)
p(w)p(c)

,0)

(6.18)

more formally, let   s assume we have a co-occurrence matrix f with w rows (words)
and c columns (contexts), where fi j gives the number of times word wi occurs in

ppmi

4 pointwise mutual information is based on the mutual information between two random variables x
and y , which is de   ned as:

i(x,y ) =

p(x,y)log2

p(x,y)
p(x)p(y)

(6.15)

(cid:88)

(cid:88)

x

y

in a confusion of terminology, fano used the phrase mutual information to refer to what we now call
pointwise mutual information and the phrase expectation of the mutual information for what we now call
mutual information
5 positive pmi also cleanly solves the problem of what to do with zero counts, using 0 to replace the
       from log(0).

(6.19)

(6.20)

6.7

    optional: pointwise mutual information (pmi)

17

context c j. this can be turned into a ppmi matrix where ppmii j gives the ppmi
value of word wi with context c j as follows:

pi j =

(cid:80)w

i=1

(cid:80)c

fi j

j=1 fi j

pi    =

(cid:80)c
(cid:80)w
(cid:80)c

j=1 fi j

i=1

j=1 fi j

(cid:80)w
(cid:80)w
(cid:80)c

i=1 fi j

i=1

j=1 fi j

p    j =

ppmii j = max(log2

pi j

pi    p    j

,0)

thus for example we could compute ppmi(w=information,c=data), assuming we
pretended that fig. 6.5 encompassed all the relevant word contexts/dimensions, as
follows:

p(w=information,c=data) =

p(w=information) =

p(c=data) =

6
19
11
19
7
19

= .316

= .579

= .368

ppmi(information,data) = log2(.316/(.368    .579)) = .568

fig. 6.9 shows the joint probabilities computed from the counts in fig. 6.5, and
fig. 6.10 shows the ppmi values.

computer

0
0

0.11
0.05

p(w,context)

pinch
0.05
0.05

0
0

data

0
0

0.05
.32

result

0
0

0.05
0.21

sugar
0.05
0.05

0
0

p(w)
p(w)
0.11
0.11
0.21
0.58

apricot
pineapple

digital

information

p(context)
figure 6.9 replacing the counts in fig. 6.5 with joint probabilities, showing the marginals
around the outside.

0.26

0.11

0.37

0.16

0.11

computer

data

apricot
pineapple

digital

0
0

1.66

0
0
0

pinch
2.25
2.25

0
0

result

0
0
0

sugar
2.25
2.25

0
0

0

0.57

information
figure 6.10 the ppmi matrix showing the association between words and context words,
computed from the counts in fig. 6.5 again showing    ve dimensions. note that the 0
ppmi values are ones that had a negative pmi; for example pmi(information,computer) =
log2(.05/(.16     .58)) =    0.618, meaning that information and computer co-occur in this
mini-corpus slightly less often than we would expect by chance, and with ppmi we re-
place negative values by zero. many of the zero ppmi values had a pmi of       , like
pmi(apricot,computer) = log2(0/(0.16    0.11)) = log2(0) =       .

0.47

pmi has the problem of being biased toward infrequent events; very rare words
tend to have very high pmi values. one way to reduce this bias toward low frequency
events is to slightly change the computation for p(c), using a different function p   (c)
that raises contexts to the power of   :

ppmi   (w,c) = max(log2

p(w,c)

p(w)p   (c)

,0)

(6.21)

18 chapter 6

    vector semantics

(cid:80)

count(c)  
c count(c)  

p   (c) =

(6.22)

levy et al. (2015) found that a setting of    = 0.75 improved performance of
embeddings on a wide range of tasks (drawing on a similar weighting used for skip-
grams described below in eq. 6.31). this works because raising the id203 to
   = 0.75 increases the id203 assigned to rare contexts, and hence lowers their
pmi (p   (c) > p(c) when c is rare).

another possible solution is laplace smoothing: before computing pmi, a small
constant k (values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting) all the non-zero values. the larger the k, the more the non-zero counts
are discounted.

computer

data

pinch

result

sugar

information
figure 6.11 laplace (add-2) smoothing of the counts in fig. 6.5.

apricot
pineapple

digital

apricot
pineapple

digital

2
2
4
3

0
0

0.62

computer

data

2
2
3
8

0
0
0

3
3
2
2

2
2
3
6

pinch
0.56
0.56

0
0

result

0
0
0

3
3
2
2

sugar
0.56
0.56

0
0

information
figure 6.12 the add-2 laplace smoothed ppmi matrix from the add-2 smoothing counts
in fig. 6.11.

0.37

0.58

0

6.8 id97

in the previous sections we saw how to represent a word as a sparse, long vector with
dimensions corresponding to the words in the vocabulary, and whose values were tf-
idf or ppmi functions of the count of the word co-occurring with each neighboring
word. in this section we turn to an alternative method for representing a word: the
use of vectors that are short (of length perhaps 50-500) and dense (most values are
non-zero).

it turns out that dense vectors work better in every nlp task than sparse vec-
tors. while we don   t complete understand all the reasons for this, we have some
intuitions. first, dense vectors may be more successfully included as features in
machine learning systems; for example if we use 100-dimensional word embed-
dings as features, a classi   er can just learn 100 weights to represent a function of
word meaning; if we instead put in a 50,000 dimensional vector, a classi   er would
have to learn tens of thousands of weights for each of the sparse dimensions. sec-
ond, because they contain fewer parameters than sparse vectors of explicit counts,
dense vectors may generalize better and help avoid over   tting. finally, dense vec-
tors may do a better job of capturing synonymy than sparse vectors. for example,
car and automobile are synonyms; but in a typical sparse vector representation, the
car dimension and the automobile dimension are distinct dimensions. because the

skip-gram
sgns
id97

6.8

    id97

19

relationship between these two dimensions is not modeled, sparse vectors may fail
to capture the similarity between a word with car as a neighbor and a word with
automobile as a neighbor.

in this section we introduce one method for very dense, short vectors, skip-
gram with negative sampling, sometimes called sgns. the skip-gram algorithm
is one of two algorithms in a software package called id97, and so sometimes
the algorithm is loosely referred to as id97 (mikolov et al. 2013, mikolov
et al. 2013a). the id97 methods are fast, ef   cient to train, and easily avail-
able online with code and pretrained embeddings. we point to other embedding
methods, like the equally popular glove (pennington et al., 2014), at the end of the
chapter.

the intuition of id97 is that instead of counting how often each word w oc-
curs near, say, apricot, we   ll instead train a classi   er on a binary prediction task:    is
word w likely to show up near apricot?    we don   t actually care about this prediction
task; instead we   ll take the learned classi   er weights as the id27s.

the revolutionary intuition here is that we can just use running text as implicitly
supervised training data for such a classi   er; a word s that occurs near the target
word apricot acts as gold    correct answer    to the question    is word w likely to show
up near apricot?    this avoids the need for any sort of hand-labeled supervision
signal. this idea was    rst proposed in the task of neural id38, when
bengio et al. (2003) and collobert et al. (2011) showed that a neural language model
(a neural network that learned to predict the next word from prior words) could just
use the next word in running text as its supervision signal, and could be used to learn
an embedding representation for each word as part of doing this prediction task.

we   ll see how to do neural networks in the next chapter, but id97 is a
much simpler model than the neural network language model, in two ways. first,
id97 simpli   es the task (making it binary classi   cation instead of word pre-
diction). second, id97 simpli   es the architecture (training a id28
classi   er instead of a multi-layer neural network with hidden layers that demand
more sophisticated training algorithms). the intuition of skip-gram is:

1. treat the target word and a neighboring context word as positive examples.
2. randomly sample other words in the lexicon to get negative samples
3. use id28 to train a classi   er to distinguish those two cases
4. use the regression weights as the embeddings

6.8.1 the classi   er
let   s start by thinking about the classi   cation task, and then turn to how to train.
imagine a sentence like the following, with a target word apricot and assume we   re
using a window of   2 context words:

... lemon,

a [tablespoon of apricot jam,

c1

c2

t

c3

a] pinch ...
c4

our goal is to train a classi   er such that, given a tuple (t,c) of a target word
t paired with a candidate context word c (for example (apricot, jam), or perhaps
(apricot, aardvark) it will return the id203 that c is a real context word (true
for jam, false for aardvark):

p(+|t,c)

(6.23)

20 chapter 6

    vector semantics

the id203 that word c is not a real context word for t is just 1 minus

eq. 6.23:

p(   |t,c) = 1    p(+|t,c)

(6.24)
how does the classi   er compute the id203 p? the intuition of the skip-
gram model is to base this id203 on similarity: a word is likely to occur near
the target if its embedding is similar to the target embedding. how can we compute
similarity between embeddings? recall that two vectors are similar if they have a
high dot product (cosine, the most popular similarity metric, is just a normalized dot
product). in other words:

similarity(t,c)     t    c

(6.25)
of course, the dot product t    c is not a id203, it   s just a number ranging from
0 to    . (recall, for that matter, that cosine isn   t a id203 either). to turn the
dot product into a id203, we   ll use the logistic or sigmoid function    (x), the
fundamental core of id28:

   (x) =

1

1 + e   x

(6.26)

the id203 that word c is a real context word for target word t is thus computed
as:

p(+|t,c) =

1

1 + e   t  c

(6.27)

the sigmoid function just returns a number between 0 and 1, so to make it a proba-
bility we   ll need to make sure that the total id203 of the two possible events (c
being a context word, and c not being a context word) sum to 1.

the id203 that word c is not a real context word for t is thus:

p(   |t,c) = 1    p(+|t,c)

e   t  c
1 + e   t  c

=

(6.28)

equation 6.27 give us the id203 for one word, but we need to take account of
the multiple context words in the window. skip-gram makes the strong but very
useful simplifying assumption that all context words are independent, allowing us to
just multiply their probabilities:

p(+|t,c1:k) =

logp(+|t,c1:k) =

1

1 + e   t  ci

log

1

1 + e   t  ci

(6.29)

(6.30)

k(cid:89)
k(cid:88)

i=1

i=1

in summary, skip-gram trains a probabilistic classi   er that, given a test target word
t and its context window of k words c1:k, assigns a id203 based on how similar
this context window is to the target word. the id203 is based on applying the
logistic (sigmoid) function to the dot product of the embeddings of the target word
with each context word. we could thus compute this id203 if only we had
embeddings for each word target and context word in the vocabulary. let   s now turn
to learning these embeddings (which is the real goal of training this classi   er in the
   rst place).

6.8

    id97

21

6.8.2 learning skip-gram embeddings
id97 learns embeddings by starting with an initial set of embedding vectors
and then iteratively shifting the embedding of each word w to be more like the em-
beddings of words that occur nearby in texts, and less like the embeddings of words
that don   t occur nearby.

let   s start by considering a single piece of the training data, from the sentence

above:

... lemon,

a [tablespoon of apricot jam,

c1

c2

t

c3

this example has a target word t (apricot), and 4 context words in the l =   2

window, resulting in 4 positive training instances (on the left below):
negative examples -
c
twelve

c

a] pinch ...
c4

t

t
apricot aardvark apricot
apricot puddle
apricot where
apricot coaxial

apricot hello
apricot dear
apricot forever

c
tablespoon

positive examples +
t
apricot
apricot of
apricot preserves
apricot or

for training a binary classi   er we also need negative examples, and in fact skip-
gram uses more negative examples than positive examples, the ratio set by a param-
eter k. so for each of these (t,c) training instances we   ll create k negative samples,
each consisting of the target t plus a    noise word   . a noise word is a random word
from the lexicon, constrained not to be the target word t. the right above shows the
setting where k = 2, so we   ll have 2 negative examples in the negative training set
    for each positive example t,c.

the noise words are chosen according to their weighted unigram frequency
p   (w), where    is a weight. if we were sampling according to unweighted fre-
quency p(w), it would mean that with unigram id203 p(   the   ) we would choose
the word the as a noise word, with unigram id203 p(   aardvark   ) we would
choose aardvark, and so on. but in practice it is common to set    = .75, i.e. use the
weighting p 3

4 (w):

(cid:80)

p   (w) =

count(w)  
w(cid:48) count(w(cid:48))  

(6.31)

setting    = .75 gives better performance because it gives rare noise words slightly
higher id203: for rare words, p   (w) > p(w). to visualize this intuition, it
might help to work out the probabilities for an example with two events, p(a) = .99
and p(b) = .01:

p   (a) =

p   (b) =

.99.75

.99.75 + .01.75 = .97
.99.75 + .01.75 = .03

.01.75

(6.32)

given the set of positive and negative training instances, and an initial set of
embeddings, the goal of the learning algorithm is to adjust those embeddings such
that we

    maximize the similarity of the target word, context word pairs (t,c) drawn

from the positive examples

22 chapter 6

    vector semantics

    minimize the similarity of the (t,c) pairs drawn from the negative examples.
we can express this formally over the whole training set as:

l(   ) =

logp(+|t,c) +

logp(   |t,c)

(6.33)

(cid:88)

(t,c)   +

or, focusing in on one word/context pair (t,c) with its k noise words n1...nk, the

learning objective l is:

l(   ) = logp(+|t,c) +

logp(   |t,ni)

= log   (c  t) +

log   (   ni   t)

(cid:88)

(t,c)      

i=1

k(cid:88)
k(cid:88)
k(cid:88)

i=1

i=1

1

= log

1 + e   c  t +

log

1

1 + eni  t

(6.34)

that is, we want to maximize the dot product of the word with the actual context
words, and minimize the dot products of the word with the k negative sampled non-
neighbor words.

we can then use stochastic id119 to train to this objective, iteratively
modifying the parameters (the embeddings for each target word t and each context
word or noise word c in the vocabulary) to maximize the objective.

note that the skip-gram model thus actually learns two separate embeddings
for each word w: the target embedding t and the context embedding c. these
embeddings are stored in two matrices, the target matrix t and the context matrix
c. so each row i of the target matrix t is the 1   d vector embedding ti for word
i in the vocabulary v , and each column i of the context matrix c is a d    1 vector
embedding ci for word i in v . fig. 6.13 shows an intuition of the learning task for
the embeddings encoded in these two matrices.

target
embedding
context
embedding

figure 6.13 the skip-gram model tries to shift embeddings so the target embedding (here
for apricot) are closer to (have a higher dot product with) context embeddings for nearby
words (here jam) and further from (have a lower dot product with) context embeddings for
words that don   t occur nearby (here aardvark).

just as in id28, then, the learning algorithm starts with randomly
initialized w and c matrices, and then walks through the training corpus using gra-
dient descent to move w and c so as to maximize the objective in eq. 6.34. thus
the matrices w and c function as the parameters    that id28 is tuning.

1.k.n.v1.2      .j         v1...dwc1. ..          dincreasesimilarity( apricot , jam)wj . ckjamapricotaardvarkdecreasesimilarity( apricot , aardvark)wj . cn      apricot jam      neighbor wordrandom noiseword6.9

    visualizing embeddings

23

once the embeddings are learned, we   ll have two embeddings for each word wi:
ti and ci. we can choose to throw away the c matrix and just keep w , in which case
each word i will be represented by the vector ti.

alternatively we can add the two embeddings together, using the summed em-
bedding ti + ci as the new d-dimensional embedding, or we can concatenate them
into an embedding of dimensionality 2d.

as with the simple count-based methods like tf-idf, the context window size l
effects the performance of skip-gram embeddings, and experiments often tune the
parameter l on a dev set. one difference from the count-based methods is that for
skip-grams, the larger the window size the more computation the algorithm requires
for training (more neighboring words must be predicted).

6.9 visualizing embeddings

visualizing embeddings is an important goal in helping understands, apply, and im-
prove these models of word meaning. but how can we visualize a (for example)
100-dimensional vector?

the simplest way to visualize the meaning of a word w embedded in a space
is to list the most similar words to w sorting all words in the vocabulary by their
cosines. for example the 7 closest words to frog using the glove embeddings are:
frogs, toad, litoria, leptodactylidae, rana, lizard, and eleutherodactylus (pennington
et al., 2014)

yet another visualization method is to use a clus-
tering algorithm to show a hierarchical representa-
tion of which words are similar to others in the em-
bedding space. the example on the right uses hi-
erarchical id91 of some embedding vectors for
nouns as a visualization method (rohde et al., 2006).
probably the most common visualization method,
however, is to project the 100 dimensions of a word
down into 2 dimensions. fig. 6.1 showed one such
visualization, using a projection method called t-
sne (van der maaten and hinton, 2008).

6.10 semantic properties of embeddings

vector semantic models have a number of parameters. one parameter that is relevant
to both sparse tf-idf vectors and dense id97 vectors is the size of the context
window used to collect counts. this is generally between 1 and 10 words on each
side of the target word (for a total context of 3-20 words).

the choice depends on on the goals of the representation. shorter context win-
dows tend to lead to representations that are a bit more syntactic, since the infor-
mation is coming from immediately nearby words. when the vectors are computed
from short context windows, the most similar words to a target word w tend to be
semantically similar words with the same parts of speech. when vectors are com-
puted from long context windows, the highest cosine words to a target word w tend
to be words that are topically related but not similar.

rohde,gonnerman,plautmodelingwordmeaningusinglexicalco-occurrenceheadhandfacedogamericacateyeeuropefootchinafrancechicagoarmfingernoselegrussiamouseafricaatlantaearshoulderasiacowbullpuppylionhawaiimontrealtokyotoemoscowtoothnashvillebrazilwristkittenankleturtleoysterfigure8:multidimensionalscalingforthreenounclasses.wristankleshoulderarid113ghandfootheadnosefingertoefaceeareyetoothdogcatpuppykittencowmouseturtleoysterlionbullchicagoatlantamontrealnashvilletokyochinarussiaafricaasiaeuropeamericabrazilmoscowfrancehawaiifigure9:hierarchicalid91forthreenounclassesusingdistancesbasedonvectorcorrelations.2024 chapter 6

    vector semantics

   rst-order
co-occurrence

second-order
co-occurrence

for example levy and goldberg (2014a) showed that using skip-gram with a
window of   2, the most similar words to the word hogwarts (from the harry potter
series) were names of other    ctional schools: sunnydale (from buffy the vampire
slayer) or evernight (from a vampire series). with a window of   5, the most similar
words to hogwarts were other words topically related to the harry potter series:
dumbledore, malfoy, and half-blood.

it   s also often useful to distinguish two kinds of similarity or association between
words (sch  utze and pedersen, 1993). two words have    rst-order co-occurrence
(sometimes called syntagmatic association) if they are typically nearby each other.
thus wrote is a    rst-order associate of book or poem. two words have second-order
co-occurrence (sometimes called paradigmatic association) if they have similar
neighbors. thus wrote is a second-order associate of words like said or remarked.

analogy another semantic property of embeddings is their ability to capture re-
lational meanings. mikolov et al. (2013b) and levy and goldberg (2014b) show
that the offsets between vector embeddings can capture some analogical relations
between words. for example, the result of the expression vector(   king   ) - vec-
tor(   man   ) + vector(   woman   ) is a vector close to vector(   queen   ); the left panel
in fig. 6.14 visualizes this, again projected down into 2 dimensions. similarly, they
found that the expression vector(   paris   ) - vector(   france   ) + vector(   italy   ) results
in a vector that is very close to vector(   rome   ).

(a)

(b)

figure 6.14 relational properties of the vector space, shown by projecting vectors onto two dimensions. (a)
   king    -    man    +    woman    is close to    queen    (b) offsets seem to capture comparative and superlative morphology
(pennington et al., 2014).

embeddings and historical semantics: embeddings can also be a useful tool
for studying how meaning changes over time, by computing multiple embedding
spaces, each from texts written in a particular time period. for example fig. 6.15
shows a visualization of changes in meaning in english words over the last two
centuries, computed by building separate embedding spaces for each decade from
historical corpora like google id165s (lin et al., 2012) and the corpus of histori-
cal american english (davies, 2012).

6.11

    bias and embeddings

25

figure 6.15 a id167 visualization of the semantic change of 3 words in english using
id97 vectors. the modern sense of each word, and the grey context words, are com-
puted from the most recent (modern) time-point embedding space. earlier points are com-
puted from earlier historical embedding spaces. the visualizations show the changes in the
word gay from meanings related to    cheerful    or    frolicsome    to referring to homosexuality,
the development of the modern    transmission    sense of broadcast from its original sense of
sowing seeds, and the pejoration of the word awful as it shifted from meaning    full of awe   
to meaning    terrible or appalling    (hamilton et al., 2016).

6.11 bias and embeddings

in addition to their ability to learn word meaning from text, embeddings, alas, also
reproduce the implicit biases and stereotypes that were latent in the text. recall that
embeddings model analogical relations;    queen    as the closest word to    king    -    man   
+    woman    implies the analogy man:woman::king:queen. but embedding analogies
also exhibit gender stereotypes. for example bolukbasi et al. (2016)    nd that the
closest occupation to    man    -    computer programmer    +    woman    in id97 em-
beddings trained on news text is    homemaker   , and that the embeddings similarly
suggest the analogy    father    is to    doctor    as    mother    is to    nurse   . algorithms that
used embeddings as part of an algorithm to search for potential programmers or
doctors might thus incorrectly downweight documents with women   s names.

embeddings also encode the implicit associations that are a property of human
reasoning. the implicit association test (greenwald et al., 1998) measures peo-
ple   s associations between concepts (like       owers    or    insects   ) and attributes (like
   pleasantness    and    unpleasantness   ) by measuring differences in the latency with
which they label words in the various categories.6 using such methods, people
in the united states have been shown to associate african-american names with
unpleasant words (more than european-american names), male names more with
mathematics and female names with the arts, and old people   s names with unpleas-
ant words (greenwald et al. 1998, nosek et al. 2002a, nosek et al. 2002b). caliskan
et al. (2017) replicated all these    ndings of implicit associations using glove vec-
tors and cosine similarity instead of human latencies. for example afrian american
names like    leroy    and    shaniqua    had a higher glove cosine with unpleasant words
while european american names (   brad   ,    greg   ,    courtney   ) had a higher cosine
with pleasant words. any embedding-aware algorithm that made use of word senti-
ment could thus lead to bias against african americans.

6 roughly speaking, if humans associate       owers    with    pleasantness    and    insects    with    unpleasant-
ness   , when they are instructed to push a red button for       owers    (daisy, iris, lilac) and    pleasant words   
(love, laughter, pleasure) and a green button for    insects    (   ea, spider, mosquito) and    unpleasant words   
(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for
      owers    and    unpleasant words    and a green button for    insects    and    pleasant words   .

chapter5.dynamicsocialrepresentationsofwordmeaning79figure5.1:two-dimensionalvisualizationofsemanticchangeinenglishusingsgnsvectors(seesection5.8forthevisualizationalgorithm).a,thewordgayshiftedfrommeaning   cheerful   or   frolicsome   toreferringtohomosexuality.a,intheearly20thcenturybroadcastreferredto   castingoutseeds   ;withtheriseoftelevisionandradioitsmeaningshiftedto   transmittingsignals   .c,awfulunderwentaprocessofpejoration,asitshiftedfrommeaning   fullofawe   tomeaning   terribleorappalling   [212].thatadverbials(e.g.,actually)haveageneraltendencytoundergosubjecti   cationwheretheyshiftfromobjectivestatementsabouttheworld(e.g.,   sorry,thecarisactuallybroken   )tosubjectivestatements(e.g.,   ican   tbelieveheactuallydidthat   ,indicatingsurprise/disbelief).5.2.2computationallinguisticstudiestherearealsoanumberofrecentworksanalyzingsemanticchangeusingcomputationalmethods.[200]uselatentsemanticanalysistoanalyzehowwordmeaningsbroadenandnarrowovertime.[113]userawco-occurrencevectorstoperformanumberofhistoricalcase-studiesonsemanticchange,and[252]performasimilarsetoid122all-scalecase-studiesusingtemporaltopicmodels.[87]constructpoint-wisemutualinformation-basedembeddingsandfoundthatsemanticchangesuncoveredbytheirmethodhadreasonableagreementwithhumanjudgments.[129]and[119]use   neural   word-embeddingmethodstodetectlinguisticchangepoints.finally,[257]analyzehistoricalco-occurrencestotestwhethersynonymstendtochangeinsimilarways.26 chapter 6

    vector semantics

recent research focuses on ways to try to remove the kinds of biases, for example
by developing a transformation of the embedding space that removes gender stereo-
types but preserves de   nitional gender (bolukbasi et al. 2016, zhao et al. 2017).

historical embeddings are also being used to measure biases in the past. garg
et al. (2018) used embeddings from historical texts to measure the association be-
tween embeddings for occupations and embeddings for names of various ethnici-
ties or genders (for example the relative cosine similarity of women   s names versus
men   s to occupation words like    librarian    or    carpenter   ) across the 20th century.
they found that the cosines correlate with the empirical historical percentages of
women or ethnic groups in those occupation. historical embeddings also replicated
old surveys of ethnic stereotypes; the tendency of experimental participants in 1933
to associate adjectives like    industrious    or    superstitious    with, e.g., chinese eth-
nicity, correlates with the cosine between chinese last names and those adjectives
using embeddings trained on 1930s text. they also were able to document historical
gender biases, such as the fact that embeddings for adjectives related to competence
(   smart   ,    wise   ,    thoughtful   ,    resourceful   ) had a higher cosine with male than fe-
male words, and showed that this bias has been slowly decreasing since 1960.

we will return in later chapters to this question about the role of bias in natural

language processing and machine learning in general.

6.12 evaluating vector models

the most important evaluation metric for vector models is extrinsic evaluation on
tasks; adding them as features into any nlp task and seeing whether this improves
performance over some other model.

nonetheless it is useful to have intrinsic evaluations. the most common metric
is to test their performance on similarity, computing the correlation between an
algorithm   s word similarity scores and word similarity ratings assigned by humans.
wordsim-353 (finkelstein et al., 2002) is a commonly used set of ratings from 0
to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77.
siid113x-999 (hill et al., 2015) is a more dif   cult dataset that quanti   es similarity
(cup, mug) rather than relatedness (cup, coffee), and including both concrete and
abstract adjective, noun and verb pairs. the toefl dataset is a set of 80 questions,
each consisting of a target word with 4 additional word choices; the task is to choose
which is the correct synonym, as in the example: levied is closest in meaning to:
imposed, believed, requested, correlated (landauer and dumais, 1997). all of these
datasets present words without context.

slightly more realistic are intrinsic similarity tasks that include context. the
stanford contextual word similarity (scws) dataset (huang et al., 2012) offers a
richer evaluation scenario, giving human judgments on 2,003 pairs of words in their
sentential context, including nouns, verbs, and adjectives. this dataset enables the
evaluation of word similarity algorithms that can make use of context words. the
semantic textual similarity task (agirre et al. 2012, agirre et al. 2015) evaluates the
performance of sentence-level similarity algorithms, consisting of a set of pairs of
sentences, each pair with human-labeled similarity scores.

another task used for evaluate is an analogy task, where the system has to solve
problems of the form a is to b as c is to d, given a, b, and c and having to    nd d.
thus given athens is to greece as oslo is to
, the system must    ll in the word
norway. or more syntactically-oriented examples: given mouse, mice, and dollar

the system must return dollars. large sets of such tuples have been created (mikolov
et al. 2013, mikolov et al. 2013b).

6.13

    summary

27

6.13 summary

    in vector semantics, a word is modeled as a vector   a point in high-dimensional

space, also called an embedding.

    vector semantic models fall into two classes: sparse and dense. in sparse
models like tf-idf each dimension corresponds to a word in the vocabulary v ;
    cell in sparse models are functions of co-occurrence counts. the term-
document matrix has rows for each word (term) in the vocabulary and a
column for each document.

    the word-context matrix has a row for each (target) word in the vocabulary

and a column for each context term in the vocabulary.

    a common sparse weighting is tf-idf, which weights each cell by its term

frequency and inverse document frequency.

    word and document similarity is computed by computing the dot product
between vectors. the cosine of two vectors   a normalized dot product   is
the most popular such metric.

    ppmi (pointwise positive mutual information) is an alternative weighting

    dense vector models have dimensionality 50-300 and the dimensions are harder

scheme to tf-idf.

to interpret.

    the id97 family of models, including skip-gram and cbow, is a pop-

ular ef   cient way to compute dense embeddings.

    skip-gram trains a id28 classi   er to compute the id203 that
two words are    likely to occur nearby in text   . this id203 is computed
from the dot product between the embeddings for the two words,

    skip-gram uses stochastic id119 to train the classi   er, by learning
embeddings that have a high dot-product with embeddings of words that occur
nearby and a low dot-product with noise words.

    other important embedding algorithms include glove, a method based on ra-
tios of word co-occurrence probabilities, and fasttext, an open-source library
for computing id27s by summing embeddings of the bag of char-
acter id165s that make up a word.

bibliographical and historical notes

the idea of vector semantics arose out of research in the 1950s in three distinct
   elds: linguistics, psychology, and computer science, each of which contributed a
fundamental aspect of the model.

the idea that meaning was related to distribution of words in context was widespread

in linguistic theory of the 1950s, among distributionalists like zellig harris, martin
joos, and j. r. firth, and semioticians like thomas sebeok. as joos (1950) put it,

28 chapter 6

    vector semantics

the linguist   s    meaning    of a morpheme. . . is by de   nition the set of conditional
probabilities of its occurrence in context with all other morphemes.

mechanical
indexing

semantic
feature

the idea that the meaning of a word might be modeled as a point in a multi-
dimensional semantic space came from psychologists like charles e. osgood, who
had been studying how people responded to the meaning of words by assigning val-
ues along scales like happy/sad, or hard/soft. osgood et al. (1957) proposed that
the meaning of a word in general could be modeled as a point in a multidimensional
euclidean space, and that the similarity of meaning between two words could be
modeled as the distance between these points in the space.

a    nal intellectual source in the 1950s and early 1960s was the    eld then called
mechanical indexing, now known as information retrieval. in what became known
as the vector space model for information retrieval (salton 1971,sparck jones 1986),
researchers demonstrated new ways to de   ne the meaning of words in terms of vec-
tors (switzer, 1965), and re   ned methods for word similarity based on measures
of statistical association between words like mutual information (giuliano, 1965)
and idf (sparck jones, 1972), and showed that the meaning of documents could be
represented in the same vector spaces used for words.

more distantly related is the idea of de   ning words by a vector of discrete fea-
tures, which has a venerable history in our    eld, with roots at least as far back as
descartes and leibniz (wierzbicka 1992, wierzbicka 1996). by the middle of the
20th century, beginning with the work of hjelmslev (hjelmslev, 1969) and    eshed
out in early models of generative grammar (katz and fodor, 1963), the idea arose of
representing meaning with semantic features, symbols that represent some sort of
primitive meaning. for example words like hen, rooster, or chick, have something
in common (they all describe chickens) and something different (their age and sex),
representable as:

+chicken, -adult

hen
+female, +chicken, +adult
rooster -female, +chicken, +adult
chick
the dimensions used by vector models of meaning to de   ne words, however, are
only abstractly related to this idea of a small    xed number of hand-built dimensions.
nonetheless, there has been some attempt to show that certain dimensions of em-
bedding models do contribute some speci   c compositional aspect of meaning like
these early semantic features.

the    rst use of dense vectors to model word meaning was the latent seman-
tic indexing (lsi) model (deerwester et al., 1988) recast as lsa (latent semantic
analysis) (deerwester et al., 1990). in lsa svd is applied to a term-document ma-
trix (each cell weighted by log frequency and normalized by id178), and then using
the    rst 300 dimensions as the embedding. lsa was then quickly widely applied:
as a cognitive model landauer and dumais (1997), and tasks like spell checking
(jones and martin, 1997), id38 (bellegarda 1997, coccaro and ju-
rafsky 1998, bellegarda 2000) morphology induction (schone and jurafsky 2000,
schone and jurafsky 2001), and essay grading (rehder et al., 1998). related mod-
els were simultaneously developed and applied to id51 by
sch  utze (1992). lsa also led to the earliest use of embeddings to represent words in
a probabilistic classi   er, in the id28 document router of sch  utze et al.
(1995). the idea of svd on the term-term matrix (rather than the term-document
matrix) as a model of meaning for nlp was proposed soon after lsa by sch  utze
(1992). sch  utze applied the low-rank (97-dimensional) embeddings produced by
svd to the task of id51, analyzed the resulting semantic

bibliographical and historical notes

29

space, and also suggested possible techniques like dropping high-order dimensions.
see sch  utze (1997).

a number of alternative matrix models followed on from the early svd work,
including probabilistic id45 (plsi) (hofmann, 1999) latent
dirichlet allocation (lda) (blei et al., 2003). nonnegative id105
(nmf) (lee and seung, 1999).

by the next decade, bengio et al. (2003) and bengio et al. (2006) showed that
neural language models could also be used to develop embeddings as part of the task
of word prediction. collobert and weston (2007), collobert and weston (2008), and
collobert et al. (2011) then demonstrated that embeddings could play a role for rep-
resenting word meanings for a number of nlp tasks. turian et al. (2010) compared
the value of different kinds of embeddings for different nlp tasks. mikolov et al.
(2011) showed that recurrent neural nets could be used as language models. the
idea of simplifying the hidden layer of these neural net language models to create
the skip-gram and cbow algorithms was proposed by mikolov et al. (2013). the
negative sampling training algorithm was proposed in mikolov et al. (2013a).

studies of embeddings include results showing an elegant mathematical relation-
ship between sparse and dense embeddings (levy and goldberg, 2014c), as well
as numerous surveys of embeddings and their parameterizations. (bullinaria and
levy 2007, bullinaria and levy 2012, lapesa and evert 2014, kiela and clark 2014,
levy et al. 2015).

there are many other embedding algorithms, using methods like non-negative
id105 (fyshe et al., 2015), or by converting sparse ppmi embeddings
to dense vectors by using svd (levy and goldberg, 2014c). the most widely-
used embedding model besides id97 is glove (pennington et al., 2014). the
name stands for global vectors, because the model is based on capturing global
corpus statistics. glove is based on ratios of probabilities from the word-word co-
occurrence matrix, combining the intuitions of count-based models like ppmi while
also capturing the linear structures used by methods like id97.

an extension of id97, fasttext (bojanowski et al., 2017), deals with un-
known words and sparsity in languages with rich morphology, by using subword
models. each word in fasttext is represented as itself plus a bag of constituent n-
grams, with special boundary symbols < and > added to each word. for example,
with n = 3 the word where would be represented by the character id165s:

fasttext

<wh, whe, her, ere, re>

plus the sequence

<where>

then a skipgram embedding is learned for each constituent id165, and the word
where is represented by the sum of all of the embeddings of its constituent id165s.
a fasttext open-source library, including pretrained embeddings for 157 languages,
is available at https://fasttext.cc.

see manning et al. (2008) for a deeper understanding of the role of vectors in in-
formation retrieval, including how to compare queries with documents, more details
on tf-idf, and issues of scaling to very large datasets.

cruse (2004) is a useful introductory linguistic text on lexical semantics.

30 chapter 6

    vector semantics

exercises

agirre, e., banea, c., cardie, c., cer, d., diab, m.,
gonzalez-agirre, a., guo, w., lopez-gazpio, i., maritx-
alar, m., mihalcea, r., rigau, g., uria, l., and wiebe,
j. (2015). 2015 semeval-2015 task 2: semantic textual
similarity, english, spanish and pilot on interpretability.
in semeval-15, pp. 252   263.

agirre, e., diab, m., cer, d., and gonzalez-agirre, a.
(2012). semeval-2012 task 6: a pilot on semantic textual
similarity. in semeval-12, pp. 385   393.

bellegarda, j. r. (1997). a latent semantic analysis frame-
work for large-span id38. in eurospeech-97,
rhodes, greece.

bellegarda, j. r. (2000). exploiting latent semantic infor-
mation in statistical id38. proceedings of
the ieee, 89(8), 1279   1296.

bengio, y., courville, a., and vincent, p. (2013). repre-
sentation learning: a review and new perspectives. ieee
transactions on pattern analysis and machine intelli-
gence, 35(8), 1798   1828.

bengio, y., ducharme, r., vincent, p., and jauvin, c. (2003).
a neural probabilistic language model. journal of machine
learning research, 3(feb), 1137   1155.

bengio, y., schwenk, h., sen  ecal, j.-s., morin, f., and gau-
vain, j.-l. (2006). neural probabilistic language models. in
innovations in machine learning, pp. 137   186. springer.
blei, d. m., ng, a. y., and jordan, m. i. (2003). latent
journal of machine learning re-

dirichlet allocation.
search, 3(5), 993   1022.

bojanowski, p., grave, e., joulin, a., and mikolov, t.
(2017). enriching word vectors with subword information.
tacl, 5, 135   146.

bolukbasi, t., chang, k.-w., zou, j. y., saligrama, v., and
kalai, a. t. (2016). man is to computer programmer as
woman is to homemaker? debiasing id27s. in
nips 16, pp. 4349   4357.

br  eal, m. (1897). essai de s  emantique: science des signi   -

cations. hachette, paris, france.

budanitsky, a. and hirst, g. (2006). evaluating id138-
based measures of lexical semantic relatedness. computa-
tional linguistics, 32(1), 13   47.

bullinaria, j. a. and levy, j. p. (2007). extracting seman-
tic representations from word co-occurrence statistics: a
computational study. behavior research methods, 39(3),
510   526.

bullinaria, j. a. and levy, j. p. (2012). extracting se-
mantic representations from word co-occurrence statistics:
stop-lists, id30, and svd. behavior research methods,
44(3), 890   907.

caliskan, a., bryson, j. j., and narayanan, a. (2017). se-
mantics derived automatically from language corpora con-
tain human-like biases. science, 356(6334), 183   186.

church, k. w. and hanks, p. (1989). word association
norms, mutual information, and id69. in acl-89,
vancouver, b.c., pp. 76   83.

church, k. w. and hanks, p. (1990). word association
norms, mutual information, and id69. computa-
tional linguistics, 16(1), 22   29.

clark, e. (1987). the principle of contrast: a constraint on
in macwhinney, b. (ed.), mecha-

id146.
nisms of id146, pp. 1   33. lea.

exercises

31

coccaro, n. and jurafsky, d. (1998). towards better inte-
gration of semantic predictors in statistical language mod-
eling. in icslp-98, sydney, vol. 6, pp. 2403   2406.

collobert, r. and weston, j. (2007). fast semantic extraction
using a novel neural network architecture. in acl-07, pp.
560   567.

collobert, r. and weston, j. (2008). a uni   ed architec-
ture for natural language processing: deep neural networks
with multitask learning. in icml, pp. 160   167.

collobert, r., weston,

j., bottou, l., karlen, m.,
kavukcuoglu, k., and kuksa, p. (2011). natural language
processing (almost) from scratch. the journal of machine
learning research, 12, 2493   2537.

cruse, d. a. (2004). meaning in language: an introduction
to semantics and pragmatics. oxford university press.
second edition.

dagan, i., marcus, s., and markovitch, s. (1993). contex-
in

tual word similarity and estimation from sparse data.
acl-93, columbus, ohio, pp. 164   171.

davies, m. (2012). expanding horizons in historical linguis-
tics with the 400-million word corpus of historical amer-
ican english. corpora, 7(2), 121   157.

deerwester, s. c., dumais, s. t., furnas, g. w., harshman,
r. a., landauer, t. k., lochbaum, k. e., and streeter, l.
(1988). computer information retrieval using latent seman-
tic structure: us patent 4,839,853..

deerwester, s. c., dumais, s. t., landauer, t. k., furnas,
indexing by latent

g. w., and harshman, r. a. (1990).
semantics analysis. jasis, 41(6), 391   407.

fano, r. m. (1961). transmission of information: a statis-

tical theory of communications. mit press.

finkelstein, l., gabrilovich, e., matias, y., rivlin, e., solan,
z., wolfman, g., and ruppin, e. (2002). placing search in
context: the concept revisited. acm transactions on in-
formation systems, 20(1), 116      131.

firth, j. r. (1957). a synopsis of linguistic theory 1930   
1955. in studies in linguistic analysis. philological soci-
ety. reprinted in palmer, f. (ed.) 1968. selected papers of
j. r. firth. longman, harlow.

fyshe, a., wehbe, l., talukdar, p. p., murphy, b., and
mitchell, t. m. (2015). a compositional and interpretable
semantic space. in naacl hlt 2015.

garg, n., schiebinger, l., jurafsky, d., and zou, j. (2018).
id27s quantify 100 years of gender and eth-
nic stereotypes. proceedings of the national academy of
sciences, 115(16), e3635   e3644.

giuliano, v. e. (1965). the interpretation of word as-
sociations.
in stevens, m. e., giuliano, v. e., and
heilprin, l. b. (eds.), statistical association methods
for mechanized documentation. symposium proceed-
ings. washington, d.c., usa, march 17, 1964, pp. 25   
32. https://nvlpubs.nist.gov/nistpubs/legacy/
mp/nbsmiscellaneouspub269.pdf.

gould, s. j. (1980). the panda   s thumb. penguin group.
greenwald, a. g., mcghee, d. e., and schwartz, j. l. k.
(1998). measuring individual differences in implicit cog-
nition: the implicit association test.. journal of personality
and social psychology, 74(6), 1464   1480.

32 chapter 6     vector semantics

hamilton, w. l., leskovec, j., and jurafsky, d. (2016). di-
achronic id27s reveal statistical laws of seman-
tic change. in acl 2016.

luhn, h. p. (1957). a statistical approach to the mechanized
encoding and searching of literary information. ibm jour-
nal of research and development, 1(4), 309   317.

harris, z. s. (1954). distributional structure. word, 10,
146   162. reprinted in j. fodor and j. katz, the struc-
ture of language, prentice hall, 1964 and in z. s. har-
ris, papers in structural and transformational linguistics,
reidel, 1970, 775   794.

hill, f., reichart, r., and korhonen, a. (2015). siid113x-999:
evaluating semantic models with (genuine) similarity esti-
mation. computational linguistics, 41(4), 665   695.

hjelmslev, l. (1969). prologomena to a theory of lan-
guage. university of wisconsin press. translated by fran-
cis j. whit   eld; original danish edition 1943.

hofmann, t. (1999). probabilistic id45.

in sigir-99, berkeley, ca.

huang, e. h., socher, r., manning, c. d., and ng, a. y.
(2012). improving word representations via global context
and multiple word prototypes. in acl 2012, pp. 873   882.
jones, m. p. and martin, j. h. (1997). contextual spelling
correction using latent semantic analysis. in anlp 1997,
washington, d.c., pp. 166   173.

joos, m. (1950). description of language design. jasa, 22,

701   708.

jurafsky, d. (2014). the language of food. w. w. norton,

new york.

katz, j. j. and fodor, j. a. (1963). the structure of a seman-

tic theory. language, 39, 170   210.

kiela, d. and clark, s. (2014). a systematic study of seman-
tic vector space model parameters. in proceedings of the
eacl 2nd workshop on continuous vector space models
and their compositionality (cvsc), pp. 21   30.

landauer, t. k. and dumais, s. t. (1997). a solution to
plato   s problem: the latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
psychological review, 104, 211   240.

lapesa, g. and evert, s. (2014). a large scale evaluation
of distributional semantic models: parameters, interactions
and model selection. tacl, 2, 531   545.

lee, d. d. and seung, h. s. (1999). learning the parts
of objects by non-negative id105. nature,
401(6755), 788   791.

levy, o. and goldberg, y. (2014a). dependency-based word

embeddings. in acl 2014.

levy, o. and goldberg, y. (2014b). linguistic regularities in

sparse and explicit word representations. in conll-14.

levy, o. and goldberg, y. (2014c). neural id27
in nips 14, pp. 2177   

as implicit id105.
2185.

levy, o., goldberg, y., and dagan, i. (2015). improving dis-
tributional similarity with lessons learned from word em-
beddings. tacl, 3, 211   225.

li, j., chen, x., hovy, e. h., and jurafsky, d. (2015). visual-
izing and understanding neural models in nlp. in naacl
hlt 2015.

lin, y., michel, j.-b., lieberman aiden, e., orwant, j.,
brockman, w., and petrov, s. (2012). syntactic annota-
tions for the google books ngram corpus. in acl 2012, pp.
169   174.

manning, c. d., raghavan, p., and sch  utze, h. (2008). in-

troduction to information retrieval. cambridge.

mikolov, t., chen, k., corrado, g. s., and dean, j.
(2013). ef   cient estimation of word representations in vec-
tor space. in iclr 2013.
mikolov, t., kombrink, s., burget, l.,   cernock`y, j. h., and
khudanpur, s. (2011). extensions of recurrent neural net-
work language model. in icassp-11, pp. 5528   5531.

mikolov, t., sutskever, i., chen, k., corrado, g. s., and
dean, j. (2013a). distributed representations of words and
phrases and their compositionality. in nips 13, pp. 3111   
3119.

mikolov, t., yih, w.-t., and zweig, g. (2013b). linguistic
in

regularities in continuous space word representations.
naacl hlt 2013, pp. 746   751.

niwa, y. and nitta, y. (1994). co-occurrence vectors from
corpora vs. distance vectors from dictionaries. in acl-94,
pp. 304   309.

nosek, b. a., banaji, m. r., and greenwald, a. g. (2002a).
harvesting implicit group attitudes and beliefs from a
demonstration web site. group dynamics: theory, re-
search, and practice, 6(1), 101.

nosek, b. a., banaji, m. r., and greenwald, a. g. (2002b).
math=male, me=female, therefore math(cid:54)= me. journal of
personality and social psychology, 83(1), 44.

osgood, c. e., suci, g. j., and tannenbaum, p. h. (1957).
the measurement of meaning. university of illinois press.
pennington, j., socher, r., and manning, c. d. (2014).
glove: global vectors for word representation. in emnlp
2014, pp. 1532   1543.

rehder, b., schreiner, m. e., wolfe, m. b. w., laham, d.,
landauer, t. k., and kintsch, w. (1998). using latent
semantic analysis to assess knowledge: some technical
considerations. discourse processes, 25(2-3), 337   354.

rohde, d. l. t., gonnerman, l. m., and plaut, d. c. (2006).
an improved model of semantic similarity based on lexical
co-occurrence. communications of the acm, 8, 627   633.
salton, g. (1971). the smart retrieval system: experi-

ments in automatic document processing. prentice hall.

schone, p. and jurafsky, d. (2000). knowlege-free induction
of morphology using latent semantic analysis. in conll-
00.

schone, p. and jurafsky, d. (2001). knowledge-free induc-

tion of in   ectional morphologies. in naacl 2001.

sch  utze, h. (1992). dimensions of meaning. in proceedings

of supercomputing    92, pp. 787   796. ieee press.

sch  utze, h. (1997). ambiguity resolution in language
learning     computational and cognitive models. csli,
stanford, ca.

sch  utze, h., hull, d. a., and pedersen, j. (1995). a com-
parison of classi   ers and id194s for the
routing problem. in sigir-95, pp. 229   237.

sch  utze, h. and pedersen, j. (1993). a vector model for syn-
tagmatic and paradigmatic relatedness. in proceedings of
the 9th annual conference of the uw centre for the new
oed and text research, pp. 104   113.

sparck jones, k. (1972). a statistical interpretation of term
speci   city and its application in retrieval. journal of doc-
umentation, 28(1), 11   21.

sparck jones, k. (1986). synonymy and semantic classi   -
cation. edinburgh university press, edinburgh. republi-
cation of 1964 phd thesis.

exercises

33

switzer, p.

vector

images in document

re-
(1965).
in stevens, m. e., giuliano, v. e., and
trieval.
(eds.), statistical association meth-
heilprin, l. b.
ods for mechanized documentation. symposium pro-
ceedings. washington, d.c., usa, march 17, 1964,
pp. 163   171. https://nvlpubs.nist.gov/nistpubs/
legacy/mp/nbsmiscellaneouspub269.pdf.

turian, j., ratinov, l., and bengio, y. (2010). word
representations: a simple and general method for semi-
supervised learning. in acl 2010, pp. 384   394.

van der maaten, l. and hinton, g. e. (2008). visualizing
journal of machine

high-dimensional data using id167.
learning research, 9, 2579   2605.

wierzbicka, a. (1992). semantics, culture, and cognition:
university human concepts in culture-speci   c con   gura-
tions. oxford university press.

wierzbicka, a. (1996). semantics: primes and universals.

oxford university press.
wittgenstein, l. (1953).

philosophical investigations.

(translated by anscombe, g.e.m.). blackwell.

zhao, j., wang, t., yatskar, m., ordonez, v., and chang, k.-
w. (2017). men also like shopping: reducing gender bias
in emnlp
ampli   cation using corpus-level constraints.
2017.

