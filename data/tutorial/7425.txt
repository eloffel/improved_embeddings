speech and language processing. daniel jurafsky & james h. martin.
rights reserved.

draft of september 23, 2018.

copyright c(cid:13) 2018.

all

chapter

7 neural networks and neural

language models

   [m]achines of this character can behave in a very complicated manner when
the number of units is large.   

alan turing (1948)    intelligent machines   , page 6

deep learning
deep

neural networks are an essential computational tool for language processing, and
a very old one. they are called neural because their origins lie in the mcculloch-
pitts neuron (mcculloch and pitts, 1943), a simpli   ed model of the human neuron
as a kind of computing element that could be described in terms of propositional
logic. but the modern use in language processing no longer draws on these early
biological inspirations.

instead, a modern neural network is a network of small computing units, each
of which takes a vector of input values and produces a single output value. in this
chapter we introduce the neural net applied to classi   cation. the architecture we
introduce is called a feed-forward network because the computation proceeds iter-
atively from one layer of units to the next. the use of modern neural nets is often
called deep learning, because modern networks are often deep (have many layers).
neural networks share much of the same mathematics as id28. but
neural networks are a more powerful classi   er than id28, and indeed a
minimal neural network (technically one with a single    hidden layer   ) can be shown
to learn any function.

neural net classi   ers are different from id28 in another way. with
id28, we applied the regression classi   er to many different tasks by
developing many rich kinds of feature templates based on domain knowledge. when
working with neural networks, it is more common to avoid the use of rich hand-
derived features, instead building neural networks that take raw words as inputs
and learn to induce features as part of the process of learning to classify. we saw
examples of this kind of representation learning for embeddings in chapter 6. nets
that are very deep are particularly good at representation learning for that reason
deep neural nets are the right tool for large scale problems that offer suf   cient data
to learn features automatically.

in this chapter we   ll see feedforward networks as classi   ers, and apply them to
the simple task of id38: assigning probabilities to word sequences and
predicting upcoming words. in later chapters we   ll introduce many other aspects of
neural models, such as the recurrent neural network and the encoder-decoder
model.

2 chapter 7

    neural networks and neural language models

7.1 units

(cid:88)

the building block of a neural network is a single computational unit. a unit takes
a set of real valued numbers as input, performs some computation on them, and
produces an output.

at its heart, a neural unit is taking a weighted sum of its inputs, with one addi-
tional term in the sum called a bias term. thus given a set of inputs x1...xn, a unit
has a set of corresponding weights w1...wn and a bias b, so the weighted sum z can
be represented as:

bias term

z = b +

wixi

(7.1)

vector

often it   s more convenient to express this weighted sum using vector notation;
recall from id202 that a vector is, at heart, just a list or array of numbers.
thus we   ll talk about z in terms of a weight vector w, a scalar bias b, and an input
vector x, and we   ll replace the sum with the convenient dot product:

i

z = w   x + b

(7.2)

activation

as de   ned in eq. 7.2, z is just a real valued number.
finally, instead of using z, a linear function of x, as the output, neural units
apply a non-linear function f to z. we will refer to the output of this function as
the activation value for the unit, a. since we are just modeling a single unit, the
activation for the node is in fact the    nal output of the network, which we   ll generally
call y. so the value y is de   ned as:

y = a = f (z)

(7.3)

we   ll discuss three popular non-linear functions f () below (the sigmoid, the
tanh, and the recti   ed linear relu) but it   s pedagogically convenient to start with
the sigmoid function since we saw it in chapter 5:

sigmoid

y =    (z) =

1

1 + e   z

(7.4)

the sigmoid (shown in fig. 7.1) has a number of advantages; it maps the output
into the range [0,1], which is useful in squashing outliers toward 0 or 1. and it   s
differentiable, which as we saw in section ?? will be handy for learning.

substituting the sigmoid equation into eq. 7.2 gives us the    nal value for the

output of a neural unit:

y =    (w   x + b) =

1

1 + exp(   (w   x + b))

(7.5)

fig. 7.2 shows a    nal schematic of a basic neural unit. in this example the unit
takes 3 input values x1,x2, and x3, and computes a weighted sum, multiplying each
value by a weight (w1, w2, and w3, respectively), adds them to a bias term b, and then
passes the resulting sum through a sigmoid function to result in a number between 0
and 1.

let   s walk through an example just to get an intuition. let   s suppose we have a

unit with the following weight vector and bias:

7.1

    units

3

figure 7.1 the sigmoid function takes a real value and maps it to the range [0,1]. because
it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier
values toward 0 or 1.

figure 7.2 a neural unit, taking 3 inputs x1, x2, and x3 (and a bias b that we represent as a
weight for an input clamped at +1) and producing an output y. we include some convenient
intermediate variables: the output of the summation, z, and the output of the sigmoid, a. in
this case the output of the unit y is the same as a, but in deeper networks we   ll reserve y to
mean the    nal output of the entire network, leaving a as the activation of an individual node.

w = [0.2,0.3,0.9]
b = 0.5

what would this unit do with the following input vector:

x = [0.5,0.6,0.1]

the resulting output y would be:
y =    (w   x + b) =

1

=

1 + e   (w  x+b)

1

1 + e   (.5   .2+.6   .3+.1   .9+.5)

= e   0.87 = .70

tanh

in practice, the sigmoid is not commonly used as an activation function. a
function that is very similar but almost always better is the tanh function shown
in fig. 7.3a; tanh is a variant of the sigmoid that ranges from -1 to +1:

y =

ez     e   z
ez + e   z

(7.6)

the simplest activation function, and perhaps the most commonly used, is the
recti   ed linear unit, also called the relu, shown in fig. 7.3b. it   s just the same as x

relu

x1x2x3yw1w2w3   b  +1za4 chapter 7

    neural networks and neural language models

when x is positive, and 0 otherwise:

y = max(x,0)

(7.7)

figure 7.3 the tanh and relu id180.

(a)

(b)

saturated

these id180 have different properties that make them useful for
different language applications or network architectures. for example the recti   er
function has nice properties that result from it being very close to linear. in the sig-
moid or tanh functions, very high values of z result in values of y that are saturated,
i.e., extremely close to 1, which causes problems for learning. recti   ers don   t have
this problem, since the output of values close to 1 also approaches 1 in a nice gentle
linear way. by contrast, the tanh function has the nice properties of being smoothly
differentiable and mapping outlier values toward the mean.

7.2 the xor problem

early in the history of neural networks it was realized that the power of neural net-
works, as with the real neurons that inspired them, comes from combining these
units into larger networks.

one of the most clever demonstrations of the need for multi-layer networks was
the proof by minsky and papert (1969) that a single neural unit cannot compute
some very simple functions of its input. consider the very simple task of computing
simple logical functions of two inputs, like and, or, and xor. as a reminder,
here are the truth tables for those functions:

and

x1 x2 y
0
0
0
0
0
1
1
1

0
1
0
1

or

x1 x2 y
0
0
1
0
1
1
1
1

0
1
0
1

xor

x1 x2 y
0
0
1
0
1
1
1
0

0
1
0
1

id88

this example was    rst shown for the id88, which is a very simple neural
unit that has a binary output and no non-linear activation function. the output y of

7.2

    the xor problem 5

a id88 is 0 or 1, and just computed as follows (using the same weight w, input
x, and bias b as in eq. 7.2):

(cid:26) 0,

1,

y =

if w   x + b     0
if w   x + b > 0

(7.8)

it   s very easy to build a id88 that can compute the logical and and or

functions of its binary inputs; fig. 7.4 shows the necessary weights.

(a)

(b)

figure 7.4 the weights w and bias b for id88s for computing logical functions. the
inputs are shown as x1 and x2 and the bias as a special node with value +1 which is multiplied
with the bias weight b. (a) logical and, showing weights w1 = 1 and w2 = 1 and bias weight
b =    1. (b) logical or, showing weights w1 = 1 and w2 = 1 and bias weight b = 0. these
weights/biases are just one from an in   nite number of possible sets of weights and biases that
would implement the functions.

it turns out, however, that it   s not possible to build a id88 to compute

logical xor! (it   s worth spending a moment to give it a try!)

the intuition behind this important result relies on understanding that a percep-
tron is a linear classi   er. for a two-dimensional input x0 and x1, the perception
equation, w1x1 + w2x2 + b = 0 is the equation of a line (we can see this by putting
it in the standard linear format: x2 =    (w1/w2)x1     b.) this line acts as a decision
boundary in two-dimensional space in which the output 0 is assigned to all inputs
lying on one side of the line, and the output 1 to all input points lying on the other
side of the line. if we had more than 2 inputs, the decision boundary becomes a
hyperplane instead of a line, but the idea is the same, separating the space into two
categories.

fig. 7.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn
by one possible set of parameters for an and and an or classi   er. notice that there
is simply no way to draw a line that separates the positive cases of xor (01 and 10)
from the negative cases (00 and 11). we say that xor is not a linearly separable
function. of course we could draw a boundary with a curve, or some other function,
but not a single line.

7.2.1 the solution: neural networks
while the xor function cannot be calculated by a single id88, it can be cal-
culated by a layered network of units. let   s see an example of how to do this from
goodfellow et al. (2016) that computes xor using two layers of relu-based units.
fig. 7.6 shows a    gure with the input being processed by two layers of neural units.
the middle layer (called h) has two units, and the output layer (called y) has one
unit. a set of weights and biases are shown for each relu that correctly computes
the xor function

let   s walk through what happens with the input x = [0 0]. if we multiply each
input value by the appropriate weight, sum, and then add the bias b, we get the vector

decision
boundary

linearly
separable

x1x2+1-111x1x2+10116 chapter 7

    neural networks and neural language models

figure 7.5 the functions and, or, and xor, represented with input x0 on the x-axis and input x1 on the
y axis, filled circles represent id88 outputs of 1, and white circles id88 outputs of 0. there is no
way to draw a line that correctly separates the two categories for xor. figure styled after russell and norvig
(2002).

figure 7.6 xor solution after goodfellow et al. (2016). there are three relu units, in
two layers; we   ve called them h1, h2 (h for    hidden layer   ) and y1. as before, the numbers
on the arrows represent the weights w for each unit, and we represent the bias b as a weight
on a unit clamped to +1, with the bias weights/units in gray.

[0 -1], and we then we apply the recti   ed linear transformation to give the output
of the h layer as [0 0]. now we once again multiply by the weights, sum, and add
the bias (0 in this case) resulting in the value 0. the reader should work through the
computation of the remaining 3 possible input pairs to see that the resulting y values
correctly are 1 for the inputs [0 1] and [1 0] and 0 for [0 0] and [1 1].

it   s also instructive to look at the intermediate results, the outputs of the two
hidden nodes h0 and h1. we showed in the previous paragraph that the h vector for
the inputs x = [0 0] was [0 0]. fig. 7.7b shows the values of the h layer for all 4
inputs. notice that hidden representations of the two input points x = [0 1] and x
= [1 0] (the two cases with xor output = 1) are merged to the single point h = [1
0]. the merger makes it easy to linearly separate the positive and negative cases
of xor. in other words, we can view the hidden layer of the network is forming a
representation for the input.

in this example we just stipulated the weights in fig. 7.6. but for real exam-
ples the weights for neural networks are learned automatically using the error back-
propagation algorithm to be introduced in section 7.4. that means the hidden layers
will learn to form useful representations. this intuition, that neural networks can au-
tomatically learn useful representations of the input, is one of their key advantages,

0011x1x20011x1x20011x1x2a)  x1 and x2b)  x1 or x2c)  x1 xor x2?x1x2h1h2y1+11-1111-201+107.3

    feed-forward neural networks

7

figure 7.7 the hidden layer forming a new representation of the input. here is the rep-
resentation of the hidden layer, h, compared to the original input representation x. notice
that the input point [0 1] has been collapsed with the input point [1 0], making it possible to
linearly separate the positive and negative cases of xor. after goodfellow et al. (2016).

and one that we will return to again and again in later chapters.

note that the solution to the xor problem requires a network of units with non-
linear id180. a network made up of simple linear (id88) units
cannot solve the xor problem. this is because a network formed by many layers
of purely linear units can always be reduced (shown to be computationally identical
to) a single layer of linear units with appropriate weights, and we   ve already shown
(visually, in fig. 7.5) that a single unit cannot solve the xor problem.

7.3 feed-forward neural networks

feed-forward
network

multi-layer
id88s
mlp

hidden layer

fully-connected

let   s now walk through a slightly more formal presentation of the simplest kind of
neural network, the feed-forward network. a feed-forward network is a multilayer
network in which the units are connected with no cycles; the outputs from units in
each layer are passed to units in the next higher layer, and no outputs are passed
back to lower layers. (in chapter 9 we   ll introduce networks with cycles, called
recurrent neural networks.)

for historical reasons multilayer networks, especially feedforward networks, are
sometimes called multi-layer id88s (or mlps); this is a technical misnomer,
since the units in modern multilayer networks aren   t id88s (id88s are
purely linear, but modern networks are made up of units with non-linearities like
sigmoids), but at some point the name stuck.

simple feed-forward networks have three kinds of nodes: input units, hidden

units, and output units. fig. 7.8 shows a picture.

the input units are simply scalar values just as we saw in fig. 7.2.
the core of the neural network is the hidden layer formed of hidden units,
each of which is a neural unit as described in section 7.1, taking a weighted sum of
its inputs and then applying a non-linearity. in the standard architecture, each layer
is fully-connected, meaning that each unit in each layer takes as input the outputs
from all the units in the previous layer, and there is a link between every pair of units
from two adjacent layers. thus each hidden unit sums over all the input units.

0011x0x1a) the original x space0011h0h12b) the new h space8 chapter 7

    neural networks and neural language models

figure 7.8 a simple 2-layer feed-forward network, with one hidden layer, one output layer,
and one input layer (the input layer is usually not counted when enumerating layers).

recall that a single hidden unit has parameters w (the weight vector) and b (the
bias scalar). we represent the parameters for the entire hidden layer by combining
the weight vector wi and bias bi for each unit i into a single weight matrix w and
a single bias vector b for the whole layer (see fig. 7.8). each element wi j of the
weight matrix w represents the weight of the connection from the ith input unit xi to
the the jth hidden unit h j.

the advantage of using a single matrix w for the weights of the entire layer is
that now that hidden layer computation for a feedforward network can be done very
ef   ciently with simple matrix operations. in fact, the computation only has three
steps: multiplying the weight matrix by the input vector x, adding the bias vector b,
and applying the activation function g (such as the sigmoid, tanh, or relu activation
function de   ned above).

the output of the hidden layer, the vector h, is thus the following, using the

sigmoid function   :

h =    (w x + b)

(7.9)

notice that we   re applying the    function here to a vector, while in eq. 7.4 it was
applied to a scalar. we   re thus allowing    (  ), and indeed any activation function
g(  ), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].

let   s introduce some constants to represent the dimensionalities of these vectors
and matrices. we   ll refer to the input layer as layer 0 of the network, and use have
n0 represent the number of inputs, so x is a vector of real numbers of dimension
n0, or more formally x     rn0. let   s call the hidden layer layer 1 and the output
layer layer 2. the hidden layer has dimensionality n1, so h     rn1 and also b     rn1
(since each hidden unit can take a different bias value). and the weight matrix w
has dimensionality w     rn1  n0.

take a moment to convince yourself that the id127 in eq. 7.9 will

compute the value of each hi j as(cid:80)nx

as we saw in section 7.2, the resulting value h (for hidden but also for hypoth-
esis) forms a representation of the input. the role of the output layer is to take
this new representation h and compute a    nal output. this output could be a real-
valued number, but in many cases the goal of the network is to make some sort of
classi   cation decision, and so we will focus on the case of classi   cation.

i=1 wi jxi + b j.

if we are doing a binary task like sentiment classi   cation, we might have a single

x1x2h1h2y1xn0   h3hn1   +1b   uwy2yn2normalizing

softmax

7.3

    feed-forward neural networks

9

output node, and its value y is the id203 of positive versus negative sentiment.
if we are doing multinomial classi   cation, such as assigning a part-of-speech tag, we
might have one output node for each potential part-of-speech, whose output value
is the id203 of that part-of-speech, and the values of all the output nodes must
sum to one. the output layer thus gives a id203 distribution across the output
nodes.

let   s see how this happens. like the hidden layer, the output layer has a weight
matrix (let   s call it u), but output layers may not t have a bias vector b, so we   ll sim-
plify by eliminating the bias vector in this example. the weight matrix is multiplied
by its input vector (h) to produce the intermediate output z.

z = uh

there are n2 output nodes, so z     rn2, weight matrix u has dimensionality u    
rn2  n1, and element ui j is the weight from unit j in the hidden layer to unit i in the
output layer.

however, z can   t be the output of the classi   er, since it   s a vector of real-valued
numbers, while what we need for classi   cation is a vector of probabilities. there is
a convenient function for normalizing a vector of real values, by which we mean
converting it to a vector that encodes a id203 distribution (all the numbers lie
between 0 and 1 and sum to 1): the softmax function that we saw on page ?? of
chapter 5. for a vector z of dimensionality d, the softmax is de   ned as:

softmax(zi) =

1     i     d

(7.10)

ezi(cid:80)d

j=1 ez j

thus for example given a vector z=[0.6 1.1 -1.5 1.2 3.2 -1.1], softmax(z) is [ 0.055
0.090 0.0067 0.10 0.74 0.010].

you may recall that softmax was exactly what is used to create a id203
distribution from a vector of real-valued numbers (computed from summing weights
times features) in id28 in chapter 5.

that means we can think of a neural network classi   er with one hidden layer
as building a vector h which is a hidden layer representation of the input, and then
running standard id28 on the features that the network develops in h.
by contrast, in chapter 5 the features were mainly designed by hand via feature
templates. so a neural network is like id28, but (a) with many layers,
since a deep neural network is like layer after layer of id28 classi   ers,
and (b) rather than forming the features by feature templates, the prior layers of the
network induce the feature representations themselves.

here are the    nal equations for a feed-forward network with a single hidden
layer, which takes an input vector x, outputs a id203 distribution y, and is pa-
rameterized by weight matrices w and u and a bias vector b:

h =    (w x + b)
z = uh
y = softmax(z)

(7.11)

we   ll call this network a 2-layer network (we traditionally don   t count the input
layer when numbering layers, but do count the output layer). so by this terminology
id28 is a 1-layer network.

let   s now set up some notation to make it easier to talk about deeper networks
of depth more than 2. we   ll use superscripts in square brackets to mean layer num-
bers, starting at 0 for the input layer. so w [1] will mean the weight matrix for the

10 chapter 7

    neural networks and neural language models

(   rst) hidden layer, and b[1] will mean the bias vector for the (   rst) hidden layer. n j
will mean the number of units at layer j. we   ll use g(  ) to stand for the activation
function, which will tend to be relu or tanh for intermediate layers and softmax
for output layers. we   ll use a[i] to mean the output from layer i, and z[i] to mean the
combination of weights and biases w [i]a[i   1] +b[i]. the 0th layer is for inputs, so the
inputs x we   ll refer to more generally as a[0].

thus we   ll represent a 3-layer net as follows:

z[1] = w [1]a[0] + b[1]
a[1] = g[1](z[1])
z[2] = w [2]a[1] + b[2]
a[2] = g[2](z[2])

  y = a[2]

(7.12)

note that with this notation, the equations for the computation done at each layer are
the same. the algorithm for computing the forward step in an n-layer feed-forward
network, given the input vector a[0] is thus simply:

for i in 1..n

z[i] = w [i] a[i   1] + b[i]
a[i] = g[i](z[i])

  y = a[n]

the

id180 g(  ) are generally different at the    nal layer. thus g[2] might
be softmax for multinomial classi   cation or sigmoid for binary classi   cation, while
relu or tanh might be the activation function g() at the internal layers.

7.4 training neural nets

a feedforward neural net is an instance of supervised machine learning in which we
know the correct output y for each observation x. what the system produces, via
eq. 7.12, is   y, the system   s estimate of the true y. the goal of the training procedure
is to learn parameters w [i] and b[i] for each layer i that make   y for each training
observation as close as possible to the true y .

in general, we do all this by drawing on the methods we introduced in chapter 5
for id28, so the reader should be comfortable with that chapter before
proceeding.

first, we   ll need a id168 that models the distance between the system
output and the gold output, and it   s common to use the loss used for logistic regres-
sion, the cross-id178 loss.

second, to    nd the parameters that minimize this id168, we   ll use the
id119 optimization algorithm introduced in chapter 5. there are some
differences

third, id119 requires knowing the gradient of the id168, the
vector that contains the partial derivative of the id168 with respect to each of
the parameters. here is one part where learning for neural networks is more complex
than for logistic id28. in id28, for each observation we
could directly compute the derivative of the id168 with respect to an individ-
ual w or b. but for neural networks, with millions of parameters in many layers, it   s

7.4

    training neural nets

11

much harder to see how to compute the partial derivative of some weight in layer 1
when the loss is attached to some much later layer. how do we partial out the loss
over all those intermediate layers?

the answer is the algorithm called error back-propagation or reverse differ-

entiation.

cross id178
loss

7.4.1 id168
the cross id178 loss, that is used in neural networks is the same one we saw for
id28.

in fact, if the neural network is being used as a binary classi   er, with the sig-
moid at the    nal layer, the id168 is exactly the same as we saw with logistic
regression in eq. ??:

lce (   y,y) =    log p(y|x) =     [ylog   y + (1    y)log(1      y)]

(7.13)

what about if the neural network is being used as a multinomial classi   er? let
y be a vector over the c classes representing the true output id203 distribution.
the cross id178 loss here is

lce (   y,y) =    

yi log   yi

(7.14)

i=1

we can simplify this equation further. assume this is a hard classi   cation task,
meaning that only one class is the correct one, and that there is one output unit in y
for each class. if the true class is i, then y is a vector where yi = 1 and y j = 0     j (cid:54)= i.
a vector like this, with one value=1 and the rest 0, is called a one-hot vector. now
let   y be the vector output from the network. the sum in eq. 7.14 will be 0 except
for the true class. hence the cross-id178 loss is simply the log id203 of the
correct class, and we therefore also call this the negative log likelihood loss:

lce (   y,y) =    log   yi

(7.15)

negative log
likelihood loss

c(cid:88)

plugging in the softmax formula from eq. 7.10, and with k the number of classes:

lce (   y,y) =    log

ezi(cid:80)k

j   1 ez j

(7.16)

7.4.2 computing the gradient
how do we compute the gradient of this id168? computing the gradient
requires the partial derivative of the id168 with respect to each parameter.
for a network with one weight layer and sigmoid output (which is what logistic
regression is), we could simply use the derivative of the loss that we used for logistic
regression in: eq. 7.17 (and derived in section ??):

    lce (w,b)

    w j

= (   y    y) x j
= (   (w   x + b)    y) x j

(7.17)

12 chapter 7

    neural networks and neural language models

or for a network with one hidden layer and softmax output, we could use the deriva-
tive of the softmax loss from eq. ??:

    lce
    wk

(cid:32)
= (1{y = k}    p(y = k|x))xk
1{y = k}    ewk  x+bk

(cid:80)k
j=1 ew j  x+b j

=

(cid:33)

xk

(7.18)

error back-
propagation

but these derivatives only give correct updates for one weight layer: the last one!
for deep networks, computing the gradients for each weight is much more complex,
since we are computing the derivative with respect to weight parameters that appear
all the way back in the very early layers of the network, even though the loss is
computed only at the very end of the network.

the solution to computing this gradient is an algorithm called error backprop-
agation or backprop (rumelhart et al., 1986). while backprop was invented spe-
cially for neural networks, it turns out to be the same as a more general procedure
called backward differentiation, which depends on the notion of computation
graphs. let   s see how that works in the next subsection.

7.4.3 computation graphs
a computation graph is a representation of the process of computing a mathematical
expression, in which the computation is broken down into separate operations, each
of which is modeled as a node in a graph.

consider computing the function l(a,b,c) = c(a + 2b). if we make each of the
component addition and multiplication operations explicit, and add names (d and e)
for the intermediate outputs, the resulting series of computations is:

d = 2    b
e = a + d
l = c    e

we can now represent this as a graph, with nodes for each operation, and di-
rected edges showing the outputs from each operation as the inputs to the next, as
in fig. 7.9. the simplest use of computation graphs is to compute the value of the
function with some given inputs. in the    gure, we   ve assumed the inputs a = 3,
b = 1, c =    1, and we   ve shown the result of the forward pass to compute the re-
sult l(3,1,   1) = 10. in the forward pass of a computation graph, we apply each
operation left to right, passing the outputs of each computation as the input to the
next node.

7.4.4 backward differentiation on computation graphs
the importance of the computation graph comes from the backward pass, which
is used to compute the derivatives that we   ll need for the weight update. in this
example our goal is to compute the derivative of the output function l with respect
to each of the input variables, i.e.,     l
    a , tells us how
much a small change in a affects l.

    c . the derivative     l

    b , and     l

    a ,     l

backwards differentiation makes use of the chain rule in calculus. suppose we
are computing the derivative of a composite function f (x) = u(v(x)). the derivative

chain rule

7.4

    training neural nets

13

figure 7.9 computation graph for the function l(a,b,c) = c(a + 2b), with values for input
nodes a = 3, b = 1, c =    1, showing the forward pass computation of l.

of f (x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with
respect to x:

d f
dx =

du
dv

   dv
dx

(7.19)

the chain rule extends to more than two functions. if computing the derivative of a
composite function f (x) = u(v(w(x))), the derivative of f (x) is:

d f
dx =

du
dv

   dv
dw

   dw
dx

(7.20)

let   s now compute the 3 derivatives we need. since in the computation graph

l = ce, we can directly compute the derivative     l
    c :

    l
    c = e

for the other two, we   ll need to use the chain rule:

    l
    a =
    l
    b =

    l
    e
    l
    e

    e
    a
    e
    d

    d
    b

(7.21)

(7.22)

eq. 7.22 thus requires four intermediate derivatives:     l

    b , which
are as follows (making use of the fact that the derivative of a sum is the sum of the
derivatives):

    d , and     d

    a,     e

    e ,     e

l = ce :

e = a + d :

d = 2b :

    l
    c = e
    e
    d = 1

    l
    e = c,
    e
    a = 1,
    d
    b = 2

(7.23)

in the backward pass, we compute each of these partials along each edge of
the graph from right to left, multiplying the necessary partials to result in the    nal
derivative we need. thus we begin by annotating the    nal node with     l
    l = 1. moving
to the left, we then compute     l
    e , and so on, until we have annotated the graph

    c and     l

e=d+ad = 2bl=ce31-2e=5d=2l=-10forward passabc14 chapter 7

    neural networks and neural language models

all the way to the input variables. the forward pass conveniently already will have
computed the values of the forward intermediate variables we need (like d and e)
to compute these derivatives. fig. 7.10 shows the backward pass. at each node we
need to compute the local partial derivative with respect to the parent, multiply it by
the partial derivative that is being passed down from the parent, and then pass it to
the child.

figure 7.10 computation graph for the function l(a,b,c) = c(a + 2b), showing the back-
ward pass computation of     l

    a ,     l

    b , and     l
    c .

of course computation graphs for real neural networks are much more complex.
fig. 7.11 shows a sample computation graph for a 2-layer neural network with n0 =
2, n1 = 2, and n2 = 1, assuming binary classi   cation and hence using a sigmoid
output unit for simplicity. the weights that need updating (those for which we need
to know the partial derivative of the id168) are shown in orange.

figure 7.11 sample computation graph for a simple 2-layer neural net (= 1 hidden layer)
with two input dimensions and 2 hidden dimensions.

in order to do the backward pass, we   ll need to know the derivatives of all the
functions in the graph. we already saw in section ?? the derivative of the sigmoid
  :

d   (z)
dz =    (z)(1    z)

(7.24)

we   ll also need the derivatives of each of the other id180. the

derivative of tanh is:

d tanh(z)

dz

= 1    tanh2(z)

(7.25)

e=d+ad = 2bl=cea=3b=1e=5d=2l=-10    l=1   l   l=-4   b   l=-2   dabc   a=-2   b   l=5   c   l=-2   e   l=-2   e   e=1   d   l=5   c   d=2   b   e=1   abackward passc=-2z = +a[2] =    a[1] = reluz[1] = +b[1]****x1x2a[1] = reluz[1] = +b[1]**w[2]11w[1]11w[1]21w[1]12w[1]22b[1]w[2]21l (a[2],y)7.5

    neural language models

15

the derivative of the relu is

d relu(z)

dz

(cid:26) 0 f or x < 0

1 f or x     0

=

(7.26)

7.4.5 more details on learning
optimization in neural networks is a non-id76 problem, more com-
plex than for id28, and for that and other reasons there are many best
practices for successful learning.

for id28 we can initialize id119 with all the weights and
biases having the value 0. in neural networks, by contrast, we need to initialize the
weights with small random numbers. it   s also helpful to normalize the input values
to have 0 mean and unit variance.

various forms of id173 are used to prevent over   tting. one of the most
important is dropout: randomly dropping some units and their connections from the
network during training (hinton et al. 2012, srivastava et al. 2014).

hyperparameter tuning is also important. the parameters of a neural network
are the weights w and biases b; those are learned by id119. the hyperpa-
rameters are things that are set by the algorithm designer and not learned in the same
way, although they must be tuned. hyperparameters include the learning rate   , the
minibatch size, the model architecture (the number of layers, the number of hidden
nodes per layer, the choice of id180), how to regularize, and so on.
id119 itself also has many architectural variants such as adam (kingma
and ba, 2015).

finally, most modern neural networks are built using computation graph for-
malisms that make all the work of gradient computation and parallelization onto
vector-based gpus (graphic processing units) very easy and natural. pytorch (paszke
et al., 2017) and tensorflow (abadi et al., 2015) are two of the most popular. the
interested reader should consult a neural network textbook for further details; some
suggestions are at the end of the chapter.

dropout

hyperparameter

7.5 neural language models

as our    rst application of neural networks, let   s consider id38: pre-
dicting upcoming words from prior word context.

neural net-based language models turn out to have many advantages over the n-
gram language models of chapter 3. among these are that neural language models
don   t need smoothing, they can handle much longer histories, and they can general-
ize over contexts of similar words. for a training set of a given size, a neural lan-
guage model has much higher predictive accuracy than an id165 language model
furthermore, neural language models underlie many of the models we   ll introduce
for tasks like machine translation, dialog, and language generation.

on the other hand, there is a cost for this improved performance: neural net
language models are strikingly slower to train than traditional language models, and
so for many tasks an id165 language model is still the right tool.

in this chapter we   ll describe simple feedforward neural language models,    rst
introduced by bengio et al. (2003). modern neural language models are generally
not feedforward but recurrent, using the technology that we will introduce in chap-
ter 9.

16 chapter 7

    neural networks and neural language models

a feedforward neural lm is a standard feedforward network that takes as input
at time t a representation of some number of previous words (wt   1,wt   2, etc) and
outputs a id203 distribution over possible next words. thus   like the id165
lm   the feedforward neural lm approximates the id203 of a word given the
entire prior context p(wt|wt   1

) by approximating based on the n previous words:

1

(7.27)
in the following examples we   ll use a 4-gram example, so we   ll show a net to

t   n+1)

1

p(wt|wt   1

)     p(wt|wt   1

estimate the id203 p(wt = i|wt   1,wt   2,wt   3).

7.5.1 embeddings
in neural language models, the prior context is represented by embeddings of the
previous words. representing the prior context as embeddings, rather than by ex-
act words as used in id165 language models, allows neural language models to
generalize to unseen data much better than id165 language models. for example,
suppose we   ve seen this sentence in training:

i have to make sure when i get home to feed the cat.

but we   ve never seen the word    dog    after the words    feed the   . in our test set we
are trying to predict what comes after the pre   x    i forgot when i got home to feed
the   .

an id165 language model will predict    cat   , but not    dog   . but a neural lm,
which can make use of the fact that    cat    and    dog    have similar embeddings, will
be able to assign a reasonably high id203 to    dog    as well as    cat   , merely
because they have similar vectors.

let   s see how this works in practice. let   s assume we have an embedding dic-
tionary e that gives us, for each word in our vocabulary v , the embedding for that
word, perhaps precomputed by an algorithm like id97 from chapter 6.

fig. 7.12 shows a sketch of this simpli   ed ffnnlm with n=3; we have a mov-
ing window at time t with an embedding vector representing each of the 3 previous
words (words wt   1, wt   2, and wt   3). these 3 vectors are concatenated together to
produce x, the input layer of a neural network whose output is a softmax with a
id203 distribution over words. thus y42, the value of output node 42 is the
id203 of the next word wt being v42, the vocabulary word with index 42.

the model shown in fig. 7.12 is quite suf   cient, assuming we learn the embed-
dings separately by a method like the id97 methods of chapter 6. the method
of using another algorithm to learn the embedding representations we use for input
words is called pretraining. if those pretrained embeddings are suf   cient for your
purposes, then this is all you need.

however, often we   d like to learn the embeddings simultaneously with training
the network. this is true when whatever task the network is designed for (sentiment
classi   cation, or translation, or parsing) places strong constraints on what makes a
good representation.

let   s therefore show an architecture that allows the embeddings to be learned.
to do this, we   ll add an extra layer to the network, and propagate the error all the
way back to the embedding vectors, starting with embeddings with random values
and slowly moving toward sensible representations.
for this to work at the input layer, instead of pre-trained embeddings, we   re
going to represent each of the n previous words as a one-hot vector of length |v|, i.e.,
with one dimension for each word in the vocabulary. a one-hot vector is a vector

pretraining

one-hot vector

7.5

    neural language models

17

figure 7.12 a simpli   ed view of a feedforward neural language model moving through a text. at each
timestep t the network takes the 3 context words, converts each to a d-dimensional embeddings, and concate-
nates the 3 embeddings together to get the 1   nd unit input layer x for the network. these units are multiplied
by a weight matrix w and bias vector b and then an activation function to produce a hidden layer h, which
is then multiplied by another weight matrix u. (for graphic simplicity we don   t show b in this and future
pictures). finally, a softmax output layer predicts at each node i the id203 that the next word wt will be
vocabulary word vi. (this picture is simpli   ed because it assumes we just look up in an embedding dictionary
e the d-dimensional embedding vector for each word, precomputed by an algorithm like id97.)

that has one element equal to 1   in the dimension corresponding to that word   s
index in the vocabulary    while all the other elements are set to zero.
thus in a one-hot representation for the word    toothpaste   , supposing it happens
to have index 5 in the vocabulary, x5 is one and and xi = 0    i (cid:54)= 5, as shown here:

[0 0 0 0 1 0 0 ... 0 0 0 0]
... |v|

1 2 3 4 5 6 7 ...

fig. 7.13 shows the additional layers needed to learn the embeddings during lm
training. here the n=3 context words are represented as 3 one-hot vectors, fully
connected to the embedding layer via 3 instantiations of the e embedding matrix.
note that we don   t want to learn separate weight matrices for mapping each of the 3
previous words to the projection layer, we want one single embedding dictionary e
that   s shared among these three. that   s because over time, many different words will
appear as wt   2 or wt   1, and we   d like to just represent each word with one vector,
whichever context position it appears in. the embedding weight matrix e thus has
a row for each word, each a vector of d dimensions, and hence has dimensionality
v    d.

let   s walk through the forward pass of fig. 7.13.
1. select three embeddings from e: given the three previous words, we look
up their indices, create 3 one-hot vectors, and then multiply each by the em-
bedding matrix e. consider wt   3. the one-hot vector for    the    is (index 35) is
multiplied by the embedding matrix e, to give the    rst part of the    rst hidden
layer, called the projection layer. since each row of the input matrix e is just
an embedding for a word, and the input is a one-hot columnvector xi for word

projection layer

h1h2y1h3hdh      uwy42y|v|projection layer1   3dconcatenated embeddingsfor context wordshidden layeroutput layer p(w|u)   inthehole......groundtherelivedword 42embedding forword 35embedding for word 9925embedding for word 45180wt-1wt-2wtwt-3dh   3d1   dh|v|   dhp(wt=v42|wt-3,wt-2,wt-3)1   |v|18 chapter 7

    neural networks and neural language models

figure 7.13
the 3 context words.

learning all the way back to embeddings. notice that the embedding matrix e is shared among

vi, the projection layer for input w will be exi = ei, the embedding for word i.
we now concatenate the three embeddings for the context words.

2. multiply by w: we now multiply by w (and add b) and pass through the

recti   ed linear (or other) activation function to get the hidden layer h.

3. multiply by u: h is now multiplied by u
4. apply softmax: after the softmax, each node i in the output layer estimates
the id203 p(wt = i|wt   1,wt   2,wt   3)

in summary, if we use e to represent the projection layer, formed by concatenat-
ing the 3 embedding for the three context vectors, the equations for a neural language
model become:

e = (ex1,ex2, ...,ex)
h =    (we + b)
z = uh
y = softmax(z)

(7.28)
(7.29)
(7.30)
(7.31)

7.5.2 training the neural language model
to set all the parameters    = e,w,u,b, we do gradient
to train the model, i.e.
descent (fig. ??), using error back propagation on the computation graph to compute
the gradient. training thus not only sets the weights w and u of the network, but
also as we   re predicting upcoming words, we   re learning the embeddings e for each
words that best predict upcoming words.

h1h2y1h3hdh      uwy42y|v|projection layer1   3dhidden layeroutput layer p(w|context)   inthehole......groundtherelivedword 42wt-1wt-2wtwt-3dh   3d1   dh|v|   dhp(wt=v42|wt-3,wt-2,wt-3)1   |v|input layerone-hot vectorsindexword 35001001|v|35001001|v|45180001001|v|992500index word 9925index word 45180e1   |v|d   |v|e is sharedacross words7.6

    summary

19

generally training proceedings by taking as input a very long text, concatenating
all the sentences, start with random weights, and then iteratively moving through
the text predicting each word wt. at each word wt, the cross-id178 (negative log
likelihood) loss is:

l =    log p(wt|wt   1, ...,wt   n+1)

the gradient is for this loss is then:

  t+1 =   t       

        log p(wt|wt   1, ...,wt   n+1)

     

(7.32)

(7.33)

this gradient can be computed in any standard neural network framework which

will then backpropagate through u, w , b, e.

training the parameters to minimize loss will result both in an algorithm for
id38 (a word predictor) but also a new set of embeddings e that can
be used as word representations for other tasks.

7.6 summary

to each unit in layer i + 1, and there are no cycles.

neurons but now simple an abstract computational device.

    neural networks are built out of neural units, originally inspired by human
    each neural unit multiplies input values by a weight vector, adds a bias, and
then applies a non-linear activation function like sigmoid, tanh, or recti   ed
linear.
    in a fully-connected, feedforward network, each unit in layer i is connected
    the power of neural networks comes from the ability of early layers to learn
    neural networks are trained by optimization algorithms like gradient de-
    error back propagation, backward differentiation on a computation graph,
    neural language models use a neural network as a probabilistic classi   er, to
    neural language models can use pretrained embeddings, or can learn embed-

compute the id203 of the next word given the previous n words.

is used to compute the gradients of the id168 for a network.

representations that can be utilized by later layers in the network.

scent.

dings from scratch in the process of id38.

bibliographical and historical notes

the origins of neural networks lie in the 1940s mcculloch-pitts neuron (mccul-
loch and pitts, 1943), a simpli   ed model of the human neuron as a kind of com-
puting element that could be described in terms of id118. by the late
1950s and early 1960s, a number of labs (including frank rosenblatt at cornell and
bernard widrow at stanford) developed research into neural networks; this phase
saw the development of the id88 (rosenblatt, 1958), and the transformation
of the threshold into a bias, a notation we still use (widrow and hoff, 1960).

20 chapter 7

    neural networks and neural language models

connectionist

the    eld of neural networks declined after it was shown that a single percep-
tron unit was unable to model functions as simple as xor (minsky and papert,
1969). while some small amount of work continued during the next two decades,
a major revival for the    eld didn   t come until the 1980s, when practical tools for
building deeper networks like error back propagation became widespread (rumel-
hart et al., 1986). during the 1980s a wide variety of neural network and related
architectures were developed, particularly for applications in psychology and cog-
nitive science (rumelhart and mcclelland 1986b, mcclelland and elman 1986,
rumelhart and mcclelland 1986a,elman 1990), for which the term connection-
ist or parallel distributed processing was often used (feldman and ballard 1982,
smolensky 1988). many of the principles and techniques developed in this period
are foundational to modern work, including the ideas of distributed representations
(hinton, 1986), recurrent networks (elman, 1990), and the use of tensors for com-
positionality (smolensky, 1990).

by the 1990s larger neural networks began to be applied to many practical lan-
guage processing tasks as well, like handwriting recognition (lecun et al. 1989,
lecun et al. 1990) and id103 (morgan and bourlard 1989, morgan
and bourlard 1990). by the early 2000s, improvements in computer hardware and
advances in optimization and training techniques made it possible to train even larger
and deeper networks, leading to the modern term deep learning (hinton et al. 2006,
bengio et al. 2007). we cover more related history in chapter 9.

there are a number of excellent books on the subject. goldberg (2017) has a
superb and comprehensive coverage of neural networks for natural language pro-
cessing. for neural networks in general see goodfellow et al. (2016) and nielsen
(2015).

bibliographical and historical notes

21

morgan, n. and bourlard, h. (1990). continuous speech
recognition using multilayer id88s with hidden
markov models. in icassp-90, pp. 413   416.

nielsen, m. a. (2015). neural networks and deep learning.

determination press usa.

paszke, a., gross, s., chintala, s., chanan, g., yang, e., de-
vito, z., lin, z., desmaison, a., antiga, l., and lerer, a.
(2017). automatic differentiation in pytorch. in nips-w.
rosenblatt, f. (1958). the id88: a probabilistic model
for information storage and organization in the brain.. psy-
chological review, 65(6), 386   408.

rumelhart, d. e., hinton, g. e., and williams, r. j. (1986).
learning internal representations by error propagation. in
rumelhart, d. e. and mcclelland, j. l. (eds.), parallel
distributed processing, vol. 2, pp. 318   362. mit press.

rumelhart, d. e. and mcclelland, j. l. (1986a). on learn-
ing the past tense of english verbs. in rumelhart, d. e. and
mcclelland, j. l. (eds.), parallel distributed processing,
vol. 2, pp. 216   271. mit press.

rumelhart, d. e. and mcclelland, j. l. (eds.). (1986b). par-

allel distributed processing. mit press.

russell, s. and norvig, p. (2002). arti   cial intelligence: a

modern approach (2nd ed.). prentice hall.

smolensky, p. (1988). on the proper treatment of connec-

tionism. behavioral and brain sciences, 11(1), 1   23.

smolensky, p. (1990). tensor product variable binding and
the representation of symbolic structures in connectionist
systems. arti   cial intelligence, 46(1-2), 159   216.

srivastava, n., hinton, g. e., krizhevsky, a., sutskever, i.,
and salakhutdinov, r. r. (2014). dropout: a simple way
to prevent neural networks from over   tting.. journal of
machine learning research, 15(1), 1929   1958.

widrow, b. and hoff, m. e. (1960). adaptive switching cir-
in ire wescon convention record, vol. 4, pp.

cuits.
96   104.

abadi, m., agarwal, a., barham, p., brevdo, e., chen, z.,
citro, c., corrado, g. s., davis, a., dean, j., devin, m.,
ghemawat, s., goodfellow, i., harp, a., irving, g., isard,
m., jia, y., jozefowicz, r., kaiser, l., kudlur, m., lev-
enberg, j., man  e, d., monga, r., moore, s., murray, d.,
olah, c., schuster, m., shlens, j., steiner, b., sutskever,
i., talwar, k., tucker, p., vanhoucke, v., vasudevan, v.,
vi  egas, f., vinyals, o., warden, p., wattenberg, m., wicke,
m., yu, y., and zheng, x. (2015). tensorflow: large-
scale machine learning on heterogeneous systems.. soft-
ware available from tensor   ow.org.

bengio, y., ducharme, r., vincent, p., and jauvin, c. (2003).
a neural probabilistic language model. journal of machine
learning research, 3(feb), 1137   1155.

bengio, y., lamblin, p., popovici, d., and larochelle, h.
(2007). greedy layer-wise training of deep networks. in
nips 2007, pp. 153   160.

elman, j. l. (1990). finding structure in time. cognitive

science, 14(2), 179   211.

feldman, j. a. and ballard, d. h. (1982). connectionist
models and their properties. cognitive science, 6, 205   
254.

goldberg, y. (2017). neural network methods for natural
language processing, vol. 10 of synthesis lectures on hu-
man language technologies. morgan & claypool.

goodfellow, i., bengio, y., and courville, a. (2016). deep

learning. mit press.

hinton, g. e. (1986). learning distributed representations

of concepts. in cogsci-86, pp. 1   12.

hinton, g. e., osindero, s., and teh, y.-w. (2006). a fast
learning algorithm for deep belief nets. neural computa-
tion, 18(7), 1527   1554.

hinton, g. e., srivastava, n., krizhevsky, a., sutskever,
i., and salakhutdinov, r. r. (2012).
improving neural
networks by preventing co-adaptation of feature detectors.
arxiv preprint arxiv:1207.0580.

kingma, d. and ba, j. (2015). adam: a method for stochas-

tic optimization. in iclr 2015.

lecun, y., boser, b., denker, j. s., henderson, d., howard,
r. e., hubbard, w., and jackel, l. d. (1989). backpropa-
gation applied to handwritten zip code recognition. neural
computation, 1(4), 541   551.

lecun, y., boser, b. e., denker, j. s., henderson, d.,
howard, r. e., hubbard, w. e., and jackel, l. d. (1990).
handwritten digit recognition with a back-propagation net-
work. in nips 1990, pp. 396   404.

mcclelland, j. l. and elman, j. l. (1986). the trace
model of speech perception. cognitive psychology, 18, 1   
86.

mcculloch, w. s. and pitts, w. (1943). a logical calculus of
ideas immanent in nervous activity. bulletin of mathemat-
ical biophysics, 5, 115   133. reprinted in neurocomput-
ing: foundations of research, ed. by j. a. anderson and e
rosenfeld. mit press 1988.

minsky, m. and papert, s. (1969). id88s. mit press.
morgan, n. and bourlard, h. (1989). generalization and pa-
rameter estimation in feedforward nets: some experiments.
in advances in neural information processing systems, pp.
630   637.

