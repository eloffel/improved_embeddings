speech and language processing. daniel jurafsky & james h. martin.
rights reserved.

draft of september 23, 2018.

copyright c(cid:13) 2018.

all

chapter

8 part-of-speech tagging

parts-of-speech

pos

dionysius thrax of alexandria (c. 100 b.c.), or perhaps someone else (it was a long
time ago), wrote a grammatical sketch of greek (a    techn  e   ) that summarized the
linguistic knowledge of his day. this work is the source of an astonishing proportion
of modern linguistic vocabulary, including words like syntax, diphthong, clitic, and
analogy. also included are a description of eight parts-of-speech: noun, verb,
pronoun, preposition, adverb, conjunction, participle, and article. although earlier
scholars (including aristotle as well as the stoics) had their own lists of parts-of-
speech, it was thrax   s set of eight that became the basis for practically all subsequent
part-of-speech descriptions of most european languages for the next 2000 years.

schoolhouse rock was a series of popular animated educational television clips
from the 1970s. its grammar rock sequence included songs about exactly 8 parts-
of-speech, including the late great bob dorough   s conjunction junction:

conjunction junction, what   s your function?
hooking up words and phrases and clauses...

although the list of 8 was slightly modi   ed from thrax   s original, the astonishing
durability of the parts-of-speech through two millenia is an indicator of both the
importance and the transparency of their role in human language.1

parts-of-speech (also known as pos, word classes, or syntactic categories) are
useful because they reveal a lot about a word and its neighbors. knowing whether a
word is a noun or a verb tells us about likely neighboring words (nouns are preceded
by determiners and adjectives, verbs by nouns) and syntactic structure word (nouns
are generally part of noun phrases), making part-of-speech tagging a key aspect
of parsing (chapter 11). parts of speech are useful features for labeling named
entities like people or organizations in information extraction (chapter 17), or for
coreference resolution (chapter 20). a word   s part-of-speech can even play a role
in id103 or synthesis, e.g., the word content is pronounced content
when it is a noun and content when it is an adjective.

this chapter introduces parts-of-speech, and then introduces two algorithms for
part-of-speech tagging, the task of assigning parts-of-speech to words. one is
generative    hidden markov model (id48)   and one is discriminative   the max-
imum id178 markov model (memm). chapter 9 then introduces a third algorithm
based on the recurrent neural network (id56). all three have roughly equal perfor-
mance but, as we   ll see, have different tradeoffs.

8.1

(mostly) english word classes

until now we have been using part-of-speech terms like noun and verb rather
freely. in this section we give a more complete de   nition of these and other classes.
while word classes do have semantic tendencies   adjectives, for example, often

1 nonetheless, eight isn   t very many and, as we   ll see, recent tagsets have more.

2 chapter 8

    part-of-speech tagging

closed class
open class

function word

noun

proper noun

common noun
count noun
mass noun

verb

adjective

adverb

describe properties and nouns people    parts-of-speech are traditionally de   ned in-
stead based on syntactic and morphological function, grouping words that have sim-
ilar neighboring words (their distributional properties) or take similar af   xes (their
morphological properties).

parts-of-speech can be divided into two broad supercategories: closed class
types and open class types. closed classes are those with relatively    xed member-
ship, such as prepositions   new prepositions are rarely coined. by contrast, nouns
and verbs are open classes   new nouns and verbs like iphone or to fax are contin-
ually being created or borrowed. any given speaker or corpus may have different
open class words, but all speakers of a language, and suf   ciently large corpora,
likely share the set of closed class words. closed class words are generally function
words like of, it, and, or you, which tend to be very short, occur frequently, and
often have structuring uses in grammar.

four major open classes occur in the languages of the world: nouns, verbs,
adjectives, and adverbs. english has all four, although not every language does.
the syntactic class noun includes the words for most people, places, or things, but
others as well. nouns include concrete terms like ship and chair, abstractions like
bandwidth and relationship, and verb-like terms like pacing as in his pacing to and
fro became quite annoying. what de   nes a noun in english, then, are things like its
ability to occur with determiners (a goat, its bandwidth, plato   s republic), to take
possessives (ibm   s annual revenue), and for most but not all nouns to occur in the
plural form (goats, abaci).

open class nouns fall into two classes. proper nouns, like regina, colorado,
and ibm, are names of speci   c persons or entities. in english, they generally aren   t
preceded by articles (e.g., the book is upstairs, but regina is upstairs). in written
english, proper nouns are usually capitalized. the other class, common nouns, are
divided in many languages, including english, into count nouns and mass nouns.
count nouns allow grammatical enumeration, occurring in both the singular and plu-
ral (goat/goats, relationship/relationships) and they can be counted (one goat, two
goats). mass nouns are used when something is conceptualized as a homogeneous
group. so words like snow, salt, and communism are not counted (i.e., *two snows
or *two communisms). mass nouns can also appear without articles where singular
count nouns cannot (snow is white but not *goat is white).

verbs refer to actions and processes, including main verbs like draw, provide,
and go. english verbs have in   ections (non-third-person-sg (eat), third-person-sg
(eats), progressive (eating), past participle (eaten)). while many researchers believe
that all human languages have the categories of noun and verb, others have argued
that some languages, such as riau indonesian and tongan, don   t even make this
distinction (broschart 1997; evans 2000; gil 2000) .

the third open class english form is adjectives, a class that includes many terms
for properties or qualities. most languages have adjectives for the concepts of color
(white, black), age (old, young), and value (good, bad), but there are languages
without adjectives.
in korean, for example, the words corresponding to english
adjectives act as a subclass of verbs, so what is in english an adjective    beautiful   
acts in korean like a verb meaning    to be beautiful   .

the    nal open class form, adverbs, is rather a hodge-podge in both form and

meaning. in the following all the italicized words are adverbs:

actually, i ran home extremely quickly yesterday

what coherence the class has semantically may be solely that each of these
words can be viewed as modifying something (often verbs, hence the name    ad-

8.1

   

(mostly) english word classes

3

verb   , but also other adverbs and entire verb phrases). directional adverbs or loca-
tive adverbs (home, here, downhill) specify the direction or location of some action;
degree adverbs (extremely, very, somewhat) specify the extent of some action, pro-
cess, or property; manner adverbs (slowly, slinkily, delicately) describe the manner
of some action or process; and temporal adverbs describe the time that some ac-
tion or event took place (yesterday, monday). because of the heterogeneous nature
of this class, some adverbs (e.g., temporal adverbs like monday) are tagged in some
tagging schemes as nouns.

the closed classes differ more from language to language than do the open

classes. some of the important closed classes in english include:
prepositions: on, under, over, near, by, at, from, to, with
particles: up, down, on, off, in, out, at, by
determiners: a, an, the
conjunctions: and, but, or, as, if, when
pronouns: she, who, i, others
auxiliary verbs: can, may, should, are
numerals: one, two, three,    rst, second, third

prepositions occur before noun phrases. semantically they often indicate spatial
or temporal relations, whether literal (on it, before then, by the house) or metaphor-
ical (on time, with gusto, beside herself), but often indicate other relations as well,
like marking the agent in (haid113t was written by shakespeare, a particle resembles
a preposition or an adverb and is used in combination with a verb. particles often
have extended meanings that aren   t quite the same as the prepositions they resemble,
as in the particle over in she turned the paper over.

a verb and a particle that act as a single syntactic and/or semantic unit are
called a phrasal verb. the meaning of phrasal verbs is often problematically non-
compositional   not predictable from the distinct meanings of the verb and the par-
ticle. thus, turn down means something like    reject   , rule out    eliminate   ,    nd out
   discover   , and go on    continue   .

a closed class that occurs with nouns, often marking the beginning of a noun
phrase, is the determiner. one small subtype of determiners is the article: english
has three articles: a, an, and the. other determiners include this and that (this chap-
ter, that page). a and an mark a noun phrase as inde   nite, while the can mark it
as de   nite; de   niteness is a discourse property (chapter 21). articles are quite fre-
quent in english; indeed, the is the most frequently occurring word in most corpora
of written english, and a and an are generally right behind.

conjunctions join two phrases, clauses, or sentences. coordinating conjunc-
tions like and, or, and but join two elements of equal status. subordinating conjunc-
tions are used when one of the elements has some embedded status. for example,
that in    i thought that you might like some milk    is a subordinating conjunction
that links the main clause i thought with the subordinate clause you might like some
milk. this clause is called subordinate because this entire clause is the    content    of
the main verb thought. subordinating conjunctions like that which link a verb to its
argument in this way are also called complementizers.

pronouns are forms that often act as a kind of shorthand for referring to some
noun phrase or entity or event. personal pronouns refer to persons or entities (you,
she, i, it, me, etc.). possessive pronouns are forms of personal pronouns that in-
dicate either actual possession or more often just an abstract relation between the
person and some object (my, your, his, her, its, one   s, our, their). wh-pronouns
(what, who, whom, whoever) are used in certain question forms, or may also act as

locative
degree
manner
temporal

preposition

particle

phrasal verb

determiner
article

conjunctions

complementizer
pronoun
personal
possessive

wh

4 chapter 8

    part-of-speech tagging

auxiliary

copula
modal

interjection
negative

complementizers (frida, who married diego. . . ).

a closed class subtype of english verbs are the auxiliary verbs. cross-linguist-
ically, auxiliaries mark semantic features of a main verb: whether an action takes
place in the present, past, or future (tense), whether it is completed (aspect), whether
it is negated (polarity), and whether an action is necessary, possible, suggested, or
desired (mood). english auxiliaries include the copula verb be, the two verbs do and
have, along with their in   ected forms, as well as a class of modal verbs. be is called
a copula because it connects subjects with certain kinds of predicate nominals and
adjectives (he is a duck). the verb have can mark the perfect tenses (i have gone, i
had gone), and be is used as part of the passive (we were robbed) or progressive (we
are leaving) constructions. modals are used to mark the mood associated with the
event depicted by the main verb: can indicates ability or possibility, may permission
or possibility, must necessity. there is also a modal use of have (e.g., i have to go).
english also has many words of more or less unique function, including inter-
jections (oh, hey, alas, uh, um), negatives (no, not), politeness markers (please,
thank you), greetings (hello, goodbye), and the existential there (there are two on
the table) among others. these classes may be distinguished or lumped together as
interjections or adverbs depending on the purpose of the labeling.

8.2 the id32 part-of-speech tagset

an important tagset for english is the 45-tag id32 tagset (marcus et al.,
1993), shown in fig. 8.1, which has been used to label many corpora.
in such
labelings, parts-of-speech are generally represented by placing the tag after each
word, delimited by a slash:

example
tag description
and, but, or pdt predeterminer

example tag description example
all, both

vbp verb non-3sg

eat

tag
cc

description
coordinating
conjunction
cardinal number
determiner
existential    there   

adverb

rbr comparative

pos possessive ending    s
prp
personal pronoun
prp$ possess. pronoun

one, two
a, the
there
mea culpa rb
of, in, by

cd
dt
ex
fw foreign word
in
preposition/
subordin-conj
adjective
comparative adj
superlative adj
list item marker
modal
sing or mass noun llama
llamas
noun, plural
ibm
proper noun, sing.

jj
jjr
jjs
ls
md
nn
nns
nnp
nnps proper noun, plu. carolinas vbn verb past part.
figure 8.1 id32 part-of-speech tags (including punctuation).

yellow
rbs superlatv. adverb
bigger
rp
particle
wildest
sym symbol
1, 2, one
to
   to   
can, should uh
interjection
vb
verb base form
vbd verb past tense
vbg verb gerund

fastest
up, off
+,%, &
to
ah, oops
eat
ate
eating
eaten

adverb

present

vbz verb 3sg pres

i, you, he wdt wh-determ.
your, one   s wp
wh-pronoun
quickly
wp$ wh-possess.
faster
wrb wh-adverb

eats
which, that
what, who
whose
how, where

$
#
   
   
(
)
,
.
:

$
#
    or    
    or    
[, (, {, <
], ), }, >
,

dollar sign
pound sign
left quote
right quote
left paren
right paren
comma
sent-end punc . ! ?
sent-mid punc : ; ...     -

(8.1) the/dt grand/jj jury/nn commented/vbd on/in a/dt number/nn of/in

other/jj topics/nns ./.

(8.2) there/ex are/vbp 70/cd children/nns there/rb

brown

wsj
switchboard

8.2

    the id32 part-of-speech tagset

5

(8.3) preliminary/jj    ndings/nns were/vbd reported/vbn in/in today/nn
   s/pos new/nnp england/nnp journal/nnp of/in medicine/nnp ./.

example (8.1) shows the determiners the and a, the adjectives grand and other,
the common nouns jury, number, and topics, and the past tense verb commented.
example (8.2) shows the use of the ex tag to mark the existential there construction
in english, and, for comparison, another use of there which is tagged as an adverb
(rb). example (8.3) shows the segmentation of the possessive morpheme    s a pas-
sive construction,    were reported   , in which reported is marked as a past participle
(vbn). note that since new england journal of medicine is a proper noun, the tree-
bank tagging chooses to mark each noun in it separately as nnp, including journal
and medicine, which might otherwise be labeled as common nouns (nn).

corpora labeled with parts-of-speech are crucial training (and testing) sets for
statistical tagging algorithms. three main tagged corpora are consistently used for
training and testing part-of-speech taggers for english. the brown corpus is a mil-
lion words of samples from 500 written texts from different genres published in the
united states in 1961. the wsj corpus contains a million words published in the
wall street journal in 1989. the switchboard corpus consists of 2 million words
of telephone conversations collected in 1990-1991. the corpora were created by
running an automatic part-of-speech tagger on the texts and then human annotators
hand-corrected each tag.

there are some minor differences in the tagsets used by the corpora. for example
in the wsj and brown corpora, the single penn tag to is used for both the in   nitive
to (i like to race) and the preposition to (go to the store), while in switchboard the
tag to is reserved for the in   nitive use of to and the preposition is tagged in:

well/uh ,/, i/prp ,/, i/prp want/vbp to/to go/vb to/in a/dt restaurant/nn

finally, there are some idiosyncracies inherent in any tagset. for example, be-
cause the penn 45 tags were collapsed from a larger 87-tag tagset, the original
brown tagset, some potential useful distinctions were lost. the penn tagset was
designed for a treebank in which sentences were parsed, and so it leaves off syntac-
tic information recoverable from the parse tree. thus for example the penn tag in is
used for both subordinating conjunctions like if, when, unless, after:
after/in spending/vbg a/dt day/nn at/in the/dt beach/nn

and prepositions like in, on, after:

after/in sunrise/nn

words are generally tokenized before tagging. the id32 and the

british national corpus split contractions and the    s-genitive from their stems:2

would/md n   t/rb
children/nns    s/pos

the treebank tagset assumes that id121 of multipart words like new
york is done at whitespace, thus tagging. a new york city    rm as a/dt new/nnp
york/nnp city/nnp    rm/nn.

another commonly used tagset, the universal pos tag set of the universal de-
pendencies project (nivre et al., 2016), is used when building systems that can tag
many languages. see section 8.7.

2

indeed, the treebank tag pos is used only for    s, which must be segmented in id121.

6 chapter 8

    part-of-speech tagging

8.3 part-of-speech tagging

part-of-speech
tagging

ambiguous

ambiguity
resolution

part-of-speech tagging is the process of assigning a part-of-speech marker to each
word in an input text.3 the input to a tagging algorithm is a sequence of (tokenized)
words and a tagset, and the output is a sequence of tags, one per token.

tagging is a disambiguation task; words are ambiguous    have more than one
possible part-of-speech   and the goal is to    nd the correct tag for the situation.
for example, book can be a verb (book that    ight) or a noun (hand me that book).
that can be a determiner (does that    ight serve dinner) or a complementizer (i
thought that your    ight was earlier). the goal of pos-tagging is to resolve these
ambiguities, choosing the proper tag for the context. how common is tag ambiguity?
fig. 8.2 shows that most word types (80-86%) are unambiguous (janet is always
nnp, funniest jjs, and hesitantly rb). but the ambiguous words, though accounting
for only 14-15% of the vocabulary, are very common words, and hence 55-67% of
word tokens in running text are ambiguous.4

types:

tokens:

unambiguous (1 tag)
ambiguous

(2+ tags)

unambiguous (1 tag)
ambiguous

(2+ tags)

wsj

44,432 (86%)
7,025 (14%)

brown

45,799 (85%)
8,050 (15%)

577,421 (45%) 384,349 (33%)
711,780 (55%) 786,646 (67%)

figure 8.2 tag ambiguity for word types in brown and wsj, using treebank-3 (45-tag)
tagging. punctuation were treated as words, and words were kept in their original case.

some of the most ambiguous frequent words are that, back, down, put and set;

here are some examples of the 6 different parts-of-speech for the word back:

earnings growth took a back/jj seat
a small building in the back/nn
a clear majority of senators back/vbp the bill
dave began to back/vb toward the door
enable the country to buy back/rp about debt
i was twenty-one back/rb then

nonetheless, many words are easy to disambiguate, because their different tags
aren   t equally likely. for example, a can be a determiner or the letter a, but the
determiner sense is much more likely. this idea suggests a simplistic baseline algo-
rithm for part-of-speech tagging: given an ambiguous word, choose the tag which is
most frequent in the training corpus. this is a key concept:

most frequent class baseline: always compare a classi   er against a baseline at
least as good as the most frequent class baseline (assigning each token to the class
it occurred in most often in the training set).

accuracy

how good is this baseline? a standard way to measure the performance of part-
of-speech taggers is accuracy: the percentage of tags correctly labeled (matching

3 tags are also applied to punctuation, so assumes tokenzing of commas, quotation marks, etc., and
disambiguating end-of-sentence periods from periods inside words (e.g., etc.).
4 note the large differences across the two genres, especially in token frequency. tags in the wsj corpus
are less ambiguous; its focus on    nancial news leads to a more limited distribution of word usages than
the diverse genres of the brown corpus.

8.4

    id48 part-of-speech tagging

7

human labels on a test set). if we train on the wsj training corpus and test on sec-
tions 22-24 of the same corpus the most-frequent-tag baseline achieves an accuracy
of 92.34%. by contrast, the state of the art in part-of-speech tagging on this dataset
is around 97% tag accuracy, a performance that is achievable by most algorithms
(id48s, memms, neural networks, rule-based algorithms). see section 8.7 on
other languages and genres.

8.4 id48 part-of-speech tagging

sequence model

markov chain

in this section we introduce the use of the hidden markov model for part-of-speech
tagging. the id48 is a sequence model. a sequence model or sequence classi-
   er is a model whose job is to assign a label or class to each unit in a sequence,
thus mapping a sequence of observations to a sequence of labels. an id48 is a
probabilistic sequence model: given a sequence of units (words, letters, morphemes,
sentences, whatever), it computes a id203 distribution over possible sequences
of labels and chooses the best label sequence.

8.4.1 markov chains
the id48 is based on augmenting the markov chain. a markov chain is a model
that tells us something about the probabilities of sequences of random variables,
states, each of which can take on values from some set. these sets can be words, or
tags, or symbols representing anything, for example the weather. a markov chain
makes a very strong assumption that if we want to predict the future in the sequence,
all that matters is the current state. all the states before the current state have no im-
pact on the future except via the current state. it   s as if to predict tomorrow   s weather
you could examine today   s weather but you weren   t allowed to look at yesterday   s
weather.

(a)

(b)

figure 8.3 a markov chain for weather (a) and one for words (b), showing states and
transitions. a start distribution    is required; setting    = [0.1, 0.7, 0.2] for (a) would mean a
id203 0.7 of starting in state 2 (cold), id203 0.1 of starting in state 1 (hot), etc.

markov
assumption

more formally, consider a sequence of state variables q1,q2, ...,qi. a markov
model embodies the markov assumption on the probabilities of this sequence: that
when predicting the future, the past doesn   t matter, only the present.

markov assumption: p(qi = a|q1...qi   1) = p(qi = a|qi   1)

(8.4)

figure 8.3a shows a markov chain for assigning a id203 to a sequence of
weather events, for which the vocabulary consists of hot, cold, and warm. the

warm3hot1cold2.8.6.1.1.3.6.1.1.3charminguniformlyare.1.4.5.5.5.2.6.28 chapter 8

    part-of-speech tagging

states are represented as nodes in the graph, and the transitions, with their probabil-
ities, as edges. the transitions are probabilities: the values of arcs leaving a given
state must sum to 1. figure 8.3b shows a markov chain for assigning a id203 to
a sequence of words w1...wn. this markov chain should be familiar; in fact, it repre-
sents a bigram language model, with each edge expressing the id203 p(wi|w j)!
given the two models in fig. 8.3, we can assign a id203 to any sequence from
our vocabulary.

formally, a markov chain is speci   ed by the following components:

q = q1q2 . . .qn
a = a11a12 . . .an1 . . .ann

   =   1,  2, ...,  n

a set of n states
a transition id203 matrix a, each ai j represent-
ing the id203 of moving from state i to state j, s.t.

(cid:80)n
j=1 ai j = 1    i
be initial states. also,(cid:80)n

an initial id203 distribution over states.   i is the
id203 that the markov chain will start in state i.
some states j may have    j = 0, meaning that they cannot

i=1   i = 1

before you go on, use the sample probabilities in fig. 8.3a (with    = [.1, .7.,2])

to compute the id203 of each of the following sequences:

(8.5) hot hot hot hot
(8.6) cold hot cold hot

what does the difference in these probabilities tell you about a real-world weather
fact encoded in fig. 8.3a?

8.4.2 the hidden markov model
a markov chain is useful when we need to compute a id203 for a sequence
of observable events. in many cases, however, the events we are interested in are
hidden: we don   t observe them directly. for example we don   t normally observe
part-of-speech tags in a text. rather, we see words, and must infer the tags from the
word sequence. we call the tags hidden because they are not observed.

a hidden markov model (id48) allows us to talk about both observed events
(like words that we see in the input) and hidden events (like part-of-speech tags) that
we think of as causal factors in our probabilistic model. an id48 is speci   ed by
the following components:

hidden

hidden
markov model

q = q1q2 . . .qn
a = a11 . . .ai j . . .ann

o = o1o2 . . .ot

b = bi(ot )

   =   1,  2, ...,  n

a set of n states
a transition id203 matrix a, each ai j representing the id203

of moving from state i to state j, s.t.(cid:80)n

j=1 ai j = 1    i

a sequence of t observations, each one drawn from a vocabulary v =
v1,v2, ...,vv
a sequence of observation likelihoods, also called emission probabili-
ties, each expressing the id203 of an observation ot being generated
from a state i
an initial id203 distribution over states.   i is the id203 that
the markov chain will start in state i. some states j may have    j = 0,

meaning that they cannot be initial states. also,(cid:80)n

i=1   i = 1

8.4

    id48 part-of-speech tagging

9

a    rst-order hidden markov model instantiates two simplifying assumptions.
first, as with a    rst-order markov chain, the id203 of a particular state depends
only on the previous state:

markov assumption: p(qi|q1...qi   1) = p(qi|qi   1)

(8.7)

second, the id203 of an output observation oi depends only on the state that

produced the observation qi and not on any other states or any other observations:

output independence: p(oi|q1 . . .qi, . . . ,qt ,o1, . . . ,oi, . . . ,ot ) = p(oi|qi)

(8.8)

8.4.3 the components of an id48 tagger
let   s start by looking at the pieces of an id48 tagger, and then we   ll see how to use
it to tag. an id48 has two components, the a and b probabilities.
the a matrix contains the tag transition probabilities p(ti|ti   1) which represent
the id203 of a tag occurring given the previous tag. for example, modal verbs
like will are very likely to be followed by a verb in the base form, a vb, like race, so
we expect this id203 to be high. we compute the maximum likelihood estimate
of this transition id203 by counting, out of the times we see the    rst tag in a
labeled corpus, how often the    rst tag is followed by the second:

p(ti|ti   1) =

c(ti   1,ti)
c(ti   1)

(8.9)

in the wsj corpus, for example, md occurs 13124 times of which it is followed

by vb 10471, for an id113 estimate of

p(v b|md) =

c(md,v b)

c(md)

=

10471
13124

= .80

(8.10)

let   s walk through an example, seeing how these probabilities are estimated and

used in a sample tagging task, before we return to the algorithm for decoding.

corpus. for this example we   ll use the tagged wsj corpus.

in id48 tagging, the probabilities are estimated by counting on a tagged training
the b emission probabilities, p(wi|ti), represent the id203, given a tag (say
md), that it will be associated with a given word (say will). the id113 of the emis-
sion id203 is

p(wi|ti) =

c(ti,wi)

c(ti)

(8.11)

of the 13124 occurrences of md in the wsj corpus, it is associated with will 4046
times:

p(will|md) =

c(md,will)

c(md)

=

4046
13124

= .31

(8.12)

we saw this kind of bayesian modeling in chapter 4; recall that this likelihood
term is not asking    which is the most likely tag for the word will?    that would be
the posterior p(md|will). instead, p(will|md) answers the slightly counterintuitive
question    if we were going to generate a md, how likely is it that this modal would
be will?   

the a transition probabilities, and b observation likelihoods of the id48 are
illustrated in fig. 8.4 for three states in an id48 part-of-speech tagger; the full
tagger would have one state for each tag.

10 chapter 8

    part-of-speech tagging

figure 8.4 an illustration of the two parts of an id48 representation:
the a transition
probabilities used to compute the prior id203, and the b observation likelihoods that are
associated with each state, one likelihood for each possible observation word.
8.4.4 id48 tagging as decoding
for any model, such as an id48, that contains hidden variables, the task of deter-
mining the hidden variables sequence corresponding to the sequence of observations
is called decoding. more formally,

decoding: given as input an id48    = (a,b) and a sequence of ob-
servations o = o1,o2, ...,ot ,    nd the most probable sequence of states
q = q1q2q3 . . .qt .

decoding

for id52, the goal of id48 decoding is to choose the tag
1 that is most probable given the observation sequence of n words words

sequence tn
wn
1:

  tn
1 = argmax

tn
1

p(tn

1|wn
1)

(8.13)

the way we   ll do this in the id48 is to use bayes    rule to instead compute:

  tn
1 = argmax

tn
1

p(wn

1|tn
1 )p(tn
1 )
p(wn
1)

(8.14)

(8.15)

furthermore, we simplify eq. 8.14 by dropping the denominator p(wn
1):

  tn
1 = argmax

tn
1

p(wn

1|tn
1 )p(tn
1 )

id48 taggers make two further simplifying assumptions. the    rst is that the
id203 of a word appearing depends only on its own tag and is independent of
neighboring words and tags:

p(wn

1|tn
1 )    

p(wi|ti)

(8.16)

the second assumption, the bigram assumption, is that the id203 of a tag

is dependent only on the previous tag, rather than the entire tag sequence;

p(tn

1 )    

p(ti|ti   1)

(8.17)

n(cid:89)

i=1

n(cid:89)

i=1

nn3vb1md2a22a11a12a21a13a33a32a23a31p("aardvark" | nn)...p(   will    | nn)...p("the" | nn)...p(   back    | nn)...p("zebra" | nn)b3p("aardvark" | vb)...p(   will    | vb)...p("the" | vb)...p(   back    | vb)...p("zebra" | vb)b1p("aardvark" | md)...p(   will    | md)...p("the" | md)...p(   back    | md)...p("zebra" | md)b28.4

    id48 part-of-speech tagging

11

plugging the simplifying assumptions from eq. 8.16 and eq. 8.17 into eq. 8.15
results in the following equation for the most probable tag sequence from a bigram
tagger:

  tn
1 = argmax

tn
1

p(tn

1|wn

1)     argmax

tn
1

(cid:122) (cid:125)(cid:124) (cid:123)

emission
p(wi|ti)

(cid:122)

(cid:125)(cid:124)

transition
p(ti|ti   1)

(cid:123)

n(cid:89)

i=1

(8.18)

the two parts of eq. 8.18 correspond neatly to the b emission id203 and

a transition id203 that we just de   ned above!

viterbi
algorithm

8.4.5 the viterbi algorithm
the decoding algorithm for id48s is the viterbi algorithm shown in fig. 8.5. as
an instance of id145, viterbi resembles the dynamic program-
ming minimum id153 algorithm of chapter 2.

function viterbi(observations of len t,state-graph of len n) returns best-path, path-prob

create a path id203 matrix viterbi[n,t]
for each state s from 1 to n do
viterbi[s,1]     s     bs(o1)
backpointer[s,1]   0

for each time step t from 2 to t do
for each state s from 1 to n do

; initialization step

; recursion step

viterbi[s,t]    nmax
s(cid:48) =1
backpointer[s,t]    nargmax
s(cid:48) =1

viterbi[s(cid:48),t     1]     as(cid:48),s     bs(ot )

viterbi[s(cid:48),t     1]     as(cid:48),s     bs(ot )

s=1

viterbi[s,t ]

bestpathprob    nmax
bestpathpointer    nargmax
bestpath   the path starting at state bestpathpointer, that follows backpointer[] to states back in time
return bestpath, bestpathprob

; termination step

; termination step

viterbi[s,t ]

s=1

figure 8.5 viterbi algorithm for    nding the optimal sequence of tags. given an observation sequence and an
id48    = (a,b), the algorithm returns the state path through the id48 that assigns maximum likelihood to
the observation sequence.

the viterbi algorithm    rst sets up a id203 matrix or lattice, with one col-
umn for each observation ot and one row for each state in the state graph. each col-
umn thus has a cell for each state qi in the single combined automaton. figure 8.6
shows an intuition of this lattice for the sentence janet will back the bill.

each cell of the trellis, vt ( j), represents the id203 that the id48 is in state
j after seeing the    rst t observations and passing through the most probable state
sequence q1, ...,qt   1, given the id48    . the value of each cell vt ( j) is computed
by recursively taking the most probable path that could lead us to this cell. formally,
each cell expresses the id203

vt ( j) = max

q1,...,qt   1

p(q1...qt   1,o1,o2 . . .ot ,qt = j|   )

(8.19)

we represent the most probable path by taking the maximum over all possible
. like other id145 algorithms,

previous state sequences max

q1,...,qt   1

12 chapter 8

    part-of-speech tagging

figure 8.6 a sketch of the lattice for janet will back the bill, showing the possible tags (qi)
for each word and highlighting the path corresponding to the correct tag sequence through the
hidden states. states (parts-of-speech) which have a zero id203 of generating a particular
word according to the b matrix (such as the id203 that a determiner dt will be realized
as janet) are greyed out.

viterbi    lls each cell recursively. given that we had already computed the probabil-
ity of being in every state at time t    1, we compute the viterbi id203 by taking
the most probable of the extensions of the paths that lead to the current cell. for a
given state q j at time t, the value vt ( j) is computed as

vt ( j) =

nmax
i=1

vt   1(i) ai j b j(ot )

(8.20)

the three factors that are multiplied in eq. 8.20 for extending the previous paths to
compute the viterbi id203 at time t are

vt   1(i)
ai j
b j(ot )

the previous viterbi path id203 from the previous time step
the transition id203 from previous state qi to current state q j
the state observation likelihood of the observation symbol ot given
the current state j

8.4.6 working through an example
let   s tag the sentence janet will back the bill; the goal is the correct series of tags
(see also fig. 8.6):
(8.21) janet/nnp will/md back/vb the/dt bill/nn

let the id48 be de   ned by the two tables in fig. 8.7 and fig. 8.8. figure 8.7
lists the ai j probabilities for transitioning between the hidden states (part-of-speech
tags). figure 8.8 expresses the bi(ot ) probabilities, the observation likelihoods of
words given tags. this table is (slightly simpli   ed) from counts in the wsj corpus.
so the word janet only appears as an nnp, back has 4 possible parts of speech, and
the word the can appear as a determiner or as an nnp (in titles like    somewhere
over the rainbow    all words are tagged as nnp).

figure 8.9 shows a    eshed-out version of the sketch we saw in fig. 8.6, the
viterbi trellis for computing the best hidden state sequence for the observation se-
quence janet will back the bill.

jjnnpnnpnnpmdmdmdmdvbvbjjjjjjnnnnrbrbrbrbdtdtdtdtnnpjanetwillbackthebillnnvbmdnnvbjjrbnnpdtnnvb8.4

    id48 part-of-speech tagging

13

jj

rb

dt

nn

nnp
0.2767
0.3777
0.0008
0.0322
0.0366
0.0096
0.0068
0.1147

vb
<s >
0.0031 0.0453 0.0449 0.0510 0.2026
nnp
0.0009 0.0084 0.0584 0.0090 0.0025
md
0.7968 0.0005 0.0008 0.1698 0.0041
vb
0.0050 0.0837 0.0615 0.0514 0.2231
jj
0.0001 0.0733 0.4509 0.0036 0.0036
nn
0.0014 0.0086 0.1216 0.0177 0.0068
rb
0.1011 0.1012 0.0120 0.0728 0.0479
dt
0.0002 0.2157 0.4744 0.0102 0.0017
figure 8.7 the a transition probabilities p(ti|ti   1) computed from the wsj corpus without
smoothing. rows are labeled with the conditioning event; thus p(v b|md) is 0.7968.

md
0.0006
0.0110
0.0002
0.0005
0.0004
0.0176
0.0102
0.0021

nnp
md
vb
jj
nn
rb
dt

bill

will

janet
0.000032 0
0
0
0
0
0
0

the
back
0.000048 0
0
0
0
0.308431 0
0.000028
0.000028 0.000672 0
0
0
0.000340 0
0.002337
0.000200 0.000223 0
0
0.010446 0
0
0.506099 0
0
0

figure 8.8 observation likelihoods b computed from the wsj corpus without smoothing,
simpli   ed slightly.

there are n = 5 state columns. we begin in column 1 (for the word janet) by
setting the viterbi value in each cell to the product of the    transition id203
(the start id203 for that state i, which we get from the < s > entry of fig. 8.7),
and the observation likelihood of the word janet given the tag for that cell. most of
the cells in the column are zero since the word janet cannot be any of those tags.
the reader should    nd this in fig. 8.9.

next, each cell in the will column gets updated. for each state, we compute the
value viterbi[s,t] by taking the maximum over the extensions of all the paths from the
previous column that lead to the current cell according to eq. 8.20. we have shown
the values for the md, vb, and nn cells. each cell gets the max of the 7 values
from the previous column, multiplied by the appropriate transition id203; as it
happens in this case, most of them are zero from the previous column. the remaining
value is multiplied by the relevant observation id203, and the (trivial) max is
taken. in this case the    nal value, .0000002772, comes from the nnp state at the
previous column. the reader should    ll in the rest of the trellis in fig. 8.9 and
backtrace to reconstruct the correct state sequence nnp md vb dt nn.

8.4.7 extending the id48 algorithm to trigrams
practical id48 taggers have a number of extensions of this simple model. one
important missing feature is a wider tag context. in the tagger described above the
id203 of a tag depends only on the previous tag:

p(tn

1 )    

p(ti|ti   1)

(8.22)

in practice we use more of the history, letting the id203 of a tag depend on

i=1

n(cid:89)

14 chapter 8

    part-of-speech tagging

figure 8.9 the    rst few entries in the individual state columns for the viterbi algorithm. each cell keeps the
id203 of the best path so far and a pointer to the previous cell along that path. we have only    lled out
columns 1 and 2; to avoid clutter most cells with value 0 are left empty. the rest is left as an exercise for the
reader. after the cells are    lled in, backtracing from the end state, we should be able to reconstruct the correct
state sequence nnp md vb dt nn.

the two previous tags:

n(cid:89)

i=1

p(tn

1 )    

p(ti|ti   1,ti   2)

(8.23)

extending the algorithm from bigram to trigram taggers gives a small (perhaps a
half point) increase in performance, but conditioning on two previous tags instead of
one requires a signi   cant change to the viterbi algorithm. for each cell, instead of
taking a max over transitions from each cell in the previous column, we have to take
a max over paths through the cells in the previous two columns, thus considering n2
rather than n hidden states at every observation.

in addition to increasing the context window, id48 taggers have a number of
other advanced features. one is to let the tagger know the location of the end of the
sentence by adding dependence on an end-of-sequence marker for tn+1. this gives
the following equation for part-of-speech tagging:

  tn
1 = argmax

tn
1

p(tn

1|wn

1)     argmax

tn
1

p(wi|ti)p(ti|ti   1,ti   2)

p(tn+1|tn)

(8.24)

in tagging any sentence with eq. 8.24, three of the tags used in the context will
fall off the edge of the sentence, and hence will not match regular words. these tags,

(cid:34) n(cid:89)

i=1

(cid:35)

  p(nnp|start) = .28* p(md|md)= 0*  p(md|nnp).000009*.01  = .0000009 v1(2)=.0006 x 0 = 0v1(1) = .28* .000032 = .000009tmdq2q1o1janetbillwillo2o3backvbjjv1(3)=.0031 x 0 = 0v1(4)= .045*0=0o4  *  p(md|vb) = 0 * p(md|jj)= 0p(vb|start) = .0031p(jj |start) =.045backtraceq3q4thennq5rbq6dtq7v2(2) =max * .308 =.0000002772v2(5)=max * .0002 = .0000000001v2(3)=max * .000028 =     2.5e-11 v3(6)=max * .0104v3(5)=max * .000223v3(4)=max * .00034v3(3)=max * .00067v1(5)v1(6)v1(7)v2(1)v2(4)v2(6)v2(7)backtrace* p(rb|nn)* p(nn|nn)startstartstartstartstarto5nnpp(md|start) = .00068.4

    id48 part-of-speech tagging

15

t   1, t0, and tn+1, can all be set to be a single special    sentence boundary    tag that is
added to the tagset, which assumes sentences boundaries have already been marked.
one problem with trigram taggers as instantiated in eq. 8.24 is data sparsity.
any particular sequence of tags ti   2,ti   1,ti that occurs in the test set may simply
never have occurred in the training set. that means we cannot compute the tag
trigram id203 just by the maximum likelihood estimate from counts, following
eq. 8.25:

p(ti|ti   1,ti   2) =

c(ti   2,ti   1,ti)
c(ti   2,ti   1)

(8.25)

just as we saw with id38, many of these counts will be zero
in any training set, and we will incorrectly predict that a given tag sequence will
never occur! what we need is a way to estimate p(ti|ti   1,ti   2) even if the sequence
ti   2,ti   1,ti never occurs in the training data.

the standard approach to solving this problem is the same interpolation idea
we saw in id38: estimate the id203 by combining more robust,
but weaker estimators. for example, if we   ve never seen the tag sequence prp vb
to, and so can   t compute p(to|prp,vb) from this frequency, we still could rely
on the bigram id203 p(to|vb), or even the unigram id203 p(to). the
id113 of each of these probabilities can be computed from
a corpus with the following counts:

trigrams

  p(ti|ti   1,ti   2) =
  p(ti|ti   1) =

bigrams

unigrams

  p(ti) =

c(ti   2,ti   1,ti)
c(ti   2,ti   1)
c(ti   1,ti)
c(ti   1)
c(ti)

n

(8.26)

(8.27)

(8.28)

the standard way to combine these three estimators to estimate the trigram probabil-
ity p(ti|ti   1,ti   2) is via linear interpolation. we estimate the id203 p(ti|ti   1ti   2)
by a weighted sum of the unigram, bigram, and trigram probabilities:
p(ti|ti   1ti   2) =   3   p(ti|ti   1ti   2) +   2   p(ti|ti   1) +   1   p(ti)

(8.29)

we require   1 +   2 +   3 = 1, ensuring that the resulting p is a id203 distri-
bution. the    s are set by deleted interpolation (jelinek and mercer, 1980): we
successively delete each trigram from the training corpus and choose the    s so as to
maximize the likelihood of the rest of the corpus. the deletion helps to set the    s
in such a way as to generalize to unseen data and not over   t. figure 8.10 gives a
deleted interpolation algorithm for tag trigrams.

8.4.8 id125
when the number of states grows very large, the vanilla viterbi algorithm be slow.
the complexity of the algorithm is o(n2t ); n (the number of states) can be large
for trigram taggers, which have to consider every previous pair of the 45 tags, re-
sulting in 453 = 91,125 computations per column. n can be even larger for other
applications of viterbi, for example to decoding in neural networks, as we will see
in future chapters.

one common solution to the complexity problem is the use of id125
decoding. in id125, instead of keeping the entire column of states at each

deleted
interpolation

id125

16 chapter 8

    part-of-speech tagging

function deleted-interpolation(corpus) returns   1,  2,  3

  1,   2,   3   0
foreach trigram t1,t2,t3 with c(t1,t2,t3) > 0

depending on the maximum of the following three values

case c(t1,t2,t3)   1
case c(t2,t3)   1
case c(t3)   1

c(t1,t2)   1 : increment   3 by c(t1,t2,t3)
c(t2)   1 : increment   2 by c(t1,t2,t3)
n   1 : increment   1 by c(t1,t2,t3)

end

end
normalize   1,  2,  3
return   1,  2,  3

figure 8.10 the deleted interpolation algorithm for setting the weights for combining un-
igram, bigram, and trigram tag probabilities. if the denominator is 0 for any case, we de   ne
the result of that case to be 0. n is the number of tokens in the corpus. after brants (2000).

time point t, we just keep the best few hypothesis at that point. at time t this requires
computing the viterbi score for each of the n cells, sorting the scores, and keeping
only the best-scoring states. the rest are pruned out and not continued forward to
time t + 1.

one way to implement id125 is to keep a    xed number of states instead of
all n current states. here the beam width    is a    xed number of states. alternatively
   can be modeled as a    xed percentage of the n states, or as a id203 threshold.
figure 8.11 shows the search lattice using a beam width of 2 states.

beam width

figure 8.11 a id125 version of fig. 8.6, showing a beam width of 2. at each time
t, all (non-zero) states are computed, but then they are sorted and only the best 2 states are
propagated forward and the rest are pruned, shown in orange.

jjnnpnnpnnpmdmdmdmdvbvbjjjjjjnnnnrbrbrbrbdtdtdtdtnnpjanetwillbackthebillnnvbmdnnvbjjrbnnpdtnnvb8.5

    maximum id178 markov models

17

8.4.9 unknown words

words people
never use    
could be
only i
know them

ishikawa takuboku 1885   1912

unknown
words

to achieve high accuracy with part-of-speech taggers, it is also important to have
a good model for dealing with unknown words. proper names and acronyms are
created very often, and even new common nouns and verbs enter the language at a
surprising rate. one useful feature for distinguishing parts of speech is word shape:
words starting with capital letters are likely to be proper nouns (nnp).

but the strongest source of information for guessing the part-of-speech of un-
known words is morphology. words that end in -s are likely to be plural nouns
(nns), words ending with -ed tend to be past participles (vbn), words ending with
-able adjectives (jj), and so on. we store for each    nal letter sequence (for sim-
plicity referred to as word suf   xes) of up to 10 letters the statistics of the tag it was
associated with in training. we are thus computing for each suf   x of length i the
id203 of the tag ti given the suf   x letters (samuelsson 1993, brants 2000):

p(ti|ln   i+1 . . .ln)

(8.30)

back-off is used to smooth these probabilities with successively shorter suf   xes.
because unknown words are unlikely to be closed-class words like prepositions,
suf   x probabilities can be computed only for words whose training set frequency is
    10, or only for open-class words. separate suf   x tries are kept for capitalized and
uncapitalized words.
finally, because eq. 8.30 gives a posterior estimate p(ti|wi), we can compute
the likelihood p(wi|ti) that id48s require by using bayesian inversion (i.e., using
bayes rule and computation of the two priors p(ti) and p(ti|ln   i+1 . . .ln)).

in addition to using capitalization information for unknown words, brants (2000)
also uses capitalization for known words by adding a capitalization feature to each
tag. thus, instead of computing p(ti|ti   1,ti   2) as in eq. 8.26, the algorithm com-
putes the id203 p(ti,ci|ti   1,ci   1,ti   2,ci   2). this is equivalent to having a cap-
italized and uncapitalized version of each tag, doubling the size of the tagset.

combining all these features, a trigram id48 like that of brants (2000) has a
tagging accuracy of 96.7% on the id32, perhaps just slightly below the
performance of the best memm and neural taggers.

8.5 maximum id178 markov models

while an id48 can achieve very high accuracy, we saw that it requires a number of
architectural innovations to deal with unknown words, backoff, suf   xes, and so on.
it would be so much easier if we could add arbitrary features directly into the model
in a clean way, but that   s hard for generative models like id48s. luckily, we   ve
already seen a model for doing this: the id28 model of chapter 5! but
id28 isn   t a sequence model; it assigns a class to a single observation.
however, we could turn id28 into a discriminative sequence model
simply by running it on successive words, using the class assigned to the prior word

18 chapter 8

    part-of-speech tagging

memm

let the sequence of words be w = wn

as a feature in the classi   cation of the next word. when we apply id28
in this way, it   s called the maximum id178 markov model or memm5
1 and the sequence of tags t = tn
1. in an
id48 to compute the best tag sequence that maximizes p(t|w ) we rely on bayes   
rule and the likelihood p(w|t ):
  t = argmax

p(t|w )
p(w|t )p(t )

p(wordi|tagi)

t

= argmax

t

= argmax

t

(cid:89)

i

(cid:89)

i

p(tagi|tagi   1)

(8.31)

in an memm, by contrast, we compute the posterior p(t|w ) directly, training it to
discriminate among the possible tag sequences:
p(t|w )

  t = argmax

t

= argmax

t

(cid:89)

i

p(ti|wi,ti   1)

(8.32)

consider tagging just one word. a multinomial id28 classi   er could
compute the single id203 p(ti|wi,ti   1) in a different way that an id48. fig. 8.12
shows the intuition of the difference via the direction of the arrows; id48s compute
likelihood (observation word conditioned on tags) but memms compute posterior
(tags conditioned on observation words).

figure 8.12 a schematic view of the id48 (top) and memm (bottom) representation of
the id203 computation for the correct sequence of tags for the back sentence. the id48
computes the likelihood of the observation given the hidden state, while the memm computes
the posterior of each state, conditioned on the previous state and current observation.

8.5.1 features in a memm
of course we don   t build memms that condition just on wi and ti   1. the reason to
use a discriminative sequence model is that it   s easier to incorporate a lots of fea-
tures.6 figure 8.13 shows a graphical intuition of some of these additional features.

   maximum id178 model    is an outdated name for id28; see the history section.

5
6 because in id48s all computation is based on the two probabilities p(tag|tag) and p(word|tag), if
we want to include some source of knowledge into the tagging process, we must    nd a way to encode
the knowledge into one of these two probabilities. each time we add a feature we have to do a lot of
complicated conditioning which gets harder and harder as we have more and more such features.

willmdvbdtnnjanetbackthebillnnpwillmdvbdtnnjanetbackthebillnnp8.5

    maximum id178 markov models

19

figure 8.13 an memm for part-of-speech tagging showing the ability to condition on
more features.

a basic memm part-of-speech tagger conditions on the observation word it-
self, neighboring words, and previous tags, and various combinations, using feature
templates like the following:

templates

(cid:104)ti,wi   2(cid:105),(cid:104)ti,wi   1(cid:105),(cid:104)ti,wi(cid:105),(cid:104)ti,wi+1(cid:105),(cid:104)ti,wi+2(cid:105)
(cid:104)ti,ti   1(cid:105),(cid:104)ti,ti   2,ti   1(cid:105),
(cid:104)ti,ti   1,wi(cid:105),(cid:104)ti,wi   1,wi(cid:105)(cid:104)ti,wi,wi+1(cid:105),

(8.33)

recall from chapter 5 that feature templates are used to automatically populate the
set of features from every instance in the training and test set. thus our example
janet/nnp will/md back/vb the/dt bill/nn, when wi is the word back, would gen-
erate the following features:

ti = vb and wi   2 = janet
ti = vb and wi   1 = will
ti = vb and wi = back
ti = vb and wi+1 = the
ti = vb and wi+2 = bill
ti = vb and ti   1 = md
ti = vb and ti   1 = md and ti   2 = nnp
ti = vb and wi = back and wi+1 = the

also necessary are features to deal with unknown words, expressing properties of
the word   s spelling or shape:

wi contains a particular pre   x (from all pre   xes of length     4)
wi contains a particular suf   x (from all suf   xes of length     4)
wi contains a number
wi contains an upper-case letter
wi contains a hyphen
wi is all upper case
wi   s word shape
wi   s short word shape
wi is upper case and has a digit and a dash (like cfc-12)
wi is upper case and followed within 3 words by co., inc., etc.

word shape

word shape features are used to represent the abstract letter pattern of the word
by mapping lower-case letters to    x   , upper-case to    x   , numbers to    d   , and retaining
punctuation. thus for example i.m.f would map to x.x.x. and dc10-30 would
map to xxdd-dd. a second class of shorter word shape features is also used. in these
features consecutive character types are removed, so dc10-30 would be mapped to
xd-d but i.m.f would still map to x.x.x. for example the word well-dressed would
generate the following non-zero valued feature values:

willmdvbjanetbackthebillnnp<s>wiwi+1wi-1ti-1ti-2wi-120 chapter 8

    part-of-speech tagging

pre   x(wi) = w
pre   x(wi) = we
pre   x(wi) = wel
pre   x(wi) = well
suf   x(wi) = ssed
suf   x(wi) = sed
suf   x(wi) = ed
suf   x(wi) = d
has-hyphen(wi)
word-shape(wi) = xxxx-xxxxxxx
short-word-shape(wi) = x-x

features for known words, like the templates in eq. 8.33, are computed for every
word seen in the training set. the unknown word features can also be computed for
all words in training, or only on training words whose frequency is below some
threshold. the result of the known-word templates and word-signature features is a
very large set of features. generally a feature cutoff is used in which features are
thrown out if they have count < 5 in the training set.

8.5.2 decoding and training memms
the most likely sequence of tags is then computed by combining these features of
the input word wi, its neighbors within l words wi+l
i   k as
follows (using    to refer to feature weights instead of w to avoid the confusion with
w meaning words):

i   l, and the previous k tags ti   1

p(t|w )

  t = argmax

t

= argmax

t

= argmax

t

(cid:89)

i

(cid:89)

i

p(ti|wi+l

i   l,ti   1
i   k )

      (cid:88)
      (cid:88)

exp

j

j

exp

(cid:88)

t(cid:48)   tagset

      

      

   j f j(ti,wi+l

i   l,ti   1
i   k )

   j f j(t(cid:48),wi+l

i   l,ti   1
i   k )

(8.34)

how should we decode to    nd this optimal tag sequence   t ? the simplest way
to turn id28 into a sequence model is to build a local classi   er that
classi   es each word left to right, making a hard classi   cation of the    rst word in
the sentence, then a hard decision on the second word, and so on. this is called a
greedy decoding algorithm, because we greedily choose the best tag for each word,
as shown in fig. 8.14.

greedy

function greedy sequence decoding(words w, model p) returns tag sequence t

for i = 1 to length(w)

  ti = argmax

t(cid:48)    t

p(t(cid:48) | wi+l

i   l,ti   1
i   k )

figure 8.14
each time making a hard decision of which is the best tag.

in greedy decoding we simply run the classi   er on each token, left to right,

8.6

    bidirectionality

21

viterbi

the problem with the greedy algorithm is that by making a hard decision on
each word before moving on to the next word, the classi   er can   t use evidence from
future decisions. although the greedy algorithm is very fast, and occasionally has
suf   cient accuracy to be useful, in general the hard decision causes too much a drop
in performance, and we don   t use it.

instead we decode an memm with the viterbi algorithm just as with the id48,

   nding the sequence of part-of-speech tags that is optimal for the whole sentence.
for example, assume that our memm is only conditioning on the previous tag
ti   1 and observed word wi. concretely, this involves    lling an n    t array with
the appropriate values for p(ti|ti   1,wi), maintaining backpointers as we proceed. as
with id48 viterbi, when the table is    lled, we simply follow pointers back from the
maximum value in the    nal column to retrieve the desired set of labels. the requisite
changes from the id48-style application of viterbi have to do only with how we
   ll each cell. recall from eq. 8.20 that the recursive step of the viterbi equation
computes the viterbi value of time t for state j as

vt ( j) =

nmax
i=1

vt   1(i)ai j b j(ot ); 1     j     n,1 < t     t

(8.35)

which is the id48 implementation of

vt ( j) =

nmax
i=1

vt   1(i) p(s j|si) p(ot|s j) 1     j     n,1 < t     t

(8.36)

the memm requires only a slight change to this latter formula, replacing the a and
b prior and likelihood probabilities with the direct posterior:

vt ( j) =

nmax
i=1

vt   1(i) p(s j|si,ot ) 1     j     n,1 < t     t

(8.37)

learning in memms relies on the same supervised learning algorithms we presented
for id28. given a sequence of observations, feature functions, and cor-
responding hidden states, we use id119 to train the weights to maximize
the log-likelihood of the training corpus.

8.6 bidirectionality

label bias
observation
bias

the one problem with the memm and id48 models as presented is that they are
exclusively run left-to-right. while the viterbi algorithm still allows present deci-
sions to be in   uenced indirectly by future decisions, it would help even more if a
decision about word wi could directly use information about future tags ti+1 and ti+2.
adding bidirectionality has another useful advantage. memms have a theoret-
ical weakness, referred to alternatively as the label bias or observation bias prob-
lem (lafferty et al. 2001, toutanova et al. 2003). these are names for situations
when one source of information is ignored because it is explained away by another
source. consider an example from toutanova et al. (2003), the sequence will/nn
to/to    ght/vb. the tag to is often preceded by nn but rarely by modals (md),
and so that tendency should help predict the correct nn tag for will. but the previ-
ous transition p(twill|(cid:104)s(cid:105)) prefers the modal, and because p(to|to,twill) is so close
to 1 regardless of twill the model cannot make use of the transition id203 and
incorrectly chooses md. the strong information that to must have the tag to has ex-
plained away the presence of to and so the model doesn   t learn the importance of

22 chapter 8

    part-of-speech tagging

crf

stanford tagger

the previous nn tag for predicting to. bidirectionality helps the model by making
the link between to available when tagging the nn.

one way to implement bidirectionality is to switch to a more powerful model
called a conditional random    eld or crf. the crf is an undirected graphical
model, which means that it   s not computing a id203 for each tag at each time
step. instead, at each time step the crf computes log-linear functions over a clique,
a set of relevant features. unlike for an memm, these might include output features
of words in future time steps. the id203 of the best sequence is similarly
computed by the viterbi algorithm. because a crf normalizes probabilities over all
tag sequences, rather than over all the tags at an individual time t, training requires
computing the sum over all possible labelings, which makes crf training quite slow.
simpler methods can also be used; the stanford tagger uses a bidirectional
version of the memm called a cyclic dependency network (toutanova et al., 2003).
alternatively, any sequence model can be turned into a bidirectional model by
using multiple passes. for example, the    rst pass would use only part-of-speech
features from already-disambiguated words on the left. in the second pass, tags for
all words, including those on the right, can be used. alternately, the tagger can be run
twice, once left-to-right and once right-to-left. in greedy decoding, for each word
the classi   er chooses the highest-scoring of the tag assigned by the left-to-right and
right-to-left classi   er. in viterbi decoding, the classi   er chooses the higher scoring
of the two sequences (left-to-right or right-to-left). these bidirectional models lead
directly into the bi-lstm models that we will introduce in chapter 9 as a standard
neural sequence model.

8.7 part-of-speech tagging for other languages

augmentations to tagging algorithms become necessary when dealing with lan-
guages with rich morphology like czech, hungarian and turkish.

these productive word-formation processes result in a large vocabulary for these
languages: a 250,000 word token corpus of hungarian has more than twice as many
word types as a similarly sized corpus of english (oravecz and dienes, 2002), while
a 10 million word token corpus of turkish contains four times as many word types
as a similarly sized english corpus (hakkani-t  ur et al., 2002). large vocabular-
ies mean many unknown words, and these unknown words cause signi   cant per-
formance degradations in a wide variety of languages (including czech, slovene,
estonian, and romanian) (haji  c, 2000).

highly in   ectional languages also have much more information than english
coded in word morphology, like case (nominative, accusative, genitive) or gender
(masculine, feminine). because this information is important for tasks like pars-
ing and coreference resolution, part-of-speech taggers for morphologically rich lan-
guages need to label words with case and gender information. tagsets for morpho-
logically rich languages are therefore sequences of morphological tags rather than a
single primitive tag. here   s a turkish example, in which the word izin has three pos-
sible morphological/part-of-speech tags and meanings (hakkani-t  ur et al., 2002):

1. yerdeki izin temizlenmesi gerek.

the trace on the    oor should be cleaned.
  uzerinde parmak izin kalmis  
your    nger print is left on (it).

2.

iz + noun+a3sg+pnon+gen

iz + noun+a3sg+p2sg+nom

3. ic  eri girmek ic  in izin alman gerekiyor.

you need a permission to enter.

8.8

    summary

23

izin + noun+a3sg+pnon+nom

using a morphological parse sequence like noun+a3sg+pnon+gen as the part-
of-speech tag greatly increases the number of parts-of-speech, and so tagsets can
be 4 to 10 times larger than the 50   100 tags we have seen for english. with such
large tagsets, each word needs to be morphologically analyzed to generate the list
of possible morphological tag sequences (part-of-speech tags) for the word. the
role of the tagger is then to disambiguate among these tags. this method also helps
with unknown words since morphological parsers can accept unknown stems and
still segment the af   xes properly.

for non-word-space languages like chinese, id40 (chapter 2) is
either applied before tagging or done jointly. although chinese words are on aver-
age very short (around 2.4 characters per unknown word compared with 7.7 for en-
glish) the problem of unknown words is still large. while english unknown words
tend to be proper nouns in chinese the majority of unknown words are common
nouns and verbs because of extensive compounding. tagging models for chinese
use similar unknown word features to english, including character pre   x and suf-
   x features, as well as novel features like the radicals of each character in a word.
(tseng et al., 2005).

a stanford for multilingual tagging is the universal pos tag set of the universal
dependencies project, which contains 16 tags plus a wide variety of features that
can be added to them to create a large tagset for any language (nivre et al., 2016).

8.8 summary

this chapter introduced parts-of-speech and part-of-speech tagging:

    languages generally have a small set of closed class words that are highly
frequent, ambiguous, and act as function words, and open-class words like
nouns, verbs, adjectives. various part-of-speech tagsets exist, of between 40
and 200 tags.

    part-of-speech tagging is the process of assigning a part-of-speech label to

each of a sequence of words.

    two common approaches to sequence modeling are a generative approach,
id48 tagging, and a discriminative approach, memm tagging. we will see
a third, discriminative neural approach in chapter 9.

    the probabilities in id48 taggers are estimated by maximum likelihood es-
timation on tag-labeled training corpora. the viterbi algorithm is used for
decoding,    nding the most likely tag sequence

    id125 is a variant of viterbi decoding that maintains only a fraction of

high scoring states rather than all states during decoding.

    maximum id178 markov model or memm taggers train logistic regres-
sion models to pick the best tag given an observation word and its context and
the previous tags, and then use viterbi to choose the best sequence of tags.

    modern taggers are generally run bidirectionally.

24 chapter 8

    part-of-speech tagging

bibliographical and historical notes

what is probably the earliest part-of-speech tagger was part of the parser in zellig
harris   s transformations and discourse analysis project (tdap), implemented be-
tween june 1958 and july 1959 at the university of pennsylvania (harris, 1962),
although earlier systems had used part-of-speech dictionaries. tdap used 14 hand-
written rules for part-of-speech disambiguation; the use of part-of-speech tag se-
quences and the relative frequency of tags for a word pre   gures all modern algo-
rithms. the parser was implemented essentially as a cascade of    nite-state trans-
ducers; see joshi and hopely (1999) and karttunen (1999) for a reimplementation.
the computational grammar coder (cgc) of klein and simmons (1963) had
three components: a lexicon, a morphological analyzer, and a context disambiguator.
the small 1500-word lexicon listed only function words and other irregular words.
the morphological analyzer used in   ectional and derivational suf   xes to assign part-
of-speech classes. these were run over words to produce candidate parts-of-speech
which were then disambiguated by a set of 500 context rules by relying on sur-
rounding islands of unambiguous words. for example, one rule said that between an
article and a verb, the only allowable sequences were adj-noun, noun-
adverb, or noun-noun. the taggit tagger (greene and rubin, 1971) used
the same architecture as klein and simmons (1963), with a bigger dictionary and
more tags (87). taggit was applied to the brown corpus and, according to francis
and ku  cera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the
brown corpus was then tagged by hand. all these early algorithms were based on
a two-stage architecture in which a dictionary was    rst used to assign each word a
set of potential parts-of-speech, and then lists of hand-written disambiguation rules
winnowed the set down to a single part-of-speech per word.

soon afterwards probabilistic architectures began to be developed. probabili-
ties were used in tagging by stolz et al. (1965) and a complete probabilistic tagger
with viterbi decoding was sketched by bahl and mercer (1976). the lancaster-
oslo/bergen (lob) corpus, a british english equivalent of the brown corpus, was
tagged in the early 1980   s with the claws tagger (marshall 1983; marshall 1987;
garside 1987), a probabilistic algorithm that approximated a simpli   ed id48 tag-
ger. the algorithm used tag bigram probabilities, but instead of storing the word
likelihood of each tag, the algorithm marked tags either as rare (p(tag|word) < .01)
infrequent (p(tag|word) < .10) or normally frequent (p(tag|word) > .10).
derose (1988) developed a quasi-id48 algorithm, including the use of dy-
namic programming, although computing p(t|w)p(w) instead of p(w|t)p(w). the
same year, the probabilistic parts tagger of church (1988), (1989) was probably
the    rst implemented id48 tagger, described correctly in church (1989), although
church (1988) also described the computation incorrectly as p(t|w)p(w) instead
of p(w|t)p(w). church (p.c.) explained that he had simpli   ed for pedagogical pur-
poses because using the id203 p(t|w) made the idea seem more understandable
as    storing a lexicon in an almost standard form   .

later taggers explicitly introduced the use of the hidden markov model (ku-
piec 1992; weischedel et al. 1993; sch  utze and singer 1994). merialdo (1994)
showed that fully unsupervised em didn   t work well for the tagging task and that
reliance on hand-labeled data was important. charniak et al. (1993) showed the im-
portance of the most frequent tag baseline; the 92.3% number we give above was
from abney et al. (1999). see brants (2000) for many implementation details of an
id48 tagger whose performance is still roughly close to state of the art taggers.

exercises

25

ratnaparkhi (1996) introduced the memm tagger, called mxpost, and the

modern formulation is very much based on his work.

the idea of using letter suf   xes for unknown words is quite old; the early klein
and simmons (1963) system checked all    nal letter suf   xes of lengths 1-5. the
probabilistic formulation we described for id48s comes from samuelsson (1993).
the unknown word features described on page 19 come mainly from (ratnaparkhi,
1996), with augmentations from toutanova et al. (2003) and manning (2011).

state of the art taggers use neural algorithms or (bidirectional) id148
toutanova et al. (2003). id48 (brants 2000; thede and harper 1999) and memm
tagger accuracies are likely just a tad lower.

an alternative modern formalism, the english constraint grammar systems (karls-

son et al. 1995; voutilainen 1995; voutilainen 1999), uses a two-stage formalism
much like the early taggers from the 1950s and 1960s. a morphological analyzer
with tens of thousands of english word stem entries returns all parts-of-speech for a
word, using a large feature-based tagset. so the word occurred is tagged with the op-
tions (cid:104)v pcp2 sv(cid:105) and (cid:104)v past vfin sv(cid:105), meaning it can be a participle (pcp2)
for an intransitive (sv) verb, or a past (past)    nite (vfin) form of an intransitive
(sv) verb. a set of 3,744 constraints are then applied to the input sentence to rule
out parts-of-speech inconsistent with the context. for example here   s a rule for the
ambiguous word that that eliminates all tags except the adv (adverbial intensi   er)
sense (this is the sense in the sentence it isn   t that odd):

adverbial-that rule given input:    that   
if (+1 a/adv/quant); /* if next word is adj, adverb, or quanti   er */

(+2 sent-lim);
(not -1 svoc/a); /* and the previous word is not a verb like */

/* and following which is a sentence boundary, */

then eliminate non-adv tags else eliminate adv tag

/*    consider    which allows adjs as object complements */

manning (2011) investigates the remaining 2.7% of errors in a state-of-the-art
tagger, the bidirectional memm-style model described above (toutanova et al.,
2003). he suggests that a third or half of these remaining errors are due to errors or
inconsistencies in the training data, a third might be solvable with richer linguistic
models, and for the remainder the task is underspeci   ed or unclear.

supervised tagging relies heavily on in-domain training data hand-labeled by
experts. ways to relax this assumption include unsupervised algorithms for cluster-
ing words into part-of-speech-like classes, summarized in christodoulopoulos et al.
(2010), and ways to combine labeled and unlabeled data, for example by co-training
(clark et al. 2003; s  gaard 2010).

see householder (1995) for historical notes on parts-of-speech, and sampson
(1987) and garside et al. (1997) on the provenance of the brown and other tagsets.

exercises

8.1

find one tagging error in each of the following sentences that are tagged with
the id32 tagset:

1. i/prp need/vbp a/dt    ight/nn from/in atlanta/nn
2. does/vbz this/dt    ight/nn serve/vb dinner/nns
3. i/prp have/vb a/dt friend/nn living/vbg in/in denver/nnp
4. can/vbp you/prp list/vb the/dt nonstop/jj afternoon/nn    ights/nns

26 chapter 8

    part-of-speech tagging

8.2 use the id32 tagset to tag each word in the following sentences
from damon runyon   s short stories. you may ignore punctuation. some of
these are quite dif   cult; do your best.

1. it is a nice night.
2. this crap game is over a garage in fifty-second street. . .
3.
4. he is a tall, skinny guy with a long, sad, mean-looking kisser, and a

. . . nobody ever takes the newspapers she sells . . .

5.

mournful voice.
. . . i am sitting in mindy   s restaurant putting on the ge   llte    sh, which is
a dish i am very fond of, . . .

6. when a guy and a doll get to taking peeks back and forth at each other,

why there you are indeed.

8.3 now compare your tags from the previous exercise with one or two friend   s

8.4

answers. on which words did you disagree the most? why?
implement the    most likely tag    baseline. find a pos-tagged training set,
and use it to compute for each word the tag that maximizes p(t|w). you will
need to implement a simple tokenizer to deal with sentence boundaries. start
by assuming that all unknown words are nn and compute your error rate on
known and unknown words. now write at least    ve rules to do a better job of
tagging unknown words, and show the difference in error rates.

8.5 build a bigram id48 tagger. you will need a part-of-speech-tagged corpus.
first split the corpus into a training set and test set. from the labeled training
set, train the transition and observation probabilities of the id48 tagger di-
rectly on the hand-tagged data. then implement the viterbi algorithm so that
you can label an arbitrary test sentence. now run your algorithm on the test
set. report its error rate and compare its performance to the most frequent tag
baseline.

8.6 do an error analysis of your tagger. build a confusion matrix and investigate
the most frequent errors. propose some features for improving the perfor-
mance of your tagger on these errors.

abney, s. p., schapire, r. e., and singer, y. (1999). boosting
applied to tagging and pp attachment. in emnlp/vlc-99,
college park, md, pp. 38   45.

bahl, l. r. and mercer, r. l. (1976). part of speech assign-
in proceedings
ment by a statistical decision algorithm.
ieee international symposium on id205, pp.
88   89.

brants, t. (2000). tnt: a statistical part-of-speech tagger.

in anlp 2000, seattle, wa, pp. 224   231.

broschart, j. (1997). why tongan does it differently. lin-

guistic typology, 1, 123   165.

charniak, e., hendrickson, c., jacobson, n., and perkowitz,
m. (1993). equations for part-of-speech tagging. in aaai-
93, washington, d.c., pp. 784   789. aaai press.

christodoulopoulos, c., goldwater, s., and steedman, m.
(2010). two decades of unsupervised pos induction: how
far have we come?. in emnlp-10.

church, k. w. (1988). a stochastic parts program and noun
phrase parser for unrestricted text. in anlp 1988, pp. 136   
143.

church, k. w. (1989). a stochastic parts program and noun
phrase parser for unrestricted text. in icassp-89, pp. 695   
698.

clark, s., curran, j. r., and osborne, m. (2003). bootstrap-
ping pos taggers using unlabelled data. in conll-03, pp.
49   55.

derose, s. j. (1988). grammatical category disambiguation
by statistical optimization. computational linguistics, 14,
31   39.

evans, n. (2000). word classes in the world   s languages. in
booij, g., lehmann, c., and mugdan, j. (eds.), morphol-
ogy: a handbook on in   ection and word formation, pp.
708   732. mouton.

francis, w. n. and ku  cera, h. (1982). frequency analysis

of english usage. houghton mif   in, boston.

garside, r. (1987). the claws word-tagging system. in
garside, r., leech, g., and sampson, g. (eds.), the com-
putational analysis of english, pp. 30   41. longman.

garside, r., leech, g., and mcenery, a. (1997). corpus

annotation. longman.

gil, d. (2000). syntactic categories, cross-linguistic varia-
tion and universal grammar. in vogel, p. m. and comrie,
b. (eds.), approaches to the typology of word classes, pp.
173   216. mouton.

greene, b. b. and rubin, g. m. (1971). automatic grammat-
ical tagging of english. department of linguistics, brown
university, providence, rhode island.

haji  c, j. (2000). morphological tagging: data vs. dictionar-

ies. in naacl 2000. seattle.

hakkani-t  ur, d., o   azer, k., and t  ur, g. (2002). statistical
id60 for agglutinative languages.
journal of computers and humanities, 36(4), 381   410.

harris, z. s. (1962). string analysis of sentence structure.

mouton, the hague.

householder, f. w. (1995). dionysius thrax, the technai,
and sextus empiricus. in koerner, e. f. k. and asher, r. e.
(eds.), concise history of the language sciences, pp. 99   
103. elsevier science.

exercises

27

jelinek, f. and mercer, r. l. (1980). interpolated estimation
of markov source parameters from sparse data. in gelsema,
e. s. and kanal, l. n. (eds.), proceedings, workshop on
pattern recognition in practice, pp. 381   397. north hol-
land.

joshi, a. k. and hopely, p. (1999). a parser from antiq-
uity. in kornai, a. (ed.), extended finite state models of
language, pp. 6   15. cambridge university press.

karlsson, f., voutilainen, a., heikkil  a, j., and anttila,
a. (eds.). (1995). constraint grammar: a language-
independent system for parsing unrestricted text. mouton
de gruyter.

karttunen, l. (1999). comments on joshi.

in kornai, a.
(ed.), extended finite state models of language, pp. 16   
18. cambridge university press.

klein, s. and simmons, r. f. (1963). a computational ap-
proach to grammatical coding of english words. journal of
the association for computing machinery, 10(3), 334   347.
kupiec, j. (1992). robust part-of-speech tagging using a
hidden markov model. computer speech and language,
6, 225   242.

lafferty, j. d., mccallum, a., and pereira, f. c. n. (2001).
conditional random    elds: probabilistic models for seg-
menting and labeling sequence data. in icml 2001, stan-
ford, ca.

manning, c. d. (2011). part-of-speech tagging from 97% to
100%: is it time for some linguistics?. in cicling 2011,
pp. 171   189.

marcus, m. p., santorini, b., and marcinkiewicz, m. a.
(1993). building a large annotated corpus of english: the
id32. computational linguistics, 19(2), 313   
330.

marshall, i. (1983). choice of grammatical word-class with-
out global syntactic analysis: tagging words in the lob
corpus. computers and the humanities, 17, 139   150.

marshall, i. (1987). tag selection using probabilistic meth-
ods. in garside, r., leech, g., and sampson, g. (eds.), the
computational analysis of english, pp. 42   56. longman.
merialdo, b. (1994). tagging english text with a probabilis-

tic model. computational linguistics, 20(2), 155   172.

nivre, j., de marneffe, m.-c., ginter, f., goldberg, y., haji  c,
j., manning, c. d., mcdonald, r. t., petrov, s., pyysalo,
s., silveira, n., tsarfaty, r., and zeman, d. (2016). uni-
versal dependencies v1: a multilingual treebank collec-
tion. in lrec.

oravecz, c. and dienes, p. (2002). ef   cient stochastic part-
of-speech tagging for hungarian. in lrec-02, las palmas,
canary islands, spain, pp. 710   717.

ratnaparkhi, a. (1996). a maximum id178 part-of-speech

tagger. in emnlp 1996, philadelphia, pa, pp. 133   142.

sampson, g. (1987). alternative grammatical coding sys-
in garside, r., leech, g., and sampson, g.
tems.
(eds.), the computational analysis of english, pp. 165   
183. longman.

samuelsson, c. (1993). morphological tagging based en-
tirely on bayesian id136. in 9th nordic conference on
computational linguistics nodalida-93. stockholm.

sch  utze, h. and singer, y. (1994). part-of-speech tagging
using a variable memory markov model. in acl-94, las
cruces, nm, pp. 181   187.

28 chapter 8     part-of-speech tagging

s  gaard, a. (2010). simple semi-supervised training of part-

of-speech taggers. in acl 2010, pp. 205   208.

stolz, w. s., tannenbaum, p. h., and carstensen, f. v.
(1965). a stochastic approach to the grammatical coding
of english. communications of the acm, 8(6), 399   405.

thede, s. m. and harper, m. p. (1999). a second-order hid-
den markov model for part-of-speech tagging. in acl-99,
college park, ma, pp. 175   182.

toutanova, k., klein, d., manning, c. d., and singer, y.
(2003). feature-rich part-of-speech tagging with a cyclic
dependency network. in hlt-naacl-03.

tseng, h., jurafsky, d., and manning, c. d. (2005). mor-
phological features help id52 of unknown words
in proceedings of the 4th
across language varieties.
sighan workshop on chinese language processing.

voutilainen, a. (1995). id60. in
karlsson, f., voutilainen, a., heikkil  a, j., and anttila,
a. (eds.), constraint grammar: a language-independent
system for parsing unrestricted text, pp. 165   284. mouton
de gruyter.

voutilainen, a. (1999). handcrafted rules.

in van hal-
teren, h. (ed.), syntactic wordclass tagging, pp. 217   246.
kluwer.

weischedel, r., meteer, m., schwartz, r., ramshaw, l. a.,
and palmucci, j. (1993). coping with ambiguity and un-
known words through probabilistic models. computational
linguistics, 19(2), 359   382.

