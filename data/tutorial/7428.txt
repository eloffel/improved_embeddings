speech and language processing. daniel jurafsky & james h. martin.
rights reserved.

draft of september 23, 2018.

copyright c(cid:13) 2018.

all

chapter

9 sequence

recurrent networks

processing with

time will explain.
jane austin, persuasion

in chapter 7, we explored feedforward neural networks along with their applications
to neural language models and text classi   cation. in the case of language models,
we saw that such networks can be trained to make predictions about the next word in
a sequence given a limited context of preceding words     an approach that is remi-
niscent of the markov approach to id38 discussed in chapter 3. these
models operated by accepting a small    xed-sized window of tokens as input; longer
sequences are processed by sliding this window over the input making incremental
predictions, with the end result being a sequence of predictions spanning the input.
fig. 9.1, reproduced here from chapter 7, illustrates this approach with a window
of size 3. here, we   re predicting which word will come next given the window the
ground there. subsequent words are predicted by sliding the window forward one
word at a time.

unfortunately, the sliding window approach is problematic for a number of rea-
sons. first, it shares the primary weakness of markov approaches in that it limits
the context from which information can be extracted; anything outside the context
window has no impact on the decision being made. this is problematic since there
are many language tasks that require access to information that can be arbitrarily dis-
tant from the point at which processing is happening. second, the use of windows
makes it dif   cult for networks to learn systematic patterns arising from phenomena
like constituency. for example, in fig. 9.1 the phrase the ground appears twice in
different windows: once, as shown, in the    rst and second positions in the window,
and in in the preceding step in the second and third slots, thus forcing the network
to learn two separate patterns for a single constituent.

the subject of this chapter is recurrent neural networks, a class of networks
designed to address these problems by processing sequences explicitly as sequences,
allowing us to handle variable length inputs without the use of arbitrary    xed-sized
windows.

9.1 simple recurrent networks

a recurrent neural network is any network that contains is a cycle within its network
connections. that is, any network where the value of a unit is directly, or indirectly,
dependent on its own output as an input. in general, such networks are dif   cult to
reason about, and to train. however, within the general class of recurrent networks

2 chapter 9

    sequence processing with recurrent networks

figure 9.1 a simpli   ed view of a feedforward neural language model moving through a text. at each
timestep t the network takes the 3 context words, converts each to a d-dimensional embeddings, and con-
catenates the 3 embeddings together to get the 1   nd unit input layer x for the network.

simple
recurrent
networks
elman
networks

there are constrained architectures that have proven to be extremely useful when
applied to language problems. in this section, we   ll introduce a class of recurrent
networks referred to as simple recurrent networks (srns) or elman networks
(elman, 1990). these networks are useful in their own right, and will serve as the
basis for more complex approaches to be discussed later in this chapter and again in
chapter 22.

fig. 9.2 abstractly illustrates the recurrent structure of an srn. as with ordinary
feed-forward networks, an input vector representing the current input element, xt,
is multiplied by a weight matrix and then passed through an activation function to
compute an activation value for a layer of hidden of units. this hidden layer is,
in turn, used to calculate a corresponding output, yt. sequences are processed by

figure 9.2 simple recurrent neural network after elman (elman, 1990). the hidden layer
includes a recurrent connection as part of its input. that is, the activation value of the hidden
layer depends on the current input as well as the activation value of the hidden layer from the
previous timestep.

h1h2y1h3hdh      uwy42y|v|projection layer1   3dconcatenated embeddingsfor context wordshidden layeroutput layer p(w|u)   inthehole......groundtherelivedword 42embedding forword 35embedding for word 9925embedding for word 45180wt-1wt-2wtwt-3dh   3d1   dh|v|   dhp(wt=v42|wt-3,wt-2,wt-3)1   |v|htytxt9.1

    simple recurrent networks

3

figure 9.3 simple recurrent neural network illustrated as a feed-forward network.

presenting one element at a time to the network. the key difference from a feed-
forward network lies in the recurrent link shown in the    gure with the dashed line.
this link augments the input to the hidden layer with the activation value of the
hidden layer from the preceding point in time.

the hidden layer from the previous timestep provides a form of memory, or
context, that encodes earlier processing and informs the decisions to be made at later
points in time. importantly, the architecture does not impose a    xed-length limit
on this prior context; the context embodied in the previous hidden layer includes
information extending back to the beginning of the sequence.

adding this temporal dimension makes recurrent networks appear to be more
exotic than non-recurrent architectures. but in reality, they   re not all that different.
given an input vector and the values for the hidden layer from the previous time
step, we   re still performing the standard feed-forward calculation. to see this, con-
sider fig. 9.3 which clari   es the nature of the recurrence and how it factors into the
computation at the hidden layer. the most signi   cant addition lies in the new set of
weights, u, that connect the hidden layer from the previous timestep to the current
hidden layer. these weights determine how the network should make use of past
context in calculating the output for the current input. as with the other weights in
the network, these connections will be trained via id26.

id136 in simple id56s

9.1.1
forward id136 (mapping a sequence of inputs to a sequence of outputs) in an
srn is nearly identical to what we   ve already seen with feedforward networks. to
compute an output yt for an input xt, we need the activation value for the hidden
layer ht. to calculate this, we compute the dot product of the input xt with the weight
matrix w , and the dot product of the hidden layer from the previous time step ht   1
with the weight matrix u. we add these values together and pass them through a
suitable activation function, g, to arrive at the activation value for the current hidden
layer, ht. once we have the values for the hidden layer, we proceed with the usual
computation to generate the output vector.

ht = g(uht   1 +w xt )
yt = f (v ht )

in the commonly encountered case of soft classi   cation,    nding yt consists of
a softmax computation that provides a normalized id203 distribution over the

uvwytxththt-14 chapter 9

    sequence processing with recurrent networks

figure 9.4 a simple recurrent neural network shown unrolled in time. network layers are copied for each
timestep, while the weights u, v and w are shared in common across all timesteps.

possible output classes.

yt = softmax(v ht )

the sequential nature of simple recurrent networks can be illustrated by un-
rolling the network in time as is shown in fig. 9.4.
in    gures such as this, the
various layers of units are copied for each time step to illustrate that they will have
differing values over time. however the weights themselves are shared across the
various timesteps. finally, the fact that the computation at time t requires the value
of the hidden layer from time t   1 mandates an incremental id136 algorithm that
proceeds from the start of the sequence to the end as shown in fig. 9.5.

function forwardid56(x, network) returns output sequence y

h0   0
for i   1 to length(x) do
hi   g(u hi   1 + w xi)
yi    f (v hi)

return y

figure 9.5 forward id136 in a simple recurrent network.

9.1.2 training
as we did with feed-forward networks, we   ll use a training set, a id168, and
id26 to adjust the sets of weights in these recurrent networks. as shown
in fig. 9.3, we now have 3 sets of weights to update: w , the weights from the input

uvwuvwuvwx1x2x3y1y2y3h1h3h2h09.1

    simple recurrent networks

5

layer to the hidden layer, u, the weights from the previous hidden layer to the current
hidden layer, and    nally v , the weights from the hidden layer to the output layer.

before going on, let   s    rst review some of the notation that we introduced in
chapter 7. assuming a network with an input layer x and a non-linear activation
function g, we   ll use a[i] to refer to the activation value from a layer i, which is the
result of applying g to z[i], the weighted sum of the inputs to that layer. a simple
two-layer feedforward network with w and v as the    rst and second sets of weights
respectively, would be characterized as follows.
z[1] = w x
a[1] = g(z[1])
z[2] = ua[1]
a[2] = g(z[2])

y = a[2]

fig. 9.4 illustrates the two considerations that we didn   t have to worry about with
id26 in feed-forward networks. first, to compute the id168 for
the output at time t we need the hidden layer from time t     1. second, the hidden
layer at time t in   uences both the output at time t and the hidden layer at time t + 1
(and hence the output and loss at t + 1). it follows from this that to assess the error
accruing to ht, we   ll need to know its in   uence on both the current output as well as
the next one.

consider the situation where we are examining an input/output pair at time 2 as
shown in fig. 9.4. what do we need to compute the gradients needed to update the
weights u, v , and w here? let   s start by reviewing how we compute the gradients
required to update v (this computation is unchanged from feed-forward networks).
to review from chapter 7, we need to compute the derivative of the id168 l
with respect to the weights v . however, since the loss is not expressed directly in
terms of the weights, we apply the chain rule to get there indirectly.

    l
   v =

    l
    a

    a
    z

    z
   v

the    rst term is just the derivative of the id168 with respect to the network
output, which is just the activation of the output layer, a. the second term is the
derivative of the network output with respect to the intermediate network activation
z, which is a function of the activation function g. the    nal term in our application of
the chain rule is the derivative of the network activation with respect to the weights
v , which is just the activation value of the current hidden layer ht.

it   s useful here to use the    rst two terms to de   ne    , an error term that represents

how much of the scalar loss is attributable to each of the units in the output layer.

    l
  out =
    a
  out = l(cid:48)g(cid:48)

    a
    z
(z)

therefore, the    nal gradient we need to update the weight matrix v is just:

    l
   v =   outht

(9.1)

(9.2)

(9.3)

moving on, we need to compute the corresponding gradients for the weight ma-
   u . here we encounter the    rst substantive change from

   w and     l

trices w and u:     l

6 chapter 9

    sequence processing with recurrent networks

figure 9.6 the id26 of errors in an srn. the ti vectors represent the targets for each element
of the sequence from the training data. the red arrows illustrate the    ow of backpropagated errors required to
calculate the updates for u, v and w at time 2. the two incoming arrows converging on h2 signal that these
errors need to be summed.

feed-forward networks. the hidden state at time t contributes to the output and asso-
ciated error at time t and to the output and error at the next timestep, t +1. therefore,
the error term,   h, for the hidden layer must be the sum of the error term from the
current output and its error from the next time step.

  h = g(cid:48)

(z)v  out +   next

given this total error term for the hidden layer, we can compute the gradients for

the weights u and w in the usual way using the chain rule as we did in chapter 7.

dl
dw =
dl
du =

dl
dz
dl
dz

dz
da
dz
da

da
dw
da
du

    l
   w =   hxt
    l
   u =   hht   1

these gradients provide us with the information needed to update the matrices u
and w through ordinary id26.

we   re not quite done yet, we still need to assign proportional blame (compute
the error term) back to the previous hidden layer ht   1 for use in further processing.

uvwuvwuvwx1x2x3y1y2y3h1h3h2h0t1t2t39.1

    simple recurrent networks

7

function backpropthroughtime(sequence, network) returns gradients for weight
updates
forward pass to gather the loss
backward pass compute error terms and assess blame

figure 9.7 id26 training through time. the forward pass computes the re-
quired loss values at each time step. the backward pass computes the gradients using the
values from the forward pass.

this involves backpropagating the error from   h to ht   1 proportionally based on the
weights in u.

  next = g(cid:48)

(z)u  h

(9.4)

at this point we have all the gradients needed to perform weight updates for each
of our three sets of weights. note that in this simple case there is no need to back-
propagate the error through w to the input x, since the input training data is assumed
to be    xed. if we wished to update our input word or character embeddings we
would backpropagate the error through to them as well. we   ll discuss this more in
section 9.5.

taken together, all of these considerations lead to a two-pass algorithm for train-
ing the weights in srns. in the    rst pass, we perform forward id136, computing
ht, yt, and an loss at each step in time, saving the value of the hidden layer at each
step for use at the next time step. in the second phase, we process the sequence
in reverse, computing the required error terms gradients as we go, computing and
saving the error term for use in the hidden layer for each step backward.

unfortunately, computing the gradients and updating weights for each item of a
sequence individually would be extremely time-consuming. instead, much as we did
with mini-batch training in chapter 7, we will accumulate gradients for the weights
incrementally over the sequence, and then use those accumulated gradients in per-
forming weight updates.

9.1.3 unrolled networks as computational graphs
we used the unrolled network shown in fig. 9.4 as a way to understand the dynamic
behavior of these networks over time. however, with modern computational frame-
works and adequate computing resources, explicitly unrolling a recurrent network
into a deep feed-forward computational graph is quite practical for word-by-word
approaches to sentence-level processing. in such an approach, we provide a tem-
plate that speci   es the basic structure of the srn, including all the necessary pa-
rameters for the input, output, and hidden layers, the weight matrices, as well as the
activation and output functions to be used. then, when provided with an input se-
quence such as a training sentence, we can compile a feed-forward graph speci   c to
that input, and use that graph to perform forward id136 or training via ordinary
id26.

for applications that involve much longer input sequences, such as speech recog-
nition, character-by-character sentence processing, or streaming of continuous in-
puts, unrolling an entire input sequence may not be feasible. in these cases, we can
unroll the input into manageable    xed-length segments and treat each segment as a
distinct training item. this approach is called truncated id26 through
time (tbtt).

8 chapter 9

    sequence processing with recurrent networks

figure 9.8 part-of-speech tagging as sequence labeling with a simple id56. pre-trained
id27s serve as inputs and a softmax layer provides a id203 distribution over
the part-of-speech tags as output at each time step.

9.2 applications of id56s

simple recurrent networks have proven to be an effective approach to language mod-
eling, sequence labeling tasks such as part-of-speech tagging, as well as sequence
classi   cation tasks such as id31 and topic classi   cation. and as we   ll
see in chapter 22, they form the basic building blocks for sequence to sequence
approaches to applications such as summarization and machine translation.

9.2.1 generation with neural language models
[coming soon]

9.2.2 sequence labeling
in sequence labeling, the network   s job is to assign a label to each element of a
sequence chosen from a small    xed set of labels. the canonical example of such a
task is part-of-speech tagging, discussed in chapter 8. in a recurrent network-based
approach to id52, inputs are words and the outputs are tag probabilities
generated by a softmax layer over the pos tagset, as illustrated in fig. 9.8.

in this    gure, the inputs at each time step are pre-trained id27s cor-
responding to the input tokens. the id56 block is an abstraction that represents
an unrolled simple recurrent network consisting of an input layer, hidden layer, and
output layer at each time step, as well as the shared u, v and w weight matrices that
comprise the network. the outputs of the network at each time step represent the
distribution over the pos tagset generated by a softmax layer. to generate an actual
tag sequence as output, we can run forward id136 over the input sequence and
select the most likely tag from the softmax at each step. since we   re using a softmax
layer to generate the id203 distribution over the output tagset at each timestep,
we   ll rely on the cross id178 loss introduced in chapter 7 to train the network.

a closely related, and extremely useful, application of sequence labeling is to
   nd and classify spans of text corresponding to items of interest in some task do-
main. an example of such a task is id39     the problem of

named entity
recognition

janetwillbackid56thebill9.2

    applications of id56s

9

   nding all the spans in a text that correspond to names of people, places or organi-
zations (a problem we   ll study in gory detail in chapter 17).

to turn a problem like this into a per-word sequence labeling task, we   ll use a
technique called iob encoding (ramshaw and marcus, 1995). in its simplest form,
we   ll label any token that begins a span of interest with the label b, tokens that occur
inside a span are tagged with an i, and any tokens outside of any span of interest are
labeled o. consider the following example:
(9.5) united

cancelled
o

the
o

   ight
o

from
o

denver
b

to
o

san
b

francisco.
i

b

here, the spans of interest are united, denver and san francisco.

in applications where we are interested in more than one class of entity (e.g.,
   nding and distinguishing names of people, locations, or organizations), we can
specialize the b and i tags to represent each of the more speci   c classes, thus ex-
panding the tagset from 3 tags to 2    n + 1 where n is the number of classes we   re
interested in.
(9.6) united
b-org

francisco.
i-loc

cancelled
o

denver
b-loc

san
b-loc

   ight
o

from
o

the
o

to
o

with such an encoding, the inputs are the usual id27s and the output
consistes of a sequence of softmax distributions over the tags at each point in the
sequence.

9.2.3 viterbi and id49 (crfs)
as we saw with applying id28 to part-of-speech tagging, choosing the
maximum id203 label for each element in a sequence does not necessarily re-
sult in an optimal (or even very good) tag sequence. in the case of iob tagging, it
doesn   t even guarantee that the resulting sequence will be well-formed. for exam-
ple, nothing in approach described in the last section prevents an output sequence
from containing an i following an o, even though such a transition is illegal. simi-
larly, when dealing with multiple classes nothing would prevent an i-loc tag from
following a b-per tag.

a simple solution to this problem is to use combine the sequence of id203
distributions provided by the softmax outputs with a tag-level language model as we
did with memms in chapter 8. thereby allowing the use of the viterbi algorithm
to select the most likely tag sequence.
[or a crf layer... coming soon]

9.2.4 id56s for sequence classi   cation
another use of id56s is to classify entire sequences rather than the tokens within
a sequence. we   ve already encountered this task in chapter 4 with our discussion
of id31. other examples include document-level topic classi   cation,
spam detection, message routing for customer service applications, and deception
detection. in all of these applications, sequences of text are classi   ed as belonging
to one of a small number of categories.

to apply id56s in this setting, the hidden layer from the    nal state of the network
is taken to constitute a compressed representation of the entire sequence. this com-
pressed sequence representation can then in turn serve as the input to a feed-forward
network trained to select the correct class. fig. 9.10 illustrates this approach.

10 chapter 9

    sequence processing with recurrent networks

figure 9.9 sequence classi   cation using a simple id56 combined with a feedforward net-
work.

note that in this approach, there are no intermediate outputs for the items in the
sequence preceding the last element, and therefore there are no loss terms associ-
ated with those individual items. instead, the loss used to train the network weights
is based on the loss from the    nal classi   cation task. speci   cally, we use the output
from the softmax layer from the    nal classi   er along with a cross-id178 loss func-
tion to drive our network training. the loss is backpropagated all the way through
the weights in the feedforward classi   er through to its input, and then through to the
three sets of weights in the id56 as described earlier in section 9.1.2. this combina-
tion of a simple recurrent network with a feedforward classi   er is our    rst example
of a deep neural network.

9.3 deep networks: stacked and bidirectional id56s

as suggested by the sequence classi   cation architecture shown in fig. 9.9, recur-
rent networks are in fact quite    exible. combining the feedforward nature of un-
rolled computational graphs with vectors as common inputs and outputs, complex
networks can be treated as modules that can be combined in creative ways. this
section introduces two of the more common network architectures used in language
processing with id56s.

9.3.1 stacked id56s
in our examples thus far, the inputs to our id56s have consisted of sequences of
word or character embeddings (vectors) and the outputs have been vectors useful for
predicting words, tags or sequence labels. however, nothing prevents us from using
the entire sequence of outputs from one id56 as an input sequence to another one.
stacked id56s consist of multiple networks where the output of one layer serves as
the input to a subsequent layer, as shown in fig. 9.10.

it has been demonstrated across numerous tasks that stacked id56s can outper-

stacked id56s

x1x2x3xnid56hnsoftmax9.3

    deep networks: stacked and bidirectional id56s

11

figure 9.10 stacked recurrent networks. the output of a lower level serves as the input to
higher levels with the output of the last network serving as the    nal output.

form single-layer networks. one reason for this success has to do with the networks
ability to induce representations at differing levels of abstraction across layers. just
as the early stages of the human visual system detects edges that are then used for
   nding larger regions and shapes, the initial layers of stacked networks can induce
representations that serve as useful abstractions for further layers     representations
that might prove dif   cult to induce in a single id56.

9.3.2 bidirectional id56s
in an simple recurrent network, the hidden state at a given time t represents every-
thing the network knows about the sequence up to that point in the sequence. that
is, the hidden state at time t is the result of a function of the inputs from the start up
through time t. we can think of this as the context of the network to the left of the
current time.

h f orward
t

= srnf orward(x1 : xt )

where h f orward
everything the network has gleaned from the sequence to that point.

corresponds to the normal hidden state at time t, and represents

t

of course, in text-based applications we have access to the entire input sequence
all at once. we might ask whether its helpful to take advantage of the context to
the right of the current input as well. one way to recover such information is to
train a recurrent network on an input sequence in reverse, using the same kind of
network that we   ve been discussing. with this approach, the hidden state at time t
now represents information about the sequence to the right of the current input.

hbackward
t

= srnbackward(xn : xt )

bidirectional
id56

here, the hidden state hbackward
about the sequence from t to the end of the sequence.

t

represents all the information we have discerned

putting these networks together results in a bidirectional id56. a bi-id56 con-
sists of two independent recurrent networks, one where the input is processed from

y1y2y3ynx1x2x3xnid56 1id56 3id56 212 chapter 9

    sequence processing with recurrent networks

figure 9.11 a bidirectional id56. separate models are trained in the forward and backward
directions with the output of each model at each time point concatenated to represent the state
of affairs at that point in time. the box wrapped around the forward and backward network
emphasizes the modular nature of this architecture.

the start to the end, and the other from the end to the start. we can then combine the
outputs of the two networks into a single representation that captures the both the
left and right contexts of an input at each point in time.
    hbackward

ht = h f orward

(9.7)

t

t

fig. 9.11 illustrates a bidirectional network where the outputs of the forward and
backward pass are concatenated. other simple ways to combine the forward and
backward contexts include element-wise addition or multiplication. the output at
each step in time thus captures information to the left and to the right of the current
input. in sequence labeling applications, these concatenated outputs can serve as the
basis for a local labeling decision.

bidirectional id56s have also proven to be quite effective for sequence classi-
   cation. recall from fig. 9.10, that for sequence classi   cation we used the    nal
hidden state of the id56 as the input to a subsequent feedforward classi   er. a dif-
   culty with this approach is that the    nal state naturally re   ects more information
about the end of the sentence than its beginning. bidirectional id56s provide a
simple solution to this problem; as shown in fig. 9.12, we simply combine the    nal
hidden states from the forward and backward passes and use that as input for follow-
on processing. again, concatenation is a common approach to combining the two
outputs but element-wise summation, multiplication or averaging are also used.

9.4 managing context in id56s: lstms and grus

in practice, it is quite dif   cult to train simple id56s for tasks that require a network
to make use of information distant from the current point of processing. despite hav-
ing access to the entire preceding sequence, the information encoded in hidden states
tends to be fairly local, more relevant to the most recent parts of the input sequence
and recent decisions. however, it is often the case that long-distance information is
critical to many language applications.

y1x1x2x3xnid56 1 (left to right)id56 2 (right to left)+y2+y3+yn+9.4

    managing context in id56s: lstms and grus

13

figure 9.12 a bidirectional id56 for sequence classi   cation. the    nal hidden units from
the forward and backward passes are combined to represent the entire sequence. this com-
bined representation serves as input to the subsequent classi   er.

consider the following example in the context of language models.

(9.8) the    ights the airline was cancelling were full.
assigning a high id203 to was following airline is straightforward since was
provides a strong local context for the singular agreement. however, assigning an
appropriate id203 to were is quite dif   cult, not only because the plural    ights
is quite distant, but also because the more recent context contains singular con-
stituents. ideally, a network should be able to retain the distant information about
plural    ights until it is needed, all the while processing intermediate parts of the
sequence correctly.

one reason for the inability of srns to carry forward critical information is that
the hidden layer in srns, and, by extension, the weights that determine the values
in the hidden layer, are being asked to perform two tasks simultaneously: provide
information useful to the decision being made in the current context, and updating
and carrying forward information useful for future decisions.

a second dif   culty to successfully training simple recurrent networks arises
from the need to backpropagate training error back in time through the hidden lay-
ers. recall from section 9.1.2 that the hidden layer at time t contributes to the loss
at the next time step since it takes part in that calculation. as a result, during the
backward pass of training, the hidden layers are subject to repeated dot products, as
determined by the length of the sequence. a frequent result of this process is that
the gradients are either driven to zero or saturate. situations that are referred to as
vanishing gradients or exploding gradients, respectively.

to address these issues more complex network architectures have been designed
to explicitly manage the task of maintaining contextual information over time. these
approaches treat context as a kind of memory unit that needs to be managed explic-
itly. more speci   cally, the network needs to forget information that is no longer
needed and to remember information as needed for later decisions.

x1x2x3xnid56 1 (left to right)id56 2 (right to left)+hn_forwh1_backsoftmax14 chapter 9

    sequence processing with recurrent networks

figure 9.13 a single lstm memory unit displayed as a computation graph.

9.4.1 long short-term memory

long short-term memory (lstm) networks, divide the context management prob-
lem into two sub-problems: removing information no longer needed from the con-
text, and adding information likely to be needed for later decision making. the key
to the approach is to learn how to manage this context rather than hard-coding a
strategy into the architecture.

lstms accomplish this through the use of specialized neural units that make use
of gates that control the    ow of information into and out of the units that comprise
the network layers. these gates are implemented through the use of additional sets
of weights that operate sequentially on the context layer.

gt = tanh(ught   1 +wgxt )
it =    (uiht   1 +wixt )
ft =    (uf ht   1 +wf xt )
ot =    (uoht   1 +woxt )
ct = ft (cid:12) ct   1 + it (cid:12) gt
ht = ot (cid:12)tanh(ct )

[more on this]

+ost-1ifgx tht-1yyystht9.4

    managing context in id56s: lstms and grus

15

figure 9.14 basic neural units used in feed-forward, simple recurrent networks (srn),
long short-term memory (lstm) and gate recurrent units.

9.4.2 id149

while relatively easy to deploy, lstms introduce a considerable number of param-
eters to our networks, and hence carry a much larger training burden. gated recur-
rent units (grus) try to ease this burden by collapsing the forget and add gates of
lstms into a single update gate with a single set of weights.

[coming soon]

9.4.3 gated units, layers and networks

the neural units used in lstms and grus are obviously much more complex than
basic feed-forward networks. fortunately, this complexity is largely encapsulated
within the basic processing units, allowing us to maintain modularity and to eas-
ily experiment with different architectures. to see this, consider fig. 9.14 which
illustrates the inputs/outputs and weights associated with each kind of unit.

at the far left, (a) is the basic feed-forward unit h = g(w x + b). a single set of
weights and a single activation function determine its output, and when arranged in
a layer there is no connection between the units in the layer. next, (b) represents the
unit in an srn. now there are two inputs and additional set of weights to go with it.
however, there is still a single activation function and output. when arranged as a
layer the hidden layer from each unit feeds in as an input to the next.

fortunately, the increased complexity of the lstm and gru units is encapsu-
lated within the units themselves. the only additional external complexity over the
basic recurrent unit (b) is the presence of the additional context vector input and out-
put. this modularity is key to the power and widespread applicability of lstm and
gru units. speci   cally, lstm and gru units can be substituted into any of the
network architectures described in section 9.3. and, as with srns, multi-layered
networks making use of gated units can be unrolled into deep feed-forward networks
and trained in the usual fashion with id26.

hxxtxtht-1hthtct-1ctht-1xthtct-1ctht-1(b)(a)(c)(d)   gza   gzlstmunitgruunita16 chapter 9

    sequence processing with recurrent networks

figure 9.15 sequence labeling id56 that accepts distributional id27s aug-
mented with character-level id27s.

9.5 words, characters and byte-pairs

to this point, we   ve assumed that the inputs to our networks would be either pre-
trained or trained id27s. as we   ve seen, word-based embeddings are
great at    nding distributional (syntactic and semantic) similarity between words.
however, there are signi   cant issues with any solely word-based approach:

    for some languages and applications, the lexicon is simply too large to prac-
tically represent every possible word as an embedding. some means of com-
posing words from smaller bits is needed.
    no matter how large the lexicon, we will always encounter unknown words
due to new words entering the language, misspellings and borrowings from
other languages.
    morphological information, below the word level, is clearly an important
source of information for many applications. word-based methods are blind
to such regularities.

we can overcome some of these issues by augmenting our input word repre-
sentations with embeddings derived from the characters that make up the words.
fig. 9.15 illustrates an approach in the context of part-of-speech tagging. the upper
part of the diagram consists of an id56 that accepts an input sequence and outputs
a softmax distribution over the tags for each element of the input. note that this
id56 can be arbitrarily complex, consisting of stacked and/or bidirectional network
layers.

the inputs to this network consist of ordinary id27s enriched with
character information. speci   cally, each input consists of the concatenation of the
normal id27 with embeddings derived from a bidirectional id56 that
accepts the character sequences for each word as input, as shown in the lower part
of the    gure.

the character sequence for each word in the input is run through a bidirectional
id56 consisting of two independent id56s     one that processes the sequence left-

janetid56bi-id56jante+willbi-id56will+      9.6

    summary

17

figure 9.16 bi-id56 accepts word character sequences and emits embeddings derived
from a forward and backward pass over the sequence. the network itself is trained in the
context of a larger end-application where the loss is propagated all the way through to the
character vector embeddings.

to-right and the other right-to-left. as discussed in section ??, the    nal hidden
states of the left-to-right and right-to-left networks are concatenated to represent the
composite character-level representation of each word. critically, these character
embeddings are trained in the context of the overall task; the loss from the part-of-
speech softmax layer is propagated all the way back to the character embeddings.

[more on byte-pair encoding approach]

9.6 summary

    simple recurrent networks
    id136 and training in srns.
    common use cases for id56s

    id38
    sequence labeling
    sequence classi   cation

    lstms and grus
    characters as inputs

janecharacter projection layerlstm1lstm1lstm1lstm1lstm2lstm2lstm2lstm2right-to-left lstid113ft-to-right lstmtlstm2lstm1concatenationcharacter-level id27character embeddings18 chapter 9     sequence processing with recurrent networks

elman, j. l. (1990). finding structure in time. cognitive

science, 14(2), 179   211.

ramshaw, l. a. and marcus, m. p. (1995). text chunking
using transformation-based learning. in proceedings of the
3rd annual workshop on very large corpora, pp. 82   94.

