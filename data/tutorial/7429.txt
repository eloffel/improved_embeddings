computing with a thesaurus
word senses and word relations
terminology: lemma and wordform
a lemma or citation form
same stem, part of speech, rough semantics
a wordform
the inflected word as it appears in text
lemmas have senses
one lemma    bank    can have many meanings:
   a bank can hold the investments in a custodial account   
      as agriculture burgeons on the east bank the river will shrink even more   
sense (or word sense)
a discrete representation 
                  of an aspect of a word   s meaning.
the lemma bank here has two senses
1
2
sense 1:
sense 2:
homonymy
homonyms: words that share a form but have unrelated, distinct meanings:
bank1: financial institution,    bank2:  sloping land
bat1: club for hitting a ball,    bat2:  nocturnal flying mammal
homographs (bank/bank, bat/bat)
homophones:
write and right
piece and peace
homonymy causes problems for nlp applications
information retrieval
   bat care   
machine translation
bat:  murci  lago  (animal) or  bate (for baseball)
text-to-speech
bass (stringed instrument) vs. bass (fish)

polysemy
1. the bank was constructed in 1875 out of local red brick.
2. i withdrew the money from the bank 
are those the same sense?
sense 2:    a financial institution   
sense 1:    the building belonging to a financial institution   
a polysemous word has related meanings
most non-rare words have multiple meanings

lots of types of polysemy are systematic
school, university, hospital
all can mean the institution or the building.
a systematic relationship:
building            organization
other such kinds of systematic polysemy: 
author (jane austen wrote emma)                 
	works of author (i love jane austen)
tree (plums have beautiful blossoms)    
 	fruit (i ate a preserved plum)
metonymy or systematic polysemy: 
a systematic relationship between senses



how do we know when a word has more than one sense?
the    zeugma    test: two senses of serve?
which flights serve breakfast?
does lufthansa serve philadelphia?
?does lufthansa serve breakfast and san jose?
since this conjunction sounds weird, 
we say that these are two different senses of    serve   
synonyms
word that have the same meaning in some or all contexts.
filbert / hazelnut
couch / sofa
big / large
automobile / car
vomit / throw up
water / h20
two lexemes are synonyms 
if they can be substituted for each other in all situations
if so they have the same propositional meaning
synonyms
but there are few (or no) examples of perfect synonymy.
even if many aspects of meaning are identical
still may not preserve the acceptability based on notions of politeness, slang, register, genre, etc.
example:
water/h20
big/large
brave/courageous
synonymy is a relation 
between senses rather than words
consider the words big and large
are they synonyms?
how big is that plane?
would i be flying on a large or small plane?
how about here:
miss nelson became a kind of big sister to benjamin.
?miss nelson became a kind of large sister to benjamin.
why?
big has a sense that means being older, or grown up
large lacks this sense
antonyms
senses that are opposites with respect to one feature of meaning
otherwise, they are very similar!
dark/light   short/long	fast/slow	rise/fall
hot/cold	    up/down	      in/out
more formally: antonyms can
define a binary opposition
 or be at opposite ends of a scale
 long/short, fast/slow
be reversives:
 rise/fall, up/down
hyponymy and hypernymy
one sense is a hyponym of another if the first sense is more specific, denoting a subclass of the other
car is a hyponym of vehicle
mango is a hyponym of fruit
conversely hypernym/superordinate (   hyper is super   )
vehicle is a hypernym  of car
fruit is a hypernym of mango

hyponymy more formally
extensional:
the class denoted by the superordinate extensionally includes the class denoted by the hyponym
entailment:
a sense a is a hyponym of sense b if being an a entails being a b
hyponymy is usually transitive 
(a hypo b and b hypo c entails a hypo c)
another name: the is-a hierarchy
a is-a b      (or a isa b)
b subsumes a
hyponyms and instances
id138 has both classes and instances.
an instance is an individual, a proper noun that is a unique entity
san francisco is an instance of city
but city is a class
city is a hyponym of    municipality...location...
15
meronymy
the part-whole relation
a leg is part of a chair; a wheel is part of a car. 
wheel is a meronym of car, and car is a holonym of wheel. 

16
computing with a thesaurus
word senses and word relations
computing with a thesaurus
id138
id138 3.0
a hierarchically organized lexical database
on-line thesaurus + aspects of a dictionary
some other languages available or under development
(arabic, finnish, german, portuguese   )

senses of    bass    in id138
how is    sense    defined in id138?
the synset (synonym set), the set of near-synonyms, instantiates a sense or concept, with a gloss
example: chump as a noun with the gloss:
   a person who is gullible and easy to take advantage of   
this sense of    chump    is shared by 9 words:
chump1, fool2, gull1, mark9, patsy1, fall guy1, sucker1, soft touch1, mug2
each of these senses have this same gloss
(not every sense; sense 2 of gull is the aquatic bird)

id138 hypernym hierarchy for    bass   
id138 noun relations
id138 verbrelations
id138: viewed as a graph
25
   supersenses   
the top level hypernyms in the hierarchy
26
   (counts from schneider and smith 2013   s streusel corpus)
supersenses
a word   s supersense can be a useful coarse-grained representation of word meaning for nlp tasks
27
id138 3.0
where it is:
http://id138web.princeton.edu/perl/webwn
libraries
python:  id138  from nltk
http://www.nltk.org/home
java:
jwnl, extjwnl on sourceforge


other (domain specific) thesauri



synset
mesh (medical subject headings)
177,000 entry terms  that correspond to 26,142 biomedical    headings   

hemoglobins
entry terms:  eryhem, ferrous hemoglobin, hemoglobin
definition:  the oxygen-carrying proteins of erythrocytes. they are found in all vertebrates and some invertebrates. the number of globin subunits in the hemoglobin quaternary structure differs between species. structures range from monomeric to a variety of multimeric arrangements
mesh: medical subject headings
thesaurus from the national library of medicine
the mesh hierarchy
a
31
uses of the mesh ontology
provide synonyms (   entry terms   )
e.g., glucose and dextrose
provide hypernyms (from the hierarchy)
e.g., glucose isa monosaccharide
indexing in medline/pubmed database
nlm   s bibliographic database: 
20 million journal articles
each article hand-assigned 10-20 mesh terms
computing with a thesaurus
id138
computing with a thesaurus
word similarity: thesaurus methods
word similarity
synonymy: a binary relation
two words are either synonymous or not
similarity (or distance): a looser metric
two words are more similar if they share more features of meaning
similarity is properly a relation between senses
the word    bank    is not similar to the word    slope   
bank1 is similar to fund3
bank2 is similar to slope5
but we   ll compute similarity over both words and senses
why word similarity
a practical component in lots of nlp tasks
id53
id86
automatic essay grading
plagiarism detection
a theoretical component in many linguistic and cognitive tasks
historical semantics
models of human word learning
morphology and grammar induction

word similarity and word relatedness
we often distinguish word similarity  from word relatedness
similar words: near-synonyms
related words: can be related any way
car, bicycle:    similar
car, gasoline:   related, not similar
two classes of similarity algorithms
thesaurus-based algorithms
are words    nearby    in hypernym hierarchy?
do words have similar glosses (definitions)?
distributional algorithms
do words have similar distributional contexts?
distributional (vector) semantics on thursday!
path based similarity
two concepts (senses/synsets) are similar if they are near each other in the thesaurus hierarchy 
=have a short path between them
concepts have path 1 to themselves
refinements to path-based similarity
pathlen(c1,c2) = 1 + number of edges in the shortest path in the hypernym graph between sense nodes c1 and c2
ranges from 0 to 1 (identity)

simpath(c1,c2) = 

wordsim(w1,w2) =   max         sim(c1,c2)
c1   senses(w1),c2   senses(w2)
example: path-based similarity
simpath(c1,c2) = 1/pathlen(c1,c2)
simpath(nickel,coin) = 1/2 = .5
simpath(fund,budget) = 1/2 = .5
simpath(nickel,currency) = 1/4 = .25
simpath(nickel,money) = 1/6 = .17
simpath(coinage,richter scale) = 1/6 = .17 



problem with basic path-based similarity
assumes each link represents a uniform distance
but nickel to money seems to us to be closer than nickel to standard
nodes high in the hierarchy are very abstract
we instead want a metric that
represents the cost of each edge independently
words connected only through abstract nodes 
are less similar

information content similarity metrics
let   s define p(c) as:
the id203 that a randomly selected word in a corpus is an instance of concept c
formally: there is a distinct random variable, ranging over words, associated with each concept in the hierarchy
for a given concept, each observed noun is either
 a member of that concept  with id203 p(c)
not a member of that concept with id203 1-p(c)
all words are members of the root node (entity)
p(root)=1
the lower a node in hierarchy, the lower its id203
resnik 1995
information content similarity
train by counting in a corpus
each instance of hill counts toward frequency 
of natural elevation, geological formation, entity, etc
let words(c) be the set of all words that are children of node c
words(   geo-formation   ) = {hill,ridge,grotto,coast,cave,shore,natural elevation}
words(   natural elevation   ) = {hill, ridge}



information content similarity
id138 hierarchy augmented with probabilities p(c)
d. lin. 1998. an information-theoretic definition of similarity. icml 1998
information content and id203
the self-information of an event, also called its surprisal:
how surprised we are to know it; how much we learn by knowing it.
the more surprising something is, the more it tells us when it happens
we   ll measure self-information in bits.
i(w)= -log2 p(w)
i flip a coin; p(heads)= 0.5
how many bits of information do i learn by flipping it?
i(heads) = -log2(0.5) = -log2 (1/2) = log2 (2) = 1 bit
i flip a biased coin: p(heads )= 0.8 i don   t learn as much
i(heads) = -log2(0.8) = -log2(0.8) = .32 bits


46
information content: definitions
information content:
ic(c) = -log p(c)
most informative subsumer (lowest common subsumer)
lcs(c1,c2) = 
the most informative (lowest) node in the hierarchy subsuming both c1 and c2
1.3 bits
5.9 bits
15.7 bits
9.1 bits
using information content for similarity:  the resnik method
the similarity between two words is related to their common information
the more two words have in common, the more similar they are
resnik: measure common information as:
the information content of the most informative
 (lowest) subsumer (mis/lcs) of the two nodes
simresnik(c1,c2) = -log p( lcs(c1,c2) )
philip resnik. 1995. using information content to evaluate semantic similarity in a taxonomy. ijcai 1995.
philip resnik. 1999. semantic similarity in a taxonomy: an information-based measure and its application to problems of ambiguity in natural language. jair 11, 95-130.
dekang lin method
intuition: similarity between a and b is not just what they have in common
the more differences between a and b, the less similar they are:
commonality: the more a and b have in common, the more similar they are
difference: the more differences between a and b, the less similar
commonality: ic(common(a,b))
difference: ic(description(a,b)-ic(common(a,b))
dekang lin. 1998. an information-theoretic definition of similarity. icml
dekang lin similarity theorem
the similarity between a and b is measured by the ratio between the amount of information needed to state the commonality of a and b and the information needed to fully describe what a and b are


lin (altering resnik) defines ic(common(a,b)) as 2 x information of the lcs
lin similarity function
the (extended) lesk algorithm 
a thesaurus-based measure that looks at glosses
two concepts are similar if their glosses contain similar words
drawing paper: paper that is specially prepared for use in drafting
decal: the art of transferring designs from specially prepared paper to a wood or glass or metal surface
for each n-word phrase that   s in both glosses
add a score of n2 
paper and specially prepared for 1 + 22 = 5
compute overlap also for other relations
glosses of hypernyms and hyponyms

summary: thesaurus-based similarity
libraries for computing thesaurus-based similarity
nltk
http://nltk.github.com/api/nltk.corpus.reader.html?highlight=similarity - nltk.corpus.reader.id138corpusreader.res_similarity

id138::similarity
http://wn-similarity.sourceforge.net/
web-based interface:
http://marimba.d.umn.edu/cgi-bin/similarity/similarity.cgi


54
evaluating similarity
extrinsic (task-based, end-to-end) evaluation:
id53
spell checking
essay grading
intrinsic evaluation:
correlation between algorithm and human word similarity ratings
wordsim353: 353 noun pairs rated 0-10.   sim(plane,car)=5.77
taking toefl multiple-choice vocabulary tests
levied is closest in meaning to:
 imposed, believed, requested, correlated

