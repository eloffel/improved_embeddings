dan jurafsky and james martin
speech and language processing

chapter 6:
vector semantics
what do words mean?
first thought: look in a dictionary

http://www.oed.com/



words, lemmas, senses, definitions
sense





definition
lemma pepper
sense 1: spice from pepper plant
sense 2: the pepper plant itself
sense 3: another similar plant (jamaican pepper)
sense 4: another plant with peppercorns (california pepper)
sense 5: capsicum (i.e. chili, paprika, bell pepper, etc)
a sense or    concept    is the meaning component of a word

there are relations between senses

relation: synonymity
synonyms have the same meaning in some or all contexts.
filbert / hazelnut
couch / sofa
big / large
automobile / car
vomit / throw up
water / h20
relation: synonymity
note that there are probably no examples of perfect synonymy.
even if many aspects of meaning are identical
still may not preserve the acceptability based on notions of politeness, slang, register, genre, etc.
the linguistic principle of contrast:
difference in form -> difference in meaning
relation: synonymity?
water/h20
big/large
brave/courageous
relation: antonymy
senses that are opposites with respect to one feature of meaning
otherwise, they are very similar!
dark/light   short/long	fast/slow	rise/fall
hot/cold	    up/down	      in/out
more formally: antonyms can
define a binary opposition
 or be at opposite ends of a scale
 long/short, fast/slow
be reversives:
 rise/fall, up/down
relation: similarity
words with similar meanings.  not synonyms, but sharing some element of meaning

car, bicycle
cow, horse

ask humans how similar 2 words are
siid113x-999 dataset (hill et al., 2015) 
relation: word relatedness
also called "word association"
words be related in any way, perhaps via a semantic frame or field

car, bicycle:    similar
car, gasoline:   related, not similar
semantic field
words that 
cover a particular semantic domain 
bear structured relations with each other. 

hospitals
	surgeon, scalpel, nurse, anaesthetic, hospital
restaurants 
	waiter, menu, plate, food, menu, chef), 
houses
	door, roof, kitchen, family, bed

relation: superordinate/ subordinate
one sense is a subordinate of another if the first sense is more specific, denoting a subclass of the other
car is a subordinate of vehicle
mango is a subordinate of fruit
conversely superordinate
vehicle is a superordinate of car
fruit is a subodinate of mango

these levels are not symmetric
one level of category is distinguished from the others
the "basic level"
name these items

superordinate        basic	           subordinate

			        chair		   office chair 
					               piano chair 					  	            rocking chair	
furniture	           lamp	         torchiere
						    desk lamp
	 		            table 	   end table					  		      coffee table 











cluster of interactional properties
basic level things are    human-sized   
consider chairs
we know how to interact with a chair (sitting)
not so clear for superordinate categories like furniture
   imagine a furniture without thinking of a bed/table/chair/specific basic-level category   

the basic level
is the level of distinctive actions
is the level which is learned earliest and at which things are first named
it is the level at which names are shortest and used most frequently
connotation
words have affective meanings
positive connotations (happy) 
negative connotations (sad)

positive evaluation (great, love) 
negative evaluation (terrible, hate). 

so far
concepts or word senses
have a complex many-to-many association with words (homonymy, multiple senses)
have relations with each other
synonymy
antonymy
similarity
relatedness
superordinate/subordinate
connotation
but how to define a concept?

classical (   aristotelian   ) theory of concepts
the meaning of a word:
a concept defined by necessary and sufficient conditions
a necessary condition for being an x is a condition c that x must satisfy in order for it to be an x.
if not c, then not x
   having four sides    is necessary to be a square.
a sufficient condition for being an x is condition such that if something satisfies condition c, then it must be an x.
if and only if c, then x
the following necessary conditions, jointly, are sufficient to be a square
x has (exactly) four sides
each of x's sides is straight
x is a closed figure
x lies in a plane
each of x's sides is equal in length to each of the others
each of x's interior angles is equal to the others (right angles)
the sides of x are joined at their ends

example from norman swartz, sfu
problem 1: the features are complex and may be context-dependent
william labov. 1975

what are these?
cup or bowl?

the category depends on complex features of the object (diameter, etc)
the category depends on the context! (if there is food in it, it   s a bowl)
labov   s definition of cup
ludwig wittgenstein (1889-1951)
philosopher of language
in his late years, a proponent of studying    ordinary language   
wittgenstein (1945)
philosophical
investigations.
paragraphs 66,67
what is a game?

wittgenstein   s thought experiment on "what is a game   :
pi #66: 
   don   t say    there must be something common, or they would not be called `games         but look and see whether there is anything common to all   

is it amusing?
is there competition?
is there long-term strategy?
is skill required?
must luck play a role?
are there cards?
is there a ball?
family resemblance
   each item has at least one, and probably several, elements in common with one or more items, but no, or few, elements are common to all items       rosch and mervis
how about a radically different approach?

ludwig wittgenstein
pi #43: 
"the meaning of a word is its use in the language"
let's define words by their usages
in particular, words are defined by their environments (the words around them)

zellig harris (1954): if a and b have almost identical environments we say that they are synonyms.


what does ongchoi mean?
suppose you see these sentences:
ong choi is delicious saut  ed with garlic. 
ong choi is superb over rice
ong choi leaves with salty sauces
and you've also seen these:
   spinach saut  ed with garlic over rice
chard stems and leaves are delicious
collard greens and other salty leafy greens
conclusion:
ongchoi is a leafy green like spinach, chard, or collard greens

ong choi: ipomoea aquatica 
"water spinach"
yamaguchi, wikimedia commons, public domain

we'll build a new model of meaning focusing on similarity
each word = a vector 
not just "word" or word45.
similar words are "nearby in space"


we define a word as a vector
called an "embedding" because it's embedded into a space
the standard way to represent meaning in nlp
fine-grained model of meaning for similarity 
nlp tasks like id31
with words,  requires same word to be in training and test
with embeddings: ok if similar words occurred!!! 
id53, conversational agents, etc


we'll introduce 2 kinds of embeddings
tf-idf 
a common baseline model
sparse vectors
words are represented by a simple function of the counts of nearby words
id97
dense vectors
representation is created by training a classifier to distinguish nearby and far-away words
review: words, vectors, and co-occurrence matrices

term-document matrix
each document is represented by a vector of words
visualizing document vectors
vectors are the basis of information retrieval
vectors are similar for the two comedies
different than the history
	
comedies have more fools and wit and fewer battles.
words can be vectors too
battle is "the kind of word that occurs in julius caesar and henry v"

fool is "the kind of word that occurs in comedies, especially twelfth night"




more common: word-word matrix
(or "term-context matrix")
two words are similar in meaning if their context vectors are similar

47



reminders from id202
vector length
cosine for computing similarity
vi is the count for word v in context i 
wi is the count for word w in context i. 

cos(v,w) is the cosine similarity of v and w
sec. 6.3
cosine as a similarity metric
-1: vectors point in opposite directions 
+1:  vectors point in same directions
0: vectors are orthogonal


frequency is non-negative, so  cosine range 0-1
51

52
which pair of words is more similar?
cosine(apricot,information) = 

cosine(digital,information) =

cosine(apricot,digital) =

visualizing cosines 
(well, angles)
 but raw frequency is a bad representation
frequency is clearly useful; if sugar appears a lot near apricot, that's useful information.
but overly frequent words like the, it, or they are not very informative about the context
need a function that resolves this frequency paradox!


tf-idf: combine two factors
tf: term frequency. frequency count (usually log-transformed):


idf: inverse document frequency: tf-
total # of  docs in collection
# of  docs that have word i


tf-idf value for word t in document d:
words like "the" or "good" have very low idf
summary: tf-idf
compare two words using tf-idf cosine to see if they are similar
compare two documents
take the centroid of vectors of all the words in the document
centroid document vector is:
an alternative to tf-idf
ask whether a context word is particularly informative about the target word.
positive pointwise mutual information (ppmi)
57
pointwise mutual information
positive pointwise mutual information
computing ppmi on a term-context matrix
matrix f with w rows (words) and c columns (contexts)
fij is # of times wi occurs in context cj
60
p(w=information,c=data) = 
p(w=information) =
p(c=data) =

61
= .32
6/19
11/19
= .58
7/19
= .37
62
pmi(information,data) = log2 (
.32 /
(.37*.58) )
 = .58
(.57 using full precision)
weighting pmi
pmi is biased toward infrequent events
very rare words have very high pmi values
two solutions:
give rare words slightly higher probabilities
use add-one smoothing (which has a similar effect)


63
weighting pmi: giving rare context words slightly higher id203
64
use laplace (add-1) smoothing

65


66
ppmi versus add-2 smoothed ppmi
67
summary for part i
survey of lexical semantics
idea of embeddings: represent a word as a function of its distribution with other words
tf-idf
cosines
ppmi

next lecture: sparse embeddings, id97
