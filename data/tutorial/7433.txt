dan jurafsky and james martin
speech and language processing

chapter 6:
vector semantics, part ii
tf-idf and ppmi are 
sparse representations
tf-idf and ppmi vectors are
long (length |v|= 20,000 to 50,000)
sparse (most elements are zero)
alternative: dense vectors
vectors which are
short (length 50-1000)
dense (most elements are non-zero)
3
sparse versus dense vectors
why dense vectors?
short vectors may be easier to use as features in machine learning (less weights to tune)
dense vectors may generalize better than storing explicit counts
they may do better at capturing synonymy:
car and automobile are synonyms; but are distinct dimensions
a word with car as a neighbor and a word with automobile as a neighbor should be similar, but aren't
in practice, they work better
4
dense embeddings you can download!
id97 (mikolov et al.)
https://code.google.com/archive/p/id97/
fasttext http://www.fasttext.cc/

glove (pennington, socher, manning)
http://nlp.stanford.edu/projects/glove/

id97
popular embedding method
very fast to train
code available on the web
idea: predict rather than count 
id97
instead of counting how often each word w occurs near "apricot"
train a classifier on a binary prediction task:
is w likely to show up near "apricot"?

we don   t actually care about this task
but we'll take the learned classifier weights as the id27s

brilliant insight: use running text as implicitly supervised training data!
a word s near apricot 
acts as gold    correct answer    to the question 
   is word w likely to show up near apricot?    
no need for hand-labeled supervision
the idea comes from neural id38 
bengio et al. (2003)
collobert et al. (2011) 


id97: skip-gram task
id97 provides a variety of options. let's do
 "skip-gram with negative sampling" (sgns)
skip-gram algorithm
treat the target word and a neighboring context word as positive examples.
randomly sample other words in the lexicon to get negative samples
use id28 to train a classifier to distinguish those two cases
use the weights as the embeddings


9/7/18
10
skip-gram training data
training sentence:
... lemon, a tablespoon of apricot jam   a   pinch ... 
                         c1            c2   target  c3    c4

9/7/18
11
asssume context words are those in +/- 2 word window
skip-gram goal
given a tuple (t,c)  = target, context
(apricot, jam)
(apricot, aardvark)
return id203 that c is a real context word:

p(+|t,c)
p(   |t,c) = 1   p(+|t,c)


9/7/18
12
how to compute p(+|t,c)?
intuition:
words are likely to appear near similar words
model similarity with dot-product!
similarity(t,c)      t     c
problem:
dot product is not a id203!
(neither is cosine)
	  

turning dot product into a id203
the sigmoid lies between 0 and 1:

turning dot product into a id203

for all the context words:
assume all context words are independent
skip-gram training data
training sentence:
... lemon, a tablespoon of apricot jam   a   pinch ... 
                         c1              c2     t        c3    c4

training data: input/output pairs centering on apricot 
asssume a +/- 2 word window
9/7/18
17
skip-gram training
training sentence:
... lemon, a tablespoon of apricot jam   a   pinch ... 
                         c1              c2     t        c3    c4
9/7/18
18
for each positive example, we'll create k negative examples.
using noise words
any random word that isn't t

skip-gram training
training sentence:
... lemon, a tablespoon of apricot jam   a   pinch ... 
                         c1              c2     t        c3    c4
9/7/18
19
k=2
choosing noise words
could pick w according to their unigram frequency p(w)
more common to chosen then according to p  (w)



  =    works well because it gives rare noise words slightly higher id203
to show this, imagine two events p(a)=.99 and p(b) = .01:
setup
let's represent words as vectors of some length (say 300), randomly initialized. 
so we start with 300 * v random parameters
over the entire training set, we   d like to adjust those word vectors such that we
maximize the similarity of the target word, context word pairs (t,c) drawn from the positive data
minimize the similarity of the (t,c) pairs drawn from the negative data. 

9/7/18
21
learning the classifier
iterative process.
we   ll start with 0 or random weights
then adjust the word weights to
make the positive pairs more likely 
and the negative pairs less likely
over the entire training set:

objective criteria
we want to maximize   


maximize the + label for the pairs from the positive training data, and the     label for the pairs sample from the negative data.

9/7/18
23
focusing on one target word t:



train using id119
actually learns two separate embedding matrices w and c
can use w and throw away c, or merge them somehow
summary: how to learn id97 (skip-gram) embeddings
start with v random 300-dimensional vectors as initial embeddings
use id28, the second most basic classifier used in machine learning after na  ve bayes
take a corpus and take pairs of words that co-occur as positive examples
take pairs of words that don't co-occur as negative examples
train the classifier to distinguish these by slowly adjusting all the embeddings to improve the classifier performance
throw away the classifier code and keep the embeddings.
evaluating embeddings
compare to human scores on word similarity-type tasks:
wordsim-353 (finkelstein et al., 2002)
siid113x-999 (hill et al., 2015)
stanford contextual word similarity (scws) dataset (huang et al., 2012) 
toefl dataset: levied is closest in meaning to: imposed, believed, requested, correlated 

properties of embeddings
29
c =   2 the nearest words to hogwarts:
sunnydale
evernight
c =   5 the nearest words to hogwarts:
dumbledore
malfoy
halfblood

similarity depends on window size c
analogy: embeddings capture relational meaning!
30


embeddings can help study word history!
train embeddings on old books to study changes in word meaning!!
will hamilton
diachronic id27s for studying language change!
34
   
   

1900
1950
2000


visualizing changes
project 300 dimensions down into 2
~30 million books, 1850-1990, google books data

36
the evolution of sentiment words
negative words change faster than positive words
embeddings and bias

embeddings reflect cultural bias
ask    paris : france :: tokyo : x    
x = japan
ask    father : doctor :: mother : x    
x = nurse
ask    man : computer programmer :: woman : x    
x = homemaker

bolukbasi, tolga, kai-wei chang, james y. zou, venkatesh saligrama, and adam t. kalai. "man is to computer programmer as woman is to homemaker? debiasing id27s." in  advances in neural information processing systems, pp. 4349-4357. 2016.
embeddings reflect cultural bias
implicit association test (greenwald et al 1998): how associated are 
concepts (flowers, insects) &  attributes (pleasantness, unpleasantness)?
studied by measuring timing latencies for categorization.
psychological findings on us participants:
african-american names are associated with unpleasant words (more than european-american names)
male names associated more with math, female names with arts
old people's names with unpleasant words, young people with pleasant words.
caliskan et al. replication with embeddings:
african-american names (leroy, shaniqua) had a higher glove cosine with unpleasant words  (abuse, stink, ugly)
european american names (brad, greg, courtney) had a higher cosine with pleasant words (love, peace, miracle)
embeddings reflect and replicate all sorts of pernicious biases.


caliskan, aylin, joanna j. bruson and arvind narayanan. 2017. semantics derived automatically from language corpora contain human-like biases. science 356:6334, 183-186.
directions
debiasing algorithms for embeddings
bolukbasi, tolga, chang, kai-wei, zou, james y., saligrama, venkatesh, and kalai, adam t. (2016). man is to computer programmer as woman is to homemaker? debiasing id27s. in advances in neural infor- mation processing systems, pp. 4349   4357. 
use embeddings as a historical tool to study bias
embeddings as a window onto history
use the hamilton historical embeddings
the cosine similarity of embeddings for decade x for occupations (like teacher) to male vs female names
is correlated with the actual percentage of women teachers in decade x



garg, nikhil, schiebinger, londa, jurafsky, dan, and zou, james (2018). id27s quantify 100 years of gender and ethnic stereotypes. proceedings of the national academy of sciences, 115(16), e3635   e3644 
history of biased framings of women
embeddings for competence adjectives are biased toward men
smart, wise, brilliant, intelligent, resourceful, thoughtful, logical, etc.
this bias is slowly decreasing 

garg, nikhil, schiebinger, londa, jurafsky, dan, and zou, james (2018). id27s quantify 100 years of gender and ethnic stereotypes. proceedings of the national academy of sciences, 115(16), e3635   e3644 
embeddings reflect ethnic stereotypes over time
princeton trilogy experiments
attitudes toward ethnic groups (1933, 1951, 1969) scores for adjectives
industrious, superstitious, nationalistic, etc
cosine of chinese name embeddings with those adjective embeddings correlates with human ratings.



garg, nikhil, schiebinger, londa, jurafsky, dan, and zou, james (2018). id27s quantify 100 years of gender and ethnic stereotypes. proceedings of the national academy of sciences, 115(16), e3635   e3644 
change in linguistic framing 1910-1990
change in association of chinese names with adjectives framed as "othering" (barbaric, monstrous, bizarre)
garg, nikhil, schiebinger, londa, jurafsky, dan, and zou, james (2018). id27s quantify 100 years of gender and ethnic stereotypes. proceedings of the national academy of sciences, 115(16), e3635   e3644 
changes in framing:
adjectives associated with chinese
garg, nikhil, schiebinger, londa, jurafsky, dan, and zou, james (2018). id27s quantify 100 years of gender and ethnic stereotypes. proceedings of the national academy of sciences, 115(16), e3635   e3644 
conclusion
concepts or word senses
have a complex many-to-many association with words (homonymy, multiple senses)
have relations with each other
synonymy, antonymy, superordinate
but are hard to define formally (necessary & sufficient conditions)
embeddings = vector models of meaning
more fine-grained than just a string or index
especially good at modeling similarity/analogy
just download them and use cosines!!
can use sparse models (tf-idf) or dense models (id97, glove)
useful in practice but know they encode cultural stereotypes
