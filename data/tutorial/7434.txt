vc-dimension for 

characterizing 

classifiers 

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

andrew w. moore
associate professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2001, andrew w. moore

nov 20th, 2001

a learning machine

    a learning machine f takes an input x and 

transforms it, somehow using weights a
predicted output yest = +/- 1

, into a 

is some vector of 

adjustable parameters

x

f 

yest

copyright    2001, andrew w. moore

vc-dimension: slide 2

1

a
a
examples

denotes +1

denotes -1

x

f 

yest

f(x,b) = sign(x.x     b)

copyright    2001, andrew w. moore

vc-dimension: slide 3

examples

denotes +1

denotes -1

x

f 

yest

f(x,w) = sign(x.w)

copyright    2001, andrew w. moore

vc-dimension: slide 4

2

a
a
examples

denotes +1

denotes -1

x

f 

yest

f(x,w,b) = sign(x.w+b)

copyright    2001, andrew w. moore

vc-dimension: slide 5

how do we characterize    power   ?
    different machines have different amounts of 

   power   .

    tradeoff between:

    more power: can model more complex 

classifiers but might overfit.

    less power: not going to overfit, but restricted in 

what it can model.

    how do we characterize the amount of power?

copyright    2001, andrew w. moore

vc-dimension: slide 6

3

a
some definitions

    given some machine f
    and under the assumption that all training points (xk,yk) were drawn i.i.d 

from some distribution.

    and under the assumption that future test points will be drawn from the 

same distribution

    define

a
r
(

)

=

testerr

a
(

)

=

e

1
2

y

,(
xf

a

)

=  

probabilit
misclassif

ofy 
ication

official terminology

terminology we   ll use

copyright    2001, andrew w. moore

vc-dimension: slide 7

some definitions

    given some machine f
    and under the assumption that all training points (xk,yk) were drawn i.i.d 

from some distribution.

    and under the assumption that future test points will be drawn from the 

same distribution

    define

a
r
(

)

=

testerr

a
(

)

=

e

1
2

y

,(
xf

a

)

=  

probabilit
misclassif

ofy 
ication

official terminology

terminology we   ll use

emp

r

a
(

)

=

a
(
trainerr

)

=

1
r

r

=
1

k

1
2

y

k

xf
(

k

,

a

)

=

r = #training set 

data points

fraction 
set 

training
ied

misclassif

copyright    2001, andrew w. moore

vc-dimension: slide 8

4

  
  
  
  
  
-
  
  
  
  
  
-
-
(cid:229)
vapnik-chervonenkis dimension
xf
testerr
(

a
(
trainerr

,(
xf

a
(

a

e

=

=

y

y

)

)

)

k

,

a

)

k

1
r
r 1
=
k

1
2

    given some machine f, let h be its vc dimension.
    h is a measure of f   s power (h does not depend on the choice of training set)
    vapnik showed that with id203 1-h

testerr

a
(

)

a
(
trainerr

)

h

(log(

+

hr
/2

-+
)1)
r

h
log(

)4/

this gives us a way to estimate the error on 
future data based only on the training error 
and the vc-dimension of f

copyright    2001, andrew w. moore

vc-dimension: slide 9

1
2

1
2

what vc-dimension is used for
xf
(

a
(
trainerr

,(
xf

a
(

a

e

=

=

y

y

)

)

)

testerr

k

,

a

)

k

1
r
r 1
=
k

1
2

    given some machine f, let h be its vc dimension.
,  
    h is a measure of f   s power.
e  
h i n
    vapnik showed that with id203 1-h
e  
e fi n
?
u t e   h
(log(
h
+

c
n   m a
o   w e   d
o m p
c

e
b u t   g i v
o w   d
a
trainerr
(
h
d  
n
a

testerr

a
(

)

)

f

hr
/2

h
log(

)4/

-+
)1)
r

this gives us a way to estimate the error on 
future data based only on the training error 
and the vc-dimension of f

copyright    2001, andrew w. moore

vc-dimension: slide 10

5

  
  
  
  
  
  
-
(cid:229)
-
  
  
  
  
  
  
  
-
(cid:229)
-
  
shattering

    machine f can shatter a set of points x1, x2 .. xr if and only if   

for every possible training set of the form (x1,y1) , (x2,y2) ,    (xr ,yr)

   there exists some value of a

that gets zero training error.

there are 2r such training sets 

to consider, each with a 

different combination of +1   s 

and    1   s for the y   s

copyright    2001, andrew w. moore

vc-dimension: slide 11

shattering

    machine f can shatter a set of points x1, x2 .. xr if and only if   

for every possible training set of the form (x1,y1) , (x2,y2) ,    (xr ,yr)
   there exists some value of a

that gets zero training error.

    question: can the following f shatter the following points?

f(x,w) = sign(x.w)

copyright    2001, andrew w. moore

vc-dimension: slide 12

6

shattering

    machine f can shatter a set of points x1, x2 .. xr if and only if   

for every possible training set of the form (x1,y1) , (x2,y2) ,    (xr ,yr)
   there exists some value of a

that gets zero training error.

    question: can the following f shatter the following points?

f(x,w) = sign(x.w)

    answer: no problem. there are four training sets to consider

w=(0,1)

w=(-2,3)

w=(2,-3)

w=(0,-1)

copyright    2001, andrew w. moore

vc-dimension: slide 13

shattering

    machine f can shatter a set of points x1, x2 .. xr if and only if   

for every possible training set of the form (x1,y1) , (x2,y2) ,    (xr ,yr)
   there exists some value of a

that gets zero training error.

    question: can the following f shatter the following points?

f(x,b) = sign(x.x-b)

copyright    2001, andrew w. moore

vc-dimension: slide 14

7

shattering

    machine f can shatter a set of points x1, x2 .. xr if and only if   

for every possible training set of the form (x1,y1) , (x2,y2) ,    (xr ,yr)
   there exists some value of a

that gets zero training error.

    question: can the following f shatter the following points?

f(x,b) = sign(x.x-b)

    answer: no way my friend. 

copyright    2001, andrew w. moore

vc-dimension: slide 15

definition of vc dimension

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: what   s vc dimension of f(x,b) = sign(x.x-b)

copyright    2001, andrew w. moore

vc-dimension: slide 16

8

vc dim of trivial circle

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: what   s vc dimension of f(x,b) = sign(x.x-b)

answer = 1: we can   t even shatter two points! (but it   s 
clear we can shatter 1)

copyright    2001, andrew w. moore

vc-dimension: slide 17

reformulated circle

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: for 2-d inputs, what   s vc dimension of f(x,q,b) = 

sign(qx.x-b)

copyright    2001, andrew w. moore

vc-dimension: slide 18

9

reformulated circle

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: what   s vc dimension of f(x,q,b) = sign(qx.x-b)

    answer = 2

q,b are  -ve

copyright    2001, andrew w. moore

vc-dimension: slide 19

reformulated circle

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: what   s vc dimension of f(x,q,b) = sign(qx.x-b)

    answer = 2 (clearly can   t do 3)

q,b are  -ve

copyright    2001, andrew w. moore

vc-dimension: slide 20

10

vc dim of separating line

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: for 2-d inputs, what   s vc-dim of f(x,w,b) = sign(w.x+b)?
well, can f shatter these three points?

copyright    2001, andrew w. moore

vc-dimension: slide 21

vc dim of line machine

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: for 2-d inputs, what   s vc-dim of f(x,w,b) = sign(w.x+b)?
well, can f shatter these three points?

yes, of course.

all -ve or all +ve is trivial

one +ve can be picked off by a line

one -ve can be picked off too. 

copyright    2001, andrew w. moore

vc-dimension: slide 22

11

vc dim of line machine

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: for 2-d inputs, what   s vc-dim of f(x,w,b) = sign(w.x+b)?
well, can we find four points that f can shatter?

copyright    2001, andrew w. moore

vc-dimension: slide 23

vc dim of line machine

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: for 2-d inputs, what   s vc-dim of f(x,w,b) = sign(w.x+b)?
well, can we find four points that f can shatter?

can always draw six lines between pairs of four 
points.

copyright    2001, andrew w. moore

vc-dimension: slide 24

12

vc dim of line machine

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: for 2-d inputs, what   s vc-dim of f(x,w,b) = sign(w.x+b)?
well, can we find four points that f can shatter?

can always draw six lines between pairs of four 
points.

two of those lines will cross.

copyright    2001, andrew w. moore

vc-dimension: slide 25

vc dim of line machine

given machine f, the vc-dimension h is

the maximum number of points that can be 
arranged so that f shatter them. 

example: for 2-d inputs, what   s vc-dim of f(x,w,b) = sign(w.x+b)?
well, can we find four points that f can shatter?

can always draw six lines between pairs of four 
points.

two of those lines will cross.

if we put points linked by the crossing lines in the 
same class they can   t be linearly separated

so a line can shatter 3 points but not 4

so vc-dim of line machine is 3

copyright    2001, andrew w. moore

vc-dimension: slide 26

13

vc dim of linear classifiers in m-dimensions

if input space is m-dimensional and if f is sign(w.x-b), what is 

the vc-dimension?

proof that h >= m: show that m points can be shattered
can you guess how?

copyright    2001, andrew w. moore

vc-dimension: slide 27

vc dim of linear classifiers in m-dimensions

if input space is m-dimensional and if f is sign(w.x-b), what is 

the vc-dimension?

proof that h >= m: show that m points can be shattered
define m input points thus:

x1 = (1,0,0,   ,0)
x2 = (0,1,0,   ,0)
:
xm = (0,0,0,   ,1)       so xk[j] = 1 if k=j  and 0 otherwise

let y1, y2,    ym , be any one of the 2m combinations of class 

labels.

guess how we can define w1, w2,    wm and b to ensure 

sign(w. xk + b) = yk for all k ? note:

copyright    2001, andrew w. moore

sign

(

kxw

.

+

b

)

=

sign

+

b

m

=
1j

x.w

j

k

][
j

vc-dimension: slide 28

14

(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
(cid:229)
vc dim of linear classifiers in m-dimensions

if input space is m-dimensional and if f is sign(w.x-b), what is 

the vc-dimension?

proof that h >= m: show that m points can be shattered
define m input points thus:

x1 = (1,0,0,   ,0)
x2 = (0,1,0,   ,0)
:
xm = (0,0,0,   ,1)       so xk[j] = 1 if k=j  and 0 otherwise

let y1, y2,    ym , be any one of the 2m combinations of class 

labels.

guess how we can define w1, w2,    wm and b to ensure 

sign(w. xk + b) = yk for all k ? note:

answer: b=0 and wk = yk for all k.

copyright    2001, andrew w. moore

sign

(

kxw

.

+

b

)

=

sign

+

b

m

=
1j

x.w

j

k

][
j

vc-dimension: slide 29

vc dim of linear classifiers in m-dimensions

if input space is m-dimensional and if f is sign(w.x-b), what is 

the vc-dimension?

in fact, h=m+1

    now we know that h >= m
   
    proof that h >= m+1 is easy
    proof that h < m+2 is moderate

copyright    2001, andrew w. moore

vc-dimension: slide 30

15

(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
(cid:229)
what does vc-dim measure?

    is it the number of parameters?

related but not really the same.
    i can create a machine with one numeric 

parameter that really encodes 7 parameters 
(how?)

    and i can create a machine with 7 parameters 

which has a vc-dim of 1 (how?)

    andrew   s private opinion: it often is the number of parameters that counts.

copyright    2001, andrew w. moore

vc-dimension: slide 31

structural risk minimization

2

f
)

(
f
)

)
f
1
(
fh

    let f(f) = the set of functions representable by f.
    suppose 
(
fh
    then                                                  (hey, can you formally prove this?)
1
    we   re trying to decide which machine to use.
    we train each machine and make a table   
hr
/2

l  
(
nfh

)
f
(
l  

h
log(

(log(

)4/

nf

f
)

)

(

h

2

testerr

a
(

)

a
(
trainerr

)

+

-+
)1)
r

i

fi

trainer
r

vc-conf

probable upper bound 
on testerr

choice

f1
f2
f3
f4
f5
f6

1
2
3
4
5
6
copyright    2001, andrew w. moore

(cid:214)

vc-dimension: slide 32

16

  
  
  
using vc-dimensionality

that   s what vc-dimensionality is about
people have worked hard to find vc-dimension for..

    id90
   
id88s
    neural nets
    decision lists
   
   

support vector machines
and many many more

all with the goals of

1. understanding which learning machines are more or 

less powerful under which circumstances

2. using structural risk minimization for to choose the 

best learning machine

copyright    2001, andrew w. moore

vc-dimension: slide 33

alternatives to vc-dim-based model selection
    what could we do instead of the scheme below?

testerr

a
(

)

a
(
trainerr

)

h

(log(

+

hr
/2

-+
)1)
r

h
log(

)4/

i

fi

trainer
r

vc-conf

probable upper bound 
on testerr

choice

f1
f2
f3
f4
f5
f6

1
2
3
4
5
6
copyright    2001, andrew w. moore

(cid:214)

vc-dimension: slide 34

17

  
alternatives to vc-dim-based model selection
    what could we do instead of the scheme below?

1. cross-validation

trainer
r

10-fold-cv-err

choice

(cid:214)

i
1
2
3
4
5
6

fi
f1
f2
f3
f4
f5
f6

copyright    2001, andrew w. moore

vc-dimension: slide 35

alternatives to vc-dim-based model selection
    what could we do instead of the scheme below?

1. cross-validation
2. aic (akaike information criterion)

aicscore

=

ll

(

data

|

 id113

)
params

 #(

parameters
)

as the amount of data 
goes to infinity, aic 
promises* to select the 
model that   ll have the 
best likelihood for future 
data
*subject to about a million 
caveats

loglike(trainerr)

fi
f1
f2
f3
f4
f5
f6

i
1
2
3
4
5
6
copyright    2001, andrew w. moore

#parameters

aic

choice

(cid:214)

vc-dimension: slide 36

18

-
alternatives to vc-dim-based model selection
    what could we do instead of the scheme below?

1. cross-validation
2. aic (akaike information criterion)
3. bic (bayesian information criterion)

bicscore

=

ll

(

data

|

 id113

params
)

 #

params

2

log

r

as the amount of data 
goes to infinity, bic 
promises* to select the 
model that the data was 
generated from. more 
conservative than aic.
*another million caveats

loglike(trainerr)

fi
f1
f2
f3
f4
f5
f6

i
1
2
3
4
5
6
copyright    2001, andrew w. moore

#parameters

bic

choice

(cid:214)

vc-dimension: slide 37

which model selection method is best?

(cv) cross-validation

1.
2. aic (akaike information criterion)
3. bic (bayesian information criterion)
4.

(srmvc) structural risk minimize with vc-
dimension

    aic, bic and srmvc have the advantage that you only need the 

training error.

    cv error might have more variance
    srmvc is wildly conservative
    asymptotically aic and leave-one-out cv should be the same
    asymptotically bic and a carefully chosen k-fold should be the same
    bic is what you want if you want the best structure instead of the best 

predictor (e.g. for id91 or bayes net structure finding)

    many alternatives to the above including proper bayesian approaches.
    it   s an emotional issue.
copyright    2001, andrew w. moore

vc-dimension: slide 38

19

-
extra comments

    beware: that second    vc-confidence    term is 

usually very very conservative (at least hundreds 
of times larger than the empirical overfitting effect).
    an excellent tutorial on vc-dimension and support 

vector machines (which we   ll be studying soon): 
c.j.c. burges. a tutorial on support vector machines 
for pattern recognition. data mining and knowledge 
discovery, 2(2):955-974, 1998. 
http://citeseer.nj.nec.com/burges98tutorial.html 

copyright    2001, andrew w. moore

vc-dimension: slide 39

what you should know
    the definition of a learning machine: f(x,a) 
    the definition of shattering
    be able to work through simple examples of 

shattering

    the definition of vc-dimension
    be able to work through simple examples of vc-

dimension

    structural risk minimization for model selection
    awareness of other model selection methods

copyright    2001, andrew w. moore

vc-dimension: slide 40

20

