   iframe: [1]//www.googletagmanager.com/ns.html?id=gtm-wcf9z9

   [2]skip to main content [3]skip to sections

   this service is more advanced with javascript available, learn more at
   [4]http://activatejavascript.org

   advertisement
   (button) hide
   springerlink
   search springerlink
   ____________________ submit
   [5]search
     * [6]home
     * [7]log in

   [8]information retrieval journal
   [9]download pdf

   [10]information retrieval journal

   june 2018, volume 21, [11]issue 2   3, pp 111   182 | [12]cite as

neural information retrieval: at the end of the early years

     * authors
     * [13]authors and affiliations

     * kezban dilek onal[14] email author
     * ye zhang
     * ismail sengor altingovde
     * md mustafizur rahman
     * pinar karagoz
     * alex braylan
     * brandon dang
     * heng-lu chang
     * henna kim
     * quinten mcnamara
     * aaron angert
     * edward banner
     * vivek khetan
     * tyler mcdonnell
     * an thanh nguyen
     * dan xu
     * byron c. wallace
     * maarten de rijke
     * matthew lease

   open access
   article
   first online: 10 november 2017
     * [15]12 shares
     * 7.7k downloads
     * [16]2 citations

abstract

   a recent    third wave    of neural network (nn) approaches now delivers
   state-of-the-art performance in many machine learning tasks, spanning
   id103, id161, and natural language processing.
   because these modern nns often comprise multiple interconnected layers,
   work in this area is often referred to as deep learning. recent years
   have witnessed an explosive growth of research into nn-based approaches
   to information retrieval (ir). a significant body of work has now been
   created. in this paper, we survey the current landscape of neural ir
   research, paying special attention to the use of learned distributed
   representations of textual units. we highlight the successes of neural
   ir thus far, catalog obstacles to its wider adoption, and suggest
   potentially promising directions for future research.

keywords

   deep learning distributed representation neural network recurrent
   neural network search engine id27 semantic matching semantic
   compositionality

   kezban dilek onal and ye zhang contributed equally.

   maarten de rijke and matthew lease contributed equally.

1 introduction

   we are in the midst of a tremendous resurgence of interest and
   renaissance in research on id158 (nn) models for
   machine learning, now commonly referred to as deep learning.^[17]1
   while many valuable introductory readings, tutorials, and surveys
   already exist for deep learning at large, we are not familiar with any
   existing literature review surveying nn approaches to information
   retrieval (ir). given the great recent rise of interest in such neural
   ir from academic and industrial researchers (gao et al. [18]2015; li
   and lu [19]2016) and practitioners (metz [20]2016; ordentlich et al.
   [21]2016) alike, and given the significant body of work that has been
   created in just a few years, we believe that such a literature review
   is now timely. ir researchers interested in getting started with neural
   ir currently must identify and compile many scattered works. unifying
   these into a coherent resource provides a single point of reference for
   those interested in learning about these emerging approaches, as well
   as provide a valuable reference compendium for more experienced neural
   ir researchers.

   to address this need, this literature review surveys recent work in
   neural ir. our survey is intended for ir researchers (e.g., typical
   readers of this journal) already familiar with fundamental ir concepts
   and so requiring few definitions or explanations of these. those less
   familiar with ir may wish to consult existing reference materials
   (croft et al. [22]2009; manning et al. [23]2008) for unfamiliar terms
   or concepts. however, we anticipate many readers will have relatively
   less familiarity and experience with nns and deep learning.
   consequently, we point exemplary introductory resources on deep
   learning and briefly introduce key terms, definitions, and concepts in
   sect. [24]3.

   in terms of scope, the survey is limited to textual ir. for nn
   approaches to content-based id162, see wan et al. ([25]2014).
   similarly, we exclude work on nn approaches to acoustic or multi-modal
   ir, such as mixing text with imagery (see ma et al. [26]2015b,
   [27]2016). we also intentionally focus on the current    third wave   
   revival of nn research, excluding earlier work.

   for the purposes of this survey, the early years of neural ir refers to
   the period up to the end of 2016. relevant research from this period
   has mainly been focused on the long standing vocabulary mismatch
   problem in textual ir: the phenomenon that the vocabulary of the
   searcher and the vocabulary used in relevant documents may be
   different. this focus was motivated by the success of neural network
   models in learning distributed representations for words (baroni et al.
   [28]2014; bengio et al. [29]2003a; mikolov et al. [30]2013a; pennington
   et al. [31]2014) and larger textual units (hill et al. [32]2016; le and
   mikolov [33]2014). a distributed representation for a textual unit is a
   dense real-valued vector that somehow encodes the semantics of the
   textual unit (mcclelland et al. [34]1986). distributed representations
   hold the promise of aiding semantic matching: by mapping words and
   other textual units to their representations, semantic matches can be
   computed in the representation space (li and xu [35]2013). indeed,
   recent improvements in obtaining distributed representations using
   neural models have quickly been used for semantic matching of textual
   units in ir tasks.

   the problem of mapping words to a representation that can capture their
   meanings is referred as id65 and has been studied
   for a very long time; see turney and pantel ([36]2010) for an overview.
   neural language models, which may be viewed as a particular flavor of
   distributional semantic models, so-called context-predicting
   distributional semantic models, have been shown to outperform so-called
   context-counting models such as hyperspace analog to language (hal)
   (lund and burgess [37]1996), latent semantic analysis (lsa) (deerwester
   et al. [38]1990), on word analogy and semantic relatedness tasks
   (baroni et al. [39]2014). moreover, levy et al. ([40]2015) improve
   context-counting models by adopting lessons from context-predicting
   models. bengio et al. ([41]2003a) seem to have been the first to
   propose a neural language model; they introduce the idea of
   simultaneously learning a language model that predicts a word given its
   context and its representation, a so-called id27. this idea
   has since been adopted by many follow-up studies. the most well-known
   and most widely used context-predicting models, id97 (mikolov
   et al. [42]2013a) and global vectors (glove) (pennington et al.
   [43]2014), have been used extensively in recent work on web search. the
   success of neural id27s has also given rise to work on
   computing context-predicting representations of larger textual units,
   including paragraphs and documents (hill et al. [44]2016).

   we review the literature in the area and consider both word
   representations and representations of larger textual units, such as
   sentences and paragraphs. we survey the use of neural language models
   and id27s, and detail applications of so-called neural
   semantic compositionality models that determine semantic
   representations of larger text units from smaller ones, and we present
   novel neural semantic compositionality models designed specifically for
   ir tasks. in addition to surveying relevant literature, we provide the
   reader with a foundation on neural language models and with pointers to
   relevant resources that those who are new to the area should
   appreciate.

   regarding the set of reviewed textual ir tasks, we largely follow the
   traditional divide between ir and natural language processing (nlp),
   including search-related ir research and excluding syntactic and
   semantic nlp work. however, this division is perhaps most difficult to
   enforce in regard to two different classes of ir tasks. first is
   id53 (qa) and community id53 (cqa) tasks,
   which perhaps represent the greatest cross-over between the nlp and ir
   fields (e.g., see dumais et al. [45]2002). recent years have shown that
   qa research is often more focused on semantic understanding, reasoning
   and generative models rather than on search over large collections. in
   this survey, we review qa work that is focused on retrieval of textual
   answers. for readers interested in further reading about deep qa, see
   bordes et al. ([46]2014), gao et al. ([47]2015), goldberg ([48]2016),
   kumar et al. ([49]2015). secondly, textual similarity and document
   representations are crucial to many different applications such as
   document id91 and classification. in this survey, we review
   neural models for textual similarity only if the model is evaluated for
   retrieval of similar textual units. limiting to similar item retrieval,
   we exclude works on neural models for general purpose textual
   similarity such as hill et al. ([50]2016), kenter and de rijke
   ([51]2015).

   finally, regarding nomenclature, our use of neural ir, referring to
   machine learning research on artificial nns and deep learning, should
   not be confused with cognitive science research studying actual neural
   representations of relevance in the human brain (see moshfeghi et al.
   [52]2016). in addition, note that the modern appellation of deep
   learning for the current    third wave    of nn research owes to these
   approaches using a large number of hidden layers in their nn
   architectures. for this literature review, we have chosen the term
   neural (rather than deep) because: (1) most current work on textual nns
   in nlp and ir is often actually quite shallow in the number of layers
   used (typically only a few hidden layers and often only one, though
   some notable recent exceptions exist, such as conneau et al. [53]2016);
   (2) the use of neural more clearly connects the current wave of deep
   learning to the long history of nn research; and (3) our use of neural
   ir is consistent with the naming of this journal   s special issue and
   the other articles therein.

   the remainder of this literature review is organized as follows. to
   illustrate the rough evolution of neural ir research over time, we
   begin by providing a concise history of neural ir in sect. [54]2.
   following this, in sect. [55]3 we present background and terminology;
   readers with a background in neural methods can skip over this section.
   in sect. [56]4, we detail the dimensions that we use for categorizing
   the publications we review. we classified textual ir work using five
   main classes of tasks. we review works on ad-hoc retrieval, qa, query
   tasks, sponsored search and similar item retrieval in
   sects. [57]5   [58]9, respectively. so far, there have been few
   publications devoted to neural behavioral models; we cover them in
   sect. [59]10. in sect. [60]11 we present lessons and reflections, and
   we conclude in sect. [61]12. we include multiple appendices, one with
   acronyms used in appendix [62]1, the second with a list of resources
   used in reviewed work; we believe this should help newcomers to the
   area; see appendix [63]2.

2 a brief history of neural ir

   in this section we present a bird   s eye view of key publications in
   neural ir up to the end of 2016. introductory resources on deep
   learning cited in sect. [64]3.1 (see lecun et al. [65]2015; goodfellow
   et al. [66]2016) explain how the    third wave    of interest in neural
   network approaches arose. key factors include increased availability of
      big data,    more powerful computing resources, and better nn models and
   parameter estimation techniques. while early use in id38
   for automatic id103 (asr) and machine translation (mt)
   might be loosely related to id38 for ir (ponte and croft
   [67]1998), state-of-the-art performance provided by neural language
   models trained on vast amounts of data could not be readily applied to
   the more typical sparse data setting of training document-specific
   language models in ir. similarly, neural approaches in id161
   to learn higher-level representations (i.e., visual concepts) from
   low-level pixels were not readily transferable to text-based research
   on words.

   in 2009, salakhutdinov and hinton ([68]2009) published the first    third
   wave    neural ir publications that we are aware of, employing a deep
   auto-encoder architecture (sect. [69]3.2) for semantic modeling for
   related document search (sect. [70]9.1.2). they did not have relevance
   judgments for evaluation, so instead used document corpora with
   category labels and assumed relevance if a query and document had
   matching category labels. little happened in terms of neural approaches
   to ir between 2009 and 2013.

   in 2013, the deep structured semantic model (dssm) (huang et al.
   [71]2013) was introduced, a neural model that directly addresses the
   ad-hoc search task. work by clinchant and perronnin ([72]2013) was the
   first to aggregate id27s for ir. mikolov et al. ([73]2013a,
   [74]b) proposed id97. and lu and li ([75]2013) proposed deepmatch,
   a deep matching method used on two datasets: cqa (matching questions
   with answers) and a twitter-like micro-blog task (matching tweets with
   comments).

   in 2014, the first two neural ir papers appeared in acm sigir. gupta
   et al. ([76]2014) proposed an auto-encoder approach to mixed-script
   id183, considering transliterated search queries and learning
   a character-level    topic    joint distribution over features of both
   scripts. zhang et al. ([77]2014) considered the task of local text
   reuse, with three annotators labeling whether or not a given passage
   represents reuse. new variants of dssm were proposed. and sordoni
   et al. ([78]2014) investigated a deep ir approach to id183,
   evaluating ad-hoc search on trec collections. le and mikolov ([79]2014)
   proposed their paragraph vector (pv) method for composing word
   embeddings to induce semantic representations over longer textual units
   (see sect. [80]11.2.2).

   by 2015, work on neural ir had grown beyond what can be concisely
   described here. we saw id97 enter wider adoption in ir research
   (e.g., ganguly et al. [81]2015; grbovic et al. [82]2015a; kenter and
   de rijke [83]2015; zheng and callan [84]2015; zuccon et al. [85]2015),
   as well as a flourishing of neural ir work appearing at sigir (ganguly
   et al. [86]2015; grbovic et al. [87]2015b; mitra [88]2015; severyn and
   moschitti [89]2015; vulic and moens [90]2015; zheng and callan
   [91]2015), spanning ad-hoc search (ganguly et al. [92]2015; vulic and
   moens [93]2015; zheng and callan [94]2015; zuccon et al. [95]2015), qa
   sentence selection and twitter reranking (kenter and de rijke
   [96]2015), cross-lingual ir (vulic and moens [97]2015), paraphrase
   detection (kenter and de rijke [98]2015), query completion (mitra
   [99]2015), query suggestion (sordoni et al. [100]2015), and sponsored
   search (grbovic et al. [101]2015a, [102]b). in 2015, we also saw the
   first workshop on neural ir.^[103]2

   in 2016, work on neural ir began to accelerate in terms of the volume
   of work, the sophistication of methods, and practical effectiveness
   (e.g., guo et al. [104]2016a). sigir also featured its first workshop
   on the subject,^[105]3 as well as its first tutorial on the subject (li
   and lu [106]2016). to provide as current of coverage as possible in
   this literature review, we include articles appearing up through acm
   ictir 2016 and cikm 2016 conferences.

3 background

   we assume that our readers are familiar with basic concepts from ir but
   possibly less versed in nn concepts; readers with a background in
   neural methods can skip ahead to sect. [107]4. below, in
   sect. [108]3.1, we provide pointers to existing introductory materials
   that can help the reader get started. in sect. [109]3.2, we present
   background material for the nn concepts that are crucial for the rest
   of the survey. in sect. [110]3.3, we briefly review key concepts that
   underly the use of neural models for textual ir tasks, viz.
   id65, semantic compositionality, neural language
   models, training procedures for neural language models, id97 and
   glove, and paragraph vectors.

3.1 introductory tutorials

   for general introductions to deep learning, see arel et al.
   ([111]2010), deng ([112]2014), goodfellow et al. ([113]2016), lecun
   et al. ([114]2015), schmidhuber ([115]2015), yu and deng ([116]2011),
   and bengio ([117]2009). see broder et al. ([118]2016) for a recent
   panel discussion on deep learning. for introductions to deep learning
   approaches to other domains, see hinton et al. ([119]2012) for asr,
   goldberg ([120]2016) for nlp, and wu et al. ([121]2016) for mt.
   goldberg ([122]2016) covers details on training neural networks and a
   broader set of architectures including feed-forward nn, convolutional
   neural network (id98), recurrent neural network (id56) and recursive
   neural network (reid98). cho ([123]2015) focuses on id38
   and machine translation, sketches a clear picture of encoder-decoder
   architectures, recurrent networks and attention modules.

   regarding nn approaches to ir, informative talks and tutorials have
   been presented by gao et al. ([124]2015), li and lu ([125]2016), and li
   ([126]2016). many other useful tutorials and talks can be found online
   for general deep learning and specific domains. a variety of resources
   for deep learning, including links to popular open-source software, can
   be found at [127]http://deeplearning.net.

3.2 background on neural networks

3.2.1 neural networks

   the neural models we will typically consider in this survey are
   feed-forward networks, which we refer to as nn for simplicity. a simple
   example of such an nn is shown in fig. [128]1. input features are
   extracted, or learned, by nns using multiple, stacked fully connected
   layers. each layer applies a linear transformation to the vector output
   of the last layer (performing an affine transformation). thus each
   layer is associated with a matrix of parameters, to be estimated during
   learning. this is followed by element-wise application of a non-linear
   activation function. in the case of ir, the output of the entire
   network is often either a vector representation of the input or some
   predicted scores. during training, a id168 is constructed by
   contrasting the prediction with the ground truth available for the
   training data, where training adjusts network parameters to minimize
   loss. this is typically performed via the classic back-propagation
   algorithm (rumelhart et al. [129]1988). for further details, see
   goodfellow et al. ([130]2016).
   [131]open image in new window fig. 1
   fig. 1

   feed-forward fully connected neural network

3.2.2 auto-encoder

   an auto-encoder nn is an unsupervised model used to learn a
   representation for data, typically for the purpose of dimensionality
   reduction. unlike typical nns, an auto-encoder is trained to
   reconstruct the input, and the output has the same dimension as the
   input. for more details, see erhan et al. ([132]2010), hinton and
   salakhutdinov ([133]2006). auto-encoder was applied in ir in
   salakhutdinov and hinton ([134]2009).

3.2.3 restricted boltzman machine (rbm)

   an rbm is a stochastic neural network whose binary activations depend
   on its neighbors and have a probabilistic binary activation function.
   rbms are useful for id84, classification,
   regression, id185, id171, id96,
   etc. the rbm was originally proposed by smolensky ([135]1986) and
   further popularized by nair and hinton ([136]2010).

3.2.4 convolutional neural network

   in contrast to the densely-connected networks described above, a
   convolutional neural network (id98) (lecun and bengio [137]1995) defines
   a set of linear filters (kernels) connecting only spatially local
   regions of the input, greatly reducing computation. these filters
   extract locally occurring patterns. id98s are typically built following
   a    convolution + pooling    architecture, where a pooling layer following
   convolution further extracts the most important features while at the
   same time reducing dimensionality. we show a basic example of a id98 in
   fig. [138]2. id98s were first established by their strong performance on
   image classification (krizhevsky et al. [139]2012), then later adapted
   to text-related tasks in nlp and ir (collobert et al. [140]2011;
   kalchbrenner et al. [141]2014; kim [142]2014; zhang and wallace
   [143]2015; zhang et al. [144]2017). as discussed in sect. [145]11.2.2,
   yang et al. ([146]2016b), guo et al. ([147]2016b), and severyn and
   moschitti ([148]2015) question whether id98 models developed in computer
   vision and nlp to exploit spatial and positional information are
   equally-well suited to the ir domain.
   [149]open image in new window fig. 2
   fig. 2

   one-dimensional id98 with only one convolution layer (six filters and
   six feature maps), followed by a 1-max-pooling layer

3.2.5 recurrent neural network

   a recurrent neural network (id56) (elman [150]1990) models sequential
   inputs, e.g., sequences of words in a document. we show a basic example
   of an id56 in fig. [151]3. individual input units (e.g., words) are
   typically encoded in vector representations. id56s usually read inputs
   sequentially; one can think of the input order as indexing    time.   
   thus, the first word corresponds to an observation at time 0, the
   second at time 1, and so on. the key component of id56s is a hidden
   state vector that encodes salient information extracted from the input
   read thus far. at each step during traversal (at each time point t),
   this state vector is updated as a function of the current state vector
   and the input at time t. thus, each time point is associated with its
   own unique state vector, and when the end of a piece of text is
   reached, the state vector will capture the context induced by the
   entire sequence.
   [152]open image in new window fig. 3
   fig. 3

   recurrent neural network (id56). here, x is the input, a is the
   computation unit shared across time steps, and h is the hidden state
   vector

   a technical problem with fitting the basic id56 architecture just
   described is the    vanishing gradient problem    (pascanu et al.
   [153]2013) inherent to parameter estimation via back-propagation
      through time.    the trouble is that gradients must flow from later time
   steps of the sequence back to earlier bits of the input. this is
   difficult for long sequences, as the gradient tends to degrade, or
      vanish,    as they are passed backwards through time. fortunately, there
   are two commonly used variants of id56s that aim to mitigate this
   problem (and have proven empirically successful in doing so): lstm and
   gru.

   long short term memory (lstm) (hochreiter and schmidhuber [154]1997)
   was the first approach introduced to address the vanishing gradient
   problem. in addition to the hidden state vector, lstms have a memory
   cell structure, governed by three gates. an input gate is used to
   control how much the memory cell will be influenced by the new input; a
   forget gate dictates how much previous information in the memory cell
   will be forgotten; and an output gate controls how much the memory cell
   will influence the current hidden state. all three of these gates
   depend on the previous hidden state and the current input. for a more
   detailed description of lstm and more lstm variants, see graves
   ([155]2013) and greff et al. ([156]2015).

   bahdanau et al. ([157]2014)   s gated recurrent unit (gru) is a more
   recent architecture, similar to the lstm model but simpler (and thus
   with fewer parameters). empirically, grus have been found to perform
   comparably to lstms, despite their comparative simplicity (chung et al.
   [158]2014). instead of the memory cell used by lstms, an update gate is
   used to govern the extent to which the hidden gate will be updated, and
   a reset gate is used to control the extent to which the previous hidden
   state will influence the current state.

3.2.6 attention

   the notion of attention has lately received a fair amount of interest
   from nn researchers. the idea is to imbue the model with the ability to
   learn which bits of a sequence are most important for a given task (in
   contrast, e.g., to relying only on the final hidden state vector).

   attention was first proposed in the context of neural machine
   translation model by bahdanau et al. ([159]2014). the original id56
   model for machine translation (sutskever et al. [160]2014) encodes the
   source sentence into a fixed-length vector (by passing an id56 over the
   input, as described in sect. [161]3.2.5). this is then accepted as
   input by a decoder network, which uses the single encoded vector as the
   only information pertaining to the source sentence. this means that all
   relevant information required for the translation must be stored in a
   single vector   a difficult aim.

   the attention mechanism was proposed to alleviate this requirement. at
   each time step (as the decoder generates each word), the model
   identifies a set of positions in the source sentence that is most
   relevant to its current position in the output (a function of its index
   and what it has generated thus far). these positions will be associated
   with corresponding state vectors. the current contextualizing vector
   (to be used to generate output) can then be taken as a sum of these,
   weighted by their estimated relevance. this attention mechanism has
   also been used in image id134 (xu et al. [162]2015). a
   similar line of work includes id63s by graves et al.
   ([163]2014) and memory networks by weston et al. ([164]2014).

3.3 id27s and semantic compositionality

3.3.1 id65

   a distributional semantic model (dsm) is a model that relies on the
   distributional hypothesis (harris [165]1954), according to which words
   that occur in the same contexts tend to have similar meanings, for
   associating words with vectors that can capture their meaning.
   statistics on observed contexts of words in a corpus is quantified to
   derive word vectors. the most common choice of context is the set of
   words that co-occur in a context window.

   baroni et al. ([166]2014) classify existing dsms into two categories:
   context-counting and context-predicting. the context-counting category
   includes earlier dsms such as hyperspace analog to language (hal) (lund
   and burgess [167]1996), latent semantic analysis (lsa) (deerwester
   et al. [168]1990). in these models, low-dimensional word vectors are
   obtained via factorisation of a high-dimensional sparse co-occurrence
   matrix. the context-predicting models are neural language models in
   which word vectors are modelled as additional parameters of a neural
   network that predicts co-occurrence likelihood of context-word pairs.
   neural language models comprise an embedding layer that maps a word to
   its distributed representation. a distributed representation of a
   symbol is a vector of features that characterize the meaning of the
   symbol and are not mutually exclusive (mcclelland et al. [169]1986).

   early neural language models were not aimed at learning representations
   for words. however, it soon turned out that the embedding layer
   component, which addresses the curse of dimensionality caused by
   one-hot vectors (bengio et al. [170]2003a), yields useful distributed
   word representations, so-called id27s. collobert and weston
   ([171]2008) are the first ones to show the benefit of id27s
   as features for nlp tasks. soon afterwards, id27s became
   widespread after the introduction of the shallow models skip-gram and
   continuous bag of words (cbow) in the id97 framework by mikolov
   et al. ([172]2013a, [173]b); see sect. [174]3.3.5.

   baroni et al. ([175]2014) report that context-predicting models
   outperform context-counting models on several tasks, including question
   sets, semantic relatedness, synonym detection, concept categorization
   and word analogy. in contrast, levy et al. ([176]2015) point out that
   the success of the popular context-predicting models id97 and glove
   does not originate from the neural network architecture and the
   training objective but from the choices of hyper-parameters for
   contexts. a comprehensive analysis reveals that when these
   hyper-parameter choices are applied to context-counting models, no
   consistent advantage of context-predicting models is observed over
   context-counting models.

3.3.2 semantic compositionality

   compositional id65 or semantic compositionality
   (sc) is the problem of formalizing how the meaning of larger textual
   units such as sentences, phrases, paragraphs and documents are built
   from the meanings of words (mitchell and lapata [177]2010). work on sc
   studies are motivated by the principle of compositionality which states
   that the meaning of a complex expression is determined by the meanings
   of its constituent expressions and the rules used to combine them.

   a neural sc model maps the high-dimensional representation of a textual
   unit into a distributed representation by forward propagation in a
   neural network. the neural network parameters are learned by training
   to optimize task-specific objectives. both the granularity of the
   target textual unit and the target task play an important role for the
   choice of neural network type and training objective. a sc model that
   considers the order of words in a sentence and aims to obtain a deep
   understanding may fail in an application that requires representations
   that can encode high-level concepts in a large document. a comparison
   of neural sc models of sentences learned from unlabelled data is
   presented in hill et al. ([178]2016). besides the models reviewed by
   hill et al. ([179]2016), there exist sentence-level models that are
   trained using task-specific labelled data. for instance, a model can be
   trained to encode the sentiment of a sentence using a dataset of
   sentences annotated with sentiment class labels (li et al. [180]2015).

   to the best of our knowledge, there is no survey on neural sc models
   for distributed representations of long documents, although the
   representations are useful not only for document retrieval but also for
   document classification and recommendation. in sect. [181]4.2.2 we
   review the subset of neural sc models and associated training
   objectives adopted specifically in ir tasks.

3.3.3 neural language models

   a language model is a function that predicts the acceptability of
   pieces of text in a language. acceptability scores are useful for
   ranking candidates in tasks like machine translation or speech
   recognition. the id203 of a sequence of words \(p(w_1,w_2\), ...,
   \(w_n)\) in a language, can be computed by eq. [182]1, in accordance
   with the chain rule:
   $$ p(w_1,w_2,\ldots ,w_{t-1},w_t)=p(w_1)p(w_2\mid w_1)\cdots p(w_t\mid
   w_1,w_2,\ldots ,w_{t-1}) $$
   (1)
   probabilistic language models mostly approximate eq. [183]1 by
   \(p(w_t\mid w_{t-n}\), ..., \(w_{t-1})\), considering only a limited
   context of size n, that immediately precedes \(w_t\). in neural
   language models the id203 \(p(w \mid c)\) of a word w to follow
   the context c is computed by a neural network. the neural network takes
   a context c and outputs the id155 \(p(w \mid c)\) of
   every word w in the vocabulary v of the language:
   $$ p(w \mid c,\theta ) = \frac{\exp (s_\theta (w,c))}{\sum _{w' \in v}
   \exp (s_\theta (w',c))}. $$
   (2)
   here, \(s_\theta (w,c)\) is an unnormalized score for the compatibility
   of w given the context c; \(s_\theta (w,c)\) is computed via forward
   propagation of the context c through a neural network defined with the
   set of parameters \(\theta \). note that \(p(w \mid c)\) is computed by
   the normalized exponential (softmax) function in eq. [184]2, over the
   \(s_\theta (w,c)\) scores for the entire vocabulary.
   parameters of the neural network are learned by training on a text
   corpus using gradient-descent based optimization algorithms to maximize
   the likelihood function l in eq. [185]3, on a given corpus t:
   $$ l(\theta ) = \sum _{(t,c) \in t} p(t \mid c, \theta ). $$
   (3)
   the first neural language model published is the neural network
   language model (nnlm) (bengio et al. [186]2003a). the common
   architecture shared by neural language models is depicted in fig.
   [187]4, with example input context \(c=w_1,w_2,w_3\) and the word to
   predict being \(w_4\), extracted from the observed sequence
   \(c=w_1,w_2,w_3,w_4\). although a id203 distribution over the
   vocabulary v is computed, the word that should have the maximum
   id203 is shown at the output layer, for illustration purposes.
   the neural network takes one-hot vectors \(w_1,w_2,w_3\) of the words
   in the context. the dimensionality of the one-hot vectors is \(1 \times
   |v|\). the embedding layer e in fig. [188]4 is indeed a \(|v| \times
   d\)-dimensional matrix whose ith row is the d-dimensional word
   embedding for the ith word in the vocabulary. the embedding vector of
   the ith word in the vocabulary is obtained by multiplying the one-hot
   vector of the word with the e matrix or simply extracting the ith row
   of the embedding matrix. consequently, high dimensional one-hot vectors
   of words \(w_1,w_2,w_3\) are mapped to their d-dimensional embedding
   vectors \(e_1,e_2,e_3\) by the embedding layer. note that d is usually
   chosen to be in the range 100   500 whereas |v| can go up to millions.
   [189]open image in new window fig. 4
   fig. 4

   architecture of neural language models

   the hidden layer in fig. [190]4 takes the embedding vectors
   \(e_1,e_2,e_3\) of the context words and creates a vector \(h_c\) for
   the input context. this layer differs between neural language model
   architectures. in nnlm (bengio et al. [191]2003a) it is a non-linear
   neural network layer whereas in the cbow model of id97 (mikolov
   et al. [192]2013a), it is vector addition over id27s. in the
   recurrent neural network language model (id56lm) (mikolov et al.
   [193]2010) the hidden context representation is computed by a recurrent
   neural network. besides the hidden layer, the choice of context also
   differs among models. in collobert and weston ([194]2008) and in the
   cbow model (mikolov et al. [195]2013a) context is defined by the words
   that surround a center word in a symmetric context. in the nnlm (bengio
   et al. [196]2003a) and id56lm (mikolov et al. [197]2010) models, the
   context is defined by words that precede the target word. the skip-gram
   (mikolov et al. [198]2013a) takes a single word as input and predicts
   words from a dynamically sized symmetric context window around the
   input word.
   the classifier layer in fig. [199]4, which is composed of a weights
   matrix c of dimension \(d \times |v|\) and a bias vector of dimension
   |v|, is used to compute \(s_\theta (w,c)\) using eq. [200]4:
   $$ s_\theta (w,c) = h_c c + b. $$
   (4)
   to sum up, the neural network architecture for a neural language model
   (nlm) is defined by |v|, d, the context type, the context size |c| and
   the function in the hidden layer. the parameter set \(\theta \) to be
   optimized includes the embedding matrix e, parameters from the hidden
   layer, the weights matrix c and the bias vector b of the classifier
   layer. the embedding layer e is treated as an ordinary layer of the
   network, its weights are initialized randomly and updated with
   back-propagation during training of the neural network.

3.3.4 efficient training of nlms

   as mentioned in our discussion of eq. [201]1, the output of a nlm is a
   normalized id203 distribution over the entire vocabulary.
   therefore, for each training sample (context pair (t, c)), it is
   necessary to compute the softmax function in eq. [202]2 and consider
   the whole vocabulary for computing the gradients of the likelihood
   function in back-propagation. this makes the training procedure
   computationally expensive and prevents the scalability of the models to
   very large corpora.
   several remedies for efficiently training nlms have been introduced.
   the reader may refer to chen et al. ([203]2016) for a comparison of
   these remedies for the nnlm. the first group of remedies such as
   hierarchical softmax (morin and bengio [204]2005) and differentiated
   softmax (chen et al. [205]2016) propose updates to the softmax layer
   architectures for efficient computation. the second approach, adopted
   by methods like importance sampling (is) (bengio et al. [206]2003b) and
   noise contrastive estimation (nce) (mnih and teh [207]2012), is to
   avoid the id172 by using modified id168s to approximate
   the softmax. collobert and weston ([208]2008) propose the cost function
   in eq. [209]5, which does not require id172 over the
   vocabulary. the nlm is trained to compute higher \(s_\theta \) scores
   for observed context-word pairs (c, t) compared to the negative samples
   constructed by replacing t with any other word w in v. the context is
   defined as the words in a symmetric window around the center word t.
   $$ \sum _{(t,c) \in t} \sum _{w \in v} \max (0, 1 - s_\theta (t,c) +
   s_\theta (w,c)) $$
   (5)
   mnih and teh ([210]2012) apply nce (gutmann and hyv  rinen [211]2012) to
   nlm training. by using nce, the id203 density estimation problem
   is converted to a binary classification problem. a two-class training
   data set is created from the training corpus by treating the observed
   context-word pairs (t, c) as positive samples and k noisy pairs
   \((t',c)\) constructed replacing t with a word \(t'\) sampled from the
   noise distribution q.

3.3.5 id97 and glove

   mikolov et al. ([212]2013a) introduce the skip-gram and cbow models
   that follow the nlm architecture with a linear layer for computing a
   distributed context representation. figure [213]5 illustrates the
   architecture of id97 models with context windows of size five. the
   cbow model is trained to predict the center word of a given context. in
   the cbow model, the hidden context representation is computed by the
   sum of the id27s. on the contrary, the skip-gram model is
   trained to predict words that occur in a symmetric context window given
   the center word. the name skip-gram is used since the size of symmetric
   context window is selected randomly from the range [0, c] for each
   center word. skip-gram embeddings are shown to outperform embeddings
   obtained from nnlms and id56lms in capturing the semantic and syntactic
   relationships between the words.
   [214]open image in new window fig. 5
   fig. 5

   models of the id97 framework. a continuous bag of words (cbow). b
   skip-gram

   id97 owes its widespread adoption in the nlp and ir communities to
   its scalability. efficient training of skip-gram and cbow models is
   achieved by hiearchical softmax (morin and bengio [215]2005) with a
   huffman tree (mikolov et al. [216]2013a). in follow-up work (mikolov
   et al. [217]2013b), negative sampling (neg) is proposed for efficiently
   training the skip-gram model. neg is a variant of nce. neg, differently
   from nce, assumes the noise distribution q to be uniform and \(k=|v|\)
   while computing the conditional probabilities. for a detailed
   discussion of nce and neg, see notes by dyer ([218]2014). it is crucial
   to note that the skip-gram with negative sampling (sgns) departs from
   the goal of learning a language model and only embedding layer of the
   model is used in practice.
   subsampling frequent words is another extension introduced in mikolov
   et al. ([219]2013b) for speeding up training and increasing the quality
   of embeddings. each word \(w_i\) in the corpus is discarded with
   id203 \(p(w_i)\), computed as a function of its frequency
   \(f(w_i)\) in the corpus, given in eq. [220]6:
   $$ p(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}. $$
   (6)
   glove glove (pennington et al. [221]2014) combines global context and
   local context in the training objective for learning id27s.
   in contrast to nlms, where embeddings are optimized to maximize the
   likelihood of local contexts, glove embeddings are trained to fit the
   co-occurrence ratio matrix.

   context-counting versus context-predicting levy et al. ([222]2015)
   discuss that diluting frequent words before training enlarges the
   context window size in practice. experiments show that the
   hyper-parameters about context-windows, like dynamic size and
   subsampling frequent words, have a notable impact on the performance of
   sgns and glove (levy et al. [223]2015). levy et al show that when these
   choices are applied to traditional dsms, no consistent advantage of
   sgns and glove is observed. in contrast to the conclusions obtained in
   baroni et al. ([224]2014), the success of context-predicting models is
   attributed to choice of hyper-parameters, which can also be used for
   context-counting dsms, rather than to the neural architecture or the
   training objective.

3.3.6 paragraph vector

   the pv (le and mikolov [225]2014) extends id97 in order to learn
   representations for so-called paragraph, textual units of any length.
   similar to id97, it is composed of two separate models, namely
   paragraph vector with distributed memory (pv-dm) and paragraph vector
   with distributed bag of words (pv-dbow). the architectures of pv-dm and
   pv-dbow are illustrated in fig. [226]6. the pv-dbow model is a
   skip-gram model where the input is a paragraph instead of a word. the
   pv-dbow is trained to predict a sample context given the input
   paragraph. in contrast, the pv-dm model is trained to predict a word
   that is likely to occur in the input paragraph after the sample
   context. the pv-dm model is a cbow model extended with a paragraph in
   the input layer and a document embedding matrix. in the pv-dbow model,
   only paragraph embeddings are learned whereas in the pv-dm model word
   embeddings and paragraph embeddings are learned, simultaneously.
   [227]open image in new window fig. 6
   fig. 6

   models of the paragraph vector framework. a pv-dm, b pv-dbow

   in fig. [228]6, p stands for the index of the input paragraph and
   \(w_1,w_2,w_3,w_4\) represent the indices of the words in a contiguous
   sequence of words sampled from this paragraph. a sequence of size four
   is selected just for illustration purposes. also, d represents the
   paragraph embedding matrix and e stands for the id27 matrix.
   at the lowest layer, the input paragraph p is mapped to its embedding
   by a lookup in the d matrix. the hidden context representation is
   computed by summing the embeddings of the input words and paragraph,
   which is the same as in the cbow model.

   paragraph vector models are trained on unlabelled paragraph
   collections. an embedding for each paragraph in the collection is
   learned at the end of training. the embedding for an unseen paragraph
   can be obtained by an additional id136 stage. in the id136
   stage, d is extended with columns for new paragraphs; d is updated
   using id119 while other parameters of the model are kept
   fixed.

4 taxonomy

   to organize the material surveyed in this paper, we use a simple
   taxonomy. we classify publications based on the ir task to which neural
   models are applied and how neural network models are utilized for
   solving the target ir task. in the next subsections, we first detail
   the sub-categories of the task and how features. secondly, we provide a
   roadmap for the rest of the survey in sect. [229]4.3.

4.1 task

   as mentioned previously, the majority of reviewed work is about textual
   ir tasks. each of the tasks is concerned with a different target
   textual unit (ttu) or with different ttu pairs. we group publications
   on textual ir into five classes given in table [230]1, considering a
   ttu or ttu pairs with different characteristics.
   table 1

   ttu or ttu pairs for the textual ir tasks covered in the survey

   task

   ttu or ttu pair

   relation

   ad-hoc retrieval

   query-document

   relevant

   query understanding

   query

   similar

   sponsored search

   query-ad

   relevant

   id53

   question-answer

   answer

   similar item retrieval

   document-document

   similar
   the hierarchy of tasks in the rest of the survey is as follows:
    1. 1.
       ad-hoc retrieval: ad-hoc retrieval refers to a single search
       performed by a user: a single query, with no further interaction or
       feedback, on the basis of which an ir system strives to return an
       accurate document ranking. this comprises of the following tasks:
          + document ranking.
          + id183.
          + query re-weighting.
          + result diversification.
          + semantic expertise retrieval.
          + product search.

    2. 2.
       query understanding: this category includes ir tasks concerned with
       understanding the user intent in order to assist the user in typing
       queries or improving document retrieval. publications here are
       focused on distributed representations of queries and finding
       similar queries that can better express user intent. we distinguish
       the following tasks:
          + query suggestion.
          + query auto completion.
          + query classification.

    3. 3.
       id53: this class includes tasks that are focused on
       retrieval of text segments that answers the user question. answer
       segments may have different granularities, such as sentence,
       passage or even the entire document. here we identify two tasks:
          + answer sentence retrieval.
          + conversational agents.

    4. 4.
       sponsored search: this category includes tasks related to retrieval
       of ads relevant to user queries.

    5. 5.
       similar item retrieval: this category includes tasks related to
       retrieving similar items of the same type as the query. there exist
       a large number of neural semantic compositionality models for
       textual similarity. to remain focused we limit ourselves to
       publications that focus on the retrieval effectiveness for similar
       item search. the following tasks are considered:
          + related document search.
          + detecting text re-use.
          + similar question retrieval in cqa.
          + content-based recommendation.

4.2 how

   the how feature defines how neural models and distributed
   representations are adopted and utilized in an ir task. the methods can
   be grouped in two main classes: aggregate and learn. availability of
   code and existing embeddings from id97 (mikolov et al. [231]2013b)
   and glove (pennington et al. [232]2014) (see appendix    word
   embeddings   ) motivated the aggregate approaches for neural ir,
   especially for extending traditional ir models to integrate word
   embeddings. in contrast, the learn category covers conceptually
   different approaches which directly incorporate id27s within
   nn models, reflecting a more significant shift toward pursuing
   end-to-end nn architectures in ir.

4.2.1 aggregate

   publications in this category are focused on the following research
   question raised by clinchant and perronnin ([233]2013): if we were
   provided with an embedding of words in a continuous space, how could we
   best use it in ir/id91 tasks? the methods in this category rely
   on pre-trained id27s as external resources in order to build
   or extend relevance matching functions. existing work can be split into
   two sub-categories depending on how the embeddings are utilized:

   explicit
          id27s are considered as building blocks for
          distributed ttu representations. publications that follow this
          pattern treat a ttu as a bag of embedded words (boew) or a set
          of points in the id27 space. the boew is aggregated to
          build a single vector for the ttu. the most common aggregation
          method is averaging or summing the vectors of the terms in the
          ttu.

   implicit
          here, one utilizes the vector similarity in the embedding space
          in id38 frameworks without explicit computation of
          distributed representations for ttus pairs. for instance, zuccon
          et al. ([234]2015) compute translation probabilities of word
          pairs in a translation language model retrieval framework with
          cosine similarity of skip-gram vectors.

4.2.2 learn

   this category covers work on learning end-to-end neural models for ir,
   specifically for semantic matching of ttu pairs. the neural models in
   this category are designed and trained to learn id27s and
   semantic compositionality functions for building distributed
   representations for ttus or ttu pairs, simultaneously, from scratch,
   given only the raw text of ttus. we observed that in some recent
   publications, the similarity or relevance function on top of
   distributed representations is also modeled with a neural network and
   learned, simultaneously.
   in the rest of the survey, semantic compositionality network (scn)
   refers to a neural network that composes distributed ttu
   representations based on either the embeddings of words in the ttu or
   the one-hot term vector. publications in this category mainly vary in
   the training objectives defined based on the distributed representation
   for ttus. four separate training objectives are observed in the
   reviewed work. based on these objectives, we define the four
   sub-categories, namely
     * learn to autoencode,
     * learn to match,
     * learn to predict and
     * learn to generate,

   all of which are detailed below.
   learn to autoencode this category covers mostly relatively early work
   that relies on auto-encoder (see sect. [235]3.2.2) architectures for
   learning ttu representations. as depicted in fig. [236]7, the training
   objective is to restore the input x based on the distributed
   representation \(h_x\) learned by the encoder. generally, the encoder
   and decoder are mirrored neural networks with different architectures.
   [237]open image in new window fig. 7
   fig. 7

   auto-encoder architecture. note that the encoder is the semantic
   compositionality network (scn)
   learn to match learning to match (li and xu [238]2013) is the problem
   of learning a matching function f(x, y) that computes a degree of
   similarity between two objects x and y from two different spaces x and
   y. given training data t composed of triples (x, y, r), learning to
   match is the optimization problem in eq. [239]7:
   $$ \mathop {\mathrm {arg\,min}}\limits _{f \in f} \sum _{(x,y,r) \in t}
   l(r,f(x,y)). $$
   (7)
   here, l denotes a id168 between the actual similarity score r
   and the score predicted by the f function. learn to match models
   introduce neural architectures for computing the relevance matching
   function f.

   learn to match models require similarity assessments for training.
   since it is difficult to obtain large amounts of supervised data, click
   information in click-through logs are exploited to derive similarity
   assessments for query-document and query-ad pairs. if a pair (x, y) is
   associated with clicks, the objects are assumed to be similar; they are
   dissimilar in the absence of clicks. in the case of query-query pairs,
   co-occurrence in a session is accepted as similarity signal that can be
   extracted from query logs. we replace x and y of eq. [240]7 with q and
   d to represent a query and a document object, respectively. the
   document object can be any textual unit that needs to be matched
   against a query, such as a document, query or ad; \((q,d^+)\) denotes a
   (query-clicked document) pair extracted from logs.
   a training objective adopted by the majority of the publications in
   this category is to minimize the negative log likelihood function in
   eq. [241]8:
   $$ l = - \log \prod _{(q,d^+)} p(d^+ \mid q) $$
   (8)
   the likelihood of a document d given a query q, is computed by eq.
   [242]9 with a softmax over similarity scores of distributed
   representations:
   $$ p(d\mid q) = \frac{\exp (f(q,d))}{\sum _{d' \in d} \exp (f(q,d')} $$
   (9)
   here, f is a relevance matching function; l requires the computation of
   a id203 distribution over the entire document collection d for
   each \((q,d^+)\) pair. since this is computationally expensive, d is
   approximated by a randomly selected small set of unclicked documents,
   similar to the softmax approximation methods for neural language models
   in sect. [243]3.3.4.
   there are two patterns of neural architectures for the relevance
   matching function f, namely representation based and interaction based,
   as illustrated in fig. [244]8. in representation based architectures,
   the query and document are independently run through mirrored neural
   models (typically this would be a siamese architecture broid113y et al.
   [245]1993, in which weights are shared between the networks); relevance
   scoring is then performed by a model operating over the two induced
   representations. in contrast, in an interaction based architecture, one
   first constructs a joint representation of the query and document pair
   and then runs this joint input through a network. the architectures
   differ in their inputs and the semantics of the hidden/representational
   layers. guo et al. ([246]2016a) point out that representation based
   architectures fail to catch exact matching signals which are crucial
   for retrieval tasks.
   [247]open image in new window fig. 8
   fig. 8

   two basic neural architectures for scoring the relevance of a ttu pair,
   query and document for illustrative purposes. a representation based, b
   interaction based. this figure is inspired by figure 1 in guo et al.
   ([248]2016a)
   in representation based architectures, input vector representations of
   q, d are mapped into distributed representations \(h_q\), \(h_d\) by
   the semantic compositionality network (scn) and the similarity score of
   the objects is computed by the similarity of the distributed
   representations. usually, cosine similarity is used to compute the
   similarity of representation vectors created by the scn, as given in
   eqs. [249]10 and [250]11:
   $$ h_q= scn(q), h_d = scn(d) $$
   (10)
   $$ f(q,d)= \frac{h_q \cdot h_d}{\left| {h_q}\right| \left| {h_d}\right|
   }. $$
   (11)
   in fig. [251]9, the training objective in eq. [252]8 is illustrated on
   a representation based architecture with four randomly selected
   non-relevant documents. let \(d_1\) be a relevant document and
   \({d_2,d_3,d_4,d_5}\) non-relevant documents for the query q. the
   siamese network is applied to compute similarity scores for each pair
   \((q,d_i)\). the target values for the output nodes of the entire
   network is forced to be [1.0, 0.0, 0.0, 0.0, 0.0] during training.
   [253]open image in new window fig. 9
   fig. 9

   architecture of representation based learn to match models adapted from
   deng and yu ([254]2013)

   learn to predict the success of word-based neural language models has
   motivated models for learning representations of larger textual units
   from unlabelled data (hill et al. [255]2016; le and mikolov [256]2014).
   as mentioned previously, neural language models are context-predicting
   distributional semantic model (dsms). learn to predict models rely on
   an extension of the context-prediction idea to larger linguistic units,
   which is that similar textual units occur in similar contexts. for
   instance, sentences within a paragraph and paragraphs within a document
   are semantically related. the organization of textual units   of
   different granularity   in corpora can be used to learn distributed
   representations. contextual relationships of textual units are
   exploited to design training objectives similar to neural language
   models.

   we refer to models that rely on the extended id65
   principle to obtain distributed ttu representations as learn to predict
   models, in accordance with the context-predicting label used for neural
   language models. learn to predict models are neural network models
   trained using unlabelled data to maximize the likelihood of the context
   of a ttu. among the models reviewed in hill et al. ([257]2016),
   skip-thought vector (kiros et al. [258]2015) and paragraph vector (pv)
   (le and mikolov [259]2014) are successful representatives of the learn
   to predict context idea. the training objective of the skip-thought
   vector is to maximize the likelihood of the previous and the next
   sentences given an input sentence. the context of the sentence is
   defined as its neighboring sentences. for details of the pv model, see
   sect. [260]3.

   the main context of a textual unit consists of the words it contains.
   containment should be seen as a form of co-occurrence and the content
   of a document is required to define its textual context. two documents
   are similar if they contain similar words. besides the content,
   temporal context is useful for defining ttu contexts. for instance, the
   context of the query can be defined by other queries in the same
   session (grbovic et al. [261]2015b). finally, the context of a document
   is defined by joining its content and the neighboring documents in a
   document stream in djuric et al. ([262]2015).
   learn to generate this category covers work in which a synthetic
   textual unit is generated based on the distributed representation of an
   input ttu. studies in this category are motivated by successful
   applications of id56s, id137, and encoder-decoder architectures,
   illustrated in fig. [263]10, to sequence-to-sequence learning (graves
   [264]2012) tasks such as machine translation (cho et al. [265]2014) and
   image captioning (xu et al. [266]2015). in these applications, an input
   textual or visual object is encoded into a distributed representation
   with a neural network and a target sequence of words   a translation in
   the target language or a caption   is generated by a decoder network.
   [267]open image in new window fig. 10
   fig. 10

   encoder decoder architecture. note that the encoder is the scn

4.3 roadmap

   in table [268]2, we provide a classification of reviewed work with
   respect to the features task and approach.
   table 2

   classification of reviewed work

   section

   task

   approach


   publications

   [269]5

   ad-hoc retrieval

    [270]5.1.1

   document ranking

   aggregate

   explicit

   boytsov et al. ([271]2016), clinchant and perronnin ([272]2013),
   ganguly et al. ([273]2016), mitra et al. ([274]2016), nalisnick et al.
   ([275]2016), vulic and moens ([276]2015)


   implicit

   ganguly et al. ([277]2015), rekabsaz et al. ([278]2016b), roy et al.
   ([279]2016a), zuccon et al. ([280]2015)

    [281]5.1.2

   document ranking

   learn

   learn to match

   guo et al. ([282]2016b), huang et al. ([283]2013), li et al.
   ([284]2014), nguyen et al. ([285]2016), palangi et al. ([286]2014,
   [287]2016), shen et al. ([288]2014a, [289]b), ye et al. ([290]2015)


   learn to predict

   ai et al. ([291]2016a), djuric et al. ([292]2015)


   learn to generate

   lioma et al. ([293]2016)

    [294]5.2

   query re-weighting


   explicit

   zheng and callan ([295]2015)

    [296]5.3.1

   id183

   aggregate

   explicit

   almasri et al. ([297]2016), amer et al. ([298]2016), rekabsaz et al.
   ([299]2016a), roy et al. ([300]2016b)


   implicit

   diaz et al. ([301]2016), zamani and croft ([302]2016a, [303]b)

    [304]5.3.2


   learn

   learn to autoencode

   gupta et al. ([305]2014)


   learn to match

   sordoni et al. ([306]2014)

    [307]5.4.1

   diversification

   aggregate

   explicit

   onal et al. ([308]2015)

    [309]5.4.2


   learn

   learn to match

   xia et al. ([310]2016)

    [311]5.5.1

   expertise retrieval

   learn

   learn to predict

   van gysel et al. ([312]2016b)

    [313]5.6.1

   product search

   learn

   learn to match

   van gysel et al. ([314]2016a)

   [315]6

   query tasks

    [316]6.1

   query auto completion

   aggregate

   implicit

   cai and de rijke ([317]2016a)


   learn

   learn to match

   mitra ([318]2015), mitra and craswell ([319]2015)

    [320]6.2.1

   query suggestion

   learn

   learn to generate

   sordoni et al. ([321]2015)

    [322]6.3

   query classification

   aggregate

   explicit

   zamani and croft ([323]2016b)

    [324]6.4

   proactive search

   learn

   learn to generate

   luukkonen et al. ([325]2016)

   [326]7

   id53

    [327]7.1.1

   answer sentence retrieval

   learn

   learn to match

   severyn and moschitti ([328]2015), suggu et al. ([329]2016), wang and
   nyberg ([330]2015), yan et al. ([331]2016), yu et al. ([332]2014)

    [333]7.2.1

   conversational agents

   learn

   learn to match

   yan et al. ([334]2016)

   [335]8

   sponsored search

    [336]8.1

   sponsored search

   learn

   learn to match

   azimi et al. ([337]2015), zhai et al. ([338]2016a, [339]b)

    [340]8.2


   learn

   learn to predict

   grbovic et al. ([341]2015a, [342]b), zhang et al. ([343]2016a)

   [344]9

   similar item retrieval

    [345]9.1.1

   related document search

   aggregate

   explicit

   kim et al. ([346]2016), kusner et al. ([347]2014)

    [348]9.1.2


   learn

   learn to autoencode

   salakhutdinov and hinton ([349]2009)


   learn to predict

   dai et al. ([350]2014), djuric et al. ([351]2015), le and mikolov
   ([352]2014)

    [353]9.2

   detecting text reuse

   aggregate

   explicit

   zhang et al. ([354]2014)

    [355]9.3.1

   similar question retrieval

   aggregate

   explicit

   zhou et al. ([356]2015)

    [357]9.3.2


   learn

   learn to match

   lei et al. ([358]2016)


   learn to generate

   lei et al. ([359]2016)

    [360]9.4.1

   content-based recommendation

   aggregate

   explicit

   manotumruksa et al. ([361]2016)

    [362]9.4.2


   learn

   learn to match

   gao et al. ([363]2014)

   [364]10

   non-textual/behavioural


   borisov et al. ([365]2016b), zhang et al. ([366]2016a)

   in sects. [367]5   [368]9 below we survey work on neural models for
   ad-hoc retrieval, query understanding, id53, sponsored
   search and similar item retrieval, respectively.

5 ad-hoc retrieval

   in this section we survey work on neural models for ad-hoc retrieval
   tasks. we devote a subsection per task listed in table [369]2 under
   ad-hoc retrieval and follow the how feature as explained in
   sect. [370]4 for organizing the subsections.

5.1 document ranking

5.1.1 aggregate

   in this section, we present publications that rely on pre-trained word
   embeddings for ad-hoc retrieval under the implicit and explicit
   categories.

   explicit clinchant and perronnin ([371]2013) present the earliest work
   using id27s in ir. the context-counting model latent semantic
   indexing (lsi) (deerwester et al. [372]1990) is used to induce word
   embeddings, which are then transformed into fixed-length fisher vector
   (fvs) via jaakkola et al. ([373]1999)   s fisher kernel (fk) framework.
   the fvs are then compared via cosine similarity for document ranking.
   experiments on ad-hoc search using lemur are reported for three
   collections: trec robust04, trec disks 1&2, and english clef 2003
   ad-hoc. while results show improvements over standard lsi on all three
   collections, standard tf-idf performs slightly better on two of the
   three collections, and divergence from randomness (dfr) (amati and
   van rijsbergen [374]2002) performs far better on all collections. the
   authors note that thse    results are not surprising as it has been shown
   experimentally in many studies that latent-based approaches such as lsi
   are generally outperformed by state-of-the-art ir models in ad-hoc
   tasks.   

   vulic and moens ([375]2015) are the first to aggregate id27s
   learned with a context-predicting distributional semantic model (dsm).
   query and document are represented as a sum of id27s learned
   from a pseudo-bilingual document collection with a skip-gram model
   (ganguly et al. [376]2016 discusses why such a representation for
   documents could be noisy). each document pair in a document-aligned
   translation corpus is mapped to a pseudo-bilingual document by merging
   source and target documents, removing sentence boundaries and shuffling
   the complete document. owing to these shuffled pseudo-bilingual
   documents, words from the source and target language are mapped to the
   same embedding space. this approach for learning bilingual word
   embeddings is referred to as bilingual id27s skip-gram
   (bwesg). documents are ranked by the cosine similarity of their
   embedding vector to the query vector. the query-document
   representations are evaluated both on cross-lingual and mono-lingual
   retrieval tasks. for the monolingual experiments, ranking the proposed
   distributed representations outperforms ranking lda representations.
   while this approach is simple and able to benefit from a potentially
   vast body of comparable versus parallel corpora for training, random
   shuffling loses the precise local context windows exploited by id97
   training (which parallel corpora would provide), effectively setting
   context window size to the length of the entire document. the approach
   and experimental setup otherwise follows the monolingual version of the
   authors    method. cross-lingual results for clef 2001   2003 ad-hoc
   english-dutch show that the embedding approach outperforms the unigram
   baseline and is comparable to the lda baseline. as with monolingual
   results, the mixture models perform better, with a cross-lingual three
   way mixture of unigram, lda, and embedding. no experiments are reported
   with parallel corpora for comparison, which would be interesting for
   future work.

   mitra et al. ([377]2016) and nalisnick et al. ([378]2016) propose the
   dual embedding space model (desm), writing that    a crucial detail often
   overlooked when using id97 is that there are two different sets of
   vectors ... in and out embedding spaces [produced by id97].... by
   default, id97 discards \(w_ out \) at the end of training and
   outputs only \(w_ in \)...    in contrast, the authors retain both input
   and output embeddings. nalisnick et al. ([379]2016) point out that
   within the same embedding space, either in or out, the neighbors are
   functionally similar words. however, the neighbors of a word
   represented with its in embedding vector in the out space, are
   topically similar words. topically similar words are likely to co-occur
   in a local context whereas functionally similar words are likely to
   occur in similar contexts. for instance, for the term harvard, the
   terms faculty, alumni, and graduate are topically similar terms and
   yale, stanford, cornell are functionally similar terms. motivated by
   this observation, nalisnick et al. ([380]2016) propose the desm.

   in the desm, query terms are mapped to the in space and document words
   to the out space. documents are embedded by taking an average (weighted
   by document term frequency) over embedding vectors of document terms.
   query-document relevance is computed by average cosine similarity
   between each query term and the document embedding. the authors induce
   id27s via id97 cbow only, though they note that skip-gram
   embeddings could be used interchangeably. experiments with ad-hoc
   search are carried out on a proprietary web collection using both
   explicit and implicit relevance judgments. in contrast with dssm (huang
   et al. [381]2013), full web page documents are indexed instead of only
   page titles. desm   s in and out embedding space combinations are
   compared to baseline retrieval by bm25 and latent semantic analysis
   (lsa) (deerwester et al. [382]1990). out of vocabulary (oov) query
   terms are ignored for the desm approach but retained for the baselines.
   results show    desm to be a poor standalone ranking signal on a larger
   set of documents,    so a re-ranking approach is proposed in which the
   collection is first pruned to all documents retrieved by an initial
   bing search, and then re-ranked by desm. re-ranking results show
   improvements over baselines, especially on the implicit feedback test
   set, with best performance obtained when id27s are trained on
   queries and using in-out embedding spaces in document ranking. the
   authors surmise that training on queries performs better due to users
   tending to include only significant terms from their queries. learned
   id27s are shared online (see appendix    id27s   ).

   in ganguly et al. ([383]2016), documents are modelled as a mixture
   distribution that generates the observed terms in the document. ganguly
   et al. ([384]2016) estimate this distribution with id116 id91
   of embeddings of the terms in the document. the likelihood of a query
   to be generated by the document is computed by the average
   inter-similarity of the set of query terms to the centroids of clusters
   in the document, in the id27 space. for efficiency, the
   global vocabulary is clustered using id97 embeddings in advance and
   document specific clusters are created by grouping the terms according
   to their global cluster ids. a centroid-based query likelihood function
   is evaluated in combination with id38 with jelinek-mercer
   smoothing on the trec 6-7-8 and trec robust data sets. a significant
   improvement is observed by the inclusion of id27-based
   query-likelihood function over the standalone language model (lm)
   baseline.

   boytsov et al. ([385]2016) consider the cosine similarity between the
   averaged id27 vectors of the query and document as a
   similarity function, in id92 based retrieval. the authors propose to
   replace the traditional term-based search by id92 based retrieval.
   exact id92 search fails to be efficient yet approximation algorithms,
   such as small-world graph and neighbourhood approximations (napp), are
   proposed as remedies. experiments are performed on the yahoo answers
   and stack overflow data sets. the cosine-similarity between averaged
   id27s is found to be less effective than bm25 and cosine
   similarity of tf-idf vectors.

   implicit zuccon et al. ([386]2015) propose a neural translation
   language model (nltm), which integrates id27s into berger and
   lafferty ([387]1999)   s classic translation model approach to
   query-likelihood ir. they estimate the translation id203 between
   terms as the cosine similarity of the two terms divided by the sum of
   the cosine similarities between the translating term and all of the
   terms in the vocabulary. previous state-of-the-art translation models
   use mutual information (mi) embeddings to estimate translation
   probabilities. experiments evaluating nltm on ad-hoc search are
   reported on the trec datasets ap87-88, wsj87-92, dotgov, and medtrack.
   results indicate that nltm provides moderate improvements over the mi
   and classic tm systems, based on modest improvements to a large number
   of topics, rather than large differences on a few topics. sensitivity
   analysis of the various model hyper-parameters for inducing word
   embeddings shows that manipulations of embedding dimensionality,
   context window size, and model objective (cbow vs. skip-gram) have no
   consistent impact upon nltm   s performance versus the baselines.
   regarding the choice of training corpus for learning embeddings versus
   search effectiveness, although effectiveness typically appears highest
   when embeddings are estimated using the same collection in which search
   is to be performed, the differences are not statistically significant.
   source code and learned embeddings are shared online (see appendix
      id27s   ).

   rekabsaz et al. ([388]2016b) investigate a set of existing models
   including pivoted document id172, bm25, bm25 verboseness aware,
   multi-aspect tf, and id38 by generalizing the translation
   model to the probabilistic relevance framework. they extend the
   translation models in pseudo-relevance (pr) framework by integrating
   the effect of changing term frequencies. for experimental evaluation 6
   test collections are used: a combination of trec 1 to 3, trec-6,
   trec-7, and trec-8 of the adhoc track, trec-2005 hard track, and clef
   ehealth 2015 task 2 user-centred health information retrieval. in terms
   of baseline models, rekabsaz et al use (1) the original version of the
   extended models; (2) the query expanded model using the logarithm
   weighting model; and (3) the query expanded model with the
   id172 over the expanded terms. the id74 are map
   and ndcg@20. experimental results show that the newly proposed models
   achieve state-of-the-art results.

   cosine similarity of id97 embeddings is used in a similar way in
   the generalized language model (glm) (ganguly et al. [389]2015).
   ganguly et al. ([390]2015) propose glm for integrating id27s
   with the query-likelihood language modelling. semantic similarity
   between query and document/collection terms is measured by cosine
   similarity between id27s induced via id97 cbow. the
   authors frame their approach in the context of classic global versus
   local term similarity, with id27s trained without reference
   to queries representing a global approach akin to the latent dirichlet
   allocation (lda) of wei and croft ([391]2006). like rekabsaz et al.
   ([392]2016a) and zuccon et al. ([393]2015), the authors build on berger
   and lafferty ([394]1999)   s    noisy channel    translation model. smoothing
   of mixture model components resembles classic cluster-based lm
   smoothing of liu and croft ([395]2004). ad-hoc search results reported
   for trec 6   8 and robust using lucene show improvements over both
   unigram query-likelihood and lda. however, the parameters appear to be
   tuned on the test collections, and lda results are much lower than wei
   and croft ([396]2006)   s, which the authors hypothesize is due to
   training lda on the entire collection rather than on collection
   subsets. the authors do not compare their global approach versus local
   pseudo-relevance feedback (prf) (lavrenko and croft [397]2001), which
   prior work has shown to outperform lda (yi and allan [398]2009); while
   typically a query-time technique, it can be approximated for greater
   efficiency (cartright et al. [399]2010).

   roy et al. ([400]2016a) proposes a relevance feedback model that
   employs id27s, based on the intuition that adding word
   embeddings can result in semantic composition of individual words. they
   buid a id81s around the query id27s that are
   treated as data points. then they estimate the kernel density of the
   id203 density function that generates the query id27s.
   the query term can control the shape of the estimated id203
   density function. in this way, they develop a relevance feedback model
   that integrates semantic relationships and compositionality of words.
   documents are then ranked according to their kl divergence from the
   estimated kernel density function. they evaluate their approach on trec
   6   8 and robust adhoc news retrieval and the trec 9   10 wt10g web
   retrieval test collections. they compare their method with the standard
   relevance model that only uses statistical co-occurrences between query
   terms and words in top ranked documents, and find that it significantly
   outperforms the baseline. a future direction might be exploring larger
   units embeddings such as sentence and paragraph embeddings.

5.1.2 learn

   learn to match

   representation based the deep structured semantic model (dssm) (huang
   et al. [401]2013) is the earliest neural representation based learn to
   match model for document ranking. dssm was one of the pioneering models
   incorporating click-through data in deep nns. it has been built on by a
   variety of others (mitra [402]2015; mitra and craswell [403]2015; shen
   et al. [404]2014a, [405]b; ye et al. [406]2015). other work in this
   category is either an architectural variant of dssm with different scns
   or they propose novel ways of using distributed representations by dssm
   variants in order to improve retrieval effectiveness. architectural
   variants such as the convolutional latent semantic model (clsm) (shen
   et al. [407]2014a) and lstm deep structured semantic model (lstm-dssm)
   (palangi et al. [408]2014, [409]2016) differ from dssm in the input
   representations and architecture of the scn component. besides
   architectural variants with different scn types, nguyen et al.
   ([410]2016) propose two high level views of how to incorporate a
   knowledge base (kb) graph into dssm (huang et al. [411]2013). li et al.
   ([412]2014) utilize distributed representations produced by dssm and
   clsm in order to re-rank documents based on in-session contextual
   information. ye et al. ([413]2015) question the assumptions about
   clicked query-document pairs in order to derive triplets for training
   variants of the dssm models.

   the scn of the dssm model is composed of a deep neural network with
   three non-linear layers placed on top of a word hashing layer.
   documents are indexed only by title text rather than the entire body
   text. the query and the document are first modeled as two high
   dimensional term vectors (i.e., a bag-of-words representation). each
   term vector is mapped to a trigram vector by the word hashing layer, in
   order to cope with a large vocabulary. for instance, the word vector is
   mapped to {.ve, vec, ect, cto, tor ,or.} where the dot sign is used as
   the start and end character. this low-dimensional trigram vector is
   then considered as input to the scn. the vocabulary size is reduced
   from 500k to 30k by replacing each term with its letter trigrams.
   trigram hashing also helps to address out of vocabulary (oov) query
   terms not seen in training data. the authors do not discuss how to
   mitigate hashing collisions; while they show that such collisions are
   relatively rare (e.g., \(0.0044\%\) for the 500k vocabulary size), this
   stems in part from indexing document titles only.

   convolutional deep structured semantic models (c-dssm) (shen et al.
   [414]2014b) extend dssm by introducing a id98 with max-pooling as the
   scn in the dssm architecture (c-dssm). it first uses word hashing to
   transform each word into a vector. a convolutional layer then projects
   each word vector within a context window to a local contextual feature
   vector. it also incorporates a max-pooling layer to extract the most
   salient local features to form a fixed-length global feature vector for
   queries and web documents. the main motivation for the max-pooling
   layer is that because the overall meaning of a sentence is often
   determined by a few key words, simply mixing all words together (e.g.,
   by summing over all local feature vectors) may introduce unnecessary
   divergence and hurt overall semantic representation effectiveness. this
   is a key difference between dssm and c-dssm.

   both dssm (huang et al. [415]2013) and c-dssm (shen et al. [416]2014b)
   fail to capture contextual information of queries and documents. to
   address this, shen et al. ([417]2014a) propose a convolutional latent
   semantic model (clsm) built on top of dssm. clsm captures contextual
   information by a series of projections from one layer to another in a
   id98 architecture (lecun and bengio [418]1995). the first layer consists
   of a word id165 followed by a letter trigram layer where each word
   id165 is composed of its trigram representation, a form of word
   hashing technique developed in dssm. then, a convolution layer
   transforms these trigrams into contextual feature vectors using the
   convolution matrix \(w_c\), which is shared among all word id165s.
   max-pooling is then performed against each dimension on a set of
   contextual feature vectors. this generates the global sentence level
   feature vector v. finally, using the non-linear transformation \(\tanh
   \) and the semantic projection matrix \(w_s\), they compute the final
   latent semantic vector for the query/document. the parameters \(w_c\)
   and \(w_s\) are optimized to maximize the same id168 used by
   huang et al. ([419]2013) for dssm. even though clsm introduces word
   id165s to capture contextual information, it suffers from the same
   problems as dssm, including scalability. for example, clsm performs
   worse when trained on a whole document than when trained on only the
   document title (guo et al. [420]2016a).

   to sum up the architectural variants, dssm takes a term vector of the
   textual unit and treats it as a bag of words. in contrast, c-dssm, clsm
   and lstm-dssm take a sequence of one-hot vectors of terms and treat the
   ttus as a sequence of words. clsm includes a convolutional neural
   network and lstm-dssm includes an lstm network as scn. the word hashing
   layer is common to all of these models. dssm, clsm and lstm-dssm are
   evaluated on large-scale data sets from bing in huang et al.
   ([421]2013), shen et al. ([422]2014a, [423]b). in huang et al.
   ([424]2013), dssm is trained on document title-query pairs and is shown
   to outperform the word translation model, bm25, tf-idf and bilingual
   topic models with posterior id173 in terms of ndcg at cutoff
   values 1, 3 and 10. however, later work guo et al. ([425]2016a)
   performs two further experiments with dssm: indexing only document
   titles versus indexing entire documents (i.e., full-text search). guo
   et al. ([426]2016a)   s results indicate that full-text search with dssm
   does not perform as well as traditional ir models. in shen et al.
   ([427]2014a), clsm is shown to be more effective with document titles.
   finally, shen et al. ([428]2014a) and palangi et al. ([429]2016) report
   that clsm outperforms dssm and lstm-dssm outperforms clsm when document
   titles are used instead of full documents.

   liu et al. ([430]2015) propose a neural model with multi-task
   objectives. the model integrates a deep neural network for query
   classification and the dssm model for web document ranking via shared
   layers. the word hashing layer and semantic representation layer of
   dssm are shared between the two models. the integrated network
   comprises separate task-specific semantic representation layers and
   output layers for two different tasks. a separate cost function is
   defined for each task. during training, in each iteration, a task is
   selected randomly and the model is updated only according to the
   selected cost function. the proposed model is evaluated on large-scale
   commercial search logs. experimental results show improvements by the
   integrated model over both standalone deep neural networks for query
   classification and a standalone dssm for web search ranking.

   li et al. ([431]2014) utilize distributed representations produced by
   dssm and clsm in order to re-rank documents based on in-session
   contextual information. similarity of query-query and query-document
   pairs extracted from the session context is computed using dssm and
   clsm vectors. these similarity scores are included as additional
   features to represent session context in a context-aware learning to
   rank framework. they evaluate xcode (internally developed by the
   authors), dssm (huang et al. [432]2013), and c-dssm (shen et al.
   [433]2014b) as models for deriving these contextual features and find
   that dssm offers the highest performance, followed by c-dssm. though
   they expected c-dssm to offer the highest performance, they note that
   c-dssm could only be trained on a small dataset with bounded size, in
   contrast to dssm, which could be trained on a larger dataset.
   additionally, they note that observed performance differences may be
   due to imperfect tuning of model parameters, such as sliding window
   size for c-dssm. nevertheless, contextual features derived using both
   dssm and c-dssm offer performance benefits for re-ranking.

   nguyen et al. ([434]2016) propose two high level views of how to
   incorporate a knowledge base (kb) graph into a ranking model like dssm
   (huang et al. [435]2013). to the best of our knowledge, this is one of
   the first ir studies that tries to incorporate kbs into a deep neural
   structure. the authors    first model exploits kbs to enhance the
   representation of a document-query pair and its similarity score by
   exploiting concept embeddings learned from the kb distributed
   representation (faruqui et al. [436]2014; xu et al. [437]2014) as input
   to deep nnss like dssm. the authors argue that this hybrid
   representation of the id65 (namely, word
   embeddings) and the symbolic semantics (namely, concept embeddings
   taking into account the graph structure) would enhance document-query
   matching. their second model uses the knowledge resource as an
   intermediate component that helps to translate the deep representation
   of the query towards the deep representation of documents for an ad-hoc
   ir task. strong empirical evidence is still needed to demonstrate that
   adding a kb does in indeed benefit the deep neural architecture for
   capturing semantic similarity.

   ye et al. ([438]2015) question assumptions about clicked query-document
   pairs in order to derive triplets for training dssm variant models.
   according to ye et al. ([439]2015) these three assumptions are:
   (1) each clicked query-document pair is equally weighted; (2) each
   clicked query-document pair is a 100% positive example; and (3) each
   click is solely due to semantic similarity. the authors relax these
   assumption and propose two generalized extensions to dssm: gdssm1 and
   gdssm2. while dssm models the id203 of a document being clicked
   given a query and the semantic similarity between document and query,
   gdssm1 uses more information in its id168, incorporating the
   number of users seeing and the proportion clicking for each
   query-document pair. gdssm2 conditions on lexical similarity in
   addition to semantic similarity.
   interaction based guo et al. ([440]2016a)   s deep relevance matching
   model (drmm) is one of the first nn ir models to show improvement over
   traditional ir models (though the comparison is against bag-of-words
   approaches rather than term proximity baselines). the authors
   hypothesize that deep learning methods developed and/or commonly
   applied in nlp for semantic matching may not be suited for ad-hoc
   search, which is most concerned with relevance matching. they
   articulate three key differences they perceive between semantic and
   relevance matching:
    1. 1.
       semantic matching looks for semantic similarity between terms;
       relevance matching puts more emphasis on exact matching.

    2. 2.
       semantic matching is often concerned with how composition and
       grammar help to determine meaning; varying importance among query
       terms is more crucial than grammar in relevance matching.

    3. 3.
       semantic matching compares two whole texts in their entirety;
       relevance matching might only compare parts of a document to a
       query.

   the raw inputs to the drmm are term embeddings. the first layer
   consists of matching histograms of each query term   s cosine similarity
   scores with each of the document terms. on top of this histogram layer
   is an nn outputting a single node for each query term. these outputs
   are multiplied by query term importance weights calculated by a term
   gating network and then summed to produce the final predicted matching
   score for the query/document pair. the whole network is trained using a
   margin ranking id168. ad-hoc search experiments are reported on
   trec robust04 and clueweb09-cat-b. baselines include: (1) traditional
   retrieval models such as bm25 and query likelihood;
   (2) representation-focused nn models, including dssm (huang et al.
   [441]2013) and c-dssm (shen et al. [442]2014b) (indexing titles vs.
   full documents), arc-i (hu et al. [443]2014); and
   (3) interaction-focused nn models, such as arc-ii (hu et al. [444]2014)
   and matchpyramid (pang et al. [445]2016b). oov terms are represented by
   random vectors (as in kenter and de rijke [446]2015), effectively
   allowing only exact matching. results show that as the cbow dimension
   increases (50, 100, 300 and 500), the performance of drrm first
   increases then slightly drops. drrm using idf and log-count histogram
   (lch) also significantly outperforms all baselines. in addition, none
   of the representation-focused and interaction-focused baselines are
   competitive with traditional retrieval models, supporting the authors   
   hypothesis that deep matching models focused on semantic matching may
   not be well-suited to ad-hoc search.

5.1.3 learn to predict

   ai et al. ([447]2016a, [448]b) investigate the use of the pv-dbow model
   as a document language model for retrieval. three shortcomings of the
   pv-dbow model are identified and an extended paragraph vector (pv)
   model is proposed with remedies for these shortcomings. first, the
   pv-dbow model is found to be biased towards short documents due to
   overfitting in training and the training objective is updated with l2
   id173. secondly, the pv-dbow model trained with neg implicitly
   weights terms with respect to inverse corpus frequencies (icf) which
   has been shown to be inferior to inverse document frequency (idf) in
   robertson ([449]2004). a document frequency based negative sampling
   strategy, which converts the problem into factorization of a shifted
   tf-idf matrix, is adopted. thirdly, the two layer pv-dbow architecture
   depicted in fig. [450]11a is introduced since word substitution
   relations, such as the relation in car-vehicle, underground-subway
   pairs, are not captured by pv-dbow. the extended paragraph vector (epv)
   is evaluated in re-ranking the set of top 2000 documents retrieved by a
   query likelihood retrieval function. document language models based on
   epv and lda are compared on trec robust04 and gov2 data sets. an
   epv-based model yields higher effectiveness scores than the lda-based
   model.
   the hierarchical document vector (hdv) (djuric et al. [451]2015) model
   extends the pv-dm model to predict not only words in a document but
   also its temporal neighbors in a document stream. the architecture of
   this model is depicted in fig. [452]11b with a word context and
   document context size of five. there, \(w_1\), \(w_2\), \(w_3\),
   \(w_4\), \(w_5\) represent a sample context of words from the input
   paragraph; \(p_1\), \(p_2\), \(p_3\), \(p_4\), \(p_5\) represent a set
   of documents that occur in the same context. in hdv, the content of the
   documents in the temporal context also contributes to the document
   representation. similar to pv-dm, the words and documents are mapped to
   d-dimensional embedding vectors. djuric et al. ([453]2015) point out
   that words and documents are embedded in the same space and this makes
   the model useful for both recommendation and retrieval tasks including
   document retrieval, document recommendation, document tag
   recommendation and keyword suggestion. given a keyword, titles of
   similar documents in the embedding space are presented to give an idea
   of the effectiveness of the model on the ad-hoc retrieval task.
   however, a quantitative evaluation is not provided for the document
   retrieval and keyword suggestion tasks.
   [454]open image in new window fig. 11
   fig. 11

   learn to predict context models for document retrieval. a two layer
   pv-dbow (ai et al. [455]2016a). b hdv (djuric et al. [456]2015) (and
   context-content2vec grbovic et al. [457]2015b, section 8.2)

5.1.4 learn to generate

   lioma et al. ([458]2016) ask whether it is possible to generate
   relevant documents given a query. a character level lstm network is
   optimized to generate a synthetic document. the network is fed with a
   sequence of words constructed by concatenating the query and context
   windows around query terms in all relevant documents for the query. for
   each query, a separate model is generated and a synthetic document is
   generated for the same query with the learned model. the synthetic
   document was evaluated in a id104 setting. users are provided
   with four word clouds that belong to three known relevant documents and
   the synthetic document. each word cloud is built by selection of top
   frequent terms from the document. users are asked to select the most
   relevant word cloud. author report that the word cloud of the synthetic
   document ranked the first or second for most of the queries.
   experiments were performed on the trec disks 4, 5 test collection with
   title-only queries from trec 6, 7, 8.

5.2 query re-weighting

5.2.1 aggregate

   implicit palakodety and callan ([459]2014)   s work on id183 is
   based on id27s; although originally proposed for result
   merging in federated web search, we note it here since it introduces
   the idea of an importance vector in the query re-weighting method in
   zheng and callan ([460]2015). palakodety and callan ([461]2014)
   represent a query by the mean vector of query term embeddings and
   k-nearest neighbors of the query vector are selected to expand the
   query. expansion terms are re-weighted with respect to their distance
   to the query vector. a query term that is more distant to the query
   vector is assumed to contain more information and is assigned a higher
   weight. query re-weighting is modelled as a id75 problem
   from importance vectors to term weights in zheng and callan
   ([462]2015). the importance vector, which is the offset between the
   query vector and the term vector, is used as the feature vector for the
   query term. a id75 model is trained using the ground-truth
   term weights computed by term recall weights which is the ratio of
   relevant documents that contain the query term t to the total number of
   relevant documents to the query q. weighted queries based on a learned
   regression model are evaluated in a retrieval setting and compared
   against the lm and bm-25 models using the data sets robust04, wt10g,
   gov2, clueweb09b. two variants of the model, deeptr-bow and deeptr-sd,
   are compared against unweighted queries, sequential dependency models
   and weighted sequential dependency models. statistically significant
   improvements are observed at high precision levels and throughout the
   rankings compared to the first two methods.

5.3 id183

5.3.1 aggregate

   explicit almasri et al. ([463]2016) propose a heuristic method vexp for
   term-by-term id183 using embedding vectors. for each query
   term, the most similar terms in the embedding space are added to the
   query. expansion terms are weighted in proportion to their frequency in
   the expanded query. experiments with ad-hoc search use indri on four
   clef medical test collections: image2010   2012 (short documents and
   queries, text-only) and case2011 (long documents and queries).
   baselines include pseudo-relevance feedback (lavrenko and croft
   [464]2001) and mutual information. they evaluate both cbow and
   skip-gram id97 embeddings (using default dimensionality and context
   window settings) but present only skip-gram results, noting    there was
   no big difference in retrieval performance between the two.    the
   authors consider adding a fixed number of 1   10 expansion terms per
   query term and also compare two smoothing methods: linear
   jelineck-mercer versus dirichlet. results show vexp achieves higher
   mean average precision (map) than other methods.

   roy et al. ([465]2016b) propose a set of id183 methods, based
   on selecting k nearest neighbors of the query terms in the word
   embedding space and ranking these terms with respect to their
   similarity to the whole query. for each nearest neighbor, they
   calculate the average cosine similarity versus all query terms,
   selecting the top-k terms according to average cosine score. the second
   approach (reduces the vocabulary space considered by id92 by only
   considering terms appearing in the top m pseudo-relevant documents
   retrieved by the query. in the third approach, an iterative (and
   computationally expensive) pruning strategy is applied to reduce the
   number of nearest neighbors, assuming that nearest neighbors are
   similar to one another. all expansion methods yielded lower
   effectiveness scores than a statistical co-occurrence based feedback
   method, in experiments with the trec 6, 7 and 8 and trec robust data
   set and the lm with jelinek mercer smoothing as the retrieval function.

   amer et al. ([466]2016) investigate id27s for personalized
   id183 in the domain of social book search.^[467]4 while
   personalized id183 is not new (carmel et al. [468]2009;
   chirita et al. [469]2007), use of id27s for personalization
   is novel. the proposed method consists of three steps: user modeling,
   term filtering and selection of expansion terms. a user is modeled as a
   collection of documents, and query terms are filtered to remove
   adjectives, which may lead to noisy expansion terms. for each remaining
   query term, similar to roy et al. ([470]2016b)   s id92 approach, the
   top-k most similar terms are selected based on cosine similarity in the
   embedding space. evaluation on the social book search task compares
   id97 trained on personalized versus non-personalized training sets.
   however, results show that expansion via id27s strictly hurts
   performance versus no expansion at all, in contrast to mitra and
   craswell ([471]2015), roy et al. ([472]2016b). this may stem from
   training id97 embeddings only on social book search documents.
   results further suggest that personalized id183 does not
   provide improvements over non-personalized id183 using word
   embedding. the authors postulate that sparse training data for
   personalization is the main problem here.

   rekabsaz et al. ([473]2016a) recommend choosing similar terms based on
   a global similarity threshold rather than by id92 because some terms
   should naturally have more similar terms than others. they choose the
   threshold by setting it such that for any term, the expected number of
   related terms within the threshold is equal to the average number of
   synonyms over all words in the language. this method avoids having to
   constrain or prune the id92 technique as in roy et al. ([474]2016b).
   they use multiple initializations of the id97 skip-gram model to
   produce a id203 distribution used to calculate the expected
   cosine similarity, making the measure more robust against noise.
   experiments on trec 6   8 and hard 2005 incorporate this
   threshold-setting method into a translation language model (berger and
   lafferty [475]1999) for ad-hoc retrieval and compare against both a
   language model baseline and a translation language model that uses id92
   to select similar words. the threshold-based translation language model
   achieves the highest mean average precision (map).

   implicit in diaz et al. ([476]2016), zamani and croft ([477]2016a),
   id27s are used for defining a new query language model (qlm).
   a qlm specifies a id203 distribution \(p(w\mid q)\) over all
   terms in the vocabulary. in id183 with id38, the
   top m terms w that have the highest \(p(w\mid q)\) value are selected
   as expansion terms (carpineto and romano [478]2012). diaz et al.
   ([479]2016), propose a id183 language model based on word
   embeddings learned from topic-constrained corpora. when id27s
   are learned from a topically-unconstrained corpora, they can be very
   general. therefore, a query language model is defined based on word
   embeddings learned using a subset of documents sampled from a
   multinomial created by applying softmax on kl divergence scores of all
   documents in the corpus. the original query language model is
   interpolated with the id183 language model which is defined
   by weights of terms computed by the \(uu^tq\) where u is the \(|v|
   \times d\) dimensional embedding matrix and q is the \(|v| \times 1\)
   dimensional term matrix. locally-trained embeddings are compared
   against global embeddings on trec12, robust and clueweb 2009 category b
   web corpus. local embeddings are shown to yield higher ndcg@10 scores.
   besides the local and global option, the authors also investigate the
   effect of using the target corpus versus an external corpus for
   learning id27s. a topically-constrained set of documents
   sampled from a general-purpose large corpus achieves the highest
   effectiveness scores.

   zamani and croft ([480]2016a) propose two separate qlms and an extended
   relevance model (lavrenko and croft [481]2001) based on word
   embeddings. in the first qlm, \(p(w\mid q)\) is computed by multiplying
   likelihood scores \(p(w\mid t)\) given individual query terms whereas
   in the second qlm, \(p(w\mid q)\) is estimated by an additive model
   over \(p(w\mid t)\) scores. the \(p(w\mid t)\) scores are based on
   similarity of id27s. for measuring similarity of embeddings,
   a sigmoid function is applied on top of the cosine similarity in order
   to increase the discriminative ability. the reason for this choice is
   the observation that the cosine similarity of the 1000th closest
   neighbor to a word is not much lower than the similarity of the first
   closest neighbor. besides the qlms, a relevance model (lavrenko and
   croft [482]2001), which computes a feedback query language model using
   embedding similarities in addition to term matching, is introduced. the
   proposed query language models are compared to maximum likelihood
   estimation (id113), glm (ganguly et al. [483]2015; almasri et al.
   [484]2016) on ap, robust and gov2 collections from trec. the first qlm
   is shown to be more effective in id183 experiments. regarding
   the prf experiments, the embedding based relevance model combined with
   expansion using the first qlm produces the highest scores.

   zamani and croft ([485]2016b) develop a method for query embedding
   based on the embedding vectors of its individual words. the intuitiion
   is that a good query vector should yield a id203 distribution
   \(p(w\mid q) = \delta (w,q) / z\) induced over the vocabulary terms
   similar to the query language model id203 distribution (as
   measured by the kl divergence). here, w is the embedding vector of the
   word, q is the query embedding vector, \(\delta (w,q)\) is the
   similarity between the two embedding vectors, and z is a id172
   constant. for similarity, they use the softmax and sigmoid
   transformations of cosine similarity. they show that the common
   heuristic of averaging individual query term vectors to induce the
   overall query embedding is actually a special case of their proposed
   theoretical framework for the case when vector similarity is measured
   by softmax and the query language model is estimated by maximum
   likelihood. experiments with ad-hoc search using galago are reported
   for three trec test collections: ap, robust04, and gov2, using keyword
   trec title queries. id27s are induced by glove (pennington
   et al. [486]2014). the glove are trained from a 6 billion token
   collection (wikipedia 2014 plus gigawords 5). two different methods are
   used for estimating the query language model: via pseudo relevance
   feedback (prf) (lavrenko and croft [487]2001), which they refer to as
   pseudo query vector (pqv), and via maximum-likelihood estimation (id113).
   two methods are used for calculating similarity: softmax and sigmoid.
   softmax is easy to use because of the closed form solution, but the
   sigmoid function consistently showed better performance, likely due to
   the flexibility provided by its two free parameters. although pqv shows
   higher mean average precision, precision of top documents appears to
   suffer. results also emphasize the importance of training data
   selection. embeddings trained from the same domain as the documents
   being searched perform better than embeddings trained on a larger
   dataset from a different domain.

5.3.2 learn

5.3.3 learn to autoencode

   gupta et al. ([488]2014) explore id183 in mixed-script ir
   (msir), a task in which documents and queries in non-roman written
   languages can contain both native and transliterated scripts together.
   id30 from the observation that id68s generally use roman
   letters in such a way as to preserve the original-language
   pronunciation, the authors develop a method to convert both scripts
   into a bilingual embedding space. therefore, they convert terms into
   feature vectors of the count of each roman letter. an auto-encoder is
   then trained for id183 by feeding in the concatenated native
   term feature vector and transliterated term feature vector. the output
   is the low-dimensional embedding in the abstract space. the training of
   the autoencoder includes greedy layer-wise pretraining and fine-tuning
   through id26. experiments are conducted on shared task data
   from fire (see appendix    test corpora used for retrieval experiments   ).
   results suggest that their method significantly outperforms other
   state-of-the-art methods. source code for the model is shared online
   (see appendix    publicly available implementations of existing neural ir
   models   ).

5.3.4 learn to match

   sordoni et al. ([489]2014) propose a supervised quantum id178
   minimization (qem) approach for finding semantic representations of
   concepts, such as words or phrases, for id183. the authors
   suggest that text sequences should not lie in the same semantic space
   as single terms because their information content is higher. to this
   end, concepts are embedded in rank-one matrices while queries and
   documents are embedded as a mixture of rank-one matrices. this allows
   documents and queries to lie in a larger space and carry more semantic
   information than concepts, thereby achieving greater semantic
   resolution. for learning parameters of density matrices, qem   s gradient
   updates are a refinement of updates in both bai et al. ([490]2009)   s
   supervised semantic indexing (ssi) and rocchio ([491]1971); a query
   concept embedding moves toward the embeddings of relevant documents   
   concepts and away from the embeddings of non-relevant documents   
   concepts. this has the effect of selecting which document concepts the
   query concept should be aligned with and also leads to a refinement of
   rocchio: the update direction for id183 is obtained by
   weighting relevant and non-relevant documents according to their
   similarity to the query. due to lack of public query logs, qem is
   trained on wikipedia anchor logs (dang and croft [492]2010).
   experiments conducted on clueweb09-cat-b with trec 2010   2012 web track
   topics show qem outperforms gao et al. ([493]2010)   s concept
   translation model (ctm) (statistically significant differences), and
   ssi (occasionally statistically significant differences). qem notably
   achieves improved precision at top-ranks. also notable is qem   s ability
   to find useful expansion terms for longer queries due to higher
   semantic resolution. additional preliminary experiments with weston
   et al. ([494]2010)   s weighted approximate-rank pairwise loss yields
   further improvements for qem over baselines.

5.4 result diversification

5.4.1 aggregate

   explicit onal et al. ([495]2015) propose to utilize glove embeddings in
   order to overcome the vocabulary gap between various ttu pairs, such as
   tweet-tweet, query-tweet and aspect-tweet pairs, in tweet search result
   diversification. diversification algorithms rely on textual similarity
   functions in order to select the optimal set of results that maximizes
   both novelty and relevance. exact matching functions fail to
   distinguish the aspects expressed with very short textual content. ttus
   are expanded with k nearest neighbors of the terms in the glove
   embedding space. as similarity functions cannot distinguish details at
   the aspect level, marginal improvement are not observed in \(\alpha
   \)-ndcg scores when explicit and implicit diversification algorithms
   are run with the expanded ttus.

5.4.2 learn

   learn to match prior state-of-the-art methods for diversifying search
   results include the relational learning-to-rank framework (r-ltr) (zhu
   et al. [496]2014) and the id88 algorithm using measures as
   margins (pamm) (xia et al. [497]2015). these prior methods either use a
   heuristic ranking model based on a predefined document similarity
   function, or they automatically learn a ranking model from predefined
   novelty features often based on cosine similarity. in contrast, xia
   et al. ([498]2016) take automation a step further, using neural tensor
   networks (ntn) to learn the novelty features themselves. the ntn
   architecture was first proposed to model the relationship between
   entities in a id13 via a bilinear tensor product (socher
   et al. [499]2013). the model here takes a document and a set of other
   documents as input. the architecture uses a tensor layer, a max-pooling
   layer, and a linear layer to output a document novelty score. the ntn
   augmentations of r-ltr and pamm perform at least as well as those
   baselines, showing how the ntn can remove the need for manual design of
   functions and features. it is not clear yet whether using a full tensor
   network works much better than just using a single slice of the tensor.

5.5 expertise retrieval

5.5.1 learn

   learn to predict  van gysel et al. ([500]2016b) propose an unsupervised
   discriminative log-linear model for the retrieval of the authors with
   the most expertise, given a topic as the query. contextual relations to
   be used for training are derived from the author-document relations.
   authors split each document into id165s and compute id203
   distribution of authors given an id165. the training objective is to
   optimize the cross-id178 between the predicted distribution and the
   oracle distribution derived from the contextual relations, over the
   collection of id165s. the neural model is a single layer neural
   network which takes a word as the input and predicts a id203
   distribution over the authors. the id203 of an author given a
   sequence of words is computed by the multiplication of the id203
   values given each word, assuming conditional independence of expertise
   given words. id27s and the author embeddings are learned
   jointly. the proposed log-linear model is shown to outperform the
   state-of-the-art vector space-based entity ranking and language
   modeling approaches, in terms of precision, in experiments on the trec
   enterprise track 2005   2008 data sets and a university of tilburg
   dataset. the authors note the need for improving the scalability of the
   model with respect to the number of authors.

5.6 product search

5.6.1 learn

   learn to match  van gysel et al. ([501]2016a) describe a latent
   semantic entities (lse) method for learning a latent space model for
   product entity retrieval. products are entities that are each
   associated with a text description accompanied by user reviews. lse is
   a discriminative model that predicts id203 distributions over a
   collection of product entities given a descriptive text. the lse model
   comprises a id27 matrix, an entity embedding matrix and a
   mapping from words to entities. id27s and entity embeddings
   are learned jointly. id27s and entity embeddings have
   different dimensionality. for a given textual description, embedding of
   the predicted entity is computed by a hidden neural network over the
   average of id27s. the objective function maximizes similarity
   between the predicted and actual entity embeddings while minimizing
   similarity between the predicted embeddings and randomly sampled
   negative instances. for scalability, nce is used to approximate the
   id203 distribution over the products. although this work is
   classified as learn to match due to the training objective, embeddings
   for a fixed collection products are learned. as an outlier in the learn
   to match category, the input to the model is not a pair of textual
   units. one element of the pair is a product-id. experiments are
   conducted on amazon product data, comparing ndcg scores against three
   baseline methods including lsi, id44 (lda), and
   id97. in all four product domains from the dataset, lse outperforms
   baselines over all tested entity dimensionality sizes. lse is also
   found to provide useful additional features to a query-likelihood
   language model in learning-to-rank experiments.

6 query tasks

6.1 query auto completion

6.1.1 aggregate

   implicit  the work by cai and de rijke ([502]2016a) on query auto
   completion introduces semantic features computed using skip-gram
   embeddings, for learning to rank query auto completion candidates.
   query similarity computed by sum and maximum of the embedding
   similarity of term-pairs from queries are used as two separate
   features. the maximal embedding similarity of term pairs is found to be
   the most important feature in a diverse feature set including
   popularity-based features, a lexical similarity and another semantic
   similarity feature based on co-occurrence of term pairs in sessions.

6.1.2 learn

   learn to match the convolutional latent semantic model (clsm) has been
   used to learn distributed representations of query reformulations
   (mitra [503]2015) and of queries (mitra and craswell [504]2015). in
   both studies, clsm representations are used to build additional
   features in an existing learning to rank framework for query auto
   completion.

   mitra ([505]2015) learns embedding vectors for query reformulation
   based on query logs, representing query reformulation as a vector using
   clsm. in mitra ([506]2015), a clsm model is trained on query pairs that
   are observed in succession in search logs. this work provides an
   analysis of clsm vectors for queries similar to the id27
   space analysis in mikolov et al. ([507]2013c). mitra ([508]2015) found
   that offsets between clsm query vectors can represent intent transition
   patterns. to illustrate, the nearest neighbor query of the vector
   computed by \( vector ( university~of~washington ) - vector ( seattle )
   + vector ( chicago )\) is found to be \( vector ( university~of~chicago
   )\). besides, the offset of the vectors for \( university~of~washington
   \) and \( seattle \) is similar to the offset of the vectors for \(
   chicago~ state~university \) and \( chicago \). motivated by this
   feature of the clsm vectors, query reformulations are represented as
   the offset vector from the source query to target query (mitra
   [509]2015). id91 of query reformulations represented by the
   offset vectors yields clusters that contain pairs with similar intent
   transitions. for instance, the query reformulations in which there is
   an intent jump like avatar dragons \(\rightarrow \) facebook, are
   observed grouped in a cluster. two sets of features are then developed.
   the first feature set captures topical similarity via cosine similarity
   between the candidate embedding vector and embedding vectors of some
   past number of queries in the same session. the second set of
   reformulation features captures the difference between the candidate
   embedding vector and that of the immediately preceding query. other
   features used include non-contextual features, such as most popular
   completion, as well as contextual features, such as id165 similarity
   features and pairwise frequency features. lambdamart (wu et al.
   [510]2010) is trained on click-through data and features. results
   suggest that embedding features give considerable improvement over most
   probable completion (mpc) (bar-yossef and kraus [511]2011), in addition
   to other models lacking the embedding-based features.

   whereas popularity query auto completion methods perform well for head
   queries having abundant training data, prior methods often fail to
   recommend completions for tail queries having less usual prefixes,
   including both probabilistic algorithms (bar-yossef and kraus
   [512]2011; cai et al. [513]2014; shokouhi and radinsky [514]2012) and
   learning-based qac approaches (cai and de rijke [515]2016a; jiang
   et al. [516]2014). to address this, mitra and craswell ([517]2015)
   develop a query auto-completion procedure for such rare prefixes which
   enables query auto completion even for query prefixes never seen in
   training. the authors mine the most popular query suffixes (e.g.,
   id165s that appear at the end of a query) and append them to the user
   query prefixes, thus generating possible synthetic candidate solutions.
   to recommend possible id183, they use lambdamart (wu et al.
   [518]2010) with two sets of ranking features. the first feature set is
   the frequency of query id165s in the search log. the second feature
   set is generated by training clsm on the prefix-suffix dataset,
   sampling queries in the search logs and segmenting each query at every
   possible word boundary. results on the aol search log show significant
   improvements over mpc (bar-yossef and kraus [519]2011). the authors
   also find id165 features to be more important than clsm features in
   contributing to overall model performance.

6.2 query suggestions

6.2.1 learn to generate

   we know of no work using id56s for query suggestion prior to
   hierarchical recurrent encoder decoder (hred) by sordoni et al.
   ([520]2015) that trains a hierarchical gru model to generate
   context-aware suggestions. they first use a gru layer to encode the
   queries in each session into vector representations, then build another
   gru layer on sequences of query vectors in a session and encode the
   session into a vector representation. the model learns its parameters
   by maximizing the log-likelihood of observed query sessions. to
   generate query suggestions, the model uses a gru decoder on each word
   conditioned on both the previous words generated and the previous
   queries in the same session. the model estimates the likelihood of a
   query suggestion given the previous query. a learning-to-rank system is
   trained to rank query suggestions, incorporating the likelihood score
   of each suggestion as a feature. results on the aol query log show that
   the proposed approach outperforms several baselines that use only
   hand-crafted features. the model is also seen to be robust when the
   previous query is noisy. the authors then conduct a user study to
   evaluate the synthetic suggestions generated by the hred model. users
   are asked to classify the suggested queries as useful, somewhat useful,
   not useful categories. a total of 64% of the queries generated by the
   hred model was found to be either useful or somewhat useful by users.
   this score is higher than all the other baselines where the highest
   score for    useful or somewhat useful    is about 45%. because the model
   can generate synthetic queries, it can effectively handle long tail
   queries. however, only previous queries from the same session are used
   to provide the contextual query suggestion; the authors do not utilize
   click-through data from previous sessions. because click-through data
   provides important feedback for synthetic query suggestions,
   incorporating such click-through data from previous sessions represents
   a possible direction for future work.

6.3 query classification

6.3.1 aggregate

   explicit using data from the kdd cup 2005 task,^[521]5 zamani and croft
   ([522]2016b) propose an embedding method for categorizing queries. see
   sect. [523]5.1.1 for a description of zamani and croft ([524]2016b)   s
   overall method and results for ad-hoc search. for query classification,
   given an item from the training data query-category, the authors first
   calculate the centroid vector of all query embedding vectors under a
   category. then for a test query, they use the query embedding form and
   calculate the distance to the k nearest neighbor centroid vector.
   finally, they use a softmax function over the set of calculated
   distances to determine the final set of categories for the test query.
   because only embedding-based baselines are included in the evaluation,
   it is unclear how the proposed approach would perform versus
   traditional ir models.

6.4 proactive search

6.4.1 learn to generate

   in the only work we know of investigating neural ir for proactive
   retrieval, luukkonen et al. ([525]2016) propose lstm-based text
   prediction for id183. intended to better support proactive
   intelligent agents such as apple   s siri, ok google, etc., the lstm is
   used to generate sequences of words based on all previous words written
   by users. a id125 is used to prune out low id203 sequences.
   finally, words remaining in the pruned tree are used for query
   expansion. they evaluate their method on the abstracts of the computer
   science branch of the arxiv preprint database, downlaoded on october
   28, 2015. experimental results show that the proposed method can
   proactively generate relevant resources and improve retrieval
   precision. the authors provide several possible future directions, such
   as using the model to automatically suggest different continuations for
   user text as it is written, as done in the reactive keyboard (darragh
   et al. [526]1990) and akin to query auto completion in search engines.
   user studies are also needed to test the system   s effectiveness in the
   context of users    real-world tasks.

7 id53

7.1 answer sentence selection

7.1.1 learn to match

   methods proposed in this section all evaluate on wang et al.
   ([527]2007)   s dataset derived from trec qa 8   13. wang and nyberg
   ([528]2015) use a stacked bi-directional lstm (blstm) to read a
   question and answer sequentially and then combine the hidden memory
   vectors from lstms of both question and answer. they use mean, sum and
   max-pooling as features. this model needs to incorporate key-word
   matching as a feature to outperform previous approaches that do not
   utilize deep learning. they use bm25 as the key-word matching feature
   and use gradient boosted regression tree (gbdt) (friedman [529]2001) to
   combine it with the lstm model.

   to rank pairs of short texts, severyn and moschitti ([530]2015) propose
   a convolutional deep neural network (cdnn). their deep learning
   architecture has 2 stages. the first stage is a sentence embedding
   model using a id98 to embed question and answer sentences into
   intermediate representative vectors, which are used to compute their
   similarity. the second stage is a nn ranking model whose features
   include intermediate representative sentence vectors, similarity score,
   and some additional features such as word overlap between sentences.
   results show improvements of about 3.5% in map versus results reported
   by yu et al. ([531]2014), in which a id98 is used followed by logistic
   regression (lr) to rank qa pairs. the authors attribute this
   improvement to the larger width (of 5) of the convolutional filter in
   their id98 for capturing long term dependencies, versus the unigram and
   bigram models used by yu et al. ([532]2014). beyond the similarity
   score, their second stage nn also takes intermediate question and
   answer representations as features to constitute a much richer
   representation than that of yu et al. ([533]2014).

   yang et al. ([534]2016b)   s approach starts by considering the
   interaction between question and answer at the id27 level.
   they first build a question-answer interaction matrix using pre-trained
   embeddings. they then use a novel value-shared weight id98 layer
   (instead of a position-shared id98) in order to induce a hidden layer.
   the motivation for this is that different matching ranges between a
   question term and answer will influence the later ranking score
   differently. after this, they incorporate an attention network for each
   question term to explicitly encode the importance of each question term
   and produce the final ranking score. they rank the answer sentences
   based on the predicted score and calculate map and mrr. whereas severyn
   and moschitti ([535]2015) and wang and nyberg ([536]2015) need to
   incorporate additional features in order to achieve comparative
   performance, yang et al. ([537]2016b) do not require any feature
   engineering.

   suggu et al. ([538]2016) propose a deep feature fusion network (dffn)
   to exploit both hand-crafted features (hcf) and deep learning based
   systems for answer question prediction. specifically, query/answer
   sentence representations are embedded using a id98. a single feature
   vector of 601 dimensions serves as input to a second stage
   fully-connected nn. features include sentence representations, hcf
   (e.g., tagme, google cross-lingual dictionary (gcd), and named entities
   (nes)), similarity measures between questions and answers, and metadata
   such as an answer author   s reputation score. the output of the second
   stage nn is a score predicting the answer quality. they compare their
   approach to the top two best performing hcf based systems from semeval
   2015 and a pure deep learning system. for semeval 2016, dffn was
   compared with their corresponding top two best performing system.
   results show that dffn performs better than the top systems across all
   metrics (map, f1 and accuracy) in both semeval 2015 and 2016 datasets.
   the authors attribute this to fusing the features learned from deep
   learning and hcf, since some important features are hard to learn
   automatically. as an example, question and answer often consists of
   several nes along with variants, which are hard to capture using deep
   learning. however, nes can be extracted from qa and their similarity
   used as a feature. their use of hcf was also motivated by the many
   available similarity resources, such as wikipedia, gcd, and
   click-through data, which could be leveraged to capture richer
   syntactic and semantic similarities between qa pairs.

7.2 conversational agents

7.2.1 learn

   learn to match an automatic conversation response system called deep
   learning-to-respond (dl2r) is proposed by yan et al. ([539]2016). they
   train and test on 10 million posting-reply pairs of human conversation
   web data from various sources, including microblog websites, forums,
   community id53 (cqa) bases, etc. for a given query they
   reformulate it using other contextual information and retrieve the most
   likely candidate reply. they model the total score as a function of
   three scores: query-reply, query-posting, and query-context, each fed
   into a neural network consisting of bi-directional lstm id56 layers,
   convolution and pooling layers, and several feed-forward layers. the
   strength of dl2r comes from the incorporation of reply, posting, and
   context with the query.

8 sponsored search

8.1 learn to match

   azimi et al. ([540]2015) use dssm represetantations for ad keyword
   re-writing. in paid search, each ad is associated with a set of
   keywords called bided keywords. the ads are ranked against a query and
   the ads at high rank are displayed to the user. in order to overcome
   the vocabulary mismatch between user queries and bided keywords, bided
   keywords are replaced with more common keywords. a set of candidate
   keywords are extracted from the set of documents returned by a search
   engine in response to the bided keyword query. the dssm model is
   leveraged to rank the candidate keywords against the original keywords.

   the deep-intent model proposed by zhai et al. ([541]2016a, [542]b)
   comprises a bidirectional recurrent neural network (bid56) combined with
   an attention module as the scn. the attention module, first introduced
   in bahdanau et al. ([543]2014) for id4, is
   referred to as attention pooling layer. this is the first work that
   employs an attention module for a web search task. a recurrent neural
   network takes a sequence of words and generates a sequence of
   distributed representations, so-called context-vectors, aligned with
   each word. each context vector encodes the semantics of the context
   from the start to the corresponding word. a bidirectional recurrent
   neural network (bid56) processes the input word sequence in both forward
   and backward directions.

   context vectors generated by a bid56 encode the context after and before
   the associated word. the pooling strategy is merging the context
   vectors vectors into a single vector that encodes the semantics of the
   whole sequence. the sequence vector is assigned the last context vector
   in last pooling, whereas an element-wise max operation is applied on
   context vectors in max-pooling. in attention pooling, the sequence
   vector is computed by a weighted sum of context vectors where the
   weights are determined by the attention module. the attention module
   takes a sequence of context vectors and outputs a weight for each
   vector. the similarity score between a query and an ad is obtained by
   the dot product of their distributed representations. similar query-ad
   pairs for training are extracted from the click logs. query-ad pairs
   that have a click are selected as training samples and for each such
   sample, a randomly selected set of query-ad pairs (without click) are
   used as negative samples. distributed representations are evaluated on
   click-logs from a product ad search engine with 966k pairs manually
   labeled by human judges. in the experiments, models that are built from
   different choices of id56 type (id56, bidirectional id56, lstm and
   lstm-id56) and pooling strategy (max-pooling, last pooling and attention
   pooling) are compared. the attention layer provides a significant gain
   in the auc (area-under-curve of the receiver operating characteristic)
   scores when used with id56 and bid56 whereas it performs on a par with
   last pooling when used with lstm-based networks. this can be attributed
   to the power of lstm units for capturing long-term contextual
   relations. besides the evaluation of distributed representations for
   matching, the attention scores are used to extract a subset of query
   words. words that have the highest attention scores are selected to
   rewrite the query.

8.2 learn to predict

   grbovic et al. ([544]2015a) present a pair of learn to predict models
   in grbovic et al. ([545]2015a, [546]b). in grbovic et al. ([547]2015a),
   grbovic et al propose query2vec, a two-layer architecture, where the
   upper layer models the temporal context of a query session using a
   skip-gram model, and the lower layer models word sequences within a
   query using id97 cbow. they also introduce two incremental models:
   ad-query2vec, which incorporates the learning of ad click vectors in
   the upper layer by inserting them into query sequences after queries
   that occurred immediately prior to an ad click; and directed
   ad-query2vec, which uses past queries as context for a directed
   language model in the upper layer. the models are trained using 12
   billion sessions collected on yahoo search and evaluated offline using
   historical activity logs, where success is measured by the
   click-through rate of ads served. all three query2vec models show
   improvement over sponsored keyword lists and search retargeting using
   id97 and query flow graph.

   in their subsequent, longer study, grbovic et al. ([548]2015b) propose
   a method to train context and content-aware id27s. the first
   proposal is context2vec. it treats a search session as a sentence and
   each query from the session as a word from the sentence. it uses
   id97   s skip-gram model. queries with similar context will result in
   similar embeddings. the second model is content2vec. this method is
   similar to le and mikolov ([549]2014)   s pv in that it uses the query as
   a paragraph to predict the word in its context. the third model,
   context-content2vec, similar to their earlier query2vec, combines
   context2vec and content2vec to build a two-layer model which jointly
   considers the query session context and the query context. the context
   of the query is defined both by its content and other queries in the
   same session. the models are trained on yahoo search logs that contain
   12 billion sessions and embeddings for approximately 45 million queries
   are learned. learned query embeddings are leveraged for rewriting
   queries in order to improve search re-targeting. the original query is
   expanded with its k nearest neighbor queries in the query embedding
   space. the learned model is evaluated on trec web track 2009   2013
   queries and an in-house data set from yahoo. for queries in the trec
   data set, the query rewrites obtained by the proposed models are
   editorially judged. the pv-dm model that only predicts context yields
   lower editorial grades than the query flow graph (qfg) baseline.
   rewrites by context2vec and context-content2vec embeddings outperform
   the baseline. the rewrites by the context-content2vec \(_{ad}\) model,
   which extends context-content2vec by adding the ads and links clicked
   in the same session to the ttu context, are assigned the highest
   editorial grades on average.

9 similar item retrieval

9.1 related document search

9.1.1 aggregate

   explicit  kusner et al. ([550]2014) propose the word mover   s distance
   (wmd) for computing distances between two documents. wmd is defined as
   the minimum cumulative distance that all words in the first document of
   the pair need to travel to exactly match the other document of the
   pair. the distance between two words is computed by the euclidean
   distance in the id97 space. authors evaluate the wmd metric on 8
   supervised document datasets: bbc sports articles, tweets labeled with
   sentiments, recipe procedure descriptions, medical abstracts with
   different disease groups, academic papers with publisher names, news
   datasets with different topics, amazon reviews with different category
   products. they compare their method with baselines including
   bag-of-words, tf-idf, bm25 okapi, lsi, lda and so on. their model
   outperforms these baselines.

   kim et al. ([551]2016) propose another version of wmd specific to
   query-document similarity. the high computational cost of wmd is
   tackled by mapping queries to documents using a id27 model
   trained on a document set. they make several changes to the original
   wmd methods: changing the weight of term by introducing inverse
   document frequency, and changing the original dissimilarity measure to
   cosine similarity. however, they do not provide any comparison versus
   wmd as a baseline.

9.1.2 learn

   learn to match semantic hashing is proposed by salakhutdinov and hinton
   ([552]2009) to map semantically similar documents near to one another
   in hashing space, facilitating easy search for similar documents.
   multiple layers of restricted id82s (rbms) are used to
   learn the semantic structure of documents. the final layer is used as a
   hash code that compactly represents the document. the lowest layer is
   simply word-count data and is modeled by the poisson distribution. the
   hidden layers are binary vectors of lower dimensions. the deep
   generative model is learned by first pre-training the rbms one layer at
   a time (from bottom to top). the network is then    unrolled   , i.e., the
   layers are turned upside down and stacked on top of the current
   network. the final result is an auto-encoder that learns a
   low-dimensional hash code from the word-count vector and uses that hash
   code to reconstruct the original word-count vector. the auto-encoder is
   then fine-tuned by back-propagation. results show that semantic hashing
   is much faster than locality sensitive hashing (andoni and indyk
   [553]2006; datar et al. [554]2004) and can find semantically similar
   documents in time independent of document collection size. however,
   semantic hashing difficult optimization procedures and a slow training
   mechanism, reducing applicability to large-scale tasks (liu et al.
   [555]2012).

   learn to predict  djuric et al. ([556]2015) point out use of the hdv
   model, reviewed in sect. [557]5.1.3, for exploring similar documents in
   a document collection.

   le and mikolov ([558]2014) assess vectors obtained by averaging pv-dm
   and pv-dbow vectors (see sect. [559]3.3.6 for details of pv) on snippet
   retrieval tasks. the snippet retrieval experiments are performed on a
   dataset of triplets created using snippets of the top 10 results
   retrieved by a search engine, for a set of 1 million queries. each
   triplet is composed of two relevant snippets for a query and a randomly
   selected irrelevant snippet from the collection. cosine similarity
   between paragraph vectors is shown to be an effective similarity metric
   for distinguishing similar snippets in such triplets. dai et al.
   ([560]2014) show that paragraph vectors outperform the vector
   representations obtained by lda (blei et al. [561]2003), average of
   id27s and tf-idf weighted one-hot vector representations, on
   a set of document triplets constructed with the same strategy in le and
   mikolov ([562]2014), using wikipedia and arxiv documents.

9.2 detecting text reuse

9.2.1 aggregate

   explicit the goal of zhang et al. ([563]2014) is to efficiently
   retrieve passages that are semantically similar to a query, making use
   of hashing methods on word vectors that are learned in advance. other
   than the given word vectors, no further deep learning is used. like
   clinchant and perronnin ([564]2013) and zhou et al. ([565]2015), they
   adopt the fisher kernel framework to convert variable-size
   concatenations of id27s to fixed length. however, this
   resulting fixed-length fisher vector is very high-dimensional and
   dense, so they test various state-of-the-art hashing methods (e.g.,
   spectral hashing weiss et al. [566]2009; charikar [567]2002) for
   reducing the fisher vector to a lower-dimensional binary vector.
   experiments are conducted on six collections including tipster (volumes
   1   3), clueweb09-cat-b, tweets2011, sogout 2.0, baidu zhidao, and sina
   weibo, with some sentences manually annotated for semantic similarity.
   hashing methods that use fisher vector representations based on word
   embeddings achieve higher precision-recall curves than hashing methods
   without vector representations and have comparable computational
   efficiency.

9.3 similar question retrieval

9.3.1 aggregate

   explicit learning of id27s coupled with category metadata for
   cqa is proposed by zhou et al. ([568]2015). they adopt id97   s
   skip-gram model augmented with category metadata from online questions,
   with category information encoding the attributes of words in the
   question (see zhang et al. [569]2016a for another example of
   integrating categorical data with id27s). in this way, they
   group similar words based on their categories. they incorporate the
   category constraint into the original skip-gram objective function.
   after the id27 is learned, they use fisher kernel (fk)
   framework to convert the question into a fixed length vector (similar
   to clinchant and perronnin [570]2013; zhang et al. [571]2014). to
   retrieve similar questions, they use the dot product of fvs to
   calculate the semantic similarities. for their experiments, zhou et al.
   ([572]2015) train id27s on yahoo! answers and baidu zhidao
   for english and chinese, respectively. results show that the category
   metadata powered model outperforms all the other baselines not using
   metadata. future work might include exploring how to utilize other
   metadata information, such as user ratings, to train more powerful word
   embeddings.

9.3.2 learn

   learn to match  lei et al. ([573]2016) address the similar question
   retrieval in cqa platforms with a framework that applies learn to match
   and learn to generate in two separate stages of training. in the
   pre-training phase, an encoder-decoder network that generates the title
   of a question given title, body or merged title-body of the question,
   is trained. the encoder network is based on gated (non-consecutive)
   convolutions. owing to the pre-training step, unsupervised question
   collection is utilized to tailor task-specific id27s and
   learn a rough semantic compositionality function for the questions. the
   encoder learned in the first step is used as the scn for a training
   learn to match model on the user-annotated similar question pairs.
   evaluation is done on the stackexchange askubuntu data set. negative
   pairs are constructed by selecting random questions from the
   collection. for the test set, similar pairs are annotated manually as
   the user-marked pairs are found to be noisy. in order to evaluate
   effectiveness, a set of questions retrieved by bm25 are ranked based on
   similarity scores computed by the learned learn to match model. lei et
   al present a comprehensive set of experiments that analyse both the
   overall gain by pre-training and the effect of the design choices such
   as the encoder neural network type, pooling strategies and inclusion of
   question body. highest effectiveness scores are achieved by the
   combination of the gated convolutions, last-pooling and pre-training.

9.4 recommendation

9.4.1 aggregate

   explicit  manotumruksa et al. ([574]2016) models user preferences using
   id27s for the task of context-aware venue recommendations
   (cavr). in cavr, each user can express a set of contextual aspects for
   their preference. each aspect has multiple contextual dimension term,
   with each term having a list of related term. the task then is to rank
   a list of venues by measuring how well each of venue matches user   s
   context preferences. it develops two approaches to model user-venue
   preferences and context-venue preferences using id27. first,
   it infers a vector representation for each venue using the comments on
   the venue, and it models the user-venue preferences using the rated
   venues    vector representation in the user   s profile. second, it models
   each dimension of each aspect in the context-venue preferences by
   identifying several most similar terms of that dimension term.

   to evaluate their user-venue preferences model, manotumruksa et al.
   ([575]2016) firstly train id27s using skip-gram model on the
   venues    comments dataset from foursquare. then it calculates the cosine
   similarity between the vector representation of venue and the user, and
   use it as a feature in the learning to rank system. it conducts the
   experiment on trec 2013 and 2014 contextual suggestion tracks, and
   reports p@5 and mrr. the result shows the system with the proposed
   features using id27 outperforms those without using word
   embedding.

   then, to evaluate its context-aware preference model, manotumruksa
   et al. ([576]2016) use cosine similarity between the venue and each of
   the contextual aspects as a feature in the ranking system. it also
   incorporates venue-dependent features and user-venue preference
   features. this experiment on trec 2015 contextual suggestion task shows
   that the proposed new system outperforms the baseline that does not
   utilize user information and their contextual preferences, and it also
   outperforms the top performing system under p@5.

9.4.2 learn

   learn to match  gao et al. ([577]2014) propose using dssm for both
   automatic highlighting of relevant keywords in documents and
   recommendation of alternative relevant documents based upon these
   keywords. they evaluate their framework based on what they call
   interestingness tasks, derived from wikipedia anchor text and web
   traffic logs. they find that feeding dssm derived features into a
   supervised classifier for recommendation offers state-of-the-art
   performance and is more effective than simply computing distance in the
   dssm latent space. future work could incorporate complete user
   sessions, since prior browsing and interaction history recorded in the
   session provide useful additional signals for predicting
   interestingness. this signal might be modeled most easily by using an
   id56.

10 non textual (or behavioural)

   borisov et al. ([578]2016b) propose a distributed representation which
   captures the user information need as sequence of vector states instead
   of traditional binary event used in a probabilistic graphical model
   (pgm) based click model (chuklin et al. [579]2015). existing pgm based
   click models capture a user   s behavior as a sequence of observable and
   latent events. however, one serious limitation of these click models is
   that the dependencies among the sequence of the events are
   hand-crafted. the basic idea of borisov et al   s proposed model is as
   follows: they initialize their vector state with the initial user query
   and then update the vector states based on the user interaction and the
   next document. for transitioning from one state to another they define
   three mapping functions which they learn from the training data using
   two different neural networks architectures: id56 and lstm. they perform
   their experimental analysis using yandex (a major commercial search
   engine in russia) relevance prediction dataset^[580]6 on a user click
   prediction task and a relevance prediction task. as a baseline they use
   the dbn, dcm, ccm and ubm click models.^[581]7 as a performance metrics
   they have used perplexity and log-likelihood and for relevance
   prediction task they have use ndcg at the 1, 3 , 5 and 10 level. their
   experimental evidence shows that their proposed model has better
   predictive power than all those baseline.

   as a follow-up to borisov et al. ([582]2016a, [583]b) focus on
   behavioral signals based on times between user actions. the ability to
   accurately predict (1) click dwell time (i.e., time spent by a user on
   the landing page of a search result), and (2) times from submission of
   a query to the first/last click on the results and to the next query
   submission (if none of the results will be clicked) allows us to
   optimize search engines for constructing result pages and suggesting
   query reformulations that minimize time it takes users to satisfy their
   information needs. at the heart of the solution proposed by the authors
   an lstm is used to capture the contexts in which user actions take
   place. the proposed context-aware time model is evaluated on four
   temporal prediction tasks and a ranking task. the results show that the
   proposed context-aware time model, which makes use of the context in
   which actions take place, provides a better means to explain times
   between user actions than existing methods.

   predicting click-through rate (ctr) and conversion rate from
   categorical inputs such as region, ad slot size, user agent, etc., is
   important in sponsored search. zhang et al. ([584]2016a) propose the
   first neural approach we know of to predict ctr for advertising. the
   authors develop two deep learning approaches to this problem, a
   factorization machine supported neural network (fnn) and sampling-based
   neural network (snn). the factorization machine is a non-linear model
   that can efficiently estimate feature interactions of any order even in
   problems with high sparsity by approximating higher order interaction
   parameters with a low-rank factorized parameterization. the use of an
   fm-based bottom layer in the deep network, therefore, naturally solves
   the problem of high computational complexity of training neural
   networks with high-dimensional binary inputs. the snn is augmented
   either by a sampling-based restricted id82 (snn-rbm) or a
   sampling-based denoising auto-encoder (snn-dae). the main challenge is
   that given many possible values of several categorical fields,
   converting them into dummy variables results in a very high-dimensional
   and sparse input space. for example, thirteen categorical fields can
   become over 900,000 binary inputs in this problem. the fnn and snn
   reduce the complexity of using a neural network on such a large input
   by limiting the connectivity in the first layer and by pre-training by
   selective sampling, respectively. after pre-training, the weights are
   fine-tuned in a supervised manner using back-propagation. evaluation
   focuses on the tuning of snn-rbm and snn-dae models and their
   comparison against id28, fm and fnn, on the ipinyou
   dataset^[585]8 (liao et al. [586]2014) with click data. results show
   that one of the proposed methods performs best, though the baselines
   are often close behind and twice take second place. the authors also
   find a diamond-shape architecture is better than increasing,
   decreasing, or constant hidden-layer sizes and that dropout works
   better than l2 id173. though this method can extract
   non-linear features, it is only very effective when dealing with
   advertisements without images. consequently, further research on
   multi-modal sponsored search to model images and text would be useful
   to pursue.

11 lessons and reflections

   at the end of the early years of neural ir, it is natural to ask the
   following question: is there a set of established guidelines for neural
   ir? is it sufficient to aggregate embeddings or do we really need deep
   neural network models? in this section, we summarize the lessons and
   reflections compiled from the reviewed work. apart from the high-level
   choice between the aggregate and learn methods, there are separate
   design choices to be made for each category identified in table [587]2.

   as to a high level view regarding the choice between aggregate and
   learn, no consistent advantage of id27s has emerged. among
   the models that fall within the aggregate category, directly using word
   embeddings provides consistent gains in ganguly et al. ([588]2015) but
   not in diaz et al. ([589]2016), nalisnick et al. ([590]2016), zamani
   and croft ([591]2016a, [592]b), zuccon et al. ([593]2015). in zuccon
   et al. ([594]2015), id27 similarity achieves comparable
   effectiveness to mutual information (mi) based term similarity. for
   query-document similarity, nalisnick et al. ([595]2016) point out that
   utilising relations between the in and out embedding spaces learned by
   cbow yields a more effective similarity function for query-document
   pairs. diaz et al. ([596]2016) propose to learn id27s from a
   topically constrained corpora since the id27s learned from an
   unconstrained corpus are found to be too general. zamani and croft
   ([597]2016a, [598]b) apply a sigmoid function on the cosine similarity
   scores in order to increase the discriminative power.

   similar to past development of new modeling techniques in ir, there is
   a common theme of researchers starting first with bag-of-words models
   then wanting to move toward modeling longer phrases in their future
   work. ganguly et al. ([599]2015) suggest future work should investigate
   compositionality of term embeddings. zuccon et al. ([600]2015) propose
   incorporating distributed representations of phrases to better model
   query term dependencies and compositionality. zheng and callan
   ([601]2015) propose direct modeling of bigrams and proximity terms.
   zamani and croft ([602]2016b) suggest query language models based on
   mutual-information and more complex language models (bigram, trigram,
   etc.) could be pursued.

11.1 reflections on aggregate

   in this section, we present the reflections on the evaluation of neural
   ir systems that follow the aggregate approach and rely on word
   embeddings.

11.1.1 is there an established guideline for the choices to be made for
learning id27s?

   use of id27s involves the design decisions presented below.

   nlm
          should one use id97 (cbow or skip-gram) (mikolov et al.
          [603]2013a, [604]b), glove (pennington et al. [605]2014), or
          something else (such as count-based embeddings)? can embeddings
          from multiple sets be selected between dynamically or combined
          together (neelakantan et al. [606]2014; zhang et al.
          [607]2016b)?

   corpora
          what training data/corpora should be used? does performance vary
          much if we simply use off-the-shelf embeddings (e.g., from
          id97 or glove) versus re-training embeddings for a target
          domain, either by fine-tuning (mesnil et al. [608]2013)
          off-the-shelf embeddings or re-training from scratch?
          presumably, larger training data is better, along with in-domain
          data similar to the test data on which the given system is to be
          applied, but how does one trade-off greater size of
          out-of-domain data versus smaller in-domain data? how might they
          be best used in combination?

   hyperparameters
          how should hyper-parameters be set (e.g., dimensionality of
          embedding space, window size, etc.)?

   oov words
          how should one deal with out-of-vocabulary (oov) terms not found
          in the id27 training data?

   there is no complete analysis that would help to derive guidelines. we
   summarize the reported partial observations from the literature.

   choice of nlm selection among id97 cbow or skip-gram or glove
   appears quite varied. zuccon et al. ([609]2015) compare cbow versus
   skip-gram, finding    no statistical significant differences between the
   two ...    kenter and de rijke ([610]2015) use both id97 and glove
   (pennington et al. [611]2014) embeddings (both the originally released
   embeddings as well training their own embeddings) in order to induce
   features for their machine learning model. they report model
   effectiveness using the original public embeddings with or without
   their own additional embeddings, but do not report further ablation
   studies to understand the relative contribution of different embeddings
   used. grbovic et al. ([612]2015a)   s query2vec uses a two-level
   architecture in which the upper layer models the temporal context of
   query sequences via skip-gram, while the bottom layer models word
   sequences within a query using cbow. however, these choices are not
   justified, and their later work grbovic et al. ([613]2015b) uses
   skip-gram only. almasri et al. ([614]2016) evaluate both cbow and
   skip-gram id97 embeddings (using default dimensionality and context
   window settings) but present only skip-gram results, writing that
      there was no big difference in retrieval performance between the two.   
   zamani and croft ([615]2016a, [616]b) adopt glove without explanation.
   similarly for id97, mitra et al. ([617]2016) simply adopt cbow,
   while others adopt skip-gram (de vine et al. [618]2014; manotumruksa
   et al. [619]2016; vulic and moens [620]2015; yang et al. [621]2016b; ye
   et al. [622]2016; zhou et al. [623]2015). zhang and wallace ([624]2015)
   perform an empirical analysis in the context of using id98s for short
   text classification. they found that the    best    embedding to use for
   initialization depended on the dataset. motivated by this observation,
   the authors proposed a method for jointly exploiting multiple sets of
   embeddings (e.g., one embedding set induced using glove on some corpus
   and another using a id97 variant on a different corpus) (zhang
   et al. [625]2016b). this may also be fruitful for ir tasks, suggesting
   a potential direction for future work.

   corpora in the work that we have reviewed, embeddings are either
   learned from a general-purpose corpus like wikipedia (general purpose
   embeddings) or a task-specific corpus. for ad-hoc retrieval, the
   retrieval corpus itself is considered as a corpus to learn embeddings.
   besides, id27s are learned from various task-specific corpora
   of community questions and their metadata (zhou et al. [626]2015),
   journal abstracts and patient records (de vine et al. [627]2014) and
   venues    comments from foursquare (manotumruksa et al. [628]2016). vulic
   and moens ([629]2015) learn bilingual id27s from a corpus of
   pseudo-bilingual documents obtained via a merge-and-shuffle approach on
   a document level parallel corpora, applying their model to
   cross-lingual ir.
   for ad-hoc retrieval, embeddings learned from wikipedia (general
   purpose embeddings) and embeddings learned from the retrieval corpus
   itself (corpus-specific id27s) are compared in zheng and
   callan ([630]2015), zuccon et al. ([631]2015). the authors note that no
   significant effect of this choice is observed for query-reweighting
   (zheng and callan [632]2015) or in a translation language model for
   computing term similarities (zuccon et al. [633]2015). zamani and croft
   ([634]2016b) train glove on three external corpora and report,    there
   is no significant differences between the values obtained by employing
   different corpora for learning the embedding vectors.    similarly, zheng
   and callan ([635]2015), regarding their query-reweighting model, write:

        [the system] performed equally well with all three external
     corpora; the differences among them were too small and inconsistent
     to support any conclusion about which is best. however, although no
     external corpus was best for all datasets ... the corpus-specific
     word vectors were never best in these experiments ... given the wide
     range of training data sizes     varying from 250 million words to 100
     billion words     it is striking how little correlation there is
     between search accuracy and the amount of training data.   

   in contrast, diaz et al. ([636]2016) highlight that id183
   with id27s learned from a topic-constrained collection of
   documents yields higher effectiveness scores compared to embeddings
   learned from a general-purpose corpora. besides, yang et al.
   ([637]2016c) considers classification in which one background corpus
   (used to train id27s) is a spanish wikipedia dump which
   contains over 1 million articles, while another is a collection of 20
   million tweets having more than 10 words per tweet. as expected, they
   find that when the background training text matches the classification
   text, the performance is improved.

   hyperparameters  vulic and moens ([638]2015), yang et al. ([639]2016c),
   zheng and callan ([640]2015), zuccon et al. ([641]2015) compare
   different training hyper-parameters such as window size and
   dimensionality of id27s. zuccon et al. ([642]2015)   s
   sensitivity analysis of the various model hyperparameters for inducing
   id27s shows that manipulations of embedding dimensionality,
   context window size, and model objective (cbow vs. skip-gram) have no
   consistent impact on model performance versus baselines. vulic and
   moens ([643]2015) find that while increasing dimensionality provides
   more semantic expressiveness, the impact on retrieval performance is
   relatively small. zheng and callan ([644]2015) find that 100 dimensions
   work best for estimating term weights, better than 300 and 500. in
   experiments using the terrier platform, yang et al. ([645]2016c) find
   that for the twitter election classification task using id98s, word
   embeddings with a large context window and dimension size can achieve
   statistically significant improvements.

   out of vocabulary terms regarding the handling of oov terms, the
   easiest solution is to discard or ignore the oov terms. for example,
   zamani and croft ([646]2016b) only consider queries where the embedding
   vectors of all terms are available. however, in end-to-end systems,
   where we are jointly estimating (or refining) embeddings alongside
   other model parameters, it is intuitive to randomly initialize
   embeddings for oov words. for instance, in the context of id98s for text
   classification, kim ([647]2014) adopted this approach. the intuition
   behind this is two-fold. first, if the same oov appears in a pair of
   texts, queries, or documents being compared, this contributes to the
   similarity scores between those two. second, if two different oov terms
   appear in the same pair, their dissimilarity will not contribute in the
   similarity function. however, this does not specifically address
   accidental misspellings or creative spellings (e.g.,    kool   ) commonly
   found in social media. one might address this by hashing words to
   character id165s (see sect. [648]11.2.6) or character-based modeling
   more generally (e.g., conneau et al. [649]2016; dhingra et al.
   [650]2016; zhang et al. [651]2015).

11.1.2 how to use id27s?

   as mentioned previously, publications in the implicit category are
   characterized by integrating id27 similarity into existing
   retrieval frameworks. in contrast, publications in the explicit
   category build ttu representations based on id27s, in an
   unsupervised way. for the implicit approach, the term similarity
   function is a design choice whereas the ttu similarity function stands
   as a design choice for the explicit category.

   besides the similarity functions, for the explicit category, it is
   crucial to note the choice of the aggregation function that builds the
   ttu vector from id27s. there are some publications that
   diverge from the simple aggregation method averaging/summing embedding
   vectors. clinchant and perronnin ([652]2013), zhang et al. ([653]2014),
   zhou et al. ([654]2015) use a fisher kernel (jaakkola et al. [655]1999)
   in order to compute ttu representations. ganguly et al. ([656]2016)
   opine that simply adding vectors of id27 cannot sufficiently
   capture the context of longer textual units. instead, the authors
   propose a new form of similarity metric based on the assumption that
   the document can be represented as a mixture of p-dimensional gaussian
   id203 density functions and each id97 embedding
   (p-dimensions) is an observed sample. then, using the em algorithm,
   they estimate the id203 density function that can be incorporated
   into the query likelihood language model using linear interpolation.

   word similarity function cosine similarity is the choice adopted by
   almost all of the publications reviewed. however, zamani and croft
   ([657]2016a, [658]b) propose using sigmoid and softmax transformations
   of cosine similarity on the grounds that cosine similarity values are
   not discriminative enough. their empirical analysis shows that there
   are no substantial differences (e.g., two times more) between the
   similarity of the most similar term and the 1000th similar term to a
   given term w, while the 1000th word is unlikely to have any semantic
   similarity with w. consequently, they propose using monotone mapping
   functions (e.g., sigmoid or softmax) to transform the cosine similarity
   scores.

   ttu similarity function publications under the explicit category are
   characterized by building ttu representations based on pre-trained
   embeddings and computing ttu pair similarity by cosine similarity of
   the distributed ttu representations. deviating from this generic
   approach, ganguly et al. ([659]2016) opine that simply adding vectors
   of id27 cannot sufficiently capture the context of longer
   textual units. instead, the authors propose a new form of similarity
   metric based on the assumption that the document can be represented as
   a mixture of p-dimensional gaussian id203 density functions and
   each id97 embedding (p-dimensions) is an observed sample. then,
   using the em algorithm, they estimate the id203 density function
   which can be incorporated to the query likelihood language model using
   linear interpolation. finally, word mover   s distance (wmd) (kusner
   et al. [660]2014) is an alternative distance metric defined as the
   minimum cumulative distance that all words in the first document of the
   pair need to travel to exactly match the other document of the pair.
   the distance between two words is computed by the euclidean distance in
   the id97 space.

11.2 reflections on learn

   in this subsection, we present design choices for the models reviewed
   in the learn category of the how dimension of our taxonomy
   (table [661]2).

11.2.1 how to choose the appropriate category of learn?

   when making a choice for a training objective (a subcategory of learn),
   the following three points should be considered: training data,
   representations for unseen ttus, and ttu length.

   training data learn to match models are trained to maximize the
   relevance scores of pairs that have been annotated or inferred to be
   relevant and minimize the scores of irrelevant pairs. in contrast,
   learn to predict and learn to generate models assume that ttus that
   co-occur in the same context are similar.

   computing representations for unseen ttus in the learn to autoencode,
   learn to generate and learn to match models, distributed
   representations of unseen ttus can be computed by forward propagation
   through the scn. in contrast, learn to predict models are focused on
   learning embeddings for a fixed collection of ttus. an additional
   id136 step, which requires including the representations of the
   entire collection, is required for obtaining the embedding of an
   unobserved ttu. while pv (the first learn to predict model) has been
   adopted in many studies (e.g., xia et al. [662]2016 use pv-dbow to
   represent documents) and is often reported as a baseline (e.g., tai
   et al. [663]2015), concerns about reproducibility have also been
   raised. kiros et al. ([664]2015) report results below id166 when
   re-implementing pv. kenter and de rijke ([665]2015) note,    it is not
   clear, algorithmically, how the second step     the id136 for new,
   unseen texts     should be carried out.    perhaps most significantly,
   later work co-authored by mesnil et al. ([666]2014)^[667]9 has
   disavowed the original findings of le and mikolov ([668]2014), writing,
      to match the results from le and mikolov ([669]2014), we followed the
   [author   s] suggestion ... however, this produces the [reported] result
   only when the training and test data are not shuffled. thus, we
   consider this result to be invalid.   

   ttu length learn to generate models are designed to generate unseen
   synthetic textual units. to this end, generating textual units has been
   shown to be successful for queries (luukkonen et al. [670]2016; sordoni
   et al. [671]2015). generating long textual units is not well-studied up
   to this point, except (lioma et al. [672]2016). the dssm variants from
   the learn to match are shown to work better on document titles instead
   of the entire document content (guo et al. [673]2016a; shen et al.
   [674]2014a).

11.2.2 choice of semantic compositionality network: how to represent text
beyond single words? does text granularity influence the choice?

   the simplest way to represent longer textual units, such as phrases,
   sentences, or entire documents, is to sum or average their constituent
   id27s. however such bag-of-words compositions ignore word
   ordering, and simple averaging treats all words as equally important in
   composition (though some work has considered weighted operations, e.g.,
   vulic and moens [675]2015). ganguly et al. ([676]2016) opine that:

        ... adding the constituent word vectors ... to obtain the vector
     representation of the whole document is not likely to be useful,
     because ... compositionality of the word vectors [only] works well
     when applied over a relatively small number of words ... [and] does
     not scale well for a larger unit of text, such as passages or full
     documents, because of the broad context present within a whole
     document.   

   fortunately, a variety of scns, including feed forward nn, id98, id56,
   lstm and their variants, have been proposed for inducing
   representations of longer textual units. however, there is no evidence
   that any one method consistently outperforms the others, with the
   performance of each method instead appearing to often depend on the
   specific task and dataset being studied. we further discuss the use of
   id98s and id56s below.

   convolutional neural networks (id98s)  shen et al. ([677]2014a, [678]b)
   propose a convolutional latent semantic model (clsm) to encode queries
   and documents into fix-length vectors, following a popular    convolution
   + pooling    id98 architecture. the first layer of clsm is a word hashing
   layer that can encode words into vectors. clsm does not utilize word
   embeddings as input, seemingly distinguishing it from all other works
   using id98s. mitra ([679]2015) use clsms to encode query reformulations
   for query prediction, while mitra and craswell ([680]2015) use clsms on
   query prefix-suffix pairs corpus for query auto-completion. they sample
   queries from the search query logs and split the query at every
   possible word boundary to form prefix-suffix pairs.

   severyn and moschitti ([681]2015) and suggu et al. ([682]2016) adopt a
   similar    convolution + pooling    id98 architecture to encode question and
   answer sentence representations, which serve as features for a
   second-stage ranking nn. see mitra et al. ([683]2016) for further
   discussion of such two-stage telescoping approaches.
   yang et al. ([684]2016b) develop a novel value-shared id98, and apply it
   on the query-answer matching matrix to extract the semantic matching
   between the query and answer. this model can capture the interaction
   between intermediate terms in the query and answer, rather than only
   considering the final representation of the query and answer. the
   motivation behind the value-shared id98 is that semantic matching value
   regularities between a question and answer is more important than
   spatial regularities typical in id161. similarly,
   contemporaneous work by guo et al. ([685]2016a) notes:

        existing interaction-focused models, e.g., arc-ii and matchpyramid,
     employ a id98 to learn hierarchical matching patterns over the
     matching matrix. these models are basically position-aware using
     convolutional units with a local    receptive field    and learning
     positional regularities in matching patterns. this may be suitable
     for the image recognition task, and work well on semantic matching
     problems due to the global matching requirement (i.e., all the
     positions are important). however, it may not be suitable for the
     ad-hoc retrieval task, since such positional regularity may not
     exist ...   

   recurrent neural networks (id56s)   sordoni et al. ([686]2015) build a
   hierarchical gru to encode the query and the query session into vector
   representations. song et al. ([687]2016) use an lstm to model user
   interests at different time steps and encode them into a vector. lioma
   et al. ([688]2016) create new relevant information using an lstm that
   takes the concatenated text of a query and its known relevant documents
   as input using id27s. however, rather than take the whole
   text of a document, they extract a context window of \(\pm n\) terms
   around every query term occurrence. yan et al. ([689]2016) use a
   bidirectional lstm followed by a id98 to model the original query,
   reformulated query, candidate reply, and antecedent post in their
   human-computer conversation system. wang and nyberg ([690]2015) use a
   stacked bidirectional lstm to sequentially read words from both
   question and answer sentences, calculating relevance scores for answer
   sentence selection through mean pooling across all time steps. section
   [691]11.2.3 presents cohen et al. ([692]2016)   s comparison of id98s
   versus id56s by document length.

11.2.3 text granularity in ir: does text granularity have an effect on the
optimal choice for scn?

   many studies to date focus on short text matching rather than longer
   documents more typical of modern ir ad-hoc search. cohen et al.
   ([693]2016) study how the effectiveness of nn models for ir vary as a
   function of document length (i.e., text granularity). they consider
   three levels of granularity: (1) fine, where documents often contain
   only a single sentence and relevant passages span only a few words
   (e.g., id53); (2) medium, where documents consist of
   passages with a mean length of 75 characters and relevant information
   may span multiple sentences (e.g., passage retrieval); and (3) coarse,
   or typical modern ad-hoc retrieval. for fine granularity, they evaluate
   models using the trec qa dataset and find that id98s outperform id56s and
   lstms, as their filter lengths are able to effectively capture language
   dependencies. for medium granularity, they evaluate using the yahoo
   webscope l4 cqa dataset and conclude that id137 outperform id98s
   due to their ability to model syntactic and semantic dependencies
   independent of position in sequence. in contrast, id56s without lstm
   cells do not perform as well, as they tend to    forget    information due
   to passage length.

   for ad-hoc retrieval performance on robust04, comparing id56s, id98s,
   lstm, dssm, clsm, vulic and moens ([694]2015)   s approach, and le and
   mikolov ([695]2014)   s pv, cohen et al. ([696]2016) find that all neural
   models perform poorly. they note that neural methods often convert
   documents into fixed-length vectors, which can introduce a bias for
   either short or long documents. however, they find that the approaches
   of vulic and moens ([697]2015) and le and mikolov ([698]2014) perform
   well when combined with id38 approaches that explicitly
   capture matching information of queries and documents. similar
   observation is also reported in ganguly et al. ([699]2016).

11.2.4 choice of similarity function: which function should i choose to
measure the similarity between two text snippets?

   similarity here is not strictly limited to linguistic semantics, but
   more generally includes relevance matching between queries and
   documents or questions and answers. while cosine similarity is the
   simplest and the most common approach, a variety of more sophisticated
   methods have been proposed.

   wang and nyberg ([700]2015) use a stacked bidirectional lstm to
   sequentially read words from both question and answer sentences,
   calculating relevance scores for answer sentence selection through mean
   pooling across all time steps. yan et al. ([701]2016) match sentences
   by concatenating their vector representations and feeding them into a
   multi-layer fully-connected neural network, matching a query with the
   posting and reply in a human computer conversation system. xia et al.
   ([702]2016) propose a neural tensor network (ntn) approach to model
   document novelty. this model takes a document and a set of other
   documents as input. the architecture uses a tensor layer, a max-pooling
   layer, and a linear layer to output a document novelty score. guo
   et al. ([703]2016a)   s recent deep relevance matching model appears to
   be one of the most successful to date for ad-hoc search over longer
   document lengths. in terms of textual similarity, they argue that the
   ad-hoc retrieval task is mainly about relevance matching, different
   from semantic matching in nlp. they model the interaction between query
   terms and document terms, building a matching histogram on top of the
   similarities. they then feed the histogram into a feed forward neural
   network. they also use a term gating network to model the importance of
   each query term.

   yang et al. ([704]2016b) propose an attention-based neural matching
   model (anmm) for id53. similar to guo et al.
   ([705]2016a), they first model the interaction between query terms and
   document terms to build a matching matrix. they then apply a novel
   value-shared id98 on the matrix. since not all query terms are equally
   important, they use a softmax gate function as an attention mechanism
   in order to learn the importance of each query term when calculating
   the matching between the query and the answer.

   to summarize, simply calculating cosine similarity between two text
   vector representations might not be the best choice to capture the
   relevance relation between texts. people develop neural models to learn
   similarity, and model the interaction between texts in a finer
   granularity.

11.2.5 initializing id27s in nns: should they be tuned or not
during the training process?

   we observed three approaches for initializing and updating the
   embeddings during training of learn models:
     * the embedding layer of the models are initialized randomly and
       learned from scratch during training of the task-specific model.
     * the embeddings layer is initialized with pre-trained embeddings and
       is kept fixed during training of the task-specific model.
     * the embedding layer is initialized with pre-trained embeddings and
       is fine-tuned for the task during training of the task-specific
       model.

   at present, there is no clear evidence showing that either method
   consistently works best. however, we may note that learning word
   embeddings is possible only with large corpora; thus in cases where
   task-specific training data is limited as in trec ad-hoc collections
   (guo et al. [706]2016a) and the trec-qa data set (see sect. [707]7.1
   for the related work), pre-trained id27s have been preferred
   to initialize, in order to shift focus of training towards learning
   semantic compositionality.

11.2.6 how to cope with large vocabularies?

   training neural networks is computationally expensive and today it is
   not possible to train a id98 or id56 with a 1m word vocabulary. id97
   models, being shallow networks with a single linear layer can be
   trained with large vocabularies owing to softmax approximation methods
   such as negative sampling and nce. in cases where it is crucial to
   learn a semantic compositionality model with a id98 or id56 architecture,
   the large vocabulary brings a challenge. word hashing, first introduced
   in huang et al. ([708]2013) together with the dssm model and adopted by
   mitra ([709]2015), mitra and craswell ([710]2015), shen et al.
   ([711]2014a, [712]b), ye et al. ([713]2015), reduces large vocabularies
   to a moderate sized vocabulary of 30k of character trigrams. words are
   broken down into letter id165s and then represented as a vector of
   letter id165s.

   huang et al. ([714]2013) provide an empirical analysis where the
   original vocabulary size of 500k is reduced to only 30k owing to word
   hashing. while the number of english words can be unlimited, the number
   of letter id165s in english is often limited, thus word hashing can
   resolve the out-of-vocabulary (oov) problem as well. however, one
   inherent problem of word hashing is the hashing conflict, which can be
   serious for a very large corpus. another set of methods, such as byte
   pair encoding (sennrich et al. [715]2016) have been proposed in order
   to cope with large vocabularies in id4. although
   only word hashing has been used for neural ir to this end, other
   vocabulary reduction methods should be considered in future work.

12 conclusion

   the purpose of this survey is to offer an introduction to neural models
   for information retrieval by surveying the state of knowledge up to the
   end of 2016. to this end we have reviewed and classified existing work
   in the area. we used a taxonomy in which we recognize different target
   textual unit (ttu), different types of usage of learned text
   representations (   usage   ) ir tasks, as well as different methods for
   building representations (   how   ). within the latter category we
   identified two sub-categories: the aggregate and learn categories. the
   aggregate category includes methods based on pre-computed word
   embeddings for computing semantic similarity, while the learn category
   covers neural semantic compositionality models.

   within the aggregate category we observed two major patterns of
   exploiting id27s. in the explicit type of use of embeddings,
   ttus are associated with a representation in the id27 space
   and semantic similarity of ttus is computed based on these
   representations. in the implicit type of use, similarity of word
   embeddings is plugged in as term similarity in an existing statistical
   id38 frameworks for retrieval. several strategies for
   adapting id27s for document retrieval have been introduced,
   such as topically constraining the document collection, new similarity
   functions and the inclusion of tf-idf weights for aggregating word
   embeddings. this may be understood as an indication that we need to
   design ir specific objectives for learning distributed representations.
   are the training objective and semantic relationships encoded by the
   embedding vectors useful for the target retrieval task? a future
   direction would be to identify the types of semantic word relations
   that matter to semantic matching in web search, across multiple tasks.

   we classified the neural semantic compositionality models reviewed in
   the learn category, into four sub-categories learn to autoencode, learn
   to match, learn to predict and learn to generate, considering the
   training objectives optimized for learning representations. learn to
   match models are trained using noisy relevance signals from click
   information in click-through logs whereas the models in the other
   categories are designed to predict or generate task-specific context of
   ttus. the majority of the learn to match and learn to predict models
   are evaluated on datasets extracted from commercial search engine logs.
   a comparative evaluation of models from different sub-categories on
   publicly available data sets, is required in order to gain a deeper
   understanding of semantic compositionality for matching.

   currently, existing learn to predict/generate context and learn to
   generate models mostly rely on temporal context windows. it would be
   interesting to examine other types of contextual relations in search
   logs, such as long term search history of users and noisy relevance
   signals exploited by learn to match models. another future direction
   would concern applications of the attention mechanism (bahdanau et al.
   [716]2014) for designing models that can predict where a user would
   attend in document, given a query.

   looking forward, we believe there are several key directions where
   progress is needed. first, we presented the document retrieval, query
   suggestion and ad retrievalsponsored search tasks as largely disjoint
   tasks. however, the models proposed for one task may be useful for
   another. for instance, the context-content2vec model (grbovic et al.
   [717]2015b) was evaluated only on matching ads to queries yet the
   distributed query representations could also be evaluated for query
   suggestion or query auto completion (cai and de rijke [718]2016b). in
   particular, there is a need to compare distributed query
   representations and similarity/likelihood scores produced by the
   proposed models on query tasks. in some work, the representations were
   used as features in learning to rank frameworks and there are no clues
   about the power of these representations in capturing semantics.

   more broadly, there is a need for systematic and broad task-based
   experimental surveys that focus on comparative evaluations of models
   from different categories, but for the same tasks and under the same
   experimental conditions, very much like the reliable information access
   (ria) workshop that was run in the early 2000s to gain a deeper
   understanding of id183 and pseudo relevance feedback (harman
   and buckley [719]2009).

   another observation is that recently introduced generative
   models   mostly based on recurrent neural networks   can generate unseen
   (synthetic) textual units. the generated textual units have been
   evaluated through user studies (lioma et al. [720]2016; sordoni et al.
   [721]2015). for the query suggestion task, generated queries have been
   found to be useful; and so have word clouds of a synthetic document.
   the impact of these recent neural models on user satisfaction or
   retrieval scenarios should be investigated on real scenarios.

   over the years, ir has made tremendous progress by learning from user
   behavior, either by introducing, e.g., click-based rankers (radlinski
   and joachims [722]2005) or, more abstractly, by using models that
   capture behavioral notions such as examination id203 and
   attractiveness of search results through click models (chuklin et al.
   [723]2015). how can such implicit signals be used to train neural
   models for semantic matching in web search? so far, we have only seen
   limited examples of the use of click models in training neural models
   for web search tasks.

   interest in neural ir has never been greater, spanning both active
   research and deployment in practice^[724]10 (metz [725]2016). neural ir
   continues to accelerate in quantity of work, sophistication of methods,
   and practical effectiveness (guo et al. [726]2016a). new methods are
   being explored that may be computationally infeasible today (see diaz
   et al. [727]2016), but if proven effective, could motivate future
   optimization work to make them more practically viable (e.g., jurgovsky
   et al. [728]2016; ordentlich et al. [729]2016). nn approaches have come
   to dominate id103 (2011), id161 (2013), and nlp
   (2015). similarly, deep learning will come to dominate information
   retrieval as well (manning [730]2016).

   at the same time, healthy skepticism about neural ir also remains. the
   key question in ir today might be most succinctly expressed as:    will
   it work?    while nn methods have worked quite well on short texts,
   effectiveness on longer texts typical of ad-hoc search has been
   problematic (cohen et al. [731]2016; huang et al. [732]2013), with only
   very recent evidence to the contrary (guo et al. [733]2016a). side by
   side comparisons of lexical versus neural methods often show at least
   as many losses as gains for neural methods, with at best an advantage
      on average    (van gysel et al. [734]2016a, [735]b). in addition, while
   great strides have been made in id161 through employing a
   very large number of hidden layers (hence    deep    learning), such deep
   structures have typically been less effective in nlp and ir than more
   shallow architectures (pang et al. [736]2016a), though again with
   notable recent exceptions  (conneau et al. [737]2016). when neural ir
   has led to improvements in ad-hoc search results, improvements appear
   relatively modest (diaz et al. [738]2016; zamani and croft [739]2016a)
   when compared to traditional id183 techniques for addressing
   vocabulary mismatch, such as pseudo-relevance feedback (prf). both
   ganguly et al. ([740]2016) and diaz et al. ([741]2016) have noted that
   global id27s, trained without reference to user queries,
   versus local methods like prf for exploiting query-context, appear
   limited similarly to the traditional global-local divide seen with
   existing approaches like id96 (yi and allan [742]2009).

   as li ([743]2016) put it,    does ir need deep learning?    such a
   seemingly simple question requires careful unpacking. much of the above
   discussion assumes neural ir should deliver new state-of-the-art
   quality of search results for traditional search tasks. while it may do
   so, this framing may be far too narrow, as li ([744]2016)   s
   presentation suggests. the great strength of neural ir may lie in
   enabling a new generation of search scenarios and modalities, such as
   searching via conversational agents (yan et al. [745]2016), multi-modal
   retrieval (ma et al. [746]2015a, [747]b), knowledge-based search ir
   (nguyen et al. [748]2016), or synthesis of relevant material (lioma
   et al. [749]2016). it may also be that neural ir will provide greater
   traction for other future search scenarios not yet considered.

   given that the efficacy of deep learning approaches is often driven by
      big data   , will neural ir represent yet another fork in the road
   between industry and academic research, where massive commercial query
   logs deliver neural ir   s true potential? or should we frame this more
   positively as an opportunity for research on generating training
   material or even simulation, as has previously been pursued for, e.g.,
   learning to rank (liu [750]2009), see, e.g., azzopardi et al.
   ([751]2007), berendsen et al. ([752]2013b)? there is also an important
   contrast to note here between supervised scenarios, such as learning to
   rank versus unsupervised learning of id27s or typical queries
   (see mitra [753]2015; mitra and craswell [754]2015; sordoni et al.
   [755]2015; van gysel et al. [756]2016a, [757]b). lecun et al.
   ([758]2015) wrote,    we expect unsupervised learning to become far more
   important in the longer term.    just as the rise of the web drove work
   on unsupervised and semi-supervised approaches by the sheer volume of
   unlabeled data it made available, the greatest value of neural ir may
   naturally arise where the biggest data is found: continually generated
   and ever-growing behavioral traces in search logs, as well as
   ever-growing online content.

   while skepticism of neural ir may well remain for some time, the
   practical importance of search today, coupled with the potential for
   significantly new traction offered by this    third wave    of nns, makes
   it unlikely that researchers will abandon neural ir anytime soon
   without having first exhaustively tested its limits. as such, we expect
   the pace and interest in neural ir will only continue to blossom, both
   in new research and increasing application in practice. consequently,
   this first special-issue journal on neural ir in 2017 will likely
   attract tremendous interest and is well-poised for timely impact on
   research and practice.

footnotes

    1. [759]1.
       while not all nns are    deep    and not all    deep    models are neural,
       the terms are often conflated in practice.
    2. [760]2.
       [761]http://research.microsoft.com/en-us/um/beijing/events/dl-wsdm-
       2015/.
    3. [762]3.
       [763]https://www.microsoft.com/en-us/research/event/neuir2016/.
    4. [764]4.
       [765]http://social-book-search.humanities.uva.nl.
    5. [766]5.
       [767]http://www.kdd.org/kdd-cup/view/kdd-cup-2005.
    6. [768]6.
       [769]http://imat-relpred.yandex.ru/en/datasets.
    7. [770]7.
       implementations of all of these click models are available at
       [771]https://github.com/markovi/pyclick.
    8. [772]8.
       [773]http://data.computational-advertising.org.
    9. [774]9.
       [775]https://github.com/mesnilgr/iclr15.
   10. [776]10.
       [777]https://en.wikipedia.org/wiki/rankbrain.
   11. [778]11.
       [779]https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-
       articles.xml.bz2.
   12. [780]12.
       [781]https://catalog.ldc.upenn.edu/ldc2011t07.
   13. [782]13.
       [783]http://nlp.stanford.edu/projects/glove/.
   14. [784]14.
       [785]https://www.microsoft.com/en-us/download/details.aspx?id=52597
       .
   15. [786]15.
       [787]http://www.zuccon.net/ntlm.html.
   16. [788]16.
       [789]http://nlp.stanford.edu/projects/glove/.
   17. [790]17.
       [791]https://bitbucket.org/omerlevy/hyperwords.
   18. [792]18.
       [793]https://radimrehurek.com/gensim/.
   19. [794]19.
       [795]https://lvdmaaten.github.io/tsne/.
   20. [796]20.
       [797]http://www.bigdatalab.ac.cn/benchmark/bm/bd?code=drmm(lch-idf)
       .
   21. [798]21.
       [799]https://github.com/sordonia/hred-qs.
   22. [800]22.
       [801]https://github.com/cvangysel/sert.
   23. [802]23.
       [803]https://github.com/aseveryn/deep-qa.
   24. [804]24.
       [805]https://ciir.cs.umass.edu/downloads/deepmerge/.
   25. [806]25.
       [807]http://www.cs.cmu.edu/~gzheng/code/termrecallkit-v2.tar.bz2.
   26. [808]26.
       [809]http://www.dsic.upv.es/~pgupta/mixed-script-ir.
   27. [810]27.
       [811]https://github.com/ielab/adcs2015-ntlm.
   28. [812]28.
       [813]https://github.com/yangliuy/anmm-cikm16.
   29. [814]29.
       [815]https://github.com/gdebasis/kderlm.

notes

acknowledgements

   we would like to thank christophe van gysel from the university of
   amsterdam, for his valuable feedback and comments. we would also like
   to thank our anonymous reviewers for their constructive comments and
   the guest editors for their advice. the following additional students
   at the university of texas at austin contributed indirectly to the
   writing of this literature review: manu agarwal, edward babbe, anuparna
   banerjee, jason cai, dillon caryl, yung-sheng chang, shobhit chaurasia,
   linli ding, brian eggert, michael feilbach, alan gee, jeremy gin, rahul
   huilgol, miles hutson, neha javalagi, yan jiang, kunal lad, yang liu,
   amanda lucio, kristen moor, daniel nelson, geoffrey potter, harshal
   priyadarshi, vedhapriya raman, eric roquemore, juliette seive, abhishek
   sinha, ashwini venkatesh, yuxuan wang, and xu zhang.

funding

   i.s. altingovde is supported by turkish academy of sciences
   distinguished young scientist award (t  ba-geb  p 2016). p. karagoz is
   partially funded by tubitak grant no. 112e275. m. de rijke was
   supported by ahold delhaize, amsterdam data science, the bloomberg
   research grant program, the dutch national program commit, elsevier,
   the european community   s seventh framework programme (fp7/2007   2013)
   under grant agreement no. 312827 (vox-pol), the microsoft research
   ph.d. program, the netherlands institute for sound and vision, the
   netherlands organisation for scientific research (nwo) under project
   nos. 612.001.116, hor-11-10, ci-14-25, 652.002.001, 612.001.551,
   652.001.003, and yandex. all content represents the opinion of the
   authors, which is not necessarily shared or endorsed by their
   respective employers and/or sponsors.

appendix 1: acronyms used

   ai
          artificial intelligence

   asr
          automatic id103

   boew
          bag of embedded words

   bwesg
          bilingual id27s skip-gram

   c-dssm
          convolutional deep structured semantic models

   cbow
          continuous bag of words

   cdnn
          convolutional deep neural network

   clsm
          convolutional latent semantic model

   id98
          convolutional neural network

   cqa
          community id53

   desm
          dual embedding space model

   dfr
          divergence from randomness

   drmm
          deep relevance matching model

   dsm
          distributional semantic model

   dssm
          deep structured semantic model

   epv
          extended paragraph vector

   fk
          fisher kernel

   fv
          fisher vector

   glm
          generalized language model

   glove
          global vectors

   gru
          gated recurrent unit

   hal
          hyperspace analog to language

   hdv
          hierarchical document vector

   hred
          hierarchical recurrent encoder decoder

   ir
          information retrieval

   is
          importance sampling

   kb
          knowledge base

   lda
          id44

   lm
          language model

   lsa
          latent semantic analysis

   lsi
          id45

   lstm-dssm
          lstm deep structured semantic model

   lstm
          long short term memory

   mi
          mutual information

   mt
          machine translation

   nce
          noise contrastive estimation

   neg
          negative sampling

   nlm
          neural language model

   nlp
          natural language processing

   nltm
          neural translation language model

   nn
          neural network

   nnlm
          neural network language model

   ntn
          neural tensor networks

   oov
          out of vocabulary

   pamm
          id88 algorithm using measures as margins

   pgm
          probabilistic graphical model

   plsa
          probabilistic latent semantic analysis

   qa
          id53

   pv-dbow
          paragraph vector with distributed bag of words

   pv-dm
          paragraph vector with distributed memory

   pv
          paragraph vector

   qlm
          query language model

   r-ltr
          relational learning-to-rank framework

   rbm
          restricted boltzman machine

   reid98
          id56

   id56
          recurrent neural network

   id56lm
          recurrent neural network language model

   sc
          semantic compositionality

   scn
          semantic compositionality network

   sgns
          skip-gram with negative sampling

   ttu
          target textual unit

   wmd
          word mover   s distance

appendix 2: resources

   in this section, we present pointers to publicly available resources
   and tools which the reader would benefit for getting started with
   neural models, distributed representations, and information retrieval
   experiments with semantic matching.

id27s

   corpora used wikipedia and gigaword5 are the corpora widely used for
   learning id27s. the latest wikipedia dump can be obtained
   from wikimedia.^[816]11 the gigaword5 data set is accessible through
   the ldc.^[817]12 several authors have learned embeddings from query
   logs (cai and de rijke [818]2016a; kanhabua et al. [819]2016; sordoni
   et al. [820]2015).

   pre-trained id27s it is possible to obtain pre-trained glove
   embeddings^[821]13 learned from large corpora, cbow embeddings trained
   on large-scale bing query logs (mitra et al. [822]2016)^[823]14 and
   different sets of id27s^[824]15 used for evaluating nltm in
   zuccon et al. ([825]2015).

   learning id27s the source code for glove (pennington et al.
   [826]2014)^[827]16 and the models introduced in levy et al.
   ([828]2015)^[829]17 is publicly shared by the authors. implementations
   of the id97 and paragraph vector models are included in the gensim
   library.^[830]18

   visualizing id27s the id84 technique
   t-distributed stochastic neighbor embedding (id167) (van der maaten and
   hinton [831]2008) is commonly used for visualizing id27
   spaces and for trying to understand the structures learned by neural
   models.^[832]19

test corpora used for retrieval experiments

   for retrieval experiments regarding neural models for semantic
   matching, the full range of clef, fire and trec test collections has
   been used. in table [833]3, we present the list of data sets used in
   experimental frameworks of reviewed work. for each data set, the list
   of studies that report related experiment results is given in the
   table.
   table 3

   datasets used in experimental setups

   english data

   study

   20-newsgroup corpus

   salakhutdinov and hinton ([834]2009)

   amazon product data (mcauley et al. [835]2015)

   van gysel et al. ([836]2016a)

   aol query logs

   kanhabua et al. ([837]2016), mitra ([838]2015), mitra and craswell
   ([839]2015), sordoni et al. ([840]2015)

   bing query logs and webcrawl

   mitra ([841]2015), mitra and craswell ([842]2015), mitra et al.
   ([843]2016), nalisnick et al. ([844]2016)

   clef03 english ad-hoc

   clinchant and perronnin ([845]2013)

   clef 2016 social book search

   amer et al. ([846]2016)

   clef medical corpora

   almasri et al. ([847]2016)

   ehealth

   rekabsaz et al. ([848]2016b)

   msn query log

   kanhabua et al. ([849]2016)

   msr paraphrase corpus

   kenter and de rijke ([850]2015)

   ohsumed

   de vine et al. ([851]2014)

   pubmed

   balikas and amini ([852]2016)

   reuters volume i (rcv1-v2)

   salakhutdinov and hinton ([853]2009)

   semeval 2015   2016

   dffn (suggu et al. [854]2016)

   stack overflow

   boytsov et al. ([855]2016)

   tipster (volume 1   3)

   zhang et al. ([856]2014)

   trec 1   2 ad-hoc (ap 88   89)

   clinchant and perronnin ([857]2013), zamani and croft ([858]2016a,
   [859]b), zuccon et al. ([860]2015)

   trec 1   3 ad-hoc

   zuccon et al. ([861]2015), rekabsaz et al. ([862]2016b)

   trec 6   8 ad-hoc

   glm (ganguly et al. [863]2015), lioma et al. ([864]2016), rekabsaz
   et al. ([865]2016a), roy et al. ([866]2016b), rekabsaz et al.
   ([867]2016b), roy et al. ([868]2016a)

   trec 9   10

   roy et al. ([869]2016a)

   trec 12 ad-hoc

   diaz et al. ([870]2016)

   trec 2005 hard

   rekabsaz et al. ([871]2016a, [872]b)

   trec 2007   2008 million query

   yang et al. ([873]2016a)

   trec 2009   2011 web

   xia et al. ([874]2016)

   trec 2009   2013 web

   pv (grbovic et al. [875]2015b)

   trec 2010   2012 web

   qem (sordoni et al. [876]2014)

   trec 2011 microblog

   cdnn (severyn and moschitti [877]2015), zhang et al. ([878]2014)

   trec 2012 microblog

   cdnn (severyn and moschitti [879]2015)

   trec 2015 contextual suggestion

   manotumruksa et al. ([880]2016)

   trec clueweb09-cat-b

   drmm (guo et al. [881]2016a), qem (sordoni et al. [882]2014), zhang
   et al. ([883]2014), zheng and callan ([884]2015)

   trec dotgov

   zuccon et al. ([885]2015)

   trec enterprise track 2005   2008

   van gysel et al. ([886]2016b)

   trec gov2

   yang et al. ([887]2016a), zamani and croft ([888]2016a, [889]b), zheng
   and callan ([890]2015)

   trec medtrack

   de vine et al. ([891]2014), zuccon et al. ([892]2015)

   trec qa 8   13

   anmm (yang et al. [893]2016b), blstm (wang and nyberg [894]2015),
   cdnn (severyn and moschitti [895]2015), yu et al. ([896]2014), cohen
   et al. ([897]2016)

   trec robust

   glm (ganguly et al. [898]2015), roy et al. ([899]2016b, [900]2016a)

   trec robust 2004

   clinchant and perronnin ([901]2013), diaz et al. ([902]2016), guo
   et al. ([903]2016a), zamani and croft ([904]2016a, [905]b), zheng and
   callan ([906]2015)

   trec wsj87   92

   zuccon et al. ([907]2015)

   trec wt10g

   roy et al. ([908]2016b), zheng and callan ([909]2015)

   yahoo! answers

   boytsov et al. ([910]2016), zhou et al. ([911]2015)

   chinese data

   study

   baidu tieba

   yan et al. ([912]2016)

   baidu zhidao

   yan et al. ([913]2016), zhang et al. ([914]2014), zhou et al.
   ([915]2015)

   douban forum

   yan et al. ([916]2016)

   sina weibo

   yan et al. ([917]2016), zhang et al. ([918]2014)

   sogout 2.0

   zhang et al. ([919]2014)

   multi-lingual data

   study

   clef 2001   2003 ad-hoc

   vulic and moens ([920]2015)

   fire 2013

   gupta et al. ([921]2014)

   ipinyou liao et al. ([922]2014)

   zhang et al. ([923]2016a)

   ut expert retrieval (berendsen et al. [924]2013a)

   van gysel et al. ([925]2016b)

   yandex

   borisov et al. ([926]2016b)

implementing neural sc models

   theano, tensorflow torch and pytorch are libraries that are widely used
   by the deep learning community for implementing neural network models.
   these libraries enable construction of neural network models from
   pre-defined high-level building blocks such as hidden units and layers.
   it is possible to define neural network models with different choices
   of architectures, non-linearity functions, etc.

   gpu support and automatic differentiation (baydin et al. [927]2015) are
   crucial features required for fast training neural networks. theano and
   tensorflow, enable performing matrix operations efficiently, in
   parallel, on gpu. training neural networks with back-propagation
   requires computation of derivatives of the cost function with respect
   to every parameter of the network. automatic differentiation in theano
   and tensorflow relieve the users from the effort on the manual
   derivation of derivatives of the objective function. these libraries
   compute the derivatives automatically given the definition of the
   neural network architecture and the cost function.

publicly available implementations of existing neural ir models

   the deep learning community has a strong tradition of sharing, in some
   version, the code used to produce the experimental results reported in
   the field   s publications. the information retrieval community is
   increasingly adopting this attitude too. some authors of publications
   on neural ir models have shared their code; see e.g., drmm (guo et al.
   [928]2016a),^[929]20 hred (sordoni et al. [930]2015),^[931]21 sert
   (van gysel et al. [932]2016a, [933]b),^[934]22 cdnn (severyn and
   moschitti [935]2015),^[936]23 deepmerge (lee et al. [937]2015),^[938]24
   deeptr (zheng and callan [939]2015),^[940]25 mixed deep (gupta et al.
   [941]2014),^[942]26 ntlm (zuccon et al. [943]2015),^[944]27 anmm (yang
   et al. [945]2016b),^[946]28 kedrlm (roy et al. [947]2016a).^[948]29

references

    1. ai, q., yang, l., guo, j., & croft, w. b. (2016a). analysis of the
       paragraph vector model for information retrieval. in proceedings of
       the 2016 acm international conference on the theory of information
       retrieval, acm, new york, ny, usa, ictir   16 (pp.
       133   142).[949]google scholar
    2. ai, q., yang, l., guo, j., & croft, w. b. (2016b). improving
       language estimation with the paragraph vector model for ad-hoc
       retrieval. in proceedings of the 39th international acm sigir
       conference on research and development in information retrieval,
       acm, new york, ny, usa, sigir   16 (pp. 869   872).[950]google scholar
    3. almasri, m., berrut, c., & chevallet, j. p. (2016). a comparison of
       deep learning based id183 with pseudo-relevance feedback
       and mutual information. in european conference on information
       retrieval (pp. 709   715). springer.[951]google scholar
    4. amati, g., & van rijsbergen, c. j. (2002). probabilistic models of
       information retrieval based on measuring the divergence from
       randomness. acm transactions on information systems (tois), 20(4),
       357   389.[952]crossref[953]google scholar
    5. amer, n. o., mulhem, p., & gery, m. (2016). toward id27
       for personalized information retrieval. in acm sigir workshop on
       neural information retrieval (neu-ir).[954]google scholar
    6. andoni, a., & indyk, p. (2006). near-optimal hashing algorithms for
       approximate nearest neighbor in high dimensions. in 2006 47th
       annual ieee symposium on foundations of computer science (focs   06)
       (pp. 459   468). ieee.[955]google scholar
    7. arel, i., rose, d. c., & karnowski, t. p. (2010). deep machine
       learning   a new frontier in artificial intelligence research. ieee
       computational intelligence magazine, 5(4),
       13   18.[956]crossref[957]google scholar
    8. azimi, j., alam, a., & zhang, r. (2015) .ads keyword rewriting
       using search engine results. in proceedings of the 24th
       international conference on world wide web, international world
       wide web conferences steering committee, republic and canton of
       geneva, switzerland, www   15 companion (pp. 3   4).
       [958]https://doi.org/10.1145/2740908.2742739.
    9. azzopardi, l., de rijke, m., & balog, k. (2007). building simulated
       queries for known-item topics: an analysis using six european
       languages. in 30th annual international acm sigir conference on
       research & development on information retrieval, acm.[959]google
       scholar
   10. bahdanau, d., cho, k., & bengio, y. (2014). neural machine
       translation by jointly learning to align and translate. arxiv
       preprint [960]arxiv:14090473.
   11. bai, b., weston, j., grangier, d., collobert, r., sadamasa, k., qi,
       y., et al. (2009). supervised semantic indexing. in proceedings of
       the 18th acm conference on information and knowledge management (pp
       187   196). acm.[961]google scholar
   12. balikas, g., & amini, m.r. (2016). an empirical study on large
       scale text classification with skip-gram embeddings. in acm sigir
       workshop on neural information retrieval (neu-ir).[962]google
       scholar
   13. bar-yossef, z., & kraus, n. (2011). context-sensitive query
       auto-completion. in proceedings of the 20th international
       conference on world wide web (pp 107   116). acm.[963]google scholar
   14. baroni, m., dinu, g., & kruszewski, g. (2014). don   t count,
       predict! a systematic comparison of context-counting vs.
       context-predicting semantic vectors. in proceedings of the 52nd
       annual meeting of the association for computational linguistics
       (volume 1: long papers) (pp. 238   247). association for
       computational linguistics.[964]google scholar
   15. baydin, a. g., pearlmutter, b. a., radul, a. a., & siskind, j. m.
       (2015). automatic differentiation in machine learning: a survey.
       arxiv preprint [965]arxiv:150205767.
   16. bengio, y. (2009). learning deep architectures for ai. foundations
       and trends in machine learning, 2(1),
       1   127.[966]crossref[967]zbmath[968]google scholar
   17. bengio, y., ducharme, r., vincent, p., & janvin, c. (2003a). a
       neural probabilistic language model. journal of machine learning
       research, 3, 1137   1155.[969]zbmath[970]google scholar
   18. bengio, y., & sen  cal, j. s. (2003b). quick training of
       probabilistic neural nets by importance sampling. in
       aistats.[971]google scholar
   19. berendsen, r., balog, k., bogers, t., van den bosch, a., & de
       rijke, m. (2013a). on the assessment of expertise profiles. journal
       of the american society for information science and technology,
       64(10), 2024   2044.[972]crossref[973]google scholar
   20. berendsen, r., tsagkias, m., weerkamp, w., & de rijke, m. (2013b).
       pseudo test collections for training and tuning microblog rankers.
       in sigir   13: 36th international acm sigir conference on research
       and development in information retrieval. acm.[974]google scholar
   21. berger, a., & lafferty, j. (1999). information retrieval as
       statistical translation. in proceedings of the 22nd annual
       international acm sigir conference on research and development in
       information retrieval (pp. 222   229). acm.[975]google scholar
   22. blei, d. m., ng, a. y., & jordan, m. i. (2003). latent dirichlet
       allocation. journal of machine learning research, 3(jan),
       993   1022.[976]zbmath[977]google scholar
   23. bordes, a., chopra, s., & weston, j. (2014). id53
       with subgraph embeddings. in proceedings of the 2014 conference on
       empirical methods in natural language processing (emnlp) (pp.
       615   620). association for computational linguistics.[978]google
       scholar
   24. borisov, a., markov, i., de rijke, m., & serdyukov, p. (2016a). a
       context-aware time model for web search. in sigir 2016: 39th
       international acm sigir conference on research and development in
       information retrieval (pp. 205   214). acm.[979]google scholar
   25. borisov, a., markov, i., de rijke, m., & serdyukov, p. (2016b). a
       neural click model for web search. in proceedings of the 25th
       international conference on world wide web, international world
       wide web conferences steering committee (pp. 531   541).[980]google
       scholar
   26. boytsov, l., novak, d., malkov, y., & nyberg, e. (2016). off the
       beaten path: let   s replace term-based retrieval with id92 search.
       in proceedings of the 25th acm international on conference on
       information and knowledge management (pp. 1099   1108).
       acm.[981]google scholar
   27. broder, a., domingos, p., de freitas, n., guyon, i., malik, j., &
       neville, j. (2016). is deep learning the new 42? in plenary panel
       at the 22nd acm sigkdd international conference on knowledge
       discovery and data mining.[982]google scholar
   28. broid113y, j., bentz, j. w., bottou, l., guyon, i., lecun, y., moore,
       c., et al. (1993). signature verification using a siamese time
       delay neural network. international journal of pattern recognition
       and artificial intelligence, 7(04),
       669   688.[983]crossref[984]google scholar
   29. cai, f., & de rijke, m. (2016a). learning from homologous queries
       and semantically related terms for query auto completion.
       information processing & management, 52(4),
       628   643.[985]crossref[986]google scholar
   30. cai, f., & de rijke, m. (2016b). a survey of query auto completion
       in information retrieval. foundations and trends in information
       retrieval, 10(4), 273   363.[987]crossref[988]google scholar
   31. cai, f., liang, s., & de rijke, m. (2014). time-sensitive
       personalized query auto-completion. in proceedings of the 23rd acm
       international conference on conference on information and knowledge
       management (pp. 1599   1608). acm.[989]google scholar
   32. carmel, d., zwerdling, n., guy, i., ofek-koifman, s., har   el, n.,
       ronen, i., et al. (2009). personalized social search based on the
       user   s social network. in proceedings of the 18th acm conference on
       information and knowledge management (pp. 1227   1236).
       acm.[990]google scholar
   33. carpineto, c., & romano, g. (2012). a survey of automatic query
       expansion in information retrieval. acm computing surveys, 44(1),
       1:1   1:50.[991]crossref[992]zbmath[993]google scholar
   34. cartright, m. a., allan, j., lavrenko, v., & mcgregor, a. (2010).
       fast id183 using approximations of relevance models. in
       proceedings of the 19th acm international conference on information
       and knowledge management (pp. 1573   1576). acm.[994]google scholar
   35. charikar, m. s. (2002). similarity estimation techniques from
       rounding algorithms. in proceedings of the thirty-fourth annual acm
       symposium on theory of computing (pp. 380   388). acm.[995]google
       scholar
   36. chen, w., grangier, d., & auli, m. (2016). strategies for training
       large vocabulary neural language models. in proceedings of the 54th
       annual meeting of the association for computational linguistics
       (volume 1: long papers) (pp. 1975   1985). association for
       computational linguistics, berlin, germany.[996]google scholar
   37. chirita, p. a., firan, c. s., & nejdl, w. (2007). personalized
       id183 for the web. in proceedings of the 30th annual
       international acm sigir conference on research and development in
       information retrieval (pp. 7   14). acm.[997]google scholar
   38. cho, k. (2015). natural language understanding with distributed
       representation. arxiv preprint [998]arxiv:151107916.
   39. cho, k., van merrienboer, b., g  l  ehre,   ., bougares, f., schwenk,
       h., & bengio, y. (2014). learning phrase representations using id56
       encoder-decoder for id151. corr
       [999]arxiv:1406.1078.
   40. chuklin, a., markov, i., & de rijke m. (2015). click models for web
       search. synthesis lectures on information concepts, retrieval, and
       services. morgan & claypool publishers.[1000]google scholar
   41. chung, j., gulcehre, c., cho, k., & bengio, y. (2014). empirical
       evaluation of gated recurrent neural networks on sequence modeling.
       in nips workshop on deep learning.[1001]google scholar
   42. clinchant, s., & perronnin, f. (2013). aggregating continuous word
       embeddings for information retrieval. in proceedings of the
       workshop on continuous vector space models and their
       compositionality (pp. 100   109).[1002]google scholar
   43. cohen, d., ai, q., & croft, w. b. (2016). adaptability of neural
       networks on varying granularity ir tasks. in acm sigir workshop on
       neural information retrieval (neu-ir).[1003]google scholar
   44. collobert, r., & weston, j. (2008). a unified architecture for
       natural language processing: deep neural networks with multitask
       learning. in proceedings of the 25th international conference on
       machine learning (pp. 160   167). acm.[1004]google scholar
   45. collobert, r., weston, j., bottou, l., karlen, m., kavukcuoglu, k.,
       & kuksa, p. (2011). natural language processing (almost) from
       scratch. journal of machine learning research, 12(aug),
       2493   2537.[1005]zbmath[1006]google scholar
   46. conneau, a., schwenk, h., barrault, l., & lecun, y. (2016). very
       deep convolutional networks for natural language processing. arxiv
       preprint [1007]arxiv:160601781.
   47. croft, b., metzler, d., & strohman, t. (2009). search engines:
       information retrieval in practice. boston:
       addison-wesley.[1008]google scholar
   48. dai, a. m., olah, c., le, q. v., & corrado, g. s. (2014). document
       embedding with paragraph vectors. in nips deep learning
       workshop.[1009]google scholar
   49. dang, v., & croft, b. w. (2010). query reformulation using anchor
       text. in proceedings of the third acm international conference on
       web search and data mining (pp. 41   50). acm.[1010]google scholar
   50. darragh, j. j., witten, i. h., & james, m. l. (1990). the reactive
       keyboard: a predictive typing aid. computer, 23(11),
       41   49.[1011]crossref[1012]google scholar
   51. datar, m., immorlica, n., indyk, p., & mirrokni, v. s. (2004).
       locality-sensitive hashing scheme based on p-stable distributions.
       in proceedings of the twentieth annual symposium on computational
       geometry (pp. 253   262). acm.[1013]google scholar
   52. de vine, l., zuccon, g., koopman, b., sitbon, l., & bruza, p.
       (2014). medical semantic similarity with a neural language model.
       in proceedings of the 23rd acm international conference on
       conference on information and knowledge management (pp. 1819   1822).
       acm.[1014]google scholar
   53. deerwester, s., dumais, s. t., furnas, g. w., landauer, t. k., &
       harshman, r. (1990). indexing by latent semantic analysis. journal
       of the american society for information science, 41(6),
       391.[1015]crossref[1016]google scholar
   54. deng, l. (2014). a tutorial survey of architectures, algorithms,
       and applications for deep learning. apsipa transactions on signal
       and information processing, 3, e2.[1017]crossref[1018]google
       scholar
   55. deng, l., & yu, d. (2013). deep learning: methods and applications.
       foundations and trends in signal processing, 7(3   4),
       197   387.[1019]mathscinet[1020]zbmath[1021]google scholar
   56. dhingra, b., zhou, z., fitzpatrick, d., muehl, m., & cohen, w.
       (2016). tweet2vec: character-based distributed representations for
       social media. in proceedings of the 54th annual meeting of the
       association for computational linguistics (volume 2: short papers)
       (pp. 269   274), association for computational
       linguistics.[1022]google scholar
   57. diaz, f., mitra, b., & craswell, n. (2016). id183 with
       locally-trained id27s. in 54th annual meeting of the
       association for computational linguistics (volume 1: long papers)
       (pp. 367   377), association for computational linguistics, berlin,
       germany.[1023]google scholar
   58. djuric, n., wu, h., radosavljevic, v., grbovic, m., & bhamidipati,
       n. (2015). hierarchical neural language models for joint
       representation of streaming documents and their content. in
       proceedings of the 24th international conference on world wide web,
       acm, new york, ny, usa, www   15 (pp. 248   255).[1024]google scholar
   59. dumais, s., banko, m., brill, e., lin, j., & ng, a. (2002). web
       id53: is more always better? in proceedings of the
       25th annual international acm sigir conference on research and
       development in information retrieval (pp. 291   298).
       acm.[1025]google scholar
   60. dyer, c. (2014). notes on noise contrastive estimation and negative
       sampling. corr [1026]arxiv:1410.8251.
   61. elman, j. l. (1990). finding structure in time. cognitive science,
       14(2), 179   211.[1027]crossref[1028]google scholar
   62. erhan, d., bengio, y., courville, a., manzagol, p. a., vincent, p.,
       & bengio, s. (2010). why does unsupervised pre-training help deep
       learning? journal of machine learning research, 11(feb),
       625   660.[1029]mathscinet[1030]zbmath[1031]google scholar
   63. faruqui, m., dodge, j., jauhar, s. k., dyer, c., hovy, e., & smith,
       n. a. (2014). retrofitting word vectors to semantic lexicons. arxiv
       preprint [1032]arxiv:14114166.
   64. friedman, j. h. (2001). greedy function approximation: a gradient
       boosting machine. annals of statistics, 29(5),
       1189   1232.[1033]mathscinet[1034]crossref[1035]zbmath[1036]google
       scholar
   65. ganguly, d., roy, d., mitra, m., & jones, g. j. (2015). word
       embedding based generalized language model for information
       retrieval. in proceedings of the 38th international acm sigir
       conference on research and development in information retrieval
       (pp. 795   798). acm.[1037]google scholar
   66. ganguly, d., roy, d., mitra, m., & jones, g. (2016). representing
       documents and queries as sets of word embedded vectors for
       information retrieval. in acm sigir workshop on neural information
       retrieval (neu-ir).[1038]google scholar
   67. gao, j., he, x., & li, d. (2015). deep learning for web search and
       natural language processing. microsoft research technical report.
       redmond, wa: microsoft corporation[1039]google scholar
   68. gao, j., he, x., & nie, j. y. (2010). clickthrough-based
       translation models for web search: from word models to phrase
       models. in proceedings of the 19th acm international conference on
       information and knowledge management (cikm) (pp. 1139   1148).
       acm.[1040]google scholar
   69. gao, j., pantel, p., gamon, m., he, x., & deng, l. (2014). modeling
       interestingness with deep neural networks. in proceedings of the
       2014 conference on empirical methods in natural language processing
       (emnlp), association for computational linguistics, doha, qatar
       (pp. 2   13).[1041]google scholar
   70. goldberg, y. (2016). a primer on neural network models for natural
       language processing. journal of artificial intelligence research,
       57, 345   420.[1042]mathscinet[1043]zbmath[1044]google scholar
   71. goodfellow, i., bengio, y., & courville, a. (2016). deep learning.
       cambridge: mit press.[1045]zbmath[1046]google scholar
   72. graves, a. (2012). neural networks. in supervised sequence
       labelling with recurrent neural networks (pp. 15   35).
       springer.[1047]google scholar
   73. graves, a. (2013). generating sequences with recurrent neural
       networks. arxiv preprint [1048]arxiv:13080850.
   74. graves, a., wayne, g., & danihelka, i. (2014). neural turing
       machines. arxiv preprint [1049]arxiv:14105401.
   75. grbovic, m., djuric, n., radosavljevic, v., & bhamidipati, n.
       (2015a). search retargeting using directed query embeddings. in
       proceedings of the 24th international conference on world wide web,
       acm, new york, ny, usa, www   15 companion (pp. 37   38).[1050]google
       scholar
   76. grbovic, m., djuric, n., radosavljevic, v., silvestri, f., &
       bhamidipati, n. (2015b). context-and content-aware embeddings for
       query rewriting in sponsored search. in proceedings of the 38th
       international acm sigir conference on research and development in
       information retrieval (pp. 383   392). acm.[1051]google scholar
   77. greff, k., srivastava, r. k., koutn  k, j., steunebrink, b. r., &
       schmidhuber, j. (2015). lstm: a search space odyssey. arxiv
       preprint [1052]arxiv:150304069.
   78. guo, j., fan, y., ai, q., & croft, w. b. (2016a). a deep relevance
       matching model for ad-hoc retrieval. in the 25th acm international
       conference on information and knowledge management, indianapolis,
       united states.[1053]google scholar
   79. guo, j., fan, y., ai, q., & croft, w. b. (2016b). a deep relevance
       matching model for ad-hoc retrieval. in cikm 2016: 25th acm
       conference on information and knowledge management,
       acm.[1054]google scholar
   80. gupta, p., bali, k., banchs, r. e., choudhury, m., & rosso, p.
       (2014). id183 for mixed-script information retrieval. in
       proceedings of the 37th international acm sigir conference on
       research & development in information retrieval (pp. 677   686).
       acm.[1055]google scholar
   81. gutmann, m. u., & hyv  rinen, a. (2012). noise-contrastive
       estimation of unnormalized statistical models, with applications to
       natural image statistics. journal of machine learning research,
       13(1), 307   361.[1056]mathscinet[1057]zbmath[1058]google scholar
   82. harman, d., & buckley, c. (2009). overview of the reliable
       information access workshop. information retrieval, 12(6),
       615   641.[1059]crossref[1060]google scholar
   83. harris, z. s. (1954). distributional structure. word, 10(2   3),
       146   162.[1061]crossref[1062]google scholar
   84. hill, f., cho, k., & korhonen, a. (2016). learning distributed
       representations of sentences from unlabelled data. in proceedings
       of the 2016 conference of the north american chapter of the
       association for computational linguistics: human language
       technologies, association for computational linguistics, san diego,
       california, (pp. 1367   1377).[1063]google scholar
   85. hinton, g., deng, l., yu, d., dahl, g. e., mohamed, ar., jaitly,
       n., et al. (2012). deep neural networks for acoustic modeling in
       id103: the shared views of four research groups. ieee
       signal processing magazine, 29(6), 82   97.[1064]google scholar
   86. hinton, g. e., & salakhutdinov, r. r. (2006). reducing the
       dimensionality of data with neural networks. science, 313(5786),
       504   507.[1065]mathscinet[1066]crossref[1067]zbmath[1068]google
       scholar
   87. hochreiter, s., & schmidhuber, j. (1997). long short-term memory.
       neural computation, 9(8), 1735   1780.[1069]crossref[1070]google
       scholar
   88. hu, b., lu, z., li, h., & chen, q. (2014). convolutional neural
       network architectures for matching natural language sentences. in
       advances in neural information processing systems (pp.
       2042   2050).[1071]google scholar
   89. huang, p. s., he, x., gao, j., deng, l., acero, a., & heck, l.
       (2013). learning deep structured semantic models for web search
       using clickthrough data. in proceedings of the 22nd acm
       international conference on information & knowledge management (pp.
       2333   2338). acm.[1072]google scholar
   90. jaakkola, t. s., & haussler, d. (1999). exploiting generative
       models in discriminative classifiers. advances in neural
       information processing systems (pp. 487   493).[1073]google scholar
   91. jiang, j. y., ke, y. y., chien, p. y., & cheng, p. j. (2014).
       learning user reformulation behavior for query auto-completion. in
       proceedings of the 37th international acm sigir conference on
       research & development in information retrieval (pp. 445   454).
       acm.[1074]google scholar
   92. jurgovsky, j., granitzer, m., & seifert, c. (2016) evaluating
       memory efficiency and robustness of id27s. in european
       conference on information retrieval (pp. 200   211).
       springer.[1075]google scholar
   93. kalchbrenner, n., grefenstette, e., & blunsom, p. (2014). a
       convolutional neural network for modelling sentences. arxiv
       preprint [1076]arxiv:14042188.
   94. kanhabua, n., ren, h., & moeslund, t. b. (2016). learning dynamic
       classes of events using stacked multilayer id88 networks. in
       acm sigir workshop on neural information retrieval
       (neu-ir).[1077]google scholar
   95. kenter, t., & de rijke, m. (2015). short text similarity with word
       embeddings. in proceedings of the 24th acm international on
       conference on information and knowledge management (pp. 1411   1420).
       acm.[1078]google scholar
   96. kim, s., wilbur, w. j., & lu, z. (2016). bridging the gap: a
       semantic similarity measure between queries and documents. arxiv
       preprint [1079]arxiv:160801972.
   97. kim, y. (2014). convolutional neural networks for sentence
       classification. arxiv preprint [1080]arxiv:14085882.
   98. kiros, r., zhu, y., salakhutdinov, r. r., zemel, r., urtasun, r.,
       torralba, a., et al. (2015). skip-thought vectors. in advances in
       neural information processing systems (pp. 3294   3302).[1081]google
       scholar
   99. krizhevsky, a., sutskever, i., & hinton, g. e. (2012). id163
       classification with deep convolutional neural networks. in advances
       in neural information processing systems (pp.
       1097   1105).[1082]google scholar
   100. kumar, a., irsoy, o., su, j., bradbury, j., english, r., pierce,
       b., et al. (2015). ask me anything: dynamic memory networks for
       natural language processing. arxiv preprint [1083]arxiv:150607285.
   101. kusner, m. j., sun, y., kolkin, n. i., & weinberger, k. q. (2015).
       from id27s to document distances. in proceedings of the
       32nd international conference on machine learning (icml 2015) (pp.
       957   966).[1084]google scholar
   102. lavrenko, v., & croft, w. b. (2001). relevance based language
       models. in proceedings of the 24th annual international acm sigir
       conference on research and development in information retrieval,
       acm, new york, ny, usa, sigir   01 (pp. 120   127).[1085]google scholar
   103. le, q. v., & mikolov, t. (2014). distributed representations of
       sentences and documents. in icml (vol. 14, pp.
       1188   1196).[1086]google scholar
   104. lecun, y., & bengio, y. (1995). convolutional networks for images,
       speech, and time series. the handbook of brain theory and neural
       networks, 3361(10), 1995.[1087]google scholar
   105. lecun, y., bengio, y., & hinton, g. (2015). deep learning. nature,
       521(7553), 436   444.[1088]crossref[1089]google scholar
   106. lee, c. j., ai, q., croft, w. b., & sheldon, d. (2015). an
       optimization framework for merging multiple result lists. in
       proceedings of the 24th acm international on conference on
       information and knowledge management (pp. 303   312).
       acm.[1090]google scholar
   107. lei, t., joshi, h., barzilay, r., jaakkola, t. s., tymoshenko, k.,
       moschitti, a., et al. (2016). semi-supervised question retrieval
       with gated convolutions. in naacl hlt 2016, the 2016 conference of
       the north american chapter of the association for computational
       linguistics: human language technologies, san diego california,
       usa, june 12   17, 2016 (pp. 1279   1289).[1091]google scholar
   108. levy, o., goldberg, y., & dagan, i. (2015). improving
       distributional similarity with lessons learned from word
       embeddings. transactions of the association for computational
       linguistics, 3, 211   225.[1092]google scholar
   109. li, h. (2016, july). does ir need deep learning? in keynote speech
       at sigir 2016 neu-ir workshop, pisa.[1093]google scholar
   110. li, h., & lu, z. (2016). deep learning for information retrieval.
       in sigir, pisa, italy, vol tutorial at the 39th international acm
       sigir conference on research and development in information
       retrieval (pp. 1203   1206).[1094]google scholar
   111. li, h., & xu, j. (2013). semantic matching in search. foundations
       and trends in information retrieval, 7(5),
       343   469.[1095]crossref[1096]google scholar
   112. li, j., luong, m. t., jurafsky, d., & hovy, e. (2015). when are
       tree structures necessary for deep learning of representations?
       arxiv preprint [1097]arxiv:150300185.
   113. li, x., guo, c., chu, w., wang, y.-y., & shavlik, j. (2014). deep
       learning powered in-session contextual ranking using clickthrough
       data. in workshop on personalization: methods and applications,
       neural information processing systems (nips).[1098]google scholar
   114. liao, h., peng, l., liu, z., & shen, x. (2014). ipinyou global rtb
       bidding algorithm competition dataset. in proceedings of the eighth
       international workshop on data mining for online advertising (pp.
       1   6). acm.[1099]google scholar
   115. lioma, c., larsen, b., petersen, c., & simonsen, j. g. (2016).
       deep learning relevance: creating relevant information (as opposed
       to retrieving it). in acm sigir workshop on neural information
       retrieval (neu-ir).[1100]google scholar
   116. liu, t. y. (2009). learning to rank for information retrieval.
       foundations and trends in information retrieval, 3(3),
       225   331.[1101]crossref[1102]google scholar
   117. liu, w., wang, j., ji, r., jiang, y. g., & chang, s. f. (2012).
       supervised hashing with kernels. in 2012 ieee conference on
       id161 and pattern recognition (cvpr) (pp. 2074   2081).
       ieee.[1103]google scholar
   118. liu, x., & croft, w. b. (2004). cluster-based retrieval using
       language models. in proceedings of the 27th annual international
       acm sigir conference on research and development in information
       retrieval (pp. 186   193). acm.[1104]google scholar
   119. liu, x., gao, j., he, x., deng, l., duh, k., & wang, y. y. (2015).
       representation learning using multi-task deep neural networks for
       semantic classification and information retrieval. in proceedings
       of the 2015 conference of the north american chapter of the
       association for computational linguistics: human language
       technologies, association for computational linguistics, denver,
       colorado (pp. 912   921).[1105]google scholar
   120. lu, z., & li, h. (2013). a deep architecture for matching short
       texts. in advances in neural information processing systems (pp.
       1367   1375).[1106]google scholar
   121. lund, k., & burgess, c. (1996). producing high-dimensional
       semantic spaces from lexical co-occurrence. behavior research
       methods, instruments, & computers, 28(2),
       203   208.[1107]crossref[1108]google scholar
   122. luukkonen, p., koskela, m., & floreen, p. (2016). lstm-based
       predictions for proactive information retrieval. in acm sigir
       workshop on neural information retrieval (neu-ir).[1109]google
       scholar
   123. ma, l., lu, z., & li, h. (2015a). learning to answer questions
       from image using convolutional neural network. arxiv preprint
       [1110]arxiv:150600333.
   124. ma, l., lu, z., shang, l., & li, h. (2015b). multimodal
       convolutional neural networks for matching image and sentence. in
       proceedings of the ieee international conference on id161
       (pp. 2623   2631).[1111]google scholar
   125. ma, l., lu, z., & li, h. (2016). learning to answer questions from
       image using convolutional neural network. in aaai conference on
       artificial intelligence (pp. 3567   3573).[1112]google scholar
   126. manning, c. (2016). understanding human language: can nlp and deep
       learning help? in proceedings of the 39th international acm sigir
       conference on research and development in information retrieval,
       sigir    16, pisa, italy. new york, ny: acm.[1113]google scholar
   127. manning, c. d., raghavan, p., & sch  tze, h. (2008). introduction
       to information retrieval. cambridge: cambridge university
       press.[1114]crossref[1115]zbmath[1116]google scholar
   128. manotumruksa, j., macdonald, c., & ounis, i. (2016). modelling
       user preferences using id27s for context-aware venue
       recommendation. in acm sigir workshop on neural information
       retrieval (neu-ir).[1117]google scholar
   129. mcauley, j., pandey, r., & leskovec, j. (2015). inferring networks
       of substitutable and complementary products. in kdd: acm (pp.
       785   794).[1118]google scholar
   130. mcclelland, j. l., rumelhart, d. e., pdp research group. (1986).
       parallel distributed processing. explorations in the microstructure
       of cognition (vol. 2, pp. 216   271).[1119]google scholar
   131. mesnil, g., he, x., deng, l., & bengio, y. (2013). investigation
       of recurrent-neural-network architectures and learning methods for
       spoken language understanding. in interspeech (pp.
       3771   3775).[1120]google scholar
   132. mesnil, g., mikolov, t., ranzato, m., & bengio, y. (2014).
       ensemble of generative and discriminative techniques for sentiment
       analysis of movie reviews. in international conference on learning
       representations (iclr).[1121]google scholar
   133. metz, c. (2016). ai is transforming google search. the rest of the
       web is next. wired magazine.[1122]google scholar
   134. mikolov, t., karafi  t, m., burget, l., cernock   , j., & khudanpur,
       s. (2010). recurrent neural network based language model. in
       interspeech (pp. 1045   1048).[1123]google scholar
   135. mikolov, t., chen, k., corrado, g., & dean, j. (2013a). efficient
       estimation of word representations in vector space. corr
       [1124]arxiv:1301.3781.
   136. mikolov, t., sutskever, i., chen, k., corrado, g. s., & dean, j.
       (2013b). distributed representations of words and phrases and their
       compositionality. in c. j. c. burges, l. bottou, m. welling, z.
       ghahramani, & k. q. weinberger (eds.), advances in neural
       information processing systems (vol. 26, pp. 3111   3119). curran
       associates, inc.[1125]google scholar
   137. mikolov, t., yih, w., & zweig, g. (2013c). linguistic regularities
       in continuous space word representations. in proceedings of the
       2013 conference of the north american chapter of the association
       for computational linguistics: human language technologies,
       association for computational linguistics, atlanta, georgia, (pp.
       746   751).[1126]google scholar
   138. mitchell, j., & lapata, m. (2010). composition in distributional
       models of semantics. cognitive science, 34(8), 1388   1429.
       [1127]https://doi.org/10.1111/j.1551-6709.2010.01106.x.[1128]crossr
       ef[1129]google scholar
   139. mitra, b. (2015). exploring session context using distributed
       representations of queries and reformulations. in proceedings of
       the 38th international acm sigir conference on research and
       development in information retrieval (pp. 3   12). acm.[1130]google
       scholar
   140. mitra, b., & craswell, n. (2015). query auto-completion for rare
       prefixes. in proceedings of the 24th acm international on
       conference on information and knowledge management (pp. 1755   1758).
       acm.[1131]google scholar
   141. mitra, b., nalisnick, e., craswell, n., & caruana, r. (2016). a
       dual embedding space model for document ranking. arxiv preprint
       [1132]arxiv:160201137.
   142. mnih, a., & teh, y. w. (2012). a fast and simple algorithm for
       training neural probabilistic language models. arxiv preprint
       [1133]arxiv:12066426.
   143. morin, f., & bengio, y. (2005). hierarchical probabilistic neural
       network language model. in proceedings of the tenth international
       workshop on artificial intelligence and statistics, aistats 2005,
       bridgetown, barbados, january 6   8, 2005.[1134]google scholar
   144. moshfeghi, y., triantafillou, p., & pollick, f. e. (2016).
       understanding information need: an fmri study. in proceedings of
       the 39th international acm sigir conference on research and
       development in information retrieval, acm, new york, ny, usa,
       sigir   16 (pp. 335   344).[1135]google scholar
   145. nair, v., & hinton, g. e. (2010). rectified linear units improve
       restricted id82s. in proceedings of the 27th
       international conference on machine learning (icml-10) (pp.
       807   814).[1136]google scholar
   146. nalisnick, e., mitra, b., craswell, n., & caruana, r. (2016).
       improving document ranking with dual id27s. in 25th world
       wide web (www) conference companion volume, international world
       wide web conferences steering committee (pp. 83   84).[1137]google
       scholar
   147. neelakantan, a., shankar, j., passos, a., & mccallum, a. (2014).
       efficient non-parametric estimation of multiple embeddings per word
       in vector space. in proceedings of the 2014 conference on empirical
       methods in natural language processing (emnlp) (pp.
       1059   1069).[1138]google scholar
   148. nguyen, g. h., tamine, l., soulier, l., & bricon-souf, n. (2016).
       toward a deep neural approach for knowledge-based ir. in acm sigir
       workshop on neural information retrieval (neu-ir).[1139]google
       scholar
   149. onal, k. d., altingovde, i. s., & karagoz, p. (2015). utilizing
       id27s for result diversification in tweet search (pp.
       366   378). cham: springer.
       [1140]https://doi.org/10.1007/978-3-319-28940-3_29.
   150. ordentlich, e., yang, l., feng, a., cnudde, p., grbovic, m.,
       djuric, n., et al. (2016). network-efficient distributed id97
       training system for large vocabularies. in the 25th acm
       international conference on information and knowledge management,
       indianapolis, united states.[1141]google scholar
   151. palakodety, s., & callan, j. (2014). query transformations for
       result merging. in proceedings of the twenty-third text retrieval
       conference, trec 2014, gaithersburg, maryland, usa, november 19   21,
       2014.[1142]google scholar
   152. palangi, h., deng, l., shen, y., gao, j., he, x., chen, j., et al.
       (2014). semantic modelling with long-short-term memory for
       information retrieval. corr [1143]arxiv:1412.6629.
   153. palangi, h., deng, l., shen, y., gao, j., he, x., chen, j., et al.
       (2016). deep sentence embedding using long short-term memory
       networks: analysis and application to information retrieval.
       ieee/acm transactions on audio, speech, and language processing,
       24(4), 694   707.[1144]crossref[1145]google scholar
   154. pang, l., lan, y., guo, j., xu, j., & cheng, x. (2016a) a study of
       matchpyramid models on ad-hoc retrieval. in acm sigir workshop on
       neural information retrieval (neu-ir).[1146]google scholar
   155. pang, l., lan, y., guo, j., xu, j., wan, s., & cheng, x. (2016b).
       text matching as image recognition. in 30th aaai conference on
       artificial intelligence (pp. 2793   2799).[1147]google scholar
   156. pascanu, r., mikolov, t., & bengio, y. (2013). on the difficulty
       of training recurrent neural networks. icml (3), 28,
       1310   1318.[1148]google scholar
   157. pennington, j., socher, r., & manning, c. d. (2014). glove: global
       vectors for word representation. in emnlp (vol. 14, pp.
       1532   1543).[1149]google scholar
   158. ponte, j. m., & croft, w. b. (1998). a id38 approach
       to information retrieval. in proceedings of the 21st annual
       international acm sigir conference on research and development in
       information retrieval (pp. 275   281). acm.[1150]google scholar
   159. radlinski, f., & joachims, t. (2005). query chains: learning to
       rank from implicit feedback. in proceedings of the eleventh acm
       sigkdd international conference on knowledge discovery in data
       mining, acm, new york, ny, usa, kdd   05 (pp. 239   248).[1151]google
       scholar
   160. rekabsaz, n., lupu, m., & hanbury, a. (2016a). uncertainty in
       neural network id27 exploration of potential threshold.
       in acm sigir workshop on neural information retrieval
       (neu-ir).[1152]google scholar
   161. rekabsaz, n., lupu, m., hanbury, a., & zuccon, g. (2016b)
       generalizing translation models in the probabilistic relevance
       framework. in proceedings of the 25th acm international on
       conference on information and knowledge management, cikm    16,
       indianapolis, indiana (pp. 711   720). new york, ny: acm.[1153]google
       scholar
   162. robertson, s. (2004). understanding inverse document frequency: on
       theoretical arguments for idf. journal of documentation, 60(5),
       503   520.[1154]crossref[1155]google scholar
   163. rocchio, j. j. (1971). relevance feedback in information
       retrieval. the smart retrieval system-experiments in automatic
       document processing (pp. 313   323).[1156]google scholar
   164. roy, d., ganguly, d., mitra, m., & jones, g. j. (2016a). word
       vector compositionality based relevance feedback using kernel
       density estimation. in proceedings of the 25th acm international on
       conference on information and knowledge management, acm, new york,
       ny, usa, cikm   16 (pp. 1281   1290).[1157]google scholar
   165. roy, d., paul, d., & mitra, m. (2016b). using id27s for
       automatic id183. in acm sigir workshop on neural
       information retrieval (neu-ir).[1158]google scholar
   166. rumelhart, d. e., hinton, g. e., & williams, r. j. (1988).
       learning representations by back-propagating errors. cognitive
       modeling, 5(3), 1.[1159]zbmath[1160]google scholar
   167. salakhutdinov, r., & hinton, g. (2009). semantic hashing.
       international journal of approximate reasoning, 50(7),
       969   978.[1161]crossref[1162]google scholar
   168. schmidhuber, j. (2015). deep learning in neural networks: an
       overview. neural networks, 61, 85   117.
       [1163]https://doi.org/10.1016/j.neunet.2014.09.003.[1164]crossref[1
       165]google scholar
   169. sennrich, r., haddow, b., & birch, a. (2016). neural machine
       translation of rare words with subword units. in proceedings of the
       54th annual meeting of the association for computational
       linguistics (volume 1: long papers), association for computational
       linguistics, berlin, germany (pp. 1715   1725).[1166]google scholar
   170. severyn, a., & moschitti, a. (2015). learning to rank short text
       pairs with convolutional deep neural networks. in proceedings of
       the 38th international acm sigir conference on research and
       development in information retrieval (pp. 373   382).
       acm.[1167]google scholar
   171. shen, y., he, x., gao, j., deng, l., & mesnil, g. (2014a). a
       latent semantic model with convolutional-pooling structure for
       information retrieval. in proceedings of the 23rd acm international
       conference on conference on information and knowledge management
       (pp. 101   110). acm.[1168]google scholar
   172. shen, y., he, x., gao, j., deng, l., & mesnil, g. (2014b).
       learning semantic representations using convolutional neural
       networks for web search. in proceedings of the 23rd international
       conference on world wide web (pp. 373   374). acm.[1169]google
       scholar
   173. shokouhi, m., & radinsky, k. (2012). time-sensitive query
       auto-completion. in proceedings of the 35th international acm sigir
       conference on research and development in information retrieval
       (pp. 601   610). acm.[1170]google scholar
   174. smolensky, p. (1986). information processing in dynamical systems:
       foundations of harmony theory. technical report, dtic
       document.[1171]google scholar
   175. socher, r., chen, d., manning, c. d., & ng, a. (2013). reasoning
       with neural tensor networks for knowledge base completion. in
       advances in neural information processing systems (pp.
       926   934).[1172]google scholar
   176. song, y., elkahky, a. m., & he, x. (2016). multi-rate deep
       learning for temporal recommendation. in proceedings of the 39th
       international acm sigir conference on research and development in
       information retrieval, acm, new york, ny, usa, sigir   16 (pp.
       909   912).[1173]google scholar
   177. sordoni, a., bengio, y., & nie, j.y. (2014). learning concept
       embeddings for id183 by quantum id178 minimization. in
       aaai (pp. 1586   1592).[1174]google scholar
   178. sordoni, a., bengio, y., vahabi, h., lioma, c., grue simonsen, j.,
       & nie, j. y. (2015). a hierarchical recurrent encoder-decoder for
       generative context-aware query suggestion. in proceedings of the
       24th acm international on conference on information and knowledge
       management (cikm) (pp. 553   562). acm.[1175]google scholar
   179. suggu, s. p., goutham, k. n., chinnakotla, m. k., & shrivastava,
       m. (2016). deep feature fusion network for answer quality
       prediction in community id53. in acm sigir workshop
       on neural information retrieval (neu-ir).[1176]google scholar
   180. sutskever, i., vinyals, o., & le ,q. v. (2014). sequence to
       sequence learning with neural networks. in advances in neural
       information processing systems (pp. 3104   3112).[1177]google scholar
   181. tai, k. s., socher, r., & manning, c. d. (2015). improved semantic
       representations from tree-structured long short-term memory
       networks. in proceedings of the 53rd annual meeting of the
       association for computational linguistics and the 7th international
       joint conference on natural language processing (volume 1: long
       papers), association for computational linguistics, beijing, china
       (pp. 1556   1566).[1178]google scholar
   182. turney, p. d., & pantel, p. (2010). from frequency to meaning:
       vector space models of semantics. journal of artificial
       intelligence research (jair), 37, 141   188.
       [1179]https://doi.org/10.1613/jair.2934.[1180]mathscinet[1181]zbmat
       h[1182]google scholar
   183. van der maaten, l., & hinton, g. (2008). visualizing data using
       id167. journal of machine learning research, 9,
       2579   2605.[1183]zbmath[1184]google scholar
   184. van gysel, c., de rijke, m., & kanoulas, e. (2016a). learning
       latent vector spaces for product search. in cikm 2016: 25th acm
       conference on information and knowledge management.
       acm.[1185]google scholar
   185. van gysel, c., de rijke, m., & worring, m. (2016b). unsupervised,
       efficient and semantic expertise retrieval. in proceedings of the
       25th international conference on world wide web, international
       world wide web conferences steering committee, republic and canton
       of geneva, switzerland, www   16 (pp. 1069   1079).
       [1186]https://doi.org/10.1145/2872427.2882974.
   186. vulic, i., & moens, m. f. (2015). monolingual and cross-lingual
       information retrieval models based on (bilingual) id27s.
       in proceedings of the 38th international acm sigir conference on
       research and development in information retrieval (pp. 363   372).
       acm.[1187]google scholar
   187. wan, j., wang, d., hoi, s. c. h., wu, p., zhu, j., zhang, y., et
       al. (2014). deep learning for content-based id162: a
       comprehensive study. in proceedings of the 22nd acm international
       conference on multimedia (pp. 157   166). acm.[1188]google scholar
   188. wang, d., & nyberg, e. (2015). a long short-term memory model for
       answer sentence selection in id53. acl,
       july.[1189]google scholar
   189. wang, m., smith, n. a., & mitamura, t. (2007). what is the
       jeopardy model? a quasi-synchronous grammar for qa. in emnlp-conll
       (vol. 7, pp. 22   32).[1190]google scholar
   190. wei, x., & croft, w. b. (2006). lda-based document models for
       ad-hoc retrieval. in proceedings of the 29th annual international
       acm sigir conference on research and development in information
       retrieval (pp. 178   185). acm.[1191]google scholar
   191. weiss, y., torralba. a., & fergus, r. (2009). spectral hashing. in
       advances in neural information processing systems (pp.
       1753   1760).[1192]google scholar
   192. weston, j., bengio, s., & usunier, n. (2010). large scale image
       annotation: learning to rank with joint word-image embeddings.
       machine learning, 81(1),
       21   35.[1193]mathscinet[1194]crossref[1195]google scholar
   193. weston, j., chopra, s., & bordes, a. (2014). memory networks. in
       international conference on learning representations
       (iclr).[1196]google scholar
   194. wu, q., burges, c. j., svore, k. m., & gao, j. (2010). adapting
       boosting for information retrieval measures. information retrieval,
       13(3), 254   270.[1197]crossref[1198]google scholar
   195. wu, y., schuster, m., chen, z., le, q. v., norouzi, m., macherey,
       w., et al. (2016). google   s id4 system:
       bridging the gap between human and machine translation. arxiv
       preprint [1199]arxiv:160908144.
   196. xia, l., xu, j., lan, y., guo, j., & cheng, x. (2015). learning
       maximal marginal relevance model via directly optimizing diversity
       evaluation measures. in proceedings of the 38th international acm
       sigir conference on research and development in information
       retrieval (pp. 113   122). acm.[1200]google scholar
   197. xia, l., xu, j., lan, y., guo, j., & cheng, x. (2016). modeling
       document novelty with neural tensor network for search result
       diversification. in proceedings of the 39th international acm sigir
       conference on research and development in information retrieval,
       pisa, italy (pp. 395   404).[1201]google scholar
   198. xu, c., bai, y., bian, j., gao, b., wang, g., liu, x., et al.
       (2014). rc-net: a general framework for incorporating knowledge
       into word representations. in proceedings of the 23rd acm
       international conference on conference on information and knowledge
       management (pp. 1219   1228). acm.[1202]google scholar
   199. xu, k., ba, j., kiros, r., cho, k., courville, a.c.,
       salakhutdinov, r., et al. (2015). show, attend and tell: neural
       image id134 with visual attention. corr
       [1203]arxiv:1502.03044.
   200. yan, r., song, y., & wu, h. (2016) learning to respond with deep
       neural networks for retrieval-based human   computer conversation
       system. in proceedings of the 39th international acm sigir
       conference on research and development in information retrieval
       (pp. 55   64). acm.[1204]google scholar
   201. yang, j., stones, r., wang, g., & liu, x. (2016a). selective term
       proximity scoring via bp-ann. in acm sigir workshop on neural
       information retrieval (neu-ir).[1205]google scholar
   202. yang, l., ai, q., guo, j., & croft, w. b. (2016b). anmm: ranking
       short answer texts with attention-based neural matching model. in
       the 25th acm international conference on information and knowledge
       management, indianapolis, united states.[1206]google scholar
   203. yang, x., macdonald, c., & ounis, i. (2016c). using word
       embeddings in twitter election classification. in acm sigir
       workshop on neural information retrieval (neu-ir).[1207]google
       scholar
   204. ye, x., qi, z., & massey, d. (2015). learning relevance from click
       data via neural network based similarity models. in 2015 ieee
       international conference on big data (big data) (pp. 801   806).
       ieee.[1208]google scholar
   205. ye, x., shen, h., ma, x., bunescu, r., & liu, c. (2016). from word
       embeddings to document similarities for improved information
       retrieval in software engineering. in proceedings of the 38th
       international conference on software engineering (pp. 404   415).
       acm.[1209]google scholar
   206. yi, x., & allan, j. (2009). a comparative study of utilizing topic
       models for information retrieval. in european conference on
       information retrieval (pp. 29   41). springer.[1210]google scholar
   207. yu, d., & deng, l. (2011). deep learning and its applications to
       signal and information processing. ieee signal processing magazine,
       28(1), 145   154.[1211]crossref[1212]google scholar
   208. yu, l., hermann, k. m., blunsom, p., & pulman, s. (2014). deep
       learning for answer sentence selection. arxiv preprint
       [1213]arxiv:14121632.
   209. zamani, h., & croft, w. b. (2016a). embedding-based query language
       models. in proceedings of the 2016 acm international conference on
       the theory of information retrieval (pp. 147   156).[1214]google
       scholar
   210. zamani, h., & croft, w. b. (2016b). estimating embedding vectors
       for queries. in proceedings of the 2016 acm on international
       conference on the theory of information retrieval (pp. 123   132).
       acm.[1215]google scholar
   211. zhai, s., chang, k., zhang, r., & zhang, z. (2016a). attention
       based recurrent neural networks for online advertising. in
       proceedings of the 25th international conference companion on world
       wide web, international world wide web conferences steering
       committee, republic and canton of geneva, switzerland, www   16
       companion (pp. 141   142).[1216]google scholar
   212. zhai, s., chang, k., zhang, r., & zhang, z. m. (2016b).
       deepintent: learning attentions for online advertising with
       recurrent neural networks. in proceedings of the 22nd acm sigkdd
       international conference on knowledge discovery and data mining,
       acm, new york, ny, usa, kdd   16 (pp. 1295   1304).[1217]google scholar
   213. zhang, q., kang, j., qian, j., & huang, x. (2014). continuous word
       embeddings for detecting local text reuses at the semantic level.
       in proceedings of the 37th international acm sigir conference on
       research & development in information retrieval (pp. 797   806).
       acm.[1218]google scholar
   214. zhang, w., du, t., & wang, j. (2016a). deep learning over
       multi-field categorical data. in european conference on information
       retrieval (pp. 45   57). springer.[1219]google scholar
   215. zhang, x., zhao, j., & lecun, y. (2015). character-level
       convolutional networks for text classification. in advances in
       neural information processing systems (pp. 649   657).[1220]google
       scholar
   216. zhang, y., & wallace, b. (2015). a sensitivity analysis of (and
       practitioners    guide to) convolutional neural networks for sentence
       classification. arxiv preprint [1221]arxiv:151003820.
   217. zhang, y., roller, s., & wallace, b. c. (2016b). mgnc-id98: a
       simple approach to exploiting multiple id27s for sentence
       classification. in proceedings of the 2016 conference of the north
       american chapter of the association for computational linguistics:
       human language technologies, association for computational
       linguistics, san diego, california (pp. 1522   1527).[1222]google
       scholar
   218. zhang, y., lease, m., & wallace, b. c. (2017). exploiting domain
       knowledge via grouped weight sharing with application to text
       categorization. arxiv preprint [1223]arxiv:170202535.
   219. zheng, g., & callan, j. (2015). learning to reweight terms with
       distributed representations. in proceedings of the 38th
       international acm sigir conference on research and development in
       information retrieval, acm, new york, ny, usa, sigir   15 (pp.
       575   584).[1224]google scholar
   220. zhou, g., he, t., zhao, j., & hu, p. (2015). learning continuous
       id27 with metadata for question retrieval in community
       id53. in proceedings of acl (pp.
       250   259).[1225]google scholar
   221. zhu, y., lan, y., guo, j., cheng, x., & niu, s. (2014). learning
       for search result diversification. in proceedings of the 37th
       international acm sigir conference on research & development in
       information retrieval (pp. 293   302). acm.[1226]google scholar
   222. zuccon, g., koopman, b., bruza, p., & azzopardi, l. (2015).
       integrating and evaluating neural id27s in information
       retrieval. in proceedings of the 20th australasian document
       computing symposium (p. 12). acm.[1227]google scholar

copyright information

      the author(s) 2017

   open accessthis article is distributed under the terms of the creative
   commons attribution 4.0 international license
   ([1228]http://creativecommons.org/licenses/by/4.0/), which permits
   unrestricted use, distribution, and reproduction in any medium,
   provided you give appropriate credit to the original author(s) and the
   source, provide a link to the creative commons license, and indicate if
   changes were made.

authors and affiliations

     * kezban dilek onal
          + 1
          + 2
       [1229]email author
     * ye zhang
          + 3
     * ismail sengor altingovde
          + 1
     * md mustafizur rahman
          + 3
     * pinar karagoz
          + 1
     * alex braylan
          + 3
     * brandon dang
          + 3
     * heng-lu chang
          + 3
     * henna kim
          + 3
     * quinten mcnamara
          + 3
     * aaron angert
          + 4
     * edward banner
          + 5
     * vivek khetan
          + 3
     * tyler mcdonnell
          + 3
     * an thanh nguyen
          + 3
     * dan xu
          + 3
     * byron c. wallace
          + 5
     * maarten de rijke
          + 2
     * matthew lease
          + 3

    1. 1.middle east technical universityankaraturkey
    2. 2.university of amsterdamamsterdamthe netherlands
    3. 3.university of texas at austinaustinusa
    4. 4.ibmnew yorkusa
    5. 5.college of computer and information sciencenortheastern
       universitybostonusa

about this article

   [1230]crossmark

   cite this article as:
          onal, k.d., zhang, y., altingovde, i.s. et al. inf retrieval j
          (2018) 21: 111. https://doi.org/10.1007/s10791-017-9321-y

     * received 06 november 2016
     * accepted 20 october 2017
     * first online 10 november 2017
     * doi https://doi.org/10.1007/s10791-017-9321-y
     * publisher name springer netherlands
     * print issn 1386-4564
     * online issn 1573-7659

     * [1231]about this journal
     * [1232]reprints and permissions

personalised recommendations

cite article

     * [1233]how to cite?
     * [1234].ris papers reference manager refworks zotero
     * [1235].enw endnote
     * [1236].bib bibtex jabref mendeley

   [1237]share article
   [1238]download pdf

actions

   [1239]download pdf

cite article

     * [1240]how to cite?
     * [1241].ris papers reference manager refworks zotero
     * [1242].enw endnote
     * [1243].bib bibtex jabref mendeley

   [1244]share article

table of contents

     * [1245]article
     * [1246]abstract
     * [1247]1 introduction
     * [1248]2 a brief history of neural ir
     * [1249]3 background
     * [1250]4 taxonomy
     * [1251]5 ad-hoc retrieval
     * [1252]6 query tasks
     * [1253]7 id53
     * [1254]8 sponsored search
     * [1255]9 similar item retrieval
     * [1256]10 non textual (or behavioural)
     * [1257]11 lessons and reflections
     * [1258]12 conclusion
     * [1259]footnotes
     * [1260]notes
     * [1261]appendix 1: acronyms used
     * [1262]appendix 2: resources
     * [1263]references
     * [1264]copyright information
     * [1265]authors and affiliations
     * [1266]about this article

   advertisement
   (button) hide

   over 10 million scientific documents at your fingertips

switch edition

     * [1267]academic edition
     * [1268]corporate edition

     * [1269]home
     * [1270]impressum
     * [1271]legal information
     * [1272]privacy statement
     * [1273]how we use cookies
     * [1274]accessibility
     * [1275]contact us

   [1276]springer nature

      2018 springer nature switzerland ag. part of [1277]springer nature.

   not logged in not affiliated 73.4.230.149

references

   visible links
   1. https://www.googletagmanager.com/ns.html?id=gtm-wcf9z9
   2. https://link.springer.com/article/10.1007/s10791-017-9321-y#main-content
   3. https://link.springer.com/article/10.1007/s10791-017-9321-y#article-contents
   4. http://activatejavascript.org/
   5. https://link.springer.com/article/10.1007/s10791-017-9321-y#search-container
   6. https://link.springer.com/
   7. https://link.springer.com/signup-login?previousurl=https://link.springer.com/article/10.1007/s10791-017-9321-y
   8. https://link.springer.com/journal/10791
   9. https://link.springer.com/content/pdf/10.1007/s10791-017-9321-y.pdf
  10. https://link.springer.com/journal/10791
  11. https://link.springer.com/journal/10791/21/2/page/1
  12. https://link.springer.com/article/10.1007/s10791-017-9321-y#citeas
  13. https://link.springer.com/article/10.1007/s10791-017-9321-y#authorsandaffiliations
  14. mailto:dilek@ceng.metu.edu.tr
  15. http://www.altmetric.com/details.php?citation_id=28914182&domain=link.springer.com
  16. https://citations.springer.com/item?doi=10.1007/s10791-017-9321-y
  17. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn1
  18. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr67
  19. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr111
  20. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr135
  21. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr154
  22. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr47
  23. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr129
  24. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec3
  25. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr190
  26. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr125
  27. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr126
  28. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr14
  29. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr17
  30. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr138
  31. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr161
  32. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr84
  33. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
  34. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr132
  35. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr112
  36. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr186
  37. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr122
  38. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr53
  39. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr14
  40. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr108
  41. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr17
  42. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr138
  43. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr161
  44. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr84
  45. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr59
  46. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr23
  47. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr67
  48. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr70
  49. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr100
  50. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr84
  51. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr95
  52. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr148
  53. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr46
  54. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec2
  55. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec3
  56. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec19
  57. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec25
  58. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec63
  59. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec75
  60. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec76
  61. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec87
  62. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec88
  63. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec89
  64. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec4
  65. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr105
  66. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr71
  67. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr162
  68. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr171
  69. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec5
  70. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec66
  71. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
  72. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr42
  73. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr138
  74. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr140
  75. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr121
  76. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr80
  77. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
  78. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr181
  79. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
  80. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec82
  81. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
  82. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr75
  83. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr95
  84. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
  85. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
  86. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
  87. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr76
  88. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
  89. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr174
  90. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
  91. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
  92. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
  93. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
  94. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
  95. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
  96. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr95
  97. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
  98. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr95
  99. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 100. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr182
 101. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr75
 102. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr76
 103. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn2
 104. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 105. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn3
 106. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr111
 107. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec19
 108. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec4
 109. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec5
 110. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec12
 111. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr7
 112. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr54
 113. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr71
 114. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr105
 115. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr172
 116. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr210
 117. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr16
 118. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr27
 119. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr85
 120. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr70
 121. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr198
 122. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr70
 123. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr38
 124. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr67
 125. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr111
 126. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr109
 127. http://deeplearning.net/
 128. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig1
 129. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr170
 130. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr71
 131. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig1_html.gif
 132. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr62
 133. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr86
 134. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr171
 135. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr178
 136. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr149
 137. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr104
 138. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig2
 139. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr99
 140. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr45
 141. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr93
 142. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr97
 143. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr219
 144. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr221
 145. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec82
 146. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr205
 147. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr79
 148. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr174
 149. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig2_html.gif
 150. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr61
 151. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig3
 152. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig3_html.gif
 153. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr160
 154. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr87
 155. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr73
 156. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr77
 157. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr10
 158. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr41
 159. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr10
 160. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr184
 161. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec10
 162. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr202
 163. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr74
 164. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr196
 165. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr83
 166. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr14
 167. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr122
 168. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr53
 169. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr132
 170. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr17
 171. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr44
 172. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr138
 173. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr140
 174. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec17
 175. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr14
 176. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr108
 177. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr142
 178. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr84
 179. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr84
 180. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr113
 181. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec23
 182. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ1
 183. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ1
 184. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ2
 185. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ3
 186. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr17
 187. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig4
 188. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig4
 189. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig4_html.gif
 190. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig4
 191. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr17
 192. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr138
 193. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr137
 194. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr44
 195. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr138
 196. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr17
 197. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr137
 198. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr138
 199. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig4
 200. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ4
 201. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ1
 202. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ2
 203. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr36
 204. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr147
 205. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr36
 206. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr18
 207. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr146
 208. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr44
 209. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ5
 210. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr146
 211. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr81
 212. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr138
 213. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig5
 214. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig5_html.gif
 215. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr147
 216. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr138
 217. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr140
 218. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr60
 219. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr140
 220. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ6
 221. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr161
 222. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr108
 223. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr108
 224. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr14
 225. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 226. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig6
 227. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig6_html.gif
 228. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig6
 229. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec24
 230. https://link.springer.com/article/10.1007/s10791-017-9321-y#tab1
 231. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr140
 232. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr161
 233. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr42
 234. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 235. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec7
 236. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig7
 237. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig7_html.gif
 238. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr112
 239. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ7
 240. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ7
 241. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ8
 242. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ9
 243. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec16
 244. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig8
 245. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr28
 246. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 247. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig8_html.gif
 248. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 249. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ10
 250. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ11
 251. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig9
 252. https://link.springer.com/article/10.1007/s10791-017-9321-y#equ8
 253. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig9_html.gif
 254. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr55
 255. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr84
 256. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 257. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr84
 258. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr98
 259. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 260. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec3
 261. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr76
 262. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr58
 263. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig10
 264. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr72
 265. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr39
 266. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr202
 267. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig10_html.gif
 268. https://link.springer.com/article/10.1007/s10791-017-9321-y#tab2
 269. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec25
 270. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec27
 271. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr26
 272. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr42
 273. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr66
 274. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr145
 275. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr150
 276. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
 277. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
 278. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr165
 279. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr168
 280. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 281. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec28
 282. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr79
 283. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 284. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr114
 285. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr152
 286. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr156
 287. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr157
 288. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr175
 289. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr176
 290. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr207
 291. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr1
 292. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr58
 293. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr116
 294. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec31
 295. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 296. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec34
 297. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr3
 298. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr5
 299. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr164
 300. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr169
 301. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 302. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 303. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 304. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec35
 305. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr80
 306. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr181
 307. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec39
 308. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr153
 309. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec40
 310. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr200
 311. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec42
 312. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr188
 313. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec44
 314. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr187
 315. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec45
 316. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec46
 317. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr29
 318. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 319. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr144
 320. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec50
 321. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr182
 322. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec51
 323. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 324. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec53
 325. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr123
 326. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec55
 327. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec57
 328. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr174
 329. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr183
 330. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr191
 331. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr203
 332. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr211
 333. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec59
 334. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr203
 335. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec60
 336. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec61
 337. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr8
 338. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr214
 339. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr215
 340. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec62
 341. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr75
 342. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr76
 343. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr217
 344. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec63
 345. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec65
 346. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr96
 347. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr101
 348. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec66
 349. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr171
 350. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr48
 351. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr58
 352. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 353. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec67
 354. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
 355. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec70
 356. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr223
 357. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec71
 358. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr107
 359. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr107
 360. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec73
 361. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr130
 362. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec74
 363. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr69
 364. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec75
 365. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr25
 366. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr217
 367. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec25
 368. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec63
 369. https://link.springer.com/article/10.1007/s10791-017-9321-y#tab2
 370. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec19
 371. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr42
 372. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr53
 373. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr90
 374. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr4
 375. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
 376. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr66
 377. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr145
 378. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr150
 379. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr150
 380. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr150
 381. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 382. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr53
 383. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr66
 384. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr66
 385. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr26
 386. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 387. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr21
 388. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr165
 389. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
 390. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
 391. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr193
 392. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr164
 393. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 394. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr21
 395. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr119
 396. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr193
 397. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr102
 398. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr209
 399. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr34
 400. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr168
 401. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 402. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 403. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr144
 404. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr175
 405. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr176
 406. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr207
 407. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr175
 408. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr156
 409. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr157
 410. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr152
 411. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 412. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr114
 413. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr207
 414. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr176
 415. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 416. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr176
 417. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr175
 418. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr104
 419. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 420. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 421. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 422. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr175
 423. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr176
 424. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 425. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 426. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 427. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr175
 428. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr175
 429. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr157
 430. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr120
 431. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr114
 432. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 433. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr176
 434. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr152
 435. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 436. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr63
 437. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr201
 438. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr207
 439. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr207
 440. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 441. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 442. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr176
 443. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr88
 444. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr88
 445. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr159
 446. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr95
 447. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr1
 448. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr2
 449. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr166
 450. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig11
 451. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr58
 452. https://link.springer.com/article/10.1007/s10791-017-9321-y#fig11
 453. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr58
 454. https://media.springernature.com/original/springer-static/image/art:10.1007/s10791-017-9321-y/mediaobjects/10791_2017_9321_fig11_html.gif
 455. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr1
 456. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr58
 457. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr76
 458. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr116
 459. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr155
 460. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 461. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr155
 462. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 463. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr3
 464. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr102
 465. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr169
 466. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr5
 467. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn4
 468. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr32
 469. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr37
 470. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr169
 471. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr144
 472. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr169
 473. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr164
 474. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr169
 475. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr21
 476. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 477. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 478. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr33
 479. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 480. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 481. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr102
 482. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr102
 483. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
 484. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr3
 485. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 486. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr161
 487. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr102
 488. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr80
 489. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr181
 490. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr11
 491. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr167
 492. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr49
 493. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr68
 494. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr195
 495. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr153
 496. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr224
 497. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr199
 498. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr200
 499. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr179
 500. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr188
 501. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr187
 502. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr29
 503. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 504. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr144
 505. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 506. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 507. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr141
 508. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 509. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 510. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr197
 511. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr13
 512. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr13
 513. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr31
 514. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr177
 515. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr29
 516. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr91
 517. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr144
 518. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr197
 519. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr13
 520. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr182
 521. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn5
 522. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 523. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec27
 524. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 525. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr123
 526. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr50
 527. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr192
 528. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr191
 529. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr64
 530. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr174
 531. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr211
 532. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr211
 533. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr211
 534. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr205
 535. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr174
 536. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr191
 537. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr205
 538. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr183
 539. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr203
 540. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr8
 541. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr214
 542. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr215
 543. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr10
 544. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr75
 545. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr75
 546. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr76
 547. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr75
 548. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr76
 549. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 550. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr101
 551. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr96
 552. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr171
 553. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr6
 554. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr51
 555. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr118
 556. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr58
 557. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec29
 558. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 559. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec18
 560. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr48
 561. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr22
 562. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 563. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
 564. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr42
 565. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr223
 566. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr194
 567. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr35
 568. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr223
 569. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr217
 570. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr42
 571. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
 572. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr223
 573. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr107
 574. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr130
 575. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr130
 576. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr130
 577. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr69
 578. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr25
 579. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr40
 580. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn6
 581. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn7
 582. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr24
 583. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr25
 584. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr217
 585. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn8
 586. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr115
 587. https://link.springer.com/article/10.1007/s10791-017-9321-y#tab2
 588. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
 589. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 590. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr150
 591. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 592. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 593. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 594. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 595. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr150
 596. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 597. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 598. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 599. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
 600. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 601. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 602. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 603. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr138
 604. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr140
 605. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr161
 606. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr151
 607. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr220
 608. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr133
 609. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 610. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr95
 611. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr161
 612. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr75
 613. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr76
 614. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr3
 615. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 616. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 617. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr145
 618. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr52
 619. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr130
 620. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
 621. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr205
 622. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr208
 623. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr223
 624. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr219
 625. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr220
 626. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr223
 627. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr52
 628. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr130
 629. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
 630. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 631. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 632. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 633. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 634. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 635. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 636. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 637. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr206
 638. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
 639. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr206
 640. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 641. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 642. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 643. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
 644. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 645. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr206
 646. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 647. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr97
 648. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec86
 649. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr46
 650. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr56
 651. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr218
 652. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr42
 653. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
 654. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr223
 655. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr90
 656. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr66
 657. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 658. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 659. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr66
 660. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr101
 661. https://link.springer.com/article/10.1007/s10791-017-9321-y#tab2
 662. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr200
 663. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr185
 664. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr98
 665. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr95
 666. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr134
 667. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn9
 668. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 669. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 670. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr123
 671. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr182
 672. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr116
 673. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 674. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr175
 675. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
 676. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr66
 677. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr175
 678. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr176
 679. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 680. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr144
 681. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr174
 682. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr183
 683. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr145
 684. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr205
 685. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 686. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr182
 687. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr180
 688. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr116
 689. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr203
 690. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr191
 691. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec83
 692. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr43
 693. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr43
 694. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
 695. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 696. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr43
 697. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
 698. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr103
 699. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr66
 700. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr191
 701. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr203
 702. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr200
 703. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 704. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr205
 705. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 706. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 707. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec56
 708. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 709. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 710. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr144
 711. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr175
 712. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr176
 713. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr207
 714. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 715. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr173
 716. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr10
 717. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr76
 718. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr30
 719. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr82
 720. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr116
 721. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr182
 722. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr163
 723. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr40
 724. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn10
 725. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr135
 726. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 727. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 728. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr92
 729. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr154
 730. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr128
 731. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr43
 732. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr89
 733. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 734. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr187
 735. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr188
 736. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr158
 737. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr46
 738. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 739. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 740. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr66
 741. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 742. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr209
 743. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr109
 744. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr109
 745. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr203
 746. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr124
 747. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr125
 748. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr152
 749. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr116
 750. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr117
 751. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr9
 752. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr20
 753. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 754. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr144
 755. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr182
 756. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr187
 757. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr188
 758. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr105
 759. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn1_source
 760. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn2_source
 761. http://research.microsoft.com/en-us/um/beijing/events/dl-wsdm-2015/
 762. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn3_source
 763. https://www.microsoft.com/en-us/research/event/neuir2016/
 764. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn4_source
 765. http://social-book-search.humanities.uva.nl/
 766. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn5_source
 767. http://www.kdd.org/kdd-cup/view/kdd-cup-2005
 768. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn6_source
 769. http://imat-relpred.yandex.ru/en/datasets
 770. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn7_source
 771. https://github.com/markovi/pyclick
 772. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn8_source
 773. http://data.computational-advertising.org/
 774. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn9_source
 775. https://github.com/mesnilgr/iclr15
 776. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn10_source
 777. https://en.wikipedia.org/wiki/rankbrain
 778. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn11_source
 779. https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
 780. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn12_source
 781. https://catalog.ldc.upenn.edu/ldc2011t07
 782. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn13_source
 783. http://nlp.stanford.edu/projects/glove/
 784. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn14_source
 785. https://www.microsoft.com/en-us/download/details.aspx?id=52597
 786. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn15_source
 787. http://www.zuccon.net/ntlm.html
 788. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn16_source
 789. http://nlp.stanford.edu/projects/glove/
 790. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn17_source
 791. https://bitbucket.org/omerlevy/hyperwords
 792. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn18_source
 793. https://radimrehurek.com/gensim/
 794. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn19_source
 795. https://lvdmaaten.github.io/tsne/
 796. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn24_source
 797. http://www.bigdatalab.ac.cn/benchmark/bm/bd?code=drmm(lch-idf)
 798. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn25_source
 799. https://github.com/sordonia/hred-qs
 800. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn26_source
 801. https://github.com/cvangysel/sert
 802. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn27_source
 803. https://github.com/aseveryn/deep-qa
 804. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn28_source
 805. https://ciir.cs.umass.edu/downloads/deepmerge/
 806. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn29_source
 807. http://www.cs.cmu.edu/~gzheng/code/termrecallkit-v2.tar.bz2
 808. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn30_source
 809. http://www.dsic.upv.es/~pgupta/mixed-script-ir
 810. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn31_source
 811. https://github.com/ielab/adcs2015-ntlm
 812. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn32_source
 813. https://github.com/yangliuy/anmm-cikm16
 814. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn33_source
 815. https://github.com/gdebasis/kderlm
 816. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn11
 817. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn12
 818. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr29
 819. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr94
 820. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr182
 821. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn13
 822. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr145
 823. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn14
 824. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn15
 825. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 826. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr161
 827. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn16
 828. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr108
 829. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn17
 830. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn18
 831. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr127
 832. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn19
 833. https://link.springer.com/article/10.1007/s10791-017-9321-y#tab3
 834. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr171
 835. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr131
 836. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr187
 837. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr94
 838. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 839. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr144
 840. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr182
 841. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr143
 842. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr144
 843. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr145
 844. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr150
 845. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr42
 846. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr5
 847. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr3
 848. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr165
 849. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr94
 850. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr95
 851. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr52
 852. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr12
 853. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr171
 854. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr183
 855. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr26
 856. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
 857. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr42
 858. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 859. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 860. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 861. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 862. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr165
 863. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
 864. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr116
 865. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr164
 866. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr169
 867. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr165
 868. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr168
 869. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr168
 870. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 871. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr164
 872. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr165
 873. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr204
 874. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr200
 875. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr76
 876. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr181
 877. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr174
 878. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
 879. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr174
 880. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr130
 881. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 882. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr181
 883. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
 884. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 885. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 886. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr188
 887. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr204
 888. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 889. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 890. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 891. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr52
 892. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 893. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr205
 894. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr191
 895. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr174
 896. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr211
 897. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr43
 898. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr65
 899. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr169
 900. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr168
 901. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr42
 902. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr57
 903. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 904. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr212
 905. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr213
 906. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 907. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 908. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr169
 909. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 910. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr26
 911. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr223
 912. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr203
 913. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr203
 914. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
 915. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr223
 916. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr203
 917. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr203
 918. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
 919. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr216
 920. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr189
 921. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr80
 922. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr115
 923. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr217
 924. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr19
 925. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr188
 926. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr25
 927. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr15
 928. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr78
 929. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn24
 930. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr182
 931. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn25
 932. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr187
 933. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr188
 934. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn26
 935. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr174
 936. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn27
 937. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr106
 938. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn28
 939. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr222
 940. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn29
 941. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr80
 942. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn30
 943. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr225
 944. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn31
 945. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr205
 946. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn32
 947. https://link.springer.com/article/10.1007/s10791-017-9321-y#cr168
 948. https://link.springer.com/article/10.1007/s10791-017-9321-y#fn33
 949. https://scholar.google.com/scholar?q=ai, q., yang, l., guo, j., & croft, w. b. (2016a). analysis of the paragraph vector model for information retrieval. in proceedings of the 2016 acm international conference on the theory of information retrieval, acm, new york, ny, usa, ictir   16 (pp. 133   142).
 950. https://scholar.google.com/scholar?q=ai, q., yang, l., guo, j., & croft, w. b. (2016b). improving language estimation with the paragraph vector model for ad-hoc retrieval. in proceedings of the 39th international acm sigir conference on research and development in information retrieval, acm, new york, ny, usa, sigir   16 (pp. 869   872).
 951. https://scholar.google.com/scholar?q=almasri, m., berrut, c., & chevallet, j. p. (2016). a comparison of deep learning based id183 with pseudo-relevance feedback and mutual information. in european conference on information retrieval (pp. 709   715). springer.
 952. https://doi.org/10.1145/582415.582416
 953. http://scholar.google.com/scholar_lookup?title=probabilistic models of information retrieval based on measuring the divergence from randomness&author=g. amati&author=cj. rijsbergen&journal=acm transactions on information systems (tois)&volume=20&issue=4&pages=357-389&publication_year=2002
 954. https://scholar.google.com/scholar?q=amer, n. o., mulhem, p., & gery, m. (2016). toward id27 for personalized information retrieval. in acm sigir workshop on neural information retrieval (neu-ir).
 955. https://scholar.google.com/scholar?q=andoni, a., & indyk, p. (2006). near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. in 2006 47th annual ieee symposium on foundations of computer science (focs   06) (pp. 459   468). ieee.
 956. https://doi.org/10.1109/mci.2010.938364
 957. http://scholar.google.com/scholar_lookup?title=deep machine learning   a new frontier in artificial intelligence research&author=i. arel&author=dc. rose&author=tp. karnowski&journal=ieee computational intelligence magazine&volume=5&issue=4&pages=13-18&publication_year=2010
 958. https://doi.org/10.1145/2740908.2742739
 959. https://scholar.google.com/scholar?q=azzopardi, l., de rijke, m., & balog, k. (2007). building simulated queries for known-item topics: an analysis using six european languages. in 30th annual international acm sigir conference on research & development on information retrieval, acm.
 960. http://arxiv.org/abs/14090473
 961. https://scholar.google.com/scholar?q=bai, b., weston, j., grangier, d., collobert, r., sadamasa, k., qi, y., et al. (2009). supervised semantic indexing. in proceedings of the 18th acm conference on information and knowledge management (pp 187   196). acm.
 962. https://scholar.google.com/scholar?q=balikas, g., & amini, m.r. (2016). an empirical study on large scale text classification with skip-gram embeddings. in acm sigir workshop on neural information retrieval (neu-ir).
 963. https://scholar.google.com/scholar?q=bar-yossef, z., & kraus, n. (2011). context-sensitive query auto-completion. in proceedings of the 20th international conference on world wide web (pp 107   116). acm.
 964. https://scholar.google.com/scholar?q=baroni, m., dinu, g., & kruszewski, g. (2014). don   t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers) (pp. 238   247). association for computational linguistics.
 965. http://arxiv.org/abs/150205767
 966. https://doi.org/10.1561/2200000006
 967. http://www.emis.de/math-item?1192.68503
 968. http://scholar.google.com/scholar_lookup?title=learning deep architectures for ai&author=y. bengio&journal=foundations and trends in machine learning&volume=2&issue=1&pages=1-127&publication_year=2009
 969. http://www.emis.de/math-item?1061.68157
 970. http://scholar.google.com/scholar_lookup?title=a neural probabilistic language model&author=y. bengio&author=r. ducharme&author=p. vincent&author=c. janvin&journal=journal of machine learning research&volume=3&pages=1137-1155&publication_year=2003
 971. https://scholar.google.com/scholar?q=bengio, y., & sen  cal, j. s. (2003b). quick training of probabilistic neural nets by importance sampling. in aistats.
 972. https://doi.org/10.1002/asi.22908
 973. http://scholar.google.com/scholar_lookup?title=on the assessment of expertise profiles&author=r. berendsen&author=k. balog&author=t. bogers&author=a. bosch&author=m. rijke&journal=journal of the american society for information science and technology&volume=64&issue=10&pages=2024-2044&publication_year=2013
 974. https://scholar.google.com/scholar?q=berendsen, r., tsagkias, m., weerkamp, w., & de  rijke, m. (2013b). pseudo test collections for training and tuning microblog rankers. in sigir   13: 36th international acm sigir conference on research and development in information retrieval. acm.
 975. https://scholar.google.com/scholar?q=berger, a., & lafferty, j. (1999). information retrieval as statistical translation. in proceedings of the 22nd annual international acm sigir conference on research and development in information retrieval (pp. 222   229). acm.
 976. http://www.emis.de/math-item?1112.68379
 977. http://scholar.google.com/scholar_lookup?title=id44&author=dm. blei&author=ay. ng&author=mi. jordan&journal=journal of machine learning research&volume=3&issue=jan&pages=993-1022&publication_year=2003
 978. https://scholar.google.com/scholar?q=bordes, a., chopra, s., & weston, j. (2014). id53 with subgraph embeddings. in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp) (pp. 615   620). association for computational linguistics.
 979. https://scholar.google.com/scholar?q=borisov, a., markov, i., de  rijke, m., & serdyukov, p. (2016a). a context-aware time model for web search. in sigir 2016: 39th international acm sigir conference on research and development in information retrieval (pp. 205   214). acm.
 980. https://scholar.google.com/scholar?q=borisov, a., markov, i., de  rijke, m., & serdyukov, p. (2016b). a neural click model for web search. in proceedings of the 25th international conference on world wide web, international world wide web conferences steering committee (pp. 531   541).
 981. https://scholar.google.com/scholar?q=boytsov, l., novak, d., malkov, y., & nyberg, e. (2016). off the beaten path: let   s replace term-based retrieval with id92 search. in proceedings of the 25th acm international on conference on information and knowledge management (pp. 1099   1108). acm.
 982. https://scholar.google.com/scholar?q=broder, a., domingos, p., de  freitas, n., guyon, i., malik, j., & neville, j. (2016). is deep learning the new 42? in plenary panel at the 22nd acm sigkdd international conference on knowledge discovery and data mining.
 983. https://doi.org/10.1142/s0218001493000339
 984. http://scholar.google.com/scholar_lookup?title=signature verification using a siamese time delay neural network&author=j. broid113y&author=jw. bentz&author=l. bottou&author=i. guyon&author=y. lecun&author=c. moore&author=e. s  ckinger&author=r. shah&journal=international journal of pattern recognition and artificial intelligence&volume=7&issue=04&pages=669-688&publication_year=1993
 985. https://doi.org/10.1016/j.ipm.2015.12.008
 986. http://scholar.google.com/scholar_lookup?title=learning from homologous queries and semantically related terms for query auto completion&author=f. cai&author=m. rijke&journal=information processing & management&volume=52&issue=4&pages=628-643&publication_year=2016
 987. https://doi.org/10.1561/1500000055
 988. http://scholar.google.com/scholar_lookup?title=a survey of query auto completion in information retrieval&author=f. cai&author=m. rijke&journal=foundations and trends in information retrieval&volume=10&issue=4&pages=273-363&publication_year=2016
 989. https://scholar.google.com/scholar?q=cai, f., liang, s., & de  rijke, m. (2014). time-sensitive personalized query auto-completion. in proceedings of the 23rd acm international conference on conference on information and knowledge management (pp. 1599   1608). acm.
 990. https://scholar.google.com/scholar?q=carmel, d., zwerdling, n., guy, i., ofek-koifman, s., har   el, n., ronen, i., et al. (2009). personalized social search based on the user   s social network. in proceedings of the 18th acm conference on information and knowledge management (pp. 1227   1236). acm.
 991. https://doi.org/10.1145/2071389.2071390
 992. http://www.emis.de/math-item?1293.68103
 993. http://scholar.google.com/scholar_lookup?title=a survey of automatic id183 in information retrieval&author=c. carpineto&author=g. romano&journal=acm computing surveys&volume=44&issue=1&pages=1:1-1:50&publication_year=2012
 994. https://scholar.google.com/scholar?q=cartright, m. a., allan, j., lavrenko, v., & mcgregor, a. (2010). fast id183 using approximations of relevance models. in proceedings of the 19th acm international conference on information and knowledge management (pp. 1573   1576). acm.
 995. https://scholar.google.com/scholar?q=charikar, m. s. (2002). similarity estimation techniques from rounding algorithms. in proceedings of the thirty-fourth annual acm symposium on theory of computing (pp. 380   388). acm.
 996. https://scholar.google.com/scholar?q=chen, w., grangier, d., & auli, m. (2016). strategies for training large vocabulary neural language models. in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers) (pp. 1975   1985). association for computational linguistics, berlin, germany.
 997. https://scholar.google.com/scholar?q=chirita, p. a., firan, c. s., & nejdl, w. (2007). personalized id183 for the web. in proceedings of the 30th annual international acm sigir conference on research and development in information retrieval (pp. 7   14). acm.
 998. http://arxiv.org/abs/151107916
 999. http://arxiv.org/abs/1406.1078
1000. https://scholar.google.com/scholar?q=chuklin, a., markov, i., & de  rijke m. (2015). click models for web search. synthesis lectures on information concepts, retrieval, and services. morgan & claypool publishers.
1001. https://scholar.google.com/scholar?q=chung, j., gulcehre, c., cho, k., & bengio, y. (2014). empirical evaluation of gated recurrent neural networks on sequence modeling. in nips workshop on deep learning.
1002. https://scholar.google.com/scholar?q=clinchant, s., & perronnin, f. (2013). aggregating continuous id27s for information retrieval. in proceedings of the workshop on continuous vector space models and their compositionality (pp. 100   109).
1003. https://scholar.google.com/scholar?q=cohen, d., ai, q., & croft, w. b. (2016). adaptability of neural networks on varying granularity ir tasks. in acm sigir workshop on neural information retrieval (neu-ir).
1004. https://scholar.google.com/scholar?q=collobert, r., & weston, j. (2008). a unified architecture for natural language processing: deep neural networks with multitask learning. in proceedings of the 25th international conference on machine learning (pp. 160   167). acm.
1005. http://www.emis.de/math-item?1280.68161
1006. http://scholar.google.com/scholar_lookup?title=natural language processing (almost) from scratch&author=r. collobert&author=j. weston&author=l. bottou&author=m. karlen&author=k. kavukcuoglu&author=p. kuksa&journal=journal of machine learning research&volume=12&issue=aug&pages=2493-2537&publication_year=2011
1007. http://arxiv.org/abs/160601781
1008. http://scholar.google.com/scholar_lookup?title=search engines: information retrieval in practice&author=b. croft&author=d. metzler&author=t. strohman&publication_year=2009
1009. https://scholar.google.com/scholar?q=dai, a. m., olah, c., le, q. v., & corrado, g. s. (2014). document embedding with paragraph vectors. in nips deep learning workshop.
1010. https://scholar.google.com/scholar?q=dang, v., & croft, b. w. (2010). query reformulation using anchor text. in proceedings of the third acm international conference on web search and data mining (pp. 41   50). acm.
1011. https://doi.org/10.1109/2.60879
1012. http://scholar.google.com/scholar_lookup?title=the reactive keyboard: a predictive typing aid&author=jj. darragh&author=ih. witten&author=ml. james&journal=computer&volume=23&issue=11&pages=41-49&publication_year=1990
1013. https://scholar.google.com/scholar?q=datar, m., immorlica, n., indyk, p., & mirrokni, v. s. (2004). locality-sensitive hashing scheme based on p-stable distributions. in proceedings of the twentieth annual symposium on computational geometry (pp. 253   262). acm.
1014. https://scholar.google.com/scholar?q=de  vine, l., zuccon, g., koopman, b., sitbon, l., & bruza, p. (2014). medical semantic similarity with a neural language model. in proceedings of the 23rd acm international conference on conference on information and knowledge management (pp. 1819   1822). acm.
1015. https://doi.org/10.1002/(sici)1097-4571(199009)41:6<391::aid-asi1>3.0.co;2-9
1016. http://scholar.google.com/scholar_lookup?title=indexing by latent semantic analysis&author=s. deerwester&author=st. dumais&author=gw. furnas&author=tk. landauer&author=r. harshman&journal=journal of the american society for information science&volume=41&issue=6&pages=391&publication_year=1990
1017. https://doi.org/10.1017/atsip.2013.9
1018. http://scholar.google.com/scholar_lookup?title=a tutorial survey of architectures, algorithms, and applications for deep learning&author=l. deng&journal=apsipa transactions on signal and information processing&volume=3&pages=e2&publication_year=2014
1019. http://www.ams.org/mathscinet-getitem?mr=3295556
1020. http://www.emis.de/math-item?1315.68208
1021. http://scholar.google.com/scholar_lookup?title=deep learning: methods and applications&author=l. deng&author=d. yu&journal=foundations and trends in signal processing&volume=7&issue=3   4&pages=197-387&publication_year=2013
1022. https://scholar.google.com/scholar?q=dhingra, b., zhou, z., fitzpatrick, d., muehl, m., & cohen, w. (2016). tweet2vec: character-based distributed representations for social media. in proceedings of the 54th annual meeting of the association for computational linguistics (volume 2: short papers) (pp. 269   274), association for computational linguistics.
1023. https://scholar.google.com/scholar?q=diaz, f., mitra, b., & craswell, n. (2016). id183 with locally-trained id27s. in 54th annual meeting of the association for computational linguistics (volume 1: long papers) (pp. 367   377), association for computational linguistics, berlin, germany.
1024. https://scholar.google.com/scholar?q=djuric, n., wu, h., radosavljevic, v., grbovic, m., & bhamidipati, n. (2015). hierarchical neural language models for joint representation of streaming documents and their content. in proceedings of the 24th international conference on world wide web, acm, new york, ny, usa, www   15 (pp. 248   255).
1025. https://scholar.google.com/scholar?q=dumais, s., banko, m., brill, e., lin, j., & ng, a. (2002). web id53: is more always better? in proceedings of the 25th annual international acm sigir conference on research and development in information retrieval (pp. 291   298). acm.
1026. http://arxiv.org/abs/1410.8251
1027. https://doi.org/10.1207/s15516709cog1402_1
1028. http://scholar.google.com/scholar_lookup?title=finding structure in time&author=jl. elman&journal=cognitive science&volume=14&issue=2&pages=179-211&publication_year=1990
1029. http://www.ams.org/mathscinet-getitem?mr=2600623
1030. http://www.emis.de/math-item?1242.68219
1031. http://scholar.google.com/scholar_lookup?title=why does unsupervised pre-training help deep learning?&author=d. erhan&author=y. bengio&author=a. courville&author=pa. manzagol&author=p. vincent&author=s. bengio&journal=journal of machine learning research&volume=11&issue=feb&pages=625-660&publication_year=2010
1032. http://arxiv.org/abs/14114166
1033. http://www.ams.org/mathscinet-getitem?mr=1873328
1034. https://doi.org/10.1214/aos/1013203451
1035. http://www.emis.de/math-item?1043.62034
1036. http://scholar.google.com/scholar_lookup?title=greedy function approximation: a gradient boosting machine&author=jh. friedman&journal=annals of statistics&volume=29&issue=5&pages=1189-1232&publication_year=2001
1037. https://scholar.google.com/scholar?q=ganguly, d., roy, d., mitra, m., & jones, g. j. (2015). id27 based generalized language model for information retrieval. in proceedings of the 38th international acm sigir conference on research and development in information retrieval (pp. 795   798). acm.
1038. https://scholar.google.com/scholar?q=ganguly, d., roy, d., mitra, m., & jones, g. (2016). representing documents and queries as sets of word embedded vectors for information retrieval. in acm sigir workshop on neural information retrieval (neu-ir).
1039. https://scholar.google.com/scholar?q=gao, j., he, x., & li, d. (2015). deep learning for web search and natural language processing. microsoft research technical report. redmond, wa: microsoft corporation
1040. https://scholar.google.com/scholar?q=gao, j., he, x., & nie, j. y. (2010). clickthrough-based translation models for web search: from word models to phrase models. in proceedings of the 19th acm international conference on information and knowledge management (cikm) (pp. 1139   1148). acm.
1041. https://scholar.google.com/scholar?q=gao, j., pantel, p., gamon, m., he, x., & deng, l. (2014). modeling interestingness with deep neural networks. in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), association for computational linguistics, doha, qatar (pp. 2   13).
1042. http://www.ams.org/mathscinet-getitem?mr=3584073
1043. http://www.emis.de/math-item?06673551
1044. http://scholar.google.com/scholar_lookup?title=a primer on neural network models for natural language processing&author=y. goldberg&journal=journal of artificial intelligence research&volume=57&pages=345-420&publication_year=2016
1045. http://www.emis.de/math-item?1373.68009
1046. http://scholar.google.com/scholar_lookup?title=deep learning&author=i. goodfellow&author=y. bengio&author=a. courville&publication_year=2016
1047. https://scholar.google.com/scholar?q=graves, a. (2012). neural networks. in supervised sequence labelling with recurrent neural networks (pp. 15   35). springer.
1048. http://arxiv.org/abs/13080850
1049. http://arxiv.org/abs/14105401
1050. https://scholar.google.com/scholar?q=grbovic, m., djuric, n., radosavljevic, v., & bhamidipati, n. (2015a). search retargeting using directed query embeddings. in proceedings of the 24th international conference on world wide web, acm, new york, ny, usa, www   15 companion (pp. 37   38).
1051. https://scholar.google.com/scholar?q=grbovic, m., djuric, n., radosavljevic, v., silvestri, f., & bhamidipati, n. (2015b). context-and content-aware embeddings for query rewriting in sponsored search. in proceedings of the 38th international acm sigir conference on research and development in information retrieval (pp. 383   392). acm.
1052. http://arxiv.org/abs/150304069
1053. https://scholar.google.com/scholar?q=guo, j., fan, y., ai, q., & croft, w. b. (2016a). a deep relevance matching model for ad-hoc retrieval. in the 25th acm international conference on information and knowledge management, indianapolis, united states.
1054. https://scholar.google.com/scholar?q=guo, j., fan, y., ai, q., & croft, w. b. (2016b). a deep relevance matching model for ad-hoc retrieval. in cikm 2016: 25th acm conference on information and knowledge management, acm.
1055. https://scholar.google.com/scholar?q=gupta, p., bali, k., banchs, r. e., choudhury, m., & rosso, p. (2014). id183 for mixed-script information retrieval. in proceedings of the 37th international acm sigir conference on research & development in information retrieval (pp. 677   686). acm.
1056. http://www.ams.org/mathscinet-getitem?mr=2913702
1057. http://www.emis.de/math-item?1283.62064
1058. http://scholar.google.com/scholar_lookup?title=noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics&author=mu. gutmann&author=a. hyv  rinen&journal=journal of machine learning research&volume=13&issue=1&pages=307-361&publication_year=2012
1059. https://doi.org/10.1007/s10791-009-9101-4
1060. http://scholar.google.com/scholar_lookup?title=overview of the reliable information access workshop&author=d. harman&author=c. buckley&journal=information retrieval&volume=12&issue=6&pages=615-641&publication_year=2009
1061. https://doi.org/10.1080/00437956.1954.11659520
1062. http://scholar.google.com/scholar_lookup?title=distributional structure&author=zs. harris&journal=word&volume=10&issue=2   3&pages=146-162&publication_year=1954
1063. https://scholar.google.com/scholar?q=hill, f., cho, k., & korhonen, a. (2016). learning distributed representations of sentences from unlabelled data. in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, association for computational linguistics, san diego, california, (pp. 1367   1377).
1064. https://scholar.google.com/scholar?q=hinton, g., deng, l., yu, d., dahl, g. e., mohamed, ar., jaitly, n., et al. (2012). deep neural networks for acoustic modeling in id103: the shared views of four research groups. ieee signal processing magazine, 29(6), 82   97.
1065. http://www.ams.org/mathscinet-getitem?mr=2242509
1066. https://doi.org/10.1126/science.1127647
1067. http://www.emis.de/math-item?1226.68083
1068. http://scholar.google.com/scholar_lookup?title=reducing the dimensionality of data with neural networks&author=ge. hinton&author=rr. salakhutdinov&journal=science&volume=313&issue=5786&pages=504-507&publication_year=2006
1069. https://doi.org/10.1162/neco.1997.9.8.1735
1070. http://scholar.google.com/scholar_lookup?title=long short-term memory&author=s. hochreiter&author=j. schmidhuber&journal=neural computation&volume=9&issue=8&pages=1735-1780&publication_year=1997
1071. https://scholar.google.com/scholar?q=hu, b., lu, z., li, h., & chen, q. (2014). convolutional neural network architectures for matching natural language sentences. in advances in neural information processing systems (pp. 2042   2050).
1072. https://scholar.google.com/scholar?q=huang, p. s., he, x., gao, j., deng, l., acero, a., & heck, l. (2013). learning deep structured semantic models for web search using clickthrough data. in proceedings of the 22nd acm international conference on information & knowledge management (pp. 2333   2338). acm.
1073. https://scholar.google.com/scholar?q=jaakkola, t. s., & haussler, d. (1999). exploiting generative models in discriminative classifiers. advances in neural information processing systems (pp. 487   493).
1074. https://scholar.google.com/scholar?q=jiang, j. y., ke, y. y., chien, p. y., & cheng, p. j. (2014). learning user reformulation behavior for query auto-completion. in proceedings of the 37th international acm sigir conference on research & development in information retrieval (pp. 445   454). acm.
1075. https://scholar.google.com/scholar?q=jurgovsky, j., granitzer, m., & seifert, c. (2016) evaluating memory efficiency and robustness of id27s. in european conference on information retrieval (pp. 200   211). springer.
1076. http://arxiv.org/abs/14042188
1077. https://scholar.google.com/scholar?q=kanhabua, n., ren, h., & moeslund, t. b. (2016). learning dynamic classes of events using stacked multilayer id88 networks. in acm sigir workshop on neural information retrieval (neu-ir).
1078. https://scholar.google.com/scholar?q=kenter, t., & de  rijke, m. (2015). short text similarity with id27s. in proceedings of the 24th acm international on conference on information and knowledge management (pp. 1411   1420). acm.
1079. http://arxiv.org/abs/160801972
1080. http://arxiv.org/abs/14085882
1081. https://scholar.google.com/scholar?q=kiros, r., zhu, y., salakhutdinov, r. r., zemel, r., urtasun, r., torralba, a., et al. (2015). skip-thought vectors. in advances in neural information processing systems (pp. 3294   3302).
1082. https://scholar.google.com/scholar?q=krizhevsky, a., sutskever, i., & hinton, g. e. (2012). id163 classification with deep convolutional neural networks. in advances in neural information processing systems (pp. 1097   1105).
1083. http://arxiv.org/abs/150607285
1084. https://scholar.google.com/scholar?q=kusner, m. j., sun, y., kolkin, n. i., & weinberger, k. q. (2015). from id27s to document distances. in proceedings of the 32nd international conference on machine learning (icml 2015) (pp. 957   966).
1085. https://scholar.google.com/scholar?q=lavrenko, v., & croft, w. b. (2001). relevance based language models. in proceedings of the 24th annual international acm sigir conference on research and development in information retrieval, acm, new york, ny, usa, sigir   01 (pp. 120   127).
1086. https://scholar.google.com/scholar?q=le, q. v., & mikolov, t. (2014). distributed representations of sentences and documents. in icml (vol. 14, pp. 1188   1196).
1087. http://scholar.google.com/scholar_lookup?title=convolutional networks for images, speech, and time series&author=y. lecun&author=y. bengio&journal=the handbook of brain theory and neural networks&volume=3361&issue=10&pages=1995&publication_year=1995
1088. https://doi.org/10.1038/nature14539
1089. http://scholar.google.com/scholar_lookup?title=deep learning&author=y. lecun&author=y. bengio&author=g. hinton&journal=nature&volume=521&issue=7553&pages=436-444&publication_year=2015
1090. https://scholar.google.com/scholar?q=lee, c. j., ai, q., croft, w. b., & sheldon, d. (2015). an optimization framework for merging multiple result lists. in proceedings of the 24th acm international on conference on information and knowledge management (pp. 303   312). acm.
1091. https://scholar.google.com/scholar?q=lei, t., joshi, h., barzilay, r., jaakkola, t. s., tymoshenko, k., moschitti, a., et al. (2016). semi-supervised question retrieval with gated convolutions. in naacl hlt 2016, the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, san diego california, usa, june 12   17, 2016 (pp. 1279   1289).
1092. http://scholar.google.com/scholar_lookup?title=improving distributional similarity with lessons learned from id27s&author=o. levy&author=y. goldberg&author=i. dagan&journal=transactions of the association for computational linguistics&volume=3&pages=211-225&publication_year=2015
1093. https://scholar.google.com/scholar?q=li, h. (2016, july). does ir need deep learning? in keynote speech at sigir 2016 neu-ir workshop, pisa.
1094. https://scholar.google.com/scholar?q=li, h., & lu, z. (2016). deep learning for information retrieval. in sigir, pisa, italy, vol tutorial at the 39th international acm sigir conference on research and development in information retrieval (pp. 1203   1206).
1095. https://doi.org/10.1561/1500000035
1096. http://scholar.google.com/scholar_lookup?title=semantic matching in search&author=h. li&author=j. xu&journal=foundations and trends in information retrieval&volume=7&issue=5&pages=343-469&publication_year=2013
1097. http://arxiv.org/abs/150300185
1098. https://scholar.google.com/scholar?q=li, x., guo, c., chu, w., wang, y.-y., & shavlik, j. (2014). deep learning powered in-session contextual ranking using clickthrough data. in workshop on personalization: methods and applications, neural information processing systems (nips).
1099. https://scholar.google.com/scholar?q=liao, h., peng, l., liu, z., & shen, x. (2014). ipinyou global rtb bidding algorithm competition dataset. in proceedings of the eighth international workshop on data mining for online advertising (pp. 1   6). acm.
1100. https://scholar.google.com/scholar?q=lioma, c., larsen, b., petersen, c., & simonsen, j. g. (2016). deep learning relevance: creating relevant information (as opposed to retrieving it). in acm sigir workshop on neural information retrieval (neu-ir).
1101. https://doi.org/10.1561/1500000016
1102. http://scholar.google.com/scholar_lookup?title=learning to rank for information retrieval&author=ty. liu&journal=foundations and trends in information retrieval&volume=3&issue=3&pages=225-331&publication_year=2009
1103. https://scholar.google.com/scholar?q=liu, w., wang, j., ji, r., jiang, y. g., & chang, s. f. (2012). supervised hashing with kernels. in 2012 ieee conference on id161 and pattern recognition (cvpr) (pp. 2074   2081). ieee.
1104. https://scholar.google.com/scholar?q=liu, x., & croft, w. b. (2004). cluster-based retrieval using language models. in proceedings of the 27th annual international acm sigir conference on research and development in information retrieval (pp. 186   193). acm.
1105. https://scholar.google.com/scholar?q=liu, x., gao, j., he, x., deng, l., duh, k., & wang, y. y. (2015). representation learning using multi-task deep neural networks for semantic classification and information retrieval. in proceedings of the 2015 conference of the north american chapter of the association for computational linguistics: human language technologies, association for computational linguistics, denver, colorado (pp. 912   921).
1106. https://scholar.google.com/scholar?q=lu, z., & li, h. (2013). a deep architecture for matching short texts. in advances in neural information processing systems (pp. 1367   1375).
1107. https://doi.org/10.3758/bf03204766
1108. http://scholar.google.com/scholar_lookup?title=producing high-dimensional semantic spaces from lexical co-occurrence&author=k. lund&author=c. burgess&journal=behavior research methods, instruments, & computers&volume=28&issue=2&pages=203-208&publication_year=1996
1109. https://scholar.google.com/scholar?q=luukkonen, p., koskela, m., & floreen, p. (2016). lstm-based predictions for proactive information retrieval. in acm sigir workshop on neural information retrieval (neu-ir).
1110. http://arxiv.org/abs/150600333
1111. https://scholar.google.com/scholar?q=ma, l., lu, z., shang, l., & li, h. (2015b). multimodal convolutional neural networks for matching image and sentence. in proceedings of the ieee international conference on id161 (pp. 2623   2631).
1112. https://scholar.google.com/scholar?q=ma, l., lu, z., & li, h. (2016). learning to answer questions from image using convolutional neural network. in aaai conference on artificial intelligence (pp. 3567   3573).
1113. https://scholar.google.com/scholar?q=manning, c. (2016). understanding human language: can nlp and deep learning help? in proceedings of the 39th international acm sigir conference on research and development in information retrieval, sigir    16, pisa, italy. new york, ny: acm.
1114. https://doi.org/10.1017/cbo9780511809071
1115. http://www.emis.de/math-item?1160.68008
1116. http://scholar.google.com/scholar_lookup?title=introduction to information retrieval&author=cd. manning&author=p. raghavan&author=h. sch  tze&publication_year=2008
1117. https://scholar.google.com/scholar?q=manotumruksa, j., macdonald, c., & ounis, i. (2016). modelling user preferences using id27s for context-aware venue recommendation. in acm sigir workshop on neural information retrieval (neu-ir).
1118. https://scholar.google.com/scholar?q=mcauley, j., pandey, r., & leskovec, j. (2015). inferring networks of substitutable and complementary products. in kdd: acm (pp. 785   794).
1119. https://scholar.google.com/scholar?q=mcclelland, j. l., rumelhart, d. e., pdp research group. (1986). parallel distributed processing. explorations in the microstructure of cognition (vol. 2, pp. 216   271).
1120. https://scholar.google.com/scholar?q=mesnil, g., he, x., deng, l., & bengio, y. (2013). investigation of recurrent-neural-network architectures and learning methods for spoken language understanding. in interspeech (pp. 3771   3775).
1121. https://scholar.google.com/scholar?q=mesnil, g., mikolov, t., ranzato, m., & bengio, y. (2014). ensemble of generative and discriminative techniques for id31 of movie reviews. in international conference on learning representations (iclr).
1122. https://scholar.google.com/scholar?q=metz, c. (2016). ai is transforming google search. the rest of the web is next. wired magazine.
1123. https://scholar.google.com/scholar?q=mikolov, t., karafi  t, m., burget, l., cernock   , j., & khudanpur, s. (2010). recurrent neural network based language model. in interspeech (pp. 1045   1048).
1124. http://arxiv.org/abs/1301.3781
1125. https://scholar.google.com/scholar?q=mikolov, t., sutskever, i., chen, k., corrado, g. s., & dean, j. (2013b). distributed representations of words and phrases and their compositionality. in c. j. c. burges, l. bottou, m. welling, z. ghahramani, & k. q. weinberger (eds.), advances in neural information processing systems (vol. 26, pp. 3111   3119). curran associates, inc.
1126. https://scholar.google.com/scholar?q=mikolov, t., yih, w., & zweig, g. (2013c). linguistic regularities in continuous space word representations. in proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: human language technologies, association for computational linguistics, atlanta, georgia, (pp. 746   751).
1127. https://doi.org/10.1111/j.1551-6709.2010.01106.x
1128. https://doi.org/10.1111/j.1551-6709.2010.01106.x
1129. http://scholar.google.com/scholar_lookup?title=composition in distributional models of semantics&author=j. mitchell&author=m. lapata&journal=cognitive science&volume=34&issue=8&pages=1388-1429&publication_year=2010&doi=10.1111/j.1551-6709.2010.01106.x
1130. https://scholar.google.com/scholar?q=mitra, b. (2015). exploring session context using distributed representations of queries and reformulations. in proceedings of the 38th international acm sigir conference on research and development in information retrieval (pp. 3   12). acm.
1131. https://scholar.google.com/scholar?q=mitra, b., & craswell, n. (2015). query auto-completion for rare prefixes. in proceedings of the 24th acm international on conference on information and knowledge management (pp. 1755   1758). acm.
1132. http://arxiv.org/abs/160201137
1133. http://arxiv.org/abs/12066426
1134. https://scholar.google.com/scholar?q=morin, f., & bengio, y. (2005). hierarchical probabilistic neural network language model. in proceedings of the tenth international workshop on artificial intelligence and statistics, aistats 2005, bridgetown, barbados, january 6   8, 2005.
1135. https://scholar.google.com/scholar?q=moshfeghi, y., triantafillou, p., & pollick, f. e. (2016). understanding information need: an fmri study. in proceedings of the 39th international acm sigir conference on research and development in information retrieval, acm, new york, ny, usa, sigir   16 (pp. 335   344).
1136. https://scholar.google.com/scholar?q=nair, v., & hinton, g. e. (2010). rectified linear units improve restricted id82s. in proceedings of the 27th international conference on machine learning (icml-10) (pp. 807   814).
1137. https://scholar.google.com/scholar?q=nalisnick, e., mitra, b., craswell, n., & caruana, r. (2016). improving document ranking with dual id27s. in 25th world wide web (www) conference companion volume, international world wide web conferences steering committee (pp. 83   84).
1138. https://scholar.google.com/scholar?q=neelakantan, a., shankar, j., passos, a., & mccallum, a. (2014). efficient non-parametric estimation of multiple embeddings per word in vector space. in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp) (pp. 1059   1069).
1139. https://scholar.google.com/scholar?q=nguyen, g. h., tamine, l., soulier, l., & bricon-souf, n. (2016). toward a deep neural approach for knowledge-based ir. in acm sigir workshop on neural information retrieval (neu-ir).
1140. https://doi.org/10.1007/978-3-319-28940-3_29
1141. https://scholar.google.com/scholar?q=ordentlich, e., yang, l., feng, a., cnudde, p., grbovic, m., djuric, n., et al. (2016). network-efficient distributed id97 training system for large vocabularies. in the 25th acm international conference on information and knowledge management, indianapolis, united states.
1142. https://scholar.google.com/scholar?q=palakodety, s., & callan, j. (2014). query transformations for result merging. in proceedings of the twenty-third text retrieval conference, trec 2014, gaithersburg, maryland, usa, november 19   21, 2014.
1143. http://arxiv.org/abs/1412.6629
1144. https://doi.org/10.1109/taslp.2016.2520371
1145. http://scholar.google.com/scholar_lookup?title=deep sentence embedding using id137: analysis and application to information retrieval&author=h. palangi&author=l. deng&author=y. shen&author=j. gao&author=x. he&author=j. chen&author=x. song&author=r. ward&journal=ieee/acm transactions on audio, speech, and language processing&volume=24&issue=4&pages=694-707&publication_year=2016
1146. https://scholar.google.com/scholar?q=pang, l., lan, y., guo, j., xu, j., & cheng, x. (2016a) a study of matchpyramid models on ad-hoc retrieval. in acm sigir workshop on neural information retrieval (neu-ir).
1147. https://scholar.google.com/scholar?q=pang, l., lan, y., guo, j., xu, j., wan, s., & cheng, x. (2016b). text matching as image recognition. in 30th aaai conference on artificial intelligence (pp. 2793   2799).
1148. http://scholar.google.com/scholar_lookup?title=on the difficulty of training recurrent neural networks&author=r. pascanu&author=t. mikolov&author=y. bengio&journal=icml (3)&volume=28&pages=1310-1318&publication_year=2013
1149. https://scholar.google.com/scholar?q=pennington, j., socher, r., & manning, c. d. (2014). glove: global vectors for word representation. in emnlp (vol. 14, pp. 1532   1543).
1150. https://scholar.google.com/scholar?q=ponte, j. m., & croft, w. b. (1998). a id38 approach to information retrieval. in proceedings of the 21st annual international acm sigir conference on research and development in information retrieval (pp. 275   281). acm.
1151. https://scholar.google.com/scholar?q=radlinski, f., & joachims, t. (2005). query chains: learning to rank from implicit feedback. in proceedings of the eleventh acm sigkdd international conference on knowledge discovery in data mining, acm, new york, ny, usa, kdd   05 (pp. 239   248).
1152. https://scholar.google.com/scholar?q=rekabsaz, n., lupu, m., & hanbury, a. (2016a). uncertainty in neural network id27 exploration of potential threshold. in acm sigir workshop on neural information retrieval (neu-ir).
1153. https://scholar.google.com/scholar?q=rekabsaz, n., lupu, m., hanbury, a., & zuccon, g. (2016b) generalizing translation models in the probabilistic relevance framework. in proceedings of the 25th acm international on conference on information and knowledge management, cikm    16, indianapolis, indiana (pp. 711   720). new york, ny: acm.
1154. https://doi.org/10.1108/00220410410560582
1155. http://scholar.google.com/scholar_lookup?title=understanding inverse document frequency: on theoretical arguments for idf&author=s. robertson&journal=journal of documentation&volume=60&issue=5&pages=503-520&publication_year=2004
1156. https://scholar.google.com/scholar?q=rocchio, j. j. (1971). relevance feedback in information retrieval. the smart retrieval system-experiments in automatic document processing (pp. 313   323).
1157. https://scholar.google.com/scholar?q=roy, d., ganguly, d., mitra, m., & jones, g. j. (2016a). word vector compositionality based relevance feedback using kernel density estimation. in proceedings of the 25th acm international on conference on information and knowledge management, acm, new york, ny, usa, cikm   16 (pp. 1281   1290).
1158. https://scholar.google.com/scholar?q=roy, d., paul, d., & mitra, m. (2016b). using id27s for automatic id183. in acm sigir workshop on neural information retrieval (neu-ir).
1159. http://www.emis.de/math-item?1369.68284
1160. http://scholar.google.com/scholar_lookup?title=learning representations by back-propagating errors&author=de. rumelhart&author=ge. hinton&author=rj. williams&journal=cognitive modeling&volume=5&issue=3&pages=1&publication_year=1988
1161. https://doi.org/10.1016/j.ijar.2008.11.006
1162. http://scholar.google.com/scholar_lookup?title=semantic hashing&author=r. salakhutdinov&author=g. hinton&journal=international journal of approximate reasoning&volume=50&issue=7&pages=969-978&publication_year=2009
1163. https://doi.org/10.1016/j.neunet.2014.09.003
1164. https://doi.org/10.1016/j.neunet.2014.09.003
1165. http://scholar.google.com/scholar_lookup?title=deep learning in neural networks: an overview&author=j. schmidhuber&journal=neural networks&volume=61&pages=85-117&publication_year=2015&doi=10.1016/j.neunet.2014.09.003
1166. https://scholar.google.com/scholar?q=sennrich, r., haddow, b., & birch, a. (2016). id4 of rare words with subword units. in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), association for computational linguistics, berlin, germany (pp. 1715   1725).
1167. https://scholar.google.com/scholar?q=severyn, a., & moschitti, a. (2015). learning to rank short text pairs with convolutional deep neural networks. in proceedings of the 38th international acm sigir conference on research and development in information retrieval (pp. 373   382). acm.
1168. https://scholar.google.com/scholar?q=shen, y., he, x., gao, j., deng, l., & mesnil, g. (2014a). a latent semantic model with convolutional-pooling structure for information retrieval. in proceedings of the 23rd acm international conference on conference on information and knowledge management (pp. 101   110). acm.
1169. https://scholar.google.com/scholar?q=shen, y., he, x., gao, j., deng, l., & mesnil, g. (2014b). learning semantic representations using convolutional neural networks for web search. in proceedings of the 23rd international conference on world wide web (pp. 373   374). acm.
1170. https://scholar.google.com/scholar?q=shokouhi, m., & radinsky, k. (2012). time-sensitive query auto-completion. in proceedings of the 35th international acm sigir conference on research and development in information retrieval (pp. 601   610). acm.
1171. https://scholar.google.com/scholar?q=smolensky, p. (1986). information processing in dynamical systems: foundations of harmony theory. technical report, dtic document.
1172. https://scholar.google.com/scholar?q=socher, r., chen, d., manning, c. d., & ng, a. (2013). reasoning with neural tensor networks for knowledge base completion. in advances in neural information processing systems (pp. 926   934).
1173. https://scholar.google.com/scholar?q=song, y., elkahky, a. m., & he, x. (2016). multi-rate deep learning for temporal recommendation. in proceedings of the 39th international acm sigir conference on research and development in information retrieval, acm, new york, ny, usa, sigir   16 (pp. 909   912).
1174. https://scholar.google.com/scholar?q=sordoni, a., bengio, y., & nie, j.y. (2014). learning concept embeddings for id183 by quantum id178 minimization. in aaai (pp. 1586   1592).
1175. https://scholar.google.com/scholar?q=sordoni, a., bengio, y., vahabi, h., lioma, c., grue  simonsen, j., & nie, j. y. (2015). a hierarchical recurrent encoder-decoder for generative context-aware query suggestion. in proceedings of the 24th acm international on conference on information and knowledge management (cikm) (pp. 553   562). acm.
1176. https://scholar.google.com/scholar?q=suggu, s. p., goutham, k. n., chinnakotla, m. k., & shrivastava, m. (2016). deep feature fusion network for answer quality prediction in community id53. in acm sigir workshop on neural information retrieval (neu-ir).
1177. https://scholar.google.com/scholar?q=sutskever, i., vinyals, o., & le ,q. v. (2014). sequence to sequence learning with neural networks. in advances in neural information processing systems (pp. 3104   3112).
1178. https://scholar.google.com/scholar?q=tai, k. s., socher, r., & manning, c. d. (2015). improved semantic representations from tree-structured id137. in proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: long papers), association for computational linguistics, beijing, china (pp. 1556   1566).
1179. https://doi.org/10.1613/jair.2934
1180. http://www.ams.org/mathscinet-getitem?mr=2602620
1181. http://www.emis.de/math-item?1185.68765
1182. http://scholar.google.com/scholar_lookup?title=from frequency to meaning: vector space models of semantics&author=pd. turney&author=p. pantel&journal=journal of artificial intelligence research (jair)&volume=37&pages=141-188&publication_year=2010&doi=10.1613/jair.2934
1183. http://www.emis.de/math-item?1225.68219
1184. http://scholar.google.com/scholar_lookup?title=visualizing data using id167&author=l. maaten&author=g. hinton&journal=journal of machine learning research&volume=9&issue=nov&pages=2579-2605&publication_year=2008
1185. https://scholar.google.com/scholar?q=van  gysel, c., de  rijke, m., & kanoulas, e. (2016a). learning latent vector spaces for product search. in cikm 2016: 25th acm conference on information and knowledge management. acm.
1186. https://doi.org/10.1145/2872427.2882974
1187. https://scholar.google.com/scholar?q=vulic, i., & moens, m. f. (2015). monolingual and cross-lingual information retrieval models based on (bilingual) id27s. in proceedings of the 38th international acm sigir conference on research and development in information retrieval (pp. 363   372). acm.
1188. https://scholar.google.com/scholar?q=wan, j., wang, d., hoi, s. c. h., wu, p., zhu, j., zhang, y., et al. (2014). deep learning for content-based id162: a comprehensive study. in proceedings of the 22nd acm international conference on multimedia (pp. 157   166). acm.
1189. https://scholar.google.com/scholar?q=wang, d., & nyberg, e. (2015). a long short-term memory model for answer sentence selection in id53. acl, july.
1190. https://scholar.google.com/scholar?q=wang, m., smith, n. a., & mitamura, t. (2007). what is the jeopardy model? a quasi-synchronous grammar for qa. in emnlp-conll (vol. 7, pp. 22   32).
1191. https://scholar.google.com/scholar?q=wei, x., & croft, w. b. (2006). lda-based document models for ad-hoc retrieval. in proceedings of the 29th annual international acm sigir conference on research and development in information retrieval (pp. 178   185). acm.
1192. https://scholar.google.com/scholar?q=weiss, y., torralba. a., & fergus, r. (2009). spectral hashing. in advances in neural information processing systems (pp. 1753   1760).
1193. http://www.ams.org/mathscinet-getitem?mr=3108171
1194. https://doi.org/10.1007/s10994-010-5198-3
1195. http://scholar.google.com/scholar_lookup?title=large scale image annotation: learning to rank with joint word-image embeddings&author=j. weston&author=s. bengio&author=n. usunier&journal=machine learning&volume=81&issue=1&pages=21-35&publication_year=2010
1196. https://scholar.google.com/scholar?q=weston, j., chopra, s., & bordes, a. (2014). memory networks. in international conference on learning representations (iclr).
1197. https://doi.org/10.1007/s10791-009-9112-1
1198. http://scholar.google.com/scholar_lookup?title=adapting boosting for information retrieval measures&author=q. wu&author=cj. burges&author=km. svore&author=j. gao&journal=information retrieval&volume=13&issue=3&pages=254-270&publication_year=2010
1199. http://arxiv.org/abs/160908144
1200. https://scholar.google.com/scholar?q=xia, l., xu, j., lan, y., guo, j., & cheng, x. (2015). learning maximal marginal relevance model via directly optimizing diversity evaluation measures. in proceedings of the 38th international acm sigir conference on research and development in information retrieval (pp. 113   122). acm.
1201. https://scholar.google.com/scholar?q=xia, l., xu, j., lan, y., guo, j., & cheng, x. (2016). modeling document novelty with neural tensor network for search result diversification. in proceedings of the 39th international acm sigir conference on research and development in information retrieval, pisa, italy (pp. 395   404).
1202. https://scholar.google.com/scholar?q=xu, c., bai, y., bian, j., gao, b., wang, g., liu, x., et al. (2014). rc-net: a general framework for incorporating knowledge into word representations. in proceedings of the 23rd acm international conference on conference on information and knowledge management (pp. 1219   1228). acm.
1203. http://arxiv.org/abs/1502.03044
1204. https://scholar.google.com/scholar?q=yan, r., song, y., & wu, h. (2016) learning to respond with deep neural networks for retrieval-based human   computer conversation system. in proceedings of the 39th international acm sigir conference on research and development in information retrieval (pp. 55   64). acm.
1205. https://scholar.google.com/scholar?q=yang, j., stones, r., wang, g., & liu, x. (2016a). selective term proximity scoring via bp-ann. in acm sigir workshop on neural information retrieval (neu-ir).
1206. https://scholar.google.com/scholar?q=yang, l., ai, q., guo, j., & croft, w. b. (2016b). anmm: ranking short answer texts with attention-based neural matching model. in the 25th acm international conference on information and knowledge management, indianapolis, united states.
1207. https://scholar.google.com/scholar?q=yang, x., macdonald, c., & ounis, i. (2016c). using id27s in twitter election classification. in acm sigir workshop on neural information retrieval (neu-ir).
1208. https://scholar.google.com/scholar?q=ye, x., qi, z., & massey, d. (2015). learning relevance from click data via neural network based similarity models. in 2015 ieee international conference on big data (big data) (pp. 801   806). ieee.
1209. https://scholar.google.com/scholar?q=ye, x., shen, h., ma, x., bunescu, r., & liu, c. (2016). from id27s to document similarities for improved information retrieval in software engineering. in proceedings of the 38th international conference on software engineering (pp. 404   415). acm.
1210. https://scholar.google.com/scholar?q=yi, x., & allan, j. (2009). a comparative study of utilizing topic models for information retrieval. in european conference on information retrieval (pp. 29   41). springer.
1211. https://doi.org/10.1109/msp.2010.939038
1212. http://scholar.google.com/scholar_lookup?title=deep learning and its applications to signal and information processing&author=d. yu&author=l. deng&journal=ieee signal processing magazine&volume=28&issue=1&pages=145-154&publication_year=2011
1213. http://arxiv.org/abs/14121632
1214. https://scholar.google.com/scholar?q=zamani, h., & croft, w. b. (2016a). embedding-based query language models. in proceedings of the 2016 acm international conference on the theory of information retrieval (pp. 147   156).
1215. https://scholar.google.com/scholar?q=zamani, h., & croft, w. b. (2016b). estimating embedding vectors for queries. in proceedings of the 2016 acm on international conference on the theory of information retrieval (pp. 123   132). acm.
1216. https://scholar.google.com/scholar?q=zhai, s., chang, k., zhang, r., & zhang, z. (2016a). attention based recurrent neural networks for online advertising. in proceedings of the 25th international conference companion on world wide web, international world wide web conferences steering committee, republic and canton of geneva, switzerland, www   16 companion (pp. 141   142).
1217. https://scholar.google.com/scholar?q=zhai, s., chang, k., zhang, r., & zhang, z. m. (2016b). deepintent: learning attentions for online advertising with recurrent neural networks. in proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, acm, new york, ny, usa, kdd   16 (pp. 1295   1304).
1218. https://scholar.google.com/scholar?q=zhang, q., kang, j., qian, j., & huang, x. (2014). continuous id27s for detecting local text reuses at the semantic level. in proceedings of the 37th international acm sigir conference on research & development in information retrieval (pp. 797   806). acm.
1219. https://scholar.google.com/scholar?q=zhang, w., du, t., & wang, j. (2016a). deep learning over multi-field categorical data. in european conference on information retrieval (pp. 45   57). springer.
1220. https://scholar.google.com/scholar?q=zhang, x., zhao, j., & lecun, y. (2015). character-level convolutional networks for text classification. in advances in neural information processing systems (pp. 649   657).
1221. http://arxiv.org/abs/151003820
1222. https://scholar.google.com/scholar?q=zhang, y., roller, s., & wallace, b. c. (2016b). mgnc-id98: a simple approach to exploiting multiple id27s for sentence classification. in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, association for computational linguistics, san diego, california (pp. 1522   1527).
1223. http://arxiv.org/abs/170202535
1224. https://scholar.google.com/scholar?q=zheng, g., & callan, j. (2015). learning to reweight terms with distributed representations. in proceedings of the 38th international acm sigir conference on research and development in information retrieval, acm, new york, ny, usa, sigir   15 (pp. 575   584).
1225. https://scholar.google.com/scholar?q=zhou, g., he, t., zhao, j., & hu, p. (2015). learning continuous id27 with metadata for question retrieval in community id53. in proceedings of acl (pp. 250   259).
1226. https://scholar.google.com/scholar?q=zhu, y., lan, y., guo, j., cheng, x., & niu, s. (2014). learning for search result diversification. in proceedings of the 37th international acm sigir conference on research & development in information retrieval (pp. 293   302). acm.
1227. https://scholar.google.com/scholar?q=zuccon, g., koopman, b., bruza, p., & azzopardi, l. (2015). integrating and evaluating neural id27s in information retrieval. in proceedings of the 20th australasian document computing symposium (p. 12). acm.
1228. http://creativecommons.org/licenses/by/4.0/
1229. mailto:dilek@ceng.metu.edu.tr
1230. https://crossmark.crossref.org/dialog/?doi=10.1007/s10791-017-9321-y
1231. https://www.springer.com/journal/10791/about
1232. https://s100.copyright.com/appdispatchservlet?publishername=springernature&orderbeanreset=true&ordersource=springerlink&copyright=the+author(s)&author=kezban+dilek+onal,+ye+zhang,+ismail+sengor+altingovde+et+al&issuenum=2&contentid=10.1007/s10791-017-9321-y&endpage=182&publicationdate=2017&startpage=111&volumenum=21&title=neural+information+retrieval:+at+the+end+of+the+early+years&imprint=the+author(s)&publication=1386-4564&oa=cc+by
1233. https://link.springer.com/article/10.1007/s10791-017-9321-y#citeas
1234. https://citation-needed.springer.com/v2/references/10.1007/s10791-017-9321-y?format=refman&flavour=citation
1235. https://citation-needed.springer.com/v2/references/10.1007/s10791-017-9321-y?format=endnote&flavour=citation
1236. https://citation-needed.springer.com/v2/references/10.1007/s10791-017-9321-y?format=bibtex&flavour=citation
1237. https://link.springer.com/sharelink/10.1007/s10791-017-9321-y
1238. https://link.springer.com/content/pdf/10.1007/s10791-017-9321-y.pdf
1239. https://link.springer.com/content/pdf/10.1007/s10791-017-9321-y.pdf
1240. https://link.springer.com/article/10.1007/s10791-017-9321-y#citeas
1241. https://citation-needed.springer.com/v2/references/10.1007/s10791-017-9321-y?format=refman&flavour=citation
1242. https://citation-needed.springer.com/v2/references/10.1007/s10791-017-9321-y?format=endnote&flavour=citation
1243. https://citation-needed.springer.com/v2/references/10.1007/s10791-017-9321-y?format=bibtex&flavour=citation
1244. https://link.springer.com/sharelink/10.1007/s10791-017-9321-y
1245. https://link.springer.com/article/10.1007/s10791-017-9321-y#enumeration
1246. https://link.springer.com/article/10.1007/s10791-017-9321-y#abs1
1247. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec1
1248. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec2
1249. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec3
1250. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec19
1251. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec25
1252. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec45
1253. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec55
1254. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec60
1255. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec63
1256. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec75
1257. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec76
1258. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec87
1259. https://link.springer.com/article/10.1007/s10791-017-9321-y#footnotes
1260. https://link.springer.com/article/10.1007/s10791-017-9321-y#notes
1261. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec88
1262. https://link.springer.com/article/10.1007/s10791-017-9321-y#sec89
1263. https://link.springer.com/article/10.1007/s10791-017-9321-y#bib1
1264. https://link.springer.com/article/10.1007/s10791-017-9321-y#copyrightinformation
1265. https://link.springer.com/article/10.1007/s10791-017-9321-y#authorsandaffiliations
1266. https://link.springer.com/article/10.1007/s10791-017-9321-y#aboutcontent
1267. https://link.springer.com/siteedition/link?previousurl=/article/10.1007/s10791-017-9321-y&id=siteedition-academic-link
1268. https://link.springer.com/siteedition/rd?previousurl=/article/10.1007/s10791-017-9321-y&id=siteedition-corporate-link
1269. https://link.springer.com/
1270. https://link.springer.com/impressum
1271. https://link.springer.com/termsandconditions
1272. https://link.springer.com/privacystatement
1273. https://link.springer.com/cookiepolicy
1274. https://link.springer.com/accessibility
1275. https://link.springer.com/contactus
1276. https://www.springernature.com/
1277. https://www.springernature.com/

   hidden links:
1279. https://link.springer.com/
