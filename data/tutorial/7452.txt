literature survey:

id20 algorithms for natural

language processing

qi li

qli@gc.cuny.edu

department of computer science

the graduate center, the city university of new york

june 13, 2012

contents

1 introduction

1.1 notations

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 de   nition of domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3 overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.4 general comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 id20 algorithms for nlp

1

2

3

4

8

10

2.1 feature space transformation . . . . . . . . . . . . . . . . . . . . . . . .

10

2.1.1

simple feature augmentation . . . . . . . . . . . . . . . . . . . .

10

2.1.2

structural correspondence learning

. . . . . . . . . . . . . . . .

12

2.1.3 generalizable feature selection . . . . . . . . . . . . . . . . . . .

14

2.1.4 distributional representation . . . . . . . . . . . . . . . . . . . .

17

2.2 prior based adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

2.2.1 map adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

2.2.2 hierarchical bayesian framework . . . . . . . . . . . . . . . . . .

24

2.2.3 estimating priors in target domain . . . . . . . . . . . . . . . .

26

2.3

instance selection and weighting . . . . . . . . . . . . . . . . . . . . . .

28

2.3.1 pseudo in-domain data selection . . . . . . . . . . . . . . . . . .

29

2.3.2

self-training and tradaboost . . . . . . . . . . . . . . . . . . . .

32

2.3.3

instance weighting framework . . . . . . . . . . . . . . . . . . .

34

2.4 conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

38

3 applications

42

i

references

47

ii

abstract

traditional supervised learning algorithms assume that the training data and

the test data are drawn from the same distribution. models that are purely trained

from training data are applied directly to the test data. unfortunately, in practice

this assumption is often too strong. consider a common situation in which a large

amount of manually labeled data is available in one domain, but the task is to

make predictions on examples from a new domain with little or no labeled data.

given that the instances in the two domains are drawn from di   erent distributions,

traditional supervised learning can not achieve high performance on the new do-

main. for example, in the part-of-speech tagging task, the word monitor is likely

to be a verb in the    nance domain, while in the corpus of computer hardware, it   s

more likely to be a noun. therefore id20 algorithms are designed

to bridge the distribution gap between the training data and the test data.

in this literature survey, we study the motivations, challenges and recently

developed algorithms of id20 in the    eld of natural language pro-

cessing (nlp). we    rst categorize recent work into three classes according to

their methodologies, and then review and compare some preventative algorithms

in each category. finally we summarize popular nlp applications involve domain

adaptation.

i

1 introduction

the notion of id20 is closely related to id21. transfer

learning is a general term that refers to a class of machine learning problems that

involve di   erent tasks or domains.

in the literature, there isn   t yet a standard

de   nition of id21. in some papers it   s interchangeable with domain

adaptation. pan and yang (2010) is a comprehensive survey of id21

techniques, which provided a clear de   nition of id21:

given a source domain ds and its learning task ts, a target domain
dt and its learning task tt , id21 aims to help improve the
learning of the target predictive function ft in dt using the knowledge
from ds and ts, where ds (cid:54)= dt and ts (cid:54)= tt .

if a learning problem also aims to improve the performance on ts simultaneously,
then it   s usually called id72. given di   erent settings of ds, ts, dt
and tt , id21 can be roughly divided into two categories.

1. inductive id21, where tt is di   erent from ts.
2. transductive id21, where tt and ts are the same.

in each category, there are several di   erent cases on the availability of labeled

data:

1. labeled data in ds is available, but in dt is not available.

2. labeled data in both domains is available, but the amount of labeled data

in dt is small.

3. labeled data in ds is not available, but in dt is available.

4. labeled data in both domain are not available.

1

the    rst two cases are the most common situations, since in many applications

it is very expensive to obtain labeled data in a new domain, while there is large

amount of data available in some old domains. for example, manual annotations

for id52, syntactic parsing, id39, relation

and id37 are available for news documents in english and other high-

density languages, such as tree bank1, automatic content extraction (ace)

corpus2 and conll id39 corpus3. however, such training

data is rarely available for other genres and domains such as emails, web forums

and biomedical papers.

in this situation, if tt and ts are the same, then the learning task is often

referred as id20. for example, consider we have a large collection of

training data for parsing in news documents, if we want to obtain high performance

parsing model for emails, gathering labeled data for emails is costly. fortunately

we can design algorithms to transfer knowledge from labeled data in news domain

to the new email domain. such techniques can relax the necessity of expensive

manual annotation on the new data while keep high performance.

in this survey, we mainly focus on id20 on natural language pro-

cessing, which includes tasks such as part-of-speech tagging, named entity recog-

nition, machine translation and id31.

1.1 notations

in this survey, we assume that training data is d = {(xi, yi)}n
i=1, where each
xi     x is an observed variable, and yi     y is the output label/class of xi. we

1http://www.cis.upenn.edu/ treebank/
2http://www.ldc.upenn.edu/catalog/catalogentry.jsp?catalogid=ldc2006t06
3http://www.cnts.ua.ac.be/conll2003/ner/

2

use subscript s and t to distinguish source domain and target domain respec-
tively. therefore ds means training data in the source domain, and dt stands for

training data in the target domain. in some cases, we also use subscript l and u
to distinguish labeled and unlabeled data, for example dt,l refers to labeled data

in the target domain. throughout this paper, we assume    refers to parameters
to any machine learning model we will discuss. therefore p(y|x;   ) means the

id155 of y given x under the model parameters   .

1.2 de   nition of domain

in the literature, many de   nitions of the source domain and the target domain are

proposed. in this survey, we restrict the notation of domain to some particular

settings which are common in natural language processing tasks:

    adaptation between di   erent corpora:

in this setting, each corpus can be

considered as a unique domain. for example, we can consider ace corpus

as the source domain and conll corpus as the target domain to perform

id39 (finkel and manning, 2009).

    adaptation from a general dataset to a speci   c dataset: in this setting, the

general dataset which consists of di   erent corpora can be considered as source

domain, while the speci   c dataset is considered as target domain. for exam-

ple, in (amittai et al., 2011)   s id20 for machine translation,

a combination of various available chinese-english parallel corpora is con-

sidered as the source domain. the mt system developed on this source is

then adapted to the chinese-english task in the international workshop on

spoken language translation (iwslt).

3

    adaptation between subtopics in the same corpus: in this setting, each subset

of the whole corpus is considered as a domain. for example, (wu et al.,

2009) manually classi   ed the ace corpus into several topics, and treated

the documents in each topic as one domain.

    cross-lingual adaptation: if we have training data in one language, but want

to build model for another language, then the datasets in each domain can

be considered as one domain. for example, (xu, xu, and wang, 2011) used

english id31 corpus as the source domain to build the model

for chinese test data.

there are many other settings of id20, for example, image classi-

   cation based on text, and wi    localization in di   erent time periods or mobile

devices (pan and yang, 2010). they are beyond the scope of this survey.

1.3 overview

jiang and zhai (2007a) analyzed some general properties of the id20
problem in discriminative models from the theoretical point of view. let x be the
feature space for observed data, and y be the set of output labels. in the fully

supervised setting, each training instance (x, y) is drawn from a joint distribution
p(x, y), where x     x and y     y. p(x, y) can be represented as p(y|x)    p(x). the

objective of discriminative learning is to estimate model parameters    in order
to recover the conditional distribution p(y|x), while common generative learning

aims to recover the joint distribution p(x, y). after the training process, given an

unseen test example xi, the model can predict its output label y by evaluating

p(y|x;   ).

arg max

y

in the id113 (id113) framework, the objective of the

4

learning problem can be interpreted as the following process:    rst, from the space
of x , the samples (x, y) are drawn from the joint distribution p(x, y). and p(x, y)
can further be decomposed as p(x)   p(y|x). with the instances (x, y) from the true
distribution, the learning algorithm de   nes the model distribution log p(y|x;   ) to

predict y given x. the model parameters    are then estimated to maximize the

log-likelihood of the whole sampled data set. this process can be generalized to

generative learning easily, in this case, joint distribution p(x, y;   ) is considered
instead of the conditional distribution p(y|x;   ).

in practice, since the real distribution p(x, y) is unknown, the empirical esti-

mation   p(x, y) from the sampled training data is used for approximation. in the

fully supervised learning, the basic assumption is that if we have large amount

of training data, then   p(x, y) can be a fairly good estimation of the real joint

distribution p(x, y).

in contrast, in the case of id20, the train-

ing data and the test data are drawn from di   erent underlying distributions, i.e.
ps(x, y) (cid:54)= pt (x, y). as a result, the above assumption doesn   t hold. therefore

the challenge becomes how to approximate pt (x, y) instead of just using training

data in ds or even a small amount of training data in the target domain dt,l.

given this analysis, there are three general types of id20 algo-

rithms in the previous work. the    rst class of algorithms we will discuss in this

survey is based on feature transformation (daum  e iii, 2007; blitzer, mcdonald,

and pereira, 2006; jiang and zhai, 2007b; guo et al., 2009; xue et al., 2008).
in this class, the assumption is that pt (y|x) di   ers from ps(y|x), but there exist
some general features xg     x which have identical or similar conditional distribu-

tions in both source and target domains. even when such features are very few or
don   t exist in the original feature space x , there is still some correspondence be-

5

tween ps(y|x) and pt (y|x), which means it   s possible to project the original feature
space x into a new space xt ran by using some projection methods     x . let   s con-

sider id20 for part-of-speech tagging from the source domain wall

street journal to the target domain biomedical abstract (blitzer, mcdonald, and

pereira, 2006), the part-of-speech tags of some words di   er in two domains either

because they have di   erent meanings in the two domains or one feature occurs

in one domain but rarely occurs in another domain. for instance, the the noun

investment only occurs in the wall street journal. in contrast, some features are

general in both domains. for instance, the verb required and the prepositions from

and for have same meanings in the two domains. to illustrate this general picture,

we show an example from blitzer, mcdonald, and pereira (2006) in table 1.

biomedical

wsj

the signal required to

investment required

stimulatory signal from buyouts from buyers

essential signal for

to jail for violating

table 1: example of general and domain-speci   c features (blitzer et al., 06) italicized

are general features and bold are domain-speci   c features

however, just using these obvious general features is inadequate. there are two

main challenges in such algorithms: (i) how to distinguish domain-speci   c features
and general features (ii) how to    nd a new feature space x t rans to encode the cor-

respondence between source and target domains. to address these problems, some

algorithms are proposed such as correspond structural learning (blitzer, mcdon-

ald, and pereira, 2006) and id96 (guo et al., 2009; xue et al., 2008).

6

we will discuss these algorithms in section 2.1.

the second class of algorithms exploit model priors to overcome the distribution

gap between two domains (finkel and manning, 2009; chelba and acero, 2006;

chan and ng, 2006). in discriminative learning, gaussian prior is often used for

id173. parameters    is considered to be governed by the prior distribution
p (  ). in order to approximate ps(y|x) from a large amount of source training data

and a small set of target domain training data, adjusting the prior distribution
p (  ) to some appropriate value can help produce a reasonable ps(y|x;   ). this

direction was studied in (finkel and manning, 2009; chelba and acero, 2006).

in generative learning, the notation of prior refers to the prior distribution of
an output label p(y). one assumption is that the conditional distribution p(x|y)

is the same or similar in both domains, the gap between posteriori distribution
p(y|x) mainly comes from the di   erence between ps(y) and pt (y). therefore a

good estimation pt (y) based on the available data can boost the performance.

chan and ng (2006) studied this direction in naive bayes framework.

di   erent from the above two classes, the third class of algorithms focus on

instance level without changing the underlying machine learning models (amittai

et al., 2011; xu, xu, and wang, 2011; jiang and zhai, 2007a). this type of

algorithms is closely related to common semi-supervised learning framework such

as self-training. the general idea is as follows: by weighting and selecting training
instances, one can make p(y|x;   ) close to pt (y|x) by down-weighting or discarding
instances which have di   erent ps(y|x) and pl(y|x). in another case, assume p(y|x)
stays similar but p(x) di   ers sharply in two domains, the resulting model p(y|x;   )

is highly in   uenced by the empirical distribution   p(x), then weighting and selecting

training instances can recover such impact. the main challenge in this class is

7

class

level

dependence

of

domain gap extendability

to

the

underlying

multiple domains

classi   ers

learning

feature space trans-

feature level moderate

large

strong

formation (fst)

prior based adapta-

model level

strong

large

strong

tion (pba)

instance

selec-

instance level weak

small

weak

tion/weighting

(isw)

table 2: comparison of three classes of algorithms

how to determine the instance weighting parameters or select which instances for

training. note that instance selection can be viewed as a special case of instance

weighting in which the weighting parameters are binary-valued, i.e. either 1, which

means to keep the instance, or 0, which means to discard the instance.

1.4 general comparison

table 2 compares some general aspects of these three categories. please note that

the conclusion is general for each class, but maybe not applicable to some par-

ticular algorithms. the    rst aspect is the level of adaptation: fst approaches

are classi   ed as feature level methods, since they studied the problem of design-

ing new feature space to reduce the domain gap. pba algorithms operate on

the model level, in which we need to change the underlying model itself so that

priors can be utilized for adaptation. isw works on the instance level, since the

algorithms in this class only focus on selecting and weighting instances without

changing feature space or basic classi   ers. the second aspect is the dependence

8

of the underlying classi   ers. if an adaptation method is more dependent on the

underlying classi   er, then it   s less adaptable to other machine learning tasks. isw

is the most independent of the underlying classi   ers, in theory, after the instance

selection and weighting, any classi   er can be applied, therefore these methods can

be easily applied to complicated nlp tasks such as machine translation. pba

is most dependent of the underlying classi   ers, these methods can not easily be

adapted to new classi   ers. the third aspect is the domain gap. here domain gap

means the di   erence between source and target domain. isw is the most restric-

tive method in this aspect, it   s suitable to the situation in which there exists many
examples xi in the source domain for which ps(y|xi) is similar to pt (y|xi). the

fourth aspect is the ability of the method to deal with the case where multiple

domains are available. in most cases, there is only one source domain and one

particular target domain, in practice, multiple source domains can be available

for adaptation. isw methods can be hardly extended to this scenario, but many

algorithms in fst and pba are highly applicable to this case.

the remaining part of this paper is organized as follows: in section 2, we review

and critique some representative id20 algorithms in each category.

in section 3, we summarize the common nlp applications of id20,

and compare the di   erent aspects and datasets in each task.

9

2 id20 algorithms for nlp

in this section, we will study the three categories of adaptation algorithms in detail.

section 2.1 discusses adaptation based on feature space transformation. section 2.2

reviews methods based on changing priors, finally section 2.3 describes instance

level adaptation approaches based on instance selection and weighting.

2.1 feature space transformation

changing the feature space or distinguishing subset of features can alleviate the
gap between ps(y|x) and pt (y|x). the challenge is how to transfer original feature
space x to a new space x (cid:48) which is predictive for the target domain, and how to

distill features that are predicative for both domains.

2.1.1 simple feature augmentation

daum  e iii (2007) introduced a very simple but interesting method, which can be

considered as preprocessing for any statistical learning algorithm. in this approach,

assume there is a large set of training data in the source domain, and a few

labeled instances in the target domain. each feature xi in the source domain

is duplicated to three versions: < xi, xi, 0 >. the    rst one is source domain

speci   c version, the second one refers to domain independent version, and the last

one means target domain speci   c version. since xi is from the source domain,

the last column in this example is 0. correspondingly, each feature xi in the

target domain is replicated as < 0, xi, xi >. after this simple step, the new

feature vectors are fed into common supervised training algorithm. this method

is extremely simple but the experiment results are very promising. the author gave

an intuitive justi   cation for this method. consider the part-of-speech tagging task:

10

if a high weight is assigned to the domain-independent version, it means the feature

behaves similarly in both domains. for example, the word    the    is commonly used

as determiner, so the indicator feature of determiner should attain high weight

for domain-independent version. on the other hand, if a high weight is assigned

to the domain-speci   c feature, it   s likely that this feature only occur frequently at

this domain. for example, the word monitor is more likely to be a none rather

than verb in computer hardware domain, then the indicator feature of none for

this domain is very likely to attain high weight.

the most appealing advantage of this method is that it   s very easy to im-

plement, it just needs several lines of codes to duplicate the feature vectors. in

addition, this simple method can be easily applied to multiple domains: each fea-

ture is duplicated for each domain plus a general version. finally, it can be used

with any statistical classi   ers, since the feature duplication doesn   t rely on any

knowledge from the classi   er. however, it has some limitations. first, it requires

training data in the target domain, and the performance highly relies on the size of

the target domain training data. it restricts the method to the cases where reason-

able size of target domain training data is available. second, this method is unable

to deal with the case where domain-speci   c features are di   erent but have some

hidden correlations. later we will compare some methods such as (blitzer, mc-

donald, and pereira, 2006) and (guo et al., 2009) which exploit such correlations.

finally, this approach doesn   t make formal distinction and connection between

id173 terms for domain-speci   c and domain-independent features. the

domain-independent feature weights and the domain-independent feature weights

are governed by the same gaussian prior. in later section we will show that this

approach is a special case of hierarchical bayesian model (finkel and manning,

11

2009), which allows weights of domain-speci   c features and domain-independent

features to have di   erent priors.

2.1.2 structural correspondence learning

daum  e iii (2007)   s method simply duplicates each feature as domain-independent

and domain-speci   c versions, it   s unable to capture the potential correspondence if

the context features in two domains are di   erent. blitzer, mcdonald, and pereira

(2006) proposed structural correspondence learning (scl) for domain adapta-

tion. scl is borrowed from a id72 algorithm called structural

learning, which was proposed by (ando and zhang, 2005).

the central idea of this method is as follows:    rst, de   ne m pivot features which

behave similarly in both domains, and then m predictors of the pivot features are

built based on simple linear classi   ers to model the correspondence between the

pivot features and non-pivot features. finally the original feature matrix x is
projected to a low-dimensional shared feature space       x. during training and
test, both original features x and the new features       x are used.

the projection matrix    is obtained by using the singular value decomposition

(svd) on the weight matrix of the m predictors. if some non-pivot features from

di   erent domains are highly correlated to the same pivot features, then they are

projected to same area in the new feature space. this is superior than daum  e iii

(2007) because even some context features from two domains are quite di   erent,

the scl model can still estimate the correspondence between them.

there are two very interesting features in the algorithm. the    rst is the pivot

feature predictors, which can be considered as pseudo tasks in (ando and zhang,

2005). the intuition is that predicting pivot features is highly related to the ob-

12

jective task (in the paper the task was id52). another notable feature

of this algorithm is the use of svd. svd was employed for latent semantic asso-

ciation (deerwester et al., 1990) in 1990s. it can compress the information from

original feature space into a low-dimensional shared space. in scl the svd can

e   ciently compute    based on the weight matrix of the m predictors. although

the scl algorithm is designed for the situation where labeled data is unavailable in

the target domain, the author presented a method to combine the scl algorithm

with labeled data in the target domain based on classi   er combination techniques

introduced in (florian et al., 2004).

in sum, the key advantage of scl is that it can model and utilize the correlation

between features in di   erent domains, even if these features are not the same. in

addition, it doesn   t need any labeled data in the target domain, which makes it

useful in most practical situations.

a limitation lies in the key step of this method, i.e. de   ning the set of pivot

features. it   s unclear that how much the performance relies on the quality of the

pivot feature set, and what happens if there isn   t such explicit pivot features. in

some cases, training data includes examples which don   t have any correspondence.

for instance, in the application of cross-lingual id31, the source

training data is machine translation results from the source language. therefore

the source domain instances include numerous translation errors, it   s challenging to

determine such correspondence between bad translations and the target instances.

in addition, the intuition of scl is that the tasks of learning m pivot feature

predictors are bene   cial to the task of id52, therefore jointly modeling

these tasks can bene   t id52. although it seems reasonable, this assump-

tion lacks a strict justi   cation.

if we want to apply scl to a new task other

13

than id52, it   s di   cult to tell whether the pseudo tasks are bene   cial or

not. for example, in the task of id39(ner), many features

which behave similarly in two domains may be not related to named entities, this

makes the task of predicting such features somehow irrelevant to ner. taking the

following sentence in a news article as an example:

since october of last year , the hulun buir league promulgated a

new preferential policy for opening up to the outside world.

   the hulun buir league    is an organization name. words in italic are frequent

words in news articles which have consistent senses. they can be considered as

pivot features for p os tagging. however, they are almost irrelevant to common

named entities. therefore such pivot features are not quite bene   cial to ner

tagging.

another problem is that all features are equally treated in the pivot feature

predictors, thus the    nal shared low-dimensional space is very likely to be in-

   uenced by frequent features (tan and cheng, 2009). consider again the ner

application, assuming we have de   ned a set of important pivot features, frequent

non-pivot features such as reports and they could dominate the predictors, thus

low-frequency features which are important to ner will have little impact on the

shared feature space. it would be bene   cial if we can design an e   ective feature

selection method to    nd pivot features and non-pivot features which are highly

correlated to the task.

2.1.3 generalizable feature selection

the assumption of usefulness of pseudo tasks in blitzer, mcdonald, and pereira

(2006)   s work needs more justi   cation in real application. but the general idea

14

of producing a shared feature space is an interesting direction. jiang and zhai

(2007b) proposed a two-stage id20 method to estimate and make

use of generalizable features across domains. to distinguish this approach from

other related work which also involves    two stages   , we refer to it as generalizable
features selection. this method assumes there is a set of source domains {dk}k

k=1

with labeled training data, but no labeled data in the target domain. an important

assumption is that those multiple source domains and the target domain share

some features, the author called these features as generalizable features. this idea

is similar to the use of shared feature space in (blitzer, mcdonald, and pereira,

2006)   s work. the major di   erence is that the so-called generalizable features is

a subset of original feature set x. in contrast, (blitzer, mcdonald, and pereira,

2006) compressed the original feature set into a low-dimensional feature space
    x. another di   erence is that (blitzer, mcdonald, and pereira, 2006)   s algorithm

learns the new feature space using unlabeled data since it   s easy to obtain    training

data    for the m pseudo tasks. this framework needs multiple source domain

training data to    nd the generalizable feature sets. similar to (daum  e iii, 2007)   s

work, the drawback is that it doesn   t model the correspondence between di   erent

but related features.

roughly speaking, this framework consists of two steps:

    domain selection: identify a set of generalizable features that are shared by

all source domains.

    id20: with the generalizable feature set in hand, a classi   er

is trained from the union of all source domains and    labeled data    in the

target domain from id64.

in the    rst step, the algorithm is designed to    nd an optimal feature selection

15

matrix a such that z = ax is the selected generalizable features from the original

feature vector x. a must satisfy two requirements: (i) each entry ai,j in a must be
either 0 or 1; (ii) a    at = i. with this matrix, the original weight matrix w for

x in the k-th domain can be re-written as wk = at v + uk, where at v corresponds

to the weights for the generalizable features, and uk is the domain-speci   c feature

weights. the matrix a is similar to the notion of mapping matrix    in the scl

model (blitzer, mcdonald, and pereira, 2006).

with this de   nition, the key challenge is how to estimate a    and the weights,

since enumerating all possible a is computationally prohibitive. to address this

problem, the author proposed two methods: (i) jointly optimize v and a using

an alternating optimization procedure, (ii) domain cross validation based on
a heuristic method. both methods can not    nd exact solution of the best a   .

the    rst approach is computational costly, since it needs to re-train classi   ers

iteratively. for this reason, the second approach is superior given that it only

trains 2k classi   ers if there are k source domains.

in the second stage, given a    xed a   , the    nal model is trained on the combi-

nation of labeled data in the source domains and pseudo labeled data in the target

domain. the weights for generalizable features are shared by all domains. here the

pseudo labeled data is gained from a id64 procedure. the technique of

id64 is often used in semi-supervised learning. the author didn   t provide

any stop criteria for the id64 iterations. if the id64 procedure

stops too early, then the pseudo labeled data has strong bias to the source domain

data, only a little information about the target domain can be covered in the se-

lected dataset. otherwise, if the procedure stops too late, then it   s likely to include

many incorrect labels and out-of-domain instances. it would be bene   cial if we

16

can de   ne a certain metric to decide when to stop the id64 iterations,

or train the model in the id64 purely based on generalizable features.

in general this approach is similar to the idea of (ando and zhang, 2005). the

key distinction is as follows: in this work, the assumption is that the generalizable

feature space is shared by all domains, while in (ando and zhang, 2005) and its

derivation (blitzer, mcdonald, and pereira, 2006), the projected feature space is

shared by multiple mutually bene   cial tasks. therefore the assumption in this

work is more suitable for the situation where the source domain and the target

domain are from a same large dataset. another restriction of this method is

that it requires more than one source domains with labeled training data.

in

common applications, if we don   t have many source domains, a simple solution

is to manually split the source domain into several subsets. however, it remains

unclear what the performance is if the subsets have the same distribution.

in

this case it would be preferable if we can design some criteria to split the source

domain dataset such that the resulting subsets can have di   erent properties. for

example, if the source domain is a large general dataset, while the target domain

consists of documents about a particular topic. then we can split the general

dataset according to its topic distribution into di   erent clusters as multiple source

domains. therefore the generalizable features learned from the source domains

can encode the information which is independent from any particular topics.

2.1.4 distributional representation

id96 is a class of generative models which recovers hidden topics from

unlabeled text.

it   s a natural way to capture latent semantic concepts of text.

di   erent contexts can be correlated through the latent topic distributions.

in

17

the problem of id20, latent concepts learned from multiple domains

can help bridge the distribution gap across domains. for example, suppose the

news articles in our source domain contains the politician name jon huntsman,

but the name doesn   t occur in our target domain. instead, the name of another

politician jingtao hu, who is the president of china, often occurs in the target

domain. using id96 we can infer that jon huntsman and jingtao hu

have high distributional similarity since they have similar topic-term distributions.

furthermore, common id96 techniques such as plsa (hofmann, 1999)

and lda (blei, ng, and jordan, 2003) are based on unsupervised learning. they

don   t require any labeled data, therefore it can be employed in many nlp tasks.

guo et al. (2009) applied lda for id20 of named entity recog-

nition. the lda model is used to draw hidden topics for each token from a large

set of unlabeled data containing both the source data and the target data. then

the algorithm takes topics with high id155 as features to train

a model on the source domain, and test on the target domain. in order to pre-

dict latent topics distribution associated with each word xi, the author proposed

an interesting strategy called virtual context document to characterize each word

xi. each virtual context document vdxi consists of context snippets in the whole

corpus which contains the word xi.

during the training of lda, the vdxi for each word xi is collected, then the

lda model    is trained on top of all virtual contexts. the authors proposed a

novel method to train the lda. instead of using counts as the value of each term in

(blei, ng, and jordan, 2003), they used point-wise mutual information between

each context word and xi. with the lda model    on virtual context documents,

the hidden topics for each word xi are generated from   . they are    nally injected

18

to the original feature vector of each word. such features are connections between

di   erent context words in di   erent domains.

the latent topic distribution can be learned from any other probabilistic topic

modeling methods such as plsa. the main di   erence between lda and other

topic models is that lda use a dirichlet prior distribution on the topic mixture

parameters corresponding to the documents. therefore it can induce better topic

distribution for each document (blei, ng, and jordan, 2003).

the use of virtual context document is an interesting idea. however, in practice

a single word can have di   erent latent topic distribution under di   erent surround-

ing contexts. therefore it would be better if we can di   erentiate topic-distribution

for any single word in di   erent contexts.

another problem of this method is that it models the topic distributions in the

source and the target domains in one lda model, which means the source contexts

and the target contexts are generated by the same distribution. in the real-world

distribution, the priori topic distribution should be diverse across domains. for

example, articles from the political domain are more likely to report political news,

while in the    nance domain, the articles tend to cover    nance-related topics.

(xue et al., 2008) proposed an extension of plsa for cross-domain text clas-

si   cation, which is called topic-bridged plsa (tplsa). as before, they assume
that the training data dl in the source domain is available, while the target do-
main data is unlabeled du . the central idea of this approach is: although the two

domains are di   erent, they still share some common latent topics. assume the two
models are sharing the same term-topic distributions, but dl     dl is generated
from the conditional distribution p r(dl|z), and each du     du is generated from
another distribution p r(du|z). based on this assumption, the author proposed

19

to couple two independent p lsas models p r(du|z) and p r(dl|z) on the source

and the target domains to a joint model by a convex combination with a hyper-
parameter        (0, 1), the term-topic distribution p r(z|w) is shared by the two

p lsas, therefore mixing topics z can bridge the two domains.

another interesting point in this framework is as follows:

in order to uti-

lize the ground-truth classes in dl, the author applied must-link constraints and

cannot-link constraint idea from (cohn and mccallum, 2003). for any di

l and dj

l

occurring in the same class, there is a must-link constraint in the log-likelihood

function. conversely, for any di

l and dj

l which are not in the same class, a cannot-

link constraint is injected to the log-likelihood function.

the parameters p(z|w), p(dl|z) and p(du|z) can be estimated by an em pro-

cedure. after that, a simple majority voting schedule is applied to determine

mapping between class c and hidden topics z, and then classify each unlabeled

document du.

one limitation of tplsa is that it assumes the topic-word distribution p(z|w)

is constant across domains. this assumption is too strong. for example, in the

document about software, the word bug refers to software defect, but in the article
about biology, it means insects, therefore the id203 p(z|bug) should be dif-

ferent in the two domains. another problem is how to    x the hyper-parameter of

  , the setting of    can directly determine to what extent to rely on which domain

during the training. the author simply set the value of    as 0.5, which means that

two domains are equally important. but it would be better if we can determine   

automatically.

in addition, this model is specially designed for the problem of text classi   ca-

tion, therefore, it can not be easily adapted to other nlp applications. in other

20

tasks such as id52 and ner, the topic-word distribution p(z|w) is more
important than the document-topic distribution p(d|z). the di   erence between
p(du|z) and p(dl|z) is less useful in such applications.

in sum, the methods of distributional representation recover latent topics

from the both source and target domains in unsupervised fashion. the common

advantage is that they don   t require labeled data in the target domain, this is

superior than methods such as (daum  e iii, 2007) and many other methods based

on priors (in section 2.2) which also need labeled data in the target domain. the

common drawback of using id96 is that setting the number of topics

or other hyper-parameters such as    in the tplsas is a hard problem, since we

don   t know the ground truth of the latent topics

2.2 prior based adaptation

priors are usually used in both discriminative and generative models. in generative

models, such as bayes models, the distribution of output y given input x is usually

de   ned as:

p (y|x) =

(cid:88)

y(cid:48)

p(x|y)    p(y)
p(x|y(cid:48))    p(y(cid:48))

where p(y) is priori id203 of output y, and p (y|x) is posteriori id203.
given a test instance x, the model should    nd y    that maximizes a posteriori

id203 (map) as output.

y    = arg max

y

p(y|x)

in the scenario of id20, the prior probabilities drawn from di   erent

domains may di   er. the performance will be decreased dramatically if the model

trained from the source domain is applied directly to the target domain. therefore

21

it   s crucial to change the prior p(y) estimated from the source domain to a proper

value pt (y) for the target domain.

in discriminative models, priors were originally introduced as id173

term for parameter smoothing, which can in   uence the estimation of parameter   .
therefore with di   erent priors, the model distribution p(y|x,   ) learned from the

same training data may di   er. one commonly used prior in discriminative models
is gaussian prior n (  i, diag(  2

i )), where    is the mean and    is the variance.

(cid:88)

for the purpose of id173,    is often set as 0, then the gaussian prior for

  2
i
2  2 . assuming the original log-likelihood of the training
parameter set    is
data under parameter setting    is l  (d), by incorporating the gaussian prior

i

term, the log-likelihood of training data becomes:

l  (d)    (cid:88)

  2
i
2  2

i

using this modi   ed log-likelihood, large parameter is penalized to avoid over   tting.

although the notion of prior is quite di   erent in generative models and dis-

criminative models, we can think of the prior in both cases encodes some prior

knowledge before the model estimation. in the id20, appropriate

prior knowledge can help estimate the model for the target domain.

2.2.1 map adaptation

chelba and acero (2006) designed a very simple but promising adaptation method

based on gaussian priors in log-linear model. in this work, the maximum id178

markov model was applied in automatic capitalization task: given uniformly cased

text, the goal is to recover the capitalization automatically. the author assumed
that there is a large amount of source domain training data ds,l, and a small
amount of target domain training data dt,l. at    rst, a common maximum id178

22

model with zero-mean gaussian priors is trained on ds,l. this step is identical

to traditional supervised learning. the feature weight matrix    is estimated to

optimize the log-likelihood of the source-domain training data.

in the next step, in order to shift the model p(y|x;   ) to match distribution of
the target domain, another maximum id178 classi   er is trained on dt,l. the

most novel part in this work is: unlike common supervised learning, in this process

the mean of the gaussian prior for each feature weight is changed. if a feature
weight   i is de   ned in the    rst model (this feature appears in ds,l, then in the

new model, the mean of its gaussian prior is set as the value in the    rst model.

it can be viewed as    default value    for the   i in the second model. if the feature

only occurs in the target domain, then the prior mean is set as 0 as usual.

the basic intuition of the second training process is that it keeps   i as close
as to the value in the source-domain model, unless there is strong evidence in dt,l

to move the parameter away from the prior mean. the cost of such change is

dominated by the variance of the gaussian prior   2
i .

although the authors only experimented with this approach with the maximum

id178 markov model, it can be easily adapted to any other discriminative models

using gaussian priors, especially for id148. the advantages of this

method are two-fold: (i) it   s easy to implement, since it only needs to change the

gaussian prior for a given log-linear model. (ii) it doesn   t rely much on labeled

data in the target domain, like the title of the paper says: little data can help a lot.

the reason is the    default    parameter values are obtained from the source-domain

model, thus the training process on the target domain only aims to change the

parameter values of the target domain-speci   c features. in contrast, in (daum  e

iii, 2007)   s work, both the general features and domain-speci   c features have same

23

prior with zero-mean, and the objective of the training process is to optimize log-

likelihood of both the source domain and target domain training data. therefore

the size of target training data must be large enough to estimate domain-speci   c

parameters.

one limitation of this approach is that it   s only suitable for an adaptation

problem between two domains, while methods such as (daum  e iii, 2007) can be

applied when there are multiple source and target domains.

although in the second training process the prior mean of di   erent features

are di   erentiated, the prior variance   2

i is set as the same value which is optimized

in the development set. it would be preferable if di   erent features have di   erent

  2
i such that the target-domain speci   c features and the common features have

di   erent penalty if they deviate from their prior mean.

2.2.2 hierarchical bayesian framework

while (chelba and acero, 2006) had to train two separate models on the source

and the target training data, finkel and manning (2009) proposed a joint model

called hierarchical bayesian id20 framework. in this method, hier-

archical bayesian priors are extended to the id49 classi   er.

at    rst, similar to (daum  e iii, 2007), each feature weight   i is replicated for each

domain and a general version. after that, two di   erent types of gaussian priors

are introduced in the hierarchy: the    rst type is domain-speci   c priors for each

domain, and the second type is a top level domain-independent prior. the prior

of each domain d controls feature weight for this domain   d,i. and the prior mean

is the general version of the feature weight      ,i.      ,i is in turn governed by the

top-level domain-independent prior with zero-mean.

24

in such a hierarchy, the feature weights in di   erent domains in   uence each other

through the top level domain-independent prior. since the domain independent

parameters      ,i are used as the prior mean of the domain-speci   c parameters, they

can in   uence the value of domain-speci   c parameters   d,i. conversely, the domain-

speci   c parameters jointly a   ect the value of domain-independent parameters. if

there is a strong evidence in the contexts of domain d to force   d,i to be a large

value, then the prior has little impact on   d,i. conversely, if there is little evidence

about   d,i, then the value is largely dominated by      ,i, which is similar to the

feature weight of the source-domain model in (chelba and acero, 2006). the

author used an interesting example to illustrate this idea: leeds is normally a

location in many domains, but in the sports domain, its usually an organization,

since a football team is called leeds.

it   s shown that the method introduced in daum  e iii (2007) is a special case of

this method where the domain-speci   c prior mean is 0 and the variances   d =      .

the distinction between domain-speci   c and domain-independent prior variances

makes this approach more advantageous than (daum  e iii, 2007). also it can be

viewed as an extension of (chelba and acero, 2006)   s work, where the source

domain prior and feature weights correspond to the top level prior and the general

version of feature weights in this work. the key advantage of this work compared

to (chelba and acero, 2006) is that it supports multiple domains while (chelba

and acero, 2006) only works for two domains.

since the key idea of this approach lies in the hierarchy of gaussian priors,

we must change the objective function (log-likelihood for statistical classi   ers) of

the underlying algorithm. it   s not as easy as (chelba and acero, 2006; daum  e

iii, 2007) to implement. in addition, the classi   ers for each domain are connected

25

together as a big joint model. it consumes much more memory to train such a

model than (chelba and acero, 2006).

the proposed hierarchical priors allow classi   ers of di   erent domains to share

the information of the general version of the features. if the values of the feature

  i in all domains are close to its general version, then this feature behaves similarly

across domains. it corresponds to the notion of pivot feature in (blitzer, mcdon-

ald, and pereira, 2006)   s work. while the advantage of scl algorithm is that

it can model the correspondence between pivot features and non-pivot features,

therefore di   erent features in two domains can be projected to the same area in

the new feature space if they are highly correlated.

it could be an interesting

direction if we can combine scl and this work. for instance, we can learn the

pivot features from the joint model, and then train the    nal classi   er based on the
new feature space       x which is provided by scl.

2.2.3 estimating priors in target domain

the above two prior-based methods are designed for discriminative models with

gaussian prior. in this section we review chan and ng (2006)   s method, which

studied estimating class priors in the target domain for the naive bayesian model.

in this work, naive bayes algorithm is applied for id51.

the author assumed that there is training data in the source domain, but no
labeled data in the target domain, and further made an assumption that p(xk|y),

the distribution of instance xk given output (word sense) y, does not change across

domains. therefore estimating exact priors in the target domain is very important.

based on this assumption, they proposed a prior estimation method and the new

priors are used to classify the target domain data in the naive bayes framework.

26

the overall adaptation framework can be decomposed to three steps: at    rst,
a naive bayes model m odels is trained from source domain data ds, if we apply

this model to classify the target domain instances, it su   ers from the problem that

class priors are di   erent across domains. therefore in the second step, they use the
posteriori id203 ps(y|x) provided by m odels to perform an em algorithm

to estimate word sense prior pt (y) in the target domain. finally, pt (y) and other

parameters from m odels are used to output an adjusted posteriori id203
padjust(y|x) for the target domain data.

in this framework, the most novel and crucial part is the second step. the

author proposed an em algorithm to estimate the class priors in the target do-
main. let ps(y|x) be the posteriori id203 of instance x in the target domain
estimated using m odels. let pk(y) and pk(y|x) be the new priori and posteriori

probabilities in k-th iteration in the em procedure. the em algorithm    rst ini-
tializes p0(y) as ps(y). in the e-step in each iteration, pk(y|x) is calculated based

ps (y|x)
y(cid:48) ps (y(cid:48)|x) which is weighted by pk(y)

ps (y) . in the m-step, pk+1(y) is updated by
on
summing over pk(y|x) of all instances in the target domain. at the beginning,

(cid:80)

p(y) is close to ps(y). after a number of iterations, it will be close to the true

distribution of p(y) in the target domain.

the author claimed that well-calibrated probabilities are important to yield
an accurate estimation of pt(y|xk), but the posteriori probabilities estimated from

the naive bayes model doesn   t meet the requirement of calibrated probabilities.
therefore they further proposed two methods to calibrate ps(y|xk) instead of just

using the estimation from m odels. in the    rst method, pair-adjacent violators al-
gorithm (pav) (miriam ayer and silverman, 1955) is deployed to modify ps(y|xk)

from m odels. in the second approach, a id28 model trained from ds

27

is used to provide posteriori id203 ps(y|xk). after estimating priors in the

target domain, the new priors are applied with another parameters in m odels to

estimate adjusted posteriori. the author proved that the estimation of priors gives

signi   cant improvement compared to directly applying naive bayes model trained

from ds to dt directly, and the calibrated probabilities can further improve the

performance dramatically.

unlike the two prior-based methods in the previous sections, this method is

designed for generative classi   ers. the introduction of calibrated probabilities

provides a critical suggestion for further research on this direction. its main lim-

itations are two-folds: first, since it only focuses on estimating appropriate class
priors in the target domain, it doesn   t address the problem where p(x|y) di   ers

across domains. second, the algorithm in this approach is strongly coupled with

the naive bayes classi   er, it would be di   cult for it to be adapted to other classi-

   ers. despite these limitations, the general idea of estimating target domain priors

for bayes models is very interesting, since the di   erence of priori probabilities in

two domains should be a common problem in the id20 task. fur-

thermore, after estimating priors in the target domain, we can still use the model

learned from the source domain with minor changes.

2.3

instance selection and weighting

unlike algorithms in the previous sections, which seek to explore feature space

or adapt the models to accommodate the target domain, the techniques we will

examine in this section mainly focus on the instance level, i.e. they are based on

selecting and/or weighting instances.

in the case that pt (y|x) in the target domain di   ers from ps(y|x) in the source

28

domain, if we can predict instances xi for which pt (yi|xi) is quite di   erent from
ps(yi|xi), then we can remove these misleading instances from the source domain
data. conversely, if some instances xi for which pt (yi|xi) is similar to ps(yi|xi), we

can select these instances for inclusion into training data. in addition, to address

to the problem where pt (x) is di   erent from ps(x), one can assign high weight to

the instances in the target domain to balance the empirical distribution.

2.3.1 pseudo in-domain data selection

amittai et al. (2011) presented a data selection method called pseudo in-domain

data selection. they used a small dataset that selected from a the large general-

domain parallel corpus to train a id151 model for the
target domain. this assumes that the large general domain ds corpus includes

some data ds,t which are drown from the same distribution as the target domain
dt . therefore the selection method is designed to dig out the ds,t which is hiding
in ds. the authors refer to the selected subset as pseudo in-domain training data.

the resulting system trained on a small set of pseudo in-domain data achieved

equal or better performance than the model trained on the whole large data set.

the main challenge of this approach is how to evaluate the importance of the

instances in the general source domain according to its relevance to the target

domain.

in this work, three di   erent but similar instance selection criteria are

employed for ranking and selecting instances: the    rst criterion is cross id178.

assuming a target domain language model lmt has been trained in advance, this

criterion calculates cross entry h(s, lmt) between empirical distribution in the

candidate segment s and the language model lmt, and further uses it to calculate

the perplexity 2h(s,lmt) as evaluating score. the sentences with lowest perplexity

29

are chosen as the pseudo target domain data. the intuition of this criterion is to

select instances whose distribution is similar to the target domain data.

the second criterion cross-id178 di   erence calculates the di   erence be-

tween the cross id178 between s and the language model lmt learned from the

in-domain data, and the language model lms learned from the general-domain:
h(s, lmt)     ho(s, lms). again, the sentences with lowest di   erence is selected

as pseudo target-domain data. this criterion not only considers the instances

with similar distribution to the target-domain data, but also biases against the

instances whose distribution is close to the general domain.

the above two methods only take into account monolingual distribution. for

machine translation problem, it   s bene   cial to jointly take into account distri-

butions in both the source and the target languages. the last criterion bilin-

gual cross-id178 di   erence is designed to meet this requirement.

it sums
[h1(s, lmt)     h1(s, lms)] +

cross-id178 di   erence in the two languages:
[h1(s, lmt)     h1(s, lms)].

despite the di   erence of the three methods, the last step of this framework is

simply to train a model on the selected training instances. the selected training

dataset is much smaller than the large general source domain dataset. experi-

mental results showed that all three methods yield state-of-the-art performance

on machine translation by using only a small set of pseudo target domain data.

and the last approach gives best performance.

in general, this framework is very easy to implement, one just needs to build

language models on the two domains and then calculate the selection metric scores.

another advantage is that it   s totally independent of the underlying machine learn-

ing model and feature representation. this makes it possible to perform domain

30

adaptation on complicated systems such as machine translation without modifying

the detailed learning algorithm. furthermore, it doesn   t need any labeled data in

the target domain.

a problem of this simple framework is that it only provides instance selection

criteria, but doesn   t de   ne the threshold for the selection. therefore it   s hard to

determine how many instances to be selected into the    nal training data. if we

manually set a threshold to the data size or the evaluation scores, then it   s likely

that the resulting training dataset contains many instances whose distribution is

di   erent from the target domain, or it only includes a small set of biased instances

which can not cover the distribution of the target domain.

furthermore, it   s only suitable to the scenario where the source domain con-

tains examples similar to target domain. in practice, the source domain and the

target domain are often two speci   c domains with limited similarity. for example,

consider id20 for ner task. if the source domain is news articles

and the target domain is scienti   c papers, then such framework is not suitable be-

cause only uninformative examples can be selected from the source domain, which

is not useful for the task.

another disadvantage is that it only considers hard selection, where one ex-

ample in the source domain is either discarded or kept. it would be bene   cial if
it allows soft selection, where each example is assigned a weight w     (0, 1). in

contrast, the methods of (xu, xu, and wang, 2011) and (jiang and zhai, 2007a)

use weighing to select instances softly. the advantage of soft selection is that it   s

possible that the instances with low weights are useful in some cases.

31

2.3.2 self-training and tradaboost

xu, xu, and wang (2011) studied id20 algorithms for cross-lingual

id31, which addresses instance selection problem by using trad-

aboost (dai et al., 2007) and transfer self-training.

in their work, the source

domain dataset consists of machine translated sentiment training data from the

source language, obviously the data is very noisy but retains some useful infor-

mation for the task in the target language.

the tradaboost algorithm is an extension to the original adaboost algo-

rithm (freund and schapire, 1996), which is tailored for id21 purposes.

the central idea of applying the tradaboost algorithm to id20 is

to iteratively reduce low quality source domain data, and train better model on

the rest. in this framework, the union of the source-domain and the target-domain

training data is used. at each iteration t, the algorithm samples the training set

from the whole dataset according to a distribution p t(x), which is initialized as

uniform distribution. then a weak classi   er is trained on the sampled training

set. the sampling distribution p (x) is updated according to the training errors.
unlike the traditional adaboost, classi   cation errors made on ds,l and dt,l are

responded in di   erent ways, since the objective is to optimize the model for the

target domain rather than the whole dataset:

    if a classi   cation error is made on xi     ds,l, then xi is likely to be useless
for the adaptation, therefore the weight of xi     d(x) in p t(x) is decreased

so that xi has less impact on the parameter estimation.

    in contrast, if a classi   cation error is made on xi     dt,l, then the weight of

xi in p t(x) is increased to boost the performance on the target domain.

32

during this iterative learning process, low quality translations are    ltered out

gradually, and a better model is obtained after convergence.

the self-training procedure employs an opposite strategy, which starts from

training a classi   er on the target labeled data, and then adds high-quality source

domain examples iteratively. at the beginning, a classi   er is trained purely using
the training data in dt,l. at each iteration, the previous classi   er is applied to

the source domain data, and then correctly classi   ed source domain examples with

high con   dence is added into the training set. after that, a new classi   er is trained

on the augmented training set. in this iterative procedure, these selected exam-

ples are high quality translations. the algorithm iteratively re-trains classi   ers

on augmented training sets, and stops when there isn   t any more quality source

domain instances. the author called this algorithm as transfer self-training.

the di   erence between the transfer self-training and the traditional self-

training is that in the case of id20, the ground truth in the source

domain is known so that we can determine whether an instance in ds,l is correctly

classi   ed or not. this is more reliable than just considering the con   dence value

from the classi   er in the traditional self-training.

this approach is also similar to (amittai et al., 2011)   s framework, because they

both expand the training dataset according to the similarity between the source

domain data and the target domain data. the key di   erence is that (amittai

et al., 2011)   s work doesn   t require manual annotation on the target domain, the

training instances are selected purely based on language model. in contrast, in the

transfer self-training, instances are selected based on supervised classi   er, which

is more computationally costly. another di   erence is that, in the transfer self-

training, instances in the source domain are repeatedly added to the training set,

33

semantic drift could happen when more instances are added. it would be better if

we can design some stop criteria in which the distribution di   erence (for example,

according to a language model) between the selected training set and the target

domain is taken into account.

the common disadvantage of the proposed two solutions is that they only
down-weight or discard the examples in the source domain for which ps(y|x) dif-
fers from pt (y|x), but doesn   t study the di   erence between pt (x) and ps(x). in

contrast, jiang and zhai (2007a)   s work, which we will review in later section, pro-

posed a balanced id64 framework to address the problem where pl(x)

and ps(x) are di   erent. besides, the target-domain labeled instances must play a

key role in both of them, but in practice it   s not possible that we can have labeled

data for every new domain. finally, both algorithms train the underlying classi   er

repeatedly, which is computationally costly compared to methods only train one

model after instance selection such as (amittai et al., 2011).

2.3.3

instance weighting framework

in the case that pt (x) is quite di   erent from ps(x), instance weighting can be

used to balance the   p(x) so that   p(x) is close to   pt (x). jiang and zhai (2007a)

proposed an instance weighting framework for id20, in which three

di   erent datasets are used to approximate the target domain distribution pt (x),
namely the source labeled dataset ds, the target labeled dataset dt,l and the
target unlabeled dataset dt,u.

assuming discriminative models are employed, the author framed the goal of
adaptation as    nding the optimal solution       such that p(y|x;      ) is close to the
real conditional distribution p(y|x) in the target domain. since the distribution

34

of the target-domain data is unknown, the author proposed three complementary
methods to approximate p(y|x) in the target domain using ds, dt,l and dt,u

respectively.

the    rst method only relies on ds. the idea is to use p(y|x;   ) that learned
from ds to approximate pt(y|x). this is similar to the naive adaptation method in

which the model learned from the source domain is directly applied to the target

domain instances. the di   erence is that an instance weighting term pt(xs)

ps(xs) is added

to the objective function to measure the distribution di   erence between pt(xs) and

ps(xs). so the challenge becomes how to evaluate pt(xs)

ps(xs) for each instance in ds.
the second method estimates p(y|x;   ) from dt,l, this is identical to the stan-
dard supervised learning, since the amount of dt,l is small, this method can not

yield good performance.

in the last method, if we know pt(y|xi;   ) for each xi     dt,u and each label
y     y, then we can weight each possible pair of (xi, y), and then add the dt,u to
the training data. the problem here is how to estimate pt(y|xi;   ).

given the above three complementary solutions, the author then proposed

a general weighting framework which combines them together as a joint model.

four types of weighting parameters are crucial to this framework, namely   ,   ,   
i     ds), the value of   i indicates how

and   . for each training example (xs

i , ys

i |xs

i |xs

likely pt(ys

i ) is close to ps(ys

i ). large   i means two distributions are close

while small   i means they are pretty di   erent such that pt(ys

i |xs

i ) can not be

trusted to learn a classi   er for the target domain. to estimate the value of   i,

the authors applied a simple strategy similar to self-learning: at    rst, a model
  t,l is trained on dt,l to approximate the distribution pt(y|x), then each instance

xs
i

is classi   ed by the model   t,l.

if it   s correctly classi   ed, pt(ys

i ) is pretty

i |xs

35

close to ps(ys
i |xs

from ps(ys

i ). conversely, if it   s misclassi   ed, then pt(ys

i |xs
i ) di   ers much
i ). therefore, for instances in ds which are misclassi   ed with highest

i |xs

con   dence, the value of   i is set to 0,   i of all others are set to 1. this is closely

related to the idea of transfer self-training proposed by (xu, xu, and wang,

2011), the di   erence is (xu, xu, and wang, 2011)   s algorithm is an iterative

process, it adds source domain labeled instances repeatedly.

for each xt,u

k     dt,u, and each possible label y     y, parameter   k(y) indicates
k . the estimating   k(y) is related to the

how likely y is a correct label for xt,u

weighting of pseudo labeled data in semi-supervised learning. this paper discussed

two types of solutions: (i) set   k(y) as a function of model parameter    such as
  k(y) = p(y|xt,u;   ). (ii) set   k(y) in a id64 manner, in which the value

of    is updated iteratively.

finally the three parameters   s,   t,l and   t,u combine the three approximation

methods together in the learning objective function. for the id20,

more emphasis should be paid to the target domain, so   t,l and   t,u should be

larger than   s. similar to this work, (jiang and zhai, 2007b) also emphasizes the

target domain instance in the training objective function. the di   erence is that

in (jiang and zhai, 2007b)   s work this is done by setting a lower penalty on the

feature weights in the target domain training instances.

the beauty of this framework is that it doesn   t only down-weight the instances
for which p(y|x) is di   erent in two domains, but also addresses the problem where

p(x) di   ers in di   erent domains by using the instances in the target domain. in

the traditional id64, instances in dt,u are selected to the training set

based on their predication con   dence. after this step, the selected instances are

considered as important as the data in ds,l. in contrast, this work suggests that

36

the performance may be in   uenced by the di   erence between ps(x) and pt (x).
therefore the instances from dt,u are attached high weights such that the empirical

distribution   p(x) is balanced towards pt (x).

another signi   cant advantage lies in the    exibility of this framework, it takes

advantage of both labeled and unlabeled data in the target domain.

if labeled

data in the target domain is unavailable, the algorithm would still work. it can

be shown that the standard id64 is a special case of this framework.

one problem of this framework is that there are several hyper-parameters to

tune, and it remains unclear how to set the hyper-parameter   , which is used to

approximate pt(xs
i )
i ) . compared to the pseudo in-domain data selection method in
ps(xs
it requires

(amittai et al., 2011)   work, this framework is much more complex.

several training process in order to set the parameters, which is computational

costly.

although this framework can be applied to various machine learning models (in

the paper they used id28), we still need to modify the log-likelihood

function in the original training process in order to accommodate these weighting

parameters. finally, for the models that globally model structured data, each

instance stands for a set of instances in a structure, and the output is a set of

assignments. consider the crfs model for id52, each instance x is a

sequence of words, and the output y is the sequence of labels for the sentence.

it   s too coarse to perform the instance selection on sentence level. for instance,

if some words of a sentence have desired distribution but others do not, it   s not

easy to decide whether to select or discard the whole sentence. on the other

hand, if we use models that locally model each word such as id28,

it su   ers from the problem that the predications are not globally optimized. it

37

would be preferable if we can enhance this framework such that structured learning

algorithms can be applied.

2.4 conclusions

id20 approaches in nlp tasks can be roughly categorized into three

classes: feature space transformation, prior adaptation and instance selection

and weighting. following this categorization, we reviewed some representative

work in each category. in general the algorithms we studied in this section can

be applied to various machine learning models and nlp tasks. in the rest of this

section, we summarize these algorithms in several aspects: supervised or unsuper-

vised, dependency on the underlying models, assumption of data availability, and

computational cost. table 3 shows a brief comparison according to these aspects.

supervised vs. unsupervised in some algorithms, the central part of the

adaptation is conducted in unsupervised fashion (blitzer, mcdonald, and pereira,

2006; guo et al., 2009; amittai et al., 2011).

it doesn   t rely on the labels in

the corpora. for example, amittai et al. (2011) addressed the instance selection

problem based on language model distributions. guo et al. (2009) learned latent

semantic association using lda. in blitzer, mcdonald, and pereira (2006)   s work,

although the pivot feature predictors are learned in a supervised way, the training

data of the predictors can be obtained automatically from unlabeled documents.

other algorithms perform the adaptation using supervised approaches. for in-

stance, jiang and zhai (2007b) extracted generalizable features using the labeled

data in the source domains. chelba and acero (2006) estimated domain-speci   c

feature weights under the supervision of labeled data in the target domain. xu,

xu, and wang (2011)   s algorithm weights instances in the source domain under

38

algorithm

supervised / unsu-

dependency on the

assumption of data

computational cost

pervised

underlying models

availability

daum  e iii (2007)

unsupervised

weak

training data

in

low

the target domain

blitzer, mcdonald,

unsupervised

weak

-

low

and pereira (2006)

jiang

and

zhai

supervised

moderate

multiple source do-

high

(2007b)

guo et al. (2009)

unsupervised

weak

xue et al. (2008)

semi-supervised

strong

mains

-

-

low

low

chelba and acero

supervised

moderate

training data

in

low

(2006)

the target domain

finkel and man-

supervised

moderate

training data

in

moderate

ning (2009)

the target domain

chan

and

ng

unsupervised

strong

(2006)

amittai

et

al.

unsupervised

very weak

-

-

low

low

(2011)

xu, xu, and wang

supervised

weak

training data

in

high

(2011)

the target domain

jiang

and

zhai

supervised

weak

-

high

(2007a)

table 3: summarization of the algorithms.    -    means there isn   t any special restriction

39

the supervision of labeled in the target domain.

dependency on underlying models most algorithms in instance selec-

tion and weighting are agnostic of the underlying machine learning algorithms.

therefore they are preferable for the cases where the underlying tasks or classi   ers

are complex, such as id151. for example, amittai et al.

(2011)   s work can be easily applied to various applications since it can be consid-

ered as a preprocessing step before conducting any applications. however, some of

them are not suitable for a structured learning algorithms because they are work-

ing on the instance level. for instance, in jiang and zhai (2007a)   s framework,

if the basic classi   er is structured learning model such as crfs, the instance

becomes sentences rather than words or chunks.

it would be too coarse to do

weighting on the sentence level. since feature space transformation algorithms

aim to provide predicative feature representation, in theory they are applicable

to most nlp tasks which are based on feature vectors such as id52 and

id39. algorithms in prior adaptation are highly coupled with

the basic learning models. therefore we need to modify the original learning ob-

jective functions for this type of methods. the algorithms in chelba and acero

(2006) and finkel and manning (2009) are suitable for discriminative classi   ers

with gaussian priors, while the idea in chan and ng (2006) is applicable to bayes

models which depend on priori distributions.

assumption of data availability the assumption of data availability is

critical to id20. (daum  e iii, 2007; chelba and acero, 2006; finkel

and manning, 2009; xu, xu, and wang, 2011) require available training data in the

target domain. among them, (daum  e iii, 2007; finkel and manning, 2009) can

be viewed as multi-domain learning methods, in which the same task on di   erent

40

domains mutually bene   t each other. in contrast, (chelba and acero, 2006; xu,

xu, and wang, 2011) are designed for the cases where only a small amount of

training data in the target domain is available. (blitzer, mcdonald, and pereira,

2006; jiang and zhai, 2007b; guo et al., 2009; xue et al., 2008; chan and ng,

2006; amittai et al., 2011) don   t need labeled data in the target domain. therefore

they are preferable in most practical settings. jiang and zhai (2007a)   s framework

is very    exible in the sense that it is independent of the availability of labeled data

in the target domain.

computational cost

some algorithms need to train the underlying ma-

chine learning models for several times, either because the frameworks need to

iteratively select features or instances, or because they need to estimate domain-

speci   c feature weights. (jiang and zhai, 2007b)   s framework iteratively trains

models in order to select the generalizable features. in addition, a id64

procedure is required in the    nal model to provide training data in the target do-

main. (jiang and zhai, 2007a; xu, xu, and wang, 2011) needs to iteratively train
models to re-weight instances in ds and dt,u. therefore their algorithms are more

computational costly than others during the training stage. (chelba and acero,

2006)   s framework only needs to train two classi   ers, where the second model is

to adjust target-domain speci   c feature weights. it   s less computational expensive

compared to the above two methods.

41

3 applications

in this section, we summarize some common nlp applications of domain adapta-

tion in recent publications. the adaptation algorithms are reviewed in the previous

section, therefore in this section we will focus on di   erent applications and their

experimental settings.

although some algorithms we studied in section 2 are applicable to general ma-

chine learning problems, most of them are designed for special applications. also

since there isn   t a standard setting for id20, various experimental

setups are studied in the previous work. in table 3 we summarize the properties

of di   erent adaptation applications, and list common settings and suitable algo-

rithms for each application.

in the remaining part of this section, we compare

di   erent experimental settings in detail.

adaptation between di   erent corpora

this is the most common setting of id20 in nlp applications. finkel

and manning (2009) experimented adaptation between conll data and muc

data. the main domain di   erence is that the    rst dataset is in british english

while the latter is in american english. arnold, nallapati, and cohen (2008) used

muc data and cspace personal email data as two separate domains for tagging

person names. the gap between muc and cspace is much larger than that

between conll and muc. chelba and acero (2006) studied id20

for automatic capitalization from wall street journal dataset to broadcast news

dataset.

the key problem of this setting is that the task de   nitions di   er in di   erent

datasets. taking the ner task as an example, the de   nitions of entity boundaries

and types di   er across domains. therefore the task in the target domain is limited

42

setting

application

challenge

suitable algorithm

id39

(1)same entity are represented by dif-

finkel and manning (2009)

di   erent corpora

ferent words, (2) contexts/vocabulary

arnold, nallapati, and cohen

di   erence

(2008)

id52

(1)

homonym

(2)

con-

daum  e iii (2007) jiang and

texts/vocabulary di   erence

zhai (2007b) blitzer, mcdon-

ald, and pereira (2006)

automatic capitalization

contexts and vocabulary di   erences

chelba and acero (2006)

daum  e iii (2007)

id51 sense priori distribution di   erence

chan and ng (2006)

general to speci   c

machine translation

(1) misleading instances in other do-

amittai et al. (2011)

mains (2) model complexity

sub-topics of the

id39

(1)same entity are represented by dif-

daum  e iii

(2007) wu et

same corpus

ferent words (2) contexts/vocabulary

al.

(2009) jiang and zhai

di   erence

(2007b)

id31

di   erent aspects of opinions

blitzer, dredze, and pereira

cross-lingual

id31

translation quality

xu, xu, and wang (2011)

(2007) blitzer, dredze, and

pereira (2007)

table 4: application summarization

43

to the overlapped entity types. for instance, arnold, nallapati, and cohen (2008)

restricted the task as    nding person name mentions when performing adaptation

between muc and cspace.

adaptation between topics of the same corpus

some corpora can be divided by genres and topics. for instance: given that genes

and proteins associated with di   erent organisms have various spellings and occur in

quite di   erent contexts as well, jiang and zhai (2007b) split biocreative dataset

into 3 domains according to the organisms, namely    y, mouse and yeast. daum  e

iii (2007) did experiments with the 6 subsets of ace 2005 corpus, namely bc, bn,

cts, nw, wl and un. wu et al. (2009) manually classi   ed the documents in ace

2005 corpus into 4 topics: military operations, political relationship or politicians,

terrorism-related, and all others. blitzer, dredze, and pereira (2007) and glorot,

bordes, and bengio (2011) considered customer reviews of di   erent products in a

large id31 corpus as di   erent domains.

an advantage of the adaptation between subsets of the same corpus is that the

task de   nition is identical. for example, in the ner task, the de   nition of entity

types and boundaries is identical, therefore the experiments can be done in all

types of names. in addition, the split according to genres and/or topics is natural

in practical scenarios. the di   erence and similarity between domains are more

clear compared to the    rst type of setup. for example, in the id31

task, reviews about di   erent tasks are based on di   erent aspects. for instance, in

the reviews of kitchen applications, the words such as malfunctioning or reliable

are about subjective opinions. while in the domain of dvd, reviews may contain

words like horri   c or thrilling (glorot, bordes, and bengio, 2011).

a potential problem of this setup is that the data size of both training and

44

test data is probably not big enough to obtain statistically signi   cant results.

for example, in the ace 2005 corpus, there are merely 39 documents in cts, 49

documents in un, and 60 documents in bc. this is too small compared to the

common benchmark for id39 experiments. for example in

the conll-2003 ner shared task, the english training and test data consists

of 946 and 231 articles respectively. this drawback makes it di   cult to show

statistically signi   cant improvement.

cross-lingual adaptation

id20 task in this category assumes that there is enough training

data in one language, and the goal is to build a classi   er for the dataset in another

language. (xu, xu, and wang, 2011) studied this setting for id31,

i.e. transfer the id31 model learned from the source language to

the target language. they performed experiments between english and chinese

for two sub-tasks: sentence-level opinionated sentence recognition, using dataset

of ntcir-7 multilingual opinion analysis tasks (seki et al., 2008), another is

document-level review polarity classi   cation using datasets in (wan, 2009). the

english sentences are translated to chinese by google translate as the source

domain training data. di   erent from other id20 settings, the do-

main di   erence in this scenario is mainly caused by machine translation errors.

therefore the empirical results are to some extent determined by the machine

translation system.

adaptation from general domain to speci   c domain

in the case that a large general-domain training data is available for a task, if we

want to build model for a particular domain, it   s preferable to use some labeled

instances which are similar to instances in the target domain, instead of using the

45

whole general-domain corpus. because the target domain should have its own dis-

tributions, instances in the general-domain corpus which are in di   erent domains

will harm the performance. therefore some id20 algorithms are

designed for this setting to improve the performance for the target domain. amit-

tai et al. (2011) proposed a solution for this setting based on instance selection.

in their work, the target domain data is the corpus from the international work-

shop on spoken language translation (iwslt) chinese-to-english dialog task,

while the general source domain is a combination of various datasets. compared

to other settings, the key assumption in this setting is that the instances from the

target domain are hidden in the large general corpus. therefore instance selection

methods such as amittai et al. (2011) is suitable to this setting.

46

references

amittai, axelrod, xiaodong he, jianfeng gao, and jianfeng gao. 2011. domain

adaptation via pseudo in-domain data selection.

in proceedings of the 2011

conference on empirical methods in natural language processing, pages 355   

362, july.

ando, rie kubota and tong zhang. 2005. a framework for learning predic-

tive structures from multiple tasks and unlabeled data. journal of machine

learning research, 6.

arnold, andrew, ramesh nallapati, and william w. cohen. 2008. exploiting

feature hierarchy for id21 in id39. in acl,

pages 245   253.

blei, david m., andrew y. ng, and michael i. jordan. 2003. latent dirichlet

allocation. journal of machine learning research, 3:993   1022.

blitzer, john, mark dredze, and fernando pereira. 2007. biographies, bollywood,

boom-boxes and blenders: id20 for sentiment classi   cation. in

acl.

blitzer, john, ryan t. mcdonald, and fernando pereira. 2006. domain adapta-

tion with structural correspondence learning. in emnlp, pages 120   128.

bunescu, razvan c. and marius pasca. 2006. using encyclopedic knowledge for

named entity disambiguation. in eacl.

chan, yee seng and hwee tou ng. 2006. estimating class priors in domain

adaptation for id51. in acl.

chelba, ciprian and alex acero. 2006. adaptation of maximum id178 capital-

izer: little data can help a lot. computer speech & language.

47

cohn, d., caruana r. and mccallum. 2003. a. semi-supervised id91 with

user feedback.technical report tr2003-1892, cornell university.

cucerzan, silviu.

2007. large-scale named entity disambiguation based on

wikipedia data. in emnlp-conll, pages 708   716.

dai, wenyuan, qiang yang, gui-rong xue, and yong yu. 2007. boosting for

id21. in icml, pages 193   200.

daum  e iii, hal. 2007. frustratingly easy id20. in acl.

deerwester, scott, susan t. dumais, george w. furnas, thomas k. landauer,

and richard harshman. 1990. indexing by latent semantic analysis. jour-

nal of the american society for information science,

41(6):391   407.

finkel, jenny rose and christopher d. manning. 2009. hierarchical bayesian

id20. in hlt-naacl, pages 602   610.

florian, radu, hany hassan, abraham ittycheriah, hongyan jing, nanda kamb-

hatla, xiaoqiang luo, nicolas nicolov, and salim roukos. 2004. a statistical

model for multilingual entity detection and tracking. in hlt-naacl.

freund, yoav and robert e. schapire. 1996. experiments with a new boosting

algorithm. in icml, pages 148   156.

gabrilovich, evgeniy and shaul markovitch. 2005. feature generation for text

categorization using world knowledge. in ijcai, pages 1048   1053.

gabrilovich, evgeniy and shaul markovitch. 2006. enhancing text categorization

with encyclopedic knowledge. in aaai, pages 1301   1306.

glorot, xavier, antoine bordes, and yoshua bengio. 2011. id20

for large-scale sentiment classi   cation: a deep learning approach. in icml.

48

guo, honglei, huijia zhu, zhili guo, xiaoxun zhang, xian wu, and zhong su.

2009. id20 with latent semantic association for named entity

recognition. in hlt-naacl, pages 281   289.

hofmann, thomas. 1999. probabilistic id45. in sigir, pages

50   57.

jiang, jing and chengxiang zhai. 2007a. instance weighting for domain adapta-

tion in nlp. in acl.

jiang, jing and chengxiang zhai. 2007b. a two-stage approach to domain adap-

tation for statistical classi   ers. in cikm, pages 401   410.

liu, chen, sai wu, shouxu jiang, and anthony tung. 2012. cross domain search

by exploiting wikipedia. in icde.

mihalcea, rada and andras csomai. 2007. wikify!: linking documents to ency-

clopedic knowledge. in cikm, pages 233   242.

miriam ayer, h. d. brunk, g. m. ewing w. t. reid and edward silverman. 1955.

an empirical distribution function for sampling with incomplete information.

in annals of mathematical statistics.

pan, sinno jialin and qiang yang. 2010. a survey on id21. ieee

trans. knowl. data eng., 22(10):1345   1359.

ratinov, lev-arie, dan roth, doug downey, and mike anderson. 2011. local and

global algorithms for disambiguation to wikipedia. in acl, pages 1375   1384.

seki, yohei, david kirk evans, lun-wei ku, le sun, hsin-hsi chen, and noriko

kando. 2008. overview of multilingual opinion analysis task at ntcir-7.

tan, songbo and xueqi cheng. 2009. improving scl model for sentiment-transfer

learning. in hlt-naacl (short papers), pages 181   184.

49

wan, xiaojun. 2009. co-training for cross-lingual sentiment classi   cation. in 47th

annual meeting of the acl and the 4th ijcnlp of the afnlp.

wu, dan, wee sun lee, nan ye, and hai leong chieu. 2009. domain adaptive

id64 for id39. in emnlp, pages 1523   1532.

xu, ruifeng, jun xu, and xiaolong wang. 2011. instance level id21

for cross lingual opinion analysis.

in proceedings of the 2nd workshop on

computational approaches to subjectivity and id31 (wassa

2.011), portland, oregon, june.

xue, gui-rong, wenyuan dai, qiang yang, and yong yu. 2008. topic-bridged

plsa for cross-domain text classi   cation. in sigir, pages 627   634.

50

