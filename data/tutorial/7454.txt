id110s:
independencies and id136

scott davies and andrew moore

note to other teachers and users of these slides. andrew and scott would 
be delighted if you found this source material useful in giving your own 
lectures. feel free to use these slides verbatim, or to modify them to fit 
your own needs. powerpoint originals are available. if you make use of a 
significant portion of these slides in your own lecture, please include this 
message, or the following link to the source repository of andre w   s 
tutorials: http://www.cs.cmu.edu/~awm/tutorials . comments and 
corrections gratefully received. 

what independencies does a bayes net model?

    in order for a id110 to model a 

id203 distribution, the following must be true by 
definition:

each variable is conditionally independent of all its non-
descendants in the graph given the value of all its parents.

    this implies

xp

( k

1

x

n

)

= n

=
1

i

xp

(

i

|

parents

(

x

))

i

    but what else does it imply?

1

(cid:213)
what independencies does a bayes net model?

    example:

z

y

x

given y, does learning the value of z tell us

nothing new about x? 

i.e., is p(x|y, z) equal to p(x | y)?

yes.  since we know the value of all of x   s 

parents (namely, y), and z is not a 
descendant of x, x is conditionally 

independent of z.

also, since independence is symmetric, 

p(z|y, x) = p(z|y).

quick proof that independence is symmetric

    assume: p(x|y, z) = p(x|y)
    then:

,
yxzp

(

|

=

)

zpzyxp
(

(

|

)
,
yxp

(

,

)

)

(bayes   s rule)

|

(

zpzyxpzyp=
(

,
)
ypyxp
)(

(
|

)
(

|
)

)

(chain rule)

)

(

zpyxpzyp=
(

|
)
ypyxp
)(

(
|

(

)

|

)

(by assumption)

=

zpzyp
(

(

|
)
)(
yp

)

=

yzp

(

|

)

(bayes   s rule)

2

what independencies does a bayes net model?

    let i<x,y,z> represent x and z being conditionally 

independent given y.

y

x

z

    i<x,y,z>?  yes, just as in previous example: all x   s 

parents given, and z is not a descendant.

what independencies does a bayes net model?

u

z

x

v

    i<x,{u},z>?  no.
    i<x,{u,v},z>?  yes.
    maybe i<x, s, z> iff s acts a cutset between x and z 

in an undirected version of the graph   ?

3

things get a little more confusing

x

z

y

    x has no parents, so we   re know all its parents    

values trivially

    z is not a descendant of x
    so, i<x,{},z>, even though there   s a undirected path 

from x to z through an unknown variable y.

    what if we do know the value of y, though?  or one 

of its descendants?

the    burglar alarm    example

burglar

earthquake

alarm

phone call

    your house has a twitchy burglar alarm that is also 

sometimes triggered by earthquakes.

    earth arguably doesn   t care whether your house is 

currently being burgled

    while you are on vacation, one of your neighbors 

calls and tells you your home   s burglar alarm is 
ringing.  uh oh!

4

things get a lot more confusing

burglar

earthquake

alarm

phone call

    but now suppose you learn that there was a medium-sized 

earthquake in your neighborhood.  oh, whew!  probably not a 
burglar after all.

    earthquake    explains away    the hypothetical burglar.
    but then it must not be the case that 

i<burglar,{phone call}, earthquake>, even though
i<burglar,{}, earthquake>!

d-separation to the rescue

    fortunately, there is a relatively simple algorithm for 

determining whether two variables in a bayesian 
network are conditionally independent: d-separation.

    definition: x and z are d-separated by a set of 

evidence variables e iff every undirected path from x
to z is    blocked   , where a path is    blocked    iff one 
or more of the following conditions is true: ...

5

a path is    blocked    when...

    there exists a variable v on the path such that

    it is in the evidence set e
    the arcs putting v in the path are    tail-to-tail   

v

    or, there exists a variable v on the path such that

    it is in the evidence set e
    the arcs putting v in the path are    tail-to-head   

v

    or, ...

a path is    blocked    when     (the funky case)

        or, there exists a variable v on the path such that

    it is not in the evidence set e
    neither are any of its descendants
    the arcs putting v on the path are    head-to-head   

v

6

d-separation to the rescue, cont   d 

    theorem [verma & pearl, 1998]:

    if a set of evidence variables e d-separates x and 
z in a id110   s graph, then i<x, e, z>.
    d-separation can be computed in linear time using a 

depth-first-search-like algorithm.

    great!  we now have a fast algorithm for 

automatically inferring whether learning the value of 
one variable might give us any additional hints about 
some other variable, given what we already know. 
       might   : variables may actually be independent when they   re not d-

separated, depending on the actual probabilities involved

d-separation example

a

c

e

g

i

b

d

f

h

j

   i<c, {}, d>?
   i<c, {a}, d>?
   i<c, {a, b}, d>?
   i<c, {a, b, j}, d>?
   i<c, {a, b, e, j}, d>?

7

id110 id136

    id136: calculating p(x|y) for some variables or 

sets of variables x and y.

    id136 in id110s is #p-hard!

inputs: prior probabilities of .5

i1

i2

i3

i4

i5

reduces to

how many satisfying assignments? 

o

p(o) must be

(#sat. assign.)*(.5^#inputs)

id110 id136

    but   id136 is still tractable in some cases.
    let   s look a special class of networks: trees / forests

in which each node has at most one parent.

8

decomposing the probabilities

    suppose we want p(xi | e) where e is some set of 

evidence variables.

    let   s split e into two parts:

    ei

- is the part consisting of assignments to variables in the 

subtree rooted at xi
+ is the rest of it

    ei

xi

decomposing the probabilities, cont   d

exp

(

|

i

=

)

eexp

(

,

|

i

i

+
i

)

xi

9

-
decomposing the probabilities, cont   d

=

|

i
(

)
exp
|

+
(
eexp
i
+
expexep
i

|
)
(
+
|
eep
i

(
+
i

,
|

=

)

(

,

i

i

i

i

)
)

xi

decomposing the probabilities, cont   d

+
i
+
i

)
)

xi

=

|

exp
)
|

(
eexp
expexep

i
(

,
|

i

i

(
+
i

i

,

)
eep
|

i

(

)
+
expxep
i

(

)

|
(
i
eep
(

|

+
i

i

|
(
+
i
|
)

=

=

)

10

-
-
-
-
-
-
-
-
decomposing the probabilities, cont   d

+
i
+
i

)
)

xi

=

|

exp
)
|

(
eexp
expexep

i
(

,
|

(
+
i

i

i

i

,

)
|
eep

(

)
+
expxep
i

(

i

)

)

(
|
i
eep
(
i
x
(?)
)

|
x

i

i

+
i

(ap

|
(
+
i
|
)

=

=

=

is a constant independent of xi

where:
    a 
    p (xi) = p(xi |ei
+)
    l (xi) = p(ei
-| xi)

using the decomposition for id136

    we can use this decomposition to do id136 as 
follows.  first, compute l (xi) = p(ei
-| xi) for all xi
recursively, using the leaves of the tree as the base 
case.

    if xi is a leaf:

    if xi is in e: l (xi) = 1 if xi matches e, 0 otherwise
    if xi is not in e: ei

- is the null set, so 

p(ei

-| xi) = 1 (constant)

11

-
-
-
-
-
quick aside:    virtual evidence   

    for theoretical simplicity, but without loss of 

generality, let   s assume that all variables in e (the 
evidence set) are leaves in the tree.

    why can we do this wlog:
equivalent to

xi

xi

observe xi

xi   

observe xi   

where p(xi   | xi) =1 if xi   =xi, 0 otherwise 

calculating l (xi) for non-leaves

    suppose xi has one child, xc. 

xi

    then:
x
(?

=

)

i

ep

(

i

|

x

)

i

=

xc

12

-
calculating l (xi) for non-leaves

    suppose xi has one child, xc. 

xi

    then:

xc

(?

x

)

i

=

ep

(

i

|

x

)

i

=

ep

(

i

,

x

c

=

xj
|

)

i

j

calculating l (xi) for non-leaves

    suppose xi has one child, xc. 

xi

    then:

xc

(?

x

)

i

=

ep

(

i

|

x

)

i

=

ep

(

i

,

x

c

=

xj
|

)

i

=

j

xp

(

c

=

j
epxj
|

(

)

i

|

i

xx

,

i

=

j

)

c

13

(cid:229)
-
-
(cid:229)
(cid:229)
-
-
-
calculating l (xi) for non-leaves

    suppose xi has one child, xc. 

xi

    then:

xc

(?

x

)

i

=

ep

(

i

|

x

)

i

=

ep

(

i

,

x

c

=

xj
|

)

i

=

=

=

j

j

j

xp

(

xp

(

xp

(

c

c

c

=

=

=

j
epxj
|

(

)

i

epxj
|

(

)

i

|

|

i

i

xx

,

i

=

j

)

c

x

c

=

j

)

xj
|

(?)

x

i

c

=

j

)

calculating l (xi) for non-leaves

    now, suppose xi has a set of children, c.
    since xi d-separates each of its subtrees, the 

contribution of each subtree to l (xi) is independent:

(?

x

)

i

=

xep

(

|

i

=

)

i

(?
j

x

i

)

cx

j

=

cx

j

x

j

xxp

(

|

j

?()

x

)

j

i

where l

j(xi) is the contribution to p(ei

-| xi) of the part of 
the evidence lying in the subtree rooted at one of xi   s
children xj.

14

(cid:229)
(cid:229)
(cid:229)
(cid:229)
-
-
-
-
(cid:213)
(cid:229)
(cid:213)
  
  
-
  
  
  
  
  
  
  
  
we are now l -happy

    so now we have a way to recursively compute all the 
l (xi)   s, starting from the root and using the leaves as 
the base case.

    if we want, we can think of each node in the network 

as an autonomous processor that passes a little    l
message    to its parent.

the other half of the problem

    remember, p(xi|e) = a

(xi)l (xi).  now that we have 

all the l (xi)   s, what about the p (xi)   s?

p (xi) = p(xi |ei

+).

    what about the root of the tree, xr?  in that case, er
+ 
is the null set, so p (xr) = p(xr).  no sweat.  since we 
also know l (xr), we can compute the final p(xr).

    so for an arbitrary xi with parent xp, let   s inductively 
assume we know p (xp) and/or p(xp|e).  how do we 
get p (xi)?

15

l
l
l
l
l
l
p
computing p (xi)

(p

x

)

i

=

xp

xi

computing p (xi)

(p

x

)

i

=

xp

xi

exp
i

(

|

i

+)

=

exp

(

|

i

=

+
i

)

j

xxp

(

,

i

=

p

ej
|

+
i

)

16

(cid:229)
computing p (xi)

(p

x

)

i

=

exp

(

|

i

=

+
i

)

xxp

(

|

i

=

p

=

j

xp

xi

xxp

(

,

i

=

p

ej
|

+
i

)

j
)

xpej
,

(

+
i

=

p

ej
|

+
i

)

computing p (xi)

(p

x

)

i

=

exp

(

|

i

=

+
i

)

xxp

(

,

i

=

p

ej
|

+
i

)

xp

xi

=

=

j

j

xxp

(

|

i

xxp

(

|

i

=

=

p

p

j
)

+
i

xpej
,

(

=

ej
|

+
i

)

p

xpj
)

(

p

=

ej
|

+
i

)

17

(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
computing p (xi)

(p

x

)

i

=

+
exp
i

(

|

i

=

)

xxp

(

,

i

=

p

+
ej
|
i

)

xp

xi

=

=

=

j

j

j

xxp

(

|

i

xxp

(

|

i

xxp

(

|

i

p

p

p

j
)

+
i

xpej
,

(

=

p

+
ej
|
i

)

xpj
)

(

p

=

+
ej
|
i

)

=

=

=

j

)

xp
(
(?
i

p
x

=

p

)

ej
|
=
j
)

computing p (xi)

(p

x

)

i

=

+
exp
i

(

|

i

=

)

xxp

(

,

i

=

+
ej
|
i

)

p

j
()

xpej
,

+
i

=

p

+
ej
|
i

)

xp

xi

=

=

=

=

j

j

j

j

xxp

(

|

i

xxp

(

|

i

xxp

(

|

i

xxp

(

|

i

=

=

=

=

p

p

p

p

xpj
)

(

p

=

+
ej
|
i

)

j

)

xp
(
(?
i
x

=

p
=

p
x

p

)

ej
|
=
j
)

j

)

j

(p)
i

where p

i(xp) is defined as 

|

exp
(
(?
)
i

p
x

p

)

18

(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
we   re done.  yay!  

    thus we can compute all the p (xi)   s, and, in turn, all 

the p(xi|e)   s.

    can think of nodes as autonomous processors passing 

l and p messages to their neighbors

conjunctive queries

    what if we want, e.g., p(a, b | c) instead of just 

marginal distributions p(a | c) and p(b | c)?

    just use chain rule:

    p(a, b | c) = p(a | c) p(b | a, c)
    each of the latter probabilities can be computed 

using the technique just discussed.

19

l
l
l
l
l
l
p
p
p
p
p
p
polytrees

    technique can be generalized to polytrees: 

undirected versions of the graphs are still trees, but 
nodes can have more than one parent 

dealing with cycles

    can deal with undirected cycles in graph by

    id91 variables together

b

a

d

c

a

bc

d

    conditioning

set to 0

set to 1

20

join trees

    arbitrary id110 can be transformed via 
some evil graph-theoretic magic into a join tree in 
which a similar method can be employed.

b

e

a

d

f

c

g

abc

bcd

bcd

df

in the worst case the join tree nodes must take on exponentially
many combinations of values, but often works well in practice

21

