bayes nets for representing 

and reasoning about 

uncertainty
andrew w. moore
associate professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

copyright    2001, andrew w. moore

oct 15th, 2001

what we   ll discuss

    recall the numerous and dramatic benefits 

of joint distributions for describing uncertain 
worlds

    reel with terror at the problem with using 

joint distributions

    discover how bayes net methodology 
allows us to built joint distributions in 
manageable chunks

    discover there   s still a lurking problem   
       start to solve that problem

copyright    2001, andrew w. moore

bayes nets: slide 2

1

why this matters

    in andrew   s opinion, the most important 

technology in the machine learning / ai field 
to have emerged in the last 10 years.

    a clean, clear, manageable language and 

methodology for expressing what you   re 
certain and uncertain about

    already, many practical applications in 

medicine, factories, helpdesks:

p(this problem | these symptoms)
anomalousness of this observation
choosing next diagnostic test | these observations

copyright    2001, andrew w. moore

bayes nets: slide 3

why this matters

    in andrew   s opinion, the most important 

technology in the machine learning / ai field 
to have emerged in the last 10 years.

active data 
    a clean, clear, manageable language and 
collection

methodology for expressing what you   re 
certain and uncertain about

id136

    already, many practical applications in 

medicine, factories, helpdesks:

anomaly 
detection

p(this problem | these symptoms)
anomalousness of this observation
choosing next diagnostic test | these observations

copyright    2001, andrew w. moore

bayes nets: slide 4

2

ways to deal with uncertainty
    three-valued logic: true / false / maybe
    fuzzy logic (truth values between 0 and 1)
    non-monotonic reasoning (especially 

focused on penguin informatics)

    dempster-shafer theory (and an extension 

known as quasi-bayesian theory)

    possibabilistic logic
    id203

copyright    2001, andrew w. moore

bayes nets: slide 5

discrete random variables

    a is a boolean-valued random variable if a 
denotes an event, and there is some degree 
of uncertainty as to whether a occurs.

    examples
    a = the us president in 2023 will be male
    a = you wake up tomorrow with a headache
    a = you have ebola

copyright    2001, andrew w. moore

bayes nets: slide 6

3

probabilities

    we write p(a) as    the fraction of possible 

worlds in which a is true   

    we could at this point spend 2 hours on the 

philosophy of this.

    but we won   t.

copyright    2001, andrew w. moore

bayes nets: slide 7

visualizing a

event space of 
all possible 
worlds

its area is 1

worlds in which 
a is true

p(a) = area of
reddish oval

worlds in which a is false

copyright    2001, andrew w. moore

bayes nets: slide 8

4

interpreting the axioms

    0 <= p(a) <= 1
    p(true) = 1
    p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

the area of a can   t get 
any smaller than 0

and a zero area would 
mean no world could 
ever have a true 

copyright    2001, andrew w. moore

bayes nets: slide 9

interpreting the axioms

    0 <= p(a) <= 1
    p(true) = 1
    p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

the area of a can   t get 
any bigger than 1

and an area of 1 would 
mean all worlds will have 
a true 

copyright    2001, andrew w. moore

bayes nets: slide 10

5

interpreting the axioms

    0 <= p(a) <= 1
    p(true) = 1
    p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

a

b

copyright    2001, andrew w. moore

bayes nets: slide 11

interpreting the axioms

    0 <= p(a) <= 1
    p(true) = 1
    p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

a

b

p(a or b)

p(a and b)

b

simple addition and subtraction

copyright    2001, andrew w. moore

bayes nets: slide 12

6

these axioms are not to be 

trifled with

    there have been attempts to do different 

methodologies for uncertainty

    fuzzy logic
    three-valued logic
    dempster-shafer
    non-monotonic reasoning

    but the axioms of id203 are the only 

system with this property: 
if you gamble using them you can   t be unfairly exploited by 
an opponent using some other system [di finetti 1931]

copyright    2001, andrew w. moore

bayes nets: slide 13

theorems from the axioms

    0 <= p(a) <= 1, p(true) = 1, p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)
from these we can prove:
p(not a) = p(~a) = 1-p(a)

    how?

copyright    2001, andrew w. moore

bayes nets: slide 14

7

side note

   

i am inflicting these proofs on you for two 
reasons:
1. these kind of manipulations will need to be 
second nature to you if you use probabilistic 
analytics in depth

2. suffering is good for you

copyright    2001, andrew w. moore

bayes nets: slide 15

another important theorem

    0 <= p(a) <= 1, p(true) = 1, p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)
from these we can prove:
p(a) = p(a ^ b) + p(a ^ ~b)

    how?

copyright    2001, andrew w. moore

bayes nets: slide 16

8

id155

    p(a|b) = fraction of worlds in which b is true 

that also have a true

f

h

h =    have a headache   
f =    coming down with flu   

p(h) = 1/10
p(f) = 1/40
p(h|f) = 1/2

   headaches are rare and flu 
is rarer, but if you   re coming 
down with    flu there   s a 50-
50 chance you   ll have a 
headache.   

copyright    2001, andrew w. moore

bayes nets: slide 17

id155

f

h

p(h|f) = fraction of flu-inflicted 
worlds in which you have a 
headache

= #worlds with flu and headache

------------------------------------

#worlds with flu

h =    have a headache   
f =    coming down with flu   

p(h) = 1/10
p(f) = 1/40
p(h|f) = 1/2

= area of    h and f    region

------------------------------
area of    f    region

= p(h ^ f)
-----------
p(f) 

copyright    2001, andrew w. moore

bayes nets: slide 18

9

definition of id155

p(a ^ b) 
p(a|b)  =  -----------
p(b) 

corollary: the chain rule

p(a ^ b) = p(a|b) p(b) 

copyright    2001, andrew w. moore

bayes nets: slide 19

bayes rule

p(a ^ b)     p(a|b) p(b)
p(b|a) = ----------- = ---------------

p(a)             p(a)

this is bayes rule

bayes, thomas (1763) an essay 
towards solving a problem in the doctrine 
of chances. philosophical transactions 
of the royal society of london, 53:370-
418

copyright    2001, andrew w. moore

bayes nets: slide 20

10

using bayes rule to gamble

r  r  b   b

$1.00

r  b   b

the    win    envelope 

has a dollar and four 
beads in it

the    lose    envelope 

has three beads and 
no money

trivial question: someone draws an envelope at random and offers to 
sell it to you. how much should you pay?

copyright    2001, andrew w. moore

bayes nets: slide 21

using bayes rule to gamble

$1.00

the    win    envelope 

has a dollar and four 
beads in it

the    lose    envelope 

has three beads and 
no money

interesting question: before deciding, you are allowed to see one bead 
drawn from the envelope.

suppose it   s black: how much should you pay? 
suppose it   s red: how much should you pay?

copyright    2001, andrew w. moore

bayes nets: slide 22

11

calculation   

$1.00

copyright    2001, andrew w. moore

bayes nets: slide 23

multivalued random variables
    suppose a can take on more than 2 values
    a is a random variable with arity k if it can 
take on exactly one value out of {v1,v2, .. vk}

    thus   

vap
(
i
vap
(
1

va
=   =
va
=   =

j

2

i
 if 0)
=
kva
=   

j
   
1)
=

copyright    2001, andrew w. moore

bayes nets: slide 24

12

an easy fact about multivalued random variables:

    using the axioms of id203   

0 <= p(a) <= 1, p(true) = 1, p(false) = 0
p(a or b) = p(a) + p(b) - p(a and b)

    and assuming that a obeys   

vap
(
i
vap
(
1

va
=   =
va
=   =

j

2

i
 if 0)
=
kva
=   

j
   
1)
=

   

it   s easy to prove that

vap
(
1

va
=   =

2

va
=   
i

)

=

i

   

j

1
=

vap
(
=

)

j

copyright    2001, andrew w. moore

bayes nets: slide 25

an easy fact about multivalued random variables:

    using the axioms of id203   

0 <= p(a) <= 1, p(true) = 1, p(false) = 0
p(a or b) = p(a) + p(b) - p(a and b)

    and assuming that a obeys   

vap
(
i
vap
(
1

va
=   =
va
=   =

j

2

i
 if 0)
=
kva
=   

j
   
1)
=

   

it   s easy to prove that

vap
(
1

va
=   =

2

va
=   
i

)

=

    and thus we can prove

=   

jvap
(

1)
=

k

j

1
=

i

   

j

1
=

vap
(
=

)

j

copyright    2001, andrew w. moore

bayes nets: slide 26

13

another fact about multivalued random variables:

    using the axioms of id203   

0 <= p(a) <= 1, p(true) = 1, p(false) = 0
p(a or b) = p(a) + p(b) - p(a and b)

    and assuming that a obeys   

vap
(
i
vap
(
1

va
=   =
va
=   =

j

2

i
 if 0)
=
kva
=   

j
   
1)
=

   

it   s easy to prove that
bp
(

va
1

   

[

va
=   =

va
=   
i

])

=

2

i

   

j

1
=

vabp
(
=   

)

j

copyright    2001, andrew w. moore

bayes nets: slide 27

another fact about multivalued random variables:

    using the axioms of id203   

0 <= p(a) <= 1, p(true) = 1, p(false) = 0
p(a or b) = p(a) + p(b) - p(a and b)

    and assuming that a obeys   

vap
(
i
vap
(
1

va
=   =
va
=   =

j

2

i
 if 0)
=
kva
=   

j
   
1)
=

   

it   s easy to prove that
bp
(

va
1

   

[

va
=   =

2

    and thus we can prove
bp
(

)

=

k

   

j

1
=

va
=   
i

])

=

vabp
(
=   

)

j

i

   

j
jvabp
(

=   

1
=
)

copyright    2001, andrew w. moore

bayes nets: slide 28

14

more general forms of bayes rule

bap
)|
(

=

apabp
(
)()
)()
|~(

|
+

apabpapabp
(
)

(~)

|

xbap
(

   

|

)

=

xapxabp
(

   

|

()
   
xbp
(
)
   

)

copyright    2001, andrew w. moore

bayes nets: slide 29

more general forms of bayes rule

bvap
(
)|

=

i

=

i

|

=

()

vapvabp
(
)
=
i
an
   
vapvabp
(
=
k

()

=

|

k

k

1
=

)

copyright    2001, andrew w. moore

bayes nets: slide 30

15

useful easy-to-prove facts

1)
=

|

|

)
  +

bapbap
(
(
=   

k bvap
(

an

|

1)
=

k

1
=

copyright    2001, andrew w. moore

bayes nets: slide 31

the joint distribution

recipe for making a joint distribution 

of m variables:

example: boolean 
variables a, b, c

copyright    2001, andrew w. moore

bayes nets: slide 32

16

the joint distribution

recipe for making a joint distribution 

of m variables:

1. make a truth table listing all 

combinations of values of your 
variables (if there are m boolean 
variables then the table will have 
2m rows).

a
0
0
0
0
1
1
1
1

example: boolean 
variables a, b, c
b
0
0
1
1
0
0
1
1

c
0
1
0
1
0
1
0
1

copyright    2001, andrew w. moore

bayes nets: slide 33

the joint distribution

recipe for making a joint distribution 

of m variables:

1. make a truth table listing all 

combinations of values of your 
variables (if there are m boolean 
variables then the table will have 
2m rows).

2. for each combination of values, 

say how probable it is.

a
0
0
0
0
1
1
1
1

example: boolean 
variables a, b, c
b
prob
0
0.30
0
0.05
1
0.10
1
0.05
0
0.05
0
0.10
1
0.25
1
0.10

c
0
1
0
1
0
1
0
1

copyright    2001, andrew w. moore

bayes nets: slide 34

17

the joint distribution

recipe for making a joint distribution 

of m variables:

1. make a truth table listing all 

combinations of values of your 
variables (if there are m boolean 
variables then the table will have 
2m rows).

2. for each combination of values, 

say how probable it is.
if you subscribe to the axioms of 
id203, those numbers must 
sum to 1.

3.

example: boolean 
variables a, b, c
b
prob
0
0.30
0
0.05
1
0.10
1
0.05
0
0.05
0
0.10
1
0.25
1
0.10

c
0
1
0
1
0
1
0
1

a
0
0
0
0
1
1
1
1

a

0.05

0.25

0.05

0.10
0.10

0.05

c

0.10

b

0.30

copyright    2001, andrew w. moore

bayes nets: slide 35

using the 

joint

once you have the jd you 
can ask for the id203 of 
any logical expression 
involving your attribute

ep
(

)

   =

p
row(
e
 
 
matching
rows

)

copyright    2001, andrew w. moore

bayes nets: slide 36

18

using the 

joint

p(poor male) = 0.4654

ep
(

)

   =

p
row(
e
 
 
matching
rows

)

copyright    2001, andrew w. moore

bayes nets: slide 37

using the 

joint

p(poor) = 0.7604

ep
(

)

   =

p
row(
e
 
 
matching
rows

)

copyright    2001, andrew w. moore

bayes nets: slide 38

19

id136 
with the 
joint

eep
(

|

1

)

=

2

)

2

ep
(
   
1
ep
(

2

e
)

=

)

   
p
row(
e
e
and
 
 
 
matching
rows
 1
   
p
row(
e
 
rows
matching
 

)

 2

2

copyright    2001, andrew w. moore

bayes nets: slide 39

id136 
with the 
joint

eep
(

|

1

)

=

2

)

2

ep
(
   
1
ep
(

2

e
)

=

)

   
p
row(
e
e
and
 
 
 
matching
rows
 1
   
p
row(
e
 
rows
matching
 

)

 2

2

p(male | poor) = 0.4654 / 0.7604 = 0.612  

copyright    2001, andrew w. moore

bayes nets: slide 40

20

joint distributions
    bad news

    good news

once you have a joint 
distribution, you can 
ask important 
questions about 
stuff that involves a 
lot of uncertainty

impossible to create 
for more than about 
ten attributes 
because there are 
so many numbers 
needed when you 
build the damn 
thing.

copyright    2001, andrew w. moore

bayes nets: slide 41

using fewer numbers

suppose there are two events:

    m: manuela teaches the class (otherwise it   s andrew)
    s: it is sunny

the joint p.d.f. for these events contain four entries.

if we want to build the joint p.d.f. we   ll have to invent those 

four numbers.  or will we??
    we don   t have to specify with bottom level conjunctive 

events such as p(~m^s) if   

       instead it may sometimes be more convenient for us 

to specify things like: p(m), p(s).

but just p(m) and  p(s) don   t derive the joint distribution.  so

you can   t answer all questions.

copyright    2001, andrew w. moore

bayes nets: slide 42

21

using fewer numbers

suppose there are two events:

    m: manuela teaches the class (otherwise it   s andrew)
    s: it is sunny

the joint p.d.f. for these events contain four entries.

if we want to build the joint p.d.f. we   ll have to invent those 

four numbers.  or will we??
    we don   t have to specify with bottom level conjunctive 

events such as p(~m^s) if   

       instead it may sometimes be more convenient for us 

but just p(m) and  p(s) don   t derive the joint distribution.  so

to specify things like: p(m), p(s).
make?

what extra assumption can you 

you can   t answer all questions.

copyright    2001, andrew w. moore

bayes nets: slide 43

independence

   the sunshine levels do not depend on and do not 
influence who is teaching.   

this can be specified very simply:

p(s     m) = p(s)

this is a powerful statement!

it required extra domain knowledge. a different kind 
of knowledge than numerical probabilities.  it needed 
an understanding of causation.

copyright    2001, andrew w. moore

bayes nets: slide 44

22

independence

from  p(s     m) = p(s), the rules of id203 imply:  (can 
you prove these?)

    p(~s     m) = p(~s)

    p(m     s) = p(m)

    p(m ^ s) = p(m) p(s)

    p(~m ^ s) = p(~m) p(s), (pm^~s) = p(m)p(~s),

p(~m^~s) = p(~m)p(~s)

copyright    2001, andrew w. moore

bayes nets: slide 45

independence

from  p(s     m) = p(s), the rules of id203 imply:  (can 
you prove these?)

and in general:

    p(~s     m) = p(~s)

    p(m     s) = p(m)

p(m=u ^ s=v) = p(m=u) p(s=v)
for each of the four combinations of

    p(m ^ s) = p(m) p(s)

u=true/false
v=true/false

    p(~m ^ s) = p(~m) p(s), (pm^~s) = p(m)p(~s),

p(~m^~s) = p(~m)p(~s)

copyright    2001, andrew w. moore

bayes nets: slide 46

23

independence

we   ve stated:

p(m) = 0.6
p(s) = 0.3
p(s     m) = p(s)

from these statements, we can 
derive the full joint pdf.

m

s

prob

t
t
f
f

t
f
t
f

and since we now have the joint pdf, we can make 
any queries we like.

copyright    2001, andrew w. moore

bayes nets: slide 47

a more interesting case

    m : manuela teaches the class
    s : it is sunny
    l : the lecturer arrives slightly late.

assume both lecturers are sometimes delayed by bad 
weather. andrew is more likely to arrive late than manuela.

copyright    2001, andrew w. moore

bayes nets: slide 48

24

a more interesting case

    m : manuela teaches the class
    s : it is sunny
    l : the lecturer arrives slightly late.

assume both lecturers are sometimes delayed by bad 
weather. andrew is more likely to arrive late than manuela.

let   s begin with writing down knowledge we   re happy about:

p(s     m) = p(s),  p(s) = 0.3,   p(m) = 0.6

lateness is not independent of the weather and is not 

independent of the lecturer.  

copyright    2001, andrew w. moore

bayes nets: slide 49

a more interesting case

    m : manuela teaches the class
    s : it is sunny
    l : the lecturer arrives slightly late.

assume both lecturers are sometimes delayed by bad 
weather. andrew is more likely to arrive late than manuela.

let   s begin with writing down knowledge we   re happy about:

p(s     m) = p(s),  p(s) = 0.3,   p(m) = 0.6

lateness is not independent of the weather and is not 

independent of the lecturer.  

we already know the joint of s and m, so all we need now is

in the 4 cases of u/v = true/false.

p(l     s=u, m=v)

copyright    2001, andrew w. moore

bayes nets: slide 50

25

a more interesting case

    m : manuela teaches the class
    s : it is sunny
    l : the lecturer arrives slightly late.

assume both lecturers are sometimes delayed by bad 
weather. andrew is more likely to arrive late than manuela.

p(s     m) = p(s)
p(s) = 0.3
p(m) = 0.6

p(l     m ^ s) = 0.05
p(l     m ^ ~s) = 0.1
p(l     ~m ^ s) = 0.1
p(l     ~m ^ ~s) = 0.2

now we can derive a full joint 
p.d.f. with a    mere    six numbers 
instead of seven*

*savings are larger for larger numbers of variables.

copyright    2001, andrew w. moore

bayes nets: slide 51

a more interesting case

    m : manuela teaches the class
    s : it is sunny
    l : the lecturer arrives slightly late.

assume both lecturers are sometimes delayed by bad 
weather. andrew is more likely to arrive late than manuela.

p(s     m) = p(s)
p(s) = 0.3
p(m) = 0.6

p(l     m ^ s) = 0.05
p(l     m ^ ~s) = 0.1
p(l     ~m ^ s) = 0.1
p(l     ~m ^ ~s) = 0.2

question:  express

p(l=x ^ m=y ^ s=z)

in terms that only need the above 
expressions, where x,y and z may 
each be true or false.

copyright    2001, andrew w. moore

bayes nets: slide 52

26

a bit of notation

p(s     m) = p(s)
p(s) = 0.3
p(m) = 0.6

p(l     m ^ s) = 0.05
p(l     m ^ ~s) = 0.1
p(l     ~m ^ s) = 0.1
p(l     ~m ^ ~s) = 0.2

p(s)=0.3

s

m

p(m)=0.6

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

copyright    2001, andrew w. moore

bayes nets: slide 53

a bit of notation

p(s     m) = p(s)
p(s) = 0.3
p(m) = 0.6

p(l     m ^ s) = 0.05
p(l     m ^ ~s) = 0.1
p(l     ~m ^ s) = 0.1
p(l     ~m ^ ~s) = 0.2

read the absence of an arrow 
between s and m to mean    it 
would not help me predict m if i 

knew the value of s   

p(s)=0.3

s

m

p(m)=0.6

t

h
o
r
o
u
g
h
y
 
f

l

i

t
h
s
 
k
n
d
o

 

i

f
 
s
t
u

f
f
 

w

i
l
l
 

b
e

 

o
r
m
a

l
i

z
e
d

 
l

t

a
e
r

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

read the two arrows into l to 
mean that if i want to know the 
value of l it may help me to 

know m and to know s.

copyright    2001, andrew w. moore

bayes nets: slide 54

27

an even cuter trick

suppose we have these three events:
    m : lecture taught by manuela
    l : lecturer arrives late
    r : lecture concerns robots
suppose:
    andrew has a higher chance of being late than manuela.
    andrew has a higher chance of giving robotics lectures.
what kind of independence can we find?

how about:

    p(l     m) = p(l) ?
    p(r     m) = p(r) ?
    p(l     r) = p(l) ?

copyright    2001, andrew w. moore

bayes nets: slide 55

conditional independence
once you know who the lecturer is, then whether 
they arrive late doesn   t affect whether the lecture 
concerns robots.

p(r     m,l) = p(r     m) and
p(r     ~m,l) = p(r     ~m)

we express this in the following way:

   r and l are conditionally independent given m   

..which is also 
notated by the 
following diagram.

copyright    2001, andrew w. moore

m

l

r

given knowledge of m, 
knowing anything else in 
the diagram won   t help 
us with l, etc.

bayes nets: slide 56

28

conditional independence formalized
r and l are conditionally independent given m if
for all x,y,z in {t,f}:

p(r=x     m=y ^ l=z) = p(r=x     m=y)

more generally:

let s1 and s2 and s3 be sets of variables.

set-of-variables s1 and set-of-variables s2 are 
conditionally independent given s3 if for all 
assignments of values to the variables in the sets,

p(s1   s assignments     s2   s assignments & s3   s assignments)= 

p(s1   s assignments     s3   s assignments)

copyright    2001, andrew w. moore

bayes nets: slide 57

example:

   shoe-size is conditionally independent of glove-size given 

height weight and age   
r and l are conditionally independent given m if
for all x,y,z in {t,f}:

forall s,g,h,w,a
p(r=x     m=y ^ l=z) = p(r=x     m=y)

means

p(shoesize=s|height=h,weight=w,age=a)

more generally:

p(shoesize=s|height=h,weight=w,age=a,glovesize=g)

let s1 and s2 and s3 be sets of variables.

=

set-of-variables s1 and set-of-variables s2 are 
conditionally independent given s3 if for all 
assignments of values to the variables in the sets,

p(s1   s assignments     s2   s assignments & s3   s assignments)= 

p(s1   s assignments     s3   s assignments)

copyright    2001, andrew w. moore

bayes nets: slide 58

29

example:

   shoe-size is conditionally independent of glove-size given 

height weight and age   
r and l are conditionally independent given m if
for all x,y,z in {t,f}:

forall s,g,h
p(r=x     m=y ^ l=z) = p(r=x     m=y)

p(shoesize=s|height=h)

does not mean

more generally:

p(shoesize=s|height=h, glovesize=g)

let s1 and s2 and s3 be sets of variables.

=

set-of-variables s1 and set-of-variables s2 are 
conditionally independent given s3 if for all 
assignments of values to the variables in the sets,

p(s1   s assignments     s2   s assignments & s3   s assignments)= 

p(s1   s assignments     s3   s assignments)

copyright    2001, andrew w. moore

bayes nets: slide 59

conditional 
independence

m

l

r

we can write down p(m).  and then, since we know 
l is only directly influenced by m, we can write 
down the values of p(l   m) and p(l   ~m) and know 
we   ve fully specified l   s behavior.  ditto for r.

p(m) = 0.6
p(l     m) = 0.085
p(l     ~m) = 0.17
p(r     m) = 0.3
p(r     ~m) = 0.6

   r and l conditionally 
independent given m   

copyright    2001, andrew w. moore

bayes nets: slide 60

30

conditional independence

m

l

r

conditional independence:
p(r   m,l) = p(r   m),
p(r   ~m,l) = p(r   ~m)

p(m) = 0.6
p(l     m) = 0.085
p(l     ~m) = 0.17
p(r     m) = 0.3
p(r     ~m) = 0.6
again, we can obtain any member of the joint 
prob dist that we desire:
p(l=x ^ r=y ^ m=z) =

copyright    2001, andrew w. moore

bayes nets: slide 61

assume five variables

t: the lecture started by 10:35
l: the lecturer arrives late
r: the lecture concerns robots
m: the lecturer is manuela
s: it is sunny

    t only directly influenced by l (i.e. t is 

conditionally independent of r,m,s given l)

    l only directly influenced by m and s (i.e. l is 

conditionally independent of r given m & s)

    r only directly influenced by m (i.e. r is 

conditionally independent of l,s, given m)

    m and s are independent

copyright    2001, andrew w. moore

bayes nets: slide 62

31

making a bayes net

t: the lecture started by 10:35
l: the lecturer arrives late
r: the lecture concerns robots
m: the lecturer is manuela
s: it is sunny

s

m

l

t

r

step one: add variables.
    just choose the variables you   d like to be included in the 

net.

copyright    2001, andrew w. moore

bayes nets: slide 63

making a bayes net

t: the lecture started by 10:35
l: the lecturer arrives late
r: the lecture concerns robots
m: the lecturer is manuela
s: it is sunny

s

m

l

t

r

step two: add links.
    the link structure must be acyclic.
   

if node x is given parents q1,q2,..qn you are promising 
that any variable that   s a non-descendent of x is 
conditionally independent of x given {q1,q2,..qn}

copyright    2001, andrew w. moore

bayes nets: slide 64

32

making a bayes net

p(s)=0.3

s

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

t

t: the lecture started by 10:35
l: the lecturer arrives late
r: the lecture concerns robots
m: the lecturer is manuela
s: it is sunny

m

p(m)=0.6

p(r   m)=0.3
p(r   ~m)=0.6

p(t   l)=0.3
p(t   ~l)=0.8

r

step three: add a id203 table for each node.
    the table for node x must list p(x|parent values) for each 

possible combination of parent values

copyright    2001, andrew w. moore

bayes nets: slide 65

making a bayes net

p(s)=0.3

s

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

t

t: the lecture started by 10:35
l: the lecturer arrives late
r: the lecture concerns robots
m: the lecturer is manuela
s: it is sunny

m

p(m)=0.6

p(r   m)=0.3
p(r   ~m)=0.6

p(t   l)=0.3
p(t   ~l)=0.8

r

    two unconnected variables may still be correlated
    each node is conditionally independent of all non-

descendants in the tree, given its parents.

    you can deduce many other conditional independence 

relations from a bayes net. see the next lecture.

copyright    2001, andrew w. moore

bayes nets: slide 66

33

bayes nets formalized
a bayes net (also called a belief network) is an 
augmented directed acyclic graph, represented by 
the pair v , e where:

    v is a set of vertices.
    e is a set of directed edges joining vertices.  no 

loops of any length are allowed.

each vertex in v contains the following information:

    the name of a random variable
    a id203 distribution table indicating how the 

id203 of this variable   s values depends on 
all possible combinations of parental values.

copyright    2001, andrew w. moore

bayes nets: slide 67

building a bayes net

1. choose a set of relevant variables.
2. choose an ordering for them
3. assume they   re called x1 .. xm (where x1 is the 

first in the ordering, x1  is the second, etc)

4. for i = 1 to m:

1. add the xi node to the network
2. set parents(xi ) to be a minimal subset of 

{x1   xi-1} such that we have conditional 
independence of xi and all other members of 
{x1   xi-1} given parents(xi )

3. define the id203 table of 

p(xi =k     assignments of parents(xi ) ).

copyright    2001, andrew w. moore

bayes nets: slide 68

34

example bayes net building
suppose we   re building a nuclear power station.
there are the following random variables:

grl : gauge reads low.
ctl : core temperature is low.
fg : gauge is faulty.
fa : alarm is faulty
as : alarm sounds

    if alarm working properly, the alarm is meant to 

sound if the gauge stops reading a low temp.

    if gauge working properly, the gauge is meant to 

read the temp of the core.

copyright    2001, andrew w. moore

bayes nets: slide 69

computing a joint entry
how to compute an entry in a joint distribution?
e.g: what is p(s ^ ~m ^ l ~r ^ t)?
p(m)=0.6

m

p(s)=0.3

s

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

t

p(t   l)=0.3
p(t   ~l)=0.8

r

p(r   m)=0.3
p(r   ~m)=0.6

copyright    2001, andrew w. moore

bayes nets: slide 70

35

computing with bayes net

p(s)=0.3

s

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

t

m

p(m)=0.6

p(r   m)=0.3
p(r   ~m)=0.6

p(t   l)=0.3
p(t   ~l)=0.8

r

p(t ^ ~r ^ l ^ ~m ^ s) =
p(t     ~r ^ l ^ ~m ^ s) * p(~r ^ l ^ ~m ^ s) = 
p(t     l) *  p(~r ^ l ^ ~m ^ s) =
p(t     l) *  p(~r     l ^ ~m ^ s) * p(l^~m^s) =
p(t     l) *  p(~r     ~m) * p(l^~m^s) =
p(t     l) *  p(~r     ~m) * p(l   ~m^s)*p(~m^s) =
p(t     l) *  p(~r     ~m) * p(l   ~m^s)*p(~m | s)*p(s) =
p(t     l) *  p(~r     ~m) * p(l   ~m^s)*p(~m)*p(s).

copyright    2001, andrew w. moore

bayes nets: slide 71

the general case

p(x1
=x1 ^ x2=x2 ^    .xn-1=xn-1 ^ xn=xn) =
p(xn=xn ^ xn-1=xn-1 ^    .x2=x2 ^ x1=x1) =
p(xn=xn     xn-1=xn-1 ^    .x2=x2 ^ x1=x1) * p(xn-1=xn-1 ^   . x2=x2 ^ x1=x1) =
p(xn=xn     xn-1=xn-1 ^    .x2=x2 ^ x1=x1) * p(xn-1=xn-1       . x2=x2 ^ x1=x1) *
p(xn-2=xn-2 ^   . x2=x2 ^ x1=x1) =

:
:
(
(
xp

n

=
   
i
1
=
=
n
   

i

1
=

=

x
i

i

) (
(

x

i

1
   

=

x
i

1
   

)

   

(
k

x

1

=

)
)
)

x
1

(
(
xp

i

=

)

x
i

assignment

 of s

(
parents

)
)

x

i

so any entry in joint pdf table can be computed. and so any 
id155 can be computed.

copyright    2001, andrew w. moore

bayes nets: slide 72

36

where are we now?
    we have a methodology for building bayes nets.
    we don   t require exponential storage to hold our id203 
table.  only exponential in the maximum number of parents 
of any node.

    we can compute probabilities of any given assignment of 

truth values to the variables.  and we can do it in time 
linear with the number of nodes.

    so we can also compute answers to any questions.

p(s)=0.3

s

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

t

m

p(m)=0.6

p(r   m)=0.3
p(r   ~m)=0.6

p(t   l)=0.3
p(t   ~l)=0.8

r

e.g. what could we do to compute p(r     t,~s)?

copyright    2001, andrew w. moore

bayes nets: slide 73

where are we now?
step 1: compute p(r ^ t ^ ~s)
    we have a methodology for building bayes nets.
    we don   t require exponential storage to hold our id203 
step 2: compute p(~r ^ t ^ ~s)
table.  only exponential in the maximum number of parents 
of any node.

step 3: return
    we can compute probabilities of any given assignment of 

p(r ^ t ^ ~s)

truth values to the variables.  and we can do it in time 
linear with the number of nodes.
-------------------------------------
p(r ^ t ^ ~s)+ p(~r ^ t ^ ~s)

    so we can also compute answers to any questions.

p(s)=0.3

s

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

t

m

p(m)=0.6

p(r   m)=0.3
p(r   ~m)=0.6

p(t   l)=0.3
p(t   ~l)=0.8

r

e.g. what could we do to compute p(r     t,~s)?

copyright    2001, andrew w. moore

bayes nets: slide 74

37

where are we now?
step 1: compute p(r ^ t ^ ~s)
sum of all the rows in the joint 
    we have a methodology for building bayes nets.
    we don   t require exponential storage to hold our id203 
step 2: compute p(~r ^ t ^ ~s)
table.  only exponential in the maximum number of parents 
of any node.
sum of all the rows in the joint 

step 3: return
    we can compute probabilities of any given assignment of 

that match ~r ^ t ^ ~s

that match r ^ t ^ ~s

p(r ^ t ^ ~s)

truth values to the variables.  and we can do it in time 
linear with the number of nodes.
-------------------------------------
p(r ^ t ^ ~s)+ p(~r ^ t ^ ~s)

    so we can also compute answers to any questions.

p(s)=0.3

s

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

t

m

p(m)=0.6

p(r   m)=0.3
p(r   ~m)=0.6

p(t   l)=0.3
p(t   ~l)=0.8

r

e.g. what could we do to compute p(r     t,~s)?

copyright    2001, andrew w. moore

bayes nets: slide 75

where are we now?
step 1: compute p(r ^ t ^ ~s)
sum of all the rows in the joint 
    we have a methodology for building bayes nets.
    we don   t require exponential storage to hold our id203 
step 2: compute p(~r ^ t ^ ~s)
table.  only exponential in the maximum number of parents 
of any node.
sum of all the rows in the joint 

step 3: return
    we can compute probabilities of any given assignment of 

that match ~r ^ t ^ ~s

that match r ^ t ^ ~s

4 joint computes

p(r ^ t ^ ~s)

truth values to the variables.  and we can do it in time 
linear with the number of nodes.
-------------------------------------
p(r ^ t ^ ~s)+ p(~r ^ t ^ ~s)

each of these obtained by 
    so we can also compute answers to any questions.
the    computing a joint 
p(m)=0.6
id203 entry    method of 
the earlier slides
p(r   m)=0.3
p(r   ~m)=0.6

p(s)=0.3

m

s

4 joint computes

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

t

p(t   l)=0.3
p(t   ~l)=0.8

r

e.g. what could we do to compute p(r     t,~s)?

copyright    2001, andrew w. moore

bayes nets: slide 76

38

the good news

we can do id136. we can compute any 

id155:
p( some variable     some other variable values )

eep
(

|

1

)

=

2

)

2

ep
(
   
1
ep
(

2

e
)

=

joint 

   
p
joint 
(
e
e
 
matching
and
 
 
entries
 1
   
p
joint 
(
e
matching
 
 
entries

 2

2

joint 

entry

)

entry

)

copyright    2001, andrew w. moore

bayes nets: slide 77

the good news

we can do id136. we can compute any 

id155:
p( some variable     some other variable values )

eep
(

|

1

)

=

2

)

2

ep
(
   
1
ep
(

2

e
)

=

joint 

   
p
joint 
(
e
e
 
matching
and
 
 
entries
 1
   
p
joint 
(
e
matching
 
 
entries

 2

2

joint 

entry

)

entry

)

suppose you have m binary-valued variables in your bayes 
net and expression e2 mentions k variables.
how much work is the above computation?

copyright    2001, andrew w. moore

bayes nets: slide 78

39

the sad, bad news

conditional probabilities by enumerating all matching entries 

in the joint are expensive:

exponential in the number of variables.

copyright    2001, andrew w. moore

bayes nets: slide 79

the sad, bad news

conditional probabilities by enumerating all matching entries 

in the joint are expensive:

exponential in the number of variables.

but perhaps there are faster ways of querying bayes nets?
    in fact, if i ever ask you to manually do a bayes net 

id136, you   ll find there are often many tricks to save you 
time.

    so we   ve just got to program our computer to do those tricks 

too, right?

copyright    2001, andrew w. moore

bayes nets: slide 80

40

the sad, bad news

conditional probabilities by enumerating all matching entries 

in the joint are expensive:

exponential in the number of variables.

but perhaps there are faster ways of querying bayes nets?
    in fact, if i ever ask you to manually do a bayes net 

id136, you   ll find there are often many tricks to save you 
time.

    so we   ve just got to program our computer to do those tricks 

too, right?
sadder and worse news:

general querying of bayes nets is np-complete.

copyright    2001, andrew w. moore

bayes nets: slide 81

bayes nets id136 algorithms

a poly-tree is a directed acyclic graph in which no two nodes have more than one 
path between them.

s

x1
l

x3
x5
a poly tree

t

x2

m

r

x4

x1

s

x2

m

l

x3
x5
not a poly tree
(but still a legal bayes net)

t

r

x4

   

if net is a poly-tree, there is a linear-time algorithm (see a later 
andrew lecture).

    the best general-case algorithms convert a general net to a poly-

tree (often at huge expense) and calls the poly-tree algorithm.

    another popular, practical approach (doesn   t assume poly-tree): 

stochastic simulation.

copyright    2001, andrew w. moore

bayes nets: slide 82

41

sampling from the joint distribution

p(s)=0.3

s

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

t

m

p(m)=0.6

p(r   m)=0.3
p(r   ~m)=0.6

p(t   l)=0.3
p(t   ~l)=0.8

r

it   s pretty easy to generate a set of variable-assignments at random with 
the same id203 as the underlying joint distribution.

how?
copyright    2001, andrew w. moore

bayes nets: slide 83

sampling from the joint distribution

p(s)=0.3

s

p(l   m^s)=0.05
p(l   m^~s)=0.1
p(l   ~m^s)=0.1
p(l   ~m^~s)=0.2

l

t

m

p(m)=0.6

p(r   m)=0.3
p(r   ~m)=0.6

p(t   l)=0.3
p(t   ~l)=0.8

r

1.  randomly choose s.  s = true with prob 0.3
2.  randomly choose m.  m = true with prob 0.6
3.  randomly choose l.  the id203 that l is true 

depends on the assignments of s and m.  e.g. if steps 
1 and 2 had produced s=true, m=false, then 
id203 that l is true is 0.1

4.  randomly choose r.  id203 depends on m.
5.  randomly choose t.  id203 depends on l

copyright    2001, andrew w. moore

bayes nets: slide 84

42

a general sampling algorithm

let   s generalize the example on the previous slide to a general bayes net.

as in slides 16-17 , call the variables x1 .. xn, where parents(xi) must be a 

subset of {x1 .. xi-1}.

for i=1 to n:

1.

2.

3.

4.

find parents, if any, of xi.  assume n(i) parents.  call them xp(i,1), xp(i,2), 
   xp(i,n(i)).
recall the values that those parents were randomly given: xp(i,1), xp(i,2), 
   xp(i,n(i)).
look up in the lookup-table for:                                     
p(xi=true     xp(i,1)=xp(i,1),xp(i,2)=xp(i,2)   xp(i,n(i))=xp(i,n(i)))
randomly set xi=true according to this id203

x1, x2,   xn are now a sample from the joint distribution of x1, x2,   xn.

copyright    2001, andrew w. moore

bayes nets: slide 85

stochastic simulation example
someone wants to know p(r = true     t = true ^ s = false )

we   ll do lots of random samplings and count the number of 
occurrences of the following:

    nc : num. samples in which t=true and s=false.
    ns : num. samples in which r=true, t=true and s=false.
    n : number of random samplings

now if n is big enough:
nc /n is a good estimate of p(t=true and s=false).
ns /n is a good estimate of p(r=true ,t=true , s=false).
p(r   t^~s) = p(r^t^~s)/p(t^~s), so ns / nc can be a good 
estimate of p(r   t^~s).

copyright    2001, andrew w. moore

bayes nets: slide 86

43

general stochastic simulation

someone wants to know p(e1     e2 )

we   ll do lots of random samplings and count the number of 
occurrences of the following:

    nc : num. samples in which e2
    ns : num. samples in which e1 and e2
    n : number of random samplings

now if n is big enough:
nc /n is a good estimate of p(e2).
ns /n is a good estimate of p(e1 , e2).
p(e1     e2) = p(e1^ e2)/p(e2), so ns / nc can be a good estimate 
of p(e1    e2).

copyright    2001, andrew w. moore

bayes nets: slide 87

likelihood weighting

problem with stochastic sampling:

with lots of constraints in e, or unlikely events in e, then most of the 
simulations will be thrown away, (they   ll have no effect on nc, or ns). 

imagine we   re part way through our simulation.

in e2 we have the constraint xi = v
we   re just about to generate a value for xi at random.  given the values 
assigned to the parents, we see that p(xi = v     parents) = p .
now we know that with stochastic sampling:
    we   ll generate    xi = v    proportion p of the time, and proceed.
    and we   ll generate a different value proportion 1-p of the time, and the 
simulation will be wasted.

instead, always generate xi = v, but weight the answer by weight    p    to 
compensate. 
copyright    2001, andrew w. moore

bayes nets: slide 88

44

likelihood weighting

set nc :=0, ns :=0
1. generate a random assignment of all variables that 

matches e2.  this process returns a weight w.

2. define w to be the id203 that this assignment would 

have been generated instead of an unmatching 
assignment during its generation in the original 
algorithm.fact: w is a product of all likelihood factors 
involved in the generation.

if our sample matches e1 then ns := ns + w

3. nc := nc + w
4.
5. go to 1
again, ns / nc estimates p(e1     e2 )

copyright    2001, andrew w. moore

bayes nets: slide 89

case study i

pathfinder system.  (heckerman 1991, probabilistic similarity networks, 

mit press, cambridge ma).

    diagnostic system for lymph-node diseases.
    60 diseases and 100 symptoms and test-results.
    14,000 probabilities
    expert consulted to make net.

    8 hours to determine variables.
    35 hours for net topology.
    40 hours for id203 table values.

    apparently, the experts found it quite easy to invent the causal links 

and probabilities.

    pathfinder is now outperforming the world experts in diagnosis.  being 

extended to several dozen other medical domains.

copyright    2001, andrew w. moore

bayes nets: slide 90

45

questions

    what are the strengths of probabilistic networks 

compared with id118?

    what are the weaknesses of probabilistic networks 

compared with id118?

    what are the strengths of probabilistic networks 

compared with predicate logic?

    what are the weaknesses of probabilistic networks 

compared with predicate logic?

    (how) could predicate logic and probabilistic 

networks be combined?

copyright    2001, andrew w. moore

bayes nets: slide 91

what you should know

    the meanings and importance of independence 

and conditional independence.
    the definition of a bayes net.
    computing probabilities of assignments of 

variables (i.e. members of the joint p.d.f.) with a 
bayes net.

    the slow (exponential) method for computing 

arbitrary, conditional probabilities.

    the stochastic simulation method and likelihood 

weighting.

copyright    2001, andrew w. moore

bayes nets: slide 92

46

