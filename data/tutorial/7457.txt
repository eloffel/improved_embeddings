eight more classic 
machine learning 

note to other teachers and users of these 
slides. andrew would be delighted if you 
found this source material useful in giving 
your own lectures. feel free to use these 
slides verbatim, or to modify them to fit 
your own needs. powerpoint originals are 
available. if you make use of a significant 
portion of these slides in your own lecture, 
please include this message, or the 
following link to the source repository of 
andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

algorithms
andrew w. moore
associate professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2001, andrew w. moore

oct 29th, 2001

8: polynomial regression
so far we   ve mainly been dealing with id75
x1
3
1
:

2
1
:

3
1
:

7
3
:

x=

y=

x1=(3,2)..
y=

y1=7..

yx2
7
2
1
3
:
:
1
1
:

z=

3
1

2
1
:

7
3
:

z1=(1,3,2)..
zk=(1,xk1,xk2)

y1=7..

b=(ztz)-1(zty)

yest = b0+ b1 x1+ b2 x2

copyright    2001, andrew w. moore

machine learning favorites: slide  2

1

quadratic regression

x=

it   s trivial to do linear fits of fixed nonlinear basis functions
x1
3
1
:

2
1
:

3
1
:

7
3
:

y=

y1=7..

z=

2
1

9
1

x1=(3,2)..
6
y=
7
1
3
:

4
1
:

z=(1 ,  x1,   x2 ,   x1

2
2, x1x2,x2
,)

yx2
7
2
1
3
:
:
1
3
1
1
:

b=(ztz)-1(zty)

yest = b0+ b1 x1+ b2 x2+
2 + b4 x1x2 + b5 x2
b3 x1
2

copyright    2001, andrew w. moore

machine learning favorites: slide  3

quadratic regression

y=

x=

7
3
:

3
1
:

2
1
:

each component of a z vector is called a term.

it   s trivial to do linear fits of fixed nonlinear basis functions
x1
3
1
:

yx2
each column of the z matrix is called a term column
7
2
how many terms in a quadratic regression with m
inputs?
1
3
   1 constant term
:
:
x1=(3,2)..
   m linear terms
1
3
6
y=
7
   (m+1)-choose-2 = m(m+1)/2 quadratic terms
1
1
1
3
(m+2)-choose-2 terms in total = o(m2)
:
:

b=(ztz)-1(zty)

y1=7..

4
1
:

2
1

9
1

z=

z=(1 ,  x1,   x2 ,   x1

2
2, x1x2,x2
,)

note that solving b=(ztz)-1(zty) is thus o(m6)

yest = b0+ b1 x1+ b2 x2+
b3 x1
2 + b4 x1x2 + b5 x2
2

copyright    2001, andrew w. moore

machine learning favorites: slide  4

2

qth-degree polynomial regression

x1
3
1
:

z=

yx2
7
2
1
3
:
:
1
3
1
1
:

x=

3
1
:

2
1
:

y=

7
3
:

y1=7..

2
1

9
1

x1=(3,2)..
6
y=
1

   
   
   

7
3
:

z=(all products of powers of inputs in 
which sum of powers is q or less,)

b=(ztz)-1(zty)

yest = b0+ 
b1 x1+   

copyright    2001, andrew w. moore

machine learning favorites: slide  5

m inputs, degree q: how many terms?
= the number of unique terms of the form

q
q
xx
1
2
1

2

q
x
...
m
m

 where
 

qq
i

= the number of unique terms of the form
=

m

2

 where
 

q
1
0

q
q
xx
1
2
1

q
x
...
m
m

qq
i

m

=1

i

=0

i

= the number of lists of non-negative integers [q0,q1,q2,..qm] 
in which s qi = q
= the number of ways of placing q red disks on a row of 
squares of length q+m       = (q+m)-choose-q

q0=2

q1=2

q2=0

q3=4

q4=3

copyright    2001, andrew w. moore

machine learning favorites: slide  6

q=11, m=4

3

  
(cid:229)
(cid:229)
7: radial basis functions (rbfs)

x1
3
1
:

z=

yx2
7
2
1
3
:
:
   
   
   
   
   
   

x=

3
1
:

2
1
:

y=

7
3
:

y1=7..

   
   
   

   
   
   

x1=(3,2)..
   
y=
   
   

   
   
   

7
3
:

z=(list of radial basis function evaluations)

b=(ztz)-1(zty)

yest = b0+ 
b1 x1+   

copyright    2001, andrew w. moore

machine learning favorites: slide  7

1-d rbfs

y

c1

x

c1

c1

yest = b1 f1(x) + b2 f2(x) + b3 f3(x)
where
fi(x) = kernelfunction( | x - ci | / kw)

copyright    2001, andrew w. moore

machine learning favorites: slide  8

4

example

y

c1

x

c1

c1

yest = 2f1(x) + 0.05f 2(x) + 0.5f3(x)
where
fi(x) = kernelfunction( | x - ci | / kw)

copyright    2001, andrew w. moore

machine learning favorites: slide  9

rbfs with id75

kw also held constant 
(initialized to be large 

y

all ci    s are held constant 
(initialized randomly or 
c1
dimensional input space)

on a grid in m -

c1

x

enough that there   s decent 

overlap between basis 

functions*

c1

*usually much better than the crappy 

overlap on my diagram

yest = 2f1(x) + 0.05f 2(x) + 0.5f3(x)
where
fi(x) = kernelfunction( | x - ci | / kw)

copyright    2001, andrew w. moore

machine learning favorites: slide  10

5

rbfs with id75

kw also held constant 
(initialized to be large 

y

all ci    s are held constant 
(initialized randomly or 
c1
dimensional input space)

on a grid in m -

c1

x

enough that there   s decent 

overlap between basis 

functions*

c1

*usually much better than the crappy 

overlap on my diagram

yest = 2f1(x) + 0.05f 2(x) + 0.5f3(x)
where
fi(x) = kernelfunction( | x - ci | / kw)
then given q basis functions, define the matrix z such that zkj = 
kernelfunction( | xk - ci | / kw) where xk is the kth vector of inputs
and as before, b=(zt z)-1(zty)

copyright    2001, andrew w. moore

machine learning favorites: slide  11

rbfs with nonid75

allow the ci    s to adapt to 

y
the data (initialized 

randomly or on a grid in 
c1

m-dimensional input 

c1

x

space)

kw allowed to adapt to the data.
(some folks even let each basis 
function have its own 
kwj,permitting fine detail in 
dense regions of input space)

c1

yest = 2f1(x) + 0.05f 2(x) + 0.5f3(x)
where
fi(x) = kernelfunction( | x - ci | / kw)
but how do we now find all the bj   s, ci    s and kw ?

copyright    2001, andrew w. moore

machine learning favorites: slide  12

6

rbfs with nonid75

allow the ci    s to adapt to 

y
the data (initialized 

randomly or on a grid in 
c1

m-dimensional input 

c1

x

space)

kw allowed to adapt to the data.
(some folks even let each basis 
function have its own 
kwj,permitting fine detail in 
dense regions of input space)

c1

yest = 2f1(x) + 0.05f 2(x) + 0.5f3(x)
where
fi(x) = kernelfunction( | x - ci | / kw)
but how do we now find all the bj   s, ci    s and kw ?

copyright    2001, andrew w. moore

machine learning favorites: slide  13

answer: id119

rbfs with nonid75

allow the ci    s to adapt to 

y
the data (initialized 

randomly or on a grid in 
c1

m-dimensional input 

c1

x

space)

kw allowed to adapt to the data.
(some folks even let each basis 
function have its own 
kwj,permitting fine detail in 
dense regions of input space)

c1

yest = 2f1(x) + 0.05f 2(x) + 0.5f3(x)
where
fi(x) = kernelfunction( | x - ci | / kw)
but how do we now find all the bj   s, ci    s and kw ?

(but i   d like to see, or hope someone   s already done, a 
hybrid, where the ci    s and kw are updated with gradient 
descent while the bj   s use matrix inversion)

answer: id119

copyright    2001, andrew w. moore

machine learning favorites: slide  14

7

radial basis functions in 2-d

two inputs.
outputs (heights 
sticking out of page) 
not shown.

x2

x1

center

sphere of 
significant 
influence of 
center

copyright    2001, andrew w. moore

machine learning favorites: slide  15

happy rbfs in 2-d

blue dots denote 
coordinates of 
input vectors

x2

x1

center

sphere of 
significant 
influence of 
center

copyright    2001, andrew w. moore

machine learning favorites: slide  16

8

crabby rbfs in 2-d

blue dots denote 
coordinates of 
input vectors

what   s the 
problem in this 
example?

center

x2

x1

sphere of 
significant 
influence of 
center

copyright    2001, andrew w. moore

machine learning favorites: slide  17

more crabby rbfs

blue dots denote 
coordinates of 
input vectors

and what   s the 
problem in this 
example?

center

x2

x1

sphere of 
significant 
influence of 
center

copyright    2001, andrew w. moore

machine learning favorites: slide  18

9

hopeless!

even before seeing the data, you should 
understand that this is a disaster!

center

sphere of 
significant 
influence of 
center

x2

x1

copyright    2001, andrew w. moore

machine learning favorites: slide  19

unhappy

even before seeing the data, you should 
understand that this isn   t good either..

center

sphere of 
significant 
influence of 
center

x2

x1

copyright    2001, andrew w. moore

machine learning favorites: slide  20

10

6: robust regression

y

x

copyright    2001, andrew w. moore

machine learning favorites: slide  21

robust regression

this is the best fit that 
quadratic regression can 
manage

y

x

copyright    2001, andrew w. moore

machine learning favorites: slide  22

11

robust regression

   but this is what we   d 
probably prefer

y

x

copyright    2001, andrew w. moore

machine learning favorites: slide  23

loess-based robust regression

after the initial fit, score 
each datapoint according to 
how well it   s fitted   

y

you are a very good 

datapoint.

x

copyright    2001, andrew w. moore

machine learning favorites: slide  24

12

loess-based robust regression

after the initial fit, score 
each datapoint according to 
how well it   s fitted   

y

you are a very good 

datapoint.

you are not too 

shabby.

x

copyright    2001, andrew w. moore

machine learning favorites: slide  25

loess-based robust regression

after the initial fit, score 
each datapoint according to 
how well it   s fitted   

but you are 

pathetic.

y

you are a very good 

datapoint.

you are not too 

shabby.

x

copyright    2001, andrew w. moore

machine learning favorites: slide  26

13

y

x

robust regression

for k = 1 to r   
   let (xk,yk) be the kth datapoint
   let yest
k be predicted value of 
yk
   let wk be a weight for 
datapoint k that is large if the 
datapoint fits well and small if it 
fits badly:

wk = kernelfn([yk- yest

k]2)

copyright    2001, andrew w. moore

machine learning favorites: slide  27

robust regression

for k = 1 to r   
   let (xk,yk) be the kth datapoint
   let yest
k be predicted value of 
yk
   let wk be a weight for 
datapoint k that is large if the 
datapoint fits well and small if it 
fits badly:

wk = kernelfn([yk- yest

k]2)

y

x

then redo the regression 
using weighted datapoints.
i taught you how to do this in the    instance-
based    lecture (only then the weights 
depended on distance in input-space)
guess what happens next?

copyright    2001, andrew w. moore

machine learning favorites: slide  28

14

robust regression

for k = 1 to r   
   let (xk,yk) be the kth datapoint
   let yest
k be predicted value of 
yk
   let wk be a weight for 
datapoint k that is large if the 
datapoint fits well and small if it 
fits badly:

wk = kernelfn([yk- yest

k]2)

y

x

then redo the regression 
using weighted datapoints.
i taught you how to do this in the    instance-
based    lecture (only then the weights 
depended on distance in input-space)
repeat whole thing until 
converged!

copyright    2001, andrew w. moore

machine learning favorites: slide  29

robust regression---what we   re 

doing

what regular regression does:
assume yk was originally generated using the 
following recipe:

yk = b0+ b1 xk+ b2 xk

2 +n(0,s2)

computational task is to find the maximum 
likelihood b0 , b1 and b2 

copyright    2001, andrew w. moore

machine learning favorites: slide  30

15

robust regression---what we   re 

doing

what loess robust regression does:
assume yk was originally generated using the 
following recipe:

with id203 p:

yk = b0+ b1 xk+ b2 xk

2 +n(0,s2)

but otherwise

yk ~ n(m,shuge

2)

computational task is to find the maximum 
likelihood b0 , b1 , b2 , p, m and shuge

copyright    2001, andrew w. moore

machine learning favorites: slide  31

robust regression---what we   re 

doing

what loess robust regression does:
assume yk was originally generated using the 
following recipe:

with id203 p:

yk = b0+ b1 xk+ b2 xk

2 +n(0,s2)

but otherwise

mysteriously, the 
reweighting procedure 
does this computation 
for us.

your first glimpse of 

two spectacular letters: 

yk ~ n(m,shuge

2)

e.m.

computational task is to find the maximum 
likelihood b0 , b1 , b2 , p, m and shuge

copyright    2001, andrew w. moore

machine learning favorites: slide  32

16

5: regression trees

       id90 for regression   

copyright    2001, andrew w. moore

machine learning favorites: slide  33

a regression tree leaf

predict age = 47

mean age of records 
matching this leaf node

copyright    2001, andrew w. moore

machine learning favorites: slide  34

17

a one-split regression tree

gender?

female

male

predict age = 39

predict age = 36

copyright    2001, andrew w. moore

machine learning favorites: slide  35

choosing the attribute to split on

gender

rich?

female
male
male
:

no
no
yes
:

num. 
children
2
0
0
:

num. beany
babies
1
0
5+
:

age

38
24
72
:

    we can   t use 

information gain.

    what should we use?

copyright    2001, andrew w. moore

machine learning favorites: slide  36

18

choosing the attribute to split on

gender

rich?

female
male
male
:

no
no
yes
:

num. 
children
2
0
0
:

num. beany
babies
1
0
5+
:

age

38
24
72
:

mse(y|x) = the expected squared error if we must predict a record   s y 

value given only knowledge of the record   s x value

if we   re told x=j, the smallest expected error comes from predicting the 

mean of the y-values among those records in which x=j. call this mean 
quantity my

x=j

then   

xymse

(

|

=
)

1
r

n

x

=

j

1

(
x
such that 
k

=
jx
k   
y
y
=
)
j

k
 (

2)

copyright    2001, andrew w. moore

machine learning favorites: slide  37

choosing the attribute to split on

gender

rich?

female
male
male
:

no
no
yes
:

num. 
children
2
0
0
:

num. beany
babies
1
0
5+
:

age

38
24
72
:

regression tree attribute selection: greedily 
choose the attribute that minimizes mse(y|x) 

guess what we do about real-valued inputs?
guess how we prevent overfitting

mse(y|x) = the expected squared error if we must predict a record   s y 

value given only knowledge of the record   s x value

if we   re told x=j, the smallest expected error comes from predicting the 

mean of the y-values among those records in which x=j. call this mean 
quantity my

x=j

then   

xymse

(

|

=
)

1
r

n

x

=

j

1

(
x
such that 
k

=
jx
k   
y
y
=
)
j

k
 (

2)

copyright    2001, andrew w. moore

machine learning favorites: slide  38

19

(cid:229)
(cid:229)
-
(cid:229)
(cid:229)
-
pruning decision

   property-owner = yes

gender?

female

male

do i deserve 

to live?

predict age = 39
# property-owning females = 56712
mean age among pofs = 39
age std dev among pofs = 12

predict age = 36

# property-owning males = 55800
mean age among poms = 36
age std dev among poms = 11.5

use a standard chi-squared test of the null-
hypothesis    these two populations have the same 
mean    and bob   s your uncle.

copyright    2001, andrew w. moore

machine learning favorites: slide  39

id75 trees
   property-owner = yes

also known as 
   model trees   

gender?

female

male

predict age = 
26 + 6 * numchildren -
2 * yearseducation

predict age = 

24 + 7 * numchildren -
2.5 * yearseducation

leaves contain linear 
functions (trained using 
id75 on all 
records matching that leaf)

split attribute chosen to minimize 
mse of regressed children.

pruning with a different chi-
squared

copyright    2001, andrew w. moore

machine learning favorites: slide  40

20

id75 trees
   property-owner = yes

also known as 
   model trees   

n

a

d  

s t e

female

gender?
y  
male
n  t e
o r e   a
e
e
s   b
n
e  
a lly  i g
g  t h
a t  h
predict age = 
predict age = 
u ri n
p ic
u t e  t h
d   a ttri b
u  t y
d  
e   d
s t e
a l  a ttri b
e  tr e
o
e t a il:  y
n t e
26 + 6 * numchildren -
24 + 7 * numchildren -
p  i n  t h
e   a ll  u
a l u
o ric
- v
e
v
2.5 * yearseducation
2 * yearseducation
s
g
e r  u
u t  u
a t e
al
d   a
e  r e
h
n .  b
n   h i g
s t e
s
d   u
n  t e
si o
o
n
s
split attribute chosen to minimize 
leaves contain linear 
e
g r e
s,  a
e
e   b
r e
u t e
y   v
functions (trained using 
mse of regressed children.
a ttri b
n  if t h
id75 on all 
e
v
e
records matching that leaf)

pruning with a different chi-
squared

u t e

s  

d

c

e

e

o

b

copyright    2001, andrew w. moore

machine learning favorites: slide  41

test your understanding
assuming regular regression trees, can you sketch a 
graph of the fitted function yest(x) over this diagram?

y

x

copyright    2001, andrew w. moore

machine learning favorites: slide  42

21

test your understanding

assuming id75 trees, can you sketch a graph 
of the fitted function yest(x) over this diagram?

y

x

copyright    2001, andrew w. moore

machine learning favorites: slide  43

4: gmdh (c.f. bacon, aim)

a very simple but very good idea:

    group method data handling
   
1. do id75
2. use cross-validation to discover whether any 
quadratic term is good. if so, add it as a basis 
function and loop.

3. use cross-validation to discover whether any of a 

set of familiar functions (log, exp, sin etc) 
applied to any previous basis function helps. if 
so, add it as a basis function and loop.

4. else stop

copyright    2001, andrew w. moore

machine learning favorites: slide  44

22

gmdh (c.f. bacon, aim)

typical learned function:

a very simple but very good idea:

    group method data handling
   
1. do id75
2. use cross-validation to discover whether any 
quadratic term is good. if so, add it as a basis 
function and loop.

ageest = height - 3.1 sqrt(weight) + 

4.3 income / (cos (numcars))

3. use cross-validation to discover whether any of a 

set of familiar functions (log, exp, sin etc) 
applied to any previous basis function helps. if 
so, add it as a basis function and loop.

4. else stop

copyright    2001, andrew w. moore

machine learning favorites: slide  45

3: cascade correlation

    a super-fast form of neural network learning
    begins with 0 hidden units
    incrementally adds hidden units, one by one, 

doing ingeniously little recomputation between 
each unit addition

copyright    2001, andrew w. moore

machine learning favorites: slide  46

23

cascade beginning

begin with a regular dataset

nonstandard notation:
   x(i) is the i   th attribute 
k is the value of the i   th 
   x(i)
attribute in the k   th record

x(0)

x(0)
1

x(0)
2

:

x(1)

x(1)
1

x(1)
2

:

   
   
   
:

x(m)

x(m)
1

x(m)
2

:

y

y1

y2
:

copyright    2001, andrew w. moore

machine learning favorites: slide  47

cascade first step

begin with a regular dataset
find weights w(0)

i  to best fit y. 

i.e. to minimize

r

=
1

k

(

y

k

y

2)0(
)
k

 where

 

y

)0(
k

=

m

=
1

j

)0(
xw
j

(
k

j

)

x(0)

x(0)
1

x(0)
2

:

x(1)

x(1)
1

x(1)
2

:

   
   
   
:

x(m)

x(m)
1

x(m)
2

:

y

y1

y2
:

copyright    2001, andrew w. moore

machine learning favorites: slide  48

24

(cid:229)
(cid:229)
-
consider our errors   

begin with a regular dataset
find weights w(0)

i  to best fit y. 

i.e. to minimize

r

=
1

k

(

y

k

y

2)0(
)
k

 where

 

y

)0(
k

define

)0( 
e
k

=

y

k

)0(
xw
j

(
k

j

)

=

m

=
1

j

y

)0(
k

x(0)

x(0)
1

x(0)
2

:

x(1)

x(1)
1

x(1)
2

:

   
   
   
:

x(m)

x(m)
1

x(m)
2

:

y

y1

y2
:

y(0)

y(0)
1

y(0)
2

:

e(0)

e(0)
1

e(0)
2

:

copyright    2001, andrew w. moore

machine learning favorites: slide  49

create a hidden unit   

find weights u(0)
function h(0)(x) of the inputs.

i  to define a new basis 

make it specialize in predicting the 
errors in our original fit:
find {u(0)
between h(0)(x) and e(0) where

i } to maximize correlation 

h

)0(

)(x

=

g

m

=
1

j

j
)(

)0(
j xu

x(0)

x(0)
1

x(0)
2

:

x(1)

x(1)
1

x(1)
2

:

   
   
   
:

x(m)

x(m)
1

x(m)
2

:

y

y1

y2
:

y(0)

y(0)
1

y(0)
2

:

e(0)

e(0)
1

e(0)
2

:

h(0)

h(0)
1

h(0)
2

:

copyright    2001, andrew w. moore

machine learning favorites: slide  50

25

(cid:229)
(cid:229)
-
-
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
(cid:229)
cascade next step
i p(1)

0 to better fit y. 

find weights w(1)
i.e. to minimize

r

=
1

k

(

y

k

y

2)1(
)
k

 where
 

y

)1(
k

=

m

=
1

j

)0(
xw
j

(
k

j

)

+

)0(
)0(
hp
j
k

x(0)

x(0)
1

x(0)
2

:

x(1)

x(1)
1

x(1)
2

:

   
   
   
:

x(m)

x(m)
1

x(m)
2

:

y

y1

y2
:

y(0)

y(0)
1

y(0)
2

:

e(0)

e(0)
1

e(0)
2

:

h(0)

h(0)
1

h(0)
2

:

y(1)

y(1)
1

y(1)
2

:

copyright    2001, andrew w. moore

machine learning favorites: slide  51

now look at new errors

find weights w(1)

i p(1)

0 to better fit y. 

define

)1( 
e
k

=

y

k

y

)1(
k

x(0)

x(0)
1

x(0)
2

:

x(1)

x(1)
1

x(1)
2

:

   
   
   
:

x(m)

x(m)
1

x(m)
2

:

y

y1

y2
:

y(0)

y(0)
1

y(0)
2

:

e(0)

e(0)
1

e(0)
2

:

h(0)

h(0)
1

h(0)
2

:

y(1)

y(1)
1

y(1)
2

:

e(1)

e(1)
1

e(1)
2

:

copyright    2001, andrew w. moore

machine learning favorites: slide  52

26

-
(cid:229)
(cid:229)
-
create next hidden unit   

find weights u(1)
basis function h(1)(x) of the inputs.

0 to define a new 

i v(1)

make it specialize in predicting the 
errors in our original fit:
find {u(1)
between h(1)(x) and e(1) where

0} to maximize correlation 
)(
x

i , v(1)

h

)1(

x(0)

x(0)
1

x(0)
2

:

x(1)

x(1)
1

x(1)
2

:

   
   
   
:

x(m)

x(m)
1

x(m)
2

:

y

y1

y2
:

y(0)

y(0)
1

y(0)
2

:

e(0)

e(0)
1

e(0)
2

:

=

g

h(0)

h(0)
1

h(0)
2

:

)(
j

)1(
xu
j

+

)0(

)1(
hv
0

m

=
1

j

y(1)

y(1)
1

y(1)
2

:

e(1)

e(1)
1

e(1)
2

:

h(1)

h(1)
1

h(1)
2

:

copyright    2001, andrew w. moore

machine learning favorites: slide  53

cascade n   th step
i p(n)

j to better fit y. 

find weights w(n)
i.e. to minimize

r

=
1

k

(

y

k

y

(
k

n

2)
)

 where
 

n

)

y

(
k

=

m

=
1

j

n

)
xw

(
j

+

j

)

(
k

n

1

=
1

j

n

)
(
(
hp
j
k

j

)

x(0)

x(1)

x(0)

x(0)

1

2

:

x(1)

x(1)

1

2

:

   
   
   
:

x(m)

x(m)

x(m)

1

2

:

y
y1
y2
:

y(0)

e(0)

h(0)

y(1)

e(1)

y(0)

y(0)

1

2

:

e(0)

e(0)

1

2

:

h(0)

h(0)

1

2

:

y(1)

y(1)

1

2

:

e(1)

e(1)

1

2

:

h(1)

h(1)

h(1)

1

2

:

   

   

   

:

y(n)

y(n)

y(n)

1

2

:

copyright    2001, andrew w. moore

machine learning favorites: slide  54

27

(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
-
-
now look at new errors

find weights w(n)
i.e. to minimize

i p(n)

j to better fit y. 

define

)( 
n
e
k

=

y

k

y

)(
n
k

x(0)

x(0)

x(0)

1

2

:

x(1)

x(1)

x(1)

1

2

:

   
   
   
:

x(m)

x(m)

x(m)

1

2

:

y
y1
y2
:

y(0)

y(0)

y(0)

1

2

:

e(0)

e(0)

e(0)

1

2

:

h(0)

h(0)

h(0)

1

2

:

y(1)

y(1)

y(1)

1

2

:

e(1)

e(1)

e(1)

1

2

:

h(1)

h(1)

h(1)

1

2

:

   

   

   

:

y(n)

y(n)

y(n)

1

2

:

e(n)

e(n)

e(n)

1

2

:

copyright    2001, andrew w. moore

machine learning favorites: slide  55

create n   th hidden unit   

find weights u(n)
the inputs.

i v(n)

i to define a new basis function h(n)(x) of 

make it specialize in predicting the errors in our previous fit:
find {u(n)
j} to maximize correlation between h(n)(x) and 
e(n) where

i , v(n)

)(
n

h x
)(

=

g

)(
j

)(
n
xu
j

+

)(
j

)(
n
hv
j

m

=
1

j

n

1

=
1

j

x(0)

x(1)

x(0)

x(0)

1

2

:

x(1)

x(1)

1

2

:

   
   
   
:

x(m)

x(m)

x(m)

1

2

:

y
y1
y2
:

y(0)

e(0)

h(0)

y(1)

e(1)

y(0)

y(0)

1

2

:

e(0)

e(0)

1

2

:

h(0)

h(0)

1

2

:

y(1)

y(1)

1

2

:

e(1)

e(1)

1

2

:

h(1)

h(1)

h(1)

1

2

:

   

   

   

:

y(n)

e(n)

y(n)

y(n)

1

2

:

e(n)

e(n)

1

2

:

h(n)

h(n)

h(n)

1

2

:

continue until satisfied with fit   

copyright    2001, andrew w. moore

machine learning favorites: slide  56

28

-
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
(cid:229)
(cid:229)
-
visualizing first iteration

copyright    2001, andrew w. moore

machine learning favorites: slide  57

visualizing second iteration

copyright    2001, andrew w. moore

machine learning favorites: slide  58

29

visualizing third iteration

copyright    2001, andrew w. moore

machine learning favorites: slide  59

example: cascade correlation for 

classification

copyright    2001, andrew w. moore

machine learning favorites: slide  60

30

training two spirals: steps 1-6

copyright    2001, andrew w. moore

machine learning favorites: slide  61

training two spirals: steps 2-12

copyright    2001, andrew w. moore

machine learning favorites: slide  62

31

if you like cascade correlation   
see also
    projection pursuit

in which you add together many non-linear non-

parametric scalar functions of carefully chosen directions
each direction is chosen to maximize error-reduction from 

the best scalar function

    ada-boost

an additive combination of regression trees in which the 

n+1   th tree learns the error of the n   th tree

copyright    2001, andrew w. moore

machine learning favorites: slide  63

2: multilinear interpolation
consider this dataset. suppose we wanted to create a 
continuous and piecewise linear fit to the data

y

x

copyright    2001, andrew w. moore

machine learning favorites: slide  64

32

multilinear interpolation
create a set of knot points: selected x-coordinates 
(usually equally spaced) that cover the data

y

q1

q2

q3

q4

q5

x
copyright    2001, andrew w. moore

machine learning favorites: slide  65

multilinear interpolation

we are going to assume the data was generated by a 
noisy version of a function that can only bend at the 
knots. here are 3 examples (none fits the data well)

y

q1

q2

q3

q4

q5

x
copyright    2001, andrew w. moore

machine learning favorites: slide  66

33

how to find the best fit?
idea 1: simply perform a separate regression in each 
segment for each part of the curve

what   s the problem with this idea?

y

q1

q2

q3

q4

q5

x
copyright    2001, andrew w. moore

machine learning favorites: slide  67

how to find the best fit?

let   s look at what goes on in the red segment

y est

)(
x

(

q
3

=

x

)

+

h
2

(

q
2

x

)

w

w

 
 where
qw

=

h
3

3

q

2

h2

h3

y

q1

q2

q3

q4

q5

x
copyright    2001, andrew w. moore

machine learning favorites: slide  68

34

-
-
-
in the red segment   

how to find the best fit?
)(
fh
x
33
q
-=
x
1)(
3

y est
)(
x
-=
1)(
x

 
f
where

)(
x

+

f

,

3

2

=
fh
22
qx
w

2

x

w

h2

h3

y

f2(x)

q1

q2

q3

q4

q5

x
copyright    2001, andrew w. moore

machine learning favorites: slide  69

in the red segment   

how to find the best fit?
)(
fh
x
33
q
-=
x
1)(
3

y est
)(
x
-=
1)(
x

 
f
where

)(
x

+

f

,

3

2

=
fh
22
qx
w

2

x

w

h2

h3

y

f2(x)

q1

q2

q3

x
copyright    2001, andrew w. moore

q4

q5

f3(x)

machine learning favorites: slide  70

35

-
-
-
-
in the red segment   

how to find the best fit?
)(
fh
x
33
|
-=
x
1)(

y est
-=
x
1)(

where
f
 

)(
x
|

)(
x

+

f

,

3

2

=
fh
22
qx
|
2
w

|

qx
3
w

h2

h3

y

f2(x)

q1

q2

q3

x
copyright    2001, andrew w. moore

q4

q5

f3(x)

machine learning favorites: slide  71

in the red segment   

how to find the best fit?
)(
fh
x
33
|
-=
1)(
x

y est
-=
1)(
x

where
f
 

)(
x
|

)(
x

+

f

,

3

2

=
fh
22
qx
|
2
w

|

qx
3
w

h2

h3

y

f2(x)

q1

q2

q3

x
copyright    2001, andrew w. moore

q4

q5

f3(x)

machine learning favorites: slide  72

36

-
-
-
-
how to find the best fit?

in general

est

y

x
)(

= kn

fh
i

x
)(

i

 
where
f

)(
x

i

=

|1

=
1

i
qx
i
w

0

|

| if

<
 
wqx
|
otherwise

i

h2

h3

y

f2(x)

q1

q2

q3

x
copyright    2001, andrew w. moore

q4

q5

f3(x)

machine learning favorites: slide  73

how to find the best fit?

in general

est

y

x
)(

= kn

fh
i

x
)(

i

 
where
f

)(
x

i

=

|1

=
1

i
qx
i
w

0

|

| if

<
 
wqx
|
otherwise

i

h2

and this is simply a basis function 

regression problem!

y

h3

we know how to find the least 

squares hiis!

f2(x)

q1

q2

q3

x
copyright    2001, andrew w. moore

q4

q5

f3(x)

machine learning favorites: slide  74

37

(cid:229)
(cid:239)
(cid:238)
(cid:239)
(cid:237)
(cid:236)
-
-
-
(cid:229)
(cid:239)
(cid:238)
(cid:239)
(cid:237)
(cid:236)
-
-
-
in two dimensions   

blue dots show 
locations of input 
vectors (outputs 
not depicted)

x2

x1

copyright    2001, andrew w. moore

machine learning favorites: slide  75

in two dimensions   

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface

blue dots show 
locations of input 
vectors (outputs 
not depicted)

x2

x1

copyright    2001, andrew w. moore

machine learning favorites: slide  76

38

in two dimensions   

9

7

3

8

blue dots show 
locations of input 
vectors (outputs 
not depicted)

x2

x1

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface

but how do we 
do the 
interpolation to 
ensure that the 
surface is 
continuous?

copyright    2001, andrew w. moore

machine learning favorites: slide  77

in two dimensions   

9

7

3

8

blue dots show 
to predict the 
locations of input 
value here   
vectors (outputs 
not depicted)

x2

x1

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface

but how do we 
do the 
interpolation to 
ensure that the 
surface is 
continuous?

copyright    2001, andrew w. moore

machine learning favorites: slide  78

39

in two dimensions   

blue dots show 
locations of input 
vectors (outputs 
not depicted)

to predict the 
value here   
first interpolate 
its value on two 
opposite edges   

x2

9

7

7

7.33

3

8

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface

but how do we 
do the 
interpolation to 
ensure that the 
surface is 
continuous?

x1

copyright    2001, andrew w. moore

machine learning favorites: slide  79

in two dimensions   

blue dots show 
locations of input 
vectors (outputs 
not depicted)

to predict the 
value here   
first interpolate 
its value on two 
opposite edges   
then interpolate 
between those 
x2
two values

x1

9

7

7.05

7

7.33

3

8

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface

but how do we 
do the 
interpolation to 
ensure that the 
surface is 
continuous?

copyright    2001, andrew w. moore

machine learning favorites: slide  80

40

in two dimensions   

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface

but how do we 
do the 
interpolation to 
ensure that the 
surface is 
continuous?

9

7

3

7.05

7

notes:

8

blue dots show 
locations of input 
vectors (outputs 
not depicted)

to predict the 
value here   
first interpolate 
its value on two 
opposite edges   
then interpolate 
between those 
x2
two values

7.33

this can easily be generalized 
to m dimensions.

it should be easy to see that it 
ensures continuity
the patches are not linear

x1

copyright    2001, andrew w. moore

machine learning favorites: slide  81

doing the regression

9

7

3

8

x2

x1

given data, how 
do we find the 
optimal knot 
heights?

happily, it   s 
simply a two-
dimensional 
basis function 
problem.

(working out 
the basis 
functions is 
tedious, 
unilluminating, 
and easy)

what   s the 
problem in 
higher 
dimensions?

copyright    2001, andrew w. moore

machine learning favorites: slide  82

41

1: mars

    multivariate adaptive regression splines
    invented by jerry friedman (one of 

andrew   s heroes)
    simplest version:

let   s assume the function we are learning is of the 

following form:
est

y

)(x

= m

=
1

k

xg
(
k

k

)

instead of a linear combination of the inputs, it   s a linear 
combination of non-linear functions of individual inputs

copyright    2001, andrew w. moore

machine learning favorites: slide  83

mars

est

y

)(x

= m

=
1

k

xg
(
k

k

)

instead of a linear combination of the inputs, it   s a linear 
combination of non-linear functions of individual inputs

idea: each 
gk is one of 

these

y

q1

q2

q3

q4

q5

copyright    2001, andrew w. moore

x

machine learning favorites: slide  84

42

(cid:229)
(cid:229)
mars

est

y

)(x

= m

=
1

k

xg
(
k

k

)

instead of a linear combination of the inputs, it   s a linear 
combination of non-linear functions of individual inputs

est

y

)(x

n

k

= m

=
1

k

=
1

j

k
fh
j

k
j

(

x

k

)

 
where
f

y

)(
x

k
j

1

=

|

x
k

q

k
j

|

w
k
0

| if

 

<
|
w
k

k
j

q

x
k
otherwise

q1

q2

q3

q4

q5

j : the regressed 

qk
j : the location of 
the j   th knot in the 
k   th dimension
hk
height of the j   th
knot in the k   th
dimension
wk: the spacing 
between knots in 
the kth dimension

copyright    2001, andrew w. moore

x

machine learning favorites: slide  85

that   s not complicated enough!
    okay, now let   s get serious. we   ll allow 

arbitrary    two-way interactions   :

est

y

)(x

=

xg
(
k

k

)

+

m

=
1

k

m

=
1

k

m

g
+=
1
kt

(

xx
,
k

t

)

kt

the function we   re 

learning is allowed to be 

a sum of non-linear 

functions over all one-d 

and 2-d subsets of 

attributes

can still be expressed as a linear 
combination of basis functions

thus learnable by id75

full mars: uses cross-validation to 
choose a subset of subspaces, knot 
resolution and other parameters.

copyright    2001, andrew w. moore

machine learning favorites: slide  86

43

(cid:229)
(cid:229)
(cid:229)
(cid:239)
(cid:238)
(cid:239)
(cid:237)
(cid:236)
-
-
-
(cid:229)
(cid:229)
(cid:229)
if you like mars   

   see also cmac (cerebellar model articulated 

controller) by james albus (another of 
andrew   s heroes)
    many of the same gut-level intuitions
    but entirely in a neural-network, biologically 

plausible way
    (all the low dimensional functions are by 

means of lookup tables, trained with a delta-
rule and using a clever blurred update and 
hash-tables)

copyright    2001, andrew w. moore

machine learning favorites: slide  87

where are we now?

s
t
u
p
n
i

s
t
u
p
n
i

id136

engine learn

p(e1|e2) joint de, bayes net structure learning

classifier

predict
category

dec tree, sigmoid id88, sigmoid n.net, 
gauss/joint bc, gauss na  ve bc, n.neigh, bayes 
net based bc, cascade correlation

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

prob-
ability

predict
real no.

joint de, na  ve de, gauss/joint de, gauss na  ve 
de, bayes net structure learning

id75, polynomial regression, 
id88, neural net, n.neigh, kernel, lwr, 
rbfs, robust regression, cascade correlation, 
regression trees, gmdh, multilinear interp, mars

copyright    2001, andrew w. moore

machine learning favorites: slide  88

44

what you should know

    for each of the eight methods you should be able 

to summarize briefly what they do and outline 
how they work.

    you should understand them well enough that 

given access to the notes you can quickly re-
understand them at a moments notice

    but you don   t have to memorize all the details
    in the right context any one of these eight might 

end up being really useful to you one day! you 
should be able to recognize this when it occurs.

copyright    2001, andrew w. moore

machine learning favorites: slide  89

citations

radial basis functions
t. poggio and f. girosi, id173 algorithms 
for learning that are equivalent to multilayer 
networks, science, 247, 978--982, 1989

regression trees etc
l. breiman and j. h. friedman and r. a. olshen 

and c. j. stone, classification and regression 
trees, wadsworth, 1984

loess
w. s. cleveland, robust locally weighted 

regression and smoothing scatterplots, 
journal of the american statistical association, 
74, 368, 829-836, december, 1979

gmdh etc
http://www.inf.kiev.ua/gmdh-home/
p. langley and g. l. bradshaw and h. a. simon, 

rediscovering chemistry with the bacon 
system, machine learning: an artificial 
intelligence approach, r. s. michalski and j. 
g. carbonnell and t. m. mitchell, morgan 
kaufmann, 1983

j. r. quinlan, combining instance-based and 
model-based learning, machine learning: 
proceedings of the tenth international 
conference, 1993

cascade correlation etc
s. e. fahlman and c. lebiere. the cascade-

correlation learning architecture. technical 
report cmu -cs-90-100, school of computer 
science, carnegie mellon university, 
pittsburgh, pa, 1990. 
http://citeseer.nj.nec.com/fahlman91cascadec
orrelation.html

j. h. friedman and w. stuetzle, projection pursuit 
regression, journal of the american statistical 
association, 76, 376, december, 1981

mars
j. h. friedman, multivariate adaptive regression 
splines, department for statistics, stanford 
university, 1988, technical report no. 102

copyright    2001, andrew w. moore

machine learning favorites: slide  90

45

