simons institute representation learning workshop

semi-random units for 
learning neural networks 

with guarantees

bo xie 

georgia tech

joint work with yingyu liang,

kenji kawaguchi and le song

learning neural networks

neural networks are extremely successful in 
learning many nonlinear functions

most are trained with simple stochastic 
id119 (sgd)

highly non-convex objective function

why sgd work so well?

 (u)

learning neural networks

one-hidden-layer neural networks with relu activation

f (x) =

least-squares loss

vk (w>k x)

nxk=1

l(f ) =

1
2m

mxl=1

(yl   f (xl))2

main results:

vk

wk

for    nice    neural weights, with high id203, 

any stationary point is a global optimum

the structure of the gradient

gradient w.r.t.    rst layer weights

@l
@wk

=

1
m

mxl=1

(f (xl)   yl) vk 0(w>k xl)xl

the structure of the gradient

gradient w.r.t.    rst layer weights

@l
@wk

=

1
m

mxl=1

(f (xl)   yl) vk 0(w>k xl)xl

266664

@l
@w1
. . .
@l
@wk
. . .
@l
@wn

377775

=

0bbbb@

v1 0(w>1 x1)x1

v1 0(w>1 xm)xm

vk 0(w>k x1)x1

vk 0(w>k xm)xm

vn 0(w>n x1)x1

vn 0(w>n xm)xm

      
      

      
      

      
      
      
      
      

1cccca

   

1

m0@

. . .

f (x1)   y1
f (xm)   ym

1a

the structure of the gradient

gradient w.r.t.    rst layer weights

@l
@wk

=

1
m

mxl=1

(f (xl)   yl) vk 0(w>k xl)xl

266664

@l
@w1
. . .
@l
@wk
. . .
@l
@wn

377775

=

0bbbb@

v1 0(w>1 x1)x1

v1 0(w>1 xm)xm

vk 0(w>k x1)x1

vk 0(w>k xm)xm

vn 0(w>n x1)x1

vn 0(w>n xm)xm

      
      

      
      

      
      
      
      
      

@l
@w

= d r

1cccca

   

1

m0@

. . .

f (x1)   y1
f (xm)   ym

1a

the structure of the gradient

gradient w.r.t.    rst layer weights

@l
@wk

=

1
m

mxl=1

(f (xl)   yl) vk 0(w>k xl)xl

266664

@l
@w1
. . .
@l
@wk
. . .
@l
@wn

377775

=

0bbbb@

v1 0(w>1 x1)x1

v1 0(w>1 xm)xm

vk 0(w>k x1)x1

vk 0(w>k xm)xm

vn 0(w>n x1)x1

vn 0(w>n xm)xm

      
      

      
      

      
      
      
      
      

1cccca

   

1

m0@

. . .

f (x1)   y1
f (xm)   ym

1a

@l
@w

= d r

non-singular?

the intuition

key inequality

krk    

training error

1

sm(d)    

@l

@w    

minimum  
singular value

norm of 
gradient

the intuition

key inequality

krk    

1

sm(d)    

@l

@w    

need to lower bound minimum singular value

bounding the error

key inequality

krk    

1

sm(d)    

@l

@w    

need to lower bound minimum singular value

directly analyze the singular value

gn = d>d/n

it is a function of the weights; 

dif   cult to analyze

bounding the error

key inequality

krk    

1

sm(d)    

@l

@w    

need to lower bound minimum singular value

directly analyze the singular value

gn = d>d/n

g = ew[gn]

introduce an intermediate variable 

that has uniform weights

bounding the error

key inequality

krk    

1

sm(d)    

@l

@w    

need to lower bound minimum singular value

directly analyze the singular value

gn = d>d/n

g = ew[gn]

decompose into two parts
 m(gn)  

i. ideal spectrum

ii. discrepancy

 m(g)

| {z }

  kg   gnk

|

{z

}

bounding the    rst term

id81 associated with relu

arccoshxi, xji

gij = ew    0(w>xi) 0(w>xj)   hxi, xji
   hxi, xji
=    1
1xu=1

 u u(xi) u(xj)

2  

2   

=

spherical harmonics 

decomposition

bounding the    rst term

id81 associated with relu

arccoshxi, xji

gij = ew    0(w>xi) 0(w>xj)   hxi, xji
   hxi, xji
=    1
1xu=1

 u u(xi) u(xj)

2  

2   

=

with high id203

 m(g)   m m/2

the spectrum of relu in between              and 

o(1/m)

o(1/pm)

bounding the second term

the di   erence between true weights and the expected one

kg   gnk     o(   (l2(w ))

bounding the second term

the di   erence between true weights and the expected one

kg   gnk     o(   (l2(w ))

weight discrepancy

(l2(w ))2 =

where

1
n2

nxi,j=1

k(x, y) =

1
2  

difference of expected 

and actual weights

k(wi, wj)2   eu,v   k(u, v)2   
arccoshx, yi

2   

a bound on the minimum singular value

with high id203

sm(d)2   nm m/2   cn   (l2(w ))

a simpli   ed result

with high id203

sm(d)2   nm m/2   cn   (l2(w ))

suppose     and    are large enough and weight discrepancy is small

d

n

n =      (1/ m)

d =      (1/ m)

l2(w ) =   o(n 1/4d 1/4)

then with high id203

sm(d)2      (m)

final error

d

n

for      and    large enough
for any       that has small weight discrepancy
with high id203

w

1
2m

mxl=1

(f (xl)   yl)2     o     

2!

@l

@w    

final error

d

n

for      and    large enough
for any       that has small weight discrepancy
with high id203

w

1
2m

mxl=1

(f (xl)   yl)2     o     

2!

@l

@w    

small gradient means small error!

final error

d

n

for      and    large enough
for any       that has small weight discrepancy
with high id203

w

1
2m

mxl=1

(f (xl)   yl)2     o     

and     are between                  and

d

o(pm)

n

2!

@l

@w    

o(m)

most       satisfy weight discrepancy small enough

w

recap

analyzed optimization landscape of one-hidden layer network

technical di   culty on ensuring small weight discrepancy

next: semi-random units

semi-random units

the main technical di   culty comes from the nonlinearity part

decouple relu: semi-random units

 (w>x) = i   w>x > 0    w>x
 (w>x) = i   r>x > 0    w>x

replace by random projections!

10

5

0

-5

-10
-5

5

0

0

5

-5

semi-random units

properties of semi-random units

   

it sits between fully-random features and fully-adjustable units


    linear in the parameters, but nonlinear in the input


    guaranteed to converge to global optimum w.h.p.


    has universal approximation ability

experiment results

matching the performance of relu

covtype dataset

webspam dataset

experiment results

width vs depth; depth helps more

covtype dataset

webspam dataset

random features suffer from huge errors.
table 2: test error (in %) of different methods on three image classi   cation benchmark datasets. 2   , 4    and 16   
mean the number of units used is 2 times, 4 times and 16 times of that used in neural network with relu respectively.

experiment results

image classi   cation benchmarks

neuron type mnist cifar10
relu
rf
rf 2   
rf 4   
rf 16   
sr
sr 2   
sr 4   

0.70
8.80
5.71
4.10
2.69
0.97
0.78
0.71

16.3
59.2
55.8
49.8
40.7
21.4
17.4
18.7

svhn

3.9
73.9
70.5
58.4
37.1
7.6
6.9
6.4

conclusion

for one-hidden-layer neural network, under weight diversity 
condition, any critical points are w.h.p. global optimal

the result depends on the spectrum decay of the kernel 
associated with the activation function

propose semi-random units and networks with these units are 
guaranteed to converge to global optimal

matching the performance of relu with slightly more units but 
much better than random features

