reinforcement 

learning

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

andrew w. moore
associate professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2002, andrew w. moore

april 23rd, 2002

predicting delayed rewards in a 

0.4

discounted markov system
0.6

0.5

r=0

s1

r=-5

s4

0.2

r=3

s2

0.2

0.5

r=0

0.5

s5

r=0

0.4

s3

r=1

0.4

s6

0.8

0.5

1

prob(next state = s5|this state = s4) = 0.8 etc   
what is expected sum of future rewards (discounted) ?

[ ]
(
)
sr
t

t

g

[ ]
0s

=

s

=

t

0

just solve it! we use standard markov system theory

copyright    2002, andrew w. moore

id23: slide  2

1

  
  
  
  
  
  
(cid:247)
  
(cid:246)
(cid:231)
  
(cid:230)
e
(cid:229)
  
learning delayed rewards   

s1

r=?

?

?
?
?

s4

r=?

?

?
?

s2

r=?

s5

?

?
?

r=?

?

?
?

?

?
?

s3

r=?

s6

r=?

?

?
?

all you can see is a series of states and rewards:

s1(r=0)    s2(r=0)    s3(r=4)    s2(r=0)    s4(r=0)    s5(r=0)

task: based on this sequence, estimate  j*(s1),j*(s2)      j*(s6)

copyright    2002, andrew w. moore

id23: slide  3

idea 1: supervised learning
assume
?=1/2
s1(r=0)    s2(r=0)    s3(r=4)    s2(r=0)    s4(r=0)    s5(r=0)

at t=1 we were in state s1 and eventually got a long term discounted 

reward of 0+?0+?24+?30+?40   = 1

at t=2 in state s2
at t=3 in state s3
at t=4 in state s2

ltdr = 2
ltdr = 4
ltdr = 0

at t=5 in state s4
at t=6 in state s5

ltdr = 0
ltdr = 0

state

observations 

mean ltdr

s1
s2
s3
s4
s5

of ltdr

1

2 , 0

4
0
0

1
1
4
0
0

=jest(s1)
=jest(s2)
=jest(s3)
=jest(s4)
=jest(s5)

copyright    2002, andrew w. moore

id23: slide  4

2

supervised learning alg

    watch a trajectory

s[0] r[0] s[1] r[1]          s[t]r[t]
][j
t

    for t=0,1,        t , compute

=   

g

i

[
tr

+

i

]

    compute

(

s

i

)

=

est

j

    you   re done!

copyright    2002, andrew w. moore

(
s

i

)

est

j

among

in 

beginning
y

= 0
i
mean value
]j[ of 
t
 transitio
 
all
 ns
 
on the
state
 
s
 trajector
[ ]
(
)
=
s
tst
{
[ ]
j
t
)
(
matches
s
(
i
matches
s

=

)

i

t

i

=

i

let 

matches

s

i

},

 then 

define

id23: slide  5

supervised learning alg

for the timid

       

if you have an anxious 
personality you may be worried 
about edge effects for some of 
the final transitions.  with large 
trajectories these are negligible.

copyright    2002, andrew w. moore

id23: slide  6

3

(cid:229)
(cid:247)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
(cid:231)
  
(cid:230)
(cid:229)
  
online supervised learning

initialize:   count[si] = 0   " si
sumj[s i] = 0   " si
eligibility[si] = 0   " si

observe:

when we experience s i with reward r

do this:
j    elig[sj]      ?elig[sj]

elig[sj]     elig[sj] + 1

j  sumj[sj]     sumj[s j]+rxelig[sj]
count[si]     count[si] + 1

then at any time,
jest(sj)= sumj[sj]/count[sj]

copyright    2002, andrew w. moore

id23: slide  7

online supervised learning 

economics

given n states  s1        sn ,  osl needs o(n) memory.
each update needs o(n) work since we must update all 

elig[  ] array elements

idea:  be sparse and only update/process elig[ ] 
elements with values >? for tiny ?
there are only
such elements

1
x

1
g

log

log

easy to prove:

t as

(
sj , 
i

est

)

(
sj

i

)
    

s

i

copyright    2002, andrew w. moore

id23: slide  8

4

"
"
"
   
  
   
*
(cid:247)
  
(cid:246)
(cid:231)
  
(cid:230)
(cid:247)
  
(cid:246)
(cid:231)
  
(cid:230)
online supervised learning

let   s grab osl off the street, bundle it into a 
black van, take it to a bunker and interrogate it 
under 600 watt lights.
s1(r=0)    s2(r=0)     s3(r=4)    s2(r=0)    s4(r=0)    s5(r=0)

complaint

state

observations of 

ltdr

1

2 , 0

4
0
0

s1
s2
s3
s4
s5

^  
j(si)

1
1
4
0
0

there   s something a little suspicious about this (efficiency-wise)

copyright    2002, andrew w. moore

id23: slide  9

certainty-equivalent (ce) learning

idea:  use your data to estimate the underlying 
markov system, instead of trying to estimate j 
directly.

s1(r=0)    s2(r=0)     s3(r=4)    s2(r=0)    s4(r=0)    s5(r=0)
estimated markov system:

you draw in the 
transitions + 
probs

s1

rest = 0

s2

rest = 0

s3

rest = 4

s4

rest = 0

s5

rest = 0

what   re the estimated j values?

copyright    2002, andrew w. moore

id23: slide  10

5

c.e. method for markov systems
initialize:

" si
" sj

#times visited si
count[si] = 0
sum of rewards from si
sumr[si] = 0 
trans[si,sj] = 0
#times transitioned from si sj
when we are in state si , and we receive reward r , and we 
move to sj    

count[si]     count[si] + 1
sumr[si]  sumr[si] + r

trans[si,sj]     trans[si,sj] + 1
then at any time

rest(sj) = sumr[si] / count[si] 

pest

ij = estimated prob(next = sj | this = si )
= trans[si,sj] / count[si] 

copyright    2002, andrew w. moore

id23: slide  11

c.e. for markov systems 

(continued)    

so at any time we have

rest(sj)  and  pest (next=sj | this=si )
" sisj
(
s
i

= pest
ij
)

(
s
i

(cid:229)+
g

=

(

(
est
sjssp

)

)

est

est

est

r

j

i

j

so at any time we can solve the set of linear equations

)

j

s

j

[in vector notation,

jest = rest + ?pestj

=>   jest = (i-?pest)-1rest

where  jest rest are vectors of length n

is an nxn matrix

pest
n = # states ]

copyright    2002, andrew w. moore

id23: slide  12

6

c.e. online economics

memory:   o(n2)
time to update counters:   o(1)
time to re-evaluate  jest

    o(n3)  if use matrix inversion
    o(n2kcrit)  if use value iteration and we need 

kcrit iterations to converge

    o(nkcrit)  if use value iteration, and kcrit to 
converge, and m.s. is sparse (i.e. mean # 
successors is constant)

copyright    2002, andrew w. moore

id23: slide  13

certainty equivalent learning

complaint

memory use could be  o(n2) !
and time per update could be  o(nkcrit)  up to 
o(n3) !

too expensive for some people.

prioritized sweeping will help, (see later), but first 
let   s review a very inexpensive approach

copyright    2002, andrew w. moore

id23: slide  14

7

why this obsession with 

onlineiness?

i really care about supplying up-to-date   jest
estimates all the time.

can you guess why?

if not, all will be revealed in good time   

copyright    2002, andrew w. moore

id23: slide  15

less time: more data

limited backups

   do previous c.e. algorithm.
   at each time timestep we observe si(r)   sj and update 

count[si], sumr[si], trans[si,sj]
   and thus also update estimates

est

r

i

and
  

  p  

est

ij

(
)i
 
outcomes
s

j

but instead of re-solving for jest, do much less work.  
just do one    backup    of

[
est sj

]

i

[
est
sj
i

]

(cid:229)+
g

est

r

i

j

[
est
sjp
ij

est

j

]

copyright    2002, andrew w. moore

id23: slide  16

8

  
"
   
   one backup c.e.    economics

space  :  o(n2)

no improvement 
there!

time to update statistics  : o(1)

time to update jest :  o(1)

v good news:  much cheaper per transition

v good news:  contraction mapping proof (modified) 

promises convergence to optimal

v bad news:  wastes data

copyright    2002, andrew w. moore

id23: slide  17

prioritized sweeping

[moore + atkeson,    93]
tries to be almost as data-efficient as full ce but not 
much more expensive than    one backup    ce.

on every transition, some number (  ) of states may 
have a backup applied.  which ones?

    the most    deserving   
    we keep a priority queue of which states have 
the biggest potential for changing their jest(sj) 
value

copyright    2002, andrew w. moore

id23: slide  18

9

where are we?
trying to do online jest prediction from streams 
of transitions

space

0(ns)

0(nso)

supervised 
learning
full c.e. 
learning

one backup c.e. 
learning
prioritized 
sweeping

0(nso)

0(nso)

jest update cost

0(         )

__1__
log(1/?)

0(nsons)
0(nsokcrit)
0(1)

0(1)

nso= # state-outcomes (number of arrows on the m.s. diagram)
ns= # states

what next ?

sample backups !!!

d
a
a

t

 

e

i

f
f
i
c
e
n
c
y
:

       

       

       

       

copyright    2002, andrew w. moore

id23: slide  19

temporal 
difference 
learning

only maintain a jest array   
nothing else

so you   ve got

jest (s1)  jest (s2) ,        jest (sn)

and you observe

r

si

sj

what should you do?

[sutton 1988]

a transition from i that receives 
an immediate reward of r and 
jumps to j

can you guess ?

copyright    2002, andrew w. moore

id23: slide  20

10

td learning

r

si

sj
we update  =
we nudge it to be closer to expected future rewards

(
est sj
i

)

est

)

(
(
)
)
(
+
a
est
sj
s
1
j
i
i
[
expected future
a
          
          
          
        
rewards
[
(
)
(
+
r +
-=
est
a
g
a
sj
  
   
1
a
  

)

i

d
e
t
h
g
e
w

i

m
u
s

]
)

j

]
(
est
sj

is called a    learning rate    parameter.  (see 

   ?    in the neural lecture)

copyright    2002, andrew w. moore

id23: slide  21

simplified td analysis

p1=?

p2=?

r1= ?

terminate

r2= ?

terminate

pm=?

:

s0

r=0

rm= ?

terminate

    suppose you always begin in s0
    you then transition at random to one of m places.  you don   t know the 
transition probs.  you then get a place-dependent reward (unknown in 
advance).

    then the trial terminates.
define j*(s0)= expected reward

let   s estimate it with td

copyright    2002, andrew w. moore

id23: slide  22

11

-
   
p(1)

p(2)

  
  
  

p(n)

s0

r=0

r(1)

r(2)

r(n)

r(k) = reward of k   th terminal 

state

p(k) = prob of k   th terminal 

state

we   ll do a series of trials.  reward on t   th 

trail is rt

e=

[ ]
r
t

(cid:229)=
k

n

=
1

( )
k
rp

( )
k

[
    

 note

[ ]
 isr
t

independen

]t
 of
t 

define j*(s0) = j* = e[rt]

copyright    2002, andrew w. moore

id23: slide  23

let   s run td-learning, where
jt = estimate   jest(s0) before the t   th trial.

from definition of td-learning:

jt+1 = (1-a)jt + art
useful quantity:  define
=
 
 of
  
variance
(
( )
m
(cid:229)=
k
rp 
=
k
1

     

s

j

2

(

)

k

e=
 
reward
)2

[
(
r
t

]

)

2

j

copyright    2002, andrew w. moore

id23: slide  24

12

e
*
*
-
-
j* = e[rt], s 2 = e[(rt-j*)2]
jt+1 = art + (1-a)jt

)
j

t

a

]

j

?

 

 

y
h
w

 

]

j

t

remember

[
j 1
[
+
t
e=
a
r
t

]
=
j
(
-+
1

a

) [
(
-=
j
1
thus...
     
[
]
lim   
j

t

t

=

j

is this 

impressive??

copyright    2002, andrew w. moore

id23: slide  25

remember

j* = e[rt],  s 2 = e[(rt-j*)2]
jt+1 = art + (1-a)jt

write st = expected squared error between
jt and j* before the t   th iteration

st+1 = e[(jt+1-j*)2]

= e[(art+(1-a)jt - j*)2]
= e[(a[rt-j*]+(1-a)[jt - j*])2]
= e[a2(rt-j*)2+a(1-a)(rt-j*)(jt - j*)+(1-a)2(jt - j*)2]
= a2e[(rt-j*)2]+a(1-a)e[(rt-j*)(jt - j*)]+(1-a)2e[(jt - j*)2]
=
= a2s 2+(1-a)2st

w h y ?

copyright    2002, andrew w. moore

id23: slide  26

13

*
  
   
*
*
*
e
-
e
-
-
e
and it is thus easy to show that    .

lim
t

s 

t

=

lim
t

[
(
j

)

2

j

t

]

=

as
2(

2

a

)

    what do you think of td learning?
    how would you improve it?

copyright    2002, andrew w. moore

id23: slide  27

est

)

(
s
i

decaying learning rate
[dayan 1991ish] showed that for general td
learning of a markow system (not just our simple 
model) that if you use update rule
]
)
(
-+
1
then, as number of observations 
(
) i
* sj
goes to infinity
provided
    all states visited 8 ly often
   

(
est
g
sj
)

)
(
est
sj

(
est
sj

[
r
i

means

 this

 .t.

  =

a

a

+

)

j

a

>

k

k

i

i

i

t

j

t

=

t

t

1

t

t

 this

means

a
1

=

t

a
1

=

t

   

  <

2

t

copyright    2002, andrew w. moore

k

.

 t.

a

2
t

<

k

t

=
1

t

id23: slide  28

14

-
-
e
*
  
   
  
   
   
"
   
(cid:229)
(cid:229)
  
  
(cid:229)
(cid:229)
"
$
$
"
decaying learning rate

this works:   at = 1/t
this doesn   t: at = a0
this works: at =   /(  +t)   [e.g.   =1000]
this doesn   t: at =   at-1 (  <1)
in our example   .use at = 1/t
remember

j
          

e=

e=

2

[ ]
s
   ,r
t
(
)
-+
1
j
(
)
-=
t
j1

a
t

t

=

1
t
and

   

t

2

)j

]
[
r(
)
(
t
-+
j11

r
t

 

you'

j

+
1

t

=

a

r
t

t

c   write

t

t

t
 that
see ll
t

+

r
t

=
1

i

1
t

          

c  

+
1

t

+=

so   cr
t

t

j   

+
1

t

=

j

0

and   

copyright    2002, andrew w. moore

id23: slide  29

decaying learning rate con   t   

[
(
j-j
t

]

)

2

s

=

         
   

 so,

ultimately

lim   

t

2

2

+

(
j-j
0
[
t
)
(
j-j
t

)
] 0

=

2

copyright    2002, andrew w. moore

id23: slide  30

15

  
  
  
  
  
  
(cid:229)
-
*
*
e
e
*
  
   
*
*
a fancier td   

write s[t] = state at time t
suppose a = 1/4   ? = 1/2
assume jest(s23)=0   jest (s17)=0   jest (s44)=16
assume t = 405   and   s[t] = s23

observe s23

s17 with reward 0

(r=0)

now   t = 406,   s[t] = s17,   s[t-1] = s23

jest (s23)=          , jest (s17)=         , jest (s44)=

observe s17

(r=0)

s44

now   t = 407,   s[t] = s44

jest (s23)=          , jest (s17)=         , jest (s44)=
insight: jest (s23)  might think

i gotta get me some of that !!!

copyright    2002, andrew w. moore

id23: slide  31

td(?) comments

td(?=0) is the original td
td(?=1) is almost the same as supervised learning (except it 

uses a learning rate instead of explicit counts)
td(?=0.7) is often empirically the best performer
    dayan   s proof holds for all 0=?=1
    updates can be made more computationally efficient with 

   eligibility    traces (similar to o.s.l.)

    question:

vcan you invent a problem that would make td(0) look 

bad and td(1) look good?

vhow about td(0) look good & td(1) bad??

copyright    2002, andrew w. moore

id23: slide  32

16

learning m.s. summary

space

supervised learning

0(ns)

full c.e. learning

one backup c.e. 
learning
prioritized sweeping

td(0)

0(nso)

0(nso)

0(nso)

0(ns)

td(?) , 0<?=1

0(ns)

d
e
s
a
b

-
l
e
d
o
m

 

e
e
r
f
l
e
d
o
m

j update 
cost

0

1
log

1
g

0(nsons)
0(nsokcrit)
0(1)

0(1)

0(1)

0

1
1
log
gl

data 

efficiency

       
       

       
       
       
       

copyright    2002, andrew w. moore

id23: slide  33

learning policies 

for mdps

see previous lecture 
slides for definition of and 
computation with mdps.

the heart

of

reinforcement

learning

state

copyright    2002, andrew w. moore

id23: slide  34

17

(cid:247)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
(cid:231)
  
(cid:230)
(cid:247)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
(cid:231)
  
(cid:230)
the task:

world: you are in state 34.

your immediate reward is 3.  you have 3 actions.
i   ll take action 2.

robot:
world: you are in state 77.

your immediate reward is -7.  you have 2 actions.
i   ll take action 1.

robot:
world: you   re in state 34 (again).

your immediate reward is 3.  you have 3 actions.      
the markov property means once you   ve selected an 
action the p.d.f. of your next state is the same as the 
last time you tried the action in this state.

copyright    2002, andrew w. moore

id23: slide  35

the    credit assignment    problem

i   m in state 43,
                     39,   
                     22,

reward = 0,
         = 0,
         = 0,

action = 2
        = 4
        = 1

                     21,

         = 0,

        = 1

                     21,
                     13,
                     54,
                     26,

         = 0,
         = 0,
         = 0,
      = 100,

        = 1
        = 2
        = 2

yippee!  i got to a state with a big reward!  but which of my 
actions along the way actually helped me get there??
this is the credit assignment problem.
it makes supervised learning approaches (e.g. boxes
[michie & chambers]) very, very slow.
using the mdp assumption helps avoid this problem.

copyright    2002, andrew w. moore

id23: slide  36

18

mdp policy learning

space

update cost

data 

full c.e. 
learning
one backup 
c.e. learning
prioritized 
sweeping

0(nsao)

0(nsao)

0(nsao)

0(nsaokcrit)

0(n?0)

0(  n?0)

efficiency

       
       
       

    we   ll think about model-free in a moment   
    the c.e. methods are very similar to the ms case, except now do 

value-iteration-for-mdp backups

)

=

(
s
i

est

j

est
rmax
i

a

+

g

p
(
s
succs
i

est
)

s

j

(
,ss
i

j

(
)
est
sj

a

j

)

copyright    2002, andrew w. moore

id23: slide  37

choosing actions

si
we   re in state    
we can estimate     ri
est
                                  pest(next = sj | this = si , action a)
                                 
jest (next = sj )
so what action should we choose ?

(

est
,ssp
i

j

a

(
)
est
sj

)

j

idea  

   :1
a

=

arg

+

g

rmax
i
a

j

=

   :2
a

random

idea  
    any problems with these ideas?
    any other suggestions?
    could we be optimal?

copyright    2002, andrew w. moore

id23: slide  38

19

  
  
  
  
  
  
  
  
(cid:229)
  
  
  
  
  
  
  
  
(cid:229)
  
model-free r.l.

why not use  t.d. ?
observe

r

update
(
)
j
s

est

i

si

a

sj

(
r
i

(
est
g
sj

+

j

)
)

(
-+
1

a
  

)
j

a

est

)

(
s
i

what   s wrong with this?

copyright    2002, andrew w. moore

id23: slide  39

id24: model-free r.l.

[watkins, 1988]

define
q*(si,a)= expected sum of discounted future 

rewards if i start in state si, if i then take action a, 
and if i   m subsequently optimal

questions:

define q*(si,a) in terms of j*

define j*(si) in terms of q*

copyright    2002, andrew w. moore

id23: slide  40

20

   
id24 update

note that
)
q
a

(
s,

+=

r
i

(
,ssp
i

j

a

)

max

a

q

(
,s
j

a

)

g

s

j

succs

(

s
i

)

in id24 we maintain a table of qest values instead 
of jest values   
when you see si
   +
(
est
r
,s
i
i

sj do   
(
est
a
,s
j

)
(
-+  
1

max

)
q

action a

reward

q

a

a

a

)

g

est

1

q

(
,s
i

)a

a

this is even cleverer than it looks:  the qest values are 
not biased by any particular exploration policy.  it 
avoids the credit assignment problem.

copyright    2002, andrew w. moore

id23: slide  41

id24: choosing actions
same issues as for ce choosing actions

    don   t always be greedy, so don   t always choose:
    don   t always be random (otherwise it will take a long time 

to reach somewhere exciting)

arg

(

)asi
,q max
a

    boltzmann exploration  [watkins]

prob(choose action a) 

exp

q

)

as
,

est

(
k

t

    optimism in the face of uncertainty  [sutton    90, kaelbling 

   90]
(cid:216) initialize q-values optimistically high to encourage exploration
(cid:216) or take into account how often each s,a pair has been tried

copyright    2002, andrew w. moore

id23: slide  42

21

(cid:229)
  
*
  
*
  
  
  
  
  
   
  
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
-
(cid:181)
id24 comments

    [watkins] proved that id24 will eventually 

converge to an optimal policy.

    empirically it is cute
    empirically it is very slow
    why not do q(?) ?

(cid:216) would not make much sense [reintroduce the credit 

assignment problem]

(cid:216) some people (e.g. peng & williams) have tried to work 

their way around this.

copyright    2002, andrew w. moore

id23: slide  43

if we had time   

    value function approximation

(cid:216) use a neural net to represent  jest [e.g. tesauro]
(cid:216) use a neural net to represent qest [e.g. crites]
(cid:216) use a decision tree

   with id24  [chapman + kaelbling    91]
   with c.e. learning  [moore    91]
   how to split up space?

    significance test on q values  [chapman + 
kaelbling]
    execution accuracy monitoring  [moore    91]
    game theory  [moore + atkeson    95]
    new influence/variance criteria  [munos    99]

copyright    2002, andrew w. moore

id23: slide  44

22

if we had time   

    r.l. theory

(cid:216) counterexamples  [boyan + moore], [baird]
(cid:216) value function approximators with averaging will 

converge to something  [gordon]

(cid:216) neural nets can fail  [baird]
(cid:216) neural nets with residual gradient updates will 

converge to something

(cid:216) linear approximators for td learning will converge 

to something useful  [tsitsiklis + van roy]

copyright    2002, andrew w. moore

id23: slide  45

what you should know

    supervised learning for predicting delayed rewards
    certainty equivalent learning for predicting delayed 

rewards

    model free learning (td) for predicting delayed 

rewards

    id23 with mdps: what   s the 

task?

    why is it hard to choose actions?
    id24 (including being able to work through 

small simulated examples of rl)

copyright    2002, andrew w. moore

id23: slide  46

23

