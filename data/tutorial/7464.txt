symbolic memories 
in the brain

christos papadimitriou
uc berkeley
   work with   

                        santosh vempala


                                           

                                wolfgang maass

brain and computation:


babies vs computers
clever algorithms vs what happens in cortex
deep nets vs the brain
understanding brain anatomy and function vs understanding the emergence of the mind



the great
disconnects
how does the mind 
emerge from the brain? 

how does the mind 
emerge from the brain? 

how does one think computationally about the brain?

good question!

john von neumann 1950:
[the way the brain works] 
may be characterized by less 
logical and arithmetical depth 
than we are normally used to 
  
les valiant on the brain (1994)
a computational 
theory of the brain is 
possible and essential

the neuroidal model 
 and vicinal algorithms

david marr (1945     1980)



the three-step program:
	specs
	algorithm
	hardware
so, can we use marr   s framework 
to identify the algorithm run by the brain??
sgd?
kernel trick?
pca?
lp?    
        sdp?
j-lindenstrauss?       fft?
em?
adaboost?
hashing?
id90?
id166?
our approach
come up with computational theories consistent as much as possible with what we know from neuroscience
start by admitting defeat: expect large-scale algorithmic heterogeneity
 start at the boundary between    symbolic/subsymbolic    brain function
one candidate: assemblies of excitatory neurons in medial temporal lobe (mtl)
the experiment by 
[ison et al. 2016]


the experiment by 
[ison et al. 2016]



the experiment by 
[ison et al. 2016]


the experiment by 
[ison et al. 2016]



the experiment by 
[ison et al. 2016]


the experiment by 
[ison et al. 2016]



the experiment by 
[ison et al. 2016]



the experiment by 
[ison et al. 2016]


the experiment by 
[ison et al. 2016]



the challenge:

these are the specs (marr)
what is the hardware?
what is the algorithm?

speculating on the hardware
a little analysis first
they recorded from ~102 out of ~107 mtl neurons in every subject
showed ~102 pictures of familiar persons/places, with repetitions
~ each of ~10 neurons responded consistently to one image
id48mm...

speculating on hardware (cont.)
each memory is represented by an assembly of many (perhaps  ~ 104 - 105 ) neurons; 
    cf  [hebb 1949], [buzsaki 2003, 2010]
highly connected, therefore stable
it is somehow formed by sensory stimuli
every time we think of this memory, ~ all these neurons fire
two memories can be associated by    seeping    into each other


algorithm?

how are assemblies formed?
how are they recalled?
how does association happen?
the theory challenge

in a sparse graph, how can you select a dense induced subgraph?



mtl, ~107 neurons
   sensory
cortex   



~104 neurons
stimulus
~104 neurons
[selection by inhibition]






















nb: these are scattered
neurons!
~104 neurons

a metaphor: olfaction 
in the mouse [al. et axel 2011]

1


2


3


from the discussion section
of [al. et axel]
   an odorant may evoke suprathreshold input in a small subset of     neurons. this small fraction of ... cells would then generate sufficient recurrent excitation  to recruit a larger population of neurons... the strong feedback inhibition resulting from activation of this larger population of neurons would then suppress further spiking    in the extreme, some cells could receive enough recurrent input to fire     without receiving [initial] input   

mtl, ~107 neurons
   sensory
cortex   



~104 neurons
~104 neurons
random graph, 
 p ~ 10   2
synapses
plasticity

hebb 1949:    fire together, wire together   
stdp: near synchronous firing of two cells connected by a synapse increases synaptic weight (by a small factor, up to a limit)
but how does one verify 
such a theory? 


it is reproduced in simulations by id158s with realistic (scaled down) parameters [pokorny et al 2017]

math?

linearized model 

xj(t+1) = sj +   i   j   xi(t) wij(t)


wij(t+1) = wij(t) [1 +    xi(t) xj(t + 1)]


activation
synaptic weights
stimulus
plasticity
linearized model: result 
theorem:  the linearized dynamics converges geometrically to
          xj = sj +      i   j   xi2






nonlinear model: 
a quantitative narrative
                     xj = sj +      i   j  xi2
the high sj cells fire
next, high connectivity cells fire
next, among the high sj cells, the ones with high connectivity fire again
   the rich get stably rich    through plasticity
a part of the assembly may keep oscillating (periods of 2 and 3 are common)
mysteries remain...

how can a set of random neurons have exceptionally strong connectivity?
and how are associations (obama + eiffel) formed?

 






high connectivity? associations?
random id207 does not seem to suffice


[song et al 2005]: reciprocity
    and triangle completion
 





gn,p++
p ~ 10   1
gn,p
p ~ 10   2






birthday
paradox!
also, 
inside 
assemblies


recall the theory challenge

in a sparse graph, how can you select a dense induced subgraph?

answer:  through
recruiting highly connected nodes (recall equation)
plasticity
triangle completion and birthday paradox



remember marr?



the three-step program:
	specs
	algorithm
	hardware
another operation: bind 
e. g.,    give    isa    verb    
not between assemblies, but...
...between an assembly and a brain area
a    pointer assembly,    a surrogate for    give,    is formed in the    verb    area
also supported by simulations [legenstein et al. 2017] and same math








bind:

mtl
   give   

   verb area   
   assembly pointer   
cf  [valiant 1994]
items: internal connectivity immaterial
operations join, link     association
formed by recruiting completely new cells
through orchestrated precise setting of the parameters (strengths, thresholds) 
also, predictive join [p. vempala colt 2015]
can do a bunch of feats, but is subject to same criticism viz plausibility

incidentally, a theory problem:
association graphs
are these
legitimate strengths
of associations 
between ~equal
assemblies?
connection to the 
cut norm
(also with anari & saberi)






0.4
0.5
0.2
0.6
0.3
btw, the mystery of invariants
how can very different stimuli elicit the same memory?
e.g. different projections, rotations, and zooms of a familiar face, the person   s voice, gait, and name
association gone awry?







   eifel   
finally: the brain in context
the environment is paramount
language:  an environment created by us a few thousand generations ago
a    last-minute adaptation   
hypothesis: it evolved so as to exploit the brain   s strengths
language is optimized so babies can learn it
language!

knowledge of language = grammar
some grammatical knowledge may predate experience (is innate)
grammatical minimalism: s     vp np 
assemblies, association and bind (and pjoin) seem ideally suited for implementing grammar and language in the brain.


is s     np vp innate?
sooooooo   
how does one think computationally about the brain? 
assemblies may be one path
experimental neuroscience and cognitive science provide specs, hardware
the algorithm [rather often] vanishes
ultimately, language






next spring, 
in an oddly familiar building   








program in computation and the brain!
be there   
 thank you!
