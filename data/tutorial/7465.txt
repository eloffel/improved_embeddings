an introduction to id21 and domain

adaptation

amaury habrard

laboratoire hubert curien, umr cnrs 5516, universit  e de saint-etienne

epat 2014

(lahc)

id20 - epat   14

1 / 95

some resources

list of id21 papers
http://www1.i2r.a-star.edu.sg/~jspan/conferencetl.htm
list of available softwares
http://www.cse.ust.hk/tl/index.html
surveys

patel, gopalan, chellappa. visual id20: an overview of
recent advances. tech report, 2014.
qi li. literature survey: id20 algorithms for natural
language processing, tech report, 2012
margolis. a literature review of id20 with unlabeled
data. tech report 2011.
pan and yang. a survey on id21   , tkde 2010.
j. quionero-candela and m. sugiyama and a. schwaighofer and n.d.
lawrence. dataset shift in machine learning. mit press.

(lahc)

id20 - epat   14

2 / 95

credits and acknowledgments

documents used for this talk:

d. xu, k. saenko, i. tsang. tutorial on domain id21 for vision
applications, cvpr   12.

s. pan, q. yang and w. fan. tutorial: id21 with applications,
ijcai   13.

s. ben-david. towards theoretical understanding of id20
learning, workshop lniid at ecml   09.

f. sha and b. kingsbury. id20 in machine learning and speech
recognition, tutorial - interspeech 2012.

k. grauman. adaptation for objects and attributes, workshop visda at iccv   13

j. blitzer and h. daum  eiii. id20, tutorial - icml 2010.

a. habrard, jp peyrache, m. sebban. iterative self-labeling id20
for linear structured image classi   cation, ijait 2013.

a. habrard, jp peyrache, m. sebban. boosting for unsupervised domain
adaptation, ecml 2013

acknowledegments: b. fernando, p. germain, e. morvant, jp peyrache, m. sebban.

(lahc)

id20 - epat   14

3 / 95

id21

de   nition [pan, tl-ijcai   13 tutorial]
ability of a system to recognize and apply knowledge and skills learned in
previous domains/tasks to novel domains/tasks

an example
    we have labeled images from a web image corpus
    is there a person in unlabeled images from a video corpus ?

person

no person

is there a person?

(lahc)

id20 - epat   14

4 / 95

?outline

1

introduction/motivation

2 reweighting/instance based methods

3 theoretical frameworks

4 feature/projection based methods

5 adjusting/iterative methods

6 a quick word on model selection

(lahc)

id20 - epat   14

5 / 95

introduction

(lahc)

id20 - epat   14

6 / 95

settings

domains are modeled as id203 distributions over an instance
space

tasks associated to a domain (classi   cation, regression, id91, ...)
objective: from source to target
    improve a target predictive function in the target domain using
knowledge from the source domain

(lahc)

id20 - epat   14

7 / 95

?from the same domain?from different domainsa taxonomy of id21

   a survey on id21    [pan and yang, tkde 2010]

(lahc)

id20 - epat   14

8 / 95

in this presentation

we will make a focus on id20

we will focus on classi   cation tasks

    how can we learn, using labeled data from a source distribution, a
low-error classi   er for another related target distribution?
       hot topic    - tutorials at icml 2010, cvpr 2012, interspeech 2012,
workshops at iccv 2013, nips 2013,ecml 2014
    motivating examples

(lahc)

id20 - epat   14

9 / 95

a toy problem: inter-twinning moons

(a) 10   

(b) 20   

(c) 30   

(d) 40   

(e) 50   

(f) 70   

(lahc)

id20 - epat   14

10 / 95

intuition and motivation from a cv perspective

   can we train classi   ers with flickr photos, as they have already been
collected and annotated, and hope the classi   ers still work well on
mobile camera images?    [gonq et al., cvpr 2012]
   object classi   ers optimized on benchmark dataset often exhibit
signi   cant degradation in recognition accuracy when evaluated on
another one    [gonq et al.,icml 2013, torralba et al., cvpr 2011,
perronnin et al., cvpr 2010]
   hot topic    -visual id20 [tutorial cvpr   12, iccv   13]

(lahc)

id20 - epat   14

11 / 95

brief recap on id161 issues [slides from j. sivic]

(lahc)

id20 - epat   14

12 / 95

brief recap on id161 issues (2) [slides from j.
sivic]

(lahc)

id20 - epat   14

13 / 95

brief recap on id161 issues (3) [slides from j.
sivic]

(lahc)

id20 - epat   14

14 / 95

brief recap on id161 issues (4) [slides from j.
sivic]

(lahc)

id20 - epat   14

15 / 95

brief recap on id161 issues (5) [slides from j.
sivic]

(lahc)

id20 - epat   14

16 / 95

problems with data representations

[xu,saenko,tsang, domain transfer tutorial - cvpr   12]

id20 - epat   14

(lahc)

17 / 95

hard to predict what will change in the new domain

[xu,saenko,tsang, domain transfer tutorial - cvpr   12]

(lahc)

id20 - epat   14

18 / 95

natural language processing

text are represented by    words    (bag of words)

id52: adapt a tagger learned from medical papers
to a journal (wall street journal) - newsgroup

spam detection: adapt a classi   er from one mailbox to another

id31:

(lahc)

id20 - epat   14

19 / 95

id20 for id31

(lahc)

id20 - epat   14

20 / 95

id20 for id31 - ex
[pan-ijcai   13 tutorial]

electronics

video games

(1) compact; easy to operate; very
good picture quality; looks sharp!

(3) i purchased this unit from circuit
city and i was very excited about the
quality of the picture. it is really nice
and sharp.

(5) it is also quite blurry in very dark
settings. i will never buy hp again.

(2) a very good game!
packed and full of excitement.
very much hooked on this game.

it is action
i am

(4) very realistic shooting action and
good plots. we played this and were
hooked.

(6) it is so boring.
i am extremely
unhappy and will probably never buy
ubisoft again.

source speci   c: compact, sharp, blurry.

target speci   c: hooked, realistic, boring.

domain independent: good, excited, nice, never buy, unhappy.

(lahc)

id20 - epat   14

21 / 95

other applications

id103 [tutorial at interspeech   12]

medecine

biology

time series

wi    localization

(lahc)

id20 - epat   14

22 / 95

notations

notations

x     rd input space, y = {   1, +1} output space
ps source domain: distribution over x    y
ds marginal distribution over x
pt target domain: di   erent distribution over x    y
dt marginal distribution over x
h     y x : hypothesis class

expected error of a hypothesis h : x     y

i(cid:2)h(xs ) (cid:54)= y s(cid:3) source domain error
i(cid:2)h(xt) (cid:54)= y t(cid:3) target domain error

rps (h) =

rpt (h) =

e

(xs ,y s )   ps

e

(xt ,y t )   pt

id20:    nd h     h with rpt small from data     dt and ps

(lahc)

id20 - epat   14

23 / 95

classical result in supervised learning

empirical error
rs = {(xs
associated empirical error of an hypothesis h:

i )}ms
i=1     (ps )ms a labeled sample drawn i.i.d. from ps
ms(cid:88)

i , y s

(cid:3)

i(cid:2)h(xs

i ) (cid:54)= y s

i

rs (h) =

1
ms

i=1

classical pac result: from the same distribution

rps (h)     rs (h) + o(

complexity (h)

   

ms

)

    occam razor principle
what about rpt if we have no or very few labeled data?     try to make
use of source information

(lahc)

id20 - epat   14

24 / 95

id20
setting

i=1 source sample drawn i.i.d. from ps

labeled source sample
s = {(xi , yi )}ms
unlabeled target sample
t = {xj}mt
optionnal: a few labeled target examples

j=1 target sample drawn i.i.d. from dt

if h is learned from source domain, how does it perform on target
domain?

(lahc)

id20 - epat   14

25 / 95

id20
setting

i=1 source sample drawn i.i.d. from ps

labeled source sample
s = {(xi , yi )}ms
unlabeled target sample
t = {xj}mt
optionnal: a few labeled target examples

j=1 target sample drawn i.i.d. from dt

if h is learned from source domain, how does it perform on target
domain?

(lahc)

id20 - epat   14

25 / 95

illustration settings

classical supervised learning

id20

(lahc)

id20 - epat   14

26 / 95

apprentissage supervis    chantillon  tiquet  i.i.d. selon pmod  leapprentissagedistributionpss  chantillonnon   tiquet  i.i.d. selon dadaptation de domaine  chantillon  tiquet  i.i.d. selon pmod  leapprentissagedistributionpstdistributiondiff  rente ptsa bit of vocabulary

unsupervised id21
no labels

unsupervised da
presence of source labels, no target labels

semi-supervised da
presence of source labels, few target labels and a lot of unlabeled data
(cid:54)= semi-supervised learning
no distribution shift, few labeled data and a lot of unlabeled data from the
same domain

(lahc)

id20 - epat   14

27 / 95

some key points

estimating of the distribution shift

deriving generalization guarantees
rpt (h)    ?rps (h)?+?

(lahc)

id20 - epat   14

28 / 95

some key points

estimating of the distribution shift

deriving generalization guarantees
rpt (h)    ?rps (h)?+?
characterizing when the adaptation is possible

(lahc)

id20 - epat   14

28 / 95

some key points

estimating of the distribution shift

deriving generalization guarantees
rpt (h)    ?rps (h)?+?
characterizing when the adaptation is possible

de   ning algorithms
underlying idea: try to move closer the two distributions

(lahc)

id20 - epat   14

28 / 95

some key points

estimating of the distribution shift

deriving generalization guarantees
rpt (h)    ?rps (h)?+?
characterizing when the adaptation is possible

de   ning algorithms
underlying idea: try to move closer the two distributions

applying model selection principle
how to tune hyperparameters with no labeled information from target

(lahc)

id20 - epat   14

28 / 95

3 main classes of algorithms

reweighting/instance-based methods

correct a sample bias by reweighting source labeled data:
source instances close to target instances are more important.

feature-based methods/find new representation spaces

find a common space where source and target are close
(projection, new features, etc)

ajustement/iterative methods

modify the model by incorporating pseudo-labeled information

(lahc)

id20 - epat   14

29 / 95

much recent researchcorrecting samplingbias[shimodaira,    00][huang et al., bickel et al.,    07][sugiyamaet al.,    08][sethy et al.,    06][sethy et al.,    09][this work]adjusting mismatched models[evgeniou and pontil,    05][duan et al.,    09][duan et al., daum   iii et al., saenko et al.,    10][kulis et al., chen et al.,    11]+----+++----++inferring domain-invariant features[pan et al.,    09][blitzer et al.,    06][gopalan et al.,    11][chen et al.,    12][daum   iii,    07][argyriou et al,    08][gong et al.,    12][muandet et al.,    13]+++--+-+-+much recent researchcorrecting samplingbias[shimodaira,    00][huang et al., bickel et al.,    07][sugiyamaet al.,    08][sethy et al.,    06][sethy et al.,    09][this work]adjusting mismatched models[evgeniou and pontil,    05][duan et al.,    09][duan et al., daum   iii et al., saenko et al.,    10][kulis et al., chen et al.,    11]+----+++----++inferring domain-invariant features[pan et al.,    09][blitzer et al.,    06][gopalan et al.,    11][chen et al.,    12][daum   iii,    07][argyriou et al,    08][gong et al.,    12][muandet et al.,    13]+++--+-+-+much recent researchcorrecting samplingbias[shimodaira,    00][huang et al., bickel et al.,    07][sugiyamaet al.,    08][sethy et al.,    06][sethy et al.,    09][this work]adjusting mismatched models[evgeniou and pontil,    05][duan et al.,    09][duan et al., daum   iii et al., saenko et al.,    10][kulis et al., chen et al.,    11]+----+++----++inferring domain-invariant features[pan et al.,    09][blitzer et al.,    06][gopalan et al.,    11][chen et al.,    12][daum   iii,    07][argyriou et al,    08][gong et al.,    12][muandet et al.,    13]+++--+-+-+reweighting/instance based
methods

(lahc)

id20 - epat   14

30 / 95

context

motivation

domains share the same support (i.e. bag of words)
distribution shift is caused by sampling bias/shift between
marginals

idea
reweight or select instances to reduce the discrepancy between source
and target domains.

(lahc)

id20 - epat   14

31 / 95

a    rst analysis

i(cid:2)h(xt) (cid:54)= y t(cid:3)

rpt (h) =

e

(xt ,y t )   pt

(lahc)

id20 - epat   14

32 / 95

a    rst analysis

rpt (h) =

e

(xt ,y t )   pt

i(cid:2)h(xt) (cid:54)= y t(cid:3)

i(cid:2)h(xt) (cid:54)= y t(cid:3)

=

e

(xt ,y t )   pt

ps (xt, y t)
ps (xt, y t)

(lahc)

id20 - epat   14

32 / 95

a    rst analysis

rpt (h) =

e

(xt ,y t )   pt

i(cid:2)h(xt) (cid:54)= y t(cid:3)

ps (xt, y t)
ps (xt, y t)

i(cid:2)h(xt) (cid:54)= y t(cid:3)

i(cid:2)h(xt) (cid:54)= y t(cid:3)

(xt ,y t )   pt

e

(cid:88)

=

=

(xt ,y t )

pt (xt, y t)

ps (xt, y t)
ps (xt, y t)

(lahc)

id20 - epat   14

32 / 95

a    rst analysis

rpt (h) =

e

(xt ,y t )   pt

i(cid:2)h(xt) (cid:54)= y t(cid:3)

ps (xt, y t)
ps (xt, y t)

i(cid:2)h(xt) (cid:54)= y t(cid:3)
i(cid:2)h(xt) (cid:54)= y t(cid:3)

i(cid:2)h(xt) (cid:54)= y t(cid:3)

(xt ,y t )   pt

e

(cid:88)

=

=

(xt ,y t )

pt (xt, y t)

ps (xt, y t)
ps (xt, y t)

=

e

(xt ,y t )   ps

pt (xt, y t)
ps (xt, y t)

(lahc)

id20 - epat   14

32 / 95

covariate shift [shimodaira,   00]
    assume similar tasks, ps (y|x) = pt (y|x), then:
dt (xt)pt (y t|xt)
ds (xt)ps (y t|xt)
dt (xt)
ds (xt)

i(cid:2)h(xt) (cid:54)= y t(cid:3)

(xt ,y t )   ps

(xt ,y t )   ps

=

=

e

e

= e

(xt )   ds

dt (xt)
ds (xt)

e

y t   ps (y t|xt )

i(cid:2)h(xt) (cid:54)= y t(cid:3)
}i(cid:2)h(xt) (cid:54)= y t(cid:3)

    weighted error on the source domain:   (x t) = dt (xt )
ds (xt )

idea reweight labeled source data according to an estimate of   (x t):

  (xt)i(cid:2)h(xt) (cid:54)= y t(cid:3)

e

(xt ,y t )   ps

(lahc)

id20 - epat   14

33 / 95

illustration

no bias
ds (x) = dt (x)       (x) = 1

with bias
ds (x) (cid:54)= dt (x)       (x) (cid:54)= 1

(lahc)

id20 - epat   14

34 / 95

di   cult case

no shared support
   x, ds (x) = 0 and dt (x) (cid:54)= 0

shared support
ds (x) = 0 if and only if dt (x) = 0
intuition: the quality of the adaptation depends on the magnitude on the
weights

(lahc)

id20 - epat   14

35 / 95

how to deal with the sample selection bias?

setting
a source sample s = {(xs

i )}ms

i=1 and a target sample t = {xt

j}mt

j=1

i , y s

estimate new weights without using labels
  pr t (xs
i )
  pr s (xs
i )

i ) =

    (xs

(cid:88)

i )   s

learn a classi   er on the classi   er w.r.t.     

    (xs

i )i [h(xs

i ) (cid:54)= y s
i ]

i ,y s

(xs
(other losses: margin-based hinge-loss, least-square)

(lahc)

id20 - epat   14

36 / 95

much recent researchcorrecting samplingbias[shimodaira,    00][huang et al., bickel et al.,    07][sugiyamaet al.,    08][sethy et al.,    06][sethy et al.,    09][this work]adjusting mismatched models[evgeniou and pontil,    05][duan et al.,    09][duan et al., daum   iii et al., saenko et al.,    10][kulis et al., chen et al.,    11]+----+++----++inferring domain-invariant features[pan et al.,    09][blitzer et al.,    06][gopalan et al.,    11][chen et al.,    12][daum   iii,    07][argyriou et al,    08][gong et al.,    12][muandet et al.,    13]+++--+-+-+illustration

(lahc)

id20 - epat   14

37 / 95

++++++-++illustration

(lahc)

id20 - epat   14

37 / 95

++++++-++illustration

(lahc)

id20 - epat   14

37 / 95

++++++-++illustration

(lahc)

id20 - epat   14

37 / 95

++++++-++illustration

(lahc)

id20 - epat   14

37 / 95

++++++-++some existing approaches (1/2)

density estimators
build density estimators for source and target domains and estimate the
ratio between them - ex [sugiyama et al.,nips   07]:

    (x) =(cid:80)b

l=1   l   l (x)

learning: argmin  kl(    ds , dt )

learn the weights discriminatively [bickel et al.,icml   07]

ds (xi )    

1

assume dt (xi )
label source with label 1, target with label 0 and train a classi   er (    )
to classify examples 1 or 0 (e.g. with id28)

p(q=1|x,  )

compute the new weights     (xs

i ) =

1

p(q = 1|xs

i ;     )

(lahc)

id20 - epat   14

38 / 95

some existing approaches (2/2)

kernel mean matching [huang et al.,nips   06]

j )(cid:107)h

ms

(cid:80)ms
maximum mean discrepancy
i )     1
mmd(s, t ) = (cid:107) 1
mt(cid:88)
i=1   (xs
i )     1
(cid:80)ms
ms
i=1   (xs

ms(cid:88)
min  (cid:107) 1
i )  (xs
ms
i )     [0, b] and | 1
s.t.   (xs
  t kt          t

min  

  (xs

j=1

i=1

mt

(cid:80)mt
j=1   (xt
j )(cid:107)h

  (xt
i )     1| <  

ms

s,t s.t.   i     [0, b] and |(cid:80)ms
(cid:114) o(1/  ) + o(maxx   (x)2)

1
2

i=1   (xs

i )     ms| < ms  

guarantees [gretton et al.,2008] - under covariate shift assumptions
|rpt (h)     weighted(rs (h))| <
mt(cid:88)
(cid:107) 1
ms

ms
j )(cid:107)     o((1/  )

  2
max/ms + 1/mt

i )     1
mt

ms(cid:88)

+ c   and

(cid:113)

i )  (xs

  (xs

  (xt

i=1

j=1

(lahc)

id20 - epat   14

39 / 95

bad news

da is hard, even under covariate shift [ben-david et al.,alt   12]
    to learn a classi   er the number of examples depend on |h| (   nite)
or exponentially on the dimension of x
covariate shift assumption may fail: tasks are not similar in general
ps (y|x) (cid:54)= pt (y|x)

we did not consider the hypothesis space.
can de   ne a general theory about da?

(lahc)

id20 - epat   14

40 / 95

theoretical frameworks for
id20

(lahc)

id20 - epat   14

41 / 95

a keypoint: estimating the distribution shift

first idea: total variation measure
dl1(ds , dt ) = supb   x|ds (b)     dt (b)|
subset of points maximizing the divergence

but:

not computable in general, and thus not estimable from    nite samples

(lahc)

id20 - epat   14

42 / 95

a keypoint: estimating the distribution shift

first idea: total variation measure
dl1(ds , dt ) = supb   x|ds (b)     dt (b)|
subset of points maximizing the divergence

but:

not computable in general, and thus not estimable from    nite samples

not related to the hypothesis class
do not characterize the di   culty of the problem for h

(lahc)

id20 - epat   14

42 / 95

the h   h-divergence [ben-david et al.,nips   06;mlj   10]

de   nition

dh   h(ds , dt ) = sup

(h,h(cid:48))   h2

= sup

(h,h(cid:48))   h2

(cid:12)(cid:12)(cid:12)rdt (h, h(cid:48))     rds (h, h(cid:48))
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12) e
i(cid:2)h(xt) (cid:54)= h(cid:48)(xt)(cid:3)     e

xt   dt

xs   ds

i(cid:2)h(xs ) (cid:54)= h(cid:48)(xs )(cid:3)(cid:12)(cid:12)(cid:12)

(lahc)

id20 - epat   14

43 / 95

the h   h-divergence [ben-david et al.,nips   06;mlj   10]

de   nition

dh   h(ds , dt ) = sup

(cid:12)(cid:12)(cid:12)rdt (h, h(cid:48))     rds (h, h(cid:48))
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12) e
i(cid:2)h(xt) (cid:54)= h(cid:48)(xt)(cid:3)     e
xs   ds
illustration with only 2 hypothesis in h h and h(cid:48)

(h,h(cid:48))   h2

(h,h(cid:48))   h2

= sup

xt   dt

i(cid:2)h(xs ) (cid:54)= h(cid:48)(xs )(cid:3)(cid:12)(cid:12)(cid:12)

note: with a larger h, the distance will be high since we can easily    nd
two hypothesis able to distinguish the two domains

(lahc)

id20 - epat   14

43 / 95

computable from samples

consider two samples s, t of size m from ds and dt

dh   h(ds , dt )     dh   h(s, t ) + o(complexity(h)

(cid:113) log(m)

m )

complexity(h): vc-dimension [ben-david et al.,06;   10], rademacher [mansour et al.,   09]

empirical estimation

  dh   h(s, t ) = 2

      1     minh   h

       1

m

(cid:88)

x:h(x)=   1

i [x     s] +

1
m

            

i [x     t ]

(cid:88)

x:h(x)=1

    already seen: label source examples as -1, target ones as +1 and try to
learn a classi   er in h minimizing the associated empirical error

(lahc)

id20 - epat   14

44 / 95

going to a generalization bound

preliminaries

i [h(x) (cid:54)= h(cid:48)(x)]

(x,y )   ps

i [h(x) (cid:54)= h(cid:48)(x)] = e
x   dt

rpt (h, h(cid:48)) = e
rpt (rps ) ful   lls the triangle inequality
|rpt (h, h(cid:48))     rps (h, h(cid:48))|     1
since dh   h(ds , dt ) = 2 sup(h,h(cid:48))   h2

2 dh   h(ds , dt )

(cid:12)(cid:12)(cid:12)rdt (h, h(cid:48))     rds (h, h(cid:48))
(cid:12)(cid:12)(cid:12)

h   
s = argminh   hrps (h): best on source
h   
t = argminh   hrpt (h): best on target

ideal joint hypothesis
h    = argminh   hrps (h) + rpt (h) ;    = rps (h   ) + rpt (h   )

(lahc)

id20 - epat   14

45 / 95

a    rst bound

rpt (h)    

(lahc)

id20 - epat   14

46 / 95

a    rst bound

rpt (h)     rpt (h   ) + rpt (h, h   )

(lahc)

id20 - epat   14

46 / 95

a    rst bound

rpt (h)     rpt (h   ) + rpt (h, h   )

    rpt (h   ) + rps (h, h   ) + rpt (h, h   )     rps (h, h   )

(lahc)

id20 - epat   14

46 / 95

a    rst bound

rpt (h)     rpt (h   ) + rpt (h, h   )

    rpt (h   ) + rps (h, h   ) + rpt (h, h   )     rps (h, h   )
    rpt (h   ) + rps (h, h   ) + |rpt (h, h   )     rps (h, h   )|

(lahc)

id20 - epat   14

46 / 95

a    rst bound

rpt (h)     rpt (h   ) + rpt (h, h   )

    rpt (h   ) + rps (h, h   ) + rpt (h, h   )     rps (h, h   )
    rpt (h   ) + rps (h, h   ) + |rpt (h, h   )     rps (h, h   )|
    rpt (h   ) + rps (h, h   ) +

dh   h(ds , dt )

1
2

(lahc)

id20 - epat   14

46 / 95

a    rst bound

rpt (h)     rpt (h   ) + rpt (h, h   )

    rpt (h   ) + rps (h, h   ) + rpt (h, h   )     rps (h, h   )
    rpt (h   ) + rps (h, h   ) + |rpt (h, h   )     rps (h, h   )|
    rpt (h   ) + rps (h, h   ) +
    rpt (h   ) + rps (h) + rps (h   ) +

dh   h(ds , dt )

dh   h(ds , dt )

1
2

1
2

(lahc)

id20 - epat   14

46 / 95

a    rst bound

rpt (h)     rpt (h   ) + rpt (h, h   )

    rpt (h   ) + rps (h, h   ) + rpt (h, h   )     rps (h, h   )
    rpt (h   ) + rps (h, h   ) + |rpt (h, h   )     rps (h, h   )|
    rpt (h   ) + rps (h, h   ) +
    rpt (h   ) + rps (h) + rps (h   ) +
    rps (h) +

dh   h(ds , dt ) +   

dh   h(ds , dt )

dh   h(ds , dt )

1
2

1
2

1
2

(lahc)

id20 - epat   14

46 / 95

a    rst bound

rpt (h)     rpt (h   ) + rpt (h, h   )

1
2

    rpt (h   ) + rps (h, h   ) + rpt (h, h   )     rps (h, h   )
    rpt (h   ) + rps (h, h   ) + |rpt (h, h   )     rps (h, h   )|
    rpt (h   ) + rps (h, h   ) +
    rpt (h   ) + rps (h) + rps (h   ) +
    rps (h) +
1
dh   h(ds , dt ) +   
2
dh   h(s, t ) + o(complexity(h)
    rs (h) +
1
2

dh   h(ds , dt )

dh   h(ds , dt )

(cid:114)

1
2

m

log(m)

) +   

(lahc)

id20 - epat   14

46 / 95

main theoretical bound

theorem [ben-david et al.,mlj   10,nips   06]
let h a symmetric hypothesis space. if ds and dt are respectively
the marginal distributions of source and target instances, then for all
       (0, 1], with id203 at least 1        :

   h     h, rpt (h)     rps (h) +

1
2

dh   h(ds , dt ) +   

formalizes a natural approach: move closer the two distributions while
ensuring a low error on the source domain.
justi   es many algorithms:

reweighting methods,

feature-based methods,

adjusting/iterative methods.

(lahc)

id20 - epat   14

47 / 95

another analysis [mansour et al.,colt   09]

rpt (h)    

(lahc)

id20 - epat   14

48 / 95

another analysis [mansour et al.,colt   09]

rpt (h)     rpt (h, h   

s ) + rpt (h   

s , h   

t ) + rpt (h   
t )

(lahc)

id20 - epat   14

48 / 95

another analysis [mansour et al.,colt   09]

rpt (h)     rpt (h, h   
= rpt (h, h   

s ) + rpt (h   
s ) +   

s , h   

t ) + rpt (h   
t )

(lahc)

id20 - epat   14

48 / 95

another analysis [mansour et al.,colt   09]

rpt (h)     rpt (h, h   
= rpt (h, h   
    rps (h, h   

s ) + rpt (h   
s ) +   
s ) + rpt (h, h   

s , h   

t ) + rpt (h   
t )

s )     rps (h, h   

s ) +   

(lahc)

id20 - epat   14

48 / 95

another analysis [mansour et al.,colt   09]

s , h   

rpt (h)     rpt (h, h   
= rpt (h, h   
    rps (h, h   
    rps (h, h   

s ) + rpt (h   
s ) +   
s ) + rpt (h, h   
s ) + |rpt (h, h   

t ) + rpt (h   
t )

s )     rps (h, h   
s )     rps (h, h   

s ) +   
s )| +   

(lahc)

id20 - epat   14

48 / 95

another analysis [mansour et al.,colt   09]

s , h   

t ) + rpt (h   
t )

rpt (h)     rpt (h, h   
= rpt (h, h   
    rps (h, h   
    rps (h, h   
    rps (h, h   

s ) + rpt (h   
s ) +   
s ) + rpt (h, h   
s ) + |rpt (h, h   
s )) +

s )     rps (h, h   
s )     rps (h, h   
dh   h(ds , dt ) +   

1
2

s ) +   
s )| +   

(lahc)

id20 - epat   14

48 / 95

another analysis [mansour et al.,colt   09]

s , h   

rpt (h)     rpt (h, h   
= rpt (h, h   
    rps (h, h   
    rps (h, h   
    rps (h, h   
(    rps (h) +

t ) + rpt (h   
t )

s ) + rpt (h   
s ) +   
s ) + rpt (h, h   
s ) + |rpt (h, h   
s )) +

s )     rps (h, h   
s )     rps (h, h   
dh   h(ds , dt ) +   
dh   h(ds , dt ) + rps (h   

1
2

1
2

s ) +   
s )| +   

s ) +   ) if h   

s is not the true labeling function

(lahc)

id20 - epat   14

48 / 95

another analysis [mansour et al.,colt   09]

s , h   

rpt (h)     rpt (h, h   
= rpt (h, h   
    rps (h, h   
    rps (h, h   
    rps (h, h   
(    rps (h) +

t ) + rpt (h   
t )

s ) + rpt (h   
s ) +   
s ) + rpt (h, h   
s ) + |rpt (h, h   
s )) +

s )     rps (h, h   
s )     rps (h, h   
dh   h(ds , dt ) +   
dh   h(ds , dt ) + rps (h   

1
2

1
2

s ) +   
s )| +   

s ) +   ) if h   

s is not the true labeling function

this analysis can lead to smaller when adaptation is possible

leads to the same type of bound, just the constant changes .....

(lahc)

id20 - epat   14

48 / 95

characterization of the possibility of id20

constants characterize when adaptation is possible

   = rps (h   ) + rpt (h   ), h    = argminh   hrps (h) + rpt (h)
there must exist an ideal joint hypothesis with small error
   = rpt (h   
there must exist a very good hypothesis on the target and the best
hypothesis on source must be close to the best on target w.r.t to dt

t ) + rpt (h   
t )

s , h   

(lahc)

id20 - epat   14

49 / 95

other settings

discrepancy [mansour et al.,colt   09]

instead of the 0-1 loss, more general id168s (cid:96) (i.e. lp norms)
disc(cid:96)(ds , dt ) = suph,h(cid:48)   h |r (cid:96)
this discrepancy can be minimized and used as a reweighting method
([mansour et al.,colt   09] - polynomial for l2 norm for example)

(h, h(cid:48))     r (cid:96)

(h, h(cid:48))|

dt

ds

using some target labeled data

weighting the empirical source and target risks [ben david et
al.,2010]

using a divergence taking into account target labels [zhang et
al.,nips   12] (a divergence must take into account marginals over x
and y , the    constant counts for y )

(lahc)

id20 - epat   14

50 / 95

other settings

averaged quantities [germain et al.,icml   13]

consider a id203 distribution    (posterior) over h to learn and
the following risk: e
h     

rpt (h)

de   nition of an averaged distance

dis(ds , dt ) =

[rdt (h, h(cid:48))     rds (h, h(cid:48))]

(cid:12)(cid:12)(cid:12)(cid:12) e

h,h(cid:48)     2

(cid:12)(cid:12)(cid:12)(cid:12)

rpt (h)     e
h     

similar generalization bound
e
h     
estimation from samples controlled by pac-bayesian theory

rps (h) + dis(ds , dt ) +        

bound tighter without a supremum

(lahc)

id20 - epat   14

51 / 95

feature/projection based
approaches

(lahc)

id20 - epat   14

52 / 95

idea

change the feature representation x to better represent shared
characteristics between the two domains

some features are domain-speci   c,
others are generalizable
or there exist mappings from the original space

    make source and target domain explicitely similar
    learn a new feature space by embedding or projection

(lahc)

id20 - epat   14

53 / 95

joint feature space(smaller or higher dimension)targetsourcemetric learning [kulis et al.,   11;saenko et al.,   10]

mahalanobis: d 2

w(x, x(cid:48)) = (x     x(cid:48))t w(x     x(cid:48))

psd matrix w = lt l,
l projection space of dimension rrank(w)  d
(lx     lx(cid:48))t (lx     lx(cid:48))

pair-wise constraints: source ex. (xs

i , xt
i , xt

j )     u if y s
j )     l if y s

i = y t
w(xs
d 2
(cid:54)= y t
d 2
w(xs
require some target labels

i

i , y s

j , y t
j )
j (source and target must be similar)

i ) and target (xt

j (source and target must be dissimilar)

(lahc)

id20 - epat   14

54 / 95

metric learning [kulis et al.,cvpr   11;saenko et
al.,eccv   10]

[saenko et al.,eccv   10]

formulation (based on itml [davis et al.,icml   07])

minw(cid:23)0

s.t.

    can be kernelized

tr (w)     log detw
j )     u,   (xs
j )     l,   (xs

w(xs
d 2
w(xs
d 2

i , xt
i , xt

j )     similarset
j )     dissimilarset

i , xt
i , xt

(lahc)

id20 - epat   14

55 / 95

(simple) feature augmentation [daume iii et al.,   07;   10]

  (x) =< x, x, 0 > for source instances
  (x) =< x, 0, x > for target instances
    share some relevant features and not irrelevant ones (e.g. in text
id31:    nd shared words)
    a way to allow the existency of the ideal joint hypothesis h   

learn in the new space   

require target labels
bound: rdt    
2 (rt + rs ) + o(complexity ) + ( 1
1
   ) + o(dh   h(ds , dt ))
ms
kernelized and semi-supervised versions [add: (+1, < 0, x,   x >) and
(   1, < 0, x,   x >) to learning sample]

+ 1
mt

)o( 1

(lahc)

id20 - epat   14

56 / 95

find latent spaces - structural correspondence learning
[blitzer et al.,   07]

identify shared features

id31 - bag of words (bigrams)

choose k pivot features (frequent words in both domains, highly
correlated with labels)

learn k classi   ers to predict pivot features from remaining features

for each feature add k new features

represents source and target data with these features

(lahc)

id20 - epat   14

57 / 95

find latent spaces - structural correspondence learning
[blitzer et al.,   07]

apply pca source+target new features to get a low rank latent
representation

learn a classi   er in the new projection space de   ned by pca

(lahc)

id20 - epat   14

58 / 95

manifold-based methods

assume x     rn
apply pca on source data     matrix s1 of rank d
apply pca on target data     matrix s2 of rank d
geodesic path on the grassman manifold gn,d (d-dimensional vector
subspaces     rn ) between s1 and s2

(lahc)

id20 - epat   14

59 / 95

manifold-based methods

[gopalan et al.,   10]

use of an exponential    ow   (t(cid:48)) = qexp(t(cid:48)b)j
with q n    n matrix with determinant 1 s.t. qt s1 = j and jt = [id 0n   d,d ]
intermediate subspaces are obtained by computing b (skew block-diagonal matrix)
and varying t(cid:48) between 0 and 1
take a collection s(cid:48) of l subspaces between s1 and s2 on the manifold
project the data on s(cid:48) and learn in that new space

(lahc)

id20 - epat   14

60 / 95

a simpler approach - subspace alignment [fernando et
al.,iccv   13]

move closer pca-based representations
totally unsupervised

(lahc)

id20 - epat   14

61 / 95

subspace alignment algorithm

algorithm 1: subspace alignment da algorithm
data: source data s, target data t , source labels ys , subspace dimension d
result: predicted target labels yt
s1     pca(s, d)
s2     pca(t , d)
xa     s1s1
one);
sa = sxa
tt = t s2
yt     classi   er (sa, tt , ys ) ;

(source subspace de   ned by the    rst d eigenvectors) ;
(target subspace de   ned by the    rst d eigenvectors);
(operator for aligning the source subspace to the target

(new source data in the aligned space);
(new target data in the aligned space);

(cid:48)s2

(cid:48)s2 corresponds to the    subspace alignment matrix   :

m    = s1
m    = argminm (cid:107)s1m     s2(cid:107)
xa = s1s1
subspace
a natural similarity: sim(xs , xt) = xss1m   s1

(cid:48)s2 = s1m    projects the source data to the target

(cid:48)x(cid:48)
t = xsax(cid:48)

t

(lahc)

id20 - epat   14

62 / 95

some results

adaptation from o   ce/caltech-10 datasets (four domains to
adapt) is used as source and one as target
comparisons

baseline 1: projection on the source subspace
baseline 2: projection on the target subspace
2 related methods : gfk [gong et al., cvpr   12] and gfs [gopalan et
al.,iccv   11]

(lahc)

id20 - epat   14

63 / 95

some results
    o   ce/caltech-10 datasets with 4 domains a, b, c ,d

    divergences

(lahc)

id20 - epat   14

64 / 95

feature-based method

feature-based approaches are very popular
many other (id166/kernel-based, mkl, deep learning [glorot et
al.,icml   11], ...) methods not covered here,
subspace-based methods        hot topic   
embed similarity map: de   ne feature as similarity to landmarks points
- labeled source instances distributed similarly to the target domain

   

[grauman,visda-ws iccv   13]     subsampling: work with instances
facilitating adaptation
or use distances to headphones as a representation
< k(  , x1), k(  , x2), k(  , x3), k(  , x4), k(  , x5), ... >, ...

(lahc)

id20 - epat   14

65 / 95

adjusting/iterative methods

(lahc)

id20 - epat   14

66 / 95

principle

integrate some information about the target samples iteratively
    use of pseudo-labels
   move    closer distributions
    remove/add some instances     take into account a divergence
measure

repeat the process until convergence or no remaining instances

(lahc)

id20 - epat   14

67 / 95

daid166 [bruzzone et al.,   10]

a brief recap on id166

learning sample ls = {(xi , yi )}n
learn a classi   er h(x) = (cid:104)w, x(cid:105) + b

i=1

formulation: min
w,b,  

subject to (cid:96)i ((cid:104)w, xi(cid:105) + b)     1       i , 1     i     n

2 + c(cid:80)n

i=1   i

1

2(cid:107)w(cid:107)2

   (cid:23) 0

(lahc)

id20 - epat   14

68 / 95

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:11)(cid:1)(cid:11)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:15)(cid:16)(cid:3)(cid:17)(cid:7)(cid:18)(cid:19)(cid:20)(cid:17)(cid:16)(cid:3)(cid:13)(cid:21)(cid:8)(cid:2)(cid:12)(cid:22)(cid:9)(cid:21)(cid:8)(cid:2)(cid:12)(cid:23)(cid:24)(cid:9)daid166 principle

1 ls = s
2 learn a classi   er h0 from the learning sample ls
3 repeat until stopping criterion

select the    rst k target examples xt s.t. 0 < h(xt) < 1 with highest
margin and a   ect the pseudo-label    1
select the    rst k target examples xt s.t.    1 < h(xt) < 0 with highest
margin and a   ect them the pseudo-label +1
add these 2k examples (pseudo-labeled) to ls
remove from ls the    rst k positive and k negative source instances
with highest margin

4 output the last classi   er

algorithm stops when the number of selected instances at each step downs
to a threshold.

(lahc)

id20 - epat   14

69 / 95

daid166 - graphical illustration

(lahc)

id20 - epat   14

70 / 95

++++++daid166 - graphical illustration

(lahc)

id20 - epat   14

70 / 95

++++++daid166 - graphical illustration

(lahc)

id20 - epat   14

70 / 95

+++++++++daid166 - graphical illustration

(lahc)

id20 - epat   14

70 / 95

++++++daid166 - graphical illustration

(lahc)

id20 - epat   14

70 / 95

+++++++++hfinalconvergence - theoretical guarantees

what we need?: at each step pseudo-labels on target are su   ciently
reasonable.
    can be tackled with a notion of weak classi   er

weak classi   er on a single domain
an hypothesis hn, learned at iteration n, is a weak learner over a labeled
sample s if it performs a bit better than a random guessing:      n    ]0; 1

2

rs (hn) =   pr (xs

i ,yi )   s n[hn(xi ) (cid:54)= yi ] = 1

2       n

(lahc)

id20 - epat   14

71 / 95

a notion of weak learner for controlling pseudo-labels

self labeling weak learner
a classi   er h(i) learned at iteration over a current learning sample ls i is
self labeling weak learner w.r.t. a set slj of 2k pseudo-labeled examples
introduced at step j if its true error over slj is strictly better than random
guessing:

rslj (h(i)) = prxt

i    slj [h(xt

i ) (cid:54)= y t

i ] < 1
2

(lahc)

id20 - epat   14

72 / 95

a    rst necessary condition

theorem
let h(i) a weak learner output at iteration i from ls (i), let
   
r ls i (h(i)) = 1
let r (i)

ls the associated empirical error.
t the true empirical error over t .
h(i) is a self-labeling weak learner if   i

2       i
2       (i)

t = 1

ls > 0

    h(i) will be able to correctly classify (w.r.t. their unknown true label)
more than k pseudo-labeled target examples among 2k if at least half of
them have been correctly pseudo-labeled.

(lahc)

id20 - epat   14

73 / 95

a second result

theorem
let s a labeled source sample of ms instances and t a target sample of
mt     ms unlabeled instances. let a an iterative labeling algorithm using
2k examples at each step. a is able to perform an adaptation if

(cid:114)
s       (i)
t ,   i = 1... ms
  (i)
  max
s

  (0)
t
2

>

2k

    h(i) has to perform su   ciently well on the data it has been learned from
    a the    nal classi   er has to work better on t than a classi   er learned
only from source data.

(lahc)

id20 - epat   14

74 / 95

a simple illustration

task: handwritten digit recognition. p1 scaling problem between 1 and 0
and p2 rotation problem between 5 and 7.

for p1, we can check   (i)

s       (i)

t ,   i = 1... ms

2k , and   max

s

(cid:114)

>

  (0)
2 .
t

(lahc)

id20 - epat   14

75 / 95

interpretation summary

h(i) must work well on t
h(i) must work well on s
a works better than a non adaptation process

    necessary and reasonable conditions
    condition on t hard to check in practise
    boosting

(lahc)

id20 - epat   14

76 / 95

ensemble methods and boosting

de   nition
ensemble methods infer a set of classi   ers h1, . . . , hn whose individual
decisions are combined in some way to classify new examples.

necessary conditions for an ensemble method to be e   cient

the individual classi   ers are better than random guessing.

they are diverse, i.e. they make di   erent errors on new data points.

adaboost

learns step by step weak binary classi   ers.
optimizes a convex loss by increasing the weights of misclassi   ed
examples.
builds a convex combination of the weak classi   ers.

(lahc)

id20 - epat   14

77 / 95

adaboost

data: a learning sample s, a number of iterations n, a weak learner l
result: a global hypothesis hn
for i = 1 to m do d1(xi) = 1/m;
for t = 1 to t do

   n =(cid:80)

hn = learn(s, dn);

;

  n = 1
for i = 1 to m do

xi s.t. yi(cid:54)=hn(xi) dn(xi);
2 ln 1      n
dt+1(xi) = dn(xi) exp (     tyi ht(xi)) /zn;
/* zn is a id172 coe   cient*/

   n

end

f (x) =(cid:80)t

end

t=1   nhn(x);

hn (x) = sign (f (x));

(lahc)

id20 - epat   14

78 / 95

theoretical result on the empirical error

theorem
upper bound on the empirical error of the    nal classi   er hn

   hn     t(cid:89)

t(cid:88)

zn     exp(   2

  2
n)

t=1

where    n = 1
   hn is optimized with   n = 1

2       n (weak hypothesis).
2 ln 1      n

.

   n

t=1

meaning
this theorem means that the empirical error exponentially decreases
towards 0 with the number t of iterations.

(lahc)

id20 - epat   14

79 / 95

explanation in terms of margins of the training examples

      (cid:115)

theorem
      > 0, with id203 1       , any classi   er ensemble ht satis   es:

 ht     ex   s [margin(x)       ] + o

dh
m

log 2(m/dh)

  2

+ log (1/  )

where ex   s [margin(x]       ] exponentially decreases towards 0 with t .
    apply this idea on source and target (pseudo-labels)

(lahc)

id20 - epat   14

80 / 95

       ,

idea for id20

double weighting strategy

keep the same weak hypothesis for both domains h1, . . . , hn, . . . , hn
learn two functions

s =(cid:80)n
t =(cid:80)n

source domain: f n
target domain: f n

n=1   nhn(x)
n=1   nhn(x)

  n depends on the (pseudo-)margin of the examples and a divergence
measure

(lahc)

id20 - epat   14

81 / 95

a notion of weak da learner

weak da learner
a classi   er hn learned at iteration n from a s and t and a divergence gn     [0, 1]
between s and t and fda(hn(xi )) = |hn(xi )|      gn, is a weak da learner for t if:

1 hn is a weak learner for s.
2   ln = exi   t [|fda(hn(xi ))|       ] <

  

  +max(  ,  gn)

fda: obtaining high margin with small divergence

if max(  ,   gn) =   : divergence is small, we are close to a semi-supervised
setting

if max(  ,   gn) =   gn: divergence is high and the reweighting scheme
requires a speci   c attention to the divergence.

(lahc)

id20 - epat   14

82 / 95

algorithm sldab

sldab

data: learning sample s, nb of iterations n, unlabeled sample t ,        [0, 1],        [0, 1]
result: source and target hypothesis hs , ht
foreach    (xs
for n = 1 to n do

j     t do d s

i ) = 1/ms ; d t

i )     s, xt

j ) = 1/mt ;

1 (xt

1 (xs

i , y s

learn hn to produce a weak da learner; compute gn;
  n = 1
for (xs
for xt

i )     s do dn+1(xi ) = dn(xs

i ) exp (     ny s

  +max(  ,  gn) ln

max(  ,  gn)w

;   n =

  w +
n

1

;

   
n

j ) exp(cid:0)     ny n

t (xt

j ))(cid:1) /z n

t ;

j fda(hn(xt

   sn (hn)

2 ln 1      sn (hn)
i , y s
j     t do
d n+1
t (xj ) = d n
where y n
otherwise;

i sign(hn(xs

i ))) /z n
s ;

j =    sign(fda(hn(x))) if |fda(hn(x))| >    and y n

j =    sign(fda(hn(x)))

end

s (xs ) =(cid:80)n
t (xt ) =(cid:80)n

end
f n
h n

n=1   nsign(hn(xs));
n=1   nsign(hn(xt));

(lahc)

id20 - epat   14

83 / 95

conclusion

theoretical results

convergence of the empirical losses (exponentially fast to 0)    
(pseudo-)margin increases
no generalization bound over the true error on pt

di   cult aspects

de   ning divergence gn: avoid degenerate cases
finding/learning weak da learned: need to take into account both
source error and divergence information

(lahc)

id20 - epat   14

84 / 95

model selection?

(lahc)

id20 - epat   14

85 / 95

reverse validation [zhong et al.,ecml   10;bruzzone et
al.,pami   10]

reverse classi   er hr

(lahc)

id20 - epat   14

86 / 95

reverse validation [zhong et al.,ecml   10;bruzzone et
al.,pami   10]

reverse classi   er hr

(lahc)

id20 - epat   14

86 / 95

+++++-----lstsreverse validation [zhong et al.,ecml   10;bruzzone et
al.,pami   10]

reverse classi   er hr

(lahc)

id20 - epat   14

86 / 95

hl1 learning of h  from ls u tsl+++++-----lstsreverse validation [zhong et al.,ecml   10;bruzzone et
al.,pami   10]

reverse classi   er hr

(lahc)

id20 - epat   14

86 / 95

hl1 learning of h  from ls u tsl+++++-----lststsreverse validation [zhong et al.,ecml   10;bruzzone et
al.,pami   10]

reverse classi   er hr

(lahc)

id20 - epat   14

86 / 95

+++++hl1 learning of 2 auto labeling h  from ls u tsof ts with hll+++++-----lstststs-----reverse validation [zhong et al.,ecml   10;bruzzone et
al.,pami   10]

reverse classi   er hr

(lahc)

id20 - epat   14

86 / 95

+++++hlhlr1 learning of 2 auto labeling 3 learning ofh  from ls u tsof ts with hllh  from ts auto labeledlr+++++-----lstststs-----reverse validation [zhong et al.,ecml   10;bruzzone et
al.,pami   10]

reverse classi   er hr

(lahc)

id20 - epat   14

86 / 95

+++++hlhlr1 learning of 2 auto labeling 3 learning ofh  from ls u tsof ts with hllh  from ts auto labeledlr4 evaluationof h  on lsby cross-validationlr+++++-----lstststs-----ls+++++-----reverse validation [zhong et al.,ecml   10;bruzzone et
al.,pami   10]

reverse classi   er hr

two domains are related     hr
used with target labels to have an estimation of rpt
used to heuristically estimate theoretical constants of adaptability (  )
[morvant et al.,icdm   11;kais   12]

l performs well on the source domain

(lahc)

id20 - epat   14

87 / 95

+++++hlhlr1 learning of 2 auto labeling 3 learning ofh  from ls u tsof ts with hllh  from ts auto labeledlr4 evaluationof h  on lsby cross-validationlr+++++-----lstststs-----ls+++++-----conclusion

(lahc)

id20 - epat   14

88 / 95

conclusion

very active domains - lots of methods (sometimes di   cult to follow)
approaches not covered here: probabilistic-based, bayesian, deep
learning methods, etc.

same idea: moving closer the distributions while ensuring good
accuracy on labeled data

can we imagine general e   cient frameworks
    probably no: da is di   cult [ben-david et al.,alt   12]
    choose a method in function of the task/data
importance of data preparation

importance of divergence measures

(lahc)

id20 - epat   14

89 / 95

perspectives

understanding negative transfer

model selection

heterogeneous data

large scale

links with multi-tasks and multi-source learning, lifelong learning,
concept drift, etc.
    a large room for more research

(lahc)

id20 - epat   14

90 / 95

references (partial) (1/5)

j. huang, a. j. smola, a. gretton, k. m. borgwardt, and b. schlkopf,    correcting
sample selection bias by unlabeled data,   , nips 2007.

s. j. pan and q. yang,    a survey on id21   , tkde 2010.

h. shimodaira,    improving predictive id136 under covariate shift by weighting
the log-likelihood function   , journal of statistical planning and id136, 2000.

h. daum  e iii,    frustratingly easy id20   , acl, 2007.

h. daum  e iii, a. kumar, et a. saha. co-id173 based semi-supervised
id20. nips 2010.

r. gopalan, r. li, and r. chellappa,    id20 for object recognition:
an unsupervised approach   , iccv   11.

b. gong, y. shi, f. sha, and k. grauman,    geodesic    ow kernel for unsupervised
id20   , cvpr 2012.

k. saenko, b. kulis, m. fritz, and t. darrell.,    adapting visual category models
to new domains   , eccv   2010.

j. v. davis, b. kulis, p. jain, s. sra, and i. s. dhillon,    information-theoretic
metric learning   , icml 2007.

(lahc)

id20 - epat   14

91 / 95

references (partial) (2/5)

b. kulis, k. saenko, and t. darrell,    what you saw is not what you get: domain
adaptation using asymmetric kernel transforms   , cvpr   11

b. fernando, a. habrard, m. sebban, and t. tuytelaars,   unsupervised visual
id20 using subspace alignment   , iccv 2013

l. bruzzone and m. marconcini,    id20 problems: a daid166
classi   cation technique and a circular validation strategy   , pami 2010.

b. gong, k. grauman, and f. sha,    connecting the dots with landmarks:
discriminatively learning domain-invariant features for unsupervised domain
adaptation   , icml 2013

x. glorot, a. bordes, and y. bengio,    id20 for large-scale
sentiment classi   cation: a deep learning approach   , icml 2011.

p. germain, a. habrard, f. laviolette, et e. morvant.    pac-bayesian domain
adaptation bound with specialization to linear classi   ers   , icml 2013.

c. cortes et m. mohri. id20 in regression. alt 2011.

s. ben-david, j. blitzer, k. crammer, et f. pereira. analysis of representations for
id20. nips 2006.

(lahc)

id20 - epat   14

92 / 95

references (partial) (3/5)

s. ben-david, j. blitzer, k. crammer, a. kulesza, f. pereira, et j.w. vaughan. a
theory of learning from di   erent domains. machine learning 2010.
s. ben-david et r. urner. on the hardness of id20 and the utility of
unlabeled target samples. alt 2012.
s. ben-david et r. urner. on the hardness of id20 and the utility of
unlabeled target samples. aistat 2010.
j. ye c. zhang, l. zhang. generalization bounds for id20. nips
2012.
m. chen, k. q. weinberger, et j. blitzer. co-training for id20.
nips 2011.
a. habrard, jp peyrache, m. sebban. iterative self-labeling id20
for linear structured image classi   cation, ijait 2013.
a. habrard, jp peyrache, m. sebban. boosting for unsupervised domain
adaptation, ecml 2013
y. mansour, m. mohri, et a. rostamizadeh. domain adap- tation : learning
bounds and algorithms. colt 2009.
y. mansour, m. mohri, et a. rostamizadeh. multiple source adaptation and the
r  enyi divergence. uai 2009.

(lahc)

id20 - epat   14

93 / 95

references (partial) (4/5)

m. mohri et a.m. medina. new analysis and algorithm for learning with drifting
distributions. alt 2012.

e. morvant, a. habrard, et s. ayache. sparse id20 in projection
spaces based on good similarity functions. icdm 2011.

e. morvant, a. habrard, et s. ayache. parsimonious unsupervised and
semi-supervised id20 with good similarity functions. kais 2012.

e. zhong, w. fan, q. yang, o. verscheure, et j. ren. cross validation framework
to choose amongst models and datasets for id21. ecml 2010.

c. cortes and m. mohri. id20 and sample bias correction theory
and algorithm for regression. theoretical computer science, 2013.

m. sugiyama, s. nakajima, h. kashima, p. von bunau, and m. kawanabe. direct
importance estimation with model selection and its application to covariate shift
adaptation. nips 2008.

c. cortes, m. mohri, a. medina. adaptation algorithm and theory based on
generalized discrepancy, 2014.

s. bickel, m. bruckner, and t. sche   er,    discriminative learning for di   ering
training and test distributions   , 2007

(lahc)

id20 - epat   14

94 / 95

references (partial) (4/5)

b. gong, y. shi, f. sha, and k. grauman,    geodesic    ow kernel for unsupervised
id20   , cvpr 2012

a. torralba and a. a. efros,    unbiased look at dataset bias   , cvpr 2011.

a. gretton, a. smola, j. huang, m. schmittfull, k. borgwardt, b. scholkopf.
covariate shift by kernel mean matching. 2008.

(lahc)

id20 - epat   14

95 / 95

