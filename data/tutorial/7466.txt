bair retreat 3/28/17

trevor darrell
uc berkeley

overview

adversarial id20

learning end-to-end driving models from crowdsourced dashcams

vision and language: learning to reason to answer and explain

2

adversarial id20

eric 
tzeng

judy

hoffman

trevor 
darrell

uc berkeley

uc berkeley/

uc berkeley

stanford

3

adapting across domains ?

backpack

chair

bike

source data

conv1

conv5

fc
6

fc
7

source 
data

fc
8

classification

loss

adapting across domains ?

backpack

chair

bike

source data

conv1

conv5

fc
6

fc
7

fc
8

classification

loss

    applying source classifier to target domain can 

yield inferior performance   

target 
data

?

adapting across domains ?

backpack

chair

bike

source data

backpack

target data

conv1

conv5

fc
6

fc
7

source 
data

d
e
r
a
h
s

d
e
r
a
h
s

d
e
r
a
h
s

d
e
r
a
h
s

conv1

conv5

fc
6

fc
7

labeled 

target data

fc
8

d
e
r
a
h
s

fc
8

classification

loss

?

    fine tune?  

   ..zero or few labels in target domain

    siamese network?

   ..no paired / aligned instance examples!

adapting across domains: minimize discrepancy

object 
classifier

[iccv 2015]

adapting across domains: minimize discrepancy

discrepency

object 
classifier

[iccv 2015]

adapting across domains: minimize discrepancy

object 
classifier

domain 
classifier

[iccv 2015]

adapting across domains: minimize discrepancy

domain 
classifier

[iccv 2015]

adapting across domains: minimize discrepancy

domain 
classifier

[iccv 2015]

adapting across domains: minimize discrepancy

domain 
classifier

[iccv 2015]

adapting across domains: minimize discrepancy

object 
classifier

[iccv 2015]

deep domain confusion

[tzeng iccv15 ]

backpac

k

chair

bike

source data

unlabeled target data

conv1

conv5

fc
6

fc
7

source 

data

fc
8

d
e
r
a
h
s

d
e
r
a
h
s

d
e
r
a
h
s

d
e
r
a
h
s

fc
d

conv1

conv5

fc
6

fc
7

classification

loss

d
e
r
a
h
s

domain 
confusion

loss

domain 
classifier

loss

fc
8

?

labeled target 

data

adversarial training of domain label predictor 

and domain confusion loss: 

domain label cross-id178 with uniform distribution

deep domain confusion

[tzeng iccv15 ]

train a network to minimize classification loss and 
confuse two domains

source 
inputs

target
inputs

network
parameters
(fixed)

domain
classifier
(learn)

domain classifier loss

domain classifier prediction

domain
classifier (fixed)

network 
parameters (learn)

=     (         = 1|    )

domain confusion loss

iterate

(cross-id178 with uniform distribution)

16

adversarial discriminative id20 (adda)

(in submission)

backpac

k

chair

bike

source data + labels

generative or
discriminative?

unlabeled target data

?

encoder

shared or not?

encoder

r
e

i
f
i
s
s
a
c

l

classification

loss

gan
which loss?

discriminator

adversarial 

loss

adversarial discriminative id20 (adda)

(in submission)

source images + labelsclassifierpre-trainingclasslabelsource imagessourceid98discriminatoradversarial adaptationdomainlabeltargetid98target imagesclassifiertestingclasslabeltargetid98target imagesourceid98adda: adaptation on digits

(in submission)

adda: adaptation on rgb-d

(in submission)

train on rgb

test on depth

autonomous driving paradigms

1) learn affordances to predict state; apply rules or learned classic 
controllers

2) abandon engineering principles, learn    end-to-end    policy

autonomous driving paradigms

1) learn affordances to predict state; apply rules or learned classic 
controllers

how can visual sensing be robust to new enviroments? 

2) abandon engineering principles, learn    end-to-end    policy

how to learn generic driving policies from diverse data?

autonomous driving paradigms

1) learn affordances to predict state; apply rules or learned classic 
controllers

how can visual sensing be robust to new enviroments? 
   fully convolutional id20    in the wild   

2) abandon engineering principles, learn    end-to-end    policy

how to learn generic driving policies from diverse data?
   learning end-to-end driving policy/model from crowdsourced videos

bdd dataset

bdd video

bdd segmentation

    720p 30fps 40s video clips
    ~50k clips
    gps + imu

    720p images
    fine instance segmentation
    compatible with cityscapes

in-domain fully supervised fcn

train on cityscapes, test on cityscapes

domain shift: cityscapes to sf

train on cityscapes, test on san francisco dashcam

no tunnels in cityscapes?...

medium shift: cross seasons adaptation

small shift: cross city adaptation

effect of domain confusion loss 

before domain confusion

after domain confusion

effect of domain confusion loss 

before domain confusion

after domain confusion

effect of domain confusion loss 

before domain confusion

after domain confusion

effect of domain confusion loss 

before domain confusion

after domain confusion

bdd dataset     static 

overview

adversarial id20

learning end-to-end driving models from crowdsourced dashcams

vision and language: learning to reason to answer and explain

37

learning and adapting from large-scale driving 

data

    fully convolutional id20    in the wild   

    learning end-to-end driving policy/model from dashcam videos

end-to-end paradigm

    alvinn
    dave
    nvidia
    bdd rc cars
    bdd webcam

avlinn
(1989)

dave (2003)

yann lecun, eric cosatto, jan ben, urs muller, beat flepp: end-to-end learning of vision-based obstacle avoidance for off-
road robots. delivered at the learning@snowbird workshop, april 2004.

nvidia (2016)

[karl zipser]

model driving car,    direct    mode

steering

10 future
timepoints

motor

10 future
timepoints

fully connected

512 channels

conv2

384 channels

metadata input
conv1

4 channels

96 channels

camera input

4 channels

driving policy

learning a universal driving policy

    self driving as egomotion prediction

    learn general driving policy that is 

applicable to all car models.

    use a large number of easily accessible 

dashcam videos as self-supervision.

fcn-lstm

visual encoder

    dilated fully convolutional nets could provide more spatial details than id98

temporal fusion

    fuse the visual information, vehicle state (speed and angular velocity) from 

each frame

fcn-lstm

privileged learning

    the model should implicitly know what 

objects are in the scene

    we use the semantic segmentation mask 

from cityscapes as extra source of 
supervision

   

it ultimately improves the learnt 
representation of the dilated fcn

vapnik v, vashist a. a new learning paradigm: learning using privileged information[j]. neural networks the official journal of the 
international neural network society, 2009, 22(5-6):544-57.

dataset

    real first person driving 

videos

    diverse

    city
    highway
    rainy days
    nights and evenings
    construction zones

sample frames from the dataset

scene and trajectory reconstruction of 

crowd-sourced driving videos using 

semantic filtered sfm

yang gao*, huazhe xu*, christian hane, fisher yu,

trevor darrell

challenging driving videos in the wild

challenges

moving objects

subtle behaviors

lane changing

slight steering

unknown camera calibration

rolling shutters

existing motion-based method failed to reject moving object from
the scene

keypoints from motion-based keypoints rejection
methods

keypoints from our semantic filtered sfm pipeline. most
moving keypoints have been filtered out.

semantically filtered sfm:          2    

classical keypoints matching as points pair preference ranking

         1,     2 =

1

         1,     1 ,          2,     2

2

m is the preference score over point pair      1,     2 , defined by distance between two low level descriptors     (   ,   ). 

classical matchings could be formulated as ranking based on         ,   

semantics should be incorporated in sfm to be robust to moving objects

         1,     2 =

                                     1,    2 [    1,    2]

         1,    1 ,         2,    2

2

use the fcn as a semantic term

                                     1,     2     1,     2 =                  1     1                 (    2)[    2]

city turning example

lots of moving vehicles example

recover the subtle car backing behavior

experiments     continuous actions

lane following: left and right

experiments     continuous actions

intersection

experiments     continuous actions

side walk

bdd dataset     video 

overview

adversarial id20

learning end-to-end driving models from crowdsourced dashcams

vision and language: learning to reason to answer and explain

65

explainable ai (xai): visual explanations

image relevance

image

visual 

descriptions

explanations

class definitions

class discriminating

this bird has black and white feathers, with a white neck and a yellow beak.

western grebe

this is a western grebe because 
this bird has a long white neck, 
pointy yellow beak and red eye.

laysan albatros

this is a laysan albatross because 
this bird has a hooked yellow beak 
white neck and black back. 

explainable models with implicit capabilities

   translate dnn hidden state into 
    human-interpretable language 
    visualizations and exemplars 

can you park here?

visual id53

naacl 2016: mcb with attention

    predict spatial attentions with mcb

2048x14x14

c
n
n

2048x14x14

t

i
l

e

(
r
e
s
n
e
t
1
5
2
)

w
e
,
 
l
s
t
m

mcb

s
i
g
n
e
d
s
q
r
t

 

what is the 
yellow food?

 

l
2
n
o
r
m
a

l
i
z
a
t
i
o
n

i

w
e
g
h
t
e
d
s
u
m

 

16k x14x14

c
o
n
v
,
 

r
e
u

l

c
o
n
v

s
o
f
t

m
a
x

1
 
x
 
1
4
 
x
 
1
4

5
1
2
 
x
 
1
4
 
x
 
1
4

2048

2048

mcb

16k

s
i
g
n
e
d
s
q
r
t

 

1
6
k

 

l
2
n
o
r
m
a

l
i
z
a
t
i
o
n

1
6
k

attention for captioning : 
- k. xu, show, attend and tell: neural image id134 with visual attention
attention for  vqa : 
- h. xu, k. saenko ask, attend and answer: exploring question-guided spatial attention for visual id53
- j.lu hierarchal question-image co-attention for visual id53 

f
u

l
l
 

c
o
n
n
e
c
t
e
d

s
o
f
t

m
a
x

3
0
0
0

corn

winner vqa challenge 2016 (real open ended)

y
c
a
r
u
c
c
a
e
g
n
e

 

l
l

a
h
c
-
t
s
e
t

67
66
65
64
63
62
61
60
59
58
57
56
55
54
53

 

 

y
s
n
r
o
u
s
o
&
y
e
e
k
r
e
b
c
u

l

 

h
c
e
t
s
o
p

s
i
e
d
n
a
r
b

t
i
a
l
d

s
b
a
l
 
r
e
v
a
n

s
b
a
l
r
e
v
a
n
-
i
b
u
n
s

n
o
s
i
v
r
e
t
u
p
m
o
c
t
v

b
a
l
k

t
u
-
l
i

m

x
c
m
m

6
2
0
1
_
b
h
s

s
u
n
-
v
l

n
e
s
a
i
j
_
v
c
_
t
v

i

l

e
d
a
e
d
a
_
t
v
c
a

)

n
m
n
d

(
 

t
t
a
n
n
c

l

y
e
e
k
r
e
b
c
u

 

n
a
s

)

n
m
n

(
 

l

y
e
e
k
r
e
b
c
u

 

n
o
i
s
i
v
_
a
b
o
g

l

l

t
t
a

i

l

e
o
b

t
i
r

b
u
_
v
p
u

c
p
u

n
n
c
_
m

t
s
l
-

m
a
e
t
a
q
v

 

n
a
s
a
h
a
b
a
t
j
u
m

n
n
c
e
z
i
l

m
r
o
n
_
m
t
s
l
r
e
p
e
e
d
-
m
a
e
t
a
q

attention visualizations

what is the woman feeding the giraffe?
carrot
[groundtruth: carrot]

attention visualizations

what color is her shirt?
purple
[groundtruth: purple]

attention visualizations

what is her hairstyle for the picture?
ponytail
[groundtruth: ponytail]

attention visualizations

what color is the chain on the red dress?
pink
[groundtruth: gold]

    correct attention, incorrect fine-grained recognition

attention visualizations

is the man going to fall down?
no
[groundtruth: no]

attention visualizations

what is the surface of the court made of?
clay
[groundtruth: clay]

attention visualizations

what sport is being played?
tennis
[groundtruth: tennis]

attentive explanations: 

justifying decisions and pointing to the evidence

human ground truth for the textual justification task.

human ground truth for the pointing task.

discussing different evidence for different images.

discussing different evidence for different questions.

discussing different evidence for different questions.

differentiating between some activities requires understanding 
special equipment.

differentiating between some activities requires recognizing 
specific context.

differentiating between some activities requires recognizing 
specific context.

explanations when the model predicts the wrong answer.

explainable models with explicit capabilities

explain higher-level reasoning in dnns 

explainable decision path for multi-task, control and planning

provide structure and intermediate state

can you park here?

explainable models with 
explicit and implicit capabilities

no, because there is a person in the bus.

grounded id53

is there a red 
shape above 

a circle?

yes

93

neural nets learn lexical groundings

is there a red 
shape above 

a circle?

yes

[iyyer et al. 2014, bordes et al. 2014, 
yang et al. 2015, malinowski et al., 2015]

94

semantic parsers learn composition

is there a red 
shape above 

a circle?

yes

[wong & mooney 2007, kwiatkowski et al. 2010,
liang et al. 2011, a et al. 2013]

97

neural module networks learn both!

is there a red 
shape above 

a circle?

andand

red

yes

98

neural module networks

is there a red shape 

above a circle?

red

exists

above

   

true   

   

99

neural module networks

is there a red shape 

above a circle?

red

exists

above

   

true   

   

exists

and

red

above

circle

100

neural module networks

yes

exists

and

red

above

circle

is there a red shape 

above a circle?

101

   

true   

   

red

exists

above

q: are there an equal number of large things and metal spheres?
q: what size is the cylinder that is left of the brown 
metal thingthat is left of the big sphere?
q: there is a sphere with the same size as the metal cube; is 
it made of the same material as the small red sphere?
q: how many objects are either small cylinders or red things?

questions in clevr test various aspects of visual reasoning 
including attribute identification,counting, comparison, spatial 
relationships, andlogical operations.

learning to reason: end-to-end module 
networks for visual id53

r. hu, j. andreas, m. rohrbach, t. darrell, k. saenko

background

natural language is compositional: the meaning of a sentence comes from the meanings of its components.

different questions may require significantly different reasoning procedure.

    what kind of vehicle is the one on the left of the brown car that is next to the building?

    why is the person running away?

background

    neural module networks: dynamic id136 structure for each question

    previous work: structure from nlp parser or parser re-ranking

    this work: learned layout policy to dynamically build a network

end-to-end module networks (n2nmn)

components

    layout policy p(l | q) with sequence-to-sequence id56

    neural modules with co-attention, dynamically assembled into a network

end-to-end training

loss

where

is the softmax loss of the answer

optimization: policy gradient method

easier: behavior cloning from expert layout policy

experiments on the clevr dataset

cloning 
expert

end-to-end 
optimization 
after cloning

policy search from scratch (no experts used)

even without resorting to an expert policy during training, our method still achieves state-of-the-art 
performance with id23 from scratch.

accuracy v.s. question length

even on long questions with 30+ words, our method still achieves relatively high accuracy (figure a).

detailed output visualization

overview

adversarial id20

learning end-to-end driving models from crowdsourced dashcams

vision and language: learning to reason to answer and explain

113

