performance guarantees for transferring

representations

daniel mcnamara

phd candidate, australian national university and data61

fulbright postgraduate scholar visiting carnegie mellon university

joint work with nina balcan, carnegie mellon university

march 31, 2017

performance guarantees for transferring representations

introduction

motivation

1 / 16

performance guarantees for transferring representations

introduction

motivation

shared

input
space x

representation
f : x     z

feature
space z

gs : z     y

separate
specialized
classi   ers

gt : z     y

output
space y

(cid:73) want to learn a task for which labelled data is scarce, but

have abundant data for another related task

(cid:73) transferring representations between tasks is empirically

successful [djv+14, hgt+14, gddm14, bgl14]

(cid:73) natural language processing example: id27s

outperform unigram features [qfz+15]

(cid:73) id161 example: pre-trained neural network with    ne

tuning outperforms random initialization [ycbl14]

(cid:73) when and why does this procedure work?

2 / 16

performance guarantees for transferring representations

introduction

notation

(cid:73) f is a class of representations, f : x     z for f     f
(cid:73) g is a class of specialized classi   ers, g : z     y for g     g
(cid:73) h := {h :    f     f , g     g s.t. h = g     f }, vc dimension dh
(cid:73) source task s and target task t have labeling functions

hs , ht : x     y and input distributions ps , pt

(cid:73) ms labelled points for s and mt labelled points for t
(cid:73) rs (h) := ex   ps [hs (x) (cid:54)= h(x)],   rs (h) is empirical risk on s
(cid:73) rt (h) := ex   ps [ht (x) (cid:54)= h(x)],   rt (h) is empirical risk on t

3 / 16

performance guarantees for transferring representations

introduction

high-level idea

learning representation

transferring representation

transferring representation

from scratch

without    ne-tuning

with    ne-tuning

f

f

f

  f

f

f
  f

  f

f

(cid:73) learn   f : x     z from source task s
(cid:73) can we restrict the representation class f when learning

target task t ?

(cid:73) use statistical learning theory to provide tighter risk upper

bounds for t , inspired by [bdbcp07, bax00, mprp16]

4 / 16

performance guarantees for transferring representations

representation    xed by source task

risk bound

representation    xed by source task

(cid:73) learn   gs       f     h on s, extract   f     f
(cid:73) then conduct empirical risk minimization over

g       f := {g       f : g     g} on t , yield   gt := arg min
g   g

  rt (g       f )

theorem 1 (risk upper bound for    xed representation)
let    : r     r be non-decreasing and ps , pt , hs , ht ,   f , g satisfy
     gs     g , min
rt (g       f )       (rs (  gs       f )). then with id203 at
g   g
least 1        over pairs of training sets for tasks s and t ,
rt (  gt       f )       (   rs (  gs       f ) + 2
4

(cid:113) 2dg log(2emt /dg )+2 log(8/  )

(cid:113) 2dh log(2ems /dh )+2 log(8/  )

) +

ms

.

mt

(cid:73) if   (r) = o(r),   rs (  gs       f ) is a small constant, ms (cid:29) mt
and dh (cid:29) dg , bound intheorem 1 is tighter than learning t
from scratch and using vc dimension-based risk bound

5 / 16

performance guarantees for transferring representations

representation    xed by source task

neural network example with    xed representation

neural network example with    xed representation

learn t

from scratch

learn   gs       f

on s
  f

fix   f ,

learn   gt on t

  f

  gs

  gt

(cid:73) transfer lower-level weights learned on s, corresponding to   f
(cid:73) only the upper-level weights have to be learned on t
(cid:73) under network architecture and distributional assumptions,
(cid:73) rs (  gs       f ) reliably indicates usefulness of   f if    defects    of   f

can de   ne    parameterized by constants c and  

cannot be hidden either through either low ps or low
magnitude upper-level weights

6 / 16

performance guarantees for transferring representations

representation    xed by source task

neural network example with    xed representation

neural network example with    xed representation
(cid:73) x = rn and z = rk , where 2k     n
(cid:73) f is the function class s.t. f (x) = [a(w1    x), . . . , a(wk    x)],
wi     rn, a : r     r odd,   f (x) := [a(   w1    x), . . . , a(   wk    x)]
(cid:73) g is the function class s.t. g (z) = sign(v    z), v     {   1, 1}k
(cid:73)    f     f , gs , gt     g s.t. max[rs (gs     f ), rt (gt     f )]      
(cid:73) suppose ||wi|| = ||  i   wi       i wi|| and wi    (  i   wi       i wi ) = 0
(cid:73) m is a full rank 2k    n matrix with rows wi ,   i   wi       i wi
(cid:73) let ps , pt be distributions on x with the property    x, x(cid:48) s.t.

||mx|| = ||mx(cid:48)||, pt (x)     cps (x(cid:48)) for some c     1

theorem 2 (   for neural network,    xed representation)
rt (g       f )       (rs (  gs       f )).
  (r) := cr +  (1 + c).      gs     g , min
g   g

7 / 16

performance guarantees for transferring representations

representation    xed by source task

neural network example with    xed representation

(cid:73) compare learning over g       f to from scratch over h on t
(cid:73) set    = 0.05, n = 10, k = 5. consider the limit
(cid:73) we use dh = |nk + k| log |nk + k| and dg     k

      0,   rs (  gs       f )     0, ms        , and hence   (  )     0.

8 / 16

performance guarantees for transferring representations

representation    ne-tuned using target task

representation    ne-tuned using target task

(cid:73) g       f := {h :    f       f , g     g s.t. h = g     f }, often dg      f = dh
(cid:73)   hg   f

is a distribution on h (i.e. a stochastic hypothesis)

corresponding to g     f (e.g. g     f plus noise)

hypothesis class   h := {  hg   f : f     f , g     g}

stochastic hypothesis class   hg      f := {  hg   f : f       f , g     g}

x   pt ,h     h[ht (x) (cid:54)= h(x)],   rt (  h) is empirical risk
(cid:73) rt (  h) := e
(cid:73) could learn t from scratch with    xed prior   h0 and stochastic
(cid:73) alternatively, use   gs       f to construct prior   h  gs      f and
(cid:73) pac-bayes result bounds generalization error using kl
(cid:73) want   f    small enough    s.t. kl(  h||  h  gs      f )       (rs (  gs       f ))
(cid:73) also want   f    large enough    s.t.      hgt   f       hg      f s.t.

     h       hg      f for some transferrability function   
rt (  hgt   f )      

divergence between prior and posterior hypotheses

9 / 16

performance guarantees for transferring representations

representation    ne-tuned using target task

risk bound

theorem 3 (risk upper bound with    ne-tuning)
suppose it is possible to construct   hg      f with the property
     h       hg      f , kl(  h||  h  gs      f )       (rs (  gs       f )). then with id203
at least 1        over pairs of training sets for s and t ,      h       hg      f ,
rt (  h)       rt (  h) +

  (   rs (  gs      f )+2

2dh log(2ems /dh )+2 log(8/  )

(cid:115)

)+log 2mt /  

(cid:114)

.

ms
2(mt   1)

(cid:73) if   (r) = o(r),   rs (  gs       f ) is a small constant, and

ms (cid:29) mt , improve on the pac-bayes bound for   h and   h0
(cid:73) neural network with similar assumptions to previous example

allows us to de   ne    and   f

10 / 16

performance guarantees for transferring representations

applications

modi   ed id173 penalty

m(cid:88)

i=1

[   yi log   yi     (1     yi ) log(1       yi )] +

l(cid:88)

[

j=1

  1(j)

2

||w (j)       w (j)||2

2 +

  2(j)

2

||w (j)||2
2]

(cid:73) relax hard constraint on   f by using a modi   ed id168
(cid:73) let y and   y be labels and predictions over m points
(cid:73) neural network with l layers of weights, let w (j) be the jth

weight matrix and   w (j) be its estimate from s

(cid:73) assuming lower level features are more transferrable,   1 is a

decreasing function

11 / 16

performance guarantees for transferring representations

applications

experiments

(cid:73) experiments on mnist and 20 newsgroups datasets
(cid:73) randomly partition label classes into s+ and s   , |s+| = |s   |
|s+   t+|
(cid:73) construct t+ randomly picking from s+ up to    :=
,

then randomly picking from s    such that |t+| = |t   |

|s+|

(cid:73) let s be the task of distinguishing between s+ and s    and t

be that of distinguishing t+ and t   

(cid:73)   1(1) =   2(2) =    := 1 and   1(2) =   2(1) = 0
(cid:73) mt = 500, l = 2, sigmoid activation, average over 10 runs
(cid:73) mnist: pixel features, 784    50    1 network, ms = 50000
(cid:73) 20 newsgroups: tf-idf weighted word frequency features,

2000    50    1 network, ms = 15000

12 / 16

performance guarantees for transferring representations

applications

results

technique

base
fine-tune   f
fix   f
fix   gs       f

mnist,    =
1

0.6
88.4
91.9
87.5
67.4

0.8
87.9
93.9
92.3
85.6

87.9
95.4
97.3
98.1

newsgroups,    =
0.6
1
62.6
66.1
83.3
62.3
83.3
52.2
83.6
55.5

0.8
63.2
72.3
69.6
70.7

(cid:73) learn t from scratch (base)
(cid:73) transfer   f from s, tune f and train g on t (fine-tune   f )
(cid:73) transfer   f from s and    x, train g on t (fix   f )
(cid:73) transfer   gs       f from s and    x (fix   gs       f )

13 / 16

performance guarantees for transferring representations

conclusion

conclusion

(cid:73) step towards theoretical foundation for transferring
representations, both with and without    ne tuning

(cid:73) theory motivates transfer id173 penalty to prevent

target task over   tting

14 / 16

performance guarantees for transferring representations

references

references

[bax00]

jonathan baxter, a model of inductive bias learning, journal of arti   cial intelligence research 12
(2000), no. 3, 149   198.

[bdbcp07] shai ben-david, john blitzer, koby crammer, and fernando pereira, analysis of representations for

id20, advances in neural information processing systems (2007), 137   144.

[bgl14]

[djv+14]

mohit bansal, kevin gimpel, and karen livescu, tailoring continuous word representations for
id33., association for computational linguistics, 2014, pp. 809   815.

je    donahue, yangqing jia, oriol vinyals, judy ho   man, ning zhang, eric tzeng, and trevor
darrell, decaf: a deep convolutional activation feature for generic visual recognition, international
conference on machine learning, 2014, pp. 647   655.

[gddm14] ross girshick, je    donahue, trevor darrell, and jitendra malik, rich feature hierarchies for accurate

id164 and semantic segmentation, proceedings of the ieee conference on computer
vision and pattern recognition, 2014, pp. 580   587.

[hgt+14]

judy ho   man, sergio guadarrama, eric s tzeng, ronghang hu, je    donahue, ross girshick, trevor
darrell, and kate saenko, lsda: large scale detection through adaptation, advances in neural
information processing systems, 2014, pp. 3536   3544.

[mprp16]

andreas maurer, massimiliano pontil, and bernardino romera-paredes, the bene   t of multitask
representation learning, journal of machine learning research 17 (2016), no. 81, 1   32.

[qfz+15]

lizhen qu, gabriela ferraro, liyuan zhou, weiwei hou, nathan schneider, and timothy baldwin,
big data small data, in domain out-of domain, known word unknown word: the impact of word
representation on sequence labelling tasks, proceedings of the nineteenth conference on
computational natural language learning, 2015, pp. 89   93.

[ycbl14]

jason yosinski, je    clune, yoshua bengio, and hod lipson, how transferable are features in deep

neural networks?, advances in neural information processing systems, 2014, pp. 3320   3328. 15 / 16

performance guarantees for transferring representations

questions

questions?

16 / 16

