   iframe: [1]//www.googletagmanager.com/ns.html?id=gtm-wcf9z9

   [2]skip to main content [3]skip to sections

   this service is more advanced with javascript available, learn more at
   [4]http://activatejavascript.org

   advertisement
   (button) hide
   springerlink
   search springerlink
   ____________________ submit
   [5]search
     * [6]home
     * [7]log in

   [8]journal of big data
   [9]download pdf

   [10]journal of big data

   december 2016, 3:9 | [11]cite as

a survey of id21

     * authors
     * [12]authors and affiliations

     * karl weiss[13] email author
     * taghi m. khoshgoftaar
     * dingding wang

   open access
   survey paper
   first online: 28 may 2016
     * [14]6 shares
     * 45k downloads
     * [15]131 citations

abstract

   machine learning and data mining techniques have been used in numerous
   real-world applications. an assumption of traditional machine learning
   methodologies is the training data and testing data are taken from the
   same domain, such that the input feature space and data distribution
   characteristics are the same. however, in some real-world machine
   learning scenarios, this assumption does not hold. there are cases
   where training data is expensive or difficult to collect. therefore,
   there is a need to create high-performance learners trained with more
   easily obtained data from different domains. this methodology is
   referred to as id21. this survey paper formally defines
   id21, presents information on current solutions, and
   reviews applications applied to id21. lastly, there is
   information listed on software downloads for various id21
   solutions and a discussion of possible future research work. the
   id21 solutions surveyed are independent of data size and
   can be applied to big data environments.

keywords

   id21 survey id20 machine learning data
   mining

background

   the field of data mining and machine learning has been widely and
   successfully used in many applications where patterns from past
   information (training data) can be extracted in order to predict future
   outcomes [[16]129]. traditional machine learning is characterized by
   training data and testing data having the same input feature space and
   the same data distribution. when there is a difference in data
   distribution between the training data and test data, the results of a
   predictive learner can be degraded [[17]107]. in certain scenarios,
   obtaining training data that matches the feature space and predicted
   data distribution characteristics of the test data can be difficult and
   expensive. therefore, there is a need to create a high-performance
   learner for a target domain trained from a related source domain. this
   is the motivation for id21.

   id21 is used to improve a learner from one domain by
   transferring information from a related domain. we can draw from
   real-world non-technical experiences to understand why transfer
   learning is possible. consider an example of two people who want to
   learn to play the piano. one person has no previous experience playing
   music, and the other person has extensive music knowledge through
   playing the guitar. the person with an extensive music background will
   be able to learn the piano in a more efficient manner by transferring
   previously learned music knowledge to the task of learning to play the
   piano [[18]84]. one person is able to take information from a
   previously learned task and use it in a beneficial way to learn a
   related task.

   looking at a concrete example from the domain of machine learning,
   consider the task of predicting text sentiment of product reviews where
   there exists an abundance of labeled data from digital camera reviews.
   if the training data and the target data are both derived from digital
   camera reviews, then traditional machine learning techniques are used
   to achieve good prediction results. however, in the case where the
   training data is from digital camera reviews and the target data is
   from food reviews, then the prediction results are likely to degrade
   due to the differences in domain data. digital camera reviews and food
   reviews still have a number of characteristics in common, if not
   exactly the same. they both are written in textual form using the same
   language, and they both express views about a purchased product.
   because these two domains are related, id21 can be used to
   potentially improve the results of a target learner [[19]84]. an
   alternative way to view the data domains in a id21
   environment is that the training data and the target data exist in
   different sub-domains linked by a high-level common domain. for
   example, a piano player and a guitar player are subdomains of a
   musician domain. further, a digital camera review and a food review are
   subdomains of a review domain. the high-level common domain determines
   how the subdomains are related.

   as previously mentioned, the need for id21 occurs when
   there is a limited supply of target training data. this could be due to
   the data being rare, the data being expensive to collect and label, or
   the data being inaccessible. with big data repositories becoming more
   prevalent, using existing datasets that are related to, but not exactly
   the same as, a target domain of interest makes id21
   solutions an attractive approach. there are many machine learning
   applications that id21 has been successfully applied to
   including text sentiment classification [[20]121], image classification
   [[21]30, [22]58, [23]146], human activity classification [[24]46],
   software defect classification [[25]77], and multi-language text
   classification [[26]145, [27]91, [28]144].

   this survey paper aims to provide a researcher interested in transfer
   learning with an overview of related works, examples of applications
   that are addressed by id21, and issues and solutions that
   are relevant to the field of id21. this survey paper
   provides an overview of current methods being used in the field of
   id21 as it pertains to data mining tasks for
   classification, regression, and id91 problems; however, it does
   not focus on id21 for id23 (for more
   information on id23 see taylor [[29]112]).
   information pertaining to the history and taxonomy of id21
   is not provided in this survey paper, but can be found in the paper by
   pan [[30]84]. since the publication of the id21 survey
   paper by pan [[31]84] in 2010, there have been over 700 academic papers
   written addressing advancements and innovations on the subject of
   id21. these works broadly cover the areas of new algorithm
   development, improvements to existing id21 algorithms, and
   algorithm deployment in new application domains. the selected surveyed
   works in this paper are meant to be diverse and representative of
   id21 solutions in the past 5 years. most of the surveyed
   papers provide a generic id21 solution; however, some
   surveyed papers provide solutions that are specific to individual
   applications. this paper is written with the assumption the reader has
   a working knowledge of machine learning. for more information on
   machine learning see witten [[32]129]. the surveyed works in this paper
   are intended to present a high-level description of proposed solutions
   with unique and salient points being highlighted. experiments from the
   surveyed papers are described with respect to applied applications,
   other competing solutions tested, and overall relative results of the
   experiments. this survey paper provides a section on heterogeneous
   id21 which, to the best of our knowledge, is unique.
   additionally, a list of software downloads for various surveyed papers
   is provided, which is unique to this paper.

   the remainder of this paper is organized as follows.    [33]definitions
   of id21    section provides definitions and notations of
   id21.    [34]homogeneous id21    and
      [35]heterogeneous id21    sections provide solutions on
   homogeneous and heterogeneous id21,    [36]negative
   transfer    section provides information on negative transfer as it
   pertains to id21.    id21 application    section
   provides examples of id21 applications.    [37]conclusion
   and discussion    section summarizes and discusses potential future
   research work. [38]appendix provides information on software downloads
   for id21.

definitions of id21

   the following section lists the notation and definitions used for the
   remainder of this paper. the notation and definitions in this section
   match those from the survey paper by pan [[39]84], if present in both
   papers, to maintain consistency across both surveys. to provide
   illustrative examples of the definitions listed below, a machine
   learning application of software module defect classification is used
   where a learner is trained to predict whether a software module is
   defect prone or not.

   a domain \({\mathcal{d}}\) is defined by two parts, a feature space
   \({\mathcal{x}}\) and a marginal id203 distribution p(x), where
   \({\text{x}} = \left\{ {{\text{x}}_{1} \, ,\ldots
   ,\,{\text{x}}_{\text{n}} } \right\} \in {\mathcal{x}}\). for example,
   if the machine learning application is software module defect
   classification and each software metric is taken as a feature, then
   x[i]is the i-th feature vector (instance) corresponding to the i-th
   software module, n is the number of feature vectors in x,
   \({\mathcal{x}}\) is the space of all possible feature vectors, and x
   is a particular learning sample. for a given domain \({\mathcal{d}}\),
   a task \({\mathcal{t}}\) is defined by two parts, a label space
   \({\mathcal{y}}\), and a predictive function f(  ), which is learned
   from the feature vector and label pairs {x[i], y[i]} where
   \({\text{x}}_{\text{i}} \in {\text{x}}\) and \({\text{y}}_{\text{i}}
   \in {\mathcal{y}}\). referring to the software module defect
   classification application, \({\mathcal{y}}\) is the set of labels and
   in this case contains true and false, y[i] takes on a value of true or
   false, and f(x) is the learner that predicts the label value for the
   software module x. from the definitions above, a domain \({\mathcal{d}}
   = \left\{ {{\mathcal{x}},\,{\text{p}}\left( {\text{x}} \right)}
   \right\}\) and a task \({\mathcal{t}} = \left\{ {{\mathcal{y}},f( \cdot
   )} \right\}\). now, d[s] is defined as the source domain data where
   \({\text{d}}_{\text{s}} = \left\{ {\left( {{\text{x}}_{\text{s1}}
   ,{\text{y}}_{\text{s1}} } \right) \ldots ,\left(
   {{\text{x}}_{\text{sn}} ,{\text{y}}_{\text{sn}} } \right)} \right\}\),
   where \({\text{x}}_{\text{si}} \in {\mathcal{x}}_{s}\) is the ith data
   instance of d[s] and \({\text{y}}_{\text{si}} \in {\mathcal{y}}_{s}\)
   is the corresponding class label for x[si]. in the same way, d[t] is
   defined as the target domain data where \({\text{d}}_{\text{t}} =
   \left\{ {\left( {{\text{x}}_{\text{t1}} ,{\text{y}}_{\text{t1}} }
   \right) \ldots ,\left( {{\text{x}}_{\text{tn}} ,{\text{y}}_{\text{tn}}
   } \right)} \right\}\), where \({\text{x}}_{\text{ti}} \in
   {\mathcal{x}}_{{\mathcal{t}}}\) is the ith data instance of d[t] and
   \({\text{y}}_{\text{ti}} , \in {\mathcal{y}}_{{\mathcal{t}}}\) is the
   corresponding class label for x[ti]. further, the source task is
   notated as \({\mathcal{t}}_{{\mathcal{s}}}\), the target task as
   \({\mathcal{t}}_{{\mathcal{t}}}\), the source predictive function as
   f[s](  ), and the target predictive function as f[t](  ).
   id21 is now formally defined. given a source domain
   \({\mathcal{d}}_{\text{s}}\) with a corresponding source task
   \({\mathcal{t}}_{{\mathcal{s}}}\) and a target domain
   \({\mathcal{d}}_{\text{t}}\) with a corresponding task
   \({\mathcal{t}}_{{\mathcal{t}}}\), id21 is the process of
   improving the target predictive function f[t](   ) by using the related
   information from \({\mathcal{d}}_{\text{s}}\) and
   \({\mathcal{t}}_{{\mathcal{s}}}\), where \({\mathcal{d}}_{\text{s}} \,
   \ne {\mathcal{d}}_{\text{t}}\) or \({\mathcal{t}}_{{\mathcal{s}}} \,
   \ne \,{\mathcal{t}}_{{\mathcal{t}}}\). the single source domain defined
   here can be extended to multiple source domains. given the definition
   of id21, since \({\mathcal{d}}_{\text{s}} = \left\{
   {{\mathcal{x}}_{{\mathcal{s}}} ,\,{\text{p}}\left(
   {{\text{x}}_{\text{s}} } \right)} \right\}\) and
   \({\mathcal{d}}_{\text{t}} = \left\{ {{\mathcal{x}}_{{\mathcal{t}}}
   ,\,{\text{p}}\left( {{\text{x}}_{\text{t}} } \right)} \right\}\), the
   condition where \({\mathcal{d}}_{\text{s}} \, \ne
   {\mathcal{d}}_{\text{t}}\) means that \({\mathcal{x}}_{s} \, \ne
   {\mathcal{x}}_{{\mathcal{t}}}\) and/or
   \({\text{p}}({\text{x}}_{\text{s}} )\,\, \ne \,{\text{p}}\left(
   {{\text{x}}_{\text{t}} } \right)\text{ }\). the case where
   \({\mathcal{x}}_{{\mathcal{s}}} \, \ne {\mathcal{x}}_{{\mathcal{t}}}\)
   with respect to id21 is defined as heterogeneous transfer
   learning. the case where \({\mathcal{x}}_{{\mathcal{s}}} \, =
   {\mathcal{x}}_{{\mathcal{t}}}\) with respect to id21 is
   defined as homogeneous id21. going back to the example of
   software module defect classification, heterogeneous id21
   is the case where the source software project has different metrics
   (features) than the target software project. alternatively, homogeneous
   id21 is when the software metrics are the same for both
   the source and the target software projects. continuing with the
   definition of id21, the case where
   \({\text{p}}({\text{x}}_{\text{s}} )\, \ne \,{\text{p}}\left(
   {{\text{x}}_{\text{t}} } \right)\text{ }\) means the marginal
   distributions in the input spaces are different between the source and
   the target domains. shimodaira [[40]107] demonstrated that a learner
   trained with a given source domain will not perform optimally on a
   target domain when the marginal distributions of the input domains are
   different. referring to the software module defect classification
   application, an example of marginal distribution differences is when
   the source software program is written for a user interface system and
   the target software program is written for dsp signaling decoder
   algorithm. another possible condition of id21 (from the
   definition above) is \({\mathcal{t}}_{{\mathcal{s}}} \, \ne
   \,{\mathcal{t}}_{{\mathcal{t}}}\), and it was stated that
   \({\mathcal{t}} = \left\{ {{\mathcal{y}},f( \cdot )} \right\}\) or to
   rewrite this, \({\mathcal{t}} = \left\{ {{\mathcal{y}},{\text{p}}\left(
   {{\text{y}}\left| {\text{x}} \right.} \right)} \right\}\). therefore,
   in a id21 environment, it is possible that
   \({\mathcal{y}}_{{\mathcal{s}}} \, \ne
   \,{\mathcal{y}}_{{\mathcal{t}}}\) and/or \({\text{p}}(\left.
   {{\text{y}}_{\text{s}} } \right|{\text{x}}_{\text{s}} )\, \ne
   \,{\text{p}}\left( {{\text{y}}_{\text{t}} \left| {{\text{x}}_{\text{t}}
   } \right.} \right)\text{ }\). the case where \({\text{p}}(\left.
   {{\text{y}}_{\text{s}} } \right|{\text{x}}_{\text{s}} )\, \ne
   \,{\text{p}}\left( {{\text{y}}_{\text{t}} \left| {{\text{x}}_{\text{t}}
   } \right.} \right)\text{ }\) means the id155
   distributions between the source and target domains are different. an
   example of a conditional distribution mismatch is when a particular
   software module yields different fault prone results in the source and
   target domains. the case of \({\mathcal{y}}_{{\mathcal{s}}} \, \ne
   \,{\mathcal{y}}_{{\mathcal{t}}}\) refers to a mismatch in the class
   space. an example of this case is when the source software project has
   a binary label space of true for defect prone and false for not defect
   prone, and the target domain has a label space that defines five levels
   of fault prone modules. another case that can cause discriminative
   classifier degradation is when \({\text{p}}({\text{y}}_{\text{s}} )\,
   \ne \,{\text{p}}\left( {{\text{y}}_{\text{t}} } \right)\), which is
   caused by an unbalanced labeled data set between the source and target
   domains. the case of traditional machine learning is
   \({\mathcal{d}}_{\text{s}} = {\mathcal{d}}_{\text{t}}\) and
   \({\mathcal{t}}_{{\mathcal{s}}} \,\, = {\mathcal{t}}_{{\mathcal{t}}}\).
   the common notation used in this paper is summarized in table [41]1.
   table 1

   summary of commonly used notation

   notation

   description

   notation

   description

   \({\mathcal{x}}\)

   input feature space

   p (x)

   marginal distribution

   \({\mathcal{y}}\)

   label space

   p (y|x)

   conditional distribution

   \({\mathcal{t}}\)

   predictive learning task

   p (y)

   label distribution

   subscript s

   denotes source

   d[s]

   source domain data

   subscript t

   denotes target

   d[t]

   target domain data

   to elaborate on the distribution issues that can occur between the
   source and target domains, the application of natural language
   processing is used to illustrate. in natural language processing, text
   instances are often modeled as a bag-of-words where a unique word
   represents a feature. consider the example of review text where the
   source covers movie reviews and the target covers book reviews. words
   that are generic and domain independent should occur at a similar rate
   in both domains. however, words that are domain specific are used more
   frequently in one domain because of the strong relationship with that
   domain topic. this is referred to as frequency feature bias and will
   cause the marginal distribution between the source and target domains
   to be different \(\left( {{\text{p}}({\text{x}}_{\text{s}} )\, \ne
   \,{\text{p}}\left( {{\text{x}}_{\text{t}} } \right)} \right)\text{ }\).
   another form of bias is referred to as context feature bias and this
   will cause the conditional distributions to be different between the
   source and target domains \({\text{p}}(\left. {{\text{y}}_{\text{s}} }
   \right|{\text{x}}_{\text{s}} )\, \ne \,{\text{p}}\left(
   {{\text{y}}_{\text{t}} \left| {{\text{x}}_{\text{t}} } \right.}
   \right)\text{ }\). an example of context feature bias is when a word
   can have different meanings in two domains. a specific example is the
   word    monitor    where in one domain it is used as a noun and in another
   domain it is used as a verb. another example of context feature bias is
   with sentiment classification when a word has a positive meaning in one
   domain and a negative meaning in another domain. the word    small    can
   have a good meaning if describing a cell phone but a bad meaning if
   describing a hotel room. a further example of context feature bias is
   demonstrated in the case of document sentiment classification of
   reviews where the source domain contains reviews of one product written
   in german and the target domain contains reviews of a different product
   written in english. the translated words from the source document may
   not accurately represent the actual words used in the target documents.
   an example is the case of the german word    betonen   , which translates
   to the english word    emphasize    by google translator. however, in the
   target documents the corresponding english word used is    highlight   
   (zhou [[42]144]).

   negative transfer, with regards to id21, occurs when the
   information learned from a source domain has a detrimental effect on a
   target learner. more formally, given a source domain
   \({\mathcal{d}}_{{\mathcal{s}}}\), a source task
   \({\mathcal{t}}_{{\mathcal{s}}}\), a target domain
   \({\mathcal{d}}_{{\mathcal{t}}}\), a target task
   \({\mathcal{t}}_{{\mathcal{t}}}\), a predictive learner f[t1](  )
   trained only with \({\mathcal{d}}_{{\mathcal{t}}}\), and a predictive
   learner f[t2](  ) trained with a id21 process combining
   \({\mathcal{d}}_{{\mathcal{t}}}\) and
   \({\mathcal{d}}_{{\mathcal{s}}}\), negative transfer occurs when the
   performance of f[t1](  ) is greater than the performance of f[t2](  ).
   the topic of negative transfer addresses the need to quantify the
   amount of relatedness between the source domain and the target domain
   and whether an attempt to transfer knowledge from the source domain
   should be made. extending the definition above, positive transfer
   occurs when the performance of f[t2](  ) is greater than the performance
   of f[t1](  ).

   throughout the literature on id21, there are a number of
   terminology inconsistencies. phrases such as id21 and
   id20 are used to refer to similar processes. the following
   definitions will be used in this paper. id20, as it
   pertains to id21, is the process of adapting one or more
   source domains for the means of transferring information to improve the
   performance of a target learner. the id20 process attempts
   to alter a source domain in an attempt to bring the distribution of the
   source closer to that of the target. another area of literature
   inconsistencies is in characterizing the id21 process with
   respect to the availability of labeled and unlabeled data. for example,
   daum   [[43]22] and chattopadhyay [[44]14] define supervised transfer
   learning as the case of having abundant labeled source data and limited
   labeled target data, and semi-supervised id21 as the case
   of abundant labeled source data and no labeled target data. in gong
   [[45]42] and blitzer [[46]5], semi-supervised id21 is the
   case of having abundant labeled source data and limited labeled target
   data, and unsupervised id21 is the case of abundant
   labeled source data and no labeled target data. cook [[47]19] and feuz
   [[48]36] provide a different variation where the definition of
   supervised or unsupervised refers to the presence or absence of labeled
   data in the source domain and informed or uninformed refers to the
   presence or absence of labeled data in the target domain. with this
   definition, a labeled source and limited labeled target domain is
   referred to as informed supervised id21. pan [[49]84]
   refers to inductive id21 as the case of having available
   labeled target domain data, transductive id21 as the case
   of having labeled source and no labeled target domain data, and
   unsupervised id21 as the case of having no labeled source
   and no labeled target domain data. this paper will explicitly state
   when labeled and unlabeled data are being used in the source and target
   domains.
   there are different strategies and implementations for solving a
   id21 problem. the majority of the homogeneous transfer
   learning solutions employ one of three general strategies which include
   trying to correct for the marginal distribution difference in the
   source, trying to correct for the conditional distribution difference
   in the source, or trying to correct both the marginal and conditional
   distribution differences in the source. the majority of the
   heterogeneous id21 solutions are focused on aligning the
   input spaces of the source and target domains with the assumption that
   the domain distributions are the same. if the domain distributions are
   not equal, then further id20 steps are needed. another
   important aspect of a id21 solution is the form of
   information transfer (or what is being transferred). the form of
   information transfer is categorized into four general transfer
   categories [[50]84]. the first transfer category is id21
   through instances. a common method used in this case is for instances
   from the source domain to be reweighted in an attempt to correct for
   marginal distribution differences. these reweighted instances are then
   directly used in the target domain for training (examples in huang
   [[51]51], jiang [[52]53]). these reweighting algorithms work best when
   the conditional distribution is the same in both domains. the second
   transfer category is id21 through features. feature-based
   id21 approaches are categorized in two ways. the first
   approach transforms the features of the source through reweighting to
   more closely match the target domain (e.g. pan [[53]82]). this is
   referred to as asymmetric feature transformation and is depicted in
   fig. [54]1b. the second approach discovers underlying meaningful
   structures between the domains to find a common latent feature space
   that has predictive qualities while reducing the marginal distribution
   between the domains (e.g. blitzer [[55]5]). this is referred to as
   symmetric feature transformation and is depicted in fig. [56]1a. the
   third transfer category is to transfer knowledge through shared
   parameters of source and target domain learner models or by creating
   multiple source learner models and optimally combining the reweighted
   learners (ensemble learners) to form an improved target learner
   (examples in gao [[57]37], bonilla [[58]8], and evgeniou [[59]33]). the
   last transfer category (and the least used approach) is to transfer
   knowledge based on some defined relationship between the source and
   target domains (examples in mihalkova [[60]74] and li [[61]62]).
   [62]open image in new window fig. 1
   fig. 1

   a the symmetric transformation mapping (t[s] and t[t]) of the source
   (x[s]) and target (x[t]) domains into a common latent feature space. b
   the asymmetric transformation (t[t]) of the source domain (x[s]) to the
   target domain (x[t])

   detailed information on specific id21 solutions are
   presented in    [63]homogeneous id21       [64]heterogeneous
   id21    and    [65]negative transfer    sections. these sections
   represent the majority of the works surveyed in this paper.
      [66]homogeneous id21       [67]heterogeneous transfer
   learning    and    [68]negative transfer    sections cover homogeneous
   id21 solutions, heterogeneous id21 solutions,
   and solutions addressing negative transfer, respectively. the section
   covering id21 applications focuses on the general
   applications that id21 is applied to, but does not
   describe the solution details.

homogeneous id21

   this section presents surveyed papers covering homogeneous transfer
   learning solutions and is divided into subsections that correspond to
   the transfer categories of instance-based, feature-based (both
   asymmetric and symmetric), parameter-based, and relational-based.
   recall that homogeneous id21 is the case where
   \({\mathcal{x}}_{{\mathcal{s}}} = {\mathcal{x}}_{{\mathcal{t}}}\). the
   algorithms surveyed are summarized in table [69]2.
   table 2

   homogeneous id21 approaches surveyed in    [70]homogeneous
   id21    section listing different characteristics of each
   approach

   approach

   transfer category

   source data

   target data

   multiple sources

   generic solution

   negative transfer

   cp-mda [[71]14]

   parameter

   labeled

   limited labels

      

      

   2sw-mda [[72]14]

   instance

   labeled

   unlabeled

      

      

   fam [[73]22]

   asymmetric feature

   labeled

   limited labels

      

      

   dtmkl [[74]27]

   asymmetric feature

   labeled

   unlabeled


      

   jda [[75]69]

   asymmetric feature

   labeled

   unlabeled


      

   artl [[76]68]

   asymmetric feature

   labeled

   unlabeled


      

   tca [[77]87]

   symmetric feature

   labeled

   unlabeled


      

   sfa [[78]83]

   symmetric feature

   labeled

   limited labels

      

      

   sda [[79]41]

   symmetric feature

   labeled

   unlabeled


      

   gfk [[80]42]

   symmetric feature

   labeled

   unlabeled


      

      

   dcp [[81]106]

   symmetric feature

   labeled

   unlabeled


      

   tid98 [[82]81]

   symmetric feature

   labeled

   limited labels


      

   mmkt [[83]114]

   parameter

   labeled

   limited labels

      

      

      

   dsm [[84]28]

   parameter

   labeled

   unlabeled

      


      

   mstradaboost [[85]138]

   instance

   labeled

   limited labels

      

      

      

   tasktradaboost [[86]138]

   parameter

   labeled

   limited labels

      

      

      

   rap [[87]62]

   relational

   labeled

   unlabeled

   ssfe [[88]132]

   hybrid (instance and feature)

   labeled

   limited labels

   the methodology of homogeneous id21 is directly applicable
   to a big data environment. as repositories of big data become more
   available, there is a desire to use this abundant resource for machine
   learning tasks, avoiding the timely and potentially costly collection
   of new data. if there is an available dataset that is drawn from a
   domain that is related to, but does not an exactly match a target
   domain of interest, then homogeneous id21 can be used to
   build a predictive model for the target domain as long as the input
   feature space is the same.

instance-based id21

   the paper by chattopadhyay [[89]14] proposes two separate solutions
   both using multiple labeled source domains. the first solution is the
   id155 based multi-source id20 (cp-mda)
   approach, which is a id20 process based on correcting the
   conditional distribution differences between the source and target
   domains. the cp-mda approach assumes a limited amount of labeled target
   data is available. the main idea is to use a combination of source
   domain classifiers to label the unlabeled target data. this is
   accomplished by first building a classifier for each separate source
   domain. then a weight value is found for each classifier as a function
   of the closeness in conditional distribution between each source and
   the target domain. the weighted source classifiers are summed together
   to create a learning task that will find the pseudo labels (estimated
   labels later used for training) for the unlabeled target data. finally,
   the target learner is built from the labeled and pseudo labeled target
   data. the second proposed solution is the two stage weighting framework
   for multi-source id20 (2sw-mda) which addresses both
   marginal and conditional distribution differences between the source
   and target domains. labeled target data is not required for the 2sw-mda
   approach; however, it can be used if available. in this approach, a
   weight for each source domain is computed based on the marginal
   distribution differences between the source and target domains. in the
   second step, the source domain weights are modified as a function of
   the difference in the conditional distribution as performed in the
   cp-mda approach previously described. finally, a target classifier is
   learned based on the reweighted source instances and any labeled target
   instances that are available. the work presented in chattopadhyay
   [[90]14] is an extension of duan [[91]29] where the novelty is in
   calculating the source weights as a function of conditional
   id203. note, the 2sw-mda approach is an example of an
   instance-based transfer category, but the cp-mda approach is more
   appropriately classified as a parameter-based transfer category (see
      [92]parameter-based id21    section). experiments are
   performed for muscle fatigue classification using surface
   electromyography data where classification accuracy is measured as the
   performance metric. each source domain represents one person   s surface
   electromyography measurements. a baseline approach is constructed using
   a support vector machine (id166) classifier trained on the combination of
   seven sources used for this test. the id21 approaches that
   are tested against include an approach proposed by huang [[93]51], pan
   [[94]87], zhong [[95]143], gao [[96]37], and duan [[97]29]. the order
   of performance from best to worst is 2sw-mda, cp-mda, duan [[98]29],
   zhong [[99]143], gao [[100]37], pan [[101]87], huang [[102]51], and the
   baseline approach. all the id21 approaches performed
   better than the baseline approach.

asymmetric feature-based id21

   in an early and often cited work, daum   [[103]22] proposes a simple
   id20 algorithm, referred to as the feature augmentation
   method (fam), requiring only ten lines of perl script that uses labeled
   source data and limited labeled target data. in a id21
   environment, there are scenarios where a feature in the source domain
   may have a different meaning in the target domain. the issue is
   referred to as context feature bias, which causes the conditional
   distributions between the source and target domains to be different. to
   resolve context feature bias, a method to augment the source and target
   feature space with three duplicate copies of the original feature set
   is proposed. more specifically, the three duplicate copies of the
   original feature set in the augmented source feature space represent a
   common feature set, a source specific feature set, and a target
   specific feature set which is always set to zero. in a similar way, the
   three duplicate copies of the original feature set in the augmented
   target feature space represent a common feature set, a source specific
   feature set which is always set to zero, and a target specific feature
   set. by performing this feature augmentation, the feature space is
   duplicated three times. from the feature augmentation structure, a
   classifier learns the individual feature weights for the augmented
   feature set, which will help correct for any feature bias issues. using
   a text document example where features are modeled as a bag-of-words, a
   common word like    the    would be assigned (through the learning process)
   a high weight for the common feature set, and a word that is different
   between the source and target like    monitor    would be assigned a high
   weight for the corresponding domain feature set. the duplication of
   features creates feature separation between the source and target
   domains, and allows the final classifier to learn the optimal feature
   weights. for the experiments, a number of different natural language
   processing applications are tested and in each case the classification
   error rate is measured as the performance metric. an id166 learner is
   used to implement the daum   [[104]22] approach. a number of baseline
   approaches with no id21 techniques are measured along with
   a method by chelba [[105]15]. the test results show the daum   [[106]22]
   method is able to outperform the other methods tested. however, when
   the source and target domains are very similar, the daum   [[107]22]
   approach tends to underperform. the reason for the underperformance is
   the duplication of feature sets represents irrelevant and noisy
   information when the source and target domains are very similar.

   multiple kernel learning is a technique used in traditional machine
   learning algorithms as demonstrated in the works of wu [[108]130] and
   vedaldi [[109]118]. multiple kernel learning allows for an optimal
   id81 to be learned in a computationally efficient manner.
   the paper by duan [[110]27] proposes to implement a multiple kernel
   learning framework for a id21 environment called the
   domain transfer multiple kernel learning (dtmkl). instead of learning
   one kernel, multiple kernel learning assumes the kernel is comprised of
   a linear combination of multiple predefined base kernels. the final
   classifier and the id81 are learned simultaneously which has
   the advantage of using labeled data during the kernel learning process.
   this is an improvement over pan [[111]82] and huang [[112]51] where a
   two-stage approach is used. the final classifier learning process
   minimizes the structural risk functional [[113]117] and the marginal
   distribution between domains using the maximum mean discrepancy measure
   [[114]10]. pseudo labels are found for the unlabeled target data to
   take advantage of this information during the learning process. the
   pseudo labels are found as a weighted combination of base classifiers
   (one for each feature) trained from the labeled source data. a
   id173 term is added to the optimization problem to ensure the
   predicted values from the final target classifier and the base
   classifiers are similar for the unlabeled target data. experiments are
   performed on the applications of video concept detection, text
   classification, and email spam detection. the methods tested against
   include a baseline approach using an id166 classifier trained on the
   labeled source data, the feature replication method from daum  
   [[115]22], an adaptive id166 method from yang [[116]135], a cross-domain
   id166 method proposed by jiang [[117]55], and a kernel mean matching
   method by huang [[118]51]. the dtmkl approach uses an id166 learner for
   the experiments. average precision and classification accuracy are
   measured as the performance metrics. the dtmkl method performed the
   best for all applications, and the baseline approach is consistently
   the worst performing. the other methods showed better performance over
   the baseline which demonstrated a positive id21 effect.

   the work by long [[119]69] is a joint id20 (jda) solution
   that aims to simultaneously correct for the marginal and conditional
   distribution differences between the labeled source domain and the
   unlabeled target domain. principal component analysis (pca) is used for
   optimization and id84. to address the difference in
   marginal distribution between the domains, the maximum mean discrepancy
   distance measure [[120]10] is used to compute the marginal distribution
   differences and is integrated into the pca optimization algorithm. the
   next part of the solution requires a process to correct the conditional
   distribution differences, which requires labeled target data. since the
   target data is unlabeled, pseudo labels (estimated target labels) are
   found by learning a classifier from the labeled source data. the
   maximum mean discrepancy distance measure is modified to measure the
   distance between the conditional distributions and is integrated into
   the pca optimization algorithm to minimize the conditional
   distributions. finally, the features identified by the modified pca
   algorithm are used to train the final target classifier. experiments
   are performed for the application of image recognition and
   classification accuracy is measured as the performance metric. two
   baseline approaches of a 1-nearest neighbor classifier and a pca
   approach trained on the source data are tested. id21
   approaches tested for this experiment include the approach by pan
   [[121]87], gong [[122]42], and si [[123]109]. these id21
   approaches only attempt to correct for marginal distribution
   differences between domains. the long [[124]69] approach is the best
   performing, followed by the pan [[125]87] and si [[126]109] approaches
   (a tie), then the gong [[127]42] approach, and finally the baseline
   approaches. all id21 approaches perform better than the
   baseline approaches. the possible reason behind the underperformance of
   the gong [[128]42] approach is the data smoothness assumption that is
   made for the gong [[129]42] solution may not be intact for the data
   sets tested.
   the paper by long [[130]68] proposes an adaptation id173 based
   id21 (artl) framework for scenarios of labeled source data
   and unlabeled target data. this id21 framework proposes to
   correct the difference in marginal distribution between the source and
   target domains, correct the difference in conditional distribution
   between the domains, and improve classification performance through a
   manifold id173 [[131]4] process (which optimally shifts the
   hyperplane of an id166 learner). this complete framework process is
   depicted in fig. [132]2. the proposed artl framework will learn a
   classifier by simultaneously performing structural risk minimization
   [[133]117], reducing the marginal and conditional distributions between
   the domains, and optimizing the manifold consistency of the marginal
   distribution. to resolve the conditional distribution differences,
   pseudo labels are found for the target data in the same way as proposed
   by long [[134]69]. a difference between the artl approach and long
   [[135]69] is artl learns the final classifier simultaneously while
   minimizing the domain distribution differences, which is claimed by
   long [[136]68] to be a more optimal solution. unfortunately, the
   solution by long [[137]69] is not included in the experiments.
   experiments are performed on the applications of text classification
   and image classification where classification accuracy is measured as
   the performance metric. there are three baseline methods tested where
   different classifiers are trained with the labeled source data. there
   are five id21 methods tested against, which include
   methods by ling [[138]66], pan [[139]83], pan [[140]87], quanz
   [[141]94], and xiao [[142]133]. the order of performance from best to
   worst is artl, xiao [[143]133], pan [[144]87], pan [[145]83], quanz
   [[146]94] and ling [[147]66] (tie), and the baseline approaches. the
   baseline methods underperformed all other id21 approaches
   tested.
   [148]open image in new window fig. 2
   fig. 2

   artl overview showing mda marginal distribution adaptation, cda
   conditional distribution adaptation, and mr manifold id173

   diagram adapted from long [[149]68]

symmetric feature-based id21

   the paper by pan [[150]87] proposes a feature transformation approach
   for id20 called transfer component analysis (tca), which
   does not require labeled target data. the goal is to discover common
   latent features that have the same marginal distribution across the
   source and target domains while maintaining the intrinsic structure of
   the original domain data. the latent features are learned between the
   source and target domains in a reproducing kernel hilbert space
   [[151]111] using the maximum mean discrepancy [[152]10] as a marginal
   distribution measurement criteria. once the latent features are found,
   traditional machine learning is used to train the final target
   classifier. the tca approach extends the work of pan [[153]82] by
   improving computational efficiency. experiments are conducted for the
   application of wifi localization where the location of a particular
   device is being predicted. the source domain is comprised of data
   measured from different room and building topologies. the performance
   metric measured is the average error distance of the position of a
   device. the id21 methods tested against are from blitzer
   [[154]5] and huang [[155]51]. the tca method performed the best
   followed by the huang [[156]51] approach and the blitzer [[157]5]
   approach. for the blitzer [[158]5] approach, the manual definition of
   the pivot functions (functions that define the correspondence) is
   important to performance and specific to the end application. there is
   no mention as to how the pivot functions are defined for wifi
   localization.

   the work by pan [[159]83] proposes a spectral feature alignment (sfa)
   id21 algorithm that discovers a new feature representation
   for the source and target domain to resolve the marginal distribution
   differences. the sfa method assumes an abundance of labeled source data
   and a limited amount of labeled target data. the sfa approach
   identifies domain-specific and domain-independent features and uses the
   domain-independent features as a bridge to build a bipartite graph
   modeling the co-occurrence relationship between the domain-independent
   and domain-specific features. if the graph shows two domain-specific
   features having connections to common domain-independent feature, then
   there is a higher chance the domain-specific features are aligned. a
   spectral id91 algorithm based on graph spectral theory [[160]17]
   is used on the bipartite graph to align domain-specific features and
   domain-independent features into a set of clusters representing new
   features. these clusters are used to reduce the difference between
   domain-specific features in the source and the target domains. all the
   data instances are projected into this new feature space and a final
   target classifier is trained using the new feature representation. the
   sfa algorithm is a type of correspondence learning where the
   domain-independent features act as pivot features (see blitzer [[161]5]
   and prettenhofer [[162]91] for further information on correspondence
   learning). the sfa method is well-suited for the application of text
   document classification where a bag-of-words model is used to define
   features. for this application there are domain-independent words that
   will appear often in both domains and domain-specific words that will
   appear often only in a specific domain. this is referred to as
   frequency feature bias, which causes marginal distribution differences
   between the domains. an example of domain-specific features being
   combined is the word    sharp    appearing often in the source domain but
   not in the target domain, and the word    hooked    appearing often in the
   target but not in the source domain. these words are both connected to
   the same domain-independent words (for example    good    and    exciting   ).
   further, when the words    sharp    or    hooked    appear in text instances,
   the labels are the same. the idea is to combine (or align) these two
   features (in this case    sharp    and    hooked   ) to form a new single
   invariant feature. the experiments are performed on sentiment
   classification where classification accuracy is measured as the
   performance metric. a baseline approach is tested where a classifier is
   trained only on source data. an upper limit approach is also tested
   where a classifier is trained on a large amount of labeled target data.
   the competing id21 approach tested against is by blitzer
   [[163]5]. the order of performance for the tests from best to worst is
   the upper limit approach, sfa, blitzer [[164]5], and baseline approach.
   not only does the sfa approach demonstrate better performance than
   blitzer [[165]5], the sfa approach does not need to manually define
   pivot functions as in the blitzer [[166]5] approach. the sfa approach
   only addresses the issue of marginal distribution differences and does
   not address any context feature bias issues, which would represent
   conditional distribution differences.

   the work by glorot [[167]41] proposes a deep learning algorithm for
   id21 called a stacked denoising autoencoder (sda) to
   resolve the marginal distribution differences between a labeled source
   domain and an unlabeled target domain. deep learning algorithms learn
   intermediate invariant concepts between two data sources, which are
   used to find a common latent feature set. the first step in this
   process is to train the stacked denoising autoencoders [[168]119] with
   unlabeled data from the source and target domains. this transforms the
   input space to discover the common invariant latent feature space. the
   next step is to train a classifier using the transformed latent
   features with the labeled source data. experiments are performed on
   text review sentiment classification where transfer loss is measured as
   the performance metric. transfer loss is defined as the classification
   error rate using a learner only trained on the source domain and tested
   on the target minus the classification error rate using a learner only
   trained on the target domain and tested on the target. there are 12
   different source and target domain pairs that are created from four
   unique review topics. a baseline method is tested where an id166
   classifier is trained on the source domain. the id21
   approaches that are tested include an approach by blitzer [[169]5], li
   [[170]63], and pan [[171]83]. the glorot [[172]41] approach performed
   the best with the blitzer [[173]5], li [[174]63], and pan [[175]83]
   methods all having similar performance and all outperforming the
   baseline approach.

   in the paper by gong [[176]42], a id20 technique called
   the geodesic flow kernel (gfk) is proposed that finds a low-dimensional
   feature space, which reduces the marginal distribution differences
   between the labeled source and unlabeled target domains. to accomplish
   this, a geodesic flow kernel is constructed using the source and target
   input feature data, which projects a large number of subspaces that lie
   on the geodesic flow curve. the geodesic flow curve represents
   incremental differences in geometric and statistical properties between
   the source and target domain spaces. a classifier is then learned from
   the geodesic flow kernel by selecting the features from the geodesic
   flow curve that are domain invariant. the work of gong [[177]42]
   directly enhances the work of gopalan [[178]43] by eliminating tuning
   parameters and improving computational efficiency. in addition, a rank
   of domain (rod) metric is developed to evaluate which of many source
   domains is the best match for the target domain. the rod metric is a
   function of the geometric alignment between the domains and the
   kullback   leibler divergence in data distributions between the projected
   source and target subspaces. experiments are performed for the
   application of image classification where classification accuracy is
   measured as the performance metric. the tests use pairs of source and
   target data sets from four available data sets. a baseline approach is
   defined that does not use id21, along with the approach
   defined by gopalan [[179]43]. additionally, the gong [[180]42] approach
   uses a 1-nearest neighbor classifier. the results in order from best to
   worst performance are gong [[181]42], gopalan [[182]43], and the
   baseline approach. the rod measurements between the different source
   and target domain pairs tested have a high correlation to the actual
   test results, meaning the domains that are found to be more related
   with respect to the rod measurement had higher classification
   accuracies.

   the solution by shi [[183]106], referred to as the discriminative
   id91 process (dcp), proposes to equalize the marginal
   distribution of the labeled source and unlabeled target domains. a
   discriminative id91 process is used to discover a common latent
   feature space that is domain invariant while simultaneously learning
   the final target classifier. the motivating assumptions for this
   solution are the data in both domains form well-defined clusters which
   correspond to unique class labels, and the clusters from the source
   domain are geometrically close to the target clusters if they share the
   same label. through id91, the source domain labels can be used to
   estimate the target labels. a one-stage solution is formulated that
   minimizes the marginal distribution differences while minimizing the
   predicted classification error in the target domain using a nearest
   neighbor classifier. experiments are performed for object recognition
   and sentiment classification where classification accuracy is measured
   as the performance metric. the approach described above is tested
   against a baseline approach taken from weinberger [[184]126] with no
   id21. other id21 approaches tested include an
   approach from pan [[185]87], blitzer [[186]5], and gopalan [[187]43].
   the blitzer [[188]5] approach is not tested for the object recognition
   application because the pivot functions are not easily defined for this
   application. for the object recognition tests, the shi [[189]106]
   method is best in five out of six comparison tests. for the text
   classification tests, the shi [[190]106] approach is the best
   performing overall, with the blitzer [[191]5] approach a close second.
   an important point to note is the baseline method outperformed the pan
   [[192]87] and gopalan [[193]43] methods in both tests. both the pan
   [[194]87] and gopalan [[195]43] methods are two-stage id20
   processes where the first stage reduces the marginal distributions
   between the domains and the second stage trains a classifier with the
   adapted domain data. this paper offers a hypothesis that two-stage
   processes are actually detrimental to id21 (causes
   negative transfer). the one-stage learning process is a novel idea
   presented by this paper. the hypothesis that the two-stage transfer
   learning process creates low performing learners does not agree with
   the results presented in the individual papers by gopalan [[196]43] and
   pan [[197]87] and other previously surveyed works.

   convolutional neural networks (id98) have been successfully used in
   traditional data mining environments [[198]59]. however, a id98 requires
   a large amount of labeled training data to be effective, which may not
   be available. the paper by oquab [[199]81] proposes a id21
   method of training a id98 with available labeled source data (a source
   learner) and then extracting the id98 internal layers (which represent a
   generic mid-level feature representation) to a target id98 learner. this
   method is referred to as the transfer convolutional neural network
   (tid98). to correct for any further distribution differences between the
   source and the target domains, an adaptation layer is added to the
   target id98 learner, which is trained from the limited labeled target
   data. the experiments are run on the application of object image
   classification where average precision is measured as the performance
   metric. the oquab [[200]81] method is tested against a method proposed
   by marszalek [[201]73] and a method proposed by song [[202]110]. both
   the marszalek [[203]73] and song [[204]110] approaches are not transfer
   learning approaches and are trained on the limited labeled target data.
   the first experiment is performed using the pascal voc 2007 data set as
   the target and id163 2012 as the source. the oquab [[205]81] method
   outperformed both song [[206]110] and marszalek [[207]73] approaches
   for this test. the second experiment is performed using the pascal voc
   2012 data set as the target and id163 2012 as the source. in the
   second test, the oquab [[208]81] method marginally outperformed the
   song [[209]110] method (the marszalek [[210]73] method was not tested
   for the second test). the tests successfully demonstrated the ability
   to transfer information from one id98 learner to another.

parameter-based id21

   the paper by tommasi [[211]114] addresses the id21
   environment characterized by limited labeled target data and multiple
   labeled source domains where each source corresponds to a particular
   class. in this case, each source is able to build a binary learner to
   predict that class. the objective is to build a target binary learner
   for a new class using minimal labeled target data and knowledge
   transferred from the multiple source learners. an algorithm is proposed
   to transfer the id166 hyperplane information of each of the source
   learners to the new target learner. to minimize the effects of negative
   transfer, the information transferred from each source to the target
   will be weighted such that the most related source domains receive the
   highest weighting. the weights are determined through a leave out one
   process as defined by cawley [[212]13]. the tommasi [[213]114]
   approach, called the multi-model knowledge transfer (mmkt) method,
   extends the method proposed by tommasi [[214]113] that only transfers a
   single source domain. experiments are performed on the application of
   image recognition where classification accuracy is measured as the
   performance metric. id21 methods tested include an average
   weight approach (same as tommasi [[215]114] but all source weights are
   equal), and the tommasi [[216]113] approach. a baseline approach is
   tested, which is trained on the limited labeled target data. the best
   performing method is tommasi [[217]114], followed by the average
   weight, tommasi [[218]113], and the baseline approach. as the number of
   labeled target instances goes up, the tommasi [[219]114] and average
   weight methods converge to the same performance. this is because the
   adverse effects of negative transfer are lessened as the labeled target
   data increases. this result demonstrates the tommasi [[220]114]
   approach is able to lessen the effects of negative transfer from
   unrelated sources.

   the id21 approach presented in the paper by duan
   [[221]28], referred to as the domain selection machine (dsm), is
   tightly coupled to the application of event recognition in consumer
   videos. event recognition in videos is the process of predicting the
   occurrence of a particular event or topic (e.g.    show    or
      performance   ) in a given video. in this scenario, the target domain is
   unlabeled and the source information is obtained from annotated images
   found via web searches. for example, a text query of the event    show   
   for images on photosig.com represents one source and the same query on
   flickr.com represents another separate source. the domain selection
   machine proposed in this paper is realized as follows. for each
   individual source, an id166 classifier is created using sift (lowe
   [[222]70]) image features. the final target classifier is made up of
   two parts. the first part is a weighted sum of the source classifier
   outputs whose input is the sift features from key frames of the input
   video. the second part is a learning function whose inputs are
   space   time features [[223]123] from the input video and is trained from
   target data where the target labels are estimated (pseudo labels) from
   the weighted sum of the source classifiers. to combat the effects of
   negative transfer from unrelated sources, the most relevant source
   domains are selected by using an alternating optimization algorithm
   that iteratively solves the target decision function and the domain
   selection vector. experiments are performed in the application of event
   recognition in videos as described above where the mean average
   precision is measured as the performance metric. a baseline method is
   created by training a separate id166 classifier on each source domain and
   then equally combining the classifiers. the other id21
   approaches tested include the approach by bruzzone [[224]11],
   schweikert [[225]101], duan [[226]29], and chattopadhyay [[227]14]. the
   duan [[228]29] approach outperforms all the other approaches tested.
   the other approaches all have similar results, meaning the transfer
   learning methods did not outperform the baseline approach. the possible
   reason for this result is the existence of unrelated sources in the
   experiment. the other id21 approaches tested had no
   mechanism to guard against negative transfer from unrelated sources.

   the paper by yao [[229]138] first presents an instance-based transfer
   learning approach followed by a separate parameter-based transfer
   learning approach. in the id21 process, if the source and
   target domains are not related enough, negative transfer can occur.
   since it is difficult to measure the relatedness between any particular
   source and target domain, yao [[230]138] proposes to transfer knowledge
   from multiple source domains using a boosting method in an attempt to
   minimize the effects of negative transfer from a single unrelated
   source domain. the boosting process requires some amount of labeled
   target data. yao [[231]138] effectively extends the work of dai
   [[232]21] (tradaboost) by expanding the transfer boosting algorithm to
   multiple source domains. in the tradaboost algorithm, during every
   boosting iteration, a so-called weak classifier is built using weighted
   instance data from the previous iteration. then, the misclassified
   source instances are lowered in importance and the misclassified target
   instances are raised in importance. in the multi-source tradaboost
   algorithm (called mstradaboost), each iteration step first finds a weak
   classifier for each source and target combination, and then the final
   weak classifier is selected for that iteration by finding the one that
   minimizes the target classification error. the instance reweighting
   step remains the same as in the tradaboost. an alternative multi-source
   boosting method (tasktradaboost) is proposed that transfers internal
   learner parameter information from the source to the target. the
   tasktradaboost algorithm first finds candidate weak classifiers from
   each individual source by performing an adaboost process on each source
   domain. then an adaboost process is performed on the labeled target
   data, and at every boosting iteration, the weak classifier used is
   selected from the candidate weak source classifiers (found in the
   previous step) that has the lowest classification error using the
   labeled target data. experiments are performed for the application of
   object category recognition where the area under the curve (auc) is
   measured as the performance metric. an adaboost baseline approach using
   only the limited labeled target data is measured along with a
   tradaboost approach using a single source (the multiple sources are
   combined to one) and the limited labeled target data. linear id166
   learners are used as the base classifiers in all approaches. both the
   mstradaboost and tasktradaboost approaches outperform the baseline
   approach and tradaboost approach. the mstradaboost and tasktradaboost
   demonstrated similar performance.

relational-based id21

   the specific application addressed in the paper by li [[233]62] is to
   classify words from a text document into one of three classes (e.g.
   sentiments, topics, or neither). in this scenario, there exists a
   labeled text source domain on one particular subject matter and an
   unlabeled text target domain on a different subject matter. the main
   idea is that sentiment words remain constant between the source and
   target domains. by learning the grammatical and sentence structure
   patterns of the source, a relational pattern is found between the
   source and target domains, which is used to predict the topic words in
   the target. the sentiment words act as a common linkage or bridge
   between the source and target domains. a bipartite word graph is used
   to represent and score the sentence structure patterns. a id64
   algorithm is used to iteratively build a target classifier from the two
   domains. the id64 process starts with defining seeds which are
   instances from the source that match frequent patterns in the target. a
   cross domain classifier is then trained with the seed information and
   extracted target information (there is no target information in the
   first iteration). the classifier is used to predict the target labels
   and the top confidence rated target instances are selected to
   reconstruct the bipartite word graph. the bipartite word graph is now
   used to select new target instances that are added to the seed list.
   this id64 process continues over a selected number of
   iterations, and the cross domain classifier learned in the
   id64 process is now available to predict target samples. this
   method is referred to as the relational adaptive id64 (rap)
   approach. the experiments tested the li [[234]62] approach against an
   upper bound method where a standard classifier is trained with a large
   amount of target data. other id21 methods tested include
   an approach by hu [[235]50], qiu [[236]93], jakob [[237]52], and dai
   [[238]21]. the application tested is word classification as described
   above where the f1 score is measured as the performance metric. the two
   domains tested are related to movie reviews and product reviews. the li
   [[239]62] method performed better than the other id21
   methods, but fell short of the upper bound method as expected. in its
   current form, this algorithm is tightly coupled with its underlying
   text application, which makes it difficult to use for other non-text
   applications.

hybrid-based (instance and parameter) id21

   the paper by xia [[240]132] proposes a two step approach to address
   marginal distribution differences and conditional distribution
   differences between the source and target domains called the sample
   selection and feature ensemble (ssfe) method. a sample selection
   process, using a modified version of principal component analysis, is
   employed to select labeled source domain samples such that the source
   and target marginal distributions are equalized. next, a feature
   ensemble step attempts to resolve the conditional distribution
   differences between the source and target domains. four individual
   classifiers are defined corresponding to parts of speech of noun, verb,
   adverb/adjective, and other. the four classifiers are trained using
   only the features that correspond to that part of speech. the training
   data is the limited labeled target and the labeled source selected in
   the previous sample selection step. the four classifiers are weighted
   as a function of minimizing the classification error using the limited
   labeled target data. the weighted output of the four classifiers is
   used as the final target classifier. this work by xia [[241]132]
   extends the earlier work of xia [[242]131]. the experiments are
   performed for the application of review sentiment classification using
   four different review categories, where each category is combined to
   create 12 different source and target pairs. classification accuracy is
   measured as the performance metric. a baseline approach using all the
   training data from the source is constructed, along with a sample
   selection approach (only using the first step defined above), a feature
   ensemble approach (only using the second step defined above) and the
   complete approach outlined above. the complete approach is the best
   performing, followed by sample selection and feature ensemble
   approaches, and the baseline approach. the sample selection and feature
   ensemble approaches perform equally as well in head-to-head tests. the
   weighting of the four classifiers (defined by the corresponding parts
   of speech) in the procedure above gives limited resolution in
   attempting to adjust for context feature bias issues. a method of
   having more classifiers in the ensemble step could yield better
   performance at the expense of higher complexity.

discussion of homogeneous id21

   the previous surveyed homogeneous id21 works (summarized
   in table [243]2) demonstrate many different characteristics and
   attributes. which homogeneous id21 solution is best for a
   particular application? an important characteristic to evaluate in the
   selection process is what type of differences exist between a given
   source and target domain. the previous solutions surveyed address
   id20 by correcting for marginal distribution differences,
   correcting for conditional distribution differences, or correcting for
   both marginal and conditional distribution differences. the surveyed
   works of duan [[244]27], gong [[245]42], pan [[246]87], li [[247]62],
   shi [[248]106], oquab [[249]81], glorot [[250]41], and pan [[251]83]
   are focused on solving the differences in marginal distribution between
   the source and target domains. the surveyed works of daum   [[252]22],
   yao [[253]138], tommasi [[254]114] are focused on solving the
   differences in conditional distribution between the source and target
   domains. lastly, the surveyed works of long [[255]68], xia [[256]132],
   chattopadhyay [[257]14], duan [[258]28], and long [[259]69] correct the
   differences in both the marginal and conditional distributions.
   correcting for the conditional distribution differences between the
   source and target domain can be problematic as the nature of a transfer
   learning environment is to have minimal labeled target data. to
   compensate for the limited labeled target data, many of the recent
   id21 solutions create pseudo labels for the unlabeled
   target data to facilitate the conditional distribution correction
   process between the source and target domains. to further help
   determine which solution is best for a given id21
   application, the information in table [260]2 should be used to match
   the characteristics of the solution to that of the desired application
   environment. if the application domain contains multiple sources where
   the sources are not mutually uniformly distributed, a solution that
   guards against negative transfer may be of greater benefit. a recent
   trend in the development of id21 solutions is for
   solutions to address both marginal and conditional distribution
   differences between the source and target domains. another emerging
   solution trend is the implementation of a one-stage process as compared
   to a two-stage process. in the recent works of long [[261]68], duan
   [[262]27], shi [[263]106], and xia [[264]132], a one-stage process is
   employed that simultaneously performs the id20 process
   while learning the final classifier. a two-stage solution first
   performs the id20 process and then independently learns
   the final classifier. the claim by long [[265]68] is a one-stage
   solution achieves enhanced performance because the simultaneous solving
   of id20 and the classifier establishes mutual
   reinforcement. the surveyed homogeneous id21 works are not
   specifically applied to big data solutions; however, there is nothing
   to preclude their use in a big data environment.

heterogeneous id21

   heterogeneous id21 is the scenario where the source and
   target domains are represented in different feature spaces. there are
   many applications where heterogeneous id21 is beneficial.
   heterogeneous id21 applications that are covered in this
   section include image recognition [[266]30, [267]58, [268]146,
   [269]105, [270]92, [271]64], multi-language text classification
   [[272]145, [273]30, [274]91, [275]144, [276]64, [277]124], single
   language text classification [[278]121], drug efficacy classification
   [[279]105], human activity classification [[280]46], and software
   defect classification [[281]77]. heterogeneous id21 is
   also directly applicable to a big data environment. as repositories of
   big data become more available, there is a desire to use this abundant
   resource for machine learning tasks, avoiding the timely and
   potentially costly collection of new data. if there is an available
   dataset drawn from a target domain of interest that has a different
   feature space from another target dataset (also drawn from the same
   target domain), then heterogeneous id21 can be used to
   bridge the difference in the feature spaces and build a predictive
   model for that target domain. heterogeneous id21 is still
   a relatively new area of study as the majority of the works covering
   this topic have been published in the last 5 years. from a high-level
   view, there are two main approaches to solving the heterogeneous
   feature space difference. the first approach, referred to as symmetric
   transformation shown in fig. [282]1a, separately transforms the source
   and target domains into a common latent feature space in an attempt to
   unify the input spaces of the domains. the second approach, referred to
   as asymmetric transformation as shown in fig. [283]1b, transforms the
   source feature space to the target feature space to align the input
   feature spaces. the asymmetrical transformation approach is best used
   when the same class instances in the source and target can be
   transformed without context feature bias. many of the heterogeneous
   id21 solutions surveyed make the implicit or explicit
   assumption that the source and the target domain instances are drawn
   from the same domain space. with this assumption there should be no
   significant distribution differences between the domains. therefore,
   once the differences in input feature spaces are resolved, no further
   id20 needs to be performed.

   as is the case with homogeneous id21 solutions, whether
   the source and target domains contain labeled data drives the solution
   formulation for heterogeneous approaches. data label availability is a
   function of the underlying application. the solutions surveyed in this
   paper have different labeled data requirements. for id21
   to be feasible, the source and the target domains must be related in
   some way. some heterogeneous solutions require an explicit mapping of
   the relationship or correspondence between the source and target
   domains. for example, the solutions defined for prettenhofer [[284]91]
   and wei [[285]125] require manual definitions of source and target
   correspondence.

symmetric feature-based id21

   the id21 approach proposed by prettenhofer [[286]91]
   addresses the heterogeneous scenario of a source domain containing
   labeled and unlabeled data, and a target domain containing unlabeled
   data. the structural correspondence learning technique from blitzer
   [[287]5] is applied to this problem. structural correspondence learning
   depends on the manual definition of pivot functions that capture
   correspondence between the source and target domains. effective pivot
   functions should use features that occur frequently in both domains and
   have good predictive qualities. each pivot function is turned into a
   linear classifier using data from the source and target domains. from
   these pivot classifiers, correspondences between features are
   discovered and a latent feature space is learned. the latent feature
   space is used to train the final target classifier. the paper by
   prettenhofer [[288]91] uses this solution to solve the problem of text
   classification where the source is written in one language and the
   target is written in a different language. in this specific
   implementation referred to as cross-language structural correspondence
   learning (clscl), the pivot functions are defined by pairs of words,
   one from the target and one from the source, that represent direct word
   translations from one language to the other. the experiments are
   performed on the applications of document sentiment classification and
   document topic classification. english documents are used in the source
   and other language documents are used in the target. the baseline
   method used in this test trains a learner on the labeled source
   documents, then translates the target documents to the source language
   and tests the translated version. an upper bound method is established
   by training a learner with the labeled target documents and testing
   with the target documents. average classification accuracy is measured
   as the performance metric. the average results show the upper bound
   method performing the best and the prettenhofer [[289]91] method
   performing better than the baseline method. an issue with using
   structural correspondence learning is the difficulty in generalizing
   the pivot functions. for this solution, the pivot functions need to be
   manually and uniquely defined for a specific application, which makes
   it very difficult to port to other applications.

   the paper by shi [[290]105], referred to as heterogeneous spectral
   mapping (hemap), addresses the specific id21 scenario
   where the input feature space is different between the source and
   target \({\mathcal{x}}_{{\mathcal{s}}} \, \ne
   \,{\mathcal{x}}_{{\mathcal{t}}}\), the marginal distribution is
   different between the source and the target
   (\({\text{p}}({\text{x}}_{\text{s}} )\, \ne \,{\text{p}}\left(
   {{\text{x}}_{\text{t}} } \right)\text{ }\)), and the output space is
   different between the source and the target
   (\({\mathcal{y}}_{{\mathcal{s}}} \, \ne
   {\mathcal{y}}_{{\mathcal{t}}}\)). this solution uses labeled source
   data that is related to the target domain and limited labeled target
   data. the first step is to find a common latent input space between the
   source and target domains using a spectral mapping technique. the
   spectral mapping technique is modeled as an optimization objective that
   maintains the original structure of the data while minimizing the
   difference between the two domains. the next step is to apply a
   id91 based sample selection method to select related instances as
   new training data, which resolves the marginal distribution differences
   in the latent input space. finally, a bayesian based method is used to
   find the relationship and resolve the differences in the output space.
   experiments are performed for the applications of image classification
   and drug efficacy prediction. classification error rate is measured as
   the performance metric. this solution demonstrated better performance
   as compared to a baseline approach; however, details on the baseline
   approach are not documented in the paper and no other id21
   solutions are tested.

   the algorithm by wang [[291]121], referred to as the id20
   manifold alignment (dama) algorithm, proposes using a manifold
   alignment [[292]45] process to perform a symmetric transformation of
   the domain input spaces. in this solution, there are multiple labeled
   source domains and a limited labeled target domain for a total of k
   domains where all k domains share the same output label space. the
   approach is to create a separate mapping function for each domain to
   transform the heterogeneous input space to a common latent input space
   while preserving the underlying structure of each domain. each domain
   is modeled as a manifold. to create the latent input space, a larger
   matrix model is created that represents and captures the joint manifold
   union of all input domains. in this manifold model, each domain is
   represented by a laplacian matrix that captures the closeness to other
   instances sharing the same label. the instances with the same labels
   are forced to be neighbors while separating the instances with
   different labels. a id84 step is performed through
   a generalized eigenvalue decomposition process to eliminate feature
   redundancy. the final learner is built in two stages. the first stage
   is a id75 model trained on the source data using the
   latent feature space. the second stage is also a id75
   model that is summed with the first stage. the second stage uses a
   manifold id173 [[293]4] process to ensure the prediction error
   is minimized when using the labeled target data. the first stage is
   trained only using the source data and the second stage compensates for
   the domain differences caused by the first stage to achieve enhanced
   target predictions. the experiments are focused on the application of
   document text classification where classification accuracy is measured
   as the performance metric. the methods tested against include a
   canonical correlation analysis approach and a manifold id173
   approach, which is considered the baseline method. the baseline method
   uses the limited labeled target domain data and does not use source
   domain information. the approach presented in this paper substantially
   outperforms the canonical correlation analysis and baseline approach;
   however, these approaches are not directly referenced so it is
   difficult to understand the significance of the test results. a unique
   aspect of this paper is the modeling of multiple source domains in a
   heterogeneous solution.

   there are scenarios where a large amount of unlabeled heterogeneous
   source data is readily available that could be used to improve the
   predictive performance of a particular target learner. the paper by zhu
   [[294]146], which presents the method called the heterogeneous transfer
   learning image classification (htlic), addresses this scenario with the
   assumption of having access to a sufficiently large amount of labeled
   target data. the objective is to use the large supply of available
   unlabeled source data to create a common latent feature input space
   that will improve prediction performance in the target classifier. the
   solution proposed by zhu [[295]146] is tightly coupled to the
   application of image classification and is described as follows. images
   with labeled categories (e.g. dog, cake, starfish, etc.) are available
   in the target domain. to obtain the source data, a web search is
   performed from flickr for images that    relate    to the labeled
   categories. for example, for the category of dog, the words dog, doggy,
   and greyhound may be used in the flickr search. as a reference point,
   the idea of using annotated images from flickr as unlabeled source data
   was first proposed by yang [[296]137]. the retrieved images from flickr
   have one or more word tags associated with each image. these tagged
   image words are then used to search for text documents using google
   search. next, a two-layer bipartite graph is constructed where the
   first layer represents linkages between the source images and the image
   tags. the second layer represents linkages between the image tags and
   the text documents. if an image tag appears in a text document, then a
   link is created, otherwise there is no link. images in both the source
   and the target are initially represented by an input feature set that
   is derived from the pixel information using sift descriptors [[297]70].
   using the initial source image features and the bipartite graph
   representation derived only from the source image tags and text data, a
   common latent semantic feature set is learned by employing latent
   semantic analysis [[298]24]. a learner is now trained with the
   transformed labeled target instances. experiments are performed on the
   proposed approach where 19 different image categories are selected.
   binary classification is performed testing different image category
   pairs. a baseline method is tested using an id166 classifier trained only
   with the labeled target data. methods by raina [[299]95] and by wang
   [[300]122] are also tested. the approach proposed by zhu [[301]146]
   performed the best overall followed by raina [[302]95], wang
   [[303]122], and baseline approach. the idea of using an abundant source
   of unlabeled data available through an internet search to improve
   prediction performance is a very alluring premise. however, this method
   is very specific to image classification and is enabled by having a web
   site like flickr, which essentially provides unlimited labeled image
   data. this method is difficult to port to other applications.

   the id21 solution proposed by qi [[304]92] is another
   example of an approach that specifically addresses the application of
   image classification. in the paper by qi [[305]92], the author claims
   the application of image classification is inherently more difficult
   than text classification because image features are not directly
   related to semantic concepts inherent in class labels. image features
   are derived from pixel information, which is not semantically related
   to class labels, as opposed to word features that have semantic
   interpretability to class labels. further, labeled image data is more
   scarce as compared to labeled text data. therefore, a id21
   environment for image classification is desired where an abundance of
   labeled text data (source) is used to enhance a learner trained on
   limited labeled image data (target). in this solution, text documents
   are identified by performing a web search (from wikipedia for example)
   on class labels. in order to perform the knowledge transfer from the
   text documents (source) to the image (target) domain, a bridge in the
   form of a co-occurrence matrix is used that relates the text and image
   information. the co-occurrence matrix contains text instances with the
   corresponding image instances that are found in that particular text
   document. the co-occurrence matrix can be programmatically built by
   crawling web pages and extracting the relevant text and image feature
   information. using the co-occurrence matrix, a common latent feature
   space is found between the text and image feature, which is used to
   learn the final target classifier. this approach, called the text to
   image (tti) method, is similar to zhu [[306]146]. however, zhu
   [[307]146] does not use labeled source data to enhance the knowledge
   transfer, which will result in degraded performance when there is
   limited labeled target data. experiments are performed with the methods
   proposed by qi [[308]92], dai [[309]20], zhu [[310]146], and a baseline
   approach using a standard id166 classifier trained on the limited labeled
   target data. the text documents are collected from wikipedia, and
   classification error rate is measured as the performance metric. the
   results show the zhu [[311]146] method performing the best in 15 % of
   the trials, the dai [[312]20] method being the best in 10 % of the
   trials, and the qi [[313]92] method leading in 75 % of the trials. as
   with the case of zhu [[314]146], this method is very specific to the
   application of image classification and is difficult to port to other
   applications.

   the scenario addressed in the paper by duan [[315]30] is focused on
   heterogeneous id20 with a single labeled source domain and
   a target domain with limited labeled samples. the solution proposed is
   called heterogeneous feature augmentation (hfa). a transformation
   matrix p is defined for the source and a transformation matrix q is
   defined for the target to project the feature spaces to a common latent
   space. the latent feature space is augmented with the original source
   and target feature set and zeros where appropriate. this means the
   source input data projection has the common latent features, the
   original source features, and zeros for the original target features.
   the target input data projection has the common latent features, zeros
   for the original source features, and the original target features.
   this feature augmentation method was first introduced by daum  
   [[316]22] and is used to correct for conditional distribution
   differences between the domains. for computational simplification, the
   p and q matrices are not directly found but combined and represented by
   an h matrix. an optimization problem is defined by minimizing the
   structural risk functional [[317]117] of id166 as a function of the h
   matrix. the final target prediction function is found using an
   alternating optimization algorithm to simultaneously solve the dual
   problem of id166 and the optimal transformation h matrix. the experiments
   are performed for the applications of image classification and text
   classification. the source contains labeled image data and the target
   contains limited labeled image data. for the image features, surf
   [[318]3] features are extracted from the pixel information and then
   clustered into different dimension feature spaces creating the
   heterogeneous source and target environment. for the text
   classification experiments, the target contains spanish language
   documents and the source contains documents in four different
   languages. the experiments test against a baseline method, which is
   constructed by training an id166 learner on the limited labeled target
   data. other heterogeneous adaptation methods that are tested include
   the method by wang [[319]121], shi [[320]105], and kulis [[321]58]. for
   the image classification test, the hfa method outperforms all the
   methods tested by an average of one standard deviation with respect to
   classification accuracy. the kulis [[322]58] method has comparable
   results to the baseline method (possibly due to some uniqueness in the
   data set) and the wang [[323]121] method slightly outperforms the
   baseline method (possibly due to a weak manifold structure in the data
   set). for the text classification test, the hfa method outperforms all
   methods tested by an average of 1.5 standard deviation. for this test,
   the kulis [[324]58] method is second in performance, followed by wang
   [[325]121], and then the baseline method. the shi [[326]105] method
   performed worse than the baseline method in both tests. a possible
   reason for this result is the shi [[327]105] method does not
   specifically use the labeled information from the target when
   performing the symmetric transformation, which will result in degraded
   classification performance [[328]64].
   the work of li [[329]64], called the semi-supervised heterogeneous
   feature augmentation (shfa) approach, addresses the heterogeneous
   scenario of an abundance of labeled source data and limited target
   data, and directly extends the work of duan [[330]30]. in this work,
   the h transformation matrix, which is described above by duan
   [[331]30], is decomposed into a linear combination of a set of rank-one
   positive semi-definite matrices that allow for multiple kernel learning
   solvers (defined by kloft [[332]57]) to be used to find a solution. in
   the process of learning the h transformation matrix, the labels for the
   unlabeled target data are estimated (pseudo labels created) and used
   while learning the final target classifier. the pseudo labels for the
   unlabeled target data are found from an id166 classifier trained on the
   limited labeled target data. the high-level id20 is shown
   in fig. [333]3. experiments are performed for three applications which
   include image classification (where 31 unique classes are defined),
   multi-language text document classification (where six unique classes
   are defined), and multi-language text sentiment classification.
   classification accuracy is measured as the performance metric. the
   method by li [[334]64] is tested against a baseline method using an id166
   learner and trained on the limited labeled target data. further, other
   heterogeneous methods tested include duan [[335]30], shi [[336]105],
   wang [[337]121], and kulis [[338]58]. by averaging the three different
   application test results, the order of performance from best to worst
   is li [[339]64], duan [[340]30], wang [[341]121], baseline and kulis
   [[342]58] (tie), and shi [[343]105].
   [344]open image in new window fig. 3
   fig. 3

   depicts algorithm approach by li [[345]64] where the heterogeneous
   source and target features are transformed to an augmented latent
   feature space. t[s] and t[t] are transformation functions. p and q are
   projection matrices as described in duan [[346]30]

   diagram adapted from li [[347]64]

asymmetric feature-based id21

   the work of kulis [[348]58], referred to as the asymmetric regularized
   cross-domain transformation (arc-t), proposes an asymmetric
   transformation algorithm to resolve the heterogeneous feature space
   between domains. for this scenario, there is an abundance of labeled
   source data and limited labeled target data. an objective function is
   first defined for learning the transformation matrix. the objective
   function contains a regularizer term and a cost function term that is
   applied to each pair of cross-domain instances and the learned
   transformation matrix. the construction of the objective function is
   responsible for the domain invariant transformation process. the
   optimization of the objective function aims to minimize the regularizer
   and the cost function terms. the transformation matrix is learned in a
   non-linear gaussian rbf kernel space. the method presented is referred
   to as the asymmetric regularized cross-domain transformation. two
   experiments using this approach are performed for image classification
   where classification accuracy is measured as the performance metric.
   there are 31 image classes defined for these experiments. the first
   experiment (test 1) is where instances of all 31 image classes are
   included in the source and target training data. in the second
   experiment (test 2), only 16 image classes are represented in the
   target training data (all 31 are represented in the source). to test
   against other baseline approaches, a method is needed to bring the
   source and target input domains together. a preprocessing step called
   kernel canonical correlation analysis (proposed by shawe-taylor
   [[349]104]) is used to project the source and target domains into a
   common domain space using symmetric transformation. baseline approaches
   tested include k-nearest neighbors, id166, metric learning proposed by
   davis [[350]23], feature augmentation proposed by daum   [[351]22], and
   a cross domain metric learning method proposed by saenko [[352]100].
   for test 1, the kulis [[353]58] approach performs marginally better
   than the other methods tested. for test 2, the kulis [[354]58] approach
   performs significantly better compared to the k-nearest neighbors
   approach (note the other methods cannot be tested against as they
   require all 31 classes to be represented in the target training data).
   the kulis [[355]58] approach is best suited for scenarios where all of
   the classes are not represented in the target training data as
   demonstrated in test 2.

   the problem domain defined by harel [[356]46] is of limited labeled
   target data and multiple labeled data sources where an asymmetric
   transformation is desired for each source to resolve the mismatch in
   feature space. the first step in the process is to normalize the
   features in the source and target domains, then group the instances by
   class in the source and target domains. for each class grouping, the
   features are mean adjusted to zero. next, each individual source class
   group is paired with the corresponding target class group, and a
   singular value decomposition process is performed to find the specific
   transformation matrix for that class grouping. once the transformation
   is performed, the features are mean shifted back reversing the previous
   step, and the final target classifier is trained using the transformed
   data. finding the transformation matrix using the singular value
   decomposition process allows for the marginal distributions within the
   class groupings to be aligned while maintaining the structure of the
   data. this approach is referred to as the multiple outlook mapping
   algorithm (momap). the experiments use data taken from wearable sensors
   for the application of activity classification. there are five
   different activities defined for the experiment which include walking,
   running, going upstairs, going downstairs, and lingering. the source
   domain contains similar (but different) sensor readings as compared to
   the target. the method proposed by harel [[357]46] is compared against
   a baseline method that trains a classifier with the limited labeled
   target data and an upper bound method that uses a significantly larger
   set of labeled target data to train a classifier. an id166 learner is
   used as the base classifier and a balanced error rate (due to an
   imbalance in the test data) is measured as the performance metric. the
   harel [[358]46] approach outperforms the baseline method in every test
   and falls short of the upper bound method in every test with respect to
   the balanced error rate.

   the heterogeneous id21 scenario addressed by zhou
   [[359]145] requires an abundance of labeled source data and limited
   labeled target data. an asymmetric transformation function is proposed
   to map the source features to the target features. to learn the
   transformation matrix, a id72 method based on ando
   [[360]2] is adopted. the solution, referred to as the sparse
   heterogeneous feature representation (shfr), is implemented by creating
   a binary classifier for each class in the source and the target domains
   separately. each binary classifier is assigned a weight term where the
   weight terms are learned by combining the weighted classifier outputs,
   while minimizing the classification error of each domain. the weight
   terms are now used to find the transformation matrix by minimizing the
   difference between the target weights and the transformed source
   weights. the final target classifier is trained using the transformed
   source data and original target data. experiments are performed for
   text document classification where the target domain contains documents
   written in one language and the source domain contains documents
   written in different languages. a baseline method using a linear id166
   classifier trained on the labeled target is established along with
   testing against the methods proposed by wang [[361]121], kulis
   [[362]58], and duan [[363]30]. the method proposed by zhou [[364]145]
   performed the best for all tests with respect to classification
   accuracy. the results of the other approaches are mixed as a function
   of the data sets used where the duan [[365]30] method performed either
   second or third best.

   the application of software module defect prediction is usually
   addressed by training a classifier with labeled data taken from the
   software project of interest. the environment described in nam
   [[366]77] for software module defect prediction attempts to use labeled
   source data from one software project to train a classifier to predict
   unlabeled target data from another project. the source and target
   software projects collect different metrics making the source and
   target feature spaces heterogeneous. the proposed solution, referred to
   as the heterogeneous defect prediction (hdp) approach, is to first
   select the important features from the source domain using a feature
   selection method to eliminate redundant and irrelevant features.
   feature selection methods used include gain ratio, chi square,
   relief-f, and significance attribute evaluation (see gao [[367]39] and
   shivaji [[368]108]). the next step is to statistically match the
   selected source domain features to ones in the target using a
   kolmogorov   smirnov test that measures the closeness of the empirical
   distribution between the two sources. a learner is trained with the
   source features that exhibit a close statistical match to the
   corresponding target features. the target data is tested with the
   trained classifier using the corresponding matched features of the
   target. even though the approach by nam [[369]77] is applied directly
   to the application of software module defect prediction, this method
   can be used for other applications. experiments are performed using
   five different software defect data sets with heterogeneous features.
   the proposed method by nam [[370]77] uses id28 as the
   base learner. the other approaches tested include a within project
   defect prediction (wpdp) approach where the learner is trained on
   labeled target data, a cross project defect prediction (cpdp-cm)
   approach where the source and target represent different software
   projects but have homogeneous features, and a cross project defect
   prediction approach with heterogeneous features (cpdp-ifs) as proposed
   by he [[371]47]. the results of the experiment show the nam [[372]77]
   method significantly outperformed all other approaches with respect to
   area under the curve measurement. the wpdp approach is next best
   followed by the cpdp-cm approach and the cpdp-ifs approach. these
   results can be misleading as the nam [[373]77] approach could only
   match at least one or more input features between the source and target
   domains in 37 % of the tests. therefore, in 63 % of the cases, the nam
   [[374]77] method could not be used and these cases are not counted. the
   wpdp method represents an upper bound and it is an unexpected result
   that the nam [[375]77] approach would outperform the wpdp method.

   the paper by zhou [[376]144] claims that previous heterogeneous
   solutions assume the instance correspondence between the source and
   target domains are statistically representative (distributions are
   equal), which may not always be the case. an example of this claim is
   in the application of text sentiment classification where the word bias
   problem previously discussed causes distribution differences between
   the source and target domains. the paper by zhou [[377]144] proposes a
   solution called the hybrid heterogeneous id21 (hhtl)
   method for a heterogeneous environment with abundant labeled source
   data and abundant unlabeled target data. the idea is to first learn an
   asymmetric transformation from the target to the source domain, which
   reduces the problem to a homogeneous id20 issue. the next
   step is to discover a common latent feature space using the transformed
   data (from the previous step) to reduce the distribution bias between
   the transformed unlabeled target domain and the labeled source domain.
   finally, a classifier is trained using the common latent feature space
   from the labeled source data. this solution is realized using a deep
   learning method employing a marginalized stacked denoised autoencoder
   as proposed by chen [[378]16] to learn the asymmetric transformation
   and the mapping to a common latent feature space. the previous surveyed
   paper by glorot [[379]41] demonstrated a deep learning approach finding
   a common latent feature space for homogeneous source and target feature
   set. the experiments focused on multiple language text sentiment
   classification where english is used in the source and three other
   languages are separately used in the target. classification accuracy is
   measured as the performance metric. other methods tested include a
   heterogeneous spectral mapping approach proposed by shi [[380]105], a
   method proposed by vinokourov [[381]120], and a multimodal deep
   learning approach proposed by ngiam [[382]79]. an id166 learner is used
   as the base classifier for all methods. the results of the experiment
   from best to worst performance are zhou [[383]144], ngiam [[384]79],
   vinokourov [[385]120], and shi [[386]105].

improvements to heterogeneous solutions

   the paper by yang [[387]136] proposes to quantify the amount of
   knowledge that can be transferred between domains in a heterogeneous
   id21 environment. in other words, it attempts to measure
   the    relatedness    of the domains. this is accomplished by first
   building a co-occurrence matrix for each domain. the co-occurrence
   matrix contains the set of instances represented in every domain. for
   example, if one particular text document is an instance in the
   co-occurrence matrix, that text document is required to be represented
   in every domain. next, principal component analysis is used to select
   the most important features in each domain and assign the principal
   component coefficient to those features. the principal component
   coefficients are used to form a directed cyclic network (dcn) where
   each node represents a domain (either source or target) and each node
   connection (edge weight) is the conditional dependence from one domain
   to another. the dcn is built using a id115 method.
   the edge weights represent the potential amount of knowledge that can
   be transferred between domains where a higher value means higher
   knowledge transfer. these edge weights are then used as tuning
   parameters in different heterogeneous id21 solutions,
   which include works from yang [[388]137], ng [[389]78], and zhu
   [[390]146] (the weights are calculated first using yang [[391]136] and
   then applied as tuning values in the other solutions). note, that
   integrating the edge weight values into a particular approach is
   specific to the implementation of the solution and cannot be
   generically applied. the experiments are run on the three different
   learning solutions comparing the original solution against the solution
   using the weighted edges of the dcn as the tuned parameters. in all
   three solutions, the classification accuracy is improved using the dcn
   tuned parameters. one potential issue with this approach is the
   construction of the co-occurrence matrix. the co-occurrence matrix
   contains many instances; however, each instance must be represented in
   each domain. this may be an unrealistic constraint in many real-world
   applications.

experiment results

   in reviewing the experiment results of the previous surveyed papers,
   there are instances where one solution can show varying results over a
   range of different experiments. there are many reasons why this can
   happen which include varying test environments, different test
   implementations, different applications being tested, and different
   data sets being used. an interesting area of future work is to evaluate
   the solutions presented to determine the best performing solutions as a
   function of specific datasets. to facilitate that goal, a repository of
   open-source software containing the software implementations for
   solutions used in each paper would be extremely beneficial.
   table [392]3 lists a compilation of head-to-head results for the most
   commonly tested solutions contained in the    [393]heterogeneous transfer
   learning    section. the results listed in table [394]3 represent a win,
   loss, and tie performance record of the head-to-head solution
   comparisons. note, these results are compiled directly from the
   surveyed papers. it is difficult to draw exact conclusions from this
   information because of the reasons just outlined; however, it provides
   some interesting insight into the comparative performances of the
   solutions.
   table 3

   lists the head-to-head results of experiments performed in the
   heterogeneous id21 works surveyed

   methods

   hemap

   arc-t

   dama

   hfa

   shfr

   shfa

   hemap [[395]105]

      

   0   5   0

   0   5   0

   0   5   0

   0   0   0

   0   3   0

   arc-t [[396]58]

   5   0   0

      

   4   2   0

   1   7   0

   0   3   0

   0   3   0

   dama [[397]121]

   5   0   0

   2   4   0

      

   0   8   0

   0   3   0

   0   3   0

   hfa [[398]30]

   5   0   0

   7   1   0

   8   0   0

      

   0   3   0

   0   3   0

   shfr [[399]145]

   0   0   0

   3   0   0

   3   0   0

   3   0   0

      

   0   0   0

   shfa [[400]64]

   3   0   0

   3   0   0

   3   0   0

   3   0   0

   0   0   0

      

   the numbers (x   y   z) in the table indicate the far left column method
   outperforms the top row method x times, underperforms y times, and has
   similar performance z times

discussion of heterogeneous solutions

   the previous surveyed heterogeneous id21 works demonstrate
   many different characteristics and attributes. which heterogeneous
   id21 solution is best for a particular application? the
   heterogeneous id21 solutions use either a symmetric
   transformation or an asymmetric transformation process in an attempt to
   resolve the differences between the input feature space (as shown in
   fig. [401]1). the asymmetrical transformation approach is best used
   when the same class instances in the source and target domains can be
   transformed without context feature bias. many of the surveyed
   heterogeneous id21 solutions only address the issue of the
   input feature space being different between the source and target
   domains and do not address other id20 steps needed for
   marginal and/or conditional distribution differences. if further domain
   adaptation needs to be performed after the input feature spaces are
   aligned, then an appropriate homogeneous solution should be used. to
   further help determine which solution is best for a given transfer
   learning application, the information in table [402]4 should be used to
   match the characteristics of the solution to that of the desired
   application environment. none of the surveyed heterogeneous transfer
   learning solutions have a means to guard against negative transfer
   effects. however, the paper by yang [[403]136] demonstrates that
   negative transfer guards can benefit heterogeneous id21
   solutions. it seems likely that future heterogeneous id21
   works will integrate means for negative transfer protection. many of
   the same heterogeneous id21 solutions are tested in the
   surveyed solution experiments. these head-to-head comparisons are
   summarized in table [404]3 and can be used as a starting point to
   understand the relative performance between the solutions. as observed
   as a trend in the previous homogeneous solutions, the recent
   heterogeneous solution by duan [[405]30] employs a one-stage solution
   that simultaneously performs the feature input space alignment process
   while learning the final classifier. as is the case for the surveyed
   homogeneous id21 works, the surveyed heterogeneous
   id21 works are not specifically applied to big data
   solutions; however, there is nothing to preclude their use in a big
   data environment.
   table 4

   heterogeneous id21 approaches surveyed in
      [406]heterogeneous id21    section listing various
   characteristics of each approach

   approach

   transfer category

   source

   data

   target data

   multiple sources

   generic solution

   negative transfer

   clscl [[407]91]

   symmetric feature

   labeled

   unlabeled

   hemap [[408]105]

   symmetric feature

   labeled

   limited labels


      

   dama [[409]121]

   symmetric feature

   labeled

   limited labels

      

      

   htlic [[410]146]

   symmetric feature

   unlabeled

   abundant labels

   tti [[411]92]

   symmetric feature

   labeled

   limited labels

   hfa [[412]30]

   symmetric feature

   labeled

   limited labels


      

   shfa [[413]64]

   symmetric feature

   labeled

   limited labels


      

   arc-t [[414]58]

   asymmetric feature

   labeled

   limited labels


      

   momap [[415]46]

   asymmetric feature

   labeled

   limited labels

      

   shfr [[416]145]

   asymmetric feature

   labeled

   limited labels


      

   hdp [[417]77]

   asymmetric feature

   labeled

   unlabeled


      

   hhtl [[418]144]

   asymmetric feature

   labeled

   unlabeled


      

negative transfer

   the high-level concept of id21 is to improve a target
   learner by using data from a related source domain. but what happens if
   the source domain is not well-related to the target? in this case, the
   target learner can be negatively impacted by this weak relation, which
   is referred to as negative transfer. in a big data environment, there
   may be a large dataset where only a portion of the data is related to a
   target domain of interest. for this case, there is a need to divide the
   dataset into multiple sources and employ negative transfer methods when
   using id21 algorithm. in the scenario where multiple
   datasets are available that initially appear to be related to the
   target domain of interest, it is desired to select the datasets that
   provide the best information transfer and avoid the datasets that cause
   negative transfer. this allows for the best use of the available large
   datasets. how related do the source and target domains need to be for
   id21 to be advantageous? the area of negative transfer has
   not been widely researched, but the following papers begin to address
   this issue.

   an early paper by rosenstein [[419]98] discusses the concept of
   negative transfer in id21 and claims that the source
   domain needs to be sufficiently related to the target domain;
   otherwise, the attempt to transfer knowledge from the source can have a
   negative impact on the target learner. cases of negative transfer are
   demonstrated by rosenstein [[420]98] in experiments using a
   hierarchical naive bayes classifier. the author also demonstrates the
   chance of negative transfer goes down as the number of labeled target
   training samples goes up.

   the paper by eaton [[421]31] proposes to build a target learner based
   on a transferability measure from multiple related source domains. the
   approach first builds a id28 learner for each source
   domain. next, a model transfer graph is constructed to represent the
   transferability between each source learner. in this case,
   transferability from a first learner to a second learner is defined as
   the performance of the second learner with learning from the first
   learner minus the performance of the second learner without learning
   from the first learner. next, the model transfer graph is modified by
   adding the transferability measures between the target learner and all
   the source learners. using spectral id207 [[422]17] on the model
   transfer graph, a transfer function is derived that maintains the
   geometry of the model transfer graph and is used in the final target
   learner to determine the level of transfer from each source.
   experiments are performed in the applications of document
   classification and alphabet classification. source domains are
   identified that are either related or unrelated to the target domain.
   the method by eaton [[423]31] is tested along with a handpicked method
   where the source domains are manually selected to be related to the
   target, an average method that uses all sources available, and a
   baseline method that does not use id21. classification
   accuracy is the performance metric measured in the experiments. the
   source and target domains are represented by a homogeneous feature
   input space. the results of the experiments are mixed. overall, the
   eaton [[424]31] approach performs the best; however, there are certain
   instances where eaton [[425]31] performed worse than the handpicked,
   average, and baseline methods. in the implementation of the algorithm,
   the transferability measure between two sources is required to be the
   same; however, the transferability from source 1 to source 2 is not
   always equal to the transferability from source 2 to source 1. a
   suggestion for future improvement is to use directed graphs to specify
   the bidirectional nature of the transferability measure between two
   sources.

   the paper by ge [[426]40] claims that knowledge transfer can be
   inhibited due to the existence of unrelated or irrelevant source
   domains. further, current id21 solutions are focused on
   transferring knowledge from source domains to a target domain, but are
   not concerned about different source domains that could potentially be
   irrelevant and cause negative transfer. in the model presented by ge
   [[427]40], there is a single target domain with limited labeled data
   and multiple labeled source domains for knowledge transfer. to reduce
   negative transfer effects from unrelated source domains, each source is
   assigned a weight (called the supervised local weight) corresponding to
   how related the source is with the target (the higher the weight the
   more it is related). the supervised local weight is found by first
   using a spectral id91 algorithm (chung [[428]17]) on the
   unlabeled target information and propagating labels to the clusters
   from the labeled target information. next, each source is separately
   clustered and labels assigned to the clusters from the labeled source.
   the supervised local weight of each source cluster is computed by
   comparing the source and target clusters. this solution further
   addresses the issue of imbalanced class distribution in source domains
   by preventing a high-weight class assignment in the case of
   high-accuracy predictions in a minority target class. the final target
   learner uses the supervised local weights to attenuate the effects of
   negative transfer. experiments are performed in three application areas
   including cardiac arrhythmia detection, spam email filtering, and
   intrusion detection. area under the curve is measured as the
   performance metric. the source and target domains are represented by a
   homogeneous feature input space. the method presented in this paper is
   compared against methods by luo [[429]71], by gao [[430]38], by
   chattopadhyay [[431]14], and by gao [[432]37]. the luo [[433]71] and
   gao [[434]38] methods are the worst performing, most likely due to the
   fact that these solutions do not attempt to combat negative transfer
   effects. the chattopadhyay [[435]14] and gao [[436]37] methods are the
   next best performing, which have means in place to reduce the effects
   of negative transfer from the source domains. the chattopadhyay
   [[437]14] and gao [[438]37] methods do address the negative transfer
   problem but do not address the imbalanced distribution issue. the ge
   [[439]40] method does exhibit the best overall performance due to the
   handling of negative transfer and imbalanced class distribution.

   the paper by seah [[440]102] claims the root cause of negative transfer
   is mainly due to conditional distribution differences between source
   domains \(\left( {{\text{p}}_{{{\text{s}}1}} ({\text{y}}\left|
   {\text{x}} \right.)\,\, \ne \,{\text{p}}_{{{\text{s}}2}} \left(
   {{\text{y}}\left| {\text{x}} \right.} \right)} \right)\) and a
   difference in class distribution (class imbalance) between the source
   and target \(\left( {{\text{p}}_{\text{s}} ({\text{y}})\,\, \ne
   \,{\text{p}}_{\text{t}} \left( {\text{y}} \right)} \right)\). because
   the target domain usually contains a small number of labeled instances,
   it is difficult to find the true class distribution of the target
   domain. a predictive distribution matching (pdm) framework is proposed
   to align the conditional distributions of the source domains and target
   domain in an attempt to minimize negative transfer effects. a positive
   transferability measure is defined that measures the transferability of
   instance pairs with the same label from the source and target domains.
   the first step in the pdm framework is to assign pseudo labels to the
   unlabeled target data. this is accomplished by an iterative process
   that forces source and target instances which are similar (as defined
   by the positive transferability measure) to have the same label. next,
   irrelevant source data are removed by identifying data that does not
   align with the conditional distribution of the pseudo labeled target
   data for each class. both id28 and id166 classifiers are
   implemented using the pdm framework. experiments are performed on
   document classification using the pdm method described in this paper,
   the approach from daum   [[441]22], the approach from huang [[442]51],
   and the approach from bruzzone [[443]11]. classification accuracy is
   measured as the performance metric. the source and target domains are
   represented by a homogeneous feature input space. the pdm approach
   demonstrates better performance as compared to the other approaches
   tested as these solutions do not attempt to account for negative
   transfer effects.

   a select number of previously surveyed papers contain solutions
   addressing negative transfer. the paper by yang [[444]136] addresses
   the negative transfer issue, which is presented in the
      [445]heterogeneous id21    section. the homogeneous
   solution by gong [[446]42] defines an rod value that measures the
   relatedness between a source and target domain. the work presented in
   chattopadhyay [[447]14] is a multiple source id21 approach
   that calculates the source weights as a function of conditional
   id203 differences between the source and target domains
   attempting to give the most related sources the highest weights. duan
   [[448]28] proposes a id21 approach that only uses source
   domains that are deemed relevant and test data demonstrates better
   performance compared to methods with no negative transfer protection.

   the previous papers attempt to measure how related source data is to
   the target data in a id21 environment and then selectively
   transfer the information that is highly related. the experiments in the
   above papers demonstrate that accounting for negative transfer effects
   from source domain data can improve target learner performance.
   however, most id21 solutions do not attempt to account for
   negative transfer effects. robust negative transfer measurements are
   difficult to define. since the target domain typically has limited
   labeled data, it is inherently difficult to find a true measure of the
   relatedness between the source and target domains. further, by
   selectively transferring information that seems related to the limited
   labeled target domain, a risk of overfitting in the target learner is a
   concern. the topic of negative transfer is a fertile area for further
   research.

id21 applications

   the surveyed works in this paper demonstrate that id21 has
   been applied to many real-world applications. there are a number of
   application examples pertaining to natural language processing, more
   specifically in the areas of sentiment classification, text
   classification, spam email detection, and multiple language text
   classification. other well-represented id21 applications
   include image classification and video concept classification.
   applications that are more selectively addressed in the previous papers
   include wifi localization classification, muscle fatigue
   classification, drug efficacy classification, human activity
   classification, software defect classification, and cardiac arrhythmia
   classification.

   the majority of the solutions surveyed are generic, meaning the
   solution can be easily applied to applications other than the ones
   implemented and tested in the papers. the application-specific
   solutions tend to be related to the field of natural language
   processing and image processing. in the literature, there are a number
   of id21 solutions that are specific to the application of
   id126s. id126s provide users with
   recommendations or ratings for a particular domain (e.g. movies, books,
   etc.), which are based on historical information. however, when the
   system does not have sufficient historical information (referred to as
   the data sparsity issue presented in moreno [[449]76]), then the
   recommendations are not reliable. in the cases where the system does
   not have sufficient domain data to make reliable predictions (for
   example when a movie is just released), there is a need to use
   previously collected information from a different domain (using books
   for example). the aforementioned problem has been directly addressed
   using id21 methodologies and captured in papers by moreno
   [[450]76], cao [[451]12], li [[452]60], li [[453]61], pan [[454]86],
   zhang [[455]140], pan [[456]85], roy [[457]99], jiang [[458]54], and
   zhao [[459]142].

   id21 solutions continue to be applied to a diverse number
   of real-world applications, and in some cases the applications are
   quite obscure. the application of head pose classification finds a
   learner trained with previously captured labeled head positions to
   predict a new head position. head pose classification is used for
   determining the attentiveness of drivers, analyzing social behavior,
   and human interaction with robots. head positions captured in source
   training data will have different head tilt ranges and angles than that
   of the predicted target. the paper by rajagopal [[460]96] addresses the
   head pose classification issues using id21 solutions.

   other id21 applications include the paper by ma [[461]72]
   that uses id21 for atmospheric dust aerosol particle
   classification to enhance global climate models. here the tradaboost
   algorithm proposed by dai [[462]21] is used in conjunction with an id166
   classifier to improve on classification results. being able to identify
   areas of low income in developing countries is important for disaster
   relief efforts, food security, and achieving sustainable growth. to
   better predict poverty mapping, xie [[463]134] proposes an approach
   similar to oquab [[464]81] that uses a convolution neural network
   model. the first prediction model is trained to predict night time
   light intensity from source image data. the final target prediction
   model predicts the poverty mapping from source night time light
   intensity data. in the paper by ogoe [[465]80], id21 in
   used to enhance disease prediction. in this solution, a rule-based
   learning approach is formulated to use abstract source domain data to
   perform modeling of multiple types of gene expression data. online
   display web advertising is a growing industry where id21
   is used to optimally predict targeted ads. in the paper by perlich
   [[466]90], a id21 approach is employed that uses the
   weighted outputs of multiple source classifiers to enhance a target
   classifier trained to predict targeted online display advertising
   results. the paper by kan [[467]56] addresses the field of facial
   recognition and is able to use face image information from one ethnic
   group to improve the learning of a classifier for a different ethnic
   group. the paper by farhadi [[468]35] is focused on the application of
   sign language recognition where the model is able to learn from
   different people signing at various angles. id21 is
   applied to the field of biology in the paper by widmer [[469]127].
   specifically, a id72 approach is used in the prediction
   of splice sites in genome biology. predicting if patients will contract
   particular bacteria when admitted to a hospital is addressed in the
   paper by wiens [[470]128]. information taken from different hospitals
   is used to predict the infection rate for a different hospital. in the
   paper by romera-paredes [[471]97], a multi-task id21
   approach is used to predict pain levels from an individual   s facial
   expression by using labeled source facial images from other
   individuals. the paper by deng [[472]25] applies id21 to
   the application of speech emotion recognition where information is
   transferred from multiple labeled speech sources. the application of
   wine quality classification is implemented in zhang [[473]141] using a
   multi-task id21 approach. as a reference, the survey paper
   by cook [[474]19] covers id21 for the application of
   activity recognition and the survey papers by patel [[475]89] and shao
   [[476]103] address id21 in the domain of image
   recognition.

conclusion and discussion

   the subject of id21 is a well-researched area as evidenced
   with more than 700 academic papers addressing the topic in the last
   5 years. this survey paper presents solutions from the literature
   representing current trends in id21. homogeneous transfer
   learning papers are surveyed that demonstrate instance-based,
   feature-based, parameter-based, and relational-based information
   transfer techniques. solutions having various requirements for labeled
   and unlabeled data are also presented as a key attribute. the
   relatively new area of heterogeneous id21 is surveyed
   showing the two dominant approaches for id20 being
   asymmetric and symmetric transformations. many real-world applications
   that id21 is applied to are listed and discussed in this
   survey paper. in some cases, the proposed id21 solutions
   are very specific to the underlying application and cannot be
   generically used for other applications. a list of software downloads
   implementing a portion of the solutions surveyed is presented in the
   appendix of this paper. a great benefit to researchers is to have
   software available from previous solutions so experiments can be
   performed more efficiently and more reliably. a single open-source
   software repository for published id21 solutions would be
   a great asset to the research community.

   in many id21 solutions, the id20 process
   performed is focused either on correcting the marginal distribution
   differences or the conditional distribution differences between the
   source and target domains. correcting the conditional distribution
   differences is a challenging problem due to the lack of labeled target
   data. to address the lack of labeled target data, some solutions
   estimate the labels for the target data (called pseudo labels), which
   are then used to correct the conditional distribution differences. this
   method is problematic because the conditional distribution corrections
   are being made with the aid of pseudo labels. improved methods for
   correcting the conditional distribution differences is a potential area
   of future research. a number of more recent works attempt to correct
   both the marginal distribution differences and the conditional
   distribution differences during the id20 process. an area
   of future work is to quantify the advantage of correcting both
   distributions and in what scenarios it is most effective. further, long
   [[477]68] states that the simultaneous solving of marginal and
   conditional distribution differences is preferred over serial alignment
   as it reduces the risk of overfitting. another area of future work is
   to quantify any performance gains for simultaneously solving both
   distribution differences. in addition to solving for distribution
   differences in the id20 process, exploring possible data
   preprocessing steps using heuristic knowledge of the domain features
   can be used as a method to improve the target learner performance. the
   heuristic knowledge would represent a set of complex rules or relations
   that standard id21 techniques cannot account for. in most
   cases, this heuristic knowledge would be specific to each domain, which
   would not lead to a generic solution. however, if such a preprocessing
   step leads to improved target learner performance, it is likely worth
   the effort.

   a trend observed in the formulation of id21 solutions is
   in the implementation of a one-stage process as opposed to a two-stage
   process. a two-stage solution first performs the id20
   process and then independently learns the final classifier. a one-stage
   process simultaneously performs the id20 process while
   learning the final classifier. recent solutions employing a one-stage
   solution include long [[478]68], duan [[479]27], shi [[480]106], xia
   [[481]132], and duan [[482]30]. with respect to the one-stage solution,
   long [[483]68] claims the simultaneous solving of id20 and
   the classifier establishes mutual reinforcement for enhanced
   performance. an area of future work is to better quantify the effects
   of a one-stage approach over a two-stage approach.

   this paper surveys a number of works addressing the topic of negative
   transfer. the subject of negative transfer is still a lightly
   researched area. the expanded integration of negative transfer
   techniques into id21 solutions is a natural extension for
   future research. solutions supporting multiple source domains enabling
   the splitting of larger source domains into smaller domains to more
   easily discriminate against unrelated source data are a logical area
   for continued research. additionally, optimal transfer is another
   fertile area for future research. negative transfer is defined as a
   source domain having a negative impact on a target learner. the concept
   of optimal transfer is when select information from a source domain is
   transferred to achieve the highest possible performance in a target
   learner. there is overlap between the concepts of negative transfer and
   optimal transfer; however, optimal transfer attempts to find the best
   performing target learner, which goes well beyond the negative transfer
   concept.

   with the recent proliferation of sensors being deployed in cell phones,
   vehicles, buildings, roadways, and computers, larger and more diverse
   information is being collected. the diversity in data collection makes
   heterogeneous id21 solutions more important moving
   forward. larger data collection sizes highlight the potential for big
   data solutions being deployed concurrent with current id21
   solutions. how the diversity and large size of sensor data integrates
   into id21 solutions is an interesting topic of future
   research. another area of future work pertains to the scenario where
   the output label space is different between domains. with new data sets
   being captured and being made available, this topic could be a needed
   area of focus for the future. lastly, the literature has very few
   id21 solutions addressing the scenario of unlabeled source
   and unlabeled target data, which is certainly an area for expanded
   research.

notes

authors    contributions

   tmk introduced this topic to kw and provided technical guidance
   throughout the research and writing phases of the manuscript. kw
   performed the literature review, analysis, and writing of the
   manuscript. ddw provided a full review of the final manuscript. all
   authors read and approved the final manuscript.

competing interests

   the authors declare that they have no competing interests.

appendix

   the majority of id21 solutions surveyed are complex and
   implemented with non-trivial software. it is a great advantage for a
   researcher to have access to software implementations of transfer
   learning solutions so comparisons with competing solutions are
   facilitated more quickly and fairly. table [484]5 provides a list of
   available software downloads for a number of the solutions surveyed in
   this paper. table [485]6 provides a resource for useful links that
   point to id21 tutorials and other interesting articles on
   the topic of id21.
   table 5

   software downloads for various id21 solutions

   approach

   location

   prettenhofer [[486]91]

   [487]https://github.com/pprett/bolt [[488]7]

   zhu [[489]146]

   [490]http://www.cse.ust.hk/~yinz/ [[491]139]

   dai [[492]21]

   [493]https://github.com/bochen90/machine-learning-matlab/blob/master/tr
   adaboost.m [[494]6]

   daum   [[495]22]

   [496]http://hal3.name/easyadapt.pl.gz [[497]32]

   duan [[498]30]

   [499]https://sites.google.com/site/xyzliwen/publications/hfa_release_03
   15.rar [[500]49]

   kulis [[501]58]

   [502]http://vision.cs.uml.edu/adaptation.html [[503]18]

   qi [[504]92]

   [505]http://www.eecs.ucf.edu/~gqi/publications.html [[506]44]

   li [[507]64]

   [508]http://www.lxduan.info/#sourcecode_hfa [[509]67]

   gong [[510]42]

   [511]http://www-scf.usc.edu/~boqinggo/ [[512]9]

   long [[513]68]

   [514]http://ise.thss.tsinghua.edu.cn/~mlong/ [[515]75]

   oquab [[516]81]

   [517]http://leon.bottou.org/papers/oquab-2014 [[518]88]

   long [[519]69]

   [520]http://ise.thss.tsinghua.edu.cn/~mlong/ [[521]75]

   other id21 code

   [522]http://www.cse.ust.hk/tl/ [[523]115]
   table 6

   useful links for id21 information

   item

   location

   slides for nam [[524]77]

   [525]http://www.slideshare.net/hunkim/heterogeneous-defect-prediction-e
   secfse-2015 [[526]48]

   code for id166lib

   [527]http://www.csie.ntu.edu.tw/~cjlin/libid166 [[528]65]

   slide for kulis [[529]58]

   [530]https://www.eecs.berkeley.edu/~jhoffman/domainadapt/ [[531]26]

   tutorial on id21

   [532]http://tommasit.wix.com/datl14tutorial [[533]116]

   tutorial on id21

   [534]http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/da_surve
   y.html [[535]1]

   overview of duan [[536]28]

   [537]http://lxduan.info/papers/duancvpr2012_poster.pdf [[538]34]

references

    1. 1.
       a literature survey on id20 of statistical
       classifiers.
       [539]http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/da_s
       urvey.html. accessed 4 mar 2016.
    2. 2.
       ando rk, zhang t. a framework for learning predictive structures
       from multiple tasks and unlabeled data. j mach learn res.
       2005;6:1817   53.[540]mathscinet[541]zbmath[542]google scholar
    3. 3.
       bay h, tuytelaars t, gool lv. surf: speeded up robust features.
       comput vis image underst.
       2006;110(3):346   59.[543]crossref[544]google scholar
    4. 4.
       belkin m, niyogi p, sindhwani v. manifold id173: a
       geometric framework for learning from examples. j mach learn res
       arch. 2006;7:2399   434.[545]mathscinet[546]zbmath[547]google scholar
    5. 5.
       blitzer, j, mcdonald r, pereira f. id20 with
       structural correspondence learning. in: proceedings of the 2006
       conference on empirical methods in natural language processing.
       2006;120   8.[548]google scholar
    6. 6.
       bochen90 update tradaboost.m.
       [549]https://github.com/bochen90/machine-learning-matlab/blob/maste
       r/tradaboost.m. accessed 4 mar 2016.
    7. 7.
       bolt online learning toolbox. [550]http://pprett.github.com/bolt/.
       accessed 4 mar 2016.
    8. 8.
       bonilla e, chai km, williams c. multi-task gaussian process
       prediction. in: proceedings of the 20th annual conference of neural
       information processing systems. 2008. 153   60.[551]google scholar
    9. 9.
       gong b. [552]http://www-scf.usc.edu/~boqinggo/. accessed 4 mar
       2016.
   10. 10.
       borgwardt km, gretton a, rasch mj, kriegel hp, sch  lkopf b, smola
       aj. integrating structured biological data by kernel maximum mean
       discrepancy. bioinformatics.
       2006;22(4):49   57.[553]crossref[554]google scholar
   11. 11.
       bruzzone l, marconcini m. id20 problems: a daid166
       classification technique and a circular validation strategy. ieee
       trans pattern anal mach intell.
       2010;32(5):770   87.[555]crossref[556]google scholar
   12. 12.
       cao b, liu n, yang q. id21 for collective link
       prediction in multiple heterogeneous domains. in: proceedings of
       the 27th international conference on machine learning. 2010. p.
       159   66.[557]google scholar
   13. 13.
       cawley g. leave-one-out cross-validation based model selection
       criteria for weighted ls-id166s. in: ieee 2006 international joint
       conference on neural network proceedings 2006. p.
       1661   68.[558]google scholar
   14. 14.
       chattopadhyay r, ye j, panchanathan s, fan w, davidson i.
       multi-source id20 and its application to early
       detection of fatigue. acm trans knowl dis data (best of sigkdd 2011
       tkdd homepage archive) 2011; 6(4) (article 18).[559]google scholar
   15. 15.
       chelba c, acero a. adaptation of maximum id178 classifier: little
       data can help a lot. comput speech lang.
       2004;20(4):382   99.[560]crossref[561]google scholar
   16. 16.
       chen m, xu ze, weinberger kq, sha f (2012) marginalized denoising
       autoencoders for id20. icml. arxiv
       preprintarxiv:1206.4683.[562]google scholar
   17. 17.
       chung frk. spectral id207. in: cbms regional conference
       series in mathematics, no. 92. providence: american mathematical
       society; 1994.[563]google scholar
   18. 18.
       id161 and learning group.
       [564]http://vision.cs.uml.edu/adaptation.html. accessed 4 mar 2016.
   19. 19.
       cook dj, feuz kd, krishnan nc. id21 for activity
       recognition: a survey. knowl inf syst.
       2012;36(3):537   56.[565]crossref[566]google scholar
   20. 20.
       dai w, chen y, xue gr, yang q, yu y. translated learning: transfer
       learning across different feature spaces. adv neural inform process
       syst. 2008;21:353   60.[567]google scholar
   21. 21.
       dai w, yang q, xue gr, yu y (2007) boosting for id21.
       in: proceedings of the 24th international conference on machine
       learning. p. 193   200.[568]google scholar
   22. 22.
       daum   h iii. frustratingly easy id20. in: proceedings
       of acl. 2007. p. 256   63.[569]google scholar
   23. 23.
       davis j, kulis b, jain p, sra s, dhillon i. information theoretic
       metric learning. in: proceedings of the 24th international
       conference on machine learning. 2007. p. 209   16.[570]google scholar
   24. 24.
       deerwester s, dumais st, furnas gw, landauer tk, harshman r.
       indexing by latent semantic analysis. j am soc inf sci.
       1990;41:391   407.[571]crossref[572]google scholar
   25. 25.
       deng j, zhang z, marchi e, schuller b. sparse autoencoder based
       feature id21 for speech emotion recognition. in:
       humaine association conference on affective computing and
       intelligent interaction. 2013. p. 511   6.[573]google scholar
   26. 26.
       id20 project.
       [574]https://www.eecs.berkeley.edu/~jhoffman/domainadapt/. accessed
       4 mar 2016.
   27. 27.
       duan l, tsang iw, xu d. domain transfer multiple kernel learning.
       ieee trans pattern anal mach intell.
       2012;34(3):465   79.[575]crossref[576]google scholar
   28. 28.
       duan l, xu d, chang sf. exploiting web images for event recognition
       in consumer videos: a multiple source id20 approach.
       in: ieee 2012 conference on id161 and pattern
       recognition. 2012. p. 1338   45.[577]google scholar
   29. 29.
       duan l, xu d, tsang iw. id20 from multiple sources: a
       domain-dependent id173 approach. ieee trans neural netw
       learn syst. 2012;23(3):504   18.[578]crossref[579]google scholar
   30. 30.
       duan l, xu d, tsang iw. learning with augmented features for
       heterogeneous id20. ieee trans pattern anal mach
       intell. 2012;36(6):1134   48.[580]google scholar
   31. 31.
       eaton e, des jardins m, lane t. modeling transfer relationships
       between learning tasks for improved inductive transfer. proc mach
       learn knowl disc database.
       2008;5211:317   32.[581]crossref[582]google scholar
   32. 32.
       easyadapt.pl.gz (download). [583]http://hal3.name/easyadapt.pl.gz
       accessed 4 mar 2016.
   33. 33.
       evgeniou t, pontil m (2004) regularized id72. in:
       proceedings of the 10th acm sigkdd international conference on
       knowledge discovery and data mining. p. 109   17.[584]google scholar
   34. 34.
       exploiting web images for event recognition in consumer videos: a
       multiple source id20 approach.
       [585]http://lxduan.info/papers/duancvpr2012_poster.pdf. accessed 4
       mar 2016.
   35. 35.
       farhadi a, forsyth d, white r. id21 in sign language.
       in: ieee 2007 conference on id161 and pattern
       recognition. 2007. p. 1   8.[586]google scholar
   36. 36.
       feuz kd, cook dj. id21 across feature-rich
       heterogeneous feature spaces via feature-space remapping (fsr). j
       acm trans intell syst technol. 2014;6(1):1   27 (article
       3).[587]google scholar
   37. 37.
       gao j, fan w, jiang j, han j (2008) knowledge transfer via multiple
       model local structure mapping. in: proceedings of the 14th acm
       sigkdd international conference on knowledge discovery and data
       mining. p. 283   91.[588]google scholar
   38. 38.
       gao j, liang f, fan w, sun y, han j. graph based consensus
       maximization among multiple supervised and unsupervised models. adv
       neural inf process syst. 2009;22:1   9.[589]google scholar
   39. 39.
       gao k, khoshgoftaar tm, wang h, seliya n. choosing software metrics
       for defect prediction: an investigation on feature selection
       techniques. j softw pract exp.
       2011;41(5):579   606.[590]crossref[591]google scholar
   40. 40.
       ge l, gao j, ngo h, li k, zhang a. on handling negative transfer
       and imbalanced distributions in multiple source id21.
       in: proceedings of the 2013 siam international conference on data
       mining. 2013. p. 254   71.[592]google scholar
   41. 41.
       glorot x, bordes a, bengio y. id20 for large-scale
       sentiment classification: a deep learning approach. in: proceedings
       of the twenty-eight international conference on machine learning,
       vol. 27. 2011. p. 97   110.[593]google scholar
   42. 42.
       gong b, shi y, sha f, grauman k. geodesic flow kernel for
       unsupervised id20. in: proceedings of the 2012 ieee
       conference on id161 and pattern recognition. 2012. p.
       2066   73.[594]google scholar
   43. 43.
       gopalan r, li r, chellappa r. id20 for object
       recognition: an unsupervised approach. in: 2011 international
       conference on id161. 2011. p. 999   1006.[595]google
       scholar
   44. 44.
       guo-jun qi   s publication list.
       [596]http://www.eecs.ucf.edu/~gqi/publications.html. accessed 4 mar
       2016.
   45. 45.
       ham jh, lee dd, saul lk. learning high dimensional correspondences
       from low dimensional manifolds. in: proceedings of the twentieth
       international conference on machine learning. 2003. p.
       1   8.[597]google scholar
   46. 46.
       harel m, mannor s. learning from multiple outlooks. in: proceedings
       of the 28th international conference on machine learning. 2011. p.
       401   8.[598]google scholar
   47. 47.
       he p, li b, ma y (2014) towards cross-project defect prediction
       with imbalanced feature sets. [599]http://arxiv.org/abs/1411.4228.
   48. 48.
       heterogeneous defect prediction.
       [600]http://www.slideshare.net/hunkim/heterogeneous-defect-predicti
       on-esecfse-2015. accessed 4 mar 2016.
   49. 49.
       hfa_release_0315.rar (download).
       [601]https://sites.google.com/site/xyzliwen/publications/hfa_releas
       e_0315.rar. accessed 4 mar 2016.
   50. 50.
       hu m, liu b. mining and summarizing customer reviews. in:
       proceedings of the 10th acm sigkdd international conference on
       knowledge discovery and data mining. 2004. p. 168   77.[602]google
       scholar
   51. 51.
       huang j, smola a, gretton a, borgwardt km, sch  lkopf b. correcting
       sample selection bias by unlabeled data. in: proceedings of the
       2006 conference. adv neural inf process syst. 2006. p.
       601   8.[603]google scholar
   52. 52.
       jakob n, gurevych i. extracting opinion targets in a single and
       cross-domain setting with id49. in:
       proceedings of the 2010 conference on empirical methods in nlp.
       2010. p. 1035   45.[604]google scholar
   53. 53.
       jiang j, zhai c. instance weighting for id20 in nlp.
       in: proceedings of the 45th annual meeting of the association of
       computational linguistics. 2007. p. 264   71.[605]google scholar
   54. 54.
       jiang m, cui p, wang f, yang q, zhu w, yang s. social
       recommendation across multiple relational domains. in: proceedings
       of the 21st acm international conference on information and
       knowledge management. 2012. p. 1422   31.[606]google scholar
   55. 55.
       jiang w, zavesky e, chang sf, loui a. cross-domain learning methods
       for high-level visual concept classification. in: ieee 2008 15th
       international conference on image processing. 2008. p.
       161   4.[607]google scholar
   56. 56.
       kan m, wu j, shan s, chen x. id20 for face
       recognition: targetize source domain bridged by common subspace.
       int j comput vis.
       2014;109(1   2):94   109.[608]crossref[609]zbmath[610]google scholar
   57. 57.
       kloft m, brefeld u, sonnenburg s, zien a. lp-norm multiple kernel
       learning. j mach learn res.
       2011;12:953   97.[611]mathscinet[612]zbmath[613]google scholar
   58. 58.
       kulis b, saenko k, darrell t. what you saw is not what you get:
       id20 using asymmetric kernel transforms. in: ieee 2011
       conference on id161 and pattern recognition. 2011. p.
       1785   92.[614]google scholar
   59. 59.
       lecun y, bottou l, huangfu j. learning methods for generic object
       recognition with invariance to pose and lighting. in: proceedings
       of the 2004 ieee computer society conference on id161 and
       pattern recognition, vol. 2. 2004. p. 97   104.[615]google scholar
   60. 60.
       li b, yang q, xue x. can movies and books collaborate? cross-domain
       id185 for sparsity reduction. in: proceedings of
       the 21st international joint conference on artificial intelligence.
       2009. p. 2052   57.[616]google scholar
   61. 61.
       li b, yang q, xue x. id21 for id185
       via a rating-matrix generative model. in: proceedings of the 26th
       annual international conference on machine learning. 2009. p.
       617   24.[617]google scholar
   62. 62.
       li f, pan sj, jin o, yang q, zhu x. cross-domain co-extraction of
       sentiment and topic lexicons. in: proceedings of the 50th annual
       meeting of the association for computational linguistics long
       papers, vol. 1. 2012. p. 410   19.[618]google scholar
   63. 63.
       li s, zong c. multi-id20 for sentiment classification:
       using multiple classifier combining methods. in: proceedings of the
       conference on natural language processing and knowledge
       engineering. 2008. p. 1   8.[619]google scholar
   64. 64.
       li w, duan l, xu d, tsang iw. learning with augmented features for
       supervised and semi-supervised heterogeneous id20.
       ieee trans pattern anal mach intell.
       2014;36(6):1134   48.[620]crossref[621]google scholar
   65. 65.
       libid166 (2016) a library for support vector machines.
       [622]http://www.csie.ntu.edu.tw/~cjlin/libid166. accessed 4 mar 2016.
   66. 66.
       ling x, dai w, xue gr, yang q, yu y. spectral domain-transfer
       learning. in: proceedings of the 14th acm sigkdd international
       conference on knowledge discovery and data mining. 2008. p.
       488   96.[623]google scholar
   67. 67.
       lixin duan. [624]http://www.lxduan.info/#sourcecode_hfa. accessed 4
       mar 2016.
   68. 68.
       long m, wang j, ding g, pan sj, yu ps. adaptation id173: a
       general framework for id21. ieee trans knowl data eng.
       2014;26(5):1076   89.[625]crossref[626]google scholar
   69. 69.
       long m, wang j, ding g, sun j, yu ps. transfer id171
       with joint distribution adaptation. in: proceedings of the 2013
       ieee international conference on id161. 2013. p.
       2200   07.[627]google scholar
   70. 70.
       lowe dg. distinctive image features from scale-invariant keypoints.
       int comput vis. 2004;60(2):91   110.[628]crossref[629]google scholar
   71. 71.
       luo p, zhuang f, xiong h, xiong y, he q. id21 from
       multiple source domains via consensus id173. in:
       proceedings of the 17th acm conference on information and knowledge
       management. 2008. p. 103   12.[630]google scholar
   72. 72.
       ma y, gong w, mao f. id21 used to analyze the dynamic
       evolution of the dust aerosol. j quant spectrosc radiat transf.
       2015;153:119   30.[631]crossref[632]google scholar
   73. 73.
       marszalek m, schmid c, harzallah h, van de weijer j. learning
       object representations for visual object class recognition. in:
       visual recognition challenge workshop iccv. 2007. p.
       1   10.[633]google scholar
   74. 74.
       mihalkova l, mooney rj. id21 by mapping with minimal
       target data. in: proc. assoc. for the advancement of artificial
       intelligence workshop id21 for complex tasks. 2008. p.
       31   6.[634]google scholar
   75. 75.
       long m. [635]http://ise.thss.tsinghua.edu.cn/~mlong/. accessed 4
       mar 2016.
   76. 76.
       moreno o, shapira b, rokach l, shani g (2012) talmud   transfer
       learning for multiple domains. in: proceedings of the 21st acm
       international conference on information and knowledge management.
       2012. p. 425   34.[636]google scholar
   77. 77.
       nam j, kim s (2015) heterogeneous defect prediction. in:
       proceedings of the 2015 10th joint meeting on foundations of
       software engineering. 2015. p. 508   19.[637]google scholar
   78. 78.
       ng mk, wu q, ye y. co-id21 via joint transition
       id203 graph based method. in: proceedings of the 1st
       international workshop on cross domain knowledge discovery in web
       and social network mining. 2012. p. 1   9.[638]google scholar
   79. 79.
       ngiam j, khosla a, kim m, nam j, lee h, ng ay. multimodal deep
       learning. in: the 28th international conference on machine
       learning. 2011. p. 689   96.[639]google scholar
   80. 80.
       ogoe ha, visweswaran s, lu x, gopalakrishnan v. knowledge transfer
       via classification rules using functional mapping for integrative
       modeling of gene expression data. bmc bioinform. 2015. p.
       1   15.[640]google scholar
   81. 81.
       oquab m, bottou l, laptev i, sivic j. learning and transferring
       mid-level image representations using convolutional neural
       networks. in: proceedings of the 2014 ieee conference on computer
       vision and pattern recognition. 2013. p. 1717   24.[641]google
       scholar
   82. 82.
       pan sj, kwok jt, yang q. id21 via dimensionality
       reduction. in: proceedings of the 23rd national conference on
       artificial intelligence, vol. 2. 2008. p. 677   82.[642]google
       scholar
   83. 83.
       pan sj, ni x, sun jt, yang q, chen z. cross-domain sentiment
       classification via spectral feature alignment. in: proceedings of
       the 19th international conference on world wide web. 2010. p.
       751   60.[643]google scholar
   84. 84.
       pan sj, yang q. a survey on id21. ieee trans knowl
       data eng. 2010;22(10):1345   59.[644]crossref[645]google scholar
   85. 85.
       pan w, liu nn, xiang ew, yang q. id21 to predict
       missing ratings via heterogeneous user feedbacks. in: proceedings
       of the 22nd international joint conference on artificial
       intelligence. 2011. p. 2318   23.[646]google scholar
   86. 86.
       pan w. xiang ew, liu nn, yang q. id21 in collaborative
       filtering for sparsity reduction. in: twenty-fourth aaai conference
       on artificial intelligence, vol. 1. 2010. p. 230   235.[647]google
       scholar
   87. 87.
       pan sj, tsang iw, kwok jt, yang q. id20 via transfer
       component analysis. ieee trans neural netw.
       2009;22(2):199   210.[648]google scholar
   88. 88.
       papers:oquab-2014. [649]http://leon.bottou.org/papers/oquab-2014.
       accessed 4 mar 2016.
   89. 89.
       patel vm, gopalan r, li r, chellappa r. visual id20: a
       survey of recent advances. ieee signal process mag.
       2014;32(3):53   69.[650]crossref[651]google scholar
   90. 90.
       perlich c, dalessandro b, raeder t, stitelman o, provost f. machine
       learning for targeted display advertising: id21 in
       action. mach learn.
       2014;95:103   27.[652]mathscinet[653]crossref[654]google scholar
   91. 91.
       prettenhofer p, stein b. (2010) cross-language text classification
       using structural correspondence learning. in: proceedings of the
       48th annual meeting of the association for computational
       linguistics. 2010. p. 1118   27.[655]google scholar
   92. 92.
       qi gj, aggarwal c, huang t. towards semantic knowledge propagation
       from text corpus to web images. in: proceedings of the 20th
       international conference on world wide web. p. 297   306.[656]google
       scholar
   93. 93.
       qiu g, liu b, bu j, chen c. expanding domain sentiment lexicon
       through double propagation. in: proceedings of the 21st
       international joint conference on artificial intelligence. p.
       1199   204.[657]google scholar
   94. 94.
       quanz b, huan j. large margin transductive id21. in:
       proceedings of the 18th acm conference on information and knowledge
       management. 2009. p. 1327   36.[658]google scholar
   95. 95.
       raina r, battle a, lee h, packer b, ng ay. self-taught learning:
       id21 from unlabeled data. in: proceedings of the 24th
       international conference on machine learning. 2007. p.
       759   66.[659]google scholar
   96. 96.
       rajagopal an, subramanian r, ricci e, vieriu rl, lanz o,
       ramakrishnan kr, sebe n. exploring id21 approaches for
       head pose classification from multi-view surveillance images. int j
       comput vis. 2014;109(1   2):146   67.[660]crossref[661]google scholar
   97. 97.
       romera-paredes b, aung msh, pontil m, bianchi-berthouze n, williams
       ac de c, watson p. id21 to account for idiosyncrasy in
       face and body expressions. in: proceedings of the 10th
       international conference on automatic face and gesture recognition
       (fg). 2013. p. 1   6.[662]google scholar
   98. 98.
       rosenstein mt, marx z, kaelbling lp, dietterich tg. to transfer or
       not to transfer. in: proceedings nips   05 workshop, inductive
       transfer. 10 years later. 2005. p. 1   4.[663]google scholar
   99. 99.
       roy s.d., mei t., zeng w., li s. social transfer: cross-domain
       id21 from social streams for media applications. in:
       proceedings of the 20th acm international conference on multimedia.
       p. 649   58.[664]google scholar
   100. 100.
       saenko k, kulis b, fritz m, darrell t. adapting visual category
       models to new domains. comput vision eccv.
       2010;6314:213   26.[665]google scholar
   101. 101.
       schweikert g, widmer c, sch  lkopf b, r  tsch g. an empirical
       analysis of id20 algorithms for genomic sequence
       analysis. adv neural inf process syst. 2009;21:1433   40.[666]google
       scholar
   102. 102.
       seah cw, ong ys, tsang iw. combating negative transfer from
       predictive distribution differences. ieee trans cybern.
       2013;43(4):1153   65.[667]crossref[668]google scholar
   103. 103.
       shao l, zhu f, li x. id21 for visual categorization: a
       survey. ieee trans neural netw learn syst.
       2014;26(5):1019   34.[669]mathscinet[670]crossref[671]google scholar
   104. 104.
       shawe-taylor j, cristianini n. kernel methods for pattern analysis.
       cambridge: cambridge university press; 2004.[672]google scholar
   105. 105.
       shi x, liu q, fan w, yu ps, zhu r. id21 on
       heterogeneous feature spaces via spectral transformation. in: 2010
       ieee international conference on data mining. 2010. p.
       1049   1054.[673]google scholar
   106. 106.
       shi y, sha f. information-theoretical learning of discriminative
       clusters for unsupervised id20. in: proceedings of the
       29th international conference on machine learning. 2012. p.
       1   8.[674]google scholar
   107. 107.
       shimodaira h. improving predictive id136 under covariate shift
       by weighting the log-likelihood function. j stat plan inf.
       2000;90(2):227   44.[675]mathscinet[676]crossref[677]zbmath[678]googl
       e scholar
   108. 108.
       shivaji s, whitehead ej, akella r, kim s. reducing features to
       improve code change-based bug prediction. ieee trans softw eng.
       2013;39(4):552   69.[679]crossref[680]google scholar
   109. 109.
       si s, tao d, geng b. bregman divergence-based id173 for
       transfer subspace learning. ieee trans knowl data eng.
       2010;22(7):929   42.[681]crossref[682]google scholar
   110. 110.
       song z, chen q, huang z, hua y, yan s. contextualizing object
       detection and classification. ieee trans pattern anal mach intell.
       2011;37(1):13   27.[683]google scholar
   111. 111.
       steinwart i. on the influence of the kernel on the consistency of
       support vector machines. jmlr.
       2001;2:67   93.[684]mathscinet[685]zbmath[686]google scholar
   112. 112.
       taylor me, stone p. id21 for id23
       domains: a survey. jmlr.
       2009;10:1633   85.[687]mathscinet[688]zbmath[689]google scholar
   113. 113.
       tommasi t, caputo b. the more you know, the less you learn: from
       knowledge transfer to id62 of object categories. bmvc.
       2009;1   11.[690]google scholar
   114. 114.
       tommasi t, orabona f, caputo b. safety in numbers: learning
       categories from few examples with multi model knowledge transfer.
       ieee conf comput vision pattern recog. 2010;2010:3081   8.[691]google
       scholar
   115. 115.
       id21 resources. [692]http://www.cse.ust.hk/tl/.
       accessed 4 mar 2016.
   116. 116.
       tutorial on id20 and id21.
       [693]http://tommasit.wix.com/datl14tutorial. accessed 4 mar 2016.
   117. 117.
       vapnik v. principles of risk minimization for learning theory. adv
       neural inf process syst. 1992;4:831   8.[694]google scholar
   118. 118.
       vedaldi a, gulshan v, varma m, zisserman a. multiple kernels for
       id164. in: 2009 ieee 12th international conference on
       id161. 2009. p. 606   13.[695]google scholar
   119. 119.
       vincent p, larochelle h, bengio y, manzagol pa. extracting and
       composing robust features with denoising autoencoders. in:
       proceedings of the 25th international conference on machine
       learning. 2008. p. 1096   103.[696]google scholar
   120. 120.
       vinokourov a, shawe-taylor j, cristianini n. inferring a semantic
       representation of text via crosslanguage correlation analysis. adv
       neural inf proces syst. 2002;15:1473   80.[697]google scholar
   121. 121.
       wang c, mahadevan s. heterogeneous id20 using manifold
       alignment. in: proceedings of the 22nd international joint
       conference on artificial intelligence, vol. 2. 2011. p.
       541   46.[698]google scholar
   122. 122.
       wang g, hoiem d, forsyth da. building text features for object
       image classification. in: 2009 ieee conference on id161
       and pattern recognition. 2009. p. 1367   74.[699]google scholar
   123. 123.
       wang h, klaser a, schmid c, liu cl (2011) action recognition by
       dense trajectories. in: ieee 2011 conference on id161 and
       pattern recognition. 2011. p. 3169   76.[700]google scholar
   124. 124.
       wei b, pal c (2010) cross-lingual adaptation: an experiment on
       sentiment classifications. in: proceedings of the acl 2010
       conference short papers. 2010. p. 258   62.[701]google scholar
   125. 125.
       wei b, pal c (2011) heterogeneous id21 with rbms. in:
       proceedings of the twenty-fifth aaai conference on artificial
       intelligence. 2011. p. 531   36.[702]google scholar
   126. 126.
       weinberger kq, saul lk. distance metric learning for large margin
       nearest neighbor classification. jmlr.
       2009;10:207   44.[703]zbmath[704]google scholar
   127. 127.
       widmer c, ratsch g. multitask learning in computational biology.
       jmlr. 2012;27:207   16.[705]google scholar
   128. 128.
       wiens j, guttag j, horvitz ej. a study in id21:
       leveraging data from multiple hospitals to enhance
       hospital-specific predictions. j am med inform assoc.
       2013;21(4):699   706.[706]crossref[707]google scholar
   129. 129.
       witten ih, frank e. data mining, practical machine learning tools
       and techniques. 3rd ed. san francisco: morgan kaufmann publishers;
       2011.[708]zbmath[709]google scholar
   130. 130.
       wu x, xu d, duan l, luo j (2011) action recognition using context
       and appearance distribution features. in: ieee 2011 conference on
       id161 and pattern recognition. 2011. p.
       489   96.[710]google scholar
   131. 131.
       xia r, zong c. a pos-based ensemble model for cross-domain
       sentiment classification. in: proceedings of the 5th international
       joint conference on natural language processing. 2011. p.
       614   22.[711]google scholar
   132. 132.
       xia r, zong c, hu x, cambria e. feature ensemble plus sample
       selection: id20 for sentiment classification. ieee
       intell syst. 2013;28(3):10   8.[712]crossref[713]google scholar
   133. 133.
       xiao m, guo y. semi-supervised kernel matching for domain
       adaptation. in: proceedings of the twenty-sixth aaai conference on
       artificial intelligence. 2012. p. 1183   89.[714]google scholar
   134. 134.
       xie m, jean n, burke m, lobell d, ermon s. id21 from
       deep features for remote sensing and poverty mapping. in:
       proceedings 30th aaai conference on artificial intelligence. 2015.
       p. 1   10.[715]google scholar
   135. 135.
       yang j, yan r, hauptmann ag. cross-domain video concept detection
       using adaptive id166s. in: proceedings of the 15th acm international
       conference on multimedia. 2007. p. 188   97.[716]google scholar
   136. 136.
       yang l, jing l, yu j, ng mk. learning transferred weights from
       co-occurrence data for heterogeneous id21. ieee trans
       neural netw learn syst. 2015;pp(99):1   14.[717]google scholar
   137. 137.
       yang q, chen y, xue gr, dai w, yu y. heterogeneous transfer
       learning for image id91 via the social web. in: proceedings
       of the joint conference of the 47th annual meeting of the acl, vol.
       1. 2009. p. 1   9.[718]google scholar
   138. 138.
       yao y, doretto g. boosting for id21 with multiple
       sources. in: proceedings of the ieee computer society conference on
       id161 and pattern recognition. 2010. p.
       1855   62.[719]google scholar
   139. 139.
       yin z. [720]http://www.cse.ust.hk/~yinz/. accessed 4 mar 2016.
   140. 140.
       zhang y, cao b, yeung d. multi-domain id185. in:
       proceedings of the 26th conference on uncertainty in artificial
       intelligence. 2010. p. 725   32.[721]google scholar
   141. 141.
       zhang y, yeung dy. transfer metric learning by learning task
       relationships. in: proceedings of the 16th acm sigkdd international
       conference on knowledge discovery and data mining. 2010. p.
       1199   208.[722]google scholar
   142. 142.
       zhao l, pan sj, xiang ew, zhong e, lu z, yang q. active transfer
       learning for cross-system recommendation. in: proceedings of the
       27th aaai conference on artificial intelligence. 2013. p.
       1205   11.[723]google scholar
   143. 143.
       zhong e, fan w, peng j, zhang k, ren j, turaga d, verscheure o.
       cross domain distribution adaptation via kernel mapping. in:
       proceedings of the 15th acm sigkdd. 2009. p. 1027   36.[724]google
       scholar
   144. 144.
       zhou jt, pan s, tsang iw, yan y. hybrid heterogeneous transfer
       learning through deep learning. in: proceedings of the national
       conference on artificial intelligence, vol. 3. 2014. p.
       2213   20.[725]google scholar
   145. 145.
       zhou jt, tsang iw, pan sj tan m. heterogeneous id20
       for multiple classes. in: international conference on artificial
       intelligence and statistics. 2014. p. 1095   103.[726]google scholar
   146. 146.
       zhu y, chen y, lu z, pan s, xue g, yu y, yang q. heterogeneous
       id21 for image classification. in: proceedings of the
       national conference on artificial intelligence, vol. 2. 2011. p.
       1304   9.[727]google scholar

copyright information

      the author(s) 2016

   open accessthis article is distributed under the terms of the creative
   commons attribution 4.0 international license
   ([728]http://creativecommons.org/licenses/by/4.0/), which permits
   unrestricted use, distribution, and reproduction in any medium,
   provided you give appropriate credit to the original author(s) and the
   source, provide a link to the creative commons license, and indicate if
   changes were made.

authors and affiliations

     * karl weiss
          + 1
       [729]email author
     * taghi m. khoshgoftaar
          + 1
     * dingding wang
          + 1

    1. 1.florida atlantic universityboca ratonusa

about this article

   [730]crossmark

   cite this article as:
          weiss, k., khoshgoftaar, t.m. & wang, d. j big data (2016) 3: 9.
          https://doi.org/10.1186/s40537-016-0043-6

     * received 04 march 2016
     * accepted 17 may 2016
     * first online 28 may 2016
     * doi https://doi.org/10.1186/s40537-016-0043-6
     * publisher name springer international publishing
     * online issn 2196-1115

     * [731]about this journal
     * [732]reprints and permissions

personalised recommendations

cite article

     * [733]how to cite?
     * [734].ris papers reference manager refworks zotero
     * [735].enw endnote
     * [736].bib bibtex jabref mendeley

   [737]share article
   [738]download pdf

actions

   [739]download pdf

cite article

     * [740]how to cite?
     * [741].ris papers reference manager refworks zotero
     * [742].enw endnote
     * [743].bib bibtex jabref mendeley

   [744]share article

table of contents

     * [745]article
     * [746]abstract
     * [747]background
     * [748]definitions of id21
     * [749]homogeneous id21
     * [750]heterogeneous id21
     * [751]negative transfer
     * [752]id21 applications
     * [753]conclusion and discussion
     * [754]notes
     * [755]appendix
     * [756]references
     * [757]copyright information
     * [758]authors and affiliations
     * [759]about this article

   advertisement
   (button) hide

   over 10 million scientific documents at your fingertips

switch edition

     * [760]academic edition
     * [761]corporate edition

     * [762]home
     * [763]impressum
     * [764]legal information
     * [765]privacy statement
     * [766]how we use cookies
     * [767]accessibility
     * [768]contact us

   [769]springer nature

      2018 springer nature switzerland ag. part of [770]springer nature.

   not logged in not affiliated 73.4.230.149

references

   visible links
   1. https://www.googletagmanager.com/ns.html?id=gtm-wcf9z9
   2. https://link.springer.com/article/10.1186/s40537-016-0043-6#main-content
   3. https://link.springer.com/article/10.1186/s40537-016-0043-6#article-contents
   4. http://activatejavascript.org/
   5. https://link.springer.com/article/10.1186/s40537-016-0043-6#search-container
   6. https://link.springer.com/
   7. https://link.springer.com/signup-login?previousurl=https://link.springer.com/article/10.1186/s40537-016-0043-6
   8. https://link.springer.com/journal/40537
   9. https://link.springer.com/content/pdf/10.1186/s40537-016-0043-6.pdf
  10. https://link.springer.com/journal/40537
  11. https://link.springer.com/article/10.1186/s40537-016-0043-6#citeas
  12. https://link.springer.com/article/10.1186/s40537-016-0043-6#authorsandaffiliations
  13. mailto:kweiss6@fau.edu
  14. http://www.altmetric.com/details.php?citation_id=10765399&domain=link.springer.com
  15. https://citations.springer.com/item?doi=10.1186/s40537-016-0043-6
  16. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr129
  17. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr107
  18. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr84
  19. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr84
  20. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
  21. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
  22. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
  23. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
  24. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr46
  25. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
  26. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr145
  27. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr91
  28. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr144
  29. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr112
  30. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr84
  31. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr84
  32. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr129
  33. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec2
  34. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec3
  35. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec7
  36. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec17
  37. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec19
  38. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec1
  39. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr84
  40. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr107
  41. https://link.springer.com/article/10.1186/s40537-016-0043-6#tab1
  42. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr144
  43. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
  44. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
  45. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
  46. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
  47. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr19
  48. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr36
  49. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr84
  50. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr84
  51. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr51
  52. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr53
  53. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr82
  54. https://link.springer.com/article/10.1186/s40537-016-0043-6#fig1
  55. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
  56. https://link.springer.com/article/10.1186/s40537-016-0043-6#fig1
  57. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr37
  58. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr8
  59. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr33
  60. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr74
  61. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr62
  62. https://media.springernature.com/original/springer-static/image/art:10.1186/s40537-016-0043-6/mediaobjects/40537_2016_43_fig1_html.gif
  63. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec3
  64. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec7
  65. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec17
  66. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec3
  67. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec7
  68. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec17
  69. https://link.springer.com/article/10.1186/s40537-016-0043-6#tab2
  70. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec3
  71. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
  72. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
  73. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
  74. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr27
  75. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr69
  76. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
  77. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
  78. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr83
  79. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr41
  80. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
  81. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr106
  82. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr81
  83. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr114
  84. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr28
  85. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr138
  86. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr138
  87. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr62
  88. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr132
  89. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
  90. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
  91. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr29
  92. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec3
  93. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr51
  94. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
  95. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr143
  96. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr37
  97. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr29
  98. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr29
  99. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr143
 100. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr37
 101. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 102. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr51
 103. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
 104. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
 105. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr15
 106. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
 107. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
 108. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr130
 109. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr118
 110. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr27
 111. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr82
 112. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr51
 113. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr117
 114. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr10
 115. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
 116. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr135
 117. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr55
 118. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr51
 119. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr69
 120. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr10
 121. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 122. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 123. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr109
 124. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr69
 125. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 126. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr109
 127. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 128. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 129. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 130. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
 131. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr4
 132. https://link.springer.com/article/10.1186/s40537-016-0043-6#fig2
 133. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr117
 134. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr69
 135. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr69
 136. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
 137. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr69
 138. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr66
 139. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr83
 140. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 141. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr94
 142. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr133
 143. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr133
 144. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 145. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr83
 146. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr94
 147. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr66
 148. https://media.springernature.com/original/springer-static/image/art:10.1186/s40537-016-0043-6/mediaobjects/40537_2016_43_fig2_html.gif
 149. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
 150. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 151. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr111
 152. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr10
 153. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr82
 154. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 155. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr51
 156. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr51
 157. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 158. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 159. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr83
 160. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr17
 161. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 162. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr91
 163. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 164. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 165. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 166. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 167. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr41
 168. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr119
 169. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 170. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr63
 171. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr83
 172. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr41
 173. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 174. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr63
 175. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr83
 176. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 177. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 178. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr43
 179. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr43
 180. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 181. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 182. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr43
 183. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr106
 184. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr126
 185. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 186. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 187. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr43
 188. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 189. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr106
 190. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr106
 191. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 192. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 193. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr43
 194. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 195. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr43
 196. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr43
 197. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 198. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr59
 199. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr81
 200. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr81
 201. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr73
 202. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr110
 203. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr73
 204. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr110
 205. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr81
 206. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr110
 207. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr73
 208. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr81
 209. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr110
 210. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr73
 211. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr114
 212. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr13
 213. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr114
 214. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr113
 215. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr114
 216. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr113
 217. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr114
 218. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr113
 219. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr114
 220. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr114
 221. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr28
 222. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr70
 223. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr123
 224. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr11
 225. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr101
 226. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr29
 227. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
 228. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr29
 229. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr138
 230. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr138
 231. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr138
 232. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr21
 233. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr62
 234. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr62
 235. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr50
 236. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr93
 237. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr52
 238. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr21
 239. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr62
 240. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr132
 241. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr132
 242. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr131
 243. https://link.springer.com/article/10.1186/s40537-016-0043-6#tab2
 244. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr27
 245. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 246. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr87
 247. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr62
 248. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr106
 249. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr81
 250. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr41
 251. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr83
 252. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
 253. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr138
 254. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr114
 255. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
 256. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr132
 257. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
 258. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr28
 259. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr69
 260. https://link.springer.com/article/10.1186/s40537-016-0043-6#tab2
 261. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
 262. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr27
 263. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr106
 264. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr132
 265. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
 266. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 267. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 268. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 269. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 270. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr92
 271. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 272. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr145
 273. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 274. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr91
 275. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr144
 276. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 277. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr124
 278. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
 279. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 280. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr46
 281. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
 282. https://link.springer.com/article/10.1186/s40537-016-0043-6#fig1
 283. https://link.springer.com/article/10.1186/s40537-016-0043-6#fig1
 284. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr91
 285. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr125
 286. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr91
 287. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr5
 288. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr91
 289. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr91
 290. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 291. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
 292. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr45
 293. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr4
 294. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 295. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 296. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr137
 297. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr70
 298. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr24
 299. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr95
 300. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr122
 301. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 302. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr95
 303. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr122
 304. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr92
 305. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr92
 306. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 307. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 308. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr92
 309. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr20
 310. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 311. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 312. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr20
 313. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr92
 314. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 315. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 316. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
 317. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr117
 318. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr3
 319. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
 320. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 321. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 322. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 323. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
 324. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 325. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
 326. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 327. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 328. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 329. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 330. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 331. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 332. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr57
 333. https://link.springer.com/article/10.1186/s40537-016-0043-6#fig3
 334. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 335. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 336. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 337. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
 338. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 339. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 340. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 341. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
 342. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 343. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 344. https://media.springernature.com/original/springer-static/image/art:10.1186/s40537-016-0043-6/mediaobjects/40537_2016_43_fig3_html.gif
 345. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 346. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 347. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 348. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 349. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr104
 350. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr23
 351. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
 352. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr100
 353. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 354. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 355. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 356. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr46
 357. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr46
 358. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr46
 359. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr145
 360. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr2
 361. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
 362. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 363. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 364. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr145
 365. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 366. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
 367. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr39
 368. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr108
 369. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
 370. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
 371. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr47
 372. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
 373. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
 374. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
 375. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
 376. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr144
 377. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr144
 378. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr16
 379. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr41
 380. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 381. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr120
 382. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr79
 383. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr144
 384. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr79
 385. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr120
 386. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 387. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr136
 388. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr137
 389. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr78
 390. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 391. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr136
 392. https://link.springer.com/article/10.1186/s40537-016-0043-6#tab3
 393. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec11
 394. https://link.springer.com/article/10.1186/s40537-016-0043-6#tab3
 395. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 396. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 397. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
 398. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 399. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr145
 400. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 401. https://link.springer.com/article/10.1186/s40537-016-0043-6#fig1
 402. https://link.springer.com/article/10.1186/s40537-016-0043-6#tab4
 403. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr136
 404. https://link.springer.com/article/10.1186/s40537-016-0043-6#tab3
 405. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 406. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec7
 407. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr91
 408. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr105
 409. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr121
 410. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 411. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr92
 412. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 413. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 414. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 415. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr46
 416. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr145
 417. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
 418. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr144
 419. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr98
 420. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr98
 421. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr31
 422. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr17
 423. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr31
 424. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr31
 425. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr31
 426. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr40
 427. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr40
 428. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr17
 429. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr71
 430. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr38
 431. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
 432. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr37
 433. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr71
 434. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr38
 435. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
 436. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr37
 437. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
 438. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr37
 439. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr40
 440. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr102
 441. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
 442. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr51
 443. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr11
 444. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr136
 445. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec11
 446. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 447. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr14
 448. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr28
 449. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr76
 450. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr76
 451. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr12
 452. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr60
 453. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr61
 454. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr86
 455. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr140
 456. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr85
 457. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr99
 458. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr54
 459. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr142
 460. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr96
 461. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr72
 462. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr21
 463. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr134
 464. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr81
 465. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr80
 466. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr90
 467. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr56
 468. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr35
 469. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr127
 470. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr128
 471. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr97
 472. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr25
 473. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr141
 474. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr19
 475. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr89
 476. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr103
 477. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
 478. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
 479. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr27
 480. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr106
 481. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr132
 482. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 483. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
 484. https://link.springer.com/article/10.1186/s40537-016-0043-6#tab5
 485. https://link.springer.com/article/10.1186/s40537-016-0043-6#tab6
 486. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr91
 487. https://github.com/pprett/bolt
 488. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr7
 489. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr146
 490. http://www.cse.ust.hk/~yinz/
 491. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr139
 492. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr21
 493. https://github.com/bochen90/machine-learning-matlab/blob/master/tradaboost.m
 494. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr6
 495. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr22
 496. http://hal3.name/easyadapt.pl.gz
 497. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr32
 498. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr30
 499. https://sites.google.com/site/xyzliwen/publications/hfa_release_0315.rar
 500. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr49
 501. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 502. http://vision.cs.uml.edu/adaptation.html
 503. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr18
 504. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr92
 505. http://www.eecs.ucf.edu/~gqi/publications.html
 506. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr44
 507. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr64
 508. http://www.lxduan.info/#sourcecode_hfa
 509. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr67
 510. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr42
 511. http://www-scf.usc.edu/~boqinggo/
 512. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr9
 513. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr68
 514. http://ise.thss.tsinghua.edu.cn/~mlong/
 515. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr75
 516. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr81
 517. http://leon.bottou.org/papers/oquab-2014
 518. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr88
 519. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr69
 520. http://ise.thss.tsinghua.edu.cn/~mlong/
 521. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr75
 522. http://www.cse.ust.hk/tl/
 523. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr115
 524. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr77
 525. http://www.slideshare.net/hunkim/heterogeneous-defect-prediction-esecfse-2015
 526. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr48
 527. http://www.csie.ntu.edu.tw/~cjlin/libid166
 528. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr65
 529. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr58
 530. https://www.eecs.berkeley.edu/~jhoffman/domainadapt/
 531. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr26
 532. http://tommasit.wix.com/datl14tutorial
 533. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr116
 534. http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/da_survey.html
 535. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr1
 536. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr28
 537. http://lxduan.info/papers/duancvpr2012_poster.pdf
 538. https://link.springer.com/article/10.1186/s40537-016-0043-6#cr34
 539. http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/da_survey.html
 540. http://www.ams.org/mathscinet-getitem?mr=2249873
 541. http://www.emis.de/math-item?1222.68133
 542. http://scholar.google.com/scholar_lookup?title=a framework for learning predictive structures from multiple tasks and unlabeled data&author=rk. ando&author=t. zhang&journal=j mach learn res&volume=6&pages=1817-1853&publication_year=2005
 543. https://doi.org/10.1016/j.cviu.2007.09.014
 544. http://scholar.google.com/scholar_lookup?title=surf: speeded up robust features&author=h. bay&author=t. tuytelaars&author=lv. gool&journal=comput vis image underst&volume=110&issue=3&pages=346-359&publication_year=2006
 545. http://www.ams.org/mathscinet-getitem?mr=2274444
 546. http://www.emis.de/math-item?1222.68144
 547. http://scholar.google.com/scholar_lookup?title=manifold id173: a geometric framework for learning from examples&author=m. belkin&author=p. niyogi&author=v. sindhwani&journal=j mach learn res arch&volume=7&pages=2399-2434&publication_year=2006
 548. https://scholar.google.com/scholar?q=blitzer, j, mcdonald r, pereira f. id20 with structural correspondence learning. in: proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120   8.
 549. https://github.com/bochen90/machine-learning-matlab/blob/master/tradaboost.m
 550. http://pprett.github.com/bolt/
 551. https://scholar.google.com/scholar?q=bonilla e, chai km, williams c. multi-task gaussian process prediction. in: proceedings of the 20th annual conference of neural information processing systems. 2008. 153   60.
 552. http://www-scf.usc.edu/~boqinggo/
 553. https://doi.org/10.1093/bioinformatics/btl242
 554. http://scholar.google.com/scholar_lookup?title=integrating structured biological data by kernel maximum mean discrepancy&author=km. borgwardt&author=a. gretton&author=mj. rasch&author=hp. kriegel&author=b. sch  lkopf&author=aj. smola&journal=bioinformatics&volume=22&issue=4&pages=49-57&publication_year=2006
 555. https://doi.org/10.1109/tpami.2009.57
 556. http://scholar.google.com/scholar_lookup?title=id20 problems: a daid166 classification technique and a circular validation strategy&author=l. bruzzone&author=m. marconcini&journal=ieee trans pattern anal mach intell&volume=32&issue=5&pages=770-787&publication_year=2010
 557. https://scholar.google.com/scholar?q=cao b, liu n, yang q. id21 for collective link prediction in multiple heterogeneous domains. in: proceedings of the 27th international conference on machine learning. 2010. p. 159   66.
 558. https://scholar.google.com/scholar?q=cawley g. leave-one-out cross-validation based model selection criteria for weighted ls-id166s. in: ieee 2006 international joint conference on neural network proceedings 2006. p. 1661   68.
 559. https://scholar.google.com/scholar?q=chattopadhyay r, ye j, panchanathan s, fan w, davidson i. multi-source id20 and its application to early detection of fatigue. acm trans knowl dis data (best of sigkdd 2011 tkdd homepage archive) 2011; 6(4) (article 18).
 560. https://doi.org/10.1016/j.csl.2005.05.005
 561. http://scholar.google.com/scholar_lookup?title=adaptation of maximum id178 classifier: little data can help a lot&author=c. chelba&author=a. acero&journal=comput speech lang&volume=20&issue=4&pages=382-399&publication_year=2004
 562. https://scholar.google.com/scholar?q=chen m, xu ze, weinberger kq, sha f (2012) marginalized denoising autoencoders for id20. icml. arxiv preprintarxiv:1206.4683.
 563. https://scholar.google.com/scholar?q=chung frk. spectral id207. in: cbms regional conference series in mathematics, no. 92. providence: american mathematical society; 1994.
 564. http://vision.cs.uml.edu/adaptation.html
 565. https://doi.org/10.1007/s10115-013-0665-3
 566. http://scholar.google.com/scholar_lookup?title=id21 for activity recognition: a survey&author=dj. cook&author=kd. feuz&author=nc. krishnan&journal=knowl inf syst&volume=36&issue=3&pages=537-556&publication_year=2012
 567. http://scholar.google.com/scholar_lookup?title=translated learning: id21 across different feature spaces&author=w. dai&author=y. chen&author=gr. xue&author=q. yang&author=y. yu&journal=adv neural inform process syst&volume=21&pages=353-360&publication_year=2008
 568. https://scholar.google.com/scholar?q=dai w, yang q, xue gr, yu y (2007) boosting for id21. in: proceedings of the 24th international conference on machine learning. p. 193   200.
 569. https://scholar.google.com/scholar?q=daum   h iii. frustratingly easy id20. in: proceedings of acl. 2007. p. 256   63.
 570. https://scholar.google.com/scholar?q=davis j, kulis b, jain p, sra s, dhillon i. information theoretic metric learning. in: proceedings of the 24th international conference on machine learning. 2007. p. 209   16.
 571. https://doi.org/10.1002/(sici)1097-4571(199009)41:6<391::aid-asi1>3.0.co;2-9
 572. http://scholar.google.com/scholar_lookup?title=indexing by latent semantic analysis&author=s. deerwester&author=st. dumais&author=gw. furnas&author=tk. landauer&author=r. harshman&journal=j am soc inf sci&volume=41&pages=391-407&publication_year=1990
 573. https://scholar.google.com/scholar?q=deng j, zhang z, marchi e, schuller b. sparse autoencoder based feature id21 for speech emotion recognition. in: humaine association conference on affective computing and intelligent interaction. 2013. p. 511   6.
 574. https://www.eecs.berkeley.edu/~jhoffman/domainadapt/
 575. https://doi.org/10.1109/tpami.2011.114
 576. http://scholar.google.com/scholar_lookup?title=domain transfer multiple kernel learning&author=l. duan&author=iw. tsang&author=d. xu&journal=ieee trans pattern anal mach intell&volume=34&issue=3&pages=465-479&publication_year=2012
 577. https://scholar.google.com/scholar?q=duan l, xu d, chang sf. exploiting web images for event recognition in consumer videos: a multiple source id20 approach. in: ieee 2012 conference on id161 and pattern recognition. 2012. p. 1338   45.
 578. https://doi.org/10.1109/tnnls.2011.2178556
 579. http://scholar.google.com/scholar_lookup?title=id20 from multiple sources: a domain-dependent id173 approach&author=l. duan&author=d. xu&author=iw. tsang&journal=ieee trans neural netw learn syst&volume=23&issue=3&pages=504-518&publication_year=2012
 580. http://scholar.google.com/scholar_lookup?title=learning with augmented features for heterogeneous id20&author=l. duan&author=d. xu&author=iw. tsang&journal=ieee trans pattern anal mach intell&volume=36&issue=6&pages=1134-1148&publication_year=2012
 581. https://doi.org/10.1007/978-3-540-87479-9_39
 582. http://scholar.google.com/scholar_lookup?title=modeling transfer relationships between learning tasks for improved inductive transfer&author=e. eaton&author=m. des jardins&author=t. lane&journal=proc mach learn knowl disc database&volume=5211&pages=317-332&publication_year=2008
 583. http://hal3.name/easyadapt.pl.gz
 584. https://scholar.google.com/scholar?q=evgeniou t, pontil m (2004) regularized id72. in: proceedings of the 10th acm sigkdd international conference on knowledge discovery and data mining. p. 109   17.
 585. http://lxduan.info/papers/duancvpr2012_poster.pdf
 586. https://scholar.google.com/scholar?q=farhadi a, forsyth d, white r. id21 in sign language. in: ieee 2007 conference on id161 and pattern recognition. 2007. p. 1   8.
 587. https://scholar.google.com/scholar?q=feuz kd, cook dj. id21 across feature-rich heterogeneous feature spaces via feature-space remapping (fsr). j acm trans intell syst technol. 2014;6(1):1   27 (article 3).
 588. https://scholar.google.com/scholar?q=gao j, fan w, jiang j, han j (2008) knowledge transfer via multiple model local structure mapping. in: proceedings of the 14th acm sigkdd international conference on knowledge discovery and data mining. p. 283   91.
 589. http://scholar.google.com/scholar_lookup?title=graph based consensus maximization among multiple supervised and unsupervised models&author=j. gao&author=f. liang&author=w. fan&author=y. sun&author=j. han&journal=adv neural inf process syst&volume=22&pages=1-9&publication_year=2009
 590. https://doi.org/10.1002/spe.1043
 591. http://scholar.google.com/scholar_lookup?title=choosing software metrics for defect prediction: an investigation on feature selection techniques&author=k. gao&author=tm. khoshgoftaar&author=h. wang&author=n. seliya&journal=j softw pract exp&volume=41&issue=5&pages=579-606&publication_year=2011
 592. https://scholar.google.com/scholar?q=ge l, gao j, ngo h, li k, zhang a. on handling negative transfer and imbalanced distributions in multiple source id21. in: proceedings of the 2013 siam international conference on data mining. 2013. p. 254   71.
 593. https://scholar.google.com/scholar?q=glorot x, bordes a, bengio y. id20 for large-scale sentiment classification: a deep learning approach. in: proceedings of the twenty-eight international conference on machine learning, vol. 27. 2011. p. 97   110.
 594. https://scholar.google.com/scholar?q=gong b, shi y, sha f, grauman k. geodesic flow kernel for unsupervised id20. in: proceedings of the 2012 ieee conference on id161 and pattern recognition. 2012. p. 2066   73.
 595. https://scholar.google.com/scholar?q=gopalan r, li r, chellappa r. id20 for object recognition: an unsupervised approach. in: 2011 international conference on id161. 2011. p. 999   1006.
 596. http://www.eecs.ucf.edu/~gqi/publications.html
 597. https://scholar.google.com/scholar?q=ham jh, lee dd, saul lk. learning high dimensional correspondences from low dimensional manifolds. in: proceedings of the twentieth international conference on machine learning. 2003. p. 1   8.
 598. https://scholar.google.com/scholar?q=harel m, mannor s. learning from multiple outlooks. in: proceedings of the 28th international conference on machine learning. 2011. p. 401   8.
 599. http://arxiv.org/abs/1411.4228
 600. http://www.slideshare.net/hunkim/heterogeneous-defect-prediction-esecfse-2015
 601. https://sites.google.com/site/xyzliwen/publications/hfa_release_0315.rar
 602. https://scholar.google.com/scholar?q=hu m, liu b. mining and summarizing customer reviews. in: proceedings of the 10th acm sigkdd international conference on knowledge discovery and data mining. 2004. p. 168   77.
 603. https://scholar.google.com/scholar?q=huang j, smola a, gretton a, borgwardt km, sch  lkopf b. correcting sample selection bias by unlabeled data. in: proceedings of the 2006 conference. adv neural inf process syst. 2006. p. 601   8.
 604. https://scholar.google.com/scholar?q=jakob n, gurevych i. extracting opinion targets in a single and cross-domain setting with id49. in: proceedings of the 2010 conference on empirical methods in nlp. 2010. p. 1035   45.
 605. https://scholar.google.com/scholar?q=jiang j, zhai c. instance weighting for id20 in nlp. in: proceedings of the 45th annual meeting of the association of computational linguistics. 2007. p. 264   71.
 606. https://scholar.google.com/scholar?q=jiang m, cui p, wang f, yang q, zhu w, yang s. social recommendation across multiple relational domains. in: proceedings of the 21st acm international conference on information and knowledge management. 2012. p. 1422   31.
 607. https://scholar.google.com/scholar?q=jiang w, zavesky e, chang sf, loui a. cross-domain learning methods for high-level visual concept classification. in: ieee 2008 15th international conference on image processing. 2008. p. 161   4.
 608. https://doi.org/10.1007/s11263-013-0693-1
 609. http://www.emis.de/math-item?1328.68244
 610. http://scholar.google.com/scholar_lookup?title=id20 for face recognition: targetize source domain bridged by common subspace&author=m. kan&author=j. wu&author=s. shan&author=x. chen&journal=int j comput vis&volume=109&issue=1   2&pages=94-109&publication_year=2014
 611. http://www.ams.org/mathscinet-getitem?mr=2786915
 612. http://www.emis.de/math-item?1280.68173
 613. http://scholar.google.com/scholar_lookup?title=lp-norm multiple kernel learning&author=m. kloft&author=u. brefeld&author=s. sonnenburg&author=a. zien&journal=j mach learn res&volume=12&pages=953-997&publication_year=2011
 614. https://scholar.google.com/scholar?q=kulis b, saenko k, darrell t. what you saw is not what you get: id20 using asymmetric kernel transforms. in: ieee 2011 conference on id161 and pattern recognition. 2011. p. 1785   92.
 615. https://scholar.google.com/scholar?q=lecun y, bottou l, huangfu j. learning methods for generic object recognition with invariance to pose and lighting. in: proceedings of the 2004 ieee computer society conference on id161 and pattern recognition, vol. 2. 2004. p. 97   104.
 616. https://scholar.google.com/scholar?q=li b, yang q, xue x. can movies and books collaborate? cross-domain id185 for sparsity reduction. in: proceedings of the 21st international joint conference on artificial intelligence. 2009. p. 2052   57.
 617. https://scholar.google.com/scholar?q=li b, yang q, xue x. id21 for id185 via a rating-matrix generative model. in: proceedings of the 26th annual international conference on machine learning. 2009. p. 617   24.
 618. https://scholar.google.com/scholar?q=li f, pan sj, jin o, yang q, zhu x. cross-domain co-extraction of sentiment and topic lexicons. in: proceedings of the 50th annual meeting of the association for computational linguistics long papers, vol. 1. 2012. p. 410   19.
 619. https://scholar.google.com/scholar?q=li s, zong c. multi-id20 for sentiment classification: using multiple classifier combining methods. in: proceedings of the conference on natural language processing and knowledge engineering. 2008. p. 1   8.
 620. https://doi.org/10.1109/tpami.2013.167
 621. http://scholar.google.com/scholar_lookup?title=learning with augmented features for supervised and semi-supervised heterogeneous id20&author=w. li&author=l. duan&author=d. xu&author=iw. tsang&journal=ieee trans pattern anal mach intell&volume=36&issue=6&pages=1134-1148&publication_year=2014
 622. http://www.csie.ntu.edu.tw/~cjlin/libid166
 623. https://scholar.google.com/scholar?q=ling x, dai w, xue gr, yang q, yu y. spectral domain-id21. in: proceedings of the 14th acm sigkdd international conference on knowledge discovery and data mining. 2008. p. 488   96.
 624. http://www.lxduan.info/#sourcecode_hfa
 625. https://doi.org/10.1109/tkde.2013.111
 626. http://scholar.google.com/scholar_lookup?title=adaptation id173: a general framework for id21&author=m. long&author=j. wang&author=g. ding&author=sj. pan&author=ps. yu&journal=ieee trans knowl data eng&volume=26&issue=5&pages=1076-1089&publication_year=2014
 627. https://scholar.google.com/scholar?q=long m, wang j, ding g, sun j, yu ps. transfer id171 with joint distribution adaptation. in: proceedings of the 2013 ieee international conference on id161. 2013. p. 2200   07.
 628. https://doi.org/10.1023/b:visi.0000029664.99615.94
 629. http://scholar.google.com/scholar_lookup?title=distinctive image features from scale-invariant keypoints&author=dg. lowe&journal=int comput vis&volume=60&issue=2&pages=91-110&publication_year=2004
 630. https://scholar.google.com/scholar?q=luo p, zhuang f, xiong h, xiong y, he q. id21 from multiple source domains via consensus id173. in: proceedings of the 17th acm conference on information and knowledge management. 2008. p. 103   12.
 631. https://doi.org/10.1016/j.jqsrt.2014.09.025
 632. http://scholar.google.com/scholar_lookup?title=id21 used to analyze the dynamic evolution of the dust aerosol&author=y. ma&author=w. gong&author=f. mao&journal=j quant spectrosc radiat transf&volume=153&pages=119-130&publication_year=2015
 633. https://scholar.google.com/scholar?q=marszalek m, schmid c, harzallah h, van de weijer j. learning object representations for visual object class recognition. in: visual recognition challenge workshop iccv. 2007. p. 1   10.
 634. https://scholar.google.com/scholar?q=mihalkova l, mooney rj. id21 by mapping with minimal target data. in: proc. assoc. for the advancement of artificial intelligence workshop id21 for complex tasks. 2008. p. 31   6.
 635. http://ise.thss.tsinghua.edu.cn/~mlong/
 636. https://scholar.google.com/scholar?q=moreno o, shapira b, rokach l, shani g (2012) talmud   id21 for multiple domains. in: proceedings of the 21st acm international conference on information and knowledge management. 2012. p. 425   34.
 637. https://scholar.google.com/scholar?q=nam j, kim s (2015) heterogeneous defect prediction. in: proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508   19.
 638. https://scholar.google.com/scholar?q=ng mk, wu q, ye y. co-id21 via joint transition id203 graph based method. in: proceedings of the 1st international workshop on cross domain knowledge discovery in web and social network mining. 2012. p. 1   9.
 639. https://scholar.google.com/scholar?q=ngiam j, khosla a, kim m, nam j, lee h, ng ay. multimodal deep learning. in: the 28th international conference on machine learning. 2011. p. 689   96.
 640. https://scholar.google.com/scholar?q=ogoe ha, visweswaran s, lu x, gopalakrishnan v. knowledge transfer via classification rules using functional mapping for integrative modeling of gene expression data. bmc bioinform. 2015. p. 1   15.
 641. https://scholar.google.com/scholar?q=oquab m, bottou l, laptev i, sivic j. learning and transferring mid-level image representations using convolutional neural networks. in: proceedings of the 2014 ieee conference on id161 and pattern recognition. 2013. p. 1717   24.
 642. https://scholar.google.com/scholar?q=pan sj, kwok jt, yang q. id21 via id84. in: proceedings of the 23rd national conference on artificial intelligence, vol. 2. 2008. p. 677   82.
 643. https://scholar.google.com/scholar?q=pan sj, ni x, sun jt, yang q, chen z. cross-domain sentiment classification via spectral feature alignment. in: proceedings of the 19th international conference on world wide web. 2010. p. 751   60.
 644. https://doi.org/10.1109/tkde.2009.191
 645. http://scholar.google.com/scholar_lookup?title=a survey on id21&author=sj. pan&author=q. yang&journal=ieee trans knowl data eng&volume=22&issue=10&pages=1345-1359&publication_year=2010
 646. https://scholar.google.com/scholar?q=pan w, liu nn, xiang ew, yang q. id21 to predict missing ratings via heterogeneous user feedbacks. in: proceedings of the 22nd international joint conference on artificial intelligence. 2011. p. 2318   23.
 647. https://scholar.google.com/scholar?q=pan w. xiang ew, liu nn, yang q. id21 in id185 for sparsity reduction. in: twenty-fourth aaai conference on artificial intelligence, vol. 1. 2010. p. 230   235.
 648. http://scholar.google.com/scholar_lookup?title=id20 via transfer component analysis&author=sj. pan&author=iw. tsang&author=jt. kwok&author=q. yang&journal=ieee trans neural netw&volume=22&issue=2&pages=199-210&publication_year=2009
 649. http://leon.bottou.org/papers/oquab-2014
 650. https://doi.org/10.1109/msp.2014.2347059
 651. http://scholar.google.com/scholar_lookup?title=visual id20: a survey of recent advances&author=vm. patel&author=r. gopalan&author=r. li&author=r. chellappa&journal=ieee signal process mag&volume=32&issue=3&pages=53-69&publication_year=2014
 652. http://www.ams.org/mathscinet-getitem?mr=3179981
 653. https://doi.org/10.1007/s10994-013-5375-2
 654. http://scholar.google.com/scholar_lookup?title=machine learning for targeted display advertising: id21 in action&author=c. perlich&author=b. dalessandro&author=t. raeder&author=o. stitelman&author=f. provost&journal=mach learn&volume=95&pages=103-127&publication_year=2014
 655. https://scholar.google.com/scholar?q=prettenhofer p, stein b. (2010) cross-language text classification using structural correspondence learning. in: proceedings of the 48th annual meeting of the association for computational linguistics. 2010. p. 1118   27.
 656. https://scholar.google.com/scholar?q=qi gj, aggarwal c, huang t. towards semantic knowledge propagation from text corpus to web images. in: proceedings of the 20th international conference on world wide web. p. 297   306.
 657. https://scholar.google.com/scholar?q=qiu g, liu b, bu j, chen c. expanding domain sentiment lexicon through double propagation. in: proceedings of the 21st international joint conference on artificial intelligence. p. 1199   204.
 658. https://scholar.google.com/scholar?q=quanz b, huan j. large margin transductive id21. in: proceedings of the 18th acm conference on information and knowledge management. 2009. p. 1327   36.
 659. https://scholar.google.com/scholar?q=raina r, battle a, lee h, packer b, ng ay. self-taught learning: id21 from unlabeled data. in: proceedings of the 24th international conference on machine learning. 2007. p. 759   66.
 660. https://doi.org/10.1007/s11263-013-0692-2
 661. http://scholar.google.com/scholar_lookup?title=exploring id21 approaches for head pose classification from multi-view surveillance images&author=an. rajagopal&author=r. subramanian&author=e. ricci&author=rl. vieriu&author=o. lanz&author=kr. ramakrishnan&author=n. sebe&journal=int j comput vis&volume=109&issue=1   2&pages=146-167&publication_year=2014
 662. https://scholar.google.com/scholar?q=romera-paredes b, aung msh, pontil m, bianchi-berthouze n, williams ac de c, watson p. id21 to account for idiosyncrasy in face and body expressions. in: proceedings of the 10th international conference on automatic face and gesture recognition (fg). 2013. p. 1   6.
 663. https://scholar.google.com/scholar?q=rosenstein mt, marx z, kaelbling lp, dietterich tg. to transfer or not to transfer. in: proceedings nips   05 workshop, inductive transfer. 10  years later. 2005. p. 1   4.
 664. https://scholar.google.com/scholar?q=roy s.d., mei t., zeng w., li s. social transfer: cross-domain id21 from social streams for media applications. in: proceedings of the 20th acm international conference on multimedia. p. 649   58.
 665. http://scholar.google.com/scholar_lookup?title=adapting visual category models to new domains&author=k. saenko&author=b. kulis&author=m. fritz&author=t. darrell&journal=comput vision eccv&volume=6314&pages=213-226&publication_year=2010
 666. http://scholar.google.com/scholar_lookup?title=an empirical analysis of id20 algorithms for genomic sequence analysis&author=g. schweikert&author=c. widmer&author=b. sch  lkopf&author=g. r  tsch&journal=adv neural inf process syst&volume=21&pages=1433-1440&publication_year=2009
 667. https://doi.org/10.1109/tsmcb.2012.2225102
 668. http://scholar.google.com/scholar_lookup?title=combating negative transfer from predictive distribution differences&author=cw. seah&author=ys. ong&author=iw. tsang&journal=ieee trans cybern&volume=43&issue=4&pages=1153-1165&publication_year=2013
 669. http://www.ams.org/mathscinet-getitem?mr=3454260
 670. https://doi.org/10.1109/tnnls.2014.2330900
 671. http://scholar.google.com/scholar_lookup?title=id21 for visual categorization: a survey&author=l. shao&author=f. zhu&author=x. li&journal=ieee trans neural netw learn syst&volume=26&issue=5&pages=1019-1034&publication_year=2014
 672. https://scholar.google.com/scholar?q=shawe-taylor j, cristianini n. kernel methods for pattern analysis. cambridge: cambridge university press; 2004.
 673. https://scholar.google.com/scholar?q=shi x, liu q, fan w, yu ps, zhu r. id21 on heterogeneous feature spaces via spectral transformation. in: 2010 ieee international conference on data mining. 2010. p. 1049   1054.
 674. https://scholar.google.com/scholar?q=shi y, sha f. information-theoretical learning of discriminative clusters for unsupervised id20. in: proceedings of the 29th international conference on machine learning. 2012. p. 1   8.
 675. http://www.ams.org/mathscinet-getitem?mr=1795598
 676. https://doi.org/10.1016/s0378-3758(00)00115-4
 677. http://www.emis.de/math-item?0958.62011
 678. http://scholar.google.com/scholar_lookup?title=improving predictive id136 under covariate shift by weighting the log-likelihood function&author=h. shimodaira&journal=j stat plan inf&volume=90&issue=2&pages=227-244&publication_year=2000
 679. https://doi.org/10.1109/tse.2012.43
 680. http://scholar.google.com/scholar_lookup?title=reducing features to improve code change-based bug prediction&author=s. shivaji&author=ej. whitehead&author=r. akella&author=s. kim&journal=ieee trans softw eng&volume=39&issue=4&pages=552-569&publication_year=2013
 681. https://doi.org/10.1109/tkde.2009.126
 682. http://scholar.google.com/scholar_lookup?title=bregman divergence-based id173 for transfer subspace learning&author=s. si&author=d. tao&author=b. geng&journal=ieee trans knowl data eng&volume=22&issue=7&pages=929-942&publication_year=2010
 683. http://scholar.google.com/scholar_lookup?title=contextualizing id164 and classification&author=z. song&author=q. chen&author=z. huang&author=y. hua&author=s. yan&journal=ieee trans pattern anal mach intell&volume=37&issue=1&pages=13-27&publication_year=2011
 684. http://www.ams.org/mathscinet-getitem?mr=1883281
 685. http://www.emis.de/math-item?1009.68143
 686. http://scholar.google.com/scholar_lookup?title=on the influence of the kernel on the consistency of support vector machines&author=i. steinwart&journal=jmlr&volume=2&pages=67-93&publication_year=2001
 687. http://www.ams.org/mathscinet-getitem?mr=2534874
 688. http://www.emis.de/math-item?1235.68196
 689. http://scholar.google.com/scholar_lookup?title=id21 for id23 domains: a survey&author=me. taylor&author=p. stone&journal=jmlr&volume=10&pages=1633-1685&publication_year=2009
 690. https://scholar.google.com/scholar?q=tommasi t, caputo b. the more you know, the less you learn: from knowledge transfer to id62 of object categories. bmvc. 2009;1   11.
 691. http://scholar.google.com/scholar_lookup?title=safety in numbers: learning categories from few examples with multi model knowledge transfer&author=t. tommasi&author=f. orabona&author=b. caputo&journal=ieee conf comput vision pattern recog&volume=2010&pages=3081-3088&publication_year=2010
 692. http://www.cse.ust.hk/tl/
 693. http://tommasit.wix.com/datl14tutorial
 694. http://scholar.google.com/scholar_lookup?title=principles of risk minimization for learning theory&author=v. vapnik&journal=adv neural inf process syst&volume=4&pages=831-838&publication_year=1992
 695. https://scholar.google.com/scholar?q=vedaldi a, gulshan v, varma m, zisserman a. multiple kernels for id164. in: 2009 ieee 12th international conference on id161. 2009. p. 606   13.
 696. https://scholar.google.com/scholar?q=vincent p, larochelle h, bengio y, manzagol pa. extracting and composing robust features with denoising autoencoders. in: proceedings of the 25th international conference on machine learning. 2008. p. 1096   103.
 697. http://scholar.google.com/scholar_lookup?title=inferring a semantic representation of text via crosslanguage correlation analysis&author=a. vinokourov&author=j. shawe-taylor&author=n. cristianini&journal=adv neural inf proces syst&volume=15&pages=1473-1480&publication_year=2002
 698. https://scholar.google.com/scholar?q=wang c, mahadevan s. heterogeneous id20 using manifold alignment. in: proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541   46.
 699. https://scholar.google.com/scholar?q=wang g, hoiem d, forsyth da. building text features for object image classification. in: 2009 ieee conference on id161 and pattern recognition. 2009. p. 1367   74.
 700. https://scholar.google.com/scholar?q=wang h, klaser a, schmid c, liu cl (2011) action recognition by dense trajectories. in: ieee 2011 conference on id161 and pattern recognition. 2011. p. 3169   76.
 701. https://scholar.google.com/scholar?q=wei b, pal c (2010) cross-lingual adaptation: an experiment on sentiment classifications. in: proceedings of the acl 2010 conference short papers. 2010. p. 258   62.
 702. https://scholar.google.com/scholar?q=wei b, pal c (2011) heterogeneous id21 with rbms. in: proceedings of the twenty-fifth aaai conference on artificial intelligence. 2011. p. 531   36.
 703. http://www.emis.de/math-item?1235.68204
 704. http://scholar.google.com/scholar_lookup?title=distance metric learning for large margin nearest neighbor classification&author=kq. weinberger&author=lk. saul&journal=jmlr&volume=10&pages=207-244&publication_year=2009
 705. http://scholar.google.com/scholar_lookup?title=multitask learning in computational biology&author=c. widmer&author=g. ratsch&journal=jmlr&volume=27&pages=207-216&publication_year=2012
 706. https://doi.org/10.1136/amiajnl-2013-002162
 707. http://scholar.google.com/scholar_lookup?title=a study in id21: leveraging data from multiple hospitals to enhance hospital-specific predictions&author=j. wiens&author=j. guttag&author=ej. horvitz&journal=j am med inform assoc&volume=21&issue=4&pages=699-706&publication_year=2013
 708. http://www.emis.de/math-item?1076.68555
 709. http://scholar.google.com/scholar_lookup?title=data mining, practical machine learning tools and techniques&author=ih. witten&author=e. frank&publication_year=2011
 710. https://scholar.google.com/scholar?q=wu x, xu d, duan l, luo j (2011) action recognition using context and appearance distribution features. in: ieee 2011 conference on id161 and pattern recognition. 2011. p. 489   96.
 711. https://scholar.google.com/scholar?q=xia r, zong c. a pos-based ensemble model for cross-domain sentiment classification. in: proceedings of the 5th international joint conference on natural language processing. 2011. p. 614   22.
 712. https://doi.org/10.1109/mis.2013.27
 713. http://scholar.google.com/scholar_lookup?title=feature ensemble plus sample selection: id20 for sentiment classification&author=r. xia&author=c. zong&author=x. hu&author=e. cambria&journal=ieee intell syst&volume=28&issue=3&pages=10-18&publication_year=2013
 714. https://scholar.google.com/scholar?q=xiao m, guo y. semi-supervised kernel matching for id20. in: proceedings of the twenty-sixth aaai conference on artificial intelligence. 2012. p. 1183   89.
 715. https://scholar.google.com/scholar?q=xie m, jean n, burke m, lobell d, ermon s. id21 from deep features for remote sensing and poverty mapping. in: proceedings 30th aaai conference on artificial intelligence. 2015. p. 1   10.
 716. https://scholar.google.com/scholar?q=yang j, yan r, hauptmann ag. cross-domain video concept detection using adaptive id166s. in: proceedings of the 15th acm international conference on multimedia. 2007. p. 188   97.
 717. https://scholar.google.com/scholar?q=yang l, jing l, yu j, ng mk. learning transferred weights from co-occurrence data for heterogeneous id21. ieee trans neural netw learn syst. 2015;pp(99):1   14.
 718. https://scholar.google.com/scholar?q=yang q, chen y, xue gr, dai w, yu y. heterogeneous id21 for image id91 via the social web. in: proceedings of the joint conference of the 47th annual meeting of the acl, vol. 1. 2009. p. 1   9.
 719. https://scholar.google.com/scholar?q=yao y, doretto g. boosting for id21 with multiple sources. in: proceedings of the ieee computer society conference on id161 and pattern recognition. 2010. p. 1855   62.
 720. http://www.cse.ust.hk/~yinz/
 721. https://scholar.google.com/scholar?q=zhang y, cao b, yeung d. multi-domain id185. in: proceedings of the 26th conference on uncertainty in artificial intelligence. 2010. p. 725   32.
 722. https://scholar.google.com/scholar?q=zhang y, yeung dy. transfer metric learning by learning task relationships. in: proceedings of the 16th acm sigkdd international conference on knowledge discovery and data mining. 2010. p. 1199   208.
 723. https://scholar.google.com/scholar?q=zhao l, pan sj, xiang ew, zhong e, lu z, yang q. active id21 for cross-system recommendation. in: proceedings of the 27th aaai conference on artificial intelligence. 2013. p. 1205   11.
 724. https://scholar.google.com/scholar?q=zhong e, fan w, peng j, zhang k, ren j, turaga d, verscheure o. cross domain distribution adaptation via kernel mapping. in: proceedings of the 15th acm sigkdd. 2009. p. 1027   36.
 725. https://scholar.google.com/scholar?q=zhou jt, pan s, tsang iw, yan y. hybrid heterogeneous id21 through deep learning. in: proceedings of the national conference on artificial intelligence, vol. 3. 2014. p. 2213   20.
 726. https://scholar.google.com/scholar?q=zhou jt, tsang iw, pan sj tan m. heterogeneous id20 for multiple classes. in: international conference on artificial intelligence and statistics. 2014. p. 1095   103.
 727. https://scholar.google.com/scholar?q=zhu y, chen y, lu z, pan s, xue g, yu y, yang q. heterogeneous id21 for image classification. in: proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304   9.
 728. http://creativecommons.org/licenses/by/4.0/
 729. mailto:kweiss6@fau.edu
 730. https://crossmark.crossref.org/dialog/?doi=10.1186/s40537-016-0043-6
 731. https://www.springer.com/journal/40537/about
 732. https://s100.copyright.com/appdispatchservlet?publishername=springernature&orderbeanreset=true&ordersource=springerlink&copyright=the+author(s)&author=karl+weiss,+taghi+m.+khoshgoftaar,+dingding+wang&issuenum=1&contentid=10.1186/s40537-016-0043-6&publicationdate=2016&startpage=9&volumenum=3&title=a+survey+of+transfer+learning&imprint=the+author(s)&publication=2196-1115&oa=cc+by
 733. https://link.springer.com/article/10.1186/s40537-016-0043-6#citeas
 734. https://citation-needed.springer.com/v2/references/10.1186/s40537-016-0043-6?format=refman&flavour=citation
 735. https://citation-needed.springer.com/v2/references/10.1186/s40537-016-0043-6?format=endnote&flavour=citation
 736. https://citation-needed.springer.com/v2/references/10.1186/s40537-016-0043-6?format=bibtex&flavour=citation
 737. https://link.springer.com/sharelink/10.1186/s40537-016-0043-6
 738. https://link.springer.com/content/pdf/10.1186/s40537-016-0043-6.pdf
 739. https://link.springer.com/content/pdf/10.1186/s40537-016-0043-6.pdf
 740. https://link.springer.com/article/10.1186/s40537-016-0043-6#citeas
 741. https://citation-needed.springer.com/v2/references/10.1186/s40537-016-0043-6?format=refman&flavour=citation
 742. https://citation-needed.springer.com/v2/references/10.1186/s40537-016-0043-6?format=endnote&flavour=citation
 743. https://citation-needed.springer.com/v2/references/10.1186/s40537-016-0043-6?format=bibtex&flavour=citation
 744. https://link.springer.com/sharelink/10.1186/s40537-016-0043-6
 745. https://link.springer.com/article/10.1186/s40537-016-0043-6#enumeration
 746. https://link.springer.com/article/10.1186/s40537-016-0043-6#abs1
 747. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec1
 748. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec2
 749. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec3
 750. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec11
 751. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec17
 752. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec18
 753. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec19
 754. https://link.springer.com/article/10.1186/s40537-016-0043-6#notes
 755. https://link.springer.com/article/10.1186/s40537-016-0043-6#sec20
 756. https://link.springer.com/article/10.1186/s40537-016-0043-6#bib1
 757. https://link.springer.com/article/10.1186/s40537-016-0043-6#copyrightinformation
 758. https://link.springer.com/article/10.1186/s40537-016-0043-6#authorsandaffiliations
 759. https://link.springer.com/article/10.1186/s40537-016-0043-6#aboutcontent
 760. https://link.springer.com/siteedition/link?previousurl=/article/10.1186/s40537-016-0043-6&id=siteedition-academic-link
 761. https://link.springer.com/siteedition/rd?previousurl=/article/10.1186/s40537-016-0043-6&id=siteedition-corporate-link
 762. https://link.springer.com/
 763. https://link.springer.com/impressum
 764. https://link.springer.com/termsandconditions
 765. https://link.springer.com/privacystatement
 766. https://link.springer.com/cookiepolicy
 767. https://link.springer.com/accessibility
 768. https://link.springer.com/contactus
 769. https://www.springernature.com/
 770. https://www.springernature.com/

   hidden links:
 772. https://link.springer.com/
