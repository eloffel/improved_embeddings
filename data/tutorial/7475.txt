algorithms for 

playing and solving 

games*

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

  z e r o - s u m  
t e  
i c   g a m e s   o f
i n f o r m a t

i o n

*   t w o   p l a y e r
d i s c r e t e   f i n i
d e t e r m i n i s t
p e r

f e c t

 

 

small print

slide 1

note to other teachers and users of these slides. andrew would be delighted if you found this source 
material useful in giving your own lectures. feel free to use these slides verbatim, or to modify them to fit 
your own needs. powerpoint originals are available. if you make use of a significant portion of these 
slides in your own lecture, please include this message, or the following link to the source repository of 
andrew   s tutorials: http://www.cs.cmu.edu/~awm/tutorials . comments and corrections gratefully received. 

overview

    definition of games and game terminology
    game trees and game-theoretic values
    computing game-theoretic values with recursive 

minimax.

    other ways to compute game-theoretic value: dynamic 

programming copes with stalemates.

    alpha-beta algorithm (good news.. it   s not really as fiddly 

as is looks)

    playing games in real-time
    non-determinism

slide 2

2-player zero-sum discrete finite 

deterministic games of perfect information
what do these terms mean?
    two player: duh!
    zero-sum: in any outcome of any game, player a   s gains 

equal player b   s losses.  (doesn   t mean fairness:     on average, two equal 
players will win or lose equal amounts    not necessary for zero-sum.)

    discrete: all game states and decisions are discrete values.
    finite: only a finite number of states and decisions.
    deterministic: no chance (no die rolls).
    games: see next page
    perfect information: both players can see the state, and 

each decision is made sequentially (no simultaneous moves).

slide 3

which of these are: 2-player zero-sum discrete finite 

deterministic games of perfect information

    two player: duh!
    zero-sum: in any outcome of any 
game, player a   s gains equal player b   s 
losses. 
    discrete: all game states and decisions 
are discrete values.
    finite: only a finite number of states and 
decisions.
    deterministic: no chance (no die 
rolls).
    games: see next page
    perfect information: both players 
can see the state, and each decision is 
made sequentially (no simultaneous 
moves).

slide 4

which of these are: 2-player zero-sum discrete finite 

deterministic games of perfect information

n  
e
r m a ti o

n

d
h i d
i n f o

n o t fi n it e

s ti c

a

h

c

s t o

r

e

y

e   p l a

o n

r

e

y

m u lti p l a

    two player: duh!
    zero-sum: in any outcome of any 
game, player a   s gains equal player b   s 
losses. 
    discrete: all game states and decisions 
are discrete values.
    finite: only a finite number of states and 
decisions.
    deterministic: no chance (no die 
rolls).
    games: see next page
    perfect information: both players 
can see the state, and each decision is 
made sequentially (no simultaneous 
moves).

b l e  

a

r

o

b
v i o

r
a

e

s  i m p
o l v
h
v
a n i m a l  b e

i n

slide 5

definition

a two-player zero-sum discrete finite deterministic game of perfect information is a 
quintuplet:  ( s , i , succs , t , v ) where

s

i
succs

t

v

= a finite set of states (note: state includes information 

sufficient to deduce who is due to move)

= the initial state
= a function which takes a state as input and returns a set of 

possible next states available to whoever is due to move
= a subset of s.  it is the terminal states: the set of states at 

which the game is over

= a mapping from terminal states to real numbers.  it is the 
amount that a wins from b. (if it   s negative a loses money 
to b).

convention: assume player a  moves first.
for convenience: assume turns alternate.

slide 6

nim: informal description

1. we begin with a number of piles of matches.
2.
3.

in one   s turn one may remove any number of matches from one pile.
the last person to remove a match loses.

in ii-nim, one begins with two piles, each with two matches   

s =

( _ , _ )-a
( i  , _ )-a
( ii , _ )-a

( _ , i )-a
( i  , i )-a
( ii , i )-a

( _ , ii )-a
( i  , ii )-a
( ii , ii )-a

( _ , _ )-b
( i  , _ )-b
( ii , _ )-b

( _ , i )-b
( i  , i )-b
( ii , i )-b

( _ , ii )-b
( i  , ii )-b
( ii , ii )-b

slide 7

nim: informal description
t h e   s t a t e s   a r e  
  s o m e   o f
  m a k e  
1. we begin with a number of piles of matches.
  p il e   n e v e r
 
.
- a )
r y ,
ii , _ )
(
- a   a n d  
t
in one   s turn one may remove any number of matches from the pile.
2.
  l e f
( e . g .
i p t i o n  
the last person to remove a match loses.
3.

b y   s y m m e t
)
( _ , ii
i c k :
( e . g .
a   c o m m o n  
s t a t e   b y   s o m e   c a n o n i c a l   d e s c r
 
i v i a ll y   e q u i v a l e n t
r
t
in ii-nim, one begins with two matches, each with two piles   
i g h t
r
t h a n  
l a r g e r
s =

t h e m   o n e  

( _ , _ )-a

t

 

.

 

 

 

r

)

( _ , i )-a
( i  , i )-a

( _ , ii )-a
( i  , ii )-a
( ii , ii )-a

( _ , _ )-b

( _ , i )-b
( i  , i )-b

( _ , ii )-b
( i  , ii )-b
( ii , ii )-b

slide 8

s

i
succs

=
=

= a finite set of states (note: 
state includes information 
sufficient to deduce who is 
due to move)
the initial state

a function which takes a 
state as input and returns a 
set of possible next states 
available to whoever is due 
to move

t
v

=
=

a subset of s.  it is the 
terminal states
maps from terminal states 
to real numbers.  it is the 
amount that a wins from b. 

ii-nim

( _ , _ )-a ( _ , i )-a ( _ , ii )-a ( i , i )-a ( i , ii )-a ( ii , ii )-a 
( _ , _ )-b ( _ , i )-b ( _ , ii )-b ( i , i )-b ( i , ii )-b ( ii , ii )-b 
( ii , ii )-a
succs(_,i)-a = { (_,_)-b }
succs(_,ii)-a = { (_,_)-b , (_,i)-b }
succs(i,i)-a = { (_,i)-b }
succs(i,ii)-a = { (_,i)-b (_,ii)-b (i,i)-b} succs(i,ii)-b = { (_,i)-a , (_,ii)-a (i,i)-a }
succs(ii,ii)-a = { (_,ii)-b , (i,ii)-b }
( _ , _ )-a
v( _ , _ )-a = +1

succs(ii,ii)-b = { (_,ii)-a , (i,ii)-a }
( _ , _ )-b 
v( _ , _ )-b = -1

succs(_,i)-b = { (_,_)-a }
succs(_,ii)-b = { (_,_)-a , (_,i)-a }
succs(i,i)-b = { (_,i)-a }

slide 9

ii-nim game 

tree

s

i

succs
=

(ii ii) a

t
v

=

=

=

=

( _ , _ )-a ( _ , i )-a ( _ , ii )-a ( i , i )-a ( i , ii )-a ( ii , ii )-a 
( _ , _ )-b ( _ , i )-b ( _ , ii )-b ( i , i )-b ( i , ii )-b ( ii , ii )-b 
( ii , ii )-a
succs(_,i)-a = { (_,_)-b }
succs(_,ii)-a = { (_,_)-b , (_,i)-b }
succs(i,i)-a = { (_,i)-b }
succs(i,ii)-a = { (_,i)-b (_,ii)-b (i,i)-b} succs(i,ii)-b = { (_,i)-a , (_,ii)-a (i,i)-a }
succs(ii,ii)-a = { (_,ii)-b , (i,ii)-b }
( _ , _ )-a
v( _ , _ )-a = +1

succs(ii,ii)-b = { (_,ii)-a , (i,ii)-a }
( _ , _ )-b 
v( _ , _ )-b = -1

succs(_,i)-b = { (_,_)-a }
succs(_,ii)-b = { (_,_)-a , (_,i)-a }
succs(i,i)-b = { (_,i)-a }

(i  ii) b

(- ii) b

(- ii) a

(i i) a

(- i) a

(- i) a

(- -) a +1

(- i) b

(- -) b -1

(- i) b

(- -) b -1

(- -) b -1

(- -) a +1

(- -) a +1

slide 10

ii-nim game 

tree

s

i

succs
=

(ii ii) a

t
v

=

=

=

=

( _ , _ )-a ( _ , i )-a ( _ , ii )-a ( i , i )-a ( i , ii )-a ( ii , ii )-a 
( _ , _ )-b ( _ , i )-b ( _ , ii )-b ( i , i )-b ( i , ii )-b ( ii , ii )-b 
( ii , ii )-a
succs(_,i)-a = { (_,_)-b }
succs(_,ii)-a = { (_,_)-b , (_,i)-b }
succs(i,i)-a = { (_,i)-b }
succs(i,ii)-a = { (_,i)-b (_,ii)-b (i,i)-b} succs(i,ii)-b = { (_,i)-a , (_,ii)-a (i,i)-a }
succs(ii,ii)-a = { (_,ii)-b , (i,ii)-b }
( _ , _ )-a
v( _ , _ )-a = +1

succs(ii,ii)-b = { (_,ii)-a , (i,ii)-a }
( _ , _ )-b 
v( _ , _ )-b = -1

succs(_,i)-b = { (_,_)-a }
succs(_,ii)-b = { (_,_)-a , (_,i)-a }
succs(i,i)-b = { (_,i)-a }

(i  ii) b

(- ii) b

(- ii) a

(i i) a

(- i) a

(- i) a

(- -) a +1

(- i) b +1

(- -) b -1

(- i) b

(- -) b -1

(- -) b -1

(- -) a +1

(- -) a +1

slide 11

ii-nim game 

tree

s

i

succs
=

(ii ii) a

t
v

=

=

=

=

( _ , _ )-a ( _ , i )-a ( _ , ii )-a ( i , i )-a ( i , ii )-a ( ii , ii )-a 
( _ , _ )-b ( _ , i )-b ( _ , ii )-b ( i , i )-b ( i , ii )-b ( ii , ii )-b 
( ii , ii )-a
succs(_,i)-a = { (_,_)-b }
succs(_,ii)-a = { (_,_)-b , (_,i)-b }
succs(i,i)-a = { (_,i)-b }
succs(i,ii)-a = { (_,i)-b (_,ii)-b (i,i)-b} succs(i,ii)-b = { (_,i)-a , (_,ii)-a (i,i)-a }
succs(ii,ii)-a = { (_,ii)-b , (i,ii)-b }
( _ , _ )-a
v( _ , _ )-a = +1

succs(ii,ii)-b = { (_,ii)-a , (i,ii)-a }
( _ , _ )-b 
v( _ , _ )-b = -1

succs(_,i)-b = { (_,_)-a }
succs(_,ii)-b = { (_,_)-a , (_,i)-a }
succs(i,i)-b = { (_,i)-a }

(i  ii) b

(- ii) b

(- ii) a

(i i) a

(- i) a

(- i) a

(- -) a +1

(- i) b +1

(- -) b -1

(- i) b +1

(- -) b -1

(- -) b -1

(- -) a +1

(- -) a +1

slide 12

ii-nim game 

tree

s

i

succs
=

(ii ii) a

t
v

=

=

=

=

( _ , _ )-a ( _ , i )-a ( _ , ii )-a ( i , i )-a ( i , ii )-a ( ii , ii )-a 
( _ , _ )-b ( _ , i )-b ( _ , ii )-b ( i , i )-b ( i , ii )-b ( ii , ii )-b 
( ii , ii )-a
succs(_,i)-a = { (_,_)-b }
succs(_,ii)-a = { (_,_)-b , (_,i)-b }
succs(i,i)-a = { (_,i)-b }
succs(i,ii)-a = { (_,i)-b (_,ii)-b (i,i)-b} succs(i,ii)-b = { (_,i)-a , (_,ii)-a (i,i)-a }
succs(ii,ii)-a = { (_,ii)-b , (i,ii)-b }
( _ , _ )-a
v( _ , _ )-a = +1

succs(ii,ii)-b = { (_,ii)-a , (i,ii)-a }
( _ , _ )-b 
v( _ , _ )-b = -1

succs(_,i)-b = { (_,_)-a }
succs(_,ii)-b = { (_,_)-a , (_,i)-a }
succs(i,i)-b = { (_,i)-a }

(i  ii) b

(- ii) b

(- ii) a +1

(i i) a +1

(- i) a -1

(- i) a -1

(- -) a +1

(- i) b +1

(- -) b -1

(- i) b +1

(- -) b -1

(- -) b -1

(- -) a +1

(- -) a +1

slide 13

ii-nim game 

tree

s

i

succs
=

(ii ii) a

t
v

=

=

=

=

( _ , _ )-a ( _ , i )-a ( _ , ii )-a ( i , i )-a ( i , ii )-a ( ii , ii )-a 
( _ , _ )-b ( _ , i )-b ( _ , ii )-b ( i , i )-b ( i , ii )-b ( ii , ii )-b 
( ii , ii )-a
succs(_,i)-a = { (_,_)-b }
succs(_,ii)-a = { (_,_)-b , (_,i)-b }
succs(i,i)-a = { (_,i)-b }
succs(i,ii)-a = { (_,i)-b (_,ii)-b (i,i)-b} succs(i,ii)-b = { (_,i)-a , (_,ii)-a (i,i)-a }
succs(ii,ii)-a = { (_,ii)-b , (i,ii)-b }
( _ , _ )-a
v( _ , _ )-a = +1

succs(ii,ii)-b = { (_,ii)-a , (i,ii)-a }
( _ , _ )-b 
v( _ , _ )-b = -1

succs(_,i)-b = { (_,_)-a }
succs(_,ii)-b = { (_,_)-a , (_,i)-a }
succs(i,i)-b = { (_,i)-a }

(i  ii) b -1

(- ii) b -1

(- ii) a +1

(i i) a +1

(- i) a -1

(- i) a -1

(- -) a +1

(- i) b +1

(- -) b -1

(- i) b +1

(- -) b -1

(- -) b -1

(- -) a +1

(- -) a +1

slide 14

ii-nim game 

tree

s

i

succs
=

(ii ii) a -1

t
v

=

=

=

=

( _ , _ )-a ( _ , i )-a ( _ , ii )-a ( i , i )-a ( i , ii )-a ( ii , ii )-a 
( _ , _ )-b ( _ , i )-b ( _ , ii )-b ( i , i )-b ( i , ii )-b ( ii , ii )-b 
( ii , ii )-a
succs(_,i)-a = { (_,_)-b }
succs(_,ii)-a = { (_,_)-b , (_,i)-b }
succs(i,i)-a = { (_,i)-b }
succs(i,ii)-a = { (_,i)-b (_,ii)-b (i,i)-b} succs(i,ii)-b = { (_,i)-a , (_,ii)-a (i,i)-a }
succs(ii,ii)-a = { (_,ii)-b , (i,ii)-b }
( _ , _ )-a
v( _ , _ )-a = +1

succs(ii,ii)-b = { (_,ii)-a , (i,ii)-a }
( _ , _ )-b 
v( _ , _ )-b = -1

succs(_,i)-b = { (_,_)-a }
succs(_,ii)-b = { (_,_)-a , (_,i)-a }
succs(i,i)-b = { (_,i)-a }

(i  ii) b -1

(- ii) b -1

(- ii) a +1

(i i) a +1

(- i) a -1

(- i) a -1

(- -) a +1

(- i) b +1

(- -) b -1

(- i) b +1

(- -) b -1

(- -) b -1

(- -) a +1

(- -) a +1

slide 15

game theoretic value

game theoretic value (also know as the minimax value) of a state is:
   the value of a terminal that will be reached assuming both players use 

their optimal strategy.   

easy to fill in the tree bottom up to find minimax values of all states:

let d = max depth of game tree
for i = d to 1

for each node n at depth i
if n is a terminal node

mmv(n) = v(n)

= 1 + maximum number of 
moves in any possible game

else if player a is due to move at node n
n
)'(mmv

n
)(mmv

=

max
n    
'
succs
(

n

)

else (player b must be due to move and..)

n
)(mmv

=

min
succs
(

n    
'

n

)

n
)'(mmv

this must   ve been 
defined because it is 
at depth i+1

ditto

slide 16

game theoretic value

game theoretic value (also know as the minimax value) of a state is:
   the value of a terminal that will be reached assuming both players use 

e  

easy to fill in the tree bottom up to find minimax values of all states:

s  i n  t h
b d )
s   s

p

a

s

?

e

c

= 1 + maximum number of 
moves in any possible game

e
v
their optimal strategy.   
d   d   m o
e   o (
n
r   b   a
c
a
c t o
p
d   s
g  f a
n
s  ti m e   a
h i n
g   w it h  l e
let d = max depth of game tree
c
n
w it h   b r
e
for i = d to 1
k
a m e  t h i n
a m e  t h i s  t a
e   s
o  t h
g
n   w e   d
mmv(n) = v(n)
c a

for each node n at depth i
if n is a terminal node

a

else if player a is due to move at node n
n
)'(mmv

n
)(mmv

=

max
n    
'
succs
(

n

)

else (player b must be due to move and..)

n
)(mmv

=

min
succs
(

n    
'

n

)

n
)'(mmv

this must   ve been 
defined because its 
at depth i+1

ditto

slide 17

minimax algorithm

is it really necessary to explicitly store the whole tree in memory?  of 
course not.  we can do the same trick that depth first search and use 
only o(d) space

minimaxvalue(s)=

if (s is a terminal)
return v(s)

else

let { s1, s2,     sk } = succs(s)
let vi = minimaxvalue(si) for each i
if player-to-move(s) = a
v
i
}
v
i
}

max
i
k
2,1{
k   
min
i
k
2,1{
k   

return 

return

else

slide 18

    what if there are loops 
possible in the game?

questions

minimaxvalue(s)=

if (s is a terminal)
return v(s)

else

let { s1, s2,     sk } = succs(s)
let vi = minimaxvalue(si) for each i
if player-to-move(s) = a

else

max
return 
i
k
2,1{
k   
min
i
k
2,1{
k   

return

v
i
}
v
i
}

    this is a depth-first search 
algorithm.  would a breadth-
first version be possible?  
how would it work?

slide 19

questions

minimaxvalue(s)=

if (s is a terminal)
return v(s)

else

let { s1, s2,     sk } = succs(s)
let vi = minimaxvalue(si) for each i
if player-to-move(s) = a

else

max
return 
i
k
2,1{
k   
min
i
k
2,1{
k   

return

v
i
}
v
i
}

    this is a depth-first search 
algorithm.  would a breadth-
first version be possible?  
how would it work?

    what if there are loops 
possible in the game?
   

is our recursive-minimax 
guaranteed to succeed?
is our recursive-minimax 
guaranteed to fail?

   

    what problems do loops 

cause for our definition 
of minimax value (i.e. 
game-theoretic value)?

    how could we fix our 

recursive minimax 
program?

slide 20

id145

say you have a game with n states.  the length of the 
game is usually l moves.  there are b successors of each 
state.
minimax requires o(bl) states expanded.
this is best-case as well as worst-case (unlike dfs for 
simple search problems, which in best-case could be o(l)).
what if the number of states is smaller than bl?  e.g.. in 
chess, bl=10120, but n= a mere 1040
id145 is a better method in those cases, if 
you can afford the memory.
dp costs only o(nl)

slide 21

dp for chess endgames

suppose one has only, say, 4 pieces in total left on the board. 
with enough compute power you can compute, for all such 
positions, whether the position is a win for black, white, or a 
draw.

assume n such positions.

1. with each state, associate an integer.  a state code, so there   s a 
1-1 mapping between board positions and integers from 0   n-1.
2. make a big array (2 bits per array entry) of size n.  each element 

in the array may have one of three values: 
?:  we don   t know who wins from this state
w:  we know white   s won from here
b:  we know black   s won from here

   
   
   

slide 22

dp for chess endgames (ctd)

3. mark all terminal states with their values (w or b)
4.
look through all states that remain marked with ?.

for states in which w is about to move:

if all successor states are marked b, mark the current state as b.
if any successor state is marked w, mark the current state as w.
else leave current state unchanged.

for states in which b is about to move:

if all successor states are marked w, mark the current state as w.
if any successor state is marked b, mark the current state as b.
else leave current state unchanged

   
   
   

   
   
   

5.

6.

goto 4, but stop when one whole iteration of 4 produces no 
changes.
any state remaining at ? is a state from which no-one can force a 
win.

slide 23

suppose you knew that the only possible outcomes of the 
game were -1 and 1.  what computation could be saved?

(ii ii) a

(i  ii) b

(- ii) b

(- ii) a

(i i) a

(- i) a

(- i) a

(- -) a +1

(- i) b

(- -) b -1

(- i) b

(- -) b -1

(- -) b -1

(- -) a +1

(- -) a +1

slide 24

suppose you knew that the only possible outcomes of the 
game were -1 and 1.  what computation could be saved?

(ii ii) a

(i  ii) b

(- ii) b

(- ii) a

(i i) a

(- i) a

(- i) a

(- -) a +1

(- i) b

(- -) b -1

(- i) b

(- -) b

(- -) b -1

(- -) a +1

(- -) a +1

answer: in general a lot (though not much here).  if any successor is a forced 
win for the current player, don   t bother with expanding further successors.
what if you didn   t know the range of possible outcome values?  we   ll see 
that this is an important question.

slide 25

how can you cut-off 
with arbitrary terminal 

values?
(- ii) b

(ii ii) a

(i  ii) b

(- ii) a

(i i) a

(- i) a

(- i) a

(- -) a +3.7

(- i) b

(- -) b -1.9

(- i) b

(- -) b -0.3

(- -) b -8.1

(- -) a +2.4

(- -) a +0.08

slide 26

how can you cut-off 
with arbitrary terminal 

values?
(- ii) b

(ii ii) a

(i  ii) b

(- ii) a

(i i) a

(- i) a

(- i) a

(- -) a +3.7

(- i) b

(- -) b -1.9

(- i) b

(- -) b -0.3

(- -) b -8.1

(- -) a +2.4

(- -) a +0.08

just do the depth first search as normal, but when you discover something that 
means your parent would definitely not choose you, don   t bother with the rest of 
in fact, it   s not just your parent you should worry about, but any of your ancestors.

your successors.

slide 27

an ancestor causing cut-off

(    )-a

(    )-b

+2

( * )-a
+1

(    )-b

(    )-a

(    )-a

(    )-b

(    )-b

(    )-a

suppose we   ve so far done a full depth first search, expanding left-most 
successors first, and have arrived at the node marked * (and 
discovered its value is +1).
what can we cut off in the rest of the search, and why?
general rule. we can be sure a node will not be visited if we   re sure 
that either player has a better alternative at any ancestor of that node.

slide 28

the general cutoff rule

in example: let    = max(v1, v3, 
v5).  if min(v6, v7)     , then we can 
be certain that it is worthless 
searching the tree from the 
current node or the sibling on its 
right.
in general: if at a b-move node, 
let    = max of all a   s choices 
expanded on current path.  let   
= min of b   s choices, including 
those at current node.  cutoff is  
         .
in general: converse rule at an 
a-move node.

(  )-a

v2

(  )-a

v1

v3

(  )-b

?

?

?

(  )-b

?

?

v4

(  )-a

v5

v6

(  )-b

?

v7

current
node

slide 29

alpha-beta pruning* (from russell)

function max-value (s,  ,  )
inputs:

s: current state in game, a about to play
  : best score (highest) for a along path to s
  : best score (lowest) for b along path to s

output: min(   , best-score (for a) available from s)

if ( s is a terminal state )
then return ( terminal value of s )
else for each s    in succ(s)

   := max(    , min-value(s   ,  ,  ))
if (           ) then return   

return   

function min-value(s   ,  ,  )
output: max(   , best-score (for b) available from s )

if ( s is a terminal state )
then return ( terminal value of s)
else for each s    in succs(s)

   := min(    , max-value(s   ,  ,  ))
if (          ) then return   

return   

 

t o   a m e y a g u j a r
i e r
l
  a n   e a r
t h a n k s  
i n g   o u t
  t h e   v e r s i o n  
  p o i n t
f o r
t y p o   h e r e .
t h e  
i s  
y o u   n o w   s e e  
  v e r s i o n .
r e c t
c o r

*assumes moves are alternate

slide 30

how useful is alpha-beta?

what is the best possible case performance of alpha beta?  suppose 
that you were very lucky in the order in which you tried all the node 
successors.  how much of the tree would you examine?
in the best case, the number of nodes you need to search in the tree is 
o(bd/2)   the square root of the recursive minimax cost.
questions:

does alpha-beta behave sensibly with loops?
what can we do about large realsized games with huge numbers of 
states (e.g. chess)?

slide 31

game-playing and game-solving

two very different activities.
so far, we have been solely concerned with finding the true game-
theoretic value of a state.
but what do real chess-playing programs do?
they have a couple of interesting features that the search and planning 
problems we   ve discussed to date on this course don   t have:
    they cannot possibly find guaranteed solution.
    they must make their decisions quickly, in real time.
    it is not possible to pre-compute a solution.
the overwhelmingly popular solution to these problems are the well-
known heuristic evaluation functions for games.

slide 32

eval. functions in games

an evaluation function maps states to a number.  the larger the 
number, the larger the true game-theoretic position is estimated to be.

search a tree as deeply as affordable.
leaves of the tree you search are not leaves of the game tree, but 
are instead intermediate nodes.
the value assigned to the leaves are from the evaluation function.

intuitions
visibility: the evaluation function will be more accurate nearer the end 
of the game, so worth using heuristic estimates from there.
filtering: if we used the evaluation function without searching, we   d be 
using a handful of inaccurate estimates.  by searching we are 
combining thousands of these estimates, & we hope, eliminating noise.
dubious intuition.  counter-examples.  but often works very well in real games.

slide 33

other important issues for real 

game playing programs

    how to decide how far to search if you only have a fixed time to
    quiescence.  what if you stop the search at a state where 

make a decision.  what   s the obvious sensible answer?

subsequent moves dramatically change the evaluation?

    the solution to the quiescence problem is a sensible technique called 
quiescence search.
    the horizon problem.  what if s is a state which is clearly bad 

because your opponent will inevitably be able to do something bad 
to you?  but you have some delaying tactics.  the search algorithm 
won   t recognize the state   s badness if the number of delaying moves 
exceeds the search horizon.

    endgames are easy to play well.  how?
    openings fairly easy to play well.  how?

slide 34

what if you think you   re certainly going to lose?

(ii ii) a

(i  ii) b

(- ii) b

(- ii) a

(i i) a

(- i) a

(- i) a

(- -) a +1

(- i) b

(- -) b -1

(- i) b

(- -) b -1

(- -) b -1

(- -) a +1

(- -) a +1

what should a do in this situation?
what heuristics/assumptions could be used to cause a to 
make that decision?  two common methods.

slide 35

solving games

solving a game means proving the game-theoretic value of the start 
state.
some games have been solved.  usually by brute force dynamic 
programming.  (e.g. four-in-a-row, many chess endgames)
or brute force id145 back from end of game, to create 
an end-game database, in combination with alpha-beta search from the 
start of the game.  (nine men   s morris)
or mostly brute force, with some game specific analysis (connect-4)
checkers may not be far from being solved.

solving a game is often very different from playing well at the game.

slide 36

2 player zero-sum finite nondeterministic

games of perfect information

nondeterministic 

= stochastic

the search tree now includes states where neither player 
makes a choice, but instead a random decision is made 
according to a known set of outcome probabilities.

(  )-chance

p=0.5

(  )-b

+4   

(  )-a

p=0.5

(  )-b

-20

(  )-b

p=0.2

(  )-a

(  )-chance

p=0.5

(  )-a

(  )-a

game theory value of a state is the expected final value if both players 
are optimal.
if no loops, computing this is almost as easy as recursive minimax.  is there alpha-beta 
version?

-5

+10

+3

slide 37

what you should know

    what makes a game a two player zero-sum discrete 

finite deterministic game of perfect information

    what is the formal definition of the above 
    what is a game tree
    what is the minimax value of a game
    what assumptions minimax makes about the game
    minimax search
    alpha beta search
    use of evaluation functions for very big games
    why it   s easy to extend this to two player zero-sum 
discrete finite stochastic game of perfect information

slide 38

what you should know

    what makes a game a two player zero-sum discrete 

finite deterministic game of perfect information

e

    what is the formal definition of the above 
n ,  
    what is a game tree
p ti o
c
e
u t  
g ,   d
o
b
g   b l u f fi n
    what is the minimax value of a game
  a
y
r t a i n t
g  
t h i n
u i r i n
e
y
c
n
r
q
e
d   u
e
    what assumptions minimax makes about the game
,   r
n t     e
r l d !
n
s
g   a
a m e
t   u p :
a l  w o
e m i n
  w a
  o f   g
x
n e
e
a ll y
h
e   r
s
c
s
e
r t  i n   t h
    minimax search
 
r
y
s
   
k
s
c l a
a
d
e
n
r  
n
a
d       f r i e
s
e
g   p
d  
o t h
k i n
u i s m   a
a ll e
r   t a
    alpha beta search
c
d   f o
-
o
a lt r
s
e
u
o
  n
y
t e m s
a t  
    use of evaluation functions for very big games
w h
s
r   a i  
u
o
    why it   s easy to extend this to two player zero-sum 
discrete finite stochastic game of perfect information

n
r  
s

e

e

v

s

y

slide 39

