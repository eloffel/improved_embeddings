note to other teachers and users of these slides. andrew would be 
delighted if you found this source material useful in giving you r own 
lectures. feel free to use these slides verbatim, or to modify them to fit 
your own needs. powerpoint originals are available. if you make use of 
a significant portion of these slides in your own lecture, please include 
this message, or the following link to the source repository of andrew   s 
tutorials: http://www.cs.cmu.edu/~awm/tutorials . comments and 
corrections gratefully received. 

learning gaussian 
bayes classifiers

andrew w. moore
associate professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2001, andrew w. moore

sep 10th, 2001

maximum likelihood learning of 

gaussians for classification

    why we should care
    3 seconds to teach you a new learning 

algorithm

    what if there are 10,000 dimensions?
    what if there are categorical inputs?
    examples    out the wazoo   

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  2

1

why we should care

    one of the original    data mining    algorithms
    very simple and effective
    demonstrates the usefulness of our earlier 

groundwork

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  3

where we were at the end of the 

id113 lecture   

categorical 
inputs only

real-valued 
inputs only

mixed real / 
cat okay

s
t
u
p
n
i

classifier

predict
category

joint bc

na  ve bc

dec tree

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

prob-
ability

predict
real no.

joint de

na  ve de

gauss de

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  4

2

this lecture   

categorical 
inputs only

real-valued 
inputs only

mixed real / 
cat okay

s
t
u
p
n
i

classifier

predict
category

joint bc

na  ve bc

gauss bc

dec tree

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

prob-
ability

predict
real no.

joint de

na  ve de

gauss de

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  5

road map

id203

pdfs

density
estimation

decision
trees

id113

gaussians

bayes

classifiers

id113 of

gaussians

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  6

3

road map

id203

pdfs

density
estimation

decision
trees

id113

gaussians

id113 of

gaussians

bayes

classifiers

gaussian

bayes

classifiers

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  7

gaussian bayes classifier 

assumption

    the i   th record in the database is created 

using the following algorithm

1. generate the output (the    class   ) by 
drawing yi~multinomial(p1,p2,   pny)

2. generate the inputs from a gaussian pdf 

that depends on the value of yi :

xi ~ n(mi ,si).

test your understanding. given ny classes and m input attributes, how 
many distinct scalar parameters need to be estimated?

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  8

4

id113 gaussian bayes classifier
let dbi = subset of
    the i   th record in the database is created 
database db in which
the output class is y = i
1. generate the output (the    class   ) by 
drawing yi~multinomial(p1,p2,   pny)

using the following algorithm

|dbi|
id113 = ------
|db|

pi

2. generate the inputs from a gaussian pdf 

that depends on the value of yi :

xi ~ n(mi ,si).

test your understanding. given ny classes and m input attributes, how 
many distinct scalar parameters need to be estimated?

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  9

using the following algorithm

id113 gaussian bayes classifier
let dbi = subset of
    the i   th record in the database is created 
database db in which
the output class is y = i
1. generate the output (the    class   ) by 
(m i
id113, si
drawing yi~multinomial(p1,p2,   pny)
2. generate the inputs from a gaussian pdf 

id113 )= id113 gaussian for dbi

that depends on the value of yi :

xi ~ n(mi ,si).

test your understanding. given ny classes and m input attributes, how 
many distinct scalar parameters need to be estimated?

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  10

5

using the following algorithm

id113 gaussian bayes classifier
let dbi = subset of
    the i   th record in the database is created 
database db in which
the output class is y = i
1. generate the output (the    class   ) by 
(m i
id113, si
drawing yi~multinomial(p1,p2,   pny)
2. generate the inputs from a gaussian pdf 

id113 )= id113 gaussian for dbi

that depends on the value of yi :

r

1

)
=
x
k
test your understanding. given ny classes and m input attributes, how 
k db
i
many distinct scalar parameters need to be estimated?

|db|
i

|db|
i

tid113
i

)(
x

(
x

id113
i

id113
i

k db

  

  

x

x

k

k

i

xi ~ n(mi ,si).
s

=

1

r

  

id113
i

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  11

gaussian bayes classification

(
yp

=

i

|

x

)

=

p

(

x

|

y

=

i

)

=
p

(
ypi
)
)(
x

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  12

6

(cid:229)
  
(cid:229)
  
-
-
gaussian bayes classification

(
yp

=

i

|

x

)

=

p

(

x

|

y

=

i

)

=
p

(
ypi
)
)(
x

1
2/
||

p
)2(

m

exp

s

i

2/1
||

(
yp

=

i

|

x

)

=

)

t

(
xs

i

k

  

i

)

  

i

p

i

k

(

x

1
2
p
)(
x

how do we deal with that?

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  13

here is a dataset

job

relation

race

gender hours_worked

country

wealth

age

employmenteducation edunummarital    
   
13 never_married   
39 state_gov bachelors
13 married    
bachelors
51 self_emp_not_inc
9 divorced    
hs_grad
39 private
7 married    
11th
54 private
13 married    
bachelors
28 private
14 married    
masters
38 private
5 married_spouse_absent
   
9th
50 private
9 married    
52 self_emp_not_inc
hs_grad
14 never_married   
31 private
masters
bachelors
42 private
13 married    
some_college10 married    
37 private
13 married    
30 state_gov bachelors
bachelors
24 private
13 never_married   
assoc_acdm12 never_married   
33 private
assoc_voc 11 married    
41 private
4 married    
7th_8th
34 private
26 self_emp_not_inc
hs_grad
9 never_married   
9 never_married   
hs_grad
33 private
7 married    
38 private
11th
14 divorced    
44 self_emp_not_incmasters
41 private
16 married    

:

:

:

:

:

doctorate
:

adm_clericalnot_in_familywhite
husband white
exec_managerial
not_in_familywhite
handlers_cleaners
husband black
handlers_cleaners
black
prof_specialtywife
exec_managerialwife
white
other_servicenot_in_familyblack
exec_managerial
husband white
prof_specialtynot_in_familywhite
husband white
exec_managerial
exec_managerial
husband black
prof_specialtyhusband asian
adm_clericalown_child white
sales
not_in_familyblack
craft_repairhusband asian
transport_moving
farming_fishingown_child white
unmarried white
machine_op_inspct
husband white
sales
exec_managerial
unmarried white
prof_specialtyhusband white
:

male
male
male
male
female
female
female
male
female
male
male
male
female
male
male
husband amer_indianmale
male
male
male
female
male
:

:

:

40 united_statespoor
13 united_statespoor
40 united_statespoor
40 united_statespoor
40 cuba
poor
40 united_statespoor
16 jamaica poor
45 united_statesrich
50 united_statesrich
40 united_statesrich
80 united_statesrich
40 india
rich
30 united_statespoor
50 united_statespoor
40 *missingvalue*
rich
45 mexico
poor
35 united_statespoor
40 united_statespoor
50 united_statespoor
45 united_statesrich
60 united_statesrich

:

:

:

48,000 records, 16 attributes [kohavi 1995]

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  14

7

  
  
  
  
  
  
-
-
-
predicting wealth from age

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  15

predicting wealth from age

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  16

8

wealth from hours worked

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  17

wealth from years of education

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  18

9

age, hours     wealth

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  19

age, hours     wealth

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  20

10

age, hours     wealth

having 2 inputs 
instead of one helps 
in two ways:
1. combining 
evidence from two 1d 
gaussians
2. off-diagonal 
covariance 
distinguishes class 
   shape   
gaussian bayes classifiers: slide  21

copyright    2001, andrew w. moore

age, hours     wealth

copyright    2001, andrew w. moore

having 2 inputs 
instead of one helps 
in two ways:
1. combining 
evidence from two 1d 
gaussians
2. off-diagonal 
covariance 
distinguishes class 
   shape   
gaussian bayes classifiers: slide  22

11

age, edunum     wealth

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  23

age, edunum     wealth

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  24

12

hours, edunum     wealth

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  25

hours, edunum     wealth

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  26

13

accuracy

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  27

an    mpg    example

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  28

14

an    mpg    example

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  29

an    mpg    example

things to note:

   class boundaries can be weird 
shapes (hyperconic sections)

   class regions can be non-simply-
connected

   but it   s impossible to model 
arbitrarily weirdly shaped regions

   test your understanding: with 
one input, must classes be simply 
connected?

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  30

15

overfitting dangers

    problem with    joint    bayes classifier:

#parameters exponential with #dimensions.

this means we just memorize the 

training data, and can overfit.

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  31

overfitting dangers

    problem with    joint    bayes classifier:

#parameters exponential with #dimensions.

this means we just memorize the 

training data, and can overfit.

    problemette with gaussian bayes classifier:

#parameters quadratic with #dimensions.

with 10,000 dimensions and only 1,000 

datapoints we could overfit.

question: any suggested solutions?

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  32

16

general: o(m2)

parameters

=

s

1

2

s
s
12
m

s
s

1

12
2

l
l

s
s
2
2
mom
2
s

l

2

m

m

m

m

s

1

m

s

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  33

general: o(m2)

parameters

=

s

1

2

s
s
12
m

s
s

1

12
2

l
l

s
s
2
2
mom
2
s

l

2

m

m

m

m

s

1

m

s

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  34

17

(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
  
(cid:230)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
  
(cid:230)
aligned: o(m)

parameters

=

s

1

2

s
0
0
m
0
0

2

0
2
s
0
m
0
0

0
0
2
s

l
l
l

0
0
0
3
mom
2
0
0

l
l

s

m
0

0
0
0
m
0
2

m

1

s

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  35

aligned: o(m)

parameters

=

s

1

2

s
0
0
m
0
0

2

0
2
s
0
m
0
0

0
0
2
s

l
l
l

0
0
0
3
mom
2
0
0

l
l

s

m
0

0
0
0
m
0
2

m

1

s

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  36

18

(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
  
(cid:230)
-
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
  
(cid:230)
-
spherical: o(1)
cov parameters

=

s

2

s
0
0
m
0
0

0
s
2
0
m
0
0

l
l
l

0
0
0
0
2
s
0
mom
s
0
0
0

l
l

2

0
0
0
m
0
2
s

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  37

spherical: o(1)
cov parameters

=

s

2

s
0
0
m
0
0

0
s
2
0
m
0
0

l
l
l

0
0
0
0
2
s
0
mom
s
0
0
0

l
l

2

0
0
0
m
0
2
s

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  38

19

(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
  
(cid:230)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
  
(cid:230)
bcs that have both real and 

categorical inputs?

categorical 
inputs only

real-valued 
inputs only

mixed real / 
cat okay

s
t
u
p
n
i

classifier

predict
category

joint bc

na  ve bc

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

prob-
ability

predict
real no.

joint de

na  ve de

gauss bc

gauss de

dec tree

bc here???

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  39

bcs that have both real and 

categorical inputs?

categorical 
inputs only

real-valued 
inputs only

mixed real / 
cat okay

s
t
u
p
n
i

classifier

predict
category

joint bc

na  ve bc

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

prob-
ability

predict
real no.

joint de

na  ve de

gauss bc

gauss de

dec tree

bc here???

easy!

guess how?

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  40

20

bcs that have both real and 

categorical inputs?

categorical 
inputs only

real-valued 
inputs only

mixed real / 
cat okay

s
t
u
p
n
i

classifier

predict
category

joint bc

na  ve bc

gauss bc

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

prob-
ability

predict
real no.

joint de

na  ve de

gauss de

gauss de

dec tree
gauss/joint bc
gauss na  ve bc
gauss/joint de

gauss na  ve de

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  41

bcs that have both real and 

categorical inputs?

categorical 
inputs only

real-valued 
inputs only

mixed real / 
cat okay

s
t
u
p
n
i

classifier

predict
category

joint bc

na  ve bc

gauss bc

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

prob-
ability

predict
real no.

joint de

na  ve de

gauss de

gauss de

dec tree
gauss/joint bc
gauss na  ve bc
gauss/joint de

gauss na  ve de

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  42

21

mixed categorical / real density 

estimation

    write x = (u,v) = (u1 ,u2 ,   uq ,v1 ,v2     vm-q)

real valued

categorical valued

p(x |m)= p(u,v |m)

(where m is any density estimation model)

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  43

t a s t y
  s u r e   w h i c h  
t o   e n j o y ?   t r y   o u r    

n o t
d e  

joint / gauss de 

combo

p(u,v |m) = p(u |v ,m) p(v |m) 

gaussian with

parameters

depending on v

big    m-q   -dimensional 

lookup table

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  44

22

id113 learning of the joint / 

gauss de combo

p(u,v |m) = p(u |v ,m) p(v |m) 

=mv

mean of u among 
records matching v 

=sv

cov. of u among 
records matching v

=qv

fraction of records 
that match v

u |v ,m ~ n(mv , sv)  , p(v |m) = qv

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  45

id113 learning of the joint / 

gauss de combo

p(u,v |m) = p(u |v ,m) p(v |m) 

=mv

mean of u among 
records matching v 

=sv

cov. of u among 
records matching v

=qv

fraction of records 
that match v

=

=

=

v

1
r
1
r
k
v
rv
r

u
= v

k

k

 

 s.t.

v

k

(
=

u
v

k

 

 s.t.

v

k

  

v

)(

u

k

  

v

t

)

rv = # records that match v

u |v ,m ~ n(mv , sv)  , p(v |m) = qv

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  46

23

(cid:229)
(cid:229)
-
-
gender and hours worked*

*as with all the results from the uci    adult census    dataset, we can   t 
draw any real-world conclusions since it   s such a non-real-world sample

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  47

what we just did 

joint / gauss de 

combo

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  48

24

what we do next 

joint / gauss bc

combo

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  49

joint / gauss bc

yp
(

=

=

i

|

,
vu

)

p

|,(
vu

,(
vu

combo
p
|
ypm
(
)
=
i
p
vu
,(
)
|
)
(
v
,(
vu

ypmpm
(

)
p

)

,

i

i

=

i

)

=

i

)

n

;(
  u

=

,
s
i
v
,
,(
vu

)
)

i

,
v
p

q

i

,

v

p
i

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  50

25

joint / gauss bc

=

=

i

|

,
vu

)

p

|,(
vu

,(
vu

combo
p
|
ypm
(
)
=
i
p
,(
)
vu
|
)
(
v
,(
vu

ypmpm
(

)
p

)

,

i

i

=

i

)

=

i

)

n

;(
  u

=

,
s
i
v
,
,(
vu

)
)

i

,
v
p

q

i

,

v

p
i

rather so-so-notation for 
   gaussian with mean mi,v and 
covariance s i,v evaluated at u   

yp
(
mean of u among 
records matching v 
and in which y=i
cov. of u among 
records matching v 
and in which y=i
fraction of    y=i   
records that match 
v
fraction of records 
that match    y=i   

=mi,v

=si,v

=qi,v

=pi

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  51

gender, hours    wealth

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  52

26

gender, hours    wealth

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  53

joint / gauss de combo and 
joint / gauss bc combo: the 

downside

    (yawn   we   ve done this before   )

more than a few categorical attributes blah blah 
blah massive table blah blah lots of parameters
blah blah just memorize training data blah blah 
blah do worse on future data blah blah need to 
be more conservative blah

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  54

27

na  ve/gauss combo for density 

real

categorical

estimation

p

,(
vu

|

m

)

nmu

~

|

j

q

=

=
1
j
sm
,

j

(

mup
(

|

j

)

qm

=
1

j

mvp

(

|

j

)

2
j

)

mv
j

|

~

multinomia

[
l

q

,

q

1
j

j

2

,...,

q

]

jn

j

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  55

how many parameters?

na  ve/gauss combo for density 

real

categorical

estimation

p

,(
vu

|

m

)

=

nmu

|

(

j

q

=
1
j
sm
,

j

mup
(

|

j

)

qm

=
1

j

mvp

(

|

j

)

2
j

)

mv
j

|

~

multinomia

[
l

q

,

q

1
j

j

2

,...,

q

]

jn

j

~
1
r
1 (cid:229)
r
k
 of #

k

=

m

j

=

s

2
j

=

q

jh

u

kj

(

u

kj

m

j

2

)

records

=

h

v

j

in which 
 
r

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  56

28

(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
(cid:213)
(cid:213)
-
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
(cid:213)
(cid:213)
-
(cid:229)
-
na  ve/gauss de example

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  57

na  ve/gauss de example

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  58

29

na  ve / 
gauss bc

yp
(

=

i

|

vu
,

)

=

p

,(
vu

|

=
(
)
ypi
,(
vu

)

y
p

=

i

)

=

p

1
,(
vu
=

p

q

)

=
1

j
1
,(
vu

(
up

j

|

sm

,

ij

2
ij

)

q

)

=
1

j

un
(

;

j

sm

,

ij

qm

=

j

1

2
ij

)

vp
(

j

|

q

ij

)

(
yp

=

i

)

qm

=

j

1

vq
[
ij

j

]

p
i

mij
s2
ij
qij[h]
pi

=
=
=
=

mean of uj among records in which y=i
var. of uj among records in which y=i
fraction of    y=i    records in which vj = h
fraction of records that match    y=i   

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  59

gauss / na  ve bc example

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  60

30

(cid:213)
(cid:213)
-
(cid:213)
(cid:213)
-
gauss / na  ve bc example

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  61

learn wealth from 15 attributes

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  62

31

learn wealth from 15 attributes

 
l
l

a
 
t
p
e
c
x
e
 
,
a
t
a
d
 
e
m
a
s

 
d
e
z
i
t
e
r
c
s
i
d
 
s
e
u
a
v
 
l
a
e
r

l

s
l
e
v
e
l
 
3
 
o
t

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  63

learn race from 15 attributes

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  64

32

what you should know
    a lot of this should have just been a 

corollary of what you already knew

    turning gaussian des into gaussian bcs
    mixing categorical and real-valued

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  65

questions to ponder

    suppose you wanted to create an example 

dataset where a bc involving gaussians 
crushed id90 like a bug. what 
would you do?

    could you combine id90 and 
bayes classifiers? how? (maybe there is 
more than one possible way)

copyright    2001, andrew w. moore

gaussian bayes classifiers: slide  66

33

