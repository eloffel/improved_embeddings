note to other teachers and users of these slides. 
andrew would be delighted if you found this source 
material useful in giving your own lectures. feel free 
to use these slides verbatim, or to modify them to fit 
your own needs. powerpoint originals are available. if 
you make use of a significant portion of these slides in 
your own lecture, please include this message, or the 
following link to the source repository of andrew   s 
tutorials: http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully received. 

gaussians

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    andrew w. moore

slide 1

gaussians in data mining

    why we should care
    the id178 of a pdf
    univariate gaussians
    multivariate gaussians
    bayes rule and gaussians
    maximum likelihood and map using 

gaussians

copyright    andrew w. moore

slide 2

1

why we should care

    gaussians are as natural as orange juice 

and sunshine

    we need them to understand bayes optimal 

classifiers

    we need them to understand regression
    we need them to understand neural nets
    we need them to understand mixture 

models

       

(you get the idea)

copyright    andrew w. moore

slide 3

   

w|x|
2
w|x|
2

>

the    box   
distribution

xp
)(

=

1
w
0

   
   
   
   
   

if

if

1/w

-w/2

0

w/2

copyright    andrew w. moore

slide 4

2

the    box   
distribution

xp
)(

=

1
w
0

   
   
   
   
   

if

if

1/w

   

w|x|
2
w|x|
2

>

-w/2

0

w/2

=xe
[

0]

var[

x =

]

2w
12

copyright    andrew w. moore

slide 5

id178 of a pdf
xp
 of
log)(

xhx

   =

=

]

[

   

   

x

      =

id178 

dxxp
)(

natural log (ln or loge)

the larger the id178 of a distribution   
   the harder it is to predict
   the harder it is to compress it
   the less spiky the distribution

copyright    andrew w. moore

slide 6

3

the    box   
distribution

xp
)(

=

1
w
0

   
   
   
   
   

if

if

   

w|x|
2
w|x|
2

>

1/w

xh

[

]

   =

x

-w/2

0

w/2

xp
log)(

dxxp
)(

   =

   

   

      =

w

2/

   

wx
   =

2/

1
w

log

1
w

dx

   =

1
w

log

1
w

w

2/
dx
2/

   

wx
   =

copyright    andrew w. moore

unit variance 
box distribution

xp
)(

=

1
w
0

   
   
   
   
   

if

if

   

w|x|
2
w|x|
2

>

=

log

w

slide 7

1
32

 

=xe
[
x =
var[

0]
2w
]
12

 3   
 then 

 if

w

=

32

var[

0

x

and 1]
 
=

xh

[

 3
242.1]
=

copyright    andrew w. moore

slide 8

4

the hat 
distribution

xp
)(

=

|

xw
|
   
w
2
0

   
   
   
      

if
if

|x|
|x|

   
>

w
w

 1
w

=xe
[
x =
var[

0]
2w
]
6

w   

0

w

copyright    andrew w. moore

slide 9

unit variance hat 

distribution

xp
)(

=

|

xw
|
   
w
2
0

   
   
   
      

if
if

|x|
|x|

   
>

w
w

1
6

 

=xe
[
x =
var[

0]
2w
]
6

 6   
 then 

var[

 if

w

=

6

0

x

and 1]
 
=

xh

[

 6
.1]
=

396

copyright    andrew w. moore

slide 10

5

the    2 spikes   

distribution

   
2

1
2

   =x  

(

)1

dirac delta

(
  

x

xp
)(

=

)1
+   =
2

(
  

x

=

)1

1
2

=x  

(

)1

=xe
[

0]

var[

=x
1]

-1

0

1

xh

[

]

   =

xp
log)(

dxxp
)(

      =

   

   

      =x

copyright    andrew w. moore

slide 11

entropies of unit-variance 

distributions
id178

distribution

box

hat

2 spikes

???

1.242

1.396

-infinity

1.4189

largest possible 
id178 of any unit-
variance distribution

copyright    andrew w. moore

slide 12

6

unit variance 

gaussian

xp
)(

=

1
2
  

   
exp
      
   

   

2x
2

   
      
   

=xe
[

0]

var[

=x
1]

xh

[

]

   =

xp
log)(

dxxp
)(

=

.1

4189

   

   

      =x

copyright    andrew w. moore

slide 13

general 
gaussian

xp
)(

=

1
2
    

   
exp
      
   

   

2

(

x
)
  
   
2
2
  

   
      
   

xe
[

=]

  

var[   =x
2

]

  =15

  =100

copyright    andrew w. moore

slide 14

7

general 
gaussian

also known 
as the normal 
distribution 

or bell-

shaped curve 

  =15

  =100

xp
)(

=

1
2
    

   
exp
      
   

   

2

(

x
)
  
   
2
2
  

   
      
   

xe
[

=]

  

var[   =x
2

]

shorthand: we say x ~ n(  ,  2) to mean    x is distributed as a gaussian 
with parameters    and   2   .
in the above figure, x ~ n(100,152)

copyright    andrew w. moore

slide 15

the error function

assume x ~ n(0,1)
define erf(x) = p(x<x) = cumulative distribution of x

erf

x
)(

=

z

zp
)(

dz

x

   

      =

=

1
2
  

z

x

   

      =

   
exp
      
   

   

z
2
2

   
      
   

dz

copyright    andrew w. moore

slide 16

8

using the error function

assume x ~ n(  ,  2)
p(x<x|   ,  2) =

erf

(

     x
)
2  

copyright    andrew w. moore

slide 17

the central limit theorem

    if (x1,x2,     xn) are i.i.d. continuous random 

variables

    then define 

z

=

xxf
(
,

1

2

,...

x

n

)

=

1
n

n

   

i

1
=

x
i

    as n-->infinity, p(z)--->gaussian with mean 

e[xi] and variance var[xi]

somewhat of a justification for assuming 
gaussian noise is common

copyright    andrew w. moore

slide 18

9

other amazing facts about 

gaussians
    wouldn   t you like to know?

    we will not examine them until we need to.

copyright    andrew w. moore

slide 19

bivariate gaussians
   
      
   

    nx

x
y

,(

   
      
   

~

)

to mean

 write

r.v.
 

 x 

=

then define
1
  

(
exp
   

||

2

1

||
2
  

p

x
)(

=

(

1
2

  x    x

   

   

1
   

(

)

t

))

where the gaussian   s parameters are   

  

=

  
x
  
y

   
      
   

   
      
   

  

=

2

x

   
    
   
xy
   
2
    
   

xy

y

   
   
   
   

where we insist that    is symmetric non-negative definite

copyright    andrew w. moore

slide 20

10

bivariate gaussians
   
      
   

    nx

x
y

,(

   
      
   

~

)

to mean

 write

r.v.
 

 x 

=

then define
1
  

(
exp
   

||

2

1

||
2
  

p

x
)(

=

(

1
2

  x    x

   

   

1
   

(

)

t

))

where the gaussian   s parameters are   

  

=

  
x
  
y

   
      
   

   
      
   

  

=

2

x

   
    
   
xy
   
2
    
   

xy

y

   
   
   
   

where we insist that    is symmetric non-negative definite

it turns out that e[x] =    and cov[x] =   . (note that this is a 
resulting property of gaussians, not a definition)*

*this note rates 7.4 on the pedanticness scale

copyright    andrew w. moore

slide 21

evaluating 
p(x): step 1

1. begin with vector x

p

x
)(

=

1
  
||2
  

1

2

||

exp

(
   

(

1
2

  x    x
   

   

1
   

(

)

t

))

x

  

copyright    andrew w. moore

slide 22

11

evaluating 
p(x): step 2

1. begin with vector x
2. define    = x -   

p

x
)(

=

1
  
||2
  

1

2

||

exp

(
   

(

1
2

  x    x
   

   

1
   

(

)

t

))

x

  

  

copyright    andrew w. moore

slide 23

evaluating 
p(x): step 3

1. begin with vector x
2. define    = x -   
3. count the number of contours 

crossed of the ellipsoids 
formed   -1
d= this count = sqrt(  t  -1  )
= mahalonobis distance 
between x and   

p

x
)(

=

1
  
||2
  

1

2

||

exp

(
   

(

1
2

  x    x
   

   

1
   

(

)

t

))

contours defined by 
sqrt(  t  -1  ) = constant

x

  

  

copyright    andrew w. moore

slide 24

12

  x    x
   

   

1
   

(

)

t

))

evaluating 
p(x): step 4

p

x
)(

=

1
  
||2
  

1

2

||

exp

(
   

(

1
2

1. begin with vector x
2. define    = x -   
3. count the number of contours 

crossed of the ellipsoids 
formed   -1
d= this count = sqrt(  t  -1  )
= mahalonobis distance 
between x and   

4. define w = exp(-d 2/2)

)
2
/

d 2

-
(
p
x
e

d 2

x close to    in squared mahalonobis 

space gets a large weight. far away gets 

a tiny weight

copyright    andrew w. moore

slide 25

  x    x
   

   

1
   

(

)

t

))

evaluating 
p(x): step 5

p

x
)(

=

1
  
||2
  

1

2

||

exp

(
   

(

1
2

1. begin with vector x
2. define    = x -   
3. count the number of contours 

crossed of the ellipsoids 
formed   -1
d= this count = sqrt(  t  -1  )
= mahalonobis distance 
between x and   

)
2
/

d 2

-
(
p
x
e

4. define w = exp(-d 2/2)
5.

multiply w
by 
 

1
||

 to

ensure

xx
d)p(

   

d 2
1
=

1

2

  

||

2
  

copyright    andrew w. moore

slide 26

13

example

observe: mean, principal axes, 
implication of off-diagonal 
covariance term, max gradient 
zone of p(x)

common convention: show contour 

corresponding to 2 standard 

deviations from mean

copyright    andrew w. moore

slide 27

example

copyright    andrew w. moore

slide 28

14

example

in this example, x and y are almost independent

copyright    andrew w. moore

slide 29

example

in this example, x and    x+y    are clearly not independent

copyright    andrew w. moore

slide 30

15

example

in this example, x and    20x+y    are clearly not independent

copyright    andrew w. moore

slide 31

multivariate gaussians

 write

r.v.
 
 

x

 

=

   
   
   
   
      
   

x
x

1

2

m
mx

   
   
   
   
      
   

then define

    nx

,(

~

)

to mean

p

x
)(

=

1
||
2

1

2

  

||

m

)2(
  

exp

(
   

(

1
2

  x    x
   

   

1
   

(

)

t

))

where the gaussian   s 
parameters have   

  

=

  
1
  
2

m
m  

   
   
   
   
      
   

   
   
   
   
      
   

  

=

2

1

12

    
12
2
    

   
   
   
   
   
   
    
   
2

m

m

1

2

l
l

  
1
  
2

m

m

mom
2
  

l

m

m

   
   
   
   
   
   
   

where we insist that    is symmetric non-negative definite
again, e[x] =    and cov[x] =   . (note that this is a resulting property of gaussians, not a definition)

copyright    andrew w. moore

slide 32

16

general gaussians

2

1

12

    
12
2
    

   
   
   
   
   
   
    
   
2

m

m

1

2

l
l

  
1
  
2

m

m

mom
2
  

l

m

m

   
   
   
   
   
   
   

  

=

  
1
  
2

m
m  

   
   
   
   
      
   

   
   
   
   
      
   

  

=

x2

copyright    andrew w. moore

slide 33

x1

axis-aligned gaussians

  

=

1

2
   
  
   
0
   
   
0
   
   
   
   
   
   

m
0
0

0
2
  
0

2

0
0
2
  

3

l
l
l

0
0
0

0
0
0

m
0
0

mom
0
2
  
m
0
0

l
l

1
   

m
0
2
  

m

   
   
   
   
   
   
   
   
   
   

  

=

  
1
  
2

m
m  

   
   
   
   
      
   

   
   
   
   
      
   

x

i

   

x

i

for 
 

i

   

j

x2

copyright    andrew w. moore

slide 34

x1

17

spherical gaussians

  

=

2
   
  
   
0
   
   
0
   
   
   
   
   
   

m
0
0

0
2
  
0

m
0
0

0
0
2
  

l
l
l

0
0
0

mom
0
2
  
0
0

l
l

0
0
0

m
0
2
  

   
   
   
   
   
   
   
   
   
   

  

=

  
1
  
2

m
m  

   
   
   
   
      
   

   
   
   
   
      
   

x

i

   

x

i

for 
 

i

   

j

x2

copyright    andrew w. moore

slide 35

x1

degenerate gaussians

  

=

  
1
  
2

m
m  

   
   
   
   
      
   

   
   
   
   
      
   

x2

||

=  
||

0

x1

what   s so wrong 

with clipping 

one   s toenails in 

public?

copyright    andrew w. moore

slide 36

18

where are we now?

    we   ve seen the formulae for gaussians
    we have an intuition of how they behave
    we have some experience of    reading    a 

gaussian   s covariance matrix

    coming next:

some useful tricks with gaussians

copyright    andrew w. moore

slide 37

subsets of variables
   
   
   
   
x
   
x
   
   
   
   
   

   
   
   
 as 
   
      
   

   
 where
      
   

   
   
   
   
      
   

u
v

x
x

m
x

   
      
   

x

u

v

x

=

=

=

=

 

 

 

m

2

1

x

1

m

um
(

um
(

m
x

m

)

   
   
   
   
   
 
   
1)
+
   
   
   
   

 write

  

this will be our standard notation for breaking an m-
dimensional distribution into subsets of variables

copyright    andrew w. moore

slide 38

19

gaussian marginals 

are gaussian

u
v

   
      
   

   
       
   

margin-

alize

 u

x
x

1

2

m
x

m
  
  

   
      
   

   
   
   
   
      
   
   
      
   

   
   
   
 as 
   
      
   

x

 

=

u
v

   
      
   

   
 where
      
   

  

u

=

   
   
   
   
   

x

1

m

um
(

)

x

   
   
   
   
   

,

v

=

   
   
   
   
   

x

um
(

1)
+

m
x

m

   
   
   
   
   

,

   
      
   

   
      
   

  
  

uu
t
uv

u

v

  
  

uv

vv

   
      
   

   
      
   

 write

  

x

 

=

if 

    

u
v

   
      
   

   
n~ 
      
   

then u is also distributed as a gaussian

u
 

,n~ 

(
u     

)uu

copyright    andrew w. moore

slide 39

gaussian marginals 

are gaussian

u
v

   
      
   

   
       
   

margin-

alize

 u

 write

  

x

 

=

   
   
   
 as 
   
      
   

x

 

=

u
v

   
      
   

   
 where
      
   

  

u

=

   
   
   
   
   

x

1

m

um
(

)

x

   
   
   
   
   

,

v

=

   
   
   
   
   

x

um
(

1)
+

m
x

m

   
   
   
   
   

if 

    

u
v

   
      
   

   
n~ 
      
   

,

   
      
   

   
      
   

  
  

uu
t
uv

u

v

  
  

uv

vv

   
      
   

   
      
   

then u is also distributed as a gaussian

u
 

,n~ 

(
u     

)uu

this fact is not 

immediately obvious

obvious, once we know 
it   s a gaussian (why?)

x
x

1

2

m
x

m
  
  

   
      
   

   
   
   
   
      
   
   
      
   

copyright    andrew w. moore

slide 40

20

gaussian marginals 

are gaussian

u
v

   
      
   

   
       
   

margin-

alize

 u

x
x

1

2

m
x

m
  
  

   
      
   

   
   
   
   
      
   
   
      
   

   
   
   
 as 
   
      
   

x

 

=

u
v

   
      
   

   
 where
      
   

  

u

,

   
      
   

   
      
   

  
  

uu
t
uv

u

v

  
  

uv

vv

   
      
   

   
      
   

 write

  

x

 

=

if 

    

u
v

   
      
   

   
n~ 
      
   

then u is also distributed as a gaussian

u
 

,n~ 

(
u     

)uu

1)
+

=

1

x

x

,

m

=

um
(

v

   
   
   
m
   
x
   
m
how would you prove 

   
   
   
   
   
this?

   
   
   
   
   

x

um
(

)

   
   
   
   
   

u
p
)(
vvu
d
,(
)

p

(snore...)

=    
 
=

v

copyright    andrew w. moore

slide 41

linear transforms 
remain gaussian

x

assume x is an m-dimensional gaussian r.v.

 

x

,n~ 

(
)    

matrix a

multiply

 ax

define y to be a p-dimensional r. v. thusly (note              ):

mp    

y =

ax

   where a is a p x m matrix. then   

y
 

n~ 

(
a  

,

)taa  

note: the    subset    result is 
a special case of this result

copyright    andrew w. moore

slide 42

21

adding samples of 2 
independent gaussians 

 if

x

n~ 

(
    

is gaussian
)
and 
yx

then 

n~ 

+

,

x

x

y
 
(
  

x
y
(
    
    

,

,

y

x

y

+

yx + 

yx
    

y

)
and 
)y
  
+

n~ 

+

x

why doesn   t this hold if x and y are dependent?
which of the below statements is true?

if x and y are dependent, then x+y is gaussian but possibly 
with some other covariance
if x and y are dependent, then x+y might be non-gaussian

copyright    andrew w. moore

slide 43

u
v

   
      
   

   
       
   

condition-

alize

vu |

conditional of 

gaussian is gaussian
  
   
      
  
   
) where

   
n~ 
      
   

   
      
   
(
  

 
then

vu

n~ 

vu   
|

u
v

  
  

  
  

if 

uu
t
uv

    

    

   
      
   

   
      
   

   
      
   

   
      
   

   
      
   

vu
|

uv

vv

,

,

|

u

v

  
 

vu
|

=

  

u

+

  v    

   

(

1
   
vv

t
uv

)

v

 

  

vu
|

=

  

uu

   

      

1
   
vv

t
uv

uv

copyright    andrew w. moore

slide 44

22

if 

    

u
v

   
      
   

then
 

    

v

u

,

   
      
   

  
  

  
  

uu
t
uv

   
n~ 
      
   

   
      
   
vu
n~ 

   
      
   
vu   
|
  v    
   

   
      
   
(
  
(

,

|

1
   
vv

t
uv

v

uv

   
      
   

  
   
      
  
   
) where

vv

vu
|
)

  
 

vu
|

=

  

u

+

 

  

vu
|

=

  

uu

   

      

1
   
vv

t
uv

uv

   
      
   

   
   
   
   

967
   
68.3
2
) where

2

849
   
      
967
   
   
  
,
yw
|
)76

w
y

    

if 

   
      
   
then
 

yw  
 
|

=

 

=yw  

|

|

,

   
      
   

   
n~ 
      
   
    

2977
   
   
   
      
   
76
   
   
(
  
yw
n~ 
yw
|
y
(
976
   
68.3
2
967
2
=
68.3

2977

849

   

   

2

2

2

808

copyright    andrew w. moore

slide 45

if 

    

u
v

   
      
   

then
 

    

v

u

,

   
      
   

  
  

  
  

uu
t
uv

   
n~ 
      
   

   
      
   
vu
n~ 

   
      
   
vu   
|
  v    
   

   
      
   
(
  
(

,

|

1
   
vv

t
uv

v

uv

   
      
   

  
   
      
  
   
) where

vv

vu
|
)

  
 

vu
|

=

  

u

+

 

  

vu
|

=

  

uu

   

      

1
   
vv

t
uv

uv

   
      
   

   
   
   
   

967
   
68.3
2
) where

2

849
   
      
967
   
   
  
,
yw
|
)76

w
y

    

if 

   
      
   
then
 

yw  
 
|

=

 

=yw  

|

|

,

   
      
   

   
n~ 
      
   
    

2977
   
   
   
      
   
76
   
   
(
  
yw
n~ 
yw
|
y
976
(
   
68.3
2
967
2
=
68.3

2977

849

   

   

2

2

2

808

p(w|m=82)

p(w|m=76)

p(w)

copyright    andrew w. moore

slide 46

23

if 

    

u
v

   
      
   

then
 

    

v

u

,

   
      
   

  
  

  
  

uu
t
uv

   
n~ 
      
   

   
      
   
vu
n~ 

   
      
   
vu   
|
  v    
   

   
      
   
(
  
(

,

|

1
   
vv

t
uv

v

uv

   
      
   

  
   
      
  
   
) where

vv

vu
|
)

  
 

vu
|

=

  

u

+

 

  

vu
|

=

  

uu

   

      

1
   
vv

t
uv

uv

 

   
      
   

   
   
   
   

    

if 

   
      
   
then
 

2

|

,

   
      
   

w
2977
   
967
849
   
   
   
   
   
n~ 
      
      
      
   
y
76
68.3
967
2
   
   
   
   
note: when given value of 
   
(
) where
v is   v, the conditional 
  
  
yw
n~ 
    
,
yw
yw
|
|
mean of u is   u
y
(
976
)76
   
   
68.3
2
967
2
=
68.3

808
2
note: marginal mean is 
|
a linear function of v

2977

849

=

   

2

2

=yw  

yw  
 
|

note: conditional 

variance is independent 
of the given value of v

note: conditional 

variance can only be 

equal to or smaller than 

marginal variance

p(w|m=82)

p(w|m=76)

p(w)

copyright    andrew w. moore

slide 47

gaussians and the 

chain rule

vu |
v

chain
rule

u
v

   
      
   

   
       
   

|

    

let a be a constant matrix
if 

(
  av

vu
n~ 
u
   
   
(
    
,n~ 
      
      
v
   
   

then
 

    

vu
|

,

)
and 
) with
,

  

v

(
    
,n~
v

)vv

   
 
=

v

a  
  

v

   
      
   

   
      
   

 

  

 
=

   
      
   

t

aa  
vv
a  
(

+
)

  
t

vu
|

vv

a  
  

vv

   
      
   

vv

copyright    andrew w. moore

slide 48

24

available gaussian tools
(
u     

,n~ 

then
 

u
 

if 

    

uv

,

u

u
v

   
      
   

   
n~ 
      
   

   
      
   

   
      
   

  
  

v

   
      
   

  
  

uu
t
uv

   
      
   

 u

   
      
   

   
      
   

  
  

vv

margin-

alize

)uu

matrix a

multiply

+

and

  

y =

 if 

 ax

yx + 

x

,n~ 

)    
(
(
    
,n~ 
x
x
yx
n~ 

+

x
 if
then 

y
 

then
)
and 

ax
(
    
,n~ 
y
)y
  
,
+
then
 

y

x

y
 
    

y

if 

u
   
    
      
v
   
where

   
n~ 
      
   
  
 

vu
|

   
      
   
=

  
  

   
      
   
  

+

u

uv

uu
t
uv

  
  

  
  

   
   
   
      
      
      
   
   
   
  v    

   

(

vv

1
   
vv

t
uv

x

)
and 
(
  
+
   
      
   

,

u

v

)taa  

,

n~ 

(
a  
yx
    

vu

|

n~ 

(
  

| ,
vu

  

)vu

|

)

v

 

  

vu
|

      

1
   
vv

t
uv

uv

   

uu

  
=
)vv

chain
rule

u
v

   
      
   

   
       
   

if 

    

vu

|

n~ 

(
  av

,

vu
|

)
and 

  

v

(
    
,n~
v

then
 

    

u
v

   
      
   

   
(
    
,n~ 
      
   

) with
,

 

  

 
=

   
      
   

t

aa  
vv
a  
(

+
)

vv

condition-

alize

vu |

u
v

   
      
   

   
       
   

 x
 x
 y
u
v

   
      
   

   
       
   

vu |
v

  
t

vu
|

vv

a  
  

vv

   
      
   

slide 49

copyright    andrew w. moore

assume   
    you are an intellectual snob
    you have a child

copyright    andrew w. moore

slide 50

25

intellectual snobs with children

       are obsessed with iq
    in the world as a whole, iqs are drawn from 

a gaussian n(100,152)

copyright    andrew w. moore

slide 51

iq tests

    if you take an iq test you   ll get a score that, 

on average (over many tests) will be your 
iq

    but because of noise on any one test the 
score will often be a few points lower or 
higher than your true iq.

score | iq ~ n(iq,102)

copyright    andrew w. moore

slide 52

26

assume   

    you drag your kid off to get tested
    she gets a score of 130
       yippee    you screech and start deciding how 

to casually refer to her membership of the 
top 2% of iqs in your christmas newsletter.

p(x<130|  =100,  2=152) =
p(x<2|   =0,  2=1) =
erf(2) = 0.977

copyright    andrew w. moore

slide 53

assume   

you are thinking:

    you drag your kid off to get tested
    she gets a score of 130
       yippee    you screech and start deciding how 

to casually refer to her membership of the 
top 2% of iqs in your christmas newsletter.

well sure the test isn   t accurate, so 
she might have an iq of 120 or she 
might have an 1q of 140, but the 
most likely iq given the evidence 
   score=130    is, of course, 130.

p(x<130|  =100,  2=152) =
p(x<2|   =0,  2=1) =
erf(2) = 0.977

can we trust 
this reasoning?

copyright    andrew w. moore

slide 54

27

maximum likelihood iq

    iq~n(100,152)
    s|iq ~ n(iq, 102)
    s=130

id113

 

iq

=

arg

max
iq

sp
(

=

130

|

iq

)

    the id113 is the value of the hidden parameter that 

makes the observed data most likely

    in this case

 

iq

=id113

130

copyright    andrew w. moore

slide 55

but   .

    iq~n(100,152)
    s|iq ~ n(iq, 102)
    s=130

id113

 

iq

=

arg

max
iq

sp
(

=

130

|

iq

)

    the id113 is the value of the hidden parameter that 

makes the observed data most likely

    in this case

 

iq

=id113

130

this is not the same as

   the most likely value of the 
parameter given the observed 

data   

copyright    andrew w. moore

slide 56

28

what we really want:

    iq~n(100,152)
    s|iq ~ n(iq, 102)
    s=130

    question: what is 

iq | (s=130)?

called the posterior 
distribution of iq

copyright    andrew w. moore

slide 57

which tool or tools?

    iq~n(100,152)
    s|iq ~ n(iq, 102)
    s=130

    question: what is 

iq | (s=130)?

copyright    andrew w. moore

u
v

   
      
   

   
       
   

x
x
y
u
v

   
      
   

   
       
   

vu |
v

 u

margin-

alize

matrix a

multiply

 ax

+

yx + 

condition-

alize

vu |

chain
rule

u
v

   
      
   

   
       
   

slide 58

29

plan

    iq~n(100,152)
    s|iq ~ n(iq, 102)
    s=130

    question: what is 

iq | (s=130)?

q| is
iq

chain
rule

   
      
   

s
iq

   
       
   

swap

   
      
   

iq
s

   
       
   

condition-

alize

i

|q

s

copyright    andrew w. moore

slide 59

working   

iq~n(100,152)
s|iq ~ n(iq, 102)
s=130

if 

   
    
      
   

u
v

   
n~ 
      
   
  
 

vu
|

   
      
   
=

  
  

   
      
   
  

   
      
   

u

v

+

u

,

uv

uu
t
uv

  
  

  
  

   
   
   
      
      
      
   
   
   
  v    

   

(

vv

1
   
vv

t
uv

then
 

)

v

if 

    

vu

|

n~ 

(
  av

,

vu
|

)
and 

  

v

(
    
,n~
v

)vv

question: what is iq | (s=130)?

then
 

    

u
v

   
      
   

   
(
    
,n~ 
      
   

) with
,

 

  

 
=

   
      
   

t

aa  
vv
a  
(

+
)

  
t

vu
|

vv

a  
  

vv

   
      
   

vv

copyright    andrew w. moore

slide 60

30

your pride and joy   s posterior iq
    if you did the working, you now have 

p(iq|s=130)

    if you have to give the most likely iq given 

the score you should give

map

 

iq

=

arg

max
iq

iqp
(

|

s

=

130

)

    where map means    maximum a-posteriori   

copyright    andrew w. moore

slide 61

what you should know
    the gaussian pdf formula off by heart
    understand the workings of the formula for 

a gaussian

    be able to understand the gaussian tools 

described so far

    have a rough idea of how you could prove 

them

    be happy with how you could use them

copyright    andrew w. moore

slide 62

31

