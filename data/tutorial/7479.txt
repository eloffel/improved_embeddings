note to other teachers and users of these slides. andrew would be delighted if you found 
this source material useful in giving your own lectures. feel free to use these slides 
verbatim, or to modify them to fit your own needs. powerpoint originals are available. if you 
make use of a significant portion of these slides in your own lecture, please include this 
message, or the following link to the source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . comments and corrections gratefully received. 

id91 with 
gaussian mixtures

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2001, 2004, andrew w. moore

unsupervised learning

    you walk into a bar.

a stranger approaches and tells you:

   i   ve got data from k classes.  each class produces 
observations with a normal distribution and variance 
  2i . standard simple multivariate gaussian 
assumptions. i can tell you all the p(wi)   s .   

    so far, looks straightforward.

   i need a maximum likelihood estimate of the   i   s .   

    no problem:

   there   s just one thing. none of the data are labeled. i 
have datapoints, but i don   t know what class they   re 
from (any of them!)

    uh oh!!

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 2

1

gaussian bayes classifier 

yp
(

=

i

|

x

)

=

p

(

x

|

y

reminder
ypi
(
)
=
x
p
)(

=

)

i

yp
(

=

i

|

x

)

=

1
||
2/

)2(
  

m

exp

  

i

||
2/1

   

k

   
      

   

1
(
x
2
x
p
)(

how do we deal with that?

t

(
x    
i

)

i

   

  

i

k

   
)
      

p

i

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 3

predicting wealth from age

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 4

2

predicting wealth from age

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 5

learning modelyear , 

mpg  ---> maker

  

=

2

1

12

    
12
2
    

   
   
   
   
   
   
    
   
2

m

m

1

2

l
l

  
1
  
2

m

m

mom
2
  

l

m

m

   
   
   
   
   
   
   

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 6

3

general: o(m2)

parameters

  

=

2

1

12

    
12
2
    

   
   
   
   
   
   
    
   
2

m

m

1

2

l
l

  
1
  
2

m

m

mom
2
  

l

m

m

   
   
   
   
   
   
   

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 7

aligned: o(m)
parameters

  

=

1

2
   
  
   
0
   
   
0
   
   
   
   
   
   

m
0
0

0
2
  
0

2

0
0
2
  

3

l
l
l

0
0
0

0
0
0

m
0
0

mom
0
2
  
m
0
0

l
l

1
   

m
0
2
  

m

   
   
   
   
   
   
   
   
   
   

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 8

4

aligned: o(m)
parameters

  

=

1

2
   
  
   
0
   
   
0
   
   
   
   
   
   

m
0
0

0
2
  
0

2

0
0
2
  

3

l
l
l

0
0
0

0
0
0

m
0
0

mom
0
2
  
m
0
0

l
l

1
   

m
0
2
  

m

   
   
   
   
   
   
   
   
   
   

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 9

spherical: o(1)
cov parameters

  

=

2
   
  
   
0
   
   
0
   
   
   
   
   
   

m
0
0

0
2
  
0

m
0
0

0
0
2
  

l
l
l

0
0
0

mom
0
2
  
0
0

l
l

0
0
0

m
0
2
  

   
   
   
   
   
   
   
   
   
   

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 10

5

spherical: o(1)
cov parameters

  

=

2
   
  
   
0
   
   
0
   
   
   
   
   
   

m
0
0

0
2
  
0

m
0
0

0
0
2
  

l
l
l

0
0
0

mom
0
2
  
0
0

l
l

0
0
0

m
0
2
  

   
   
   
   
   
   
   
   
   
   

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 11

making a classifier from a 

density estimator

categorical 
inputs only

real-valued 
inputs only

mixed real / 
cat okay

s
t
u
p
n
i

classifier

predict
category

joint bc
na  ve bc

gauss bc

dec tree

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

prob-
ability

predict
real no.

joint de
na  ve de

gauss de

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 12

6

next      back to density estimation

what if we want to do density estimation with 

multimodal or clumpy data?

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 13

the gmm assumption

   

   

there are k components. the 
i   th component is called   i
component   ihas an 
associated mean vector   i

  1

  2

  3

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 14

7

the gmm assumption

   

   

   

there are k components. the 
i   th component is called   i
component   ihas an 
associated mean vector   i
each component generates data 
from a gaussian with mean   i 
and covariance matrix   2i
assume that each datapoint is 
generated according to the 
following recipe: 

  1

  2

  3

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 15

the gmm assumption

   

   

   

there are k components. the 
i   th component is called   i
component   ihas an 
associated mean vector   i
each component generates data 
from a gaussian with mean   i 
and covariance matrix   2i
assume that each datapoint is 
generated according to the 
following recipe: 

  2

1. pick a component at random. 

choose component i with 
id203 p(  i).

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 16

8

the gmm assumption

   

   

   

there are k components. the 
i   th component is called   i
component   ihas an 
associated mean vector   i
each component generates data 
from a gaussian with mean   i 
and covariance matrix   2i
assume that each datapoint is 
generated according to the 
following recipe: 

  2

x

1. pick a component at random. 

choose component i with 
id203 p(  i).

2. datapoint ~ n(  i,   2i)
copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 17

the general gmm assumption

   

   

   

there are k components. the 
i   th component is called   i
component   ihas an 
associated mean vector   i
each component generates data 
from a gaussian with mean   i 
and covariance matrix   i 
assume that each datapoint is 
generated according to the 
following recipe: 

  1

  2

  3

1. pick a component at random. 

choose component i with 
id203 p(  i).

2. datapoint ~ n(  i,   i)
copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 18

9

unsupervised learning:
not as hard as it looks

sometimes easy

sometimes impossible

in case you   re 
wondering what 
these diagrams are, 
they show 2-d 
unlabeled data (x
vectors) 
distributed in 2-d 
space. the top one 
has three very 
clear gaussian 
centers
and sometimes                  

in between

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 19

computing likelihoods in 

unsupervised case

we have x1 , x2 ,     xn
we know p(w1) p(w2) .. p(wk)
we know   

p(x|wi,   i,       k) = prob that an observation from class 
wiwould have value x given class 
means   1      x

can we write an expression for that?

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 20

10

likelihoods in unsupervised case

we have x1  x2     xn
we have p(w1) .. p(wk).  we have   .
we can define, for any x , p(x|wi,   1,   2..   k)

can we define p(x |   1,   2..   k) ?

can we define p(x1, x1, .. xn|   1,   2..   k) ?

[yes, if we assume the x1   s were drawn independently]

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 21

unsupervised learning:
mediumly good news

we now have a procedure s.t. if you give me a guess at   1,   2..   k,
i can tell you the prob of the unlabeled data given those      s.

suppose x   s are 1-dimensional.
there are two classes; w1and w2
p(w1) = 1/3     p(w2) = 2/3        = 1 .
there are 25 unlabeled datapoints

(from duda and hart)

x1 =  0.608
x2 = -1.590
x3 = 0.235
x4 = 3.949

:

x25 = -0.712

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 22

11

duda & hart   s 

example

graph of 
log p(x1, x2.. x25|   1,   2)

against   1 (   ) and   2 (   )

max likelihood = (  1=-2.13,   2=1.668)
local minimum, but very close to global at (  1=2.085,   2=-1.257)*

* corresponds to switching w1 + w2.

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 23

duda & hart   s example

we can graph the 

prob. dist. function 
of data given our 
  1and   2
estimates.

we can also graph the 
true function from 
which the data was 
randomly generated.

    they are close.  good.
    the 2nd solution tries to put the    2/3    hump where the    1/3    hump should 

go, and vice versa.

    in this example unsupervised is almost as good as supervised.  if the x1.. 
x25are given the class which was used to learn them, then the results are 
(  1=-2.176,   2=1.684).  unsupervised got (  1=-2.13,   2=1.668). 

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 24

12

finding the max likelihood   1,  2..  k
we can compute  p( data |   1,  2..  k)
how do we find the   i   s which give max. likelihood?

    the normal max likelihood trick:
set   (cid:156) log prob (   .) = 0

(cid:156)   i

and solve for   i   s.

# here you get non-linear non-analytically-
solvable equations

    use id119
slow but doable

    use a much faster, cuter, and recently very popular 

method   

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 25

expectation 
maximalization

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 26

13

the e.m. algorithm

r

u

o

t

e

d

    we   ll get back to unsupervised learning soon.
    but now we   ll look at an even simpler case with 

hidden information.

    the em algorithm

(cid:137) can do trivial things, such as the contents of the next 

few slides.

(cid:137) an excellent way of doing our unsupervised learning 

problem, as we   ll see.

(cid:137) many, many other uses, including id136 of hidden 

markov models (future lecture).

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 27

silly example

let events be    grades in a class   

w1 = gets an a
w2 = gets a   b
w3 = gets a   c
w4 = gets a   d

p(a) =   
p(b) =   
p(c) = 2  
p(d) =   -3  

(note  0           1/6)

assume we want to estimate    from data.  in a given class 

there were

a   a   s
b   b   s
c   c   s
d   d   s

what   s the maximum likelihood estimate of    given a,b,c,d ?

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 28

14

silly example

let events be    grades in a class   

w1 = gets an a
w2 = gets a   b
w3 = gets a   c
w4 = gets a   d

p(a) =   
p(b) =   
p(c) = 2  
p(d) =   -3  
(note  0           1/6)

assume we want to estimate    from data.  in a given class there were

a   a   s
b   b   s
c   c   s
d   d   s

what   s the maximum likelihood estimate of    given a,b,c,d ?

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 29

trivial statistics

p(a) =       p(b) =   
p( a,b,c,d|   ) = k(  )a(  )b(2  )c(  -3  )d
log p( a,b,c,d|   ) = log k + alog    + blog    + clog 2   + dlog (  -3  )

p(d) =   -3  

p(c) = 2  

for

 

 max

like

set   ,
 

 

   

logp
   

  

=

0

   

logp
   

  

=

b
  

+

c
2
  2

   

gives

 

max 

like

 if so

class

 

got

    
 
=

d
3
  3
2/1
   
c
b
+
c
++
b
6

6
a
14

(
b

max 

like

   

=

1
10

copyright    2001, 2004, andrew w. moore

=

0

d

)

b o r

c
9

d
10

r u e !

t

 

i n g ,   b u t

id91 with gaussian mixtures: slide 30

15

same problem with hidden information
someone tells us that
number of high grades (a   s + b   s) = h
number of c   s                                 = c
number of d   s                                 = d
what is the max. like estimate of    now?

remember
p(a) =   
p(b) =   
p(c) = 2  
p(d) =   -3  

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 31

same problem with hidden information
someone tells us that
number of high grades (a   s + b   s) = h
number of c   s                                 = c
number of d   s                                 = d
what is the max. like estimate of    now?
we can answer this question circularly:
expectation

remember
p(a) =   
p(b) =   
p(c) = 2  
p(d) =   -3  

if we know the value of    we could compute the 
expected value of aand b

since the ratio a:b should be the same as the ratio    :   

maximization

a

=

1

1
2
  2
+

b
h
        

=

1

h

  
  2
+

if we know the expected values of a and b
we could compute the maximum likelihood 
value of   

     
=

cb
+
(
)dcb
6
++

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 32

16

e.m. for our trivial problem

we begin with a guess for   
we iterate between expectation and maximalization to 
improve our estimates of     and aand b.

remember
p(a) =   
p(b) =   
p(c) = 2  
p(d) =   -3  

define      (t)  the estimate of    on the t   th iteration

)0(  

tb
)(

t
(  

)1
=+

]

1

  
=

t
)(  |

e-step

[
b
  =

b(t)  the estimate of bon t   th iteration
guess
initial
 
 
=
h
  (t)
t
)(  2
+
( )
c
tb
+
(
( )
dc
tb
6
++
max 
est 
 
 
like
=
continue iterating until converged.
good news:  converging to local optimum is assured.
bad news:  i said    local    optimum.

)
    of

m-step

given 

( )tb

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 33

e.m. convergence

    convergence proof based on fact that prob(data |   ) must increase or 

remain same between each iteration [not obvious]

    but it can never exceed 1    [obvious]
so it must therefore converge   [obvious]

in our example, 
suppose we had
h = 20
c = 10
d = 10
  (0) = 0

convergence is generally linear: error 
decreases by a constant factor each time 
step.

copyright    2001, 2004, andrew w. moore

b(t)

  (t)

t
00
1
2
3
4
5
6
id91 with gaussian mixtures: slide 34

0.0833
0.0937
0.0947
0.0948
0.0948
0.0948

0
2.857
3.158
3.185
3.187
3.187
3.187

17

back to unsupervised learning of 

gmms

remember:

we have unlabeled data x1 x2     xr
we know there are k classes
we know p(w1) p(w2) p(w3)     p(wk)
we don   t know   1   2 ..   k

we can write p( data |   1   .   k) 
)

  ...  
1

=

x

r

k

(
x
p
1
r
   

...
(

p

x
i

k

      

p

i

1
=
r

i

1
=
r

j

1
=
k

=

=

=

  ...  
1
(
wx
i

j

)

k

) (
w
p  ...  ,

1

k

)

j

      

i

1
=

j

1
=

   
expk 
   
   

   

1
  2

2

(
x
i

   

  

(
w

j

)

p

2

)

j

   
   
   

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 35

e.m. for gmms
 
know
  ...  
 we
1

obpr

   

log

   
  
   
into
 
 this
 turns

i

(
data
for 

)

k

=

0

" :

max 

likelihood
for 
 ,

each 

j,

x
i
)

see

http://www.cs.cmu.edu/~awm/doc/gmm-algebra.pdf

for 

max 

likelihood

crazy 

algebra

n'

some
 wild'
r
(
   
xwp
i
i
1
==
r
   

    

(
xwp
i

j

j

j

i

1
=

  ...  ,

1

k

)

  ...  ,

1

k

this is  n  nonlinear equations in   j   s.   

if, for each xi we knew that for each wj the prob that   j was in class wj is
p(wj|xi,  1     k)   then    we would easily compute   j.

if we knew each   j then we could easily compute p(wj|xi,  1     k) for each wj
and xi.

   i feel an em experience coming on!!

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 36

18

e.m. for gmms

iterate.  on the t   th iteration let our estimates be

  t= {   1(t),   2(t)       c(t) }

e-step

compute    expected    classes of all datapoints for each class
tp
)(
(
i
xw
)
i

)
i
2
    

(
wx
i
k
(
wx
p
k

) (
w
p
i
)
  
t

wx
i
k
p

2
    

,
  
t
(
x

)
  
t

)
  
t

t
),(

p
   

t
(

),

=

=

p

(

i

,

,

,

c

k

k

j

j

i

tp
)(
j

p

m-step.  

j

1
=

just evaluate 
a gaussian at 
xk

compute max. like    given our data   s class membership distributions

(
t

)
1
=+

  

i

,

(
xw
k
i
(
xw
p
i

)
x
 
  
t
)
  
t

,

k

   
p
   

k

k

k

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 37

e.m. 

convergence

    your lecturer will 

(unless out of 
time) give you a 
nice intuitive 
explanation of 
why this rule 
works.

    as with all em 

procedures, 
convergence to a 
local optimum 
guaranteed.

    this algorithm is really used.  and 
in high dimensional state spaces, too.  
e.g. vector quantization for speech 
data

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 38

19

e.m. for general gmms
pi(t) is shorthand 
for estimate of 
p(  i)on t   th 
iteration
  t= {   1(t),   2(t)       c(t),   1(t),   2(t)       c(t), p1(t), p2(t)     pc(t) }

iterate.  on the t   th iteration let our estimates be

e-step

p

compute    expected    classes of all datapoints for each class
)
tpt
)(
)(
(
xw
)
i
tpt
)(
)(

(
wx
i
k
(
wx
p
k

) (
w
p
i
)
  
t

wx
i
k
p

,
  
t
(
x

)
  
t

)
  
t

t
(
  
i

  
j

t
(

),

),

  

  

=

=

p

(

p
   

,

,

,

c

k

k

j

j

j

i

i

m-step.  

just evaluate 
a gaussian at 
xk

compute max. like    given our data   s class membership distributions
(
t
+
  
i

(
xw
i

p

   

+

,

k

k

t

]
)
1

j

1
=

   

k

)
1
=+  

(
t

i

(
t

)
1
=+

  

i

,

(
xw
k
i
(
xw
p
i

)
x
 
  
t
)
  
t

,

k

k

   
p
   

k

k

k

)
[
x
 
  
t
   

k

(
t
   
  
i
(
xw
i

p

,

k

[
]
)
x
1
)
  
t

(
tp
i

)
1
=+

)
  ,
t

k

p

   

k

(
xw
i
r

r= #records

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 39

gaussian 
mixture 
example: 

start

advance apologies: in black 

and white this example will be 

incomprehensible

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 40

20

after first 
iteration

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 41

after 2nd 
iteration

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 42

21

after 3rd 
iteration

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 43

after 4th 
iteration

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 44

22

after 5th 
iteration

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 45

after 6th 
iteration

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 46

23

after 20th 
iteration

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 47

some bio 

assay 
data

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 48

24

gmm 

id91 

of the 

assay data

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 49

resulting 
density 
estimator

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 50

25

where are we now?

s id136

engine learn p(e1|e2) joint de, bayes net structure learning

t
u
p
n
i

s
t
u
p
n
i

classifier

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

predict
category

dec tree, sigmoid id88, sigmoid n.net, 
gauss/joint bc, gauss na  ve bc, n.neigh, bayes 
net based bc, cascade correlation

prob-
ability

predict
real no.

joint de, na  ve de, gauss/joint de, gauss na  ve 
de, bayes net structure learning, gmms

id75, polynomial regression, 
id88, neural net, n.neigh, kernel, lwr, 
rbfs, robust regression, cascade correlation, 
regression trees, gmdh, multilinear interp, mars

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 51

the old trick   

s id136

engine learn p(e1|e2) joint de, bayes net structure learning

t
u
p
n
i

s
t
u
p
n
i

classifier

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

predict
category

dec tree, sigmoid id88, sigmoid n.net, 
gauss/joint bc, gauss na  ve bc, n.neigh, bayes 
net based bc, cascade correlation, gmm-bc

prob-
ability

predict
real no.

joint de, na  ve de, gauss/joint de, gauss na  ve 
de, bayes net structure learning, gmms

id75, polynomial regression, 
id88, neural net, n.neigh, kernel, lwr, 
rbfs, robust regression, cascade correlation, 
regression trees, gmdh, multilinear interp, mars

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 52

26

three 
classes of 
assay
(each learned with 
it   s own mixture 
model)
(sorry, this will again be 
semi-useless in black and 
white)

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 53

resulting 
bayes 
classifier

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 54

27

resulting bayes 
classifier, using 
posterior 
probabilities to 
alert about 
ambiguity and 
anomalousness

yellow means 

anomalous

cyan means 
ambiguous

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 55

unsupervised learning with symbolic 
attributes

missing

nation

# kids

married

it   s just a    learning bayes net with known structure but 
hidden values    problem.
can use id119.

easy, fun exercise to do an em formulation for this case too.

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 56

28

final comments

    remember, e.m. can get stuck in local minima, and 

empirically it does.

    our unsupervised learning example assumed p(wi)   s 

known, and variances fixed and known.  easy to 
relax this.

    it   s possible to do bayesian unsupervised learning 

instead of max. likelihood.

    there are other algorithms for unsupervised 

learning. we   ll visit id116 soon. hierarchical 
id91 is also interesting.

    neural-net algorithms called    competitive learning    

turn out to have interesting parallels with the em 
method we saw.

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 57

what you should know

    how to    learn    maximum likelihood parameters 
(locally max. like.) in the case of unlabeled data.
    be happy with this kind of probabilistic analysis.
    understand the two examples of e.m. given in these 

notes.

for more info, see duda + hart.  it   s a great book.  
there   s much more in the book than in your 
handout.

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 58

29

other unsupervised learning 

methods

    id116 (see next lecture)
    hierarchical id91 (e.g. minimum spanning 

trees) (see next lecture)

    principal component analysis

simple, useful tool

    non-linear pca

neural auto-associators
locally weighted pca
others   

copyright    2001, 2004, andrew w. moore

id91 with gaussian mixtures: slide 59

30

