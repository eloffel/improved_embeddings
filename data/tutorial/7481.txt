note to other teachers and users of these slides. andrew would be delighted if you found 
this source material useful in giving your own lectures. feel free to use these slides 
verbatim, or to modify them to fit your own needs. powerpoint originals are available. if 
you make use of a significant portion of these slides in your own lecture, please include this 
message, or the following link to the source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . comments and corrections gratefully received. 

hidden markov 

models

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    andrew w. moore

slide 1

a markov system
has n states, called s1, s2 .. sn
there are discrete timesteps, 
t=0, t=1,    

s2

s3

s1

n = 3
t=0

copyright    andrew w. moore

slide 2

1

a markov system
has n states, called s1, s2 .. sn
there are discrete timesteps, 
t=0, t=1,    
on the t   th timestep the system is 
in exactly one of the available 
states. call it qt
note: qt    {s1, s2 .. sn }

s2

s3

current state
s1

n = 3
t=0
qt=q0=s3

copyright    andrew w. moore

slide 3

s2

s3

a markov system
has n states, called s1, s2 .. sn
there are discrete timesteps, 
t=0, t=1,    
on the t   th timestep the system is 
in exactly one of the available 
states. call it qt
note: qt    {s1, s2 .. sn }
between each timestep, the next 
state is chosen randomly.

current state

s1

n = 3
t=1
qt=q1=s2

copyright    andrew w. moore

slide 4

2

p(qt+1=s1|qt=s2) = 1/2
p(qt+1=s2|qt=s2) = 1/2
p(qt+1=s3|qt=s2) = 0

s2

s3

p(qt+1=s1|qt=s1) = 0
p(qt+1=s2|qt=s1) = 0
p(qt+1=s3|qt=s1) = 1

s1

n = 3
t=1
qt=q1=s2

p(qt+1=s1|qt=s3) = 1/3
p(qt+1=s2|qt=s3) = 2/3
p(qt+1=s3|qt=s3) = 0

a markov system
has n states, called s1, s2 .. sn
there are discrete timesteps, 
t=0, t=1,    
on the t   th timestep the system is 
in exactly one of the available 
states. call it qt
note: qt    {s1, s2 .. sn }
between each timestep, the next 
state is chosen randomly.
the current state determines the 
id203 distribution for the 
next state.

copyright    andrew w. moore

slide 5

p(qt+1=s1|qt=s1) = 0
p(qt+1=s2|qt=s1) = 0
p(qt+1=s3|qt=s1) = 1
1/2

s1

1/3

1

n = 3
t=1
qt=q1=s2

p(qt+1=s1|qt=s2) = 1/2
p(qt+1=s2|qt=s2) = 1/2
p(qt+1=s3|qt=s2) = 0

1/2

s2

2/3

s3

p(qt+1=s1|qt=s3) = 1/3
p(qt+1=s2|qt=s3) = 2/3
p(qt+1=s3|qt=s3) = 0

often notated with arcs 

between states

a markov system
has n states, called s1, s2 .. sn
there are discrete timesteps, 
t=0, t=1,    
on the t   th timestep the system is 
in exactly one of the available 
states. call it qt
note: qt    {s1, s2 .. sn }
between each timestep, the next 
state is chosen randomly.
the current state determines the 
id203 distribution for the 
next state.

copyright    andrew w. moore

slide 6

3

p(qt+1=s1|qt=s2) = 1/2
p(qt+1=s2|qt=s2) = 1/2
p(qt+1=s3|qt=s2) = 0

1/2

s2

2/3

s3

p(qt+1=s1|qt=s3) = 1/3
p(qt+1=s2|qt=s3) = 2/3
p(qt+1=s3|qt=s3) = 0

p(qt+1=s1|qt=s1) = 0
p(qt+1=s2|qt=s1) = 0
p(qt+1=s3|qt=s1) = 1
1/2

s1

1/3

1

n = 3
t=1
qt=q1=s2

markov property
qt+1 is conditionally independent 
of { qt-1, qt-2,     q1, q0 } given qt.
in other words:
p(qt+1 = sj |qt = si ) =
p(qt+1 = sj |qt = si ,any earlier history)
question: what would be the best 
bayes net structure to represent 
the joint distribution of ( q0, q1, 
    q3,q4 )?

copyright    andrew w. moore

slide 7

answer:

p(qt+1=s1|qt=s2) = 1/2
p(qt+1=s2|qt=s2) = 1/2
q0
p(qt+1=s3|qt=s2) = 0

p(qt+1=s1|qt=s1) = 0
p(qt+1=s2|qt=s1) = 0
p(qt+1=s3|qt=s1) = 1
1/2

q1

q2

1/3

s1

n = 3
t=1
qt=q1=s2

1/2

s2

2/3

s3

1

q3
p(qt+1=s1|qt=s3) = 1/3
p(qt+1=s2|qt=s3) = 2/3
p(qt+1=s3|qt=s3) = 0
q4

markov property
qt+1 is conditionally independent 
of { qt-1, qt-2,     q1, q0 } given qt.
in other words:
p(qt+1 = sj |qt = si ) =
p(qt+1 = sj |qt = si ,any earlier history)
question: what would be the best 
bayes net structure to represent 
the joint distribution of ( q0, q1, 
q2,q3,q4 )?

copyright    andrew w. moore

slide 8

4

answer:

p(qt+1=s1|qt=s2) = 1/2
p(qt+1=s2|qt=s2) = 1/2
q0
p(qt+1=s3|qt=s2) = 0

1/2

p(qt+1=s1|qt=si)
i
s2
a11
1
a21
2
a31
3
2/3
:
:
ai1
i
s3
an1

n

q2

1/3

1

q3
p(qt+1=s1|qt=s3) = 1/3
p(qt+1=s2|qt=s3) = 2/3
p(qt+1=s3|qt=s3) = 0
q4

p(qt+1=sj|qt=si)

p(qt+1=s2|qt=si)
a12
a22
a32
:
ai2

markov property
qt+1 is conditionally independent 
of { qt-1, qt-2,     q1, q0 } given qt.
p(qt+1=sn|qt=si)
   
a1j   
a1n
in other words:
a2n
a2j   
p(qt+1 = sj |qt = si ) =
a3n
a3j   
:
:
:
p(qt+1 = sj |qt = si ,any earlier history)
aij   
question: what would be the best 
bayes net structure to represent 
the joint distribution of ( q0, q1, 
q2,q3,q4 )?

   
   
   
   
:
ain   

ann   

anj   

an2

notation:

qp
(
t

=

1
+

qs
t
j

|

a

ij

=

=

s
i

)

slide 9

p(qt+1=s1|qt=s1) = 0
p(qt+1=s2|qt=s1) = 0
p(qt+1=s3|qt=s1) = 1
1/2

q1

s1

each of these 
n = 3
id203 
tables is 
t=1
identical
qt=q1=s2

copyright    andrew w. moore

a blind robot

a human and a 
robot wander 
around randomly 
on a grid   

r

h

state q =

location of robot,
location of human

 

 

( n u m .
  n  
n o t e :
*
  =   1 8  
s t a t e s )
1 8   =   3 2 4

copyright    andrew w. moore

slide 10

5

dynamics of system
q0 =

r

h

typical questions:

each timestep the 
human moves 
randomly to an 
adjacent cell.  and 
robot also moves 
randomly to an 
adjacent cell.

      what   s the expected time until the human is 
crushed like a bug?   
      what   s the id203 that the robot will hit the 
left wall before it hits the human?   
      what   s the id203 robot crushes human 
on next time step?   

copyright    andrew w. moore

slide 11

example question

   it   s currently time t, and human remains uncrushed.  what   s the 
id203 of crushing occurring at time t + 1 ?   
if robot is blind:

we   ll do this first

we can compute this in advance.

if robot is omnipotent:

(i.e. if robot knows state at time t), 
can compute directly.

if robot has some sensors, but 
incomplete state information    
id48 are 
applicable!

too easy. we 
won   t do this

main body
of lecture

copyright    andrew w. moore

slide 12

6

what is p(qt =s)? slow, stupid answer

step 1: work out how to compute p(q) for any path q 

= q1 q2 q3 .. qt
given we know the start state q1 (i.e. p(q1)=1)
p(q1 q2 .. qt) = p(q1 q2 .. qt-1) p(qt|q1 q2 .. qt-1) 
why?

= p(q1 q2 .. qt-1) p(qt|qt-1)
= p(q2|q1)p(q3|q2)   p(qt|qt-1)

step 2: use this knowledge to get p(qt =s)

qp
(
t

=

s

)

=

   

qp
(
)
t 
that 
end
 
in 
length 

s

q

   

paths

 of 

c o m p u t a t i o n   i s  
e x p o n e n t i a l   i n  

t

copyright    andrew w. moore

slide 13

what is p(qt =s) ? clever answer

    for each state si, define

pt(i) = prob. state is si at time t

= p(qt = si)

    easy to do inductive definition
i
   

=

)(0 ip

j
   

p
t

1
+

j
)(

=

qp
(
t

1
+

=

s

j

)

=

copyright    andrew w. moore

slide 14

7

what is p(qt =s) ? clever answer

    for each state si, define

pt(i) = prob. state is si at time t

= p(qt = si)

i
   

    easy to do inductive definition
is
start 
 the
 
otherwise
s
=

1
   
   
0
   
=

ip
)(0

j
)(

 s
i

j
   

 if

=

=

)

qp
(
t

p
t

1
+

1
+

j

state

copyright    andrew w. moore

slide 15

what is p(qt =s) ? clever answer

    for each state si, define

pt(i) = prob. state is si at time t

= p(qt = si)

i
   

    easy to do inductive definition
is
start 
 the
 
otherwise
s
=

1
   
   
0
   
=

ip
)(0

 s
i

j
   

 if

=

=

)

qp
(
t

1
+

j

state

qp
(
t

=

s

j

=   

q
t

s
i

)

=

1
+

1
+
n

j
p
)(
t
   

i

1
=

copyright    andrew w. moore

slide 16

8

what is p(qt =s) ? clever answer

    for each state si, define

pt(i) = prob. state is si at time t

= p(qt = si)

i
   

    easy to do inductive definition
is
start 
 the
 
otherwise
s
=

1
   
   
0
   
=

ip
)(0

 s
i

j
   

 if

=

=

)

qp
(
t

1
+

j

state

1
+
n

j
p
)(
t
   

i

1
=

qp
(
t

=

s

j

=   

q
t

s
i

)

=

1
+

a

ij

=

remember,
qs
qp
(
|
t
j
t

=

1
+

=

s
i

)

n

   

i

1
=

qp
(
t

1
+

=

qs
t
j

|

=

qps
(
i
t

)

=

s
i

)

=

n

   

i

1
=

ipa
)(
ij

t

copyright    andrew w. moore

slide 17

what is p(qt =s) ? clever answer

    for each state si, define

pt(i) = prob. state is si at time t

    computation is simple.
    just fill in this table in this

order:

= p(qt = si)

i
   

    easy to do inductive definition
is
start 
 the
 
otherwise
s
=

1
   
   
0
   
=

ip
)(0

 s
i

j
   

 if

=

=

)

qp
(
t

1
+

j

state

qp
(
t

=

s

j

=   

q
t

s
i

)

=

1
+

1
+
n

j
p
)(
t
   

i

1
=

pt(1)
0

pt(2)
1

   

pt(n)
0

t
0
1
:
tfinal

n

   

i

1
=

qp
(
t

1
+

=

qs
t
j

|

=

qps
(
i
t

)

=

s
i

)

=

n

   

i

1
=

ipa
)(
ij

t

copyright    andrew w. moore

slide 18

9

what is p(qt =s) ? clever answer

    for each state si, define

pt(i) = prob. state is si at time t

= p(qt = si)

i
   

    easy to do inductive definition
is
start 
 the
 
otherwise
s
=

1
   
   
0
   
=

ip
)(0

 s
i

j
   

 if

=

=

)

qp
(
t

1
+

j

state

    cost of computing pt(i) for all 

states si is now o(t n2)

    the stupid way was o(nt)
    this was a simple example
   

it was meant to warm you up 
to this trick, called dynamic 
programming, because 
id48s do many tricks like 
this.

1
+
n

j
p
)(
t
   

i

1
=

qp
(
t

=

s

j

=   

q
t

s
i

)

=

1
+

n

   

i

1
=

qp
(
t

1
+

=

qs
t
j

|

=

qps
(
i
t

)

=

s
i

)

=

n

   

i

1
=

ipa
)(
ij

t

copyright    andrew w. moore

slide 19

hidden state

   it   s currently time t, and human remains uncrushed.  what   s the 
id203 of crushing occurring at time t + 1 ?   
if robot is blind:

we   ll do this first

we can compute this in advance.

if robot is omnipotent:

(i.e. if robot knows state at time t), 
can compute directly.

if robot has some sensors, but 
incomplete state information    
id48 are 
applicable!

too easy. we 
won   t do this

main body
of lecture

copyright    andrew w. moore

slide 20

10

hidden state

    the previous example tried to estimate p(qt = si)

unconditionally (using no observed evidence).

    suppose we can observe something that   s affected 

by the true state.

    example: proximity sensors. (tell us the contents of 

the 8 adjacent squares)

r0

h

true state qt

w

w

w
  

h
what the robot sees: 
observation ot

w 
denotes 
   wall   

copyright    andrew w. moore

slide 21

noisy hidden state

    example: noisy proximity sensors. (unreliably tell us 

the contents of the 8 adjacent squares)

r0

h

true state qt

w

w

w
  

h
uncorrupted observation

w 
denotes 
   wall   

w

h

  
h

w
w

what the robot sees: 
observation ot

copyright    andrew w. moore

slide 22

11

noisy hidden state

    example: noisy proximity sensors. (unreliably tell us 

the contents of the 8 adjacent squares)

r0

2

h

w

w

w
  

w 
denotes 
   wall   

h
uncorrupted observation

true state qt
ot is noisily determined depending on 
the current state.
assume that ot is conditionally 
independent of {qt-1, qt-2,     q1, q0 ,ot-1, 
ot-2,     o1, o0 } given qt.
in other words:
p(ot = x |qt = si ) =
p(ot = x |qt = si ,any earlier history)

w

h

  
h

w
w

what the robot sees: 
observation ot

copyright    andrew w. moore

slide 23

noisy hidden state

    example: noisy proximity sensors. (unreliably tell us 

the contents of the 8 adjacent squares)

r0

2

h

w

w

w
  

w 
denotes 
   wall   

h
uncorrupted observation

true state qt
ot is noisily determined depending on 
the current state.
assume that ot is conditionally 
independent of {qt-1, qt-2,     q1, q0 ,ot-1, 
ot-2,     o1, o0 } given qt.
in other words:
p(ot = x |qt = si ) =
p(ot = x |qt = si ,any earlier history)

w

h

  
h

w
w

what the robot sees: 
observation ot
question: what   d be the best bayes net 
structure to represent the joint distribution 
of (q0, q1, q2,q3,q4 ,o0, o1, o2,o3,o4 )?

copyright    andrew w. moore

slide 24

12

answer:

q0

noisy hidden state
o0

    example: noisy proximity sensors. (unreliably tell us 

the contents of the 8 adjacent squares)

q1

h

r0

o1

2

w

w

w
  

w 
denotes 
   wall   

h
uncorrupted observation

q2

o2

q3

true state qt
ot is noisily determined depending on 
the current state.
assume that ot is conditionally 
independent of {qt-1, qt-2,     q1, q0 ,ot-1, 
o3
ot-2,     o1, o0 } given qt.
in other words:
p(ot = x |qt = si ) =
p(ot = x |qt = si ,any earlier history)

q4

o4

w

h

  
h

w
w

what the robot sees: 
observation ot
question: what   d be the best bayes net 
structure to represent the joint distribution 
of (q0, q1, q2,q3,q4 ,o0, o1, o2,o3,o4 )?

copyright    andrew w. moore

slide 25

answer:

noisy hidden state
notation:
qkop
(
o0
t
the contents of the 8 adjacent squares)

)
    example: noisy proximity sensors. (unreliably tell us 

kb
)(
i

q0

s
i

=

=

=

|

t

2

h

r0

o2

o1

q2

q1

p(ot=1|qt=si)
b1(1)
b2 (1)
b3 (1)
:
bi(1)
:
bn (1)

i
1
2
3
true state qt
:
ot is noisily determined depending on 
i
the current state.
:
n
assume that ot is conditionally 
independent of {qt-1, qt-2,     q1, q0 ,ot-1, 
o3
ot-2,     o1, o0 } given qt.
in other words:
p(ot = x |qt = si ) =
p(ot = x |qt = si ,any earlier history)

q3

q4

o4

w

w

   

   

   

   
   

   
   

p(ot=2|qt=si)
b1 (2)
b2 (2)
b3 (2)
:
bi (2)
:
bn (2)

p(ot=m|qt=si)
b1(m)
w 
b2 (m)
denotes 
b3 (m)
   wall   
h
:
uncorrupted observation
bi (m)
:
w
bn (m)
w

p(ot=k|qt=si)
b1 (k)
w
b2(k)
  
b3(k)
:
bi(k)
:
w
bn(k)
h

:
   
  
h

:
   

:
   

:
   

   

what the robot sees: 
observation ot
question: what   d be the best bayes net 
structure to represent the joint distribution 
of (q0, q1, q2,q3,q4 ,o0, o1, o2,o3,o4 )?

copyright    andrew w. moore

slide 26

13

id48

our robot with noisy sensors is a good example of an id48
    question 1: state estimation
what is p(qt=si | o1o2   ot)
it will turn out that a new cute d.p. trick will get this for us.

    question 2: most probable path

given o1o2   ot , what is the most probable path that i took?
and what is that id203?
yet another famous d.p. trick, the viterbi algorithm, gets 

this.

    question 3: learning id48s:

given o1o2   ot , what is the maximum likelihood id48 that 

could have produced this string of observations?

very very useful. uses the e.m. algorithm

copyright    andrew w. moore

slide 27

are h.m.m.s useful?

you bet !!
    robot planning + sensing when there   s uncertainty 

(e.g. reid simmons / sebastian thrun / sven 
koenig)

    id103/understanding

phones     words, signal     phones

    human genome project

complicated stuff your lecturer knows nothing 

about.

    consumer decision modeling
    economics & finance.
plus at least 5 other things i haven   t thought of.

copyright    andrew w. moore

slide 28

14

some famous id48 tasks

question 1: state estimation

what is p(qt=si | o1o2   ot)

copyright    andrew w. moore

slide 29

some famous id48 tasks

question 1: state estimation

what is p(qt=si | o1o2   ot)

copyright    andrew w. moore

slide 30

15

some famous id48 tasks

question 1: state estimation

what is p(qt=si | o1o2   ot)

copyright    andrew w. moore

slide 31

some famous id48 tasks

question 1: state estimation

what is p(qt=si | o1o2   ot)
question 2: most probable path

given o1o2   ot , what is 
the most probable path 
that i took?

copyright    andrew w. moore

slide 32

16

some famous id48 tasks

question 1: state estimation

what is p(qt=si | o1o2   ot)
question 2: most probable path

given o1o2   ot , what is 
the most probable path 
that i took?

copyright    andrew w. moore

slide 33

some famous id48 tasks

question 1: state estimation

what is p(qt=si | o1o2   ot)
question 2: most probable path

given o1o2   ot , what is 
the most probable path 
that i took?

woke up at 8.35, got on bus at 9.46, 
sat in lecture 10.05-11.22   

copyright    andrew w. moore

slide 34

17

some famous id48 tasks

question 1: state estimation

what is p(qt=si | o1o2   ot)
question 2: most probable path

given o1o2   ot , what is 
the most probable path 
that i took?

question 3: learning id48s:
given o1o2   ot , what is 
the maximum likelihood 
id48 that could have 
produced this string of 
observations?

copyright    andrew w. moore

slide 35

some famous id48 tasks

question 1: state estimation

what is p(qt=si | o1o2   ot)
question 2: most probable path

given o1o2   ot , what is 
the most probable path 
that i took?

question 3: learning id48s:
given o1o2   ot , what is 
the maximum likelihood 
id48 that could have 
produced this string of 
observations?

copyright    andrew w. moore

slide 36

18

some famous id48 tasks

ot

abb

question 1: state estimation

what is p(qt=si | o1o2   ot)
question 2: most probable path
ba(ot-1)

ot-1

given o1o2   ot , what is 
the most probable path 
that i took?

aaa

bb(ot)

bus

aba

abc

aab

eat

acb

walk

ot+1

bc(ot+1)

acc

question 3: learning id48s:
given o1o2   ot , what is 
the maximum likelihood 
id48 that could have 
produced this string of 
observations?

copyright    andrew w. moore

slide 37

basic operations in id48s

for an observation sequence o = o1   ot, the three basic id48 

operations are:

problem

algorithm

evaluation:
calculating p(qt=si | o1o2   ot)
id136:
computing q* = argmaxq p(q|o)
learning:
computing   * = argmax  p(o|  )

forward-backward

viterbi decoding

baum-welch (em)

t = # timesteps, n = # states

complexity

+

o(tn2)

o(tn2)

o(tn2)

copyright    andrew w. moore

slide 38

19

id48 notation

(from rabiner   s survey)
the states are labeled s1 s2 .. sn
for a particular trial   .

*l. r. rabiner, "a tutorial on 
id48 and 
selected applications in speech 
recognition," proc. of the ieee, 
vol.77, no.2, pp.257--286, 1989.
available from
http://ieeexplore.ieee.org/iel5/5/698/00018626.pdf?arnumber=18626

let t
t

be the number of observations
is also the number of states passed 
through

o = o1 o2 .. ot is the sequence of observations
q = q1 q2 .. qt     is the notation for a path of states

   =    n,m,{  i,},{aij},{bi(j)}   

is the specification of an 
id48

copyright    andrew w. moore

slide 39

id48 formal definition

an id48,   , is a 5-tuple consisting of
    n   the number of states
    m   the number of possible observations
   

{  1,   2, ..   n}  the starting state probabilities

p(q0 = si) =   i

a11
a21
:
an1

   

   

a22
a22
:
an2

   
   

   

b1(1)
b2(1)
:
bn(1)

b1(2)    
b2(2)    
:
bn(2)    

a1n
a2n
:
ann

b1(m) 
b2(m) 
:
bn(m)

this is new. in our 
previous example, 
start state was 
deterministic

the state transition probabilities

p(qt+1=sj | qt=si)=aij

the observation probabilities

p(ot=k | qt=si)=bi(k)

copyright    andrew w. moore

slide 40

20

start randomly in state 1 or 2
choose one of the output 
symbols in each state at 
random.

here   s an id48

s1

xy

1/3

1/3

zx

2/3

1/3

n = 3
m = 3
  1 = 1/2

a11 = 0
a12 = 1/3
a13 = 1/3

  2 = 1/2

a12 = 1/3
a22 = 0
a32 = 1/3

b1 (x) = 1/2
b2 (x) = 0
b3 (x) = 1/2

b1 (y) = 1/2
b2 (y) = 1/2
b3 (y) = 0

s2

z y

2/3

1/3

s3

1/3
  3 = 0

a13 = 2/3
a13 = 2/3
a13 = 1/3

b1 (z) = 0
b2 (z) = 1/2
b3 (z) = 1/2

copyright    andrew w. moore

slide 41

s2

z y

here   s an id48

s1

xy

1/3

1/3

zx

2/3

1/3

n = 3
m = 3
  1 =   

a11 = 0
a12 =    
a13 =    

  2 =   

a12 =    
a22 = 0
a32 =    

2/3

1/3

s3

1/3
  3 = 0

a13 =    
a13 =    
a13 =    

b1 (x) =   
b2 (x) = 0
b3 (x) =   

b1 (y) =   
b2 (y) =   
b3 (y) = 0

b1 (z) = 0
b2 (z) =   
b3 (z) =   

start randomly in state 1 or 2
choose one of the output 
symbols in each state at 
random.
let   s generate a sequence of 
observations:

50-50 choice 
between s1 and 

s2

q0=
q1=
q2=

__
__
__

o0=
o1=
o2=

__
__
__

copyright    andrew w. moore

slide 42

21

here   s an id48

s1

xy

1/3

1/3

zx

2/3

1/3

n = 3
m = 3
  1 =   

a11 = 0
a12 =    
a13 =    

  2 =   

a12 =    
a22 = 0
a32 =    

2/3

1/3

s3

1/3
  3 = 0

a13 =    
a13 =    
a13 =    

b1 (x) =   
b2 (x) = 0
b3 (x) =   

b1 (y) =   
b2 (y) =   
b3 (y) = 0

b1 (z) = 0
b2 (z) =   
b3 (z) =   

s2

z y

start randomly in state 1 or 2
choose one of the output 
symbols in each state at 
random.
let   s generate a sequence of 
observations:

50-50 choice 

between x and y

q0=
q1=
q2=

s1
__
__

o0=
o1=
o2=

__
__
__

copyright    andrew w. moore

slide 43

s2

z y

here   s an id48

s1

xy

1/3

1/3

zx

2/3

1/3

n = 3
m = 3
  1 =   

a11 = 0
a12 =    
a13 =    

  2 =   

a12 =    
a22 = 0
a32 =    

2/3

1/3

s3

1/3
  3 = 0

a13 =    
a13 =    
a13 =    

b1 (x) =   
b2 (x) = 0
b3 (x) =   

b1 (y) =   
b2 (y) =   
b3 (y) = 0

b1 (z) = 0
b2 (z) =   
b3 (z) =   

start randomly in state 1 or 2
choose one of the output 
symbols in each state at 
random.
let   s generate a sequence of 
observations:

goto s3 with 

id203 2/3 or 
s2 with prob. 1/3

q0=
q1=
q2=

s1
__
__

o0=
o1=
o2=

x
__
__

copyright    andrew w. moore

slide 44

22

here   s an id48

s1

xy

1/3

1/3

zx

2/3

1/3

n = 3
m = 3
  1 =   

a11 = 0
a12 =    
a13 =    

  2 =   

a12 =    
a22 = 0
a32 =    

2/3

1/3

s3

1/3
  3 = 0

a13 =    
a13 =    
a13 =    

b1 (x) =   
b2 (x) = 0
b3 (x) =   

b1 (y) =   
b2 (y) =   
b3 (y) = 0

b1 (z) = 0
b2 (z) =   
b3 (z) =   

s2

z y

start randomly in state 1 or 2
choose one of the output 
symbols in each state at 
random.
let   s generate a sequence of 
observations:

50-50 choice 
between z and x

q0=
q1=
q2=

s1
s3
__

o0=
o1=
o2=

x
__
__

copyright    andrew w. moore

slide 45

s2

z y

here   s an id48

s1

xy

1/3

1/3

zx

2/3

1/3

n = 3
m = 3
  1 =   

a11 = 0
a12 =    
a13 =    

  2 =   

a12 =    
a22 = 0
a32 =    

2/3

1/3

s3

1/3
  3 = 0

a13 =    
a13 =    
a13 =    

b1 (x) =   
b2 (x) = 0
b3 (x) =   

b1 (y) =   
b2 (y) =   
b3 (y) = 0

b1 (z) = 0
b2 (z) =   
b3 (z) =   

start randomly in state 1 or 2
choose one of the output 
symbols in each state at 
random.
let   s generate a sequence of 
observations:

each of the three 

next states is 
equally likely

q0=
q1=
q2=

s1
s3
__

o0=
o1=
o2=

x
x
__

copyright    andrew w. moore

slide 46

23

here   s an id48

s1

xy

1/3

1/3

zx

2/3

1/3

n = 3
m = 3
  1 =   

a11 = 0
a12 =    
a13 =    

  2 =   

a12 =    
a22 = 0
a32 =    

2/3

1/3

s3

1/3
  3 = 0

a13 =    
a13 =    
a13 =    

b1 (x) =   
b2 (x) = 0
b3 (x) =   

b1 (y) =   
b2 (y) =   
b3 (y) = 0

b1 (z) = 0
b2 (z) =   
b3 (z) =   

s2
s2

z y

start randomly in state 1 or 2
choose one of the output 
symbols in each state at 
random.
let   s generate a sequence of 
observations:

50-50 choice 
between z and x

q0=
q1=
q2=

s1
s3
s3

o0=
o1=
o2=

x
x
__

copyright    andrew w. moore

slide 47

here   s an id48

s1

xy

1/3

1/3

zx

2/3

1/3

n = 3
m = 3
  1 =   

a11 = 0
a12 =    
a13 =    

  2 =   

a12 =    
a22 = 0
a32 =    

2/3

1/3

s3

1/3
  3 = 0

a13 =    
a13 =    
a13 =    

b1 (x) =   
b2 (x) = 0
b3 (x) =   

b1 (y) =   
b2 (y) =   
b3 (y) = 0

b1 (z) = 0
b2 (z) =   
b3 (z) =   

s2

z y

start randomly in state 1 or 2
choose one of the output 
symbols in each state at 
random.
let   s generate a sequence of 
observations:

q0=
q1=
q2=

s1
s3
s3

o0=
o1=
o2=

x
x
z

copyright    andrew w. moore

slide 48

24

state estimation

s1

xy

1/3

1/3

zx

2/3

1/3

z y

2/3

1/3

s3

1/3
  3 = 0

a13 =    
a13 =    
a13 =    

  2 =   

a12 =    
a22 = 0
a32 =    

n = 3
m = 3
  1 =   

a11 = 0
a12 =    
a13 =    

b1 (x) =   
b2 (x) = 0
b3 (x) =   

b1 (y) =   
b2 (y) =   
b3 (y) = 0

b1 (z) = 0
b2 (z) =   
b3 (z) =   

s2

start randomly in state 1 or 2
choose one of the output 
symbols in each state at 
random.
let   s generate a sequence of 
observations:
this is what the 
observer has to 

work with   

q0=
q1=
q2=

?
?
?

o0=
o1=
o2=

x
x
z

copyright    andrew w. moore

slide 49

prob. of a series of observations

what is p(o) = p(o1 o2 o3) = 
p(o1 = x ^ o2 = x ^ o3 = z)?

p

slow, stupid way:
   
   

o
(

paths

=

=

 of 

)

q

   

q

   

paths

 of 

)

   

qo
p
(
length 
3
qqo
p
(
length 
3

p

(

)

|

)

s1

xy

1/3

1/3

zx

2/3

1/3

s2

z y

2/3

1/3

s3

1/3

how do we compute p(q)  for 

an arbitrary path q?

how do we compute p(o|q) 

for an arbitrary path q?

copyright    andrew w. moore

slide 50

25

prob. of a series of observations

what is p(o) = p(o1 o2 o3) = 
p(o1 = x ^ o2 = x ^ o3 = z)?

p

slow, stupid way:
   
   

o
(

paths

=

=

 of 

)

q

   

q

   

paths

 of 

)

   

qo
p
(
length 
3
qqo
p
(
length 
3

p

(

)

|

)

how do we compute p(q)  for 

an arbitrary path q?

how do we compute p(o|q) 

for an arbitrary path q?

s1

xy

1/3

1/3

zx

2/3

1/3

s2

z y

2/3

1/3

s3

1/3

p(q)= p(q1,q2,q3)
=p(q1) p(q2,q3|q1) (chain rule)
=p(q1) p(q2|q1) p(q3| q2,q1)  (chain)
=p(q1) p(q2|q1) p(q3| q2) (why?)
example in the case q = s1 s3 s3:
=1/2 * 2/3 * 1/3 = 1/9

copyright    andrew w. moore

slide 51

prob. of a series of observations

what is p(o) = p(o1 o2 o3) = 
p(o1 = x ^ o2 = x ^ o3 = z)?

p

slow, stupid way:
   
   

o
(

paths

=

=

 of 

)

q

   

q

   

paths

 of 

)

   

qo
p
(
length 
3
qqo
p
(
length 
3

p

(

)

|

how do we compute p(q)  for 

an arbitrary path q?

how do we compute p(o|q) 

for an arbitrary path q?

s1

xy

1/3

1/3

zx

2/3

1/3

s2

z y

2/3

1/3

s3

1/3

)
p(o|q)
= p(o1 o2 o3 |q1 q2 q3 )
= p(o1 | q1 ) p(o2 | q2 ) p(o3 | q3 ) (why?)
example in the case q = s1 s3 s3:
= p(x| s1) p(x| s3) p(z| s3) =
=1/2 * 1/2 * 1/2 = 1/8

copyright    andrew w. moore

slide 52

26

prob. of a series of observations

what is p(o) = p(o1 o2 o3) = 
p(o1 = x ^ o2 = x ^ o3 = z)?

p

slow, stupid way:
   
   

o
(

paths

=

=

 of 

)

q

   

q

   

paths

 of 

)

   

qo
p
(
length 
3
qqo
p
(
length 
3

p

(

)

|

how do we compute p(q)  for 

an arbitrary path q?

how do we compute p(o|q) 

for an arbitrary path q?

s1

xy

1/3

1/3

zx

2/3

1/3

s2

z y

2/3

1/3

s3

1/3

  w o u l d   n e e d   2 7   p ( q )
c o m p u t a t i o n s   a n d   2 7   p ( o | q )
c o m p u t a t i o n s
v a t i o n s   w o u l d   n e e d   3 2 0
3 . 5   b illi o n   c o m p u t a t i o n s   a n d   3 . 5   b illi o n   p ( o | q )
  2 0   o b s e r

)
p(o|q)
p ( o )
= p(o1 o2 o3 |q1 q2 q3 )
= p(o1 | q1 ) p(o2 | q2 ) p(o3 | q3 ) (why?)
example in the case q = s1 s3 s3:
= p(x| s1) p(x| s3) p(z| s3) =
a   s e q u e n c e   o f
=1/2 * 1/2 * 1/2 = 1/8
c o m p u t a t i o n s

so let   s be smarter   

=  

copyright    andrew w. moore

slide 53

the prob. of a given series of 

observations, non-exponential-cost-style

given observations o1 o2     ot
define

  t(i) = p(o1 o2     ot     qt = si |   )         where 1     t     t

  t(i) =   id203 that, in a random trial,

    we   d have seen the first t observations 
    we   d have ended up in si as the t   th state visited. 

in our example, what is   2(3) ?

copyright    andrew w. moore

slide 54

27

  t(i): easy to define recursively

  t(i) = p(o1 o2     ot     qt = si |   ) (  t(i) can be defined stupidly by considering all paths length    t   . how?)

( )
i

  
1

  
t
1
+

( )
j

i

)

(
)
qo
s
p 
=   
=
i
1
1
) (
(
s
q
qo
s
p
p
=
=
=
1
1
1
          
          
          
          
what?
=
)
(
oooo
q
s
...
p 
=
   
t
t
1
=

=

1
+

1
+

2

j

t

i

copyright    andrew w. moore

slide 55

  t(i): easy to define recursively

  t(i) = p(o1 o2     ot     qt = si |   ) (  t(i) can be defined stupidly by considering all paths length    t   . how?)

( )
i

  
1

  
t
1
+

( )
j

i

i

)

)
(
s
qo
p 
=   
=
i
1
1
) (
(
s
qo
s
q
p
p
=
=
=
1
1
1
          
          
          
          
what?
=
)
(
q
s
oooo
...
p 
=
   
t
t
1
n
   
os
i
t

(
qooo
t
1

=   

...

   

p

=

=

1
+

1
+

1
+

2

2

t

j

t

q
t

1
+

=

s

)

j

   

i

1
=
n

i

i

1
=

   
   
   
   

i

i

=

=

=

=

t

t

1
+

1
+

p

p

(
qo
,
t
(
qo
,
t
(
q
t
(
oba
ij
t

p

=

1
+

j

1
+

qs
j
t
)
  
t

=
( )i

=

=

1
+

1
+

t

1

2

qooos
=   
j
t
( )
i

...
)
  
t

qs
j
t

i

s
=
) (
qos
p
i
t

1
+

t

=

s

1
+

)
  
t

j

( )
i

) (
p

qooo
t
1

=   

...

2

t

s

i

)

s

i

copyright    andrew w. moore

slide 56

28

in our example

=   

t

  
t
  
1
  
t
1
+

(
( )
qooo
i
p
..
=
t
1
2
(
)
( )
ob
i
  
  
=
i
i
1
( )
(
   
oba
j
=
ij
t

1
+

j

)
  
t

i

)
  

s

i

( )i

s1

xy

2/3

1/3

1/3

1/3

zx

s2

z y

2/3

1/3

s3

1/3

=

  
1

     

we saw   o1 o2 o3 = x x z
( )
2
( )
2
( )
2

1
4
     0

( )
1
( )
1
( )
1

     0

  
2

  
2

  
1

=

=

  
3

  
3

=

     0

  
1

=

( )
3
( )
3
( )
3

=

=

0
1
12
1
72

  
2

  
3

=

     0
1
72

     

=

copyright    andrew w. moore

slide 57

easy question

we can cheaply compute

  t(i)=p(o1o2   ot   qt=si)

(how) can we cheaply compute

p(o1o2   ot)   ?

(how) can we cheaply compute

p(qt=si|o1o2   ot)

copyright    andrew w. moore

slide 58

29

easy question

we can cheaply compute

  t(i)=p(o1o2   ot   qt=si)

(how) can we cheaply compute

p(o1o2   ot)   ?

(how) can we cheaply compute

p(qt=si|o1o2   ot)

n

   

i

1
=

t i
)(  

  
t
n
   
  
t

i
)(
j
)(

j

1
=

copyright    andrew w. moore

slide 59

most probable path given observations

ooo
...
t
1

,

i.e.

2
?

)

1

2

q

 swhat'
iswhat 

most 
probable
path 
given 
 
(
argmax
oooq
   
p  
...
t
answer
stupid
 
 
slow,
:
(
argmax
oooq
     
p  
...
t
(
qqooo
1
p
(
)qqooo

)
(p
...
t
2
(
)
ooo
...
t
1
) (
...
p

argmax
  
argmax

p  

p

=

=

)

)

q

q

t

2

1

2

2

1

q

copyright    andrew w. moore

slide 60

30

efficient mpp computation

we   re going to compute the following variables:
  t(i)=      max        p(q1 q2 .. qt-1     qt = si     o1 .. ot)

q1q2..qt-1

=  the id203 of the path of length t-1 with the 
maximum chance of doing all these things:

   occuring

and

and

   ending up in state si

   producing output o1   ot
define:
mppt(i) =  that path
so:                   t(i)= prob(mppt(i))

copyright    andrew w. moore

slide 61

( )
i

=

  
t

mpp
t

  
1

( )
i
( )
i

1
   

max

max

q
t

q
...
t

=   

(
qq
21

ooos
..
i
t

the viterbi algorithm
q
qq
...
)
p  1
t
21
   
arg
qq
q
...
p  1
t
21
   
max
(
q
p  
 one
choice
1
) (
(
qo
q
s
p
p
=
1
1
1
)1
(
ob
  
i
i

os
=
i
1
s
=

ooos
..
i
t

(
qq
21

=   

q
...
t

   
)

q
t

   

   

)

)

1
   

2

1

2

1

i

i

=

=
=
=

now, suppose we have all the   t(i)   s and mppt(i)   s for all i.          
how to get   t+1(j) and
mppt(1) 
mppt(2) 
:
mppt(n) 

?:

mppt+1(j)?  
s1
s2

prob=  t(2)

prob=  t(1)

prob=  t(n)

sn
qt

sj

qt+1

copyright    andrew w. moore

slide 62

31

the viterbi algorithm

time t+1

sj

time t
s1
:
si
:

the most prob path with last 

two states si  sj
is

the most prob path to si , 

followed by transition si     sj

copyright    andrew w. moore

slide 63

the viterbi algorithm

time t+1

sj

time t
s1
:
si
:

the most prob path with last 

two states si  sj
is

the most prob path to si , 

followed by transition si     sj

what is the prob of that path?

  t(i) x p(si     sj     ot+1 |   )
  t(i) aij bj (ot+1)

=

so   the most probable path to sj has 

si* as its penultimate state
where  i*=argmax   t(i) aij bj (ot+1)

copyright    andrew w. moore

slide 64

i

32

the viterbi algorithm

time t+1

sj

time t
s1
:
si
:

the most prob path with last 

two states si  sj
is

the most prob path to si , 

followed by transition si     sj

what is the prob of that path?

  t(i) x p(si     sj     ot+1 |   )
  t(i) aij bj (ot+1)

=

so   the most probable path to sj has 

si* as its penultimate state
where  i*=argmax   t(i) aij bj (ot+1)

summary:
  t+1(j)  =    t(i*) aij bj (ot+1)
mppt+1(j)  =  mppt+1(i*)si*

} with i* defined 

to the left

copyright    andrew w. moore

slide 65

i

what   s viterbi used for?

classic example

id103:

signal     words

id48     observable is signal

    hidden state is part of word 

formation

what is the most probable word given this signal?

utterly gross simplification
in practice: many levels of id136; not 

one big jump.

copyright    andrew w. moore

slide 66

33

id48s are used and useful

but how do you design an id48?

occasionally, (e.g. in our robot example) it is reasonable to 
deduce the id48 from first principles.

but usually, especially in speech or genetics, it is better to infer 
it from large amounts of data.  o1 o2 .. ot with a big    t   .

observations previously

in lecture

observations in the 

next bit

o1 o2 .. ot

o1 o2 .. ot

copyright    andrew w. moore

slide 67

inferring an id48

remember, we   ve been doing things like

p(o1 o2 .. ot |    )

that          is the notation for our id48 parameters.
now we have some observations and we want to 

estimate    from them.

as usual: we could use
(i) max likelihood      = argmax p(o1 .. ot |   )

  

(ii) bayes
work out p(    | o1 .. ot )

and then take e[  ] or max p(    | o1 .. ot )

  

copyright    andrew w. moore

slide 68

34

max likelihood id48 estimation

define

  t(i) = p(qt = si | o1o2   ot ,    )
  t(i,j) = p(qt = si     qt+1 = sj | o1o2   ot ,   )

  t(i)  and   t(i,j)  can be computed efficiently      i,j,t

(details in rabiner paper)

=

( )
i

t
1
   
  
   
t
t
1
=
t
1
   
(
ji
,
  
   
t
t
1
=

expected number of transitions 
out of state i during the path

) =

expected number of transitions from 
state i to state j during the path

copyright    andrew w. moore

slide 69

t
t

1
=
1
   

   

t

1
=

id48 

estimation

( )
i
  
t
(
i
,
  
t
t
1
   
   
  
t

(
q
p
=
t
)
j
p
=
( )
i

=

1
   

ooos
..
,
i
t
2
q
=
=
t

=
(
q
t
expected
number 
 

s

1
+

i

)
  
ooos
..
t
j

2

1

,

)
  
 ns

of

 transitio

out 

 of

state

 i 

 
path
during

(
i

,

  
t

)

j

=

expected
number 
 

of

 transitio

 ns

out 

and i of
into
 

 j 

during
path
 

expected
frequency
 
j 
 i
   
frequency
expected
 
i
s 

   
      
   
   
      
   
state

 this

state

j

)

s 
i

t

t

,

j

1
   

)

=

(
i

  
t

( )
i

1
=
t
1
   

 
notice

   
   
      
   
   
   
  
      
t
   
t
1
=
(
 of 
next 
estimate
 
prob
=
 we
-re
estimate
(
)
   
j
i
,
  
t
( )
   
i
  
t
estimate
-re 
also
)
          
      
l   

can 
(
o
k

 we
b    

a
        

can 

   

ij

j

(see
rabiner)
 

copyright    andrew w. moore

slide 70

35

we want

ija
=new

new estimate of

qp
(
t

=+
1

qs
t
j

|

=

s
i

)

copyright    andrew w. moore

slide 71

we want

ija
=new
new estimate of
i
expected
# 
 transitio
 ns
   
= n
   
i
# 
 transitio
 ns
expected

qp
(
t
j

   

=+
1
|
old
  
k
|

qs
|
=
t
j
oo
,
,
,
1
2
oo
,
,
old
  
1
2

s
i

)
o
t
o
t

l

l
,

k

1
=

copyright    andrew w. moore

slide 72

36

we want

ija
=new
new estimate of
i
expected
# 
 transitio
 ns
   
= n
   
i
# 
 transitio
 ns
expected
   
= n
      

qs
,
t
k

qs
,
t
j

qp
(
t

qp
(
t

old
  

1
=
t

s
i

s
i

=

=

=

=

1
+

1
+

1
=

|

|

t

k

t

k

1
=

t

1
=

qp
(
t
j

   

=+
1
|
old
  
k
|

qs
|
=
t
j
oo
,
,
,
1
2
oo
,
,
old
  
1
2

s
i

)
o
t
o
t

l

l
,

,

oo
1
2

,

,

o
t

)

l

old
  

,

oo
1
2

,

,

o
t

)

l

copyright    andrew w. moore

slide 73

we want

ija
=new
new estimate of
i
expected
# 
 transitio
 ns
   
= n
   
i
# 
 transitio
 ns
expected
   
= n
      

qs
,
t
k

qs
,
t
j

qp
(
t

qp
(
t

old
  

1
=
t

s
i

s
i

=

=

=

=

1
+

1
+

1
=

|

|

t

k

t

k

1
=

t

1
=

qp
(
t
j

   

=+
1
|
old
  
k
|

qs
|
=
t
j
oo
,
,
,
1
2
oo
,
,
old
  
1
2

s
i

)
o
t
o
t

l

l
,

,

oo
1
2

,

,

o
t

)

l

old
  

,

oo
1
2

,

,

o
t

)

l

s
= n
   

k

1
=

where

s

ij

=

ij
s

ik

t

   

t

1
=

qp
(
t

1
+

=

qs
,
j
t

=

os
i
1

,

,

=

what?

old
  l

o
t

|

)

copyright    andrew w. moore

slide 74

37

we want

ija
=new
new estimate of
i
expected
# 
 transitio
 ns
   
= n
   
i
# 
 transitio
 ns
expected
   
= n
      

qs
,
t
k

qs
,
t
j

qp
(
t

qp
(
t

old
  

1
=
t

s
i

s
i

=

=

=

=

1
+

1
+

1
=

|

|

t

k

t

k

1
=

t

1
=

qp
(
t
j

   

=+
1
|
old
  
k
|

qs
|
=
t
j
oo
,
,
,
1
2
oo
,
,
old
  
1
2

s
i

)
o
t
o
t

l

l
,

,

oo
1
2

,

,

o
t

)

l

old
  

,

oo
1
2

,

,

o
t

)

l

s
= n
   

k

1
=

ij
s

ik

where

s

ij

=

=

t

   
t
1
=
a
ij

qp
(
t
t

   

t

1
=

=

qs
,
j
t

=

os
i
1

,

,

1
+

old
  l

o
t

|

)

)(     
t
1
+

i

t

obj
)(
t

(

j

)

1
+

copyright    andrew w. moore

we want

a
new
ij

=

s

ij

n

   

k

1
=

s

ik

where

s

ij

=

a
ij

t

   

t

1
=

)(     
t
1
+

i

t

slide 75

)

1
+

obj
)(
t

(

j

copyright    andrew w. moore

slide 76

38

we want

a
new
ij

=

s

ij

n

   

k

1
=

s

ik

where

s

ij

=

a
ij

t

   

t

1
=

)(     
t
1
+

i

t

obj
)(
t

(

j

)

1
+

t

  

n

t

  

n

copyright    andrew w. moore

slide 77

em for id48s

if we knew    we could estimate expectations of quantities 

such as
expected number of times in state i
expected number of transitions i     j

if we knew the quantities such as

expected number of times in state i
expected number of transitions i     j

we could compute the max likelihood estimate of

   =    {aij},{bi(j)},   i   

roll on the em algorithm   

copyright    andrew w. moore

slide 78

39

em 4 id48s

1. get your observations  o1    ot
2. guess your first    estimate   (0), k=0
3.
4. given o1    ot,   (k) compute

k = k+1

  t(i) ,   t(i,j)         1     t     t,         1     i     n,         1     j     n

5. compute expected freq. of state i, and expected freq. i   j
6. compute new estimates of aij, bj(k),   i accordingly.  call 

them   (k+1)

7. goto 3, unless converged.
   

also known (for the id48 case) as the baum-welch 
algorithm.

copyright    andrew w. moore

slide 79

bad news

    there are lots of local minima

good news

    the local minima are usually adequate models of the 

data.

be given.

notice

    em does not estimate the number of states. that must 

    often, id48s are forced to have some links with zero 

id203. this is done by setting aij=0 in initial estimate 
  (0)

    easy extension of everything seen today: id48s with 

real valued outputs

copyright    andrew w. moore

slide 80

40

bad news

trade-off between too few states (inadequately 
modeling the structure in the data) and too many 
(fitting the noise).
    there are lots of local minima
thus #states is a id173 parameter.
blah blah blah    bias variance tradeoff   blah 
blah   cross-validation   blah blah   .aic, 
    the local minima are usually adequate models of the 
bic   .blah blah (same ol    same ol   )

good news

data.

be given.

notice

    em does not estimate the number of states. that must 

    often, id48s are forced to have some links with zero 

id203. this is done by setting aij=0 in initial estimate 
  (0)

    easy extension of everything seen today: id48s with 

real valued outputs

copyright    andrew w. moore

slide 81

what you should know

    what is an id48 ?
    computing (and defining)   t(i)
    the viterbi algorithm
    outline of the em algorithm
    to be very happy with the kind of maths and 

analysis needed for id48s

don   t panic: 
starts on p. 257.

    fairly thorough reading of rabiner* up to page 266* 

[up to but not including    iv. types of id48s   ].
*l. r. rabiner, "a tutorial on id48 and selected

applications in id103," proc. of the ieee, vol.77, no.2, 
pp.257--286, 1989.

http://ieeexplore.ieee.org/iel5/5/698/00018626.pdf?arnumber=18626

copyright    andrew w. moore

slide 82

41

