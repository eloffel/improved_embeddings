note to other teachers and users of these slides. 
andrew would be delighted if you found this source 
material useful in giving your own lectures. feel free 
to use these slides verbatim, or to modify them to fit 
your own needs. powerpoint originals are available. if 
you make use of a significant portion of these slides in 
your own lecture, please include this message, or the 
following link to the source repository of andrew   s 
tutorials: http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully received. 

information gain

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2001, 2003, andrew w. moore

bits

you are watching a set of independent random samples of x

you see that x has four possible values
p(x=a) = 1/4

p(x=b) = 1/4

p(x=c) = 1/4

p(x=d) = 1/4

so you might see: baacbadcdaddda   
you transmit data over a binary serial link. you can encode 
each reading with two bits (e.g. a = 00, b = 01, c = 10, d = 
11)

0100001001001110110011111100   

copyright    2001, 2003, andrew w. moore

information gain: slide 2

1

fewer bits

someone tells you that the probabilities are not equal

p(x=a) = 1/2

p(x=b) = 1/4

p(x=c) = 1/8

p(x=d) = 1/8

it   s possible   

   to invent a coding for your transmission that only uses 
1.75 bits on average per symbol. how?

copyright    2001, 2003, andrew w. moore

information gain: slide 3

fewer bits

someone tells you that the probabilities are not equal

p(x=a) = 1/2
it   s possible   

p(x=b) = 1/4

p(x=c) = 1/8

p(x=d) = 1/8

   to invent a coding for your transmission that only uses 
1.75 bits on average per symbol. how?

a
b
c
d

0
10
110
111

(this is just one of several ways)

copyright    2001, 2003, andrew w. moore

information gain: slide 4

2

fewer bits

suppose there are three equally likely values   

p(x=a) = 1/3

p(x=b) = 1/3

p(x=c) = 1/3

here   s a na  ve coding, costing 2 bits per symbol

a
b
c

00
01
10

can you think of a coding that would need only 1.6 bits 
per symbol on average?

in theory, it can in fact be done with 1.58496 bits per 
symbol.

copyright    2001, 2003, andrew w. moore

information gain: slide 5

general case

suppose x can have one of mvalues    v1, v2,      vm
p(x=v1) = p1
what   s the smallest possible number of bits, on average, per 

p(x=v2) = p2

p(x=vm) = pm

   .

symbol, needed to transmit a stream of symbols drawn from 
x   s distribution? it   s
p
log
1
m
   

p
2
1
log

xh

   =

log

log

   =

k

p

p

   

   

   

p

p

p

p

)

(

m

m

2

2

2

2

2

j

j

j

1
=

h(x) = the id178 of x
       high id178    means x is from a uniform (boring) distribution
       low id178    means x is from varied (peaks and valleys) distribution

copyright    2001, 2003, andrew w. moore

information gain: slide 6

3

general case

p(x=v2) = p2

suppose x can have one of mvalues    v1, v2,      vm
   .
p(x=v1) = p1
a histogram of the 
what   s the smallest possible number of bits, on average, per 
frequency distribution of 
values of x would have 
many lows and one or 
two highs
k

symbol, needed to transmit a stream of symbols drawn from 
x   s distribution? it   s
p
log
1
m
   

a histogram of the 
frequency distribution of 
values of x would be flat
log
   =

p
2
1
log

xh

p(x=vm) = pm

   =

log

p

p

   

   

   

p

p

p

p

)

(

m

m

2

2

2

2

2

j

j

j

1
=

h(x) = the id178 of x
       high id178    means x is from a uniform (boring) distribution
       low id178    means x is from varied (peaks and valleys) distribution

copyright    2001, 2003, andrew w. moore

information gain: slide 7

general case

p(x=v2) = p2

suppose x can have one of mvalues    v1, v2,      vm
   .
p(x=v1) = p1
a histogram of the 
what   s the smallest possible number of bits, on average, per 
frequency distribution of 
values of x would have 
many lows and one or 
two highs
k

symbol, needed to transmit a stream of symbols drawn from 
x   s distribution? it   s
p
log
1
m
   

a histogram of the 
frequency distribution of 
values of x would be flat
log
   =

xh

p(x=vm) = pm

   =

log

p

   

p

p

)

(

m

m

2

2

2

2

p

p

   

   

p
2
1
log
..and so the values 
j
sampled from it would 
be all over the place

p

2

..and so the values 
sampled from it would 
be more predictable

h(x) = the id178 of x
       high id178    means x is from a uniform (boring) distribution
       low id178    means x is from varied (peaks and valleys) distribution

1
=

j

j

copyright    2001, 2003, andrew w. moore

information gain: slide 8

4

id178 in a nut-shell

low id178

high id178

copyright    2001, 2003, andrew w. moore

information gain: slide 9

id178 in a nut-shell

low id178

high id178

..the values (locations 
of soup) sampled 
entirely from within 
the soup bowl

..the values (locations of 
soup) unpredictable... 
almost uniformly sampled 
throughout our dining room

copyright    2001, 2003, andrew w. moore

information gain: slide 10

5

specific conditional id178 h(y|x=v)

suppose i   m trying to predict output y and i have input x
x = college major
y = likes    gladiator   

let   s assume this reflects the true 
probabilities
e.g. from this data we estimate

x
math
history
cs
math
math
cs
history
math

y
yes
no
yes
no
no
yes
no
yes

    p(likeg= yes) = 0.5
    p(major = math & likeg= no) = 0.25
    p(major = math) = 0.5
    p(likeg= yes | major = history) = 0

note:

    h(x) = 1.5
   h(y) = 1

copyright    2001, 2003, andrew w. moore

information gain: slide 11

specific conditional id178 h(y|x=v)

definition of specific conditional 
id178:
h(y |x=v) = the id178 of y
among only those records in which 
xhas value v

x = college major
y = likes    gladiator   

x
math
history
cs
math
math
cs
history
math

y
yes
no
yes
no
no
yes
no
yes

copyright    2001, 2003, andrew w. moore

information gain: slide 12

6

specific conditional id178 h(y|x=v)

x = college major
y = likes    gladiator   

x
math
history
cs
math
math
cs
history
math

y
yes
no
yes
no
no
yes
no
yes

definition of specific conditional 
id178:
h(y |x=v) = the id178 of y
among only those records in which 
xhas value v
example:
    h(y|x=math) = 1
    h(y|x=history) = 0
    h(y|x=cs) = 0

copyright    2001, 2003, andrew w. moore

information gain: slide 13

conditional id178 h(y|x)

x = college major
y = likes    gladiator   

x
math
history
cs
math
math
cs
history
math

y
yes
no
yes
no
no
yes
no
yes

definition of conditional 
id178:
h(y |x) = the average specific 
conditional id178 of y
= if you choose a record at random what 
will be the conditional id178 of y, 
conditioned on that row   s value of x
= expected number of bits to transmit yif 
both sides will know the value of x
=   j prob(x=vj) h(y| x = vj)

copyright    2001, 2003, andrew w. moore

information gain: slide 14

7

conditional id178

x = college major
y = likes    gladiator   

x
math
history
cs
math
math
cs
history
math

y
yes
no
yes
no
no
yes
no
yes

definition of conditional id178:
h(y|x) = the average conditional 
id178 of y
=   jprob(x=vj) h(y| x = vj)

example:

vj
math
history
cs

prob(x=vj)
0.5
0.25
0.25

h(y| x = vj)
1
0
0

h(y|x) = 0.5 * 1 + 0.25 * 0 + 0.25 * 0 = 0.5

copyright    2001, 2003, andrew w. moore

information gain: slide 15

information gain

x = college major
y = likes    gladiator   

x
math
history
cs
math
math
cs
history
math

y
yes
no
yes
no
no
yes
no
yes

definition of information gain:

ig(y|x) = i must transmit y. 
how many bits on average 
would it save me if both ends of 
the line knew x?
ig(y|x)= h(y) -h(y| x)

example:

    h(y) = 1
    h(y|x) = 0.5
    thus ig(y|x) = 1     0.5 = 0.5

copyright    2001, 2003, andrew w. moore

information gain: slide 16

8

information gain example

copyright    2001, 2003, andrew w. moore

information gain: slide 17

another example

copyright    2001, 2003, andrew w. moore

information gain: slide 18

9

relative information gain

x = college major
y = likes    gladiator   

x
math
history
cs
math
math
cs
history
math

y
yes
no
yes
no
no
yes
no
yes

definition of relative information 
gain:
rig(y|x) = i must transmit y, what 
fraction of the bits on average would 
it save me if both ends of the line 
knew x?
rig(y|x)= h(y) -h(y| x) / h(y)

example:
    h(y|x) = 0.5
    h(y) = 1
    thus ig(y|x) = (1    0.5)/1 = 0.5

copyright    2001, 2003, andrew w. moore

information gain: slide 19

what is information gain used for?

suppose you are trying to predict whether someone 
is going live past 80 years. from historical data you 
might find   

   ig(longlife | haircolor) = 0.01
   ig(longlife | smoker) = 0.2
   ig(longlife | gender) = 0.25
   ig(longlife | lastdigitofssn) = 0.00001

ig tells you how interesting a 2-d contingency table is 
going to be.

copyright    2001, 2003, andrew w. moore

information gain: slide 20

10

