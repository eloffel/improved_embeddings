predicting real-valued 
outputs: an introduction 

to regression

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

andrew w. moore

professor

school of computer science
carnegie mellon university
r e o r d e r e d   m a t e r
 
s
t h e   n e u r a l   n e t
 
s
  i
s
t h i
i
    f a v o r
t h e  
r o m  
t h m s
t u r e   a n d  
f
i
i o n   a l g o r
l e c
r e g r e s
t u r e
l e c

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

s

i a l  

t e  
 
   

copyright    2001, 2003, andrew w. moore

single-

parameter 

linear 

regression

copyright    2001, 2003, andrew w. moore

2

1

id75

dataset

   
w
   
    1    

inputs
x1 = 1
x2 = 3
x3 = 2
x4 = 1.5
x5 = 4

outputs
y1 = 1
y2 = 2.2
y3 = 2
y4 = 1.9
y5 = 3.1

id75 assumes that the expected value of 
the output given an input, e[y|x], is linear.
simplest case: out(x) = wxfor some unknown w.
given the data, we can estimate w.

copyright    2001, 2003, andrew w. moore

1-parameter id75

assume that the data is formed by
yi = wxi+ noisei

where   
    the noise signals are independent
    the noise has a normal distribution with mean 0 

and unknown variance   2

p(y|w,x) has a normal distribution with
    mean wx
    variance   2

copyright    2001, 2003, andrew w. moore

3

4

2

bayesian id75

p(y|w,x) = normal (mean wx, var   2)

we have a set of datapoints (x1,y1) (x2,y2)     (xn,yn) 
which are evidence about w.

we want to infer wfrom the data.

p(w|x1, x2, x3,   xn, y1, y2   yn)

   you can use bayes rule to work out a posterior 
distribution for wgiven the data.
   or you could do id113

copyright    2001, 2003, andrew w. moore

5

id113 of w

asks the question:
   for which value of wis this data most likely to have 

happened?   

<=>

for what wis

p(y1, y2   yn|x1, x2, x3,   xn, w) maximized?

<=>

for what w is

xwyp   
,

(

i

i

n

i

1
=

maximized?
 )

copyright    2001, 2003, andrew w. moore

6

3

for what w is
n

i

1
=
for what w is
n

i
1
=
for what w is
n

1

i
=
for what w is
   

n

i

=

1

xwyp   
,

(

i

  )

maximized?

i

       
exp(

1
2

(

y

i wx
   
i
  

 ))
2

maximized?

      

1
2

   
   
   

y

i

wx

   
  

i

2

   
   
   

maximized?
 

(

y

i

   

wx

i

2

) minimized?

 

copyright    2001, 2003, andrew w. moore

7

id75

the maximum 

likelihood wis 
the one that 
minimizes sum-
of-squares of 
residuals

e(w)

w

(

y
i

=  

   
i
   
y
i

2

)

wx
i

   
(
   
2

(
   
we want to minimize a quadratic function of w.

)
wyx
i

+

   

=

2

i

i

) 2
2
wx
i

copyright    2001, 2003, andrew w. moore

8

4

id75

easy to show the sum of 

squares is minimized 
when

w

   =
yx
i
2   
x
i

i

the maximum likelihood 
( ) wx
x =

model is

out

we can use it for 

prediction

copyright    2001, 2003, andrew w. moore

9

id75

easy to show the sum of 

squares is minimized 
when

w

   =
yx
i
2   
x
i

i

the maximum likelihood 
( ) wx
x =

model is

out

we can use it for 

prediction

p(w)

w

note:   in bayesian stats you   d have 

ended up with a prob dist of w

and predictions would have given a prob 

dist of expected output

often useful to know your confidence.  

max likelihood can give some kinds of 
confidence too.

copyright    2001, 2003, andrew w. moore

10

5

multivariate 

linear 

regression

copyright    2001, 2003, andrew w. moore

11

multivariate regression

what if the inputs are vectors?

6 .

. 8

x2

x1
dataset has form

3 .

. 4                                              

2-d input 
example

. 5

. 10

x1 
y1
x2 
y2
x3 
y3
.:                                    :
.
xr

yr

copyright    2001, 2003, andrew w. moore

12

6

multivariate regression

write matrix x and y thus:

x

=

1

2

x
.....
x
.....

   
   
   
   
m
   
x
.....
   

r

.....
.....

   
   
   
   
   
.....
   

=

   
   
   
   
   
   

x
11
x

21

x
12
x

22

x

r
1

x

r

2

...
...

m
...

x
m
1
x

2

m

x

rm

   
   
   
y
  
   
   
   

=

y
1
y

2

m
y

r

   
   
   
   
   
   

   
   
   
   
   
   

(there are r datapoints.  each input has mcomponents)
the id75 model assumes a vector w such that

out(x) = wtx= w1x[1] + w2x[2] +    .wmx[d]

the max. likelihood wis w = (xtx) -1(xty)

copyright    2001, 2003, andrew w. moore

13

multivariate regression

write matrix x and y thus:

x

=

1

2

x
.....
x
.....

   
   
   
   
m
   
x
.....
   

r

.....
.....

   
   
   
   
   
.....
   

=

   
   
   
   
   
   

x
11
x

21

x
12
x

22

x

r
1

x

r

2

...
...

m
...

x
m
1
x

2

m

x

rm

   
   
   
y
  
   
   
   

=

y
1
y

2

m
y

r

   
   
   
   
   
   

   
   
   
   
   
   

(there are r datapoints.  each input has mcomponents)
the id75 model assumes a vector w such that

prove it !!!!!

important exercise:  

out(x) = wtx= w1x[1] + w2x[2] +    .wmx[d]

the max. likelihood wis w = (xtx) -1(xty)

copyright    2001, 2003, andrew w. moore

14

7

multivariate regression (con   t)

the max. likelihood w is w = (xtx)-1(xty)

xtx is an m x m matrix:  i,j   th elt is

xty is an m-element vector:  i   th elt

r

   

k

1
=

kixx
kj

r

   

k

1
=

kiyx
k

copyright    2001, 2003, andrew w. moore

15

constant term 

in linear 
regression

copyright    2001, 2003, andrew w. moore

16

8

what about a constant term?

we may expect 
linear data that does 
not go through the 
origin.

statisticians and 
neural net folks all 
agree on a simple 
obvious hack.

can you guess??

copyright    2001, 2003, andrew w. moore

17

the constant term

    the trick is to create a fake input    x0    that 

always takes the value 1

yx2x1
2
4
16
3
4
17
5
5
20
before:
y=w1x1+ w2x2 
   has to be a poor 
model

in this example, 
you should be able 
to see the id113 w0
, w1and w2 by 
inspection 

4
4
5

x0
yx2x1
2
1
16
3
1
17
1
5
20
after:
y= w0x0+w1x1+ w2x2 
= w0+w1x1+ w2x2 
   has a fine constant 
term

copyright    2001, 2003, andrew w. moore

18

9

d

e

c

s

o

r

e

t

h e

.

.

.

y

s

a

t i c i t
linear 

regression with 
varying noise

copyright    2001, 2003, andrew w. moore

19

regression with varying noise

    suppose you know the variance of the noise that 

was added to each datapoint.

yi

xi
  i
2
4    
1
1
2
1/4
2
4
3
1/4

1
1
3
2

y=3

y=2

y=1

y=0

  =2

  =1/2

  =1

  =2

  =1/2

x=0

x=1

x=2

x=3

assume

y

i

~

wxn
i

(

,
2
  
i

)

t    s   t
a
t
ti m a

w h
s
e

copyright    2001, 2003, andrew w. moore

e  
e   m l
h
f   w ?
e   o

20

10

id113 estimation with varying noise

argmax

w

log

yyp
,
(

1

2

,...,

y

r

|

xx
,
1

2

,...,

x

r

,
2
      
r

,...,

2
1

2
2

,

,

w
=)

argmin

w

r

(

y
i wx
      
i
2
  
i

1
=

i

2)

=

assuming independence 
among noise and then 
plugging in equation for 
gaussian and simplifying.

w
 
such that

   
      
   

r

   

i

1
=

wx
i

)

=

0

yx
(
i

   
i
2
  
i

   
=      
   

setting dll/dw
equal to zero

i

r

   
   
      
   
   
      
   

1
=
r

   

1
=

i

trivial algebra

i

yx
i
2
  
i
x
2
i
2
  
i

   
      
   
   
      
   

copyright    2001, 2003, andrew w. moore

21

this is weighted regression

    we are asking to minimize the weighted sum of 

squares

argmin

w

r

   

i

1
=

2)

(

y

i wx
   
i
2
  
i

y=3

y=2

y=1

y=0

  =2

  =1/2

  =1

  =2

  =1/2

x=0

x=1

x=2

x=3

where weight for i   th datapoint is

1
2
i  

copyright    2001, 2003, andrew w. moore

22

11

non-linear 
regression

copyright    2001, 2003, andrew w. moore

23

non-id75

y=3

    suppose you know that y is related to a function of x in 
such a way that the predicted values have a non-linear 
dependence on w, e.g:
yi
xi
    
2.5
1
2
3
3
2
3
3

y=0

y=1

y=2

x=0

x=2

x=1

x=3

assume

y

i

~

xwn

+

(

,
2  i

)

t    s   t
a
t
ti m a

w h
s
e

e  
e   m l
h
f   w ?
e   o

copyright    2001, 2003, andrew w. moore

24

12

non-linear id113 estimation

argmax

w

log

yyp
(
,

1

2

,...,

y

r

|

xx
,
1

2

,...,

x

r

,
,
  

w
=)

argmin

w

r

(

      

y
i

i

1
=

xw
i

+

2

) =

assuming i.i.d. and 
then plugging in 
equation for gaussian 
and simplifying.

w
 
such that

   
   
   
   

r

y
      

i

i

1
=

+
xw
i

xw
i
+

=

0

   
   
=   
   

setting dll/dw
equal to zero

copyright    2001, 2003, andrew w. moore

25

non-linear id113 estimation

argmax

w

log

yyp
(
,

1

2

,...,

y

r

|

xx
,
1

2

,...,

x

r

,
,
  

w
=)

argmin

w

r

(

      

y
i

i

1
=

xw
i

+

2

) =

assuming i.i.d. and 
then plugging in 
equation for gaussian 
and simplifying.

w
 
such that

   
   
   
   

r

y
      

i

i

1
=

+
xw
i

xw
i
+

=

0

   
   
=   
   

setting dll/dw
equal to zero

we   re down the 
algebraic toilet
  w h
?
o

t  

s

g

o

 

a

s

s

e
u
w e

d

 

copyright    2001, 2003, andrew w. moore

26

13

non-linear id113 estimation

argmax

w

log

yyp
(
,

1

2

,...,

y

r

|

xx
,
1

2

,...,

x

r

,
,
  

w
=)

r

(

i

1
=

      

argmin

common (but not only) approach:
y
i
numerical solutions:
w
    line search
    simulated annealing
   
   
    id119
   
   
    conjugate gradient
    levenberg marquart
    newton   s method

w
 
such that

y
      

1
=

r

i

i

also, special purpose statistical-

optimization-specific tricks such as 
e.m. (see gaussian mixtures lecture 
for introduction)

copyright    2001, 2003, andrew w. moore

xw
i

+

2

) =

assuming i.i.d. and 
then plugging in 
equation for gaussian 
and simplifying.

+
xw
i

xw
i
+

=

0

   
   
=   
   

setting dll/dw
equal to zero

we   re down the 
algebraic toilet
  w h
?
o

t  

s

g

o

 

a

s

s

e
u
w e

d

 

27

polynomial 
regression

copyright    2001, 2003, andrew w. moore

28

14

polynomial regression

y=

x=

3
1
:

2
1
:

2
1
:

so far we   ve mainly been dealing with id75
yx2x1
3
7
1
3
:
:
1
1
:

x1=(3,2).. y1=7..
y=

2
1
:

7
3
:

3
1

z=

7
3
:

  =(ztz)-1(zty)

z1=(1,3,2)..
zk=(1,xk1,xk2)

y1=7..

yest=   0+   1 x1+   2 x2

copyright    2001, 2003, andrew w. moore

29

quadratic regression

y=

x=

it   s trivial to do linear fits of fixed nonlinear basis functions
yx2x1
3
7
1
3
:
:
3
1

x1=(3,2).. y1=7..
6
1

2
1
:

3
1
:

7
3
:

2
1

9
1

z=

  =(ztz)-1(zty)

2
1
:
1
1
:

4
1
:

y=
7
3
:

z=(1 ,  x1,   x2 ,   x1

2, x1x2,x2
,)
2

yest=   0+   1 x1+   2 x2+
  3x1
2 +   4 x1x2 +   5 x2
2

copyright    2001, 2003, andrew w. moore

30

15

quadratic regression

y=

x=

3
1
:

2
1
:

it   s trivial to do linear fits of fixed nonlinear basis functions
each component of a z vector is called a term.
yx2x1
each column of the z matrix is called a term column
3
7
how many terms in a quadratic regression with m
inputs?
1
3
   1 constant term
:
:
x1=(3,2).. y1=7..
   m linear terms
3
6
   (m+1)-choose-2 = m(m+1)/2 quadratic terms
1
1
(m+2)-choose-2 terms in total = o(m2)

  =(ztz)-1(zty)

7
3
:

2
1

9
1

z=

2
1
:
1
1
:

4
1
:

y=
7
3
:

z=(1 ,  x1,   x2 ,   x1

2, x1x2,x2
,)
2

note that solving   =(ztz)-1(zty) is thus o(m6)

yest=   0+   1 x1+   2 x2+
  3x1
2 +   4 x1x2 +   5 x2
2

copyright    2001, 2003, andrew w. moore

31

qth-degree polynomial regression

yx2x1
3
7
1
3
:
:
3
1

z=

2
1
:
1
1
:

2
1

9
1

x=

3
1
:

2
1
:

y=

7
3
:

x1=(3,2).. y1=7..
6
1

y=

7
3
:

   
   
   

  =(ztz)-1(zty)

z=(all products of powers of inputs in 
which sum of powers is q or less,)

yest=   0+ 
  1 x1+   

copyright    2001, 2003, andrew w. moore

32

16

m inputs, degree q: how many terms?
= the number of unique terms of the form
      
 where
 
= the number of unique terms of the form
=   
 where
 

qq
i

qq
i

xx
q
1
1

xx
q
1
1

q
1

q
m
m

q
m
m

...

...

x

x

q
2

q
2

=1

m

m

i

2

2

0

= the number of lists of non-negative integers [q0,q1,q2,..qm] 
in which   qi= q
= the number of ways of placing q red disks on a row of 
squares of length q+m       = (q+m)-choose-q

i

=0

q0=2

q1=2

q2=0

q3=4

q4=3

copyright    2001, 2003, andrew w. moore

33

q=11, m=4

radial basis 
functions

copyright    2001, 2003, andrew w. moore

34

17

radial basis functions (rbfs)

yx2x1
3
7
1
3
:
:
   
   
   

2
1
:
   
   
   

z=

   
   
   

   
   
   

x=

3
1
:

2
1
:

y=

7
3
:

y=

x1=(3,2).. y1=7..
   
   
   

   
   
   

7
3
:

z=(list of radial basis function evaluations)

  =(ztz)-1(zty)

yest=   0+ 
  1 x1+   

copyright    2001, 2003, andrew w. moore

35

1-d rbfs

y

c1

x

c1

c1

yest=   1   1(x)+   2   2(x)+   3   3(x)
where
  i(x) = kernelfunction( | x -ci| / kw)

copyright    2001, 2003, andrew w. moore

36

18

example

y

c1

x

c1

c1

yest= 2  1(x)+ 0.05  2(x)+ 0.5  3(x)
where
  i(x) = kernelfunction( | x -ci| / kw)

copyright    2001, 2003, andrew w. moore

37

rbfs with id75

kwalso held constant 
(initialized to be large 

enough that there   s decent 

overlap between basis 

functions*

c1

*usually much better than the crappy 

overlap on my diagram

y

all ci   s are held constant 
(initialized randomly or 
c1
dimensional input space)

on a grid in m-

c1

x

yest= 2  1(x)+ 0.05  2(x)+ 0.5  3(x)
where
  i(x) = kernelfunction( | x -ci| / kw)

copyright    2001, 2003, andrew w. moore

38

19

rbfs with id75

y

all ci   s are held constant 
(initialized randomly or 
c1
dimensional input space)

on a grid in m-

c1

x

kwalso held constant 
(initialized to be large 

enough that there   s decent 

overlap between basis 

functions*

c1

*usually much better than the crappy 

overlap on my diagram

yest= 2  1(x)+ 0.05  2(x)+ 0.5  3(x)
where
  i(x) = kernelfunction( | x -ci| / kw)
then given qbasis functions, define the matrix zsuch that zkj= 
kernelfunction( | xk-ci| / kw) where xkis the kth vector of inputs
and as before,   =(ztz)-1(zty)

copyright    2001, 2003, andrew w. moore

39

rbfs with nonid75

allow the ci   s to adapt to 

y
the data (initialized 

randomly or on a grid in 
c1

m-dimensional input 

c1

x

space)

kwallowed to adapt to the data.
(some folks even let each basis 
function have its own 
kwj,permitting fine detail in 
dense regions of input space)

c1

yest= 2  1(x)+ 0.05  2(x)+ 0.5  3(x)
where
  i(x) = kernelfunction( | x -ci| / kw)
but how do we now find all the   j   s, ci   s and kw ?

copyright    2001, 2003, andrew w. moore

40

20

rbfs with nonid75

allow the ci   s to adapt to 

y
the data (initialized 

randomly or on a grid in 
c1

m-dimensional input 

c1

x

space)

kwallowed to adapt to the data.
(some folks even let each basis 
function have its own 
kwj,permitting fine detail in 
dense regions of input space)

c1

yest= 2  1(x)+ 0.05  2(x)+ 0.5  3(x)
where
  i(x) = kernelfunction( | x -ci| / kw)
but how do we now find all the   j   s, ci   s and kw ?

copyright    2001, 2003, andrew w. moore

41

answer: id119

rbfs with nonid75

allow the ci   s to adapt to 

y
the data (initialized 

randomly or on a grid in 
c1

m-dimensional input 

c1

x

space)

kwallowed to adapt to the data.
(some folks even let each basis 
function have its own 
kwj,permitting fine detail in 
dense regions of input space)

c1

yest= 2  1(x)+ 0.05  2(x)+ 0.5  3(x)
where
  i(x) = kernelfunction( | x -ci| / kw)
but how do we now find all the   j   s, ci   s and kw ?

(but i   d like to see, or hope someone   s already done, a 
hybrid, where the ci   s and kw are updated with gradient 
descent while the   j   s use matrix inversion)

answer: id119

copyright    2001, 2003, andrew w. moore

42

21

radial basis functions in 2-d

two inputs.
outputs (heights 
sticking out of page) 
not shown.

x2

x1

center

sphere of 
significant 
influence of 
center

copyright    2001, 2003, andrew w. moore

43

happy rbfs in 2-d

blue dots denote 
coordinates of 
input vectors

x2

x1

center

sphere of 
significant 
influence of 
center

copyright    2001, 2003, andrew w. moore

44

22

crabby rbfs in 2-d

blue dots denote 
coordinates of 
input vectors

what   s the 
problem in this 
example?

center

x2

x1

sphere of 
significant 
influence of 
center

copyright    2001, 2003, andrew w. moore

45

more crabby rbfs and what   s the 
problem in this 
example?

blue dots denote 
coordinates of 
input vectors

center

sphere of 
significant 
influence of 
center

x2

x1

copyright    2001, 2003, andrew w. moore

46

23

hopeless!

even before seeing the data, you should 
understand that this is a disaster!

center

sphere of 
significant 
influence of 
center

x2

x1

copyright    2001, 2003, andrew w. moore

47

unhappy

even before seeing the data, you should 
understand that this isn   t good either..

center

sphere of 
significant 
influence of 
center

x2

x1

copyright    2001, 2003, andrew w. moore

48

24

robust 
regression

copyright    2001, 2003, andrew w. moore

49

robust regression

y

x

copyright    2001, 2003, andrew w. moore

50

25

robust regression

this is the best fit that 
quadratic regression can 
manage

y

x

copyright    2001, 2003, andrew w. moore

51

robust regression

   but this is what we   d 
probably prefer

y

x

copyright    2001, 2003, andrew w. moore

52

26

loess-based robust regression

after the initial fit, score 
each datapoint according to 
how well it   s fitted   

y

x

you are a very good 

datapoint.

copyright    2001, 2003, andrew w. moore

53

loess-based robust regression

after the initial fit, score 
each datapoint according to 
how well it   s fitted   

y

you are a very good 

datapoint.

you are not too 

shabby.

x

copyright    2001, 2003, andrew w. moore

54

27

loess-based robust regression

after the initial fit, score 
each datapoint according to 
how well it   s fitted   

but you are 

pathetic.

y

you are a very good 

datapoint.

you are not too 

shabby.

x

copyright    2001, 2003, andrew w. moore

55

y

x

robust regression

for k = 1 to r   
   let (xk,yk)be the kth datapoint
   let yest
kbe predicted value of 
yk
   let wkbe a weight for 
datapoint k that is large if the 
datapoint fits well and small if it 
fits badly:

wk = kernelfn([yk-yest

k]2)

copyright    2001, 2003, andrew w. moore

56

28

robust regression

for k = 1 to r   
   let (xk,yk)be the kth datapoint
   let yest
kbe predicted value of 
yk
   let wkbe a weight for 
datapoint k that is large if the 
datapoint fits well and small if it 
fits badly:

wk = kernelfn([yk-yest

k]2)

y

x

then redo the regression 
using weighted datapoints.
weighted regression was described earlier in 
the    vary noise    section, and is also discussed 
in the    memory-based learning    lecture.
guess what happens next?

copyright    2001, 2003, andrew w. moore

57

robust regression

for k = 1 to r   
   let (xk,yk)be the kth datapoint
   let yest
kbe predicted value of 
yk
   let wkbe a weight for 
datapoint k that is large if the 
datapoint fits well and small if it 
fits badly:

wk = kernelfn([yk-yest

k]2)

y

x

then redo the regression 
using weighted datapoints.
i taught you how to do this in the    instance-
based    lecture (only then the weights 
depended on distance in input-space)
repeat whole thing until 
converged!

copyright    2001, 2003, andrew w. moore

58

29

robust regression---what we   re 

doing

what regular regression does:
assume ykwas originally generated using the 
following recipe:

yk=   0+   1 xk+   2 xk

2 +n(0,  2)

computational task is to find the maximum 
likelihood   0,   1and   2 

copyright    2001, 2003, andrew w. moore

59

robust regression---what we   re 

doing

what loess robust regression does:
assume ykwas originally generated using the 
following recipe:

with id203 p:

yk=   0+   1 xk+   2 xk

2 +n(0,  2)

but otherwise

yk~ n(  ,  huge

2)

computational task is to find the maximum 
likelihood   0,   1,   2 , p,  and   huge

copyright    2001, 2003, andrew w. moore

60

30

robust regression---what we   re 

doing

what loess robust regression does:
assume ykwas originally generated using the 
following recipe:

with id203 p:

yk=   0+   1 xk+   2 xk

2 +n(0,  2)

but otherwise

mysteriously, the 
reweighting procedure 
does this computation 
for us.

your first glimpse of 

two spectacular letters: 

yk~ n(  ,  huge

2)

e.m.

computational task is to find the maximum 
likelihood   0,   1,   2 , p,  and   huge

copyright    2001, 2003, andrew w. moore

61

regression 

trees

copyright    2001, 2003, andrew w. moore

62

31

regression trees

       id90 for regression   

copyright    2001, 2003, andrew w. moore

63

a regression tree leaf

predict age = 47

mean age of records 
matching this leaf node

copyright    2001, 2003, andrew w. moore

64

32

a one-split regression tree

gender?

female

male

predict age = 39

predict age = 36

copyright    2001, 2003, andrew w. moore

65

choosing the attribute to split on

gender

rich?

female
male
male
:

no
no
yes
:

num. 
children
2
0
0
:

num. beany
babies
1
0
5+
:

age

38
24
72
:

    we can   t use 

information gain.

    what should we use?

copyright    2001, 2003, andrew w. moore

66

33

choosing the attribute to split on

gender

rich?

female
male
male
:

no
no
yes
:

num. 
children
2
0
0
:

num. beany
babies
1
0
5+
:

age

38
24
72
:

mse(y|x) = the expected squared error if we must predict a record   s y 

value given only knowledge of the record   s x value

if we   re told x=j, the smallest expected error comes from predicting the 

mean of the y-values among those records in which x=j. call this mean 
quantity   y

x=j

then   

xymse

(

|

1)
=
r

n

x

       

(
x
such that 

k
 (

1
=

j

y
k
j
=

  

jx
=
y

2)

   
)

k

copyright    2001, 2003, andrew w. moore

67

choosing the attribute to split on

gender

rich?

age

num. 
children
2
0
0
:

num. beany
babies
1
0
5+
:

no
no
yes
:

regression tree attribute selection: greedily 
choose the attribute that minimizes mse(y|x) 
guess what we do about real-valued inputs?
guess how we prevent overfitting

38
24
72
:

female
male
male
:

mse(y|x) = the expected squared error if we must predict a record   s y 

value given only knowledge of the record   s x value

if we   re told x=j, the smallest expected error comes from predicting the 

mean of the y-values among those records in which x=j. call this mean 
quantity   y

x=j

then   

xymse

(

|

1)
=
r

n

x

       

(
x
such that 

k
 (

1
=

j

y
k
j
=

  

jx
=
y

2)

   
)

k

copyright    2001, 2003, andrew w. moore

68

34

pruning decision

   property-owner = yes

gender?

female

male

do i deserve 

to live?

predict age = 39
# property-owning females = 56712
mean age among pofs = 39
age std dev among pofs = 12

predict age = 36

# property-owning males = 55800
mean age among poms = 36
age std dev among poms = 11.5

use a standard chi-squared test of the null-
hypothesis    these two populations have the same 
mean    and bob   s your uncle.

copyright    2001, 2003, andrew w. moore

69

id75 trees
   property-owner = yes

also known as 
   model trees   

gender?

female

male

predict age = 
26 + 6 * numchildren -
2 * yearseducation

predict age = 
24 + 7 * numchildren -
2.5 * yearseducation

leaves contain linear 
functions (trained using 
id75 on all 
records matching that leaf)

split attribute chosen to minimize 
mse of regressed children.
pruning with a different chi-
squared

copyright    2001, 2003, andrew w. moore

70

35

id75 trees
   property-owner = yes

also known as 
   model trees   

d  

s t e

gender?

female

y  
n  t e
n
o r e   a
male
e
e
s   b
e  
n
a lly  i g
g  t h
a
a t  h
u ri n
u t e  t h
p ic
predict age = 
predict age = 
d  
e   d
d   a ttri b
u  t y
s t e
e  tr e
a l  a ttri b
n t e
o
e t a il:  y
e   a ll  u
26 + 6 * numchildren -
24 + 7 * numchildren -
p  i n  t h
e
a l u
e
v
o ric
o
a l- v
b
s
2 * yearseducation
2.5 * yearseducation
d   a
e r  u
u t  u
g
e  r e
a t e
s t e
h
n .  b
n   h i g
s
n  t e
d   u
si o
n
e
o
s ,  a
s
e
split attribute chosen to minimize 
leaves contain linear 
g r e
e   b
u t e
r e
y   v
a ttri b
functions (trained using 
mse of regressed children.
e
n  if t h
id75 on all 
e
v
pruning with a different chi-
e
records matching that leaf)
squared

u t e

s  

c

d

copyright    2001, 2003, andrew w. moore

71

test your understanding
assuming regular regression trees, can you sketch a 
graph of the fitted function yest(x)over this diagram?

y

x

copyright    2001, 2003, andrew w. moore

72

36

test your understanding

assuming id75 trees, can you sketch a graph 
of the fitted function yest(x)over this diagram?

y

x

copyright    2001, 2003, andrew w. moore

73

multilinear
interpolation

copyright    2001, 2003, andrew w. moore

74

37

multilinear interpolation

consider this dataset. suppose we wanted to create a 
continuous and piecewise linear fit to the data

y

x

copyright    2001, 2003, andrew w. moore

75

multilinear interpolation
create a set of knot points: selected x-coordinates 
(usually equally spaced) that cover the data

y

q1

x

q2

q3

q4

q5

copyright    2001, 2003, andrew w. moore

76

38

multilinear interpolation

we are going to assume the data was generated by a 
noisy version of a function that can only bend at the 
knots. here are 3 examples (none fits the data well)

y

q1

x

q2

q3

q4

q5

copyright    2001, 2003, andrew w. moore

77

how to find the best fit?
idea 1: simply perform a separate regression in each 
segment for each part of the curve

what   s the problem with this idea?

y

q1

x

q2

q3

q4

q5

copyright    2001, 2003, andrew w. moore

78

39

how to find the best fit?

let   s look at what goes on in the red segment

yest

x
)(

=

x

)

(

q
3

   
w

h
2

+

x

)

(

q

2

   
w

h
3

qw
 
 where
3

=

   

q

2

h2

y

h3

q2

q3

q4

q5

q1

x

copyright    2001, 2003, andrew w. moore

79

in the red segment   

how to find the best fit?
x  h
)(
33
q
3

x
)(

yest

+

2

x  
 
where
2

1)(
   =

x  
3

1)(
   =

x  h
)(
=
22
qx
   
w

,

x

   
w

h2

y

h3

  2(x)

q2

q3

q4

q5

q1

x

copyright    2001, 2003, andrew w. moore

80

40

in the red segment   

how to find the best fit?
x  h
)(
33
q
3

x
)(

yest

+

2

x  
 
where
2

1)(
   =

x  
3

1)(
   =

x  h
)(
=
22
qx
   
w

,

x

   
w

h2

y

h3

  2(x)

q1

x

q2

q3

q4

q5

  3(x)

copyright    2001, 2003, andrew w. moore

81

in the red segment   

how to find the best fit?
x  h
)(
+
33
|1)(
x  
   =
3

x
)(
|1)(
x  
where
 
   =
2

x  h
)(
=
22
qx
,|
   
w

yest

2

|

qx
   
3
w

h2

y

h3

  2(x)

q1

x

q2

q3

q4

q5

  3(x)

copyright    2001, 2003, andrew w. moore

82

41

in the red segment   

how to find the best fit?
x  h
)(
+
33
|1)(
x  
   =
3

x
)(
|1)(
x  
where
 
   =
2

x  h
)(
=
22
qx
,|
   
w

yest

2

|

qx
   
3
w

h2

y

h3

  2(x)

q1

x

q2

q3

q4

q5

  3(x)

copyright    2001, 2003, andrew w. moore

83

how to find the best fit?

in general

y

est

x  h
)(
i

i

kn

   
i
1
=
qx
   
i
w

0

x
)(
=
|1
   
   
   
   
      

|

| if

i

wqx
 
|
<
   
otherwise

  2(x)

x  
)(
where
 
i

=

h2

y

h3

q1

x

q2

q3

q4

q5

  3(x)

copyright    2001, 2003, andrew w. moore

84

42

how to find the best fit?

in general

y

est

x  h
)(
i

i

kn

   
i
1
=
qx
   
i
w

0

x
)(
=
|1
   
   
   
   
      

|

| if

i

wqx
 
|
<
   
otherwise

x  
)(
where
 
i

=

h2

and this is simply a basis function 

regression problem!

y

h3

we know how to find the least 

squares hiis!

  2(x)

q1

x

q2

q3

q4

q5

  3(x)

copyright    2001, 2003, andrew w. moore

85

in two dimensions   

blue dots show 
locations of input 
vectors (outputs 
not depicted)

x2

x1

copyright    2001, 2003, andrew w. moore

86

43

in two dimensions   

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface

blue dots show 
locations of input 
vectors (outputs 
not depicted)

x2

x1

copyright    2001, 2003, andrew w. moore

87

in two dimensions   

9

7

3

8

blue dots show 
locations of input 
vectors (outputs 
not depicted)

x2

x1

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface
but how do we 
do the 
interpolation to 
ensure that the 
surface is 
continuous?

copyright    2001, 2003, andrew w. moore

88

44

in two dimensions   

9

7

3

8

blue dots show 
to predict the 
locations of input 
value here   
vectors (outputs 
not depicted)

x2

x1

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface
but how do we 
do the 
interpolation to 
ensure that the 
surface is 
continuous?

copyright    2001, 2003, andrew w. moore

89

in two dimensions   

blue dots show 
locations of input 
vectors (outputs 
not depicted)

to predict the 
value here   
first interpolate 
its value on two 
opposite edges   

x2

x1

3

8

9

7

7

7.33

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface
but how do we 
do the 
interpolation to 
ensure that the 
surface is 
continuous?

copyright    2001, 2003, andrew w. moore

90

45

in two dimensions   

blue dots show 
locations of input 
vectors (outputs 
not depicted)

to predict the 
value here   
first interpolate 
its value on two 
opposite edges   
then interpolate 
between those 
x2
two values

x1

3

8

9

7

7.05

7

7.33

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface
but how do we 
do the 
interpolation to 
ensure that the 
surface is 
continuous?

copyright    2001, 2003, andrew w. moore

91

blue dots show 
locations of input 
vectors (outputs 
not depicted)

to predict the 
value here   
first interpolate 
its value on two 
opposite edges   
then interpolate 
between those 
x2
two values

in two dimensions   

each purple dot 
is a knot point. 
it will contain 
the height of 
the estimated 
surface
but how do we 
do the 
interpolation to 
ensure that the 
surface is 
continuous?

9

7

3

7.05

7

x1

7.33

8

notes:
this can easily be generalized 
to mdimensions.
it should be easy to see that it 
ensures continuity
the patches are not linear

copyright    2001, 2003, andrew w. moore

92

46

doing the regression

9

7

3

8

x2

x1

copyright    2001, 2003, andrew w. moore

given data, how 
do we find the 
optimal knot 
heights?
happily, it   s 
simply a two-
dimensional 
basis function 
problem.
(working out 
the basis 
functions is 
tedious, 
unilluminating, 
and easy)
what   s the 
problem in 
higher 
dimensions?

93

mars: multivariate 
adaptive regression 

splines

copyright    2001, 2003, andrew w. moore

94

47

mars

    multivariate adaptive regression splines
    invented by jerry friedman (one of 

andrew   s heroes)
    simplest version:

let   s assume the function we are learning is of the 

following form:
est

y

)(x

=

m

   

k

1
=

xg
(
k

k

)

instead of a linear combination of the inputs, it   s a linear 
combination of non-linear functions of individualinputs

copyright    2001, 2003, andrew w. moore

95

mars

est

y

)(x

=

m

   

k

1
=

xg
(
k

k

)

instead of a linear combination of the inputs, it   s a linear 
combination of non-linear functions of individualinputs

idea: each 
gkis one of 

these

y

q1

q2

q3

q4

q5

copyright    2001, 2003, andrew w. moore

x

96

48

mars

est

y

)(x

=

m

   

k

1
=

xg
(
k

k

)

instead of a linear combination of the inputs, it   s a linear 
combination of non-linear functions of individualinputs

est

y

)(x

=

n

m

k

      

k

1
=

j

1
=

x  h
(
k
j

k
j

k

)

x  
k
)(
where
 
j

y

=

q

k
j

|

   
1
   
   
      

|

x

   

k

   
w
k
0

| if

 

   

wq
x
|
k
<
k
j
k
otherwise

q1

q2

q3

q4

q5

copyright    2001, 2003, andrew w. moore

x

j: the regressed 

qk
j: the location of 
the j   th knot in the 
k   th dimension
hk
height of the j   th
knot in the k   th
dimension
wk: the spacing 
between knots in 
the kth dimension

97

that   s not complicated enough!
    okay, now let   s get serious. we   ll allow 

arbitrary    two-way interactions   :

est

y

)(x

=

m

   

k

1
=

xg
(
k

k

)

+

m

m

       

g
kt
1
+=

1
=

k

(

xx
,
t
k

)

kt

the function we   re 

learning is allowed to be 

a sum of non-linear 

functions over all one-d 

and 2-d subsets of 

attributes

can still be expressed as a linear 
combination of basis functions
thus learnable by id75
full mars: uses cross-validation to 
choose a subset of subspaces, knot 
resolution and other parameters.

copyright    2001, 2003, andrew w. moore

98

49

if you like mars   

   see also cmac (cerebellar model articulated 

controller) by james albus (another of 
andrew   s heroes)
    many of the same gut-level intuitions
    but entirely in a neural-network, biologically 

plausible way
    (all the low dimensional functions are by 

means of lookup tables, trained with a delta-
rule and using a clever blurred update and 
hash-tables)

copyright    2001, 2003, andrew w. moore

99

where are we now?

s id136

engine learn p(e1|e2) joint de

t
u
p
n
i

s
t
u
p
n
i

classifier

predict
category

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

prob-
ability

predict
real no.

dec tree, gauss/joint bc, gauss na  ve bc, 

joint de, na  ve de, gauss/joint de, gauss na  ve 
de

id75, polynomial regression,  rbfs, 
robust regression regression trees,  multilinear 
interp, mars

copyright    2001, 2003, andrew w. moore

100

50

citations

radial basis functions
t. poggio and f. girosi, 

id173 algorithms for 
learning that are equivalent to 
multilayer networks, science, 
247, 978--982, 1989

loess
w. s. cleveland, robust locally 

weighted regression and 
smoothing scatterplots, journal 
of the american statistical 
association, 74, 368, 829-836, 
december, 1979

regression trees etc
l. breiman and j. h. friedman and 

r. a. olshen and c. j. stone, 
classification and regression 
trees, wadsworth, 1984

j. r. quinlan, combining instance-

based and model-based 
learning, machine learning: 
proceedings of the tenth 
international conference, 1993

mars
j. h. friedman, multivariate 

adaptive regression splines, 
department for statistics, 
stanford university, 1988, 
technical report no. 102

copyright    2001, 2003, andrew w. moore

101

51

