foundations and trends r(cid:1) in
theoretical computer science
vol. 8, nos. 1   2 (2012) 1   141
c(cid:1) 2013 n. k. vishnoi
doi: 10.1561/0400000054

lx = b

laplacian solvers and

their algorithmic applications

by nisheeth k. vishnoi

contents

preface

notation

i basics

1 basic id202

1.1 spectral decomposition of symmetric matrices
1.2 min   max characterizations of eigenvalues

2 the graph laplacian

2.1 the graph laplacian and its eigenvalues
2.2 the second eigenvalue and connectivity

3 laplacian systems and solvers

3.1 system of linear equations
3.2 laplacian systems
3.3 an approximate, linear-time laplacian solver
3.4 linearity of the laplacian solver

2

6

8

9

9
12

14

14
16

18

18
19
19
20

4 graphs as electrical networks

incidence matrices and electrical networks

4.1
4.2 e   ective resistance and the    matrix
4.3 electrical flows and energy
4.4 weighted graphs

ii applications

5 graph partitioning i the normalized laplacian

5.1 graph conductance
5.2 a mathematical program
5.3 the normalized laplacian and its second eigenvalue

6 graph partitioning ii

a spectral algorithm for conductance

6.1 sparse cuts from (cid:1)1 embeddings
6.2 an (cid:1)1 embedding from an (cid:1)2

2 embedding

7 graph partitioning iii balanced cuts

7.1 the balanced edge-separator problem
7.2 the algorithm and its analysis

8 graph partitioning iv

computing the second eigenvector

8.1 the power method
8.2 the second eigenvector via powering

9 the matrix exponential and id93

9.1 the matrix exponential
9.2 rational approximations to the exponential
9.3 simulating continuous-time id93

22

22
24
25
27

28

29

29
31
34

37

37
40

44

44
46

49

49
50

54

54
56
59

10 graph sparsi   cation i

sparsi   cation via e   ective resistances

10.1 graph sparsi   cation
10.2 spectral sparsi   cation using e   ective resistances
10.3 crude spectral spars   cation

11 graph sparsi   cation ii

computing electrical quantities

11.1 computing voltages and currents
11.2 computing e   ective resistances

12 cuts and flows

12.1 maximum flows, minimum cuts
12.2 combinatorial versus electrical flows
12.3 s, t-maxflow
12.4 s, t-min cut

iii tools

13 cholesky decomposition based linear solvers

13.1 cholesky decomposition
13.2 fast solvers for tree systems

14 iterative linear solvers i

the kaczmarz method

14.1 a randomized kaczmarz method
14.2 convergence in terms of average condition number
14.3 toward an   o(m)-time laplacian solver

15 iterative linear solvers ii

the gradient method

62

62
64
67

69

69
71

75

75
77
78
83

86

87

87
89

92

92
94
96

99

15.1 optimization view of equation solving
15.2 the id119-based solver

16 iterative linear solvers iii

the conjugate gradient method

16.1 krylov subspace and a-orthonormality
16.2 computing the a-orthonormal basis
16.3 analysis via polynomial minimization
16.4 chebyshev polynomials     why conjugate

gradient works

16.5 the chebyshev iteration
16.6 matrices with clustered eigenvalues

17 preconditioning for laplacian systems

17.1 preconditioning
17.2 combinatorial preconditioning via trees
17.3 an   o(m4/3)-time laplacian solver

18 solving a laplacian system in   o(m) time

18.1 main result and overview
18.2 eliminating degree 1,2 vertices
18.3 crude sparsi   cation using low-stretch

spanning trees

18.4 recursive preconditioning     proof of the

main theorem

18.5 error analysis and linearity of the inverse

19 beyond ax = b the lanczos method

19.1 from scalars to matrices
19.2 working with krylov subspace
19.3 computing a basis for the krylov subspace

references

99
100

103

103
105
107

110
111
112

114

114
116
117

119

119
122

123

125
127

129

129
130
132

136

foundations and trends r(cid:1) in
theoretical computer science
vol. 8, nos. 1   2 (2012) 1   141
c(cid:1) 2013 n. k. vishnoi
doi: 10.1561/0400000054

lx = b

laplacian solvers and

their algorithmic applications

nisheeth k. vishnoi

microsoft research, india, nisheeth.vishnoi@gmail.com

abstract

the ability to solve a system of linear equations lies at the heart of areas
such as optimization, scienti   c computing, and computer science, and
has traditionally been a central topic of research in the area of numer-
ical id202. an important class of instances that arise in prac-
tice has the form lx = b, where l is the laplacian of an undirected
graph. after decades of sustained research and combining tools from
disparate areas, we now have laplacian solvers that run in time nearly-
linear in the sparsity (that is, the number of edges in the associated
graph) of the system, which is a distant goal for general systems. sur-
prisingly, and perhaps not the original motivation behind this line of
research, laplacian solvers are impacting the theory of fast algorithms
for fundamental graph problems. in this monograph, the emerging
paradigm of employing laplacian solvers to design novel fast algorithms
for graph problems is illustrated through a small but carefully chosen
set of examples. a part of this monograph is also dedicated to develop-
ing the ideas that go into the construction of near-linear-time laplacian
solvers. an understanding of these methods, which marry techniques
from id202 and id207, will not only enrich the tool-set
of an algorithm designer but will also provide the ability to adapt these
methods to design fast algorithms for other fundamental problems.

preface

the ability to solve a system of linear equations lies at the heart of areas
such as optimization, scienti   c computing, and computer science and,
traditionally, has been a central topic of research in numerical linear
algebra. consider a system ax = b with n equations in n variables.
broadly, solvers for such a system of equations fall into two categories.
the    rst is gaussian elimination-based methods which, essentially, can
be made to run in the time it takes to multiply two n    n matrices,
(currently o(n2.3...) time). the second consists of iterative methods,
such as the conjugate gradient method. these reduce the problem
to computing n matrix   vector products, and thus make the running
time proportional to mn where m is the number of nonzero entries, or
sparsity, of a.1 while this bound of n in the number of iterations is
tight in the worst case, it can often be improved if a has additional
structure, thus, making iterative methods popular in practice.

an important class of such instances has the form lx = b, where l
is the laplacian of an undirected graph g with n vertices and m edges

1 strictly speaking, this bound on the running time assumes that the numbers have bounded
precision.

2

preface

3

with m (typically) much smaller than n2. perhaps the simplest setting
in which such laplacian systems arise is when one tries to compute cur-
rents and voltages in a resistive electrical network. laplacian systems
are also important in practice, e.g., in areas such as scienti   c computing
and id161. the fact that the system of equations comes from
an underlying undirected graph made the problem of designing solvers
especially attractive to theoretical computer scientists who entered the
fray with tools developed in the context of graph algorithms and with
the goal of bringing the running time down to o(m). this e   ort gained
serious momentum in the last 15 years, perhaps in light of an explosive
growth in instance sizes which means an algorithm that does not scale
near-linearly is likely to be impractical.

after decades of sustained research, we now have a solver for lapla-
cian systems that runs in o(mlog n) time. while many researchers have
contributed to this line of work, spielman and teng spearheaded this
endeavor and were the    rst to bring the running time down to   o(m)
by combining tools from graph partitioning, id93, and low-
stretch spanning trees with numerical methods based on gaussian elim-
ination and the conjugate gradient. surprisingly, and not the original
motivation behind this line of research, laplacian solvers are impacting
the theory of fast algorithms for fundamental graph problems; giving
back to an area that empowered this work in the    rst place.

that is the story this monograph aims to tell in a comprehensive
manner to researchers and aspiring students who work in algorithms
or numerical
id202. the emerging paradigm of employing
laplacian solvers to design novel fast algorithms for graph problems
is illustrated through a small but carefully chosen set of problems
such as graph partitioning, computing the matrix exponential, simulat-
ing id93, graph sparsi   cation, and single-commodity    ows. a
signi   cant part of this monograph is also dedicated to developing the
algorithms and ideas that go into the proof of the spielman   teng lapla-
cian solver. it is a belief of the author that an understanding of these
methods, which marry techniques from id202 and id207,
will not only enrich the tool-set of an algorithm designer, but will also
provide the ability to adapt these methods to design fast algorithms
for other fundamental problems.

4 preface

how to use this monograph. this monograph can be used as the
text for a graduate-level course or act as a supplement to a course on
spectral id207 or algorithms. the writing style, which deliber-
ately emphasizes the presentation of key ideas over rigor, should even
be accessible to advanced undergraduates. if one desires to teach a
course based on this monograph, then the best order is to go through
the sections linearly. essential are sections 1 and 2 that contain the
basic id202 material necessary to follow this monograph and
section 3 which contains the statement and a discussion of the main
theorem regarding laplacian solvers. parts of this monograph can also
be read independently. for instance, sections 5   7 contain the cheeger
inequality based spectral algorithm for graph partitioning. sections 15
and 16 can be read in isolation to understand the conjugate gradient
method. section 19 looks ahead into computing more general functions
than the inverse and presents the lanczos method. a dependency dia-
gram between sections appears in figure 1. for someone solely inter-
ested in a near-linear-time algorithm for solving laplacian systems, the
quick path to section 14, where the approach of a short and new proof
is presented, should su   ce. however, the author recommends going all

8

10

4

11

5

6

7

1,2,3

12

14

18

9

13

15

16

17

19

fig. 1 the dependency diagram among the sections in this monograph. a dotted line from
i to j means that the results of section j use some results of section i in a black-box manner
and a full understanding is not required.

preface

5

the way to section 18 where multiple techniques developed earlier in
the monograph come together to give an   o(m) laplacian solver.

acknowledgments. this monograph is partly based on lectures
delivered by the author in a course at the indian institute of sci-
ence, bangalore. thanks to the scribes: deeparnab chakrabarty,
avishek chatterjee, jugal garg, t. s. jayaram, swaprava nath, and
deepak r. special thanks to elisa celis, deeparnab chakrabarty,
lorenzo orecchia, nikhil srivastava, and sushant sachdeva for read-
ing through various parts of this monograph and providing valuable
feedback. finally, thanks to the reviewer(s) for several insightful com-
ments which helped improve the presentation of the material in this
monograph.

bangalore
15 january 2013

nisheeth k. vishnoi
microsoft research india

notation

    the set of real numbers is denoted by r, and r   0 denotes
the set of nonnegative reals. we only consider real numbers
in this monograph.
    the set of integers is denoted by z, and z   0 denotes the set
of nonnegative integers.
    vectors are denoted by boldface, e.g., u,v. a vector v     rn
is a column vector but often written as v = (v1, . . . , vn). the
transpose of a vector v is denoted by v(cid:3)
    for vectors u,v, their inner product is denoted by (cid:2)u,v(cid:3) or
u(cid:3)v.
    for a vector v, (cid:4)v(cid:4) denotes its (cid:1)2 or euclidean norm where
(cid:4)v(cid:4) def=
hattan distance norm (cid:4)v(cid:4)1
    the outer product of a vector v with itself is denoted by
vv(cid:3)
    matrices are denoted by capitals, e.g., a, l. the transpose
(cid:3)
of a is denoted by a
    we use ta to denote the time it takes to multiply the matrix
a with a vector.

(cid:1)(cid:2)v,v(cid:3). we sometimes also refer to the (cid:1)1 or man-

(cid:2)n
i=1|vi|.

def=

.

.

.

6

notation 7

   

its

def=

symmetric matrix a,

    the a-norm of a vector v is denoted by (cid:4)v(cid:4)a
v(cid:3)av.
    for a real
real eigenvalues
are ordered   1(a)       2(a)                  n(a). we let   (a) def=
[  1(a),   n(a)].
    a positive-semide   nite (psd) matrix is denoted by a (cid:7) 0
and a positive-de   nite matrix a (cid:8) 0.
    the norm of a symmetric matrix a is denoted by (cid:4)a(cid:4) def=
max{|  1(a)|,|  n(a)|}. for a symmetric psd matrix a,
(cid:4)a(cid:4) =   n(a).
    thinking of a matrix a as a linear operator, we denote the
image of a by im(a) and the rank of a by rank(a).
    a graph g has a vertex set v and an edge set e. all graphs
are assumed to be undirected unless stated otherwise. if the
graph is weighted, there is a weight function w : e (cid:9)    r   0.
typically, n is reserved for the number of vertices |v |, and
m for the number of edges |e|.
    ef[  ] denotes the expectation and pf[  ] denotes the proba-
bility over a distribution f. the subscript is dropped when
clear from context.
    the following acronyms are used liberally, with respect to
(w.r.t.), without loss of generality (w.l.o.g.), with high prob-
ability (w.h.p.), if and only if (i   ), right-hand side (r.h.s.),
left-hand side (l.h.s.), and such that (s.t.).
    standard big-o notation is used to describe the limiting
behavior of a function.   o denotes potential
logarithmic
i.e., f =   o(g) is equivalent to
factors which are ignored,
f = o(g logk(g)) for some constant k.

part i
basics

1

basic id202

this section reviews basics from id202, such as eigenvalues and
eigenvectors, that are relevant to this monograph. the spectral theorem
for symmetric matrices and min   max characterizations of eigenvalues
are derived.

1.1 spectral decomposition of symmetric matrices
one way to think of an m    n matrix a with real entries is as a linear
operator from rn to rm which maps a vector v     rn to av     rm.
let dim(s) be dimension of s, i.e., the maximum number of linearly
independent vectors in s. the rank of a is de   ned to be the dimension
of the image of this linear transformation. formally, the image of a
is de   ned to be im(a) def= {u     rm : u = av for some v     rn}, and the
rank is de   ned to be rank(a) def= dim(im(a)) and is at most min{m, n}.
we are primarily interested in the case when a is square, i.e., m =
(cid:3) = a. of interest are vectors v such that
n, and symmetric, i.e., a
av =   v for some   . such a vector is called an eigenvector of a with
respect to (w.r.t.) the eigenvalue   . it is a basic result in id202
that every real matrix has n eigenvalues, though some of them could

9

10 basic id202

be complex. if a is symmetric, then one can show that the eigenvalues
are real. for a complex number z = a + ib with a, b     r, its conjugate
is de   ned as   z = a     ib. for a vector v, its conjugate transpose v(cid:1) is
the transpose of the vector whose entries are conjugates of those in v.
thus, v(cid:1)v = (cid:4)v(cid:4)2.
lemma 1.1. if a is a real symmetric n    n matrix, then all of its
eigenvalues are real.

proof. let    be an eigenvalue of a, possibly complex, and v be the
corresponding eigenvector. then, av =   v. conjugating both sides we
(cid:3) =   v(cid:1), where v(cid:1) is the conjugate transpose of v.
obtain that v(cid:1)a
hence, v(cid:1)av =   v(cid:1)v, since a is symmetric. thus,   (cid:4)v(cid:4)2 =   (cid:4)v(cid:4)2
which implies that    =   . thus,        r.
let   1       2                  n be the n real eigenvalues, or the spectrum, of a
with corresponding eigenvectors u1, . . . ,un. for a symmetric matrix, its
norm is

(cid:4)a(cid:4) def= max{|  1(a)|,|  n(a)|}.

we now study eigenvectors that correspond to distinct eigenvalues.

lemma 1.2. let   i and   j be two eigenvalues of a symmetric matrix
a, and ui, uj be the corresponding eigenvectors. if   i (cid:11)=   j, then
(cid:2)ui,uj(cid:3) = 0.

proof. given aui =   iui and auj =   juj, we have the following
sequence of equalities. since a is symmetric, u(cid:3)
i a. thus,
i a
u(cid:3)
i auj =   iu(cid:3)
i uj on the
other. therefore,   ju(cid:3)
i uj = 0 since
  i (cid:11)=   j.

(cid:3) = u(cid:3)
i auj =   ju(cid:3)
i uj. this implies that u(cid:3)

i uj on the one hand, and u(cid:3)

i uj =   iu(cid:3)

hence, the eigenvectors corresponding to di   erent eigenvalues are
orthogonal. moreover, if ui and uj correspond to the same eigen-
value   , and are linearly independent, then any linear combination

1.1 spectral decomposition of symmetric matrices

11

is also an eigenvector corresponding to the same eigenvalue. the maxi-
mal eigenspace of an eigenvalue is the space spanned by all eigenvectors
corresponding to that eigenvalue. hence, the above lemma implies that
one can decompose rn into maximal eigenspaces ui, each of which cor-
responds to an eigenvalue of a, and the eigenspaces corresponding to
distinct eigenvalues are orthogonal. thus, if   1 <   2 <        <   k are the
set of distinct eigenvalues of a real symmetric matrix a, and ui is the
eigenspace associated with   i, then, from the discussion above,

k(cid:3)

i=1

dim(ui) = n.

hence, given that we can pick an orthonormal basis for each ui, we may
assume that the eigenvectors of a form an orthonormal basis for rn.
thus, we have the following spectral decomposition theorem.
theorem 1.3. let   1                  n be the spectrum of a with corre-
sponding eigenvalues u1, . . . ,un. then, a =

(cid:2)n
i=1   iuiu(cid:3)
i .

proof. let b

def=

(cid:2)n
i=1   iuiu(cid:3)

i . then,

n(cid:3)

buj =

(cid:3)
  iuiu
i uj

i=1
=   juj

= auj.

the above is true for all j. since ujs are orthonormal basis of rn, we
have for all v     rn, av = bv. this implies a = b.

thus, when a is a real and symmetric matrix, im(a) is spanned by the
eigenvectors with nonzero eigenvalues. from a computational perspec-
tive, such a decomposition can be computed in time polynomial in the
bits needed to represent the entries of a.1

1 to be very precise, one can only compute eigenvalues and eigenvectors to a high enough
precision in polynomial time. we will ignore this distinction for this monograph as we do
not need to know the exact values.

12 basic id202

1.2 min   max characterizations of eigenvalues

now we present a variational characterization of eigenvalues which is
very useful.
lemma 1.4. if a is an n    n real symmetric matrix, then the largest
eigenvalue of a is

  n(a) = max

v   rn\{0}

v(cid:3)
av
v(cid:3)v .

proof. let   1       2                  n be the eigenvalues of a, and let
u1,u2, . . . ,un be the corresponding orthonormal eigenvectors which
span rn. hence, for all v     rn, there exist c1, . . . , cn     r such that
v =

i ciui. thus,

(cid:2)

(cid:4)(cid:3)
(cid:3)
(cid:7)(cid:3)      (cid:3)

i
c2
i .

i

j

(cid:5)

(cid:3)

ciui,

ciui

i

      (cid:6)(cid:3)

k

(cid:7)

ckuk

(cid:3)
  juju
j

i uj)    (u
(cid:3)
(cid:3)
cick  j(u
j uk)

(cid:2)v,v(cid:3) =

=

ciui

i

(cid:6)(cid:3)
(cid:3)
(cid:3)
(cid:3)

i,j,k

i

c2
i   i

moreover,

(cid:3)

v

av =

=

=

      n

i =   n(cid:2)v,v(cid:3) .
c2

i

hence,     v (cid:11)= 0, v(cid:2)av

v(cid:2)v       n. this implies,
v(cid:3)
av
v(cid:3)v

v   rn\{0}

max

      n.

1.2 min   max characterizations of eigenvalues

13

now note that setting v = un achieves this maximum. hence, the
lemma follows.

if one inspects the proof above, one can deduce the following lemma
just as easily.
lemma 1.5. if a is an n    n real symmetric matrix, then the smallest
eigenvalue of a is

  1(a) = min

v   rn\{0}

v(cid:3)
av
v(cid:3)v .

more generally, one can extend the proof of the lemma above to the
following. we leave it as a simple exercise.
theorem 1.6. if a is an n    n real symmetric matrix, then for all
1     k     n, we have

  k(a) =

v   rn\{0},v(cid:2)ui=0,   i   {1,...,k   1}

min

  k(a) =

v   rn\{0},v(cid:2)ui=0,   i   {k+1,...,n}

max

v(cid:3)
av
v(cid:3)v ,

v(cid:3)
av
v(cid:3)v .

and

notes

some good texts to review basic id202 are [35, 82, 85]. theo-
rem 1.6 is also called the courant   fischer   weyl min   max principle.

2

the graph laplacian

this section introduces the graph laplacian and connects the second
eigenvalue to the connectivity of the graph.

2.1 the graph laplacian and its eigenvalues

def= |e|.
consider an undirected graph g = (v, e) with n
we assume that g is unweighted; this assumption is made to simplify
the presentation but the content of this section readily generalizes to
the weighted setting. two basic matrices associated with g, indexed
by its vertices, are its adjacency matrix a and its degree matrix d.
let di denote the degree of vertex i.

def= |v | and m

and

ai,j

def=

di,j

def=

(cid:12)
(cid:12)

1
0

di
0

if ij     e,
otherwise,

if i = j,
otherwise.

def= d     a. we often refer
the graph laplacian of g is de   ned to be l
to this as the laplacian. in section 5, we introduce the normalized

14

(cid:2)

2.1 the graph laplacian and its eigenvalues

15

laplacian which is di   erent from l. to investigate the spectral proper-
ties of l, it is useful to    rst de   ne the n    n matrices le as follows: for
every e = ij     e, let le(i, j) = le(j, i) def=    1, let le(i, i) = le(j, j) = 1,
/    {i, j}. the following then fol-
(cid:6)
and let le(i
lows from the de   nition of the laplacian and les.

/    {i, j} or j

(cid:6)) = 0, if i
(cid:6)

, j

(cid:6)

lemma 2.1. let l be the laplacian of a graph g = (v, e). then,
l =

e   e le.

this can be used to show that the smallest eigenvalue of a laplacian
is always nonnegative. such matrices are called positive semide   nite,
and are de   ned as follows.

de   nition 2.1. a symmetric matrix a is called positive semide   nite
(psd) if   1(a)     0. a psd matrix is denoted by a (cid:7) 0. further, a is
said to be positive de   nite if   1(a) > 0, denoted a (cid:8) 0.

note that the laplacian is a psd matrix.

lemma 2.2. let l be the laplacian of a graph g = (v, e). then,
l (cid:7) 0.

proof. for any v = (v1, . . . , vn),

(cid:3)

v

lv = v

(cid:3)(cid:3)
(cid:3)
(cid:3)

e   e
(cid:3)

e   e

v

e=ij   e

=

=
    0.

lev

lev

(vi     vj)2

hence, minv(cid:7)=0 v(cid:3)
that l (cid:7) 0.

lv     0. thus, appealing to theorem 1.6, we obtain

(cid:12)
(cid:12)(cid:2)

16 the graph laplacian
is l (cid:8) 0? the answer to this is no: let 1 denote the vector with all
coordinates 1. then, it follows from lemma 2.1 that l1 = 0    1. hence,
  1(l) = 0.
for weighted a graph g = (v, e) with edge weights given by a weight
function wg : e (cid:9)    r   0, one can de   ne
wg(ij)
0

if ij     e
otherwise,

ai,j

def=

and

di,j

def=

0

l wg(il)

if i = j
otherwise.

then, the de   nition of the laplacian remains the same, namely, l =
d     a.

2.2 the second eigenvalue and connectivity

what about the second eigenvalue,   2(l), of the laplacian? we will
see in later sections on graph partitioning that the second eigenvalue
of the laplacian is intimately related to the conductance of the graph,
which is a way to measure how well connected the graph is. in this
section, we establish that   2(l) determines if g is connected or not.
this is the    rst result where we make a formal connection between the
spectrum of the laplacian and a property of the graph.

theorem 2.3. let l be the laplacian of a graph g = (v, e). then,
  2(l) > 0 i    g is connected.

proof. if g is disconnected, then l has a block diagonal structure. it
su   ces to consider only two disconnected components. assume the dis-
connected components of the graph are g1 and g2, and the correspond-
ing vertex sets are v1 and v2. the laplacian can then be rearranged
as follows:

(cid:13)

(cid:14)

,

l =

l(g1)

0

0

l(g2)

def=

(cid:14)

(cid:13)

(cid:14)

(cid:13)

0|v1|
1|v2|

2.2 the second eigenvalue and connectivity

17
where l(gi) is a |vi|    |vi| matrix for i = 1,2. consider the vector
, where 1|vi| and 0|vi| denote the all 1 and all 0 vectors of
x1
dimension |vi|, respectively. this is an eigenvector corresponding to the
. since
smallest eigenvalue, which is zero. now consider x2
(cid:2)x2,x1(cid:3) = 0, the smallest eigenvalue, which is 0 has multiplicity at least
2. hence,   2(l) = 0.

(cid:2)
for the other direction, assume that   2(l) = 0 and that g is con-
nected. let u2 be the second eigenvector normalized to have length 1.
e=ij   e(u2(i)     u2(j))2 = 0. hence, for
then,   2(l) = u(cid:3)
2 lu2. thus,
all e = ij     e, u2(i) = u2(j).
since g is connected, there is a path from vertex 1 to every
vertex j (cid:11)= 1, and for each intermediate edge e = ik, u2(i) = u2(k).
hence, u2(1) = u2(j),    j. hence, u2 = (c, c, . . . , c). however, we also
know (cid:2)u2,1(cid:3) = 0 which implies that c = 0. this contradicts the fact
that u2 is a nonzero eigenvector and establishes the theorem.

1|v1|
0|v2|

def=

notes

there are several books on spectral id207 which contain numer-
ous properties of graphs, their laplacians and their eigenvalues; see
[23, 34, 88]. due to the connection of the second eigenvalue of the
laplacian to the connectivity of the graph, it is sometimes called its
algebraic connectivity or its fiedler value, and is attributed to [28].

3

laplacian systems and solvers

this section introduces laplacian systems of linear equations and notes
the properties of the near-linear-time laplacian solver relevant for the
applications presented in this monograph. sections 5   12 cover several
applications that reduce fundamental graph problems to solving a small
number of laplacian systems.

3.1 system of linear equations
an n    n matrix a and vector b     rn together de   ne a system of linear
equations ax = b, where x = (x1, . . . , xn) are variables. by de   nition,
a solution to this linear system exists if and only if (i   ) b lies in the
image of a. a is said to be invertible, i.e., a solution exists for all b, if
its image is rn, the entire space. in this case, the inverse of a is denoted
   1. the inverse of the linear operator a, when b is restricted to
by a
the image of a, is also well de   ned and is denoted by a+. this is called
the pseudo-inverse of a.

18

3.2 laplacian systems

19

3.2 laplacian systems

now consider the case when, in a system of equations ax = b, a = l
is the graph laplacian of an undirected graph. note that this sys-
tem is not invertible unless b     im(l). it follows from theorem 2.3
that for a connected graph, im(l) consists of all vectors orthogonal to
the vector 1. hence, we can solve the system of equations lx = b if
(cid:2)b,1(cid:3) = 0. such a system will be referred to as a laplacian system of
linear equations or, in short, a laplacian system. hence, we can de   ne
the pseudo-inverse of the laplacian as the linear operator which takes
a vector b orthogonal to 1 to its pre-image.

3.3 an approximate, linear-time laplacian solver

in this section we summarize the near-linear algorithm known for
solving a laplacian system lx = b. this result is the basis of the appli-
cations and a part of the latter half of this monograph is devoted to its
proof.

theorem 3.1. there is an algorithm lsolve which takes as input
a graph laplacian l, a vector b, and an error parameter    > 0, and
returns x satisfying

(cid:4)x     l+b(cid:4)l       (cid:4)l+b(cid:4)l,

bt lb. the algorithm runs in (cid:15)o(mlog 1/  ) time, where

   

where (cid:4)b(cid:4)l
m is the number of nonzero entries in l.

def=

let us    rst relate the norm in the theorem above to the euclidean
norm. for two vectors v,w and a symmetric psd matrix a,
  1(a)(cid:4)v     w(cid:4)2     (cid:4)v     w(cid:4)2
in other words,

a = (v     w)t a(v     w)       n(a)(cid:4)v     w(cid:4)2.

(cid:4)a+(cid:4)1/2(cid:4)v     w(cid:4)     (cid:4)v     w(cid:4)a     (cid:4)a(cid:4)1/2(cid:4)v     w(cid:4).

hence, the distortion in distances due to the a-norm is at most

(cid:1)

  n(a)/  1(a).

20 laplacian systems and solvers

for the graph laplacian of an unweighted and connected graph,
when all vectors involved are orthogonal to 1,   2(l) replaces   1(l).
it can be checked that when g is unweighted,   2(l)     1/poly(n) and
  n(l)     tr(l) = m     n2. when g is weighted the ratio between the
l-norm and the euclidean norm scales polynomially with the ratio of
the largest to the smallest weight edge. finally, note that for any two
vectors, (cid:4)v     w(cid:4)        (cid:4)v     w(cid:4). hence, by a choice of   
def=   /poly(n) in
theorem 3.1, we ensure that the approximate vector output by lsolve
is    close in every coordinate to the actual vector. note that since the
dependence on the tolerance on the running time of theorem 3.1 is
logarithmic, the running time of lsolve remains   o(mlog 1/  ).

3.4 linearity of the laplacian solver
while the running time of lsolve is   o(mlog 1/  ), the algorithm pro-
duces an approximate solution that is o    by a little from the exact
solution. this creates the problem of estimating the error accumula-
tion as one iterates this solver. to get around this, we note an impor-
tant feature of lsolve: on input l, b, and   , it returns the vector
x = zb, where z is an n    n matrix and depends only on l and   . z
is a symmetric linear operator satisfying

(1       )z+ (cid:14) l (cid:14) (1 +   )z+,

(3.1)

and has the same image as l. note that in applications, such as that in
section 9, for z to satisfy (cid:4)z     l+(cid:4)       , the running time is increased
to   o(mlog(1/    2(l))). since in most of our applications   2(l) is at least
1/poly(n), we ignore this distinction.

notes

a good text to learn about matrices, their norms, and the inequalities
that relate them is [17]. the pseudo-inverse is sometimes also referred to
as the moore   penrose pseudo-inverse. laplacian systems arise in many
areas such as machine learning [15, 56, 89], id161 [57, 73],
partial di   erential equations and interior point methods [24, 30], and
solvers are naturally needed; see also surveys [75] and [84].

3.4 linearity of the laplacian solver

21

to solving a small number of laplacian systems

we will see several applications that reduce fundamental graph
problems
in
sections 5   12. sections 8 and 9 (a result of which is assumed in
sections 5 and 6) require the laplacian solver to be linear. the notable
applications we will not be covering are an algorithm to sample a ran-
dom spanning tree [45] and computing multicommodity    ows [46].

theorem 3.1 was    rst proved in [77] and the full proof appears in a
series of papers [78, 79, 80]. the original running time contained a very
large power of the log n term (hidden in   o(  )). this power has since
been reduced in a series of work [8, 9, 49, 62] and,    nally, brought down
to log n in [50]. we provide a proof of theorem 3.1 in section 18 along
with its linearity property mentioned in this section. recently, a simpler
proof of theorem 3.1 was presented in [47]. this proof does not require
many of the ingredients such as spectral sparsi   ers (section 10), pre-
conditioning (section 17), and conjugate gradient (section 16). while
we present a sketch of this proof in section 14, we recommend that the
reader go through the proof in section 18 and, in the process, familiar-
ize themselves with this wide array of techniques which may be useful
in general.

4

graphs as electrical networks

this section introduces how a graph can be viewed as an electrical
network composed of resistors. it is shown how voltages and currents
can be computed by solving a laplacian system. the notions of e   ec-
tive resistance, electrical    ows, and their energy are presented. viewing
graphs as electrical networks will play an important role in applications
presented in sections 10   12 and in the approach to a simple proof of
theorem 3.1 presented in section 14.

4.1

incidence matrices and electrical networks

given an undirected, unweighted graph g = (v, e), consider an arbi-
trary orientation of its edges. let b     {   1,0,1}m  n be the matrix
whose rows are indexed by the edges and columns by the vertices of g
where the entry corresponding to (e, i) is 1 if a vertex i is the tail of the
directed edge corresponding to e, is    1 if i is the head of the directed
edge e, and is zero otherwise. b is called the (edge-vertex) incidence
matrix of g. the laplacian can now be expressed in terms of b. while
b depends on the choice of the directions to the edges, the laplacian
does not.

22

4.1 incidence matrices and electrical networks

23

lemma 4.1. let g be a graph with (arbitrarily chosen) incidence
matrix b and laplacian l. then, b

b = l.

(cid:3)

(cid:3)

(cid:3)

b)i,i =

b, i.e., (b

proof. for the diagonal elements of b
e bi,ebe,i.
the terms are nonzero only for those edges e which are incident to i, in
which case the product is 1, and hence, this sum gives the degree of ver-
b)i,j =
e bi,ebe,j.
tex i in the undirected graph. for other entries, (b
the product terms are nonzero only when the edge e is shared by i
b)i,j =    1,     i (cid:11)= j,
and j. in either case the product is    1. hence, (b
ij     e. hence, b

b = l.

(cid:3)

(cid:3)

(cid:3)

(cid:2)
(cid:2)

now we associate an electrical network to g. replace each edge
with a resistor of value 1. to make this circuit interesting, we need to
add power sources to its vertices. suppose cext     rn is a vector which
indicates how much current is going in at each vertex. this will induce
voltages at each vertex and a current across each edge. we capture
these by vectors v     rn and i     rm, respectively. kircho      s law asserts
that, for every vertex, the di   erence of the outgoing current and the
incoming current on the edges adjacent to it equals the external current
input at that vertex. thus,

(cid:3)

b

i = cext.

on the other hand, ohm   s law asserts that the current in an edge equals
the voltage di   erence divided by the resistance of that edge. since in
our case all resistances are one, this gives us the following relation.

i = bv.

combining these last two equalities with lemma 4.1 we obtain that

(cid:3)

b

bv = lv = cext.

if (cid:2)cext,1(cid:3) = 0, which means there is no current accumulation inside
the electrical network, we can solve for v = l+cext. the voltage vector
is not unique since we can add the same constant to each of its entries
and it still satis   es ohm   s law. the currents across every edge, however,

24 graphs as electrical networks
are unique. de   ne vectors ei     rn for i     v which have a 1 at the i-th
location and zero elsewhere. then the current through the edge e = ij,
taking into account the sign, is (ei     ej)(cid:3)v = (ei     ej)(cid:3)
l+cext. thus,
computing voltages and currents in such a network is equivalent to
solving a laplacian system.

4.2 e   ective resistance and the    matrix
now we consider a speci   c vector cext and introduce an important
def=
quantity related to each edge, the e   ective resistance. consider cext
ei     ej. the voltage di   erence between vertices i and j is then (ei    
l+(ei     ej). when e = ij is an edge, this quantity is called the
ej)(cid:3)
e   ective resistance of e.

de   nition 4.1. given a graph g = (v, e) with laplacian l and edge
e = ij     e,

re   (e) def= (ei     ej)
(cid:3)

l+(ei     ej).

re   (e) is the potential di   erence across e when a unit current is
inducted at i and taken out at j.

we can also consider the current through an edge f when a unit
current is inducted and taken out at the endpoints of a possibly dif-
ferent edge e     e. we capture this by the    matrix which we now
de   ne formally. let us denote the rows from the b matrix be and bf
respectively. then,

in matrix notation,

(cid:3)
  (f, e) def= b
f l+be.

   = bl+b

(cid:3)

.

note also that   (e, e) is the e   ective resistance of the edge e. the matrix
   has several interesting properties. the    rst is trivial and follows from
the fact the laplacian and, hence, its pseudo-inverse is symmetric.

4.3 electrical flows and energy

25

proposition 4.2.    is symmetric.

additionally,    is a projection matrix.

proposition 4.3.   2 =   .

proof.   2 = bl+b

(cid:3)    bl+b

(cid:16) (cid:17)(cid:18) (cid:19)

(cid:3)

b

(cid:3) = b l+b

l+b

(cid:3) = bl+b

(cid:3) =   .

the third equality comes from the fact that the rows of b are orthog-
onal to 1.

=i

proposition 4.4. the eigenvalues of    are all either 0 or 1.

proof. let v be an eigenvector of    corresponding to the eigen-
value   . hence,   v =   v       2v =     v       v =   2v       v =   2v    
(  2       )v = 0        = 0,1.

hence, it is an easy exercise to prove that if g is connected, then
rank(  ) = n     1. in this case,    has exactly n     1 eigenvalues which
are 1 and m     n + 1 eigenvalues which are 0.

finally, we note the following theorem which establishes a connec-
tion between spanning trees and e   ective resistances. while this theo-
rem is not explicitly used in this monograph, the intuition arising from
it is employed in sparsifying graphs.

theorem 4.5. let t be a spanning tree chosen uniformly at random
from all spanning trees in g. then, the id203 that an edge e
belongs to t is given by

p[e     t ] = re   (e) =   (e, e).

4.3 electrical flows and energy

given a graph g = (v, e), we    x an orientation of its edges thus
resulting in an incidence matrix b. moreover, we associate unit

26 graphs as electrical networks

resistance with each edge of the graph. now suppose that for vertices s
and t one unit of current is injected at vertex s and taken out at t. the
electrical    ow on each edge is then captured by the vector

f (cid:1) def= bl+(es     et).

in general, an s, t-   ow is an assignment of values to directed edges such
that the total incoming    ow is the same as the total outgoing    ow at
all vertices except s and t. for a    ow vector f , the energy it consumes
is de   ned to be

(cid:3)

e(f) def=

f 2
e .

(cid:3)

thus, the energy of f (cid:1) is
e l+(es     et))2 =
(cid:3)
(b

e

(cid:3)

e

e

(es     et)
(cid:3)

e l+(es     et).
(cid:3)
(cid:2)
l+beb
e beb(cid:3)
e ,

(4.1)

b =

(cid:3)

by taking the summation inside and noting that l = b
equation (4.1) is equivalent to

(es     et)
(cid:3)

l+ll+(es     et) = (es     et)
(cid:3)
thus we have proved the following proposition.

l+(es     et).

proposition 4.6. if f (cid:1) is the unit s, t-   ow, then its energy is

e(f (cid:1)) = (es     et)
(cid:3)

l+(es     et).

the following is an important property of electrical s, t-   ows and will be
useful in    nding applications related to combinatorial    ows in graphs.

theorem 4.7. given a graph g = (v, e) with unit resistances across
all edges, if f (cid:1) def= bl+(es     et), then f (cid:1) minimizes the energy con-
sumed e(f) def=

e among all unit    ows from s to t.

e f 2

(cid:2)

(cid:3) be as before. proposition 4.3 implies that
proof. let    = bl+b
  2 =    and, hence, for all vectors g, (cid:4)g(cid:4)2     (cid:4)  g(cid:4)2 where equality holds

i    g is in the image of   . let f be any    ow such that b
i.e., any unit s, t-   ow. then,

4.4 weighted graphs

27
(cid:3)f = es     et,

e(f) = (cid:4)f(cid:4)2     (cid:4)  f(cid:4)2 = f

(cid:3)

    f = f

(cid:3)

  f = f

(cid:3)

bl+bf .

hence, using the fact that b

(cid:3)f = es     et, one obtains that

e(f)     f

(cid:3)

bl+bf = (es     et)
(cid:3)

l+(es     et) prop. 4.6= e(f (cid:1)).

hence, for any unit s, t-   ow f , e(f)     e(f (cid:1)).

4.4 weighted graphs

our results also extend to weighted graphs. suppose the graph
g = (v, e) has weights given by w : e (cid:9)    r   0. let w be the m    m
diagonal matrix such that w (e, e) = w(e). then, for an incidence
matrix b given an orientation of g, the laplacian is l
w b.
the    matrix in this setting is w 1/2bl+b
w 1/2. to set up a cor-
def= 1/w(e)
responding electrical network, we associate a resistance re
with each edge. thus,
for a given voltage vector v, the current
vector, by ohm   s law, is i = w bv. the e   ective resistance remains
re   (e) = (ei     ej)(cid:3)
l+(ei     ej) where l+ is the pseudo-inverse of the
laplacian which involves w. for vertices s and t, the unit s, t-   ow is
f (cid:1) def= w bl+(es     et) and its energy is de   ned to be
e )2, which
l+(es     et). it is an easy exercise to check
turns out to be (es     et)(cid:3)
that all the results in this section hold for this weighted setting.

(cid:2)

e re(f (cid:1)

def= b

(cid:3)

(cid:3)

notes

the connection between graphs, id93, and electrical networks
is an important one, and its study has yielded many surprising results.
the books [25] and [54] are good pointers for readers intending to
explore this connection.

while we do not need theorem 4.5 for this monograph, an interested
reader can try to prove it using the matrix-tree theorem, or refer to
[34]. it forms the basis for another application that uses laplacian
solvers to develop fast algorithms: generating a random spanning tree
in time   o(mn1/2), see [45].

part ii

applications

5

graph partitioning i

the normalized laplacian

this section introduces the fundamental problem of    nding a cut of
least conductance in a graph, called sparsest cut. a quadratic pro-
gram is presented which captures the sparsest cut problem exactly.
subsequently, a relaxation of this program is considered where the
optimal value is essentially the second eigenvalue of the normalized
laplacian; this provides a lower bound on the conductance of the graph.
in sections 6 and 7 this connection is used to come up with approxima-
tion algorithms for the sparsest cut and related balanced edge-
separator problems. finally, in section 8, it is shown how laplacian
solvers can be used to compute the second eigenvector and the associ-
ated second eigenvector in   o(m) time.

5.1 graph conductance

given an undirected, unweighted graph g = (v, e) with n vertices and
m edges, we are interested in vertex cuts in the graph. a vertex cut
def= v \s, which we
is a partition of v into two parts, s     v and   s
denote by (s,   s). before we go on to de   ne conductance, we need a
way to measure the size of a cut given by s     v. one measure is its

29

30 graph partitioning i the normalized laplacian
cardinality |s|. another is the sum of degrees of all the vertices in s.
if the graph is regular, i.e., all vertices have the same degree, then
these two are the same up to a factor of this    xed degree. otherwise,
they are di   erent and a part of the latter is called the volume of the
set. formally, for s     v, the volume of s is vol(s) def=
i   s di, where
di is the degree of vertex i. by a slight abuse of notation, we de   ne
vol(g) def= vol(v ) =
i   v di = 2m. the number of edges that cross the
cut (s,   s), i.e., have one end point in s and the other in   s, is denoted
|e(s,   s)|. now we de   ne the conductance of a cut.

(cid:2)

(cid:2)

de   nition 5.1. the conductance of a cut (s,   s) (also referred to as
its normalized cut value or cut ratio) is de   ned to be

  (s) def=

|e(s,   s)|

min{vol(s),vol(   s)} .

the conductance of a cut measures the ratio of edges going out of a cut
to the total edges incident to the smaller side of the cut. this is always
a number between 0 and 1. some authors de   ne the conductance to be

|e(s,   s)|
min{|s|,|   s|} ,

where |s| denotes the cardinality of s. the former de   nition is preferred
to the latter one as   (s) lies between 0 and 1 in case of the former
while there is no such bound on   (s) in the latter. speci   cally, the
latter is not a dimension-less quantity: if we replace each edge by k
copies of itself, this value will change, while the value given by the
former de   nition remains invariant. the graph conductance problem is
to compute the conductance of the graph which is de   ned to be

  (g) def= min
   (cid:7)=s(cid:1)v

  (s).

this problem is often referred to as the sparsest cut problem and
is np-hard. this, and its cousin, the balanced edge-separator
problem (to be introduced in section 7) are intensely studied, both in
theory and practice, and have far reaching connections to spectral graph
theory, the study of id93, and metric embeddings. besides

5.2 a mathematical program 31

being theoretically rich, they are of great practical importance as they
play a central role in the design of recursive algorithms, image segmen-
tation, community detection, and id91.

another quantity which is closely related to the conductance and is

often easier to manipulate is the following.

de   nition 5.2. for a cut (s,   s), the h-value of a set is de   ned to be

h(s) def=

|e(s,   s)|

vol(s)    vol(   s)

   vol(g)

and the h-value of the graph is de   ned to be

h(g) def= min
   (cid:7)=s(cid:1)v

h(s).

we    rst observe the relation between h and   .
lemma 5.1. for all s,   (s)     h(s)     2  (s).

proof. this follows from the observations that for any cut (s,   s),

vol(g)     max{vol(s),vol(   s)}     vol(g)/2

(cid:20)

(cid:21)

vol(s),vol(   s)

= vol(s)    vol(   s).

and

max{vol(s),vol(   s)}    min
hence,   (g)     h(g)     2  (g).

thus, the h-value captures the conductance of a graph up to a factor
of 2. computing the h-value of a graph is not any easier than comput-
ing its conductance. however, as we see next, it can be formulated as
a mathematical program which can then be relaxed to an eigenvalue
problem involving the laplacian.

5.2 a mathematical program

we will write down a mathematical program which captures the h-value
of the conductance. first, we introduce some notation which will be
useful.

32 graph partitioning i the normalized laplacian

5.2.1 id203 measures on vertices and

cuts as vectors

let    : e     [0,1] be the uniform id203 measure de   ned on the set
of edges e,

1
  (e) def=
m
for a subset of edges f     e,   (f ) def=
measure on the vertices induced by the degrees. for i     v,

(cid:2)

.

e   f   (e). next, we consider a

  (i) def= di

vol(g)

= di
2m

.

note that    is a id203 measure on v. we extend    to subsets
s     v ,

(cid:3)

(cid:3)

i   s

  (s) def=

  (i) =

di

vol(g)

i   s

=

vol(s)
vol(g) .

with these de   nitions it follows that

given s     v , let 1s : v (cid:9)    {0,1} denote the indicator function for the
set s by

and h(s) =   (e(s,   s))
2  (s)  (   s) .

  (s) =

  (e(s,   s))

2min{  (s),   (   s)}
(cid:12)

1s(i) def=

if i     s
otherwise.

1
0

then,

(1s(i)     1s(j))2 =

(cid:12)

therefore,

1 if (i     s and j       s) or (i       s and j     s),
0 if (i     s and j     s) or (i       s and j       s).

[(1s(i)     1s(j))2] =

e
ij     

|e(s,   s)|

m

=   (e(s,   s)).

moreover, for any s

5.2 a mathematical program 33

[(1s(i)     1s(j))2]

e

(i,j)         

=

=

p

(i,j)         

p

(i,j)         

[(1s(i)     1s(j))2 = 1]
[i     s, j       s or i       s, j     s]
[i       s] p
j     

[j       s] + p
i     

[i     s] p
j     

= p
i     
(since i and j are chosen independently)

[j     s]

=   (s)  (   s) +   (   s)  (s) = 2  (s)  (   s).

therefore,

h(s) =   (e(s,   s))
2  (s)  (   s)

=

eij     [(1s(i)     1s(j))2]
e(i,j)         [(1s(i)     1s(j))2] .

hence, noting the one-to-one correspondence between sets s     v and
functions x     {0,1}n, we have proved the following mathematical pro-
gramming characterization of the conductance.

lemma 5.2. consider the h-value of a graph g and the id203
distributions    and    as above. then, for x (cid:11)= 0,1,
eij     [(xi     xj)2]

h(g) = min

x   {0,1}n

ei      ej     [(xi     xj)2] .

as noted before, this quantity in the right-hand side (r.h.s.) of the
lemma above is hard to compute. let us now try to relax the condition
that x     {0,1}n to x     rn. we will refer to this as the real conductance;
this notation, as we will see shortly, will go away.

de   nition 5.3. the real conductance of a graph g is

hr(g) def= min
x   rn

eij     [(xi     xj)2]
e(i,j)         [(xi     xj)2] .

since we are relaxing from a 0/1 embedding of the vertices to a real
embedding, it immediately follows from the de   nition that hr(g) is at

34 graph partitioning i the normalized laplacian

most h(g). the optimal solution of the optimization problem above
provides a real embedding of the graph. two questions need to be
addressed: is hr(g) e   ciently computable? how small can hr(g) get
when compared to h(g); in other words, how good an approximation
is hr(g) to h(g)? in the remainder of the section we address the    rst
problem. we show that computing hr(g) reduces to computing an
eigenvalue of a matrix, in fact closely related to the graph laplacian.
in the next section we lower bound hr(g) by a function in h(g) and
present an algorithm that    nds a reasonable cut using hr.

eij     [(xi   xj)2]

5.3 the normalized laplacian and its second eigenvalue
recall that hr(g) def= minx   rn
e(i,j)         [(xi   xj)2] . note that the r.h.s.
above remains unchanged if we add or subtract the same quantity
to every xi. one can thereby subtract ei     [xi] from every xi, and
assume that we optimize with an additional constraint on x, namely,
ei     [xi] = 0. this condition can be written as (cid:2)x, d1(cid:3) = 0. first note
the following simple series of equalities for any x     rn based on simple
properties of expectations.
[(xi     xj)2] =

2 + xj

[xi

e

e

(i,j)         

2     2xixj]
j]     2

e

(i,j)         

=

e

(i,j)         

[xi

2 + x2
j]     2 e
i     
[xi])2.

2 + x2
2]     2( e
i     

(i,j)         
[xj]

[xi] e
j     

= e
i     

[xi

= 2 e
i     

[xi

[xixj]

therefore, by our assumption, e(i,j)         [(xi     xj)2] = 2 ei     [xi2].
hence, we obtain

e

(i,j)         

(cid:2)
[(xi     xj)2] = 2 e
i     

[x2
i ]

=

=

2

i   v dixi2
vol(g)
2x(cid:3)
dx
vol(g) .

5.3 the normalized laplacian and its second eigenvalue

35

moreover, from the de   nition of l it follows that

(cid:22)

(xi     xj)2

e
ij     

(cid:23)

=

=

(cid:2)
e=ij   e(xi     xj)2

x(cid:3)
lx
m

m
2x(cid:3)
lx
vol(g) .

=

therefore, we can write

hr(g) =

min

x   rn, (cid:11)x,d1(cid:12)=0

x(cid:3)
lx
x(cid:3)dx .

this is not quite an eigenvalue problem. we will now reduce it to one.
substitute y def= d1/2x in the equation above. then,

hr(g) =

=

min

y   rn, (cid:11)d   1/2y,d1(cid:12)=0
y(cid:3)

min

y   rn, (cid:11)y,d1/21(cid:12)=0

   1/2y)(cid:3)

   1/2y)
(d
(d   1/2y)(cid:3)d(d   1/2y)

l(d

d

   1/2ld
y(cid:3)y

   1/2y

.

this is an eigenvalue problem for the following matrix which we refer
to as the normalized laplacian.

de   nition 5.4. the normalized laplacian of a graph g is de   ned
to be

l def= d

   1/2ld

   1/2,

where d is the degree matrix and l is the graph laplacian as in
section 2.
note that l is symmetric and, hence, has a set of orthonormal eigenvec-
tors which we denote d1/21 = u1, . . . ,un with eigenvalues 0 =   1(l)    
             n(l). hence,

hr(g) =

min

y   rn,(cid:11)y,d1/21(cid:12)=0

y(cid:3)ly
y(cid:3)y

=   2(l).

we summarize what we have proved in the following theorem.

36 graph partitioning i the normalized laplacian

theorem 5.3.   2(l) = hr(g).

we also obtain the following corollary.
corollary 5.4.   2(l)     2  (g).
thus, we have a handle on the quantity   2(l), which we can compute
e   ciently. can we use this information to recover a cut in g of conduc-
tance close to   2(l)? it turns out that the eigenvector corresponding
to   2(l) can be used to    nd a cut of small conductance, which results
in an approximation algorithm for sparsest cut. this is the content
of the next section.

notes

for a good, though out-dated, survey on the graph partitioning problem
and its applications see [74]. there has been a lot of activity on and
around this problem in the last decade. in fact, an important compo-
nent of the original proof of theorem 3.1 by spielman and teng relied
on being able to partition graphs in near-linear-time, see [79]. the proof
of np-hardness of sparsest cut can be found in [32]. theorem 5.3
and corollary 5.4 are folklore and more on them can be found in the
book [23].

6

graph partitioning ii

a spectral algorithm for conductance

this section shows that the conductance of a graph can be roughly
upper bounded by the square root of the second eigenvalue of the nor-
malized laplacian. the proof that is presented implies an algorithm to
   nd such a cut from the second eigenvector.

6.1 sparse cuts from (cid:1)1 embeddings

recall the (cid:1)2
theorem 5.31

2 problem which arose from the relaxation of h(g) and

  2(l) = min
x   rn

eij     [(xi     xj)2]
e(i,j)         [(xi     xj)2]

    h(g)     2  (g).

we will relate the mathematical program that captures graph conduc-
tance with an (cid:1)1-minimization program (theorem 6.1) and then relate
that (cid:1)1-minimization to the (cid:1)2
2 program (theorem 6.3). in particular,

1 here by (cid:6)2
2 we mean that it is an optimization problem where both the numerator and
the denominator are squared-euclidean distances. this is not to be confused with an (cid:6)2
2
metric space.

37

(cid:1)

38 graph partitioning ii a spectral algorithm for conductance

we will show that

  (g) = min
y   rn

  1/2(y)=0

eij     [|yi     yj|]

ei     [|yi|]

    2

  2(l).

here,   1/2(y) is de   ned to be a t such that   ({i : yi < t})     1/2 and
  ({i : yi > t})     1/2. note that   1/2(  ) is not uniquely de   ned.

theorem 6.1. for any graph g on n vertices, the graph conductance

  (g) =

min

y   rn,   1/2(y)=0

eij     [|yi     yj|]

ei     [|yi|]

.

min

ei     [|yi|]

y   rn,   1/2(y)=0

eij     [|yi     yj|]

proof. let (s,   s) be such that   (s) =   (g). without loss of general-
ity (w.l.o.g.) assume that   (s)       (   s). then eij     [|1s(i)     1s(j)|] =
  (e(s,   s)) and ei     [|1s(i)|] =   (s) = min{  (s),   (   s)}. since {i :
1s(i)     t} is     for t < 0 and   s for t = 0, we have   1/2(1s) = 0. combin-
ing, we obtain

    eij     [|1s(i)     1s(j)|]
it remains to show that eij     [|yi   yj|]
      (g) for every y     rn such that
ei     [|yi|]
  1/2(y) = 0. fix an arbitrary y     r with   1/2(y) = 0. for convenience
we assume that all entries of y are distinct. re-index the vertices of g
such that y1 < y2 <        < yn. this gives an embedding of the vertices
of g on r. the natural cuts on this embedding, called sweep cuts, are
given by si
bound on eij     [|yi   yj|]
ei     [|yi|]
bounded below by   (g). first, note that

def= {1, . . . , i},1     i     n     1. let (cid:24)s be the sweep cut with
minimum conductance. we show that the conductance   ((cid:24)s) is a lower
. this completes the proof since   ((cid:24)s) itself is

ei     [|1s(i)|]

=   (g).

eij     [|yi     yj|] =

=

=

ij   e

1
m

(cid:3)
(cid:3)
n   1(cid:3)

1
m

ij   e

l=1

|yi     yj|
max{i,j}   1(cid:3)

l=min{i,j}

(yl+1     yl)

  (e(sl,   sl))(yl+1     yl).

(6.1)

6.1 sparse cuts from (cid:1)1 embeddings

39

the third equality above follows since, in the double summation, the
term (yl+1     yl) is counted once for every edge that crosses (sl,   sl).
def= v as
for sake of notational convenience we assign s0
two limiting sweep cuts so that {i} = si     si   1 =   si   1       si for every
i     [n]. hence   i =   (si)       (si   1) =   (   si   1)       (   si). we use this to
express ei     [|yi|] below. assume that there is a k such that yk = 0.
thus, since   1/2(y) = 0,   (   sk)       (sk). if no such k exists, the proof is
only simpler.

def=     and sn

n(cid:3)
k(cid:3)

i=1

ei     [|yi|] =

=

  i|yi| =

  i(   yi) +

n(cid:3)

i=k+1

  i(yi)

n(cid:3)

i=k+1

(  (si)       (si   1))(   yi) +

(  (   si   1)       (   si))(yi)

i=1

=   (s0)y1 +

  (si)(yi+1     yi) +   (sk)(   yk) +
n   1(cid:3)

i=k+1

  (   sk)yk+1 +

  (   si)(yi+1     yi) +   (   sn)(   yn)
n   1(cid:3)

k   1(cid:3)
n   1(cid:3)
(since   (s0) =   (   sn) = 0, yk = 0 and   (   sk)       (sk))

  (si)(yi+1     yi) +

  (   si)(yi+1     yi)

i=k

i=1

min{  (si),   (   si)}(yi+1     yi).

=

=

(6.2)

i=1

k(cid:3)

i=1

k   1(cid:3)

i=1

where the last equality follows since   (   si)       (si) for all i     k and
  (   si)       (si) for all i     k. putting equations (6.1) and (6.2) together,
we get the desired inequality:

(cid:2)n   1
(cid:2)n   1
i=1   (e(si,   si))(yi+1     yi)
i=1 min{  (si),   (   si)}(yi+1     yi)

eij     [|yi     yj|]

ei     [|yi|]

=

40 graph partitioning ii a spectral algorithm for conductance

(cid:12)

(cid:25)

  (e(si,   si)

min{  (si),   (   si)}
  (si)

prop. 6.2   

=

=

min
i   [n   1]
min
i   [n   1]

  ((cid:24)s).

in the proof we have used the following simple proposition.

proposition 6.2. for b1, . . . , bn     0,
a1 +        + an
b1 +        + bn

    n
min
i=1

ai
bi

.

i ai          (cid:2)
(cid:2)
proof. let mini ai/bi =   . then, since bi     0, for all i, ai           bi. hence,

(cid:1)
i bi       .

i bi. thus,

i ai/

(cid:1)

6.2 an (cid:1)1 embedding from an (cid:1)2
the next theorem permits us to go from an (cid:1)2
2 solution to an (cid:1)1 solution
which satis   es the conditions of theorem 6.1 with a quadratic loss in
the objective value.

2 embedding

theorem 6.3. if there is an x     rn such that
then there exists a y     rn with   1/2(y) = 0 such that eij     [|yi   yj|]
   
ei     [|yi|]
2

e(i,j)         [(xi   xj)2] =   ,
   

eij     [(xi   xj)2]

  .

before we prove this theorem, we will prove a couple simple propositions
which are used in its proof. for y     r, let sgn(y) = 1 if y     0, and    1
otherwise.

proposition 6.4. for all y     z     r,

(cid:26)(cid:26)sgn(y)    y2     sgn(z)    z2

(cid:26)(cid:26)     (y     z)(|y| + |z|).

6.2 an (cid:1)1 embedding from an (cid:1)2

2 embedding

41

proof.

(1) if sgn(y) = sgn(z), then |sgn(y)    y2     sgn(z)    z2| = |y2    
z2| = (y     z)    |y + z| = (y     z)(|y| + |z|) as y     z.
(2) if sgn(y) (cid:11)= sgn(y), then y     z, (y     z) = |y| + |z|. hence,
|sgn(y)    y2     sgn(z)    z2| = y2 + z2     (|y| + |z|)2 = (y     z)
(|y| + |z|).

proposition 6.5. for a, b     r, (a + b)2     2(a2 + b2).
proof. observe that a2 + b2     2ab     0. hence, 2a2 + 2b2     2ab     a2 +
b2. hence,

2a2 + 2b2     a2 + 2ab + b2 = (a + b)2.

proof. [of theorem 6.3] since the left-hand side (l.h.s.) of the hypothe-
sis is shift invariant for x, we can assume w.l.o.g. that   1/2(x) = 0. let
y = (y1, . . . , yn) be de   ned such that

def= sgn(xi)x2
i .

yi

hence,   1/2(y) = 0. further,
eij     [|yi     yj|]
prop. 6.4   

cauchy-schwarz

   

prop. 6.5   

double counting

=

hence,

eij     [|yi     yj|]

ei     [|yi|]

=

(cid:28)

   

2eij     [(xi     xj)2]

ei     [|yi|]

(cid:27)
(cid:27)
eij     [|xi     xj|(|xi| + |xj|)]
(cid:27)
(cid:27)
eij     [(|xi| + |xj|)2]
eij     [|xi     xj|2]
(cid:27)
(cid:27)
eij     [(xi     xj)2]
(cid:27)
(cid:27)
eij     [(xi     xj)2]
2ei     [x2
i ]
eij     [(xi     xj)2]
2ei     [|yi|].

2eij     [xi2 + xj2]

42 graph partitioning ii a spectral algorithm for conductance

(cid:28)
(cid:28)

=

2eij     [(xi     xj)2]

ei     [x2
i ]

   

4eij     [(xi     xj)2]
e(i,j)         [(xi     xj)2]
(since e(i,j)         [(xi     xj)2]     2ei     [x2
i ])
   
= 2

  .

as a corollary we obtain the following theorem known as cheeger   s
inequality.

theorem 6.6. for any graph g,

  2(l)
2

      (g)     2

(cid:1)

2  2(l).

proof. the    rst inequality was proved in the last section, see corol-
lary 5.4. the second inequality follows from theorems 6.1 and 6.3 by
choosing x to be the vector that minimizes

eij     [(xi   xj)2]

e(i,j)         [(xi   xj)2].

it is an easy exercise to show that cheeger   s inequality is tight up to
constant factors for the cycle on n vertices.

(cid:1)
algorithm 6.1 spectralcut
input: g(v, e), an undirected graph with v = {1, . . . , n}
output: s, a subset of v with conductance   (s)     2
1: d     degree matrix of g
2: l     laplacian of g
3: l     d
4: y(cid:1)     arg miny   rn:(cid:11)y,d1/21(cid:12)=0
5: x(cid:1)     d
1     x(cid:1)
6: re-index v so that x(cid:1)
7: si     {1, . . . , i} for i = 1,2, . . . , n     1
8: return si that has minimum conductance from among 1, . . . , n     1

2                x(cid:1)

   1/2ld

y(cid:3)ly
y(cid:2)y

   1/2y(cid:1)

  (g)

   1/2

n

6.2 an (cid:1)1 embedding from an (cid:1)2

2 embedding

43

the spectral algorithm for sparsest cut is given in algorithm 6.1.
note that x(cid:1) = arg minx   rn
e(i,j)         [(xi   xj)2] is computed by solving
an eigenvector problem on the normalized laplacian l. the proof of
correctness follows from theorems 5.3 and 6.6.

eij     [(xi   xj)2]

notes

theorem 6.6 is attributed to [7, 20]. there are several ways to prove
theorem 6.3 and the proof presented here is in   uenced by the work
of [58].

in terms of approximation algorithms for sparsest cut, the best
   
log n)-factor approximation algo-
result is due to [13] who gave an o(
rithm. building on a sequence of work [10, 12, 48, 61], sherman [71]
log n in essentially
showed how to obtain this approximation ratio of
single-commodity    ow time. now there are improved algorithms for
this    ow problem, see section 12, resulting in an algorithm that runs
log n approximation in   o(m) time, pos-
in time   o(m4/3). obtaining a
sibly bypassing the reduction to single-commodity    ows, remains open.
toward this, getting a log n factor approximation in   o(m) seems like
a challenging problem. finally, it should be noted that madry [55]
gave an algorithm that, for every integer k     1, achieves, roughly, an
o((log n)(k+1/2)) approximation in   o(m + 2k    n1+2   k) time.

   

   

7

graph partitioning iii

balanced cuts

this section builds up on section 6 to introduce the problem of    nding
sparse cuts that are also balanced. here, a balance parameter b     (0, 1/2]
is given and the goal is to    nd the cut of least conductance among cuts
whose smaller side has a fractional volume of at least b. we show how
one can recursively apply algorithm 6.1 to solve this problem.

7.1 the balanced edge-separator problem

in many applications, we not only want to    nd the cut that minimizes
conductance, but would also like the two sides of the cut to be
comparable in volume. formally, given a graph g and a balance
parameter b     (0, 1/2], the goal is to    nd a cut (s,   s) with minimum
conductance such that min{  (s),   (   s)}     b. this problem, referred
to as balanced edge-separator, is also np-hard and we present
an approximation algorithm for it in this section. in fact, we recur-
sively use the algorithm spectralcut presented in the previous
section to give a pseudo-approximation algorithm to this problem:
we present an algorithm (balancedcut, see algorithm 7.1) that
accepts an undirected graph g, a balance requirement b     (0, 1/2], and
a conductance requirement        (0,1) as input. if the graph contains

44

7.1 the balanced edge-separator problem 45

   

that every b-balanced cut in g has conductance at least   .

algorithm 7.1 balancedcut
input: g(v, e), an undirected graph with v = {1, . . . , n}. a balance
parameter b     (0, 1/2]. a target conductance parameter        (0,1).
output: either a b/2-balanced cut (s,   s) with   (s)     4
   or certify
1:   g     degree measure of g
2: h     g
(cid:6)        
3: s
4: repeat
lh     normalized laplacian of h
5:
if   2(lh) > 4   then
6:
return no
7:
8:
9:
10:
11:
12:

end if
s     spectralcut(h)
(cid:6)     arg min{  g(s),   g(v (h) \ s)}     s
(cid:6)
s
(cid:6))}     b/2 then
if min{  g(s
// at least
return (s
one of the cuts will be shown to satisfy the output requirement
of being b/2 balanced and conductance at most 4

(cid:6)),   g(v (g) \ s
(cid:6)
(cid:6)) and (s,   s)
,   s

   

  .

// restrict the problem to the induced subgraph of
else
h on v \ s with every edge that crosses v \ s to s replaced by
a self-loop at the v \ s end

(cid:6)}     {ii : ij     e(v

(cid:6)

, s)} // note that e

(cid:6) is a

13:

14:
15:

(cid:6)     v (h) \ s
v
(cid:6)     {ij : i, j     v
e
multiset
(cid:6)
h = (v

(cid:6))

, e

16:
17:
18: until

end if

a b-balanced cut of conductance at most   , then balancedcut will
   
return a b/2-balanced cut of conductance o(
  ), otherwise it will
certify that every b-balanced cut in g has conductance at least   . this
b/2 can be improved to (1       )b for any    > 0. assuming this, even
if the algorithm outputs a (1       )b balanced cut, the cut with least
conductance with this volume could be signi   cantly smaller than   . in
this sense, the algorithm is a pseudo-approximation.

46 graph partitioning iii balanced cuts

7.2 the algorithm and its analysis

the following is the main theorem of this section.

theorem 7.1. given an undirected graph g, a balance parameter
b     (0, 1/2] and a target conductance        (0,1), balancedcut either
   
  ), or certi   es
   nds a b/2-balanced cut in g of conductance at most o(
that every b-balanced cut in g has conductance more than   .

the algorithm balancedcut, which appears below, starts by check-
ing if the second smallest eigenvalue of the normalized laplacian of g,
l is at most o(  ), the target conductance. if not, then it outputs no.
on the other hand, if   2(l)     o(  ), it makes a call to spectralcut
   
which returns a cut (s0,   s0) of conductance at most o(
  ) by theo-
rem 6.3. assume that   (s0)       (   s0). if (s0,   s0) is already b/2-balanced,
we return (s0,   s0). otherwise, we construct a smaller graph g1 by
removing s0 from g and replacing each edge that had crossed the cut
with a self-loop at its endpoint in   s0. we always remove the smaller
side of the cut output by spectralcut from the current graph. this
ensures that the total number of edges incident on any subset of ver-
tices of g1 is the same as it was in g, i.e., the volume of a subset does
not change in subsequent iterations.

balancedcut recurses on g1 and does so until the relative volume
of the union of all the deleted vertices exceeds b/2 for the    rst time. in
   
this case, we are able to recover a b/2-balanced cut of conductance at
  ). it is obvious that balancedcut either outputs an no or
most o(
(cid:6)). since the graph
stops with an output of pair of cuts (s,   s) and (s
is changing in every iteration, in the description of the algorithm and
the analysis we keep track of the degree measure    and the normalized
laplacian l by subscripting it with the graph involved. the following
two lemmata imply theorem 7.1.

,   s

(cid:6)

lemma 7.2. if balancedcut deletes a set of degree measure less
than b/2 before it returns an no, then every b-balanced cut in g has
conductance at least   .

proof. when balancedcut stops by returning an no, we know that

7.2 the algorithm and its analysis

47

(1)   2(lh) > 4  , and
(2) vol(v (h))     (1     b/2)vol(g).

suppose for sake of contradiction that there is a b-balanced cut in g
of conductance at most   . then, such a cut when restricted to h has
relative volume at least b/2, and the number of edges going out of it
in h is at most those going out in g. thus, since the volume has
shrunk by a factor of at most 2 and the edges have not increased, the
conductance of the cut in h is at most 2 times that in g. hence, such
a cut will have conductance at most 2   in h. but   2(lh) > 4   and
hence, from the easy side of cheeger   s inequality (corollary 5.4), we can
infer that   (h), even with degrees from g, is at least   2(lh)/2 > 2  .
thus, h does not have a cut of conductance at most 2  .

lemma 7.3. when balancedcut terminates by returning a pair of
   
cuts, one of the two cuts it outputs has a relative volume at least b/2
and conductance at most 4

  .

proof. there are two cases at the time of termination: if (s
(s,   s) are returned, either
(1) vol(s(cid:4))/vol(g)     1/2, or
(2) b/2     vol(s(cid:4))/vol(g)     1/2.

(cid:6)

,   s(cid:6)) and

   

in the    rst case, since we stopped the    rst time the relative volume
exceeded b/2 (and in fact went above 1/2), it must be the case that
output (s,   s) of spectralcut in that iteration is b/2-balanced. the
   
   by the guarantee
4   = 4
conductance of this cut in h is at most 2
of spectralcut. hence, the lemma is satis   ed.
(cid:6)) is b/2-balanced. what about
, the smaller side, consists of the union of disjoint
its conductance? s
cuts s0, s1, . . . , st for some t and each is of conductance at most 4
   in
the respective gi by the guarantee of spectralcut. we argue that
the conductance of s

in the second case, we have that (s

(cid:6) is also at most 4

   in g. first note that

,   s

   

   

(cid:6)

(cid:6)

|eg(s

(cid:6)

,   s

(cid:6)

)|     |eg(s0,   s0)| + |eg1(s1,   s1)| +        + |egt(st,   st)|.

48 graph partitioning iii balanced cuts

   

this is because every edge contributing to the l.h.s. also contributes
to the r.h.s. the inequality occurs because there could be more, e.g.,
if there is an edge going between s1 and s2. now, the conductance of
each si in gi is at most 4
      volgt(st).
|eg(s
finally, note that volgi(si) = volg(si) for all i due to the way in which
we split the graph but retain the edges adjacent to the removed edges
as self-loops. hence, volg(s

   
   
      volg1(s1)+        +4
      volg(s0)+4
(cid:2)t

  . hence,

)|     4

,   s

   

(cid:6)

(cid:6)

(cid:6)) =
i=0 volgi(si). thus,
(cid:6))|
|eg(s
,   s
volg(s(cid:6))

    4

   

  .

(cid:6)

this concludes the proof of theorem 7.1.

notes

   

theorem 7.1 is folklore and appears implicitly in [79] and [13]. there
are other nearly-linear time algorithms, where we allow the running
time to depend on 1/   for the balancededge-separator problem
   polylog n bound, see for instance [8, 9, 79].
which achieve a    versus
these algorithms are local in the sense that they instead bound the
total work by showing that each iteration runs in time proportional to
the smaller side of the cut. this approach was pioneered in [79] which
relied on mixing time results in [53]. the    rst near-linear algorithm
for balancededge-separator that removes polylog n factors in the
approximation was obtained by [62]. recently, this dependence of the
running time on    has also removed and an   o(m) time algorithm, which
   bound, has been obtained by orecchia et al. [60].
achieves a    versus
their proof relies crucially on laplacian solvers and highlights of their
algorithm are presented in sections 9 and 19.

   

8

graph partitioning iv

computing the second eigenvector

this    nal section on graph partitioning addresses the problem of how
to compute an approximation to the second eigenvector, using the near-
linear-time laplacian solver, a primitive that was needed in sections 6
and 7.

8.1 the power method

before we show how to compute the second smallest eigenvector of the
normalized laplacian of a graph, we present the power method. this
is a well-known method to compute an approximation to the largest
eigenvector of a matrix a. it start with a random unit vector and
repeatedly applies a to it while normalizing it at every step.
lemma 8.1. given a symmetric matrix a     rn  n, an error parameter
   > 0 and a positive integer k > 1/2  log(9n/4), the following holds with
id203 at least 1/2 over a vector v chosen uniformly at random
from the n-dimensional unit sphere,

(cid:4)ak+1v(cid:4)
(cid:4)akv(cid:4)     (1       )|  n(a)|,

where   n(a) is the eigenvalue with the largest magnitude of a.

49

50 graph partitioning iv computing the second eigenvector

proof. let u1, . . . ,un be the eigenvectors of a corresponding to the
eigenvalues |  1|                |  n|. we can write the random unit vector v in
the basis {ui}i   [n] as
i=1   iui. from a standard calculation involving
gaussian distributions we deduce that, with id203 at least 1/2,
|  n|     2
   

n. using h  older   s inequality,1

3

(cid:2)n
(cid:3)
(cid:6)(cid:3)
(cid:6)(cid:3)

   

i

i

(cid:4)akv(cid:4)2 =

  2
i   2k
i

(cid:7)1/k+1

(cid:6)(cid:3)

i

(cid:7)k/k+1
(cid:7)k/k+1

  2
i

i   2k+2
  2

i

i

i   2k+2
  2
=
= (cid:4)ak+1v(cid:4)2k/k+1.

i

note that (cid:4)ak+1v(cid:4)2/k+1       

2/k+1
n

  2
n. thus, it follows that

(cid:4)akv(cid:4)2     (cid:4)ak+1v(cid:4)2k/k+1    (cid:4)ak+1v(cid:4)2/k+1

2/k+1
  
n

  2
n

(cid:4)ak+1v(cid:4)2
2/k+1
  2
  
n
n

.

=

substituting k + 1     1/2  log(9n/4) in the r.h.s. above gives |  n|1/k+1    
          1       , completing the proof.
e

8.2 the second eigenvector via powering

the following is the main theorem of this section.

theorem 8.2. given an undirected, unweighted graph g on n vertices
and m edges, there is an algorithm that outputs a vector x such that

eij     [(xi     xj)2]
e(i,j)         [(xi     xj)2]

    2  2(l)

and runs in   o(m + n) time. here l is the normalized laplacian of g.

1 for vectors u,v, and p, q     0 such that 1/p + 1/q = 1, |(cid:11)u,v(cid:12)|     (cid:15)u(cid:15)p(cid:15)v(cid:15)q. in this applica-
tion we choose p def= k + 1, q def= k+1/k and ui

def=   2k/k+1

def=   2/k+1

  2k

, vi

.

i

i

i

8.2 the second eigenvector via powering

51

8.2.1 the trivial application

   1/2ad

   1/2ad

let us see what the power method in lemma 8.1 gives us when we
apply it to i     l, where l is the normalized laplacian of a graph
g. recall that l = i     d
   1/2, where d is the degree matrix of
g and a its adjacency matrix. since we know the largest eigenvector
   1/2 explicitly, i.e., d1/21, we can work orthogonal to it
for d
by removing the component along it from the random starting vector.
thus, the power method converges to the second largest eigenvector of
   1/2, which is the second smallest eigenvector of l. hence, if
   1/2ad
d
we apply the power method to estimate   2(l), we need          2(l)/2 in
order to be able to approximate   2(l) up to a factor of 2. this requires
   1/2)kv for k =   (log n/  2(l)), which gives
the computation of (d
a time bound of o(mlog n/  2(l)) since   2(l) can be as small as 1/m and,
hence, k may be large. in the next section we see how to remove this
dependency on 1/  2(l) and obtain an algorithm which runs in time   o(m)
and computes a vector that approximates   2(l) to within a factor of 2.

   1/2ad

8.2.2 invoking the laplacian solver

def=   2(l) is the largest eigenvalue of l+, the
start by observing that   2
pseudo-inverse of l. thus, if we could compute l+u, for a vector u
which is orthogonal to d1/21, it would su   ce to use the power method
with    = 1/2 in order to approximate   2 up to a factor of 2. hence, we
would only require k =   (log n). unfortunately, computing l+ exactly
is an expensive operation. theorem 3.1 from section 3 implies that
there is an algorithm, lsolve, that given v can approximate l+v in
   
  2). roughly, this implies that the total time required to
time o(mlog n/
   
estimate   2 via this method is o(m(log n)2/
  2). this is the limit of meth-
ods which do not use the structure of l. note that l+ = d1/2l+d1/2
hence, it is su   cient to compute l+v when (cid:2)v,1(cid:3) = 0.

recall the procedure lsolve in theorem 3.1 and its linearity in
section 3.4. an immediate corollary of these results is that there is
a randomized procedure which, given a positive integer k, a graph
laplacian l, a vector v     rn, and an    > 0, returns the vector zkv,
where z is the symmetric linear operator implicit in lsolve satisfying

52 graph partitioning iv computing the second eigenvector
(1       /4)z+ (cid:14) l (cid:14) (1 +   /4)z+ with the same image as l. moreover,
this algorithm can be implemented in time o(mk log nlog 1/  ).

coming back to our application, suppose we are given    < 1/5. we
choose k so that the rayleigh quotient of zkv, which is supposed to be
an approximation to (l+)kv, w.r.t. l+ becomes at most (1 +   )  2(l)
for a random vector v. thus, k is o(1/  log n/  ). the important point is
that k does not depend on   2(l) (or in the normalized case,   2(l)).
let u1, . . . ,un be the eigenvectors of z+ corresponding to the eigen-
(cid:2)
values   1                  n = 0. (for this proof it turns out to be more con-
venient to label the eigenvalues in decreasing order.) we express v
in the basis {ui}i as v =
i   iui. we know that with id203 at
   
least 1/2, we have, |  1|     2/3
n. let j be the smallest index such that
(cid:2)n   1
  j     (1 +   /8)  n   1. then,
(cid:2)n   1
(cid:2)j
(cid:2)n   1
(cid:2)j
(cid:2)j

   (2k   1)
i=1   2
i   
i
   2k
i=1   2
i   
i
   (2k   1)
i=1   2
i   
i
i=1   2
i   
   (2k   1)
i=1   2
i   
i
   2k
  2
n   1  
n   1
i=1   2
      n   1

   (2k   1)
i>j   2
i   
i
i>j   2
i   

(zkv)(cid:3)
z+(zkv)
(zkv)(cid:3)(zkv)

(cid:2)n   1
(cid:2)n   1

+ (1 +   /8)  n   1

   2k
i

   2k
i

+   j

   

   

=

+

i (1 +   /8)   (2k   1)
j(cid:3)

  2
n   1

      (2k   1)

  2
i e

16 + (1 +   /8)  n   1

i=1

      n   1    9n
4
      n   1
      n   1(1 +   /4),

  
8

+   n   1(1 +   /8)

where the third last line has used (1 + x)   1     e
   x/2 for x     1 and k    
8/  log 18n/   + 1. let   v be the unit vector zkv/(cid:15)zkv(cid:15). thus,   vl  v     (1 +
  /4)  vz+  v     (1 +   /4)2  n   1(z+)     (1+  /4)2
1     /4   2(l)     (1 +   )  2(l), where
the last inequality uses    < 1/5. this, when combined with lemma 8.1,
completes the proof of theorem 8.2.

8.2 the second eigenvector via powering

53

notes

lemma 8.1 is folklore. theorem 8.2 is from [78]. it is important to
note that there is no known proof of theorem 8.2 without invoking a
laplacian solver; it seems important to understand why.

9

the matrix exponential and id93

this section considers the problem of computing the matrix exponen-
tial, exp(   tl)v, for a graph laplacian l and a vector v. the matrix
exponential is fundamental in several areas of mathematics and sci-
ence, and of particular importance in id93 and optimization.
combining results from approximation theory that provide rational
approximations to the exponential function with the laplacian solver,
an algorithm that approximates exp(   tl)v up to   -error is obtained.
this algorithm runs in   o(mlog tlog 1/  ) time.

9.1 the matrix exponential
suppose a is a symmetric n    n matrix. the matrix exponential of a
is de   ned to be

since a is symmetric, an equivalent way to de   ne exp(a) is to    rst
write the spectral decomposition of a =
i , where   is are

   (cid:3)

i=0

ai
i! .

exp(a) def=

(cid:2)n
i=1   iuiu(cid:3)

54

9.1 the matrix exponential

55

the eigenvalues of a and uis are its orthonormal eigenvectors. then,

n(cid:3)

i=1

exp(a) =

(cid:3)
exp(  i)uiu
i .

thus, exp(a) is a symmetric psd matrix. this matrix plays a funda-
mental role in mathematics and science and, recently, has been used to
design fast algorithms to solve semide   nite programming formulations
for several graph problems. in particular, it has been used in an intricate
algorithm for balanced edge-separator that runs in time   o(m)
and achieves the spectral bound as in theorem 7.1. in such settings,
the primitive that is often required is exp(   l)v, where l is the graph
laplacian of a weighted graph. one way to compute an approximation
to exp(   l)v is to truncate the power series of exp(   l) after t terms
and output

t(cid:3)

i=0

u def=

(   1)iliv

i!

.

if we want (cid:4)u     exp(   l)v(cid:4)       (cid:4)v(cid:4), then one may have to choose
t     (cid:4)l(cid:4) + log 1/  . thus, the time to compute u could be as large as
o(m((cid:4)l(cid:4) + log 1/  )). in matrix terms, this algorithm uses the fact that
approximates exp(   l) up to an
the matrix polynomial
error of    when t     (cid:4)l(cid:4) + log 1/  . this dependence on (cid:4)l(cid:4) is prohibitive
for many applications and in this section we present an algorithm where
the dependence is brought down to log(cid:4)l(cid:4) as in the following theorem.

(cid:2)t

(   1)ili

i=0

i!

theorem 9.1. there is an algorithm that, given the graph laplacian
l of a weighted graph with n vertices and m edges, a vector v, and a
parameter 0 <        1, outputs a vector u such that (cid:4)exp(   l)v     u(cid:4)    
  (cid:4)v(cid:4) in time   o((m + n)log(1 + (cid:4)l(cid:4))polylog 1/  ).

the proof of this theorem essentially reduces computing exp(   l)v to
solving a small number of laplacian systems by employing a powerful
result from approximation theory. before we give the details of this
latter result, we mention a generalization of the laplacian solver to

56 the matrix exponential and id93

symmetric, diagonally dominant (sdd) matrices. a symmetric matrix
a is said to be sdd if for all i,
aii    

|aij|.

(cid:3)

j

notice that a graph laplacian is sdd. moreover, for any    > 0, i +   l
is also sdd. while it is not straightforward, it can be shown that if one
has black-box access to a solver for laplacian systems, such as theorem
3.1, one can solve sdd systems. we omit the proof.

theorem 9.2. given an n    n sdd matrix a with m nonzero entries,
a vector b, and an error parameter    > 0, it is possible to obtain a
vector u such that

(cid:4)u     a+b(cid:4)a       (cid:4)a+b(cid:4)a.

the time required for this computation is   o(mlog nlog(1/(  (cid:15)a+(cid:15)))).
moreover, u = zb where z depends on a and   , and is such that
(cid:4)z     a+(cid:4)       .

9.2 rational approximations to the exponential

the starting point for the proof of theorem 9.2 is the following result
which shows that simple rational functions provide uniform approxima-
tions to exp(   x) over [0,   ) where the error term decays exponentially
with the degree. the proof of this theorem is beyond the scope of this
monograph.
theorem 9.3. there exists constants c     1 and k0 such that, for any
integer k     k0, there exists a polynomial pk(x) of degree k such that,

(cid:26)(cid:26)(cid:26)(cid:26)exp(   x)     pk

(cid:13)

(cid:14)(cid:26)(cid:26)(cid:26)(cid:26)     ck    2

   k.

1

1 + x/k

sup
x   [0,   )

a corollary of this theorem is that for any unit vector v,
   k).

(cid:4)exp(   l)v     pk((i + l/k)+)v(cid:4)     o(k2

(cid:3)
(cid:3)

i

i

(cid:3)
(cid:3)

i

i

(cid:2)
9.2 rational approximations to the exponential
i   iuiu(cid:3)

i where

57

to see this,    rst write the spectral decomposition l =
{u}n
i=1 form an orthonormal basis. then, note that
   1uiu
(cid:3)
i

(i + l/k)+ =

(1 +   i/k)

and, hence,

pk((i + l/k)+) =

pk((1 +   i/k)

(cid:3)
   1)uiu
i .

(cid:2)

here, we have used the spectral decomposition theorem from section 1
on pseudo-inverses which can be readily justi   ed. thus, if v =
i   iui
with

i = 1, then

i   2

(cid:2)

and

pk((i + l/k)+)v =

exp(   l)v =

  ipk((1 +   i/k)

   1)ui,

  i exp(     i)ui.

thus, using orthonormality of {ui}n

i=1 we obtain that

(cid:4)exp(   l)v     pk((i + l/k)+)v(cid:4)2

   

i    (exp(     i)     pk((1 +   i/k)
  2

   1))2.

(cid:3)

i

hence, (cid:4)exp(   l)v     pk((i + l/k)+)v(cid:4) is at most

i    max
  2

i

|exp(     i)     pk((1 +   i/k)

   1)|.

(cid:28)(cid:3)

i

(cid:2)

since

i   2

i = 1, this implies that

(cid:4)exp(   l)v     pk((i + l/k)+)v(cid:4)

    max
x     (l)

|exp(   x)     pk((1 + x/k)

   1)|,

where   (l) def= [  1(l),   n(l)]. thus, by the choice of pk as in theo-
rem 9.3, since all eigenvalues of the laplacian are nonnegative, we have
proved the following lemma.

58 the matrix exponential and id93

lemma 9.4. there exists constants c     1 and k0 such that, for any
integer k     k0, there exists a polynomial pk(x) of degree k such that,
for any graph laplacian l and a vector v,

(cid:4)exp(   l)v     pk((i + l/k)+)v(cid:4)     o(k2

   k)(cid:4)v(cid:4).

(cid:2)k
proof of theorem 9.1
to use pk((i + l/k)+)v as an approximation to exp(   l)v,
let us
i=0 cixi with |ci| = o(kk).
assume that pk is given to us as pk(x) =
this is not obvious and a justi   cation is needed to assume this. we
omit the details. thus, we need to compute

k(cid:3)

i=0

ci((i + l/k)+)iv.

here, we assume that on input v and (i + l/k), the output of theorem
9.2 is zv where (cid:4)z     (i + l/k)+(cid:4)       . since (cid:4)(i + l/k)+(cid:4)     1 and z
is   -close to it, we obtain that (cid:4)z(cid:4)     (1 +   ). thus, using the simple
inequality that for a, b     r,

(cid:6)
j   1(cid:3)

(cid:7)

aj     bj = (a     b)   

(   1)iaj   1   ibi

,

and we obtain that

i=0

(cid:4)zjv     ((i + l/k)+)jv(cid:4)     2  j(1 +   )j(cid:4)v(cid:4).

therefore, by the triangle inequality,

(cid:29)(cid:29)(cid:29)(cid:29)(cid:29) k(cid:3)
ciziv     k(cid:3)
(cid:2)k

i=0

i=0

(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)       k2(1 +   )k max

i

|ci|.

ci((i + l/k)+)iv

since u =
we get that

i=0 ciziv, combining the inequality above with lemma 9.4,

(cid:4)exp(   l)v     u(cid:4)       k2(1 +   )kko(k)(cid:4)v(cid:4) + o(k2

   k)(cid:4)v(cid:4)

9.3 simulating continuous-time id93

59
where we have used that maxi|ci| = ko(k). for a parameter   , choose
k = log 2/   and    =   (1+(cid:15)l(cid:15))

to obtain that

2ko(k)

(cid:4)exp(   l)v     u(cid:4)       (cid:4)v(cid:4).

this completes the proof of theorem 9.1.

9.3 simulating continuous-time id93

in this section we show how one can simulate continuous-time random
walks on an undirected graph with m edges for time t in   o(mlog t) time.
not only are id93 of fundamental importance, the ability to
simulate them in near-linear-time results in near-linear-time algorithms
for the balanced edge-separator problem. from an applications
point of view, the bound on the running time is essentially independent
of t. now we phrase the problem and show how this simulation result is
an immediate corollary of theorem 9.1. for an undirected graph g, let
d denote its degree matrix and a its adjacency matrix. then, the tran-
   1. starting
sition matrix of a one-step random walk in g is w
with a distribution v at time 0, the t-step transition id203 of the
discrete-time random walk on g is given by w tv. the continuous-time
random walk on g for time t, also called the heat-kernel walk, is de   ned
by the following id203 matrix:

def= ad

(cid:30)wt

def= exp(   t(i     w )) = exp(   t)

   (cid:3)

i=0

ti
i! w i.

note that this can be interpreted as a the discrete-time random walk
after a poisson-distributed number of steps with rate t. here, given a

starting vector v, the goal is to compute (cid:30)wtv. before we show how to

do this, note that w is not symmetric. however, it can be symmetrized
   1/2w d1/2 which is
by considering the spectrally equivalent matrix d
i     l where l is the normalized laplacian of g. thus, w = d1/2(i    
l)d

   1/2, and hence, (cid:30)wt = d1/2 exp(   tl)d

   1/2.

now, it is easy to see that theorem 9.1 applies since tl (cid:7) 0 and
(cid:4)tl(cid:4)     o(t). thus, given    > 0, we can use the algorithm in the proof

60 the matrix exponential and id93

of theorem 9.1 to obtain a vector u such that
   1/2v     u(cid:4)       (cid:4)d

the approximation to (cid:30)wtv is then d1/2u. it follows that

(cid:4)exp(   tl)d

   1/2v(cid:4).

(cid:4)(cid:30)wtv     d1/2u(cid:4)       (cid:4)d1/2(cid:4)    (cid:4)d

   1/2(cid:4)(cid:4)v(cid:4).

this proves the following theorem.

theorem 9.5. there is an algorithm that, given an undirected graph
g with m edges, a vector v, a time t     0, and a    > 0, outputs a vector
  u such that

(cid:31)

(cid:4)(cid:30)wtv       u(cid:4)       

   (cid:4)v(cid:4).

dmax
dmin

the time taken by the algorithm is   o(mlog(1 + t)polylog1/  ). here,
dmax is the largest degree of g and dmin the smallest.

notes

for more on the matrix exponential and the role it plays in the design
of fast algorithms for semide   nite programs the reader is referred to
sample applications as in [12, 40, 41, 60, 61, 62]. two thesis on this
topic are [44] and [59].

the reduction from sdd systems to laplacian systems appears
in [38]. theorems 9.1 and 9.5 are implicit in the work of orecchia
et al. [60]. theorem 9.3 on rational approximations was proved in [70].
the fact that the coe   cients can be computed and are bounded is not
present in this paper, but can be proved. while this section shows how
to reduce the computation of exp(   lv) to polylog n computations of
the form l+v, one may ask if the converse is true. recently, sachdeva
and vishnoi [69] answered this question by presenting a simple reduc-
tion in the other direction: the inverse of a positive-de   nite matrix can
be approximated by a weighted-sum of a small number of matrix expo-
nentials. this proves that the problems of exponentiating and inverting
are essentially equivalent up to polylog factors. this reduction makes

9.3 simulating continuous-time id93

61

no use of the fact that l is a laplacian, but only that it is a symmetric
positive-semide   nite matrix.

for more on continuous-time id93, the reader is referred
to the book [51]. it is a challenging open problem to prove an analog
of theorem 9.5 for discrete-time (lazy) id93: find a vector u
s.t. (cid:4)w tv     u(cid:4)       (cid:4)v(cid:4) in   o(mlog(1 + t)log 1/  ) time.

10

graph sparsi   cation i

sparsi   cation via e   ective resistances

in this section a way to spectrally sparsify graphs is introduced that
uses the connection to electrical networks presented in section 4. in the
next section we show how sparse spectral sparsi   ers can be computed
in   o(m) time using laplacians. spectral sparsi   ers introduced in this
section play a crucial role in the proof of theorem 3.1 presented in
section 18.

10.1 graph sparsi   cation

in the cut sparsi   cation problem, one is given an undirected unweighted
graph g = (v, e) and a parameter    > 0, and the goal is to    nd a
(cid:6)) such that for every cut (s,   s) of v, the
weighted graph h = (v, e
weight of the edges that cross this cut in h is within a multiplicative
1       factor of the number of edges in g that cross this cut. the goal
is to keep the number of edges (not counting weights) of h small.
moreover,    nding h quickly is also important for applications. benczur
and karger gave a remarkable algorithm which produces cut sparsi   ers
of size (i.e., number of edges)   o(n/  2) in time   o(m). such a procedure
has found use in many fundamental graph algorithms where it allows
the running time to be brought down from its dependency on m to n

62

10.1 graph sparsi   cation

63

(note that m can be as big as    (n2)). in particular, cut sparsi   ers can
be used in the algorithms presented in sections 6, 7, and 9.

a stronger notion than cut sparsi   cation is that of spectral
sparsi   cation. this will play an important role in constructing
laplacian solvers in section 18. however, in this section and the next,
we show how to construct spectral sparsi   ers in   o(m) time using lapla-
cian solvers.

de   nition 10.1. given an undirected graph g = (v, e) with a param-
(cid:6)) is said to be an   -spectral
eter    > 0, a weighted graph h = (v, e
sparsi   er of g if

1

(1 +   )

    x(cid:3)

lhx
x(cid:3)lgx

    (1 +   ),    x     rn.

where lg and lh denote the graph laplacians for g and h, respec-
tively.

the goal is to minimize the number of edges of h and construct it as
quickly as possible. in particular, we consider whether spectral sparsi-
   ers with   o(n/poly(  )) edges exist and can be constructed in   o(m) time.
before we go on, notice that if h is an   -spectral sparsi   er for g then it
is an   -cut sparsi   er for g as well. to see this, plug the vectors x = 1s,
(the indicator vector for a cut (s,   s)), into the de   nition above. there
are also easy-to-construct examples that show these notions are not
the same. an   -cut sparsi   er of g can be an arbitrarily bad spectral
sparsi   er for g.

we show the existence of   -spectral sparsi   ers of g of size
o(nlog n/  2). in the next section, we show how to construct such a spar-
si   er in time   o(mlog 1/  ).1 formally, we prove the following theorem.

theorem 10.1. there exists a randomized algorithm that, given a
graph g and an approximation parameter    > 0, constructs a spectral
sparisifer h of size o(nlog n/  2) with id203 1     1/n.

1 if g is weighted, then there is also a dependency on the log of the ratio of the largest
to the smallest weight in g in the running time. the running time crucially relies on the
spielman   teng laplacian solver.

64 graph sparsi   cation i sparsi   cation via e   ective resistances

in the next section we re   ne this theorem to prove the following, which
also includes a bound on the running time.

theorem 10.2. there exists a randomized algorithm that, given a
graph g and an approximation parameter    > 0, constructs a spectral
sparisifer h of size o(nlog n/  2) in time   o(mlog 1/  ) with id203
1     1/n.

10.2 spectral sparsi   cation using e   ective resistances

the proof of theorem 10.1 relies on an edge sampling algorithm:
repeatedly sample (with replacement) edges from g according to a
carefully chosen id203 function over the edges and weight each
sampled edge inversely to this id203. the resulting multiset of
weighted edges, normalized by the number of samples, is the output of
the algorithm.2 formally, let pe be the id203 that edge e is cho-
sen by the sampling algorithm. let y be the random variable such that
p[y = e] = pe. let t be a parameter denoting the number of samples
that are taken by the algorithm. let y1, y2, . . . , yt be i.i.d. copies of y .
then the weighted multi-set of edges equals

(cid:14)

(cid:13)

(cid:14)

(cid:13)

(cid:12)(cid:13)

y1,

1

t    py1

,

y2,

, . . . ,

yt ,

(cid:14)(cid:25)

.

1

t    pyt

what is the laplacian of the associated graph h? let b be an inci-
dence matrix for g and let be be the column vector corresponding
b. for notational conve-
to edge e in b
nience, de   ne ue

pe. then, a simple calculation shows that

(cid:3). then, lg =

   
def= be/

e = b

(cid:3)

1

t    py2
(cid:2)
e beb(cid:3)

(cid:3)(cid:3)

i=1

lh

def=

1
t

(cid:3)

byib(cid:3)
pyi

yi

=

1
t

(cid:3)
uyiu
yi.

(cid:3)(cid:3)
(cid:3)

i=1

to analyze the e   ectiveness of this scheme,    rst note that

(cid:3)
y ] =
e[uy u

p[y = e]    ueu
(cid:3)
e =

(cid:3)
e = lg.
beb

e

e

2 this multi-set can be converted to a weighted graph by additively combining weights of
repeated edges.

10.2 spectral sparsi   cation using e   ective resistances

65

therefore,

(cid:3)(cid:3)

i=1

1
t

e[lh] =

e[uyiuyi] = lg.

so far this is abstract because we have not speci   ed the pes. we want
to choose pe so that lh behaves similar to lg w.h.p. this is where
e   ective resistances and the intuition from theorem 4.5 come into play.
we let

pe

def= re

n     1 ,

e l+

where re
to be b(cid:3)

(cid:2)
def= re   (e) is the e   ective resistance of the edge e de   ned
gbe.3 the id172 factor of n     1 is due to the fact
e re = n     1. what is the intuition behind choosing an edge with
id203 proportional to its e   ective resistance? unfortunately, there
does not exist a very satisfactory answer to this question. we give a
rough idea here. by theorem 4.5, re is proportional to the id203
of an edge being in a random spanning tree and choosing a small num-
ber of random spanning trees from g independently seems like a good
strategy to spectrally sparsify g. we move on to analyze this strategy.

proof of theorem 10.1
for a    xed orientation of the edges of g, recall that the matrix    def=
bl+

(cid:3) introduced in section 4 satis   es the following properties:

gb

(1)   2 =   ,
(2) let   e denote the column of    corresponding to edge e. then

(cid:2)
re = (cid:4)  e(cid:4)2,
e re = n     1, and

(3)
(4)    is unitarily equivalent to

(cid:2)n   1
j=1 eje(cid:3)

j , where ej is the j-th
standard basis vector in rm. this is because    is symmetric
and psd, and all its eigenvalues are 0 or 1. thus, if g is
connected, the multiplicity of eigenvalue 1 is n     1 and that
of 0 is m     n + 1.

3 if the graph g is weighted, then we choose pe proportional to wg(e)re.

pe. let m

y and mi

   
def=   e/

def= vy v(cid:3)

66 graph sparsi   cation i sparsi   cation via e   ective resistances
def= vyiv(cid:3)

yi, i = 1,2, . . . , t
de   ne ve
be i.i.d. copies of m. the following theorem is required. it is an analog
of cherno    bound for sums of matrix-valued random variables.
theorem 10.3. let    > 0 be a small enough constant. let m     rd  d
be a random, symmetric psd matrix such that e[m] = id, where id is
def= supm (cid:4)m(cid:4). let t be a non-
the d-dimensional identity matrix. let   
negative integer and let m1, . . . , mt be independent copies of m. then,

(cid:13)

(cid:14)

mi     id

    2d    exp

    t   2
2  

p

.

t

i=1

(cid:2)d(cid:4)
(cid:6)     d, in
j=1 eje(cid:3)
theorem 10.3 also holds when e[m] =
(cid:6) in place of d. a slightly
which case the above bound holds with d
more general result, which is needed for our application, is that the
bound also holds when e[m] is unitarily equivalent to the above matrix.
this follows because the operator norm is unitarily invariant. as an
application of theorem 10.3, we    rst calculate
(cid:3)
e =   ,

e[m] = e[vev

for some d

(cid:3)

(cid:3)
e ] =

  e  

j

t(cid:3)

 (cid:29)(cid:29)(cid:29)(cid:29)(cid:29) 1

!

(cid:29)(cid:29)(cid:29)(cid:29)(cid:29) >   

which by property (4) above is unitarily equivalent to
that property (2) implies that

e

(cid:2)n   1
j=1 eje(cid:3)

j . note

(cid:4)ve(cid:4)2 =

(cid:4)  e(cid:4)2
pe

= re
pe

therefore (cid:4)mi(cid:4)     n     1 for all i. applying theorem 10.3, we obtain

p[(cid:4)(cid:15)         (cid:4) >   ]     2(n     1)    exp

,

(10.1)

= n     1.
(cid:13)

    t   2
2(n     1)

(cid:14)

yi. setting t = o(nlog n/  2) ensures that this fail-
      (1). how do we relate this to lh? observe that

vyiv(cid:3)
(cid:3), it follows that for any e:

ve =

  e   
pe

= bl+

gbe   
pe

= bl+

gue.

where (cid:15)   def= 1

t

(cid:2)

ure id203 is n
since    = bl+

gb

(cid:3)

(cid:15)   =

therefore,
1
t

and

(cid:3)

vyiv

(cid:3)
yi =

1
t

10.3 crude spectral spars   cation

67

bl+

(cid:3)
yil+
guyiu

gb

(cid:3)

= bl+

glh l+

gb

(cid:3)

,

(cid:3)

= bl+

glgl+

gb

(cid:3)

.

now,(cid:29)(cid:29)(cid:29)(cid:15)         

(cid:29)(cid:29)(cid:29) = sup

x(cid:7)=0

gb

   = bl+

(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)x(cid:3)((cid:15)         )x

x(cid:3)x

(cid:26)(cid:26)(cid:26)(cid:26)(cid:26) = sup

x(cid:7)=0

(cid:26)(cid:26)(cid:26)(cid:26)x(cid:3)

bl+

g(lh     lg)l+
gb

x(cid:3)x

(cid:26)(cid:26)(cid:26)(cid:26).

(cid:3)x

because g is connected, if z is a vector such that bz = 0, then z must be
parallel to the all 1s vector. hence, for any z (cid:11)= 0 such that (cid:2)z,1(cid:3) = 0,
bz (cid:11)= 0. therefore, we can substitute x = bz in the above equation.
using the property that lgl+

gz = z for any such z, we obtain

(cid:29)(cid:29)(cid:29)(cid:15)         

(cid:29)(cid:29)(cid:29)     sup

z(cid:7)=0
(cid:11)z,1(cid:12)=0

= sup
z(cid:7)=0
(cid:11)z,1(cid:12)=0

= sup
z(cid:7)=0
(cid:11)z,1(cid:12)=0

(cid:26)(cid:26)(cid:26)(cid:26)

(cid:26)(cid:26)(cid:26)(cid:26)

g(lh     lg)l+
gb
z(cid:3)b(cid:3)bz

(cid:3)

bz

g(lh     lg)l+

glgz

z(cid:3)lgz

b

(cid:3)

bl+

lgl+

(cid:26)(cid:26)(cid:26)(cid:26)z(cid:3)
(cid:26)(cid:26)(cid:26)(cid:26)z(cid:3)
(cid:26)(cid:26)(cid:26)(cid:26)z(cid:3)(lh     lg)z
(cid:26)(cid:26)(cid:26)(cid:26) >   

z(cid:3)lgz

(cid:26)(cid:26)(cid:26)(cid:26).

combining with equation (10.1) yields

          sup

z(cid:7)=0
(cid:11)z,1(cid:12)=0

p

(cid:26)(cid:26)(cid:26)(cid:26)z(cid:3)(lh     lg)z

z(cid:3)lgz

              p[(cid:4)(cid:15)         (cid:4) >   ] = n

      (1)

as desired. this completes the proof of theorem 10.1.

10.3 crude spectral spars   cation

in this section we note that the algorithm and the proof underlying
theorem 10.1 can be modi   ed to obtain a crude spectral sparsi   er.

(cid:2)

68 graph sparsi   cation i sparsi   cation via e   ective resistances

the exact knowledge of re is no longer necessary. instead, the proof
above can be modi   ed to work when we have qe     re for all e. the
number of samples required to ensure that the resulting graph is a
e qe, which
spectral sparsi   er can be easily shown to be about w
replaces n     1. we record this theorem here (for weighted graphs) and
show how to use it in section 18.

(cid:2)

def=

theorem 10.4. consider graph g = (v, e) with edge weights wg,    >
0, and numbers qe such that qe     wg(e)re for all e. if w
e qe, then
the spectral sparsi   er in theorem 10.1 that takes o(w log w log 1/  )
samples from the id203 distribution induced by the qes produces
a sampled graph h that satis   es

def=

g (cid:14) 2h (cid:14) 3g

with id203 1       .

notes

the notion of cut sparsi   cation was introduced in [16]. the state of the
art on cut sparsi   cation can be found in [31].

spectral sparsi   cation was introduced in [80] and plays a crucial
role in the construction of laplacian solvers, and the    rst algorithm
for a spectral sparsi   er was given in this paper. their sparsi   er had
size o(npolylogn) and could be constructed in   o(m) time. theorem
10.1 was proved in [76] where it is showed how to construct   -spectral
sparsi   ers of size o(nlog n/  2). this has been improved to o(n/  2) in [14],
coming close to the so-called ramanujan bound.

theorem 10.4 was observed in [49] and will be used in section 18.
theorem 10.3, which is used in the proof of theorem 10.1 was proved
in [66] using a non-commutative khinchine inequality. subsequently, it
has been proved in an elementary manner using the matrix exponential
(introduced in section 9) independently by several researchers, see [4].

11

graph sparsi   cation ii

computing electrical quantities

in this section the near-linear-time laplacian solver is used to approx-
imately compute e   ective resistances for all edges in   o(m) time. this
was needed in section 10. this section starts by presenting the sim-
pler task of approximately computing currents and voltages when a
unit current is input at a vertex and taken out at another, again using
laplacian solvers. these primitives are also used in section 12.

11.1 computing voltages and currents
given s, t     v with one unit of current    owing into s and one unit
   owing out of t, the goal is to compute the vector l+(es     et); the
vector of voltages at the vertices of the graph. we employ theorem
3.1 to compute this vector in an approximate sense in   o(m) time.
recall that this theorem says there is an algorithm lsolve which
takes a graph laplacian l, a vector y, and an error parameter    > 0,
and returns an x satisfying

(cid:4)x     l+y(cid:4)l       (cid:4)l+y(cid:4)l.

note that for any two vectors (cid:4)v     w(cid:4)        (cid:4)v     w(cid:4). hence, by a choice
def=   /poly(n) in theorem 3.1, we can ensure the approximate vector of
of   

69

70 graph sparsi   cation ii computing electrical quantities

voltages we produce is   -close in every coordinate to the actual voltage
vector. since the dependence of the running time in theorem 3.1 on the
error tolerance is logarithmic, the running time remains   o(mlog 1/  ).
thus, for any small enough constant    > 0, we obtain a vector u of
voltages such that (cid:4)u     l+(es     et)(cid:4)          . further, if u is the vector
of voltages, then the vector of electrical currents, assuming all resis-
tances are 1, is bu, and can be computed in additional o(m + n)
time. note that since (cid:4)u     l+(es     et)(cid:4)          ,

(cid:4)bu     bl+(es     et)(cid:4)        2  .

hence,
(cid:4)b

(cid:3)

bu     b

(cid:3)

bl+(es     et)(cid:4)    = (cid:4)lu     (es     et)(cid:4)        2  n.

we may assume that our current vector satis   es
(cid:4)lu     (es     et)(cid:4)          .

thus, lu may not be a unit s, t-   ow, but very close to it. if one desires,
one can compute in time o(m + n) a vector   v from u such that l  v =
es     et which is indeed a unit s, t-   ow, and (cid:4)u       v(cid:4)          . we leave it
as an exercise for the reader. we summarize the results of this section
in the following theorem which states the result for a weighted graph;
the extension is straightforward.

theorem 11.1. there is an (cid:15)o(m(log r)log 1/  ) time algorithm which

def= wmax/wmin, and s, t     v,    nds
on input    > 0, g = (v, e, w) with r
vectors   v     rv and   f     re such that if v def= l+(es     et) and f def=
w bl+(es     et),

(1) l  v = es     et and   f = w b  v,
(2) (cid:4)v       v(cid:4)          ,
(3) (cid:4)f       f(cid:4)          , and

e/we    (cid:2)

(4) |(cid:2)

e/we|       .

  f 2

f 2

e

e

here w is an m    m diagonal matrix with w (e, e) = we.

11.2 computing e   ective resistances

71

11.2 computing e   ective resistances

for an edge e = ij, let re denote the e   ective resistance of the edge
e which, recall, is de   ned to be (ei     ej)(cid:3)
l+(ei     ej). for our spec-
tral sparsi   cation application we required a way to compute re for all
edges. as noted in the previous section, if we are interested in near-
linear time algorithms, we have to use an approximation   re of re. it
can be shown that any o(1) approximation su   ces for our spectral
sparsi   cation application; it just increases the sample size by a con-
stant factor. we saw in the last section that for a given    > 0 and edge
e, we can compute   re such that

(1       )re       re     (1 +   )re

in time   o(mlog 1/  ) using the lsolve. a naive extension that com-
putes   re for all edges takes   o(m2 log 1/  ) time. can we reduce this to
  o(mlog 1/  )? the answer, as we will see shortly, is yes. the key obser-
vation is that for an edge e = ij,

re = (cid:4)bl+(ei     ej)(cid:4)2.

hence, if we let wi

def= bl+ei, then

re = (cid:4)wi     wj(cid:4)2

for all e. the wis live in rm. is it necessary to compute wis if all we
are interested in the (cid:1)2 distance of m pairs among them? the answer
is no. the johnson   lindenstrauss lemma postulates that computing
the projections of these vectors on a random log m/  2 dimensional space
preserves distances up to a multiplicative factor of 1      . hence, our
approach is to choose a random matrix a which is roughly log m    m
and compute awi = abl+ei for all i. several ways to choose a work
and, in particular, we appeal to the following theorem where entries of
a are   1 up to id172. this random matrix a corresponds to
the random subspace.
theorem 11.2. given    xed vectors w1, . . . ,wn and    > 0, let a    
   
k}k  m be a random matrix with k     clog n/  2 for some con-
{   1/
stant c > 0. then, with id203 at least 1     1/n, for all 1     i, j     n

   
k, 1/

(1       )(cid:4)wi     wj(cid:4)2     (cid:4)awi     awj(cid:4)2     (1 +   )(cid:4)wi     wj(cid:4)2.

72 graph sparsi   cation ii computing electrical quantities

pute the rows of z. let the vectors zi and   zi denote the ith rows of z
(cid:3)). now we can
and   z, respectively (so that zi is the i-th column of z

def= abl+
getting back to our problem of computing re for all e, let z
where a is the random matrix promised in the theorem above. we

compute an approximation (cid:15)z by using lsolve to approximately com-
construct the matrix (cid:15)z in the following three steps:
m = (cid:15)o(m/  2) time since b has 2m entries.

   
k matrix of dimension k    m where
(1) let a be a random   1/
(2) compute y = ab. note that this takes 2m    o(log n/  2) +
i , for 1     i     k, denote the rows of y , and compute
(3) let y(cid:3)

k = o(log n/  2).

  zi

def= lsolve(l,yi,   ) for each i.

  

(cid:4)zu(cid:4)2

we now prove that, for our purposes, it su   ces to call lsolve with
def=
poly(n) . we do not explicitly specify the poly(n), but it can be
  
recovered from the proof below. first, we claim that if (cid:4)zi       zi(cid:4)       
for all i, then for any vector u with (cid:4)u(cid:4) = o(1),

          poly(n).

|(cid:4)zu(cid:4)2     (cid:4)   zu(cid:4)2|
in our application u = ei     ej for some i, j. hence, (cid:4)u(cid:4) = o(1). to see
(cid:2)
(cid:2)
def= (cid:2)  zi,u(cid:3). then, (cid:4)zu(cid:4)2 =
(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:3)
why our claim is true, let ai
i and (cid:4)   zu(cid:4)2 =
i a2
|(cid:4)zu(cid:4)2     (cid:4)   zu(cid:4)2| =
note that(cid:3)
(cid:3)

i . thus,
i     b2
(a2
i )

def= (cid:2)zi,u(cid:3) and bi

(cid:28)(cid:3)

(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)    

(ai     bi)2

(ai + bi)2.

(cid:3)

i b2

i

i

i

(a2

i + b2

i ) = 2(cid:4)zu(cid:4)2 + 2(cid:4)   zu(cid:4)2 = poly(n).

(ai + bi)2     2

i

i

this is because (cid:4)zu(cid:4)2 = (cid:4)abl+(ei     ej)(cid:4)2
for some i, j. from
johnson   lindenstrauss, we know that w.h.p. this is within a constant
factor of (cid:4)bl+(ei     ej)(cid:4)2 = (ei     ej)(cid:3)
l+(ei     ej) = poly(n) as noted
before. we need to estimate (cid:4)   zu(cid:4)2 and show that it is poly(n) as well.
note that

|ai     bi| = |(cid:2)zi       zi,u(cid:3)|     (cid:4)zi       zi(cid:4)(cid:4)u(cid:4) =   (cid:4)u(cid:4) = o(  ).

11.2 computing e   ective resistances

73

hence,

(ai     bi)2     o(  2    k).

(cid:3)

thus, bi     ai + o(  ) and, hence,
a2
i + o(  

i    
b2

(cid:3)

i

   

ai) + o(  2    k)
(cid:3)

i

   

i + o(     
a2

i ) + o(  2    k).
a2

k

i

i

(cid:3)
(cid:3)
(cid:3)

i

i

(cid:3)

therefore,
(cid:4)   zu(cid:4)2 =

i

and thus

i     a2
b2

i +   2    poly(k) = (cid:4)zu(cid:4)2 +   2    poly(k) = poly(n),

(cid:28)(cid:3)

(cid:3)

|(cid:4)zu(cid:4)2     (cid:4)   zu(cid:4)2|    

(ai     bi)2

(ai + bi)2           poly(n).

i

i

since (cid:4)zu(cid:4)2 is the e   ective resistance of some edge, it is at least 1/poly(n)
when the graph is unweighted. hence,
|(cid:4)zu(cid:4)2     (cid:4)   zu(cid:4)2|

          poly(n).

(cid:4)zu(cid:4)2

to summarize, we have proved the following lemma.

lemma 11.3. suppose

(1       )rij     (cid:4)z(ei     ej)(cid:4)2     (1 +   )rij

for every pair i, j     v . if for all i, (cid:4)zi       zi(cid:4)l       (cid:4)zi(cid:4)l where          
then

(1       )rij     (cid:4)(cid:15)z(ei     ej)(cid:4)2     (1 +   )rij

poly(n) ,

for all i, j     v .

thus, the construction of (cid:15)z takes (cid:15)o(mlog(1/  )/  2) = (cid:15)o(m/  2) time. we
can then    nd the approximate resistance (cid:4)(cid:15)z(ei     ej)(cid:4)2     rij for any

i, j     v in o(log n/  2) time. when the edges are weighted, one can
extend this analysis to prove the following theorem.

74 graph sparsi   cation ii computing electrical quantities

theorem 11.4. there is an (cid:15)o(m(log r)/  2) time algorithm which,
o(log n/  2)    n matrix (cid:15)z such that with id203 at least 1     1/n

on input    > 0 and g = (v, e, w) with r

def= wmax/wmin, computes an

(1       )rij     (cid:4)(cid:15)z(ei     ej)(cid:4)2     (1 +   )rij

for every pair of vertices i, j     v .

notes

more details concerning theorem 11.1 can be found in [22]. theo-
rem 11.2 was proved in [3]. theorem 11.4 is from [76].

12

cuts and flows

this section presents an algorithm which reduces the s, t-max-   ow/
min-cut problems in undirected graphs to their corresponding electri-
cal analogs. since electrical    ows/voltages can be computed in   o(m)
time using laplacian solvers as shown in section 11, this section is ded-
   
icated to showing how, using the multiplicative weight update method,
  o(
m)-electrical    ow computations su   ce to approximately compute
the max-   ow and min-cut between s and t.

12.1 maximum flows, minimum cuts
given an undirected graph g = (v, e) with edge capacities c : e (cid:9)   
r   0, the s, t-maxflow problem is to compute a    ow of maximum
value from a source s     v to a sink t     v. to talk about a    ow on g,
it is convenient to direct its edges arbitrarily, captured by an incidence
matrix b, and allow the    ow value on an edge to be positive or negative.
a    ow f = (fe)e   e is then a (combinatorial) s, t-   ow if

bev = 0 for all v     v \{s, t},
bes + f(cid:3)

(1) f(cid:3)
(2) f(cid:3)
(3) |fe|     ce for all e.

bet = 0, and

75

76 cuts and flows

the goal of the s, t-maxflow problem is to compute a    ow which
bes|. it is well known that the
maximizes the    ow out of s, i.e., |f(cid:3)
maximum s, t-   ow is the same as the minimum capacity cut that sep-
arates s and t. the capacity of a cut (s,   s) is the sum of the capacities
of all the edges with one endpoint in s and another in   s. this is the
max-flow   min-cut theorem. the corresponding problem of    nding the
minimum cut separating s and t is called the s, t-mincut problem.

in this section we give approximation algorithms for both the s, t-
maxflow and the s, t-mincut problem. departing from traditional
approaches, the algorithms presented in this section compute combi-
natorial    ows and cuts by combining the information obtained from
electrical    ows and potentials which can be computed quickly to a
good accuracy, see section 11. we prove the following theorems.

theorem 12.1. there is an algorithm such that, given an undi-
rected, capacitated graph g = (v, e, c), two vertices s, t, and an    > 0,
such that the maximum value from s to t is f (cid:1), the algorithm out-
puts a (combinatorial)    ow of value at least (1       )f (cid:1) and runs in
  o(m3/2poly(1/  )) time.

theorem 12.2. there is an algorithm such that, given an undirected,
capacitated graph g = (v, e, c), two vertices s, t, and an    > 0, such
that there is a cut separating s and t of value f (cid:1), the algorithm outputs
an s, t-cut of value at most (1 +   )f (cid:1) and runs in   o(m3/2poly(1/  )) time.

it is important to note that these algorithms work only for undirected
graphs. since we only compute a 1      -approximation to the max-   ow   
min-cut problems, there is a simple trick to ensure that the ratio of the
largest to the smallest capacity in the input graph is at most o(m2/  ).
hence, the running times do not depend on the capacities. proving
this is left as an exercise. finally, the algorithms in these theorems can
be modi   ed to obtain a running time which depends on m4/3 rather
than m3/2; refer to the notes at the end of this section. at this point
the reader may review the connection between graphs and electrical
networks in section 4.

12.2 combinatorial versus electrical flows

77

r

(cid:3)

def= b

(cid:3)
   1bl

12.2 combinatorial versus electrical flows
recall that given resistances re for edges in g captured by an m    m
diagonal matrix r where r(e, e) def= re and 0 otherwise, if f units of
current is injected at s and taken out at t, then the electrical    ow
f (es     et). here, the laplacian
vector f can be written as f def= r
   1b. using theorem 11.1 from section 11,
is de   ned to be l
we know that f can be computed approximately to a precision of   
in   o(mlog 1/  ) time. how good a substitute is f for a combinatorial s,
t-   ow? first note that in the input to the s, t-maxflow problem there
are no resistances. we will have to determine how to set them. the
problem with electrical    ows is that they could violate edge capacities.
there does not seem to be an easy way to    nd resistances such that
the electrical    ow obtained by the electrical network satisfies the edge
capacities and also maximizes the s, t-   ow. in fact, it is not immediately
clear that such resistances exist; we leave showing that they do as an
exercise to the reader. the algorithm in the proof of theorem 12.1
is iterative and starts with a set of resistances. at each iteration it
computes the s, t-electrical    ow and updates the resistances of the edges
based on the congestion due to this    ow. the update rule is governed
by the intuition that the higher the resistance of an edge, the less the
electrical    ow will run through it. the ingenuity is in de   ning an update
rule such that the number of iterations is small and the average of the
electrical    ows computed satisfies the capacity constraints.

energy of flows

(cid:2)

in section 4 we proved (see theorem 4.7) that for graph g = (v, e) with
   1bl+(es     et),
resistances re given by a diagonal matrix r, if f (cid:1) def= r
then f (cid:1) minimizes er(f) def=
e among all unit    ows f from s to t.
the same also holds for    ows of magnitude f. thus, any s, t    ow that
respects capacity and pumps the same    ow as the electrical    ow from s
to t has at least the energy of the corresponding electrical    ow. let g
be such a combinatorial    ow, i.e., |ge|     1. then we have that

e ref 2

(cid:3)

e

(cid:3)

re.

e

er(f (cid:1))    

e    

reg2

(12.1)

78 cuts and flows
(cid:2)
the last inequality follows from the fact that |ge|     ce. thus, if er(f (cid:1)) >
e re, we are certain there is no s, t-max-   ow of value corresponding

to that of f (cid:1). we use this observation crucially in the algorithm.

12.3 s, t-maxflow
in this section we prove theorem 12.1. let f (cid:1) be the value of the s, t-
max-   ow in g. we present an algorithm that    nds a (1       )f (cid:1) s, t-   ow
in time   o(m3/2poly(1/  )). for convenience, we assume that ce = 1,     e.
refer to section 12.3.1 for general ce.
it su   ces to present an algorithm which, given a value f , either
outputs an s, t-   ow of value at least (1     o(  ))f satisfying the capac-
ity constraints or certi   es that f > f (cid:1). the algorithm is iterative
and appeals to the multiplicative weight update (mwu) method. the
mwu technique is very general and here we present it only from the
point of view of our application. at any given time t, the resistances
for the edges are given by a vector rt. the corresponding m    m diag-
   1
t b. the
onal matrix is denoted by rt and the laplacian lt
algorithm elecflow appears below.
at any time t, the algorithm maintains weights wt
e which are positive
numbers. to start with, w0
e are set to one for all e. these weights are
used to compute the resistances rt
e is
wt
e itself. unfortunately, we do not know how to show that using these
resistances, the number of iterations is bounded via the mwu method.
the issue is that one needs to be able to control the width in the mwu
method. in our case the width is the maximum    ow across an edge at
any time. instead, we consider the update where the resistances are set
using the following equation:

e at time t. the natural choice for rt

def= b

(cid:3)

r

(cid:3)

e

e = wt
rt

e +   
3m

wt
e.

thus, the resistance of an edge at time t has a local component (wt
e)
and a global component (roughly    times the average weight). intu-
itively, the global component helps reduce the gap between the local
resistance and the average global resistance, thus, reducing the possi-
bility of a large current    owing one edge compared to the rest of the

12.3 s, t-maxflow 79

algorithm 12.1 elecflow
input: g(v, e), source s, sink t, a target    ow value f and 0 <    < 1
output: either an s, t-   ow f of value at least (1     o(  ))f or fail

m
  

(cid:1)
indicating that f > f (cid:1)
e     1 for all e     e
1: w0
2:        2
3: t       log m
4: for t = 0     t     1 do
    e     e, rt
e     wt
e +   
3m
   1
f t def= r
t bl+
if ert(f t) > (1 +   
3)

(cid:2)
(cid:2)
t f (es     et)
e rt

5:

  2

else

return fail
    e     e, wt+1

6:
7:
8:
9:
10:
end if
11:
12: end for
13: return f def= (1     )

(1+2  )

e     wt

e(1 +   |f t
e|
   )

t   (cid:2)t   1

t=0 f t

   1

e wt
e

e then

(cid:1)

def= o(

graph. this allows us to bound the width to   
m/  ) and the
average congestion w.r.t. wt
es of the    ow to (1 +   ). hence, the num-
   
ber of iterations is   o(
m/  5/2). concretely, the upper bound on the
width allows one to almost reverse the inequality 1 + x     ex, and its
use is demonstrated in lemma 12.4. the key to this algorithm is the
update step and the width calculation. once the resistances are set,
the corresponding electrical    ow f t is computed and the weights then
increase by a multiplicative factor of (1 +   |f t
e|/  ). thus, the higher the
congestion, the higher the increase in resistance. finally, the    ow f t
is computed using the laplacian solver, see theorem 11.1. it is an
approximate    ow, but we ignore this issue in the interest of readability
and leave it as exercise to adapt the proof in the approximate set-
ting. assuming we can do so in   o(m) time, the overall running time is
  o(m3/2/  5/2). we also show in section 12.4 that if the algorithm outputs
fail, we can recover an s, t cut of value at most f in additional linear
time.

80 cuts and flows

12.3.1 general capacities

(cid:2)
the energy is er(f)    (cid:2)

for general ces, the rt
  
3m

e) and |f t

e wt

e

e| is replaced by |f t

e update rule is changed to rt

e +
e|/ce. note that the new bound for
e. it is easy to verify that these changes

(wt

e/c2

ref 2

c2
e

e     1

make the algorithm work for general capacities.

12.3.2 analysis

since we do not know f (cid:1), the maximum    ow from s to t, we    rst need
to show that the number of f s we may need to run is bounded by
poly(log m, 1/  ). this can be done by binary search and is left as an
exercise. next, we need to show the following for any f for which the
algorithm terminates but does not output fail:

   1

(1+2  )

e|     ce = 1.

    the    ow value from s to t is at least (1     o(  ))f ; and
    the output    ow does not violate any capacity constraints,
i.e.,     e,|fe|     (1     )

t   (cid:2)t   1
t=0 |f t
the    rst is easily seen as the    ow output by the algorithm is essentially
(cid:2)
an average of t    ows, each pushing f units of    ow from s to t. recall
that we showed in equation (12.1) that when the algorithm    nds a
   ow f t such that ert(f t) > (1 +   
3)
e, it implies that the s, t-max-
   ow is less than f. hence, when elecflow returns fail, f (cid:1) < f. on
the other hand, when ert(f t)     (1 +   
e we can show that, in f t,
3)
the maximum congestion on an edge is bounded above and average
(cid:2)
congestion is small.
lemma 12.3. if ert(f t)     (1 +   
3)

(cid:2)

e, then

e rt

e rt

e rt

(cid:1)
    maxe|f t
e|     2
    (cid:1)
e|f t
e|
    1 +   .
(cid:1)
e wt
e wt
e

m
   , and

proof. for convenience, we drop the superscript t in this proof. the
hypothesis then implies that

(cid:3)

e

(

)(cid:3)

e    

ref 2

1 +   
3

re.

e

er(f) =

12.3 s, t-maxflow 81

(cid:3)

we,

e

e

we     (1 +   )
(cid:7)

e

we

f 2
e .

(12.2)

(cid:3)
)(cid:3)
(cid:1)
m/   for all e. using the
(cid:7)(cid:6)(cid:3)
(cid:7)
we|fe|, we get

we,

e

hence, using the update rule for re from the algorithm, we get that

(cid:2)
(

e ref 2

e is at most

(cid:6)

)(cid:3)

(cid:7)

(

)2(cid:3)

1 +   
3

e

we +   
3m

we

=

1 +   
3

where the last inequality follows from the fact that    < 1. on the other
hand,

(cid:3)

e

(cid:6)

(cid:3)

e

e

(cid:3)
(cid:27)
(cid:7)2

(cid:3)

e

ref 2

e =

we +   
3m

(

    e,

combining the two, we obtain that
we    

  f 2
e
3m
which implies that |fe|    
cauchy   schwarz inequality on

3(1+  /3)m
   

    2
   
  
we and

1 +   
3

we|fe|

(cid:6)(cid:3)
(cid:2)
(cid:2)
e we|fe|)2
e we

e

(

   

(cid:6)(cid:3)
(cid:3)

e

e

we

e

wef 2
e

,

(cid:3)

e

we.

   

e     (1 +   )

wef 2

which implies that

the last inequality follows from equation (12.2) by taking the    rst

term of summation. hence,(cid:2)
e we|fe|(cid:2)

e we

(cid:3)

(cid:13)

       

1 +        1 +   .

(cid:14)

(cid:3)

e

=

1 +   |f t
e|

  

to track progress, we use the potential function   t
  0 = m and

def=

e wt

e. clearly,

  t+1 =

wt+1

e =

wt
e

e

e

e +   
wt
  

e

e|f t
e|,
wt

(cid:3)

(cid:2)
(cid:3)

82 cuts and flows

where    = 2

(cid:1)
(cid:3)

e

(cid:2)

(

(cid:13)

m/  . thus, using lemma 12.3 we get that   t+1 is at most
e +   (1 +   )
wt
  

e wt
e

(cid:14)

(cid:13)

(cid:13)

=   t

1 +   (1 +   )

  

    m

1 +   (1 +   )

  

(cid:14)t+1

,

by repeating this argument. hence, we obtain the following lemma.

)t   1     me
(cid:14)
(cid:3)

   

e

lemma 12.4.   t   1     m

in the other direction,

1 +   (1+  )

  

  (1+  )t

  

.

wt   1
e =   t   1.

using the inequality 1 +   x     e  (1     )x for all x     [0,1], and 0 <    < 1,
we get that

    e, wt   1

t=0

e =   t   1
(cid:13)

  t   1

t=0

  

1 +   |f t
e|
(cid:14)

1 +   |f t
e|

  

      t   1
t=0 e

(1     )  |f t
e|

  

.

combining the last two inequalities we obtain that for all e,

  t   1     wt   1

e       t   1
t=0 e

(1     )  |f t
e|

  

(cid:1)

(1     )  
  

t |f t
e|

.

= e

now, using lemma 12.4, we get that

which implies that

e

(1     )  
  

(cid:1)

t |f t
e|

t f t
e

    me
(cid:2)
(1       )  
  
(cid:2)
t|f t
e|
t     1

(1       )

  (1+  )t

   = e

  (1+  )t

   +log m

,

      (1 +   )t

  

+ log m.

    (1 +   ) +   log m
t   

,

dividing by t    and multiplying by   , we get

and by setting   log m

t          , we get that for t       log m

  2

,

(cid:2)
(1       )
t|f t
e|
(1 + 2  )t

    1.

12.4 s, t-min cut

83

  log m

therefore,
  2
theorem 12.1.

iterations su   ce. this concludes the proof of

note that the number of iterations of the algorithm depends on
the value of   . an immediate question is: can it be decreased? the
following example shows that the width    =

m is tight.

   

example 12.5. consider a graph composed of k + 1 disjoint paths
between s and t; k paths of length k and one path of length 1. hence,
the total number of edges is m = k2 + 1 and the max-   ow in this graph
   
is k + 1. the electrical    ow of value k + 1 sends k+1
2    ow through the
edge of the length 1 path, which is approximately

m.

12.4 s, t-min cut
in this section we show how to complete the proof of theorem 12.2.
the algorithm is the same as elecflow except now if a valid    ow is
returned for some f, we know that the minimum cut separating s and
t is more than f. on the other hand, if for some value f elecflow
returns fail, we show how to recover an s, t-cut of value (1 + o(  ))f ,
thus completing the proof of theorem 12.2.
note that the vector of potentials used for setting r when
elecflow outputs fail is given by v def= l+f (es     vt). here, l+
corresponds to the resistances re. we again ignore the issue that v
is known only approximately. hence, the cut we recover is in fact less
than f. the vector v gives an embedding of the vertex set v on the real
line. first, note that s and t are the two endpoints of this embedding.

proposition 12.6. given the embedding of v into r given by v as
above, max

vi = vs and min

vi = vt.

i

i

this is an easy exercise and can be proved using the fact that a har-
monic function achieves its maxima and minima at the boundary. if we

84 cuts and flows

pick a value uniformly at random from the interval [vt, vs], then the
|vi   vj|
|vs   vt|. let xe be indicator
id203 that an edge e = ij is cut is
random variable that is 1 if e is cut, and 0 otherwise. the expectation
|vi   vj|
of xe is e[xe] = p[xe = 1] =
|vs   vt|. using linearity of expectation, we
get that

 (cid:3)

!

xe

=

e

e

(cid:2)
e=ij |vi     vj|
|vs     vt|

(cid:3)

e

1

.
   

e[xe] =

and
(vi     vj)2

(cid:2)
e=ij |vi     vj|
|vs     vt|
*++,(cid:3)
|vi   vj|   
(cid:28)(cid:3)
re
(cid:2)
(cid:28)(cid:3)
(cid:2)

(cid:28)(cid:2)

e re
er(f) .

(vi   vj)2

(cid:2)

re = f

e=ij

re

re

e

e

   

|vs     vt|

re.

(cid:1)

f

er(f)

er(f)

now using the fact that er(f) =
f|vs     vt|, we get that the above is at most

e=ij

and also er(f) =

thus, using cauchy   schwarz inequality on

re, we get

e re,    nally we get e[

e xe] < f ; i.e.,
since f is such that er(f) >
the expected value of cut is at most f . therefore, one of the sweep cuts
that separates s and t has to be less than the expected value. given
the vector v, the running time of this algorithm is   o(m). thus, for a
given value of f, either elecflow returns a valid s, t-   ow of value at
least (1     o(  ))f or the algorithm in this section    nds a cut of value
at most f. the running time is dominated by that of elecflow. this
completes the proof of theorem 12.2.

notes

the max-   ow/min-cut theorem dates back to at least [26, 29], and a
proof of it can be found in any text book on algorithms, see [5].

theorems 12.1 and 12.2 are from [22]. in fact, there the power in the
running time is 4/3 as opposed to the 3/2 presented in this section. using
laplacian solvers to speed up interior point methods to solve the linear
program for the max-   ow problem, daitch and spielman [24] obtained

12.4 s, t-min cut

85

the 3/2 result, which preceded the work of christiano et al. [22]. the
techniques in this section have been extended by kelner et al. [46] to
obtain algorithms for multicommodity    ows for a constant number of
commodities.

two challenging problems remain: to decrease the dependence on
the error    from poly 1/   to poly log 1/  ; and to improve the running time
to   o(m) with possibly worse dependence on   .

the multiplicative weight update paradigm has had a remarkable
success in a wide variety of areas. a recent survey of arora et al. [11]
should give the interested reader a good idea of its usefulness.

part iii
tools

13

cholesky decomposition based linear solvers

the remainder of this monograph is concerned with methods to solve a
system of linear equations, ax = b where a is an n    n matrix and b is
a vector in the column space of a. this section reviews an exact method
for solving a linear system of equations based on cholesky decomposi-
tion. subsequently, the cholesky-based method is employed to present
an o(n) time algorithm for solving a linear system of equations lx = b
when l is a laplacian of a tree.

13.1 cholesky decomposition

we are concerned with matrices which are symmetric and psd.
a matrix a is lower (respectively, upper) triangular if aij = 0 whenever
i < j (respectively, i > j). observe that if a is either lower or upper
triangular, then ax = b can be solved by back-substitution with o(m)
arithmetic operations, where m is the number of nonzero entries of a.
equivalently, a+b can be evaluated using o(m) arithmetic operations.
the undergraduate textbook algorithm to solve a system of linear equa-
tions is gaussian elimination which performs row operations on the
matrix a to convert it to a lower or upper triangular matrix. when a

87

88 cholesky decomposition based linear solvers

is symmetric and psd, a more systematic way of solving ax = b is via
its cholesky decomposition, which is a modi   cation of gaussian elimina-
tion. we present an algorithm to compute the cholesky decomposition
of a when a is positive de   nite. there are appropriate generalizations
when a is not invertible. however, we will not cover these generaliza-
tions since in order to solve a laplacian system of a connected graph
lx = b, the vector b is orthogonal to the all-ones vector. in this setting
l+ (cid:8) 0.
theorem 13.1. if a (cid:8) 0, then one can write a =     (cid:3) where    is a
lower triangular matrix. such a decomposition is called the cholesky
decomposition of a.

before we proceed with the proof of this theorem, we need the following
lemma, often referred to as schur   s lemma.

(cid:13)

(cid:14)

d1 u(cid:3)
1
u1 b1

(cid:8) 0 i   

lemma 13.2. a =

d1 > 0

and b1     u1u(cid:2)

1 /d1 (cid:8) 0.

1

1 /d1)y > 0. this implies that b1     u1u(cid:2)

proof. since a (cid:8) 0, d1 > 0. consider minimizing the quadratic form
z2d1 + 2zu(cid:3)
1 y + y(cid:3)
b1y over the variable z for a    xed y. the solution
can be seen, by di   erentiating, to be z =    u(cid:2)
y/d1. the minimum value
1 /d1)y. hence, if a (cid:8) 0, then for all y, y(cid:3)(b1    
then is y(cid:3)(b1     u1u(cid:2)
1 /d1 (cid:8) 0. the other direction
u1u(cid:2)
is straightforward.
proof. [of theorem 13.1] since a (cid:8) 0, it su   ces to express a =        (cid:3)
for a diagonal matrix    . positive de   niteness implies that    ii > 0
and thus
theorem is obtained via
a = (     1/2)(     1/2)(cid:3). for a symmetric, positive-de   nite matrix a, we
write it as

the decomposition in the

(cid:13)

(cid:14)

(cid:13)

d1 u(cid:3)
1
u1 b1

=

1

u1/d1

0(cid:3)
in   1

0(cid:3)

d1
0 b1     u1u(cid:2)

1 /d1

u(cid:2)
1
1 /d1
0 in   1

(cid:14)(cid:13)

(cid:14)(cid:13)

(cid:14)

.

13.2 fast solvers for tree systems

89

let the matrix in the middle of the r.h.s. be called a1. note that the    rst
matrix is a lower triangular matrix   1 and the third is   (cid:3)
1 . lemma 13.2
def= b1     u1u(cid:2)
1 /d1 (cid:8) 0 which gives a1 (cid:8) 0. recursively, we
implies that b
can write b =   (cid:6)   (cid:6)  (cid:6)(cid:3) as the product of (n     1)    (n     1) matrices,
(cid:14)(cid:13)
(cid:13)
and

(cid:14)(cid:13)

(cid:14)

a1 =

1 0(cid:3)
0   (cid:6)
thus, a =   1  (cid:6)(cid:6)     (cid:6)(cid:6)(cid:3)  (cid:3)
is lower triangular, the theorem follows.

d1 0(cid:3)
0    (cid:6)

0(cid:3)
1
0   (cid:6)(cid:3)

(cid:6)(cid:6)

def=   

(cid:6)(cid:6)(cid:3)

.

     

1 . since product of lower triangular matrices

given the cholesky decomposition, one can solve ax = b easily. indeed,
one    rst evaluates b(cid:6) =   +b and then evaluates x = (  (cid:3))+b(cid:6). note
that the number of such operations is of the order of nonzero entries
in   . in the method to compute the cholesky decomposition of a
in the proof above, the choice of the    rst row was arbitrary. in fact,
any ordering of the rows (and the same ordering of the columns) of
a can be used to recover a decomposition which results in a faster
algorithm to solve ax = b. formally, the cholesky decomposition of a
(cid:3)
aq, (i.e., a with its rows and columns permuted by the same
and q
permutation matrix q,) can be very di   erent.1 therefore, the number
of nonzero entries in the cholesky decomposition of a matrix, called the
   ll in, depends on the permutation of rows and columns. finding the
permutation which leads to the minimum    ll in is known to be np-hard.

13.2 fast solvers for tree systems

now we demonstrate how to use the combinatorial structure of the
linear system to get a good cholesky decomposition. this results in a
fast solver for lt x = b when t is a tree. any symmetric matrix a can
be associated with an n vertex graph (with self-loops) where we have
an edge ij of weight a(i, j) between vertices i and j. the number of
edges in the graph is precisely the number of nonzero entries in a. let
us now view the cholesky decomposition as described in the proof of

1 an n    n matrix q is said to be a permutation matrix if all its entries are either 0 or 1
and there is exactly one 1 in each row and in each column.

90 cholesky decomposition based linear solvers

theorem 13.1 as a process which modi   es the graph: after the row i
has been processed, the resulting graph (which corresponds to a1) has
the following modi   cations:

(1) all edges ij with j (cid:11)= i are deleted; this corresponds to setting

a1(i, j) = 0; and

(2) for every pair jk (j could be equal to k) neighboring to i a
(potentially new) edge is modi   ed; this corresponds to setting
a1(j, k) = b1(j, k)     a(i,j)a(i,k)

.

a(i,i)

now suppose the graph corresponding to the linear system as described
above is a tree and potentially self-loops. then in every iteration of the
cholesky decomposition, we can choose a leaf node i. since there is a
single node adjacent to i, the graph associated with the matrix a1 is
a tree as well (potentially with an extra self-loop). in particular, this
implies that we can write a as        (cid:3) where    =   1  2        n and each
  i is a lower triangular matrix with at most one nonzero o   -diagonal
entry. this gives a cholesky decomposition with at most o(n) nonzero
entries. moreover, in the computation of the cholesky decomposition,
in each iteration, at most o(1) operations are done. therefore, the
decomposition can be computed in o(n) time. thus, we have proved
the following theorem.

theorem 13.3. given a symmetric, psd matrix a and a vector b
such that the graph of a corresponds to a tree, one can    nd in o(n)
time a permutation matrix q such that the cholesky decomposition of
(cid:3)
q

aq has at most o(n) nonzero entries.

this immediately implies the following corollary which will be used in
section 17.

corollary 13.4. if lt is the laplacian of a tree t and b is a vector
such that (cid:2)b,1(cid:3) = 0, then the solution of lt x = b can found in o(n)
time.

proof. note that the graph associated with the laplacian of a tree is
the tree itself. therefore, using theorem 13.3, we can    nd the cholesky

13.2 fast solvers for tree systems

91
decomposition of the permuted laplacian to get     (cid:3)y = b(cid:6) in o(n)
(cid:3)b. note that    is not full rank
time. here, y def= q
since lt is not, however, (cid:2)b,1(cid:3) = 0 implies that b (and thus b(cid:6)) is in
the column space of lt q. therefore, we are guaranteed a solution x
and this can be calculated in the number of nonzero entries of lt ,
which is also o(n).

(cid:3)x and b(cid:6) def= q

notes

more on cholesky decomposition of factorization can be found in the
book [35]. george [33] pioneered the use of    nding the right ordering
in the cholesky decomposition for special graphs, e.g., square grids.
he showed that any psd matrix whose graph is an n    n grid can
be solved in o(n1.5) time. this is the nested dissection algorithm.
   
lipton et al. [52] generalized this result to planar graphs by showing
n)-sized separators in planar graphs. this gives
the existence of o(
an algorithm that runs in   o(n1.5) time and, in general, gives an algo-
rithm that runs roughly in time n1+   for a family of graphs which have
separators of size n   and are closed under edge removal.

14

iterative linear solvers i
the kaczmarz method

an old and simple iterative method to solve ax = b, due to kaczmarz,
starts with an arbitrary point,    nds an equation that is not satis   ed,
forces the current solution to satisfy it, and repeats. in this section, the
convergence of a randomized variant of the kaczmarz is established.
subsequently, it is shown how this method has been recently employed
to develop a new and simpler algorithm to solve laplacian systems in
approximately   o(m) time.

14.1 a randomized kaczmarz method
let ax = b be a system of equations where a is an m    n matrix and
x(cid:1) is one of its solutions. it is convenient to think of the solution x(cid:1) as
lying on all the hyperplanes

(cid:2)ai,x(cid:1)(cid:3) = bi,

where ai is the i-th row of a and bi is the corresponding entry of b.
thus, starting with an arbitrary initial point x0, consider the following

92

14.1 a randomized kaczmarz method

93

simple iterative algorithm to compute an approximation to x(cid:1): given xt
such that axt (cid:11)= b, choose any hyperplane (cid:2)ai,x(cid:3) = bi which does not
contain xt and let xt+1 be the orthogonal projection of xt onto this
hyperplane. it can be shown that this method will converge to x(cid:1) as
long as every hyperplane is considered an in   nite number of times. for
instance, one could cycle over the hyperplanes in a    xed order. however,
the convergence to x(cid:1) can be very slow and bounds on the rate of con-
vergence for general a exist for any deterministic hyperplane selection
rule. this motivates the use of randomization and we consider a simple
randomized rule called the randomized kaczmarz method: set x0 = 0.
for t     0, given xt such that axt (cid:11)= b, choose a hyperplane (cid:2)ai,x(cid:3) = bi
with id203 proportional to (cid:4)ai(cid:4)2 and let xt+1 be the orthogonal
projection of xt onto this hyperplane. the rate of convergence of this
algorithm turns out to be related to a notion of an average condition
number de   ned as follows.

de   nition 14.1. given an m    n matrix a, let (cid:4)a+(cid:4) be de   ned as
follows

(cid:4)a+(cid:4)2 def=

sup

z   rn, az(cid:7)=0

(cid:4)z(cid:4)2
(cid:4)az(cid:4)2 .

recall that (cid:4)a(cid:4)2
f is the squared-frobenius norm of a which is de   ned
to be the sum of squares of all the entries of a. the average condition
number is de   ned to be

    (a) def= (cid:4)a(cid:4)f(cid:4)a+(cid:4).

theorem 14.1. the randomized kaczmarz method presented above,
starting with x0 = 0, satis   es

(cid:4)x       x(cid:1)(cid:4)2       2(cid:4)x(cid:1)(cid:4)2

for    = o(    2(a)log 1/  ) with id203 at least 0.9.

94

iterative linear solvers i the kaczmarz method

14.2 convergence in terms of average condition number

the proof of theorem 14.1 follows from the following lemma.

lemma 14.2. fix the choice of the hyperplanes up to t and
let (cid:2)ai,x(cid:3) = bi be the random hyperplane selected with probabil-
ity (cid:15)ai(cid:15)2/(cid:15)a(cid:15)2
f . if xt+1 is the orthogonal projection of xt on to this
hyperplane,

e(cid:4)xt+1     x(cid:1)(cid:4)2     (1     1/    2(a))(cid:4)xt     x(cid:1)(cid:4)2.

as a    rst step in the proof of this lemma, let us calculate xt+1 in
terms of the hyperplane and xt. the unit normal of the hyperplane
(cid:2)ai,x(cid:3) = bi is   ai
def= ai(cid:15)ai(cid:15) . since xt+1 is an orthogonal projection of xt
onto this hyperplane, there is some        0 such that

xt     xt+1 =         ai.

since xt+1 lies on the hyperplane, we know that
(cid:2)ai,xt+1(cid:3) = bi = (cid:2)ai,x(cid:1)(cid:3).

thus,    =

, implying that

(cid:11)ai,xt   x(cid:4)(cid:12)

(cid:15)ai(cid:15)

xt+1 = xt     (cid:2)ai,xt     x(cid:1)(cid:3)

(14.1)
since xt+1 and x(cid:1) lie in the hyperplane, (cid:2)ai,xt+1     x(cid:1)(cid:3) = 0, which
implies that xt+1     x(cid:1) is orthogonal to xt+1     xt. thus, by pythagoras
theorem,

(cid:4)ai(cid:4)2

ai.

(cid:4)xt+1     x(cid:1)(cid:4)2 = (cid:4)xt     x(cid:1)(cid:4)2     (cid:4)xt+1     xt(cid:4)2.

since the hyperplane (cid:2)ai,x(cid:3) = bi was chosen with id203
combining equations (14.1) and (14.2), we obtain that

e(cid:4)xt+1     x(cid:1)(cid:4)2 = (cid:4)xt     x(cid:1)(cid:4)2    
(cid:6)
1     1
(cid:4)a(cid:4)2

= (cid:4)xt     x(cid:1)(cid:4)2

i

(cid:4)ai(cid:4)2
(cid:4)a(cid:4)2

f

|(cid:2)ai,xt     x(cid:1)(cid:3)|2
(cid:3)

(cid:4)ai(cid:4)4
|(cid:2)ai,xt     x(cid:1)(cid:3)|2
(cid:4)xt     x(cid:1)(cid:4)2

(cid:4)ai(cid:4)2
(cid:7)

f

i

(cid:3)

(14.2)
(cid:15)ai(cid:15)2
(cid:15)a(cid:15)2

,

f

14.2 convergence in terms of average condition number

95

(cid:13)

(cid:14)

= (cid:4)xt     x(cid:1)(cid:4)2

(cid:13)
1     1
(cid:4)a(cid:4)2
1    

f

(cid:4)a(xt     x(cid:1))(cid:4)2
(cid:14)
(cid:4)xt     x(cid:1)(cid:4)2
(cid:14)
(cid:4)a(cid:4)2

1
f(cid:4)a+(cid:4)2

(cid:13)
defn.14.1    (cid:4)xt     x(cid:1)(cid:4)2

= (cid:4)xt     x(cid:1)(cid:4)2

1     1

.

    2(a)

this completes the proof of the lemma 14.2. by repeated application
of this lemma we obtain that for t     0,

e(cid:4)xt+1     x(cid:1)(cid:4)2     (1     1/    2(a))t(cid:4)x0     x(cid:1)(cid:4)2 = (1     1/    2(a))t(cid:4)x(cid:1)(cid:4)2.
thus, from markov   s inequality, with id203 at least 1     1/10,

(cid:4)xt+1     x(cid:1)(cid:4)2     10(1     1/    2(a))t(cid:4)x(cid:1)(cid:4)2.
hence, by selecting    = o(    2(a)log 1/  ), we obtain that

(14.3)

(cid:4)x       x(cid:1)(cid:4)2       2(cid:4)x(cid:1)(cid:4)2

with id203 at least 0.9, completing the proof of theorem 14.1.

the kaczmarz method restricted to a subspace. note that the
randomized kaczmarz method and theorem 14.1 above can be easily
extended to the case when the optimal solution x(cid:1) is known to satisfy
(cid:3) = 0. this will be
ux(cid:1) = v for some u and v, and a is such that u a
useful in the next section. in this setting, the randomized kaczmarz
method maintains that uxt = v for all t. thus, we    rst need an initial
choice x0 s.t. ux0 = v. as a consequence, x0 may no longer be 0 and,
hence, in equation (14.3) we can no longer replace (cid:4)x0     x(cid:1)(cid:4) by (cid:4)x(cid:1)(cid:4).
further, ux0     uxt = 0 for all t. thus, a careful look at the proof of
theorem 14.1 suggests that the following de   nition of (cid:4)a+(cid:4) su   ces.

(cid:4)a+(cid:4)2 def=

sup

z   rn, az(cid:7)=0, uz=0

(cid:4)z(cid:4)2
(cid:4)az(cid:4)2 .

this can be smaller, giving rise to the possibility that     (a) in this
setting is smaller, which in turn could mean a reduced number of
iterations. without any additional e   ort, an analysis identical to that
of theorem 14.1 shows that after            2(a)log 1/   iterations, w.h.p.
(cid:4)x       x(cid:1)(cid:4)2       2(cid:4)x0     x(cid:1)(cid:4)2.

iterative linear solvers i the kaczmarz method

96
14.3 toward an   o(m)-time laplacian solver
in this section we illustrate how the randomized karcmarz method can
be applied to a laplacian system. we focus on presenting the algorithm
and derive a bound on the number of iterations. this approach has
recently led to a new algorithm and a simpler proof of the main theorem
(theorem 3.1) in this monograph.

for this section let us focus on the case of computing the    ow
through each edge when one unit of current is pumped from s to t
and the edges have unit resistances. such a computational primitive
was required in section 12. it is an easy exercise to extend this argu-
ment to solve lx = b when b is not necessarily es     et. to set things
up, let g = (v, e) be an undirected, unweighted graph on n vertices
and m edges, and let b     {   1,0,1}n  m be an edge-vertex incidence
matrix corresponding to a    xed orientation of the edges. recall from
l+(es     et). given i, it is easy
section 4 the unit s, t    ow vector i = b
to compute l+(es     et) in o(m) time. thus, let us focus on computing
an approximation to i.
for an undirected cycle c in g, let 1c     {   1,0,1}m be a vector
supported exactly on the edges of c that satis   es b1c = 0. since the
sum of voltages across any cycle sum up to zero if all resistances are
one, (cid:2)1c,i(cid:3) = 0 for all cycles c in g. fix a spanning tree t in g and,
for every e (cid:11)    t, let ce be the unique cycle formed by adding e to t.
the indicator vectors for these m     n + 1 cycles form a basis of the
space of cycle vectors and can be used to generate the indicator vector
of any cycle in g. hence, to    nd i it is su   cient to solve the following
system of equations

(cid:3)

    e (cid:11)    t (cid:2)1ce,f(cid:3) = 0

s.t. bf = es     et.

let a be an m     n + 1    m matrix whose rows are indexed by edges
e (cid:11)    t and are equal to 1ce. then the problem reduces to solving af = 0
subject to bf = es     et. we will apply the randomized karcmarz
method to the system af = 0 while working in the space bf = es     et:
initially choose f0 such that bf0 = es     et. one such choice is to put a
unit    ow from s to t on the unique path from s to t in t. at time t,
given ft, compute ft+1 by orthogonally projecting ft onto the hyperplane

14.3 toward an   o(m)-time laplacian solver

97
(cid:2)1c,f(cid:3) = 0, where c is chosen from one of the m     n + 1 cycles with
id203 proportional to (cid:4)1c(cid:4)2 = |c|. the number of iterations nec-
essary to get within    of the optimal solution, from theorem 14.1, is
o(    2(a)log 1/  ).
(cid:3)
f(cid:4)a+(cid:4)2. fixing some t, (cid:4)a(cid:4)2
    2(a) = (cid:4)a(cid:4)2
(cid:4)1ce(cid:4)2 =

in the remainder of the section we calculate     2(a). recall that

|c| = m     n + 1 +

(cid:3)

(cid:3)

f equals

strt (e),

e(cid:7)   t

e(cid:7)   t

e(cid:7)   t

where strt (e) is the stretch of e in t de   ned as the length of the unique
path in t between the endpoints of e.1 on the other hand,

(cid:4)a+(cid:4)2 =

sup

f , af(cid:7)=0, bf=0
we will now show that this is at most 1.

(cid:4)f(cid:4)2
(cid:4)af(cid:4)2 .

let ft denote the entries of f corresponding to the tree edges and
fn denote those corresponding to the non-tree edges. similarly, we can
split the columns of a into those corresponding to tree edges and non-
tree edges. it follows that one can write a = [at in ], where in is a
diagonal matrix on the non-tree edges, if the direction of each cycle is
chosen to coincide with the edge that was added to form it from t.
suppose f is supported only on one of the cycles determined by t, then
(cid:3)
t fn = ft . thus, by linearity, the following holds for any vector f such
a
that bf = 0,

(cid:3)
t fn = ft .
note that (cid:4)af(cid:4)2 = (cid:4)at ft(cid:4)2 + 2f(cid:3)
t a

t fn + (cid:4)fn(cid:4)2. thus, by (14.4),
(cid:3)
(cid:4)af(cid:4)2 = (cid:4)at ft(cid:4)2 + (cid:4)ft(cid:4)2 + (cid:4)fn(cid:4)2     (cid:4)ft(cid:4)2 + (cid:4)fn(cid:4)2 = (cid:4)f(cid:4)2.

a

(14.4)

this proves the claim that     2(a)     m     n + 1 +
(cid:2)
e(cid:7)   t strt (e). we will
see in theorem 17.3 (without proof) that one can construct, in   o(m)
e(cid:7)   t strt (e) =   o(m). since, (cid:4)f0(cid:4)2     n as the initial
time, trees with
   ow is just on a path from s to t, applying theorem 14.1 in the sub-
space restricted setting of the previous section, the number of iterations

(cid:2)

1 this notion of stretch will play a central role in the proof of theorem 3.1 presented in this
monograph and more on this appears in section 17.

98

iterative linear solvers i the kaczmarz method

required by the randomized kaczmarz method to get within    can be
seen to be bounded by   o(mlog 1/  )

in order to complete the proof of theorem 3.1 the key issues that
remain are, given t, how to choose an edge not in t with id203
proportional to the length of the cycle it forms with t and how to
update ft+1 from ft. the former is a simple exercise. for the latter,
note that the updates involve simply adding or removing    ow along
cycles, which can be implemented in logarithmic time using a data
structure called link-cut trees. the details of this data structure and
how it is employed are beyond the scope of this monograph.

notes

the kaczmarz method appears in [43]. the randomized kaczmarz
procedure and its analysis appear in [83]. this analysis is tied to the
alternating projections framework [19], which also has many other algo-
rithmic applications. section 14.3 is based on a recent result due to [47]
where details about the link-cut tree data structures can be found. it
is also shown in [47] that the inverter derived from the randomized
kaczmarz method is linear in the sense of section 3.4.

15

iterative linear solvers ii

the gradient method

this section phrases the problem of solving a system of linear equations
as an optimization problem and uses the technique of id119
to develop an iterative linear solver. roughly, iterative methods to solve
a system ax = b maintain a guess xt for the solution at an iteration t,
and update to a new guess xt+1 through feedback about how good their
guess was. the update typically uses a simple vector   matrix product
and is often very fast and popular in practice due to its small space
complexity.

15.1 optimization view of equation solving

in this section we assume that a is symmetric and positive de   nite
and, as usual, we let the eigenvalues of a be 0 <   1                  n. to start,
observe that solving ax = b is equivalent to    nding the minimum of
f(x) where f is de   ned to be

f(x) def=

(cid:3)
x

ax     bx.

1
2

observe that when a is positive de   nite,    2f = a (cid:8) 0, so f is strictly
convex and, thus, has a unique minimum x(cid:1). this x(cid:1) must satisfy
   f(x(cid:1)) = ax(cid:1)     b = 0.

99

100

iterative linear solvers ii the gradient method

since f is a convex function, we can use the id119
approach popular in id76 to solve for x(cid:1). a typical
id119 algorithm starts with an initial vector x0, and at iter-
ation t moves to a new point xt+1 in the direction opposite to the
gradient of f at xt. we would like to move in this direction because f
decreases along it. note that    f(xt) = axt     b which means that the
time it takes to compute the gradient requires a single multiplication
of a vector with a matrix a. we use ta to denote the time it takes to
multiply an n    n matrix a with a vector.1 the hope is that in a small
number of iterations, each of which is geared to reducing the function
value, the process will converge to x(cid:1). we prove the following theorem.

theorem 15.1. there is an algorithm gdsolve that, given an n    n
symmetric matrix a (cid:8) 0, a vector b, and    > 0,    nds a vector x such
that

(cid:4)x     a+b(cid:4)a       (cid:4)a+b(cid:4)a

in time o(ta      (a)log 1/  ). here the condition number of a is de   ned
to be   (a) def=   n(a)/  1(a) and, for a vector v, (cid:4)v(cid:4)a

v(cid:3)av.

def=

   

15.2 the id119-based solver
before we describe the algorithm gdsolve formally and prove theo-
rem 15.1, we provide a bit more intuition. first, let us    x some notation.
def= x(cid:1)     xt be a vector which tells us where the cur-
for t     0, let dt
def= adt = b     axt. observe that
rent solution xt is w.r.t. x(cid:1), and let rt
rt =       f(xt). think of rt as a crude approximation to dt. at itera-
tion t, there is a parameter   t which determines how much to move in
the direction of rt. we will discuss the choice of   t shortly, but for now,
the new point is

xt+1

def= xt +   trt.

(15.1)

1 ta is at least n and, even if a has m nonzero entries, can be much less than m, e.g., if
a = vv(cid:2) and is speci   ed in this form.

15.2 the id119-based solver

101

how does one choose   t? one way is to choose it greedily: given xt
(and therefore rt) choose   t which minimizes f(xt+1). towards this
end, consider the function
1
g(  ) def= f(xt +   rt) =
2

a(xt +   rt)     b
(cid:3)

(cid:3)
(xt +   rt)

(xt +   rt).

the minimum is attained when the gradient of g w.r.t.    is 0. this can
be calculated easily:

g

which implies

(cid:6)

t art     r
(cid:3)
(cid:3)
(cid:3)
t axt +   r
(  ) = r
t b,

  t =

t (b     axt)
r(cid:3)
r(cid:3)
t art

r(cid:3)
t rt
r(cid:3)
t art

.

=

gdsolve is described formally in algorithm 15.1.

algorithm 15.1 gdsolve
input: symmetric, positive-de   nite matrix a     rn  n, b     rn and t
output: xt     rn
1: x0     0
2: for t = 0     t     1 do

3:
4:

set rt = b     axt
set   t = r(cid:2)
t rt
r(cid:2)
t art
set xt+1 = xt +   trt

5:
6: end for
7: return xt

thus, to prove theorem 15.1 the only thing one needs to show is that
for t = o(  (a)log 1/  ), (cid:4)xt     a+b(cid:4)a       (cid:4)a+b(cid:4)a. towards this, we
prove lemma 15.2 which shows how the a-norm of the error vector
dt = xt     x(cid:1) drops with t.
lemma 15.2. (cid:4)dt+1(cid:4)2

a     (1     1/  (a))    (cid:4)dt(cid:4)2

a

proof. we start with the following observation: from the update rule
we get dt+1 = dt       trt and rt+1 = rt       tart. this gives, r(cid:3)
t rt+1 =
t rt     r(cid:2)
r(cid:3)
r(cid:3)
t rt
t art = 0. therefore, two consecutive update directions
r(cid:2)
t art

102

iterative linear solvers ii the gradient method

rt and rt+1 are orthonormal. this is expected since from xt we move
in the direction rt as far as we can to minimize f. now,

(cid:4)dt+1(cid:4)2

t+1rt+1 = (dt       trt)
(cid:3)
(cid:3)
(cid:3)
t+1adt+1 = d
a = d

(cid:3)
rt+1 = d
t rt+1.

the last equality used the orthonormality of rt and rt+1. thus,

t a(dt       trt).
(cid:3)
(cid:3)
(cid:3)
t adt+1 = d
t rt+1 = d
a = d

(cid:4)dt+1(cid:4)2
(cid:13)
this is the same as
1       t

(cid:4)dt(cid:4)2
a   

(cid:14)

(cid:13)

d(cid:3)
t art
d(cid:3)
t adt

= (cid:4)dt(cid:4)2
a   

1     r(cid:3)
t rt
r(cid:3)
t art

   d(cid:3)
t a2dt
d(cid:3)
t adt

(cid:14)

.

in the last inequality, we use rt = adt. now, the    rst fraction is at least
1/  n, while the second is at least   1 since
(a1/2dt)(cid:3)
a(a1/2dt)
(a1/2dt)(cid:3)(a1/2dt)

d(cid:3)
t a2dt
d(cid:3)
t adt

x(cid:3)
ax
x(cid:3)x

    min
x(cid:7)=0

here we is where we have used that a (cid:8) 0. this gives us

=   1.

=

(cid:4)dt+1(cid:4)2

a     (1     1/  (a))(cid:4)dt(cid:4)2
a,

which completes the proof of the lemma.
as a corollary we get that in t = 2  (a)log 1/   steps, (cid:4)xt     x(cid:1)(cid:4)a    
  (cid:4)x0     x(cid:1)(cid:4)a; in fact, with a better analysis the factor 2 can be removed.
this completes the proof of theorem 15.1.
to e   ectively use theorem 15.1 one needs a more tractable condi-
tion to check when to terminate. it is easy to check that (cid:4)axt     b(cid:4)       .
for this termination condition to imply the termination condition in
the theorem, one has to choose    small enough; this seems to require an
upper bound on the condition number of a. we leave it as an exercise
to check that for graph laplacians, an easy estimate can be obtained
in terms of the ratio of the largest to the smallest edge weight. in the
next section we will see how to reduce the number of iterations to about
  (a) when a is psd. this will require a deeper look into gdsolve.

(cid:1)

notes
the method gdsolve suggested here is well known and bears simi-
larity with richardson   s iterative method [64], see also [36, 37]. more
on id119 can be found in the book [19].

16

iterative linear solvers iii

the conjugate gradient method

this section presents a sophisticated iterative technique called the
conjugate gradient method which is able to approximately solve a linear
system of equations in time that depends on the square root of the con-
dition number. it introduces the notion of krylov subspace and shows
how chebyshev polynomials play a crucial role in obtaining this square
root saving. finally, the chebyshev iteration method is presented which
is used in section 18 to construct linear laplacian solvers.

16.1 krylov subspace and a-orthonormality

given a symmetric and positive-de   nite matrix a, we start where we
left o    in the previous section. recall that for a vector b, we set up the
optimization problem

minx f(x) def=

1
2

(cid:3)
x

ax     b
(cid:3)

x.

we presented an algorithm gdsolve which, in its t-th iteration, gen-
erates xt where

x2 = x1 +   1r1 = x1 +   1(r0       0ar0) = x0 +   1r0       1  0ar0,

x1 = x0 +   0r0,

103

iterative linear solvers iii the conjugate gradient method

104
and so on. here r0 = b and ri = b     axi. thus,
it follows by
induction that xt lies in x0 + kt where kt is the subspace spanned
by {r0, ar0, . . . , at   1r0} = {b, ab, . . . , at   1b}. kt is called the krylov
subspace of order t generated by a and b.
in gdsolve, although at each iteration t we move greedily along
the direction rt   1, the resulting point xt which lies in x0 + kt is not
guaranteed to be a minimizer of f over this subspace. that is, there
could be a y     x0 + kt such that f(y) < f(xt). the main idea of the
conjugate gradient method can be summarized in one sentence:

at step t, move to the f-minimizer over x0 + kt.

note that the problem of    nding x(cid:1) is precisely that of    nding the
f-minimizer over x0 + kn. in this section we will see how to make this
idea work and will prove the following theorem.
theorem 16.1. there is an algorithm cgsolve that, given an n    n
symmetric matrix a (cid:8) 0, a vector b, and    > 0,    nds a vector x such
that

in time o(ta   (cid:1)

(cid:4)x     a+b(cid:4)a       (cid:4)a+b(cid:4)a

  (a)log 1/  ).

t   1(cid:3)

the    rst question we would like to ask is: how can we    nd the f-
minimizer over x0 + kt quickly? suppose we have access to a basis
{p0,p1, . . . ,pt   1} of kt such that for any scalars   0, . . . ,   t   1, we have
the following decomposition:

(cid:6)

(cid:7)

t   1(cid:3)

f

x0 +

  ipi

    f(x0) =

(f(x0 +   ipi)     f(x0)),

(16.1)

i=0

i=0

it is su   cient to    nd,

i.e., the optimization problem becomes separable in the variables   i.
for all i,   i which is the    that
then,
minimizes f(x0 +   pi)     f(x0): observe that xt
i=1   ipi
minimizes f(y) for all y     x0 + kt. therefore, if we have access to
a basis as described above, we are able to move to a minimizer over
x0 + kt iteratively.

def= x0 +

(cid:2)t   1

16.2 computing the a-orthonormal basis

105

consider equation (16.1). if f were a linear function, then equality
clearly holds. thus, we can just look at the quadratic portion of f to
def=   ipi. evaluating
decide when this equality holds. for brevity, let vi
the l.h.s. of equation (16.1) gives

(cid:6)

1
2

(cid:3)

x +

i

(cid:3)

a

= x

vi

a

vi

x +

(cid:3)
x

    1
2

(cid:6)
(cid:7)

(cid:7)
(cid:7)(cid:3)

(cid:3)
(cid:6)(cid:3)

(cid:7)(cid:3)
(cid:6)(cid:3)
(cid:2)
i(x(cid:3)
i avi). the above equations
i avj are equal to 0. this is precisely the

(cid:6)(cid:3)

avi + v(cid:3)

(cid:7)

ax

vi

vi

vi

+

a

.

i

i

i

i

evaluating the r.h.s. gives 1
2
are equal if the cross terms v(cid:3)
de   nition of a-orthonormal vectors.

de   nition 16.1. given a symmetric matrix a, a set of vectors
p0,p1, . . . ,pt are a-orthonormal i    for all i (cid:11)= j, p(cid:3)

i apj = 0.

now we are armed to give an overview of the conjugate gradient
def= a(x(cid:1)     x0).
algorithm. let x0 be the initial vector, and let r0
let p0,p1, . . . ,pt be a set of a-orthonormal vectors spanning kt =
span{r0, ar0, . . . , at   1r0}. in the next section we will see how to com-
pute this a-orthonormal basis for kt. in fact, we will compute the vec-
tor pt itself in the (t + 1)-st iteration taking o(1) extra matrix   vector
computations; for the time being, suppose they are given.
let   t be scalars that minimize f(x0 +   pt)     f(x0); by a simple
calculation, we get   t = p(cid:2)
t r0
. the conjugate gradient algorithm
p(cid:2)
t apt
updates the vectors as follows

xt+1

def= xt +   tpt

and   t

def=

p(cid:3)
t r0
p(cid:3)
t apt

.

(16.2)

16.2 computing the a-orthonormal basis
we now show how to compute an a-orthonormal basis of {r0, ar0, . . .,
at   1r0}. one could use gram   schmidt orthoid172 which

106

iterative linear solvers iii the conjugate gradient method

iteratively computes p0, . . . ,pt, by setting
v(cid:3)
api
p(cid:3)
i api

def= v    

pt+1

(cid:3)

i   t

pi,

j api = 0 for all i (cid:11)= j     t, this ensures that p(cid:3)

where v is the new vector which one is trying to a-orthonormalize.
given that p(cid:3)
t+1api = 0
for all i     t. note that the above can take up to o(t) matrix   vector
calculations to compute pt+1. we now show that if one is trying to a-
orthonormalize the krylov subspace generated by a, one can get away
by performing only o(1) matrix   vector computations. this relies on
the fact that a is symmetric.
let p0 = r0. suppose we have constructed vectors {p0,p1, . . . ,pt}
which form an a-orthonormal basis for kt+1. inductively assume that
the vectors satisfy

ki+1 = span{r0, ar0, . . . , air0} = span{p0,p1, . . . ,pi}

(16.3)
and that api     ki+2 for all i     t     1. note that this is true when i = 0
as p0 = r0. let us consider the vector apt. if apt     kt+1, kj = kt+1
for all j > t + 1 and we can stop. on the other hand, if apt (cid:11)    kt+1, we
construct pt+1 by a-orthonormalizing it w.r.t. pi for all i     t. thus,

def= apt    

pt+1

(apt)(cid:3)
p(cid:3)
i api

api

pi.

(16.4)

(cid:3)

i   t

this way of picking pt+1 implies, from our assumption in equa-
tion (16.3), that

kt+2 = span{r0, ar0, . . . , at+1r0} = span{p0,p1, . . . ,pt+1}.

it also ensures that apt can be written as a linear combination of pis for
i     t + 1. hence, apt     kt+2, proving our induction hypothesis. thus,
for every i     t, there are constants cjs such that

(cid:3)

j   i+1

(cid:3)
cjp
t apj

(cid:3)
(apt)

(cid:3)
(api) = p
t a

(cid:3)

(cid:3)
(api) = p
t a(api) =

hence,

(cid:3)
(apt)

(api) = 0

16.3 analysis via polynomial minimization

107

for all i < t     1. hence, equation (16.4) simpli   es to
pt     p(cid:3)
t a2pt   1
p(cid:3)
t   1apt   1

pt+1 = apt     p(cid:3)
t a2pt
p(cid:3)
t apt

pt   1.

thus, to compute pt+1 we need only o(1) matrix   vector multiplications
with a, as promised. this completes the description of the conjugate
gradient algorithm which appears formally below.
algorithm 16.1 cgsolve
input: symmetric, positive-de   nite matrix a     rn  n, b     rn and t
output: xt     rn
1: x0     0
2: r0     b
3: p0 = r0
4: for t = 0     t     1 do

5:

6:
7:

set   t = p(cid:2)
t r0
p(cid:2)
t apt
set rt = b     axt
set xt+1 = xt +   tpt
set pt+1 = apt     p(cid:2)
t a2pt
p(cid:2)
t apt

8:
9: end for
10: return xt

pt     p(cid:2)
t a2pt   1
p(cid:2)
apt   1
t   1

pt   1.

since each step of the conjugate gradient algorithm requires o(1)
matrix   vector multiplications, to analyze its running time, it su   ces
to bound the number of iterations. in the next section, we analyze
the convergence time of the conjugate gradient algorithm. in partic-
ular, we call a point xt an   -approximate solution if f(xt)     f(x(cid:1))    
  (f(x0)     f(x(cid:1))). we wish to    nd how many iterations the algorithm
needs to get to an   -approximate solution. we end this section with
the following observation: in n steps, the conjugate gradient method
returns x(cid:1) exactly. this is because x(cid:1)     x0 + kn.

16.3 analysis via polynomial minimization
we now utilize the fact that xt minimizes f over the subspace x0 + kt
to prove an upper bound on f(xt)     f(x(cid:1)). the following is easily seen.

f(xt)     f(x(cid:1)) =

1
2

(xt     x(cid:1))
(cid:3)

a(xt     x(cid:1)) =

(cid:4)xt     x(cid:1)(cid:4)2
a.

1
2

(cid:2)t   1

iterative linear solvers iii the conjugate gradient method

108
since xt lies in x0 + kt, we can write xt = x0 +
i=0   iair0 for some
i=0   ixi. note
scalars   i. let p(x) be the polynomial de   ned to be
that there is a one-to-one correspondence between points in x0 + kt
and degree t     1 polynomials in one variable. then, we get xt = x0 +
p(a)r0 = x0 + p(a)a(x(cid:1)     x0). therefore, we get

(cid:2)t   1

xt     x(cid:1) = (i     p(a)a)(x0     x(cid:1)) = q(a)(x0     x(cid:1)),

where q(x) def= 1     xp(x). now, note that there is a one-to-one corre-
spondence between degree t     1 polynomials and degree t polynomials
that evaluate to 1 at 0. let this latter set of polynomials be qt. since
xt minimizes (cid:4)xt     x(cid:1)(cid:4)2

a over kt, we get that f(xt)     f(x(cid:1)) equals
(q(a)(x0     x(cid:1)))
(cid:3)

a(q(a)(x0     x(cid:1))).

(cid:4)xt     x(cid:1)(cid:4)2

(16.5)

a = min
q   qt

1
2

before we proceed with the proof of theorem 16.1, we need the follow-
ing lemma.

lemma 16.2. let a be a symmetric matrix with eigenvalues
  1, . . . ,   n. then, for any polynomial p(  ) and vector v,
|p(  i)|2.

a(p(a)v)     v

(p(a)v)

av   

(cid:3)

(cid:3)

nmax
i=1

(cid:3) where    is the diagonal matrix consisting of
proof. write a = u  u
eigenvalues and the columns of u are the corresponding orthonormal
(cid:3). now for any vector v, we get
eigenvectors. note that p(a) = u p(  )u
(cid:3)
u p(  )u
v.

ap(a)v = v

u  p2(  )u

(cid:3)
p(a)

u p(  )u

v = v

u  u

(cid:3)

(cid:3)

(cid:3)

(cid:3)

(cid:3)

(cid:3)

v

thus, if v =
to   j, we get that the l.h.s. of the above equality is
similarly, v(cid:3)

j   juj where uj is the eigenvector of a corresponding
j   jp2(  j).

j   j. the lemma follows.

av =

j   2

j   2

(cid:2)

(cid:2)

(cid:2)

plugging the above claim into equation (16.5) we get the following
upper bound on f(xt) which is crux in the analysis of the conjugate
gradient algorithm:

f(xt)     f(x(cid:1))     min
q   qt

nmax
i=1

|q(  i)|2    (f(x0)     f(x(cid:1))).

(16.6)

109
note that, since the eigenvalues of a satisfy   1                  n, the r.h.s. of
equation (16.6) can be further approximated by

16.3 analysis via polynomial minimization

f(xt)     f(x(cid:1))     min
q   qt

max

x   [  1,  n]

|q(x)|2    (f(x0)     f(x(cid:1))).

(16.7)

thus, since f(x0) = f(0) = 0 and f(x(cid:1)) =    1/2    (cid:4)x(cid:1)(cid:4)2
the following lemma about cgsolve.

a, we have proved

lemma 16.3. let a (cid:8) 0,   1,   n be the smallest and the largest eigen-
values of a, respectively, and let qt be the set of polynomials of degree
at most t which take the value 1 at 0. then,

(cid:4)xt     x(cid:1)(cid:4)2

a     (cid:4)x(cid:1)(cid:4)2

a    min
q   qt

|q(x)|2.

max

x   [  1,  n]

therefore, any polynomial in qt gives an upper bound on f(xt). in
particular, the polynomial q(x) def= (1     2x

(cid:13)

       1
   + 1

(cid:14)t

  1+  n )t gives
(f(x0)     f(x(cid:1))),

f(xt)     f(x(cid:1))    

where    =   n/  1. this is slightly better than what we obtained in
lemma 15.2. as a corollary of the discussion above, we again obtain
that after n iterations the solution computed is exact.

corollary 16.4. after n steps of cgsolve, xn = x(cid:1). hence,
o(tan) time, one can compute x(cid:1) = a+b for a positive de   nite a.

in

-n
proof. let   1                  n be the eigenvalues of a. consider the polyno-
i=1(1     x/  i). note that q(0) = 1, and q(  i) = 0 for all
mial q(x) def=
eigenvalues of a. from lemma 16.3,
a     (cid:4)x(cid:1)(cid:4)2
a   

|q(  i)|2 = 0.

(cid:4)xn     x(cid:1)(cid:4)2

nmax
i=1

110

iterative linear solvers iii the conjugate gradient method

16.4 chebyshev polynomials     why conjugate

gradient works

in the previous section we reduced the problem of bounding the error
after t iterations to a problem about polynomials. in particular, the goal
reduced to    nding a polynomial q(  ) of degree at most t which takes the
i=1|q(  i)| where   is are eigenvalues of a.
value 1 at 0 and minimizes maxn
in this section we present a family of polynomials, called chebyshev
polynomials, and use them to prove theorem 16.1.

for a nonnegative integer t, we will let tt(x) denote the degree t
chebyshev polynomial of the    rst kind, which are de   ned recursively
as follows: t0(x) def= 1, t1(x) def= x, and for t     2,

tt(x) def= 2xtt   1(x)     tt   2(x).

the following is a simple consequence of the recursive de   nition above.
proposition 16.5. if x     [   1,1], then we can de   ne   
def= arccos(x),
and the degree t chebyshev polynomial is given by tt(cos   ) = cos(t  ).
hence, tt(x)     [   1,1] for all x     [   1,1].

proof. first, note that t0(cos   ) = cos0 = 1 and t1(cos   ) = cos    = x.
additionally, cos((t + 1)  ) = cos(t  )cos(  )     sin(t  )sin(  ) and simi-
larly cos((t     1)  ) = cos(t  )cos(  )     sin(t  )sin(  ). hence, it is clear
that the recursive de   nition for chebyshev polynomials, namely
cos((t + 1)  ) = 2cos    cos(t  ) + cos((t     1)  ), applies.

thus, letting x = cos    and using de moivre   s formula, tt(x) = cos(t  ) =
2(exp(it  ) + exp(   it  )) can be written as
1

tt(x) =

1
2

((x +

x2     1)t),

which is a polynomial of degree t. for 0 < a < b, we de   ne the polyno-
mial qa,b,t as follows:

(cid:1)

(cid:1)
x2     1)t + (x    
)
) .

a+b   2x

(

(

tt

b   a
a+b
b   a

tt

qa,b,t(x) def=

16.5 the chebyshev iteration

111

note that qa,b,t is of degree t and evaluates to 1 at x = 0 and hence
lies in qt. furthermore, for x     [a, b], the numerator takes a value of
at most 1 (by the de   nition of tt). therefore, if we plug in a =   1 and
b =   n, the smallest and the largest eigenvalues of a, respectively, we
obtain
    x     [  1,   n], q  1,  n,t(x)     tt

(cid:14)   1

(cid:13)

(cid:13)

.

(cid:14)   1
  1 +   n
  n       1
   
(cid:6)(cid:1)
k+1   
k   1)t + (
2((
(cid:1)
(cid:1)

  n/  1     1
  n/  1 + 1

  n/  1 + 1
  n/  1     1
= tt
   
     1   
  +1)t), we get

(cid:7)t

since   (a) =   n/  1, and tt(   +1

     1) = 1

    x     [  1,   n], q  1,  n,t(x)     2
thus, by lemma 16.3, for any t         (
  (a)log 1/  ), after t steps of
cgsolve we get xt satisfying f(xt)     f(x(cid:1))       2    (f(x0)     f(x(cid:1))).
thus, for this value of t,

(16.8)

.

(cid:4)xt     a+b(cid:4)a       (cid:4)a+b(cid:4)a.

this completes the proof of theorem 16.1.

16.5 the chebyshev iteration

(cid:1)

in this section we consider the possibility of attaining a bound of
  (a)log 1/   iterations when xt = pt(a)b where pt is a sequence of
polynomials. when compared with cgsolve, this will have the advan-
is a linear operator applied to b. as in the proof of
tage that xt
lemma 16.3, it is su   cient to de   ne pts such that

|xpt(x)     1|     o(1    

  1/  n)t,

(16.9)

max

x   [  1,  n]

(cid:1)

(cid:1)
(cid:4)xt     a+b(cid:4)2
  (a)log 1/  ) iterations xt is    close to a+b. note that

a     o(1    

def= pt(a), then for all b,

thus, in o(
equation (16.9) implies that, if we let zt
(cid:4)ztb     a+b(cid:4)a     o(1    

(cid:1)
  1/  n)t    (cid:4)a+b(cid:4)2
a.
(cid:1)

  1/  n)t.

which gives us

iterative linear solvers iii the conjugate gradient method

112
hence, if (cid:4)ztb     a+b(cid:4)a       , then (cid:4)zt     a(cid:4)           (cid:4)a+(cid:4)1/2. in fact, we
can set pt to be q  1,  n,t, de   ned using the chebyshev polynomials
in the previous section, and use equation (16.8) to obtain the bound
in equation (16.9). the only issue that remains is how to compute
q  1,  n,t(a)b in   o(tat)-time. this can be done using the recursive de   -
nition of the chebyshev polynomials in the previous section and is left
as an exercise. the update rule starts by setting
def= b,

def= 0 and x1

x0

and for t     2,

xt

def=   2axt   1 +   1xt   2 +   0b

for    xed scalars   0,   1,   2 which depend on   1 and   n. this is called
the chebyshev iteration and, unlike cgsolve, requires the knowledge
of the smallest and the largest eigenvalues of a.1 we summarize the
discussion in this section in the following theorem.
theorem 16.6. there is an algorithm which given an n    n sym-
metric positive-de   nite matrix a, a vector b, numbers   l       1(a) and
  u       n(a) and an error parameter    > 0, returns an x such that

(1) (cid:4)x     a+b(cid:4)a           (cid:4)a+b(cid:4)a,
(2) x = zb where z depends only on a and   , and
(3) (cid:4)z     a+(cid:4)       ,

runs in (ta   (cid:1)

where we have absorbed the (cid:4)a+(cid:4)1/2 term in the error. the algorithm

  u/  l log 1/(    l)) time.

16.6 matrices with clustered eigenvalues

as another corollary of the characterization of lemma 16.3 we show
that the rate of convergence of cgslove is better if the eigenvalues
of a are clustered. this partly explains why cgsolve is attractive in
practice; it is used in section 17 to construct fast laplacian solvers.

1 for our application in section 18, we will be able to provide estimates of these eigenvalues.

16.6 matrices with clustered eigenvalues

113

(cid:1)
corollary 16.7. for a matrix a, suppose all but c eigenvalues are
contained in a range [a, b]. then, after t     c + o(
b/alog 1/  ) iterations,
(cid:4)xt     x(cid:1)(cid:4)a       (cid:4)x(cid:1)(cid:4)a.

i=1(1     x/  i) where   1, . . . ,   c are the c
proof. let q(x) def= qa,b,i(x)      c
eigenvalues outside of the interval [a, b], and qa,b,i de   ned earlier. from
lemma 16.3, since the value of q(  i) = 0 for all i = 1, . . . , c, we only
need to consider the maximum value of |q(x)|2 in the range [a, b]. by the
properties of qa,b,i described above, if we pick i =   (
b/alog 1/  ), for all
(cid:1)
x     [a, b], |q(x)|2       . therefore, cgsolve returns an   -approximation
b/alog 1/  ) steps; note that this could be much better than
in c + o(

(cid:1)

(cid:1)

  (a) since no restriction is put on the c eigenvalues.

notes

the conjugate gradient method was    rst proposed in [39]. there is a
vast amount of literature on this algorithm and the reader is directed to
the work in [36, 37, 68, 72]. more on chebyshev polynomials and their
centrality in approximation theory can be found in the books [21, 65].
an interested reader can check that a chebyshev-like iterative method
with similar convergence guarantees can also be derived by applying
nesterov   s accelerated id119 method to the convex function
in the beginning of this section.

17

preconditioning for laplacian systems

in this section, the notion of preconditioning is introduced. instead
of solving ax = b, here one tries to solve p ax = p b for a matrix p
such that   (p a) (cid:24)   (a) where it is not much slower to compute p v
than av, thus speeding up iterative methods such as the conjugate
gradient method. as an application, preconditioners are constructed
for laplacian systems from low-stretch spanning trees that result in
a   o(m4/3) time laplacian solver. preconditioning plays an important
role in the proof of theorem 3.1 presented in section 18. low-stretch
spanning trees have also been used in section 4.

17.1 preconditioning

tions ax = b in time o(ta   (cid:1)

we saw in the previous section that using the conjugate gradient
method for a symmetric matrix a (cid:8) 0, one can solve a system of equa-
  (a)log 1/  ) up to an error of   . let us
focus on the two quantities in this running time bound: ta which is the
time it takes to multiply a vector with a, and   (a), the condition num-
ber of a. note that the algorithm requires only restricted access to a:
given a vector v, output av. thus, one strategy to reduce this running

114

17.1 preconditioning

115
time is to    nd a matrix p s.t.   (p a) (cid:24)   (a) and tp     ta. indeed, if
we had access to such a matrix, we could solve the equivalent system
of equations p ax = p b.1 such a matrix p is called a preconditioner
for a. one choice for p is a+. this reduces the condition number to
1 but reduces the problem of computing a+b to itself, rendering it
useless. surprisingly, as we will see in this section, for a laplacian sys-
tem one can often    nd preconditioners by using the graph structure,
thereby reducing the running time signi   cantly. the following theorem
is the main result of this section.

theorem 17.1. for any undirected, unweighted graph g with m
edges, a vector b with (cid:2)b,1(cid:3) = 0, and    > 0, one can    nd an x such
that (cid:4)x     l+

gb(cid:4)lg in   o(m4/3 log 1/  ) time.

      (cid:4)l+

gb(cid:4)lg

there are two crucial ingredients to the proof. the    rst is the following
simple property of cgsolve.

lemma 17.2. suppose a is a symmetric positive-de   nite matrix with
minimum eigenvalue   1     1 and trace tr(a)       . then cgsolve con-
verges to an   -approximation in o(   1/3 log 1/  ) iterations.

proof. let    be the set of eigenvalues larger than   /  , where    is
a parameter to be set later. note that |  |       . therefore, apart
from these    eigenvalues, all the rest lie in the range [1,   /  ]. from
corollary 16.7, we get that cgsolve    nds an   -approximation in
def=    1/3 completes the proof
   + o(
of the lemma.

  /   log 1/  ) iterations. choosing   

(cid:1)

the second ingredient, and the focus of this section, is a construction
of a combinatorial preconditioner for lg. the choice of preconditioner
is l+
t where t is a spanning tree of g. note that from corollary 13.4,
this preconditioner satis   es the property that l+
t v can be computed in
o(n) time. thus, the thing we need to worry about is how to construct
a spanning tree t of g such that   (l+

t lg) is as small as possible.

1 one might worry that the matrix p a may not be symmetric; one normally gets around
this by preconditioning by p 1/2ap 1/2. this requires p (cid:16) 0.

116 preconditioning for laplacian systems

17.2 combinatorial preconditioning via trees

let g be an unweighted graph with an arbitrary orientation    xed for
the edges giving rise to the vectors be which are the rows of the corre-
sponding incidence matrix. we start by trying to understand why, for
a spanning tree t of a graph g, l+
t might be a natural candidate for
preconditioning lg. note that

(cid:3)

e   t

(cid:3)

e   g

lt =

e (cid:14)
(cid:3)
beb

(cid:3)
e = lg.
beb

t lg, which implies that   1(l+

t lg)     1. thus, to bound
thus, i     l+
the condition number of   (l+
t lg), it su   ces to bound   n(l+
t lg).
unfortunately, there is no easy way to bound this. since l+
t lg is
psd, an upper bound on its largest eigenvalue is its trace, tr(l+
t lg).
if this upper bound is   , lemma 17.2 would imply that the con-
jugate gradient method applied to l+
t lg takes time approximately
    o(   1/3(m + n)). thus, even though it sounds wasteful to
   1/3tl+
bound the trace by    rather than by the largest eigenvalue by   ,
lemma 17.2 allows us to improve the dependency on    from    1/2 to
   1/3. we proceed to bound the trace of l+

t lg

t lg:

(cid:6)

(cid:3)

e   g

(cid:7)

(cid:3)

e

tr(l+

t lg) = tr

l+
t

(cid:3)
beb
e

=

(cid:3)
e l+
b

t be,

e l+

where we use tr(a + b) = tr(a) + tr(b) and tr(abc ) = tr(cab).
note that b(cid:3)
t be is a scalar and is precisely the e   ective resistance
across the endpoints of e in the tree t where each edge of g has a unit
resistance. the e   ective resistance across two nodes i, j in a tree is the
sum of e   ective resistances along the unique path p (i, j) on the tree.
thus, we get

(cid:3)

e   g

tr(l+

t lg) =

|p (e)|.

a trivial upper bound on tr(l+
t lg), thus, is nm. this can also be
shown to hold when g is weighted. this leads us to the following
de   nition.

17.3 an   o(m4/3)-time laplacian solver

117

de   nition 17.1. for an unweighted graph g, the stretch of a spanning
t is de   ned to be strt (g) def= tr(l+

t lg).

thus, to obtain the best possible bound on the number of iterations of
cgsolve, we would like a spanning tree t of g which minimizes the
average length of the path an edge of g has to travel in t. the time it
takes to construct t is also important.

17.3 an   o(m4/3)-time laplacian solver
in this section we complete the proof of theorem 17.1. we start by
stating the following nontrivial structural result about the existence
and construction of low-stretch spanning trees whose proof is graph-
theoretic and outside the scope of this monograph. the theorem applies
to weighted graphs as well.

theorem 17.3. for any undirected graph g, a spanning tree t can
be constructed in   o(mlog n + nlog nlog log n) time such that strt (g) =
  o(mlog n). here   o(  ) hides log log n factors.

this immediately allows us to conclude the proof of theorem 17.1 using
lemma 17.2 and the discussion in the previous section. the only thing
that remains is to address the issue that the matrix l+
t lg is symmetric.
the following trick is used to circumvent this di   culty. recall from
theorem 13.3 that the cholesky decomposition of a tree can be done in
(cid:3), where e is a lower triangular
o(n) time. in particular, let lt = ee
matrix with at most o(n) nonzero entries. the idea is to look at the
system of equations e+lge+(cid:3)
y = e+b instead of lgx = b. if we can
solve for y then we can    nd x = e+(cid:3)
y, which is computationally fast
since e is lower triangular with at most o(n) nonzero entries. also, for
the same reason, e+b can be computed quickly.

now we are in good shape. let a

and b(cid:6) def= e+b;
note that a is symmetric. also, the eigenvalues of a are the same as
t lg. thus, the minimum eigenvalue of a is 1
those of e

def= e+lge+(cid:3)

(cid:3) = l+

(cid:3)+

ae

118 preconditioning for laplacian systems

and the trace is   o(m) by theorem 17.3. thus, using the conjugate gra-
dient method, we    nd an   -approximate solution to ay = b(cid:6) in   o(m1/3)
iterations.

in each iteration, we do o(1) matrix   vector multiplications. note
that for any vector v, av can be computed in o(n + m) operations
since e+v takes o(n) operations (since e is a lower triangular matrix
with at most o(n) nonzero entries) and lgv(cid:6) takes o(m) operations
(since lg has at most o(m) nonzero entries).

notes

while preconditioning is a general technique for laplacian systems,
its use originates in the work of vaidya [86]. using preconditioners,
vaidya obtained an   o((   n)1.75 log 1/  ) time laplacian solver for
graphs with maximum degree    . vaidya   s paper was never published
and his ideas and their derivatives appear in the thesis [42]. boman
and hendrickson [18] use low-stretch spanning trees constructed
by alon et al.
[6] to obtain   -approximate solutions to laplacian
systems roughly in m3/2 log 1/   time. the main result of this section,
theorem 17.1,
is from [81]. the    rst polylog n stretch trees in
near-linear-time were constructed by elkin et al. [27]. theorem 17.3
is originally from [1], and improvements can be found in [2]. an
important open problem is to    nd a simple proof of theorem 17.3
(potentially with worse log factors).

18

solving a laplacian system in   o(m) time

building on the techniques developed hitherto, this section presents an
algorithm which solves lx = b in   o(m) time.

18.1 main result and overview

(cid:1)

in section 17 we saw how preconditioning a symmetric psd matrix
a by another symmetric psd matrix p reduces the running time of
computing a+b for a vector b to about o((ta + tp )
  (p a)), where
ta and tp are the times required to compute matrix   vector product
with a and p , respectively. we saw that if a = lg for a graph g, then
a natural choice for p is l+
t where t is a spanning tree of g with small
total stretch.1 for a graph g with edge weights given by wg and a
tree t, the stretch of an edge e     e(g) in t is de   ned to be

strt (e) def= wg(e)

   1
g (f),

w

(cid:3)

f   pe

where pe is the set of edges on the unique path that connects the
if e     t, then strt (e) = 1. we de   ne
endpoints of e in t. thus,

1 unless speci   ed, when we talk about a spanning tree t of a weighted graph, the edges of
t inherit the weights from g.

119

120

solving a laplacian system in   o(m) time

(cid:2)

strt (g) def=

e   e strt (e). with this de   nition, it is easy to see that

strt (g) = tr(l+

t lg).

this is the same as the de   nition in section 17 where theorem 17.3
asserted a remarkable result: a spanning tree t such that strt (g) =
  o(m) can be constructed in time   o(m). note that choosing a spanning
tree is convenient since we can compute l+
t v exactly in o(n) time using
cholesky decomposition, see theorem 13.3. if t is such a spanning
tree and lt is its laplacian, then, using l+
t as a preconditioner, we
showed in theorem 17.1 how the conjugate gradient method can be
gb(cid:4)lg in
used to compute a vector x such that (cid:4)x     l+
  o(m4/3 log 1/  ) time. in this section we improve this result and prove
the following theorem; the key result of this monograph.

      (cid:4)l+

gb(cid:4)lg

theorem 18.1. for any undirected, unweighted graph g with m
edges, a vector b with (cid:2)b,1(cid:3) = 0, and    > 0, one can    nd an x such
that (cid:4)x     l+

gb(cid:4)lg in   o(mlog 1/  ) time.

      (cid:4)l+

gb(cid:4)lg

this theorem is a restatement of theorem 3.1 where the algorithm
which achieves this bound is referred to as lsolve. in this section we
present lsolve and show that it runs in   o(mlog 1/  ) time. toward
the end we discuss the linearity of lsolve: namely, the output x of
lsolve on input lg,b and    is a vector zx, where z is a n    n matrix
that depends only on g and   , and satis   es (cid:4)z     l+(cid:4)       .

proof overview

unlike the proof of theorem 17.1,
it is not clear how to prove
theorem 18.1 by restricting to tree preconditioners. the reason is that,
while for a tree t of g we can compute l+
t v in o(n) time, we do
not know how to improve upon the upper bound on   (l+
t lg) beyond
what was presented in section 17. a natural question is whether we
can reduce the condition number by allowing more edges than those
contained in a tree. after all, the cholesky decomposition-based algo-
rithm to solve tree systems in section 13 works if, at every step during
the elimination process, there is always a degree 1 or a degree 2 vertex.

18.1 main result and overview 121

the    rst observation we make, proved in section 18.2, is that if we
have a graph on n vertices and n     1 + k edges (i.e., k more edges
than in a tree), after we are done eliminating all degree 1 and 2 ver-
tices, we are left with a graph that has at most 2(k     1) vertices and
3(k     1) edges. thus, if k is not too large, there is a serious reduc-
tion in the size of the linear system that is left to solve. now, can
we    nd a subgraph h of g (whose edges are allowed to be weighted)
that has at most n     1 + k edges and the condition number of l+
h lg
is much smaller than n? the answer turns out to be yes. we achieve
this by a combination of the spectral sparsi   cation technique from sec-
tion 10 with low-stretch spanning trees. speci   cally, for a graph g with
n vertices and m edges, in   o(m) time we can construct a graph h with
h lg)     o((mlog2 n)/k). thus, if we choose
n     1 + k edges such that   (l+
k = m/(100log2 n), apply crude sparsi   cation followed by eliminating all
degree 1 and 2 vertices, we are left with a laplacian system of size
(cid:27)
o(m/(100log2 n)), call it   g. the details of how to    nd such an h appear
in section 18.3.
h lg)     10log2 n calls to the con-
  (l+
jugate gradient method to solve for l+
gb. this immediately leads us to
the problem of computing approximately 10log2 n products of the form
l+
hv for some vector v. this, in turn, requires us to compute the same
  gu products.   g is neither a tree nor does it contain any
number of l+
degree 1 or 2 vertices. thus, to proceed, we recurse on   g. fortunately,
h lg)/(100log2 n)     1/10. this
the choice of parameters ensures that
shrinkage ensures that the work done at any level remains bounded by
  o(m). we stop the recursion when the size of the instance becomes
polylog n. this means that the depth of the recursion is bounded by
o(log n/log log n). the details of how the recursion is employed and how
the parameters are chosen to ensure that the total work remains   o(m)
are presented in section 18.4.

we now need to make about

  (l+

(cid:2)

there is an important issue regarding the fact that the conjugate
gradient is error-prone. while in practice this may be    ne, in the-
ory, this can cause serious problems. to control the error, one has to
replace the conjugate gradient with its linear version, chebyshev itera-
tion, presented in section 16.5. this results in an algorithm that makes

122

solving a laplacian system in   o(m) time

lsolve a linear operator as claimed above. the details are presented
in section 18.5 and can be omitted.

if we were to present all the details that go into the proof of
theorem 18.1, including a precise description of lsolve, it would
be overwhelming to the reader and make the presentation quite long.
instead, we bring out the salient features and important ideas in the
algorithm and proof, and show how it marries ideas between graph the-
ory and id202 that we have already seen in this monograph.
the goal is not to convince the reader about the proof of theorem 18.1
to the last constants, but to give enough detail to allow a keen reader to
reconstruct the full argument and adapt it to their application. finally,
in what follows, we will ignore poly(log log n) factors.

(cid:6)

. if m

(cid:6) = n

(cid:6)     n

(cid:6)     1, then we can compute l+

18.2 eliminating degree 1, 2 vertices
(cid:6)     1 + k edges. as in
(cid:6) vertices and m
suppose h is a graph with n
section 13.2, we greedily eliminate all degree 1 and 2 vertices to obtain
(cid:6)) time
a graph g
via theorem 13.3. hence, we may assume that k     1. this reduces the
(cid:6)) time.
computation of l+
(cid:6) have? let k1 and k2 be the
number of vertices of degree 1 and 2 eliminated from h, respectively.
(cid:6))|     m
(cid:6)     k1     k2. the latter
then |v (g
occurs because we removed two edges adjacent to a degree 2 vertex and
, however, every vertex has
added one edge between its neighbors. in g
degree at least 3. hence,
(cid:6)     k1     k2)     2(m

how many edges and vertices does g
(cid:6)     k1     k2 and |e(g

(cid:6)     1 + k     k1     k2).

hv to that of computing l+

(cid:6)     k1     k2) = 2(n

g(cid:4)v in o(m

hv in o(n

(cid:6))| = n

(cid:6) + n

3(n

(cid:6)

hence, |v (g
|e(g

(cid:6))| = n
)|     m
(cid:6)

(cid:6)     k1     k2     2(k     1), and consequently,
(cid:6)     k1     k2     2(k     1) + k     1 = 3(k     1).

thus, we have proved the following lemma.

lemma 18.2. given a graph h with n
edges for k     1, the computation of l+
(cid:6) + m
l+
g(cid:4)v in time o(n
at most 3(k     1) edges.

(cid:6)) where g

(cid:6) vertices and m

(cid:6)     1 + k
hv can be reduced to computing
(cid:6) has at most 2(k     1) vertices and

(cid:6) = n

18.3 crude sparsi   cation using low-stretch spanning trees

123

18.3 crude sparsi   cation using low-stretch

spanning trees

we now present the following crude spectral sparsi   cation theorem,
which is a rephrasing of theorem 10.4.

(cid:2)

theorem 18.3. there is an algorithm that, given a graph g with edge
weights wg,    > 0, and numbers qe such that qe     wg(e)re for all e,
outputs a weighted graph h s.t.
(cid:2)

and o(w log w log 1/  ) edges. the algorithm succeeds with id203
at least 1        and runs in   o(w log w log 1/  ) time. here w
e qe
and re is the e   ective resistance of e.

lg (cid:14) 2lh (cid:14) 3lg

def=

f   pe w

(cid:2)

e     g, then re    (cid:2)

we show how one can compute qes that meet the condition of this the-
e qe in time o(mlog n + nlog2 n). thus, we do
orem and have small
not rely on the laplacian solver as in the proof of theorem 10.2. first
notice that if t is a spanning tree of g with weight function wg and
   1
g (f), where pe is the path in t with end-
points the same as e. this is because adding more edges only reduces the
e   ective resistance between the endpoints of e. hence, were     strt (e).
thus, strt (e) satis   es one property required by qe in the theorem above.
moreover, we leave it as an exercise to show that, given t, one can com-
pute strt (e) for all e     g in o(mlog n) time using elementary methods.
the thing we need to worry about is how to quickly compute a t with
e strt (e). this is exactly where the low-stretch spanning trees
small
from theorem 17.3, and mentioned in the introduction to this section,
come into play. recall that, for a graph g with weight function wg,
e   g strt (e) = o(mlog n) and
the tree t from theorem 17.3 satis   es
can be computed in time o(nlog2 n + mlog n). hence, the number of
edges in the crude sparsi   er from theorem 18.3 is o(mlog2 n) if we set
   = 1/log n. this is not good as we have increased the number of edges
in the sampled graph.
there is a trick to get around this. let 1 (cid:24)    < m be an addi-
tional parameter and let t be the low-stretch spanning tree from
theorem 17.3 for the graph g. consider the graph   g = (v, e) with

(cid:2)

solving a laplacian system in   o(m) time

124
weight function w   g(e) def=   wg(e) if e     t and w   g(e) def= wg(e) if e (cid:11)    t.
thus, in   g we have scaled the edges of t by a factor of   . this trivially
implies that

def= 1/      strt (e) if e (cid:11)    t.
we now let qe
thus, again it can be checked that the qes satisfy the conditions for
theorem 18.3 for   g with weights w   g. first note that

lg (cid:14) l   g (cid:14)   lg.
def= strt (e) = 1 if e     t and qe
(cid:3)

(cid:3)

(cid:3)

strt (e)

= n     1 + o (mlog n/  )

w =

qe =

1 +

e

e   t

e(cid:7)   t

  

as strt (g) = o(mlog n). now, let us estimate the number of edges we
obtain when we sample from the distribution {qe}e   e approximately
w log w times as in theorem 10.4. note that even if an edge is chosen
multiple times in the sampling process, there is exactly one edge in h
corresponding to it with a weight summed up over multiple selections.
thus, in h, we account for edges from t separately. these are exactly
n     1. on the other hand, if an edge e /    t , a simple cherno    bound
argument implies that with id203 at least 1     1/n0.1 (cid:26) 1     1/log n,
the number of such edges chosen is at most o(mlog2 n/  ). since it takes
o(nlog2 n + mlog n) time to    nd t and approximately w log w addi-
tional time to sample h, in

o(nlog2 n + mlog n + mlog2 n/  )

time, with id203 at least 1     1/log2 n, we obtain a graph h such
that

lg (cid:14) 2lh (cid:14) 3  lg.

moreover, the number of edges in h is n     1 + o(mlog2 n/  ). impor-
   
tantly,   (l+
  )
gv, we need about o(
computations of the form l+
hu if we deploy the conjugate gradient
method. to summarize, we have proved the following lemma.

h lg) = o(  ). thus, to compute l+

lemma 18.4. there is an algorithm that, given a graph g on n ver-
tices with m edges, a vector v, an    > 0 and a parameter   , constructs

18.4 recursive preconditioning     proof of the main theorem 125
a graph h on n vertices such that, with id203 at least 1     1/log n,

h lg) = o(  ),

(1)   (l+
(2) the number of edges in h is n     1 + o(mlog2 n/  ), and
(3) the time it takes to construct h is o(nlog2 n + mlog n +

mlog2 n/  ).

now, if we apply the greedy degree 1,2 elimination procedure on h, we
(cid:6) with o(mlog2 n/  ) edges and vertices. thus, we have reduced
obtain g
(cid:6) has
our problem to computing l+
gone down by a factor of    from that of g.

g(cid:4)u for vectors u where the size of g

18.4 recursive preconditioning     proof of the

main theorem

now we show how the ideas we developed in the last two sections can
be used recursively. our starting graph g = g1 has m1 = m edges and
n1 = n vertices. we use a parameter    < m which will be determined
later. we then crudely sparsify g1 using lemma 18.4 to obtain h1
such that, with id203 1     1/log n, (this id203 is    xed for all
iterations), the number of edges in h1 is at most n1     1 + o(m1 log2 n1/  )
and   (l+
h1lg1) = o(  ). if h1 does not satisfy these properties, then we
abort and restart from g1. otherwise, we apply greedy elimination, see
lemma 18.2, to h1 and obtain g2; this is a deterministic procedure. if
n2, m2 are the number of vertices and edges of g2, then we know that
m2 = o(m1 log2 n1/  ) and n2 = o(m1 log2 n1/  ). if n2     nf (for some nf
to be determined later), then we stop, otherwise we iterate and crude
sparsify g2. we follow this with greedy elimination to get h2 and
proceed to    nd g3 and so on. once we terminate, we have a sequence

g = g1, h1, g2, h2, . . . , hd   1, gd,

such that

(1) nd = o(nf ),
(2) for all 1     i     d     1,   (l+
(3) for all 2     i     d     1, ni, mi     mi   1 log2 ni   1

hilgi) = o(  ), and

.

  

126

solving a laplacian system in   o(m) time

note that ni, mi are the number of vertices and edges of gi, and
not hi. the id203 that this process is never restarted is at least
1     d/log n. for this discussion we assume that we use the conjugate
gradient method which, when given h and g, can compute l+
gu for a
given vector u with about
hv
for some vector v. the (incorrect) assumption here is that there is no
error in computation and we address this in the next section. the depth
of the recursion is

h lg) computations of the form l+

(cid:27)

  (l+

def=

d

log(n/nf)
log(  /log2 n) .

now to compute l+
   
   
of degree o(
roughly (
time o(n3
the last level is

g1u for a given vector u, the computation tree is
  ) and depth d. at the bottom, we have to compute
gdv. we can do this exactly in
f ) using gaussian elimination. hence, the total work done at

  )d problems of the type l+

   
f )    (

o(n3

  )d.

at level i, the amount of work is

   
(

k)i      o(mi + ni).

we want the sum over all except the top level to be o(m). this is
achieved if

   

      log2 n

  

(cid:24) 1.

tioners is(cid:3)

finally, note that the work done in building the sequence of precondi-

o(mi log ni + ni log2 ni + mi log2 ni/  ) = o(nlog2 n + mlog n)

i

if        log3 n. the depth of the recursion is d = log n/nf
choice of parameters is dictated by the following constraints:

log   /log2 n . hence, the

   
f )    (
  )d = o(mlog2 n),
(1) o(n3
   
      log2 n
   )     1
(2) o(
(3)        log3 n.

2 , and

18.5 error analysis and linearity of the inverse

127
   

under these conditions, the running time of the algorithm is   o(m
  )
def=
at the top level and o(mlog2 n) in the remaining levels. we set   
def= log n. this choice of    and nf also ensures that d is
100log4 n and nf
at most log n/(2log log n) and, hence, condition (2) above is also satis   ed.
finally, the id203 that we never restart is at least 1     d/log n (cid:26) 1    
1/100 for large enough n. the total running time is bounded by o((m +
n)log2 n). in the calculations above, note that as long as there is an
algorithm that reduces computing a+b to computing at most   (a)1     
products au, one can obtain an algorithm that runs in   o(mlogo(1/  ) n)
time. this concludes the proof of theorem 18.1 assuming that there
is no error in the application of the conjugate gradient method. in the
following section we the intuition behind managing this error.

18.5 error analysis and linearity of the inverse

as discussed above, the conjugate gradient is not error-free. since our
algorithm uses conjugate gradient recursively, the error could cascade
in a complicated manner and it is not clear how to analyze this e   ect.
here, we use the chebyshev iteration, see section 16.5, which achieves
the same running time as the conjugate gradient method but has the
additional property that on input a matrix a, a vector b, and an
   > 0, outputs x = zb where z is a matrix such that (1       )z (cid:14) a+ (cid:14)
(1 +   )z. this extra property comes at a price; it requires the knowl-
edge of the smallest and the largest eigenvalues of a. a bit more for-
(cid:1)
mally, if we are given a lower bound   l on the smallest eigenvalue
and an upper bound   u on the largest eigenvalue of a, then, after
  u/  l log 1/  ) iterations, the chebyshev iteration-based method out-
o(
puts an x = zb such that (1       )z (cid:14) a+ (cid:14) (1 +   )z.

let us see how to use this for our application. to illustrate the main

idea, consider the case when the chain consists of

g = g1, h1, g2, h2, g3,

where we compute l+
g3u exactly. in addition, we know that all the
hilgi lie in the interval [1,   ], where we    xed        log4 n.
eigenvalues of l+
given l+
h2 since g3 is obtained by elimi-
nating variables from h2; hence there is no error. thus, if we want

g3, it is easy to obtain l+

128

solving a laplacian system in   o(m) time

g2, z2, is such that (1       )z2 (cid:14) l+

to compute l+
g2u, we use the chebyshev iteration-based solver from
theorem 16.6 with error    and the guarantee that all eigenvalues of
l+
h2lg2 lie in the interval [1,   ]. thus, the linear operator that tries to
(cid:14) (1 +   )z2. this
approximate l+
is where the error creeps in and linearity is used. from z2 we can
easily construct   z1 which is supposed to approximate l+
h1; namely,
(1       )   z1 (cid:14) l+
(cid:14) (1 +   )   z1. this implies that the eigenvalues for our
approximator   z1lg1 of l+
h1lg1 which were supposed to lie in the
interval [1,   ] may spill out. however, if we enlarge the interval to
[1/1+  ,(1 +   )  ] for a small constant    su   ciently bigger than   , (at the
expense of increasing the number of iterations by a factor of 1 +   ) we
ensure that (1       )z1 (cid:14) l+
(cid:14) (1 +   )z1. this argument can be made
formal via induction by noting that   ,   , and    remain    xed through out.

h1

g1

g2

notes

theorem 18.1 was    rst proved by spielman and teng [77, 78, 79, 80] and
the proof presented in this section, using crude sparsi   ers, draws sig-
ni   cantly from [49, 50]. the idea of eliminating degree 1 and 2 vertices
was implicit in [86]. the idea of recursive preconditioning appears in
the thesis [42]. theorem 18.3 is in [49].

19

beyond ax = b

the lanczos method

this section looks beyond solving linear equations and considers the
more general problem of computing f(a)v for a given psd matrix a,
vector v and speci   ed function f(  ). a variant of the conjugate gradient
method, called the lanczos method, is introduced which uses krylov
subspace techniques to approximately compute f(a)v. the results of
this section can also be used to give an alternative proof of the main
result of section 9 on computing the matrix exponential.

19.1 from scalars to matrices

(cid:2)
suppose one is given a symmetric psd matrix a and a function
f : r (cid:9)    r. then one can de   ne f(a) as follows: let u1, . . . ,un be eigen-
i f(  i)uiu(cid:3)
vectors of a with eigenvalues   1, . . . ,   n, then f(a) def=
i .
given a vector v, we wish to compute f(a)v. one way to do this
exactly is to implement the de   nition of f(a) as above. this requires
the diagonalization of a, which is costly. hence, in the interest of speed,
we are happy with an approximation to f(a)v. the need for such
primitive often arises in theory and practice. while the conjugate gra-
dient method allows us to do this for f(x) = 1/x, it seems speci   c to
this function. for instance, it is not clear how to adapt the conjugate

129

130 beyond ax = b the lanczos method

gradient method to compute exp(a)v, a primitive central in several
areas of optimization and mathematics. see section 9 for the de   ni-
tion of matrix exponential. in this section we present a meta-method,
called the lanczos method, to compute approximations to f(a)v. the
method has a parameter k that provides better and better approxima-
tions to f(a)v as it increases: the error in the approximation after k
rounds is bounded by the maximum distance between the best degree k
polynomial and f in the interval corresponding to the smallest and the
largest eigenvalues of a. the id202 problem is, thus, reduced
to a problem in approximation theory. the following is the main result
of this section.

theorem 19.1. there is an algorithm that, given a symmetric psd
matrix a, a vector v with (cid:4)v(cid:4) = 1, a function f, and a positive integer
parameter k, computes a vector u such that,

(cid:4)f(a)v     u(cid:4)     2    min
pk     k

max
       (a)

|f(  )     pk(  )|.

here   k denotes the set of all degree k polynomials and   (a) denotes
the interval containing all the eigenvalues of of a. the time taken by
the algorithm is o((n + ta)k + k2).

we remark that this theorem can be readily applied to the computation
of exp(a)v; see the notes.

19.2 working with krylov subspace

for a given positive integer k, the lanczos method looks for an approxi-
mation to f(a)v of the form p(a)v where p is a polynomial of degree k.
note that for any polynomial p of degree at most k, the vector p(a)v is
a linear combination of the vectors {v, av, . . . , akv}. the span of these
vectors is referred to as the krylov subspace of order k of a w.r.t. v
and is de   ned below.

de   nition 19.1. given a matrix a and a vector v, the krylov sub-
space of order k, denoted by kk, is de   ned as the subspace that is
spanned by the vectors {v, av, . . . , akv}.

19.2 working with krylov subspace

131
since a and v are    xed, we denote this subspace by kk. (this
de   nition di   ers slightly from the one in section 16 where kk+1 was
used to denote this subspace.) note that any vector in kk has to be
of the form p(a)v, where p is a degree k polynomial. the lanczos
method to compute f(a)v starts by generating an orthonormal basis
for kk. let v0, . . . ,vk be any orthonormal basis for kk, and let vk be
the n    (k + 1) matrix with {vi}k
(cid:3)
k vk = ik+1
(cid:3)
k denotes the projection onto the subspace. also, let tk be
and vkv
the operator a in the basis {vi}k
i=0 restricted to this subspace, i.e.,
(cid:3)
k avk. since all the vectors v, av, . . . , akv are in the subspace,
tk
any of these vectors (or their linear combination) can be obtained by
applying tk to v (after a change of basis), instead of a. the following
lemma states this formally.

i=0 as its columns. thus, v

def= v

lemma 19.2. let vk be the orthonormal basis and tk be the oper-
ator a restricted to kk where (cid:4)v(cid:4) = 1, i.e., tk = v
(cid:3)
k avk. let p be a
polynomial of degree at most k. then,

p(a)v = vkp(tk)v

(cid:3)
k v.

(cid:3)
proof. recall that vkv
k is the orthogonal projection onto the subspace
kk. by linearity, it su   ces to prove this for p = xt for all t     k. this
k v = v. for all j     k, ajv lies in kk, thus,
(cid:3)
is true for t = 0 since vkv
(cid:3)
k ajv = ajv. hence,
vkv

atv = (vkv
= vk(v

(cid:3)
k )a(vkv
(cid:3)
k avk)(v

k )a       a(vkv
(cid:3)
(cid:3)
k )v
k avk)      (v
(cid:3)
(cid:3)
k avk)v

(cid:3)
k v = vkt t
kv

(cid:3)
k v.

(cid:3)
the next lemma shows that vkf(tk)v
k v approximates f(a)v as
well as the best degree k polynomial that uniformly approximates f.
the proof is based on the observation that if we express f as a
sum of any degree k polynomial and an error function, the above
lemma shows that the polynomial part is computed exactly in this
approximation.

132 beyond ax = b the lanczos method

lemma 19.3. let vk be the orthonormal basis, and tk be the opera-
tor a restricted to kk where (cid:4)v(cid:4) = 1, i.e., tk = v
k avk. let f : r     r
(cid:3)
be any function such that f(a) and f(tk) are well de   ned. then,

(cid:29)(cid:29)f(a)v     vkf(tk)v
(cid:13)

min
pk     k

max
       (a)

(cid:29)(cid:29) is at most

(cid:3)
k v
|f(  )     pk(  )| + max
       (tk)

(cid:14)
|f(  )     pk(  )|

.

proof. let pk be any degree k polynomial. let rk

(cid:29)(cid:29)(cid:29)f(a)v     vkf(tk)v

(cid:29)(cid:29)(cid:29)

(cid:3)
k v

def= f     pk. then,

(cid:29)(cid:29)(cid:29)
(cid:29)(cid:29)(cid:29)pk(a)v     vkpk(tk)v
(cid:29)(cid:29)(cid:29)rk(a)v     vkrk(tk)v
(cid:29)(cid:29)(cid:29)vkrk(tk)v

+
0 + (cid:4)rk(a)(cid:4) +
max
       (a)

|rk(  )| + max
       (tk)

(cid:3)
k v
(cid:3)
k v

(cid:3)
k
|rk(  )|.

(cid:29)(cid:29)(cid:29)
(cid:29)(cid:29)(cid:29)

   

(lemma 19.2)

   

=

minimizing over pk gives us our lemma.

observe that in order to compute this approximation, we do not need
to know the polynomial explicitly. it su   ces to prove that there exists
a degree k polynomial that uniformly approximates f on the inter-
val containing the spectrum of a and tk (for exact computation,
  (tk) = [  min(tk),   max(tk)]     [  min(a),   max(a)] =   (a).) moreover,
if k (cid:24) n, the computation is reduced to a much smaller matrix. we
now show that an orthonormal basis for the krylov subspace, vk, can
be computed quickly and then describe the lanczos procedure which
underlies theorem 19.1.

19.3 computing a basis for the krylov subspace
in this section, we show that if we construct the basis {vi}k
i=0 in a
particular way, the matrix tk has extra structure. in particular, if a is
symmetric, we show that tk must be tridiagonal. this helps us speed
up the construction of the basis.

19.3 computing a basis for the krylov subspace

133

algorithm 19.1 lanczos
input: a symmetric, psd matrix a, a vector v such that (cid:4)v(cid:4) = 1, a

positive integer k, and a function f : r     r

// construct an orthonormal basis to krylov

output: a vector u that is an approximation to f(a)v
1: v0     v
2: for i = 0     k     1
subspace of order k
do
if i = 0 then
w0     av0
else
wi     avi       ivi   1
end if
  i     v(cid:3)

3:
4:
5:
6:
7:
8:

i wi

// orthogonalize w.r.t. vi   1

9: w(cid:6)

def= wi       ivi

i

10:

i
  i+1

// orthogonalize w.r.t. vi
i = 0, compute the approximation with the matrices ti   1

// if w(cid:6)
and vi   1, instead of tk and vk. the error bound still holds.
  i+1     (cid:4)w(cid:6)
i(cid:4)
vi+1     w(cid:4)

11:
12: end for
13: let vk be the n    (k + 1) matrix whose columns are v0, . . . ,vk
14: let tk be the (k + 1)    (k + 1) matrix such that for all i (tk)ii =
i+1avi =   i+1 and all other
(cid:3)
k avk

v(cid:3)
i avi =   i,(tk)i,i+1 = (tk)i+1,i = v(cid:3)
entries are 0
15: compute a     f (tk) exactly via eigendecomposition
16: return vkav

// scaling it to norm 1

// compute tk

def= v

(cid:3)
k v

suppose we compute the orthonormal basis {vi}k

i=0 iteratively,
starting from v0 = v: for i = 0, . . . , k, we compute avi and remove
the components along the vectors {v0, . . . ,vi} to obtain a new vec-
tor that is orthogonal to the previous vectors. this vector, scaled to
norm 1, is de   ned to be vi+1. these vectors, by construction, are such
that for all i     k, span{v0, . . . ,vi} = span{v, av, . . . , akv}. note that
(tk)ij = v(cid:3)

i avj.

134 beyond ax = b the lanczos method

the basis

if we construct

iteratively as above, avj     span
{v0, . . . ,vj+1} by construction, and if i > j + 1, vi is orthogonal to
this subspace and hence v(cid:3)
i (avj) = 0. thus, tk satis   es (tk)ij = 0 for
i > j + 1.

moreover, if a is symmetric, v(cid:3)

i (avj), and hence tk is
symmetric and tridiagonal. this means that at most three coe   cients
are nonzero in each row. thus, while constructing the basis, at step
i + 1, we need to orthonormalize avi only w.r.t. vi   1 and vi. this fact
is used for e   cient computation of tk. the algorithm lanczos appears
in figure 19.1.

j (avi) = v(cid:3)

completing the proof of theorem 19.1
the algorithm lanczos implements the lanczos method discussed in
this section. the guarantee on u follows from lemma 19.3 and the fact
that   (tk)       (a). we use the fact that (tk)ij = v(cid:3)
i avj and that tk
must be tridiagonal to reduce our work to just computing o(k) entries
in tk. the total running time is dominated by k multiplications of a
with a vector, o(k) dot-products and the eigendecomposition of the
tridiagonal matrix tk to compute f(tk) (which can be done in o(k2)
time), giving a total running time of o((n + ta)k + k2).

notes

the presentation in this section is a variation of the lanczos method,
a term often used speci   cally to denote the application of this meta-
algorithm to computing eigenvalues and eigenvectors of matrices. it
has been used to compute the matrix exponential, see [60, 67, 87].
the lanczos method was combined with a semide   nite programming
technique from [62], a rational approximation result from [70] and the
spielman   teng laplacian solver by orecchia et al. [60] to obtain an
  o(m) time algorithm for the balanced edge-separator problem
from section 7. speci   cally, since theorem 19.1 does not require the
explicit knowledge the best polynomial, it was used in [60] to com-
pute an approximation to exp(   l)v and, hence, give an alternative
proof of theorem 9.1 from section 9. the eigendecomposition result

19.3 computing a basis for the krylov subspace

135

for tridiagonal matrices, referred to in the proof of theorem 19.1, is
from [63]. the reader is encouraged to compare the lanczos method
with the conjugate gradient method presented in section 16. con-
cretely,
it is a fruitful exercise to try to explore if the conjugate
gradient method can be derived from the lanczos method by plugging
in f(x) = x

   1.

references

[1] i. abraham, y. bartal, and o. neiman,    nearly tight low stretch spanning
trees,    in proceedings of the ieee symposium on foundations of computer
science (focs), pp. 781   790, 2008.

[2] i. abraham and o. neiman,    using petal-decompositions to build a low
stretch spanning tree,    in acm symposium on theory of computing (stoc),
pp. 395   406, 2012.

[3] d. achlioptas,    database-friendly random projections: johnson-lindenstrauss
with binary coins,    journal of computer and systems sciences, vol. 66, no. 4,
pp. 671   687, 2003.

[4] r. ahlswede and a. winter,    addendum to    strong converse for identi   cation
via quantum channels   ,    ieee transactions on id205, vol. 49,
no. 1, p. 346, 2003.

[5] r. k. ahuja, t. l. magnanti, and j. b. orlin, network flows: theory, algo-

rithms, and applications. prentice hall, february 1993.

[6] n. alon, r. m. karp, d. peleg, and d. b. west,    a graph-theoretic game and
its application to the k-server problem,    siam journal on computing, vol. 24,
no. 1, pp. 78   100, 1995.

[7] n. alon and v. d. milman,      1, isoperimetric inequalities for graphs, and
superconcentrators,    journal of combinational theory, series b, vol. 38, no. 1,
pp. 73   88, 1985.

[8] r. andersen, f. r. k. chung, and k. j. lang,    local graph partitioning using
id95 vectors,    in proceedings of the ieee symposium on foundations of
computer science (focs), pp. 475   486, 2006.

[9] r. andersen and y. peres,    finding sparse cuts locally using evolving sets,    in

acm symposium on theory of computing (stoc), pp. 235   244, 2009.

136

references

137

   
[10] s. arora, e. hazan, and s. kale,    o(

log n) approximation to sparsest cut
in   o(n2) time,    in proceedings of the ieee symposium on foundations of
computer science (focs), pp. 238   247, 2004.

[11] s. arora, e. hazan, and s. kale,    the multiplicative weights update method:
a meta-algorithm and applications,    theory of computing, vol. 8, no. 1,
pp. 121   164, 2012.

[12] s. arora and s. kale,    a combinatorial, primal-dual approach to semide   nite
programs,    in acm symposium on theory of computing (stoc), pp. 227   236,
2007.

[13] s. arora, s. rao, and u. v. vazirani,    expander    ows, geometric embeddings

and graph partitioning,    journal of the acm, vol. 56, no. 2, 2009.

[14] j. d. batson, d. a. spielman, and n. srivastava,    twice-ramanujan spar-
si   ers,    in acm symposium on theory of computing (stoc), pp. 255   262,
2009.

[15] m. belkin, i. matveeva, and p. niyogi,    id173 and semi-supervised
learning on large graphs,    in proceedings of the workshop on computational
learning theory (colt), pp. 624   638, 2004.

[16] a. a. bencz  ur and d. r. karger,    approximating s   t minimum cuts in   o(n2)
time,    in acm symposium on theory of computing (stoc), pp. 47   55, 1996.
[17] r. bhatia, matrix analysis (graduate texts in mathematics). springer, 1996.
[18] e. g. boman and b. hendrickson,    support theory for preconditioning,    siam

journal on matrix analysis applications, vol. 25, no. 3, pp. 694   717, 2003.

[19] s. boyd and l. vandenberghe, id76. cambridge university

press, march 2004.

[20] j. cheeger,    a lower bound for the smallest eigenvalue of the laplacian,    prob-

lems in analysis, pp. 195   199, 1970.

[21] e. w. cheney, introduction to approximation theory/e.w. cheney. new york:

mcgraw-hill, 1966.

[22] p. christiano, j. a. kelner, a. madry, d. a. spielman, and s. teng,    elec-
trical    ows, laplacian systems, and faster approximation of maximum    ow in
undirected graphs,    in acm symposium on theory of computing (stoc),
pp. 273   282, 2011.

[23] f. r. k. chung, spectral id207 (cbms regional conference series in

mathematics, no. 92). american mathematical society, 1997.

[24] s. i. daitch and d. a. spielman,    faster approximate lossy generalized    ow
via interior point algorithms,    in acm symposium on theory of computing
(stoc), pp. 451   460, 2008.

[25] p. g. doyle and j. l. snell, id93 and electric networks. washington,

dc: mathematical association of america, 1984.

[26] p. elias, a. feinstein, and c. shannon,    a note on the maximum    ow through a
network,    ieee transactions on id205, vol. 2, no. 4, pp. 117   119,
december 1956.

[27] m. elkin, y. emek, d. a. spielman, and s.-h. teng,    lower-stretch spanning

trees,    siam journal on computing, vol. 38, no. 2, pp. 608   628, 2008.

[28] m. fiedler,    algebraic connectivity of graphs,    czechoslovak mathematical

journal, vol. 23, pp. 298   305, 1973.

138 references

[29] l. r. ford and d. r. fulkerson,    maximal    ow through a network,    canadian

journal of mathematics, vol. 8, pp. 399   404, 1954.

[30] a. frangioni and c. gentile,    prim-based support-graph preconditioners
for min-cost    ow problems,    computational optimization and applications,
vol. 36, no. 2   3, pp. 271   287, april 2007.

[31] w. s. fung, r. hariharan, n. j. a. harvey, and d. panigrahi,    a general frame-
work for graph sparsi   cation,    in acm symposium on theory of computing
(stoc), pp. 71   80, 2011.

[32] m. r. garey and d. s. johnson, computers and intractability: a guide to the

theory of np-completeness. w.h. freeman, 1979.

[33] a. george,    nested dissection of a regular    nite element mesh,    siam journal

on numerical analysis, vol. 10, no. 2, pp. 345   363, 1973.

[34] c. d. godsil and g. royle, algebraic id207. springer, 2001.
[35] g. h. golub and c. f. van loan, matrix computations. johns hopkins univ.

press, 1996.

[36] g. h. golub and m. l. overton,    the convergence of inexact chebyshev and
richardson iterative methods for solving linear systems,    technical report,
stanford university, stanford, ca, usa, 1987.

[37] g. h. golub and r. s. varga,    chebyshev semi-iterative methods, successive
overrelaxation iterative methods, and second order richardson iterative meth-
ods,    numerische mathematik, vol. 3, pp. 157   168, 1961.

[38] k. gremban,    combinatorial preconditioners for sparse, symmetric, diagonally
dominant linear systems,    phd thesis, carnegie mellon university, pittsburgh,
cmu cs tech report cmu-cs-96-123, october 1996.

[39] m. r. hestenes and e. stiefel,    methods of conjugate gradients for solving linear
systems,    journal of research of the national bureau of standards, vol. 49,
pp. 409   436, december 1952.

[40] g. iyengar, d. j. phillips, and c. stein,    approximating semide   nite packing
programs,    siam journal on optimization, vol. 21, no. 1, pp. 231   268, 2011.
[41] g. iyengar, d. j. phillips, and c. stein,    approximation algorithms for semidef-
inite packing problems with applications to maxcut and graph coloring,    in
ipco   05: proceedings of conference on integer programming and combinato-
rial optimization, pp. 152   166, 2005.

[42] a. joshi,    topics in optimization and sparse linear systems,    phd thesis, uni-
versity of illinois at urbana-champaign, champaign, il, usa, umi order
no. gax97-17289, 1997.

[43] s. kaczmarz,       angen  aherte au     osung von systemen linearer gleichungen,   
bulletin international del    acad  emie polonaise sciences et des lettres,
pp. 355   356, 1937.

[44] s. kale,    e   cient algorithms using the multiplicative weights update method,   

phd thesis, princeton university, department of computer science, 2007.

[45] j. a. kelner and a. madry,    faster generation of random spanning trees,   
in proceedings of the ieee symposium on foundations of computer science
(focs), pp. 13   21, 2009.

[46] j. a. kelner, g. l. miller, and r. peng,    faster approximate multicommod-
ity    ow using quadratically coupled    ows,    in acm symposium on theory of
computing (stoc), pp. 1   18, 2012.

references

139

[47] j. a. kelner, l. orecchia, a. sidford, and z. a. zhu,    a simple, combinatorial
algorithm for solving sdd systems in nearly-linear time,    in acm symposium
on theory of computing (stoc), 2013.

[48] r. khandekar, s. rao, and u. v. vazirani,    graph partitioning using single

commodity    ows,    journal of the acm, vol. 56, no. 4, 2009.

[49] i. koutis, g. l. miller, and r. peng,    approaching optimality for solving sdd
linear systems,    in proceedings of the ieee symposium on foundations of com-
puter science (focs), pp. 235   244, 2010.

[50] i. koutis, g. l. miller, and r. peng,    a nearly-mlog n time solver for sdd
linear systems,    in proceedings of the ieee symposium on foundations of
computer science (focs), pp. 590   598, 2011.

[51] d. a. levin, y. peres, and e. l. wilmer, markov chains and mixing times.

american mathematical society, 2006.

[52] r. j. lipton, d. j. rose, and r. e. tarjan,    generalized nested dissection,   

siam journal on numerical analysis, vol. 16, no. 2, pp. 346   358, 1979.

[53] l. lov  asz and m. simonovits,    id93 in a convex body and an
improved volume algorithm,    random structures & algorithms, vol. 4, no. 4,
pp. 359   412, 1993.

[54] r. lyons and y. peres, id203 on trees and networks. to appear in

cambridge university press, 2012.

[55] a. madry,    fast approximation algorithms for cut-based problems in undi-
rected graphs,    in proceedings of the ieee symposium on foundations of com-
puter science (focs), pp. 245   254, 2010.

[56] m. w. mahoney, l. orecchia, and n. k. vishnoi,    a spectral algorithm for
improving graph partitions,    journal of machine learning research, vol. 13,
pp. 2339   2365, 2012.

[57] s. maji, n. k. vishnoi, and j. malik,    biased normalized cuts,    in proceedings
of the ieee conference on id161 and pattern recognition (cvpr),
pp. 2057   2064, 2011.

[58] m. mihail,    conductance and convergence of markov chains     a combinatorial
treatment of expanders,    in proceedings of the ieee symposium on founda-
tions of computer science (focs), pp. 526   531, 1989.

[59] l. orecchia,    fast approximation algorithms for graph partitioning using spec-
tral and semide   nite-programming techniques,    phd thesis, eecs depart-
ment, university of california, berkeley, may 2011.

[60] l. orecchia, s. sachdeva, and n. k. vishnoi,    approximating the exponential,
the lanczos method and an   o(m)-time spectral algorithm for balanced sepa-
rator,    in acm symposium on theory of computing (stoc), pp. 1141   1160,
2012.

[61] l. orecchia, l. j. schulman, u. v. vazirani, and n. k. vishnoi,    on parti-
tioning graphs via single commodity    ows,    in acm symposium on theory of
computing (stoc), pp. 461   470, 2008.

[62] l. orecchia and n. k. vishnoi,    towards an sdp-based approach to spectral
methods: a nearly-linear-time algorithm for graph partitioning and decom-
position,    in proceedings of the annual acm-siam symposium on discrete
algorithms (soda), pp. 532   545, 2011.

140 references

[63] v. y. pan and z. q. chen,    the complexity of the matrix eigenproblem,    in

acm symposium on theory of computing (stoc), pp. 507   516, 1999.

[64] l. f. richardson,    the approximate arithmetical solution by    nite di   erences
of physical problems involving di   erential equations with an application to
the stresses in a masonry dam,    transactions of the royal society of london,
vol. series a, no. 210, pp. 307   357, 1910.

[65] t. j. rivlin, an introduction to the approximation of functions. blaisdell book

in numerical analysis and computer science. blaisdell pub. co., 1969.

[66] m. rudelson,    random vectors in the isotropic position,    journal of functional

analysis, vol. 164, no. 1, pp. 60   72, 1999.

[67] y. saad,    analysis of some krylov subspace approximations to the matrix expo-
nential operator,    siam journal on numerical analysis, vol. 29, pp. 209   228,
february 1992.

[68] y. saad, iterative methods for sparse linear systems. society for industrial

and applied mathematics. philadelphia, pa, usa, 2nd edition, 2003.

[69] s. sachdeva and n. k. vishnoi,    inversion is as easy as exponentiation,   

manuscript, 2012.
[70] e. b. sa   , a. schonhage, and r. s. varga,    geometric convergence to
e   z by rational functions with real poles,    numerische mathematik, vol. 25,
pp. 307   322, 1975.

   
[71] j. sherman,    breaking the multicommodity    ow barrier for o(

log n)-
approximations to sparsest cut,    in proceedings of the ieee symposium on
foundations of computer science (focs), 2009.

[72] j. r. shewchuk,    an introduction to the conjugate gradient method without
the agonizing pain,    technical report, carnegie mellon university, pittsburgh,
pa, usa, 1994.

[73] j. shi and j. malik,    normalized cuts and image segmentation,    ieee trans-
actions on pattern analysis and machine intelligence, vol. 22, pp. 888   905,
1997.

[74] d. b. shmoys, cut problems and their application to divide-and-conquer.

pp. 192   235, 1997.

[75] d. a. spielman,    algorithms, id207, and linear equations in lapla-
cian matrices,    in proceedings of international congress of mathematicians
(icm   10), 2010.

[76] d. a. spielman and n. srivastava,    graph sparsi   cation by e   ective resis-

tances,    siam journal on computing, vol. 40, no. 6, pp. 1913   1926, 2011.

[77] d. a. spielman and s.-h. teng,    nearly-linear time algorithms for graph parti-
tioning, graph sparsi   cation, and solving linear systems,    in acm symposium
on theory of computing (stoc), pp. 81   90, new york, ny, usa, 2004.

[78] d. a. spielman and s.-h. teng,    nearly-linear time algorithms for precondi-
tioning and solving symmetric, diagonally dominant linear systems,    corr,
abs/cs/0607105, 2006.

[79] d. a. spielman and s.-h. teng,    a local id91 algorithm for massive
graphs and its application to nearly-linear time graph partitioning,    corr,
abs/0809.3232, 2008.

references

141

[80] d. a. spielman and s.-h. teng,    spectral sparsi   cation of graphs,    siam

journal on computing, vol. 40, no. 4, pp. 981   1025, 2011.

[81] d. a. spielman and j. woo,    a note on preconditioning by low-stretch spanning

trees,    corr, abs/0903.2816, 2009.

[82] g. strang, id202 and its applications. harcourt brace jonanovich.

san diego, 3rd edition, 1988.

[83] t. strohmer and r. vershynin,    a randomized kaczmarz algorithm with expo-
nential convergence,    journal of fourier analysis and applications, vol. 15,
pp. 262   278, 2009.

[84] s. teng,    the laplacian paradigm: emerging algorithms for massive graphs,   
in proceedings of the annual conference on theory and applications of models
of computation (tamc), pp. 2   14, 2010.

[85] l. n. trefethen and d. bau, numerical id202. siam, 1997.
[86] p. m. vaidya,    solving linear equations with symmetric diagonally dominant
matrices by constructing good preconditioners,    technical report, department
of computer science, university of illinois at urbana-champaign, urbana, il,
1990.

[87] j. vanden eshof and m. hochbruck,    preconditioning lanczos approximations
to the matrix exponential,    siam journal on scienti   c computing, vol. 27,
pp. 1438   1457, november 2005.

[88] j. h. wilkinson, the algebraic eigenvalue problem (monographs on numerical

analysis). usa: oxford university press, 1st edition, april 1988.

[89] x. zhu, z. ghahramani, and j. d. la   erty,    semi-supervised learning using
gaussian    elds and id94,    in proceedings of the international
conference on machine learning (icml), pp. 912   919, 2003.

