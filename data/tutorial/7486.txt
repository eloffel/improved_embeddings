instance-based 

learning 

(a.k.a. memory-based) (a.k.a. non-
parametric regression) (a.k.a. case-

based) (a.k.a kernel-based)

note to other teachers and users of 
these slides. andrew would be 
delighted if you found this source 
material useful in giving your own 
lectures. feel free to use these slides 
verbatim, or to modify them to fit your 
own needs. powerpoint originals are 
available. if you make use of a 
significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials
. comments and corrections gratefully 
received. 

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

software to play with the algorithms in 
this tutorial, and example data are 
available from: 
http://www.cs.cmu.edu/~awm/vizier . 
the example figures in this slide-set were 
created with the same software and data.

copyright    2001, 2005, andrew w. moore

overview

    what do we want a regressor to do?
    why not stick with polynomial regression? why not 

just    join the dots   ? 

    what   s k-nearest-neighbor all about? 
    and how about kernel regression, locally weighted 

    id48.  but what about multivariate fitting?
    and how do you compute all that stuff? and why 

regression?

should i care?

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 2

1

this tutorial   s starting point

we   ve obtained some 
numeric data.
how do we exploit it?

steel

temp

-0.4788

0.3470

0.8622

-0.2099

-0.4627

0.7341

0.4237

-0.1267

0.0332

0.2668

-0.5682

.

.

.

line

speed

0.3849

-0.6488

-0.5367

-0.5975

0.6218

0.0745

-0.5143

0.8105

0.7754

0.8777

0.8457

.

.

.

slab

width

0.0357

0.6629

-0.2459

0.0614

0.9254

-0.2650

-0.0731

-0.6619

0.7718

-0.5690

0.5669

.

.

.

temp

cool

stage2

setpt

0.8384

-0.5087

-0.1438

-0.7648

0.6081

-0.4510

0.0385

-0.0768

-0.5440

-0.5151

-0.7359

.

.

.

0.6620

0.6845

-0.7267

0.0222

-0.8739

-0.4548

-0.8068

-0.8738

0.6237

0.6493

-0.3394

.

.

.

cool

gain

-0.9512

-0.0647

-0.6119

-0.7623

-0.6439

0.5753

0.3098

-0.3367

0.5113

0.7705

0.5880

.

.

.

simple, uni-
variate case general, multi 

variate case

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 3

software and data for the algorithms in this tutorial: http://www.cs.cmu.edu/~awm/vizier . the 
example figures in this slide-set were created with the same software and data.

why not just use id75?

here, id75 
manages to capture a 
significant trend in the 
data, but there is visual 
evidence of bias.

here, id75 
appears to have a much 
better fit, but the bias is 
very clear.

here, id75 
may indeed be the right 
thing.

bias: the underlying choice of model (in this case, a line)cannot, with 
any choice of parameters (constant term and slope)and with any amount 
of data (the dots)capture the full relationship.
copyright    2001, 2005, andrew w. moore

instance-based learning: slide 4

2

why not just join the dots?

here, joining the dots is 
clearly fitting noise.

here, joining the dots 
looks very sensible.

again, a clear case of 
noise fitting.

why is fitting the noise so bad?

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 5

why not just join the dots?
   you will tend to make somewhat bigger 
prediction errors on new data than if you 
filtered the noise perfectly.
   you don   t get good gradient estimates or 
noise estimates.
   you can   t make sensible confidence intervals.
    it   s morally wrong.
   also: join the dots is much harderto 
implement for multivariate inputs.
again, a clear case of 
noise fitting.

here, joining the dots 
looks very sensible.

here, joining the dots is 
clearly fitting noise.

why is fitting the noise so bad?

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 6

3

one-nearest neighbor

   one nearest neighbor for fitting is described shortly   

similar to join the dots with two pros and one con.
    pro:  it is easy to implement with multivariate inputs.
    con:  it no longer interpolates locally.
    pro:  an excellent introduction to instance-based learning   

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 7

univariate 1-nearest neighbor
given datapoints (x1,y1) (x2,y2)..(xn,yn),where we assume 
yi=f(si) for some unknown function f.
given query point xq, your job is to predict 
nearest neighbor:
1.   find the closest xiin our set of datapoints

).qxf
(

   
y

=

   

(
nni

)

2.  predict

   
y =

)nniy
(

here   s a 
dataset with 
one input, one 
output and 
four 
datapoints.

y

copyright    2001, 2005, andrew w. moore

= argmin

i

x
i

   

x

q

here, this is 
the closest 
datapoint

x

here, this is 
the closest 
datapoint

here, this is 
the closest 
datapoint

here, this is 
the closest 
datapoint

instance-based learning: slide 8

4

1-nearest neighbor is an example of   .

instance-based learning

a function approximator 
that has been around 
since about 1910.
to make a prediction, 
search database for 
similar datapoints, and fit 
with the local points.

x1
x2
x3

xn

.
.

y1
y2
y3

yn

four things make a memory based learner:
   
   
   
   
copyright    2001, 2005, andrew w. moore

a distance metric
how many nearby neighbors to look at?
a weighting function (optional)
how to fit with the local points?

instance-based learning: slide 9

nearest neighbor

four things make a memory based learner:
1. a distance metric

euclidian

2. how many nearby neighbors to look at?

one

3. a weighting function (optional)

unused

4. how to fit with the local points?

just predict the same output as the nearest 
neighbor.

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 10

5

multivariate distance metrics

suppose the input vectors x1, x2,    xn are two dimensional:
x1= ( x11, x12) , x2= ( x21, x22) ,    xn= ( xn1, xn2).
one can draw the nearest-neighbor regions in input space.

dist(xi,xj) = (xi1    xj1)2 + (xi2    xj2)2

dist(xi,xj) =(xi1    xj1)2+(3xi2    3xj2)2

the relative scalings in the distance metric affect region shapes.
copyright    2001, 2005, andrew w. moore

instance-based learning: slide 11

euclidean distance metric

or equivalently,

d

where

d

(x,

 
 )x'
=

   

2

  
i

(

x
i

   

x

'
i

2

)

i
 )x'
(x 
(x,
=
0
  
2
   
1
   
0
  
2
   
2
   
   
   
   

0

=   

)x'-

t

l
l

   
0
0

 )x'-

(x
   
   
   
   
   
   
   

llll
0
  
2
n

l

other metrics   
    mahalanobis, rank-based, correlation-based 

(stanfill+waltz, maes    ringo system   )

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 12

6

the zen of voronoi diagrams

id98 article
mystery of renowned zen

garden revealed

thursday, september 26, 2002 posted: 10:11 am 

edt (1411 gmt)

london (reuters) -- for centuries visitors to the 

renowned ryoanji temple garden in kyoto, 
japan have been entranced and mystified by 
the simple arrangement of rocks.

the researchers discovered that the empty space of 

the garden evokes a hidden image of a branching 
tree that is sensed by the unconscious mind.

"we believe that the unconscious perception of this 

pattern contributes to the enigmatic appeal of the 
garden," van tonder added.

he and his colleagues believe that whoever created 
the garden during the muromachi era between 
1333-1573 knew exactly what they were doing 
and placed the rocks around the tree image.

the five sparse clusters on a rectangle of raked 
gravel are said to be pleasing to the eyes of 
the hundreds of thousands of tourists who visit 
the garden each year.

by using a concept called medial-axis transformation, 

the scientists showed that the hidden branched 
tree converges on the main area from which the 
garden is viewed.

scientists in japan said on wednesday they now 
believe they have discovered its mysterious 
appeal.

"we have uncovered the implicit structure of the 

ryoanji garden's visual ground and have 
shown that it includes an abstract, minimalist 
depiction of natural scenery," said gert van 
tonder of kyoto university.

copyright    2001, 2005, andrew w. moore

the trunk leads to the prime viewing site in the 

ancient temple that once overlooked the garden.

it is thought that abstract art may have a similar 

impact.

"there is a growing realisation that scientific analysis 
can reveal unexpected structural features hidden 
in controversial abstract paintings," van tonder
said 

instance-based learning: slide 13

zen part two

(photos and article extracted from www.id98.com)
question: what set of five rocks placed at a 

distance would have notproduced a tree-like 
voronoi diagram?

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 14

7

notable distance metrics

)
2
l
(
 
n
a
i
d

 

i
l
c
u
e
d
e
l
a
c
s

l

)
e
t
u
o
s
b
a
(
 

m
r
o
n
1
l

m
or
) n
x
a
m
(
infinity

l

c
i
r
t
e
m
m
y
s
 
s
i
 
t
u
b

 
,
l
a
n
o
g
a
i
d

 
s
u
o
i
v
e
r
p
e
h
t
 
n
o
  

 

 
,
e
r
e
h
(

 
y
l
i
r
a
s
s
e
c
e
n
 
t
o
n
 
s
i
 

e
d

i
l
s

 
s
i
b
o
n
a
l
a
h
a
m

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 15

..let   s leave distance metrics for now, and go back to   .

one-nearest neighbor

objection:
that noise-fitting is really objectionable.
what   s the most obvious way of dealing with it?

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 16

8

k-nearest neighbor
four things make a memory based learner:
1. a distance metric

euclidian

2. how many nearby neighbors to look at?

k

3. a weighting function (optional)

unused

4. how to fit with the local points?

just predict the average output among the k 
nearest neighbors.

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 17

k-nearest neighbor (here k=9)

a magnificent job of noise-
smoothing. three cheers for 
9-nearest-neighbor.
but the lack of gradients and 
the jerkiness isn   t good.

appalling behavior! loses 
all the detail that join-
the-dots and 1-nearest-
neighbor gave us, yet 
smears the ends.

fits much less of the noise, 
captures trends. but still, 
frankly, pathetic compared 
with id75.

k-nearest neighbor for function fitting smoothes away noise, but 
there are clear deficiencies.
what can we do about all the discontinuities that id92 gives us?
copyright    2001, 2005, andrew w. moore

instance-based learning: slide 18

9

kernel regression
four things make a memory based learner:
1. a distance metric
scaled euclidian

2. how many nearby neighbors to look at?

all of them

3. a weighting function (optional)
2)
wi= exp(-d(xi, query)2/ kw

nearby points to the query are weighted strongly, 

far points weakly. the kw parameter is the 
kernel width. very important.

4. how to fit with the local points?

predict the weighted average of the outputs:
predict =   wiyi/  wi

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 19

kernel regression in pictures

take this 
dataset   

..and do a kernel 
prediction with xq 
(query) = 310,    
kw = 50.

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 20

10

varying the query

xq= 150

xq= 395

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 21

varying the kernel width

xq = 310 (the 
same)
kw = 100

xq = 310
kw = 50 (see the 
double arrow at top of 
diagram)
increasing the kernel width kw means further away points get an 
opportunity to influence you.
as kw(cid:198)infinity, the prediction tends to the global average.
copyright    2001, 2005, andrew w. moore

xq = 310 (the 
same)
kw = 150

instance-based learning: slide 22

11

kernel regression predictions

kw=10

kw=20

kw=80

increasing the kernel width kw means further away 
points get an opportunity to influence you.
as kw(cid:198)infinity, the prediction tends to the global average.

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 23

software and data for the algorithms in this tutorial: http://www.cs.cmu.edu/~awm/vizier . the 
example figures in this slide-set were created with the same software and data.

kernel regression on our test cases

kw=1/32 of x-axis width.
it   s nice to see a smooth 
curve at last. but rather 
bumpy. if kw gets any 
higher, the fit is poor.

kw=1/32 of x-axis width.
quite splendid. well done, 
kernel regression. the 
author needed to choose 
the right kw to achieve this.

kw=1/16 axis width.
nice and smooth, but 
are the bumps 
justified, or is this 
overfitting?

choosing a good kw is important. not just for kernel regression, but 
for all the locally weighted learners we   re about to see.

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 24

12

weighting functions

let

d=d(xi,xquery)/kw

then here are some 
commonly used 
weighting functions   

(we use a gaussian)

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 25

weighting functions

let

d=d(xi,xquery)/kw

then here are some 
commonly used 
weighting functions   

(we use a gaussian)

newsflash:
the word on the street from 
recent non-parametric 
statistics papers is that the 
precise choice of kernel shape 
doesn   t matter much.

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 26

13

kernel regression can look bad

kw = best.

kw = best.

kw = best.

clearly not capturing 
the simple structure of 
the data.. note the 
complete failure to 
extrapolate at edges.

also much too local. 
why wouldn   t 
increasing kw help? 
because then it would 
all be    smeared   .

three noisy linear 
segments. but best 
kernel regression gives 
poor gradients.

time to try something more powerful   
copyright    2001, 2005, andrew w. moore

instance-based learning: slide 27

locally weighted 

regression

kernel regression:

take a very very conservative function 
approximator called averaging. locally 
weight it.

locally weighted regression:

take a conservative function approximator 
called id75. locally weight it.

let   s review id75   .

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 28

14

unweighted id75
you   re lying asleep in bed. then nature wakes you.

you:     oh. hello, nature!   
nature:     i have a coefficient    in mind. i took a bunch of 
real numbers called x1, x2..xnthus: x1=3.1,x2=2,    xn=4.5.
for each of them (k=1,2,..n), i generated yk=   xk+  k
where   k is a gaussian (i.e. normal) random variable with 
mean 0 and standard deviation   . the   k   s were generated 
independently of each other.
here are the resulting yi   s: y1=5.1 ,  y2=4.2 ,    yn=10.2   

you:    uh-huh.   
nature:    so what do you reckon    is then, eh?   
what is your response?

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 29

global id75: yk=  xk +  k

prob

(

k

xy
k
(

prob

)
 ~   ,

xy
k

k

x

  
gaussian,
 
dev.
 
std.
 ,  mean 
 
k
)
2
   
   
x
  
   
)
      
      
exp
 
   ,
k
=
2
2
  
   
   
(
y

k

   

   

y

(

n

k

y

(

,...

prob

yy
,
1

  
   
k
2
2
  
which value of    makes the y1, y2..yn values most likely?
x

   
      
exp
   

xx
,
1

   

prob

,...,

arg

,...,

,...

  ,

  ,

k

   
  

=

)

x

=

y

1
=

n

n

2

2

k

(

yy
1

,

2

xx
,
1

2

n

n

)

2

)

x

k

   
      
   

arg

max
  
max
  
max
 
  
min
arg
n
 
   
  
k
1
=

arg

n

log

k

   

(

y

k

   

  

x

k

=

=

=
copyright    2001, 2005, andrew w. moore

 
prob
log

(

yy
,
1

2

,...

y

n

xx
,
1

2

,...,

x

n

  ,

)

(

n
   
k
1
=

y

k

   

  

x

k

2

)

1
2
2
  
)

2

instance-based learning: slide 30

15

least squares unweighted id75

   

  

x

k

2

)

     so ,

=

arg
min
  

e

( )
  

( )
   write

e

=

 to

minimize

          

          

k

(
y
   
k
( )
e
 ,   
   
  
   

     

set
( )
  

e

=

0

   
  
   
    1    

e(  )=
sum of squares of 
green line lengths

so

   
  
   

      
0

=

giving

e

( )
  

2
   =

   
k

yx
k

k

+

  2

x

2
k

   
k

          

          

      

      =
   
   

k

x

2
k

1
   
   
      
   
k

yx
k

k

e(  )

  

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 31

multivariate unweighted id75
nature supplies ninput vectors.  each input vector xk
is d-dimensional: xk = ( xk1, xk2.. xkd) .  nature also supplies n 
corresponding output values y1.. yn.

x

=

x
11
x
21
:

x

n

1

   
   
   
   
   
   

x
12
x
22
:

x

n

2

l
l

l

x
1
x

d

d

2
:

x

nd

      

y

=

   
   
   
   
   
   

y
1
y
2
:
y

n

   
   
   
   
   
   

   
   
   
   
   
   

    we

are 

 told

 

y

=

k

   
      
   

d

   

j

=1

  
j

x

kj

   
+      
   

  
k

we must estimate    = (  1,   2       d). it   s easily shown using matrices 
instead of scalars on the previous slide that
yx
t

xx

   
=  

1   

t

)

note that xtxis a d x d positive definite symmetric matrix, and xtyis a 
d x 1 vector:

n

n

(

t

xx

)

ij

=

xx
ki

kj

(
     

yx
t

)    
=

i

k

1
=

yx
ki

i

(

   

k

1
=

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 32

16

the pesky constant term

now: nature doesn   t guarantee that the line/hyperplane passes 

through the origin.

in other words:  nature says

y

k

=

  

0

+

   
      
   

d

   

j

1
=

  

x

kj

j

   
+      
   

  

k

   no problem,    you reply.     just add one extra input variable, xk0, which is 
always 1   
   
   
   
   
   
   

l
l
   

1
   
   
1
   
:
   
   
1
   

x
12
x
22
:

x
11
x
21
:

x
12
x
22
:

x
11
x
21
:

l
l

   
   
   
   
   
   

   
   
   
   
   
   

x
1
x

x
1
x

   

2
:

2
:

nd

nd

x

x

x

x

x

x

l

l

d

d

d

d

n

n

n

n

2

1

2

1

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 33

locally weighted regression
four things make a memory-based learner:
1. a distance metric

2. how many nearby neighbors to look at?

scaled euclidian

all of them

3. a weighting function (optional)

wk= exp(-d(xk, xquery)2/ kw
nearby points to the query are weighted strongly, far points 
weakly. the kwparameter is the kernel width.

2)

4. how to fit with the local points?

first form a local linear model.  find the    that minimizes the 
locally weighted sum of squared residuals:

   

   argmin   
=

n

  

k

1
=

2

kw

(
y

k

   

x  
t

k

)2

then predict ypredict=  t xquery

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 34

17

how lwr works

1.
2.
x
12
x
22
:

x

n

2

x
11
x
21
:

x

n

1

   
   
   
   
   
   

find w to minimize 
  (yi-  wjtj(xi))2
directly: w=(xtx)-1xty

id75 not 
flexible but trains like 
lightning.

for each point (xk,yk) compute wk.
let wx = diag(w1,..wn)x
xw
l
1
12
xw
l
2
:
xw
nn

xw
1
11
xw
2
:
xw
nn

w
1
w
2
:
w
n

l
l

x
1
x

   

2
:

nd

x

   
   
   
   
   
   

   
   
   
   
   
   

l

l

22

21

d

d

2

1

    

d

d

xw
11
xw
22
:
xw
n

nd

- - >          wx

x
3. let wy=diag(w1,..wn)y, so that yk

(cid:198)wkyk

query

4.    = (wxtwx)-1(wxtwy)

locally weighted regression is 
very flexible and fast to train.
copyright    2001, 2005, andrew w. moore

instance-based learning: slide 35

input x matrix of inputs: x[k] [i] = i   th component of k   th input point.
input y matrix of outputs: y[k] = k   th output value.
input xq = query input.  input kwidth.
wxtwx = empty (d+1) x (d+1) matrix
wxtwy = empty (d+1) x 1        matrix
for ( k = 1 ; k <= n ; k = k + 1 )

/* compute weight of kth point  */
wk = weight_function( distance( xq , x[k] ) / kwidth )
/* add to (wx) ^t (wx) matrix */
for ( i = 0 ; i <= d ; i = i + 1 )

for ( j = 0 ; j <= d ; j = j + 1 )

if ( i == 0 ) xki = 1 else xki = x[k] [i]
if ( j == 0 ) xkj = 1 else xkj = x[k] [j]
wxtwx [i] [j] = wxtwx [i] [j] + wk * wk * xki * xkj

/*  add to (wx) ^t (wy) vector */
for ( i = 0 ; i <= d ; i = i + 1 )

if ( i == 0 ) xki = 1 else xki = x[k] [i]
wxtwy [i] = wxtwy [i] + wk * wk * xki * y[k]

/* compute the local beta.  call your favorite linear equation solver.  recommend cholesky 

decomposition for speed.  recommend singular val decomp for robustness. */

beta = (wxtwx)-1 (wxtwy)
ypredict = beta[0] + beta[1]*xq[1] + beta[2]*xq[2] +     beta[d]*xq[d]
copyright    2001, 2005, andrew w. moore

instance-based learning: slide 36

   
   
   
   
   
   

18

software and data for the algorithms in this tutorial: http://www.cs.cmu.edu/~awm/vizier . the 
example figures in this slide-set were created with the same software and data.

lwr on our test cases

kw = 1/16 of x-axis 
width.

kw = 1/32 of x-axis 
width.

kw = 1/8 of x-axis 
width.

copyright    2001, 2005, andrew w. moore

nicer and smoother, 
but even now, are 
the bumps justified, 
or is this overfitting?

instance-based learning: slide 37

software and data for the algorithms in this tutorial: http://www.cs.cmu.edu/~awm/vizier . the 
example figures in this slide-set were created with the same software and data.

locally weighted polynomial regression

kernel regression
kernel width kw at 
optimal level.

lw id75
kernel width kw at 
optimal level.

lw quadratic regression
kernel width kw at 
optimal level.

kw = 1/100 x-axis

kw = 1/40 x-axis

kw = 1/15 x-axis

local quadratic regression is easy: just add quadratic terms to the 
wxtwx matrix. as the regression degree increases, the kernel width 
can increase without introducing bias.

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 38

19

when   s quadratic better than linear?
    it can let you use a wider kernel without introducing bias.
    sometimes you want more than a prediction, you want an 
estimate of the local hessian. then quadratic is your friend!

    but in higher dimensions is appallingly expensive, and needs a 

lot of data. (why?)

    two    part-way-between-linear-and-quadratic    polynomials:

       ellipses   : add xi
(no xixjwhere i=j)

2terms to the model, but not cross-terms 

       circles   : add only one extra term to the model:

x

+ =
1

d

d

   

j

1
=

x

2
j

    incremental insertion of polynomial terms is well established in

conventional regression (gmdh,aim): potentially useful here too

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 39

multivariate locally weighted learning

s
t
u
p
n
i

locally
weighted
learner

s
t
u
p
t
u
o

all the methods described so far can generalize to 
multivariate input and output.  but new questions arise: 

(cid:131) what are good scalings for a euclidean distance metric?
(cid:131) what is a better euclidean distance metric?
(cid:131) are all features relevant?
(cid:131) do some features have a global rather than local influence?

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 40

20

software and data for the algorithms in 
this tutorial: 
http://www.cs.cmu.edu/~awm/vizier . the 
example figures in this slide-set were 
created with the same software and data.

a bivariate fit example

lwq regression

let   s graph the prediction 
surface given 100 noisy 
datapoints: each with 2 
inputs, one output
kernel width, number of fully 
weighted neighbors, distance 
metric scales all optimized.
kw = 1/16 axis width
4 nearest neighs full weight
distance metric scales each axis 
equally.

f(x,y) = sin(x) + sin(y) + 
noise

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 41

two more bivariate fits

locally weighted id75.
kw, num neighs, metric scales all 
optimized.
kw=1/50 x-axis width. no 
neighbors fully weighted. y not 
included in distance metric, but is 
included in the regression.
f(x,y) = sin(x*x)+y+noise

kernel regression.
kw, num neighs, metric scales all 
optimized.
kw=1/100 x-axis width. 1-nn fully 
weighted. y not included in 
distance metric.

f(x,y) = sin(x*x)

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 42

21

fabricated example

f(x1,x2,x3,x4,x5,x6,x7,x8,x9) = noise + x2 + x4 + 4sin(0.3x6 + 0.3x8).
(here we see the result of searching for the best metric, feature set, kernel width, 
polynomial type for a set of 300 examples generated from the above function)
recommendation.
based on the search results so far, the recommended function approximator 
encoding is l20:sn:-0-0-9-9. let me explain the meaning:

locally weighted regression.  the following features define the distance metric:

a gaussian weighting function is used with kernel width 0.0441942 in scaled 
input space.  we do a weighted least squares with the following terms:

x6   (full strength).
x8   (full strength).

term 0 = 1
term 1 = x2/10
term 2 = x4/10
term 3 = x6/10
term 4 = x8/10

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 43

locally weighted learning: variants
    range searching: average of all neighbors within a given 

    range-based id75: id75 on all 

points within a given range

    id75 on k-nearest-neighbors
    weighting functions that decay to zero at the kth nearest 

range

neighbor

    locally weighted iteratively reweighted least squares
    locally weighted id28
    locally weighted classifiers

    multilinear interpolation
    kuhn-triangulation-based interpolation
    spline smoothers

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 44

22

using locally weighted learning for 

modeling

       hands-off    non-parametric relation finding
    low dimensional supervised learning
    complex function of a subset of inputs
    simple function of most inputs but complex 

function of a few

    complex function of a few features of many 

input variables

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 45

use (1):    hands-off    non-parametric relation 
finding.
you run an hmo (or a steel tempering process) (or a 7-dof dynamic robot arm)
you want an intelligent assistant to spot patterns and regularities among pairs or 
triplets of variables in your database   

hmo variables:
physician age
patient age
charge/day
charge/discharge
discharges/100
icd-9 diagnosis
market share
mortality/100
patient zip
zip median age
   .
you especially want to find more than just the linear correlations   .

steel variables:
line speed
line spd -10mins
line spd -20mins
slab width
slab height
slab temp stg1
slab temp stg2
cooltunn2 setp
cooltunn5 sep
cooltunn2 temp
   .

robot variables:
roll
droll
ddroll
pitch
dpitch
ddpitch
sonarheight
laserheight
flighttime
thrustrate
   .

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 46

23

use (2): low dimensional supervised learning

you have lots of data, not many input variables (less than 7, say) and 
you expect a very complex non-linear function of the data.

s
t
u
p
n
i

function 
approximator

s
t
u
p
t
u
o

examples:

    skin thickness vs   ,   for face scanner
    topographical map
    tumor density vs (x,y,z)
    mean wasted aspirin vs (fill-target, mean-weight, weight-sdev, rate) for an 

aspirin-bottle filler

    object-ball collision-point vs (x,y,  ) in pool

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 47

use (3): complex function of a subset of inputs

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 48

24

use (4): simple function of most inputs but complex 

function of a few.

examples:
    f(x) = x1+ 3x2    x4+ sin(log(x5)*x6)    x7
    car engine emissions
    food cooling tunnel

2 + x8    x9 + 8x10

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 49

use (5): complex function of a few features of 

many input variables.

examples:
    mapping from acoustic signals to    id203 of machine 

breakdown   .

    time series data analysis.
    mapping from images to classifications.

 

i

s
l
e
x
p
e
g
a
m
i

r
o
s
s
e
c
o
r
p
e
r
p

i

r
o
t
a
m
x
o
r
p
p
a

 

n
o
i
t
c
n
u
f

s
t
u
p
t
u
o

    (e.g. product inspection, medical imagery, thin film imaging..)
instance-based learning: slide 50

copyright    2001, 2005, andrew w. moore

25

local weighted learning: pros & cons vs neural nets

local weighted learning has some advantages:

    can fit low dimensional, very complex, functions very accurately. neural 

nets require considerable tweaking to do this.

    you can get meaningful confidence intervals, local gradients back, not 

merely a prediction.

    training, adding new data, is almost free.
       one-shot    learning---not incremental
    variable resolution.
    doesn   t forget old training data unless statistics warrant.
    cross-validation is cheap

neural nets have some advantages:

    with large datasets, mbl predictions are slow (although kdtree 

approximations, and newer cache approximations help a lot).

    neural nets can be trained directly on problems with hundreds or

thousands of inputs (e.g. from images). mbl would need someone to 
define a smaller set of image features instead.

    nets learn incrementally.

copyright    2001, 2005, andrew w. moore

instance-based learning: slide 51

what we have covered

    problems of bias for unweighted regression, and noise-

fitting for    join the dots    methods

    nearest neighbor and k-nearest neighbor
    distance metrics
    kernel regression
    weighting functions
    stable kernel regression
    review of unweighted id75
    locally weighted regression: concept and implementation
    multivariate issues
    other locally weighted variants
    where to use locally weighted learning for modeling?
    locally weighted pros and cons
copyright    2001, 2005, andrew w. moore

instance-based learning: slide 52

26

