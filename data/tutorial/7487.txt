prediction and search in probabilistic worlds
markov systems, markov 
decision processes, and 
id145

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2002, 2004, andrew w. moore

april 21st, 2002

discounted rewards

an assistant professor gets paid, say, 20k per year.
how much, in total, will the a.p. earn in their life?
20 + 20 + 20 + 20 + 20 +     = infinity

$

$

what   s wrong with this argument?

copyright    2002, 2004, andrew w. moore

markov systems: slide 2

1

discounted rewards

   a reward (payment) in the future is not worth quite 
as much as a reward now.   

    because of chance of obliteration
    because of inflation

example:

being promised $10,000 next year is worth only 90% as 
much as receiving $10,000 right now.

assuming payment n years in future is worth only 
(0.9)n of payment now, what is the ap   s future 
discounted sum of rewards ?

copyright    2002, 2004, andrew w. moore

markov systems: slide 3

discount factors

people in economics and probabilistic decision-
making do this all the time.
the    discounted sum of future rewards    using 
discount factor       is

(reward now) +
   (reward in 1 time step) +
   2 (reward in 2 time steps) +
   3 (reward in 3 time steps) +

:
:       (infinite sum)

copyright    2002, 2004, andrew w. moore

markov systems: slide 4

2

the academic life

0.6

0.6

0.2

0.2

0.7

assistant

a.
prof
20

0.2

b.
assoc.
prof
60

0.2

tenured

t.
prof
400

0.3

c o u n t

 

s.

on the
street

dead

d.
0

define:

  

 

0.3

0.7

10

s u m e   d i s
=   0 . 9
a s
f a c t o r
ja = expected discounted future rewards starting in state a
jb = expected discounted future rewards starting in state b
                         t
jt =                                                                      
js =                                                                      
                         s
jd =                                                                      
                         d
how do we compute ja, jb, jt, js, jd ?

copyright    2002, 2004, andrew w. moore

markov systems: slide 5

computing the future rewards of an 

academic

copyright    2002, 2004, andrew w. moore

markov systems: slide 6

3

a markov system with rewards   
    has a set of states  {s1 s2      sn}
    has a transition id203 matrix

pij = prob(next = sj | this = si )

p11 p12      p1n

p=    p21
:
pn1

     pnn

    each state has a reward.  {r1 r2      rn }
    there   s a discount factor    .   0 <    < 1

on each time step    

0.  assume your state is si
1.  you get given reward ri
2.  you randomly move to another state

p(nextstate = sj | this = si ) = pij
3.  all future rewards are discounted by   

copyright    2002, 2004, andrew w. moore

markov systems: slide 7

solving a markov system

write   j*(si) = expected discounted sum of future 
rewards 
j*(si) = ri +    x (expected future rewards starting from your next state)

starting in state si

= ri +   (pi1j*(s1)+pi2j*(s2)+        pinj*(sn))

using vector notation write

j*(s1)
j*(s2) 
j*(sn) 

j=    :

r=

r1
r2
:
rn

p=      :

p11 p12      p1n
p21     .
pn1 pn2      pnn

question: can you invent a closed form expression for 
j in terms of r p and    ?

copyright    2002, 2004, andrew w. moore

markov systems: slide 8

4

solving a markov system with matrix 

inversion

    upside:   you get an exact answer

    downside:

copyright    2002, 2004, andrew w. moore

markov systems: slide 9

solving a markov system with matrix 

inversion

    upside:   you get an exact answer

    downside: if you have 100,000 states you   re 

solving a 100,000 by 100,000 system of 
equations.

copyright    2002, 2004, andrew w. moore

markov systems: slide 10

5

value iteration: another way to solve 

a markov system

define  
j1(si) = expected discounted sum of rewards over the next 1 time step.
j2(si) = expected discounted sum rewards during next 2 steps
j3(si) = expected discounted sum rewards during next 3 steps

jk(si) = expected discounted sum rewards during next k steps

j1(si) =  

j2(si) =

:

jk+1(si) =

(what?)

(what?)

(what?)

copyright    2002, 2004, andrew w. moore

markov systems: slide 11

value iteration: another way to solve 

a markov system

define  
j1(si) = expected discounted sum of rewards over the next 1 time step.
j2(si) = expected discounted sum rewards during next 2 steps
j3(si) = expected discounted sum rewards during next 3 steps

:

:

jk(si) = expected discounted sum rewards during next k steps

n = number of states

j1(si) = ri

j2(si) =

:

r
i

+

jk+1(si) =

r
i

+

n

   
  

j
1
=
n

   
  

j

1
=

copyright    2002, 2004, andrew w. moore

1

sjp
(
ij

j

)

jp
ij

k

(

s

)

j

(what?)

(what?)

(what?)

markov systems: slide 12

6

let   s do value iteration

1/2

wind
(cid:51)
0

1/2

1/2

1/2

hail
.::.:.::
-8

   = 0.5

1/2

jk(sun)

jk(wind)

jk(hail)

sun
(cid:53)
+4

1/2

k
1
2
3
4
5

copyright    2002, 2004, andrew w. moore

markov systems: slide 13

let   s do value iteration

sun
(cid:53)
+4

1/2

1/2

wind
(cid:51)
0

1/2

1/2

1/2

hail
.::.:.::
-8

   = 0.5

1/2

k
1
2
3
4
5

jk(sun)

jk(wind)

jk(hail)

4
5
5

4.94
4.88

0
-1

-1.25
-1.44
-1.52

-8
-10

-10.75

-11

-11.11

copyright    2002, 2004, andrew w. moore

markov systems: slide 14

7

value iteration for solving markov 

systems

    compute j1(si) for each j
    compute j2(si) for each j

:

    compute jk(si) for each j
as   k       jk(si)   j*(si) .  why?

when to stop?  when

max    jk+1(si)     jk(si)     <    

i

this is faster than matrix inversion (n3 style)
if the transition matrix is sparse

copyright    2002, 2004, andrew w. moore

markov systems: slide 15

a markov decision process
   = 0.9

1

s

you run a 
startup 
company.
in every 
state you 
must 
choose 
between 
saving 
money or 
advertising.

poor &
unknown

+0

1/2

a

1/2

1/2

1/2

1/2

1/2

s
a
rich &
unknown

+10

1

poor &
famous

a

+0

s
1/2

1/2

1

a

1/2

1/2

s

rich &
famous

+10

copyright    2002, 2004, andrew w. moore

markov systems: slide 16

8

id100

an mdp has   
    a set of states    {s1        sn}
    a set of actions   {a1        am}
    a set of rewards   {r1        rn} (one for each state)
    a transition id203 function

p
k
ij

=

prob

(
next

=

j

this

=

i
and 

 use i 

)k
action 

on each step:

0.  call current state si
1.  receive reward ri
2.  choose action     {a1        am}
3.  if you choose action ak you   ll move to state sj with  

id203

k
ijp

4.  all future rewards are discounted by   

copyright    2002, 2004, andrew w. moore

markov systems: slide 17

a policy

a policy is a mapping from states to actions.
examples

1

1

pf

0

a

1

1/2

1/2

:

1
 
r
e
b
m
u
n
 
y
c

i
l

o
p

:

2
 
r
e
b
m
u
n
 
y
c

i
l

o
p

state     action

pu
pf
ru
rf

s
a
s
a

state     action

pu
pf
ru
rf

a
a
a
a

s
pu

0

s
ru

+10

1/2

pu

0

ru

10

a

1/2

a

    how many possible policies in our example?
    which of the above two policies is best?
    how do you compute the optimal policy?
copyright    2002, 2004, andrew w. moore

1/2

1/2

a

rf

+10

1

1

pf

0

rf

10

a

a

markov systems: slide 18

9

interesting fact

for every m.d.p. there exists an optimal 
policy.

it   s a policy such that for every possible start 
state there is no better option than to follow 
the policy.

(not proved in this 
lecture)

copyright    2002, 2004, andrew w. moore

markov systems: slide 19

computing the optimal policy

idea one:

run through all possible policies.
select the best.

what   s the problem ??

copyright    2002, 2004, andrew w. moore

markov systems: slide 20

10

optimal value function
define j*(si) =  expected discounted future 

1

1

a

1/2

+3

rewards, starting from state si, 
assuming we use the optimal policy
b
s2

0

question
what (by inspection) is 
an optimal policy for that 
mdp?

1/2
a

b

s1

+0

1/3

1/3

1/3

a

1/2

b

s3

+2

1

1/2

what is j*(s1) ?
what is j*(s2) ?
what is j*(s3) ?

copyright    2002, 2004, andrew w. moore

(assume    = 0.9)

markov systems: slide 21

computing the optimal value 
function with value iteration

define

jk(si) = maximum possible expected 
sum of discounted rewards i 
can get if i start at state si and i 
live for k time steps.

note that  j1(si) = ri

copyright    2002, 2004, andrew w. moore

markov systems: slide 22

11

let   s compute jk(si) for our example

jk(pu)

jk(pf)

jk(ru)

jk(rf)

k

1
2
3
4
5
6

copyright    2002, 2004, andrew w. moore

markov systems: slide 23

let   s compute jk(si) for our example

k

1
2
3
4
5
6

jk(pu)

jk(pf)

jk(ru)

jk(rf)

0
0

2.03
3.852
7.22
10.03

0
4.5
6.53
12.20
15.07
17.65

10
14.5
25.08
29.63
32.00
33.58

10
19

18.55
19.26
20.40
22.43

copyright    2002, 2004, andrew w. moore

markov systems: slide 24

12

bellman   s equation
)      
   
j

(
sjp
nk
ij

n
  
   +
j
1
=

max

(
s
i

   
      

r
i

=

)

1
+

n

k

j

value iteration for solving mdps

    compute j1(si) for all i
    compute j2(si) for all i
   
    compute jn(si) for all i

:

   ..until converged
max

converged

 when 

i

   
      

n

1
+

j

(
s
i

)

   

(
sj
n

   

   
)
  i
      

   also known as

id145

copyright    2002, 2004, andrew w. moore

markov systems: slide 25

finding the optimal policy

1. compute j*(si) for all i using value 

iteration (a.k.a. id145)

2. define the best action in state si as
   
)   
   

(
+        
k
sjp
  
ij

max

arg

   
   
   

r
i

k

j

j

(why?)

copyright    2002, 2004, andrew w. moore

markov systems: slide 26

13

applications of mdps

this extends the search algorithms of your first 
lectures to the case of probabilistic next states.
many important problems are mdps   .

    robot path planning
    travel route planning
    elevator scheduling
    bank customer retention
    autonomous aircraft navigation
    manufacturing processes
    network switching & routing

copyright    2002, 2004, andrew w. moore

markov systems: slide 27

asynchronous d.p.

value iteration:

   backup s1   ,    backup s2   ,             backup sn   ,  
then    backup s1   ,    backup s2   ,         
repeat  :

:        there   s no reason that you need to do the 

backups in order!

random order    still works.  easy to parallelize (dyna, 

sutton 91)

on-policy order

efficient order

simulate the states that the system actually visits.

e.g. prioritized sweeping  [moore 93]

q-dyna  [peng & williams 93]

copyright    2002, 2004, andrew w. moore

markov systems: slide 28

14

policy iteration

another way to compute 

optimal policies

write   (si) = action selected in the i   th state.  then    is a policy.
write   t = t   th policy on t   th iteration
algorithm:

     = any randomly chosen policy

r
i

   
      

arg

max
a

  
   +
j

(
sjp
a
ij

   i compute j  (si) = long term reward starting at si using     
  1(si) =
j1 =    .
  2(si) =    .
    keep computing   1 ,   2 ,   3    . until   k =   k+1 .  you now have 

)      

   

o

j

an optimal policy.

copyright    2002, 2004, andrew w. moore

markov systems: slide 29

policy iteration & value iteration: 

which is best ???

it depends.

lots of actions?  choose policy iteration
already got a fair policy? policy iteration
few actions, acyclic?   value iteration

best of both worlds:

modified policy iteration   [puterman]

   a simple mix of value iteration and policy iteration

3rd approach
id135

copyright    2002, 2004, andrew w. moore

markov systems: slide 30

15

time to moan

what   s the biggest problem(s) with what we   ve 
seen so far?

copyright    2002, 2004, andrew w. moore

markov systems: slide 31

dealing with large numbers of states

don   t use a table   

(generalizers)                                                  

use   

state

value

s1

s2

:

s15122189

(hierarchies)

splines

a function 
approximator

variable resolution

[munos 1999]

multi resolution

state

value

memory
based

copyright    2002, 2004, andrew w. moore

markov systems: slide 32

16

function approximation for value 

functions

polynomials               [samuel, boyan, much o.r.

literature]

neural nets               [barto & sutton, tesauro, 

crites, singh, tsitsiklis]

backgammon, pole 
balancing, elevators, 
tetris, cell phones

checkers, channel 

routing, radio therapy

splines

economists, controls

downside:     all convergence guarantees disappear.

copyright    2002, 2004, andrew w. moore

markov systems: slide 33

memory-based value functions
j(   state   ) = j(most similar state in memory to    state   )

or

average   j(20 most similar states)

or

weighted average  j(20 most similar states)
[jeff peng, atkenson & schaal,
geoff gordon,        proved stuff
scheider, boyan & moore  98]

   planet mars scheduler   

copyright    2002, 2004, andrew w. moore

markov systems: slide 34

17

hierarchical methods

continuous state space:

discrete space:
chapman & kaelbling 92, 
mccallum 95 (includes 
hidden state)

a kind of decision 
tree value function

multiresolution

   split a state when statistically 
significant that a split would 
improve performance   

continuous space
e.g. simmons et al 83, chapman 
& knelbling 92, mark ring 94    , 
munos 96

with interpolation!

   prove needs a higher 
resolution   
moore 93, moore & 
atkeson 95

a hierarchy with high level    managers    abstracting low level    servants   
many o.r. papers, dayan & sejnowski   s feudal learning, dietterich 1998 (max-q 
hierarchy) moore, baird & kaelbling 2000 (airports hierarchy)
copyright    2002, 2004, andrew w. moore

markov systems: slide 35

what you should know

    definition of a markov system with discounted 

rewards

    how to solve it with matrix inversion
    how (and why) to solve it with value iteration
    definition of an mdp, and value iteration to solve 

an mdp

    policy iteration
    great respect for the way this formalism 

generalizes the deterministic searching of the start 
of the class

    but awareness of what has been sacrificed.

copyright    2002, 2004, andrew w. moore

markov systems: slide 36

18

