looking	for	
the	missing	signal

l  on	bottou
facebook	ai	research

in	search	for	lost	signal

l  on	bottou
facebook	ai	research

people

martin	arjovsky
nyu

l  on	bottou
facebook	ai	research

soumith chintala
facebook	ai	research

david	lopez-paz
facebook	ai	research

robert	nishihara
berkeley
maxime oquab
inria

alex	peysakhovich
facebook	ai	research

bernhard	sch  lkopf
mpi	t  bingen

motivation	

machine	learning	success	stories

  recognizing	objects	in	images

after	training	on	more	images	than	a	human	can	see.

   translating	natural	languages	(somehow)	

after	training	on	more	text	than	a	human	can	read.

   playing	atari	games	

after	playing	more	games	than	any	teenager	can	endure.

   playing	go	(famously)	

after	playing	more	grandmaster	level	games	than	mankind.

what	are	we	doing	wrong?

are	our	learning	algorithms	so	inefficient?
  hard	to	say	for	the	most	complex	learning	systems.
  for	simpler	systems,	in	the	absence	of	a	strong	prior,
the	cramer-rao	bound	suggests	that	this	is	not	the	case.

transfer	learning?

does	transfer	learning	give	strong	enough	priors?
   transfer	learning	works	well	across	similar	tasks.
   transfer	learning	across	all	human	experiences	is	hypothetical.
   could	there	be	something	else?

another	viewpoint

is	there	more	signal	in	data	than	we	think?
   where	to	find	it?

typical	supervised	machine	learning	systems	use

                 			        				    [    	        ]
                ,    

what	about

   beyond	correlations   

   causation
   complex	moments

causation	and	moments

causal	confounding
    =        +    (   s<,s<)
    

x

y

    =        +        +	    (   s?,s?)

    
    
    	~ bernoulli,	    =<?

z

simpson	effect

   suppose	we	only	observe	    ,    .
   if	    <0 and	    +        >0,
then		            (    ,    )>0

causal	confounding

look	at	the	scatterplot!

more	scatterplots

the	hertzsprung   russell	diagram	
shows	the	relationship	between	
the	stars'	absolute	magnitudes	or	
luminosities	versus	their	stellar	
classifications	or	effective	
temperatures.
scientists	clearly	draw	causal	
conclusions	from	a	scatterplot,	
even	when	interventions	are	
impossible.

causal	problems	with	two	variables

given	two	observed	variables	    ,    
either	     causes	    ,
or	     causes	    ,
iii. or	     and	     have	unobserved	common	causes,
iv. or	     and	     are	independent.	

i.
ii.

reichenbach

potentially	confounding

let   s	focus	on	causal	direction	detection	(i	and	ii)

how	does	causal	direction	look	like?

in	this	scatter	plot
   x	is	altitude.
   y	is	average	temperature.
does	the	scatter	plot	reveal	whether
   x	causes	y	
   or	y	causes	x	?

	    =    	    +    +                    

footprint	example	1	    additive	noise

sometimes	the	high	moments	(the	corners)	reveal	something.

(peters et al.,	14)

footprint	example	2	-- coincidences

    

    

(janzing et al.,	2011)

from	scatterplot	to	causation	direction

detecting	causation	direction	at	scale
  we	could	build	a	long	list	of	causal	footprint	examples,	then	decide	
which	example	is	most	appropriate	for	a	given	scatterplot,	etc.
  or	we	can	construct	a	classifier...

(lopez-paz,	et al.,	2015)

for	well	chosen	     and	    .			
   	   o with		    .

,    . o=    (.,.)

featurizing a	scatterplot
high	moments?

      gh= <i	j     kg    kh
ikl<
      = <ij     (    k,    k)
ikl<
      r= <ij     r(    k,    k)
ikl<

reproducing	kernel	hilbert	space?

learning	the	features	and	the	classifier

neural	causation	classifier

    r

    t

training	ncc
we	do	not	have	access	to	large	causal	direction	datasets
but	we	can	generate	artificial	scatterplots.

    =         +            

   draw	    ~    1,2,3,4,5			r,s~    [0,5]
   take	a	mixture	of	     gaussians	with	    ~    0,     and	    ~    0,    

step	1	- draw	distribution	on	x		

training	ncc

step	2 - draw	mechanism	f
   cubic	spline	with	random	number	of	random	knots   
step	3	- draw	noise

   noise	     is	gaussian	with	random	variance	~    [0,5]
   function	    (    ) is	another	cubic	spline	with	random	knots.
step	4	    generate	causal	scatter	plot	           
   draw	    k,    k then	compute	    k=        k +        k    k
   rescale	    k,    k to	enforce	marginal	mean	0	and	sdev 1

training	ncc
step	6	    generate	training	examples

   scatterplot	     k,    k
   scatterplot	     k,    k

is	associated	with	target	label	1
is	associated	with	target	label	0

repeat	100000	to	generate	a	training	set.
train	the	neural	network	classifier	with	the	usual	bag	of	tricks.
(dropout	id173,	rmsprop,	cross-validation,	etc.)

sanity	check
   after	training	on	artificial	data,	ncc	achieves	state-of-the-art		[79%]	
performance on	the	t  bingen cause-effect	dataset   ,	which	contains	100	
cause-effect	pairs	(https://webdav.tuebingen.mpg.de/cause-effect)

remarks

   this	works	also	for	detecting	confounding	variables. how	to	validate	that?	
  two-dimensional	scatterplots	are	limited   

finding	
a	causal	signal
in	static	images

(lopez-paz,	nishihara,	chintala,	schoelkopf,	bot tou,	to	appeat in	cvpr17)

counterfactual	on	images

asymmetric	relation
   how	would	this	image	would	

have	looked	like	if	one	had	
removed	the	cars?

   how	would	this	image	would	

have	looked	like	if	one	had	
removed	the	bridge?

can	we	use	image	datasets	to	
identify	the	causal	dispositions
of	object	categories?
how	to	validate	a	result?

image	datasets

images	labeled	with	
  object	of	interests	(cat,	dog,	   )
  bounding	boxes.

the	pascal	voc	dataset	contains	20	
categories,	11541	images
the	coco	dataset	is	much	larger.	after	
restricting	to	the	same	categories	than	
pascal	voc,	we	have	99309	images.

featurizing the	images

all	images	are	preprocessed	using	a	state-of-the-art	pretrained id98.
each	image	is	then	represented	by	a	vector	of	512	features.

features	scores	are	often	interpretable

features	scores	
are	often	interpretable	
as	features	of	the	scene.

(zeiler &	fergus,	2013)

causal	and	anti-causal	features

for	each	object	category,	we	can	also	define	two	sets	of	features
  the	causal	features	are	those	that	cause	the	presence	of	the	object	of	
interest.	if	the	object	of	interest	had	not	been	present	in	the	image,	these	
feature	would	still	have	appeared.
  the	anticausal	features	are	those	that	are	caused	by	the	presence	of	the	
object	of	interest.	 if	the	object	of	interest	had	not	been	present	in	the	
image,	these	feature	would	not	have	appeared.

causal	and	anti-causal	features
if	x	and	f	are	positively	correlated,
a	trained	classifier	may	rely	on	score(f).

this	correlation	may	occur	because
   x	   f

(anticausal	feature)

example	:	f	=	presence	of	wheels.

event	x

presence	of	a	
car	in	the	image

   f	   x

(causal	feature)

example:	f	=	presence	of	road.

   f	   c	   x (something	else)
example:	f	=	bike,	c=street

event	f

presence	of	a	

feature

proxy	variables

assume	there	is	
a	causal	footprint	in	the
distribution	of	variables	that	
represent the	presence	of	
an	object	or	a	feature

we	hope	to	see	a	similar
footprint	between	the
scores	computed	by	a	
well	tuned	classifier.

empirically	identifying	
causal	and	anti-causal	features

  we	apply	ncc	to	the	feature	scores	and	object	scores	to	identify	the	
top	1%	causal	and	anticausal	features		for	each	of	the	twenty	
categories.

   ncc	was	trained	using	artificial	data	only	(not	image	specific)
   the	same	ncc	classifier	is	used	for	all	categories.

computer	vision	   	statistics
context	features	vs	object	features

car	examples	in	id163

is	this	less	of	a	car

because	the	context	is	wrong?

object	features	and	context	features

in	computer	vision,	one	is	often	interested	in	another	distinction
  the	object	features	   belong   	to	the	object	and	are	
most	often	activated	inside	the	object		bounding	box.

example:	car	wheels,	person	eyes,	etc.

  the	context	features	are	those	most	often	
activated	outside	the	bounding	box.

example:	road	under	a	car,		car	shadow

empirically	identifying	
object	and	context-features
since	we	know	the	bounding	boxes,	we	can	observe	how	the	feature	
values	change	when	we	black	out	image	parts.		averaging	and	normalizing	
these	variations	gives	us	the	object-feature	ratio and	context-feature	ratio.	

hypotheses

we	expect	this	because	anticausal	features	should	often	be	features	of	subparts	
of	the	object,	likely	to	be	contained	in	the	bounding	box.	context	features	may	
cause	or	be	caused	by	the	presence	of	an	object	(e.g.,	the	shadow	of	a	car).		

hypotheses

   

results

  top	anticausal	features	have	higher	object	scores	for	all	twenty	categories.
  the	id203	that	this	happens	out	of	chance	is		2-20   	10-6.

results

no	clear	relation	between	top	causal	features	and	context	scores.

more	information

  the	effect	disappears	completely	if	we	replace	ncc	by	the	correlation	
coefficient	(or	its	absolute	value)	between	the	feature	and	the	category.
  the	effect	appears	to	be	robust	to	many	details	of	the	experiment	such	as	
the	precise	composition	of	the	ncc	data,	the	precise	computation	of	
object/context	scores,	the	methods	we	use	to	determine	a	continuous	
proxy	for	the	categories,	etc.

causal	signal	in	images
  we	have	indirectly	shown	that	high	order	statistics	in	image	datasets	can	
inform	us	about	causation	in	the	scenes.	to	our	knowledge,	no	prior	work	
has	established	or	even	considered	the	existence	of	such	a	signal.
  we	don   t	know	how	to	use	it.
  our	detection	method	is	cumbersome.

but	there	is	signal.

on	the	uses	of	a
wasserstein(ish)	
distance

(arjovsky,	bottou,	iclr	2017)
(arjovsky,	chintala,	bottou,	submitted).

the	   mythical   	unsupervised	learning

  this	is	not	about	using	unlabeled	data	to	
discover	id203	ratios.
  this	is	about	using	unlabeled	data	to	
discover	the	(causal)	generating	mechanism.
  causal	footprints	

   corners,	cliffs,	shocks,	   
   low	dimensional	causal	models

the	generator	approach	(vae,	gan,	   )

observed	data

    	~	    g (unknown)
    ~    g	(known)

typically	low	dim

    e

	

t
o
b
e
	
c
o
m
p
a
r
e
d

generated	data

    e(    )	~	    f (parametric)

low	dim	support
  cliff	shaped	   density   

comparing	distributions

requires densities, asymmetric, possibly infinite

vae

comparing	distributions

symmetric, does not require densities, 0              log	(2)

always defined, involves metric on underlying space.

generative	adversarial	network

    ~    g	

    e

    e(    )	~	    f
    	~	    g

    o

gan

	    g or		    f ?	

discriminator	maximizes	and	generator	minimizes

generative	adversarial	network

discriminator	maximizes	and	generator	minimizes

nasty	saddle	point	problem

    keeping	the	discriminator	optimal	:

mine     (           ,    ) minimizes		            g,    f
maxo     (    ,       (    )) yields	garbage

    keeping	the	generator	optimal

problem	with	gan	training
if	one	trains	the	discriminator	thoroughly,	the	generator	receives	no	gradient   

alternate	gan	training

distributions	with	
low	dimensional	support

let	  y and	  e be	two	
by	distance	    .
    

uniform	distributions	
supported	by	parallel	
line	segments	separated	

continuous	in	    

optimizing	a	wasserstein(ish)	distance

wasserstein-1	has	a	simple	dual	formulation	(kantorovich)

   parametrize	    (    ) ,	for	instance	with	a	neural	network.
   maintain	    (    ) well	trained,	and	train	    e(    )	by	back-prop	through	    (    ).

   enforce	lipschitz	constraint,	for	instance	by	aggressively	clipping	the	weights.

   no	vanishing	gradients!

no	vanishing	gradients

theorem

note:	expectations

wgan	loss	correlates	
with	sample	quality

gan	loss	does	not correlate
with	sample	quality

wgan	is	less	sensitive	
to	modeling	choices

wgan	is	less	sensitive	
to	modeling	choices

wgan	is	less	sensitive	
to	modeling	choices

wgan
   many	authors	have	advocated	using	w	distance	to	estimate	densities.

(rozasco et	al,	2012,		cuturi et	al,	2015,	   )

   maximum	mean	discrepancy	

and	the	idea	to	parametrize	     in	order	to	obtain	a	fast	algorithm.	

   our	originality	is	a		focus	on	continuous	distributions	with	low	dim	support,

(gretton et	al,	2012)

conclusion

in	search	for	lost	signal

   there	is	a	causal	signal	in	the	high	moments.
   it	takes	the	form	of	cliffs,	corners,	shocks,	etc.
   this	has	everything	to	do	with	the	mythical	unsupervised	learning
   weak	distribution	distances	such	as	wasserstein	seem	more	able	to	catch	it.
   this	is	just	a	beginning.

