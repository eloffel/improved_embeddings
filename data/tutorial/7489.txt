learning with 

maximum likelihood

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2001, 2004, andrew w. moore

sep 6th, 2001

maximum likelihood learning of 

gaussians for data mining

    why we should care
    learning univariate gaussians
    learning multivariate gaussians
    what   s a biased estimator?
    bayesian learning of gaussians

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 2

1

why we should care

    id113 is a very 
very very very fundamental part of data 
analysis. 

       id113 for gaussians    is training wheels for 

our future techniques

    learning gaussians is more useful than you 

might guess   

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 3

learning gaussians from data
    suppose you have x1, x2,     xr~ (i.i.d) n(  ,  2)
    but you don   t know   

(you do know   2)

id113: for which    is x1, x2,     xrmost likely?

map: which    maximizes p(  |x1, x2,     xr,   2)?

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 4

2

learning gaussians from data

    suppose you have x1, x2,     xr~(i.i.d) n(  ,  2)
    but you don   t know   

sneer

(you do know   2)

id113: for which    is x1, x2,     xrmost likely?

map: which    maximizes p(  |x1, x2,     xr,   2)?

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 5

learning gaussians from data

    suppose you have x1, x2,     xr~(i.i.d) n(  ,  2)
    but you don   t know   

sneer

(you do know   2)

id113: for which    is x1, x2,     xrmost likely?

map: which    maximizes p(  |x1, x2,     xr,   2)?

despite this, we   ll spend 95% of our time on id113. why? wait and see   

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 6

3

id113 for univariate gaussian
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  2)
    but you don   t know    (you do know   2)
    id113: for which    is x1, x2,     xrmost likely?

id113

 
  

=

arg

max
  

xxp
,
(

1

2

,...

x

r

|

2
    

,

)

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 7

algebra euphoria
xxp
(
,

2
    

,...

x

)

,

|

1

2

r

max
  

id113

 
  

=

arg

=

=

=

=

(by i.i.d)

(monotonicity of 
log)

(plug in formula 
for gaussian)

(after 
simplification)

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 8

4

id113

 
  

=

arg

algebra euphoria
xxp
(
,

2
    

,...

x

)

,

|

1

2

r

max
  

ixp
(

|

2)
    

,

(by i.i.d)

=

=

=

=

arg 

max
  

arg 

max
  

arg 

r

max
  
log

   
i
1
=
ixp
|
(

r

   

i

1
=

1
 2
    
r
   

min
  

1
=

i

2 )
    

,

r

   

i

1
=

   

2

(

ix
)
  
   
2
2
  

arg 

(

ix

   

2)
  

(monotonicity of 
log)

(plug in formula 
for gaussian)

(after 
simplification)

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 9

intermission: a general scalar 

id113 strategy

task: find id113    assuming known form for p(data|   ,stuff)
1. write ll = log p(data|   ,stuff)
2. work out    ll/      using high-school calculus
3.

set    ll/     =0 for a maximum, creating an equation in 
terms of   
solve it*

4.
5. check that you   ve found a maximum rather than a 

minimum or saddle-point, and be careful if    is 
constrained

*this is a perfect example of something that works perfectly in 
all textbook examples and usually involves surprising pain if 
you need it for something new.

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 10

5

the id113   
xxp
(
,
)

2
    

,...

x

,

|

1

2

r

id113

 
  

=

arg

max
  

=

arg

min
  

=

 
  

s.t.

0

=

r

   

i

(

ix

1
=
ll
   
  
   

   

2)
  

=

= (what?)

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 11

the id113   
xxp
(
,
)

2
    

,...

x

,

|

1

2

r

id113

 
  

=

arg

max
  

=

arg

min
  

r

   

i

(

ix

   

2)
  

=

 
  

s.t.

0

=

1
=
ll
   
  
   

=

ix
2)
(   

   

(2

ix

   

)
  

    r
   
  
   
r
   

1
=

   

i

i
1
=
1
r
   
r 1
i
=

ix

thus   
=

   

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 12

6

lawks-a-lawdy!

id113

   

=

1
r
   
r 1
i
=

x

i

    the best estimate of the mean of a 

distribution is the mean of the sample!

at first sight:

this kind of pedantic, algebra-filled and 
ultimately unsurprising fact is exactly the 

reason people throw down their 

   statistics    book and pick up their    agent 

based evolutionary data mining using 

the neuro-fuzz transform    book.

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 13

a general id113 strategy
suppose    = (  1,   2,    ,   n)t is a vector of parameters.
task: find id113    assuming known form for p(data|   ,stuff)
1. write ll = log p(data|   ,stuff)
2. work out    ll/      using high-school calculus

ll
   
  
   

=

ll
   
  
   
1
ll
   
  
   
2
m
ll
   
n  
   

   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 14

7

a general id113 strategy
suppose    = (  1,   2,    ,   n)t is a vector of parameters.
task: find id113    assuming known form for p(data|   ,stuff)
1. write ll = log p(data|   ,stuff)
2. work out    ll/      using high-school calculus
3.

solve the set of simultaneous equations

ll
   
  
   
1
ll
   
  
   

2
m
ll
   
n  
   

=

0

=

0

=

0

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 15

a general id113 strategy
suppose    = (  1,   2,    ,   n)t is a vector of parameters.
task: find id113    assuming known form for p(data|   ,stuff)
1. write ll = log p(data|   ,stuff)
2. work out    ll/      using high-school calculus
3.

solve the set of simultaneous equations

ll
   
  
   
1
ll
   
  
   

2
m
ll
   
n  
   

=

0

=

0

=

0

4. check that you   re at 

a maximum

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 16

8

a general id113 strategy
suppose    = (  1,   2,    ,   n)t is a vector of parameters.
task: find id113    assuming known form for p(data|   ,stuff)
1. write ll = log p(data|   ,stuff)
2. work out    ll/      using high-school calculus
3.

solve the set of simultaneous equations

if you can   t solve them, 

what should you do?

ll
   
  
   
1
ll
   
  
   

2
m
ll
   
n  
   

=

0

=

0

=

0

4. check that you   re at 

a maximum

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 17

id113 for univariate gaussian
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  2)
    but you don   t know    or   2
    id113: for which    =(  ,  2) is x1, x2,   xrmost likely?

log

 

xxp
(
,

1

2

,...

x

r

|

2
    

,

)

   =

r

(log

  

+

1
2

log

2
  

)

   

1
2
2
  

r

   

i

1
=

(

x

i

)
   
  

2

r

=

ll
1
   
2
    
   
ll
   
2
  
   

   =

)
  
   

(

ix

   
i
1
=
r
1
2
4
    

+

2

2

r

   

i

1
=

(

ix

)
  
   

2

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 18

9

id113 for univariate gaussian
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  2)
    but you don   t know    or   2
    id113: for which    =(  ,  2) is x1, x2,   xrmost likely?

log

 

xxp
(
,

1

2

,...

x

r

|

2
    

,

)

   =

r

(log

  

+

1
2

log

2
  

)

   

1
2
2
  

r

   

i

1
=

(

x

i

)
   
  

2

0

=

0

   =

)
  
   

r

(

ix

1
      
2
i
1
=
r
1
2
4
    

+

2

2

r

   

i

1
=

(

ix

)
  
   

2

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 19

id113 for univariate gaussian
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  2)
    but you don   t know    or   2
    id113: for which    =(  ,  2) is x1, x2,   xrmost likely?
log

xxp
,
(

2
    

(log

)
  
   

log

   =

2
  

,...

  

r

   

x

x

(

)

)

,

 

r

|

2

1
2
2
  

   

i

1
=

i

1

1
2
  

r

2

r

i

(

x

   
i
1
=
r
1
2
4
    

+

2

2

0

=

0

   =

=      
    

)

r

   

i

1
=

(

ix

+

1
2
   

x

r

1
r
1
=
)
      
  

2

i

i

what?

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 20

10

id113 for univariate gaussian
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  2)
    but you don   t know    or   2
    id113: for which    =(  ,  2) is x1, x2,   xrmost likely?

id113

  

=

1
r
   
r 1
i
=

x

i

2
  
id113

=

1
r

r

   

i

1
=

(

x

i

id113

  
   

2

)

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 21

unbiased estimators

    an estimator of a parameter is unbiased if the 

expected value of the estimate is the same as the 
true value of the parameters. 

    if x1, x2,     xr~(i.i.d) n(  ,  2) then

e

id113

[
  

]

=

e

1
r

   
   
   

r

   

i

1
=

x

i

   
=   
   

  

  id113is unbiased

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 22

11

biased estimators

    an estimator of a parameter is biased if the 

expected value of the estimate is different from
the true value of the parameters. 
    if x1, x2,     xr~(i.i.d) n(  ,  2) then
[
2
  
id113

   
  

e

e

e

]

=

id113

x

x

(

)

r

r

2

2

i

1
r

   
   
   

   

i

1
=

   
=   
   

   
   
   
   

1
r

   
      
   

   

i

1
=

i

r

x

   

1
r

   

   
   
      
   
   
   
   
id113is biased

1
=

j

j

  2

   

2
  

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 23

id113 variance bias

    if x1, x2,     xr~(i.i.d) n(  ,  2) then

[
2
  
id113

]

e

=

e

   
   
   
   

1
r

   
      
   

r

   

i

1
=

x

i

   

1
r

r

   

j

1
=

x

j

2

   
      
   

   
   
   
   

=

       
1
   
   

1
r

2

   
2
    
   
   

   

intuition check: consider the case of r=1
why should our guts expect that   2
underestimate of true   2?
how could you prove that?

id113 would be an 

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 24

12

unbiased estimate of variance

    if x1, x2,     xr~(i.i.d) n(  ,  2) then

[
2
  
id113

]

e

=

e

   
   
   
   

1
r

   
      
   

r

   

i

1
=

x

i

   

1
r

r

   

j

1
=

x

j

2

   
      
   

   
   
   
   

=

       
1
   
   

1
r

2

   
2
    
   
   

   

so define

2
  
unbiased

=

2
  
id113
1
       
1
   
r
   

   
   
   

 so

e

[
2
  
unbiased

]

=

2
  

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 25

unbiased estimate of variance

    if x1, x2,     xr~(i.i.d) n(  ,  2) then

[
2
  
id113

]

e

=

e

   
   
   
   

1
r

   
      
   

r

   

i

1
=

x

i

   

1
r

r

   

j

1
=

x

j

2

   
      
   

   
   
   
   

=

       
1
   
   

1
r

2

   
2
    
   
   

   

so define

2
  
unbiased

=

2
  
id113
1
       
1
   
r
   

   
   
   

 so

e

[
2
  
unbiased

]

=

2
  

2
  
unbiased

=

1
   

1

r

r

   

i

1
=

(

ix

id113

  
   

2

)

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 26

13

unbiaseditude discussion

    which is best?

2
  
id113

=

1
r

r

   

i

1
=

(

x

i

id113

  
   

2

)

2
  
unbiased

=

1
   

1

r

r

   

i

1
=

(

ix

id113

  
   

2

)

answer:
   it depends on the task
   and doesn   t make much difference once r--> large

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 27

don   t get too excited about being 

unbiased

    assume x1, x2,     xr~(i.i.d) n(  ,  2)
    suppose we had these estimators for the mean

r

   

i

1
=

suboptimal

  

=

1
7

r

+

r

crap =  

1x

x

i

are either of these unbiased?
will either of them asymptote to the 
correct value as r gets large?
which is more useful?

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 28

14

id113 for m-dimensional gaussian
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    but you don   t know    or   
    id113: for which    =(  ,  ) is x1, x2,     xrmost likely?

id113

  

=

1
r
   
r 1
k
=

x

k

id113

  

=

1
r
   
r 1
k
=

(
x

k

id113

   

  

)(
x

k

id113

   

  

t

)

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 29

id113 for m-dimensional gaussian
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    but you don   t know    or   
    id113: for which    =(  ,  ) is x1, x2,     xrmost likely?

id113

  

=

1
r
   
r 1
k
=

x

k

  

id113
i

=

1
r

r

   

k

1
=

id113

  

=

1
r
   
r 1
k
=

(
x

k

)(
   x

id113

k

   

   

id113

  

t

)

where 1     i     m

x

ki

and xki is value of the 
ith component of xk
(the ith attribute of 
the kth record)

and   i
id113 is the ith
component of   id113

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 30

15

id113 for m-dimensional gaussian
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    but you don   t know    or   
    id113: for which    =(  ,  ) is x1, x2,     xrmost likely?
where 1     i     m, 1     j     m

id113

  

=

1
r
   
r 1
k
=

x

k

id113

  

=

1
r
   
r 1
k
=

(
x

k

)(
   x

id113

k

   

   

id113

  

t

)

id113
  
ij

=

1
r
   
r 1
k
=

(
x

ki

   

id113
  
i

and xki is value of the ith
component of xk (the ith
attribute of the kth record)

id113 is the (i,j)th

and   ij
component of   id113
)(
x

id113
  
j

)

   

kj

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 31

q: how would you prove this?

id113 for m-dimensional gaussian
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    but you don   t know    or   
note how   id113is forced to be 
    id113: for which    =(  ,  ) is x1, x2,     xrmost likely?
symmetric non-negative definite
note the unbiased case
how many datapoints would you 
need before the gaussian has a 
chance of being non-degenerate? 

a: just plug through the id113 
recipe.

1
r
   
r 1
k
=

=

x

id113

k

  

id113

  

=

1
r
   
r 1
k
=

(
x

k

)(
   x

id113

k

   

   

id113

  

t

)

unbiased

  

=

copyright    2001, 2004, andrew w. moore

=

1
   

1

r

  
1
   

id113
1
r

r

   

k

1
=

(
x

k

)(
   x

id113

k

   

   

id113

  

t

)

maximum likelihood: slide 32

16

confidence intervals

we need to talk
we need to discuss how accurate we expect   id113and   id113to be as 
a function of r
and we need to consider how to estimate these accuracies from 
data   
   analytically *
   non-parametrically (using randomization and id64) *
but we won   t. not yet.

*will be discussed in future andrew lectures   just before 
we need this technology.

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 33

structural error

actually, we need to talk about something else too..
what if we do all this analysis when the true distribution is in fact 
not gaussian?
how can we tell? *
how can we survive? *

*will be discussed in future andrew lectures   just before 
we need this technology.

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 34

17

gaussian id113 in action 

using r=392 cars from the 
   mpg    uci dataset supplied 
by ross quinlan

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 35

data-starved gaussian id113

using three subsets of mpg.
each subset has 6 
randomly-chosen cars.

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 36

18

n
o
i
t
c
a
 
n
i
 
e
l
m
 
e
t
a
i
r
a
v
b

i

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 37

multivariate id113

covariance matrices are not exciting to look at

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 38

19

being bayesian: map estimates for gaussians
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    but you don   t know    or   
    map: which (  ,  ) maximizes p(  ,   |x1, x2,     xr)?

step 1: put a prior on (  ,  )

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 39

being bayesian: map estimates for gaussians
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    but you don   t know    or   
    map: which (  ,  ) maximizes p(  ,   |x1, x2,     xr)?

step 1: put a prior on (  ,  )
step 1a: put a prior on   

(  0-m-1)    ~ iw(  0, (  0-m-1)    0 )
this thing is called the inverse-wishart 
distribution.
a pdf over spd matrices!

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 40

20

  0 small:    i am not sure 
about my guess of    0    

being bayesian: map estimates for gaussians
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
   0 : (roughly) my best 
    but you don   t know    or   
    map: which (  ,  ) maximizes p(  ,   |x1, x2,     xr)?

  0 large:    i   m pretty sure 
about my guess of    0    

guess of   

  [   ] =    0

step 1: put a prior on (  ,  )
step 1a: put a prior on   

(  0-m-1)    ~ iw(  0, (  0-m-1)    0 )
this thing is called the inverse-wishart 
distribution.
a pdf over spd matrices!

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 41

being bayesian: map estimates for gaussians
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    but you don   t know    or   
    map: which (  ,  ) maximizes p(  ,   |x1, x2,     xr)?

step 1: put a prior on (  ,  )
step 1a: put a prior on   

(  0-m-1)   ~ iw(  0, (  0-m-1)   0 )

step 1b: put a prior on    |   
   |    ~ n(  0 ,    /   0)

together,          and 
      |       define a 
joint distribution 
on (  ,  )

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 42

21

being bayesian: map estimates for gaussians
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    but you don   t know    or   
    map: which (  ,  ) maximizes p(  ,   |x1, x2,     xr)?

  0 small:    i am not sure 
about my guess of    0    

   0 : my best guess of   

step 1: put a prior on (  ,  )
step 1a: put a prior on   

e[   ] =    0

  0 large:    i   m pretty sure 
about my guess of    0    

(  0-m-1)   ~ iw(  0, (  0-m-1)   0 )

step 1b: put a prior on    |   
   |    ~ n(  0 ,    /   0)

together,          and 
      |       define a 
joint distribution 
on (  ,  )

notice how we are forced to express our 

ignorance of    proportionally to   

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 43

being bayesian: map estimates for gaussians
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    but you don   t know    or   
    map: which (  ,  ) maximizes p(  ,   |x1, x2,     xr)?

why do we use this form 
of prior?

step 1: put a prior on (  ,  )
step 1a: put a prior on   

(  0-m-1)   ~ iw(  0, (  0-m-1)   0 )

step 1b: put a prior on    |   
   |    ~ n(  0 ,    /   0)

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 44

22

being bayesian: map estimates for gaussians
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    but you don   t know    or   
    map: which (  ,  ) maximizes p(  ,   |x1, x2,     xr)?

step 1: put a prior on (  ,  )
step 1a: put a prior on   

(  0-m-1)   ~ iw(  0, (  0-m-1)   0 )

step 1b: put a prior on    |   
   |    ~ n(  0 ,    /   0)

why do we use this form of
prior?
actually, we don   t have to
but it is computationally and 
algebraically convenient   
   it   s a conjugate prior.

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 45

being bayesian: map estimates for gaussians
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
    map: which (  ,  ) maximizes p(  ,   |x1, x2,     xr)?
step 1: prior: (  0-m-1)    ~ iw(  0, (  0-m-1)    0 ),     |    ~ n(  0 ,    /   0)
step 2:

x

=

1
r
   
r 1
k
=

x

k

  

r

=

x

  
  
0
0
  
0

r
+
r
+

= 0    
r
= 0    

r

+

+

(
  
r

=

  

m

)1

   +

)(
0
  
+
0
step 3: posterior: (  r+m-1)   ~ iw(  r, (  r+m-1)    r ), 

   +

)(
xx

   

(
  
0

(
x

)1

m

  

+

   

   

+

x

)

1
=

r

r

t

0

k

k

k

   |    ~ n(  r ,    /   r)

result:   map =   r, e[   |x1, x2,     xr ]=   r

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 46

r
r
(
  x  x
   
   
0
r
/1
/1

t

)

23

being bayesian: map estimates for gaussians
   look carefully at what these formulae are 
doing. it   s all very sensible.
    suppose you have x1, x2,     xr~(i.i.d) n(  ,  )
   conjugate priors mean prior form and posterior 
form are same and characterized by    sufficient 
    map: which (  ,  ) maximizes p(  ,   |x1, x2,     xr)?
statistics    of the data.
   the marginal distribution on    is a student-t
step 1: prior: (  0-m-1)    ~ iw(  0, (  0-m-1)    0 ),     |    ~ n(  0 ,    /   0)
   one point of view: it   s pretty academic if r > 30
step 2:
  

+

x

=

x

=

r

  
  
0
0
  
0

r
+
r
+

= 0    
r
= 0    

r

+

1
r
   
r 1
k
=

x

k

(
  
r

=

  

m

)1

   +

)(
0
+
  
0
step 3: posterior: (  r+m-1)   ~ iw(  r, (  r+m-1)    r ), 

   +

)(
xx

   

(
  
0

(
x

)1

m

  

+

   

   

+

x

)

1
=

r

r

t

0

k

k

k

r
r
(
  x  x
   
   
0
r
/1
/1

   |    ~ n(  r ,    /   r)

result:   map =   r, e[   |x1, x2,     xr ]=   r

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 47

where we   re at

categorical 
inputs only

real-valued 
inputs only

mixed real / 
cat okay

s
t
u
p
n
i

classifier

predict
category

joint bc
na  ve bc

dec tree

s density
estimator

t
u
p
n
i

s
t
u
p
n
i

regressor

prob-
ability

predict
real no.

joint de
na  ve de

gauss de

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 48

t

)

24

what you should know

    the recipe for id113
    what do we sometimes prefer id113 to map?
    understand id113 estimation of gaussian 

parameters

    understand    biased estimator    versus 

   unbiased estimator   

    appreciate the outline behind bayesian 

estimation of gaussian parameters

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 49

useful exercise

    we   d already done some id113 in this class 

without even telling you!

    suppose categorical arity-n inputs x1, x2,     

xr~(i.i.d.) from a multinomial 
m(p1, p2,     pn) 

where 

p(xk=j|p)=pj

    what is the id113 p=(p1, p2,     pn)?

copyright    2001, 2004, andrew w. moore

maximum likelihood: slide 50

25

