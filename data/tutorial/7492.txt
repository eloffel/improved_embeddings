regression and 
classification with
neural networks

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2001, 2003, andrew w. moore

sep 25th, 2001

id75

dataset

   
w
   
    1    

inputs
x1 = 1
x2 = 3
x3 = 2
x4 = 1.5
x5 = 4

outputs
y1 = 1
y2 = 2.2
y3 = 2
y4 = 1.9
y5 = 3.1

id75 assumes that the expected value of 
the output given an input, e[y|x], is linear.
simplest case: out(x) = wxfor some unknown w.
given the data, we can estimate w.

copyright    2001, 2003, andrew w. moore

neural networks: slide 2

1

1-parameter id75

assume that the data is formed by
yi = wxi+ noisei

where   
    the noise signals are independent
    the noise has a normal distribution with mean 0 

and unknown variance   2

p(y|w,x) has a normal distribution with
    mean wx
    variance   2

copyright    2001, 2003, andrew w. moore

neural networks: slide 3

bayesian id75

p(y|w,x) = normal (mean wx, var   2)

we have a set of datapoints (x1,y1) (x2,y2)     (xn,yn) 
which are evidence about w.

we want to infer wfrom the data.

p(w|x1, x2, x3,   xn, y1, y2   yn)

   you can use bayes rule to work out a posterior 
distribution for wgiven the data.
   or you could do id113

copyright    2001, 2003, andrew w. moore

neural networks: slide 4

2

id113 of w

asks the question:
   for which value of wis this data most likely to have 

happened?   

<=>

for what wis

p(y1, y2   yn|x1, x2, x3,   xn, w) maximized?

<=>

for what w is

xwyp   
,

(

i

i

n

i

1
=

maximized?
 )

copyright    2001, 2003, andrew w. moore

neural networks: slide 5

for what w is
n

i

1
=
for what w is
n

i
1
=
for what w is
n

1

i
=
for what w is
   

n

i

=

1

xwyp   
,

(

i

  )

maximized?

i

       
exp(

1
2

(

y

i wx
   
i
  

 ))
2

maximized?

      

1
2

   
   
   

y

i

wx

   
  

i

2

   
   
   

maximized?
 

(

y

i

   

wx

i

2

) minimized?

 

copyright    2001, 2003, andrew w. moore

neural networks: slide 6

3

id75

the maximum 

likelihood wis 
the one that 
minimizes sum-
of-squares of 
residuals

e(w)

w

(

y
i

=  

   
i
   
y
i

2

)

wx
i

   
(
   
2

(
   
we want to minimize a quadratic function of w.

)
wyx
i

+

   

=

2

i

i

) 2
2
wx
i

copyright    2001, 2003, andrew w. moore

neural networks: slide 7

id75

easy to show the sum of 

squares is minimized 
when

w

   =
yx
i
2   
x
i

i

the maximum likelihood 
( ) wx
x =

model is

out

we can use it for 

prediction

copyright    2001, 2003, andrew w. moore

neural networks: slide 8

4

id75

easy to show the sum of 

squares is minimized 
when

w

   =
yx
i
2   
x
i

i

the maximum likelihood 
( ) wx
x =

model is

out

we can use it for 

prediction

p(w)

w

note:   in bayesian stats you   d have 

ended up with a prob dist of w

and predictions would have given a prob 

dist of expected output

often useful to know your confidence.  

max likelihood can give some kinds of 
confidence too.

copyright    2001, 2003, andrew w. moore

neural networks: slide 9

multivariate regression

what if the inputs are vectors?

6 .

. 8

x2

x1
dataset has form

3 .

. 4                                              

2-d input 
example

. 5

. 10

x1 
y1
x2 
y2
x3 
y3
.:                                    :
.
xr

yr

copyright    2001, 2003, andrew w. moore

neural networks: slide 10

5

multivariate regression

write matrix x and y thus:

x

=

1

2

x
.....
x
.....

   
   
   
   
m
   
x
.....
   

r

.....
.....

   
   
   
   
   
.....
   

=

   
   
   
   
   
   

x
11
x

21

x
12
x

22

x

r
1

x

r

2

...
...

m
...

x
m
1
x

2

m

x

rm

   
   
   
y
  
   
   
   

=

y
1
y

2

m
y

r

   
   
   
   
   
   

   
   
   
   
   
   

(there are r datapoints.  each input has mcomponents)
the id75 model assumes a vector w such that

out(x) = wtx= w1x[1] + w2x[2] +    .wmx[d]

the max. likelihood wis w = (xtx) -1(xty)

copyright    2001, 2003, andrew w. moore

neural networks: slide 11

multivariate regression

write matrix x and y thus:

x

=

1

2

x
.....
x
.....

   
   
   
   
m
   
x
.....
   

r

.....
.....

   
   
   
   
   
.....
   

=

   
   
   
   
   
   

x
11
x

21

x
12
x

22

x

r
1

x

r

2

...
...

m
...

x
m
1
x

2

m

x

rm

   
   
   
y
  
   
   
   

=

y
1
y

2

m
y

r

   
   
   
   
   
   

   
   
   
   
   
   

(there are r datapoints.  each input has mcomponents)
the id75 model assumes a vector w such that

prove it !!!!!

important exercise:  

out(x) = wtx= w1x[1] + w2x[2] +    .wmx[d]

the max. likelihood wis w = (xtx) -1(xty)

copyright    2001, 2003, andrew w. moore

neural networks: slide 12

6

multivariate regression (con   t)

the max. likelihood w is w = (xtx)-1(xty)

xtx is an m x m matrix:  i,j   th elt is

xty is an m-element vector:  i   th elt

r

   

k

1
=

kixx
kj

r

   

k

1
=

kiyx
k

copyright    2001, 2003, andrew w. moore

neural networks: slide 13

what about a constant term?

we may expect 
linear data that does 
not go through the 
origin.

statisticians and 
neural net folks all 
agree on a simple 
obvious hack.

can you guess??

copyright    2001, 2003, andrew w. moore

neural networks: slide 14

7

the constant term

    the trick is to create a fake input    x0    that 

always takes the value 1

yx2x1
2
4
16
3
4
17
5
5
20
before:
y=w1x1+ w2x2 
   has to be a poor 
model

in this example, 
you should be able 
to see the id113 w0
, w1and w2 by 
inspection 

4
4
5

x0
yx2x1
2
1
16
3
1
17
1
5
20
after:
y= w0x0+w1x1+ w2x2 
= w0+w1x1+ w2x2 
   has a fine constant 
term

copyright    2001, 2003, andrew w. moore

neural networks: slide 15

regression with varying noise

    suppose you know the variance of the noise that 

was added to each datapoint.

yi

xi
  i
2
4    
1
1
2
1/4
2
4
3
1/4

1
1
3
2

y=3

y=2

y=1

y=0

  =2

  =1/2

  =1

  =2

  =1/2

x=0

x=1

x=2

x=3

assume

y

i

~

wxn
i

(

,
2
  
i

)

t    s   t
a
t
ti m a

w h
s
e

e  
e   m l
h
f   w ?
e   o

copyright    2001, 2003, andrew w. moore

neural networks: slide 16

8

id113 estimation with varying noise

argmax

w

log

yyp
,
(

1

2

,...,

y

r

|

xx
,
1

2

,...,

x

r

,
2
      
r

,...,

2
1

2
2

,

,

w
=)

argmin

w

r

(

y
i wx
      
i
2
  
i

1
=

i

2)

=

assuming i.i.d. and 
then plugging in 
equation for gaussian 
and simplifying.

w
 
such that

   
      
   

r

   

i

1
=

wx
i

)

=

0

yx
(
i

   
i
2
  
i

   
=      
   

setting dll/dw
equal to zero

i

r

   
   
      
   
   
      
   

1
=
r

   

1
=

i

trivial algebra

i

yx
i
2
  
i
x
2
i
2
  
i

   
      
   
   
      
   

copyright    2001, 2003, andrew w. moore

neural networks: slide 17

this is weighted regression

    we are asking to minimize the weighted sum of 

squares

argmin

w

r

   

i

1
=

2)

(

y

i wx
   
i
2
  
i

y=3

y=2

y=1

y=0

  =2

  =1/2

  =1

  =2

  =1/2

x=0

x=1

x=2

x=3

where weight for i   th datapoint is

1
2
i  

copyright    2001, 2003, andrew w. moore

neural networks: slide 18

9

weighted multivariate regression

the max. likelihood w is w = (wxtwx)-1(wxtwy)

(wxtwx) is an m x m matrix:  i,j   th elt is

(wxtwy) is an m-element vector:  i   th elt

r

   

k

1
=

r

   

k

1
=

kj

kixx
2  
i

k

ki yx
2  
i

copyright    2001, 2003, andrew w. moore

neural networks: slide 19

non-id75

y=3

    suppose you know that y is related to a function of x in 
such a way that the predicted values have a non-linear 
dependence on w, e.g:
yi
xi
    
2.5
1
2
3
3
2
3
3

y=0

y=1

y=2

x=0

x=2

x=1

x=3

assume

y

i

~

xwn

+

(

,
2  i

)

t    s   t
a
t
ti m a

w h
s
e

e  
e   m l
h
f   w ?
e   o

copyright    2001, 2003, andrew w. moore

neural networks: slide 20

10

non-linear id113 estimation

argmax

w

log

yyp
(
,

1

2

,...,

y

r

|

xx
,
1

2

,...,

x

r

,
,
  

w
=)

argmin

w

r

(

      

y
i

i

1
=

xw
i

+

2

) =

assuming i.i.d. and 
then plugging in 
equation for gaussian 
and simplifying.

w
 
such that

   
   
   
   

r

y
      

i

i

1
=

+
xw
i

xw
i
+

=

0

   
   
=   
   

setting dll/dw
equal to zero

copyright    2001, 2003, andrew w. moore

neural networks: slide 21

non-linear id113 estimation

argmax

w

log

yyp
(
,

1

2

,...,

y

r

|

xx
,
1

2

,...,

x

r

,
,
  

w
=)

argmin

w

r

(

      

y
i

i

1
=

xw
i

+

2

) =

assuming i.i.d. and 
then plugging in 
equation for gaussian 
and simplifying.

w
 
such that

   
   
   
   

r

y
      

i

i

1
=

+
xw
i

xw
i
+

=

0

   
   
=   
   

setting dll/dw
equal to zero

we   re down the 
algebraic toilet
  w h
?
o

t  

s

g

o

 

a

s

s

e
u
w e

d

 

copyright    2001, 2003, andrew w. moore

neural networks: slide 22

11

non-linear id113 estimation

argmax

w

log

yyp
(
,

1

2

,...,

y

r

|

xx
,
1

2

,...,

x

r

,
,
  

w
=)

r

(

i

1
=

      

argmin

common (but not only) approach:
y
i
numerical solutions:
w
    line search
    simulated annealing
   
   
    id119
   
   
    conjugate gradient
    levenberg marquart
    newton   s method

w
 
such that

y
      

1
=

r

i

i

also, special purpose statistical-

optimization-specific tricks such as 
e.m. (see gaussian mixtures lecture 
for introduction)

copyright    2001, 2003, andrew w. moore

xw
i

+

2

) =

assuming i.i.d. and 
then plugging in 
equation for gaussian 
and simplifying.

+
xw
i

xw
i
+

=

0

   
   
=   
   

setting dll/dw
equal to zero

we   re down the 
algebraic toilet
  w h
?
o

t  

s

g

o

 

a

s

s

e
u
w e

d

 

neural networks: slide 23

id119
f(w)

suppose we have a scalar function

         :

we want to find a local minimum.
assume our current weight is w

id119 rule:

ww

   
         
w
   

(

)w

f

   is called the learning rate.  a small positive 

number, e.g.    = 0.05

copyright    2001, 2003, andrew w. moore

neural networks: slide 24

12

id119
f(w)

suppose we have a scalar function

         :

we want to find a local minimum.
assume our current weight is w

id119 rule:

ww

   
         
w
   

(

)w

f

   is called the learning rate.  a small positive 

recall andrew   s favorite 
default value for anything

number, e.g.    = 0.05

question:  justify the id119 rule

copyright    2001, 2003, andrew w. moore

neural networks: slide 25

id119 in    m    dimensions

given

f(w

:)

(
wf
   

)

   
w
   
1

         m
   
)
   
   
   
   
   
   

m
(
wf

=

points in direction of steepest ascent.

(
wf

   
   
   
   
   
   
)   
mw
   
   
)wf   
is the gradient in that direction

(

id119 rule:
equivalently

w

j

   

  w
-

j

-ww

         

(
)wf

(
)wf

j

   .where wj is the jth weight
   just like a linear feedback system   

   
w
   

copyright    2001, 2003, andrew w. moore

neural networks: slide 26

13

what   s all this got to do with neural 

nets, then, eh??

for supervised learning, neural nets are also models with 
vectors of w parameters in them.  they are now called 
weights.
as before, we want to compute the weights to minimize sum-
of-squared residuals.

which turns out, under    gaussian i.i.d noise   
assumption to be max. likelihood.

instead of explicitly solving for max. likelihood weights, we 
use id119 to search for them.

   wh y?      y ou  as k,  a que ru lou s  exp ress ion in  yo ur  ey es .   
    ah a!!     i  re ply:      we    l l  s ee  lat er .   

copyright    2001, 2003, andrew w. moore

neural networks: slide 27

linear id88s

they are multivariate linear models:

out(x) = wtx

and    training    consists of minimizing sum-of-squared residuals 

by id119.
(
   
out
(
   
w

=  

   

=

  

k

x

k

2

)

k

(
x

k

)

   

   

y

k

k

y
)2

question:  derive the id88 training rule.

copyright    2001, 2003, andrew w. moore

neural networks: slide 28

14

linear id88 training rule
e
=

xw
t

2)

   

(

r

k

   

k

1
=

ky

id119 tells us 
we should update w
thusly if we wish to 
minimize e:

w

j

    -
j

e  w
   
w
   

j

so what   s

e
   
jw
   

?

copyright    2001, 2003, andrew w. moore

neural networks: slide 29

j

e
   
w
   
r
   

k

1
=

=

linear id88 training rule
e
=

xw
t

xw
t

2)

   

2)

   

=

(

y

(

r

r

k

k

ky

k

   

k

1
=

   
w
   

j

   

k

1
=

id119 tells us 
we should update w
thusly if we wish to 
minimize e:

w

j

    -
j

e  w
   
w
   

j

2
   =

so what   s

e
   
jw
   

?

(2

y

k

   

xw
t

)

k

   
w
   

j

(

y

k

   

xw
t

)

k

2
   =

r

   

k

1
=

  

   
k w
   

j

xw
t

k

xw   
t

y

=

   where   
  
k
xw
i

ki

k

k

  

k

   
w
   

j

m

   

i

1
=

r

   

k

1
=

2
   =

r

   

k

1
=

k x  

kj

copyright    2001, 2003, andrew w. moore

neural networks: slide 30

15

linear id88 training rule
e
=

xw
t

2)

   

(

r

k

   

k

1
=

ky

id119 tells us 
we should update w
thusly if we wish to 
minimize e:

w

j

    -
j

e  w
   
w
   

j

   where   
e
   
w
   

   =

j

2

r

   

k

1
=

w

j

+   

w

j

2

  

r

   

k

1
=

x  
k

kj

x  
k

kj

we frequently neglect the 2 (meaning 
we halve the learning rate)

copyright    2001, 2003, andrew w. moore

neural networks: slide 31

the    batch    id88 algorithm
1) randomly initialize weights  w1 w2     wm

2) get your dataset (append 1   s to the inputs if 

you don   t want to go through the origin).

3)

for  i= 1 to r

xw     =:  
i

y
i

i

4)

for  j= 1 to m

w

j

+   

w

j

r

   
    
i

x
ij

i

1
=

5)

    2
if             stops improving then stop.  else loop 
i  
back to 3.

copyright    2001, 2003, andrew w. moore

neural networks: slide 32

16

  
i
w

j

y
      
i
w
+   

  xw
    
i

j

i
x

ij

a rule known by

many names

e   l m s   r u l e

h

t

classical
conditioning

the delta rule

r u l e

 

f

t h e   w i d r o w h o f
the adalinerule

copyright    2001, 2003, andrew w. moore

neural networks: slide 33

if data is voluminous and arrives fast

input-output pairs (x,y) come streaming in very 
quickly.  then

don   t bother remembering old ones.  
just keep using new ones.

observe (x,y)
xw
  
x    w
  
+   

y
      
wj
  

  
   

j

j

j

copyright    2001, 2003, andrew w. moore

neural networks: slide 34

17

id119 vs matrix inversion 

for linear id88s
gd advantages (mi disadvantages):
    biologically plausible
    with very very many attributes each iteration costs only o(mr). if 

fewer than m iterations needed we   ve beaten matrix inversion

    more easily parallelizable (or implementable in wetware)?
gd disadvantages (mi advantages):
    it   s moronic
    it   s essentially a slow implementation of a way to build the xtx matrix 

and then solve a set of linear equations

    if m is small it   s especially outageous. if m is large then the direct 

matrix inversion method gets fiddly but not impossible if you want to 
be efficient.

    hard to choose a good learning rate
    matrix inversion takes predictable time. you can   t be sure when 

id119 will stop.

copyright    2001, 2003, andrew w. moore

neural networks: slide 35

id119 vs matrix inversion 

for linear id88s
gd advantages (mi disadvantages):
    biologically plausible
    with very very many attributes each iteration costs only o(mr). if 

fewer than m iterations needed we   ve beaten matrix inversion

    more easily parallelizable (or implementable in wetware)?
gd disadvantages (mi advantages):
    it   s moronic
    it   s essentially a slow implementation of a way to build the xtx matrix 

and then solve a set of linear equations

    if m is small it   s especially outageous. if m is large then the direct 

matrix inversion method gets fiddly but not impossible if you want to 
be efficient.

    hard to choose a good learning rate
    matrix inversion takes predictable time. you can   t be sure when 

id119 will stop.

copyright    2001, 2003, andrew w. moore

neural networks: slide 36

18

id119 vs matrix inversion 

for linear id88s
gd advantages (mi disadvantages):
    biologically plausible
    with very very many attributes each iteration costs only o(mr). if 

fewer than m iterations needed we   ve beaten matrix inversion

but we   ll

    more easily parallelizable (or implementable in wetware)?
gd disadvantages (mi advantages):
    it   s moronic
    it   s essentially a slow implementation of a way to build the xtx matrix 

has an important extra

soon see that

gd

and then solve a set of linear equations

    if m is small it   s especially outageous. if m is large then the direct 

matrix inversion method gets fiddly but not impossible if you want to 
be efficient.

trick up its sleeve

    hard to choose a good learning rate
    matrix inversion takes predictable time. you can   t be sure when 

id119 will stop.

copyright    2001, 2003, andrew w. moore

neural networks: slide 37

id88s for classification

what if all outputs are 0   s or 1   s ?

or

we can do a linear fit.
our prediction is   0 if out(x)   1/2
1 if out(x)>1/2

what   s the big problem with this???

copyright    2001, 2003, andrew w. moore

neural networks: slide 38

19

id88s for classification

what if all outputs are 0   s or 1   s ?

or

we can do a linear fit.
our prediction is   0 if out(x)     
1 if out(x)>  

blue = out(x)

what   s the big problem with this???

copyright    2001, 2003, andrew w. moore

neural networks: slide 39

id88s for classification

what if all outputs are 0   s or 1   s ?

or

we can do a linear fit.
our prediction is   0 if out(x)     
1 if out(x)>  

blue = out(x)

green = classification

copyright    2001, 2003, andrew w. moore

neural networks: slide 40

20

classification with id88s i

don   t minimize

   

(

iy

   

xw
  

i

2

) .

minimize number of misclassifications instead.  [assume outputs are 

+1 & -1, not +1 & 0]

(

   

iy

   

round

(
xw 
  

i

)
)

where   round(x) =    -1 if x<0
1 if x   0

note: cute &

non obvious why 

this works!!

the id119 rule can be changed to:
if (xi,yi) correctly classed, don   t change
if wrongly predicted as 1
if wrongly predicted as -1

w (cid:197) w - xi
w (cid:197) w +xi

copyright    2001, 2003, andrew w. moore

neural networks: slide 41

classification with id88s ii:

sigmoid functions

least squares fit useless

this fit would classify much 
better.  but not a least 
squares fit.

copyright    2001, 2003, andrew w. moore

neural networks: slide 42

21

classification with id88s ii:

sigmoid functions

least squares fit useless

solution:
instead of  out(x) = wtx
we   ll use     out(x) = g(wtx)
where                            is a 

(
)1,0
squashing function

( )
:       xg

this fit would classify much 
better.  but not a least 
squares fit.

copyright    2001, 2003, andrew w. moore

neural networks: slide 43

the sigmoid

hg
)(

=

1
exp(

   

h

)

1

+

note that if you rotate this 

curve through 180o

centered on (0,1/2)you get 

the same curve.

i.e. g(h)=1-g(-h)

can you prove this?

copyright    2001, 2003, andrew w. moore

neural networks: slide 44

22

the sigmoid

hg
)(

=

1
exp(

   

h

)

1

+

now we choose  w to minimize

r

   

i

1
=

[

y

i

   

)x(out
i

2

]

=

[

y

i

   

g

  

)xw(
i

2

]

r

   

i

1
=

copyright    2001, 2003, andrew w. moore

neural networks: slide 45

linear id88 classification 

regions

x2

0      0

0

1

1

x1

1

we   ll use the model         out(x) = g(wt(x,1))

= g(w1x1 + w2x2 + w0)

which region of above diagram classified with +1, and 

which with 0 ??

copyright    2001, 2003, andrew w. moore

neural networks: slide 46

23

id119 with sigmoid on a id88

xe
      
xe
   +

1

   
   
   

   

1
xe
   +

1

2

   
   
   

=

1
   
xe
   +

   
      
   

1

1

   

1
xe
   +

   
   =      
   

1

(
( )
xg
1

   

( )
)
xg

 
notice
first,

    

( )
xg
'

   

( )
)
xg

because

   :

( )
xg

=

1

so   

   

( )
xg
'

=

(
( )
xg
1
=
1
xe
   +

          

      
=

out(x)

=

g

   
   
   

1

xe
         
11
2
xe
   +
   
   
   
   
   
   
   

xw
k

k

   
   
   

k

1
xe
   +

=

1

   
   
   

2

   
   
   

   

g

i

   
   
   

   

k

xw
k
ik

2

   
   
   

   
      
   

   

i

=

=  

     
w
   

j

y

   
      
   
   

i

   

i

     
=

   
   
i
where
    

    
=

   

   
2
      
   
   
2
      
   
2
  
i

y

   

g

i

xw
ik
k

k

   

   
   
   
   
   
   
   
k
(
)
1

i

   
   
   
   
   
   
(
net

   
   
   
         
   
   
   
   
   
g
      
   
   
   
)
)
x
ij

'

i

xw
ik
k

   

g

y

i

   

g

g

(
net

   
w
   

j

g

   
   
   

   

k

xw
ik
k

xw
ik
k

   

k

   
   
   

   
w
   

j

   
   
   
   
   
   
   
   

k

xw
k
ik

  
i

   
copyright    2001, 2003, andrew w. moore

out(x

      
)

net

=

   

=

y

k

i

i

i

xw
k

k

j

r

w

w

+   

the sigmoid id88
update rule:
(
   
1    
   
=    
g
      
   
j
1
=
y    
=  
i

where

i
1
=
g

   

g

g

m

i

i

i

j

i

i

)
xg
i
ij

xw
j
ij

   
      
   

neural networks: slide 47

other things about id88s

    invented and popularized by rosenblatt (1962)

    even with sigmoid nonlinearity, correct 

convergence is guaranteed

    stable behavior for overconstrained and 

underconstrained problems

copyright    2001, 2003, andrew w. moore

neural networks: slide 48

24

id88s and boolean functions

if inputs are all 0   s and 1   s and outputs are all 0   s and 1   s   

    can learn the function   x1     x2  

x2

x1

x1

    can learn the function x1     x2 .

x2

    can learn any conjunction of literals, e.g.

x1     ~x2     ~x3     x4     x5

question:  why?

copyright    2001, 2003, andrew w. moore

neural networks: slide 49

id88s and boolean functions
    can learn any disjunction of literals

e.g. x1     ~x2     ~x3     x4     x5

    can learn majority function

f(x1,x2     xn) =    1 if  n/2 xi   s or more are = 1
0 if less than n/2 xi   s are = 1

    what about the exclusive or function?

f(x1,x2) = x1     x2 = 
(x1     ~x2)     (~ x1     x2)

copyright    2001, 2003, andrew w. moore

neural networks: slide 50

25

multilayer networks

the class of functions representable by id88s

is limited

out(x)

  
=

g

(
xw
  

)
  
=

g

   
      
   

   

j

jxw

j

   
      
   

use a wider 
representation !

   
gwg 
      
out(x)
 
=
j
   

   

j

   
   
   

   

k

xw
jk

jk

   
   
   

   
      
   

this is a nonlinear function

of a linear combination
of non linear functions

of linear combinations of inputs

copyright    2001, 2003, andrew w. moore

neural networks: slide 51

a 1-hidden layer net

ninputs = 2                                    nhidden = 3

x1

w12

x2

w11

w21

w31

w22

w32

v
1

=

g

   
      
   

v
2

=

g

   
      
   

v
3

=

g

   
      
   

n

ins

   

k

1
=

xw
kk
1

   
      
   

n

ins

   

k

1
=

xw
kk
2

n

ins

   

k

1
=

xw
kk
3

   
      
   

   
      
   

w1

w2

w3

out

hidn

=    
g

k

1
=

   
      
   

kvw

k

   
      
   

copyright    2001, 2003, andrew w. moore

neural networks: slide 52

26

other neural nets

2-hidden layers + constant term

   jump    connections

out 
 
=

g

   
      
   

n

ins

   

k

1
=

xw
k
k
0

+

1

x1
x2
x3

x1

x2

n

hid

   

k

1
=

vw
kk

   
      
   

copyright    2001, 2003, andrew w. moore

neural networks: slide 53

id26

xw
k
jk

   
   
      
   
   
   
ww
{,}

j

jk

}

=

 a 

set 

out(x)

   
   
   
   
gwg
      
   
j
   
   
k
j
 weights
of
{  
minimize
(
   
i
gradient 

descent.

(
xout

find
 to
          

by 

    

y
i

   

2

)
)

i

that   s it!
that   s it!
that   s the id26
that   s the id26

algorithm.
algorithm.

copyright    2001, 2003, andrew w. moore

neural networks: slide 54

27

id26 convergence

convergence to a global minimum is not
guaranteed.

   in practice, this is not a problem, apparently.

tweaking to find the right number of hidden 
units, or a useful learning rate   , is more 
hassle, apparently.

implementing backprop: (cid:20) differentiate monster sum-square residual  (cid:21)
write down the id119 rule  (cid:22) it turns out to be easier & 
computationally efficient to use lots of local variables with names like hj ok vj neti
etc   

copyright    2001, 2003, andrew w. moore

neural networks: slide 55

choosing the learning rate

    this is a subtle art.
    too small: can take days instead of minutes 

to converge

    too large: diverges (mse gets larger and 

larger while the weights increase and 
usually oscillate)

    sometimes the    just right    value is hard to 

find.

copyright    2001, 2003, andrew w. moore

neural networks: slide 56

28

learning-rate problems

from j. hertz, a. krogh, and r. 
g. palmer. introduction to the 
theory of neural computation. 
addison-wesley, 1994.

copyright    2001, 2003, andrew w. moore

neural networks: slide 57

improving simple id119
momentum
don   t just change weights according to the current datapoint.
re-use changes from earlier iterations.

let     w(t) = weight changes at time t.
let                 be the change we would make with

regular id119.

     

     
w   
instead we use

     
     +
w
   
( )
t
+
momentum damps oscillations.
a hack?  well, maybe.

)
   =+1
)

(
   w
t
(
t

   w

+ 1

w

w

=

   w
( )t

( )t

momentum parameter

copyright    2001, 2003, andrew w. moore

neural networks: slide 58

29

momentum illustration

copyright    2001, 2003, andrew w. moore

neural networks: slide 59

improving simple id119
newton   s method

e

(

hw

+

)

=

e

(

hw

+

)

t

e
   
w
   

+

1
2

t

h

e
2
   
w
2
   

h

+

o

(|

h

)|
3

if we neglect the o(h3)terms, this is a quadratic form

quadratic form fun facts:
if y = c + btx-1/2 xta x
and if ais spd
then
xopt= a-1bis the value of xthat maximizes y

copyright    2001, 2003, andrew w. moore

neural networks: slide 60

30

improving simple id119
newton   s method

e

(

hw

+

)

=

e

(

hw

+

)

t

e
   
w
   

+

1
2

t

h

e
2
   
w
2
   

h

+

o

(|

h

)|
3

if we neglect the o(h3)terms, this is a quadratic form

w

      

w

   
   
   

e 1
2
   
   
   
w
   
   

    e
   
w
   

2

this should send us directly to the global minimum if the 
function is truly quadratic.
and it might get us close if it   s locally quadraticish

copyright    2001, 2003, andrew w. moore

neural networks: slide 61

improving simple id119
newton   s method

e

(

hw

+

)

=

e

(

hw

+

)

t

e
   
w
   

+

1
2

t

h

e
2
   
w
2
   

h

+

o

(|

h

)|
3

if we neglect the o(h3)terms, this is a quadratic form

but (and it   s a big but)   
that second derivative matrix can be 
expensive and fiddly to compute.
if we   re not already in the quadratic bowl, 
we   ll go nuts.
this should send us directly to the global minimum if the 
function is truly quadratic.
and it might get us close if it   s locally quadraticish

e 1
2
   
   
   
w
   
   

    e
   
w
   

      

   
   
   

w

w

2

copyright    2001, 2003, andrew w. moore

neural networks: slide 62

31

improving simple id119
conjugate gradient
another method which attempts to exploit the    local 
quadratic bowl    assumption
but does so while only needing to use

   e
w   

and not

    e
2
w   
2

it is also more stable than newton   s method if the local 
quadratic bowl assumption is violated.
it   s complicated, outside our scope, but it often works well. 
more details in numerical recipes in c.

copyright    2001, 2003, andrew w. moore

neural networks: slide 63

best generalization
intuitively, you want to use the smallest, 
simplest net that seems to fit the data.

how to formalize this intuition?

1. don   t.  just use intuition
2. bayesian methods get it right
3. statistical analysis explains what   s going on
4. cross-validation

discussed in the next 

lecture

copyright    2001, 2003, andrew w. moore

neural networks: slide 64

32

what you should know
    how to implement multivariate least-

squares id75.

    derivation of least squares as max. 

likelihood estimator of linear coefficients

    the general id119 rule

copyright    2001, 2003, andrew w. moore

neural networks: slide 65

what you should know

    id88s

(cid:198) linear output, least squares
(cid:198) sigmoid output, least squares

    multilayer nets

(cid:198) the idea behind back prop
(cid:198) awareness of better minimization methods

    generalization.  what it means.

copyright    2001, 2003, andrew w. moore

neural networks: slide 66

33

applications

to discuss:

    what can non-id75 be useful for?

    what can neural nets (used as non-linear 

regressors) be useful for?

    what are the advantages of n. nets for 

nonid75?

    what are the disadvantages?

copyright    2001, 2003, andrew w. moore

neural networks: slide 67

other uses of neural nets   

    time series with recurrent nets
    unsupervised learning (id91 principal 

components and non-linear versions 
thereof)

    combinatorial optimization with hopfield 

nets, id82s

    evaluation function learning (in 

id23)

copyright    2001, 2003, andrew w. moore

neural networks: slide 68

34

