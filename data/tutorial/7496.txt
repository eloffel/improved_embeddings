cross-validation for 

detecting and preventing 

overfitting

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    andrew w. moore

slide 1

a regression problem

y = f(x) + noise
can we learn f from this data?

y

x

let   s consider three methods   

copyright    andrew w. moore

slide 2

id75

y

x

copyright    andrew w. moore

slide 3

id75
univariate id75 with a constant term:
yx
3
7
1
3
:
:

3
1
:

7
3
:

x=

y=

x1=(3)..

y1=7..

originally 
discussed in the 
previous andrew 
lecture:    neural 
nets   

copyright    andrew w. moore

slide 4

id75
univariate id75 with a constant term:
yx
3
7
1
3
:
:

3
1
:

7
3
:

x=

y=

y1=7..

z=

3
1

1
1
:

x1=(3)..
y=
7
3
:

z1=(1,3)..
zk=(1,xk)

y1=7..

copyright    andrew w. moore

slide 5

id75
univariate id75 with a constant term:
yx
3
7
1
3
:
:

3
1
:

7
3
:

x=

y=

y1=7..

z=

3
1

1
1
:

x1=(3)..
y=
7
3
:

z1=(1,3)..
zk=(1,xk)

y1=7..

  =(ztz)-1(zty)

yest =   0+   1 x

copyright    andrew w. moore

slide 6

quadratic regression

y

x

copyright    andrew w. moore

slide 7

quadratic regression

yx
3
7
1
3
:
:
1
z=
1
:

3
1

9
1

z=(1 ,  x, x2
,)

x=

3
1
:

x1=(3,2)..
y=
7
3
:

much more about 
this in the future 
andrew lecture: 
   favorite 
regression 
algorithms   

y=

7
3
:

y1=7..

  =(ztz)-1(zty)

yest =   0+   1 x+   2 x2

copyright    andrew w. moore

slide 8

join-the-dots

also known as piecewise 

linear nonparametric 

regression if that makes 

you feel better

y

x

copyright    andrew w. moore

slide 9

which is best?

y

x

y

x

why not choose the method with the 
best fit to the data?

copyright    andrew w. moore

slide 10

what do we really want?

y

x

y

x

why not choose the method with the 
best fit to the data?

   how well are you going to predict 
future data drawn from the same 

distribution?   

copyright    andrew w. moore

slide 11

the test set method

1. randomly choose 
30% of the data to be in a 
test set
2. the remainder is a 
training set

y

x

copyright    andrew w. moore

slide 12

the test set method

1. randomly choose 
30% of the data to be in a 
test set
2. the remainder is a 
training set
3. perform your 
regression on the training 
set

y

x

(id75 example)

copyright    andrew w. moore

slide 13

the test set method

1. randomly choose 
30% of the data to be in a 
test set
2. the remainder is a 
training set
3. perform your 
regression on the training 
set
4. estimate your future 
performance with the test 
set

y

x

(id75 example)
mean squared error = 2.4

copyright    andrew w. moore

slide 14

the test set method

1. randomly choose 
30% of the data to be in a 
test set
2. the remainder is a 
training set
3. perform your 
regression on the training 
set
4. estimate your future 
performance with the test 
set

y

x

(quadratic regression example)

mean squared error = 0.9

copyright    andrew w. moore

slide 15

the test set method

1. randomly choose 
30% of the data to be in a 
test set
2. the remainder is a 
training set
3. perform your 
regression on the training 
set
4. estimate your future 
performance with the test 
set

y

x

(join the dots example)
mean squared error = 2.2

copyright    andrew w. moore

slide 16

the test set method

good news:
   very very simple
   can then simply choose the method with 
the best test-set score
bad news:
   what   s the downside?

copyright    andrew w. moore

slide 17

the test set method

good news:
   very very simple
   can then simply choose the method with 
the best test-set score
bad news:
   wastes data: we get an estimate of the 
best method to apply to 30% less data
   if we don   t have much data, our test-set 
might just be lucky or unlucky

we say the 
   test-set 
estimator of 
performance 
has high 
variance   

copyright    andrew w. moore

slide 18

loocv (leave-one-out cross validation)

for k=1 to r
1. let (xk,yk) be the kth record

y

x

copyright    andrew w. moore

slide 19

loocv (leave-one-out cross validation)

for k=1 to r
1. let (xk,yk) be the kth record
2. temporarily remove (xk,yk)

from the dataset

y

x

copyright    andrew w. moore

slide 20

loocv (leave-one-out cross validation)

for k=1 to r
1. let (xk,yk) be the kth record
2. temporarily remove (xk,yk)

from the dataset

3. train on the remaining r-1 

datapoints

y

x

copyright    andrew w. moore

slide 21

loocv (leave-one-out cross validation)

for k=1 to r
1. let (xk,yk) be the kth record
2. temporarily remove (xk,yk)

from the dataset

3. train on the remaining r-1 

datapoints

4. note your error (xk,yk)

y

x

copyright    andrew w. moore

slide 22

loocv (leave-one-out cross validation)

for k=1 to r
1. let (xk,yk) be the kth record
2. temporarily remove (xk,yk)

from the dataset

3. train on the remaining r-1 

datapoints

4. note your error (xk,yk)
when you   ve done all points, 
report the mean error.

y

x

copyright    andrew w. moore

slide 23

loocv (leave-one-out cross validation)

y

y

y

x

x

x

y

y

y

x

x

x

y

y

y

x

x

x

for k=1 to r

1. let (xk,yk) be 

the kth
record

2. temporarily 

remove 
(xk,yk) from 
the dataset
3. train on the 
remaining 
r-1 
datapoints

4. note your 

error (xk,yk)

when you   ve 
done all points, 
report the mean 
error.

mseloocv 

= 2.12

copyright    andrew w. moore

slide 24

loocv for quadratic regression

y

y

y

x

x

x

y

y

y

x

x

x

y

y

y

x

x

x

for k=1 to r

1. let (xk,yk) be 

the kth
record

2. temporarily 

remove 
(xk,yk) from 
the dataset
3. train on the 
remaining 
r-1 
datapoints

4. note your 

error (xk,yk)

when you   ve 
done all points, 
report the mean 
error.

mseloocv
=0.962

copyright    andrew w. moore

slide 25

loocv for join the dots

y

y

y

x

x

x

y

y

y

x

x

x

y

y

y

x

x

x

for k=1 to r

1. let (xk,yk) be 

the kth
record

2. temporarily 

remove 
(xk,yk) from 
the dataset
3. train on the 
remaining 
r-1 
datapoints

4. note your 

error (xk,yk)

when you   ve 
done all points, 
report the mean 
error.

mseloocv
=3.33

copyright    andrew w. moore

slide 26

which kind of cross validation?

test-set

leave-
one-out

downside
variance: unreliable 
estimate of future 
performance
expensive. 
has some weird 
behavior

upside
cheap

doesn   t 
waste data

..can we get the best of both worlds?

copyright    andrew w. moore

slide 27

k-fold cross 
validation

randomly break the dataset into k 
partitions (in our example we   ll have k=3 
partitions colored red green and blue)

y

x

copyright    andrew w. moore

slide 28

k-fold cross 
validation

randomly break the dataset into k 
partitions (in our example we   ll have k=3 
partitions colored red green and blue)
for the red partition: train on all the 
points not in the red partition. find 
the test-set sum of errors on the red 
points.

y

x

copyright    andrew w. moore

slide 29

k-fold cross 
validation

y

x

randomly break the dataset into k 
partitions (in our example we   ll have k=3 
partitions colored red green and blue)
for the red partition: train on all the 
points not in the red partition. find 
the test-set sum of errors on the red 
points.

for the green partition: train on all the 

points not in the green partition. 
find the test-set sum of errors on 
the green points.

copyright    andrew w. moore

slide 30

k-fold cross 
validation

y

x

randomly break the dataset into k 
partitions (in our example we   ll have k=3 
partitions colored red green and blue)
for the red partition: train on all the 
points not in the red partition. find 
the test-set sum of errors on the red 
points.

for the green partition: train on all the 

points not in the green partition. 
find the test-set sum of errors on 
the green points.

for the blue partition: train on all the 
points not in the blue partition. find 
the test-set sum of errors on the 
blue points.

copyright    andrew w. moore

slide 31

k-fold cross 
validation

y

x

id75 
mse3fold=2.05

randomly break the dataset into k 
partitions (in our example we   ll have k=3 
partitions colored red green and blue)
for the red partition: train on all the 
points not in the red partition. find 
the test-set sum of errors on the red 
points.

for the green partition: train on all the 

points not in the green partition. 
find the test-set sum of errors on 
the green points.

for the blue partition: train on all the 
points not in the blue partition. find 
the test-set sum of errors on the 
blue points.

then report the mean error

copyright    andrew w. moore

slide 32

k-fold cross 
validation

y

x

quadratic regression 
mse3fold=1.11

randomly break the dataset into k 
partitions (in our example we   ll have k=3 
partitions colored red green and blue)
for the red partition: train on all the 
points not in the red partition. find 
the test-set sum of errors on the red 
points.

for the green partition: train on all the 

points not in the green partition. 
find the test-set sum of errors on 
the green points.

for the blue partition: train on all the 
points not in the blue partition. find 
the test-set sum of errors on the 
blue points.

then report the mean error

copyright    andrew w. moore

slide 33

k-fold cross 
validation

y

x

joint-the-dots 
mse3fold=2.93

randomly break the dataset into k 
partitions (in our example we   ll have k=3 
partitions colored red green and blue)
for the red partition: train on all the 
points not in the red partition. find 
the test-set sum of errors on the red 
points.

for the green partition: train on all the 

points not in the green partition. 
find the test-set sum of errors on 
the green points.

for the blue partition: train on all the 
points not in the blue partition. find 
the test-set sum of errors on the 
blue points.

then report the mean error

copyright    andrew w. moore

slide 34

which kind of cross validation?

downside
variance: unreliable 
estimate of future 
performance

expensive. 
has some weird behavior
wastes 10% of the data. 
10 times more expensive 
than test set
wastier than 10-fold. 
expensivier than test set
identical to leave-one-out

test-set

leave-
one-out
10-fold

3-fold

r-fold

upside
cheap

doesn   t waste data

only wastes 10%. only 
10 times more expensive 
instead of r times.
slightly better than test-
set

copyright    andrew w. moore

slide 35

which kind of cross validation?

upside
cheap

downside
variance: unreliable 
estimate of future 
performance

expensive. 
has some weird behavior
wastes 10% of the data. 
10 times more expensive 
than testset
wastier than 10-fold. 
expensivier than testset
identical to leave-one-out

test-set

leave-
one-out
10-fold

3-fold

r-fold

but note: one of 
doesn   t waste data
andrew   s joys in life is 
algorithmic tricks for 
making these cheap

only wastes 10%. only 
10 times more expensive 
instead of r times.
slightly better than test-
set

copyright    andrew w. moore

slide 36

cv-based model selection

    we   re trying to decide which algorithm to use.
    we train each machine and make a table   

trainerr

10-fold-cv-err

choice

   

i
1
2
3
4
5
6

fi
f1
f2
f3
f4
f5
f6

copyright    andrew w. moore

slide 37

cv-based model selection
    example: choosing number of hidden units in a one-

hidden-layer neural net.

    step 1: compute 10-fold cv error for six different model 

classes:

trainerr

10-fold-cv-err

choice

   

algorithm
0 hidden units
1 hidden units
2 hidden units
3 hidden units
4 hidden units
5 hidden units

    step 2: whichever model class gave best cv score: train it 
with all the data, and that   s the predictive model you   ll use.

copyright    andrew w. moore

slide 38

cv-based model selection

    example: choosing    k    for a k-nearest-neighbor regression.
    step 1: compute loocv error for six different model 

classes:

trainerr

10-fold-cv-err

choice

   

algorithm
k=1
k=2
k=3
k=4
k=5
k=6

    step 2: whichever model class gave best cv score: train it 
with all the data, and that   s the predictive model you   ll use.

copyright    andrew w. moore

slide 39

cv-based model selection

    example: choosing    k    for a k-nearest-neighbor regression.
    step 1: compute loocv error for six different model 

classes:

algorithm
k=1
k=2
k=3
k=4
k=5
k=6

trainerr

loocv-err

why did we use 10-fold-cv for 
neural nets and loocv for k-
nearest neighbor?
and why stop at k=6
are we guaranteed that a local 
optimum of k vs loocv will be 
the global optimum?
what should we do if we are 
depressed at the expense of 
doing loocv for k= 1 through 
1000?

the reason is computational. for k-
nn (and all other nonparametric 
methods) loocv happens to be as 
cheap as regular predictions.

no good reason, except it looked 
like things were getting worse as k 
was increasing

choice

sadly, no. and in fact, the 
relationship can be very bumpy.

   

idea one: k=1, k=2, k=4, k=8, 
k=16, k=32, k=64     k=1024
idea two: hillclimbing from an initial 
guess at k

    step 2: whichever model class gave best cv score: train it 
with all the data, and that   s the predictive model you   ll use.

copyright    andrew w. moore

slide 40

cv-based model selection
    can you think of other decisions we can ask cross 

validation to make for us, based on other machine learning 
algorithms in the class so far?

copyright    andrew w. moore

slide 41

cv-based model selection
    can you think of other decisions we can ask cross 

validation to make for us, based on other machine learning 
algorithms in the class so far?
    degree of polynomial in polynomial regression
    whether to use full, diagonal or spherical gaussians in a gaussian 

bayes classifier.

    the kernel width in kernel regression
    the kernel width in locally weighted regression
    the bayesian prior in bayesian regression

these involve 
choosing the value of a 
real-valued parameter. 
what should we do?

copyright    andrew w. moore

slide 42

cv-based model selection
    can you think of other decisions we can ask cross 

validation to make for us, based on other machine learning 
algorithms in the class so far?
    degree of polynomial in polynomial regression
    whether to use full, diagonal or spherical gaussians in a gaussian 

bayes classifier.

    the kernel width in kernel regression
    the kernel width in locally weighted regression
    the bayesian prior in bayesian regression

these involve 
choosing the value of a 
real-valued parameter. 
what should we do?

idea one: consider a discrete set of values 
(often best to consider a set of values with 
exponentially increasing gaps, as in the id92 
example).
 
loocv
   
idea two: compute                       and then
parameter
 
   
do gradianet descent.

copyright    andrew w. moore

slide 43

cv-based model selection
    can you think of other decisions we can ask cross 

validation to make for us, based on other machine learning 
algorithms in the class so far?
    degree of polynomial in polynomial regression
    whether to use full, diagonal or spherical gaussians in a gaussian 

bayes classifier.

    the kernel width in kernel regression
    the kernel width in locally weighted regression
    the bayesian prior in bayesian regression

a l s o :   t h e   s c a l e   f a c t o r s   o f   a   n o n -
p a r a m e t r i c   d i s t a n c e   m e t r i c

these involve 
choosing the value of a 
real-valued parameter. 
what should we do?

idea one: consider a discrete set of values 
(often best to consider a set of values with 
exponentially increasing gaps, as in the id92 
example).
 
loocv
   
idea two: compute                       and then
parameter
 
   
do gradianet descent.

copyright    andrew w. moore

slide 44

cv-based algorithm choice
    example: choosing which regression algorithm to use
    step 1: compute 10-fold-cv error for six different model 

classes:

trainerr

10-fold-cv-err

choice

   

algorithm
1-nn
10-nn
linear reg   n
quad reg   n
lwr, kw=0.1
lwr, kw=0.5

    step 2: whichever algorithm gave best cv score: train it 

with all the data, and that   s the predictive model you   ll use.

copyright    andrew w. moore

slide 45

alternatives to cv-based model selection

    model selection methods:

1. cross-validation
2. aic (akaike information criterion)
3. bic (bayesian information criterion)
4. vc-dimension (vapnik-chervonenkis  dimension)

only directly applicable to 
choosing classifiers

described in a future 
lecture

copyright    andrew w. moore

slide 46

(cv) cross-validation

which model selection method is best?
1.
2. aic (akaike information criterion)
3. bic (bayesian information criterion)
4.

(srmvc) structural risk minimize with vc-dimension
    aic, bic and srmvc advantage: you only need the training 
error.
    cv error might have more variance
    srmvc is wildly conservative
    asymptotically aic and leave-one-out cv should be the same
    asymptotically bic and carefully chosen k-fold should be same
    you want bic if you want the best structure instead of the best 
predictor (e.g. for id91 or bayes net structure finding)
    many alternatives---including proper bayesian approaches.
    it   s an emotional issue.

copyright    andrew w. moore

slide 47

other cross-validation issues
    can do    leave all pairs out    or    leave-all-

ntuples-out    if feeling resourceful.

    some folks do k-folds in which each fold is 
an independently-chosen subset of the data

    do you know what aic and bic are?

if so   
    loocv behaves like aic asymptotically. 
    k-fold behaves like bic if you choose k carefully
if not   
    nyardely nyardely nyoo nyoo

copyright    andrew w. moore

slide 48

cross-validation for regression

    choosing the number of hidden units in a 

neural net

    feature selection (see later)
    choosing a polynomial degree
    choosing which regressor to use

copyright    andrew w. moore

slide 49

supervising id119
    this is a weird but common use of test-set 

validation

    suppose you have a neural net with too 

many hidden units. it will overfit.

    as id119 progresses, maintain a 

graph of mse-testset-error vs. iteration

use the weights you 
found on this iteration

training set

test set

 
d
e
r
a
u
q
s
 
n
a
e
m

r
o
r
r

e

iteration of id119

copyright    andrew w. moore

slide 50

supervising id119
    this is a weird but common use of test-set 

validation

    suppose you have a neural net with too 
relies on an intuition that a not-fully-
many hidden units. it will overfit.
minimized set of weights is somewhat like 
    as id119 progresses, maintain a 
having fewer parameters.
works pretty well in practice, apparently

graph of mse-testset-error vs. iteration

use the weights you 
found on this iteration

training set

test set

 
d
e
r
a
u
q
s
 
n
a
e
m

r
o
r
r

e

iteration of id119

copyright    andrew w. moore

slide 51

cross-validation for classification
    instead of computing the sum squared 

errors on a test set, you should compute   

copyright    andrew w. moore

slide 52

cross-validation for classification
    instead of computing the sum squared 

errors on a test set, you should compute   

the total number of misclassifications on 
a testset.

copyright    andrew w. moore

slide 53

cross-validation for classification
    instead of computing the sum squared 

errors on a test set, you should compute   

the total number of misclassifications on 
a testset.

    what   s loocv of 1-nn?
    what   s loocv of 3-nn?
    what   s loocv of 22-nn?

copyright    andrew w. moore

slide 54

cross-validation for classification
    instead of computing the sum squared 

errors on a test set, you should compute   

the total number of misclassifications on 
a testset.

    but there   s a more sensitive alternative:

compute 
log p(all test outputs|all test inputs, your model)

copyright    andrew w. moore

slide 55

cross-validation for classification

    choosing the pruning parameter for decision 

trees

    feature selection (see later)
    what kind of gaussian to use in a gaussian-

based bayes classifier

    choosing which classifier to use

copyright    andrew w. moore

slide 56

cross-validation for density 

estimation

    compute the sum of log-likelihoods of test 

points

example uses:
    choosing what kind of gaussian assumption 

to use

    choose the density estimator
    not feature selection (testset density will 

almost always look better with fewer 
features)

copyright    andrew w. moore

slide 57

feature selection

    suppose you have a learning algorithm la 
and a set of input attributes { x1 , x2 .. xm }

    you expect that la will only find some 

subset of the attributes useful.

    question: how can we use cross-validation 

to find a useful subset?

    four ideas:

another fun area in which 

    forward selection
    backward elimination
    hill climbing
    stochastic search (simulated annealing or gas)

wild youth

andrew has spent a lot of his 

copyright    andrew w. moore

slide 58

very serious warning

    intensive use of cross validation can overfit.
    how? 

    what can be done about it?

copyright    andrew w. moore

slide 59

very serious warning

    intensive use of cross validation can overfit.
    how? 

    imagine a dataset with 50 records and 1000 

attributes.

    you try 1000 id75 models, each one 

using one of the attributes.

    what can be done about it?

copyright    andrew w. moore

slide 60

very serious warning

    intensive use of cross validation can overfit.
    how? 

    imagine a dataset with 50 records and 1000 

attributes.

    you try 1000 id75 models, each one 

using one of the attributes.

    the best of those 1000 looks good!

    what can be done about it?

copyright    andrew w. moore

slide 61

very serious warning

    intensive use of cross validation can overfit.
    how? 

    imagine a dataset with 50 records and 1000 

attributes.

    you try 1000 id75 models, each one 

using one of the attributes.

    the best of those 1000 looks good!
    but you realize it would have looked good even if the 

output had been purely random!
    what can be done about it?

    hold out an additional testset before doing any model 

selection. check the best model performs well even 
on the additional testset.

copyright    andrew w. moore

    or: randomization testing

slide 62

what you should know

    why you can   t use    training-set-error    to 

estimate the quality of your learning 
algorithm on your data.

    why you can   t use    training set error    to 

choose the learning algorithm

    test-set cross-validation
    leave-one-out cross-validation
    k-fold cross-validation
    feature selection methods
    cv for classification, regression & densities

copyright    andrew w. moore

slide 63

