note to other teachers and users of these slides. andrew would 
be delighted if you found this source material useful in giving 
your own lectures. feel free to use these slides verbatim, or to
modify them to fit your own needs. powerpoint originals are 
available. if you make use of a significant portion of these sli des 
in your own lecture, please include this message, or the 
following link to the source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . comments and 
corrections gratefully received. 

pac-learning

andrew w. moore
associate professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2001, andrew w. moore

nov 30th, 2001

probably approximately correct 

(pac) learning

    imagine we   re doing classification with categorical 

inputs.

    all inputs and outputs are binary.
    data is noiseless.
    there   s a machine f(x,h) which has h possible 
settings (a.k.a. hypotheses), called h1, h2 .. hh.

copyright    2001, andrew w. moore

pac-learning: slide  2

1

example of a machine

    f(x,h) consists of all logical sentences about x1, x2 

.. xm that contain only logical ands.

    example hypotheses:
    x1 ^ x3 ^ x19
    x3 ^ x18
    x7
    x1 ^ x2 ^ x2 ^ x4     ^ xm
    question: if there are 3 attributes, what is the 

complete set of hypotheses in f? 

copyright    2001, andrew w. moore

pac-learning: slide  3

example of a machine

    f(x,h) consists of all logical sentences about x1, x2 

.. xm that contain only logical ands.

    example hypotheses:
    x1 ^ x3 ^ x19
    x3 ^ x18
    x7
    x1 ^ x2 ^ x2 ^ x4     ^ xm
    question: if there are 3 attributes, what is the 

complete set of hypotheses in f? (h = 8)
x2 ^ x3

true

x3

x2

x1

x1 ^ x2

x1 ^ x3

x1 ^ x2 ^ x3

copyright    2001, andrew w. moore

pac-learning: slide  4

2

and-positive-literals machine

    f(x,h) consists of all logical sentences about x1, x2 

.. xm that contain only logical ands.

    example hypotheses:
    x1 ^ x3 ^ x19
    x3 ^ x18
    x7
    x1 ^ x2 ^ x2 ^ x4     ^ xm
    question: if there are m attributes, how many 

hypotheses in f?

copyright    2001, andrew w. moore

pac-learning: slide  5

and-positive-literals machine

    f(x,h) consists of all logical sentences about x1, x2 

.. xm that contain only logical ands.

    example hypotheses:
    x1 ^ x3 ^ x19
    x3 ^ x18
    x7
    x1 ^ x2 ^ x2 ^ x4     ^ xm
    question: if there are m attributes, how many 

hypotheses in f? (h = 2m)

copyright    2001, andrew w. moore

pac-learning: slide  6

3

and-literals machine

    f(x,h) consists of all logical 
sentences about x1, x2 .. 
xm or their negations that 
contain only logical ands.

    example hypotheses:
    x1 ^ ~x3 ^ x19
    x3 ^ ~x18
    ~x7
    x1 ^ x2 ^ ~x3 ^     ^ xm
    question: if there are 2 

attributes, what is the 
complete set of hypotheses 
in f? 

copyright    2001, andrew w. moore

pac-learning: slide  7

and-literals machine

    f(x,h) consists of all logical 
sentences about x1, x2 .. 
xm or their negations that 
contain only logical ands.

    example hypotheses:
    x1 ^ ~x3 ^ x19
    x3 ^ ~x18
    ~x7
    x1 ^ x2 ^ ~x3 ^     ^ xm
    question: if there are 2 

attributes, what is the 
complete set of hypotheses 
in f? (h = 9)

true
true
true
x1
x1
x1
~x1
~x1
~x1

^
^

^
^

true
x2
~x2
true
x2
~x2
true
x2
~x2

copyright    2001, andrew w. moore

pac-learning: slide  8

4

and-literals machine

    f(x,h) consists of all logical 
sentences about x1, x2 .. 
xm or their negations that 
contain only logical ands.

    example hypotheses:
    x1 ^ ~x3 ^ x19
    x3 ^ ~x18
    ~x7
    x1 ^ x2 ^ ~x3 ^     ^ xm
    question: if there are m 

attributes, what is the size of 
the complete set of 
hypotheses in f? 

true
true
true
x1
x1
x1
~x1
~x1
~x1

^
^

^
^

true
x2
~x2
true
x2
~x2
true
x2
~x2

copyright    2001, andrew w. moore

pac-learning: slide  9

and-literals machine

    f(x,h) consists of all logical 
sentences about x1, x2 .. 
xm or their negations that 
contain only logical ands.

    example hypotheses:
    x1 ^ ~x3 ^ x19
    x3 ^ ~x18
    ~x7
    x1 ^ x2 ^ ~x3 ^     ^ xm
    question: if there are m 

attributes, what is the size of 
the complete set of 
hypotheses in f? (h = 3m)

true
true
true
x1
x1
x1
~x1
~x1
~x1

^
^

^
^

true
x2
~x2
true
x2
~x2
true
x2
~x2

copyright    2001, andrew w. moore

pac-learning: slide  10

5

lookup table machine

    f(x,h) consists of all truth 

tables mapping combinations 
of input attributes to true 
and false

    example hypothesis:

    question: if there are m 

attributes, what is the size of 
the complete set of 
hypotheses in f? 

x1
0

x2
0

x3
0

x4
0

0
0
0

0
0
0

0
1
1

1
1
1

1
1
1

0
0
0

1
1
1

1
0
0

0
0
1

1
1
1

0
1
1

0
0
1

1
0
0

1
1
0

0
1
1

1
0
1

0
1
0

1
0
1

0
1
0

1
0
1

y
0

1
1
0

1
0
0

1
0
0

0
1
0

0
0
0

copyright    2001, andrew w. moore

pac-learning: slide  11

lookup table machine

    f(x,h) consists of all truth 

tables mapping combinations 
of input attributes to true 
and false

    example hypothesis:

    question: if there are m 

attributes, what is the size of 
the complete set of 
hypotheses in f? 

h

m

22=

x1
0

x2
0

x3
0

x4
0

0
0
0

0
0
0

0
1
1

1
1
1

1
1
1

0
0
0

1
1
1

1
0
0

0
0
1

1
1
1

0
1
1

0
0
1

1
0
0

1
1
0

0
1
1

1
0
1

0
1
0

1
0
1

0
1
0

1
0
1

y
0

1
1
0

1
0
0

1
0
0

0
1
0

0
0
0

copyright    2001, andrew w. moore

pac-learning: slide  12

6

a game

    we specify f, the machine
    nature choose hidden random hypothesis h*
    nature randomly generates r datapoints

   how is a datapoint generated?

1.vector of inputs xk = (xk1,xk2, xkm) is 

drawn from a fixed unknown distrib: d
2.the corresponding output yk=f(xk , h*)
    we learn an approximation of h* by choosing 
some hest for which the training set error is 0

copyright    2001, andrew w. moore

pac-learning: slide  13

test error rate

    we specify f, the machine
    nature choose hidden random hypothesis h*
    nature randomly generates r datapoints

   how is a datapoint generated?

1.vector of inputs xk = (xk1,xk2, xkm) is 

drawn from a fixed unknown distrib: d
2.the corresponding output yk=f(xk , h*)
    we learn an approximation of h* by choosing 
some hest for which the training set error is 0

    for each hypothesis h ,
    say h is correctly classified (ccd) if h has 

zero training set error
    define testerr(h ) 

= fraction of test points that h will classify 

correctly

= p(h classifies a random test point 

correctly) 

    say h is bad if testerr(h) > e

copyright    2001, andrew w. moore

pac-learning: slide  14

7

hp

)
 is (
bad is  | ccd
=
 set,
 
training
)
1(

h
,
(
hxf
r)

e-

k

=
hy
k

bad is  |

)

kp

(

test error rate

    we specify f, the machine
    nature choose hidden random hypothesis h*
    nature randomly generates r datapoints

   how is a datapoint generated?

1.vector of inputs xk = (xk1,xk2, xkm) is 

drawn from a fixed unknown distrib: d
2.the corresponding output yk=f(xk , h*)
    we learn an approximation of h* by choosing 
some hest for which the training set error is 0

    for each hypothesis h ,
    say h is correctly classified (ccd) if h has 

zero training set error
    define testerr(h ) 

= fraction of test points that i will classify 

correctly

= p(h classifies a random test point 

correctly) 

    say h is bad if testerr(h) > e

copyright    2001, andrew w. moore

pac-learning: slide  15

test error rate

    we specify f, the machine
    nature choose hidden random hypothesis h*
    nature randomly generates r datapoints

   how is a datapoint generated?

1.vector of inputs xk = (xk1,xk2, xkm) is 

drawn from a fixed unknown distrib: d
2.the corresponding output yk=f(xk , h*)
    we learn an approximation of h* by choosing 
some hest for which the training set error is 0

    for each hypothesis h ,
    say h is correctly classified (ccd) if h has 

zero training set error
    define testerr(h ) 

= fraction of test points that i will classify 

correctly

= p(h classifies a random test point 

correctly) 

    say h is bad if testerr(h) > e

kp

(

p

 we(

learn 

k

hp

h
,
(
hxf
r)

 is (
bad is  | ccd
)
=
 set,
 
training
)
e-
1(
  ) bad
h
the
set 
 
 of
contains

 ccd
h
s'
 bad
h
 a 

=(cid:247)

 a

p

(
hhp

  is  .

bad is  ^ ccd

h

=
hy
k

bad is  |

)

)=

(
h
1
(
h
2

is 
is 

p

 is 
 is 

bad)
bad)

 ^ ccd
  
h
1
 ^ ccd
  
h
2
:

 ^ ccd
  
h
h

 is 

bad)

h
(
h
h

=
1
i
is 

is 
(
hp
i
 | ccd
h
  
i

is 

  

)   
bad

 ^ ccd
 is 
h
i
) =

bad is 
)

  is 

h
 | ccd
i

bad is 

h

1(

e-

r

)

h

(
hp
i
(
hph
i

=
1

i

copyright    2001, andrew w. moore

pac-learning: slide  16

8

  
"
  
  
"
  
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
  
(cid:230)
$
  
(cid:247)
(cid:247)
(cid:247)
(cid:247)
(cid:247)
  
(cid:246)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
(cid:231)
  
(cid:230)
(cid:218)
(cid:218)
(cid:229)
(cid:229)
  
  
pac learning

    chose r such that with id203 less than d we   ll 
select a bad hest (i.e. an hest which makes mistakes 
more than fraction e of the time)
    probably approximately correct
    as we just saw, this can be achieved by choosing 

r such that
p

=

d

 we(

learn 

) bad a

hh

1(

r

e

)

    i.e. r such that

r

69.0
e

log

2 h

+

log

2

1
d

copyright    2001, andrew w. moore

pac-learning: slide  17

pac in action

machine

and-positive-
literals
and-literals

example 
hypothesis
x3 ^ x7 ^ x8

x3 ^ ~x7

h

2m

3m

r required to pac-
learn

69.0

e
69.0
e

+

m

log

1
d

2

(log

)3
2 m

+

log

2

1
d

lookup 
table

x1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1

x2
0
0
0
0
1
1
1
1
0
0
0
0
1
1
1
1

x3
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1

x4
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1

y
0
1
1
0
1
0
0
1
0
0
0
1
0
0
0
0

m22

69.0
e

+

m

2

log

2

1
d

and-lits or 
and-lits

(x1 ^ x5) v 
(x2 ^ ~x7 ^ x8)

m
)3(

2

=

2

m

3

69.0
e

log2(

)3
2 m

+

log

2

1
d

copyright    2001, andrew w. moore

pac-learning: slide  18

9

-
  
(cid:247)
  
(cid:246)
(cid:231)
  
(cid:230)
   
(cid:247)
  
(cid:246)
(cid:231)
  
(cid:230)
(cid:247)
  
(cid:246)
(cid:231)
  
(cid:230)
(cid:247)
  
(cid:246)
(cid:231)
  
(cid:230)
(cid:247)
  
(cid:246)
(cid:231)
  
(cid:230)
pac for id90 of depth k

    assume m attributes
    hk = number of id90 of depth k

    h0 =2
    hk+1 = (#choices of root attribute) *

(# possible left subtrees) *
(# possible right subtrees)

= m * hk * hk

    write lk = log2 hk
    l0 = 1
    lk+1 = log2 m + 2lk
    so lk = (2 k-1)(1+log2 m) +1
    so to pac-learn, need

r

69.0
e

k

2(

1)(1

+

log

2 m

++
1)

log

2

1
d

copyright    2001, andrew w. moore

pac-learning: slide  19

what you should know

    be able to understand every step in the math that 

gets you to
=
p

d

 we(

learn 

) bad a

hh

1(

r

e

)

    understand that you thus need this many records 

to pac-learn a machine with h hypotheses

r

69.0
e

log

2 h

+

log

2

1
d

    understand examples of deducing h for various 

machines

copyright    2001, andrew w. moore

pac-learning: slide  20

10

(cid:247)
  
(cid:246)
(cid:231)
  
(cid:230)
-
   
-
  
(cid:247)
  
(cid:246)
(cid:231)
  
(cid:230)
   
