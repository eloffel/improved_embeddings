id203 densities 

in data mining

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    andrew w. moore

slide 1

id203 densities in data mining
    why we should care
    notation and fundamentals of continuous 

pdfs

    multivariate continuous pdfs
    combining continuous and discrete random 

variables

copyright    andrew w. moore

slide 2

1

why we should care

    real numbers occur in at least 50% of 

database records

    can   t always quantize them
    so need to understand how to describe 

where they come from

    a great way of saying what   s a reasonable 

range of values

    a great way of saying how multiple 

attributes should reasonably co-occur

copyright    andrew w. moore

slide 3

why we should care

    can immediately get us bayes classifiers 

that are sensible with real-valued data

    you   ll need to intimately understand pdfs in 
order to do kernel methods, id91 with 
mixture models,  analysis of variance, time 
series and many other things

    will introduce us to linear and non-linear 

regression

copyright    andrew w. moore

slide 4

2

a pdf of american ages in 2000

copyright    andrew w. moore

slide 5

a pdf of american ages in 2000

let x be a continuous random 
variable.
if p(x) is a id203 density 
function for x then   
b
(
   
bxap

dxxp
)(

=

   

<

)

ax
=

p

(
30

<

age

   

50

)

=

= 0.36

50

   

age

=

(

age

)

d
age

p
30

copyright    andrew w. moore

slide 6

3

properties of pdfs

(
bxap

   

<

)

=

b

   

ax
=

dxxp
)(

that means   

hxxhxp
   
+   
   
2
   

<   

2

h

   
   
   

xp
)(

=

lim

h

   

0

(
xxp

   

)

=

)(xp

   
x
   

copyright    andrew w. moore

slide 7

properties of pdfs

(
bxap

   

<

)

=

b

   

ax
=

dxxp
)(

therefore   

   

   

      =x

dxxp
)(

=

1

(
xxp

   

)

=

)(xp

therefore   

   
x
   

   

xpx
:

0)(

   

copyright    andrew w. moore

slide 8

4

talking to your stomach
    what   s the gut-feel meaning of p(x)?

if 

p(5.31) = 0.06 and p(5.92) = 0.03

then 

when a value x is sampled from the 

distribution, you are 2 times as likely to find 
that x is    very close to    5.31 than that x is 
   very close to    5.92.

copyright    andrew w. moore

slide 9

talking to your stomach
    what   s the gut-feel meaning of p(x)?

if 

p(5.31) = 0.06 and p(5.92) = 0.03

b

a

then 

when a value x is sampled from the 

distribution, you are 2 times as likely to find 
that x is    very close to    5.31 than that x is 
   very close to    5.92.b

a

copyright    andrew w. moore

slide 10

5

talking to your stomach
    what   s the gut-feel meaning of p(x)?

if 

p(5.31) = 0.03 and p(5.92) = 0.06

2z

z

b

a

then 

when a value x is sampled from the 

distribution, you are 2 times as likely to find 
that x is    very close to    5.31 than that x is 
   very close to    5.92.b

a

copyright    andrew w. moore

slide 11

talking to your stomach
    what   s the gut-feel meaning of p(x)?

if 

p(5.31) = 0.03 and p(5.92) = 0.06

  z

z

b

a

then 

when a value x is sampled from the 

distribution, you are   times as likely to find 
that x is    very close to    5.31 than that x is 
   very close to    5.92.b

a

copyright    andrew w. moore

slide 12

6

talking to your stomach
    what   s the gut-feel meaning of p(x)?

if 

then 

ap
)(
bp
)(

=

  

when a value x is sampled from the 

distribution, you are   times as likely to find 
that x is    very close to    5.31 than that x is 
   very close to    5.92.b

a

copyright    andrew w. moore

slide 13

talking to your stomach
    what   s the gut-feel meaning of p(x)?

if 

then 

ap
)(
bp
)(

=

  

lim
h
0
   

haxhap
(
)
+<
hbxhbp
)
(
+<

<   
<   

=

  

copyright    andrew w. moore

slide 14

7

yet another way to view a pdf

a recipe for sampling a random 

age.

1. generate a random dot 

from the rectangle 
surrounding the pdf curve. 
call the dot (age,d)
if d < p(age) stop and 
return age

2.

3. else try again: go to step 1.

copyright    andrew w. moore

slide 15

test your understanding

    true or false:

   

xpx
:

1)(
   

   

xxpx
:

=

(

0)
=

copyright    andrew w. moore

slide 16

8

expectations

e[x] = the expected value of 
random variable x
= the average value we   d see 
if we took a very large number 
of random samples of x
dx

xpx
)(

=

   

   

x

      =

copyright    andrew w. moore

slide 17

expectations

e[x] = the expected value of 
random variable x
= the average value we   d see 
if we took a very large number 
of random samples of x
dx

xpx
)(

=

   

   

x

      =

e[age]=35.897

= the first moment of the 
shape formed by the axes and 
the blue curve
= the best value to choose if 
you must guess an unknown 
person   s age and you   ll be 
fined the square of your error

copyright    andrew w. moore

slide 18

9

expectation of a function

  =e[f(x)] = the expected 
value of f(x) where x is drawn 
from x   s distribution. 
= the average value we   d see 
if we took a very large number 
of random samples of f(x)
dx

xpxf
)(
)(

  
=

   

   

x

      =

note that in general:
xfe
xef
(
[
(
[

)]

   

])

e
]
[
age
2 =
e
age
])
[(
2 =

1786
1288

64.
62.

copyright    andrew w. moore

slide 19

variance

  2 = var[x] = the 
expected squared 
difference between 
x and e[x] 

2
  

=

x

   

   

(
      =

x

   

)
  

2

xp
)(

dx

[var

age

]

=

02.498

= amount you   d expect to lose 
if you must guess an unknown 
person   s age and you   ll be 
fined the square of your error, 
and assuming you play 
optimally

copyright    andrew w. moore

slide 20

10

standard deviation

  2 = var[x] = the 
expected squared 
difference between 
x and e[x] 

2
  

=

x

   

   

(
      =

x

   

)
  

2

xp
)(

dx

02.498

age

[var
]
=
32.22=  

= amount you   d expect to lose 
if you must guess an unknown 
person   s age and you   ll be 
fined the square of your error, 
and assuming you play 
optimally
   = standard deviation = 
   typical    deviation of x from 
its mean

=  

[var x

]

copyright    andrew w. moore

slide 21

in 2 

dimensions

p(x,y) = id203 density of 

random variables (x,y) at 

location (x,y)

copyright    andrew w. moore

slide 22

11

in 2 

dimensions

let x,y be a pair of continuous random 
variables, and let r be some region of (x,y) 
space   
yxp
,
((

=   

yxp
,(

dydx

r

)

)

)

      

ryx
)
,(
   

copyright    andrew w. moore

slide 23

in 2 

dimensions

let x,y be a pair of continuous random 
variables, and let r be some region of (x,y) 
space   
yxp
,
((

=   

yxp
,(

dydx

r

)

)

)

      

ryx
)
,(
   

p( 20<mpg<30 and

2500<weight<3000) =

area under the 2-d surface within 
the red rectangle

copyright    andrew w. moore

slide 24

12

in 2 

dimensions

let x,y be a pair of continuous random 
variables, and let r be some region of (x,y) 
space   
yxp
,
((

=   

yxp
,(

dydx

r

)

)

)

      

ryx
)
,(
   

p( [(mpg-25)/10]2 + 

[(weight-3300)/1500]2

< 1 ) =

area under the 2-d surface within 
the red oval

copyright    andrew w. moore

slide 25

in 2 

dimensions

let x,y be a pair of continuous random 
variables, and let r be some region of (x,y) 
space   
yxp
,
((

=   

yxp
,(

dydx

r

)

)

)

      

ryx
)
,(
   

take the special case of region r =    everywhere   .
remember that with id203 1, (x,y) will be drawn from 
   somewhere   . 
so..

   

   

      =

x

y

   

   

      =

yxp
,(

)

dydx

=

1

copyright    andrew w. moore

slide 26

13

in 2 

dimensions

let x,y be a pair of continuous random 
variables, and let r be some region of (x,y) 
space   
yxp
,
((

=   

yxp
,(

dydx

r

)

)

)

      

ryx
)
,(
   

yxp
,(

=)

hxxhxp
   
+   
   
2
   

<   

2

   

2

h

yhy
+   <   

y

2

h
2

   
   
   

lim

h

   

0

copyright    andrew w. moore

slide 27

in m 

dimensions
xxp
x
((
,
,...,
2
1
          
xxp
...
,
(
x
   r
)

,...,

1

m

(

xx
,
1

2

let (x1,x2,   xm)be an n-tuple of continuous 
random variables, and let r be some region 
of rm    

)
m
,...,

r
=    )
x
dx

)

m

2

,

,...

dx

,

dx
1

2

m

copyright    andrew w. moore

slide 28

14

independence
 
   
=

yxp
,( :yx,

)

iff 

yx

   

ypxp
)(
)(

if x and y are independent 
then knowing the value of x 

does not help predict the 

value of y

mpg,weight not 

independent

copyright    andrew w. moore

slide 29

independence
 
=
   

yxp
,( :yx,

)

iff 

yx

   

ypxp
)(
)(

if x and y are independent 
then knowing the value of x 

does not help predict the 

value of y

the contours say that 

acceleration and weight are 

independent

copyright    andrew w. moore

slide 30

15

multivariate expectation
  x

xx
d
)(

e
[

x

=

x

p

]

   =

e[mpg,weight] =
(24.5,2600)

the centroid of the 
cloud

copyright    andrew w. moore

slide 31

multivariate expectation
xx
fe
d
[
)(

x
)(

x

)]

p

(

f

   =

copyright    andrew w. moore

slide 32

16

test your understanding
xe
 when 
[
:
]

ever)
 
does
 

yxe
[ 

(if

=

+

]

question

+

ye
? ][

   all the time?
   only when x and y are independent?
   it can fail even if x and y are independent?

copyright    andrew w. moore

slide 33

bivariate expectation
yxfe
dydx
,(
[

yxpyxf
,(
,(

)]

)

)

   =

 if

yxf
,(

)

=

 if

yxf
,(

)

=

 if

yxf
,(

)

+=

x

x
 then 

yxfe
[
,

(

y
 then 

yxfe
[
,

(

y
 then 

yxfe
[
,

(

)]

   =

)]

   =

)]

=

   

yxpx
,(

)

dydx

yxpy
,(

)

dydx

(

x

+

yxpy
,(
)

)

dydx

yxe
[ 

+

]

=

xe
[

]

+

ye
][

copyright    andrew w. moore

slide 34

17

bivariate covariance
y
)(
cov[
=
   

xe
[(

yx
,

   

=

]

  
x

  
y

)]

  
xy

2
    
x
xx
2
    

=
=

yy

cov[
=
cov[
=

xx
,
yy
,

y

]
]

=
=

x
var
]
[
=
yvar
][
=

xe
[(
])
2
  
   
x
ye
[(
])
2
  
   
y

copyright    andrew w. moore

slide 35

bivariate covariance
y
)(
cov[
=
   

xe
[(

yx
,

   

=

]

  
x

  
y

)]

  
xy

2
    
x
xx
2
    

=
=

yy

cov[
=
cov[
=

xx
,
yy
,

y

]
]

 write

 x

=

 

var
x
]
[
=
=
yvar
][
=
=
x
   
   
, 
 then
      
      
y
   
   

xe
[(
])
2
  
   
x
ye
[(
])
2
   
  
y

cov
x
 [

(e 
]
[
=

  x  x

)(

   

   

x

)
t

 
]

=

  

=

x

2

x

   
    
   
xy
   
2
    
   

xy

y

   
   
   
   

copyright    andrew w. moore

slide 36

18

covariance intuition

e[mpg,weight] =
(24.5,2600)

weight =  

700

weight =  

700

mpg =  

mpg =  8

8

copyright    andrew w. moore

slide 37

covariance intuition

principal
eigenvector
of   

e[mpg,weight] =
(24.5,2600)

weight =  

700

weight =  

700

mpg =  

mpg =  8

8

copyright    andrew w. moore

slide 38

19

covariance fun facts
   
    
   
xy
   
2
    
   

  x  x

)(

  

)
t

 
]

   

=

=

   

xy

2

x

x

x

y

   
   
   
   

cov
x
 [

(e 
]
[
=

   true or false: if   xy = 0 then x and y are 
independent
   true or false: if x and y are independent 
then   xy = 0
   true or false: if   xy =   x   y then x and y are 
deterministically related
   true or false: if x and y are deterministically 
related then   xy =   x   y

how could 
you prove 
or disprove 
these?

copyright    andrew w. moore

slide 39

general covariance

let x= (x1,x2,    xk)be a vector of kcontinuous random variables
  

  x  x

x
cov
 [

(e 
]
[
=

)(

)
t

 
]

=

   

   

x

x

  

ij

=

xxcov

[

,

i

]

j

  =
ixx

j

s is a k x k symmetric non-negative definite matrix
if all distributions are linearly independent it is positive definite
if the distributions are linearly dependent it has determinant zero

copyright    andrew w. moore

slide 40

20

test your understanding
x
 when 
:
]

var
[
ever)
does
 
 
 

yx

var
[

(if

=

+

]

question

+

yvar

? ][

   all the time?
   only when x and y are independent?
   it can fail even if x and y are independent?

copyright    andrew w. moore

slide 41

marginal distributions

xp
)(

=

y

yxp
,(

)

dy

   

   

      =

copyright    andrew w. moore

slide 42

21

conditional 
distributions

p

(

|mpg

weight

=

4600
)

p

(

|mpg

weight

=

3200
)

p

(

|mpg

weight

=

2000
)

)

yxp
(
x
 of 

|
=
y
 when 

p.d.f.

=

y

copyright    andrew w. moore

slide 43

p

(

|mpg

weight

=

4600
)

conditional 
distributions

yxp
(

|

)

=

)

yxp
,(
yp
)(

why?

)

yxp
(
x
 of 

|
=
y
 when 

p.d.f.

=

y

copyright    andrew w. moore

slide 44

22

independence revisited
ypxp
)(
)(
   

yxp
,( :yx,

iff 

 
   

=

)

yx

it   s easy to prove that these statements are equivalent   
ypxp
)(
)(

   

=

yxp
)
,( :yx,
   
yxp
|
   
xyp
|

( :yx,

( :yx,

   

   

)

=

xp
)(

)

=

yp
)(

copyright    andrew w. moore

slide 45

more useful stuff
yxp
(

dx

1

=

)

|

   

   

(these can all be 
proved from 
definitions on 
previous slides)

      =x
zyxp
(
),
|

yxp
(

|

)

=

=

)

zyxp
,(
|
zyp
(
|
)

xpxyp
(
)(

)
|
yp
)(

bayes
rule

copyright    andrew w. moore

slide 46

23

mixing discrete and continuous variables

vaxp
,(
=

)

=

lim

h

   

0

hxxhxp
   
   
2
   

<   

2

va
=   +   

h

   
   
   

   

an

       

v

1
=

x

      =

dxvaxp
,(

=

)

=

1

axp
(
)

|

=

xpxap
(
)(

|
)
ap
)
(

xap
(

|

)

=

apaxp
(
)

(

|
)
xp
)(

bayes
rule

bayes
rule

copyright    andrew w. moore

slide 47

mixing discrete and continuous variables

p(eduyears,wealthy)

copyright    andrew w. moore

slide 48

24

mixing discrete and continuous variables

p(eduyears,wealthy)

p(wealthy| eduyears)

copyright    andrew w. moore

slide 49

mixing discrete and continuous variables

p(eduyears,wealthy)

p(eduyears|wealthy)

p(wealthy| eduyears)

d
e
z
i
l

a
m
r
o
n
e
r

s
e
x
a

copyright    andrew w. moore

slide 50

25

what you should know

    you should be able to play with discrete, 

continuous and mixed joint distributions
    you should be happy with the difference 

between p(x) and p(a)

    you should be intimate with expectations of 

continuous and discrete random variables

    you should smile when you meet a 

covariance matrix

    independence and its consequences should 

be second nature

copyright    andrew w. moore

slide 51

discussion

    are pdfs the only sensible way to handle analysis 

of real-valued variables?

    why is covariance an important concept?
    suppose x and y are independent real-valued 
random variables distributed between 0 and 1:
    what is p[min(x,y)]? 
    what is e[min(x,y)]?

    prove that e[x]is the value uthat minimizes   

e[(x-u)2]

    what is the value u that minimizes e[|x-u|]?

copyright    andrew w. moore

slide 52

26

