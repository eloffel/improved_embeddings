probabilistic and 
bayesian analytics

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    andrew w. moore

slide 1

id203

    the world is a very uncertain place
    30 years of artificial intelligence and 

database research danced around this fact
    and then a few ai researchers decided to 

use some ideas from the eighteenth century

copyright    andrew w. moore

slide 2

1

what we   re going to do
    we will review the fundamentals of 

id203.

    it   s really going to be worth it 
    in this lecture, you   ll see an example of 

probabilistic analytics in action: bayes 
classifiers

copyright    andrew w. moore

slide 3

discrete random variables

    a is a boolean-valued random variable if a 

denotes an event, and there is some degree 
of uncertainty as to whether a occurs.

    examples
    a = the us president in 2023 will be male
    a = you wake up tomorrow with a 

headache

    a = you have ebola

copyright    andrew w. moore

slide 4

2

probabilities

    we write p(a) as    the fraction of possible 

worlds in which a is true   

    we could at this point spend 2 hours on the 

philosophy of this.

    but we won   t.

copyright    andrew w. moore

slide 5

visualizing a

event space of 
all possible 
worlds

its area is 1

worlds in which 
a is true

p(a) = area of
reddish oval

worlds in which a is false

copyright    andrew w. moore

slide 6

3

the axioms of id203

copyright    andrew w. moore

slide 7

the axioms of id203

    0 <= p(a) <= 1
    p(true) = 1
    p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

where do these axioms come from? were they    discovered   ? 
answers coming up later.

copyright    andrew w. moore

slide 8

4

interpreting the axioms

    0 <= p(a) <= 1
    p(true) = 1
    p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

the area of a can   t get 
any smaller than 0

and a zero area would 
mean no world could 
ever have a true 

copyright    andrew w. moore

slide 9

interpreting the axioms

    0 <= p(a) <= 1
    p(true) = 1
    p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

the area of a can   t get 
any bigger than 1

and an area of 1 would 
mean all worlds will have 
a true 

copyright    andrew w. moore

slide 10

5

interpreting the axioms

    0 <= p(a) <= 1
    p(true) = 1
    p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

a

b

copyright    andrew w. moore

slide 11

interpreting the axioms

    0 <= p(a) <= 1
    p(true) = 1
    p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

a

b

p(a or b)

p(a and b)

b

simple addition and subtraction

copyright    andrew w. moore

slide 12

6

these axioms are not to be 

trifled with

    there have been attempts to do different 

methodologies for uncertainty

    fuzzy logic
    three-valued logic
    dempster-shafer
    non-monotonic reasoning

    but the axioms of id203 are the only 

system with this property: 
if you gamble using them you can   t be unfairly exploited 
by an opponent using some other system [di finetti 1931]

copyright    andrew w. moore

slide 13

theorems from the axioms

    0 <= p(a) <= 1, p(true) = 1, p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

from these we can prove:
p(not a) = p(~a) = 1-p(a)

    how?

copyright    andrew w. moore

slide 14

7

side note

   

i am inflicting these proofs on you for two 
reasons:
1. these kind of manipulations will need to be 
second nature to you if you use probabilistic 
analytics in depth

2. suffering is good for you

copyright    andrew w. moore

slide 15

another important theorem

    0 <= p(a) <= 1, p(true) = 1, p(false) = 0
    p(a or b) = p(a) + p(b) - p(a and b)

from these we can prove:
p(a) = p(a ^ b) + p(a ^ ~b)

    how?

copyright    andrew w. moore

slide 16

8

multivalued random variables
    suppose a can take on more than 2 values
    a is a random variable with arity kif it can 
take on exactly one value out of {v1,v2, .. 
vk}

    thus   

vap
(
i
vap
(
1

va
=   =
va
=   =

j

2

i
 if 0)
=
kva
=   

j
   
1)
=

copyright    andrew w. moore

slide 17

an easy fact about multivalued 

random variables:

    using the axioms of id203   

0 <= p(a) <= 1, p(true) = 1, p(false) = 0
p(a or b) = p(a) + p(b) - p(a and b)

    and assuming that a obeys   
va
=   =
va
=   =

vap
(
i
vap
(
1

j

2

i
 if 0)
=
kva
=   

j
   
1)
=

    it   s easy to prove that

vap
(
1

va
=   =

2

va
=   
i

)

=

i

   

j

1
=

vap
(
=

)

j

copyright    andrew w. moore

slide 18

9

an easy fact about multivalued 

random variables:

    using the axioms of id203   

0 <= p(a) <= 1, p(true) = 1, p(false) = 0
p(a or b) = p(a) + p(b) - p(a and b)

    and assuming that a obeys   
va
=   =
va
=   =

vap
(
i
vap
(
1

j

2

i
 if 0)
=
kva
=   

j
   
1)
=

    it   s easy to prove that

vap
(
1

va
=   =

2

va
=   
i

)

=

    and thus we can prove

=   

jvap
(

1)
=

k

j

1
=

i

   

j

1
=

vap
(
=

)

j

copyright    andrew w. moore

slide 19

another fact about multivalued 

random variables:

    using the axioms of id203   

0 <= p(a) <= 1, p(true) = 1, p(false) = 0
p(a or b) = p(a) + p(b) - p(a and b)

    and assuming that a obeys   
va
=   =
va
=   =

vap
(
i
vap
(
1

j

2

i
 if 0)
=
kva
=   

j
   
1)
=

    it   s easy to prove that

bp
(

   

[

va
1

va
=   =

2

va
=   
i

])

=

i

   

j

1
=

vabp
(
=   

)

j

copyright    andrew w. moore

slide 20

10

another fact about multivalued 

random variables:

    using the axioms of id203   

0 <= p(a) <= 1, p(true) = 1, p(false) = 0
p(a or b) = p(a) + p(b) - p(a and b)

    and assuming that a obeys   
va
=   =
va
=   =

vap
(
i
vap
(
1

j

2

i
 if 0)
=
kva
=   

j
   
1)
=

    it   s easy to prove that

bp
(

   

[

va
1

va
=   =

2

    and thus we can prove
bp
(

)

=

k

   

j

1
=

va
=   
i

])

=

vabp
(
=   

)

j

i

   

j
jvabp
(

=   

1
=
)

copyright    andrew w. moore

slide 21

elementary id203 in pictures
    p(~a) + p(a) = 1

copyright    andrew w. moore

slide 22

11

elementary id203 in pictures
    p(b) = p(b ^ a) + p(b ^ ~a)

copyright    andrew w. moore

slide 23

elementary id203 in pictures
=   

jvap
(

1)
=

k

j

1
=

copyright    andrew w. moore

slide 24

12

elementary id203 in pictures
bp
(

jvabp
(

=   

=

)

)

k

   

j

1
=

copyright    andrew w. moore

slide 25

id155

    p(a|b) = fraction of worlds in which b is 

true that also have a true

f

h

h =    have a headache   
f =    coming down with flu   

p(h) = 1/10
p(f) = 1/40
p(h|f) = 1/2

   headaches are rare and flu 
is rarer, but if you   re 
coming down with    flu 
there   s a 50-50 chance 
you   ll have a headache.   

copyright    andrew w. moore

slide 26

13

id155

f

h

h =    have a headache   
f =    coming down with flu   

p(h) = 1/10
p(f) = 1/40
p(h|f) = 1/2

p(h|f) = fraction of flu-inflicted 
worlds in which you have a 
headache

= #worlds with flu and headache
------------------------------------

#worlds with flu

= area of    h and f    region
------------------------------

area of    f    region

= p(h ^ f)
-----------
p(f) 

copyright    andrew w. moore

slide 27

definition of id155

p(a ^ b) 
p(a|b)  =  -----------
p(b) 

corollary: the chain rule

p(a ^ b) = p(a|b) p(b) 

copyright    andrew w. moore

slide 28

14

probabilistic id136

f

h

h =    have a headache   
f =    coming down with flu   

p(h) = 1/10
p(f) = 1/40
p(h|f) = 1/2

one day you wake up with a headache. you think:    drat! 
50% of flus are associated with headaches so i must have a 
50-50 chance of coming down with flu   

is this reasoning good?

copyright    andrew w. moore

slide 29

probabilistic id136

f

h

h =    have a headache   
f =    coming down with flu   

p(h) = 1/10
p(f) = 1/40
p(h|f) = 1/2

p(f ^ h) =    

p(f|h) =    

copyright    andrew w. moore

slide 30

15

another way to understand the 

intuition

thanks to jahanzeb sherwani for contributing this explanation:

copyright    andrew w. moore

slide 31

what we just did   

p(a ^ b)     p(a|b) p(b)
p(b|a) = ----------- = ---------------

p(a)             p(a)

this is bayes rule

bayes, thomas (1763) an essay 
towards solving a problem in the 
doctrine of chances. philosophical 
transactions of the royal society of 
london, 53:370-418

copyright    andrew w. moore

slide 32

16

using bayes rule to gamble

$1.00

the    win    envelope 

has a dollar and four 
beads in it

the    lose    envelope 

has three beads and 
no money

trivial question: someone draws an envelope at random and offers to 
sell it to you. how much should you pay?

copyright    andrew w. moore

slide 33

using bayes rule to gamble

$1.00

the    win    envelope 

has a dollar and four 
beads in it

the    lose    envelope 

has three beads and 
no money

interesting question: before deciding, you are allowed to see one bead 
drawn from the envelope.

suppose it   s black: how much should you pay? 
suppose it   s red: how much should you pay?

copyright    andrew w. moore

slide 34

17

calculation   

$1.00

copyright    andrew w. moore

slide 35

more general forms of bayes rule

bap
)|
(

=

apabp
(
)()
)()
|~(

|
+

apabpapabp
(
)

(~)

|

xbap
(

   

|

)

=

xapxabp
(

   

|

()
   
xbp
(
)
   

)

copyright    andrew w. moore

slide 36

18

more general forms of bayes rule

bvap
(
)|

=

i

=

i

|

=

()

vapvabp
(
)
=
i
an
   
vapvabp
(
=
k

()

=

|

k

k

1
=

)

copyright    andrew w. moore

slide 37

useful easy-to-prove facts

1)
=

|

|

)
  +

bapbap
(
(
=   

k bvap
(

an

|

1)
=

k

1
=

copyright    andrew w. moore

slide 38

19

the joint distribution

example: boolean 
variables a, b, c

recipe for making a joint distribution 

of m variables:

copyright    andrew w. moore

slide 39

the joint distribution

recipe for making a joint distribution 

of m variables:

1. make a truth table listing all 

combinations of values of your 
variables (if there are m boolean 
variables then the table will have 
2m rows).

a
0

0

0

0

1

1

1

1

example: boolean 
variables a, b, c
b
0

c
0

0

1

1

0

0

1

1

1

0

1

0

1

0

1

copyright    andrew w. moore

slide 40

20

the joint distribution

recipe for making a joint distribution 

of m variables:

1. make a truth table listing all 

combinations of values of your 
variables (if there are m boolean 
variables then the table will have 
2m rows).

2. for each combination of values, 

say how probable it is.

a
0

0

0

0

1

1

1

1

example: boolean 
variables a, b, c
b
prob
0
0.30

c
0

0

1

1

0

0

1

1

1

0

1

0

1

0

1

0.05

0.10

0.05

0.05

0.10

0.25

0.10

copyright    andrew w. moore

slide 41

the joint distribution

recipe for making a joint distribution 

of m variables:

1. make a truth table listing all 

combinations of values of your 
variables (if there are m boolean 
variables then the table will have 
2m rows).

2. for each combination of values, 

3.

say how probable it is.
if you subscribe to the axioms of 
id203, those numbers must 
sum to 1.

example: boolean 
variables a, b, c
b
prob
0
0.30

c
0

0

1

1

0

0

1

1

1

0

1

0

1

0

1

0.05

0.10

0.05

0.05

0.10

0.25

0.10

a
0

0

0

0

1

1

1

1

a

0.05

0.25

0.05

0.10

0.10

0.05

c

0.10

b

0.30

copyright    andrew w. moore

slide 42

21

using the 

joint

one you have the jd you can 
ask for the id203 of any 
logical expression involving 
your attribute

ep
(

)

   =

p
row(
e
 
 
matching
rows

)

copyright    andrew w. moore

slide 43

using the 

joint

p(poor male) = 0.4654

ep
(

)

   =

p
row(
e
 
 
matching
rows

)

copyright    andrew w. moore

slide 44

22

using the 

joint

p(poor) = 0.7604

ep
(

)

   =

p
row(
e
 
 
matching
rows

)

copyright    andrew w. moore

slide 45

id136 
with the 

joint

eep
(

|

1

)

=

2

)

2

ep
(
   
1
ep
(

2

e
)

=

)

   
p
row(
e
e
and
 
 
 
matching
rows
 1
   
p
row(
e
 
rows
matching
 

)

 2

2

copyright    andrew w. moore

slide 46

23

id136 
with the 

joint

eep
(

|

1

)

=

2

)

2

ep
(
   
1
ep
(

2

e
)

=

)

   
p
row(
e
e
and
 
 
 
matching
rows
 1
   
p
row(
e
 
rows
matching
 

)

 2

2

p(male | poor) = 0.4654 / 0.7604 = 0.612  

copyright    andrew w. moore

slide 47

id136 is a big deal

    i   ve got this evidence. what   s the chance 

that this conclusion is true?
    i   ve got a sore neck: how likely am i to have meningitis?
    i see my lights are out and it   s 9pm. what   s the chance 

my spouse is already asleep?

copyright    andrew w. moore

slide 48

24

id136 is a big deal

    i   ve got this evidence. what   s the chance 

that this conclusion is true?
    i   ve got a sore neck: how likely am i to have meningitis?
    i see my lights are out and it   s 9pm. what   s the chance 

my spouse is already asleep?

copyright    andrew w. moore

slide 49

id136 is a big deal

    i   ve got this evidence. what   s the chance 

that this conclusion is true?
    i   ve got a sore neck: how likely am i to have meningitis?
    i see my lights are out and it   s 9pm. what   s the chance 

my spouse is already asleep?

    there   s a thriving set of industries growing based 

around bayesian id136. highlights are: 
medicine, pharma, help desk support, engine 
fault diagnosis

copyright    andrew w. moore

slide 50

25

where do joint distributions 

come from?

    idea one: expert humans
    idea two: simpler probabilistic facts and 

some algebra

example: suppose you knew

p(a) = 0.7

p(b|a) = 0.2
p(b|~a) = 0.1

p(c|a^b) = 0.1
p(c|a^~b) = 0.8
p(c|~a^b) = 0.3
p(c|~a^~b) = 0.1

then you can automatically 
compute the jd using the 
chain rule

p(a=x ^ b=y ^ c=z) =

p(c=z|a=x^ b=y) p(b=y|a=x) p(a=x)

in another lecture: 
bayes nets, a 
systematic way to 
do this.

copyright    andrew w. moore

slide 51

where do joint distributions 

come from?

    idea three: learn them from data!

prepare to see one of the most impressive learning 

algorithms you   ll come across in the entire course   .

copyright    andrew w. moore

slide 52

26

learning a joint distribution

build a jd table for your 
attributes in which the 
probabilities are unspecified

a
0

0

0

0

1

1

1

1

b
0

0

1

1

0

0

1

1

c
0

1

0

1

0

1

0

1

prob
?

?

?

?

?

?

?

?

the fill in each row with

row(  
p

)

=

matching
 
records
total

number 

row 
records

of

a
0

0

0

0

1

1

1

1

b
0

0

1

1

0

0

1

1

c
0

1

0

1

0

1

0

1

prob
0.30

0.05

0.10

0.05

0.05

0.10

0.25

0.10

fraction of all records in which
a and b are true but c is false

copyright    andrew w. moore

slide 53

example of learning a joint

    this joint was 

obtained by 
learning from 
three 
attributes in 
the uci 
   adult   
census 
database 
[kohavi 1995]

copyright    andrew w. moore

slide 54

27

where are we?

    we have recalled the fundamentals of 

id203

    we have become content with what jds are 

and how to use them

    and we even know how to learn jds from 

data.

copyright    andrew w. moore

slide 55

density estimation

    our joint distribution learner is our first 

example of something called density 
estimation

    a density estimator learns a mapping from 

a set of attributes to a id203

input

attributes

density
estimator

id203

copyright    andrew w. moore

slide 56

28

density estimation

    compare it against the two other major 

kinds of models:

input

attributes

input

attributes

input

attributes

classifier

density
estimator

regressor

prediction of

categorical output

id203

prediction of

real-valued output

copyright    andrew w. moore

slide 57

evaluating density estimation

test-set criterion for estimating performance 
on future data*
* see the decision tree or cross validation lecture for more detail

input

attributes

input

attributes

input

attributes

classifier

density
estimator

prediction of

categorical output

test set 
accuracy

id203

?

regressor

prediction of

real-valued output

test set 
accuracy

copyright    andrew w. moore

slide 58

29

evaluating a density estimator

    given a record x, a density estimator mcan 

tell you how likely the record is:

(  
|mp x

)

    given a dataset with rrecords, a density 

estimator can tell you how likely the dataset 
is:
(under the assumption that all records were independently

generated from the density estimator   s jd)
   
dataset

|m
r

(  
p

|m

x
1

k

   

   

=

=

x

x

)

)

r

2

(  
p

(  
|mp

x

k

)

k

1
=

copyright    andrew w. moore

slide 59

a small dataset: miles per gallon

192 
training 
set 
records

mpg modelyear maker

good
bad
bad
bad
bad
bad
bad
bad
:
:
:
bad
good
bad
good
bad
good
good
bad
good
bad

75to78
70to74
75to78
70to74
70to74
70to74
70to74
75to78
:
:
:
70to74
79to83
75to78
79to83
75to78
79to83
79to83
70to74
75to78
75to78

asia
america
europe
america
america
asia
asia
america
:
:
:
america
america
america
america
america
america
america
america
europe
europe

from the uci repository (thanks to ross quinlan)

copyright    andrew w. moore

slide 60

30

a small dataset: miles per gallon

192 
training 
set 
records

mpg modelyear maker

good
bad
bad
bad
bad
bad
bad
bad
:
:
:
bad
good
bad
good
bad
good
good
bad
good
bad

75to78
70to74
75to78
70to74
70to74
70to74
70to74
75to78
:
:
:
70to74
79to83
75to78
79to83
75to78
79to83
79to83
70to74
75to78
75to78

asia
america
europe
america
america
asia
asia
america
:
:
:
america
america
america
america
america
america
america
america
europe
europe

copyright    andrew w. moore

slide 61

a small dataset: miles per gallon

mpg modelyear maker

192 
training 
set 
records

good
bad
bad
bad
bad
bad
bad
bad
:
:
:
bad
good
bad
good
bad
good
good
bad
good
bad

75to78
70to74
75to78
70to74
70to74
70to74
70to74
75to78
:
:
:
70to74
79to83
75to78
79to83
75to78
79to83
79to83
70to74
75to78
75to78

=

asia
america
europe
america
america
asia
asia
america
:
:
:
america
america
america
america
america
america
america
america
europe
europe

(  
p

dataset

|m

(  
p

)

x

   

x
k
1
2
 
 
(in this
case)
10  3.4 
 
=
=

|m
r

   

  

x

)

-

203

=

r

   

k

1
=

(  
|mp

x

k

)

copyright    andrew w. moore

slide 62

31

log probabilities

since probabilities of datasets get so 
small we usually use log probabilities

log

(  
p

dataset

|m

)

=

log

r

   

k

1
=

(  
|mp

x

k

)

=

r

   

k

1
=

log

(  
|mp

x

k

)

copyright    andrew w. moore

slide 63

a small dataset: miles per gallon

mpg modelyear maker

192 
training 
set 
records

good
bad
bad
bad
bad
bad
bad
bad
:
:
:
bad
good
bad
good
bad
good
good
bad
good
bad

75to78
70to74
75to78
70to74
70to74
70to74
70to74
75to78
:
:
:
70to74
79to83
75to78
79to83
75to78
79to83
79to83
70to74
75to78
75to78

asia
america
europe
america
america
asia
asia
america
:
:
:
america
america
america
america
america
america
america
america
europe
europe

log

(  
p

dataset

r

=

(  
   
x
|mp
|m
log
)
k
k
1
=
 
 
 
case)
(in this
 
   =
=

   
k
1
=
466.19

=

)

r

log

(  
|mp

x

k

)

copyright    andrew w. moore

slide 64

32

summary: the good news

    we have a way to learn a density estimator 

from data.

    density estimators can do many good 

things   
    can sort the records by id203, and thus 

spot weird records (anomaly detection)

    can do id136: p(e1|e2)

automatic doctor / help desk etc

    ingredient for bayes classifiers (see later)

copyright    andrew w. moore

slide 65

summary: the bad news

    density estimation by directly learning the 

joint is trivial, mindless and dangerous

copyright    andrew w. moore

slide 66

33

using a test set

an independent test set with 196 cars has a worse log likelihood

(actually it   s a billion quintillion quintillion quintillion quintillion 
times less likely)

   .density estimators can overfit. and the full joint density 
estimator is the overfittiest of them all!

copyright    andrew w. moore

slide 67

overfitting density estimators

if this ever happens, it means 
there are certain combinations 
that we learn are impossible

log

(  
p

testset

|m

)

=

log

r

r

=

)

(  
   
x
|mp
k
1
=
(   
|mpk
any 

x

k

   
k
1
=
0)
=

k

log

(  
|mp

x

k

)

 
      =

 if 

for 

copyright    andrew w. moore

slide 68

34

using a test set

the only reason that our test set didn   t score -infinity is that my 
code is hard-wired to always predict a id203 of at least one 
in 1020

we need density estimators that are less prone 

to overfitting

copyright    andrew w. moore

slide 69

na  ve density estimation

the problem with the joint estimator is that it just 
mirrors the training data.
we need something which generalizes more usefully.

the na  ve model generalizes strongly:
assume that each attribute is distributed 
independently of any of the other attributes.

copyright    andrew w. moore

slide 70

35

independently distributed data
    let x[i]denote the i   th field of record x.
    the independently distributed assumption 
says that for any i,v, u1u2   ui-1ui+1   um
ixp
][(
]

xv
]1[|

]1
=+

]2[,

ix
[,

=

=

=

u

u

,

,

xu
1

i

1
+

k

umx
[

=

)

m

2

ix
[
]1
=   
ixp
][(
=

u
i
v
)

k
=

1
   

    or in other words, x[i]is independent of 

{x[1],x[2],..x[i-1], x[i+1],   x[m]}

    this is often written as  
],1
   

x
],1[{

x
],2[

ix
][

ix
[

   

k

ix
[

+

],1

k

mx
[

]}

copyright    andrew w. moore

slide 71

a note about independence
    assume a and b are boolean random 

variables. then

   a and b are independent   

if and only if

p(a|b) = p(a)

       a and b are independent    is often notated 

as

ba    

copyright    andrew w. moore

slide 72

36

independence theorems

    assume p(a|b) = p(a)
    then p(a^b) =

    assume p(a|b) = p(a)
    then p(b|a) =

= p(a) p(b)

copyright    andrew w. moore

= p(b)

slide 73

independence theorems

    assume p(a|b) = p(a)
    then p(~a|b) =

    assume p(a|b) = p(a)
    then p(a|~b) =

= p(~a)

copyright    andrew w. moore

= p(a)

slide 74

37

multivalued independence

for multivalued random variables a and b,

ba    
(
|

:

=

   

if and only if
vbuapvu
,
=

=
from which you can then prove things like   
   

vbuapvu
,
=   =
|

)
(
=
vavbpvu
,
=

vbpuap
(
=
)
=

)
(
=
vbp
(
=

uap
(

:
   

=

=

)

)

(

:

)

)

copyright    andrew w. moore

slide 75

back to na  ve density estimation

let x[i] denote the i   th field of record x:

   
    na  ve de assumes x[i]is independent of {x[1],x[2],..x[i-1], x[i+1],   x[m]}
    example: 

    suppose that each record is generated by randomly shaking a green dice 

and a red dice

    dataset 1: a = red value, b = green value

    dataset 2: a = red value, b = sum of values

    dataset 3: a = sum of values, b = difference of values

    which of these datasets violates the na  ve assumption?

copyright    andrew w. moore

slide 76

38

using the na  ve distribution

    once you have a na  ve distribution you can easily 

compute any row of the joint distribution.
    suppose a, b, c and dare independently 

distributed. what is p(a^~b^c^~d)?

copyright    andrew w. moore

slide 77

using the na  ve distribution

    once you have a na  ve distribution you can easily 

compute any row of the joint distribution.
    suppose a, b, c and d are independently 

distributed. what is p(a^~b^c^~d)?

= p(a|~b^c^~d) p(~b^c^~d)
= p(a) p(~b^c^~d)
= p(a) p(~b|c^~d) p(c^~d)
= p(a) p(~b) p(c^~d)
= p(a) p(~b) p(c|~d) p(~d)
= p(a) p(~b) p(c) p(~d)

copyright    andrew w. moore

slide 78

39

na  ve distribution general case
    suppose x[1], x[2],    x[m]are independently 

distributed.
xp
xu
]1[(
1

=

]2[,

=

u

,

2

k

umx
[

=

]

)

=

m

m

   

k

1
=

kxp
][(

=

u

k

)

    so if we have a na  ve distribution we can 

construct any row of the implied joint distribution 
on demand.

    so we can do any id136 
    but how do we learn a na  ve density estimator?

copyright    andrew w. moore

slide 79

learning a na  ve density 

estimator

][(  
ixp

=

u

)

=

#

in which 
 
records
total

number 

of

ix
u
][
 
=
records

another trivial learning algorithm!

copyright    andrew w. moore

slide 80

40

contrast

joint de

na  ve de

can model anything

no problem to model    c 
is a noisy copy of a   
given 100 records and more than 6 
boolean attributes will screw up 
badly

can model only very 
boring distributions
outside na  ve   s scope

given 100 records and 10,000 
multivalued attributes will be fine

copyright    andrew w. moore

slide 81

empirical results:    hopeless   

the    hopeless    dataset consists of 40,000 records and 21 boolean 
attributes called a,b,c,     u. each attribute in each record is generated 
50-50 randomly as 0 or 1.

average test set log 
id203 during 
10 folds of k-fold 
cross-validation*

described in a future andrew lecture

despite the vast amount of data,    joint    overfits hopelessly and 
does much worse

copyright    andrew w. moore

slide 82

41

empirical results:    logical   

the    logical    dataset consists of 40,000 records and 4 boolean 
attributes called a,b,c,d where a,b,c are generated 50-50 randomly as 0 
or 1. d = a^~c, except that in 10% of records it is flipped

the de 

learned by 

   joint   

the de 

learned by 

   naive   

copyright    andrew w. moore

slide 83

empirical results:    logical   

the    logical    dataset consists of 40,000 records and 4 boolean 
attributes called a,b,c,d where a,b,c are generated 50-50 randomly as 0 
or 1. d = a^~c, except that in 10% of records it is flipped

the de 

learned by 

   joint   

the de 

learned by 

   naive   

copyright    andrew w. moore

slide 84

42

empirical results:    mpg   
the    mpg    dataset consists of 392 records and 8 attributes

a tiny part of 

the de 

learned by 

   joint   

the de 

learned by 

   naive   

copyright    andrew w. moore

slide 85

empirical results:    mpg   
the    mpg    dataset consists of 392 records and 8 attributes

a tiny part of 

the de 

learned by 

   joint   

the de 

learned by 

   naive   

copyright    andrew w. moore

slide 86

43

empirical results:    weight vs. mpg   

suppose we train only from the    weight    and    mpg    attributes

the de 

learned by 

   joint   

the de 

learned by 

   naive   

copyright    andrew w. moore

slide 87

empirical results:    weight vs. mpg   

suppose we train only from the    weight    and    mpg    attributes

the de 

learned by 

   joint   

the de 

learned by 

   naive   

copyright    andrew w. moore

slide 88

44

   weight vs. mpg   : the best that na  ve can do

the de 

learned by 

   joint   

the de 

learned by 

   naive   

copyright    andrew w. moore

slide 89

reminder: the good news
    we have two ways to learn a density 

estimator from data.

    *in other lectures we   ll see vastly more 
impressive density estimators (mixture models, 
id110s, density trees, kernel densities and many more)
    density estimators can do many good 

things   
    anomaly detection
    can do id136: p(e1|e2) automatic doctor / help desk etc
    ingredient for bayes classifiers

copyright    andrew w. moore

slide 90

45

bayes classifiers

    a formidable and sworn enemy of decision 

trees

input

attributes

classifier

prediction of

categorical output

dt

bc

copyright    andrew w. moore

slide 91

how to build a bayes classifier

    assume you want to predict output ywhich has arity ny and values 

v1, v2,     vny.

    assume there are minput attributes called x1, x2,     xm
    break dataset into ny smaller datasets called ds1, ds2,     dsny.
    define dsi = records in which y=vi
   

for each dsi  , learn density estimator mi  to model the input 
distribution among the y=virecords.

copyright    andrew w. moore

slide 92

46

how to build a bayes classifier

    assume you want to predict output ywhich has arity ny and values 

v1, v2,     vny.

    assume there are minput attributes called x1, x2,     xm
    break dataset into ny smaller datasets called ds1, ds2,     dsny.
    define dsi = records in which y=vi
   

for each dsi  , learn density estimator mi  to model the input 
distribution among the y=virecords.

    mi  estimates p(x1, x2,     xm| y=vi)

copyright    andrew w. moore

slide 93

how to build a bayes classifier

    assume you want to predict output ywhich has arity ny and values 

v1, v2,     vny.

    assume there are minput attributes called x1, x2,     xm
    break dataset into ny smaller datasets called ds1, ds2,     dsny.
    define dsi = records in which y=vi
   

for each dsi  , learn density estimator mi  to model the input 
distribution among the y=virecords.

    mi  estimates p(x1, x2,     xm| y=vi)

    idea: when a new set of input values (x1= u1, x2= u2,    . xm
= um) come along to be evaluated predict the value of y that 
makes p(x1, x2,     xm| y=vi) most likely
x

argmax

xp
(

predict

y

=

=

=

=

v

)

|

yu
m

u
1

m

1

l

v

is this a good idea?

copyright    andrew w. moore

slide 94

47

how to build a bayes classifier

v1, v2,     vny.

    assume you want to predict output ywhich has arity ny and values 
this is a maximum likelihood 

    assume there are minput attributes called x1, x2,     xm
classifier.
    break dataset into ny smaller datasets called ds1, ds2,     dsny.
    define dsi = records in which y=vi
   

for each dsi  , learn density estimator mi  to model the input 
distribution among the y=virecords.

it can get silly if some ys are 

very unlikely

    mi  estimates p(x1, x2,     xm| y=vi)

    idea: when a new set of input values (x1= u1, x2= u2,    . xm
= um) come along to be evaluated predict the value of y that 
makes p(x1, x2,     xm| y=vi) most likely
x

argmax

xp
(

predict

y

=

=

=

=

v

)

|

yu
m

u
1

m

1

l

v

is this a good idea?

copyright    andrew w. moore

slide 95

how to build a bayes classifier

    assume you want to predict output ywhich has arity ny and values 

v1, v2,     vny.

    assume there are minput attributes called x1, x2,     xm
    break dataset into ny smaller datasets called ds1, ds2,     dsny.
    define dsi = records in which y=vi
   

for each dsi  , learn density estimator mi  to model the input 
distribution among the y=virecords.

much better idea

    mi  estimates p(x1, x2,     xm| y=vi)

    idea: when a new set of input values (x1= u1, x2= u2,    . xm
= um) come along to be evaluated predict the value of y that 
makes p(y=vi| x1, x2,     xm) most likely
=

argmax

xv
|

yp
(

predict

x

y

=

=

=

u

)

m

m

u
1

l

1

v

is this a good idea?

copyright    andrew w. moore

slide 96

48

terminology

    id113 (maximum likelihood estimator):

predict

y

=

argmax

v

xp
(

1

=

u
1

l

x

m

=

yu
m

|

=

v

)

    map (maximum a-posteriori estimator):

predict

y

=

argmax

v

yp
(

=

xv
|

=

u
1

1

x

m

=

u

)

m

l

copyright    andrew w. moore

slide 97

getting what we need
=
=

argmax

xv
|

yp
(

x

=

=

u
1

l

m

1

u

m

)

predict

y

v

copyright    andrew w. moore

slide 98

49

getting a posterior id203

yp
(

=

xv
|

=

u
1

1

x

m

=

u

)

m

l

=

=

xp
(

1

=

xp
(

1
xp
(

=

=

1

yn

   

j

1
=

u
l
1
xp
(
u
1
u
1

l

yu
|
m
x
|

x
=
m
u
=
l
m
1
1
yu
x
=
m
yu
m

m
x

=

m

|

l

=
=
=

ypv
)
(
u
m
ypv
)
(

)

=

v

)

=

v

)

=

ypv
(
j

)

=

v

)

j

copyright    andrew w. moore

slide 99

bayes classifiers in a nutshell

1. learn the distribution over inputs for each value y.
2. this gives p(x1, x2,     xm| y=vi).
3. estimate  p(y=vi). as fraction of records with y=vi.
4. for a new prediction:

predict

y
=
argmax
=

v

argmax
v
xp
(
=

1

yp
(
u
1

l

xv
|
=
x
=

m

u
1
1
yu
m

=
|

m

x
=
ypv
(
)

l
=

u
m
=

)
v

)

copyright    andrew w. moore

slide 100

50

bayes classifiers in a nutshell

1. learn the distribution over inputs for each value y.
2. this gives p(x1, x2,     xm| y=vi).
3. estimate  p(y=vi). as fraction of records with y=vi.
4. for a new prediction:

predict

y
=
argmax
=

v

argmax
v
xp
(
=

1

yp
(
u
1

l

xv
|
=
x
=

m

we can use our favorite 
density estimator here.
u
x
=
=
right now we have two 
1
1
options:
yu
ypv
|
(
)
m
   joint density estimator
   na  ve density estimator

u
m
=

l
=

)
v

)

m

copyright    andrew w. moore

slide 101

predict

joint density bayes classifier
vypvyu
=
m

y
=
in the case of the joint bayes classifier this 
degenerates to a very simple rule:

argmax

xp
(

u
1

l

x

=

=

=

(

)

m

|

1

v

)

ypredict= the most common value of y among records 
in which x1= u1, x2= u2,    . xm= um.

note that if no records have the exact set of inputs x1
= u1, x2= u2,    . xm= um, then p(x1, x2,     xm| y=vi) 
= 0 for all values of y.

in that case we just have to guess y   s value

copyright    andrew w. moore

slide 102

51

joint bc results:    logical   

the    logical    dataset consists of 40,000 records and 4 boolean 
attributes called a,b,c,d where a,b,c are generated 50-50 randomly as 0 
or 1. d = a^~c, except that in 10% of records it is flipped

the classifier 
learned by 
   joint bc   

copyright    andrew w. moore

slide 103

joint bc results:    all irrelevant   

the    all irrelevant    dataset consists of 40,000 records and 15 boolean 
attributes called a,b,c,d..o where a,b,c are generated 50-50 randomly 
as 0 or 1. v (output) = 1 with id203 0.75, 0 with prob 0.25

copyright    andrew w. moore

slide 104

52

predict

na  ve bayes classifier
argmax
)
=

y
vypvyu
=
m
in the case of the naive bayes classifier this can be 
simplified:

xp
(

u
1

l

x

=

=

=

(

m

|

1

v

yn

   
)

j

1
=

xp
(

=

vyu
=
j

|

)

j

predict

y

=

argmax

v

vyp
(
=

copyright    andrew w. moore

predict

na  ve bayes classifier
argmax
)
=

vypvyu
y
=
m
in the case of the naive bayes classifier this can be 
simplified:

xp
(

u
1

l

x

=

=

=

(

m

|

1

v

predict

y

=

argmax

v

vyp
(
=

yn

   
)

j

1
=

xp
(

=

vyu
=
j

|

)

j

)

)

slide 105

technical hint:
if you have 10,000 input attributes that product will 
underflow in floating point math. you should use logs:

predict

y

=

   
      
argmax
   

v

log

vyp
(
=

)

+

yn

   

j

1
=

log

xp
(

=

vyu
=
j

|

)

j

   
      
   

copyright    andrew w. moore

slide 106

53

bc results:    xor   

the    xor    dataset consists of 40,000 records and 2 boolean inputs called a 
and b, generated 50-50 randomly as 0 or 1. c (output) = a xor b

the classifier 
learned by 
   joint bc   

the classifier 
learned by 
   naive bc   

copyright    andrew w. moore

slide 107

naive bc results:    logical   

the    logical    dataset consists of 40,000 records and 4 boolean 
attributes called a,b,c,d where a,b,c are generated 50-50 randomly as 0 
or 1. d = a^~c, except that in 10% of records it is flipped

the classifier 
learned by 
   naive bc   

copyright    andrew w. moore

slide 108

54

naive bc results:    logical   

the    logical    dataset consists of 40,000 records and 4 boolean 
attributes called a,b,c,d where a,b,c are generated 50-50 randomly as 0 
or 1. d = a^~c, except that in 10% of records it is flipped

this result surprised andrew until he 
had thought about it a little

the classifier 
learned by 
   joint bc   

copyright    andrew w. moore

slide 109

na  ve bc results:    all irrelevant   

the    all irrelevant    dataset consists 
of 40,000 records and 15 boolean 
attributes called a,b,c,d..o where 
a,b,c are generated 50-50 randomly 
as 0 or 1. v (output) = 1 with 
id203 0.75, 0 with prob 0.25

the classifier 
learned by 
   naive bc   

copyright    andrew w. moore

slide 110

55

bc results: 
   mpg   : 392 

records

the classifier 
learned by 
   naive bc   

copyright    andrew w. moore

slide 111

bc results: 
   mpg   : 40 

records

copyright    andrew w. moore

slide 112

56

more facts about bayes 

classifiers

    many other density estimators can be slotted in*.
    density estimation can be performed with real-valued 

inputs*

    bayes classifiers can be built with real-valued inputs*
    rather technical complaint: bayes classifiers don   t try to 
be maximally discriminative---they merely try to honestly 
model what   s going on*

    zero probabilities are painful for joint and na  ve. a hack 

(justifiable with the magic words    dirichlet prior   ) can 
help*.

    na  ve bayes is wonderfully cheap. and survives 10,000 

attributes cheerfully!

*see future andrew lectures

copyright    andrew w. moore

slide 113

what you should know

    id203

    fundamentals of id203 and bayes rule
    what   s a joint distribution
    how to do id136 (i.e. p(e1|e2)) once you 

have a jd

    density estimation

    what is de and what is it good for
    how to learn a joint de
    how to learn a na  ve de

copyright    andrew w. moore

slide 114

57

what you should know

    bayes classifiers
    how to build one
    how to predict with a bc
    contrast between na  ve and joint bcs

copyright    andrew w. moore

slide 115

interesting questions

    suppose you were evaluating naivebc, 

jointbc, and id90

    invent a problem where only naivebc would do well
    invent a problem where only dtree would do well
    invent a problem where only jointbc would do well
    invent a problem where only naivebc would do poorly
    invent a problem where only dtree would do poorly
    invent a problem where only jointbc would do poorly

copyright    andrew w. moore

slide 116

58

