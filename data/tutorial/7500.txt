support vector 

machines 

note to other teachers and users of 
these slides. andrew would be delighted 
if you found this source material useful in 
giving your own lectures. feel free to use 
these slides verbatim, or to modify them 
to fit your own needs. powerpoint 
originals are available. if you make use 
of a significant portion of these slides in 
your own lecture, please include this 
message, or the following link to the 
source repository of andrew   s tutorials: 
http://www.cs.cmu.edu/~awm/tutorials . 
comments and corrections gratefully 
received. 

andrew w. moore

professor

school of computer science
carnegie mellon university

www.cs.cmu.edu/~awm

awm@cs.cmu.edu

412-268-7599

copyright    2001, 2003, andrew w. moore

nov 23rd, 2001

linear classifiers

x

denotes +1
denotes -1

  
f 

yest

f(x,w,b) = sign(w. x-b)

how would you 
classify this data?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 2

1

linear classifiers

x

denotes +1
denotes -1

  
f 

yest

f(x,w,b) = sign(w. x-b)

how would you 
classify this data?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 3

linear classifiers

x

denotes +1
denotes -1

  
f 

yest

f(x,w,b) = sign(w. x-b)

how would you 
classify this data?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 4

2

linear classifiers

x

denotes +1
denotes -1

  
f 

yest

f(x,w,b) = sign(w. x-b)

how would you 
classify this data?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 5

linear classifiers

x

denotes +1
denotes -1

  
f 

yest

f(x,w,b) = sign(w. x-b)

any of these 
would be fine..

..but which is 
best?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 6

3

classifier margin

x

denotes +1
denotes -1

  
f 

yest

f(x,w,b) = sign(w. x-b)

define the margin
of a linear 
classifier as the 
width that the 
boundary could be 
increased by 
before hitting a 
datapoint.

copyright    2001, 2003, andrew w. moore

support vector machines: slide 7

maximum margin

x

denotes +1
denotes -1

  
f 

yest

f(x,w,b) = sign(w. x-b)

the maximum 
margin linear 
classifier is the 
linear classifier 
with the, um, 
maximum margin.
this is the 
simplest kind of 
id166 (called an 
lid166)

linear id166

copyright    2001, 2003, andrew w. moore

support vector machines: slide 8

4

maximum margin

x

denotes +1
denotes -1

support vectors 
are those 
datapoints that 
the margin 
pushes up 
against

  
f 

yest

f(x,w,b) = sign(w. x-b)

the maximum 
margin linear 
classifier is the 
linear classifier 
with the, um, 
maximum margin.
this is the 
simplest kind of 
id166 (called an 
lid166)

linear id166

copyright    2001, 2003, andrew w. moore

support vector machines: slide 9

why maximum margin?

denotes +1
denotes -1

support vectors 
are those 
datapoints that 
the margin 
pushes up 
against

1.
2.

intuitively this feels safest. 
f(x,w,b) = sign(w. x-b)
if we   ve made a small error in the 
location of the boundary (it   s been 
the maximum 
jolted in its perpendicular direction) 
margin linear 
this gives us least chance of causing a 
misclassification.
classifier is the 
linear classifier 
3. loocv is easy since the model is 
with the, um, 
maximum margin.
4. there   s some theory (using vc 
this is the 
dimension) that is related to (but not 
simplest kind of 
the same as) the proposition that this 
id166 (called an 
is a good thing.
lid166)
5. empirically it works very very well.

immune to removal of any non-
support-vector datapoints.

copyright    2001, 2003, andrew w. moore

support vector machines: slide 10

5

specifying a line and margin

1     

s s  =   +
e
n
o

d ic t  c la

z

    p r e

d ic t  c la
z

    p r e

s s  =  - 1     

e

n

o

plus-plane

classifier boundary

minus-plane

    how do we represent this mathematically?
       in minput dimensions?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 11

specifying a line and margin

plus-plane

classifier boundary

minus-plane

1     

s s  =   +
e
n
o

d ic t  c la

z

    p r e

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

d ic t  c la
z

    p r e

s s  =  - 1     

e

n

o

    plus-plane   =    { x: w. x+ b = +1 }
    minus-plane =   { x: w. x+ b = -1 }

classify as..

+1
-1
universe 
explodes

if
if
if

w. x+ b >=1
w. x+ b <=-1
-1 <w. x+ b <1

copyright    2001, 2003, andrew w. moore

support vector machines: slide 12

6

computing the margin width
m =margin width

1     

s s  =   +
e
n
o

d ic t  c la

z

    p r e

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

d ic t  c la
z

    p r e

s s  =  - 1     

e

n

o

how do we compute 

min terms of w
and b?

    plus-plane   =    { x: w. x+ b = +1 }
    minus-plane =   { x: w. x+ b = -1 }
claim: the vector w is perpendicular to the plus plane. why?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 13

computing the margin width
m =margin width

1     

s s  =   +
e
n
o

d ic t  c la

z

    p r e

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

d ic t  c la
z

    p r e

s s  =  - 1     

e

n

o

how do we compute 

min terms of w
and b?

    plus-plane   =    { x: w. x+ b = +1 }
    minus-plane =   { x: w. x+ b = -1 }
claim: the vector w is perpendicular to the plus plane. why?

let u and v be two vectors on the 

plus plane. what is w. ( u   v) ?

and so of course the vector w is also 

perpendicular to the minus plane

copyright    2001, 2003, andrew w. moore

support vector machines: slide 14

7

computing the margin width
m =margin width

1     

x+

s s  =   +
e
n
o

d ic t  c la

z

    p r e

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

d ic t  c la
z

    p r e

s s  =  - 1     

x-
e

n

o

how do we compute 

min terms of w
and b?

    plus-plane   =    { x: w. x+ b = +1 }
    minus-plane =   { x: w. x+ b = -1 }
    the vector w is perpendicular to the plus plane
    let x-be any point on the minus plane
    let x+be the closest plus-plane-point to x-.

any location in 
any location in 
(cid:168)m: not 
rm: not 
necessarily a 
necessarily a 
datapoint
datapoint

copyright    2001, 2003, andrew w. moore

support vector machines: slide 15

computing the margin width
m =margin width

1     

x+

s s  =   +
e
n
o

d ic t  c la

z

    p r e

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

d ic t  c la
z

    p r e

s s  =  - 1     

x-
e

n

o

how do we compute 

min terms of w
and b?

    plus-plane   =    { x: w. x+ b = +1 }
    minus-plane =   { x: w. x+ b = -1 }
    the vector w is perpendicular to the plus plane
    let x-be any point on the minus plane
    let x+be the closest plus-plane-point to x-.
    claim: x+= x-+   w for some value of   . why?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 16

8

1     

x+

d ic t  c la

s s  =   +
e
n
o

computing the margin width
m =margin width
the line from x-to x+is 
how do we compute 
perpendicular to the 
planes.
min terms of w
and b?
so to get from  x-to x+
travel some distance in 
direction w.

s s  =  - 1     

d ic t  c la
z

x-
e

    p r e

    p r e

= - 1

z

x

x

x

1

0

o

b

b

b

n

w

w

w

+

=

+

=

+

    plus-plane   =    { x: w. x+ b = +1 }
    minus-plane =   { x: w. x+ b = -1 }
    the vector w is perpendicular to the plus plane
    let x-be any point on the minus plane
    let x+be the closest plus-plane-point to x-.
    claim: x+= x-+   w for some value of   . why?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 17

computing the margin width
m =margin width

1     

x+

s s  =   +
e
n
o

d ic t  c la

z

    p r e

s s  =  - 1     

x-
e

n

o

d ic t  c la
z

    p r e

x

w

b

+

x

+

w

1

=

b

0

=

b

+

x

= - 1

w
what we know:
    w. x++ b = +1 
    w. x-+ b = -1 
    x+= x-+   w
    |x+- x-| = m
it   s now easy to get m
in terms of wand b

copyright    2001, 2003, andrew w. moore

support vector machines: slide 18

9

computing the margin width
m =margin width

1     

x+

s s  =   +
e
n
o

d ic t  c la

z

    p r e

d ic t  c la
z

    p r e

x

w

b

+

x

+

w

1

=

b

0

=

b

+

x

= - 1

w
what we know:
    w. x++ b = +1 
    w. x-+ b = -1 
    x+= x-+   w
    |x+- x-| = m
it   s now easy to get m
in terms of wand b

copyright    2001, 2003, andrew w. moore

s s  =  - 1     

x-
e

n

o

w. (x -+   w)+ b = 1 
=>
w. x -+ b+   w.w= 1
=>
-1 +   w.w= 1
=>

=  

2
w.w

support vector machines: slide 19

computing the margin width
m =margin width =

1     

x+

2
ww.

s s  =   +
e
n
o

d ic t  c la

z

    p r e

s s  =  - 1     

x-
e

n

o

m= |x+- x-| =|   w|=

=

  

|

w

|

=

  

ww

.

=

2

ww
.
ww
.

=

2
ww
.

d ic t  c la
z

    p r e

x

w

b

+

x

+

w

1

=

b

0

=

b

+

x

= - 1

w
what we know:
    w. x++ b = +1 
    w. x-+ b = -1 
    x+= x-+   w
    |x+- x-| = m
   

=  

2
w.w

copyright    2001, 2003, andrew w. moore

support vector machines: slide 20

10

learning the maximum margin classifier
2
ww.

m =margin width =

x+

1     

s s  =   +
e
n
o

d ic t  c la

z

    p r e

+

=

+

=

+

w

w

n

b

b

b

o

0

1

x

x

= - 1

    p r e

d ic t  c la
z
given a guess of wand bwe can
    compute whether all data points in the correct half-planes
    compute the width of the margin
so now we just need to write a program to search the space 

x

w

of w   s and b   s to find the widest margin that matches all 
the datapoints. how?

s s  =  - 1     

x-
e

id119? simulated annealing? matrix inversion? 

em? newton   s method?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 21

learning via quadratic programming
    qp is a well-studied class of optimization 

algorithms to maximize a quadratic function of 
some real-valued variables subject to linear 
constraints.

copyright    2001, 2003, andrew w. moore

support vector machines: slide 22

11

find

arg

subject to

quadratic programming
max
u

uud
t +

r
t
2

u

+

c

quadratic criterion

ua
11
1
ua
21
1

+
+

ua
12
ua
22

2

2

ua
mm
1
ua
mm
2

   
   

b
1
b
2

and subject to

ua
n
11
u
11)1
+
u
11)2

n

+

+

ua
n
22
a
a

n

(

(

n

+

+
+

u
22)1
+
u
22)2

a
(
a

(

n

ua
nm

   

)1
+

b
n
u
mm
u
mm

n

+

)2

m
a
(
a
(

n

=
=

b
(
b
(

n

)1
+

n

+

)2

...
++
...
++
:
...
++

...
++
...
++
:
...
++

nadditional linear 

inequality 
constraints

e
q
u
a

l
i
t
y
 

c
o
n
s
t
r
a
n
t
s

i

e
a
d
d
i
t
i
o
n
a
l
 
l
i

n
e
a
r
 

a
(

u
11)

+

a
(

u
22)

en
+

en
+

a
(

u
mmen
+

)

=

b
(

en
+

)

copyright    2001, 2003, andrew w. moore

support vector machines: slide 23

find

arg

subject to

and subject to

quadratic programming
max
u

uud
t +

r
t
2

u

+

c

quadratic criterion

ua
11
1
ua
21
1

2

2

+

ua
mm
1
ua
mm
2

b
ua
...
++
+
   
t h e r e   e x i s t   a l g o r i t h m s   f o r   f i n d i n g  
1
12
b
ua
...
++
+
   
s u c h   c o n s t r a i n e d   q u a d r a t i c  
2
22
o p t i m a   m u c h   m o r e   e f f i c i e n t l y  
:
i a b l y   t h a n   g r a d i e n t  
a n d   r e l
ua
b
ua
...
++
   
a s c e n t .
n
n
nm
m
22
( b u t   t h e y   a r e   v e r y   f i d d l y     y o u  
p r o b a b l y   d o n     t   w a n t   t o   w r i t e  
u
a
u
a
...
++
mm
22)1
(
+
o n e   y o u r s e l f )
u
a
u
a
...
++
mm
22)2
(
:
...
++

u
mmen
+

u
22)

+
+

a
(

a
(

u
11)

en
+

+

)1
+

)2

+

+

n

n

n

n

(

(

)

ua
n
11
u
11)1
+
u
11)2

n

+

a
(
a

(

n

a
(

en
+

nadditional linear 

inequality 
constraints

=
=

b
(
b
(

n

)1
+

n

+

)2

=

b
(

en
+

)

e
q
u
a

l
i
t
y
 

c
o
n
s
t
r
a
n
t
s

i

e
a
d
d
i
t
i
o
n
a
l
 
l
i

n
e
a
r
 

copyright    2001, 2003, andrew w. moore

support vector machines: slide 24

12

learning the maximum margin classifier
given guess of w, bwe can
    compute whether all data 

1     

m =
2
ww.

s s  =   +
e
n
o

d ic t  c la

z

    p r e

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

d ic t  c la
z

    p r e

s s  =  - 1     

e

n

o

points are in the correct 
half-planes

    compute the margin width
assume rdatapoints, each 
(xk,yk) where yk= +/-1

what should our quadratic 
optimization criterion be?

how many constraints will we 

have? 

what should they be?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 25

learning the maximum margin classifier
given guess of w, bwe can
    compute whether all data 

1     

m =
2
ww.

s s  =   +
e
n
o

d ic t  c la

z

    p r e

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

d ic t  c la
z

    p r e

s s  =  - 1     

e

n

o

what should our quadratic 
optimization criterion be?

minimize w.w

points are in the correct 
half-planes

    compute the margin width
assume rdatapoints, each 
(xk,yk) where yk= +/-1

how many constraints will we 

have? r

what should they be?
w. xk+ b >= 1 if yk= 1
w. xk+ b <=-1 if yk= -1

copyright    2001, 2003, andrew w. moore

support vector machines: slide 26

13

uh-oh!

this is going to be a problem!
what should we do?

denotes +1
denotes -1

copyright    2001, 2003, andrew w. moore

support vector machines: slide 27

uh-oh!

denotes +1
denotes -1

this is going to be a problem!
what should we do?
idea 1:

find minimum w.w, while 
minimizing number of 
training set errors.

problemette: two things 
to minimize makes for 
an ill-defined 
optimization

copyright    2001, 2003, andrew w. moore

support vector machines: slide 28

14

uh-oh!

denotes +1
denotes -1

this is going to be a problem!
what should we do?
idea 1.1:

minimize
w.w+ c (#train errors)

tradeoff parameter

there   s a serious practical 
problem that   s about to make 
us reject this approach. can 
you guess what it is?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 29

uh-oh!

denotes +1
denotes -1

this is going to be a problem!
what should we do?
idea 1.1:

minimize
w.w+ c (#train errors)

tradeoff parameter

can   t be expressed as a quadratic 

programming problem.
solving it may be too slow.

(also, doesn   t distinguish between 
disastrous errors and near misses)

there   s a serious practical 
s o       a n y  
problem that   s about to make 
 
o t h e r
us reject this approach. can 
i d e a s ?
you guess what it is?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 30

15

uh-oh!

denotes +1
denotes -1

this is going to be a problem!
what should we do?
idea 2.0:

minimize
w.w+ c (distance of error 

points to their
correct place)

copyright    2001, 2003, andrew w. moore

support vector machines: slide 31

learning maximum margin with noise
given guess of w, bwe can
    compute sum of distances 

m =
2
ww.

of points to their correct 
zones

    compute the margin width
assume rdatapoints, each 
(xk,yk) where yk= +/-1

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

what should our quadratic 
optimization criterion be?

how many constraints will we 

have? 

what should they be?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 32

16

learning maximum margin with noise
given guess of w, bwe can
    compute sum of distances 

  11

m =
2
ww.

  2

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

  7

of points to their correct 
zones

    compute the margin width
assume rdatapoints, each 
(xk,yk) where yk= +/-1

what should our quadratic 
optimization criterion be?

how many constraints will we 

have? r

minimize

1 ww
.
2

+

r

   
k  c

k

1
=

what should they be?
w. xk+ b >= 1-  kif yk= 1
w. xk+ b <=-1+  kif yk= -1

copyright    2001, 2003, andrew w. moore

support vector machines: slide 33

learning maximum margin with noise
given guess of w, bwe can
    compute sum of distances 

m = # input 
dimensions

  11

m =
2
ww.

  2

of points to their correct 
our original (noiseless data) qp had m+1 
zones

our new (noisy data) qp has m+1+r 

variables: w1, w2,     wm, and b.
  7
variables: w1, w2,     wm, b,   k ,   1 ,      r 

    compute the margin width
assume rdatapoints, each 
(xk,yk) where yk= +/-1

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

what should our quadratic 
optimization criterion be?

minimize

1 ww
.
2

+

r

   
k  c

k

1
=

how many constraints will we 

r= # records

have? r

what should they be?
w. xk+ b >= 1-  kif yk= 1
w. xk+ b <=-1+  kif yk= -1

copyright    2001, 2003, andrew w. moore

support vector machines: slide 34

17

learning maximum margin with noise
given guess of w, bwe can
    compute sum of distances 

  11

m =
2
ww.

  2

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

  7

of points to their correct 
zones

    compute the margin width
assume rdatapoints, each 
(xk,yk) where yk= +/-1

what should our quadratic 
optimization criterion be?

how many constraints will we 

have? r

minimize

1 ww
.
2

+

r

   
k  c

k

1
=

what should they be?
w. xk+ b >= 1-  kif yk= 1
w. xk+ b <=-1+  kif yk= -1

there   s a bug in this qp. can you spot it?

copyright    2001, 2003, andrew w. moore

support vector machines: slide 35

learning maximum margin with noise
given guess of w, bwe can
    compute sum of distances 

  11

m =
2
ww.

  2

x

w

1

b

+

=

+

x

+

w

b

x

w

0

=

b

= - 1

  7

of points to their correct 
zones

    compute the margin width
assume rdatapoints, each 
(xk,yk) where yk= +/-1

what should our quadratic 
optimization criterion be?

how many constraints will we 

have? 2r

minimize

1 ww
.
2

+

r

   
k  c

k

1
=

copyright    2001, 2003, andrew w. moore

what should they be?
w. xk+ b >= 1-  kif yk= 1
w. xk+ b <=-1+  kif yk= -1
  k>= 0 for all k

support vector machines: slide 36

18

an equivalent qp

warning: up until rong zhang spotted my error in 
oct 2003, this equation had been wrong in earlier 
versions of the notes. this version is correct.

maximize

r

   

k

1
=

  

k

   

1
2

r

r

      

k

1
=

l

1
=

q    
k
kl

l

where

q
kl

=

yy
k

l

(

xx
.
k

l

)

subject to these 

constraints:

   0

  k

   

c

k
   

then define:

r

   

k

=

1

k y  

k

=

0

k y  

x

k

k

then classify with:
f(x,w,b) = sign(w. x-b)

r

=

=

w

   
k
b
y
=
k
where

1
1(
k
 

   

  
=

)
k
arg

.

wx
   
k
  
max
k

k

k

copyright    2001, 2003, andrew w. moore

support vector machines: slide 37

an equivalent qp
q    
k
kl

      

   

  

   

r

r

r

k

l

maximize

k

1
=

k

1
=

l

1
=

1
2

warning: up until rong zhang spotted my error in 
oct 2003, this equation had been wrong in earlier 
versions of the notes. this version is correct.

where

q
kl

=

yy
k

l

(

xx
.
k

l

)

subject to these 

constraints:

   0

  k

   

c

k
   

r

   

k

=

1

k y  

k

=

0

k y  

x

k

k

then define:

r

=

=

w

   
k
y
b
=
k
where

1
1(
k
 

   

  
=

)
k
arg

.

wx
   
k
  
max
k

k

k

datapoints with   k> 0 

will be the support 
vectors

then classify with:
f(x,w,b) = sign(w. x-b)

..so this sum only needs 

to be over the 
support vectors.

copyright    2001, 2003, andrew w. moore

support vector machines: slide 38

19

maximize

r

   

k

1
=

  

k

an equivalent qp
1
q
   
kl
2

      

q    
k
kl

where

r

r

l

k

1
=

l

1
=

=

yy
k

l

(

xx
.
k

l

)

subject to these 

constraints:

then define:

r

=

=

w

   
k
b
y
=
k
where

k y  

1
1(
k
 

   

  
=

k y  

k

=

0

datapoints with   k> 0 

1

r

   

why did i tell you about this 

  k

k
   

   0

c
equivalent qp?

    it   s a formulation that qp 

   
k
=
packages can optimize more 
quickly
    because of further jaw-
x
k
k
dropping developments you   re 
about to learn.
)
k
k
arg

will be the support 
vectors

to be over the 
support vectors.

wx
   
k
  
max
k

.

k

..so this sum only needs 

then classify with:
f(x,w,b) = sign(w. x-b)

copyright    2001, 2003, andrew w. moore

support vector machines: slide 39

suppose we   re in 1-dimension

what would 

id166s do with 
this data?

x=0

copyright    2001, 2003, andrew w. moore

support vector machines: slide 40

20

suppose we   re in 1-dimension

not a big surprise

x=0

positive    plane   

negative    plane   

copyright    2001, 2003, andrew w. moore

support vector machines: slide 41

harder 1-dimensional dataset

that   s wiped the 
smirk off id166   s 
face.

what can be 
done about 
this?

x=0

copyright    2001, 2003, andrew w. moore

support vector machines: slide 42

21

harder 1-dimensional dataset

remember how 
permitting non-
linear basis 
functions made 
id75 
so much nicer?
let   s permit them 

here too
xx=z
,
(
k

k

)

2
k

x=0

copyright    2001, 2003, andrew w. moore

support vector machines: slide 43

harder 1-dimensional dataset

remember how 
permitting non-
linear basis 
functions made 
id75 
so much nicer?
let   s permit them 

here too
xx=z
(
,
k

k

)

2
k

x=0

copyright    2001, 2003, andrew w. moore

support vector machines: slide 44

22

common id166 basis functions

zk= ( polynomial terms of xkof degree 1 to q)
zk= ( radial basis functions of xk)
   
kernelfn
      
   

c
   
k
kw

j
][

   
      
   

  

=

=

x

x

z

)

(

|

|

k

k

j

j

zk= ( sigmoid functions of xk)
this is sensible. 
is that the end of the story?

no   there   s one more trick!

copyright    2001, 2003, andrew w. moore

support vector machines: slide 45

constant term

linear terms

quadratic 

basis functions

)(x  

=

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   

2

m

x
1
x

1
2
2
:
x
2
x
2
1
x
2
2
:
x
2
m
xx
2
21
xx
2
31
:
xx
m
1
xx
32
:
xx
1
:

m

2
2

2

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   

2

x

x
    m
1

m

copyright    2001, 2003, andrew w. moore

pure 

quadratic 

terms

quadratic 
cross-terms

number of terms (assuming m input 
dimensions) = (m+2)-choose-2
= (m+2)(m+1)/2

= (as near as makes no difference) m2/2

   s are doing.

you may be wondering what those 
2
   you should be happy that they do no 
harm
   you   ll find out why they   re there 
soon. 

support vector machines: slide 46

23

))

l

))

l

qp with basis functions
q
kl

q    
kl
k

      

   

where

  

   

r

r

r

k

l

maximize

k

1
=

k

1
=

l

1
=

1
2

subject to these 

constraints:

   0

  k

   

c

k
   

warning: up until rong zhang spotted my error in 
oct 2003, this equation had been wrong in earlier 
versions of the notes. this version is correct.

=

yy
k

l

x  x  
(
(

).

(

k

r

   

k

=

1

k y  

k

=

0

then define:
   

w

=

k

 

k  
s.t.
 
>
1(
   
k
 

b
y
=
k
where

k y  
0
  
=

)
k
arg

x  
(

)

k

k

.

wx
   
k
  
max
k

k

k

then classify with:
f(x,w,b) = sign(w.   (x)-b)

copyright    2001, 2003, andrew w. moore

support vector machines: slide 47

qp with basis functions
x  x  
yy
(
(
   
k

q    
kl
k

q
kl

      

where

).

=

(

r

r

k

k

l

l

maximize

r

   

k

1
=

  

1
2

k

1
=

l

1
=

k
   

we must do r2/2 dot products to 
r
get this matrix ready.
c
k y  
=
each dot product requires m2/2 
additions and multiplications
the whole thing costs r2 m2 /4. 
yeeks!

   

=

1

k

k

0

   or does it?
   or does it?

then classify with:
f(x,w,b) = sign(w.   (x)-b)

subject to these 

constraints:

   0

  k

   

then define:
   

w

=

k

 

k  
s.t.
 
>
1(
   
k
 

y
b
=
k
where

k y  
0
  
=

)
k
arg

x  
(

)

k

k

.

wx
   
k
  
max
k

k

k

copyright    2001, 2003, andrew w. moore

support vector machines: slide 48

24

2

m

)(

 
t
o
d
 
c
i
t
a
r
d
a
u
q

b  a  
)(

s
t
c
u
d
o
r
p

1
a
2
1
a
2
:
a
2
a
2
1
a
2
2
:
a
2
m
aa
2
21
aa
2
31
:
aa
2
m
1
aa
2
32
:
aa
1
:

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
copyright    2001, 2003, andrew w. moore

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   

=

a

a

2

2

   

   

1
   

m

m

m

1
b
2
1
b
2
2
:
b
2
m
b
2
1
b
2
2
:
b
2
m
bb
2
21
bb
2
31
:
bb
2
m
1
bb
2
32
:
bb
2
m
1
:
bb
2
m
m
1
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   

 
t
o
d
 
c
i
t
a
r
d
a
u
q

s
t
c
u
d
o
r
p

1
+
m
   

i

1
=

+

m

   

i

1
=

2

iba
i

i ba
22
i

+

m

m

       

2
i
1
+=

1
=

i

j

bbaa
i
i

j

j

support vector machines: slide 49

just out of casual, innocent, interest, 
let   s look at another function of aand 
b:

+ba
.(

2)1

+

1

=

ba
).(

2

+

=

   
   
   

m

   

i

1
=

ba
i
i

ba
.2
2
   
+   
   

2

m

   

i

1
=

ba
i
i

+

1

b  a  
)(

)(
   
m
   

i

1
=

21
+

ba
i
i

+

=
m
   

i

1
=

ba
22
i
i

+

m

m

       

2
i
1
+=

1
=

i

j

bbaa
i
i

j

j

=

=

m

m

      

i

1
=

j

1
=

baba
i
j

i

j

+

2

m

   

i

1
=

ba
i
i

+

1

m

   

i

1
=

(

ba
i
i

2

)

+

2

m

m

       

baba
i
j
i
1
+=

1
=

i

j

i

+

2

j

m

   

i

1
=

ba
i
i

+

1

copyright    2001, 2003, andrew w. moore

support vector machines: slide 50

25

 
t
o
d
 
c
i
t
a
r
d
a
u
q

s
t
c
u
d
o
r
p

just out of casual, innocent, interest, 
let   s look at another function of aand 
b:

+ba
.(

2)1

+

1

=

ba
).(

2

+

=

   
   
   

m

   

i

1
=

ba
i
i

ba
.2
2
   
+   
   

2

m

   

i

1
=

ba
i
i

+

1

b  a  
)(

)(
   
m
   

i

1
=

21
+

ba
i
i

+

=
m
   

i

1
=

ba
22
i
i

+

m

m

       

2
i
1
+=

1
=

i

j

bbaa
i
i

j

j

=

=

m

m

      

i

1
=

j

1
=

baba
i
j

i

j

+

2

m

   

i

1
=

ba
i
i

+

1

m

   

i

1
=

(

ba
i
i

2

)

+

2

m

m

       

baba
i
j
i
1
+=

1
=

i

j

i

+

2

j

m

   

i

1
=

ba
i
i

+

1

they   re the same!
and this is only o(m) to 
compute!

copyright    2001, 2003, andrew w. moore

support vector machines: slide 51

qp with quadratic basis functions
).

warning: up until rong zhang spotted my error in 
oct 2003, this equation had been wrong in earlier 
versions of the notes. this version is correct.

x  x  
(
(

q    
kl
k

yy
k

q
kl

where

      

   

  

=

   

(

r

r

r

k

k

l

l

maximize

k

1
=

k

1
=

l

1
=

1
2

subject to these 

constraints:

   0

  k

   

we must do r2/2 dot products to 
r
get this matrix ready.
c
0
=
k
each dot product now only requires 
madditions and multiplications

k y  

   

k
   

=

1

k

then define:
   

w

=

k

 

k  
s.t.
 
>
1(
   
k
 

y
b
=
k
where

k y  
0
  
=

)
k
arg

x  
(

)

k

k

.

wx
   
k
  
max
k

k

k

then classify with:
f(x,w,b) = sign(w.   (x)-b)

copyright    2001, 2003, andrew w. moore

support vector machines: slide 52

))

l

26

higher order polynomials

poly-
nomial

  (x)

quadratic

cubic

quartic

all m2/2 
terms up to 
degree 2
all m3/6 
terms up to 
degree 3
all m4/24 
terms up to 
degree 4

cost to 
build qkl
matrix 
tradition
ally
m2 r2 /4

cost if 100 
inputs

  (a).  (b)

cost to 
build qkl
matrix 
sneakily

cost if 
100 
inputs

2,500 r2

(a.b+1)2

mr2 / 2

50 r2

m3 r2 /12

83,000 r2

(a.b+1)3

mr2 / 2

50 r2

m4 r2 /48

1,960,000 r2

(a.b+1)4

mr2 / 2

50 r2

copyright    2001, 2003, andrew w. moore

support vector machines: slide 53

qp with quintic basis functions
).

where

=

x  x  
(
(

(

r

yy
k

l

q
kl

k

k

r

r

+

  

   

      

we must do r2/2 dot products to get this 
q    
matrix ready.
maximize
kl
k
in 100-d, each dot product now needs 103 
1
=
operations instead of 75 million
but there are still worrying things lurking away. 
what are they?

subject to these 

   0

c

   

1
=

1
=

k

k

l

l

  k

constraints:

k
   

r

   

k

=

1

k y  

k

=

0

then define:
   

w

=

k

 

k  
 
s.t.
>
1(
   
k
 

y
b
=
k
where

k y  
0
  
=

)
k
arg

x  
(

)

k

k

.

wx
   
k
  
max
k

k

k

then classify with:
f(x,w,b) = sign(w.   (x)-b)

copyright    2001, 2003, andrew w. moore

support vector machines: slide 54

))

l

27

))

l

))

l

qp with quintic basis functions
).

where

=

x  x  
(
(

(

r

yy
k

l

q
kl

k

k

r

r

+

  

   

      

we must do r2/2 dot products to get this 
q    
matrix ready.
maximize
kl
k
in 100-d, each dot product now needs 103 
1
=
operations instead of 75 million
but there are still worrying things lurking away. 
what are they?

subject to these 

   0

1
=

1
=

k

k

l

l

  k

constraints:

r

   

k

k

1

=

=

k
   

k y  

c
0
   
   the fear of overfitting with this enormous 
number of terms
   the evaluation phase (doing a set of 
predictions on a test set) will be very 
expensive (why?)
)

then define:
   

w

=

k

 

k  
 
s.t.
>
1(
   
k
 

b
y
=
k
where

k y  
0
  
=

)
k
arg

x  
(

k

k

.

wx
   
k
  
max
k

k

k

then classify with:
f(x,w,b) = sign(w.   (x)-b)

copyright    2001, 2003, andrew w. moore

support vector machines: slide 55

qp with quintic basis functions
).

where

(

r

k

r

r

+

  

   

      

we must do r2/2 dot products to get this 
q    
matrix ready.
maximize
kl
k
in 100-d, each dot product now needs 103 
1
=
operations instead of 75 million
but there are still worrying things lurking away. 
what are they?

subject to these 

   0

1
=

1
=

k

k

l

l

  k

constraints:

l

=

yy
k

x  x  
q
(
(
kl
the use of maximum margin 
magically makes this not a 
problem

k

r

   

k

k

1

=

=

k
   

k y  

c
0
   
   the fear of overfitting with this enormous 
number of terms
   the evaluation phase (doing a set of 
predictions on a test set) will be very 
expensive (why?)
)

then define:
   

w

=

k

 

k  
 
s.t.
>
1(
   
k
 

y
b
=
k
where

k y  
0
  
=

)
k
arg

x  
(

k

k

.

wx
   
k
  
max
k

k

k

because each w.   (x) (see below) 
needs 75 million operations. what 
can be done?

then classify with:
f(x,w,b) = sign(w.   (x)-b)

copyright    2001, 2003, andrew w. moore

support vector machines: slide 56

28

qp with quintic basis functions
).

where

(

r

k

r

r

+

  

   

      

we must do r2/2 dot products to get this 
q    
matrix ready.
maximize
kl
k
in 100-d, each dot product now needs 103 
1
=
operations instead of 75 million
but there are still worrying things lurking away. 
what are they?

subject to these 

   0

1
=

1
=

k

k

l

l

  k

constraints:

l

=

yy
k

x  x  
q
(
(
kl
the use of maximum margin 
magically makes this not a 
problem

k

r

))

l

   

k

k

1

=

=

k
   

k y  

c
0
   
   the fear of overfitting with this enormous 
number of terms
   the evaluation phase (doing a set of 
predictions on a test set) will be very 
expensive (why?)
)

then define:
   

w

=

 

k

k

x  
(

k y  
k  
k
0
 
s.t.
>
   
wx
b
  
y
1(
)
x  w
x  
x  
k y  
(
(
)
(
)
   
=
   
=
   
k
k
k
k
k
k  
0
   
x
x
k y  
5)1
(
  
k
arg
max
where
 
   
=
=
k
k  
0
k
only smoperations (s=#support vectors)
copyright    2001, 2003, andrew w. moore

.
   
+

s.t.

s.t.

>

>

k

k

k

k

k

 

 

 

 

)

because each w.   (x) (see below) 
needs 75 million operations. what 
can be done?

then classify with:
f(x,w,b) = sign(w.   (x)-b)

support vector machines: slide 57

qp with quintic basis functions
).

where

(

r

k

r

r

+

  

   

      

we must do r2/2 dot products to get this 
q    
matrix ready.
maximize
kl
k
in 100-d, each dot product now needs 103 
1
=
operations instead of 75 million
but there are still worrying things lurking away. 
what are they?

subject to these 

   0

1
=

1
=

k

k

l

l

  k

constraints:

l

=

yy
k

x  x  
q
(
(
kl
the use of maximum margin 
magically makes this not a 
problem

k

r

))

l

   

k

k

1

=

=

k
   

k y  

c
0
   
   the fear of overfitting with this enormous 
number of terms
   the evaluation phase (doing a set of 
predictions on a test set) will be very 
expensive (why?)
)

because each w.   (x) (see below) 
needs 75 million operations. what 
can be done?

then define:
   

w

=

 

k

k

x  
(

k y  
k  
k
0
 
s.t.
>
   
wx
b
  
y
1(
)
x  w
x  
x  
k y  
(
)
(
(
)
   
=
=
   
   
k
k
k
k
k
k  
0
   
x
x
k y  
(
5)1
  
k
max
arg
where
 
   
=
=
k
k  
0
k
only smoperations (s=#support vectors)
copyright    2001, 2003, andrew w. moore

.
   
+

s.t.

s.t.

>

>

k

k

k

k

k

 

 

 

 

)

then classify with:
when you see this many callout bubbles on 
a slide it   s time to wrap the author in a 
blanket, gently take him away and murmur 
f(x,w,b) = sign(w.   (x)-b)
   someone   s been at the powerpoint for too 
long.   

support vector machines: slide 58

29

maximize

subject to these 

constraints:

))

l

l

l

k

k

k

k

r

r

r

(

1
=

1
=

1
=

   

=

  

where

1
2

q
kl

yy
k

q    
kl
k

      

qp with quintic basis functions
   
x  x  
(
(
).
andrew   s opinion of why id166s don   t 
l
overfit as much as you   d think:
no matter what the basis function, 
there are really only up to r 
k
k y  
0
   
parameters:   1,   2..   r, and usually 
k
most are set to zero by the maximum 
margin.
asking for small w.w is like    weight 
decay    in neural nets and like ridge 
regression parameters in linear 
regression and like the use of priors 
in bayesian regression---all designed 
to smooth the function and reduce 
overfitting.

x  
(

   

   0

  k

c

   

=

r

=

1

k

 

then define:
   

w

=

k

k

)

k y  
k  
k
0
 
s.t.
>
   
wx
b
  
y
1(
)
x  w
x  
x  
k y  
(
(
)
(
)
   
=
   
=
   
k
k
k
k
k
k  
0
   
x
x
k y  
5)1
(
  
k
arg
max
where
 
   
=
=
k
k  
0
k
only smoperations (s=#support vectors)
copyright    2001, 2003, andrew w. moore

.
   
+

s.t.

s.t.

>

>

k

k

k

k

k

 

 

 

 

)

then classify with:
f(x,w,b) = sign(w.   (x)-b)

support vector machines: slide 59

id166 id81s

    k(a,b)=(a. b+1)dis an example of an id166 

id81

    beyond polynomials there are other very high 
dimensional basis functions that can be made 
practical by finding the right id81
    radial-basis-style id81:

bak
,(

)

=

   
exp
      
   

   

2

)

ba
(
   
2
2
  

   
      
   

    neural-net-style id81:

bak
,(

)

=

tanh(

)
        

ba
.

  ,    and    are magic 
parameters that must 
be chosen by a model 
selection method 
such as cv or 
vcsrm*

*see last lecture

copyright    2001, 2003, andrew w. moore

support vector machines: slide 60

30

vc-dimension of an id166

    very very very loosely speaking there is some theory which 
under some different assumptions puts an upper bound on 
the vc dimension as

diameter
margin

   
   
   

   
   
   

    where

    diameteris the diameter of the smallest sphere that can 

enclose all the high-dimensional term-vectors derived 
from the training set.

    marginis the smallest margin we   ll let the id166 use

    this can be used in srm (structural risk minimization) for 

choosing the polynomial degree, rbf   , etc.
    but most people just use cross-validation

copyright    2001, 2003, andrew w. moore

support vector machines: slide 61

id166 performance

    anecdotally they work very very well indeed.
    example: they are currently the best-known 

classifier on a well-studied hand-written-character 
recognition benchmark

    another example: andrew knows several reliable 
people doing practical real-world work who claim 
that id166s have saved them when their other 
favorite classifiers did poorly.

    there is a lot of excitement and religious fervor 

about id166s as of 2001.

    despite this, some practitioners (including your 

lecturer) are a little skeptical.

copyright    2001, 2003, andrew w. moore

support vector machines: slide 62

31

doing multi-class classification
    id166s can only handle two-class outputs (i.e. a 

categorical output variable with arity 2).

    what can be done?
    answer: with output arity n, learn n id166   s

    id166 1 learns    output==1    vs    output != 1   
    id166 2 learns    output==2    vs    output != 2   
    :
    id166 n learns    output==n    vs    output != n   

    then to predict the output for a new input, just 

predict with each id166 and find out which one puts 
the prediction the furthest into the positive region.

copyright    2001, 2003, andrew w. moore

support vector machines: slide 63

references

    an excellent tutorial on vc-dimension and support 

vector machines: 

c.j.c. burges. a tutorial on support vector machines 
for pattern recognition. data mining and knowledge 
discovery, 2(2):955-974, 1998. 
http://citeseer.nj.nec.com/burges98tutorial.html 

    the vc/srm/id166 bible:

statistical learning theory by vladimir vapnik, wiley-

interscience; 1998

copyright    2001, 2003, andrew w. moore

support vector machines: slide 64

32

what you should know

    linear id166s
    the definition of a maximum margin classifier
    what qp can do for you (but, for this class, you 

don   t need to know how it does it)

    how maximum margin can be turned into a qp 

problem

    how we deal with noisy (non-separable) data
    how we permit non-linear boundaries
    how id166 id81s permit us to pretend 
we   re working with ultra-high-dimensional basis-
function terms

copyright    2001, 2003, andrew w. moore

support vector machines: slide 65

33

