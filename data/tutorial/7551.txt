ef   cient processing of deep neural networks:

a tutorial and survey

vivienne sze, senior member, ieee, yu-hsin chen, student member, ieee, tien-ju yang, student

member, ieee, joel emer, fellow, ieee

1

7
1
0
2

 

g
u
a
3
1

 

 
 
]

v
c
.
s
c
[
 
 

2
v
9
3
0
9
0

.

3
0
7
1
:
v
i
x
r
a

abstract   deep neural networks (dnns) are currently widely
used for many arti   cial intelligence (ai) applications including
id161, id103, and robotics. while dnns
deliver state-of-the-art accuracy on many ai tasks, it comes at the
cost of high computational complexity. accordingly, techniques
that enable ef   cient processing of dnns to improve energy
ef   ciency and throughput without sacri   cing application accuracy
or increasing hardware cost are critical to the wide deployment
of dnns in ai systems.

this article aims to provide a comprehensive tutorial and
survey about the recent advances towards the goal of enabling
ef   cient processing of dnns. speci   cally,
it will provide an
overview of dnns, discuss various hardware platforms and
architectures that support dnns, and highlight key trends in
reducing the computation cost of dnns either solely via hardware
design changes or via joint hardware design and dnn algorithm
changes. it will also summarize various development resources
that enable researchers and practitioners to quickly get started
in this    eld, and highlight important benchmarking metrics and
design considerations that should be used for evaluating the
rapidly growing number of dnn hardware designs, optionally
including algorithmic co-designs, being proposed in academia
and industry.

the reader will take away the following concepts from this
article: understand the key design considerations for dnns; be
able to evaluate different dnn hardware implementations with
benchmarks and comparison metrics; understand the trade-offs
between various hardware architectures and platforms; be able to
evaluate the utility of various dnn design techniques for ef   cient
processing; and understand recent implementation trends and
opportunities.

i. introduction

deep neural networks (dnns) are currently the foundation
for many modern arti   cial intelligence (ai) applications [1].
since the breakthrough application of dnns to speech recogni-
tion [2] and image recognition [3], the number of applications
that use dnns has exploded. these dnns are employed in a
myriad of applications from self-driving cars [4], to detecting
cancer [5] to playing complex games [6]. in many of these
domains, dnns are now able to exceed human accuracy. the
superior performance of dnns comes from its ability to extract
high-level features from raw sensory data after using statistical
learning over a large amount of data to obtain an effective

v. sze, y.-h. chen and t.-j. yang are with the department of electrical
engineering and computer science, massachusetts institute of technol-
ogy, cambridge, ma 02139 usa. (e-mail: sze@mit.edu; yhchen@mit.edu,
tjy@mit.edu)

j. s. emer is with the department of electrical engineering and computer
science, massachusetts institute of technology, cambridge, ma 02139 usa,
and also with nvidia corporation, westford, ma 01886 usa. (e-mail:
jsemer@mit.edu)

representation of an input space. this is different from earlier
approaches that use hand-crafted features or rules designed by
experts.

the superior accuracy of dnns, however, comes at the
cost of high computational complexity. while general-purpose
compute engines, especially graphics processing units (gpus),
have been the mainstay for much dnn processing, increasingly
there is interest in providing more specialized acceleration of
the dnn computation. this article aims to provide an overview
of dnns, the various tools for understanding their behavior,
and the techniques being explored to ef   ciently accelerate their
computation.

this paper is organized as follows:
    section ii provides background on the context of why

dnns are important, their history and applications.

    section iii gives an overview of the basic components of

dnns and popular dnn models currently in use.

    section iv describes the various resources used for dnn

research and development.

    section v describes the various hardware platforms used
to process dnns and the various optimizations used
to improve throughput and energy ef   ciency without
impacting application accuracy (i.e., produce bit-wise
identical results).

    section vi discusses how mixed-signal circuits and new
memory technologies can be used for near-data processing
to address the expensive data movement that dominates
throughput and energy consumption of dnns.

    section vii describes various joint algorithm and hardware
optimizations that can be performed on dnns to improve
both throughput and energy ef   ciency while trying to
minimize impact on accuracy.

    section viii describes the key metrics that should be

considered when comparing various dnn designs.

ii. background on deep neural networks (dnn)
in this section, we describe the position of dnns in the
context of ai in general and some of the concepts that motivated
its development. we will also present a brief chronology of
the major steps in its history, and some current domains to
which it is being applied.

a. arti   cial intelligence and dnns

dnns, also referred to as deep learning, are a part of
the broad    eld of ai, which is the science and engineering
of creating intelligent machines that have the ability to

2

fig. 2. connections to a neuron in the brain. xi, wi, f (  ), and b are the
activations, weights, non-linear function and bias, respectively. (figure adopted
from [7].)

to be 1014 to 1015 synapses in the average human brain.

a key characteristic of the synapse is that it can scale the
signal (xi) crossing it as shown in fig. 2. that scaling factor
can be referred to as a weight (wi), and the way the brain is
believed to learn is through changes to the weights associated
with the synapses. thus, different weights result in different
responses to an input. note that learning is the adjustment
of the weights in response to a learning stimulus, while the
organization (what might be thought of as the program) of the
brain does not change. this characteristic makes the brain an
excellent inspiration for a machine-learning-style algorithm.

within the brain-inspired computing paradigm there is a
subarea called spiking computing. in this subarea, inspiration
is taken from the fact that the communication on the dendrites
and axons are spike-like pulses and that the information being
conveyed is not just based on a spike   s amplitude. instead,
it also depends on the time the pulse arrives and that the
computation that happens in the neuron is a function of not just
a single value but the width of pulse and the timing relationship
between different pulses. an example of a project that was
inspired by the spiking of the brain is the ibm truenorth [8].
in contrast to spiking computing, another subarea of brain-
inspired computing is called neural networks, which is the
focus of this article.1

b. neural networks and deep neural networks (dnns)

neural networks take their inspiration from the notion that
a neuron   s computation involves a weighted sum of the input
values. these weighted sums correspond to the value scaling
performed by the synapses and the combining of those values
in the neuron. furthermore, the neuron doesn   t just output that
weighted sum, since the computation associated with a cascade
of neurons would then be a simple id202 operation.
instead there is a functional operation within the neuron that
is performed on the combined inputs. this operation appears
to be a non-linear function that causes a neuron to generate
an output only if the inputs cross some threshold. thus by
analogy, neural networks apply a non-linear function to the
weighted sum of the input values. we look at what some of
those non-linear functions are in section iii-a1.

1note: recent work using truenorth in a stylized fashion allows it to be
used to compute reduced precision neural networks [9]. these types of neural
networks are discussed in section vii-a.

fig. 1. deep learning in the context of arti   cial intelligence.

achieve goals like humans do, according to john mccarthy,
the computer scientist who coined the term in the 1950s.
the relationship of deep learning to the whole of arti   cial
intelligence is illustrated in fig. 1.

within arti   cial

intelligence is a large sub-   eld called
machine learning, which was de   ned in 1959 by arthur samuel
as the    eld of study that gives computers the ability to learn
without being explicitly programmed. that means a single
program, once created, will be able to learn how to do some
intelligent activities outside the notion of programming. this is
in contrast to purpose-built programs whose behavior is de   ned
by hand-crafted heuristics that explicitly and statically de   ne
their behavior.

the advantage of an effective machine learning algorithm
is clear. instead of the laborious and hit-or-miss approach of
creating a distinct, custom program to solve each individual
problem in a domain, the single machine learning algorithm
simply needs to learn, via a processes called training, to handle
each new problem.

within the machine learning    eld, there is an area that is
often referred to as brain-inspired computation. since the brain
is currently the best    machine    we know for learning and
solving problems, it is a natural place to look for a machine
learning approach. therefore, a brain-inspired computation is
a program or algorithm that takes some aspects of its basic
form or functionality from the way the brain works. this is in
contrast to attempts to create a brain, but rather the program
aims to emulate some aspects of how we understand the brain
to operate.

although scientists are still exploring the details of how the
brain works, it is generally believed that the main computational
element of the brain is the neuron. there are approximately
86 billion neurons in the average human brain. the neurons
themselves are connected together with a number of elements
entering them called dendrites and an element leaving them
called an axon as shown in fig. 2. the neuron accepts the
signals entering it via the dendrites, performs a computation on
those signals, and generates a signal on the axon. these input
and output signals are referred to as activations. the axon of
one neuron branches out and is connected to the dendrites of
many other neurons. the connections between a branch of the
axon and a dendrite is called a synapse. there are estimated

artificial intelligence machine learning brain-inspired spiking  neural networks deep learning yj=fwixii   +b                  w1x1w2x2w0x0x0w0synapse axon dendrite axon neuron yj3

(a) neurons and synapses

(b) compute weighted sum for each layer

fig. 3.
from [7]).

simple neural network example and terminology (figure adopted

(a) compute the gradient of the loss
relative to the    lter inputs

(b) compute the gradient of the loss
relative to the weights

3(cid:80)

fig. 3(a) shows a diagrammatic picture of a computational
neural network. the neurons in the input layer receive some
values and propagate them to the neurons in the middle layer
of the network, which is also frequently called a    hidden
layer   . the weighted sums from one or more hidden layers are
ultimately propagated to the output layer, which presents the
   nal outputs of the network to the user. to align brain-inspired
terminology with neural networks, the outputs of the neurons
are often referred to as activations, and the synapses are often
referred to as weights as shown in fig. 3(a). we will use the
activation/weight nomenclature in this article.

fig. 3(b) shows an example of the computation at each
wij    xi + b), where wij, xi and yj are the
layer: yj = f (
weights, input activations and output activations, respectively,
and f (  ) is a non-linear function described in section iii-a1.
the bias term b is omitted from fig. 3(b) for simplicity.

i=1

within the domain of neural networks, there is an area called
deep learning, in which the neural networks have more than
three layers, i.e., more than one hidden layer. today, the typical
numbers of network layers used in deep learning range from
   ve to more than a thousand. in this article, we will generally
use the terminology deep neural networks (dnns) to refer to
the neural networks used in deep learning.

dnns are capable of learning high-level features with more
complexity and abstraction than shallower neural networks. an
example that demonstrates this point is using dnns to process
visual data. in these applications, pixels of an image are fed into
the    rst layer of a dnn, and the outputs of that layer can be
interpreted as representing the presence of different low-level
features in the image, such as lines and edges. at subsequent
layers, these features are then combined into a measure of the
likely presence of higher level features, e.g., lines are combined
into shapes, which are further combined into sets of shapes.
and    nally, given all this information, the network provides a
id203 that these high-level features comprise a particular
object or scene. this deep feature hierarchy enables dnns to
achieve superior performance in many tasks.

c. id136 versus training

fig. 4. an example of id26 through a neural network.

and is referred to as training the network. once trained, the
program can perform its task by computing the output of
the network using the weights determined during the training
process. running the program with these weights is referred
to as id136.

in this section, we will use image classi   cation, as shown
in fig. 6, as a driving example for training and using a dnn.
when we perform id136 using a dnn, we give an input
image and the output of the dnn is a vector of scores, one for
each object class; the class with the highest score indicates the
most likely class of object in the image. the overarching goal
for training a dnn is to determine the weights that maximize
the score of the correct class and minimize the scores of the
incorrect classes. when training the network the correct class
is often known because it is given for the images used for
training (i.e., the training set of the network). the gap between
the ideal correct scores and the scores computed by the dnn
based on its current weights is referred to as the loss (l).
thus the goal of training dnns is to    nd a set of weights to
minimize the average loss over a large training set.

when training a network, the weights (wij) are usually
updated using a hill-climbing optimization process called
id119. a multiple of the gradient of the loss relative
to each weight, which is the partial derivative of the loss with
respect to the weight, is used to update the weight (i.e., updated
wt+1
, where    is called the learning rate). note
that this gradient indicates how the weights should change in
order to reduce the loss. the process is repeated iteratively to
reduce the overall loss.

ij         l

ij = wt

   wij

an ef   cient way to compute the partial derivatives of
the gradient is through a process called id26.
id26, which is a computation derived from the
chain rule of calculus, operates by passing values backwards
through the network to compute how the loss is affected by
each weight.

this id26 computation is, in fact, very similar
in form to the computation used for id136 as shown
in fig. 4 [10].2 thus, techniques for ef   ciently performing

since dnns are an instance of a machine learning algorithm,
the basic program does not change as it learns to perform its
given tasks. in the speci   c case of dnns, this learning involves
determining the value of the weights (and bias) in the network,

2to backpropagate through each    lter: (1) compute the gradient of the loss
relative to the weights from the    lter inputs (i.e., the forward activations) and
the gradients of the loss relative to the    lter outputs; (2) compute the gradient
of the loss relative to the    lter inputs from the    lter weights and the gradients
of the loss relative to the    lter outputs.

neurons (activations) synapses (weights) x1 x2 x3 y1 y2 y3 y4 w11 w34 l1 input neurons (e.g. image pixels) layer 1 l1 output neurons a.k.a. activations layer 2 l2 output neurons id26    l   y4   l   y1   l   y2   l   y3w11 w34    l   x1   l   x2   l   x3id26    l   y4   l   y1   l   y2   l   y3x1 x3    l   w34   l   w11      .. id136 can sometimes be useful for performing training.
it is, however, important to note a couple of points. first,
id26 requires intermediate outputs of the network
to be preserved for the backwards computation, thus training
has increased storage requirements. second, due to the gradients
use for hill-climbing, the precision requirement for training
is generally higher than id136. thus many of the reduced
precision techniques discussed in section vii are limited to
id136 only.

a variety of techniques are used to improve the ef   ciency
and robustness of training. for example, often the loss from
multiple sets of input data, i.e., a batch, are collected before a
single pass of weight update is performed; this helps to speed
up and stabilize the training process.

there are multiple ways to train the weights. the most
common approach, as described above, is called supervised
learning, where all the training samples are labeled (e.g., with
the correct class). unsupervised learning is another approach
where all the training samples are not labeled and essentially
the goal is to    nd the structure or clusters in the data. semi-
supervised learning falls in between the two approaches where
only a small subset of the training data is labeled (e.g., use
unlabeled data to de   ne the cluster boundaries, and use the
small amount of labeled data to label the clusters). finally,
id23 can be used to the train weights such
that given the state of the current environment, the dnn can
output what action the agent should take next to maximize
expected rewards; however, the rewards might not be available
immediately after an action, but instead only after a series of
actions.

another commonly used approach to determine weights is
   ne-tuning, where previously-trained weights are available and
are used as a starting point and then those weights are adjusted
for a new dataset (e.g., id21) or for a new constraint
(e.g., reduced precision). this results in faster training than
starting from a random starting point, and can sometimes result
in better accuracy.

this article will focus on the ef   cient processing of dnn
id136 rather than training, since dnn id136 is often
performed on embedded devices (rather than the cloud) where
resources are limited as discussed in more details later.

d. development history

although neural nets were proposed in the 1940s, the    rst
practical application employing multiple digital neurons didn   t
appear until the late 1980s with the lenet network for hand-
written digit recognition [11]3. such systems are widely used
by atms for digit recognition on checks. however, the early
2010s have seen a blossoming of dnn-based applications with
highlights such as microsoft   s id103 system in
2011 [2] and the alexnet system for image recognition in
2012 [3]. a brief chronology of deep learning is shown in
fig. 5.

the deep learning successes of the early 2010s are believed
to be a con   uence of three factors. the    rst factor is the

3in the early 1960s, single analog neuron systems were used for adaptive

   ltering [12, 13].

4

dnn timeline

    1940s - neural networks were proposed
    1960s - deep neural networks were proposed
    1989 - neural networks for recognizing digits (lenet)
    1990s - hardware for shallow neural nets (intel etann)
    2011 - breakthrough dnn-based id103

    2012 - dnns for vision start supplanting hand-crafted

    2014+ - rise of dnn accelerator research (neu   ow,

(microsoft)

approaches (alexnet)

diannao...)

fig. 5. a concise history of neural networks.    deep    refers to the number of
layers in the network.

amount of available information to train the networks. to learn
a powerful representation (rather than using a hand-crafted
approach) requires a large amount of training data. for example,
facebook receives over 350 millions images per day, walmart
creates 2.5 petabytes of customer data hourly and youtube
has 300 hours of video uploaded every minute. as a result,
the cloud providers and many businesses have a huge amount
of data to train their algorithms.

the second factor is the amount of compute capacity
available. semiconductor device and computer architecture
advances have continued to provide increased computing
capability, and we appear to have crossed a threshold where the
large amount of weighted sum computation in dnns, which
is required for both id136 and training, can be performed
in a reasonable amount of time.

the successes of these early dnn applications opened the
   oodgates of algorithmic development. it has also inspired the
development of several (largely open source) frameworks that
make it even easier for researchers and practitioners to explore
and use dnns. combining these efforts contributes to the third
factor, which is the evolution of the algorithmic techniques that
have improved application accuracy signi   cantly and broadened
the domains to which dnns are being applied.

an excellent example of the successes in deep learning can
be illustrated with the id163 challenge [14]. this challenge
is a contest involving several different components. one of the
components is an image classi   cation task where algorithms
are given an image and they must identify what is in the image,
as shown in fig. 6. the training set consists of 1.2 million
images, each of which is labeled with one of 1000 object
categories that the image contains. for the evaluation phase,
the algorithm must accurately identify objects in a test set of
images, which it hasn   t previously seen.

fig. 7 shows the performance of the best entrants in the
id163 contest over a number of years. one sees that
the accuracy of the algorithms initially had an error rate
of 25% or more. in 2012, a group from the university of
toronto used graphics processing units (gpus) for their high
compute capability and a deep neural network approach, named
alexnet, and dropped the error rate by approximately 10% [3].
their accomplishment inspired an outpouring of deep learning
style algorithms that have resulted in a steady stream of

5

    speech and language dnns have signi   cantly improved
the accuracy of id103 [21] as well as many
related tasks such as machine translation [2], natural
language processing [22], and audio generation [23].

    medical dnns have played an important role in genomics
to gain insight into the genetics of diseases such as autism,
cancers, and spinal muscular atrophy [24   27]. they have
also been used in medical imaging to detect skin cancer [5],
brain cancer [28] and breast cancer [29].

    game play recently, many of the grand ai challenges
involving game play have been overcome using dnns.
these successes also required innovations in training
techniques and many rely on id23 [30].
dnns have surpassed human level accuracy in playing
atari [31] as well as go [6], where an exhaustive search
of all possibilities is not feasible due to the unimaginably
huge number of possible moves.

    robotics dnns have been successful in the domain of
robotic tasks such as grasping with a robotic arm [32],
motion planning for ground robots [33], visual naviga-
tion [4, 34], control to stabilize a quadcopter [35] and
driving strategies for autonomous vehicles [36].

dnns are already widely used in multimedia applications
today (e.g., id161, id103). looking
forward, we expect that dnns will likely play an increasingly
important role in the medical and robotics    elds, as discussed
above, as well as    nance (e.g., for trading, energy forecasting,
and risk assessment), infrastructure (e.g., structural safety, and
traf   c control), weather forecasting and id37 [37].
the myriad application domains pose new challenges to the
ef   cient processing of dnns; the solutions then have to be
adaptive and scalable in order to handle the new and varied
forms of dnns that these applications may employ.

f. embedded versus cloud

the various applications and aspects of dnn processing
(i.e., training versus id136) have different computational
needs. speci   cally, training often requires a large dataset5 and
signi   cant computational resources for multiple weight-update
iterations. in many cases, training a dnn model still takes
several hours to multiple days and thus is typically performed
in the cloud. id136, on the other hand, can happen either
in the cloud or at the edge (e.g., iot or mobile).

in many applications, it is desirable to have the dnn
id136 processing near the sensor. for instance, in computer
vision applications, such as measuring wait times in stores
or predicting traf   c patterns, it would be desirable to extract
meaningful information from the video right at the image
sensor rather than in the cloud to reduce the communication
cost. for other applications such as autonomous vehicles,
drone navigation and robotics, local processing is desired since
the latency and security risks of relying on the cloud are
too high. however, video involves a large amount of data,
which is computationally complex to process; thus, low cost
hardware to analyze video is challenging yet critical to enabling

fig. 6.
example of an image classi   cation task. the machine learning
platform takes in an image and outputs the con   dence scores for a prede   ned
set of classes.

fig. 7. results from the id163 challenge [14].

improvements.

in conjunction with the trend to deep learning approaches
for the id163 challenge, there has been a corresponding
increase in the number of entrants using gpus. from 2012
when only 4 entrants used gpus to 2014 when almost all
the entrants (110) were using them. this re   ects the almost
complete switch from traditional id161 approaches
to deep learning-based approaches for the competition.

in 2015, the id163 winning entry, resnet [15], exceeded
human-level accuracy with a top-5 error rate4 below 5%. since
then, the error rate has dropped below 3% and more focus
is now being placed on more challenging components of the
competition, such as id164 and localization. these
successes are clearly a contributing factor to the wide range
of applications to which dnns are being applied.

e. applications of dnn

many applications can bene   t from dnns ranging from
multimedia to medical space. in this section, we will provide
examples of areas where dnns are currently making an impact
and highlight emerging areas where dnns hope to make an
impact in the future.

    image and video video is arguably the biggest of the
big data. it accounts for over 70% of today   s internet
traf   c [16]. for instance, over 800 million hours of video
is collected daily worldwide for video surveillance [17].
id161 is necessary to extract meaningful infor-
mation from video. dnns have signi   cantly improved the
accuracy of many id161 tasks such as image
classi   cation [14], object localization and detection [18],
image segmentation [19], and action recognition [20].

4the top-5 error rate is measured based on whether the correct answer

appears in one of the top 5 categories selected by the algorithm.

5one of the major drawbacks of dnns is their need for large datasets to

prevent over-   tting during training.

dog (0.7) cat (0.1) bike (0.02) car (0.02) plane (0.02) house (0.04) machine learning  (id136) class probabilities 0 5 10 15 20 25 30 2010 2011 2012 2013 2014 2015 human accuracy (top-5 error) alexnet	overfeat	googlenet	resnet	clarifai	vgg	large error rate reduction due to deep id98 6

attention has been given to hardware acceleration speci   cally
for id56s.

dnns can be composed solely of fully-connected (fc)
layers (also referred to as multi-layer id88s, or mlp)
as shown in the leftmost layer of fig. 8(b). in a fc layer,
all output activations are composed of a weighted sum of
all input activations (i.e., all outputs are connected to all
inputs). this requires a signi   cant amount of storage and
computation. thankfully, in many applications, we can remove
some connections between the activations by setting the weights
to zero without affecting accuracy. this results in a sparsely-
connected layer. a sparsely connected layer is illustrated in
the rightmost layer of fig. 8(b).

we can also make the computation more ef   cient by limiting
the number of weights that contribute to an output. this sort of
structured sparsity can arise if each output is only a function
of a    xed-size window of inputs. even further ef   ciency can
be gained if the same set of weights are used in the calculation
of every output. this repeated use of the same weight values is
called weight sharing and can signi   cantly reduce the storage
requirements for weights.

an extremely popular windowed and weight-shared dnn
layer arises by structuring the computation as a convolution,
as shown in fig. 9(a), where the weighted sum for each output
activation is computed using only a small neighborhood of input
activations (i.e., all weights beyond beyond the neighborhood
are set to zero), and where the same set of weights are shared for
every output (i.e., the    lter is space invariant). such convolution-
based layers are referred to as convolutional (conv) layers. 6

a. convolutional neural networks (id98s)

a common form of dnns is convolutional neural nets
(id98s), which are composed of multiple conv layers as
shown in fig. 10. in such networks, each layer generates a
successively higher-level abstraction of the input data, called
a feature map (fmap), which preserves essential yet unique
information. modern id98s are able to achieve superior per-
formance by employing a very deep hierarchy of layers. id98
are widely used in a variety of applications including image
understanding [3], id103 [39], game play [6],
robotics [32], etc. this paper will focus on its use in image
processing, speci   cally for the task of image classi   cation [3].
each of the conv layers in id98 is primarily composed of
high-dimensional convolutions as shown in fig. 9(b). in this
computation, the input activations of a layer are structured as
a set of 2-d input feature maps (ifmaps), each of which is
called a channel. each channel is convolved with a distinct
2-d    lter from the stack of    lters, one for each channel; this
stack of 2-d    lters is often referred to as a single 3-d    lter.
the results of the convolution at each point are summed across
all the channels. in addition, a 1-d bias can be added to the
   ltering results, but some recent networks [15] remove its
usage from parts of the layers. the result of this computation
is the output activations that comprise one channel of output
feature map (ofmap). additional 3-d    lters can be used on

6note: the structured sparsity in conv layers is orthogonal to the sparsity

that occurs from network pruning as described in section vii-b2.

(a) feedforward versus feedback (re-
current) networks

(b) fully connected versus sparse

fig. 8. different types of neural networks (figure adopted from [7]).

these applications. id103 enables us to seaid113ssly
interact with electronic devices, such as smartphones. while
currently most of the processing for applications such as apple
siri and amazon alexa voice services is in the cloud, it is
still desirable to perform the recognition on the device itself to
reduce latency and dependency on connectivity, and to improve
privacy and security.

many of the embedded platforms that perform dnn infer-
ence have stringent energy consumption, compute and memory
cost limitations; ef   cient processing of dnns have thus become
of prime importance under these constraints. therefore, in this
article, we will focus on the compute requirements for id136
rather than training.

iii. overview of dnns

dnns come in a wide variety of shapes and sizes depending
on the application. the popular shapes and sizes are also
evolving rapidly to improve accuracy and ef   ciency. in all
cases, the input to a dnn is a set of values representing the
information to be analyzed by the network. for instance, these
values can be pixels of an image, sampled amplitudes of an
audio wave or the numerical representation of the state of some
system or game.

the networks that process the input come in two major
forms: feed forward and recurrent as shown in fig. 8(a). in
feed-forward networks all of the computation is performed as a
sequence of operations on the outputs of a previous layer. the
   nal set of operations generates the output of the network, for
example a id203 that an image contains a particular object,
the id203 that an audio sequence contains a particular
word, a bounding box in an image around an object or the
proposed action that should be taken. in such dnns, the
network has no memory and the output for an input is always
the same irrespective of the sequence of inputs previously given
to the network.

in contrast, recurrent neural networks (id56s), of which
id137 (lstms) [38] are a
popular variant, have internal memory to allow long-term
dependencies to affect the output. in these networks, some
intermediate operations generate values that are stored internally
in the network and used as inputs to other operations in
conjunction with the processing of a later input. in this article,
we will focus on feed-forward networks since (1) the major
computation in id56s is still the weighted sum, which is
covered by the feed-forward networks, and (2) to-date little

feed forward recurrent fully-connected sparsely-connected 7

shape parameter

n
m
c

h/w
r/s
e/f

description
batch size of 3-d fmaps
# of 3-d    lters / # of ofmap channels
# of ifmap/   lter channels
ifmap plane height/width
   lter plane height/width (= h or w in fc)
ofmap plane height/width (= 1 in fc)

shape parameters of a conv/fc layer.

table i

(a) 2-d convolution in traditional image processing

(b) high dimensional convolutions in id98s

fig. 9. dimensionality of convolutions.

fig. 10. convolutional neural networks.

the same input to create additional output channels. finally,
multiple input feature maps may be processed together as a
batch to potentially improve reuse of the    lter weights.

given the shape parameters in table i, the computation of

a conv layer is de   ned as

c   1(cid:88)

s   1(cid:88)

r   1(cid:88)

o[z][u][x][y] = b[u] +

i[z][k][u x + i][u y + j]    w[u][k][i][j],

0     z < n, 0     u < m, 0     x < f, 0     y < e,
e = (h     r + u )/u, f = (w     s + u )/u.

k=0

i=0

j=0

(1)
o, i, w and b are the matrices of the ofmaps, ifmaps,    lters
and biases, respectively. u is a given stride size. fig. 9(b)
shows a visualization of this computation (ignoring biases).
to align the terminology of id98s with the generic dnn,
       lters are composed of weights (i.e., synapses)
    input and output feature maps (ifmaps, ofmaps) are
composed of activations (i.e., input and output neurons)

fig. 11. various forms of non-linear id180 (figure adopted
from caffe tutorial [46]).

from    ve [3] to more than a thousand [15] conv layers
are commonly used in recent id98 models. a small number,
e.g., 1 to 3, of fully-connected (fc) layers are typically applied
after the conv layers for classi   cation purposes. a fc layer
also applies    lters on the ifmaps as in the conv layers, but
the    lters are of the same size as the ifmaps. therefore, it
does not have the weight sharing property of conv layers.
eq. (1) still holds for the computation of fc layers with a
few additional constraints on the shape parameters: h = r,
f = s, e = f = 1, and u = 1.

in addition to conv and fc layers, various optional layers
can be found in a dnn such as the non-linearity, pooling,
and id172. the function and computations for each of
these layers are discussed next.

1) non-linearity: a non-linear activation function is typi-
cally applied after each conv or fc layer. various non-linear
functions are used to introduce non-linearity into the dnn as
shown in fig. 11. these include historically conventional non-
linear functions such as sigmoid or hyperbolic tangent as well
as recti   ed linear unit (relu) [40], which has become popular
in recent years due to its simplicity and its ability to enable
fast training. variations of relu, such as leaky relu [41],
parametric relu [42], and exponential lu [43] have also been
explored for improved accuracy. finally, a non-linearity called
maxout, which takes the max value of two intersecting linear
functions, has shown to be effective in id103
tasks [44, 45].

2) pooling: a variety of computations that reduce the
dimensionality of a feature map are referred to as pooling.
pooling, which is applied to each channel separately, enables

r filter (weights) s e f partial sum (psum) accumulation input fmap output fmap element-wise multiplication h w an output  activation input fmapsfiltersoutput fmapsrsc   hwc   efmefm   rschwc1n1m1nmodern deep id98: 5     1000 layers class scores fc layer conv layer low-level features conv layer high-level features     1     3 layers convolu   on	non-linearity	  	normaliza   on	pooling	optional fully	connected	  	non-linearity	conv layer mid-level features sigmoid 1 -1 0 0 1 -1 y=1/(1+e-x)	hyperbolic tangent 1 -1 0 0 1 -1 y=(ex-e-x)/(ex+e-x)	rectified linear unit  (relu) 1 -1 0 0 1 -1 y=max(0,x)	leaky relu 1 -1 0 0 1 -1 y=max(  x,x)	exponential lu 1 -1 0 0 1 -1 				x,											  (ex-1),	x   0	x<0	y=	   = small const. (e.g. 0.1) traditional non-linear id180 modern non-linear id180 8

dnn is run only once), which is more consistent with what
would likely be deployed in real-time and/or energy-constrained
applications.

lenet [11] was one of the    rst id98 approaches introduced
in 1989. it was designed for the task of digit classi   cation in
grayscale images of size 28  28. the most well known version,
lenet-5, contains two conv layers and two fc layers [48].
each conv layer uses    lters of size 5  5 (1 channel per    lter)
with 6    lters in the    rst layer and 16    lters in the second layer.
average pooling of 2  2 is used after each convolution and a
sigmoid is used for the non-linearity. in total, lenet requires
60k weights and 341k multiply-and-accumulates (macs) per
image. lenet led to id98s       rst commercial success, as it was
deployed in atms to recognize digits for check deposits.

alexnet [3] was the    rst id98 to win the id163 challenge
in 2012. it consists of    ve conv layers and three fc layers.
within each conv layer, there are 96 to 384    lters and the
   lter size ranges from 3  3 to 11  11, with 3 to 256 channels
each. in the    rst layer, the 3 channels of the    lter correspond
to the red, green and blue components of the input image.
a relu non-linearity is used in each layer. max pooling of
3  3 is applied to the outputs of layers 1, 2 and 5. to reduce
computation, a stride of 4 is used at the    rst layer of the
network. alexnet introduced the use of lrn in layers 1 and
2 before the max pooling, though lrn is no longer popular
in later id98 models. one important factor that differentiates
alexnet from lenet is that the number of weights is much
larger and the shapes vary from layer to layer. to reduce the
amount of weights and computation in the second conv layer,
the 96 output channels of the    rst layer are split into two groups
of 48 input channels for the second layer, such that the    lters in
the second layer only have 48 channels. similarly, the weights
in fourth and    fth layer are also split into two groups. in total,
alexnet requires 61m weights and 724m macs to process
one 227  227 input image.

overfeat [49] has a very similar architecture to alexnet with
   ve conv layers and three fc layers. the main differences
are that the number of    lters is increased for layers 3 (384
to 512), 4 (384 to 1024), and 5 (256 to 1024), layer 2 is not
split into two groups, the    rst fully connected layer only has
3072 channels rather than 4096, and the input size is 231  231
rather than 227  227. as a result, the number of weights grows
to 146m and the number of macs grows to 2.8g per image.
overfeat has two different models: fast (described here) and
accurate. the accurate model used in the id163 challenge
gives a 0.65% lower top-5 error rate than the fast model at the
cost of 1.9   more macs

vgg-16 [50] goes deeper to 16 layers consisting of 13
conv layers and 3 fc layers. in order to balance out the
cost of going deeper, larger    lters (e.g., 5  5) are built from
multiple smaller    lters (e.g., 3  3), which have fewer weights,
to achieve the same receptive    elds as shown in fig. 13(a).
as a result, all conv layers have the same    lter size of 3  3.
in total, vgg-16 requires 138m weights and 15.5g macs
to process one 224  224 input image. vgg has two different
models: vgg-16 (described here) and vgg-19. vgg-19 gives
a 0.1% lower top-5 error rate than vgg-16 at the cost of
1.27   more macs.

fig. 12. various forms of pooling (figure adopted from caffe tutorial [46]).

the network to be robust and invariant to small shifts and
distortions. pooling combines, or pools, a set of values in
its receptive    eld into a smaller number of values. it can be
con   gured based on the size of its receptive    eld (e.g., 2  2)
and pooling operation (e.g., max or average), as shown in
fig. 12. typically pooling occurs on non-overlapping blocks
(i.e., the stride is equal to the size of the pooling). usually a
stride of greater than one is used such that there is a reduction
in the dimension of the representation (i.e., feature map).

3) id172: controlling the input distribution across
layers can help to signi   cantly speed up training and improve
accuracy. accordingly, the distribution of the layer input
activations (  ,   ) are normalized such that it has a zero mean
and a unit standard deviation. in batch id172 (bn),
the normalized value is further scaled and shifted, as shown
in eq. (2), where the parameters (  ,   ) are learned from
training [47].   is a small constant to avoid numerical problems.
prior to this, local response id172 (lrn) [3] was
used, which was inspired by lateral inhibition in neurobiology
where excited neurons (i.e., high value activations) should
subdue its neighbors (i.e., cause low value activations); however,
bn is now considered standard practice in the design of
id98s while lrn is mostly deprecated. note that while lrn
usually is performed after the non-linear function, bn is mostly
performed between the conv or fc layer and the non-linear
function.

y =

x          

  2 +  

   +   

(2)

b. popular dnn models

many dnn models have been developed over the past
two decades. each of these models has a different    network
architecture    in terms of number of layers, layer types, layer
shapes (i.e.,    lter size, number of channels and    lters), and
connections between layers. understanding these variations
and trends is important for incorporating the right    exibility
in any ef   cient dnn engine.

in this section, we will give an overview of various popular
dnns such as lenet [48] as well as those that competed in
and/or won the id163 challenge [14] as shown in fig. 7,
most of whose models with pre-trained weights are publicly
available for download; the dnn models are summarized in
table ii. two results for top-5 error results are reported. in the
   rst row, the accuracy is boosted by using multiple crops from
the image and an ensemble of multiple trained models (i.e.,
the dnn needs to be run several times); these results were
used to compete in the id163 challenge. the second row
reports the accuracy if only a single crop was used (i.e., the

9 3 5 3 10 32 2 2 1 3 21 9 2 6 11 7 2x2 pooling, stride 2 32 5 6 21 max pooling average pooling 18 3 3 12 9

fig. 14.
note that each conv layer is followed by a relu (not drawn).

inception module from googlenet [51] with example channel lengths.

(a) without bottleneck

(b) with bottleneck

fig. 15. shortcut module from resnet [15]. note that relu following last
conv layer in short cut is after the addition.

is used. this is similar to the id137 that are used for
sequential data. resnet also uses the    bottleneck    approach of
using 1  1    lters to reduce the number of weight parameters.
as a result, the two layers in the shortcut module are replaced
by three layers (1  1, 3  3, 1  1) where the 1  1 reduces and
then increases (restores) the number of weights. resnet-50
consists of one conv layer, followed by 16 shortcut layers
(each of which are three conv layers deep), and one fc
layer; it requires 25.5m weights and 3.9g macs per image.
there are various versions of resnet with multiple depths
(e.g., without bottleneck: 18, 34; with bottleneck: 50, 101, 152).
the resnet with 152 layers was the winner of the id163
challenge requiring 11.3g macs and 60m weights. compared
to resnet-50, it reduces the top-5 error by around 1% at the
cost of 2.9   more macs and 2.5   more weights.

several trends can be observed in the popular dnns shown
in table ii. increasing the depth of the network tends to provide
higher accuracy. controlling for number of weights, a deeper
network can support a wider range of non-linear functions

(a) constructing a 5  5 support from 3  3    lters. used in vgg-16.

(b) constructing a 5  5 support from 1  5 and 5  1    lter. used in
googlenet/inception v3 and v4.

fig. 13. decomposing larger    lters into smaller    lters.

googlenet [51] goes even deeper with 22 layers. it in-
troduced an inception module, shown in fig. 14, which is
composed of parallel connections, whereas previously there
was only a single serial connection. different sized    lters (i.e.,
1  1, 3  3, 5  5), along with 3  3 max-pooling, are used for
each parallel connection and their outputs are concatenated
for the module output. using multiple    lter sizes has the
effect of processing the input at multiple scales. for improved
training speed, googlenet is designed such that the weights
and the activations, which are stored for id26 during
training, could all    t into the gpu memory. in order to reduce
the number of weights, 1  1    lters are applied as a    bottleneck   
to reduce the number of channels for each    lter [52]. the 22
layers consist of three conv layers, followed by 9 inceptions
layers (each of which are two conv layers deep), and one fc
layer. since its introduction in 2014, googlenet (also referred
to as inception) has multiple versions: v1 (described here), v3 7
and v4. inception-v3 decomposes the convolutions by using
smaller 1-d    lters as shown in fig. 13(b) to reduce number
of macs and weights in order to go deeper to 42 layers.
in conjunction with batch id172 [47], v3 achieves
over 3% lower top-5 error than v1 with 2.5   increase in
computation [53]. inception-v4 uses residual connections [54],
described in the next section, for a 0.4% reduction in error.

resnet [15], also known as residual net, uses residual
connections to go even deeper (34 layers or more). it was
the    rst entry dnn in id163 challenge that exceeded
human-level accuracy with a top-5 error rate below 5%. one
of the challenges with deep networks is the vanishing gradient
during training: as the error backpropagates through the network
the gradient shrinks, which affects the ability to update the
weights in the earlier layers for very deep networks. residual
net introduces a    shortcut    module which contains an identity
connection such that the weight layers (i.e., conv layers)
can be skipped as shown in fig. 15. rather than learning the
function for the weight layers f (x), the shortcut module learns
the residual mapping (f (x) = h(x)     x). initially, f (x) is
zero and the identity connection is taken; then gradually during
training, the actual forward connection through the weight layer

7v2 is very similar to v3.

5x5 filter two 3x3 filters decompose apply sequentially decompose 5x5 filter 5x1 filter 1x5 filter apply sequentially 1x1 conv 3x3 conv 5x5 conv 1x1 conv 1x1 conv 1x1 conv 3x3 max pool input feature map output feature map c=64 c=192 c=32 c=32 c=128 c=64 c=192 c=64 c=256 3x3 conv relu relu 3x3 conv + x	f(x)	h(x)	=	f(x)	+	x	iden%ty	x	3x3 conv relu relu 1x1 conv + x	f(x)	h(x)	=	f(x)	+	x	1x1 conv relu iden%ty	x	10

that are more discriminative and also provides more levels
of hierarchy in the learned representation [15, 50, 51, 55].
the number of    lter shapes continues to vary across layers,
thus    exibility is still important. furthermore, most of the
computation has been placed on conv layers rather than fc
layers. in addition, the number of weights in the fc layers is
reduced and in most recent networks (since googlenet) the
conv layers also dominate in terms of weights. thus, the
focus of hardware implementations should be on addressing
the ef   ciency of the conv layers, which in many domains
are increasingly important.

iv. dnn development resources

one of the key factors that has enabled the rapid development
of dnns is the set of development resources that have been
made available by the research community and industry. these
resources are also key to the development of dnn accelerators
by providing characterizations of the workloads and facilitating
the exploration of trade-offs in model complexity and accuracy.
this section will describe these resources such that those who
are interested in this    eld can quickly get started.

b. models

pretrained dnn models can be downloaded from various
websites [56   59] for the various different frameworks. it should
be noted that even for the same dnn (e.g., alexnet) the
accuracy of these models can vary by around 1% to 2%
depending on how the model was trained, and thus the results
do not always exactly match the original publication.

c. popular datasets for classi   cation

it is important to factor in the dif   culty of the task when
comparing different dnn models. for instance, the task of
classifying handwritten digits from the mnist dataset [62]
is much simpler than classifying an object into one of 1000
classes as is required for the id163 dataset [14](fig. 16).
it is expected that the size of the dnns (i.e., number of
weights) and the number of macs will be larger for the more
dif   cult task than the simpler task and thus require more
energy and have lower throughput. for instance, lenet-5[48]
is designed for digit classi   cation, while alexnet[3], vgg-
16[50], googlenet[51], and resnet[15] are designed for the
1000-class image classi   cation.

a. frameworks

for ease of dnn development and to enable sharing of
trained networks, several deep learning frameworks have been
developed from various sources. these open source libraries
contain software libraries for dnns. caffe was made available
in 2014 from uc berkeley [46]. it supports c, c++, python
and matlab. tensor   ow was released by google in 2015,
and supports c++ and python; it also supports multiple cpus
and gpus and has more    exibility than caffe, with the
computation expressed as data   ow graphs to manage the
tensors (multidimensional arrays). another popular framework
is torch, which was developed by facebook and nyu and
supports c, c++ and lua. there are several other frameworks
such as theano, mxnet, cntk, which are described in [60].
there are also higher-level libraries that can run on top of
the aforementioned frameworks to provide a more universal
experience and faster development. one example of such
libraries is keras, which is written in python and supports
tensor   ow, cntk and theano.

the existence of such frameworks are not only a convenient
aid for dnn researchers and application designers, but they
are also invaluable for engineering high performance or more
ef   cient dnn computation engines. in particular, because the
frameworks make heavy use of a set primitive operations,
such processing of a conv layer, they can incorporate use of
optimized software or hardware accelerators. this acceleration
is transparent to the user of the framework. thus, for example,
most frameworks can use nvidia   s cudnn library for rapid
execution on nvidia gpus. similarly, transparent incorporation
of dedicated hardware accelerators can be achieved as was
done with the eyeriss chip [61].

finally, these frameworks are a valuable source of workloads
for hardware researchers. they can be used to drive experi-
mental designs for different workloads, for pro   ling different
workloads and for exploring hardware-software trade-offs.

there are many ai tasks that come with publicly available
datasets in order to evaluate the accuracy of a given dnn.
public datasets are important for comparing the accuracy of
different approaches. the simplest and most common task
is image classi   cation, which involves being given an entire
image, and selecting 1 of n classes that the image most likely
belongs to. there is no localization or detection.
mnist is a widely used dataset for digit classi   cation
that was introduced in 1998 [62]. it consists of 28  28 pixel
grayscale images of handwritten digits. there are 10 classes
(for 10 digits) and 60,000 training images and 10,000 test
images. lenet-5 was able to achieve an accuracy of 99.05%
when mnist was    rst introduced. since then the accuracy has
increased to 99.79% using id173 of neural networks
with dropconnect [63]. thus, mnist is now considered a fairly
easy dataset.
cifar is a dataset that consists of 32  32 pixel colored
images of of various objects, which was released in 2009 [64].
cifar is a subset of the 80 million tiny image dataset [65].
cifar-10 is composed of 10 mutually exclusive classes. there
are 50,000 training images (5000 per class) and 10,000 test
images (1000 per class). a two-layer convolutional deep belief
network was able to achieve 64.84% accuracy on cifar-10
when it was    rst introduced [66]. since then the accuracy has
increased to 96.53% using fractional max pooling [67].

id163

is a large scale image dataset that was    rst
introduced in 2010; the dataset stabilized in 2012 [14]. it
contains images of 256  256 pixel in color with 1000 classes.
the classes are de   ned using the id138 as a backbone to
handle ambiguous word meanings and to combine together
synonyms into the same object category. in otherwords, there
is a hierarchy for the id163 categories. the 1000 classes
were selected such that there is no overlap in the id163
hierarchy. the id163 dataset contains many    ne-grained
categories including 120 different breeds of dogs. there are
1.3m training images (732 to 1300 per class), 100,000 testing

metrics

top-5 error   

top-5 error (single crop)   

input size

lenet

5
n/a
n/a
28  28

alexnet

16.4
19.8

227  227

# of conv layers

depth in # of conv layers

filter sizes
# of channels
# of filters

stride
weights
macs

# of fc layers

filter sizes
# of channels
# of filters

weights
macs

total weights
total macs

pretrained model website

2
2
5

1

1, 20
20, 50

2.6k
283k

2
1,4

58k
58k
60k
341k
[56]   

5
5

3,5,11
3-256
96-384

1,4
2.3m
666m

3
1,6

58.6m
58.6m
61m
724m
[57, 58]

50, 500
10, 500

256-4096
1000-4096

overfeat

fast
14.2
17.0

5
5

231  231

3,5,11
3-1024
96-1024

1,4
16m
2.67g

3

1,6,12

1024-4096
1000-4096

130m
130m
146m
2.8g
n/a

11

vgg
16
7.4
8.8

224  224

13
13
3

3-512
64-512

1

14.7m
15.3g

3
1,7

512-4096
1000-4096

124m
124m
138m
15.5g
[57   59]

googlenet

resnet

v1
6.7
10.7

50
5.3
7.0

224  224

224  224

57
21

1,3,5,7
3-832
16-384

1,2
6.0m
1.43g

53
49
1,3,7
3-2048
64-2048

1,2

23.5m
3.86g

1
1

1024
1000
1m
1m
7m
1.43g
[57   59]

1
1

2048
1000
2m
2m

25.5m
3.9g
[57   59]

summary of popular dnns [3, 15, 48, 50, 51].    accuracy is measured based on top-5 error on id163 [14].    this version of lenet-5

has 431k weights for the filters and requires 2.3m macs per image, and uses relu rather than sigmoid.

table ii

be localized and classi   ed (out of 1000 classes). the dnn
outputs the top    ve categories and top    ve bounding box
locations. there is no penalty for identifying an object that
is in the image but not included in the ground truth. for
id164, all objects in the image must be localized
and classi   ed (out of 200 classes). the bounding box for all
objects in these categories must be labeled. objects that are
not labeled are penalized as are duplicated detections.

beyond id163,

there are also other popular image
datasets for id161 tasks. for id164, there
is the pascal voc (2005-2012) dataset that contains 11k
images representing 20 classes (27k object instances, 7k of
which has detailed segmentation) [68]. for id164,
segmentation and recognition in context, there is the ms coco
dataset with 2.5m labeled instances in 328k images (91 object
categories) [69]; compared to id163, coco has fewer
categories but more instances per category, which is useful for
precise 2-d localization. coco also has more labeled instances
per image to potentially help with contextual information.

most recently even larger scale datasets have been made
available. for instance, google has an open images dataset
with over 9m images [70], spanning 6000 categories. there is
also a youtube dataset with 8m videos (0.5m hours of video)
covering 4800 classes [71]. google also released an audio
dataset comprised of 632 audio event classes and a collection
of 2m human-labeled 10-second sound clips [72]. these large
datasets will be evermore important as dnns become deeper
with more weight parameters to train.

undoubtedly, both larger datasets and datasets for new
domains will serve as important resources for pro   ling and
exploring the ef   ciency of future dnn engines.

v. hardware for dnn processing

due to the popularity of dnns, many recent hardware
platforms have special features that target dnn processing. for

fig. 16. mnist (10 classes, 60k training, 10k testing) [62] vs. id163
(1000 classes, 1.3m training, 100k testing)[14] dataset.

images (100 per class) and 50,000 validation images (50 per
class).

the accuracy of the id163 challenge are reported using
two metrics: top-5 and top-1 error. top-5 error means that if
any of the top    ve scoring categories are the correct category,
it is counted as a correct classi   cation. the top-1 requires
that the top scoring category be correct. in 2012, the winner
of the id163 challenge (alexnet) was able to achieve an
accuracy of 83.6% for the top-5 (which is substantially better
than the 73.8% which was second place that year that did not
use dnns); it achieved 61.9% on the top-1 of the validation
set. in 2017, the highest accuracy was 97.7% for the top-5.

in summary of the various image classi   cation datasets, it
is clear that mnist is a fairly easy dataset, while id163
is a challenging one with a wider coverage of classes. thus
in terms of evaluating the accuracy of a given dnn, it is
important to consider that dataset upon which the accuracy is
measured.

d. datasets for other tasks

since the accuracy of the state-of-the-art dnns are perform-
ing better than human-level accuracy on image classi   cation
tasks, the id163 challenge has started to focus on more
dif   cult tasks such as single-object localization and object
detection. for single-object localization, the target object must

mnist id163 12

instance, the intel knights landing cpu features special vector
instructions for deep learning; the nvidia pascal gp100
gpu features 16-bit    oating point (fp16) arithmetic support
to perform two fp16 operations on a single precision core for
faster deep learning computation. systems have also been built
speci   cally for dnn processing such as nvidia dgx-1 and
facebook   s big basin custom dnn server [73]. dnn id136
has also been demonstrated on various embedded system-on-
chips (soc) such as nvidia tegra and samsung exynos as
well as fpgas. accordingly, it   s important to have a good
understanding of how the processing is being performed on
these platforms, and how application-speci   c accelerators can
be designed for dnns for further improvement in throughput
and energy ef   ciency.

the fundamental component of both the conv and fc lay-
ers are the multiply-and-accumulate (mac) operations, which
can be easily parallelized. in order to achieve high performance,
highly-parallel compute paradigms are very commonly used,
including both temporal and spatial architectures as shown in
fig. 17. the temporal architectures appear mostly in cpus
or gpus, and employ a variety of techniques to improve
parallelism such as vectors (simd) or parallel threads (simt).
such temporal architecture use a centralized control for a large
number of alus. these alus can only fetch data from the
memory hierarchy and cannot communicate directly with each
other. in contrast, spatial architectures use data   ow processing,
i.e., the alus form a processing chain so that they can pass data
from one to another directly. sometimes each alu can have
its own control logic and local memory, called a scratchpad or
register    le. we refer to the alu with its own local memory as
a processing engine (pe). spatial architectures are commonly
used for dnns in asic and fpga-based designs. in this
section, we will discuss the different design strategies for
ef   cient processing on these different platforms, without any
impact on accuracy (i.e., all approaches in this section produce
bit-wise identical results); speci   cally,

    for temporal architectures such as cpus and gpus, we
will discuss how computational transforms on the kernel
can reduce the number of multiplications to increase
throughput.

    for spatial architectures used in accelerators, we will
discuss how data   ows can increase data reuse from low
cost memories in the memory hierarchy to reduce energy
consumption.

a. accelerate kernel computation on cpu and gpu platforms
cpus and gpus use parallelizaton techniques such as simd
or simt to perform the macs in parallel. all the alus share
the same control and memory (register    le). on these platforms,
both the fc and conv layers are often mapped to a matrix
multiplication (i.e., the kernel computation). fig. 18 shows how
a id127 is used for the fc layer. the height of
the    lter matrix is the number of    lters and the width is the
number of weights per    lter (input channels (c)    width (w )
   height (h), since r = w and s = h in the fc layer);
the height of the input feature maps matrix is the number of
activations per input feature map (c    w    h), and the

fig. 17. highly-parallel compute paradigms.

(a) matrix vector multiplication is used when computing a single output
feature map from a single input feature map.

(b) id127s is used when computing n output feature
maps from n input feature maps.

fig. 18. mapping to id127 for fully connected layers

width is the number of input feature maps (one in fig. 18(a)
and n in fig. 18(b));    nally, the height of the output feature
map matrix is the number of channels in the output feature
maps (m), and the width is the number of output feature maps
(n), where each output feature map of the fc layer has the
dimension of 1  1  number of output channels (m).

the conv layer in a dnn can also be mapped to a matrix
multiplication using a relaxed form of the toeplitz matrix as
shown in fig. 19. the downside for using id127
for the conv layers is that there is redundant data in the input
feature map matrix as highlighted in fig. 19(a). this can lead
to either inef   ciency in storage, or a complex memory access
pattern.

there are software libraries designed for cpus (e.g., open-
blas, intel mkl, etc.) and gpus (e.g., cublas, cudnn,
etc.) that optimize for id127s. the matrix
multiplication is tiled to the storage hierarchy of these platforms,
which are on the order of a few megabytes at the higher levels.

temporal architecture (simd/simt) spatial architecture (dataflow processing) register file memory hierarchy alu alu alu alu alu alu alu alu alu alu alu alu alu alu alu alu control memory hierarchy alu alu alu alu alu alu alu alu alu alu alu alu alu alu alu alu m chw chw 1 filters input fmaps    1 output fmaps m = m chw chw n filters input fmaps    n output fmaps m = 13

(a) mapping convolution to toeplitz matrix

fig. 20. fft to accelerate dnn.

(b) extend toeplitz matrix to multiple channels and    lters

fig. 19. mapping to id127 for convolutional layers.

o n 2

f ) to o(n 2

the id127s on these platforms can be further
sped up by applying computational transforms to the data to
reduce the number of multiplications, while still giving the
same bit-wise result. often this can come at a cost of increased
number of additions and a more irregular data access pattern.
fast fourier transform (fft) [10, 74] is a well known
approach, shown in fig. 20 that reduces the number of
multiplications from o(n 2
o log2no), where the
output size is no    no and the    lter size is nf    nf . to
perform the convolution, we take the fft of the    lter and
input feature map, and then perform the multiplication in
the frequency domain; we then apply an inverse fft to the
resulting product to recover the output feature map in the
spatial domain. however, there are several drawbacks to using
fft: (1) the bene   ts of ffts decrease with    lter size; (2) the
size of the fft is dictated by the output feature map size which
is often much larger than the    lter; (3) the coef   cients in the
frequency domain are complex. as a result, while fft reduces
computation, it requires larger storage capacity and bandwidth.
finally, a popular approach for reducing complexity is to make
the weights sparse, which will be discussed in section vii-b2;
using ffts makes it dif   cult for this sparsity to be exploited.
several optimizations can be performed on fft to make it
more effective for dnns. to reduce the number of operations,
the fft of the    lter can be precomputed and stored. in addition,
the fft of the input feature map can be computed once and
used to generate multiple channels in the output feature map.
finally, since an image contains only real values, its fourier
transform is symmetric and this can be exploited to reduce
storage and computation cost.

other approaches include strassen [75] and winograd [76],
which rearrange the computation such that the number of
multiplications reduce from o(n 3) to o(n 2.807) and by 2.25  

fig. 21. read and write access per mac.

for a 3  3    lter, respectively, at the cost of reduced numeri-
cal stability, increased storage requirements, and specialized
processing depending on the size of the    lter.
in practice, different algorithms might be used for different
layer shapes and sizes (e.g., fft for    lters greater than 5  5,
and winograd for    lters 3  3 and below). existing platform
libraries, such as mkl and cudnn, dynamically chose the
appropriate algorithm for a given shape and size [77, 78].

b. energy-ef   cient data   ow for accelerators

for dnns, the bottleneck for processing is in the memory
access. each mac requires three memory reads (for    lter
weight, fmap activation, and partial sum) and one memory
write (for the updated partial sum) as shown in fig. 21. in the
worst case, all of the memory accesses have to go through the
off-chip dram, which will severely impact both throughput
and energy ef   ciency. for example, in alexnet, to support its
724m macs, nearly 3000m dram accesses will be required.
furthermore, dram accesses require up to several orders of
magnitude higher energy than computation [79].

accelerators, such as spatial architectures as shown in
fig. 17, provide an opportunity to reduce the energy cost of
data movement by introducing several levels of local memory
hierarchy with different energy cost as shown in fig. 22. this
includes a large global buffer with a size of several hundred
kilobytes that connects to dram, an inter-pe network that
can pass data directly between the alus, and a register    le
(rf) within each processing element (pe) with a size of a
few kilobytes or less. the multiple levels of memory hierarchy
help to improve energy ef   ciency by providing low-cost data
accesses. for example, fetching the data from the rf or
neighbor pes is going to cost 1 or 2 orders of magnitude
lower energy than from dram.

accelerators can be designed to support specialized process-
ing data   ows that leverage this memory hierarchy. the data   ow

1234567891234filter input fmap output fmap * = 1234123412452356457856891234   = toeplitz matrix (w/ redundant data) convolution: matrix mult: = 12341234123412341245235645785689124523564578568912123434   toeplitz matrix (w/ redundant data) chnl 1 chnl 2 filter 1 filter 2 chnl 1 chnl 2 chnl 1 chnl 2 r filter (weights) s e f input fmap output fmap h w an output  activation * = fft(w) fftfft(i) x = fft(0) fftifft        alu filter weight fmap activation partial sum updated partial sum memory read memory write mac* * multiply-and-accumulate 14

fig. 22. memory hierarchy and data movement energy [80].

fig. 23. data reuse opportunities in dnns [80].

decides what data gets read into which level of the memory
hierarchy and when are they getting processed. since there is
no randomness in the processing of dnns, it is possible to
design a    xed data   ow that can adapt to the dnn shapes and
sizes and optimize for the best energy ef   ciency. the optimized
data   ow minimizes access from the more energy consuming
levels of the memory hierarchy. large memories that can store
a signi   cant amount of data consume more energy than smaller
memories. for instance, dram can store gigabytes of data, but
consumes two orders of magnitude higher energy per access
than a small on-chip memory of a few kilobytes. thus, every
time a piece of data is moved from an expensive level to a
lower cost level in terms of energy, we want to reuse that piece
of data as much as possible to minimize subsequent accesses
to the expensive levels. the challenge, however, is that the
storage capacity of these low cost memories are limited. thus
we need to explore different data   ows that maximize reuse
under these constraints.

for dnns, we investigate data   ows that exploit three forms
of input data reuse (convolutional, feature map and    lter) as
shown in fig. 23. for convolutional reuse, the same input
feature map activations and    lter weights are used within
a given channel, just in different combinations for different
weighted sums. for feature map reuse, multiple    lters are
applied to the same feature map, so the input feature map
activations are used multiple times across    lters. finally, for
   lter reuse, when multiple input feature maps are processed at
once (referred to as a batch), the same    lter weights are used
multiple times across input features maps.

if we can harness the three types of data reuse by storing
the data in the local memory hierarchy and accessing them
multiple times without going back to the dram, it can save
a signi   cant amount of dram accesses. for example, in
alexnet, the number of dram reads can be reduced by up to
500   in the conv layers. the local memory can also be used
for partial sum accumulation, so they do not have to reach
dram. in the best case, if all data reuse and accumulation
can be achieved by the local memory hierarchy, the 3000m
dram accesses in alexnet can be reduced to only 61m.

the operation of dnn accelerators is analogous to that of
general-purpose processors as illustrated in fig. 24 [81]. in
conventional computer systems, the compiler translates the

fig. 24. an analogy between the operation of dnn accelerators (texts in
black) and that of general-purpose processors (texts in red). figure adopted
from [81].

program into machine-readable binary codes for execution
given the hardware architecture (e.g., x86 or arm); in the
processing of dnns, the mapper translates the dnn shape
and size into a hardware-compatible computation mapping
for execution given the data   ow. while the compiler usually
optimizes for performance, the mapper optimizes for energy
ef   ciency.

the following taxonomy (fig. 25) can be used to classify
the dnn data   ows in recent works [82   93] based on their
data handling characteristics [80]:

1) weight stationary (ws): the weight stationary data   ow
is designed to minimize the energy consumption of reading
weights by maximizing the accesses of weights from the register
   le (rf) at the pe (fig. 25(a)). each weight is read from
dram into the rf of each pe and stays stationary for further
accesses. the processing runs as many macs that use the
same weight as possible while the weight is present in the rf;
it maximizes convolutional and    lter reuse of weights. the
inputs and partial sums must move through the spatial array
and global buffer. the input fmap activations are broadcast to
all pes and then the partial sums are spatially accumulated
across the pe array.

one example of previous work that implement weight
stationary data   ow is nn-x, or neuflow [85], which uses
eight 2-d convolution engines for processing a 10  10    lter.
there are total 100 mac units, i.e. pes, per engine with each
pe having a weight that stays stationary for processing. the

dram global buffer pe pe pe alu fetch data to run  a mac here alu buffer alu rf alu normalized energy cost 200   6   pe alu 2   1   1   (reference) dram alu 0.5     1.0 kb 100     500 kb noc: 200     1000 pes filter reuse convolutional reuse fmap reuse conv layers only (sliding window) conv and fc layers conv and fc layers (batch size > 1) filter input fmap       filters 2 1     input fmap     filter   2 1 input fmaps     activations filter weights reuse: activations reuse: filter weights reuse: compilationexecutiondnn shape and size(program)mappinginputdataprocesseddatamapper(compiler)dnn accelerator(processor)dataflow,    (architecture)(binary)implementation details(  arch)15

(a) weight stationary

(b) output stationary

fig. 26. variations of output stationary [80].

(c) no local reuse

fig. 25. data   ows for dnns [80].

input fmap activations are broadcast to all mac units and the
partial sums are accumulated across the mac units. in order to
accumulate the partial sums correctly, additional delay storage
elements are required, which are counted into the required size
of local storage. other weight stationary examples are found
in [82   84, 86, 87].

2) output stationary (os): the output stationary data   ow is
designed to minimize the energy consumption of reading and
writing the partial sums (fig. 25(b)). it keeps the accumulation
of partial sums for the same output activation value local in the
rf. in order to keep the accumulation of partial sums stationary
in the rf, one common implementation is to stream the input
activations across the pe array and broadcast the weight to all
pes in the array.

one example that implements the output stationary data   ow
is shidiannao [89], where each pe handles the processing for
each output activation value by fetching the corresponding input
activations from neighboring pes. the pe array implements
dedicated networks to pass data horizontally and vertically.
each pe also has data delay registers to keep data around for
the required amount of cycles. at the system level, the global
buffer streams the input activations and broadcasts the weights
into the pe array. the partial sums are accumulated inside
each pe and then get streamed out back to the global buffer.
other examples of output stationary are found in [88, 90].

there are multiple possible variants of output stationary as
shown in fig. 26 since the output activations that get processed
at the same time can come from different dimensions. for
example, the variant osa targets the processing of conv
layers, and therefore focuses on the processing of output
activations from the same channel at a time in order to
maximize data reuse opportunities. the variant osc targets
the processing of fc layers, and focuses on generating output
activations from all different channels, since each channel only
has one output activation. the variant osb is something in
between osa and osc. example of variants osa, osb, and

osc are [89], [88], and [90], respectively.

3) no local reuse (nlr): while small register    les are
ef   cient in terms of energy (pj/bit), they are inef   cient in terms
of area (  m2/bit). in order to maximize the storage capacity,
and minimize the off-chip memory bandwidth, no local storage
is allocated to the pe and instead all that area is allocated
to the global buffer to increase its capacity (fig. 25(c)). the
no local reuse data   ow differs from the previous data   ows in
that nothing stays stationary inside the pe array. as a result,
there will be increased traf   c on the spatial array and to the
global buffer for all data types. speci   cally, it has to multicast
the activations, single-cast the    lter weights, and then spatially
accumulate the partial sums across the pe array.

the no local

in an example of

reuse data   ow from
ucla [91], the    lter weights and input activations are read
from the global buffer, processed by the mac units with custom
adder trees that can complete the accumulation in a single cycle,
and the resulting partial sums or output activations are then put
back to the global buffer. another example is diannao [92],
which also reads input activations and    lter weights from
the buffer, and processes them through the mac units with
custom adder trees. however, diannao implements specialized
registers to keep the partial sums in the pe array, which helps
to further reduce the energy consumption of accessing partial
sums. another example of no local reuse data   ow is found
in [93].

4) row stationary (rs): a row stationary data   ow is
proposed in [80], which aims to maximize the reuse and
accumulation at the rf level for all types of data (weights,
pixels, partial sums) for the overall energy ef   ciency. this
differs from ws or os data   ows, which optimize for only
weights and partial sums, respectively.

the row stationary data   ow assigns the processing of a
1-d row convolution into each pe for processing as shown
in fig. 27. it keeps the row of    lter weights stationary inside
the rf of the pe and then streams the input activations into
the pe. the pe does the macs for each sliding window at a
time, which uses just one memory space for the accumulation
of partial sums. since there are overlaps of input activations
between different sliding windows, the input activations can
then be kept in the rf and get reused. by going through all the
sliding windows in the row, it completes the 1-d convolution
and maximize the data reuse and local accumulation of data
in this row.

                  global buffer w0 w1 w2 w3 w4 w5 w6 w7              psum act pe weight                   global buffer p0 p1 p2 p3 p4 p5 p6 p7               act weight pe psum pe         act psum global buffer weight # output channels # output activations e e             m osb multiple multiple notes e e             m osa single multiple targeting conv layers e e             m osc multiple single targeting fc layers parallel  output region 16

(a) step 1

(b) step 2

(c) step 3

fig. 27. 1-d convolutional reuse within pe for row stationary data   ow [80].

fig. 29. multiple rows of different input feature maps,    lters and channels are
mapped to same pe within array for additional reuse in the row stationary
data   ow [80].

fig. 28. 2-d convolutional reuse within spatial array for row stationary
data   ow [80].

with each pe processing a 1-d convolution, multiple
pes can be aggregated to complete the 2-d convolution as
shown in fig. 28. for example, to generate the    rst row of
output activations with a    lter having three rows, three 1-d
convolutions are required. therefore, we can use three pes in
a column, each running one of the three 1-d convolutions. the
partial sums are further accumulated vertically across the three
pes to generate the    rst output row. to generate the second
row of output, we use another column of pes, where three
rows of input activations are shifted down by one row, and use
the same rows of    lters to perform the three 1-d convolutions.
additional columns of pes are added until all rows of the
output are completed (i.e., the number of pe columns equals
the number of output rows).

this 2-d array of pes enables other forms of reuse to reduce
accesses to the more expensive global buffer. for example, each
   lter row is reused across multiple pes horizontally. each row
of input activations is reused across multiple pes diagonally.
and each row of partial sums are further accumulated across
the pes vertically. therefore, 2-d convolutional data reuse and
accumulation are maximized inside the 2-d pe array.

to address the high-dimensional convolution of the conv
layer (i.e., multiple fmaps,    lters, and channels), multiple rows
can be mapped onto the same pe as shown in fig. 29. the
2-d convolution is mapped to a set of pes, and the additional
dimensions are handled by interleaving or concatenating the
additional data. for    lter reuse within the pe, different rows
of fmaps are concatenated and run through the same pe
as a 1-d convolution. for input fmap reuse within the pe,
different    lter rows are interleaved and run through the same
pe as a 1-d convolution. finally, to increase local partial sum
accumulation within the pe,    lter rows and fmap rows from

fig. 30. mapping optimization takes in hardware and dnns shape constraints
to determine optimal energy data   ow [80].

different channels are interleaved, and run through the same pe
as a 1-d convolution. the partial sums from different channels
then naturally get accumulated inside the pe.

the number of    lters, channels, and fmaps that can be
processed at the same time is programmable, and there exists an
optimal mapping for the best energy ef   ciency, which depends
on the shape con   guration of the dnn as well as the hardware
resources provided, e.g., the number of pes and the size of the
memory in the hierarchy. since all of the variables are known
before runtime, it is possible to build a compiler (i.e., mapper)
to perform this optimization off-line to con   gure the hardware
for different mappings of the rs data   ow for different dnns
as shown in fig. 30.
one example that implements the row stationary data   ow
is eyeriss [94]. it consists of a 14  12 pe array, a 108kb
global buffer, relu and fmap compression units as shown
in fig. 31. the chip communicates with the off-chip dram
using a 64-bit bidirectional data bus to fetch data into the
global buffer. the global buffer then streams the data into the
pe array for processing.

in order to support the rs data   ow, two problems need to be
solved in the hardware design. first, how can the    xed-size pe
array accommodate different layer shapes? second, although
the data will be passed in a very speci   c pattern, it still changes
with different shape con   gurations. how can the    xed design

* = filter a b c a b c a b c d e e d pe b a c reg file b a c a       partial sums input fmap * = a b c a b c d e partial sums input fmap pe b a c reg file c b d b       e a filter a b c * = a b c a b c d e partial sums input fmap pe b a c reg file d c e c       b a filter a b c pe 1 row 1 row 1 pe 2 row 2 row 2 pe 3 row 3 row 3 row 1 = * pe 4 row 1 row 2 pe 5 row 2 row 3 pe 6 row 3 row 4 row 2 = * pe 7 row 1 row 3 pe 8 row 2 row 4 pe 9 row 3 row 5 row 3 = * * * * * * * * * * multiplefmaps:multiplefilters:multiplechannels:perow 1row 1perow 2row 2perow 3row 3perow 1row 2perow 2row 3perow 3row 4perow 1row 3perow 2row 4perow 3row 5*********image 1=psumfilter 1**image 1=psum 1 & 2filter 1 & 2*image 1 & 2=psum 1 & 2filter 1fmapfmapfmap   m      rrrrcceehhcee1nn1mhhc1id98 configurations global bufferalualualualualualualualualualualualualualualualuhardware resources optimization compiler     row stationary mapping multiplefmaps:multiplefilters:multiplechannels:perow 1row 1perow 2row 2perow 3row 3perow 1row 2perow 2row 3perow 3row 4perow 1row 3perow 2row 4perow 3row 5*********image 1=psumfilter 1**image 1=psum 1 & 2filter 1 & 2*image 1 & 2=psum 1 & 2filter 1fmapfmapfmap17

needs of each data   ow under the same area constraint. for
example, since the no local reuse data   ow does not require any
rf in pe, it is allocated with a much larger global buffer. the
simulation uses the layer con   gurations from alexnet with a
batch size of 16. the simulation also takes into account the
fact that accessing different levels of the memory hierarchy
requires different energy cost.

fig. 33 compares the chip and dram energy consumption
of each data   ow for the conv layers of alexnet with a
batch size of 16. the ws and os data   ows have the lowest
energy consumption for accessing weights and partial sums,
respectively. however, the rs data   ow has the lowest total
energy consumption since it optimizes for the overall energy
ef   ciency instead of only for a certain data type.

fig. 33(a) shows the same results with breakdown in terms of
memory hierarchy. the rs data   ow consumes the most energy
in the rf, since by design most of the accesses have been
moved to the lowest level of the memory hierarchy. this helps
to achieve the lowest total energy consumption since rf has
the lowest energy per access. the nlr data   ow has the lowest
energy consumption at the dram level, since it has a much
larger global buffer and thus higher on-chip storage capacity
compared to others. however, most of the data accesses in
the nlr data   ow is from the global buffer, which still has a
relatively large energy consumption per access compared to
accessing data from rf or inside the pe array. as a result, the
overall energy consumption of the nlr data   ow is still fairly
high. overall, rs data   ow uses 1.4   to 2.5   lower energy
than other data   ows.

fig. 34 shows the energy ef   ciency between different
data   ows in the fc layers of alexnet with a batch size of 16.
since there is not as much data reuse in the fc layers as in
the conv layers, all data   ows spend a signi   cant amount of
energy on reading weights. however, rs data   ow still has the
lowest energy consumption because it optimizes for the energy
of accessing input activations and partial sums. for the os
data   ows, osc now consumes lower energy than osa since
it is designed for the fc layers. overall, rs still consumes
1.3   lower energy compared to other data   ows at the batch
size of 16.

fig. 35 shows the rs data   ow design with energy breakdown
in terms of different layers of alexnet. in the conv layers, the
energy is mostly consumed by the rf, while in the fc layers,
the energy is mostly consumed by dram. however, most
of the energy is consumed by the conv layers, which takes
around 80% of the energy. as recent dnn models go deeper
with more conv layers, the ratio between number of conv
and fc layers only gets larger. therefore, moving forward,
signi   cant effort should be placed on energy optimizations for
conv layers.

finally, up until now, we have been looking at architec-
tures with relatively limited storage on the order of a few
hundred kilobytes. with much larger storage on the order of
a few megabytes, additional data   ows can be considered. for
example, fused-layer looks at data   ow optimizations across
layers [96].

fig. 31. eyeriss dnn accelerator [94].

fig. 32. mapping uses replication and folding to maximized utilization of
pe array [94].

pass data in different patterns?

two mapping strategies can be used to solve the    rst problem
as shown in fig. 32. first, replication can be used to map shapes
that do not use up the entire pe array. for example, in the
third to    fth layers of alexnet, each 2-d convolution only uses
a 13  3 pe array. this structure is then replicated four times,
and runs different channels and    lters in each replication. the
second strategy is called folding. for example, in the second
layer of alexnet, it requires a 27  5 pe array to complete the
2-d convolution. in order to    t it into the 14  12 physical pe
array, it is folded into two parts, 14  5 and 13  5, and each
are vertically mapped into the physical pe array. since not all
pes are used by the mapping, the unused pes can be clock
gated to save energy consumption.

a custom multicast network is used to solve the second
problem about    exible data delivery. the simplest way to pass
data to multiple destinations is to broadcast the data to all pes
and let each pe decide if it has to process the data or not.
however, it is not very energy ef   cient especially when the
size of pe array is large. instead, a multicast network is used
to send data to only the places where it is needed.

5) energy comparison of different data   ows: to evaluate
and compare different data   ows, the same total hardware area
and number of pes (256) are used in the simulation of a spatial
architecture for all data   ows. the local memory (register    le) at
each processing element (pe) is on the order of 0.5     1.0kb and
a shared memory (global buffer) is on the order of 100     500kb.
the sizes of these memories are selected to be comparable to
a typical accelerator for multimedia processing, such as video
coding [95]. the memory sizes are further adjusted for the

                  rlcdecoderifmapofmapfilterfilterifmappsumpsumglobalbuffer108kb64 bitslink clock core clock acceleratorrlcenc.top-level controlconfig scan chainconfiguration bitsmacspadcontrol12   14pe arrayprocessingelementoff-chip dramrelureplication folding .. .. .. .. .. .. 3 13 alexnet layer 3-5 12 14 physical pe array 3 3 3 3 13 13 13 13 .. .. .. .. .. .. 5 27 alexnet layer 2 physical pe array 12 14 5 14 13 5 unused pes are clock gated 18

(a) energy breakdown across memory hierarchy

(b) energy breakdown across data type

fig. 33. comparison of energy ef   ciency between different data   ows in the
conv layers of alexnet with a batch size of 16 [3]: (a) breakdown in terms
of storage levels and alu, (b) breakdown in terms of data types. osa, osb
and osc are three variants of the os data   ow that are commonly seen in
different implementations [80].

fig. 34. comparison of energy ef   ciency between different data   ows in the
fc layers of alexnet with a batch size of 16 [80].

vi. near-data processing

the previous section highlighted that data movement domi-
nates energy consumption. while spatial architectures distribute
the on-chip memory such that it is closer to the computation
(e.g., into the pe), there have also been efforts to bring the
off-chip high density memory closer to the computation or to
integrate the computation into the memory itself; the latter is
often referred to as processing-in-memory or logic-in-memory.
in embedded systems, there have also been efforts to bring the
computation into the sensor where the data is    rst collected.

fig. 35. energy breakdown across layers of the alexnet [80]. rf energy
dominates in convolutional layers. dram energy dominates in the fully
connected layer. convolutional layer dominate energy consumption.

in this section, we will discuss how moving compute and data
closer to reduce data movement (i.e., near-data processing) can
be achieved using mixed-signal circuit design and advanced
memory technologies.

many of these works use analog processing which has the
drawback of increased sensitivity to circuit and device non-
idealities. consequentially, the computation is often performed
at reduced precision, which can be accounted for during
the training of the dnns using the techniques discussed in
section vii. another factor to take into consideration is that
dnns are often trained in the digital domain; thus for analog
processing, there is an additional overhead cost for analog-
to-digital conversion (adc) and digital-to-analog conversion
(dac).

a. dram

advanced memory technology can reduce the access energy
for high density memories such as drams. for instance,
embedded dram (edram) brings high density memory on-
chip to avoid the high energy cost of switching off-chip
capacitance [97]; edram is 2.85   higher density than sram
and 321   more energy ef   cient than dram (ddr3) [93].
edram also offers higher bandwidth and lower latency
compared to dram. in dnn processing, edram can be used
to store tens of megabytes of weights and activations on-chip
to avoid off-chip access, as demonstrated in dadiannao [93].
the downside of edram is that it has lower density than
off-chip dram and can increase the cost of the chip.

rather than integrating dram into the chip itself, the
dram can also be stacked on top of the chip using through
silicon vias (tsv). this technology is often referred to as 3-d
memory, and has been commercialized in the form of hybrid
memory cube (hmc) [98] and high bandwidth memory
(hbm) [99]. 3-d memory delivers an order of magnitude higher
bandwidth and reduces access energy by up to 5   relative to
existing 2-d drams, as tsv have lower capacitance than
typical off-chip interconnects. recent works have explored the
use of hmc for ef   cient dnn processing in a variety of ways.
for instance, neurocube [100] integrates simd processors into
the logic die of the hmc to bring the memory and computation

normalizedenergy/macrfnocbufferdramaluwsosaosboscnlrrsdnn dataflows00.511.5200.511.52normalizedenergy/macwsosaosboscnlrrspsumsweightspixelsdnn dataflows00.511.52psumsweightspixelsnormalizedenergy/macwsosaosboscnlrrsdnn dataflowsalu rf noc buffer dram 2.0e10	1.5e10	1.0e10	0.5e10	0	l1 l8 l2 l3 l4 l5 l6 l7 normalized energy (1 mac = 1) conv layers fc layers rf dominates dram dominates total energy 80% 20% 19

voltage as the input, and the current as the output as shown in
fig. 36(b). the addition is done by summing the currents of
different memristors with kirchhoff   s current law. this is the
ultimate form of a weight stationary data   ow, as the weights
are always held in place. the advantages of this approach
include reduced energy consumption since the computation
is embedded within memory which reduces data movement,
and increased density since memory and computation can be
densely packed with a similar density to dram [106].8

there are several popular candidates for non-volatile resistive
memory devices including phase change memory (pcm),
resistive ram (rram or reram), conductive bridge ram
(cbram), and spin transfer torque magnetic ram (stt-
mram) [107]. these devices have different trade-offs in terms
of endurance (i.e., how many times it can be written), retention
time, write current, density (i.e., cell size), variations and speed.
processing with non-volatile resistive memories has several
drawbacks as described in [108]. first, it suffers from the
reduced precision and adc/dac overhead of analog process-
ing described earlier. second, the array size is limited by the
wires that connect the resistive devices; speci   cally, wire energy
dominates for large arrays (e.g., 1k  1k), and the ir drop along
wire can degrade the read accuracy. third, the write energy
to program the resistive devices can be costly, in some cases
requiring multiple pulses. finally, the resistive devices can also
suffer from device-to-device and cycle-to-cycle variations with
non-linear conductance across the conductance range.

there have been several recent works that explore the use of
memristors for dnns. isaac [104] replaces the edram in
dadiannao with memristors. to address the limited precision
support, isaac computes a 16-bit dot product operation with
8 memristors each storing 2-bits; a 1-bit  2-bit multiplication
is performed at each memristor, where a 16-bit input requires
16 cycles to complete. in other words, the isaac architecture
trades off area and time for increased precision. finally, isaac
arranges its 25.1m memristors in a hierarchical structure to
avoid issues with large arrays. prime [109] also replaces the
dram main memory with memristors; speci   cally, it uses
256  256 memristor arrays that can be con   gured for 4-bit
multi-level cell computation or 1-bit single level cell storage.
it should be noted that results from isaac and prime are
obtained from simulations. the task of actually fabricating
large memristors arrays is still very much a research challenge;
for instance, [110] uses a fabricated 12  12 memristor array
to demonstrate a linear classi   er.

d. sensors

in certain applications, such as image processing, the data
movement from the sensor itself can account for a signi   cant
portion of the system energy consumption. thus there has
also been research on performing the computation as close
as possible to the sensor. in particular, much of the work
focuses on moving the computation into the analog domain to
avoid using the adc within the sensor, which accounts for a
signi   cant portion of the sensor power. however, as mentioned

8the resistive devices can be inserted between the cross-point of two wires

and in certain cases can avoid the need for an access transistor.

(a) multiplication performed by bit-cell
(figure from [102])

(b) gi is conductance of resistive
memory (figure from [104])

fig. 36. analog computation by (a) sram bit-cell and (b) non-volatile
resistive memory.

closer together. tetris [101] explores the use of hmc with
the eyeriss spatial architecture and row stationary data   ow.
it proposes allocating more area to computation than on-chip
memory (i.e., larger pe array and smaller global buffer) in
order to exploit the low energy and high throughput properties
of the hmc. it also adapts the data   ow to account for the
hmc memory and smaller on-chip memory. tetris achieves
a 1.5   reduction in energy consumption and 4.1   increase
in throughput over a baseline system with conventional 2-d
dram.

b. sram

rather than bringing the memory near the compute, recent
work has also investigated bringing the compute into the
memory. for instance, the multiply and accumulate operation
can be directly integrated into the bit-cells of an sram
array [102], as shown in fig. 36(a). in this work, a 5-bit
dac is used to drive the word line (wl) to an analog voltage
that represents the feature vector, while the bit-cells store the
binary weights   1. the bit-cell current (ibc) is effectively
a product of the value of the feature vector and the value of
the weight stored in the bit-cell; the currents from the bit-
cells within a column add together to discharge the bitline
(vbl). this approach gives 12   energy savings compared to
reading the 1-bit weights from the sram and performing the
computation separately. to counter circuit non-idealities, the
dac accounts for the non-linear bit-line discharge with respect
to the wl voltage, and boosting is used to combine the weak
classi   ers that are susceptible to device variations to form a
strong classi   er [103].

c. non-volatile resistive memories

the multiply and accumulate operation can also be directly
integrated into advanced non-volatile high density memories
by using them as programmable resistive elements, commonly
referred to as memristors [105]. speci   cally, a multiplication
is performed with the resistor   s conductance as the weight, the

0.020.040.06wldac code  vbl (v)05101520253035ideal transfer curvenominal transfer curvestandard deviation  (from monte carlo simulations) ibc wldac  code   vbl v1 g1 i1 = v1  g1 v2 g2 i2 = v2  g2 i = i1 + i2  = v1  g1 + v2  g2 earlier, lower precision is required for analog computation due
to circuit non-idealities.

in [111], the id127 is integrated into the
adc, where the most signi   cant bits of the multiplications
are performed using switched capacitors in an 8-bit successive
approximation format. this is extended in [112] to not only
perform the multiplications, but also the accumulations in the
analog domain. in this work, it is assumed that 3-bits and
6-bits are suf   cient to represent the weights and activations,
respectively. this reduces the number of adc conversions in
the sensor by 21  . redeye [113] takes this approach even
further by performing the entire convolution layer (including
convolution, max pooling and quantization) in the analog
domain at the sensor. it should be noted that [111] and [112]
report measured results from fabricated test chips, while results
in [113] are from simulations.

it is also feasible to embed the computation not just before
the adc, but into the sensor itself. for instance, in [114] an
angle sensitive pixels sensor is used to compute the gradient
of the input, which along with compression, reduces the data
movement from the sensor by 10  . in addition, since the
   rst layer of the dnn often outputs a gradient-like feature
map, it maybe possible to skip the computations in the    rst
layer, which further reduces energy consumption as discussed
in [115, 116].

vii. co-design of dnn models and hardware
in earlier work, the dnn models were designed to maximize
accuracy without much consideration of the implementation
complexity. however, this can lead to designs that are chal-
lenging to implement and deploy. to address this, recent
work has shown that dnn models and hardware can be co-
designed to jointly maximize accuracy and throughput, while
minimizing energy and cost, which increases the likelihood of
adoption. in this section, we will highlight various efforts that
have been made towards the co-design of dnn models and
hardware. note that unlike section v, the techniques discussed
in this section can affect the accuracy; thus, the goal is to
not only substantially reduce energy consumption and increase
throughput, but also to minimize any degradation in accuracy.
the co-design approaches can be loosely grouped into the

following categories:

    reduce precision of operations and operands. this in-
cludes going from    oating point to    xed point, reducing
the bitwidth, non-linear quantization and weight sharing.
    reduce number of operations and model size. this
includes techniques such as compression, pruning and
compact network architectures.

a. reduce precision

quantization involves mapping data to a smaller set of
quantization levels. the ultimate goal is to minimize the error
between the reconstructed data from the quantization levels and
the original data. the number of quantization levels re   ects the
precision and ultimately the number of bits required to represent
the data (usually log2 of the number of levels); thus, reduced
precision refers to reducing the number of levels, and thus

20

(a) linear quantization

(b) log quantization

(c) non-linear quantization

fig. 37. various methods of quantization (figures from [117, 118]).

the number of bits. the bene   ts of reduced precision include
reduced storage cost and/or reduced computation requirements.
there are several ways to map the data to quantization levels.
the simplest method is a linear mapping with uniform distance
between each quantization level (fig. 37(a)). another approach
is to use a simple mapping function such as a log function
(fig. 37(b)) where the distance between the levels varies; this
mapping can often be implemented with simple logic such as a
shift. alternatively, a more complex mapping function can be
used where the quantization levels are determined or learned
from the data (fig. 37(c)), e.g., using id116 id91; for
this approach, the mapping is usually implemented with a look
up table.

finally, the quantization can be    xed (i.e., the same method
of quantization is used for all data types and layers,    lters, and
channels in the network); or it can be variable (i.e., different
methods of quantization can be used for weights and activations,
and different layers,    lters, and channels in the network).

reduced precision research initially focused on reducing
the precision of the weights rather than the activations, since
weights directly increase the storage capacity requirement,
while the impact of activations on storage capacity depends on
the network architecture and data   ow. however, more recent
works have also started to look at the impact of quantization
on activations. most reduced precision research also focuses
on reducing the precision for id136 rather than training
(with some exceptions [88, 119, 120]) due to the sensitivity of
the gradients to quantization.

the key techniques used in recent work to reduce precision
are summarized in table iii; both linear and non-linear
quantization applied to weights and activations are explored.
the impact on accuracy is reported relative to a baseline
precision of 32-bit    oating point, which is the default precision
used on platforms such as gpus and cpus.

1) linear quantization: the    rst step of reducing precision
is usually to convert values and operations from    oating point
to    xed point. a 32-bit    oating point number, as shown in
fig. 38(a), is represented by (   1)s    m    2(e   127), where s

(a) 32-bit    oating point example

(b) 8-bit dynamic    xed point examples

fig. 38. various methods of number representations.

is the sign bit, e is the 8-bit exponent, and m is the 23-bit
mantissa, and covers the range of 10   38 to 1038.
an n-bit    xed point number is represented by (   1)s    m  
2   f , where s is the sign bit, m is the (n-1)-bit mantissa, and
f determines the location of the decimal point and acts as a
scale factor. for instance, for an 8-bit integer, when f = 0,
the dynamic range is -128 to 127, whereas when f = 10, the
dynamic range is -0.125 to 0.124023438. dynamic    xed point
representation allows f to vary based on the desired dynamic
range as shown in fig. 38(b). this is useful for dnns, since
the dynamic range of the weights and activations can be quite
different. in addition, the dynamic range can also vary across
layers and layer types (e.g., convolutional vs. fully connected).
using dynamic    xed point, the bitwidth can be reduced to 8
bits for the weights and 10 bits for the activations without any
   ne-tuning of the weights [121]; with    ne-tuning, both weights
and activations can reach 8-bits [122].

using 8-bit    xed point has the following impact on energy

and area [79]:

    an 8-bit    xed point add consumes 3.3   less energy
(3.8   less area) than a 32-bit    xed point add, and 30  
less energy (116   less area) than a 32-bit    oating point
add. the energy and area of a    xed-point add scales
approximately linearly with the number of bits.
    an 8-bit    xed point multiply consumes 15.5   less energy
(12.4   less area) than a 32-bit    xed point multiply,
and 18.5   less energy (27.5   less area) than a 32-bit
   oating point multiply. the energy and area of a    xed-
point multiply scales approximately quadratically with the
number of bits.

reducing the precision also reduces the energy and area cost
for storage, which is important since memory access and data
movement dominate energy consumption as described earlier.
the energy and area of the memory scale approximately linearly
with number of bits. it should be noted, however, that changing
from    oating point to    xed point, without reducing bit-width,
does not reduce the energy or area cost of the memory.

for completeness, it should be noted that the precision of
the internal values of a    xed-point multiply and accumulate
(mac) operation are typically higher than the weights and
activations. to guarantee no precision loss, weights and input
activations with n-bit    xed-point precision would require an
n-bit  n-bit multiplication which generates a 2n-bit output

21

fig. 39. reducing the precision of multiply and accumulate (mac).

product; that output would need to be accumulated with 2n+m-
bit precision, where m is determined based on the largest    lter
size log2(c    r    s from fig. 9(b)), which is in the range of
10 to 16 bits for the popular dnns described in section iii-b.
after accumulation, the precision of the    nal output activation
is typically reduced to n-bits [88, 121], as shown in fig. 39.
the reduced output precision does not have a signi   cant impact
on accuracy if the distribution of the weights and activations
are centered near zero such that the accumulation would not
move only in one direction; this is particularly true when batch
id172 is used.

the reduced precision is not only explored in research,
but has been used in recent commercial platforms for dnn
processing. for instance, google   s tensor processing unit
(tpu) which was announced in may 2016, was designed for
8-bit integer arithmetic [123]. similarly, nvidia   s pascal
gpu, which was announced in april 2016, also has 8-bit
integer instructions for deep learning id136 [124]. in general
purpose platforms such as cpus and gpus, the main bene   t
of using 8-bit computation is an increase in throughput, as
four 8-bit operations rather than one 32-bit operation can be
performed for a given clock cycle.

while general purpose platforms usually support 8-bit,
16-bit and/or 32-bit operations, it has been shown that the
minimum bit precision for dnns can actually vary in a more
   ne grained manner. for instance, the weight and activation
precision can vary between 4 and 9 bits for alexnet across
different layers without signi   cant impact on accuracy (i.e., a
change of less than 1%) [125, 126]. this    ne-grained variation
can be exploited for increased throughput or reduced energy
consumption with specialized hardware. for instance, if bit-
serial processing is used, where the number of clock cycles to
complete an operation is proportional to the bitwidth, adapting
to    ne-grain variations in bit precision can result in a 2.24  
speed up versus 16-bits [125]. alternatively, a multiplier can
be designed such that its critical path reduces based on the bit
precision as fewer adders are needed to resolve the product;
this can be combined with voltage scaling for a 2.56   energy
savings versus 16-bits [126]. while these bit scaling results
are reported relative to 16-bit, it would be interesting to see
their impact relative to the maximum precision required across
layers (i.e., 9-bits for [125, 126]).

the precision can be reduced even more aggressively to a
single bit; this area of research is often referred to as binary nets.
binaryconnect (bc) [127] introduced the concept of binary
weights (i.e., -1 and 1), where using a binary weight reduced
the multiplication in the mac to addition and subtraction
only. this was later extended in binarized neural networks
(bnn) [128] that uses binary weights and activations, which

1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 32-bit float exponent (8-bits) mantissa (23-bits) sign e = 70 s = 1 m = 20482 -1.42122425 x 10-13 1010010100000000010100000000010032-bit floatexponent (8-bits)mantissa (23-bits)sign8-bit dynamic fixed01100110signinteger ([7-f ]-bits)mantissa (7-bits)fractional(f-bits)e = 70s = 1m= 20482-1.42122425 x 10-13f = 3s = 012.75m=1028-bit dynamic fixed01100110signmantissa (7-bits)fractional(f-bits)f = 9s = 00.19921875m=102accumulate + weight  (n-bits) activation  (n-bits) n x n multiply 2n-bits 2n+m-bits output (n-bits) quantize to n-bits 22

reduces the mac to an xnor. however, bc and bnn have
an accuracy loss of 19% and 29.8%, respectively [129].

in order to reduce this accuracy loss, binary weight
nets (bwn) and xnor-nets introduced several signi   cant
modi   cations to the dnn processing [129]. this includes
multiplying the outputs with a scale factor to recover the
dynamic range (i.e., the weights effectively become -w and
w, where w is the average of the absolute values of the
weights in the    lter)9, keeping the    rst and last layers at 32-bit
   oating point precision, and performing id172 before
convolution to reduce the dynamic range of the activations.
with these changes, bwn reduced the accuracy loss to 0.8%,
while xnor-nets reduced the loss to 11%. the loss of xnor-
net can be further reduced by increasing the precision of the
activations to be slightly larger than one bit. for instance,
quantized neural networks (qnn) [119], dorefa-net [120],
and hwgq-net [130] allow the activations to have 2-bits,
while the weights remain at 1-bit; in hwgq-net, this reduces
the accuracy loss to 5.2%.

all the previously described binary nets limit the weights
to two values (-w and w); however, there may be bene   ts
for allowing weights to be zero (i.e., -w, 0, w). although
this requires an additional bit per weight compared to binary
weights, the sparsity of the weights can be exploited to reduce
computation and storage cost, which can potentially cancel
out the cost of the additional bit. this is explored in ternary
weight nets (twn) [131] and then extended in trained ternary
quantization (ttq) where a different scale is trained for each
weight (i.e., -w1, 0, w2) for an accuracy loss of 0.6% [132],
assuming 32-bit    oating point for the activations.

hardware implementations for binary/ternary nets have
been explored in recent publications. yodann [133] uses
binary weights, while brein [134] uses binary weights and
activations. binary weights are also used in the compute
in sram work [102] described in section vi. finally, the
nominally spike-inspired truenorth chip can implement a
reduced precision neural network with binary activations and
ternary weights using truenorth   s quantized weight table [9].
these works tend not to support state-of-the-art dnn models
(with the exception of yodann).

2) non-linear quantization: the previous works described
involve linear quantization where the levels are uniformly
spaced out. it has been shown that the distributions of the
weights and activations are not uniform [118, 135], and thus
a non-linear quantization can potentially improve accuracy.
speci   cally, there have been two popular approaches taken
in recent works: (1) log domain quantization; (2) learned
quantization or weight sharing.

log domain quantization if the quantization levels are
assigned based on a logarithmic distribution as shown in
fig 37(b),
the weights and activations are more equally
distributed across the different levels and each level is used
more ef   ciently resulting in less quantization error. for instance,
using 4 bits in linear quantization results in a 27.8% loss in
accuracy versus a 5% loss for log base-2 quantization for

fig. 40. weight sharing hardware.

vgg-16 [117]. furthermore, when weights are quantized to
powers of two, the multiplication can be replaced with a bit-
shift [122, 135].10 incremental network quantization (inq)
can be used to further reduce the loss in accuracy by dividing
the large and small weights into different groups, and then
iteratively quantizing and re-training the weights [136].

weight sharing forces several weights to share a single value.
this reduces the number of unique weights in a    lter or a
layer. one example is to group the weights by using a hashing
function and use one value for each group [137]. alternatively,
the weights can be grouped by the id116 algorithm [118].
both the shared weights and the indexes indicating which
weight to use at each position of the    lter are stored. this
leads to a two step process to fetch the weight: (1) read the
weight index; (2) using the weight index, read the shared
weights. this approach can reduce the cost of reading and
storing the weights if the weight index (log2 of the number of
unique weights) is less than the bitwidth of the weight itself.
for instance, in deep compression [118], the number of
unique weights per layer is reduced to 256 for convolutional
layers and 16 for fully-connected layers in alexnet, requiring
8-bit and 4-bit weight indexes, respectively. assuming there
are u unique weights and the size of the    lters in the layer
is c  r  s  m from fig. 9(b), there will be energy savings
if reading from a crsm    log2u-bit memory plus a u  16-
bit memory (as shown in fig. 40) cost less than reading
from a crsm  16-bit memory. note that unlike the previous
quantization methods, the weight sharing approach does not
reduce the precision of the mac computation itself and only
reduces the weight storage requirement.

b. reduce number of operations and model size

in addition to reducing the size of each operation or operand
(weight/activation), there is also a signi   cant amount of research
on methods to reduce the number of operations and model
size. these techniques can be loosely classi   ed as exploiting
activation statistics, network pruning, network architecture
design and knowledge distillation.

1) exploiting activation statistics: as discussed in sec-
tion iii-a1, relu is a popular form of non-linearity used in
dnns that sets all negative values to zero as shown in fig. 41(a).
as a result, the output activations of the feature maps after the
relu are sparse; for instance, the feature maps in alexnet
have sparsity between 19% to 63% as shown in fig. 41(b).
this sparsity gives relu an implementation advantage over
other non-linearities such as sigmoid, etc.

9this can also be thought of as a form of weights sharing, where only two

weights are used per    lter.

10note however that multiplications do not account for a signi   cant portion

of the total energy.

weight decoder/dequant u x 16b weight  index (log2u-bits) weight  (16-bits) weight  memory crsm x log2u-bits output activation (16-bits)         mac input activation  (16-bits) dynamic fixed point

reduce weight

reduce weight and activation

non-linear quantization

reduce precision method

bitwidth

weights

activations

w/o    ne-tuning [121]
w/    ne-tuning [122]
binaryconnect [127]

binary weight network (bwn) [129]
ternary weight networks (twn) [131]

trained ternary quantization (ttq) [132]

binarized neural networks (bnn) [128]

quantized neural networks (qnn) [119]

xnor-net [129]

dorefa-net [120]

hwgq-net [130]

lognet [135]

deep compression [118]

table iii

8
8
1
1*
2*
2*
1*
1
1*
1
1*

5

5 (conv), 4 (fc)

8 (conv), 4 (fc)
4 (conv), 2 (fc)

32 (   oat)
32 (   oat)
32 (   oat)
32 (   oat)

10
8

1*
1
2*
2*
2*
4

16
16

incremental network quantization (inq) [136]

32 (   oat)

23

accuracy loss vs.
32-bit    oat (%)

0.4
0.6
19.2
0.8
3.7
0.6
11
29.8
7.63
6.5
5.2
3.2
-0.2
0
2.6

methods to reduce numerical precision for alexnet. accuracy measured for top-5 error on id163. *not applied to first and/or

last layers

(a) relu non-linearity

(b) distribution of activation after relu of alexnet

fig. 41. sparsity in activations due to relu.

the sparsity can be exploited for energy and area savings
using compression, particularly for off-chip dram access
which is expensive. for instance, a simple run length coding
that involves signaling non-zero values of 16-bits and then runs
of zeros up to 31 can reduce the external memory bandwidth
of the activations by 2.1   and the overall external bandwidth
(including weights) by 1.5   [61].11 in addition to compression,
the hardware can also be modi   ed such that it skips reading the
weights and performing the mac for zero-valued activations
to reduce energy cost by 45% [94]. rather than just gating the
read and mac computation, the hardware could also skip the
cycle to increase the throughput by 1.37   [138].

the activations can be made to be even more sparse by prun-
ing the low-valued activations. for instance, if all activations
with small values are pruned, this can be translated into an
additional 11% speed up [138] or 2   power reduction [139]
with little impact on accuracy. aggressively pruning more
activations can provide additional throughput improvement at

11this simple run length compression is within 5-10% of the theoretical

id178 limit.

a cost of reduced accuracy.

2) network pruning: to make network training easier, the
networks are usually over-parameterized. therefore, a large
amount of the weights in a network are redundant and can
be removed (i.e., set to zero). this process is called network
pruning. aggressive network pruning often requires some    ne-
tuning of the weights to maintain the original accuracy. this
was    rst proposed in 1989 through a technique called optimal
brain damage [140]. the idea was to compute the impact of
each weight on the training loss (discussed in section ii-c),
referred to as the weight saliency. the low-saliency weights
were removed and the remaining weights were    ne-tuned; this
process was repeated until the desired weight reduction and
accuracy were reached.

in 2015, a similar idea was applied to modern dnns in [141].
rather than using the saliency as a metric, which is too dif   cult
to compute for the large-scaled dnns, the pruning was simply
based on the magnitude of the weights. small weights were
pruned and the model was    ne-tuned to restore the accuracy.
without    ne-tuning the weights, about 50% of the weights
could be pruned. with    ne-tuning, over 80% of the weights
were pruned. overall this approach can reduce the number
of weights in alexnet by 9   and the number of macs
by 3  . most of the weight reduction comes from the fully-
connected layers (9.9   for fully-connected layers versus 2.7  
for convolutional layers).

however, the number of weights alone is not a good metric
for energy. for instance, in alexnet, the number of weights
in the fully-connected layers is much larger than in the
convolutional layers; however, the energy of the convolutional
layers is much higher than the fully-connected layers as shown
in fig. 35 [80]. rather than using the number of weights
and mac operations as proxies for energy, the pruning of
the weights can be directly driven by energy itself [142]. an
energy evaluation method can be used to estimate the dnn
energy that accounts for the data movement from different
levels of the memory hierarchy, the number of macs, and the
data sparsity as shown in fig. 42; this energy estimation tool
is available at [143]. the resulting energy values for popular
dnn models are shown in fig. 43(a). energy-aware pruning

9 -1 -3 1 -5 5 -2 6 -1 relu 9 0 0 1 0 5 0 6 0 0 0.2 0.4 0.6 0.8 1 1 2 3 4 5 conv layer # of activations # of non-zero activations (normalized) 24

fig. 42. energy estimation methodology from [142], which estimates the
energy based on data movement from different levels of the memory hierarchy,
number of macs, and data sparsity.

(a) compressed sparse row (csr)

(a) energy versus accuracy trade-off of popular dnn models.

fig. 44. sparse matrix-vector multiplications using different storage formats
(figure from [144]).

(b) compressed sparse column (csc)

multiplication, as shown in fig. 18(a), one challenge is
to determine how to store the sparse weight matrix in a
compressed format. the compression can be applied either
in row or column order. a compressed sparse row (csr)
format, as shown in fig. 44(a), is often used to perform sparse
matrix-vector multiplication. however, the input vector needs
to be read in multiple times even though only a subset of it is
used since each row of the matrix is sparse. alternatively,
a compressed sparse column (csc) format, as shown in
fig. 44(b), can be used, where the output is updated several
times, and only one element of the input vector is read at
a time [144]. the csc format will provide an overall lower
memory bandwidth than csr if the output is smaller than the
input, or in the case of dnn, if the number of    lters is not
signi   cantly larger than the number of weights in the    lter
(c    r    s from fig. 9(b)). since this is often true, csc can
be an effective format for sparse dnn processing.

custom hardware has been explored to ef   ciently support
pruned dnn models. many works aim to perform the process-
ing without decompressing the weights or activations. eie [145]
performs the sparse matrix-vector multiplication speci   cally for
the fully connected layers. it stores the weights in a csc format
along with the start location of each column, which needs to be
stored since the compressed weights have variable length. when
the input is not zero, the compressed weight column is read and
the output is updated. to handle the sparsity, additional logic
is used to keep track of the location of the output that should
be updated. sid98 [146] supports processing of convolutional

(b) impact of energy-aware pruning.

fig. 43. energy values estimated with methodology in [142].

can then be used to prune weights based on energy to reduce
the overall energy across all layers by 3.7   for alexnet, which
is 1.74   more ef   cient than magnitude-based approaches [141]
as shown in fig. 43(b). as mentioned previously, it is well
known that alexnet is over-parameterized. the energy-aware
pruning can also be applied to googlenet, which is already a
small dnn model, for a 1.6   energy reduction.

recent works have examine how to ef   ciently support
processing of sparse weights in hardware. one area of interest
is how to best store the sparse weights after pruning. similar to
compressing the sparse activations discussed in section vii-b1,
the sparse weights can be compressed to reduce memory access
bandwidth by 20 to 30% [118].

when dnn processing is performed as a matrix-vector

id98 shape configuration (# of channels, # of filters, etc.) id98 weights and input data   [0.3, 0, -0.4, 0.7, 0, 0, 0.1,    ] id98 energy consumption  l1 l2 l3 energy     memory accesses optimization # of macs calculation             # acc. at mem. level 1 # acc. at mem. level 2 # acc. at mem. level n # of macs hardware energy costs of each mac and memory access ecomp edata alexnet	squeezenet	googlenet	resnet-50	vgg-16	77%	79%	81%	83%	85%	87%	89%	91%	93%	5e+08	5e+09	5e+10	top-5	accuracy	normalized	energy	consump9on	original	dnn	alexnet	squeezenet	googlenet	resnet-50	vgg-16	alexnet	squeezenet	alexnet	squeezenet	googlenet	77%	79%	81%	83%	85%	87%	89%	91%	93%	5e+08	5e+09	5e+10	top-5	accuracy	normalized	energy	consump9on	            original	dnn	   magnitude-based	pruning	energy-aware	pruning		1.74x # of  filters # of weights layers in a compressed format. it uses an input stationary
data   ow to deliver the compressed weights and activations to
a multiplier array followed by a scatter network to add the
scattered partial sums.

recent works have also explored the use of structured
pruning to avoid the need for custom hardware [147, 148].
rather than pruning individual weights (also referred to as    ne-
grained pruning), structured pruning involves pruning groups
of weights (also referred to as coarse-grained pruning). the
bene   ts of structured pruning are (1) the resulting weights can
better align with the data-parallel architecture (e.g., simd)
found in existing general purpose hardware, which results in
more ef   cient processing [149]; (2) it amortizes the overhead
cost required to signal the location of the non-zero weights
across a group of weights, which improves compression and
thus reduces storage cost. these groups of weights can include
a pair of neighboring weights, an entire row or column of a
   lter, an entire channel of a    lter or the entire    lter itself; using
larger groups tends to result in higher loss in accuracy [150].
3) compact network architectures: the number of weights
and operations can also be reduced by improving the network
architecture itself. the trend is to replace a large    lter with a
series of smaller    lters, which have fewer weights in total; when
the    lters are applied sequentially, they achieve the same overall
effective receptive    eld (i.e., the region the    lter uses from input
image to compute an output). this approach can be applied
during the network architecture design (before training) or by
decomposing the    lters of a trained network (after training).
the latter one avoids the hassle of training networks from
scratch. however, it is less    exible than the former one. for
example, existing methods can only decompose a    lter in a
trained network into a series of    lters without non-linearity
between them.

a) before training: in recent dnn models,    lters with
a smaller width and height are used more frequently because
concatenating several of them can emulate a larger    lter as
shown in fig. 13. for example, one 5  5 convolution can be
replaced with two 3  3 convolutions. alternatively, one n  n
convolution can be decomposed into two 1-d convolutions, one
1  n and one n  1 convolution [53]; this basically imposes
a restriction that the 2-d    lter must be separable, which is
a common constraint in image processing [151]. similarly, a
3-d convolution can be replaced by a set of 2-d convolutions
(i.e., applied only on one of the input channels) followed by
1  1 3-d convolutions as demonstrated in xception [152] and
mobilenets [153]. the order of the 2-d convolutions and 1  1
3-d convolutions can be switched.
1  1 convolutional layers can also be used to reduce the
number of channels in the output feature map for a given
layer, which reduces the number of    lter channels and thus
computation cost for the    lters in the next layer as demonstrated
in [15, 51, 52]; this is often referred to as a    bottleneck    as
discussed in section iii-b. for this purpose, the number of 1  1
   lters has to be less than the number of channels in the 1  1
   lter. for example, 32    lters of 1  1  64 can transform an input
with 64 channels to an output of 32 channels and reduce the
number of    lter channels in the next layer to 32. squeezenet
uses many 1  1    lters to aggressively reduce the number of

25

weights [154]. it proposes a    re module that    rst    squeezes   
the network with 1  1 convolution    lters and then expands
it with multiple 1  1 and 3  3 convolution    lters. it achieves
an overall 50   reduction in number of weights compared to
alexnet, while maintaining the same accuracy. it should be
noted, however, that reducing the number of weights does not
necessarily reduce energy; for instance, squeezenet consumes
more energy than alexnet, as shown in fig. 43(a).

b) after training: tensor decomposition can be used to
decompose    lters in a trained network without impacting the
accuracy. it treats weights in a layer as a 4-d tensor and breaks
it into a combination of smaller tensors (i.e., several layers).
low-rank approximation can then be applied to further increase
the compression rate at the cost of accuracy degradation, which
can be restored by    ne-tuning the weights.

this approach is demonstrated using canonical polyadic (cp)
decomposition, a high-order extension of singular value decom-
position that can be solved by various methods, such as a greedy
algorithm [155] or a non-linear least-square method [156].
combining cp-decomposition with low-rank approximation
achieves a 4.5   speed-up on cpus [156]. however, cp-
decomposition cannot be computed in a numerically stable
way when the dimension of the tensor, which represents the
weights, is larger than two [156]. to alleviate this problem,
tucker decomposition is adopted instead in [157].

4) knowledge distillation: using a deep network or av-
eraging the predictions of different models (i.e., ensemble)
gives a better accuracy than using a single shallower network.
however, the computational complexity is also higher. to get
the best of both worlds, knowledge distillation transfers the
knowledge learned by the complex model (teacher) to the
simpler model (student). the student network can therefore
achieve an accuracy that would be unachievable if it was
directly trained with the same dataset [158, 159]. for example,
[160] shows how using knowledge distillation can improve the
id103 accuracy of a student net by 2%, which is
similar to the accuracy of a teacher net that is composed of
an ensemble of 10 networks.

the

fig. 45 shows

simplest knowledge distillation
method [158]. the softmax layer is commonly used as the
output layer in the image classi   cation networks to generate
the class probabilities from the class scores12; it squashes the
class scores into values between 0 and 1 that sum up to 1.
for this knowledge distillation method, soft targets (values
between 0 and 1) such as the class scores of the teacher dnn
(or an ensemble of teacher dnns) are used instead of the
hard targets (values of either 0 or 1) such as the labels in the
dataset. the objective is to minimize the squared difference
between the soft targets and the class scores of the student dnn.
class scores are used as the soft targets instead of the class
probabilities because small values in the class scores contain
important information that may be eliminated by the softmax.
alternatively, class probabilities after the softmax layer can be
used as soft targets if the softmax is con   gured to generate
softer class probabilities where the smaller values retain more
information [160]. finally, the intermediate representations of

12also commonly referred to as logits.

26

robotics. for data analytics, high throughput means that more
data can be analyzed in a given amount of time. as the amount
of visual data is growing exponentially, high-throughput big
data analytics becomes important, particularly if an action needs
to be taken based on the analysis (e.g., security or terrorist
prevention; medical diagnosis).

low latency is necessary for real-time interactive applications.
latency measures the time between when the pixel arrives
to a system and when the result is generated. latency is
measured in terms of seconds, while throughput is measured
in operations/second. often high throughput is obtained by
batching multiple images/frames together for processing; this
results in multiple frame latency (e.g., at 30 frames per second,
a batch of 100 frames results in a 3 second delay). this delay
is not acceptable for real-time applications, such as high-speed
navigation where it would reduce the time available for course
correction. thus achieving low latency and high throughput
simultaneously can be a challenge.

hardware cost is in large part dictated by the amount of
on-chip storage and the number of cores. typical embedded
processors have limited on-chip storage on the order of a few
hundred kilobytes. since there is a trade-off between the amount
of on-chip memory and the external memory bandwidth, both
metrics should be reported. similarly, there is a correlation
between the number of cores and the throughput. in addition,
while many cores can be built on a chip, the number of cores
that can actually be used at a given time should be reported. it is
often unrealistic to assume peak utilization and performance due
to limitations of mapping and memory bandwidth. accordingly,
the power and throughput should be reported for running actual
dnns as opposed to only reporting theoretical limits.

a. metrics for dnn models

to evaluate the properties of a given dnn model, we should

consider the following metrics:

    the accuracy of the model in terms of the top-5 error
on datasets such as id163. also, the type of data
augmentation used (e.g., multiple crops, ensemble models)
should be reported.

    the network architecture of the model should be reported,
including number of layers,    lter sizes, number of    lters
and number of channels.

    the number of weights impact the storage requirement of
the model and should be reported. if possible, the number
of non-zero weights should be reported since this re   ects
the theoretical minimum storage requirements.

    the number of macs that needs to be performed should
be reported as it is somewhat indicative of the number
of operations and potential throughput of the given dnn.
if possible, the number of non-zero macs should also
be reported since this re   ects the theoretical minimum
compute requirements.

table iv shows how these metrics are reported for various
well known dnns. the accuracy is reported for the case where
only a single crop for a single model is used for classi   cation,
such that the number of weights and macs in the table are

fig. 45. knowledge distillation matches the class scores of a small dnn to
an ensemble of large dnns.

the teacher dnn can also be incorporated as the extra hints
to train the student dnn [161].

viii. benchmarking metrics for dnn evaluation

and comparison

as we have seen in this article, there has been a signi   cant
amount of research on ef   cient processing of dnns. we should
consider several key metrics to compare the various strengths
and weaknesses of different designs and proposed techniques.
these metrics should cover important attributes such as accu-
racy/robustness, power/energy consumption, throughput/latency
and cost. reporting all these metrics is important in order
to provide a complete picture of the trade-offs made by a
proposed design or technique. we have prepared a website to
collect these metrics from various publications [162].

in terms of accuracy and robustness, it is important that the
accuracy be reported on widely-accepted datasets as discussed
in section iv. the dif   culty of the dataset and/or task should
be considered when measuring the accuracy. for instance, the
mnist dataset for digit recognition is signi   cantly easier than
the id163 dataset. as a result, a dnn that performs well
on mnist may not necessarily perform well on id163.
thus it is important that the same dataset and task is used when
comparing the accuracy of different dnn models; currently
id163 is preferred since it presents a challenge for dnns,
as opposed to mnist, which can also be addressed with simple
non-dnn techniques. to demonstrate primarily hardware
innovations, it would be desirable to report results for widely-
used dnn models (e.g., alexnet, googlenet) whose accuracy
and robustness have been well studied and tested.

energy and power are important when processing dnns at
the edge in embedded devices with limited battery capacity
(e.g., smart phones, smart sensors, uavs, and wearables), or in
the cloud in data centers with stringent power ceilings due to
cooling costs, respectively. edge processing is preferred over
the cloud for certain applications due to latency, privacy or
communication bandwidth limitations. when evaluating the
power and energy consumption, it is important to account
for all aspects of the system including the chip and external
memory accesses.

high throughput is necessary to deliver real-time perfor-
mance for interactive applications such as navigation and

complex dnn b (teacher) simple dnn (student) softmax softmax complex dnn a (teacher) softmax scores class  probabilities try to match metrics

top-5 error

depth in

number of conv layers

(number of conv layers)

filter sizes

number of channels
number of filters

stride

nz weights
nz macs
fc layers
filter sizes

number of channels
number of filters

nz weights
nz macs

total nz weights
total nz macs

alexnet

dense
19.6

sparse
20.4

5
5

5
5

googlenet v1
sparse
dense
12.7
11.7
57
57
21
21

3,5,11
3-256
96-384

1,4

1,6

256-4096
1000-4096

1,3,5,7
3-832
16-384

1,2

1

1024
1000

2.3m
1.5m
395m 56.4m 806m 220m

6.0m

351k

3

3

1

1

870k
58.6m
663k
14.5m
2.4m
61m
410m 58.3m 806m 221m

5.4m
1.9m
5.7m

1m
635k
7m

table iv

metrics for popular dnn models. sparsity is account for by

reporting non-zero (nz) weights and macs.

consistent.13 note that accounting for the number of non-zero
(nz) operations signi   cantly reduces the number of macs
and weights. since the number of nz macs depends on the
input data, we propose using the publicly available 50,000
validation images from id163 for the computation. finally,
there are various methods to reduce the weights in a dnn
(e.g., network pruning in section vii-b2). table iv shows
another example of these dnn model metrics, by comparing
sparse dnns pruned using [142] to dense dnns.

b. metrics for dnn hardware

to measure the ef   ciency of the dnn hardware, we should

consider the following additional metrics:

    the power and energy consumption of the design should
be reported for various dnn models; the dnn model
speci   cations should be provided including which layers
and bit precision are supported by the hardware during
measurement. in addition, the amount of off-chip accesses
(e.g., dram accesses) should be included since it
accounts for a signi   cant portion of the system power; it
can be reported in terms of the total amount of data that
is read and written off-chip per id136.

    the latency and throughput should be reported in terms
of the batch size and the actual run time for various
dnn models, which accounts for mapping and memory
bandwidth effects. this provides a more useful and
informative metric than peak throughput.

    the cost of the chip depends on the area ef   ciency, which
accounts for the size and type of memory (e.g., registers
or sram) and the amount of control logic. it should be

13data augmentation is often used to increase accuracy. this includes using
multiple crops of an image to account for misalignment; in addition, an
ensemble of multiple models can be used where each model has different
weights due to different training settings, such as using different initializations
or datasets, or even different network architectures. if multiple crops and
models are used, then the number of macs and weights required would
increase.

27

reported in terms of the core area in squared millimeters
per multiplier along with process technology.

in terms of cost, different platforms will have different
implementation-speci   c metrics. for instance, for an fpga,
the speci   c device should be reported, along with the utilization
of resources such as dsp, bram, lut and ff; performance
density such as gops/slice can also be reported.

each processor should report various speci   cations for each
metric as shown in table v, using the eyeriss chip as an
example. it is important that all metrics and speci   cations are
accounted for in order fairly evaluate all the design trade-offs.
for instance, without the accuracy given for a speci   c dataset
and task, one could run a simple dnn and easily claim low
power, high throughput, and low cost     however, the processor
might not be usable for a meaningful task; alternatively, without
reporting the off-chip bandwidth, one could build a processor
with only multipliers and easily claim low cost, high throughput,
high accuracy, and low chip power     however, when evaluating
system power, the off-chip memory access would be substantial.
finally, the test setup should also be reported, including whether
the results are measured or obtained from simulation14 and
how many images were tested.

in summary, the evaluation process for whether a dnn
system is a viable solution for a given application might go as
follows: (1) the accuracy determines if it can perform the given
task; (2) the latency and throughput determine if it can run fast
enough and in real-time; (3) the energy and power consumption
will primarily dictate the form factor of the device where the
processing can operate; (4) the cost, which is primarily dictated
by the chip area, determines how much one would pay for this
solution.

ix. summary

the use of deep neural networks (dnns) has seen explosive
growth in the past few years. they are currently widely used
for many arti   cial intelligence (ai) applications including
id161, id103 and robotics and are often
delivering better than human accuracy. however, while dnns
can deliver this outstanding accuracy, it comes at the cost
of high computational complexity. consequently, techniques
that enable ef   cient processing of deep neural network to
improve energy-ef   ciency and throughput without sacri   cing
accuracy with cost-effective hardware are critical to expanding
the deployment of dnns in both existing and new domains.
creating a system for ef   cient dnn processing should
begin with understanding the current and future applications
and the speci   c computations required both now and the
potential evolution of those computations. this article surveys a
number of the current applications, focusing on id161
applications, the associated algorithms, and the data being used
to drive the algorithms. these applications, algorithms and
input data are experiencing rapid change. so extrapolating
these trends to determine the degree of    exibility desired to
handle next generation computations, becomes an important
ingredient of any design project.

14if obtained from simulation, it should be clari   ed whether it is from

synthesis or post place-and-route and what library corner was used.

metrics

cost

test setup

accuracy

latency and throughput

power and energy

test setup

speci   cations
process technology
total core area (mm2)
total on-chip memory (kb)
number of multipliers
core area
per multiplier (mm2)
on-chip memory
per multiplier (kb)
measured or simulated
if simulated, syn or pnr
dnn model
top-5 error on id163
dense/sparse
supported layers
bits per weight
bits per input activation
batch size
run time (msec)
power (mw)
off-chip accesses
per image id136 (mbytes)
number of images tested
table v

28

eyeriss

65nm lp tsmc (1.0v)

12.25
192
168
0.073

1.14

measured

n/a

all conv layers

all conv layers

alexnet

19.8
dense

16
16
4

115.3
278
3.85
100

vgg-16

8.8

dense

16
16
3

4309.4

236

107.03

100

example benchmark metrics for eyeriss [94].

during the design-space exploration process, it is critical to
understand and balance the important system metrics. for dnn
computation these include the accuracy, energy, throughput
and hardware cost. evaluating these metrics is, of course,
key, so this article surveys the important components of
a dnn workload. in speci   c, a dnn workload has two
major components. first, the workload is the form of each
dnn network including the    shape    of each layer and the
interconnections between layers. these can vary both within
and between applications. second, the workload consists of
the speci   c the data input to the dnn. this data will vary
with the input set used for training or the data input during
operation for id136.

this article also surveys a number of avenues that prior
work have taken to optimize dnn processing. since data
movement dominates energy consumption, a primary focus
of some recent research has been to reduce data movement
while maintaining accuracy, throughput and cost. this means
selecting architectures with favorable memory hierarchies like
a spatial array, and developing data   ows that increase data
reuse at the low-cost levels of the memory hierarchy. we
have included a taxonomy of data   ows and an analysis of
their characteristics. other work is presented that aims to save
space and energy by changing the representation of data values
in the dnn. still other work saves energy and sometimes
increases throughput by exploiting the sparsity of weights
and/or activations.

the dnn domain also affords an excellent opportunity
for joint hardware/software co-design. for example, various
efforts have noted that ef   ciency can be improved by increasing
sparsity (increasing the number of zero values) or optimizing
the representation of data by reducing the precision of values
or using more complex mappings of the stored value to the
actual value used for computation. however, to avoid losing
accuracy it is often useful to modify the network or    ne-tune the
network   s weights to accommodate these changes. thus, this

article both reviews a variety of these techniques and discusses
the frameworks that are available for describing, running and
training networks.

finally, dnns afford the opportunity to use mixed-signal
circuit design and advanced technologies to improve ef   ciency.
these include using memristors for analog computation and 3-d
stacked memory. advanced technologies can also can facilitate
moving computation closer to the source by embedding compu-
tation near or within the sensor and the memories. of course, all
of these techniques should also be considered in combination,
while being careful to understand their interactions and looking
for opportunities for joint hardware/algorithm co-optimization.
in conclusion, although much work has been done, deep
neural networks remain an important area of research with
many promising applications and opportunities for innovation
at various levels of hardware design.

acknowledgments

funding provided by darpa yfa, mit cics, and gifts
from nvidia and intel. the authors thank the anonymous
reviewers as well as james noraky, mehul tikekar and
zhengdong zhang for providing valuable feedback on this
paper.

references

[1] y. lecun, y. bengio, and g. hinton,    deep learning,    nature,

vol. 521, no. 7553, pp. 436   444, may 2015.

[2] l. deng, j. li, j.-t. huang, k. yao, d. yu, f. seide, m. seltzer,
g. zweig, x. he, j. williams et al.,    recent advances in deep
learning for speech research at microsoft,    in icassp, 2013.
[3] a. krizhevsky, i. sutskever, and g. e. hinton,    id163
classi   cation with deep convolutional neural networks,    in
nips, 2012.

[4] c. chen, a. seff, a. kornhauser, and j. xiao,    deepdriving:
learning affordance for direct perception in autonomous
driving,    in iccv, 2015.

[5] a. esteva, b. kuprel, r. a. novoa, j. ko, s. m. swetter, h. m.
blau, and s. thrun,    dermatologist-level classi   cation of skin

cancer with deep neural networks,    nature, vol. 542, no. 7639,
pp. 115   118, 2017.

[6] d. silver, a. huang, c. j. maddison, a. guez, l. sifre,
g. van den driessche, j. schrittwieser,
i. antonoglou,
v. panneershelvam, m. lanctot, s. dieleman, d. grewe,
j. nham, n. kalchbrenner, i. sutskever, t. lillicrap, m. leach,
k. kavukcuoglu, t. graepel, and d. hassabis,    mastering the
game of go with deep neural networks and tree search,    nature,
vol. 529, no. 7587, pp. 484   489, jan. 2016.

[7] f.-f. li, a. karpathy, and j. johnson,    stanford cs class
cs231n: convolutional neural networks for visual recogni-
tion,    http://cs231n.stanford.edu/.

[8] p. a. merolla, j. v. arthur, r. alvarez-icaza, a. s. cassidy,
j. sawada, f. akopyan, b. l. jackson, n. imam, c. guo,
y. nakamura et al.,    a million spiking-neuron integrated circuit
with a scalable communication network and interface,    science,
vol. 345, no. 6197, pp. 668   673, 2014.

[9] s. k. esser, p. a. merolla, j. v. arthur, a. s. cassidy,
r. appuswamy, a. andreopoulos, d. j. berg, j. l. mckinstry,
t. melano, d. r. barch et al.,    convolutional networks for
fast, energy-ef   cient neuromorphic computing,    proceedings
of the national academy of sciences, 2016.

[10] m. mathieu, m. henaff, and y. lecun,    fast training of

convolutional networks through ffts,    in iclr, 2014.

[11] y. lecun, l. d. jackel, b. boser, j. s. denker, h. p. graf,
i. guyon, d. henderson, r. e. howard, and w. hubbard,
   handwritten digit recognition: applications of neural network
chips and automatic learning,    ieee commun. mag., vol. 27,
no. 11, pp. 41   46, nov 1989.

[12] b. widrow and m. e. hoff,    adaptive switching circuits,    in

1960 ire wescon convention record, 1960.

[13] b. widrow,    thinking about thinking: the discovery of the

lms algorithm,    ieee signal process. mag., 2005.

[14] o. russakovsky, j. deng, h. su, j. krause, s. satheesh, s. ma,
z. huang, a. karpathy, a. khosla, m. bernstein, a. c. berg,
and l. fei-fei,    id163 large scale visual recognition
challenge,    international journal of id161 (ijcv),
vol. 115, no. 3, pp. 211   252, 2015.

[15] k. he, x. zhang, s. ren, and j. sun,    deep residual learning

for image recognition,    in cvpr, 2016.

[16]    complete visual networking index (vni) forecast,    cisco,

june 2016.

[17] j. woodhouse,    big, big, big data: higher and higher resolution

video surveillance,    technology.ihs.com, january 2016.

[18] r. girshick, j. donahue, t. darrell, and j. malik,    rich
feature hierarchies for accurate id164 and semantic
segmentation,    in cvpr, 2014.

[19] j. long, e. shelhamer, and t. darrell,    fully convolutional

networks for semantic segmentation,    in cvpr, 2015.

[20] k. simonyan and a. zisserman,    two-stream convolutional

networks for action recognition in videos,    in nips, 2014.

[21] g. hinton, l. deng, d. yu, g. e. dahl, a.-r. mohamed, n. jaitly,
a. senior, v. vanhoucke, p. nguyen, t. n. sainath et al.,    deep
neural networks for acoustic modeling in id103:
the shared views of four research groups,    ieee signal process.
mag., vol. 29, no. 6, pp. 82   97, 2012.

[22] r. collobert, j. weston, l. bottou, m. karlen, k. kavukcuoglu,
and p. kuksa,    natural language processing (almost) from
scratch,    journal of machine learning research, vol. 12, no.
aug, pp. 2493   2537, 2011.

[23] a. van den oord, s. dieleman, h. zen, k. simonyan,
o. vinyals, a. graves, n. kalchbrenner, a. senior, and
k. kavukcuoglu,    wavenet: a generative model for raw audio,   
corr abs/1609.03499, 2016.

[24] h. y. xiong, b. alipanahi, l. j. lee, h. bretschneider,
d. merico, r. k. yuen, y. hua, s. gueroussov, h. s. najafabadi,
t. r. hughes et al.,    the human splicing code reveals new
insights into the genetic determinants of disease,    science, vol.
347, no. 6218, p. 1254806, 2015.

29

[25] j. zhou and o. g. troyanskaya,    predicting effects of noncod-
ing variants with deep learning-based sequence model,    nature
methods, vol. 12, no. 10, pp. 931   934, 2015.

[26] b. alipanahi, a. delong, m. t. weirauch, and b. j. frey,
   predicting the sequence speci   cities of dna-and rna-binding
proteins by deep learning,    nature biotechnology, vol. 33, no. 8,
pp. 831   838, 2015.

[27] h. zeng, m. d. edwards, g. liu, and d. k. gifford,    convolu-
tional neural network architectures for predicting dna   protein
binding,    bioinformatics, vol. 32, no. 12, pp. i121   i127, 2016.
[28] m. jermyn, j. desroches, j. mercier, m.-a. tremblay, k. st-
arnaud, m.-c. guiot, k. petrecca, and f. leblond,    neural net-
works improve brain cancer detection with raman spectroscopy
in the presence of operating room light artifacts,    journal of
biomedical optics, vol. 21, no. 9, pp. 094 002   094 002, 2016.
[29] d. wang, a. khosla, r. gargeya, h. irshad, and a. h. beck,
   deep learning for identifying metastatic breast cancer,    arxiv
preprint arxiv:1606.05718, 2016.

[30] l. p. kaelbling, m. l. littman, and a. w. moore,    rein-
forcement learning: a survey,    journal of arti   cial intelligence
research, vol. 4, pp. 237   285, 1996.

[31] v. mnih, k. kavukcuoglu, d. silver, a. graves, i. antonoglou,
d. wierstra, and m. riedmiller,    playing atari with deep
id23,    in nips deep learning workshop,
2013.

[32] s. levine, c. finn, t. darrell, and p. abbeel,    end-to-end
training of deep visuomotor policies,    journal of machine
learning research, vol. 17, no. 39, pp. 1   40, 2016.

[33] m. pfeiffer, m. schaeuble, j. nieto, r. siegwart, and c. cadena,
   from perception to decision: a data-driven approach to end-
to-end motion planning for autonomous ground robots,    in
icra, 2017.

[34] s. gupta, j. davidson, s. levine, r. sukthankar, and j. malik,
   cognitive mapping and planning for visual navigation,    in
cvpr, 2017.

[35] t. zhang, g. kahn, s. levine, and p. abbeel,    learning deep
control policies for autonomous aerial vehicles with mpc-guided
policy search,    in icra, 2016.

[36] s. shalev-shwartz, s. shammah, and a. shashua,    safe, multi-
agent, id23 for autonomous driving,    in nips
workshop on learning, id136 and control of multi-agent
systems, 2016.

[37] n. hemsoth,    the next wave of deep learning applications,   

next platform, september 2016.

[38] s. hochreiter and j. schmidhuber,    long short-term memory,   

neural computation, vol. 9, no. 8, pp. 1735   1780, 1997.

[39] t. n. sainath, a.-r. mohamed, b. kingsbury, and b. ramab-
hadran,    deep convolutional neural networks for lvcsr,    in
icassp, 2013.

[40] v. nair and g. e. hinton,    recti   ed linear units improve

restricted id82s,    in icml, 2010.

[41] a. l. maas, a. y. hannun, and a. y. ng,    recti   er nonlin-
earities improve neural network acoustic models,    in icml,
2013.

[42] k. he, x. zhang, s. ren, and j. sun,    delving deep into
recti   ers: surpassing human-level performance on id163
classi   cation,    in iccv, 2015.

[43] d.-a. clevert, t. unterthiner, and s. hochreiter,    fast and
accurate deep network learning by exponential linear units
(elus),    iclr, 2016.

[44] x. zhang, j. trmal, d. povey, and s. khudanpur,    improving
deep neural network acoustic models using generalized maxout
networks,    in icassp, 2014.

[45] y. zhang, m. pezeshki, p. brakel, s. zhang, , c. laurent,
y. bengio, and a. courville,    towards end-to-end speech
recognition with deep convolutional neural networks,    in
interspeech, 2016.

[46] y. jia, e. shelhamer, j. donahue, s. karayev, j. long, r. gir-
shick, s. guadarrama, and t. darrell,    caffe: convolutional

architecture for fast feature embedding,    in acm international
conference on multimedia, 2014.

[47] s. ioffe and c. szegedy,    batch id172: accelerating
deep network training by reducing internal covariate shift,    in
icml, 2015.

[48] y. lecun, l. bottou, y. bengio, and p. haffner,    gradient-
based learning applied to document recognition,    proc. ieee,
vol. 86, no. 11, pp. 2278   2324, nov 1998.

[49] p. sermanet, d. eigen, x. zhang, m. mathieu, r. fergus, and
y. lecun,    overfeat: integrated recognition, localization and
detection using convolutional networks,    in iclr, 2014.

[50] k. simonyan and a. zisserman,    very deep convolutional
networks for large-scale image recognition,    in iclr, 2015.
[51] c. szegedy, w. liu, y. jia, p. sermanet, s. reed, d. anguelov,
d. erhan, v. vanhoucke, and a. rabinovich,    going deeper
with convolutions,    in cvpr, 2015.

[52] m. lin, q. chen, and s. yan,    network in network,    in iclr,

2014.

[53] c. szegedy, v. vanhoucke, s. ioffe, j. shlens, and z. wojna,
   rethinking the inception architecture for id161,    in
cvpr, 2016.

[54] c. szegedy, s. ioffe, v. vanhoucke, and a. alemi,    inception-
v4, inception-resnet and the impact of residual connections
on learning,    in aaai, 2017.

[55] g. urban, k. j. geras, s. e. kahou, o. aslan, s. wang,
r. caruana, a. mohamed, m. philipose, and m. richardson,
   do deep convolutional nets really need to be deep and
convolutional?    iclr, 2017.

[56]    caffe lenet mnist,    http://caffe.berkeleyvision.org/gathered/

[57]    caffe model zoo,    http://caffe.berkeleyvision.org/model zoo.

[58]    matconvnet pretrained models,   

http://www.vlfeat.org/

examples/mnist.html.

html.

matconvnet/pretrained/.

[59]    tensorflow-slim image classi   cation library,    https://github.

com/tensor   ow/models/tree/master/slim.

[60]    deep learning frameworks,    https://developer.nvidia.com/

deep-learning-frameworks.

[61] y.-h. chen, t. krishna, j. emer, and v. sze,    eyeriss: an
energy-ef   cient recon   gurable accelerator for deep convolu-
tional neural networks,    ieee j. solid-state circuits, vol. 51,
no. 1, 2017.

[62] c. j. b. yann lecun, corinna cortes,    the mnist
database of handwritten digits,    http://yann.lecun.com/exdb/
mnist/.

[63] l. wan, m. zeiler, s. zhang, y. l. cun, and r. fergus,
   id173 of neural networks using dropconnect,    in icml,
2013.

[64] a. krizhevsky, v. nair, and g. hinton,    the cifar-10 dataset,   

https://www.cs.toronto.edu/   kriz/cifar.html.

[65] a. torralba, r. fergus, and w. t. freeman,    80 million tiny
images: a large data set for nonparametric object and scene
recognition,    ieee trans. pattern anal. mach. intell., vol. 30,
no. 11, pp. 1958   1970, 2008.

[66] a. krizhevsky and g. hinton,    convolutional deep belief
networks on cifar-10,    unpublished manuscript, vol. 40, 2010.
   fractional max-pooling,    arxiv preprint

[67] b. graham,

arxiv:1412.6071, 2014.

[68]    pascal voc data sets,    http://host.robots.ox.ac.uk/pascal/

voc/.

//mscoco.org/.

[69]    microsoft common objects in context (coco) dataset,    http:

[70]    google open images,    https://github.com/openimages/dataset.
[71]    youtube-8m,    https://research.google.com/youtube8m/.
[72]    audioset,    https://research.google.com/audioset/index.html.
[73] s. condon,    facebook unveils big basin, new server geared

for deep learning,    zdnet, march 2017.

[74] c. dubout and f. fleuret,    exact acceleration of linear object

detectors,    in eccv, 2012.

30

[75] j. cong and b. xiao,    minimizing computation in convolutional

neural networks,    in icann, 2014.

[76] a. lavin and s. gray,    fast algorithms for convolutional neural

networks,    in cvpr, 2016.

[77]    intel math kernel library,    https://software.intel.com/en-us/

mkl.

[78] s. chetlur, c. woolley, p. vandermersch, j. cohen, j. tran,
b. catanzaro, and e. shelhamer,    cudnn: ef   cient primitives
for deep learning,    arxiv preprint arxiv:1410.0759, 2014.

[79] m. horowitz,    computing   s energy problem (and what we can

do about it),    in isscc, 2014.

[80] y.-h. chen, j. emer, and v. sze,    eyeriss: a spatial archi-
tecture for energy-ef   cient data   ow for convolutional neural
networks,    in isca, 2016.

[81]       ,    using data   ow to optimize energy ef   ciency of deep
neural network accelerators,    ieee micro   s top picks from the
computer architecture conferences, vol. 37, no. 3, may-june
2017.

[82] m. sankaradas, v. jakkula, s. cadambi, s. chakradhar, i. dur-
danovic, e. cosatto, and h. p. graf,    a massively parallel
coprocessor for convolutional neural networks,    in asap,
2009.

[83] v. sriram, d. cox, k. h. tsoi, and w. luk,    towards an
embedded biologically-inspired machine vision processor,    in
fpt, 2010.

[84] s. chakradhar, m. sankaradas, v. jakkula, and s. cadambi,
   a dynamically con   gurable coprocessor for convolutional
neural networks,    in isca, 2010.

[85] v. gokhale, j. jin, a. dundar, b. martini, and e. culurciello,
   a 240 g-ops/s mobile coprocessor for deep neural networks,   
in cvpr workshop, 2014.

[86] s. park, k. bong, d. shin, j. lee, s. choi, and h.-j. yoo,    a
1.93tops/w scalable deep learning/id136 processor with
tetra-parallel mimd architecture for big-data applications,    in
isscc, 2015.

[87] l. cavigelli, d. gschwend, c. mayer, s. willi, b. muheim, and
l. benini,    origami: a convolutional network accelerator,   
in glvlsi, 2015.

[88] s. gupta, a. agrawal, k. gopalakrishnan, and p. narayanan,
   deep learning with limited numerical precision,    in icml,
2015.

[89] z. du, r. fasthuber, t. chen, p. ienne, l. li, t. luo,
x. feng, y. chen, and o. temam,    shidiannao: shifting
vision processing closer to the sensor,    in isca, 2015.

[90] m. peemen, a. a. a. setio, b. mesman, and h. corporaal,
   memory-centric accelerator design for convolutional neural
networks,    in iccd, 2013.

[91] c. zhang, p. li, g. sun, y. guan, b. xiao, and j. cong,    opti-
mizing fpga-based accelerator design for deep convolutional
neural networks,    in fpga, 2015.

[92] t. chen, z. du, n. sun, j. wang, c. wu, y. chen, and
o. temam,    diannao: a small-footprint high-throughput
accelerator for ubiquitous machine-learning,    in asplos,
2014.

[93] y. chen, t. luo, s. liu, s. zhang, l. he, j. wang, l. li,
t. chen, z. xu, n. sun, and o. temam,    dadiannao: a
machine-learning supercomputer,    in micro, 2014.

[94] y.-h. chen, t. krishna, j. emer, and v. sze,    eyeriss: an
energy-ef   cient recon   gurable accelerator for deep convo-
lutional neural networks,    in isscc, 2016.

[95] v. sze, m. budagavi, and g. j. sullivan,    high ef   ciency video
coding (hevc): algorithms and architectures,    in integrated
circuit and systems. springer, 2014, pp. 1   375.

[96] m. alwani, h. chen, m. ferdman, and p. milder,    fused-layer

id98 accelerators,    in micro, 2016.

[97] d. keitel-schulz and n. wehn,    embedded dram develop-
ment: technology, physical design, and application issues,   
ieee des. test. comput., vol. 18, no. 3, pp. 7   15, 2001.

[98] j. jeddeloh and b. keeth,    hybrid memory cube new dram

architecture increases density and performance,    in symp. on
vlsi, 2012.

[99] j. standard,    high bandwidth memory (hbm) dram,   

jesd235, 2013.

[100] d. kim, j. kung, s. chai, s. yalamanchili, and s. mukhopad-
hyay,    neurocube: a programmable digital neuromorphic
architecture with high-density 3d memory,    in isca, 2016.

[101] m. gao, j. pu, x. yang, m. horowitz, and c. kozyrakis,
   tetris: scalable and ef   cient neural network acceleration
with 3d memory,    in asplos, 2017.

[102] j. zhang, z. wang, and n. verma,    a machine-learning
classi   er implemented in a standard 6t sram array,    in symp.
on vlsi, 2016.

[103] z. wang, r. schapire, and n. verma,    error-adaptive classi   er
boosting (eacb): exploiting data-driven training for highly
fault-tolerant hardware,    in icassp, 2014.

[104] a. sha   ee, a. nag, n. muralimanohar, r. balasubramonian,
j. p. strachan, m. hu, r. s. williams, and v. srikumar,    isaac:
a convolutional neural network accelerator with in-situ
analog arithmetic in crossbars,    in isca, 2016.

[105] l. chua,    memristor-the missing circuit element,    ieee trans.

circuit theory, vol. 18, no. 5, pp. 507   519, 1971.

[106] l. wilson,    international technology roadmap for semiconduc-

tors (itrs),    semiconductor industry association, 2013.

[107] lu, darsen,    tutorial on emerging memory devices,    2016.
[108] s. b. eryilmaz, s. joshi, e. neftci, w. wan, g. cauwenberghs,
and h.-s. p. wong,    neuromorphic architectures with electronic
synapses,    in isqed, 2016.

[109] p. chi, s. li, z. qi, p. gu, c. xu, t. zhang, j. zhao, y. liu,
y. wang, and y. xie,    prime: a novel processing-in-memory
architecture for neural network computation in reram-based
main memory,    in isca, 2016.

[110] m. prezioso, f. merrikh-bayat, b. hoskins, g. adam, k. k.
likharev, and d. b. strukov,    training and operation of
an integrated neuromorphic network based on metal-oxide
memristors,    nature, vol. 521, no. 7550, pp. 61   64, 2015.

[111] j. zhang, z. wang, and n. verma,    a matrix-multiplying adc
implementing a machine-learning classi   er directly with data
conversion,    in isscc, 2015.

[112] e. h. lee and s. s. wong,    a 2.5 ghz 7.7 tops/w switched-
capacitor matrix multiplier with co-designed local memory in
40nm,    in isscc, 2016.

[113] r. likamwa, y. hou, j. gao, m. polansky, and l. zhong,
   redeye: analog convnet image sensor architecture for contin-
uous mobile vision,    in isca, 2016.

[114] a. wang, s. sivaramakrishnan, and a. molnar,    a 180nm
cmos image sensor with on-chip optoelectronic image com-
pression,    in cicc, 2012.

[115] h. chen, s. jayasuriya, j. yang, j. stephen, s. sivaramakrish-
nan, a. veeraraghavan, and a. molnar,    asp vision: optically
computing the first layer of convolutional neural networks
using angle sensitive pixels,    in cvpr, 2016.

[116] a. suleiman and v. sze,    energy-ef   cient hog-based object
detection at 1080hd 60 fps with multi-scale support,    in sips,
2014.

[117] e. h. lee, d. miyashita, e. chai, b. murmann, and s. s. wong,
   lognet: energy-ef   cient neural networks using logrithmic
computations,    in icassp, 2017.

[118] s. han, h. mao, and w. j. dally,    deep compression:
compressing deep neural networks with pruning, trained
quantization and huffman coding,    in iclr, 2016.

[119] i. hubara, m. courbariaux, d. soudry, r. el-yaniv, and y. ben-
gio,    quantized neural networks: training neural networks
with low precision weights and activations,    arxiv preprint
arxiv:1609.07061, 2016.

[120] s. zhou, y. wu, z. ni, x. zhou, h. wen, and y. zou,    dorefa-
net: training low bitwidth convolutional neural networks with
low bitwidth gradients,    arxiv preprint arxiv:1606.06160, 2016.
[121] y. ma, n. suda, y. cao, j.-s. seo, and s. vrudhula,    scalable

31

and modularized rtl compilation of convolutional neural
networks onto fpga,    in fpl, 2016.

[122] p. gysel, m. motamedi, and s. ghiasi,    hardware-oriented
approximation of convolutional neural networks,    in iclr,
2016.

[123] s. higginbotham,    google takes unconventional route with
homegrown machine learning chips,    next platform, may
2016.

[124] t. p. morgan,    nvidia pushes deep learning id136 with

new pascal gpus,    next platform, september 2016.

[125] p. judd, j. albericio, t. hetherington, t. m. aamodt, and
a. moshovos,    stripes: bit-serial deep neural network comput-
ing,    in micro, 2016.

[126] b. moons and m. verhelst,    a 0.3   2.6 tops/w precision-
scalable processor for real-time large-scale convnets,    in symp.
on vlsi, 2016.

[127] m. courbariaux, y. bengio, and j.-p. david,    binaryconnect:
training deep neural networks with binary weights during
propagations,    in nips, 2015.

[128] m. courbariaux and y. bengio,    binarynet: training deep
neural networks with weights and activations constrained to+
1 or-1,    arxiv preprint arxiv:1602.02830, 2016.

[129] m. rastegari, v. ordonez, j. redmon, and a. farhadi,    xnor-
net: id163 classi   cation using binary convolutional
neural networks,    in eccv, 2016.

[130] z. cai, x. he, j. sun, and n. vasconcelos,    deep learning with
low precision by half-wave gaussian quantization,    in cvpr,
2017.

[131] f. li and b. liu,    ternary weight networks,    in nips workshop

on ef   cient methods for deep neural networks, 2016.

[132] c. zhu, s. han, h. mao, and w. j. dally,    trained ternary

quantization,    iclr, 2017.

[133] r. andri, l. cavigelli, d. rossi, and l. benini,    yodann: an
ultra-low power convolutional neural network accelerator
based on binary weights,    in isvlsi, 2016.

[134] k. ando, k. ueyoshi, k. orimo, h. yonekawa, s. sato,
h. nakahara, m. ikebe, t. asai, s. takamaeda-yamazaki, and
m. kuroda, t.and motomura,    brein memory: a 13-layer
4.2 k neuron/0.8 m synapse binary/ternary recon   gurable
in-memory deep neural network accelerator in 65nm cmos,   
in symp. on vlsi, 2017.

[135] d. miyashita, e. h. lee, and b. murmann,    convolutional
neural networks using logarithmic data representation,   
arxiv preprint arxiv:1603.01025, 2016.

[136] a. zhou, a. yao, y. guo, l. xu, and y. chen,    incremental
network quantization: towards lossless id98s with low-
precision weights,    in iclr, 2017.

[137] w. chen, j. t. wilson, s. tyree, k. q. weinberger, and y. chen,
   compressing neural networks with the hashing trick,    in
icml, 2015.

[138] j. albericio, p. judd, t. hetherington, t. aamodt, n. e. jerger,
and a. moshovos,    cnvlutin: ineffectual-neuron-free deep
neural network computing,    in isca, 2016.

[139] b. reagen, p. whatmough, r. adolf, s. rama, h. lee, s. k.
lee, j. m. hern  andez-lobato, g.-y. wei, and d. brooks,
   minerva: enabling low-power, highly-accurate deep neural
network accelerators,    in isca, 2016.

[140] y. lecun, j. s. denker, and s. a. solla,    optimal brain

damage,    in nips, 1990.

[141] s. han, j. pool, j. tran, and w. j. dally,    learning both weights
and connections for ef   cient neural networks,    in nips, 2015.
[142] t.-j. yang, y.-h. chen, and v. sze,    designing energy-ef   cient
convolutional neural networks using energy-aware pruning,   
in cvpr, 2017.

[143]    dnn energy estimation,    http://eyeriss.mit.edu/energy.html.
[144] r. dorrance, f. ren, and d. markovi  c,    a scalable sparse
matrix-vector multiplication kernel for energy-ef   cient sparse-
blas on fpgas,    in isfpga, 2014.

[145] s. han, x. liu, h. mao, j. pu, a. pedram, m. a. horowitz,

32

and w. j. dally,    eie: ef   cient id136 engine on compressed
deep neural network,    in isca, 2016.

[146] a. parashar, m. rhu, a. mukkara, a. puglielli, r. venkatesan,
b. khailany, j. emer, s. w. keckler, and w. j. dally,    sid98:
an accelerator for compressed-sparse convolutional neural
networks,    in isca, 2017.

[147] w. wen, c. wu, y. wang, y. chen, and h. li,    learning
structured sparsity in deep neural networks,    in nips, 2016.
[148] s. anwar, k. hwang, and w. sung,    structured pruning of
deep convolutional neural networks,    acm journal of emerging
technologies in computing systems, vol. 13, no. 3, p. 32, 2017.
[149] j. yu, a. lukefahr, d. palframan, g. dasika, r. das, and
s. mahlke,    scalpel: customizing dnn pruning to the underlying
hardware parallelism,    in isca, 2017.

[150] h. mao, s. han, j. pool, w. li, x. liu, y. wang, and w. j. dally,
   exploring the regularity of sparse structure in convolutional
neural networks,    in cvpr workshop on tensor methods in
id161, 2017.

[151] j. s. lim,    two-dimensional signal and image processing,   

englewood cliffs, nj, prentice hall, 1990, 710 p., 1990.

[152] f. chollet,    xception: deep learning with depthwise separa-

ble convolutions,    cvpr, 2017.

[153] a. g. howard, m. zhu, b. chen, d. kalenichenko, w. wang,
t. weyand, m. andreetto, and h. adam,    mobilenets: ef   cient
convolutional neural networks for mobile vision applications,   
arxiv preprint arxiv:1704.04861, 2017.

[154] f. n. iandola, m. w. moskewicz, k. ashraf, s. han, w. j.
dally, and k. keutzer,    squeezenet: alexnet-level accuracy
with 50x fewer parameters and <1mb model size,    iclr,
2017.

[155] e. denton, w. zaremba, j. bruna, y. lecun, and r. fergus,
   exploiting linear structure within convolutional networks
for ef   cient evaluation,    in nips, 2014.

[156] v. lebedev, y. ganin, m. rakhuba1, i. oseledets, and v. lem-
pitsky,    speeding-up convolutional neural networks using
fine-tuned cp-decomposition,    iclr, 2015.

[157] y.-d. kim, e. park, s. yoo, t. choi, l. yang, and d. shin,
   compression of deep convolutional neural networks for fast
and low power mobile applications,    in iclr, 2016.

[158] c. bucilu, r. caruana, and a. niculescu-mizil,    model

compression,    in sigkdd, 2006.

[159] l. ba and r. caurana,    do deep nets really need to be

deep?    nips, 2014.

[160] g. hinton, o. vinyals, and j. dean,    distilling the knowledge
in a neural network,    in nips deep learning workshop, 2014.
[161] a. romero, n. ballas, s. e. kahou, a. chassang, c. gatta, and
y. bengio,    fitnets: hints for thin deep nets,    iclr, 2015.
[162]    benchmarking dnn processors,    http://eyeriss.mit.edu/

benchmarking.html.

