neural models for information retrieval

bhaskar mitra
microsoft, ucl   
cambridge, uk

bmitra@microsoft.com

nick craswell

microsoft

bellevue, usa

nickcr@microsoft.com

7
1
0
2

 

y
a
m
3

 

 
 
]

r

i
.
s
c
[
 
 

1
v
9
0
5
1
0

.

5
0
7
1
:
v
i
x
r
a

abstract

neural ranking models for information retrieval (ir) use shallow or deep neural
networks to rank search results in response to a query. traditional learning to
rank models employ machine learning techniques over hand-crafted ir features.
by contrast, neural models learn representations of language from raw text that
can bridge the gap between query and document vocabulary. unlike classical ir
models, these new machine learning based approaches are data-hungry, requiring
large scale training data before they can be deployed. this tutorial introduces basic
concepts and intuitions behind neural ir models, and places them in the context of
traditional retrieval models. we begin by introducing fundamental concepts of ir
and different neural and non-neural approaches to learning vector representations
of text. we then review shallow neural ir methods that employ pre-trained neural
term embeddings without learning the ir task end-to-end. we introduce deep
neural networks next, discussing popular deep architectures. finally, we review the
current dnn models for information retrieval. we conclude with a discussion on
potential future directions for neural ir.

1

introduction

since the turn of the decade, there have been dramatic improvements in performance in computer
vision, id103, and machine translation tasks, witnessed in research and in real-world
applications [112]. these breakthroughs were largely fuelled by recent advances in neural network
models, usually with multiple hidden layers, known as deep architectures [8, 49, 81, 103, 112].
exciting novel applications, such as conversational agents [185, 203], have also emerged, as well
as game-playing agents with human-level performance [147, 180]. work has now begun in the
information retrieval (ir) community to apply these neural methods, leading to the possibility of
advancing the state of the art or even achieving breakthrough performance as in these other    elds.
retrieval of information can take many forms. users can express their information need in the form of
a text query   by typing on a keyboard, by selecting a query suggestion, or by voice recognition   or
the query can be in the form of an image, or in some cases the need can even be implicit. retrieval can
involve ranking existing pieces of content, such as documents or short-text answers, or composing
new responses incorporating retrieved information. both the information need and the retrieved
results may use the same modality (e.g., retrieving text documents in response to keyword queries),
or different ones (e.g., image search using text queries). retrieval systems may consider user history,
physical location, temporal changes in information, or other context when ranking results. they may
also help users formulate their intent (e.g., via query auto-completion or query suggestion) and/or
extract succinct summaries of results for easier inspection.
neural ir refers to the application of shallow or deep neural networks to these retrieval tasks. this
tutorial serves as an introduction to neural methods for ranking documents in response to a query, an

   the author is a part-time phd student at university college london.

draft. copyright is held by the author(s). may, 2017.

figure 1: the percentage of neural ir papers at the acm sigir conference   as determined by a
manual inspection of the paper titles   shows a clear trend in the growing popularity of the    eld.

important ir task. a search query may typically contain a few terms, while the document length,
depending on the scenario, may range from a few terms to hundreds of sentences or more. neural
models for ir use vector representations of text, and usually contain a large number of parameters
that needs to be tuned. ml models with large set of parameters typically require a large quantity
of training data [196]. unlike traditional learning to rank (l2r) approaches that train ml models
over a set of hand-crafted features, neural models for ir typically accept the raw text of a query and
document as input. learning suitable representations of text also demands large-scale datasets for
training [141]. therefore, unlike classical ir models, these neural approaches tend to be data-hungry,
with performance that improves with more training data.
text representations can be learnt in an unsupervised or supervised fashion. the supervised approach
uses ir data such as labeled query-document pairs, to learn a representation that is optimized end-to-
end for the task at hand. if suf   cient ir labels are not available, the unsupervised approach learns
a representation using just the queries and/or documents. in the latter case, different unsupervised
learning setups may lead to different vector representations, that differ in the notion of similarity
that they capture between represented items. when applying such representations, the choice of
unsupervised learning setup should be carefully considered, to yield a notion of text similarity that
is suitable for the target task. traditional ir models such as latent semantic analysis (lsa) [48]
learn dense vector representations of terms and documents. neural representation learning models
share some commonalities with these traditional approaches. much of our understanding of these
traditional approaches from decades of research can be extended to these modern representation
learning models.
in other    elds, advances in neural networks have been fuelled by speci   c datasets and application
needs. for example, the datasets and successful architectures are quite different in visual object
recognition, id103, and game playing agents. while ir shares some common attributes
with the    eld of natural language processing, it also comes with its own set of unique challenges.
ir systems must deal with short queries that may contain previously unseen vocabulary, to match
against documents that vary in length, to    nd relevant documents that may also contain large sections
of irrelevant text. ir systems should learn patterns in query and document text that indicate relevance,
even if query and document use different vocabulary, and even if the patterns are task-speci   c or
context-speci   c.
the goal of this tutorial is to introduce the fundamentals of neural ir, in context of traditional ir
research, with visual examples to illustrate key concepts and a consistent mathematical notation for
describing key models. section 2 presents a survey of ir tasks, challenges, metrics and non-neural
models. section 3 provides a brief overview of neural ir models and a taxonomy for different neural
approaches to ir. section 4 introduces neural and non-neural methods for learning term embeddings,
without the use of supervision from ir labels, and with a focus on the notion of similarity. section 5
surveys some speci   c approaches for incorporating such embeddings in ir. section 6 introduces the
fundamentals of deep models that are used in ir so far, including popular architectures and toolkits.

2

20142015201620171 % 4 % 8 % 21 %051015202530year% of sigir papersrelated to neural irsection 7 surveys some speci   c approaches for incorporating deep neural networks in ir. section 8
is our discussion, including future work, and conclusion.

motivation for this tutorial neural ir is an emerging    eld. research publication in the area has
been increasing (figure 1), along with relevant workshops [42   44], tutorials [97, 119, 140], and
plenary talks [41, 129]. because this growth in interest is fairly recent, some researchers with ir
expertise may be unfamiliar with neural models, and other researchers who have already worked
with neural models may be unfamiliar with ir. the purpose of this tutorial is to bridge the gap, by
describing the relevant ir concepts and neural methods in the current literature.

2 fundamentals of text retrieval

we focus on text retrieval in ir, where the user enters a text query and the system returns a ranked
list of search results. search results may be passages of text or full text documents. the system   s
goal is to rank the user   s preferred search results at the top. this problem is a central one in the ir
literature, with well understood challenges and solutions. this section provides an overview of those,
such that we can refer to them in subsequent sections.

2.1

ir tasks

text retrieval methods for full text documents and for short text passages have application in ad hoc
retrieval systems and id53 systems respectively.

ad-hoc retrieval ranked document retrieval is a classic problem in information retrieval, as in the
main task of the text retrieval conference [205], and performed by popular search engines such
as google, bing, baidu, or yandex. trec tasks may offer a choice of query length, ranging from
a few words to a few sentences, whereas search engine queries tend to be at the shorter end of the
range. in an operational search engine, the retrieval system uses specialized index structures to search
potentially billions of documents. the results ranking is presented in a search engine results page
(serp), with each result appearing as a summary and a hyperlink. the engine can instrument the
serp, gathering implicit feedback on the quality of search results such as click decisions and dwell
times.
a ranking model can take a variety of input features. some ranking features may depend on the
document alone, such as how popular the document is with users, how many incoming links it has,
or to what extent document seems problematic according to a web spam classi   er. other features
depend on how the query matches the text content of the document. still more features match the
query against document metadata, such as referred text of incoming hyperlink anchors, or the text of
queries from previous users that led to clicks on this document. because anchors and click queries
are a succinct description of the document, they can be a useful source of ranking evidence, but they
are not always available. a newly created document would not have much link or click text. also,
not every document is popular enough to have past links and clicks, but it still may be the best search
result for a user   s rare or tail query. in such cases, when text metadata is unavailable, it is crucial to
estimate the document   s relevance primarily based on its text content.
in the text retrieval community, retrieving documents for short-text queries by considering the long
body text of the document is an important challenge. the ad-hoc and web tracks2 at the popular
text retrieval conference (trec) [204] focus speci   cally on this task. the trec participants are
provided a set of, say    fty, search queries and a document collection containing 500-700k newswire
and other documents. top ranked documents retrieved for each query from the collection by different
competing retrieval systems are assessed by human annotators based on their relevance to the query.
given a query, the goal of the ir model is to rank documents with better assessor ratings higher
than the rest of the documents in the collection. in section 2.4, we describe popular ir metrics
for quantifying model performance given the ranked documents retrieved by the model and the
corresponding assessor judgments for a given query.

question-answering question-answering tasks may range from choosing between multiple choices
(typically entities or binary true-or-false decisions) [78, 80, 165, 212] to ranking spans of text or

2http://www10.wwwconference.org/cdrom/papers/317/node2.html

3

passages [3, 55, 162, 206, 221], and may even include synthesizing textual responses by gathering
evidence from one or more sources [145, 154]. trec question-answering experiments [206] has
participating ir systems retrieve spans of text, rather than documents, in response to questions.
ibm   s deepqa [55] system   behind the watson project that famously demonstrated human-level
performance on the american tv quiz show, "jeopardy!"   also has a primary search phase, whose
goal is to    nd as many potentially answer-bearing passages of text as possible. with respect to the
question-answering task, the scope of this tutorial is limited to ranking answer containing passages in
response to natural language questions or short query texts.
retrieving short spans of text pose different challenges than ranking documents. unlike the long
body text of documents, single sentences or short passages tend to be on point with respect to a
single topic. however, answers often tend to use different vocabulary than the one used to frame
the question. for example, the span of text that contains the answer to the question "what year was
martin luther king jr. born?" may not contain the term "year". however, the phrase "what year"
implies that the correct answer text should contain a year   such as    1929    in this case. therefore, ir
systems that focus on the question-answering task need to model the patterns expected in the answer
passage based on the intent of the question.

2.2 desiderata of ir models

before we describe any speci   c ir model, it is important for us to discuss the attributes that we desire
from a good retrieval system. for any ir system, the relevance of the retrieved items to the input
query is of foremost importance. but relevance measurements can be nuanced by the properties of
robustness, sensitivity and ef   ciency that we expect the system to demonstrate. these attributes not
only guide our model designs but also serve as yard sticks for comparing the different neural and
non-neural approaches.

semantic understanding most traditional approaches for ad-hoc retrieval count repititions of the
query terms in the document text. exact term matching between the query and the document text,
while simple, serves as a foundation for many ir systems. different weighting and id172
schemes over these counts leads to a variety of tf-idf models, such as bm25 [166].
however, by only inspecting the query terms the ir model ignores all the evidence of aboutness
from the rest of the document. so, when ranking for the query    australia   , only the occurrences of
   australia    in the document are considered, although the frequency of other words like    sydeny    or
   kangaroo    may be highly informative. in the case of the query    what channel are the seahawks on
today   , the query term    channel    implies that the ir model should pay attention to occurrences of
   espn    or    sky sports    in the document text   none of which appears in the query itself.
semantic understanding, however, goes beyond mapping query terms to document terms. a good ir
model may consider the terms    hot    and    warm    related, as well as the terms    dog    and    puppy      but
must also distinguish that a user who submits the query    hot dog    is not looking for a "warm puppy"
[118]. at the more ambitious end of the spectrum, semantic understanding would involve logical
reasons by the ir system   so for the query    concerts during sigir    it associates a speci   c edition
of the conference (the upcoming one) and considers both its location and dates when recommending
concerts nearby during the correct week.
these examples motivate that ir models should have some latent representations of intent as expressed
by the query and of the different topics in the document text   so that inexact matching can be
performed that goes beyond lexical term counting.

robustness to rare inputs query frequencies in most ir setups follow a zip   an distribution [216]
(see figure 2). in the publicly available aol query logs [159], for example, more than 70% of the
distinct queries are seen only once in the period of three months from which the queries are sampled.
in the same dataset, more than 50% of the distinct documents are clicked only once. a good ir
method must be able to retrieve these infrequently searched-for documents, and perform reasonably
well on queries containing terms that appear extremely rarely, if ever, in its historical logs.
many ir models that learn latent representations of text from data often naively assume a    xed size
vocabulary. these models perform poorly when the query consists of terms rarely (or never) seen
in the training data. even if the model does not assume a    xed vocabulary, the quality of the latent

4

(a) distribution of query impressions

(b) distribution of document clicks

figure 2: a log-log plot of frequency versus rank for query impressions and document clicks in the
aol query logs [159]. the plots highlight that these quantities follow a zip   an distribution.

representations may depend heavily on how frequently the terms under consideration appear in the
training dataset. exact matching models, like bm25 [166], on the other hand can precisely retrieve
documents containing rare terms.
semantic understanding in an ir model cannot come at the cost of poor retrieval performance on
queries containing rare terms. when dealing with a query such as    pekarovic land company    the ir
model will bene   t from considering exact matches of the rare term    pekarovic   . in practice an ir
model may need to effectively trade-off exact and inexact matching for a query term. however, the
decision of when to perform exact matching can itself be informed by semantic understanding of the
context in which the terms appear in addition to the terms themselves.

robustness to corpus variance an interesting consideration for ir models is how well they
perform on corpuses whose distributions are different from the data that the model was trained on.
models like bm25 [166] have very few parameters and often demonstrate reasonable performance
   out of the box    on new corpuses with little or no additional tuning of parameters. deep learning
models containing millions (or even billions) of parameters, on the other hand, are known to be more
sensitive to distributional differences between training and evaluation data, and has been shown to be
especially vulnerable to adversarial inputs [194].
some of the variances in performance of deep models on new corpuses is offset by better retrieval on
the test corpus that is distributionally closer to the training data, where the model may have picked up
crucial corpus speci   c patterns. for example, it maybe understandable if a model that learns term
representations based on the text of shakespeare   s haid113t is effective at retrieving passages relevant
to a search query from the bard   s other works, but performs poorly when the retrieval task involves a
corpus of song lyrics by jay-z. however, the poor performances on new corpus can also be indicative
that the model is over   tting, or suffering from the clever hans3 effect [187]. for example, an ir
model trained on recent news corpus may learn to associate    theresa may    with the query    uk prime
minister    and as a consequence may perform poorly on older trec datasets where the connection to
   john major    may be more appropriate.
ml models that are hyper-sensitive to corpus distributions may be vulnerable when faced with
unexpected changes in distributions or    black swans   4 in the test data. this can be particularly
problematic when the test distributions naturally evolve over time due to underlying changes in the
user population or behavior. the models, in these cases, may need to be re-trained periodically, or
designed to be invariant to such changes.

robustness to variable length inputs a typical text collection contains documents of varied
lengths (see figure 3). for a given query, a good ir system must be able to deal with documents of
different lengths without over-retrieving either long or short documents. relevant documents may

3https://en.wikipedia.org/wiki/clever_hans
4https://en.wikipedia.org/wiki/black_swan_theory

5

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  012345670123456log10(query id)log10(query frequency)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        012345670123456log10(document id)log10(document frequency)figure 3: distribution of wikipedia featured articles by document length (in bytes) as of june 30, 2014.
source: https://en.wikipedia.org/wiki/wikipedia:featured_articles/by_length.

contain irrelevant sections, and the relevant content may either be localized in a single section of the
document, or spread over different sections. document length id172 is well-studied in the
context of ir models (e.g., pivoted length id172 [181]), and this existing research should
inform the design of any new ir models.

robustness to errors in input no ir system should assume error-free inputs   neither when
considering the user query nor when inspecting the documents in the text collection. while traditional
ir models have typically involved speci   c components for error correction   such as automatic spell
corrections over queries   new ir models may adopt different strategies towards dealing with such
errors by operating at the character-level and/or by learning better representations from noisy texts.

sensitivity to context retrieval in the wild can leverage many implicit and explicit context infor-
mation.5 the query    weather    can refer to the weather in seattle or in london depending on where
the user is located. an ir model may retrieve different results for the query    decorations    depending
on the time of the year. the query    giants match highlights    can be better disambiguated if the ir
system knows whether the user is a fan of baseball or american football, whether she is located
on the east or the west coast of usa, or if the model has knowledge of recent sport    xtures. in
a conversational ir system, the correct response to the question "when did she become the prime
minister?" would depend on disambiguating the correct entity based on the context of references
made in the previous turns of the conversation. relevance, therefore, in many applications is situated
in the user and task context, and is an important consideration in the design of ir systems.

ef   ciency ef   ciency of retrieval is one of the salient points of any retrieval system. a typical
commercial web search engine may deal with tens of thousands of queries per second6   retrieving
results for each query from an index containing billions of documents. search engines typically
involve large multi-tier architectures and the retrieval process generally consists of multiple stages
of pruning the candidate set of documents [131]. the ir model at the bottom of this telescoping
setup may need to sift through billions of documents   while the model at the top may only need to
re-rank between tens of promising documents. the retrieval approaches that are suitable at one level
of the stack may be highly impractical at a different step   models at the bottom need to be fast but
mostly focus on eliminating irrelevant or junk results, while models at the top tend to develop more
sophisticated notions of relevance, and focus on distinguishing between documents that are much
closer on the relevance scale. so far, much of the focus on neural ir approaches have been limited to
re-ranking top-n documents.

5as an extreme example, in the proactive retrieval scenario the retrieval can be triggered based solely on

implicit context without any explicit query submission from the user.

6http://www.internetlivestats.com/one-second/#google-band

6

0   10k10   20k20   30k30   40k40   50k50   60k60   70k70   80k80   90k90   100k100   110k110   120k120   130k130   140k140   150k150   160k160   170k170   180k180   190k190   210k210   220k220   240k240   250k250   260k0200400600800page length in bytesnumber of articlestable 1: notation used in this tutorial.

meaning
single query
single document
set of queries
collection of documents
term in query q
term in document d
full vocabulary of all terms
set of ranked results retrieved for query q
result tuple (document d at rank i)
ground truth relevance label of document d for query q
di is more relevant than dj for query q
frequency of term t in document d
number of documents in d that contains term t
vector representation of text z
id203 function for an event e

notation
q
d
q
d
tq
td
t
rq
(cid:104)i, d(cid:105), where (cid:104)i, d(cid:105)     rq
relq(d)
relq(di) > relq(dj), or succinctly di (cid:31)q dj

tf (t, d)
df (t)
(cid:126)vz
p(e)

while this list of desired attributes of an ir model is in no way complete, it serves as a reference for
comparing many of the neural and non-neural approaches described in the rest of this tutorial.

2.3 notation

we adopt some common notation for this tutorial shown in table 1. we use lower-case to denote
vectors (e.g., (cid:126)x) and upper-case for tensors of higher dimensions (e.g., x). the ground truth relq(d)
in table 1 may be based on either manual relevance annotations or be implicitly derived from user
behaviour on serp (e.g., from clicks).

2.4 metrics

a large number of ir studies [52, 65, 70, 84, 92, 93, 106, 144] have demonstrated that users of
retrieval systems tend to pay attention mostly to top-ranked results. ir metrics, therefore, focus on
rank-based comparisons of the retrieved result set r to an ideal ranking of documents, as determined
by manual judgments or implicit feedback from user behaviour data. these metrics are typically
computed at a rank position, say k, and then averaged over all queries in the test set. unless otherwise
speci   ed, r refers to the top-k results retrieved by the model. next, we describe a few popular
metrics used in ir evaluations.

precision and recall precision and recall both compute the fraction of relevant documents retrieved
for a query q, but with respect to the total number of documents in the retrieved set rq and the
total number of relevant documents in the collection d, respectively. both metrics assume that the
relevance labels are binary.

(cid:80)(cid:104)i,d(cid:105)   rq
(cid:80)(cid:104)i,d(cid:105)   rq
(cid:80)

|rq|

relq(d)

relq(d)

d   d relq(d)

p recisionq =

recallq =

(1)

(2)

mean reciprocal rank (mrr) mean reciprocal rank [40] is also computed over binary relevance
judgments. it is given as the reciprocal rank of the    rst relevant document averaged over all queries.

rrq = max
(cid:104)i,d(cid:105)   rq

relq(d)

i

7

(3)

mean average precision (map) the average precision [235] for a ranked list of documents r is
given by,

(cid:80)(cid:104)i,d(cid:105)   rq
(cid:80)

avepq =

p recisionq,i    relq(d)
d   d relq(d)

(4)

where, p recisionq,i is the precision computed at rank i for the query q. the average precision metric
is generally used when relevance judgments are binary, although variants using graded judgments
have also been proposed [167]. the mean of the average precision over all queries gives the map
score for the whole set.

normalized discounted cumulative gain (n dcg) there are few different variants of the dis-
counted cumulative gain (dcgq) metric [90] which can be used when graded relevance judgments
are available for a query q   say, on a    ve-point scale between zero to four. a popular incarnation of
this metric is as follows.

(cid:88)

(cid:104)i,d(cid:105)   rq

dcgq =

2relq(d)     1
log2(i + 1)

(5)

the ideal dcg (idcgq) is computed the same way but by assuming an ideal rank order for the
documents up to rank k. the normalized dcg (n dcgq) is then given by,

ndcgq =

dcgq
idcgq

(6)

2.5 traditional ir models

in this section, we introduce a few of the traditionally popular ir approaches. the decades of insights
from these ir models not only inform the design of our new neural based approaches, but these
models also serve as important baselines for comparison. they also highlight the various desiderata
that we expect the neural ir models to incorporate.

tf-idf there is a broad family of statistical functions in ir that consider the number of occurrences
of each query term in the document (term-frequency) and the corresponding inverse document
frequency of the same terms in the full collection (as an indicator of the informativeness of the term).
one theoretical basis for such formulations is the probabilistic model of ir that yielded the popular
bm25 [166] ranking function.

bm 25(q, d) =

(cid:88)

tq   q

idf (tq)   

tf (tq, d) + k1   (cid:16)

tf (tq, d)    (k1 + 1)
1     b + b   

(cid:17)

|d|
avgdl

(7)

where, avgdl is the average length of documents in the collection d, and k1 and b are parameters
that are usually tuned on a validation dataset. in practice, k1 is sometimes set to some default value
in the range [1.2, 2.0] and b as 0.75. the idf (t) is popularly computed as,

idf (t) = log

|d|     df (t) + 0.5

df (t) + 0.5

(8)

bm25 aggregates the contributions from individual terms but ignores any phrasal or proximity signals
between the occurrences of the different query terms in the document. a variant of bm25 [229] also
considers documents as composed of several    elds (such as, title, body, and anchor texts).

8

language modelling (lm)
are ranked by the posterior id203 p(d|q).

in the language modelling based approach [79, 161, 230], documents

p(d|q) =

(cid:80)
p(q|d).p(d)
  d   d p(q|   d).p(   d)

    p(q|d).p(d)
= p(q|d)
=

p(tq|d)

tq   q

(cid:89)
(cid:89)
(cid:89)

tq   q

tq   q

(cid:18)
(cid:18)

=

=

, assuming p(d) is uniform

    p(tq|d) + (1       )  p(tq|d)

  

tf (tq, d)

|d|

+ (1       )

(cid:19)
(cid:80)
  d   d |   d|

  d   d tf (tq,   d)

(cid:80)

(9)

(10)
(11)

(12)

(13)

(cid:19)

where,   p(e) is the maximum likelihood estimate (id113) of the id203 of event e. p(q|d) indicates
the id203 of generating query q by randomly sampling terms from document d. for smoothing,
terms are sampled from both the document d and the full collection d   the two events are treated as
mutually exclusive, and their id203 is given by    and (1       ), respectively.

both tf-idf and language modelling based approaches estimate document relevance based on the
count of only the query terms in the document. the position of these occurrences and the relationship
with other terms in the document are ignored.
translation models berger and lafferty [17] proposed an alternative method to estimate p(tq|d)
in the language modelling based ir approach (equation 11), by assuming that the query q is being
generated via a "translation" process from the document d.

(cid:88)

td   d

p(tq|d) =

p(tq|td)    p(td|d)

(14)

the p(tq|td) component allows the model to garner evidence of relevance from non-query terms in
the document. berger and lafferty [17] propose to estimate p(tq|td) from query-document paired
data similar to popular techniques in id151 [22, 23]   but other approaches
for estimation have also been explored [236].

dependence model none of the three ir models described so far consider proximity between
query terms. to address this, metzler and croft [132] proposed a linear model over proximity-based
features.

dm (q, d) = (1       ow       uw)

(cid:32)

(cid:88)

log

tq   q
(1       d)

(cid:32)
(cid:32)

(1       d)

tf (tq, d)

|d|

tf#1(cq, d)

|d|

+   d

log

(1       d)

tf#uwn (cq, d)

|d|

+   d

(cid:33)

+   d

  d   d tf (tq,   d)

(cid:80)
(cid:33)
  d   d |   d|
  d   d tf#1(cq,   d)

(cid:80)

(cid:80)
(cid:80)
(cid:80)

  d   d |   d|
(cid:80)
  d   d tf#uwn (cq,   d)
  d   d |   d|

(cid:33) (15)

(cid:88)
(cid:88)

log

cq   ow(q)

+   ow

+   uw

cq   uw(q)

where, ow(q) and uw(q) are the set of all contiguous id165s (or phrases) and the set of all bags of
terms that can be generated from query q. tf#1 and tf#uwn are the ordered-window and unordered-
window operators from indri [186]. finally,   ow and   uw are the tunable parameters of the model.

9

pseudo relevance feedback (prf) prf-based methods, such as relevance models (rm) [108,
109], typically demonstrate strong performance at the cost of executing an additional round of
retrieval. the set of ranked documents r1 from the    rst round of retrieval is used to select expansion
terms to augment the query for the second round of retrieval. the ranked set r2 from the second
round are presented to the user.
the underlying approach to scoring a document in rm is by computing the kl divergence [105]
between the query language model   q and the document language model   d.

score(q, d) =    (cid:88)

p(t|  q)log

p(t|  q)
p(t|  d)

t   t

(16)

(17)

(18)

without prf,

p(t|  q) =

tf (t, q)

|q|

but under the popular rm3 [2] formulation the new query language model     q is estimated by,

p(t|     q) =   

tf (t, q)

|q| + (1       )

p(t|  d)p(d)

p(  t|  d)

(cid:88)

d   r1

(cid:89)

  t   q

by expanding the query using the results from the    rst round of retrieval prf based approaches tend
to be more robust to the vocabulary mismatch problem plaguing many other traditional ir models.

2.6 learning to rank (l2r)
in learning to rank, a query-document pair is represented by a vector of numerical features (cid:126)x     rn,
and a model f : (cid:126)x     r is trained that maps the feature vector to a real-valued score. the training
dataset for the model consists of a set of queries and a set of documents per query. depending on
the    avour of l2r, in addition to the feature vector, each query-document pair in the training data is
augmented with some relevance information. liu [121] categorized the different l2r approaches
based on their training objectives.

    in the pointwise approach, the relevance information relq(d) is in the form of a numerical
value associated with every query-document pair with feature vector (cid:126)xq,d. the numerical
relevance label can be derived from binary or graded relevance judgments or from implicit
user feedback, such as clickthrough information. a regression model is typically trained on
the data to predict the numerical value relq(d) given (cid:126)xq,d.
    in the pairwise approach, the relevance information is in the form of preferences between
pairs of documents with respect to individual queries (e.g., di (cid:31)q dj). the ranking problem
in this case reduces to binary classi   cation for predicting the more relevant document.
    finally, the listwise approach involves directly optimizing for a rank-based metric   which
is dif   cult because these metrics are often not continuous (and hence not differentiable) with
respect to the model parameters.

the input features for l2r models typically belong to one of three categories.

    query-independent or static features (e.g., id95 or spam score of the document)
    query-dependent or dynamic features (e.g., bm25)
    query-level features (e.g., number of words in query)

many machine learning models   including support vector machines, neural networks, and boosted
id90   have been employed over the years for the learning to rank task, and a correspondingly

10

figure 4: document ranking typically involves a query and a id194 steps, followed
by a matching stage. neural models can be useful either for generating good representations or in
estimating relevance, or both.

large number of different id168s have been explored. next, we brie   y describe ranknet [26]
that has been a popular choice for training neural l2r models and was also   for many years   an
industry favourite, such as at the commercial web search engine bing.7
ranknet ranknet [26] is pairwise id168. for a given query q, a pair of documents (cid:104)di, dj(cid:105),
with different relevance labels, such that di (cid:31)q dj, and feature vectors (cid:104)(cid:126)xi, (cid:126)xj(cid:105), is chosen. the model
f : rn     r, typically a neural network but can also be any other machine learning model whose
output is differentiable with respect to its parameters, computes the scores si = f ((cid:126)xi) and sj = f ((cid:126)xj),
such that ideally si > sj. given the output scores (cid:104)si, sj(cid:105) from the model corresponding to the two
documents, the id203 that di would be ranked higher than dj is given by,

pij     p(di (cid:31)q dj)    

1

1 + e     (si   sj )

(19)

where,    determines the shape of the sigmoid. let sij     {   1, 0, +1} be the true preference label
between di and dj for the training sample    denoting di is more, equal, or less relevant than dj,
respectively. then the desired id203 of ranking di over dj is given by   pij = 1
2 (1 + sij). the
cross-id178 loss l between the desired id203   pij and the predicted id203 pij is given by,

l =      pijlog(pij)     (1       pij)log(1     pij)

(1     sij)  (si     sj) + log(1 + e     (si   sj ))

1
2

=
= log(1 + e     (si   sj ))

if, documents are ordered such that di (cid:31)q dj(sij = 1)

(22)
note that l is differentiable with respect to the model output si and hence the model can be trained
using id119. we direct the interested reader to [27] for more detailed derivations for
computing the gradients for ranknet and for the evolution to the listwise models lambdarank [28]
and lambdamart [214].

(20)

(21)

3 anatomy of a neural ir model

at a high level, document ranking comprises of performing three primary steps   generate a repre-
sentation of the query that speci   es the information need, generate a representation of the document

7https://www.microsoft.com/en-us/research/blog/ranknet-a-ranking-retrospective/

11

query textgenerate query representationdoc textgenerate doc representationestimate relevancequeryvectordocvectorpoint of query representationpoint of matchpoint of doc representation(a) learning to rank using manually designed features
(e.g., liu [121])

(b) estimating relevance from patterns of exact matches
(e.g., [71, 141])

(c) learning query and id194s for
matching (e.g., [88, 143])
figure 5: examples of different neural approaches to ir. in (a) and (b) the neural network is only
used at the point of matching, whereas in (c) the focus is on learning effective representations of
text using neural methods. neural models can also be used to expand or augment the query before
applying traditional ir techniques, as shown in (d).

(d) id183 using neural embeddings (e.g.,
[51, 170])

12

query textdoc textgenerate manually designed featuresdeep neural network for matchingquery textgenerate queryterm vectordoc textgenerate docterm vectorgenerate matching patternsqueryterm vectordocterm vectordeep neural network for matchingquery textgenerate query embeddingdoc textgenerate doc embeddingcosine similarityqueryembeddingdocembeddingquery textid183 using embeddingsdoc textgenerate docterm vectorquery likelihoodqueryterm vectordocterm vector(a) local representation

(b) distributed representation

figure 6: under local representations the terms    banana   ,    mango   , and    dog    are distinct items.
but distributed vector representations may recognize that    banana    and    mango    are both fruits, but
   dog    is different.

that captures the distribution over the information contained, and match the query and the document
representations to estimate their mutual relevance. all existing neural approaches to ir can be broadly
categorized based on whether they in   uence the query representation, the id194,
or in estimating relevance. a neural approach may impact one or more of these stages shown in
figure 4.
neural networks are popular as learning to rank models discussed in section 2.6. in these models, a
joint representation of the query and the document is generated using manually designed features and
the neural network is used only at the point of match to estimate relevance, as shown in figure 5a. in
section 7.4, we will discuss deep neural network models, such as [71, 141], that estimate relevance
based on patterns of exact query term matches in the document. unlike traditional learning to rank
models, however, these architectures (shown in figure 5b) depend less on manual feature engineering
and more on automatically detecting regularities in good matching patterns.
in contrast, many (shallow and deep) neural ir models depend on learning good low-dimensional
vector representations   or embeddings   of query and document text, and using them within tradi-
tional ir models or in conjunction with simple similarity metrics (e.g., cosine similarity). these
models shown in figure 5c may learn the embeddings by optimizing directly for the ir task (e.g.,
[88]), or separately in an unsupervised fashion (e.g., [143]). finally, figure 5d shows ir approaches
where the neural models are used for id183 [51, 170].
while the taxonomy of neural approaches described in this section is rather simple, it does provide
an intuitive framework for comparing the different neural approaches in ir, and highlights the
similarities and distinctions between these different techniques.

4 term representations

4.1 a tale of two representations

vector representations are fundamental to both information retrieval and machine learning. in ir,
terms are typically the smallest unit of representation for indexing and retrieval. therefore, many ir
models   both neural and non-neural   focus on learning good vector representations of terms.
different vector representations exhibit different levels of generalization   some consider every
term as distinct entities while others learn to identify common attributes. different representation
schemes derive different notions of similarity between terms from the de   nition of the corresponding
vector spaces. some representations operate over    xed-size vocabularies, while the design of others
obviate such constraints. they also differ on the properties of compositionality that de   nes how
representations for larger units of information, such as passages and documents, can be derived
from individual term vectors. these are some of the important considerations for choosing a term
representation suitable for a speci   c task.

local representations under local (or one-hot) representations, every term in a    xed size vocab-
ulary t is represented by a binary vector (cid:126)v     {0, 1}|t|, where only one of the values in the vector
is one and all the others are set to zero. each position in the vector (cid:126)v corresponds to a term. the
term    banana   , under this representation, is given by a vector that has the value one in the position
corresponding to    banana    and zero everywhere else. similarly, the terms    mango    and    dog    are
represented by setting different positions in the vector to one.

13

bananamangodogbananamangodogfruitelongateovatebarkshas tail(a) in-document features

(b) neighbouring-word features

(c) neighbouring-word w/ distance features

(d) character-trigraph features

figure 7: examples of different feature-based distributed representations of the term    banana   . the
representations in (a), (b), and (c) are based on external contexts in which the term frequently occurs,
while (d) is based on properties intrinsic to the term. the representation scheme in (a) depends on
the documents containing the term, while the scheme shown in (b) and (c) depends on other terms
that appears in its neighbourhood. the scheme (b) ignores inter-term distances. therefore, in the
sentence    time    ies like an arrow; fruit    ies like a banana   , the feature    fruit    describes both the
terms    banana    and    arrow   . however, in the representation scheme of (c) the feature    fruit   4    is
positive for    banana   , and the feature    fruit+1    for    arrow   .

figure 6a highlights that under this scheme each term is a unique entity, and    banana    is as distinct
from    dog    as it is from    mango   . terms outside of the vocabulary either have no representation, or
are denoted by a special    unk    symbol, under this scheme.

distributed representations under distributed representations every term is represented by a
vector (cid:126)v     r|k|. (cid:126)v can be a sparse or a dense vector   a vector of hand-crafted features or a
learnt representation in which the individual dimensions are not interpretable in isolation. the key
underlying hypothesis for any distributed representation scheme, however, is that by representing a
term by its attributes allows for de   ning some notion of similarity between the different terms based
on the chosen properties. for example, in figure 6b    banana    is more similar to    mango    than    dog   
because they are both fruits, but yet different because of other properties that are not shared between
the two, such as shape.
a key consideration in any feature based distributed representation is the choice of the features them-
selves. a popular approach involves representing terms by features that capture their distributional
properties. this is motivated by the distributional hypothesis [75] that states that terms that are used
(or occur) in similar context tend to be semantically similar. firth [56] famously purported this idea
of id658 by stating    a word is characterized by the company it keeps   . however,
both distribution and semantics by themselves are not well-de   ned and under different context may
mean very different things. figure 7 shows three different sparse vector representations of the term
   banana    corresponding to different distributional feature spaces   documents containing the term
(e.g., lsa [48]), neighbouring words in a window (e.g., hal [125], coals [168], and [24]), and
neighbouring words with distance (e.g., [117]). finally, figure 7d shows a vector representation of
   banana    based on the character trigraphs in the term itself   instead of external contexts in which the
term occurs. in section 4.2 we will discuss how choosing different distributional features for term
representation leads to different nuanced notions of semantic similarity between them.

8readers should take note that while many distributed representations take advantage of distributional
properties, the two concepts are not synonymous. a term can have a distributed representation based on
non-distributional features   e.g., parts of speech classi   cation and character trigraphs in the term.

14

bananadoc 8doc 3doc 12bananalikefliesafruitbananafruit-4a-1flies-3like-2fruit+1banananan#baanana#banfigure 8: a vector space representation of terms puts    banana    closer to    mango    because they share
more common attributes than    banana    and    dog   .

when the vectors are high-dimensional, sparse, and based on distributional feature they are referred
to as explicit vector representations [117]. on the other hand, when the vectors are dense, small
(k (cid:28) |t|), and learnt from data then they are commonly referred to as embeddings. for both explicit
and embedding based representations several distance metrics can be used to de   ne similarity between
terms, although cosine similarity is commonly used.

sim((cid:126)vi, (cid:126)vj) = cos((cid:126)vi, (cid:126)vj) =

(cid:124)
(cid:126)v
i (cid:126)vj
(cid:107)(cid:126)vi(cid:107)(cid:107)(cid:126)vj(cid:107)

(23)

most embeddings are learnt from explicit vector space representations, and hence the discussions in
4.2 about different notions of similarity are also relevant to the embedding models. in section 4.3
and 4.4 we brie   y discuss explicit and embedding based representations.
with respect to compositionality, it is important to understand that distributed representations of
items are often derived from local or distributed representation of its parts. for example, a document
can be represented by the sum of the one-hot vectors or embeddings corresponding to the terms in the
document. the resultant vector, in both cases, corresponds to a distributed bag-of-word representation.
similarly, the character trigraph representation of terms in figure 7d is simply an aggregation over
the one-hot representations of the constituent trigraphs.
in the context of neural models, distributed representations generally refer to learnt embeddings. the
idea of    local    and    distributed    representations has a speci   c signi   cance in the context of neural
network models. each concept, entity, or term can be represented within a neural network by the
activation of a single neuron (local representation) or by the combined pattern of activations of several
neurons (distributed representation) [82].

4.2 notions of similarity

any vector representation inherently de   nes some notion of relatedness between terms. is    seattle   
closer to    sydney    or to    seahawks   ? the answer depends on the type of relationship we are
interested in. if we want terms of similar type to be closer, then    sydney    is more similar to    seattle   
because they are both cities. however, if we are interested to    nd terms that co-occur in the same
document or passage, then    seahawks      seattle   s football team   should be closer. the former
represents a typical, or type-based notion of similarity while the latter exhibits a more topical sense
of relatedness.
if we want to compare    seattle    with    sydeny    and    seahawks based on their respective vector
representations, then the underlying feature space needs to align with the notion of similarity that
we are interested in. it is, therefore, important for the readers to build an intuition about the choice
of features and the notion of similarity they encompass. this can be demonstrated by using a toy
corpus, such as the one in table 2. figure 9a shows that the    in documents    features naturally lend

15

bananamangodogtable 2: a toy corpus of short documents that we consider for the discussion on different notions of
similarity between terms under different distributed representations. the choice of the feature space
that is used for generating the distributed representation determines which terms are closer in the
vector space, as shown in figure 9.

sample documents

doc 01
doc 02
doc 03
doc 04
doc 05
doc 06
doc 07
doc 08

seattle map
seattle weather
seahawks jerseys
seahawks highlights
seattle seahawks wilson
seattle seahawks sherman
seattle seahawks browner
seattle seahawks ifedi

doc 09
doc 10
doc 11
doc 12
doc 13
doc 14
doc 15
doc 16

denver map
denver weather
broncos jerseys
broncos highlights
denver broncos lynch
denver broncos sanchez
denver broncos miller
denver broncos marshall

to a topical sense of similarity between the terms, while the    neighbouring terms with distances   
features in figure 9c gives rise to a more typical notion of relatedness. using    neighbouring terms   
without the inter-term distances as features, however, produces a mixture of topical and typical
relationships. this is because when the term distances are considered in feature de   nition then the
document    seattle seahawks wilson    produces the bag-of-features {seahawks+1, w ilson+2} for
   seattle    which is non-overlapping with the bag-of-features {seattle   1, w ilson+1} for    seahawks   .
however, when the feature de   nition ignores the term-distances then there is a partial overlap between
the bag-of-features {seahawks, w ilson} and {seattle, w ilson} corresponding to    seattle    and
   seahawks   . the overlap increases signi   cantly when we use a larger window-size for identifying
neighbouring terms pushing the notion of similarity closer to a topical de   nition. this effect of the
windows size on the topicality of the representation space was reported by levy and goldberg [115]
in the context of learnt embeddings.
readers should take note that the set of all inter-term relationships goes far beyond the two notions
of typical and topical that we discuss in this section. for example, vector representations could
cluster terms closer based on linguistic styles   e.g., terms that appear in thriller novels versus in
children   s rhymes, or in british versus american english. however, the notions of typical and topical
similarities popularly come up in discussions in the context of many ir and nlp tasks   sometimes
under different names such as paradigmatic and syntagmatic relations9   and the idea itself goes
back at least as far as saussure [30, 47, 74, 172].

4.3 explicit vector representations

explicit vector representations can be broadly categorized based on their choice of distributional
features (e.g., in documents, neighbouring terms with or without distances, etc.) and different
weighting schemes (e.g., tf-idf, positive pointwise mutual information, etc.) applied over the raw
counts. we direct the readers to [12, 199] which are good surveys of many existing explicit vector
representation schemes.
levy et al. [117] demonstrated that explicit vector representations are amenable to the term analogy
task using simple vector operations. a term analogy task involves answering questions of the form
   man is to woman as king is to ____?      the correct answer to which in this case happens to be
   queen   . in nlp, term analogies are typically performed by simple vector operations of the following
form followed by a nearest-neighbour search,

9interestingly, the notion of paradigmatic (typical) and syntagmatic (topical) relationships show up almost
universally   not just in text. in vision, for example, the different images of    noses    bear a typical similarity
to each other, while they share a topical relationship with images of    eyes    or    ears   . curiously, barthes [13]
even extended this analogy to garments   where paradigmatic relationships exist between items of the same type
(e.g., between hats and between boots) and the proper syntagmatic juxtaposition of items from these different
paradigms   from hats to boots    forms a fashionable ensemble .

16

(a)    in-documents    features

(b)    neighbouring terms    features

(c)    neighbouring terms w/ distances    features

figure 9: the    gure shows different distributed representations for the four terms      seattle   ,
   seahawks   ,    denver   , and    broncos      based on the toy corpus in table 2. shaded circles indicate
non-zero values in the vectors   the darker shade highlights the vector dimensions where more
than one vector has a non-zero value. when the representation is based on the documents that the
terms occur in then    seattle    is more similar to    seahawks    than to    denver   . the representation
scheme in (a) is, therefore, more aligned with a topical notion of similarity. in contrast, in (c)
each term is represented by a vector of neighbouring terms   where the distances between the terms
are taken into consideration   which puts    seattle    closer to    denver    demonstrating a typical,
or type-based, similarity. when the inter-term distances are ignored, as in (b), a mix of typical
and topical similarities is observed. finally, it is worth noting that neighbouring-terms based
vector representations leads to similarities between terms that do not necessarily occur in the same
document, and hence the term-term relationships are less sparse than when only in-document features
are considered.

17

seahawksdenverbroncosdoc 02doc 01seattledoc 04doc 03doc 06doc 05doc 08doc 07doc 10doc 09doc 12doc 11doc 14doc 13doc 16doc 15seahawksdenverbroncosdenverseattleseattlebroncosseahawksweathermaphighlightsjerseysshermanwilsonifedibrownersanchezlynchmarshallmillerseahawksdenverbroncosdenver-1seattle-1seattlebroncos+1seahawks+1weather+1map+1highlights+1jerseys+1wilson+2wilson+1sherman+2sherman+1browner+2browner+1ifedi+2ifedi+1lynch+2lynch+1sanchez+2sanchez+1miller+2miller+1marshall+2marshall+1figure 10: a visual demonstration of term analogies via simple vector algebra. the shaded circles
denote non-zero values. darker shade is used to highlight the non-zero values along the vector
dimensions for which the output of (cid:126)vseahawks     (cid:126)vseattle + (cid:126)vdenver is positive. the output vector is
closest to (cid:126)vbroncos as shown in this toy example.

(cid:126)vking     (cid:126)vman + (cid:126)vwoman     (cid:126)vqueen

(24)

it may be surprising to some readers that the vector obtained by the simple algebraic operations
(cid:126)vking     (cid:126)vman + (cid:126)vwoman produces a vector close to the vector (cid:126)vqueen. we present a visual intuition
of why this works in practice in figure 10, but we refer the readers to [7, 117] for a more rigorous
mathematical explanation.

4.4 embeddings

while explicit vector representations based on distributional features can capture interesting notions
of term-term similarity they have one big drawback   the resultant vector spaces are highly sparse
and high-dimensional. the number of dimensions is generally in the same order as the number of
documents or the vocabulary size, which is unwieldy for most practical tasks. an alternative is to
learn lower dimensional representations of terms from the data that retains similar attributes as the
higher dimensional vectors.
an embedding is a representation of items in a new space such that the properties of, and the
relationships between, the items are preserved. goodfellow et al. [64] articulate that the goal of
an embedding is to generate a simpler representation   where simpli   cation may mean a reduction
in the number of dimensions, an increase in the sparseness of the representation, disentangling the
principle components of the vector space, or a combination of these goals. in the context of term
embeddings, the explicit feature vectors   like those we discussed in section 4.3   constitutes the
original representation. an embedding trained from these features assimilate the properties of the
terms and the inter-term relationships observable in the original feature space.
the most popular approaches for learning embeddings include either factorizing the term-feature
matrix (e.g. lsa [48]) or using id119 based methods that try to predict the features given
the term (e.g., [15, 134]). baroni et al. [11] empirically demonstrate that these feature-predicting
models that learn lower dimensional representations, in fact, also perform better than explicit counting
based models on different tasks   possibly due to better generalization across terms   although some
counter evidence the claim of better performances from embedding models have also been reported
in the literature [116].
the sparse feature spaces of section 4.3 are easier to visualize and leads to more intuitive
explanations   while their corresponding embeddings are more practically useful. therefore, it
makes sense to think sparse, but act dense in many scenarios. in the rest of this section, we will
describe some of the popular neural and non-neural embedding models.

latent semantic analysis (lsa) lsa [48] involves performing singular value decomposition
(svd) [63] on a term-document (or term-passage) matrix x to obtain its low-rank approximation

18

seahawksdenverbroncosseattleseahawks    seattle + denverdenverseattlebroncosseahawksweathermaphighlightsjerseysshermanwilsonifedibrownersanchezlynchmarshallmiller[130]. svd on x involves    nding a solution to x = u   v t , where u and v are orthogonal matrices
and    is a diagonal matrix.10

                              

x
((cid:126)dj )
   
. . .

...

x1,|d|

...

x1,1
...

x|t|,1

. . .

x|t|,|d|

(cid:124)
i )   
((cid:126)t

                               = ((cid:126)t

(cid:124)
i )   

               (cid:126)u1

u

                . . .

                        

               (cid:126)ul

               
                           

               

  1
...

0

  

. . .
...
. . .

                  

               

[

[

0

...
  l

(cid:124)
v
((cid:126)dj )
   

(cid:126)v1
...
(cid:126)vl

               

]

]

(25)

where,   1, . . . ,   l, (cid:126)u1, . . . , (cid:126)ul, and (cid:126)v1, . . . ,(cid:126)vl are the singular values, the left singular vectors, and the
right singular vectors, respectively. the k largest singular values, and corresponding singular vectors
from u and v , is the rank k approximation of x (xk = uk  kv t
k ). the embedding for the ith term
is given by   k(cid:126)ti.
while lsa operate on a term-document matrix, id105 based approaches can also be
applied to term-term matrices [25, 111, 168].

neural term embedding models are typically trained by setting up a prediction task. instead of
factorizing the term-feature matrix   as in lsa   neural models are trained to predict the term
from its features. both the term and the features have one-hot representations in the input and
the output layers, respectively, and the model learns dense low-dimensional representations in the
process of minimizing the prediction error. these approaches are based on the information bottleneck
method [197]   discussed in more details in section 6.2   with the low-dimensional representations
acting as the bottleneck. the training data may contain many instances of the same term-feature
pair proportional to their frequency in the corpus (e.g., id97 [134]), or their counts can be
pre-aggregated (e.g., glove [160]).

id97 for id97 [61, 134, 136, 137, 169], the features for a term are made up of its
neighbours within a    xed size window over the text from the training corpus. the skip-gram
architecture (see figure 11a) is a simple one hidden layer neural network. both the input and the
output of the model is in the form of one-hot vectors and the id168 is as follows,

|s|(cid:88)

(cid:88)

lskip   gram =     1
|s|

log(p(ti+j|ti))

i=1

   c   j   +c,j(cid:54)=0

(cid:80)|t|
(cid:124)
exp ((wout(cid:126)vti+j )
k=1 exp ((wout(cid:126)vtk )(cid:124)(win(cid:126)vti))

(win(cid:126)vti))

where,

p(ti+j|ti) =

(26)

(27)

s is the set of all windows over the training text and c is the number of neighbours we need to predict
on either side of the term ti. the denominator for the softmax function for computing p(ti+j|ti) sums
over all the words in the vocabulary. this is prohibitively costly and in practice either hierarchical-
softmax [149] or negative sampling is employed. also, note that the model has two different weight
matrices win and wout that are learnable parameters of the models. win gives us the in embeddings
corresponding to all the input terms and wout corresponding to the out embeddings for the output
terms. generally, only win is used and wout is discarded after training, but we will discuss an ir
application that makes use of both the in and the out embeddings later in section 5.1.
the continuous bag-of-words (cbow) architecture (see figure 11b) is similar to the skip-gram
model, except that the task is to predict the middle term given the sum of the one-hot vectors
of the neighbouring terms in the window. given a middle term ti and the set of its neigbours
10the matrix visualization is adapted from https://en.wikipedia.org/wiki/latent_semantic_

analysis.

19

(a) skip-gram

(b) continuous bag-of-words (cbow)

figure 11: the (a) skip-gram and the (b) continuous bag-of-words (cbow) architectures of id97.
the architecture is a neural network with a single hidden layer whose size is much smaller than that
of the input and the output layers. both models use one-hot representations of terms in the input
and the output. the learnable parameters of the model comprise of the two weight matrices win
and wout that corresponds to the embeddings the model learns for the input and the output terms,
respectively. the skip-gram model trains by minimizing the error in predicting a term given one of
its neighbours. the cbow model, in contrast, predicts a term from a bag of its neighbouring terms.

20

winwouttiti+jwinwoutti+2ti+1ti-2ti-1ti*ti{ti   c, . . . , ti   1, ti+1, . . . , ti+c}, the cbow model creates a single training sample with the sum of
the one-hot vectors of all the neighbouring terms as input and the one-hot vector (cid:126)vti, corresponding
to the middle term, as the expected output.

lcbow =     1
|s|

log(p(ti| (cid:88)

|s|(cid:88)

i=1

   c   j   +c,j(cid:54)=0

ti+j))

(28)

contrast this with the skip-gram model that creates 2    c samples by individually pairing each of the
neighbouring terms with the middle term. during training, given the same number of windows of
text, the skip-gram model, therefore, trains orders of magnitude slower than the cbow model [134]
because it creates 2    c the number of training samples.
id97 gained particular popularity for its ability to perform word analogies using simple vector
algebra, similar to what we have already discussed in section 4.3. for domains where the inter-
pretability of the embeddings may be important, sun et al. [191] introduced an additional constraint
in the id168 to encourage more sparseness in the learnt representation.

lsparse   cbow = lsparse   cbow       

(cid:107)(cid:126)vt(cid:107)1

(29)

(cid:88)

t   t

glove the skip-gram model trains on individual term-neighbour pairs. if we aggregate all the
training samples into a matrix x, such that xij is the frequency of the pair (cid:104)ti, tj(cid:105) in the training data,
then the id168 changes to,

(30)

(31)

(32)

(33)

lskip   gram =    

xijlog(p(ti|tj))

log(p(ti|tj))

xij
xi

  p(ti|tj)log(p(ti|tj))

=

xih(  p(ti|tj)log(p(ti|tj)))

j=1

|t|(cid:88)
|t|(cid:88)
|t|(cid:88)

j=1

xi

xi

j=1

i=1

|t|(cid:88)
|t|(cid:88)
|t|(cid:88)
|t|(cid:88)

i=1

i=1

i=1

=    

=    

|t|(cid:88)

|t|(cid:88)

h(. . . ) is the cross-id178 error between the actual co-occurrence id203   p(ti|tj) and the one
predicted by the model p(ti|tj). this is similar to the id168 for glove [160] if we replace
the cross-id178 error with a squared-error and apply a saturation function f (. . . ) over the actual
co-occurrence frequencies.

lglov e =    

f (xij)(log(xij     (cid:126)v

(cid:124)
wi

(cid:126)vwj ))2

(34)

i=1

j=1

glove is trained using adagrad [53]. similar to id97, glove also generates two different (in
and out) embeddings, but unlike id97 it generally uses the sum of the in and the out vectors
as the embedding for each term in the vocabulary.

paragraph2vec following the popularity of id97 [134, 136], similar neural architectures
[4, 5, 68, 69, 110, 189] have been proposed that trains on term-document co-occurrences. the
training typically involves predicting a term given the id of a document or a passage that contains the
term. in some variants, as shown in figure 12, neighbouring terms are also provided as input.

21

figure 12: the paragraph2vec architecture as proposed by le and mikolov [110] trains by predicting
a term given a document (or passage) id containing the term. by trying to minimize the prediction
error, the model learns an embedding for the term as well as for the document. in some variants of
the architecture, optionally the neighbouring terms are also provided as input   as shown in the dotted
box.

the key motivation for training on term-document pairs is to learn an embedding that is more aligned
with a topical notion of term-term similarity   which is often more appropriate for ir tasks. the
term-document relationship, however, tends to be more sparse [219]   including neighbouring term
features may compensate for some of that sparsity.
in the context of ir tasks, ai et al. [4, 5] proposed a number of ir-motivated changes to the original
paragraph2vec [110] model training   including, document frequency based negative sampling and
document length based id173.

5 term embeddings for ir

traditional ir models use local representations of terms for query-document matching. the most
straight-forward use case for term embeddings in ir is to enable inexact matching in the embedding
space. in section 2.2, we argued the importance of inspecting non-query terms in the document for
garnering evidence of relevance. for example, even from a shallow manual inspection, it is easy to
conclude that the passage in figure 13a is about albuquerque because it contains    metropolitan   ,
   population   , and    area    among other informative terms. on the other hand, the passage in figure 13b
contains    simulator   ,    interpreter   , and    altair    which seems to suggest that the passage is instead
more likely related to computers and technology. in traditional term counting based ir approaches
these signals are often ignored.

22

wd,inwt,outdjtiti+2ti+1ti-2ti-1wt,inalbuquerque is the most populous city in the u.s.
state of new mexico. the high-altitude city serves
as the county seat of bernalillo county, and it is
situated in the central part of the state, straddling the
rio grande. the city population is 557,169 as of
the july 1, 2014 population estimate from the united
states census bureau, and ranks as the 32nd-largest
city in the u.s. the albuquerque metropolitan sta-
tistical area (or msa) has a population of 907,301
according to the united states census bureau   s most
recently available estimate for 2015.

allen suggested that they could program a basic
interpreter for the device; after a call from gates
claiming to have a working interpreter, mits re-
quested a demonstration. since they didn   t actually
have one, allen worked on a simulator for the al-
tair while gates developed the interpreter. although
they developed the interpreter on a simulator and not
the actual device, the interpreter worked    awlessly
when they demonstrated the interpreter to mits in
albuquerque, new mexico in march 1975; mits
agreed to distribute it, marketing it as altair basic.

(a) about albuquerque

(b) not about albuquerque

figure 13: two passages both containing exactly a single occurrence of the query term    albuquerque   .
however, the passage in (a) contains other terms such as    population    and    area    that are relevant to
a description of the city. in contrast, the terms in passage (b) suggest that it is unlikely to be about the
city, and only mentions the city potentially in a different context.

most existing shallow neural methods for ir focus on inexact matching using term embeddings.
these approaches can be broadly categorized as those that compare the query with the document
directly in the embedding space; and those that use embeddings to generate suitable id183
candidates from a global vocabulary and then perform retrieval based on the expanded query. we
discuss both these classes of approaches in the remainder of this section.

5.1 query-document matching

a popular strategy for using term embeddings in ir involves deriving a dense vector representation for
the query and the document from the embeddings of the individual terms in the corresponding texts.
the term embeddings can be aggregated in different ways, although using the average word (or term)
embeddings (awe) is quite popular [96, 101, 110, 143, 151, 190, 207]. non-linear combinations of
term vectors   such as using fisher kernel framework [35]   have also been explored, as well as
other families of aggregate functions of which awe has been shown to be a special case [228].
the query and the document embeddings themselves can be compared using a variety of similarity
metrics, such as cosine similarity or dot-product. for example,

sim(q, d) = cos((cid:126)vq, (cid:126)vd) =

where, (cid:126)vq =

(cid:124)
q (cid:126)vd
(cid:126)v
(cid:107)(cid:126)vq(cid:107)(cid:107)(cid:126)vd(cid:107)
1
|q|

(cid:88)
(cid:88)

tq   q

td   d

(cid:126)vtq(cid:107)(cid:126)vtq(cid:107)
(cid:126)vtd(cid:107)(cid:126)vtd(cid:107)

(cid:126)vd =

1
|d|

(35)

(36)

(37)

an important consideration here is the choice of the term embeddings that is appropriate for the
retrieval scenario. while, lsa [48], id97 [136], and glove [160] are popularly used   it is
important to understand how the notion of inter-term similarity modelled by a speci   c vector space
may in   uence its performance on a retrieval task. in the example in figure 13, we want to rank
documents that contains related terms, such as    population    or    area    higher   these terms are
topically similar to the query term    albuquerque   . intuitively, a document about    tucson      which is
typically similar to    albuquerque      is unlikely to satisfy the user intent. the discussion in section
4.2 on how input features in   uence the notion of similarity in the learnt vector space is relevant here.
models, such as lsa [48] and paragraph2vec [110], that consider term-document pairs generally
capture topical similarities in the learnt vector space. on the other hand, id97 [136] and glove
[160] embeddings may incorporate a mixture of topical and typical notions of relatedness. these

23

neural models behave more typical when trained with short window sizes or on short text, such as on
keyword queries [115] (refer to section 4.2 for more details).
in section 4.4, we made a note that the id97 model learns two different embeddings   in
and out   corresponding to the input and the output terms. mitra et al. [143] point out that when
using id97 embeddings for ir it is more appropriate to represent the query terms using the
in embeddings and the document terms using the out embeddings of the trained model. in this
dual embedding space model (desm)11 [143, 151] the id97 embeddings are trained on search
queries, which empirically performs better than training on document body text. training on short
queries, however, makes the inter-term similarity more pronouncedly typical (where,    yale    is closer
to    harvard    and    nyu   ) when both terms are represented using their in vectors   better retrieval
performance is achieved instead by using the in-out similarity (where,    yale    is closer to    faculty   
and    alumni   ) that mirrors more the topical notions of relatedness.

desmin   out(q, d) =

(cid:126)vd,out =

(cid:88)
(cid:88)

tq   q

td   d

1
|q|

1
|d|

(cid:124)
(cid:126)v
tq,in(cid:126)vd,out

(cid:107)(cid:126)vtq,in(cid:107)(cid:107)(cid:126)vd,out(cid:107)
(cid:126)vtd,out
(cid:107)(cid:126)vtd,out(cid:107)

(38)

(39)

an alternative to representing queries and documents as an aggregate of their term embeddings is
to incorporate the term representations into existing ir models, such as the ones we discussed in
section 2.5. zuccon et al. [236] proposed the neural translation language model (ntlm) that uses
the similarity between term embeddings as a measure for term-term translation id203 p(tq|td)
in equation 15.

p(tq|td) =

(cid:80)

cos((cid:126)vtq , (cid:126)vtd )
t   t cos((cid:126)vt, (cid:126)vtd )

(40)

on similar lines, ganguly et al. [58] proposed the generalized language model (glm) which
extends the language model based approach in equation 13 to,

p(d|q) =

tf (tq, d)

  

(cid:18)

(cid:89)

tq   q

(cid:80)
(cid:80)
(cid:80)
td   d (sim((cid:126)vtq , (cid:126)vtd )    tf (td, d))
(sim((cid:126)vtq , (cid:126)v  t)   (cid:80)
)    |d|2
td1   d
(cid:80)
  d   d |   d|    |nt| + (1                     )

)   (cid:80)

td2   d sim((cid:126)vtd1

  d   d tf (  t,   d))

sim((cid:126)vtd1

, (cid:126)vtd2

, (cid:126)vtd2

  t   nt

td2   nt

+   

(cid:80)

(cid:80)

|d|

(cid:80)

+   

td1   nt

(cid:19)

  d   d tf (tq,   d)

(cid:80)
  d   d |   d|

(41)

where, nt is the set of nearest-neighbours of term t. ai et al. [4] incorporate paragraph vectors [110]
into the query-likelihood model [161].
another approach, based on the earth mover   s distance (emd) [171], involves estimating similarity
between pairs of documents by computing the minimum distance in the embedding space that each
term in the    rst document needs to travel to reach the terms in the second document. this measure,
commonly referred to as the word mover   s distance (wmd), was originally proposed by wan et al.
[210, 211], but used id138 and topic categories instead of distributed representations for de   ning
distance between terms. term embeddings were later incorporated into the model by kusner et al.
[87, 104]. finally, guo et al. [72] incorporated similar notion of distance into the non-linear word
transportation (nwt) model that estimates relevance between a a query and a document. the nwt
model involves solving the following constrained optimization problem,

11the dual term embeddings trained on bing queries is available for download at https://www.microsoft.

com/en-us/download/details.aspx?id=52597

24

(42)

(43)

(44)

(45)

(46)

(47)

max (cid:88)
and (cid:88)

tq   q

tq   q

(cid:18) (cid:88)

td   u(d)

log

f (tq, td) =

subject to f (tq, td)     0,

f (tq, td)    max(cid:0)cos((cid:126)vtq , (cid:126)vtd ), 0(cid:1)idf (tq)+b(cid:19)

   tq     q, td     d
(cid:80)
(cid:80)
tf (td) +   
|d| +   

  d   d tf (tq,   d)

  d   d |   d|

   td     d

,

where,

idf (t) =

|d|     df (t) + 0.5

df (t) + 0.5

u(d) is the set of all unique terms in document d, and b is a constant.
another term-alignment based distance metric was proposed by kenter and de rijke [98] for comput-
ing short-text similarity. the design of the saliency-weighted semantic network (swsn) is motivated
by the bm25 [166] formulation.

swsn(sl, ss) =

(cid:88)

tl   sl

idf (tl)   

sem(tl, ss) + k1   (cid:16)

sem(tl, ss)    (k1 + 1)
1     b + b   

(cid:17)

|ss|
avgsl

where,

sem(t, s) = max
  t   s

cos((cid:126)vt, (cid:126)v  t)

here ss is the shorter of the two sentences to be compared, and sl the longer sentence.

telescoping evaluation figure 14 highlights the distinct strengths and weaknesses of matching
using local and distributed representations of terms for retrieval. for the query    cambridge   , a
local representation (or exact matching) based model can easily distinguish between the passage on
cambridge (figure 14a) and the one on oxford (figure 14b). however, the model is easily duped by
an non-relevant passage that has been arti   cially injected with the term    cambridge    (figure 14d).
the distributed representation based matching, on the other hand, can spot that the other terms in the
passage provide clear indication that the passage is not about a city, but fails to realize that the the
passage about oxford (figure 14b) is inappropriate for the same query.
embedding based models often perform poorly when the retrieval is performed over the full document
collection [143]. however, as seen in the example of figure 14, the errors made by embedding
based models and exact matching models are typically different   and the combination of the two
performs better than exact matching models alone [4, 58, 143]. another popular technique is to
use the embedding based model to re-rank only a subset of the documents retrieved by a different   
generally an exact matching based   ir model. the chaining of different ir models where each
successive model re-ranks a smaller number of candidate documents is called telescoping [131].
telescoping evaluations are popular in the neural ir literature [71, 88, 141, 143, 177] and the results
are representative of performances of these models on re-ranking tasks. however, as mitra et al. [143]
demonstrate, good performances on re-ranking tasks may not be indicative how the model would
perform if the retrieval involves larger document collections.

5.2 id183

instead of comparing the query and the document directly in the embedding space, an alternative
approach is to use term embeddings to    nd good expansion candidates from a global vocabulary,
and then retrieving documents using the expanded query. different functions [51, 170, 227] have
been proposed for estimating the relevance of candidate terms to the query   all of them involves
comparing the candidate term individually to every query term using their vector representations, and
then aggregating the scores. for example, [51, 170] estimate the relevance of candidate term tc as,

score(tc, q) =

1
|q|

cos((cid:126)vtc , (cid:126)vtq )

(48)

(cid:88)

tq   q

25

england . it lies in east anglia , on the river cam , about 50 miles ( 80 km ) north of london . according to the united kingdom

the city of cambridge is a university city and the county town of cambridgeshire ,
this makescambridge the second largest
evidence of settlement in the area during the bronze age and roman times ; under viking rulecambridge
became an important trading centre .

census 2011 ,
city in cambridgeshire after peterborough , and the 54th largest in the united kingdom .

the    rst town charters were granted in the 12th century , although city status was not

its population was 123867 ( including 24488 students ) .

there is archaeological

conferred until 1951 .

(a) passage about the city of cambridge

oxford is a city in the south east region of england and the county town of oxfordshire . with a population of
159994 it is the 52nd largest city in the united kingdom , and one of the fastest growing and most ethnically diverse . oxford has
its industries include motor manufacturing , education , publishing and a large number of
a broad economic base .
information technology and sciencebased businesses , some being academic offshoots .
the city is known worldwide as the home
of the university of oxford , the oldest university in the englishspeaking world . buildings in oxford demonstrate examples of every
english architectural period since the arrival of the saxons , including the m thcentury radcliffe camera . oxford is known as the city
of dreaming spires , a term coined by poet matthew arnold .

(b) passage about the city of oxford

the giraffe ( giraffa camelopardalis ) is an african eventoed ungulate mammal , the tallest living terrestrial animal and the largest ruminant .
its
ossicones , and its
species name refers to its camellike shape and its leopardlike colouring .
it is classi   ed under the family giraf   dae , along with its closest extant relative , the okapi . the nine subspecies are
distinctive coat patterns .
distinguished by their coat patterns . the scattered range of giraffes extends from chad in the north to south africa in the south , and
from niger in the west to somalia in the east . giraffes usually inhabit savannas , grasslands , and open woodlands .

its chief distinguishing characteristics are its extremely long neck and legs , its hornlike

(c) passage about giraffes

thecambridge ( giraffa camelopardalis ) is an african eventoed ungulate mammal , the tallest living terrestrial animal

ossicones , and its distinctive coat patterns .

its species name refers to its camellike shape and its leopardlike colouring .

and the largest ruminant .
its chief distinguishing characteristics are its extremely long
it is classi   ed under the family giraf   dae , along with its closest extant relative , the
neck and legs , its hornlike
the scattered range of giraffes extends from chad in the north to
okapi .
south africa in the south , and from niger in the west to somalia in the east . giraffes usually inhabit savannas , grasslands , and open
woodlands .

the nine subspecies are distinguished by their coat patterns .

(d) passage about giraffes, but    giraffe    is replaced by    cambridge   

figure 14: a visualization of in-out similarities between terms in different passages with the
query term    cambridge   . the visualization   adapted from https://github.com/bmitra-msft/
demos/blob/master/notebooks/desm.ipynb   reveal that, besides the term    cambridge   , many
other terms in the passages about both cambridge and oxford have high similarity to the query
term. the passage (d) is adapted from the passage (c) on giraffes by replacing all the occurrences
of the term    giraffe    with    cambridge   . however, none of the other terms in (d) are found to be
relevant to the query term. an embedding based approach may be able to determine that passage (d)
is non-relevant to the query    cambridge   , but fail to realize that passage (b) is also non-relevant. a
term counting-based model, on the other hand, can easily identify that passage (b) is non-relevant,
but may rank passage (d) incorrectly high.

26

(a) global embedding

(b) local embedding

figure 15: a two-dimensional visualization of term embeddings when the vector space is trained on
a (a) global corpus and a (b) query-speci   c corpus, respectively. the grey circles represent individual
terms in the vocabulary. the white circle represents the query       ocean remote sensing    by averaging
the embeddings of the individual terms in the query, and the light grey circles correspond to good
expansion terms for this query. when the representations are query-speci   c then the meaning of the
terms are better disambiguated, and more likely to result in the selection of good expansion terms.

term embedding based id183 on its own performs worse than pseudo-relevance feed-
back [170]. but like the models in the previous section, shows better performances when used in
combination with prf [227].
diaz et al. [51] explored the idea of query-speci   c term embeddings and found that they are much
more effective in identifying good expansion terms than a global representation (see figure 15). the
local model proposed by diaz et al. [51] incorporate relevance feedback in the process of learning the
term embeddings   a set of documents is retrieved for the query and a query-speci   c term embedding
model is trained. this local embedding model is then employed for identifying expansion candidates
for the query for a second round of document retrieval.

term embeddings have also been explored for re-weighting query terms [233] and    nding relevant
query re-writes [69], as well as in the context of other ir tasks such as cross-lingual retrieval [207]
and entity retrieval [200]. in the next section, we move on to neural network models with deeper
architectures and their applications to retrieval.

6 deep neural networks

deep neural network models consist of chains of tensor operations. the tensor operation can range
from parameterized linear transformations (e.g., multiplication with a weight matrix, addition of a
bias vector) to elementwise application of non-linear functions, such as tanh or recti   ed linear units
(relu) [73, 89, 150]. figure 16 shows a simple feed-forward neural network with fully-connected
layers. for an input vector (cid:126)x, the model produces the output (cid:126)y as follows,

(cid:126)y = tanh(w2    tanh(w1    (cid:126)x + (cid:126)b1) + (cid:126)b2)

(49)

the model training involves tuning the parameters w1, (cid:126)b1, w2, and (cid:126)b2 to minimize the loss between
the expected output and the actual output of the    nal layer. the parameters are usually trained
discriminatively using id26 [14, 77, 175]. during forward-pass each layer generates an
output conditioned on its input, and during backward pass each layer computes the error gradient
with respect to its parameters and its inputs.
the design of a dnn typically involves many choices of architectures and hyper-parameters. neural
networks with as few a single hidden layer   but with suf   cient number of hidden nodes   can

27

(a) a neural network with a single hidden layer.

(b) the same neural network viewed as a chain of computational steps.

figure 16: two different visualizations of a feed-forward neural network with a single hidden layer.
in (a), the addition of the bias vector and the non-linearity function is implicit. figure (b) shows
the same network but as a sequence of computational nodes. most popular neural network toolkits
implement a set of standard computational nodes that can be connected to build more sophisticated
neural architectures.

theoretically approximate any function [85]. in practice, however, deeper architectures   sometimes
with as many as 1000 layers [76]   have been shown to perform signi   cantly better than shallower
networks. for readers who are less familiar with neural network models, we present a simple example
in figure 17 to illustrate how hidden layers enable these models to capture non-linear relationships.
we direct readers to [148] for further discussions on how additional hidden layers help.
the rest of this section is dedicated to the discussion of input representations and popular architectures
for deep neural models.

6.1

input text representations

neural models that learn representations of text take raw text as input. a key consideration is how
the text should be represented at the input layer of the model. figure 18 shows some of the popular
input representations of text.
some neural models [66, 94, 100, 192] operate at the character-level. in these models, each character
is typically represented by a one-hot vector. the vector dimensions   referred to as channels   in
this case equals the number of allowed characters in the vocabulary. these models incorporate the
least amount of prior knowledge about the language in the input representation   for example, these
models are often required to learn about id121 from scratch by treating space as just another
character in the vocabulary. the representation of longer texts, such as sentences, can be derived by
concatenating or summing the character-level vectors as shown in figure 18a.
the input text can also be pre-tokenized into terms   where each term is represented by either a
sparse vector or using pre-trained term embeddings (figure 18d). terms may have a one-hot (or local)
representation where each term has an unique id (figure 18b), or the term vector can be derived by
aggregating one-hot vectors of its constituting characters (or character n-graphs) as shown in figure
18c. if pre-trained embeddings are used for term representation, then the embedding vectors can be
further tuned during training, or kept    xed.

28

forward passbackward  passw1w2inputactual outputlossexpected outputnon-linearity (tanh)inputlinear transform(w1, b1)non-linearity (tanh)linear transform(w2, b2)actual outputforward passbackward  passexpected outputlossfigure 17: consider a toy binary classi   cation task on a corpus of four short texts      surface book   ,
   kerberos library   ,    library book   , and    kerberos surface      where the model needs to predict if the
text is related to computers. the    rst two texts      surface book    and    kerberos library      are positive
under this classi   cation, and the latter two negative. the input feature space consists of four binary
features that indicate whether each of the four terms from the vocabulary is present in the text. the
table shows that the speci   ed classes are not linearly separable with respect to the input feature space.
however, if we add couple of hidden nodes, as shown in the diagram, then the classes can be linearly
separated with respect to the output of the hidden layer.

similar to character-level models, the term vectors are further aggregated (by concatenation or sum)
to obtain the representation of longer chunks of text, such as sentences. while one-hot representations
of terms (figure 18b) are common in many nlp tasks, pre-trained embeddings (e.g., [86, 158]) and
character n-graph based representations (e.g., [88, 141]) are more popularly employed in ir.

6.2 popular architectures

in this section, we describe few neural operations and architectures popular in ir. for broader
overview of different neural architectures and design patterns please refer to [64, 112, 175].

shift-invariant neural operations convolutional [89, 103, 113, 114] and recurrent [67, 83, 135,
173] architectures are commonplace in most deep learning applications. these neural operations are
part of a broader family of shift-invariant architectures. the key intuition behind these architectures
stem from the natural regularities observable in most inputs. in vision, for example, the task of
detecting a face should be invariant to whether the image is shifted, rotated, or scaled. similarly, the
meaning of an english sentence should, in most cases, stay consistent independent of which part
of the document it appears in. therefore, intuitively a neural model for object recognition or text
understanding should not learn an independent logic for the same action applied to different parts of
the input space.
all shift-invariant neural operations fundamentally employ a window-based approach. a    xed size
window is moved over the input space with    xed stride in each step. a (typically parameterized)
function   referred to as a kernel, or a    lter, or a cell   is applied over each instance of the window.
the parameters of the cell are shared across all the instances of the input window. the shared
parameters not only implies less number of total parameters in the model    but also more supervision
per parameter per training sample due to the repeated application.
figure 19a shows an example of a cell being applied on a sequence of terms   with a window size
of three terms   in each step. a popular cell implementation involves multiplying with a weight
matrix   in which case the architecture in figure 19a is referred as convolutional. an example of a
cell without any parameters is pooling   which consists of aggregating (e.g., by computing the max
or the average) over all the terms in the window12. note, that the length of the input sequence can be
variable in both cases and the length of the output of a convolutional (or pooling) layer is a function of
the input length. figure 19b shows an example of global pooling   where the window spans over the

12if the input has multiple channels per term then the aggregation is performed per channel.

29

input featureshidden layerslabelsurfacekerberosbooklibraryh1h2101010   110000   010101   001100   librarybooksurfacekerberos+0.5+0.5-1-1-1-1+1+1+0.5+0.5h1h2(a) character-level input

(b) term-level input w/ bag-of-characters per term

(c) term-level input w/ bag-of-trigraphs per term

(d) term-level input w/ pre-trained term embeddings

figure 18: examples of different representation strategies for text input to deep neural network
models. the smallest granularity of representation can be a character or a term. the vector can be a
sparse local representation, or a pre-trained embedding.

30

d o g s        h a v e         o w n e r s        c a t s         h a v e        s t a f fone-hot vectorsconcatenatechannels[chars x channels]d o g s        h a v e         o w n e r s        c a t s         h a v e        s t a f fone-hot vectorsconcatenatesumsumsumsumsumsumchannels[words x channels]# d o g s #  # h a v e #  # o w n e r s #  # c a t s #  # h a v e #  # s t a f f#one-hot vectorsconcatenate or sumsumsumsumsumsumsumchannels[words x channels] or [1 x channels]d o g s        h a v e         o w n e r s        c a t s         h a v e        s t a f fpre-trained embeddingsconcatenate or sumchannels[words x channels] or [1 x channels](a) convolution or pooling

(b) convolution w/ global pooling

(c) recurrent

(d) recursive or tree

figure 19: popular shift-invariant neural architectures including convolutional neural networks
(id98), recurrent neural networks (id56), pooling layers, and tree-structured neural networks.

whole input   being applied on top of a convolutional layer. the global pooling strategy is common
for generating a    xed size output from a variable length input.13
in convolution or pooling, each window is applied independently. in contrast, in the recurrent
architecture of figure 19c the cell not only considers the input window but also the output of the
previous instance of the cell as its input. many different cell architectures have been explored for
recurrent neural networks (id56)   although elman network [54], long short-term memory (lstm)
[83], and gated recurrent unit (gru) [32, 34] are popular. id56s are popularly applied to sequences,
but can also be useful for two (and higher) dimensional inputs [209].
one consideration when using convolutional or recurrent layers is how the window outputs are
aggregated. convolutional layers are typically followed by pooling or fully-connected layers that
perform a global aggregation over all the window instances. while a fully-connected layer is aware
of each window position, a global pooling layer is typically agnostic to it. however, unlike a fully-
connected layer, a global max-pooling operation can be applied to a variable size input. where a
global aggregation strategy may be less appropriate (e.g., long sequences), recurrent networks with
memory [18, 188, 213] and/or attention [33, 78, 126, 146, 217] may be useful.
finally, figure 19c shows tree-structured (or recursive) neural networks [20, 62, 182, 183, 195]
where the same cell is applied at multple levels in a tree-like hierarchical fashion.

13it is obvious, but may be still worth pointing out, that a global convolutional layer is exactly the same as a

fully-connected layer.

31

outputconvolutionpoolingoutputoutputoutput(a) auto-encoder

(b) siamese network

figure 20: both (a) the auto-encoder and (b) the siamese network architectures are designed to learn
compressed representations of inputs. in an auto-encoder the embeddings are learnt by minimizing
the self-reconstruction error, whereas a siamese network focuses on retaining the information that is
necessary for determining the similarity between a pair of items (say, a query and a document).

auto-encoders the auto-encoder architecture [14, 16, 164] is based on the information bottleneck
method [197]. the goal is to learn a compressed representation (cid:126)x     rk of items from their higher-
dimensional vector representations (cid:126)v     rk, such that k (cid:28) k. the model has an hour-glass shape as
shown in figure 20a and is trained by feeding in the high-dimensional vector inputs and trying to
re-construct the same representation at the output layer. the lower-dimensional middle layer forces
the encoder part of the model to extract the minimal suf   cient statistics of (cid:126)v into (cid:126)x, such that the
decoder part of the network can reconstruct the original input back from (cid:126)x. the model is trained by
minimizing the reconstruction error between the input (cid:126)v and the actual output of the decoder (cid:126)v(cid:48). the
squared-loss is popularly employed.

lauto   encoder((cid:126)v, (cid:126)v(cid:48)) = (cid:107)(cid:126)v     (cid:126)v(cid:48)(cid:107)2

(50)

siamese networks siamese networks were originally proposed for comparing    ngerprints [10]
and signatures [21]. yih et al. [222] later adapted the same architecture for comparing short texts.
the siamese network, as seen in figure 20b, resembles the auto-encoder architecture (if you squint
hard enough!)   but unlike the latter is trained on pairs of inputs (cid:104)input1, input2(cid:105). the architecture
consists of two models (model1 and model2) that project input1 and input2, respectively, to (cid:126)v1
and (cid:126)v2 in a common embedding space. a pre-de   ned metric (e.g., cosine similarity) is used to then
compute the similarity between (cid:126)v1 and (cid:126)v2. the model parameters are optimized such that (cid:126)v1 and (cid:126)v2
are closer when the two inputs are expected to be similar, and further away otherwise.
one possible id168 is the logistic loss. if each training sample consist of a triple (cid:104) (cid:126)vq, (cid:126)vd1, (cid:126)vd2(cid:105),
such that sim( (cid:126)vq, (cid:126)vd1) should be greater than sim( (cid:126)vq, (cid:126)vd2), then we minimize,

lsiamese( (cid:126)vq, (cid:126)vd1, (cid:126)vd2) = log

(cid:16)

1 + e  (sim( (cid:126)vq, (cid:126)vd2)   sim( (cid:126)vq, (cid:126)vd1))(cid:17)

(51)

where,    is a constant that is often set to 10. typically both the models   model1 and model2   share
identical architectures, but can also choose to share the same parameters.
it is important to note that, unlike the auto-encoder, the minimal suf   cient statistics retained by a
siamese network is dictated by which information it deems important for determining the similarity
between the paired items.

32

inputoutputembeddingencodedecodeinput1input2embedding1model1similarityfunctionembedding2model26.3 neural toolkits

in recent years, the advent of numerous    exible toolkits [1, 6, 31, 38, 91, 152, 198, 225] has
had a catalytic in   uence on the area of neural networks. most of the toolkits de   ne a set of
common neural operations that   like lego14 blocks   can be composed to build complex network
architectures.15 each instance of these neural operations or computation nodes can have associated
learnable parameters that are updated during training, and these parameters can be shared between
different parts of the network if necessary. every computation node under this framework must
implement the appropriate logic for,

    computing the output of the node given the input (forward-pass)
    computing the gradient of the loss with respect to the inputs, given the gradient of the loss

with respect to the output (backward-pass)

    computing the gradient of the loss with respect to its parameters, given the gradient of the

loss with respect to the output (backward-pass)

a deep neural network, such as the one in figure 16 or ones with much more complex architectures
(e.g., [76, 107, 193]), can then be speci   ed by chaining instances of these available computation
nodes, and trained end-to-end on large datasets using id26 over gpus or cpus. in ir,
various application interfaces [142, 201] bind these neural toolkits with existing retrieval/indexing
frameworks, such as indri [186].
refer to [179] for a comparison of different neural toolkits based on their speed of training using
standard performance benchmarks.

7 deep neural models for ir

traditionally, deep neural network models have much larger number of learnable parameters than
their shallower counterparts. a dnn with a large set of parameters can easily over   t to smaller
training datasets [231]. therefore, during model design it is typical to strike a balance between the
number of model parameters and the size of the data available for training. data for ad-hoc retrieval
mainly consists of,

    corpus of search queries
    corpus of candidate documents
    ground truth   in the form of either explicit human relevance judgments or implicit labels

(e.g., from clicks)   for query-document pairs

while both large scale corpora of search queries [46, 159] and documents [9, 29, 45] are publicly
available for ir research, the amount of relevance judgments that can be associated with them are
often limited outside of large industrial research labs   mostly due to user privacy concerns. we
note that we are interested in datasets where the raw text of the query and the document is available.
therefore, this excludes large scale public labelled datasets for learning-to-rank (e.g., [122]) that
don   t contain the textual contents.
the proportion of labelled and unlabelled data that is available in   uences the level of supervision
that can be employed for training these deep models. most of the models we covered in section 5
operate under the data regime where large corpus of documents or queries is available, but limited
(or no) labelled data. under such settings where no direct supervision or relevance judgments is
provided, typically an unsupervised approach is employed (e.g., [174]). the unlabelled document
(or query) corpus is used to learn good text representations, and then these learnt representations are
incorporated into an existing retrieval model or a query-document similarity metric. if small amounts
of labelled data are available, then that can be leveraged to train a retrieval model with few parameters
that in turn uses text representations that is pre-trained on larger unlabelled corpus. examples of
such semi-supervised training includes models such as [71, 157, 158]. in contrast, fully-supervised

14https://en.wikipedia.org/wiki/lego
15http://www.id136.vc/content/images/2016/01/9k-.jpg

33

table 3: comparing the nearest neighbours for "seattle" and "taylor swift" in the cdssm embedding
spaces when the model is trained on query-document pairs vs. query pre   x-suf   x pairs. the former
resembles a topical notion of similarity between terms, while the latter is more typical in the
de   nition of inter-term similarities.

seattle

query-document
weather seattle
seattle weather

seattle washington

ikea seattle

west seattle blog

pre   x-suf   x

chicago

san antonio

denver

salt lake city
seattle wa

taylor swift

query-document
taylor swift.com
taylor swift lyrics

how old is taylor swift

taylor swift twitter
taylor swift new song

pre   x-suf   x

lady gaga

meghan trainor
megan trainor
nicki minaj
anna kendrick

models such as [37, 88, 141, 176], optimize directly for the target task by training on large number of
labelled query-document pairs.
it is also useful to distinguish between deep neural models that focus on ranking long documents,
from those that rank short texts (e.g., for the question-answering task, or for document ranking
where the id194 is based on the title or on clicked queries). the challenges in
short text ranking are somewhat distinct from those involved in the ad-hoc retrieval task [36]. when
computing similarity between pairs of short-texts, vocabulary mismatches are more likely than when
the retrieved items contain long text descriptions [133]. neural models that perform matching in an
embedding space tends to be more robust towards the vocabulary mismatch problem compared to
lexical term-based matching models. on the other hand, documents with long body texts may contain
mixture of many topics and the query matches may be spread over the whole document. a neural
document ranking model (ndrm) must effectively aggregate the relevant matches from different
parts of a long document.
in the rest of this section, we discuss different types of ndrm architectures and approaches that have
been explored in the literature.

7.1 document auto-encoders

salakhutdinov and hinton [174] proposed one of the earliest deep neural models for ad-hoc retrieval.
the model is a deep auto-encoder trained on unlabelled document corpus. the model treats each
document as a bag-of-terms and uses a one-hot vector for representing the terms themselves   
considering only top two thousand most popular terms in the corpus after removing stopwords.
salakhutdinov and hinton [174]    rst pre-train the model layer-by-layer, and then train it further
end-to-end for additional tuning. the model uses binary hidden units and therefore the learnt vector
representations of documents are also binary.
the semantic hashing model generates a condensed binary vector representation (or a hash) of
documents. given a search query, a corresponding hash is generated and the relevant candidate
documents quickly retrieved that match the same hash vector. a standard ir model can then be
employed to rank between the selected documents.
semantic hashing is an example of a document encoder based approach to ir. the vocabulary size of
two thousand distinct terms may be too small for most practical ir tasks. a larger vocabulary or a
different term representation strategy   such as the character trigraph based representation of figure
18c   may be considered. another shortcoming of the auto-encoder architecture is that it minimizes
the document reconstruction error which may not align exactly with the goal of the target ir task. a
better alternative may be to train on query-document paired data where the choice of what constitutes
as the minimal suf   cient statistics of the document is in   uenced by what is important for determining
relevance of the document to likely search queries. in line with this intuition, we next discuss the
siamese architecture based models.

34

figure 21: schematic view of an interaction matrix generated by comparing windows of text from
the query and the document. a deep neural network   such as a id98   operates over the interaction
matrix to    nd patterns of matches that suggest relevance of the document to the query.

7.2 siamese networks

in recent years, several deep neural models based on the siamese architecture have been explored
especially for short text matching. the deep semantic similarity model (dssm) [88] is one such
architecture that trains on query and document title pairs where both the pieces of texts are represented
as bags-of-character-trigraphs. the dssm architecture consists of two deep models   for the query
and the document   with all fully-connected layers and cosine distance as the choice of similarity
function in the middle. huang et al. [88] proposed to train the model on clickthrough data where
each training sample consists of a query q, a positive document d+ (a document that was clicked by
a user on the serp for that query), and a set of negative documents d    randomly sampled with
uniform id203 from the full collection. the model is trained my minimizing the cross-id178
loss after taking a softmax over the model outputs for all the candidate documents,

(cid:16)

e    cos(cid:0)(cid:126)q, (cid:126)d+(cid:1)
d   d e    cos(cid:0)(cid:126)q, (cid:126)d(cid:1)(cid:17)
(cid:80)

ldssm(q, d+, d   ) =    log

where, d = {d+}     d   

(52)

(53)

while, dssm [88] employs deep fully-connected architecture for the query and the document models,
more sophisticated architectures involving convolutional layers [59, 86, 177, 178], recurrent layers
[155, 156], and tree-structured networks [195] have also been explored. the similarity function can
also be parameterized and implemented as additional layers of the neural network as in [176]. most
of these models have been evaluated on the short text matching task, but mitra et al. [141] recently
reported meaningful performances on the long document ranking task from models like dssm [88]
and cdssm [177]. mitra et al. [141] also show that sampling the negative documents uniformly
from the collection is less effective to using documents that are closer to the query intent but judged
as non-relelvant by human annotators.

notions of similarity
it is important to emphasize that our earlier discussion in section 4.2 on
different notions of similarity between terms that can be learnt by shallow embedding models is also
relevant in the context of these deeper architectures. in the case of siamese networks, such as the
convolutional-dssm (cdssm) [177], the notion of similarity being modelled depends on the choice
of the paired data that the model is trained on. when the cdssm is trained on query and document
title pairs [177] then the notion of similarity is more topical in nature. mitra and craswell [139]
trained the same cdssm architecture on query pre   x-suf   x pairs which, in contrast, captures a more
typical notion of similarity, as shown in table 3. in a related work, mitra [138] demonstrated that the
cdssm model when trained on session-query pairs is amenable to vector-based text analogies.

35

interaction matrixneural networkquerydocumentthe president of the united states of america (po-
tus) is the elected head of state and head of govern-
ment of the united states. the president leads the
executive branch of the federal government and is
the commander in chief of the united states armed
forces. barack hussein obama ii (born august 4,
1961) is an american politician who is the 44th and
current president of the united states. he is the
   rst african american to hold the of   ce and the    rst
president born outside the continental united states.

the president of the united states of america (po-
tus) is the elected head of state and head of govern-
ment of the united states. the president leads the
executive branch of the federal government and is
the commander in chief of the united states armed
forces. barack hussein obama ii (born august 4,
1961) is an american politician who is the 44th and
current president of the united states. he is the
   rst african american to hold the of   ce and the    rst
president born outside the continental united states.

(a) lexical model

(b) semantic model

figure 22: analysis of term importance for estimating the relevance of a passage to the query
   united states president    by a lexical and a semantic deep neural network model. the lexical model
only considers the matches of the query terms in the document, but gives more emphasis to earlier
occurrences. the semantic model is able to extract evidence of relevance from related terms such as
   obama    and    federal   .

(cid:126)vthings to do in london     (cid:126)vlondon + (cid:126)vnew york     (cid:126)vnew york tourist attractions
(cid:126)vuniversity of washington     (cid:126)vseattle + (cid:126)vdenver     (cid:126)vuniversity of colorado

(cid:126)vnew york + (cid:126)vnewspaper     (cid:126)vnew york times

(54)
(55)
(56)

by modelling different notions of similarity these deep neural models tend to be more suitable for
other ir tasks, such as query auto-completion [139] or session-based personalization [138].

7.3

interaction-based networks

siamese networks represent both the query and the document using single embedding vectors.
alternatively, we can individually compare different parts of the query with different parts of the
document, and then aggregate these partial evidence of relevance. especially, when dealing with
long documents   that may contain a mixture of many topics   such a strategy may be more effective
than trying to represent the full document as a single low-dimensional vector. typically, in these
approaches a sliding window is moved over both the query and the document text and each instance
of the window over the query is compared against each instance of the window over the document
text (see figure 21). the terms within each window can be represented in different ways including,
one-hot vectors, pre-trained embeddings, or embeddings that are updated during the model training. a
neural model (typically convolutional) operates over the generated interaction matrix and aggregates
the evidence across all the pairs of windows compared.
the interaction matrix based approach have been explored both for short text matching [86, 124, 158,
208, 220, 223], as well as for ranking long documents [141, 157].

7.4 lexical and semantic matching networks

much of the explorations in neural ir models have focused on learning good representations of text.
however, these representation learning models tend to perform poorly when dealing with rare terms
and search intents. in section 2.2, we highlighted the importance of modelling rare terms in ir. based
on similar motivaions, guo et al. [71] and mitra et al. [141] have recently emphasized the importance
of modelling lexical matches using deep neural networks. mitra et al. [141] argue that web search is
a    tale of two queries   . for the query    pekarovic land company   , it is easier to estimate relevance
based on patterns of exact matches of the rare term    pekarovic   . on the other hand, a neural model
focused on matching in the embedding space is unlikely to have a good representation for this rare
term. in contrast, for the query    what channel are the seahawks on today   , the target document likely
contains    espn    or    sky sports      not the term    channel   . a representation learning neural model
can associate occurrences of    espn    in the document as positive evidence towards the document
being relevant to the query. figure 22 highlights the difference between the terms that in   uence

36

figure 23: in the duet architecture [141], the two sub-networks are jointly trained and the    nal output
is a linear combination of the outputs of the lexical and the semantic matching sub-networks. the
lexical matching sub-network (left) uses a convolutional model that operates over a binary interaction
matrix.16the semantic matching sub-network (right) learns representations of query and document
text for effective matching in the embedding space. cross-id178 loss is used to train the network
similar to other models in section 7.2.

the estimation of relevance of the same query-passage pair by a lexical matching and a semantic
matching model. a good neural ir model should incorporate both lexical and semantic matching
signals [141].
guo et al. [71] proposed to use histogram-based features in their dnn model to capture lexical
notion of relevance. mitra et al. [141] leverage large scale labelled data from bing to train a duet
architecture (figure 23) that learns to identify good patterns of both lexical and semantic matches
jointly. neural models that focus on lexical matching typically have fewer parameters, and can be
trained under small data regimes   unlike their counterparts that focus on learning representations of
text.
interestingly, a query level analysis seems to indicate that both traditional non-neural ir approaches
and more recent neural methods tend to perform well on different segments of queries depending on
whether they focus on lexical or semantic matching. figure 24 plots a few of these models based on
their per-query ndcg values on a test set.

8 conclusion

we present a tutorial on neural methods for information retrieval. for machine learning researchers
who may be less familiar with ir tasks, we introduced the fundamentals of traditional ir models
and metrics. for ir researchers, we summarized key concepts related to representation learning
with (shallow or deep) neural networks. finally, we presented some of the recent neural methods for
document ranking and question-answer matching.

16it is important to emphasize, that while mitra et al. [141] and others have used interaction-based representa-
tion for modelling lexical matches, the two ideas are distinct. some interaction-matrix based representations
compare texts using their pre-trained embeddings [86, 223]. similarly, lexical matching can be modelled without
employing an interaction matrix based representation [71].

37

query textgenerate query term vectordoc textgenerate docterm vectorgenerate interaction matrixqueryterm vectordocterm vectorquery textgenerate query embeddingdoc textgenerate doc embeddinghadamardproductqueryembeddingdocembeddingfully connected layers for matchingfully connected layers for matchingsuid113xical matching modelsemantic matching modelfigure 24: a demonstration that ir models that focus on lexical matching tend to perform well on
queries that are distinct from queries on which semantic matching models achieve good relevance.
each model is represented by a vector of ndcg scores achieved on a set of test queries. for
visualization, id167 [128] is used to plot the points in a two-dimensional space. lexical matching
models (bm25, ql, dm, drmm, and duet-lexical) are seen to form a cluster   as well as the
models that focus on representation learning.

we have focused on retrieval of long and short text. in the case of long text, the model must deal
with variable length documents, where the relevant sections of a document may be surrounded by
irrelevant text. for both long and short text, but particularly for short, ir models should also deal
with the query-document vocabulary mismatch problem, by learning how patterns of query words
and (different) document words can indicate relevance. models should also consider lexical matches
when the query contains rare terms   such as a person   s name or a product model number   not seen
during training, and to avoid retrieving semantically related but irrelevant results.
an ideal model for information retrieval would be able to infer the meaning of a query from context.
given a query about the prime minister of uk, for example, it may be obvious from context whether
it refers to john major or teresa may   perhaps due to the time period of the corpus, or it may need to
be disambiguated based on other context such as the other query terms or the user   s short or long-term
history. the ideal ir model may need to encode this context, which means that the model is like
a library that effectively memorizes massive number of connections between entities and contexts.
the number of learnable parameters of a ml model, however, is typically    xed, which may imply
that there is a limited budget for how much real world knowledge the model can incorporate during
training. an ideal model, therefore, may also need to learn to be like a librarian with incomplete
domain knowledge, but capable of reading documents related to the current query and reasoning
about the meaning of the query as part of the retrieval process.
many of the breakthroughs in deep learning have been motivated by the needs of speci   c application
areas. convolutional neural networks, for example, are particularly popular with the vision community,
whereas recurrent architectures    nd more applications in id103 and nlp. it is likely that
the speci   c needs and challenges of ir tasks may motivate novel neural architectures and methods.
future ir explorations may also be motivated by developments in related areas, such as nlp. for
example, neural architectures that have been evaluated on non-ir tasks [39, 50, 95, 99, 232] can be
investigated in the retrieval context. similarly, new methods for training deep models for nlp   e.g.,
using id23 [163, 224] and id3 (gans) [226]   may
carry over to the ir setup.
however, given the pace at which the area of deep learning is growing, in terms of the number of
new architectures and training regimes, we should be wary of the combinatorial explosion of trying
every model on every ir task. we should not disproportionately focus on maximizing quantitative
improvements and in the process neglect theoretical understanding and qualitative insights. it would

38

   bm25   ql   dm   lsa   dssm   cdssm   desm   drmm   duet (lexical)   duet (semantic)   duetbe a bad outcome for the    eld if these explorations do not grow our understanding of the fundamental
principles of machine learning and information retrieval. neural models should not be the hammer
that we try on every ir task, or we may risk reducing every ir task to a nail.17 a better metaphor for
the neural models may be a mirror that allows ir researchers to gain new insights into the underlying
principles of ir. this may imply that we prefer neural models that, if not interpretable, then at least
are amenable to analysis and interrogation. we may elicit more insights from simpler models while
more sophisticated models may achieve state-of-the-art performances. as a community, we may need
to focus on both to achieve results that are both impactful as well as insightful.
the focus of this article has been on ad-hoc retrieval and to a lesser extent on question-answering.
however, neural approaches have shown interesting applications to other existing retrieval scenarios,
including query auto-completion [139], query recommendation [184], session modelling [138],
modelling diversity [215], modelling user click behaviours [19], knowledge-based ir [153], and
even optimizing for multiple ir tasks [123]. in addition, recent trends suggest that advancements
in deep neural networks methods are also fuelling emerging ir scenarios such as conversational ir
[218, 234] and multi-modal retrieval [127]. neural methods may have an even bigger impact on some
of these other ir tasks.
ir also has a role in the context of the ambitions of the machine learning community. retrieval is
key to many id62 approaches [102, 202]. ghazvininejad et al. [60] proposed to    search   
external information sources in the process of solving complex tasks using neural networks. the
idea of learning local representations proposed by diaz et al. [51] may be applicable to non-ir tasks.
while we look at applying neural methods to ir, we should also look for opportunities to leverage ir
techniques as part of   or in combination with   neural and other machine learning models.
finally, we must also renew our focus on the fundamentals, including benchmarking and reproducibil-
ity. an important prerequisite to enable the    neural ir train    to steam forward is to build shared
public resources   e.g., large scale datasets for training and evaluation, and repository of shared
model implementations   and to ensure that appropriate bindings exist (e.g., [142, 201]) between
popular ir frameworks and popular toolkits from the neural network community. the emergence
of new ir tasks also demands rethinking many of our existing metrics. the metrics that may be
appropriate for evaluating document ranking systems may be inadequate when the system generates
textual answers in response to information seeking questions. in the latter scenario, the metric should
distinguish between whether the response differs from the ground truth in the information content or
in phrasing of the answer [57, 120, 145]. as multi-turn interactions with retrieval systems become
more common, the de   nition of task success will also need to evolve accordingly. neural ir should
not only focus on novel techniques, but should also encompass all these other aspects.

references
[1] mart  n abadi, ashish agarwal, paul barham, eugene brevdo, zhifeng chen, craig citro,
greg s corrado, andy davis, jeffrey dean, matthieu devin, and others. 2016. tensor-
   ow: large-scale machine learning on heterogeneous distributed systems. arxiv preprint
arxiv:1603.04467 (2016).

[2] nasreen abdul-jaleel, james allan, w bruce croft, fernando diaz, leah larkey, xiaoyan
li, mark d smucker, and courtney wade. 2004. umass at trec 2004: novelty and hard.
(2004).

[3] eugene agichtein, david carmel, dan pelleg, yuval pinter, and donna harman. 2015. overview

of the trec 2015 liveqa track.. in trec.

[4] qingyao ai, liu yang, jiafeng guo, and w bruce croft. 2016. analysis of the paragraph vector

model for information retrieval. in proc. ictir. acm, 133   142.

[5] qingyao ai, liu yang, jiafeng guo, and w bruce croft. 2016. improving language estimation

with the paragraph vector model for ad-hoc retrieval. in proc. sigir. acm, 869   872.

[6] rami al-rfou, guillaume alain, amjad almahairi, christof angermueller, dzmitry bahdanau,
nicolas ballas, fr  d  ric bastien, justin bayer, anatoly belikov, alexander belopolsky, and

17https://en.wikipedia.org/wiki/law_of_the_instrument

39

others. 2016. theano: a python framework for fast computation of mathematical expressions.
arxiv preprint arxiv:1605.02688 (2016).

[7] sanjeev arora, yuanzhi li, yingyu liang, tengyu ma, and andrej risteski. 2015. rand-walk:
a latent variable model approach to id27s. arxiv preprint arxiv:1502.03520
(2015).

[8] dzmitry bahdanau, kyunghyun cho, and yoshua bengio. 2014. id4 by

jointly learning to align and translate. arxiv preprint arxiv:1409.0473 (2014).

[9] peter bailey, nick craswell, and david hawking. 2003. engineering a multi-purpose test
collection for web retrieval experiments. information processing & management 39, 6 (2003),
853   871.

[10] pierre baldi and yves chauvin. 1993. neural networks for    ngerprint recognition. neural

computation 5, 3 (1993), 402   418.

[11] marco baroni, georgiana dinu, and germ  n kruszewski. 2014. don   t count, predict! a
systematic comparison of context-counting vs. context-predicting semantic vectors. in proc.
acl, vol. 1. 238   247.

[12] marco baroni and alessandro lenci. 2010. distributional memory: a general framework for

corpus-based semantics. computational linguistics 36, 4 (2010), 673   721.

[13] roland barthes. 1977. elements of semiology. macmillan.
[14] yoshua bengio and others. 2009. learning deep architectures for ai. foundations and trends r(cid:13)

in machine learning 2, 1 (2009), 1   127.

[15] yoshua bengio, r  jean ducharme, pascal vincent, and christian jauvin. 2003. a neural
probabilistic language model. journal of machine learning research 3, feb (2003), 1137   1155.

[16] yoshua bengio, pascal lamblin, dan popovici, hugo larochelle, and others. 2007. greedy

layer-wise training of deep networks. proc. nips 19 (2007), 153.

[17] adam berger and john lafferty. 1999. information retrieval as statistical translation. in proc.

sigir. acm, 222   229.

[18] antoine bordes, nicolas usunier, sumit chopra, and jason weston. 2015. large-scale simple

id53 with memory networks. arxiv preprint arxiv:1506.02075 (2015).

[19] alexey borisov, ilya markov, maarten de rijke, and pavel serdyukov. 2016. a neural click

model for web search. in proc. www. proc. www, 531   541.

[20] samuel r bowman, jon gauthier, abhinav rastogi, raghav gupta, christopher d manning,
and christopher potts. 2016. a fast uni   ed model for parsing and sentence understanding. arxiv
preprint arxiv:1603.06021 (2016).

[21] jane broid113y, james w. bentz, l  on bottou, isabelle guyon, yann lecun, cliff moore, eduard
s  ckinger, and roopak shah. 1993. signature veri   cation using a "siamese" time delay
neural network. ijprai 7, 4 (1993), 669   688.

[22] peter f brown, john cocke, stephen a della pietra, vincent j della pietra, fredrick jelinek,
john d lafferty, robert l mercer, and paul s roossin. 1990. a statistical approach to machine
translation. computational linguistics 16, 2 (1990), 79   85.

[23] peter f brown, vincent j della pietra, stephen a della pietra, and robert l mercer. 1993. the
mathematics of id151: parameter estimation. computational linguistics
19, 2 (1993), 263   311.

[24] john a bullinaria and joseph p levy. 2007. extracting semantic representations from word
co-occurrence statistics: a computational study. behavior research methods 39, 3 (2007),
510   526.

40

[25] john a bullinaria and joseph p levy. 2012. extracting semantic representations from word
co-occurrence statistics: stop-lists, id30, and svd. behavior research methods 44, 3
(2012), 890   907.

[26] chris burges, tal shaked, erin renshaw, ari lazier, matt deeds, nicole hamilton, and
greg hullender. 2005. learning to rank using id119. in proceedings of the 22nd
international conference on machine learning. acm, 89   96.

[27] christopher jc burges. 2010. from ranknet to lambdarank to lambdamart: an overview.

learning 11, 23-581 (2010), 81.

[28] christopher jc burges, robert ragno, and quoc viet le. 2006. learning to rank with nons-

mooth cost functions. in nips, vol. 6. 193   200.

[29] jamie callan, mark hoy, changkuk yoo, and le zhao. 2009. clueweb09 data set. (2009).

[30] daniel chandler. 1994. semiotics for beginners. (1994).

[31] tianqi chen, mu li, yutian li, min lin, naiyan wang, minjie wang, tianjun xiao, bing xu,
chiyuan zhang, and zheng zhang. 2015. mxnet: a    exible and ef   cient machine learning
library for heterogeneous distributed systems. arxiv preprint arxiv:1512.01274 (2015).

[32] kyunghyun cho, bart van merri  nboer, dzmitry bahdanau, and yoshua bengio. 2014. on
the properties of id4: encoder-decoder approaches. arxiv preprint
arxiv:1409.1259 (2014).

[33] jan k chorowski, dzmitry bahdanau, dmitriy serdyuk, kyunghyun cho, and yoshua bengio.

2015. attention-based models for id103. in proc. nips. 577   585.

[34] junyoung chung, caglar gulcehre, kyunghyun cho, and yoshua bengio. 2014. empir-
ical evaluation of gated recurrent neural networks on sequence modeling. arxiv preprint
arxiv:1412.3555 (2014).

[35] st  phane clinchant and florent perronnin. 2013. aggregating continuous id27s for
information retrieval. in proceedings of the workshop on continuous vector space models and
their compositionality. 100   109.

[36] daniel cohen, qingyao ai, and w bruce croft. 2016. adaptability of neural networks on

varying granularity ir tasks. arxiv preprint arxiv:1606.07565 (2016).

[37] daniel cohen and w bruce croft. 2016. end to end long short term memory networks for

non-factoid id53. in proc. ictir. acm, 143   146.

[38] ronan collobert, koray kavukcuoglu, and cl  ment farabet. 2011. torch7: a matlab-like

environment for machine learning. in biglearn, nips workshop.

[39] ronan collobert, jason weston, l  on bottou, michael karlen, koray kavukcuoglu, and pavel
kuksa. 2011. natural language processing (almost) from scratch. the journal of machine
learning research 12 (2011), 2493   2537.

[40] nick craswell. 2009. mean reciprocal rank. in encyclopedia of database systems. springer,

1703   1703.

[41] nick craswell. 2017. neural models for full text search. in proceedings of the tenth acm

international conference on web search and data mining. acm, 251   251.

[42] nick craswell, w bruce croft, maarten de rijke, jiafeng guo, and bhaskar mitra. 2017.

neu-ir   17: neural information retrieval. in proc. sigir. acm.

[43] nick craswell, w bruce croft, jiafeng guo, bhaskar mitra, and maarten de rijke. 2016.

neu-ir: the sigir 2016 workshop on neural information retrieval. (2016).

[44] nick craswell, w bruce croft, jiafeng guo, bhaskar mitra, and maarten de rijke. 2016. report
on the sigir 2016 workshop on neural information retrieval (neu-ir). acm sigir forum 50,
2 (2016), 96   103.

41

[45] nick craswell, david hawking, ross wilkinson, and mingfang wu. 2003. overview of the

trec 2002 web track. in trec, vol. 3. 12th.

[46] nick craswell, rosie jones, georges dupret, and evelyne viegas. 2009. proceedings of the

2009 workshop on web search click data. acm.

[47] ferdinand de saussure, wade baskin, and perry meisel. 2011. course in general linguistics.

columbia university press.

[48] scott c. deerwester, susan t dumais, thomas k. landauer, george w. furnas, and richard a.

harshman. 1990. indexing by latent semantic analysis. jasis 41, 6 (1990), 391   407.

[49] li deng, dong yu, and others. 2014. deep learning: methods and applications. foundations

and trends r(cid:13) in signal processing 7, 3   4 (2014), 197   387.

[50] misha denil, alban demiraj, nal kalchbrenner, phil blunsom, and nando de freitas. 2014.
modelling, visualising and summarising documents with a single convolutional neural network.
arxiv preprint arxiv:1406.3830 (2014).

[51] fernando diaz, bhaskar mitra, and nick craswell. 2016. id183 with locally-trained

id27s. in proc. acl.

[52] fernando diaz, ryen white, georg buscher, and dan liebling. 2013. robust models of mouse
movement on dynamic web search results pages. in proceedings of the 22nd acm international
conference on conference on information & knowledge management. acm, 1451   1460.

[53] john duchi, elad hazan, and yoram singer. 2011. adaptive subgradient methods for online
learning and stochastic optimization. journal of machine learning research 12, jul (2011),
2121   2159.

[54] jeffrey l elman. 1990. finding structure in time. cognitive science 14, 2 (1990), 179   211.

[55] david ferrucci, eric brown, jennifer chu-carroll, james fan, david gondek, aditya a
kalyanpur, adam lally, j william murdock, eric nyberg, john prager, and others. 2010.
building watson: an overview of the deepqa project. ai magazine 31, 3 (2010), 59   79.

[56] john r firth. 1957. a synopsis of linguistic theory, 1930-1955. (1957).

[57] michel galley, chris brockett, alessandro sordoni, yangfeng ji, michael auli, chris quirk,
margaret mitchell, jianfeng gao, and bill dolan. 2015. deltaid7: a discriminative metric
for generation tasks with intrinsically diverse targets. arxiv preprint arxiv:1506.06863 (2015).

[58] debasis ganguly, dwaipayan roy, mandar mitra, and gareth jf jones. 2015. id27
based generalized language model for information retrieval. in proc. sigir. acm, 795   798.

[59] jianfeng gao, patrick pantel, michael gamon, xiaodong he, li deng, and yelong shen. 2014.
modeling interestingness with deep neural networks. in proceedings of the 2013 conference on
empirical methods in natural language processing.

[60] marjan ghazvininejad, chris brockett, ming-wei chang, bill dolan, jianfeng gao, wen-tau
yih, and michel galley. 2017. a knowledge-grounded neural conversation model. arxiv
preprint arxiv:1702.01932 (2017).

[61] yoav goldberg and omer levy. 2014. id97 explained: deriving mikolov et al.   s negative-

sampling word-embedding method. arxiv preprint arxiv:1402.3722 (2014).

[62] christoph goller and andreas kuchler. 1996. learning task-dependent distributed represen-
tations by id26 through structure. in neural networks, 1996., ieee international
conference on, vol. 1. ieee, 347   352.

[63] gene h golub and christian reinsch. 1970. singular value decomposition and least squares

solutions. numerische mathematik 14, 5 (1970), 403   420.

[64] ian goodfellow, yoshua bengio, and aaron courville. 2016. deep learning. mit press.

42

[65] laura a granka, thorsten joachims, and geri gay. 2004. eye-tracking analysis of user behavior
in www search. in proceedings of the 27th annual international acm sigir conference on
research and development in information retrieval. acm, 478   479.

[66] alex graves. 2013. generating sequences with recurrent neural networks. arxiv preprint

arxiv:1308.0850 (2013).

[67] alex graves, marcus liwicki, santiago fern  ndez, roman bertolami, horst bunke, and j  rgen
schmidhuber. 2009. a novel connectionist system for unconstrained handwriting recognition.
ieee transactions on pattern analysis and machine intelligence 31, 5 (2009), 855   868.

[68] mihajlo grbovic, nemanja djuric, vladan radosavljevic, and narayan bhamidipati. 2015.
search retargeting using directed query embeddings. in proc. www. international world
wide web conferences steering committee, 37   38.

[69] mihajlo grbovic, nemanja djuric, vladan radosavljevic, fabrizio silvestri, and narayan
bhamidipati. 2015. context-and content-aware embeddings for query rewriting in sponsored
search. in proc. sigir. acm, 383   392.

[70] zhiwei guan and edward cutrell. 2007. an eye tracking study of the effect of target rank on
web search. in proceedings of the sigchi conference on human factors in computing systems.
acm, 417   420.

[71] jiafeng guo, yixing fan, qingyao ai, and w bruce croft. 2016. a deep relevance matching

model for ad-hoc retrieval. in proc. cikm. acm, 55   64.

[72] jiafeng guo, yixing fan, qingyao ai, and w bruce croft. 2016. semantic matching by

non-linear word transportation for information retrieval. in proc. cikm. acm, 701   710.

[73] richard hr hahnloser, rahul sarpeshkar, misha a mahowald, rodney j douglas, and h se-
bastian seung. 2000. digital selection and analogue ampli   cation coexist in a cortex-inspired
silicon circuit. nature 405, 6789 (2000), 947   951.

[74] roy harris. 2001. saussure and his interpreters. edinburgh university press.

[75] zellig s harris. 1954. distributional structure. word 10, 2-3 (1954), 146   162.

[76] kaiming he, xiangyu zhang, shaoqing ren, and jian sun. 2016. deep residual learning for
image recognition. in proceedings of the ieee conference on id161 and pattern
recognition. 770   778.

[77] robert hecht-nielsen and others. 1988. theory of the id26 neural network. neural

networks 1, supplement-1 (1988), 445   448.

[78] karl moritz hermann, tomas kocisky, edward grefenstette, lasse espeholt, will kay, mustafa
suleyman, and phil blunsom. 2015. teaching machines to read and comprehend. in proc. nips.
1693   1701.

[79] djoerd hiemstra. 2001. using language models for information retrieval. taaluitgeverij neslia

paniculata.

[80] felix hill, antoine bordes, sumit chopra, and jason weston. 2015. the goldilocks prin-
ciple: reading children   s books with explicit memory representations. arxiv preprint
arxiv:1511.02301 (2015).

[81] geoffrey hinton, li deng, dong yu, george e dahl, abdel-rahman mohamed, navdeep jaitly,
andrew senior, vincent vanhoucke, patrick nguyen, tara n sainath, and others. 2012. deep
neural networks for acoustic modeling in id103: the shared views of four research
groups. signal processing magazine, ieee 29, 6 (2012), 82   97.

[82] geoffrey e hinton. 1984. distributed representations. (1984).

[83] sepp hochreiter and j  rgen schmidhuber. 1997. long short-term memory. neural computation

9, 8 (1997), 1735   1780.

43

[84] kajta hofmann, bhaskar mitra, filip radlinski, and milad shokouhi. 2014. an eye-tracking

study of user interactions with query auto completion. in proc. cikm. acm, 549   558.

[85] kurt hornik, maxwell stinchcombe, and halbert white. 1989. multilayer feedforward networks

are universal approximators. neural networks 2, 5 (1989), 359   366.

[86] baotian hu, zhengdong lu, hang li, and qingcai chen. 2014. convolutional neural network

architectures for matching natural language sentences. in proc. nips. 2042   2050.

[87] gao huang, chuan guo, matt j kusner, yu sun, fei sha, and kilian q weinberger. 2016.

supervised word mover   s distance. in proc. nips. 4862   4870.

[88] po-sen huang, xiaodong he, jianfeng gao, li deng, alex acero, and larry heck. 2013.
learning deep structured semantic models for web search using clickthrough data. in proc.
cikm. acm, 2333   2338.

[89] kevin jarrett, koray kavukcuoglu, yann lecun, and others. 2009. what is the best multi-
stage architecture for object recognition?. in id161, 2009 ieee 12th international
conference on. ieee, 2146   2153.

[90] kalervo j  rvelin and jaana kek  l  inen. 2002. cumulated gain-based evaluation of ir techniques.

acm transactions on information systems (tois) 20, 4 (2002), 422   446.

[91] yangqing jia, evan shelhamer, jeff donahue, sergey karayev, jonathan long, ross girshick,
sergio guadarrama, and trevor darrell. 2014. caffe: convolutional architecture for fast feature
embedding. in proceedings of the 22nd acm international conference on multimedia. acm,
675   678.

[92] thorsten joachims, laura granka, bing pan, helene hembrooke, and geri gay. 2005. accu-
rately interpreting clickthrough data as implicit feedback. in proceedings of the 28th annual
international acm sigir conference on research and development in information retrieval.
acm, 154   161.

[93] thorsten joachims, laura granka, bing pan, helene hembrooke, filip radlinski, and geri gay.
2007. evaluating the accuracy of implicit feedback from clicks and query reformulations in
web search. acm transactions on information systems (tois) 25, 2 (2007), 7.

[94] rafal jozefowicz, oriol vinyals, mike schuster, noam shazeer, and yonghui wu. 2016.

exploring the limits of id38. arxiv preprint arxiv:1602.02410 (2016).

[95] nal kalchbrenner, edward grefenstette, and phil blunsom. 2014. a convolutional neural

network for modelling sentences. arxiv preprint arxiv:1404.2188 (2014).

[96] tom kenter, alexey borisov, and maarten de rijke. 2016. siamese cbow: optimizing word

embeddings for sentence representations. arxiv preprint arxiv:1606.04640 (2016).

[97] tom kenter, alexey borisov, christophe van gysel, mostafa dehghani, maarten de rijke, and
bhaskar mitra. 2017. neural networks for information retrieval (nn4ir). in proc. sigir.
acm.

[98] tom kenter and maarten de rijke. short text similarity with id27s. in proc.

cikm, vol. 15. 115.

[99] yoon kim. 2014. convolutional neural networks for sentence classi   cation. arxiv preprint

arxiv:1408.5882 (2014).

[100] yoon kim, yacine jernite, david sontag, and alexander m rush. 2015. character-aware

neural language models. arxiv preprint arxiv:1508.06615 (2015).

[101] ryan kiros, richard zemel, and ruslan r salakhutdinov. 2014. a multiplicative model for

learning distributed text-based attribute representations. in proc. nips. 2348   2356.

[102] gregory koch. 2015. siamese neural networks for one-shot image recognition. ph.d. disser-

tation. university of toronto.

44

[103] alex krizhevsky, ilya sutskever, and geoffrey e hinton. 2012. id163 classi   cation with

deep convolutional neural networks. in proc. nips. 1097   1105.

[104] matt j kusner, edu yu sun, edu nicholas i kolkin, and wustl edu. from word

embeddings to document distances. (????).

[105] john lafferty and chengxiang zhai. 2001. document language models, query models, and
risk minimization for information retrieval. in proceedings of the 24th annual international
acm sigir conference on research and development in information retrieval. acm, 111   119.

[106] dmitry lagun, chih-hung hsieh, dale webster, and vidhya navalpakkam. 2014. towards
better measurement of attention and satisfaction in mobile search. in proceedings of the 37th
international acm sigir conference on research & development in information retrieval.
acm, 113   122.

[107] gustav larsson, michael maire, and gregory shakhnarovich. 2016. fractalnet: ultra-deep

neural networks without residuals. arxiv preprint arxiv:1605.07648 (2016).

[108] victor lavrenko. 2008. a generative theory of relevance. vol. 26. springer science & business

media.

[109] victor lavrenko and w bruce croft. 2001. relevance based language models. in proceedings
of the 24th annual international acm sigir conference on research and development in
information retrieval. acm, 120   127.

[110] quoc v le and tomas mikolov. 2014. distributed representations of sentences and docu-

ments.. in icml, vol. 14. 1188   1196.

[111] r  mi lebret and ronan collobert. 2013. word emdeddings through hellinger pca. arxiv

preprint arxiv:1312.5542 (2013).

[112] yann lecun, yoshua bengio, and geoffrey hinton. 2015. deep learning. nature 521, 7553

(2015), 436   444.

[113] yann lecun, fu jie huang, and leon bottou. 2004. learning methods for generic object
recognition with invariance to pose and lighting. in id161 and pattern recognition,
2004. cvpr 2004. proceedings of the 2004 ieee computer society conference on, vol. 2.
ieee, ii   104.

[114] yann lecun, koray kavukcuoglu, and cl  ment farabet. 2010. convolutional networks and
applications in vision. in circuits and systems (iscas), proceedings of 2010 ieee international
symposium on. ieee, 253   256.

[115] omer levy and yoav goldberg. 2014. dependencybased id27s. in proc. acl,

vol. 2. 302   308.

[116] omer levy, yoav goldberg, and ido dagan. 2015. improving distributional similarity with
lessons learned from id27s. transactions of the association for computational
linguistics 3 (2015), 211   225.

[117] omer levy, yoav goldberg, and israel ramat-gan. 2014. linguistic regularities in sparse and

explicit word representations. conll-2014 (2014), 171.

[118] steven levy. 2011. in the plex: how google thinks, works, and shapes our lives. simon and

schuster.

[119] hang li and zhengdong lu. deep learning for information retrieval. (????).

[120] chia-wei liu, ryan lowe, iulian v serban, michael noseworthy, laurent charlin, and joelle
pineau. 2016. how not to evaluate your dialogue system: an empirical study of unsupervised
id74 for dialogue response generation. arxiv preprint arxiv:1603.08023 (2016).

[121] tie-yan liu. 2009. learning to rank for information retrieval. foundation and trends in

information retrieval 3, 3 (march 2009), 225   331.

45

[122] tie-yan liu, jun xu, tao qin, wenying xiong, and hang li. 2007. letor: benchmark
dataset for research on learning to rank for information retrieval. in proceedings of sigir 2007
workshop on learning to rank for information retrieval. 3   10.

[123] xiaodong liu, jianfeng gao, xiaodong he, li deng, kevin duh, and ye-yi wang. repre-
sentation learning using multi-task deep neural networks for semantic classi   cation and
information retrieval. proc. naacl, may 2015 (????).

[124] zhengdong lu and hang li. 2013. a deep architecture for matching short texts. in proc. nips.

1367   1375.

[125] kevin lund and curt burgess. 1996. producing high-dimensional semantic spaces from lexical
co-occurrence. behavior research methods, instruments, & computers 28, 2 (1996), 203   208.

[126] minh-thang luong, hieu pham, and christopher d manning. 2015. effective approaches to

attention-based id4. arxiv preprint arxiv:1508.04025 (2015).

[127] lin ma, zhengdong lu, lifeng shang, and hang li. 2015. multimodal convolutional neural
networks for matching image and sentence. in proceedings of the ieee international conference
on id161. 2623   2631.

[128] laurens van der maaten and geoffrey hinton. 2008. visualizing data using id167. journal of

machine learning research 9, nov (2008), 2579   2605.

[129] christopher manning. 2016. understanding human language: can nlp and deep learning

help?. in proc. sigir. acm, 1   1.

[130] ivan markovsky. 2011. low rank approximation: algorithms, implementation, applications.

springer science & business media.

[131] irina matveeva, chris burges, timo burkard, andy laucius, and leon wong. 2006. high
accuracy retrieval with multiple nested ranker. in proceedings of the 29th annual international
acm sigir conference on research and development in information retrieval. acm, 437   444.

[132] donald metzler and w bruce croft. 2005. a markov random    eld model for term dependen-

cies. in proc. sigir. acm, 472   479.

[133] donald metzler, susan dumais, and christopher meek. 2007. similarity measures for short

segments of text. in european conference on information retrieval. springer, 16   27.

[134] tomas mikolov, kai chen, greg corrado, and jeffrey dean. 2013. ef   cient estimation of

word representations in vector space. arxiv preprint arxiv:1301.3781 (2013).

[135] tomas mikolov, martin kara     t, lukas burget, jan cernock`y, and sanjeev khudanpur. 2010.

recurrent neural network based language model.. in interspeech, vol. 2. 3.

[136] tomas mikolov, ilya sutskever, kai chen, greg s corrado, and jeff dean. 2013. distributed

representations of words and phrases and their compositionality. in proc. nips. 3111   3119.

[137] tomas mikolov, wen-tau yih, and geoffrey zweig. 2013. linguistic regularities in continuous

space word representations.. in hlt-naacl. citeseer, 746   751.

[138] bhaskar mitra. 2015. exploring session context using distributed representations of queries

and reformulations. in proc. sigir. acm, 3   12.

[139] bhaskar mitra and nick craswell. 2015. query auto-completion for rare pre   xes. in proc.

cikm. acm.

[140] bhaskar mitra and nick craswell. 2017. neural text embeddings for information retrieval.

in proc. wsdm. acm, 813   814.

[141] bhaskar mitra, fernando diaz, and nick craswell. 2017. learning to match using local and

distributed representations of text for web search. in proc. www. 1291   1299.

46

[142] bhaskar mitra, fernando diaz, and nick craswell. 2017. luandri: a clean lua interface to

the indri search engine. in proc. sigir. acm.

[143] bhaskar mitra, eric nalisnick, nick craswell, and rich caruana. 2016. a dual embedding

space model for document ranking. arxiv preprint arxiv:1602.01137 (2016).

[144] bhaskar mitra, milad shokouhi, filip radlinski, and katja hofmann. 2014. on user interac-

tions with query auto-completion. in proc. sigir. 1055   1058.

[145] bhaskar mitra, grady simon, jianfeng gao, nick craswell, and li deng. 2016. a proposal
for evaluating answer distillation from web data. in proceedings of the sigir 2016 webqa
workshop.

[146] volodymyr mnih, nicolas heess, alex graves, and others. 2014. recurrent models of visual

attention. in proc. nips. 2204   2212.

[147] volodymyr mnih, koray kavukcuoglu, david silver, andrei a rusu, joel veness, marc g
bellemare, alex graves, martin riedmiller, andreas k fidjeland, georg ostrovski, and others.
2015. human-level control through deep id23. nature 518, 7540 (2015),
529   533.

[148] guido f montufar, razvan pascanu, kyunghyun cho, and yoshua bengio. 2014. on the

number of linear regions of deep neural networks. in proc. nips. 2924   2932.

[149] frederic morin and yoshua bengio. 2005. hierarchical probabilistic neural network language

model.. in aistats, vol. 5. citeseer, 246   252.

[150] vinod nair and geoffrey e hinton. 2010. recti   ed linear units improve restricted boltzmann

machines. in proc. icml. 807   814.

[151] eric nalisnick, bhaskar mitra, nick craswell, and rich caruana. 2016. improving document

ranking with dual id27s. in proc. www.

[152] graham neubig, chris dyer, yoav goldberg, austin matthews, waleed ammar, antonios
anastasopoulos, miguel ballesteros, david chiang, daniel clothiaux, trevor cohn, and others.
2017. dynet: the dynamic neural network toolkit. arxiv preprint arxiv:1701.03980 (2017).

[153] gia-hung nguyen, lynda tamine, laure soulier, and nathalie bricon-souf. 2016. toward a

deep neural approach for knowledge-based ir. arxiv preprint arxiv:1606.07211 (2016).

[154] tri nguyen, mir rosenberg, xia song, jianfeng gao, saurabh tiwary, rangan majumder, and
li deng. 2016. ms marco: a human generated id17 dataset.
arxiv preprint arxiv:1611.09268 (2016).

[155] h palangi, l deng, y shen, j gao, x he, j chen, x song, and r ward. 2014. seman-
tic modelling with long-short-term memory for information retrieval. arxiv preprint
arxiv:1412.6629 (2014).

[156] hamid palangi, li deng, yelong shen, jianfeng gao, xiaodong he, jianshu chen, xinying
song, and rabab ward. 2015. deep sentence embedding using the long short term memory
network: analysis and application to information retrieval. arxiv preprint arxiv:1502.06922
(2015).

[157] liang pang, yanyan lan, jiafeng guo, jun xu, and xueqi cheng. 2016. a study of match-

pyramid models on ad-hoc retrieval. arxiv preprint arxiv:1606.04648 (2016).

[158] liang pang, yanyan lan, jiafeng guo, jun xu, shengxian wan, and xueqi cheng. 2016. text

matching as image recognition. in proc. aaai.

[159] greg pass, abdur chowdhury, and cayley torgeson. 2006. a picture of search. in proc.

infoscale. acm.

[160] jeffrey pennington, richard socher, and christopher d manning. 2014. glove: global vectors

for word representation. proc. emnlp 12 (2014), 1532   1543.

47

[161] jay m ponte and w bruce croft. 1998. a id38 approach to information retrieval.

in proc. sigir. acm, 275   281.

[162] pranav rajpurkar, jian zhang, konstantin lopyrev, and percy liang. 2016. squad: 100,000+

questions for machine comprehension of text. arxiv preprint arxiv:1606.05250 (2016).

[163] marc   aurelio ranzato, sumit chopra, michael auli, and wojciech zaremba. 2015. sequence

level training with recurrent neural networks. arxiv preprint arxiv:1511.06732 (2015).

[164] marc   aurelio ranzato, christopher poultney, sumit chopra, and yann lecun. 2006. ef   cient
learning of sparse representations with an energy-based model. in proceedings of the 19th
international conference on neural information processing systems. mit press, 1137   1144.

[165] matthew richardson, christopher jc burges, and erin renshaw. 2013. mctest: a challenge

dataset for the open-domain machine comprehension of text.. in emnlp, vol. 3. 4.

[166] stephen robertson, hugo zaragoza, and others. 2009. the probabilistic relevance framework:
bm25 and beyond. foundations and trends r(cid:13) in information retrieval 3, 4 (2009), 333   389.
[167] stephen e robertson, evangelos kanoulas, and emine yilmaz. 2010. extending average
precision to graded relevance judgments. in proceedings of the 33rd international acm sigir
conference on research and development in information retrieval. acm, 603   610.

[168] douglas lt rohde, laura m gonnerman, and david c plaut. 2006. an improved model of

semantic similarity based on lexical co-occurrence. commun. acm 8 (2006), 627   633.

[169] xin rong. 2014. id97 parameter learning explained. arxiv preprint arxiv:1411.2738

(2014).

[170] dwaipayan roy, debjyoti paul, mandar mitra, and utpal garain. 2016. using word embed-

dings for automatic id183. arxiv preprint arxiv:1606.07608 (2016).

[171] yossi rubner, carlo tomasi, and leonidas j guibas. 1998. a metric for distributions with
applications to image databases. in id161, 1998. sixth international conference on.
ieee, 59   66.

[172] magnus sahlgren. 2006. the word-space model: using distributional analysis to represent
syntagmatic and paradigmatic relations between words in high-dimensional vector spaces.
ph.d. dissertation. institutionen f  r lingvistik.

[173] hasim sak, andrew w senior, and fran  oise beaufays. 2014. long short-term memory
recurrent neural network architectures for large scale acoustic modeling.. in interspeech. 338   
342.

[174] ruslan salakhutdinov and geoffrey hinton. 2009. semantic hashing. international journal of

approximate reasoning 50, 7 (2009), 969   978.

[175] j  rgen schmidhuber. 2015. deep learning in neural networks: an overview. neural networks

61 (2015), 85   117.

[176] aliaksei severyn and alessandro moschitti. 2015. learning to rank short text pairs with

convolutional deep neural networks. in proc. sigir. acm, 373   382.

[177] yelong shen, xiaodong he, jianfeng gao, li deng, and gregoire mesnil. 2014. a latent
semantic model with convolutional-pooling structure for information retrieval. in proc. cikm.
acm, 101   110.

[178] yelong shen, xiaodong he, jianfeng gao, li deng, and gr  goire mesnil. 2014. learning
semantic representations using convolutional neural networks for web search. in proc. www.
373   374.

[179] shaohuai shi, qiang wang, pengfei xu, and xiaowen chu. 2016. benchmarking state-of-the-

art deep learning software tools. arxiv preprint arxiv:1608.07249 (2016).

48

[180] david silver, aja huang, chris j maddison, arthur guez, laurent sifre, george van
den driessche, julian schrittwieser, ioannis antonoglou, veda panneershelvam, marc lanctot,
and others. 2016. mastering the game of go with deep neural networks and tree search. nature
529, 7587 (2016), 484   489.

[181] amit singhal, chris buckley, and mandar mitra. 1996. pivoted document length id172.

in proc. sigir. acm, 21   29.

[182] richard socher, cliff c lin, chris manning, and andrew y ng. 2011. parsing natural scenes
and natural language with id56s. in proceedings of the 28th international
conference on machine learning (icml-11). 129   136.

[183] richard socher, jeffrey pennington, eric h huang, andrew y ng, and christopher d manning.
2011. semi-supervised recursive autoencoders for predicting sentiment distributions. in pro-
ceedings of the conference on empirical methods in natural language processing. association
for computational linguistics, 151   161.

[184] alessandro sordoni, yoshua bengio, hossein vahabi, christina lioma, jakob g simonsen, and
jian-yun nie. 2015. a hierarchical recurrent encoder-decoder for generative context-aware
query suggestion. arxiv preprint arxiv:1507.02221 (2015).

[185] alessandro sordoni, michel galley, michael auli, chris brockett, yangfeng ji, margaret
mitchell, jian-yun nie, jianfeng gao, and bill dolan. 2015. a neural network approach to
context-sensitive generation of conversational responses. arxiv preprint arxiv:1506.06714
(2015).

[186] trevor strohman, donald metzler, howard turtle, and w bruce croft. 2005. indri: a language
model-based search engine for complex queries. in proceedings of the international conference
on intelligent analysis, vol. 2. citeseer, 2   6.

[187] bob l sturm. 2014. a simple method to determine if a music information retrieval system is a

   horse   . ieee transactions on multimedia 16, 6 (2014), 1636   1644.

[188] sainbayar sukhbaatar, jason weston, rob fergus, and others. 2015. end-to-end memory

networks. in proc. nips. 2440   2448.

[189] fei sun, jiafeng guo, yanyan lan, jun xu, and xueqi cheng. 2015. learning word represen-

tations by jointly modeling syntagmatic and paradigmatic relations. in proc. acl.

[190] fei sun, jiafeng guo, yanyan lan, jun xu, and xueqi cheng. 2016. semantic regularities in

id194s. arxiv preprint arxiv:1603.07603 (2016).

[191] fei sun, jiafeng guo, yanyan lan, jun xu, and xueqi cheng. 2016. sparse id27s

using l1 regularized online learning. in proc. ijcai. 2915   2921.

[192] ilya sutskever, james martens, and geoffrey e hinton. 2011. generating text with recurrent
neural networks. in proceedings of the 28th international conference on machine learning
(icml-11). 1017   1024.

[193] christian szegedy, wei liu, yangqing jia, pierre sermanet, scott reed, dragomir anguelov,
dumitru erhan, vincent vanhoucke, and andrew rabinovich. 2015. going deeper with convo-
lutions. in proceedings of the ieee conference on id161 and pattern recognition.
1   9.

[194] christian szegedy, wojciech zaremba, ilya sutskever, joan bruna, dumitru erhan, ian
goodfellow, and rob fergus. 2013. intriguing properties of neural networks. arxiv preprint
arxiv:1312.6199 (2013).

[195] kai sheng tai, richard socher, and christopher d manning. 2015.

improved seman-
tic representations from tree-structured id137. arxiv preprint
arxiv:1503.00075 (2015).

[196] michael taylor, hugo zaragoza, nick craswell, stephen robertson, and chris burges. 2006.
optimisation methods for ranking functions with multiple parameters. in proceedings of the 15th
acm international conference on information and knowledge management. acm, 585   593.

49

[197] naftali tishby, fernando c pereira, and william bialek. 2000. the information bottleneck

method. arxiv preprint physics/0004057 (2000).

[198] seiya tokui, kenta oono, shohei hido, and justin clayton. 2015. chainer: a next-generation
open source framework for deep learning. in proceedings of workshop on machine learning
systems (learningsys) in the twenty-ninth annual conference on neural information processing
systems (nips).

[199] peter d turney and patrick pantel. 2010. from frequency to meaning: vector space models of

semantics. journal of arti   cial intelligence research 37 (2010), 141   188.

[200] christophe van gysel, maarten de rijke, and evangelos kanoulas. 2016. learning latent

vector spaces for product search. in proc. cikm. acm, 165   174.

[201] christophe van gysel, evangelos kanoulas, and maarten de rijke. 2017. pyndri: a python

interface to the indri search engine. arxiv preprint arxiv:1701.00749 (2017).

[202] oriol vinyals, charles blundell, tim lillicrap, daan wierstra, and others. 2016. matching

networks for one shot learning. in proc. nips. 3630   3638.

[203] oriol vinyals and quoc le. 2015. a neural conversational model.

arxiv:1506.05869 (2015).

arxiv preprint

[204] ellen m voorhees and donna harman. 2000. overview of the eighth text retrieval conference

(trec-8). (2000), 1   24.

[205] ellen m voorhees, donna k harman, and others. 2005. trec: experiment and evaluation in

information retrieval. vol. 1. mit press cambridge.

[206] ellen m voorhees and dawn m tice. 2000. building a id53 test collection.
in proceedings of the 23rd annual international acm sigir conference on research and
development in information retrieval. acm, 200   207.

[207] ivan vuli  c and marie-francine moens. 2015. monolingual and cross-lingual information

retrieval models based on (bilingual) id27s. in proc. sigir. acm, 363   372.

[208] shengxian wan, yanyan lan, jiafeng guo, jun xu, liang pang, and xueqi cheng. 2015.
a deep architecture for semantic matching with multiple positional sentence representations.
arxiv preprint arxiv:1511.08277 (2015).

[209] shengxian wan, yanyan lan, jun xu, jiafeng guo, liang pang, and xueqi cheng. 2016.
match-sid56: modeling the recursive matching structure with spatial id56. arxiv preprint
arxiv:1604.04378 (2016).

[210] xiaojun wan. 2007. a novel document similarity measure based on earth mover   s distance.

information sciences 177, 18 (2007), 3718   3730.

[211] xiaojun wan and yuxin peng. 2005. the earth mover   s distance as a semantic measure for
document similarity. in proceedings of the 14th acm international conference on information
and knowledge management. acm, 301   302.

[212] jason weston, antoine bordes, sumit chopra, alexander m rush, bart van merri  nboer,
armand joulin, and tomas mikolov. 2015. towards ai-complete id53: a set of
prerequisite toy tasks. arxiv preprint arxiv:1502.05698 (2015).

[213] jason weston, sumit chopra, and antoine bordes. 2014. memory networks. arxiv preprint

arxiv:1410.3916 (2014).

[214] qiang wu, christopher jc burges, krysta m svore, and jianfeng gao. 2010. adapting

boosting for information retrieval measures. information retrieval 13, 3 (2010), 254   270.

[215] long xia, jun xu, yanyan lan, jiafeng guo, and xueqi cheng. 2016. modeling document
novelty with neural tensor network for search result diversi   cation. in proc. sigir. acm,
395   404.

50

[216] yinglian xie and david o   hallaron. 2002. locality in search engine queries and its impli-
cations for caching. in infocom 2002. twenty-first annual joint conference of the ieee
computer and communications societies. proceedings. ieee, vol. 3. ieee, 1238   1247.

[217] kelvin xu, jimmy ba, ryan kiros, kyunghyun cho, aaron courville, ruslan salakhudinov,
rich zemel, and yoshua bengio. 2015. show, attend and tell: neural image id134
with visual attention. in international conference on machine learning. 2048   2057.

[218] rui yan, yiping song, and hua wu. 2016. learning to respond with deep neural networks for

retrieval-based human-computer conversation system. in proc. sigir. acm, 55   64.

[219] xiaohui yan, jiafeng guo, shenghua liu, xueqi cheng, and yanfeng wang. 2013. learn-
ing topics in short texts by non-negative id105 on term correlation matrix. in
proceedings of the siam international conference on data mining.

[220] liu yang, qingyao ai, jiafeng guo, and w bruce croft. 2016. anmm: ranking short
answer texts with attention-based neural matching model. in proceedings of the 25th acm
international on conference on information and knowledge management. acm, 287   296.

[221] yi yang, wen-tau yih, and christopher meek. 2015. wikiqa: a challenge dataset for

open-domain id53.. in emnlp. citeseer, 2013   2018.

[222] wen-tau yih, kristina toutanova, john c platt, and christopher meek. 2011. learning dis-
criminative projections for text similarity measures. in proceedings of the fifteenth conference
on computational natural language learning. association for computational linguistics,
247   256.

[223] wenpeng yin, hinrich sch  tze, bing xiang, and bowen zhou. 2015. abid98: attention-based
convolutional neural network for modeling sentence pairs. arxiv preprint arxiv:1512.05193
(2015).

[224] dani yogatama, phil blunsom, chris dyer, edward grefenstette, and wang ling. 2016.
learning to compose words into sentences with id23. arxiv preprint
arxiv:1611.09100 (2016).

[225] dong yu, adam eversole, mike seltzer, kaisheng yao, zhiheng huang, brian guenter,
oleksii kuchaiev, yu zhang, frank seide, huaming wang, and others. 2014. an introduction to
computational networks and the computational network toolkit. technical report. tech. rep.
msr, microsoft research, 2014, http://codebox/cntk.

[226] lantao yu, weinan zhang, jun wang, and yong yu. 2016. seqgan: sequence generative

adversarial nets with policy gradient. arxiv preprint arxiv:1609.05473 (2016).

[227] hamed zamani and w bruce croft. 2016. embedding-based query language models. in proc.

ictir. acm, 147   156.

[228] hamed zamani and w bruce croft. 2016. estimating embedding vectors for queries. in proc.

ictir. acm, 123   132.

[229] hugo zaragoza, nick craswell, michael j taylor, suchi saria, and stephen e robertson. 2004.

microsoft cambridge at trec 13: web and hard tracks.. in trec, vol. 4. 1   1.

[230] chengxiang zhai and john lafferty. 2001. a study of smoothing methods for language models

applied to ad hoc information retrieval. in proc. sigir. acm, 334   342.

[231] chiyuan zhang, samy bengio, moritz hardt, benjamin recht, and oriol vinyals. 2016. un-
derstanding deep learning requires rethinking generalization. arxiv preprint arxiv:1611.03530
(2016).

[232] han zhao, zhengdong lu, and pascal poupart. 2015. self-adaptive hierarchical sentence

model. arxiv preprint arxiv:1504.05070 (2015).

[233] guoqing zheng and jamie callan. 2015. learning to reweight terms with distributed

representations. in proc. sigir. acm, 575   584.

51

[234] xiangyang zhou, daxiang dong, hua wu, shiqi zhao, r yan, d yu, xuan liu, and h tian.

2016. multi-view response selection for human-computer conversation. emnlp   16 (2016).

[235] mu zhu. 2004. recall, precision and average precision. department of statistics and actuarial

science, university of waterloo, waterloo 2 (2004), 30.

[236] guido zuccon, bevan koopman, peter bruza, and leif azzopardi. 2015. integrating and eval-
uating neural id27s in information retrieval. in proceedings of the 20th australasian
document computing symposium. acm, 12.

52

