building intelligent systems with

large scale deep learning

jeff dean

google brain team

g.co/brain

presenting the work of many people at google

google brain team mission:
make machines intelligent. 
improve people   s lives.

how do we do this?

    conduct long-term research (>200 papers, see g.co/brain & g.co/brain/papers)
    unsupervised learning of cats, inception, id97, id195, deepdream, image 

captioning, neural translation, magenta, ml for robotics control, healthcare,    

    build and open-source systems like tensorflow (see tensorflow.org and 

https://github.com/tensorflow/tensorflow)

    collaborate with others at google and alphabet to get our work into 
the hands of billions of people (e.g., rankbrain for google search, gmail 
smart reply, google photos, google id103, google translate, waymo,    )

    train new researchers through internships and the google brain 

residency program

main research areas

    general machine learning algorithms and techniques
    computer systems for machine learning
    natural language understanding
    perception
    healthcare
    robotics
    music and art generation

main research areas

    general machine learning algorithms and techniques
    computer systems for machine learning
    natural language understanding
    perception
    healthcare
    robotics
    music and art generation

research.googleblog.com/2017/01
/the-google-brain-team-looking-ba
ck-on.html

1980s and 1990s

accuracy

neural networks

other approaches

scale (data size, model size)

1980s and 1990s

accuracy

more
compute

neural networks

other approaches

scale (data size, model size)

now

accuracy

more
compute

neural networks

other approaches

scale (data size, model size)

growth of deep learning at google

directories containing model description files

and many more . . . .

experiment turnaround time and research productivity
    minutes, hours:

    interactive research!  instant gratification!

    1-4 days

    tolerable
    interactivity replaced by running many experiments in parallel

    1-4 weeks

    high value experiments only
    progress stalls

    >1 month

    don   t even try

build the right tools

google confidential + proprietary (permission granted  to share within nist)

open, standard software for 
general machine learning

great for deep learning in 
particular

http://tensorflow.org/

first released nov 2015

and

https://github.com/tensorflow/tensorflow

apache 2.0 license

tensorflow goals

establish common platform for expressing machine learning ideas and systems

make this platform the best in the world for both research and production use

open source it so that it becomes a platform for everyone, not just google

tensorflow scaling

near-linear performance gains with each additional 8x nvidia   tesla   k80 
server added to the cluster

tensorflow supports many platforms

cpu

gpu

1st-gen tpu

cloud tpu

ios

android

raspberry pi

tensorflow supports many languages

java

2013

2011

2013

2013

2010

late 2015

ml is done in many places

tensorflow github stars by github user profiles w/ public locations 
source: http://jrvis.com/red-dwarf/?user=tensorflow&repo=tensorflow

tensorflow: a vibrant open-source community 

    rapid development, many outside contributors

    475+ non-google contributors to tensorflow 1.0
    15,000+ commits in 15 months
    many community created tutorials, models, translations, and projects

    ~7,000 github repositories with    tensorflow    in the title

    direct engagement between community and tensorflow team

    5000+ stack overflow questions answered
    80+ community-submitted github issues responded to weekly

    growing use in ml classes: toronto, berkeley, stanford, ...

google photos

[glacier]

google cloud platform

confidential & proprietary

24

24

reuse same model for completely 

different problems

same basic model structure

trained on different data,

useful in completely different contexts

example: given image     predict interesting pixels

www.google.com/sunroof

we have tons of vision problems

image search, streetview, satellite imagery, 

translation, robotics, self-driving cars, 

computers can now see

large implications for healthcare

google confidential + proprietary (permission granted  to share within nist)

medical imaging
using similar model for detecting diabetic 
retinopathy in retinal images

 

performance on par or slightly better than the median of 8 u.s. 
board-certified ophthalmologists (f-score of 0.95 vs. 0.91).
http://research.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html

computers can now see

large implications for robotics

google confidential + proprietary (permission granted  to share within nist)

combining vision with robotics

   deep learning for robots: learning 
from large-scale interaction   , google 
research blog, march, 2016

   learning hand-eye coordination for 
robotic grasping with deep learning 
and large-scale data collection   , 
sergey levine, peter pastor, alex 
krizhevsky, & deirdre quillen,
arxiv, arxiv.org/abs/1603.02199

self-supervised and end-to-end pose estimation

confidential + proprietary

tcn + self-supervision (no labels!)

confidential + proprietary

scientific applications of ml

google confidential + proprietary (permission granted  to share within nist)

predicting properties of molecules

aspirin

message 

passing neural 

net

toxic?

bind with a given protein?

quantum properties.

    chemical space is too big, so chemists often rely on virtual screening.
    machine learning can help search this large space.
    molecules are graphs, nodes=atoms and edges=bonds (and other stuff)
    message passing neural nets unify and extend many neural net models 

that are invariant to graph symmetries

    state of the art results predicting output of expensive quantum chemistry 

calculations, but ~300,000 times faster

https://research.googleblog.com/2017/04/predicting-properties-of-molecules-with.html and 
https://arxiv.org/abs/1702.05532 and https://arxiv.org/abs/1704.01212 (latter to appear in icml 2017)

measuring live cells with

image to image regression

   seeing more   

enabling technology: image to image regression

input

true depth

predicted depth

depth prediction on portrait data 

applications for camera effects

input

saturation

defocus

predict cellular markers 

from transmission microscopy?

human cancer cells / dic / nuclei (blue) 
and cell mask (green)

human ipsc neurons / phase contrast / 
nuclei (blue), dendrites (green), and 
axons (red)

scaling language understanding models

google confidential + proprietary (permission granted  to share within nist)

sequence-to-sequence model

[sutskever & vinyals & le nips 2014]

target sequence

 x

 y

 z

 q

deep lstm

v

 a

 b

 c

 d

__

 x

 y

 z

input sequence

sequence-to-sequence model: machine translation

[sutskever & vinyals & le nips 2014]

how

target sentence

v

quelle

est

votre

taille?

<eos>

input sentence

sequence-to-sequence model: machine translation

[sutskever & vinyals & le nips 2014]

how

tall

target sentence

v

quelle

est

votre

taille?

<eos>

how

input sentence

sequence-to-sequence model: machine translation

[sutskever & vinyals & le nips 2014]

target sentence

how

tall

are

v

quelle

est

votre

taille?

<eos>

how

tall

input sentence

sequence-to-sequence model: machine translation

[sutskever & vinyals & le nips 2014]

target sentence

how

tall

are

you?

v

quelle

est

votre

taille?

<eos>

how

tall

are

input sentence

sequence-to-sequence model: machine translation

[sutskever & vinyals & le nips 2014]

at id136 time:

id125 to choose most probable

over possible output sequences

v

quelle

est

votre

taille?

<eos>

input sentence

incoming email

smart reply

google research blog
- nov 2015

small 

feed-forward
neural network

activate

smart reply?
yes/no

incoming email

smart reply

google research blog
- nov 2015

small 

feed-forward
neural network

activate

smart reply?
yes/no

generated replies

deep recurrent
neural network

smart reply

april 1, 2009:  april fool   s day joke

nov 5, 2015:  launched real product

feb 1, 2016:  >10% of mobile inbox replies

sequence to sequence model
applied to google translate

google confidential + proprietary (permission granted  to share within nist)

https://arxiv.org/abs/1609.08144

google id4 model

one 
model 
replica: 
one 
machine 
w/ 8 
gpus

gpu8

encoder lstms

8 layers

+

+

+

gpu3

gpu2

gpu2

gpu1

y1

y2

</s>

softmax

decoder lstms

+

+

+

gpu8

gpu3

gpu2

gpu1

attention

x3

x2

</s>

<s>

y1

y3

model + data parallelism

parameters 
distributed across 
many parameter 
server machines

params

params

params

...

many 
replicas

...

id4

perfect translation

human
neural (gid4)
phrase-based (pbmt)

english 

     > 
french

english 

spanish 

french 

chinese 

> 

> 

> 

> 

english

english

chinese

english
translation model

closes gap between old system 
and human-quality translation 
by 58% to 87%

enables better communication 
across the world

6

5

4

3

2

1

0

y
t
i
l

 

a
u
q
n
o
i
t
a
s
n
a
r
t

l

english 

> 

spanish

research.googleblog.com/2016/09/a-neural-network-for-machine.html

backtranslation 
from japanese (en->ja->en)

phrase-based machine translation (old system):
kilimanjaro is 19,710 feet of the mountain covered with snow, and 
it is said that the highest mountain in africa. top of the west, 
   ngaje ngai    in the maasai language, has been referred to as the 
house of god. the top close to the west, there is a dry, frozen 
carcass of a leopard. whether the leopard had what the demand at 
that altitude, there is no that nobody explained.

google id4 (new system):
kilimanjaro is a mountain of 19,710 feet covered with snow, which is 
said to be the highest mountain in africa. the summit of the west is 
called    ngaje ngai    god    s house in masai language. there is a dried 
and frozen carcass of a leopard near the summit of the west. no one 
can explain what the leopard was seeking at that altitude.

automated machine learning

(   learning to learn   )

google confidential + proprietary (permission granted  to share within nist)

current:

solution = ml expertise + data + computation

current:

solution = ml expertise + data + computation

can we turn this into:

solution = data + 100x computation

???

early encouraging signs

trying multiple different approaches:

(1) rl-based architecture search
(2) model architecture evolution
(3) learn how to optimize

appeared in iclr 2017

idea: model-generating model trained via rl

(1) generate ten models
(2) train them for a few hours
(3) use loss of the generated models as reinforcement 

learning signal

arxiv.org/abs/1611.01578

cifar-10 image recognition task

   normal    lstm cell

penn tree bank id38 task

cell discovered by 
architecture search

learn2learn: learn the optimization update rule

neural optimizer search using id23,
irwan bello, barret zoph, vijay vasudevan, and quoc le.  to appear in icml 2017

more computational power needed

deep learning is transforming how we 

design computers

google confidential + proprietary (permission granted  to share within nist)

special computation properties

reduced
precision

ok

about 1.2
   about 0.6
about 0.7

not

1.21042
   0.61127
0.73989343

special computation properties

reduced
precision

ok

handful of 
specific 
operations

about 1.2
   about 0.6
about 0.7

not

1.21042
   0.61127
0.73989343

  

=

tensor processing unit v2

revealed in may at google i/o

google-designed device for neural net training and id136
    180 teraflops of computation, 64 gb of memory
    designed to be connected together

tpu pod 

64 2nd-gen tpus

11.5 petaflops

4 terabytes of memory

programmed via tensorflow

same program will run with only minor 
modifications on cpus, gpus, & tpus

will be available through google cloud

cloud tpu - virtual machine w/180 tflops tpuv2 device attached

making 1000 cloud tpus available for free to top researchers who are 

committed to open machine learning research

we   re excited to see what researchers will do with much more computation!

g.co/tpusignup 

machine learning in google cloud

custom ml models

pre-trained ml models

tensorflow

machine learning 

engine

vision api

speech api

jobs api

natural 

language api

translation 

api

video 

intelligence api

machine learning for

higher performance machine learning 

models

google confidential + proprietary (permission granted  to share within nist)

device placement with id23
placement model (trained 
via rl) gets graph as input 
+ set of devices, outputs 
device placement for each 
graph node

measured time 
per step gives 
rl reward signal

+19.3% faster vs. expert human for id4 model

+19.7% faster vs. expert human for inceptionv3

device placement optimization with id23, 
azalia mirhoseini, hieu pham, quoc le, mohammad norouzi, samy bengio, benoit steiner, yuefeng zhou, 
naveen kumar, rasmus larsen, and jeff dean, to appear in icml 2017, arxiv.org/abs/1706.04972

now

accuracy

more
compute

neural networks

other approaches

scale (data size, model size)

accuracy

future

more
compute

neural networks

other approaches

scale (data size, model size)

example queries of the future

which of these eye 

images shows 

symptoms of diabetic 

retinopathy?

describe this video 

in spanish

please fetch me a cup 
of tea from the kitchen

find me documents related to 

id23 for 

robotics and summarize them 

in german

conclusions

deep neural networks are making significant strides in 
speech, vision, language, search, robotics, healthcare,    

if you   re not considering how to use deep neural nets to solve 
your problems, you almost certainly should be

g.co/brain

more info about our work

g.co/brain

thanks!

