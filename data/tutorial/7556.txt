cs-tr-4956
umiacs-tr-2010-04
lamp-tr-153

april 2010

id150 for the uninitiated

philip resnik

eric hardisty

department of linguistics

institute for advanced computer studies

university of maryland

college park, md 20742-3275

resnik at umd.edu

department of computer science

institute for advanced computer studies

university of maryland

college park, md 20742-3275

hardisty at cs.umd.edu

abstract

this document is intended for computer scientists who would like to try out a markov chain monte
carlo (mcmc) technique, particularly in order to do id136 with bayesian models on problems related
to text processing. we try to keep theory to the absolute minimum needed, though we work through the
details much more explicitly than you usually see even in    introductory    explanations. that means we   ve
attempted to be ridiculously explicit in our exposition and notation.

after providing the reasons and reasoning behind id150 (and at least nodding our heads in the
direction of theory), we work through an example application in detail   the derivation of a gibbs sampler for
a na    ve bayes model. along with the example, we discuss some practical implementation issues, including
the integrating out of continuous parameters when possible. we conclude with some pointers to literature
that we   ve found to be somewhat more friendly to uninitiated readers.

keywords: id150, id115, na    ve bayes, bayesian id136,
tutorial

report documentation page

form approved

omb no. 0704-0188

public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and
maintaining the data needed, and completing and reviewing the collection of information. send comments regarding this burden estimate or any other aspect of this collection of information,
including suggestions for reducing this burden, to washington headquarters services, directorate for information operations and reports, 1215 jefferson davis highway, suite 1204, arlington
va 22202-4302. respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it
does not display a currently valid omb control number. 

1. report date 
apr 2010 

2. report type 

4. title and subtitle 
id150 for the uninitiated 

6. author(s) 

7. performing organization name(s) and address(es) 
university of maryland,department of linguistics,institute for advanced
computer studies,college park,md,20742-3275 

3. dates covered 
  00-00-2010 to 00-00-2010  

5a. contract number 
5b. grant number 
5c. program element number 
5d. project number 
5e. task number 
5f. work unit number 

8. performing organization
report number 

9. sponsoring/monitoring agency name(s) and address(es) 

10. sponsor/monitor   s acronym(s) 

11. sponsor/monitor   s report 
number(s) 

12. distribution/availability statement 
approved for public release; distribution unlimited 

13. supplementary notes 
14. abstract 
this document is intended for computer scientists who would like to try out a id115
(mcmc) technique, particularly in order to do id136 with bayesian models on problems related to text
processing. we try to keep theory to the absolute minimum needed, though we work through the details
much more explicitly than you usually see even in introductory" explanations. that means we   ve
attempted to be ridiculously explicit in our exposition and notation. after providing the reasons and
reasoning behind id150 (and at least nodding our heads in the direction of theory), we work
through an example application in detail|the derivation of a gibbs sampler for a na ve bayes model. along
with the example, we discuss some practical implementation issues, including the integrating out of
continuous parameters when possible. we conclude with some pointers to literature that we   ve found to be
somewhat more friendly to uninitiated readers. 

15. subject terms 
16. security classification of: 

a. report 

unclassified 

b. abstract 

unclassified 

c. this page 

unclassified 

17. limitation of 

abstract 
same as

report (sar) 

18. number
of pages 

19a. name of
responsible person 

23 

standard form 298 (rev. 8-98) 
prescribed by ansi std z39-18 

id150 for the uninitiated

philip resnik

eric hardisty

department of linguistics and

department of computer science and

institute for advanced computer studies

institute for advanced computer studies

university of maryland

college park, md 20742 usa

resnik at umd.edu

university of maryland

college park, md 20742 usa

hardisty at cs.umd.edu

abstract

this document is intended for computer scientists who would like to try out a id115
(mcmc) technique, particularly in order to do id136 with bayesian models on problems related to text
processing. we try to keep theory to the absolute minimum needed, though we work through the details
much more explicitly than you usually see even in    introductory    explanations. that means we   ve attempted
to be ridiculously explicit in our exposition and notation.

after providing the reasons and reasoning behind id150 (and at least nodding our heads in the
direction of theory), we work through an example application in detail   the derivation of a gibbs sampler for
a na    ve bayes model. along with the example, we discuss some practical implementation issues, including
the integrating out of continuous parameters when possible. we conclude with some pointers to literature
that we   ve found to be somewhat more friendly to uninitiated readers.

1

introduction

id115 (mcmc) techniques like id150 provide a principled way to approximate
the value of an integral.

1.1 why integrals?

ok, stop right there. many computer scientists, including a lot of us who focus in natural language processing,
don   t spend a lot of time with integrals. we spend most of our time and energy in a world of discrete events.
(the word bank can mean (1) a    nancial institution, (2) the side of a river, or (3) tilting an airplane. which
meaning was intended, based on the words that appear nearby?) take a look at manning and schuetze
[14], and you   ll see that the probabilistic models we use tend to involve sums, not integrals (the baum-welch
algorithm for id48s, for example). so we have to start by asking: why and when do we care about integrals?
one good answer has to do with id203 estimation.1 numerous computational methods involve
estimating the probabilities of alternative discrete choices, often in order to pick the single most probable
choice. as one example, the language model in an automatic id103 (asr) system estimates
the id203 of the next word given the previous context. as another example, many spam blockers use
features of the e-mail message (like the word viagra, or the phrase send this message to all your friends) to
predict the id203 that the message is spam.

sometimes we estimate probabilities by using maximimum likelihood estimation (id113). to use a standard
example, if we are told a coin may be unfair, and we    ip it 10 times and see hhhhtttttt (h=heads,
t=tails), it   s conventional to estimate the id203 of heads for the next    ip as 0.4. in practical terms,
id113 amounts to counting and then normalizing so that the probabilities sum to 1.

1this subsection is built around the very nice explication of bayesian id203 estimation by heinrich [7].

1

figure 1: id203 of generating the coin-   ip sequence hhhhtttttt, using di   erent values for
p (heads) on the x-axis. the value that maximizes the id203 of the observed sequence, 0.4, is the
maximum likelihood estimate (id113).

count(h)

count(h) + count(t)

=

4
10

= 0.4

(1)

formally, id113 produces the choice most likely to have generated the observed data.
in this case, the most natural model    has just a single parameter,   , namely the id203 of heads
(see figure 1).2 letting x = hhhhtttttt represent the observed data, and y the outcome of the next
coin    ip, we estimate

    m le = argmax
p (y|x )     p (y|    m le)

  

p (x|  )

(2)

(3)

on the other hand, sometimes we estimate probabilities using maximum a posteriori (map) estimation.

a map estimate is the choice that is most likely given the observed data. in this case,

    m ap = argmax

  

= argmax

  

= argmax

p (  |x )
p (x|  )p (  )

p (x )

p (x|  )p (  )

p (y|x )     p (y|    m ap )

  

(4)

(5)

in contrast to id113, map estimation applies bayes   s rule, so that our estimate (4) can take into account
prior knowledge about what we expect    to be in the form of a prior id203 distribution p (  ).3 so,

2speci   cally,    models each choice as a bernoulli trial, and the id203 of generating exactly this heads-tails sequence for
a given    is   4(1      )6. if you type plot[p^4(1-p)^6,{p,0,1}] into wolfram alpha, you get figure 1, and you can immediately
see that the curve tops out, i.e. the id203 of generating the sequence is highest, exactly when p = 0.4. con   rm this by
entering derivative of p^4(1-p)^6 and you   ll get 2
5 = 0.4 as the maximum. thanks to kevin knight for pointing out how
easy all this is using wolfram alpha. also see discussion in heinrich [7], section 2.1.

3we got to (4) from the desired posterior id203 by applying bayes   s rule and then ignoring the denominator since the

argmax doesn   t depend on it.

2

plot p^4 1!p ^6, p,0,1  inputinterpretation:plotp4 1!p 6p!0to1plot:0.20.40.60.81.00.00020.00040.00060.00080.00100.0012 generated by wolfram|alpha (www.wolframalpha.com) on september 24, 2009 from champaign, il.   wolfram alpha llc   a wolfram research company1e[f(z)] = (cid:88)

z   z

(cid:90)

(cid:90)

for example, we might believe that the coin    ipper is a scrupulously honest person, and choose a prior
distribution that is biased in favor of    = 0.5. the more heavily biased that prior distribution is, the more
evidence it will take to shake our pre-existing belief that the coin is fair.4

now, id113 and map estimates are both giving us the best estimate, according to their respective
de   nitions of    best.    but notice that using a single estimate     whether it   s     m le or     m ap     throws away
information. in principle,    could have any value between 0 and 1; might we not get better estimates if we
took the whole distribution p (  |x ) into account, rather than just a single estimated value for   ? if we do
that, we   re making use of all the information about    that we can wring from the observed data, x .

the way to take advantage of all that information is to calculate an expected value rather than an estimate
using the single best guess for   . recall that the expected value of a function f(z), when z is a discrete
variable, is

here z is the set of discrete values z can take, and p(z) is the id203 distribution over possible values

for z. if z is a continuous variable, the expected value is an integral rather than a sum:

f(z)p(z).

(6)

e[f(z)] =

f(z)p(z) dz.

(7)

for our example, z =   , the function f we   re interested in is f(z) = p (y|  ), and the distribution over
which we   re taking the expectation is p (  |x ), i.e. the whole distribution over possible values of    given that
we   ve observed x . that gives us the following expected value for the posterior id203 of y given x :

p (y|x ) =

p (y|  )p (  |x ) d  

where bayes   s rule de   nes

p (  |x ) = p (x|  )p (  )

p (x )

=

(cid:82)

p (x|  )p (  )
  p (x|  )p (  ) d  

.

(8)

(9)

notice that, unlike (3) and (5), equation (8) de   nes the posterior using a true equality, not an approx-
it takes fully into account our prior beliefs about what the value of    will be, along with the

imation.
interaction of those prior beliefs with observed evidence x .

equations (8) and (9) provide one compelling answer to the question we started with. why should
even discrete-minded computer scientists care about integrals? because even when the id203 space
is discrete, we often care about good estimates of posterior probabilities. computing integrals can help us
improve the parameter estimates in our models.5

4see http://www.math.uah.edu/stat/objects/experiments/betacoinexperiment.xhtml for a nice applet that lets you ex-
plore this idea. if you set a = b = 10, you get a prior strongly biased toward 0.5, and it   s hard to move the posterior too
far from that value even if you generate observed heads with id203 p = 0.8. if you set a = b = 2, there   s still a bias
toward 0.5 but it   s much easier to move the posterior o    that value. as a second pointer, see some nice, self-contained slides at
http://www.cs.cmu.edu/   lewicki/cp-s08/bayesian-id136.pdf.

5chris dyer (personal communication) points out you don   t have to be doing bayesian estimation to care about expected
values. for example, better ways to compute expected values can be useful in the e step of expectation-maximization algorithms,
which give you maximum likelihood estimates for models with latent variables. he also points out that for many models,
bayesian parameter estimation can be a whole lot easier to implement than em. the widely used giza++ implementation of
ibm model 3 (a probabilistic model used in id151 [12]) contains 2186 lines of code; chris implemented
a gibbs sampler for model 3 in 67 lines. on a related note, kevin knight   s excellent    bayesian id136 with tears: a tutorial
workbook for natural language researchers    [9] was written with goals very similar to our own, but from an almost completely
complementary angle: he emphasizes conceptual connections to em algorithms and focuses on the kinds of structured problems
you tend to see in natural language processing.

3

1.2 why sampling?

the trouble with integrals, of course, is that they can be very di   cult to calculate. the methods we learned
in calculus class are    ne for classroom exercises, but often cannot be applied to interesting problems in the
real world. indeed, analytical solutions to (8) and the denominator of (9) might be impossible to obtain, so
we might not be able to determine the exact form of p (  |x ). id150 allows us to sample from a
distribution that asymptotically follows p (  |x ) without having to explicitly calculate the integrals.

1.2.1 monte carlo: a circle, a square, and a bag of rice

id150 is an instance of a id115 technique. let   s start with the    monte
carlo    part. you can think of monte carlo methods as algorithms that help you obtain a desired value by
performing simulations involving probabilistic choices. as a simple example, here   s a cute, low-tech monte
carlo technique for estimating the value of    (the ratio of a circle   s circumference to its diameter).6

draw a perfect square on the ground. inscribe a circle in it     i.e. the circle and the square are centered
in exactly the same place, and the circle   s diameter has length identical to the side of the square. now take
a bag of rice, and scatter the grains uniformly at random inside the square. finally, count the total number
of grains of rice inside the circle (call that c), and inside the square (call that s).

you scattered rice at random. assuming you managed to do this pretty uniformly, the ratio between the
circle   s grains and the square   s grains (which include the circle   s) should approximate the ratio between the
area of the circle and the area of the square, so

c
s

      ( d
2 )2
d2

.

(10)

solving for   , we get        4c
s .
you may not have realized it, but we just solved a problem by approximating the values of integrals. the
2 )2, is the result of summing up an in   nite number of in   nitessimally small points;
true area of the circle,   ( d
similarly for the the true area d2 of the square. the more grains of rice we use, the better our approximation
will be.

1.2.2 markov chains: walking the right walk

in the circle-and-square example, we saw the value of sampling involving a uniform distribution, since the
grains of rice were distributed uniformly within the square. returning to the problem of computing expected
values, recall that we   re interested in ep(x)[f(x)] (equation 7), where we   ll assume that the distribution p(x)
is not uniform and, in fact, not easy to work with analytically.

figure 2 provides an example f(z) and p(z) for illustration. conceptually, the integral in equation (7)
sums up f(z)p(z) over in   nitely many values of z. but rather than touching each point in the sum exactly
once, here   s another way to think about it: if you sample n points z(0), z(1), z(2), . . . , z(n ) at random from
the id203 density p(z), then

ep(z)[f(z)] = lim
n      

1
n

n(cid:88)

t=1

f(z(t)).

(11)

that looks a lot like a kind of averaged value for f, which makes a lot of sense since in the discrete case
(equation 6) the expected value is nothing but a weighted average, where the weight for each value of z is
its id203.

notice, though, that the value in the sum is just f(z(t)), not f(z(t))p(z(t)) as in the integral in equation (7).
where did the p(z) part go? intuitively, if we   re sampling according to p(z), and count(z) is the number of

6we   re elaborating on the introductory example at http://en.wikipedia.org/wiki/monte carlo method.

4

figure 2: example of computing expectation ep(z)[f(z)].
(this    gure was adapted from page 7 of the
handouts for chris bishop   s presentation    nato asi: learning theory and practice   , leuven, july 2002,
http://research.microsoft.com/en-us/um/people/cmbishop/downloads/bishop-nato-2.pdf )

times we observe z in the sample, then 1
way the samples are drawn.7

n count(z) approaches p(z) as n        . so the p(z) is implicit in the
looking at equation (11), it   s clear that we can get an approximate value by sampling only a    nite number

of times, t :

ep(z)[f(z)]     1

t

t(cid:88)

t=1

f(z(t)).

(12)

progress! now we have a way to approximate the integral. the remaining problem is this: how do we

sample z(0), z(1), z(2), . . . , z(t ) according to p(z)?

there are a whole variety of ways to go about this; e.g., see bishop [2] chapter 11 for discussion of rejec-
tion sampling, adaptive rejection sampling, adaptive rejection metropolis sampling, importance sampling,
sampling-importance-sampling,... for our purposes, though, the key idea is to think of z   s as points in a
state space, and    nd ways to    walk around    the space     going from z(0) to z(1) to z(2) and so forth     so
that the likelihood of visiting any point z is proportional to p(z). figure 2 illustrates the reasoning visually:
walking around values for z, we want to spend our time adding values f(z) to the sum when p(z) is large,
e.g. devoting more attention to the space between z1 and z2, as opposed to spending time in less probable
portions of the space like the part between z2 and z3.

a walk of this kind can be characterized abstractly as follows:

1: z(0) := a random initial point
2: for t = 1 to t do
z(t+1) := g(z(t))
3:
4: end for

7okay     we   re playing a little fast and loose here by talking about counts: with z continuous, we   re not going to see two
identical samples z(i) = z(j), so it doesn   t really make sense to talk about counting how many times a value was seen. but we
did say    intuitively   , right?

5

p(z) z1   z2   z3   z4   f(z) z here g is a function that makes a probabilistic choice about what state to go to next according to an

explicit or implicit transition id203 ptrans(z(t+1)|z(0), z(1), . . . , z(t)).8
the part about probabilistic choices makes this a monte carlo technique. what will make it a markov
chain monte carlo technique is de   ning things so that the next state you visit, z(t+1), depends only on the
current state z(t). that is,

ptrans(z(t+1)|z(0), z(1), . . . , z(t)) = ptrans(z(t+1)|z(t)).

(13)

for you language folks, this is precisely the same idea as modeling word sequences using a bigram model,

where here we have states z instead of having words w.

we included the subscript trans in our notation, so that ptrans can be read as    transition id203   ,
in order to emphasize that these are state-to-state transition probabilities in a (   rst-order) markov model.
the heart of id115 methods is designing g so that the id203 of visiting a state z
will turn out to be p(z), as desired. this can be accomplished by guaranteeing that the chain, as de   ned by
the transition probabilities ptrans, meets certain conditions. id150 is one algorithm that meets
those conditions.9

1.2.3 the id150 algorithm

id150 is applicable in situations where z has at least two dimensions, i.e. each point z is really
z = (cid:104)z1, . . . , zk(cid:105), with k > 1. the basic idea in id150 is that, rather than probabilistically picking
the next state all at once, you make a separate probabilistic choice for each of the k dimensions, where each
choice depends on the other k     1 dimensions.10 that is, the probabilistic walk through the space proceeds
as follows:
1: z(0) := (cid:104)z(0)
2: for t = 1 to t do
3:
4:
5:
6: end for

k (cid:105)
1 , . . . , z(0)
for i = 1 to k do

    p (zi|z(t+1)

z(t+1)
i
end for

i+1, . . . , z(t)
k )

, . . . , z(t+1)

i   1 , z(t)

1

note that we can obtain the distribution we are sampling from by using the de   nition of conditional

id203:

p (zi|z(t+1)

1

, . . . , z(t+1)

i   1 , z(t)

i+1, . . . , z(t)

k ) =

p (z(t+1)

1

, . . . , z(t+1)

i   1 , z(t)

i
, . . . , z(t+1)

i   1 , z(t)

i+1, . . . , z(t)
, z(t)
k )
i+1, . . . , z(t)
k )

p (z(t+1)

1

.

(14)

i

notice that the only di   erence between the numerator and the denominator is that the numerator is the full
joint id203, including z(t)
(cid:104)z(t+1)

one full execution of the inner loop and you   ve just computed your new point z(t+1) = g(z(t)) =

is missing in the denominator. this will be important later.

, whereas z(t)

, . . . , z(t+1)

(cid:105).

you can think of each dimension zi as corresponding to a parameter or variable in your model. using
equation (14), we sample the new value for each variable according to its distribution based on the values
of all the other variables. during this process, new values for the variables are used as soon as you obtain
them. for the case of three variables:

k

1

i

    the new value of z1 is sampled conditioning on the old values of z2 and z3.
8we   re deliberately staying at a high level here. in the bigger picture, g might consider and reject one or more states before
   nally deciding to accept one and return it as the value for z(t+1). see discussions of the metropolis-hastings algorithm, e.g.
bishop [2], section 11.2.

9we told you we were going to keep theory to a minimum, didn   t we?
10there are, of course, variations on this basic scheme. for example,    blocked sampling    groups the variables into b < k

blocks and the variables in each block are sampled together based on the other b     1 blocks.

6

    the new value of z2 is sampled conditioning on the new value of z1 and the old value of z3.
    the new value of z3 is sampled conditioning on the new values of z1 and z2.

1.3 the remainder of this document

so, there you have it. id150 makes it possible to obtain samples from id203 distributions
without having to explicitly calculate the values for their marginalizing integrals, e.g. computing expected
values, by de   ning a conceptually straightforward approximation. this approximation is based on the idea
of a probabilistic walk through a state space whose dimensions correspond to the variables or parameters in
your model.

trouble is, from what we can tell, most descriptions of id150 pretty much stop there (if they   ve
even gone into that much detail). to someone relatively new to this territory, though, that   s not nearly far
enough to    gure out how to do id150. how exactly do you implement the    sampling from the
following distribution    part at the heart of the algorithm (equation 14) for your particular model? how do
you deal with continuous parameters in your model? how do you actually generate the expected values you   re
ultimately interested in (e.g. equation 8), as opposed to just doing the probabilistic walk for t iterations?
just as the    rst part of this document aimed at explaining why, the remainder aims to explain how.
in section 2, we take a very simple probabilistic model     na    ve bayes     and describe in considerable
(painful?) detail how to construct a gibbs sampler for it. this includes two crucial things, namely how to
employ conjugate priors and how to actually sample from conditional distributions per equation (14).

in section 3 we discuss how to actually obtain values from a gibbs sampler, as opposed to merely watching
it walk around the state space. (which might be entertaining, but wasn   t really the point.) our discussion
includes convergence and burn-in, auto-correlation and lag, and other practical issues.

in section 4 we conclude with pointers to other things you might    nd it useful to read, as well as an

invitation to tell us how we could make this document more accurate or more useful.

2 deriving a gibbs sampler for a na    ve bayes model

in this section we consider naive bayes models.11 let   s assume that items of interest are documents, that
the features under consideration are the words in the document, and that the document-level class variable
we   re trying to predict is a sentiment label whose value is either 0 or 1. for ease of reference, we present
our notation in figure 3. figure 4 describes the model as a    plate diagram   , to which we will refer when
describing the model.

2.1 modeling how documents are generated
we represent each document as a bag of words. given an unlabeled document wj, our goal is to pick
the best label lj = 0 or lj = 1. sometimes we will refer to 0 and 1 as classes instead of labels.
in
further discussion it will be convenient for us to refer to the sets of documents sharing the same label, so for
notational convenience we de   ne the sets c0 = {wj|lj = 0} and c1 = {wj|lj = 1}. the usual treatment
of these models is to equate    best    with    most probable   , and therefore our goal is to choose the label lj
for wj that maximizes p (lj|wj). applying bayes   s rule,

lj = argmax

l

p (l|wj) = argmax
= argmax

l

l

p (wj|l)p (l)

p (wj)

p (wj|l)p (l),

where the denominator p (wj) is omitted because it does not depend on l.

this application of bayes   s rule (the    bayes    part of    na    ve bayes   ) allows us to think of the model in
terms of a    generative story    that accounts for how documents are created. according to that story, we

11we assume the reader is familiar with naive bayes. for a refresher, see [14].

7

v
n
    1,     0
    
    i
cx
c
c0 (c1)
wj
wji
l
lj
rj
  i
  x,i
ncx(i)
c(   j)
l(   j)
c (   j)
  

0

(c (   j)

1

number of words in the vocabulary.
number of documents in the corpus.
hyperparameters of the beta distribution.
hyperparameter vector for the multinomial prior.
pseudocount for word i.
set of documents labeled x.
the set of all documents.
number of documents labeled 0 (1).
document j   s frequency distribution.
frequency of word i in document j.
vector of document labels.
label for document j.
number of words in document j.
id203 of word i.
id203 of word i from the distribution of class x.
number of times word i occurs in the set of all documents labeled x.
set of all documents except wj
vector of all document labels except lj
number of documents labeled 0 (1) except for wj
set of hyperparameters (cid:104)    1,     0,     (cid:105)

)

figure 3: notation.

figure 4: naive bayes plate diagram

8

2n    ljrjwjk           rst pick the class label of the document, lj; our model will assume that   s done by    ipping a coin whose
id203 of heads is some value    = p (lj = 1). we can express this a little more formally as

lj     bernoulli(  ).

then, for every one of the rj word positions in the document, we pick a word wi independently by
sampling randomly according to a id203 distribution over words. which id203 distribution we
use is based on the label lj of the document, so we   ll write them as   0 and   1. formally one would describe
the creation of document j   s bag of words as

wj     multinomial(rj,   ).

(15)

the assumption that the words are chosen independently is the reason we call the model    na    ve   .
notice that logically speaking, it made sense to describe the model in terms of two separate id203
distributions,   0 and   1, each one being a simple unigram distribution. the notation in (15) doesn   t explicitly
show that what happens in generating wj depends on whether lj was 0 or 1. unfortunately, that notational
choice seems to be standard, even though it   s less transparent.12 we indicate the existence of two   s by
including the 2 in the lower rectangle of figure 4, but many plate diagrams in the literature would not.
another, perhaps clearer way to describe the process would be

(16)
and that   s it: our    generative story    for the creation of a whole set of labeled documents (cid:104)wn, ln(cid:105),
according to the na    ve bayes model, is that this simple document-level generative story gets repeated n
times, as indicated by the n in the upper rectangle in figure 4.

wj     multinomial(rj,   lj ).

2.2 priors

well, ok, that   s not completely it. where did    come from? our generative story is going to assume that
before this whole process began, we also picked    randomly. speci   cally we   ll assume that    is sampled from
a beta distribution with parameters     1 and     0. these are referred to as hyperparameters because they are
parameters of a prior, which is itself used to pick parameters of the model. in figure 4 we represent these two
hyperparameters as a single two-dimensional vector      = (cid:104)    1,     0(cid:105). when     1 =     0 = 1, beta(    1,     0)
is just a uniform distribution, which means that any value for    is equally likely. for this reason we call
beta(1, 1) an    uninformed prior   .

similarly, where do   0 and   1 come from? just as the beta distribution can provide an uninformed prior
for a distribution making a two-way choice, the dirichlet distribution can provide an uninformed prior for
v -way choices, where v is the number of words in the vocabulary. let      be a v -dimensional vector where
the value of every dimension equals 1. if   0 is sampled from dirichlet(    ), every id203 distribution
over words will be equally likely. similarly, we   ll assume   1 is sampled from dirichlet(    ).13 formally

       beta(    )
       dirichlet(    )

choosing the beta and dirichlet distributions as priors for binomial and multinomial distributions, re-

spectively, helps the math work out cleanly. we   ll discuss this in more detail in section 2.4.2.

12the actual basis for removing the subscripts on the parameters    is that we assume the data from one class is independent
of the parameter estimation of all the other classes, so essentially when we derive the id203 expressions for one class the
others look the same [19].

13note that   0 and   1 are sampled separately. there   s no assumption that they are related to each other at all. also, it   s
worth noting that a dirichlet distribution is a beta distribution if the dimension v = 2. dirichlet generalizes beta in the same
way that multinomial generalizes binomial. now you see why we took the trouble to represent     1 and     0 as a 2-dimensional
vector     .

9

2.3 state space and initialization

following pedersen [17, 18], we   re going to describe the gibbs sampler in a completely unsupervised setting
where no labels at all are provided as training data. we   ll then brie   y explain how to take advantage of
labeled data.

state space. recall that the job of a gibbs sampler is to walk through an k-dimensional state space de   ned
by the random variables (cid:104)z1, z2, . . . zk(cid:105) in the model. every point in that walk is a collection (cid:104)z1, z2, . . . zk(cid:105)
of values for those variables.

in the na    ve bayes model we   ve just described, here are the variables that de   ne the state space.
    one scalar-valued variable   
    two vector-valued variables,   0 and   1
    binary label variables l, one for each of the n documents

we also have one vector variable wj for each of the n documents, but these are observed variables, i.e.
their values are already known (and which is why wjk is shaded in figure 4).

initialization. the initialization of our sampler is going to be very easy. pick a value    by sampling from
the beta(    1,     0) distribution. then, for each j,    ip a coin with success id203   , and assign label l(0)
    that is, the label of document j at the 0th iteration     based on the outcome of the coin    ip. similarly,
you also need to initialize   0 and   1 by sampling from dirichlet(    ).

j

2.4 deriving the joint distribution

recall that for each iteration t = 1 . . . t of sampling, we update every variable de   ning the state space by
sampling from its conditional distribution given the other variables, as described in equation (14).

here   s how we   re going to proceed:
    we will de   ne the joint distribution of all the variables, corresponding to the numerator in (14).
    we simplify our expression for the joint distribution.
    we use our    nal expression of the joint distribution to de   ne how to sample from the conditional

distribution in (14).

    we give the    nal form of the sampler as pseudocode.

2.4.1 expressing and simplifying the joint distribution
according to our model, the joint distribution for the entire document collection is p (c, l,   ,   0,   1;     1,     0,     ).
the semicolon indicates that the values to its right are parameters for this joint distribution. another way
to say this is that the variables to the left of the semicolon are conditioned on the hyperparameters given to
the right of the semicolon. using the model   s generative story, and, crucially, the independence assumptions
that are a part of that story, the joint distribution can decomposed into a product of several factors:14

p (  |    1,     0)p (l|  )p (  0|    )p (  1|    )p (c0|  0, l)p (c1|  1, l)

let   s look at each of these in turn.

14note that we can also obtain the products of the joint distribution directly from our graphical model, figure 4 by multiplying

together each latent variable conditioned on its parents.

10

    p (  |    1,     0). the    rst factor is the id203 of choosing this particular value of    given that     1
and     0 are being used as the hyperparameters of the beta distribution. by de   nition of the beta
distribution, that id203 is:

p (  |    1,     0) =

  (    1 +     0)
  (    1)  (    0)       1   1(1       )    0   1

(17)

and because

  (    1 +     0)
  (    1)  (    0)

is a constant that doesn   t depend on   , we can rewrite this as:

p (  |    1,     0) = c       1   1(1       )    0   1.

(18)
the constant c is a normalizing constant that makes sure p (  |    1,     0) sums to 1 over all   .   (x)
is the gamma function, a continuous-valued generalization of the factorial function. we could also
express (18) as

(19)
    p (l|  ). the second factor is the id203 of obtaining this speci   c sequence l of n binary labels,

p (  |    1,     0)           1   1(1       )    0   1.

given that the id203 of choosing label = 1 is   . that   s

n(cid:89)

p (l|  ) =

  ln(1       )(1   ln)

n=1

=   c1(1       )c0

(20)

(21)

recall from figure 3 that c0 and c1 are nothing more than the number of documents labeled 1 and
0 respectively.15

    p (  0|    ) and p (  1|    ). the third factors are the id203 of having sampled these particular choices

of word distributions, given that      was used as the hyperparameter of the dirichlet distribution.
since the    distributions are independent of each other, let   s make the notation a bit easier to read here
and consider each of them in isolation, allowing us to momentarily elide the subscript saying which one
is which. by the de   nition of the dirichlet distribution, the id203 of each word distribution is

p (  |    ) =

v(cid:89)

i=1

  ((cid:80)v
(cid:81)v
= c(cid:48) v(cid:89)
    v(cid:89)

i=1

i=1    i)
i=1   (    i)
      i   1

i

      i   1

i

      i   1

i

(22)

(23)

(24)

i=1

recall that     i denotes the value of vector        s ith dimension, and similarly,   i is the value for the
ith dimension of vector   , i.e. the id203 assigned by this distribution to the ith word in the
vocabulary. c(cid:48) is another id172 constant that we can discard by changing the equality to a
proportionality.

15of course we can represent these quantities given the variables we already have, but we de   ne these in the interests of

simplifying the equations somewhat.

11

v(cid:89)

    p (c0|  0, l) and p (c1|  1, l). these are the probabilities of generating the contents of the bags of

words in each of the two document classes.
generating the bag of words wn for document n depends on that document   s label, ln and the word
id203 distribution associated with that label,   ln (so   ln is either   0 or   1). for notational
simplicity, let   s let    =   ln:

p (wn|l,   ln) =

  wni
i

(25)

i=1

here   i is the id203 of word i in distribution   , and the exponent wni is the frequency of word i
in wn.
now, since the documents are generated independently of each other, we can multiply the value in (25)
for each of the documents in each class to get the combined id203 of all the observed bags of
words within a class:

p (cx|l,   x) = (cid:89)
v(cid:89)

n   cx

=

v(cid:89)

  wni
x,i

i=1
ncx (i)
x,i

  

(26)

(27)

where ncx(i) gives the count of word i in documents with class label x.

i=1

2.4.2 choice of priors and simplifying the joint id203 expression
so why did we pick the dirichlet distribution as our prior for   0 and   1 and the beta distribution as our
prior for   ? let   s look at what happens in the process of simplifying the joint distribution and see what
happens to our estimates of    (where this can be either   0 or   1) and    once we observe some evidence (i.e.
the words from a single document). using (19) and (21) from above:

p (  |l;     1,     0) = p (l|  )p (  |    1,     0)

    (cid:2)  c1(1       )c0(cid:3)(cid:2)      1   1(1       )    0   1(cid:3)

      c1+    1   1(1       )c0+    0   1

likewise, for    using (24) and (25) from above:

p (  |wn;     ) = p (wn|  )p (  |    )
      i   1

v(cid:89)

  wni
i

i

    v(cid:89)
    v(cid:89)

i=1

i=1

i=1

  wni+    i   1

i

if we use the words in all of the documents of a given class, then we have:

p (  x|cx;     )     v(cid:89)

ncx (i)+    i   1
  
x,i

i=1

12

(28)
(29)
(30)

(31)

(32)

v(cid:89)

notice that (30) is an unnormalized beta distribution, with parameters c1 +     1 and c0 +     0, and (32)
is an unnormalized dirichlet distribution, with parameter vector (cid:104)ncx(i) +     i(cid:105) for 1     i     v . when the
posterior id203 distribution is of the same family as the likelihood id203 distribution     that is, the
same functional form, just with di   erent arguments     it is said to be the conjugate prior of the posterior.
the beta distribution is the conjugate prior for binomial (and bernoulli) distributions and the dirichlet
distribution is the conjugate prior for multinomial distributions. also notice what role the hyperparameters
played   they are added just like observed evidence.
it is for this reason that the hyperparameters are
sometimes referred to as pseudocounts.
2.4.2) and let    = (cid:104)    1,     0,     (cid:105) we can express the full joint distribution as:

okay, so if we multiply together the individual factors from section 2.4.1 as simpli   ed above (in section

p (c, l,   ,   0,   1;   )       c1+    1   1(1       )c0+    0   1

nc0 (i)+    i   1

0,i

nc1 (i)+    i   1

1,i

  

  

(33)

2.4.3

integrating out   

i=1

having illustrated how to derive the joint distribution for a model, as in (33), it turns out that, for this
particular model, we can make our lives a little bit simpler: we can reduce the e   ective number of parameters
in the model by integrating our joint distribution with respect to   . this has the e   ect of taking all possible
values of    into account in our sampler, without representing it as a variable explicitly and having to sample
it at every iteration. intuitively,    integrating out    a variable is an application of precisely the same principle
as computing the marginal id203 for a discrete distribution. for example, if we have an expression for
c p (a, b, c). as
a result, c is    there    conceptually, in terms of our understanding of the model, but we don   t need to deal
with manipulating it explicitly as a parameter. with a continuous variable, the principle is the same, but
we integrate over all possible values of the variable rather than summing.

p (a, b, c), we can compute p (a, b) by summing over all possible values of c, i.e. p (a, b) =(cid:80)

so, we have

p (l, c,   0,   1;   ) =

p (l, c,   0,   1,   ;   ) d  

(cid:90)

  

(cid:90)

=
= p (  0|    )p (  1|    )p (c0|  0, l)p (c1|  1, l)

p (  |    1,     0)p (l|  )p (  0|    )p (  1|    )p (c0|  0, l)p (c1|  1, l) d  
p (  |    1,     0)p (l|  ) d  

  

(cid:90)

  

(34)

(35)

(36)

at this point let   s focus our attention on the integrand only and substitute the true distributions from

(17) and (21).(cid:90)

p (  |    1,     0)p (l|  ) d   =

  

=

(cid:90)

  (    1 +     0)
  (    1)  (    0)       1   1(1       )    0   1  c1(1       )c0 d  
  
  (    1 +     0)
  (    1)  (    0)

  c1+    1   1(1       )c0+    0   1 d  

(cid:90)

  

(37)

(38)

here   s where our use of conjugate priors pays o   . notice that the integrand in (38) is a beta distribution with
parameters c1 +     1 and c0 +     0. this means that the value of the integral is just the normalizing constant
for that distribution, which is easy to look up (e.g. in the entry for the beta distribution in wikipedia): the
normalizing constant for distribution beta(c1 +     1, c0 +     0) is

  (c0 + c1 +     1 +     0)
  (c1 +     1)  (c0 +     0)

13

(cid:90)

  

making that substitution in (38), and also substituting n = c0 + c1, we arrive at

p (  |    1,     0)p (l|  ) d   =

  (    1 +     0)
  (    1)  (    0)

  (n +     1 +     0)

  (c1 +     1)  (c0 +     0)

(39)

gives us

substituting (39) and the de   nitions of the id203 distributions from section 2.4.1 back into (36)

p (l, c,   0,   1;   )       (    1 +     0)
  (    1)  (    0)

  (n +     1 +     0)

  (c1 +     1)  (c0 +     0)

v(cid:89)

i=1

nc0 (i)+    i   1

0,i

  

  

nc1 (i)+    i   1

1,i

(40)

okay, so it would be reasonable at this point to ask,    i thought the point of integrating out    was to
simplify things, so why did we just    simplify    the joint expression by adding in a bunch of these gamma
functions everywhere?    good question   hold that thought until we derive the sampler.

2.5 building the gibbs sampler
the de   nition of a gibbs sampler speci   es that in each iteration we assign a new value to variable zi by
sampling from the conditional distribution

p (zi|z(t+1)

1

, . . . , z(t+1)

i   1 , z(t)

i+1, . . . , z(t)

r ).

so, for example, to assign the value of l(t+1)

1

, we need to compute this conditional distribution:16

p (l1|l(t)

2 , . . . , l(t)

n , c,   (t)

0 ,   (t)

1 ;   ),

to assign the value of l(t+1)

2

, we need to compute

p (l2|l(t+1)

1

, l(t)

3 , . . . , l(t)

n , c,   (t)

0 ,   (t)

1 ;   ),

and so forth for l(t+1)
similarly, to assign the value of   0 we need to sample from the conditional distribution

. to assign the value of   0 we need to compute

through l(t+1)

n

3

and, for   1,

p (  (t+1)

0

|l(t+1)

1

, l(t+1)

2

, . . . , l(t+1)

n

, c,   (t)

1 ;   ),

p (  (t+1)

1

|l(t+1)

1

, l(t+1)

2

, . . . , l(t+1)

n

, c,   (t+1)

0

;   ),

intuitively, at the start of an iteration t, we have a collection of all our current information at this
point in the sampling process. that information includes the word count for each document, the number of
documents labeled 0, the number of documents labeled 1, the word counts for all documents labeled 0, the
word counts for all documents labeled 1, the current label for each document, the current values of   0 and   1,
etc. when we want to sample the new label for document j, we temporarily remove all information (i.e. word
counts and label information) about this document from that collection of information. then we look at the
id155 that lj = 0 given all the remaining information, and the id155 that
lj = 1 given the same information, and we sample the new label l(t+1)
by choosing randomly according to
the relative weight of those two conditional probabilities. sampling to get the new values   (t+1)
and   (t+1)
operates according to the same principal.

0

1

j

16there   s no superscript on the bags of words c because they   re fully observed and don   t change from iteration to iteration.

14

2.5.1 sampling for document labels

okay, so how do we actually do the sampling? we   re almost there. as the    nal step in our journey, we show
how to select all of the new document labels lj and the new distributions   0 and   1 during each iteration
of the sampler. by de   nition of id155,

p (lj|l(   j), c(   j),   0,   1;   ) = p (lj, wj, l(   j), c(   j),   0,   1;   )

p (l(   j), c(   j),   0,   1;   )
p (l, c,   0,   1;   )

p (l(   j), c(   j),   0,   1;   )

=

(41)

(42)

where l(   j) are all the document labels except lj, and c(   j) is the set of all documents except wj. the
distribution is over two possible outcomes, lj = 0 and lj = 1.

notice that the numerator is just the complete joint id203 described in (40). in the denominator,
we have the same expression, minus all the information about document wj. therefore we can work out
what (42) should look like by considering the three factors in (40), one at a time. for each factor, we will
remind ourselves of what it looks like in the numerator (which includes wj), and work out what it should
look like in the denominator (excluding wj), for each of the two outcomes.

the    rst factor in (40),

  (    1 +     0)
  (    1)  (    0) ,

(43)

is very easy. it depends only on the hyperparameters, so removing information about wj has no impact
on its value. since it will be the same in both the numerator and denominator of (42), it will cancel out.
excellent! two factors to go.

let   s look at the second factor of (40). taking all documents into account including wj, this is

  (n +     1 +     0)

  (c1 +     1)  (c0 +     0) .

(44)

now, in order to compute the denominator of (40), we remove document wj from consideration. how does
that change things? it depends on what lj was during the previous iteration. whether lj = 0 or lj = 1
during the previous iteration, the corpus size is e   ectively reduced from n to n     1, and the size of one of
the document classes is smaller by one compared to its value in the numerator. if lj = 0 when we remove
= c0     1 and c (   j)
it, then we will have c (   j)
= c1. if lj = 1 during the previous iteration, then we will
and c (   j)
have c0 = c (   j)
= c1     1. in each case, removing wj only changes the information we know
about one class, which means that of the two terms in the denominator of this factor, one of them is going
to be the same in the numerator and the denominator of (42). that   s going to cause the terms from one of
the classes (the one wj did not belong to after the previous iteration) to cancel out.
= cx     1.17 if we use x

let x     {0, 1} be the outcome we   re considering, i.e. the one for which c (   j)
and reorganize the expression a bit, the second factor of (42) can be rewritten from

x

1

0

0

1

  (c1+    1)  (c0+    0)

  (n +    1+    0)
  (n +    1+    0   1)
(   j)
(   j)
0 +    0)  (c
1 +    1)

  (c

to

  (n +     1 +     0)  (cx +     x     1)
  (cx +     x)  (n +     1 +     0     1)

(45)

17crucially, note that x here is not l(t)

j , the label of document j at the previous iteration. we are pulling document j out
of the collection, e   ectively obviating our knowledge of its current label, and then constructing a distribution over the two
possibilities, lj = 0 and lj = 1. so we need for our expression to take x as a parameter, allowing us to build a id203
distribution by asking    what if x = 0?    and    what if x = 1?   .

15

using the fact that   (a + 1) = a  (a) for all a, we can simplify further to get

look, no more pesky gammas!

finally, the third factor in (40) is

cx +     x

n +     1 +     0     1

v(cid:89)

i=1

nc0 (i)+    i   1

0,i

  

  

nc1 (i)+    i   1

1,i

.

(46)

(47)

when we look at what changes when we remove wj from consideration, in order to write the corresponding
expression in the denominator of (42), we see that it will behave in the same way as the second factor. one
of the classes remains unchanged, so one of the terms, either the   0 or   1, will cancel out when we do the
division. if, similar to above, we let x be the class for which c (   j)
= cx     1, then we can again capture
both cases in one expression:

x

v(cid:89)

i=1

  

ncx (i)+    i   1
  
x,i
(i)+    i   1
nc(   j)

x

x,i

=

v(cid:89)

i=1

  wji
x,i

(48)

v(cid:89)

i=1

with that, we have    nished working through the three factors in (40), which means we have worked out
how to express the numerator and denominator of (42), factor by factor. recombining the factors, we get
the following expression for the conditional distribution in (42): for x     {0, 1},

pr(lj = x|l(   j), c(   j),   0,   1;   ) =

cx +     x

n +     1 +     0     1

  wji
x,i

(49)

let   s take a step back and take a look at what this equation is telling us about how a label is chosen. its
   rst factor gives us an indication of how likely it is that lj = x considering only the distribution of the other
labels. so, for example, if the corpus had more class 0 documents than class 1 documents, this factor would
tend to push the label toward class 0. its second factor is like a word distribution       tting room.    we get an
indication of how well the words in wj       t    with each of the two distributions. if, for example, the words
from wj       t    better with distribution   0 (by having a larger value for the document id203 using   0)
then this factor will push the label toward class 0 as well.

so (   nally!) here   s the actual procedure to sample from the conditional distribution in (42):

1. let value0 = expression (49) with x = 0

2. let value1 = expression (49) with x = 1
3. let the distribution be (cid:104)

value0+value1 ,

value0

value0+value1(cid:105)

value1

4. select the value of l(t+1)

as the result of a bernoulli trial (weighted coin    ip) according to this distri-

j

bution.

2.5.2 sampling for   
we   ll follow a similar procedure to determine how to sample for new values of   0 and   1. since the estimation
of the two distributions is independent of one another, we   re going to omit the subscripts on    to make the
notation a bit easier to digest. just like above, we   ll need to derive an expression for the id203 of   
given all other variables, but our work is a bit simpler in this case. observe,

p (  |c, l;   )     p (c, l|  )p (  |  )

(50)

16

furthermore, recall that, since we used conjugate priors, this posterior, like the prior, works out to be a
dirichlet distribution. we actually derived the full expression in section 2.4.2, but we don   t need the full
expression here. all we need to do to sample a new distribution is to make another draw from a dirichlet
distribution, but this time with parameters ncx(i) +     i for each i in v . for notational convenience, let   s
de   ne the v dimensional vector t such that each ti = ncx(i) +     i, where x is again either 0 or 1 depending
on which    we are resampling. we then sample a new    as:
       dirichlet(t)

(51)

how do you actually implement sampling from a new dirichlet distribution? to sample a random vector
a = (cid:104)a1, . . . , av (cid:105) from the v -dimensional dirichlet distribution with parameters (cid:104)  1, . . . ,   v (cid:105), one fast way
is to draw v independent samples y1, . . . , yv from gamma distributions, each with density

and then set ai = yi/(cid:80)v

gamma(  i, 1) = y  i   1
  (  i)

i

e   yi

,

(52)

j=1 yj (i.e., just normalize each of the gamma distribution draws).18

2.5.3 taking advantage of documents with labels

using labeled documents is relatively painless:
just don   t sample lj for those documents! always keep
lj equal to the observed label. the documents will e   ectively serve as    ground truth    evidence for the
distributions that created them. since we never sample for their labels, they will always contribute to the
counts in (49) and (51) and will never be subtracted out.

2.5.4 putting it all together

initialization. de   ne the priors as in section 2.2 and initialize them as described in section 2.3.

for j := 1 to n do

1: for t := 1 to t do
2:
3:
4:
5:

if j is not a training document then

subtract j   s word counts from the total word counts of whatever class it   s currently a member of
subtract 1 from the count of documents with label lj
assign a new label l(t+1)
add 1 to the count of documents with label l(t+1)
add j   s word counts to the total word counts for class l(t+1)

to document j as described at the end of section 2.5.1

j

j

j

end if
end for
t0 := vector of total word counts from class 0, including pseudocounts
  0     dirichlet(t0), as described in section 2.5.2
t1 := vector of total word counts from class 1, including pseudocounts
  1     dirichlet(t1), as described in section 2.5.2

6:

7:

8:
9:
10:
11:
12:
13:
14:
15: end for

sampling iterations. notice that as soon as a new label for lj is assigned, this changes the counts that
will a   ect the labeling of the subsequent documents. this is, in fact, the whole principle behind a gibbs
sampler!

that concludes the discussion of how sampling is done. we   ll see how to get from the output of the

sampler to estimated values for the variables in section 3.

18for details, see http://en.wikipedia.org/wiki/dirichlet distribution (version of april 12, 2010).

17

2.6 optional: a note on integrating out continuous parameters

at this point you might be asking yourself why we were able to integrate out the continuous parameter   
from our model, but did not do something similar with the two word distributions   0 and   1. the idea of
doing this even looks promising, but there   s a subtle problem that will get us into trouble and end up leaving
us with an expression    lled with    functions that will not cancel out. let us go through the derivations and
see where it leads us. if you follow this piece of the discussion, then you really understand the details!19

our goal here would be to obtain the id203 that a document was generated from the same dis-
tribution that generated the words of a particular class of documents, cx. we would then use this as a
replacement for the product in (48). we start    rst by calculating the id203 of making a single word
draw given the other words in the class, subtracting out the information about wj. in the equations that
follow there   s an implicit (   j) superscript on all of the counts. if we let wk denote the word at some position
k in wj then,

pr(wk = y|c(   j)

x

;     ) =

=

=

=

=

=

x

   

   

  x,y

(cid:90)
pr(wk = y|  ) pr(  |c(   j)
  ((cid:80)v
(cid:90)
(cid:81)v
i=1ncx(i) +     i)
  ((cid:80)v
i=1   (ncx(i) +     i)
(cid:81)v
i=1ncx(i) +     i)
  ((cid:80)v
i=1   (ncx(i) +     i)
(cid:81)v
i=1ncx(i) +     i)
  ((cid:80)v
i=1   (ncx(i) +     i)
(cid:81)v
i=1ncx(i) +     i)
i=1   (ncx(i) +     i)
(cid:80)v
ncx(y) +     y
i=1ncx(i) +     i

  

   

;     )d  

v(cid:89)
v(cid:89)

  

  

i=1

  x,y

ncx (i)+    i   1
x,i

ncx (i)+    i   1
x,i

(cid:90)
(cid:90)
v(cid:89)
  (ncx(y) +     y + 1)(cid:81)v

ncx (y)+    y
x,y

i=1   i(cid:54)=y

i=1

  

   

  ((cid:80)v

d  

d  

ncx (i)+    i   1
x,i

d  

i=1   i(cid:54)=y   (ncx(i) +     i)

i=1ncx(i) +     i + 1)

(53)

(54)

(55)

(56)

(57)

(58)

case. the set     is the id203 simplex of   , namely the set of all    such that(cid:80)

the process we use is actually the same as the process used to integrate   , just in the multidimensional
i   i = 1. we get to (54)
by substitution from the formulas we derived in section 2.4.2, then (55) by factoring out the id172
constant for the dirichlet distribution, since it is constant with respect to   . note that the integrand of
(56) is actually another dirichlet distribution, so its integral is its id172 constant (same reasoning
as before). we substitute this in to obtain (57). using the property of    that   (x + 1) = x  (x) for all x, we
can again cancel all of the    terms.

at this point, even though we have a simple and intuitive result for the id203 of drawing a single
word from a dirichlet, we actually need the id203 of drawing all words in wj from the dirichlet
distribution. what we   d really like to do is assume that the words within a particular document are drawn
from the same distribution, and just calculate the id203 of wj by multiplying (58) over all words in the
vocabulary. but we cannot do that, since, without the values of    being known, we cannot make independent
draws from a dirichlet distribution since our draws have an e   ect on what our estimate of    is!

we can see this two di   erent ways. first, instead of drawing one word in equation (53), do the derivation
by drawing two words at a time.20 you   ll    nd that once you hit (57), you   ll have an extra set of gamma
functions that won   t cancel out nicely. the second way to see it is actually by looking at the plate diagram
for the model, figure 4. each    e   ectively has rj arrows coming out of it to individual words, so every

19the authors thank wu ke, who really understood the details, for pointing out our error in an earlier version of this document

and providing the illustrating example we go through next.
20this is what wu ke did to demonstrate his point to us.

18

word within a document is in the markov blanket of the others; therefore we can   t assume that the words
are independent without a    xed   .21

the lesson here is to be careful when you integrate out parameters. if you   re doing a single draw from a
multinomial, then integrating out a continuous parameter can make the sampler simpler, since you won   t have
to sample for it at every iteration. if, on the other hand, you do multiple draws from the same multinomial,
integration (although possible) will result in an expression that involves gamma functions. calculating
gamma functions is undesirable since they are computationally expensive, so they can slow down a sampler
signi   cantly.

3 producing values from the output of a gibbs sampler

the initialization and sampling iterations in the id150 algorithm will produce values for each of
the variables, for iterations t = 1, 2, . . . , t . in theory, the approximated value for any variable zi can simply
be obtained by calculating:

t(cid:88)

t=1

1
t

z(t)
i

,

(59)

as discussed in equation (12). however, expression (59) is not always used directly. there are several
additional details to note that are a part of typical sampling practice.22

convergence and burn-in iterations. depending on the values chosen during the initialization step,
r (cid:105) are all
it may take some time for the gibbs sampler to reach a point where the points (cid:104)z(t)
coming from the stationary distribution of the markov chain, which is an assumption of the approximation
in (59). in order to avoid the estimates being contaminated by the values at iterations before that point,
some practitioners generally discard the values at iterations t < b, which are referred to as the    burn-in   
iterations, so that the average in (59) is taken only over iterations b + 1 through t .23

2 , . . . z(t)

1 , z(t)

autocorrelation and lag. the approximation in (59) assumes the samples for zi are independent, but
we know they   re not, because they were produced by a process that conditions on the previous point in the
chain to generate the next point. this is referred to as autocorrelation (sometimes serial autocorrelation),
i.e. correlation between successive values in the data.24 in order to avoid autocorrelation problems (so that
the chain    mixes well   ), many implementations of id150 average only every lth value, where l is
referred to as the lag.25 in this context, jordan boyd-graber (personal communication) also recommends
looking at neal   s [15] discussion of likelihood as a metric of convergence.

21the markov blanket of a node in a graphical model consists of that node   s parents, its children, and the coparents of its

children. [16].

22jason eisner (personal communication) argues, with some support from the literature, that burn-in, lag, and multiple
chains are in fact unnecessary and it is perfectly correct to do a single long sampling run and keep all samples. see [4, 5],
mackay ([13], end of section 29.9, page 381) and koller and friedman ([10], end of section 12.3.5.2, page 522).

23as far as we can tell, there is no principled way to choose the    right    value for b in advance. there are a variety of
ways to test for convergence, and to measure autocorrelation; see, e.g., brian j. smith,    boa: an r package for mcmc
output convergence assessment and posterior id136   , journal of statistical software, november 2007, volume 21, issue
11, http://www.jstatsoft.org/ for practical discussion. however, from what we can tell, many people just choose a really big
value for t , pick b to be large also, and assume that their samples are coming from a chain that has converged.
24lohninger [11] observes that    most inferential tests and modeling techniques fail if data is autocorrelated   .
25again, the choice of l seems to be more a matter of art than science: people seem to look at plots of the autocorrelation
for di   erent values of l and use a value for which the autocorrelation drops o    quickly. the autocorrelation for variable zi
with lag l is simply the correlation between the sequence z(t)
. which correlation function is used
seems to vary.

and the sequence z(t   l)

i

i

19

multiple chains. as is the case for many other stochastic algorithms (e.g. expectation maximization as
used in the forward-backward algorithm for id48s), people often try to avoid sensitivity to the starting
point chosen at initialization time by doing multiple runs from di   erent starting points. for id150
and other id115 methods, these are referred to as    multiple chains   .26

hyperparameter sampling. rather than simply picking hyperparameters, it is possible, and in fact
often critical, to assign their values via sampling (boyd-graber, personal communication). see, e.g., wallach
et al. [20] and escobar and west [3].

4 conclusions

the main point of this document has been to take some of the mystery out of id150 for computer
scientists who want to get their hands dirty and try it out. like any other technique, however, caveat lector:
using a tool with only limited understanding of its theoretical foundations can produce undetected mistakes,
misleading results, or frustration.

as a    rst step toward getting further up to speed on the relevant background, ted pedersen   s [18]
doctoral dissertation has a very nice discussion of parameter estimation in chapter 4, including a detailed
exposition of an em algorithm for na    ve bayes and his own derivation of a na    ve bayes gibbs sampler that
highlights the relationship to em. (he works through several iterations of each algorithm explicitly, which
in our opinion merits a standing ovation.) the ideas introduced in chapter 4 are applied in pedersen and
bruce [17]; note that the brief description of the gibbs sampler there makes an awful lot more sense after
you   ve read pedersen   s dissertation chapter.

we also recommend gregor heinrich   s [7]    parameter estimation for text analysis.    heinrich presents
fundamentals of bayesian id136 starting with a nice discussion of basics like maximum likelihood es-
timation (id113) and maximum a posteriori (map) estimation, all with an eye toward dealing with text.
(we followed his discussion closely above in section 1.1.) also, his is one of the few papers we   ve been
able to    nd that actually provides pseudo-code for a gibbs sampler. heinrich discusses in detail gibbs
sampling for the widely discussed id44 (lda) model, and his corresponding code is
at http://www.arbylon.net/projects/ldagibbssampler.java.

for a textbook-style exposition, see bishop [2]. the relevant pieces of the book are a little less stand-
alone than we   d like (which makes sense for a course on machine learning, as opposed to just trying to dive
straight into a speci   c topic); chapter 11 (sampling methods) is most relevant, though you may also    nd
yourself referring back to chapter 8 (id114).

those ready to dive into the topic of id115 in more depth might want to start
[1]. we and andrieu et al. appear to di   er somewhat on the semantics of the word

with andrieu et al.
   introduction,    which is one of the reasons the document you   re reading exists.

finally, for people interested in the use of id150 for structured models in nlp (e.g. parsing),
the right place to start is undoubtedly kevin knight   s excellent    bayesian id136 with tears: a tutorial
workbook for natural language researchers    [9], after which you   ll be equipped to look at johnson, gri   ths,
and goldwater [8].27 the leap from the models discussed here to those kinds of models actually turns out
to be a lot less scary than it appears at    rst. the main thing to observe is that in the crucial sampling
step (equation (14) of section 1.2.3), the denominator is just the numerator without z(t)
, the variable whose
new value you   re choosing. so when you   re sampling conditional distributions (e.g. sections 2.5.1   2.5.4)
in more complex models, the basic idea will be the same: you subtract out counts related to the variable
you   re interested in based on its current value, compute proportions based on the remaining counts, then

i

26again, there seems to be as much art as science in whether to use multiple chains, how many, and how to combine them to
get a single output. chris dyer (personal communication) reports that it is not uncommon simply to concatenate the chains
together after removing the burn-in iterations.

27as an aside, our travels through the literature in writing this document led to an interesting early use of id150
with id18s: grate et al. [6]. johnson et al. had not come across this when they wrote their seminal paper introducing gibbs
sampling for pid18s to the nlp community (and in fact a search on scholar.google.com turned up no citations in the nlp
literature). mark johnson (personal communication) observes that the    local move    gibbs sampler grate et al. describe is
specialized to a particular pid18, and it   s not clear how to generalize it to arbitrary pid18s.

20

pick probabilistically based on the result, and    nally add counts back in according to the probabilistic choice
you just made.

acknowledgments

the creation of this document has been supported in part by the national science foundation (award iis-
0838801), the gale program of the defense advanced research projects agency (contract no. hr0011-
06-2-001), and the o   ce of the director of national intelligence (odni), intelligence advanced research
projects activity (iarpa), through the army research laboratory. all statements of fact, opinion, or
conclusions contained herein are those of the authors and should not be construed as representing the o   cial
views or policies of nsf, darpa, iarpa, the odni, the u.s. government, the university of maryland,
nor of the resnik or hardisty families or any of their close relations, friends, or family pets.

the authors are grateful to jordan boyd-graber, bob carpenter, chris dyer, jason eisner, john gold-
smith, kevin knight, mark johnson, nitin madnani, neil parikh, sasa petrovic, william schuler, prithvi
sen and wu ke (so far!) for helpful comments and/or catching glitches in the manuscript. extra thanks
to jordan boyd-graber for many helpful discussions and extra assistance with plate diagrams, and to jay
resnik for his help debugging the latex document.

references

[1] andrieu, freitas, doucet, and jordan. an introduction to mcmc for machine learning. machine

learning, 50:5   43, 2003.

[2] c. bishop. pattern recognition and machine learning. springer, 2006.

[3] m. d. escobar and m. west. bayesian density estimation and id136 using mixtures. journal of the

american statistical association, 90:577   588, june 1995.

[4] c. geyer. burn-in is unnecessary, 2009. http://www.stat.umn.edu/   charlie/mcmc/burn.html, down-

loaded october 18, 2009.

[5] c. geyer. one long run, 2009. http://www.stat.umn.edu/   charlie/mcmc/one.html.
[6] l. grate, m. herbster, r. hughey, d. haussler, i. s. mian, and h. noller. rna modeling using gibbs

sampling and stochastic id18s. in ismb, pages 138   146, 1994.

[7] g. heinrich. parameter estimation for text analysis. technical note version 2.4, vsonix gmbh and

university of leipzig, august 2008. http://www.arbylon.net/publications/text-est.pdf.

[8] m. johnson, t. gri   ths, and s. goldwater. bayesian id136 for pid18s via markov chain monte
in human language technologies 2007: the conference of the north american chapter of
carlo.
the association for computational linguistics; proceedings of the main conference, pages 139   146,
rochester, new york, april 2007. association for computational linguistics.

[9] k. knight. bayesian id136 with tears: a tutorial workbook for natural language researchers, 2009.

http://www.isi.edu/natural-language/people/bayes-with-tears.pdf.

[10] d. koller and n. friedman. probabilistic id114: principles and techniques. mit press,

2009.

[11] h.

lohninger.

teach/me

data

analysis.

http://www.vias.org/tmdatanaleng/cc corr auto 1.html.

springer-verlag,

1999.

[12] a. lopez. id151. acm computing surveys, 40(3):1   49, august 2008.

[13] d. mackay.

2003.

id205, id136, and learning algorithms. cambridge university press,

21

[14] c. manning and h. schuetze. foundations of statistical natural language processing. mit press, 1999.

[15] r. m. neal. probabilistic id136 using id115 methods. technical report crg-

tr-93-1, university of toronto, 1993. http://www.cs.toronto.edu/   radford/ftp/review.pdf.

[16] j. pearl. probabilistic reasoning in intelligent systems: networks of plausible id136. morgan kauf-

mann publishers inc., san francisco, ca, usa, 1988.

[17] t. pedersen. knowledge lean id51. in aaai/iaai, page 814, 1997.

[18] t. pedersen. learning probabilistic models of id51. phd thesis, southern

methodist university, 1998. http://arxiv.org/abs/0707.3972.

[19] s. theodoridis and k. koutroumbas. pattern recognition, 4th ed. academic press, 2009.

[20] h. wallach, c. sutton, and a. mccallum. bayesian modeling of dependency trees using hierarchical
pitman-yor priors. in icml workshop on prior knowledge for text and language processing, 2008.

22

