course notes for

bayesian models for machine learning

john paisley

department of electrical engineering

columbia university

fall 2016

1

contents

contents

id203 review, bayes rule, conjugate priors

bayesian id75, bayes classi   ers,

predictive distributions

laplace approximation, id150, id28,

id105

em algorithm, probit regression

em to variational id136

variational id136,    nding optimal distributions

id44, exponential families

conjugate exponential family models, scalable id136

gaussian mixture models

bayesian nonparametric id91

id48

poisson id105

2

3

10

22

32

45

57

67

77

87

98

108

118

2

eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture1,9/8/2016instructor:johnpaisley      bayesrule   popsoutofbasicmanipulationsofid203distributions.let   sreachitthroughaverysimpleexample.examplecallthisentirespace   aiistheithcolumn(de   nedarbitrarily)biistheithrow(alsode   nedarbitrarily)   wehavepointslyinginthisspace.wepickoneofthesepointsuniformlyatrandom.   inthiscase,calculatingprobabilitiesissimplyamatterofcounting.p(x   a1)=#a1#   ,p(x   b1)=#b1#      whataboutp(x   a1|x   b1)?thisistheid203thatx   a1giventhatiknowx   b1.thisiscalledaconditionalid203.p(x   a1|x   b1)=#(a1   b1)#b1=#(a1   b1)#   #   #b1=p(x   a1&x   b1)p(x   b1)1   we   vesimplymultipliedanddividedbythesamething,butalreadywe   vemadeageneralstate-ment.amoregeneralstatement   letaandbbetwoevents,thenp(a|b)=p(a,b)p(b)   p(a|b)p(b)=p(a,b)   wehavesomenamesfortheseterms,p(a|b):conditionalid203distributionp(a,b):jointid203distributionp(b):marginalid203distribution   thislastonecouldbetricky,sinceit   salsojust   theid203ofb.   however,wecanusethesamecountingapproachasbeforetointerpretp(b)asadistributionarrivedatbyintegrating(ormarginalizing)somethingout.fromthepreviousexample,p(b)=#b#   =p3i=1#(ai   b)#   =3xi=1#(ai   b)#   =3xi=1p(ai,b)   sidenote:ingeneral,thesummationandeachaihaveaverystrictrequirement.eachaihastobedisjoint(nooverlaps)andtheunionofalltheaihastoequal   (theentirespace).gettingtobayesrule   we   reafeweasystepsaway.weshowedthatp(a,b)=p(a|b)p(b)bysymmetrywecouldjustaseasilyhaveshownthatp(a,b)=p(b|a)p(a)andtherefore,p(a|b)=p(b|a)p(a)p(b)=p(b|a)p(a)pip(ai,b)=p(b|a)p(a)pip(b|ai)p(ai)   thisequalityiscalledbayesrule.asyoucansee,ithasafewwaysitcanbewritten.bayesrule   andsowehavethatp(a|b)=p(b|a)p(a)p(b)2   thesevalueseachhaveaname:posterior=likelihood  priorevidence   imaginethatwedon   tknowa,butwegetsomeinformationaboutitintheformofb.bayesruletellsusaprincipledwaytoincorporatethisinformationinourbeliefabouta.example(medicaltest)   wehavetwobinaryvalues,aandb:a=(1personhasdisease0nodiseaseb=(1testfordisease   positive   0testis   negative      apersontests   positive   forthedisease.what   stheid203hehasit?   wewantp(a=1|b=1).doesbayesrulehelp?bayesrulesaysthatp(a=1|b=1)=p(b=1|a=1)p(a=1)p(b=1)=p(b=1|a=1)p(a=1)p(b=1|a=1)p(a=1)+p(b=1|a=0)p(a=0)   imagewecanestimatethatp(b=1|a=1)=0.95,p(b=1|a=0)=0.05,p(a=1)=0.01=1   p(a=0)thenpluggingin,wehavethatp(a=1|b=1)=0.16continuousspace   we   vebeentalkingaboutdiscretedistributionssofar.thatis,thenumberofpossiblevaluestheunknownscantakeis   nite.   whenvaluesareinacontinuousspace,weswitchtocontinuousdistributions.examplex   r,p(x)isitsdensity(representedbytheswitchtolowercase)p(x)   0andrp(x)dx=1p(x   a)=rap(x)dx   thesamerulesapplyasfordiscreterandomvariables3   p(x|y)=p(x,y)p(y),p(y)=zp(x,y)dx   p(x|y)p(y)=p(x,y)=p(y|x)p(x)   thisleadstobayesruleforcontinuousrandomvariablesp(y|x)=p(x|y)p(y)p(x)=p(x|y)p(y)rp(x|y)p(y)dy   thedifferenceisthatwearedealingwithcontinuousfunctions.bayesianmodeling   applyingbayesruletotheunknownvariablesofadatamodelingproblemiscalledbayesianmodeling.   inasimple,genericformwecanwritethisprocessasx   p(x|y)   thedata-generatingdistribution.thisisthemodelofthedata.y   p(y)   themodelpriordistribution.thisiswhatwethinkaboutyapriori.   wewanttolearny.wewritethatp(y|x)   p(x|y)p(y)   intheaboveline,wedon   tknowp(y|x)andwanttocalculateit.itanswersthequestion   havingseenx,whatcanwesayabouty?   therighttwotermsp(x|y)p(y)wedoknowbecausewehavede   nedit.thesymbol   isreadas   isdistributedas.      thisisthegeneralformoftheproblemsdiscussedinthisclass.however,itisnon-trivialbecause1.p(x|y)canbequitecomplicated2.p(y|x)canbeintractablebecausetheintegralinthenormalizingconstantdoesn   thaveaclosed-formsolution,thusrequiringanalgorithmtoapproximate   thesetwoissueswillmakeupthefocusofthisclass:de   ningvariousmodelsonthestructureofthedata-generatingphenomenon,andde   ningid136algorithmsforlearningtheposteriordistributionofthatmodel   svariables.simpleexample:beta-bernoullimodel   wehaveasequenceofobservationsx1,...,xn,wherexi=1indicates   success   andxi=0indicates   failure.   thinkofthemasresultsof   ippingacoin.   wehypothesizethateachxiisgeneratedby   ippingabiasedcoin,wherep(xi=1|  )=  .4   wefurtherassumethexiareindependentandidenticallydistributed(iid).thismeansthexiareconditionallyindependentgiven  .mathematicallythismeanswecanwritep(x1,...,xn|  )=nyi=1p(xi|  )   sincep(xi|  )=  xi(1     )1   xi,wecanwritep(x1,...,xn|  )=nyi=1  xi(1     )1   xi   weareinterestedintheposteriordistributionof  givenx1,...,xn,i.e.,p(  |x1,...,xn)   p(x1,...,xn|  )p(  )   nyi=1  xi(1     )1   xip(  )   we   vecomeacrossthenextsigni   cantproblem:whatdowesetp(  )to?a   rsttry   letp(  )=uniform(0,1)   p(  )=1(0        1)   thenbybayesrule,p(  |x1,...,xn)=  (pixi+1)   1(1     )(n   pixi+1)   1r10  pixi(1     )n   pixid     thenormalizingconstantistricky,butfortunatelymathematicianshavesolvedit,p(  |x1,...,xn)=  (n+2)  (1+pixi)  (1+n   pixi)  pixi+1   1(1     )n   pixi+1   1(  (  )iscalledthe   gammafunction   )   thisisaverycommondistributioncalledabetadistribution:beta(a,b)=  (a+b)  (a)  (b)  a   1(1     )b   1   inthecaseoftheaboveposteriordistribution,a=1+pixiandb=1+n   pixi.   noticethatwhena=b=1,beta(1,1)=uniform(0,1)whichwasthepriorwechose.5aconjugateprior   thebetadistributionlooksalotlikethelikelihoodterm.also,becauseithastheuniform(0,1)distributionasaspecialcase,itwouldgiveusawiderrangeofpriorbeliefsthatwecouldexpress.whatifwetrythebetadistributionasthepriorfor  ?p(  |x1,...,xn)   p(x1,...,xn|  )p(  )   h  pixi(1     )n   pixiih  (a+b)  (a)  (b)  a   1(1     )b   1i     a+pixi   1(1     )b+n   pixi   1   wecannoticeafewthingshere:1.ithrewaway  (a+b)  (a)  (b)inthelastline.thisisbecauseitdoesn   tdependon  ,andsoitcanbeabsorbedinthenormalizingconstant.inotherwords,thenormalizingconstantistheintegralofthenumerator.ifwedividethesecondlinebythisintegral,wecanimmediatelycancelout  (a+b)  (a)  (b).theresultisthethirdlinedividedbytheintegralofthethirdline.theonlyreasontodosomethinglikethisisforconvenienceandtomakethingslooksimpler.2.inthiscase,weknowwhattodividethisbytomakeitaid203distributionon  .thelastlineisproportionaltoabeta(a+pixi,b+n   pixi)distribution.   therefore,p(  |x1,...,xn)=beta(a+pixi,b+n   pixi).   noticethatforthisproblem,whenweselectthepriorp(  )tobeabetadistribution,we   ndthattheposteriordistributionisalsoabetadistributionwithdifferentparameters.thisisbecausethebetadistributionisaconjugatepriorforthisproblem.   wesaythatadistributionisa   conjugateprior   foraparticularvariableinalikelihood,orthatitis   conjugatetothelikelihood,   iftheposteriorisinthesamedistributionfamilyastheprior,buthasupdatedparameters.conjugatepriorsareveryconvenient,sinceweonlyneedtocollectsuf   cientstatisticsfromthedatainordertotransitionfromthepriortotheposterior.forexample,intheproblemaboveweonlyneedtocountthetotalnumberof   successes   (pixi)and   failures   (n   pixi).   willseemanyconjugatepriorsinthiscourseandhowtheycanresultinfastid136algorithmsforlearningmodels.whatdowegainbybeingbayesian?   forthepreviousmodelingproblemwithabetaprior,considertheexpectationandvarianceof  undertheposteriordistribution.e[  ]=a+pixia+b+n,var(  )=(a+pixi)(b+n   pixi)(a+b+n)2(a+b+n   1)6   noticethatasnincreases,1.theexpectationconvergestotheempiricalsuccessrate.2.thevariancedecreaseslike1/n,i.e.,it   sgoingtozero.   comparethiswithmaximumlikelihood,inwhichweseekapointestimate(onspeci   cvalue)of  tomaximizethelikelihoodtermonly  =argmax  p(x1,...,xn|  )=1nnxi=1xi   thebayesianapproachiscapturingouruncertaintyaboutthequantityweareinterestedin.maximumlikelihooddoesnotdothis.   aswegetmoreandmoredata,thebayesianandmlapproachesagreemoreandmore.however,bayesianmethodsallowforasmoothtransitionfromuncertaintytocertainty.7eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture2,9/15/2016instructor:johnpaisley   next,welookatanotherinstanceofaconjugateprior.tohelpmotivatethepracticalusefulnessofthedistributionswewillconsider,wediscussthispriorinthecontextofaregressionproblem.problemsetup   we   reoftengivenadatasetoftheformd={(xi,yi)}ni=1wherex   rdandy   r.thegoalistolearnapredictionrulefromthisdatasothat,givenanew  xwecanpredictitsassociatedanunobserved  y.thisiscalledaregressionproblem.   weoftenrefertoxas   covariates   or   features   andyasa   response.   linearregression   onemodelforthisproblemistoassumealinearrelationshipbetweentheinputsxandoutputsysuchthatyi=xtiw+ itheterm iaccountsforthefactthatweusuallycan   t   ndawsuchthatyi=xtiwforalli.themodelvalueofinterestiswandwesimplyset i=yi   xtiw.furtherassumptionsletw   rdand iiid   normal(0,  2).likelihoodtermfory   usingonlythisinformation,wehaveanimpliedlikelihoodtermofygivenxandw(wherex={xi}).   first,noticethatforeachiyiind   normal(xtiw,  2)wherewerecallthatnormal(yi|xtiw,  2)=(2    2)12exp{   12  2(yi   xtiw)2}.1   becauseoftheconditionalindependenceassumptionofyigivenw,wecanwritethelikelihoodasp(y1,...,yn|w,x)=nyi=1p(yi|w,xi)   thevectorwisthemodelparameterthatisunknownaswewanttolearnit.bayesianlinearregressiontakestheadditionalstepoftreatingwasarandomvariablewithapriordistribution.henceinthebayesiansettingwerefertowasamodelvariableinsteadofamodelparameter.   afterde   ningapriorp(w),weusebayesruletolearnaposteriordistributiononw:p(w|x,~y)=p(~y|x,w)p(w)rrdp(~y|x,w)p(w)dw=qni=1p(yi|xi,w)p(w)rrid25i=1p(yi|xi,w)p(w)dw   question:whyisxalwaysbeingconditionedon?(i.e.,ontherhsof|)   answer:lookatwhatwehavedistributionson.   themodelassumesagenerativedistributionfory   weputapriordistributiononw   wehaven   tassumedanydistributiononxi   inshort,thisresultsfromthemodelassumptionsthatwehavemadeprioronw   westillhavetode   nethepriordistributiononw.outofconveniencewede   new   normal(0,     1i)whydowedothis?becauseit   sconjugatetothelikelihood.asaresult,theposteriordistributionofwisarecogniz-abledistributionwithknownparametersthatcanbeeasilycalculatedfromthedata.posteriorofw   wenextcalculatetheposteriordistributionofwforthismodel.thestepswetakeareasfollows:usingbayesrulewehavethatp(w|x,~y)   nyi=1normal(yi|xtiw,  2)normal(w|0,     1i)bydirectionplugginginusingtheformofagaussian,wehavethatp(w|x,~y)   hnyi=1e   12  2(yi   xtiw)2ihe     2wtwi2wedon   tneedtoincludetheconstantsoutfront(2    2)   12and(  /(2  ))12becausetheydon   tdependontheunknownpartofthedistribution,w.whenwritingoutthenormalizingconstantastheintegralofthenumerator,onecanseethatthesetermsappearinthenumeratorandde-nominatorandcancelout.theonlyreasonwedothisisforconvenience(it   slesstowrite).i   llworkthroughthefullderivationbelow.youcanskiptostep7forthe   nalresult.1.the   rststeptocalculatep(w|x,~y)istocombineintooneexponentialbysummingthetermsintheexponent,p(w|x,~y)   e     2wtw   12  2pni=1(yi   xtiw)22.inthenextstep,weexpandthesquaretowriteanequationquadraticinwp(w|x,~y)   expn   12hwt(  i+1  2pixixti)w   2wt(1  2piyixi)+1  2piy2iio3.next,wewritep(w|x,~y)   expn   12hwt(  i+1  2pixixti)w   2wt(1  2piyixi)ioexpn   12h1  2piy2iio   expn   12hwt(  i+1  2pixixti)w   2wt(1  2piyixi)iointhe   rstline,wejustmovedthingsaround,butdidn   tchangetheactualfunction.wedidthistojustifythesecondline,whichagaintakesadvantageofthefactthatweonlycareaboutproportionality.if         wereinstead   =,   thiswouldofcoursebemathemati-callywrong.however,         allowsustotreatexp{   12[     2piy2i]}asapre-multiplyingconstantw.r.t.wthatwillcanceloutwhenwedividethenumeratorbyitsintegraloverwtogettheposteriordistributionandreturnto   =.   wegetridofitjustforconvenience.4.inthefourthstep,weinvertthisprocessbymultiplyingbyatermofourchoosingthat   sconstantwithrespecttow.thegoalistocompletethesquareintheexponentialterm.wechoosetomultiplythistermbythesomewhatarbitrarylookingtermexpn   12h(1  2piyixi)t(  i+1  2pixixti)   1(1  2piyixi)iohowever,noticethatthisdoesn   tinvolvew,sobymultiplyingwithitwearen   tviolatingtheproportionalityw.r.t.w.thinkofthisasworkingwithafunctionofw,andscalingthatfunctionupanddownuntilwereachthepointthatitintegratesto1.5.bymultiplyingthelastterminstep3withtheterminstep4andshuf   ingthingsaround,wegetalongtermthatiwillbreakdownasfollowsp(w|x,~y)   expn   12(w     )t     1(w     )owhere  =(  i+1  2pixixti)   1and  =  (1  2piyixi)youcanverifythisbyplugginginfor  and  ,expandingthequadraticterm,pullingouttheconstantandseeingthattheresultisthemultiplicationofstep3withstep4.36.finally,wewanttocalculatep(w|x,~y)exactly.we   rehopingwecan   ndaclosedformsolutiontotheintegralinp(w|x,~y)=exp(cid:8)   12(w     )t     1(w     )(cid:9)rexp(cid:8)   12(w     )t     1(w     )(cid:9)dwwhereiusethesamede   nitionof  and  asinstep5.approachingthisproblemfromapurelymathematicalstandpoint,thisisnoteasyatall.however,mathematiciansandstatisticianshavebeencollectingknownid203distributionsforourreference.atsomepoint,someoneactuallysolvedthisintegralandnowwehavethesolutionwithouthavingtorecalculatetheintegraleachtime.speci   cally,weknowthatad-dimensionalmultivariategaussiandistributionwithmean  andcovariance  hastheformp(w|  ,  )=(2  )   d2|  |   12e   12(w     )t     1(w     )weknowfromotherpeople   scalculationsthatthisfunctionofwintegratesto1.thisfunctionisalsoproportionaltothefunctioninstep5.instep6we   resolvingtheintegralinorderto   ndtheconstanttomultiplystep5withtomakethefunctionintegrateto1.therefore,weknowthisconstantis(2  )   d2|  |   12andzexp(cid:8)   12(w     )t     1(w     )(cid:9)dw=(2  )d2|  |12it   sanargumentbasedonlogicandbasedontrustthatpreviousproofsthatthenonnegativefunction(2  )   d2|  |   12e   12(w     )t     1(w     )integratesto1arecorrect,whichisthestoppingpointforthisclass(and99.99%ofall   bayesians   includingmyself).7.therefore,wehaveshownthattheposteriordistributionp(w|x,~y)isamultivariategaus-sianwithmean  andcovariance  ,where  =(  i+1  2pixixti)   1  =  (1  2piyixi)noticethatthisisinthesamefamilyasthepriorp(w),whichwasalsoamultivariategaussian.thereforewechoseaconjugatepriorforw.makingpredictions   formanyapplications,theposteriordistributionofamodel   svariablesgivesusefulinformation.forexample,itoftenprovidesinformationonthestructureunderlyingthedata,orcauseandeffecttyperelationships.   forexample,inthelinearregressionmodely=xtw+ we   vebeendiscussing,thevectorwtellsustherelationshipbetweentheinputsofxandoutputy.e.g.,ifwkispositivethemodelwillleadustobelievethatincreasingthekthdimensionofxwillincreaseyandbyhowmuch.wemightthenusethisinformationto   gureoutwhichdimensionsweneedto   improve.      often,however,wejustwanttomakepredictionsonnewdatausingthemodeloftheolddata.4predictivedistribution   forbayesianlinearregression,weultimatelywanttopredictanew  ygivenitsassociated  xandallpreviouslyobserved(x,y)pairs.inotherwords,wewishtoformthepredictivedistributionp(  y|  x,~y,x)=zrdp(  y|  x,w)p(w|~y,x)dw   noticethatthepredictivedistributioncanbewrittenasamarginaldistributionoverthemodelvariables.   however,impliedinthelefthandsideisthemodelde   nition,whichtellsuswhichvariablestointegrateoverontherighthandside,aswellasthespeci   cdistributionstoplugin.   thatis,justwritingapredictivedistributionp(  y|  x,~y,x)isnotenoughinformationtoknowhowtorepresentitasamarginaldistribution,wheremarginalization(i.e.,integration)takesplaceoverthemodelvariables.itisnecessarytoknowinadvancewhattheunderlyingmodelisinordertoknowwhatmodelvariablestointegrateover.   let   sstepbackandthinkmoregenerallyaboutwhatwe   redoingwhenweconstructapredictiveid203distribution.bayesianmodel:thebayesianmodelingproblemissummarizedinthefollowingsequence.modelofdata:x   p(x|  )modelprior:     p(  )modelposterior:p(  |x)=p(x|  )p(  )/p(x)predictions:thegoalofmakingpredictionsistousepastdatatopredictthefutureunderthesamemodelingassumption.thisiswhywechoosetomodeldatainthe   rstplace:weassumethereisanunderlyingrulegoverningthewaydataiscreatedwhetheritsseenorunseendata.byde   ningamodelwede   nearule.seeingsomedataletsusgetabettersenseofwhatthatruleshouldbe,lettinguslearnfromthepast.therefore,whenmakingpredictionsweassumefuturedatafollowsthesameruleaspastdata,futuredata:  x   p(x|  )   thesamemodelasthepastdatawedon   tknow  ,butthepastdatax,alongwithourpriorbelieveabout  ,givesusinformationaboutit.infactwehaveaposteriordistributiononitp(  |x).wecanusethisto   score   or   weight   eachpossiblesettingof  .thisisthemarginalizationprocedure,predictivedistribution:p(  x|x)=zp(  x|  )p(  |x)d  sobymarginalizing,weareconsideringthein   nitenumberofpossiblesettingsfor  inthelikelihoodandweightingitbyourposteriorbeliefofhowprobablethatsettingis.forcertainid203distributions,wecansolvethisintegralanalytically.5   thebayesianlinearregressionmodelwe   vebeendiscussingisoneexamplewherewecansolveforthepredictivedistribution.ourgoalistosolvetheintegralp(  y|  x,~y,x)=z(2    2)   12e   12  2(  y     xtw)2|{z}=p(  y|  x,w)(2  )   d2|  |   12e   12(w     )t     1(w     )|{z}=p(w|~y,x)dwnoticethatwekeepallthetermsthistimebecausewearewritinganequalityhere,notapropor-tionality.thespeci   cvaluesof  and  fortheregressionproblemwerederivedabove.   belowi   llderivethispredictivedistribution.thetrickthistimewillbetomultiplyanddividebythesamespeci   callychosenvalue.youcanskiptostep5fortheresult.1.first,weexpandbothsquaresandpulleverythingnotdependingonwoutinfrontoftheintegralp(  y|  x,~y,x)=(2    2)   12(2  )   d2|  |   12e   12  2  y2   12  t     1    ze   12(wt(  x  xt/  2+     1)w   2wt(  x  y/  2+     1  ))dw2.nextwemultiplyanddividebyaconstantw.r.t.w.wewanttochooseaconstanttoresultintheintegralequaling1.wenoticethatwecancompletethesquareintheexponent,allowingustoputitintotheformofagaussiandistribution.therefore,multiplyanddividebytheterm(2  )   d2(cid:12)(cid:12)     2  x  xt+     1(cid:12)(cid:12)12e   12(  x  y/  2+     1  )t(  x  xt/  2+     1)   1(  x  y/  2+     1  )3.ifweputthisextraterminthenumeratortotherightoftheintegralandtheterminthedenominatortotheleftoftheintegral,wecancompletethesquareintheright-mostexponentandseethatagaussianresults.thereforeweknowtheintegralequals1andwecansimplyremovethisterm.theresultisp(  y|  x,~y,x)=(2    2)   12(2  )   d2|  |   12e   12  2  y2   12  t     1  (2  )   d2|     2  x  xt+     1|12e   12(  x  y/  2+     1  )t(  x  xt/  2+     1)   1(  x  y/  2+     1  )4.the   nalstepusestwoimportantmatrixequalitiesthatareoftenuseful.inthecontextoftheequationabove,theseare(cid:12)(cid:12)     2  x  xt+     1(cid:12)(cid:12)=|     1|(1+  xt    x/  2)=|  |   1(1+  xt    x/  2)pluggingthisinabove,severaltermscanceloutandweendupmultiplyingtheratioofex-ponentsby(2  )   12(  2+  xt    x)   12thesecondusefulequalityiscalledthematrixinversionlemma.thisallowsustosaythat(  x  xt/  2+     1)   1=  +    x(  2+  xt    x)   1  xt  noticethattheinvertedtermisascalar,notamatrix.weplugthisinwhereitappearsintheexponent,andcombineintooneexponential.thebookkeepingisfairlyinvolved,butwegettermsthatcancelandcanwritetheresultingquadratictermasasquare.asaresult,theexponentialtermise   12(  2+  xt    x)(  y     xt  )2.65.asaresult,wehavecalculatedthatp(  y|  x,~y,x)=(2  )   12(  2+  xt    x)   12e   12(  2+  xt    x)(  y     xt  )2noticethatthisisaunivariatenormaldistributionwithmean  xt  andvariance  2+  xt    x.again,theterms  and  arecalculatedusingthepriorparametersandthedata~yandx.anothermodelingexample   let   slookatanotherbayesianmodelingexamplethatisslightlymorecomplicated.problemsetup   againwe   regivenadatasetd={(xi,yi)}ni=1wherex      x,whichisanarbitraryspace,andthistimey   {0,1}.thegoalistoclassifyanewxbyassigningalabeltoit.thatis,wewanttopredicttheunknownvalueofy   {0,1}associatedwithanewinputx.bayesianclassi   ermodel   thisapproachtoclassi   cationassumesagenerativeprocessforbothyandx.forthemodel,wewillassumethateach(x,y)pairisgeneratedi.i.d.asfollowsmodel:yiid   bernoulli(  ),x|yind   p(x|  y)todrawx,weneedtoknowy,whichiswhatx|yismeanttoconvey.   asisevident,thejointdistributionon(x,y)isnotadistributionthatcanbefactorizedintoseparatedistributionsonxandy.rather,itmustbefactorizedasp(x,y|  ,  )=p(x|y,  )p(y|  )thisfactorizationsaysthatwecan   rstsampleavalueforywithoutknowingx,afterwhichwesamplexfromadistributionthatdependsonthevalueofyandsomeparameters  .thisisalsowhatthegenerativeprocessabovesays.infact,therewefurtherassume  ={  0,  1}andletypickoutthecorrectparametersfromthisset.   wecanthinkofthisasfollows,imaginexcontainsthetextofanemailandyindicateswhetheritisspamornot.thegenerativemodelassumesthatspamandnon-spamemailsaregeneratedfromthesamedistributionfamily,butwithdifferentparameters.allspamemailsshareoneparameter,  1,andallnon-spamemailsshareanotherparameter  0.thedataisgeneratedby   rstchoosingwhetheraspamornon-spamemailwillbegeneratedby   ippingacoinwithbias  ,andthengeneratingtheemailitselffromadistributionusingtheclass-speci   cparameter.   itshouldbeclearthatxdoesn   thavetobeanemail,butsomethingelse,forexampleseveralstatisticsfromapatientduringahealthcheckupthatcanbeusedtodiagnoseanillness.thexforthisproblemwillbeinadifferentdomainthanfortheemailproblem,whichiswhyitisbene   cialtokeepthedistributionandspaceofxgeneral.7priordistributions   themodelvariablesaretheclassid203     (0,1)andclass-speci   cvariables  1,  0.wenextchoosepriordistributionsforthese.weselectfor  outofconvenienceandwriteagenericpriorforeach       beta(a,b),  0,  1iid   p(  )wearemakingoneassumptionhereabout  ,thattheclass-speci   cvariablesaredrawnindepen-dentlyfromthesamepriordistribution.thiswillletussimplifythejointlikelihoodofthemodelforposteriorcomputation.posteriorcomputation   againwewantto   ndtheposterior,butthistimeof  ,  0and  1.usingbayesrule,p(  ,  1,  0|~y,x)=p(x,~y|  ,  1,  0)p(  ,  1,  0)r     r     r10p(x,~y|  ,  1,  0)p(  ,  1,  0)d  d  1d  0   thistimewehavethreevariablesinsteadofone.however,lookingcloserwewillseethatthesedistributionsfactorizenicely.(thisistheexception,nottherule.)   prior:thepriorcanbewrittenp(  ,  1,  0)=p(  )p(  1)p(  0).ofcoursep(  1)andp(  0)arethesameexactdistribution,butevaluatedatdifferentpoints.   likelihood:bythemodelingassumption,wecanwritethelikelihoodasp(x,~y|  ,  1,  0)=nyi=1p(xi,yi|  ,  1,  0)=nyi=1p(xi|yi,  ,  1,  0)p(yi|  ,  1,  0)=nyi=1p(xi|  yi)p(yi|  )theproductisfromtheindependenceassumption.thetransitionfromthe   rsttosecondlineisaruleofid203that   salwaystrue.thetransitionfromthesecondtothirdlinesimpli   esthetore   ectthedependencestructureassumedbythemodel.   finally,wecanreturntobayesruleandtrycomputingit.inthenumerator,wegroupthemultiplicationsacrossthethreevariablesp(  ,  1,  0|~y,x)   "yi:yi=1p(xi|  1)p(  1)#"yi:yi=0p(xi|  0)p(  0)#"nyi=1p(yi|  )p(  )#8   thenormalizingconstantistheintegralofthisterm.noticethatsincewecanwritethisastheproductofthreeseparatefunctionsofovereachmodelvariable,theposteriorcanbewrittenasp(  ,  1,  0|~y,x)=qi:yi=1p(xi|  1)p(  1)rqi:yi=1p(xi|  1)p(  1)d  1  qi:yi=0p(xi|  0)p(  0)rqi:yi=0p(xi|  0)p(  0)d  0  qni=1p(yi|  )p(  )rqni=1p(yi|  )p(  )d  however,thisisjusttheproductofthreeinstancesofbayesruleandisequivalentlysayingp(  ,  1,  0|~y,x)=p(  1|{xi:yi=1})p(  0|{xi:yi=0})p(  |~y)   lastweekweworkedthroughp(  |~y).wesawthatthiswastheposteriorofabeta-bernoulliprocess,p(  |~y)   nyi=1p(yi|  )p(  )      p(  |~y)=beta(a+piyi,b+n   piyi)thatis,fortheclassid203,wesimplysumthenumberoftimesourdatawasineachclassandusethosevaluesastheparametersinthebetadistribution.   theotherclass-conditionalposteriordistributionson  0and  1areproblem-speci   c.wenoticethefeatureofthesedistributionsisthatwepartitionthedatasetintotwogroups,thosebelongingtoclass1andthoseinclass0.then,wewishtosolve(for,e.g.,class1)p(  1|{xi:yi=1})   yi:yi=1p(xi|  1)p(  1)inmanycaseswecandothis.forexample,whenx   rdandp(x|  )isagaussian,wecanpickaconjugatepriorforthemeanandcovariance.naivebayesclassi   er   whenxisacomplexmultidimensionalobject,asimplifyingassumptionaboutp(x|  )issome-timesmadeonhowthiscanbefurtherfactorized.togiveanoverview,letxbecomposedofmpiecesofinformationpossiblyindifferentdomainsx={x(1),...,x(m)}.forexample,somex(j)couldrealnumbersandsomenon-negativeintegers.anaivebayesclassi   cationmodelmakestheassumptionthatthevariables  canbeseparateintomgroupsaswellandthatp(x|  )=myj=1p(x(j)|  (j))   separatepriorsareselectedforeach  (j),possiblyindifferentdistributionfamilies,andweassume  (j)0,  (0)1iid   pj(  (j)).   thebayesclassi   eristhencomputedasabove,afterwhichone   nds(forclass1,forexample)p(  1|{xi:yi=1})=myj=1pj(  (j)1|{x(j)i:yi=1})9   spamdetection:tomakethismoreconcrete,considertheproblemofspamdetection.inthespamdetectormodelwewillconsiderhere,apair(x,y)consistsofalabely   {0,1}indicating   spam   or   notspam   andxisavdimensionalvectorofwordcountsforavocabularyofsizev.thusx(j)isacountofhowmanytimeswordj(e.g.,jmapsto   deal   )appearsintheemail.weneedtode   neadistributiononxasitsmodel,whichcanalsobethoughtofasajointid203distributiononallthevaluesinx,p(x|  )   p(x(1),...,x(v)|  )naivebayestakestheadditionalmodelingstepofbreakingthecorrelationstructurebetweenthesevaluesp(x(1),...,x(v)|  )=vyj=1p(x(j)|  (j))   amodelingassumption   wede   nethisfromagenerativeperspective,wearesayingthatforanobservationxi,each   piece   ofxiisgeneratedindependentlyfromitsowndistributiongivenitsclass.theassumptionthatxi(j)andxi(j0)areindependentgivenitsclassvariable  yiiswhatis   naive   aboutthismodel.weusebayesruletocalculatetheposteriorof  exactlyasbefore.usingittocalculatethepos-teriorofeach  ycanbesplitintotheprocessofcalculatingtheposteriorofeach  y(j)separately.forexample,inspamdetectionwecande   nep(x(j)|  y(j))tobeapoissondistribution,andp(  y(j))tobeagammadistribution.thesetwodistributionsareconjugateandsotheposteriorof  y(j)willalsobegamma.becauseofindependenceassumptions,solvingfortheposteriorof  ycanbeseparatedintovindependentapplicationsofbayesrule,oneforeach  y(j).predictivedistribution   aswithbayesianlinearregression,amajorapplicationofthebayesclassi   eristopredictingthelabelsofnewdata.thatis,givenatrainingsetd={(xi,yi)}ni=1oflabeleddatawewanttolearnamodelsothatwecanpredicttheunobservedlabel  y   {0,1}ofanewlyobserved  x.   againwewanttoformthepredictivedistributionp(  y|  x,x,~y).thiscanbedone,butisnotasstraightforwardasbefore.we   lltrytwoways.the   rstwaywillleadtoadead-end.thesecondapproachwillwork.firstattempt   wewanttocalculatep(  y|  x,x,~y)=z     z     z10p(  y|  x,  1,  0,  )p(  1,  0,  |x,~y)d  d  1d  0therearetwodistributionsweneedtoknow.thesecondoneistheposteriorthatwepreviouslycalculatedandshowedforthismodelcanbefactorizedasp(  1,  0,  |x,~y)=p(  1|x,~y)p(  0|x,~y)p(  |~y)the   rsttermistrickierandwecan   tsimplyfollowtheprocedureforthebayesianlinearregres-sionmodel.thereasonisthatweareaskedforadistributionon  yconditionedonboth  and  xplustheclass-speci   cvariables.however,accordingtoourmodelwehavey   bernoulli(  ),x|y   p(x|  y)10thegenerativemodelforthedatagivesusamarginaldistributiononyandaconditionaldistri-butiononx.therefore,unlikebeforewecan   tjustgotothemodelto   ndoutwhattopluginforp(  y|  x,  1,  0,  ).however,wenoticethatthesedistributionsincombinationwithbayesruleprovideuswithalltheinformationweneed.   thedistributionp(  y|  x,  1,  0,  )canbeinterpretedastheposteriordistributionof  ygiven  xandthemodelvariables.bybayesrule,p(  y|  x,  1,  0,  )=p(  x|    y)p(  y|  )p(  x|  1)p(  y=1|  )+p(  x|  0)p(  y=0|  )noticethatbecause  yisadiscretevariable,theintegralturnsintoasumoverthesetofpossiblevalues  ycantake.   wecanswapp(  y|  x,  1,  0,  )withthebayesruleversionofitintheintegraltoformthepredictivedistribution.we   releftwithp(  y|  x,x,~y)=z     z     z10p(  x|    y)p(  y|  )p(  1|x,~y)p(  0|x,~y)p(  |~y)p(  x|  1)p(  y=1|  )+p(  x|  0)p(  y=0|  )d  d  1d  0however,wenowhitadeadend.eventhoughwehaveactualfunctionswecanpluginevery-where,theproblemnowisthatthereisasuminthedenominator.forexample,ifwepluginthefunctionsinvolving  wecanseethattheintegralisnottractableforthisvariableandsowecan   tgetananalyticfunctionforthepredictivedistribution.secondattempt   eventhoughthe   rstattemptdidn   twork,westillhaveotheroptions.we   rstnoticethat  y   {0,1},meaningwecanonlyquerythepredictivedistributionatthesetwovalues.aboveweusedbayesruletomakeprogresstowardthe   nalcalculation.nowweuseitagain,butonthemarginaldistributionitself:p(  y|  x,x,~y)=p(  x|  y,x,~y)p(  y|x,~y)p(  x|  y=1,x,~y)p(  y=1|x,~y)+p(  x|  y=0,x,~y)p(  y=0|x,~y)=p(  x|  y,x,~y)p(  y|~y)p(  x|  y=1,x,~y)p(  y=1|~y)+p(  x|  y=0,x,~y)p(  y=0|~y)   againweneedtoknowwhattopluginforthelikelihoodp(  x|  y,x,~y)andpriorp(  y|~y).the   rstthingweshouldthinkisthatthesearebothmarginaldistributionsandcanberepresentedasanintegralovermodelvariables.   likelihood:first,p(  x|  y,x,~y)=z     z     p(  x|  y,  1,  0)p(  1,  0|x,~y)d  1d  0=z     p(  x|    y)p(    y|{xi:yi=  y})d    ynoticethat  isn   tinvolvedbecause,accordingtothemodel,conditionedon  y,  xisindependentof  .also,thesecondlineshowshow  yreallypicksouteither  1or  0andsoweonlyneedto11integrateoneofthem.thismarginaldistributionisproblem-speci   c,butasisusual,distribu-tionsareoftenselectedsothatitcanbesolved.then,wewouldsolvethisintegralfor  y=1and  y=0andusethesetwovaluesinbayesruleabove.sincewehaven   tde   nedp(x|  )orp(  )wehavetostophere.   prior:theothertermweneedisp(  y|~y),whichalsohasamarginalrepresentationaccordingtoourmodel,p(  y|~y)=z10p(  y|  )p(  |~y)d  wealreadyknowfromthemodelde   nitionthatp(  y|  )=    y(1     )1     yandfrombayesrulethattheposteriordistributionp(  |~y)=  (a+b+n)  (a+piyi   1)  (b+n   piyi)   1  a+piyi(1     )b+n   piyiwecanalsosolvethisintegral,whichwecanwriteasz10p(  y|  )p(  |~y)d  =  (a+b+n)  (a+piyi)  (b+n   piyi)z10    y+a+piyi   1(1     )1     y+b+n   piyi   1d  todothis,multiplyanddividebythevalue  (1+a+b+n)  (  y+a+piyi)  (1     y+b+n   piyi).inthenumerator,putthisvalueinsidetheintegral.inthedenominator,putitoutside.thenwecannoticethattheintegralisoverabetadistributionandequals1,sotheintegraldisappears.theresultisp(  y|~y)=  (a+b+n)  (  y+a+piyi)  (1     y+b+n   piyi)  (a+piyi)  (b+n   piyi)  (1+a+b+n)one   nalusefulpropertycomesintoplaythatisworthmemorizing:forthegammafunction,  (x)=(x   1)  (x   1).usingthispropertyontheevaluationofp(  y|~y)at  y=1and  y=0,weseethatp(  y=1|~y)=a+piyia+b+n,p(  y=0|~y)=b+n   piyia+b+nwenownoticethatwenowhaveallweneedtocalculatethepredictivedistributionp(  y|  x,x,~y).   naivebayesextension:forthenaivebayesextensionweonlyneedtomodifythelikelihoodterm.usingthenotationfromthediscussiononnaivebayesabove,wewanttocalculatep(  x|  y,x,~y)=z     vyj=1p(  x(j)|    y(j))p(    y(j)|{xi(j):yi=  y})d    y=vyj=1zp(  x(j)|    y(j))p(    y(j)|{xi(j):yi=  y})d    y(j)inotherwords,wehavevseparatemarginaldistributionstocalculate,andtheoverallmarginaldistributionistheproductofeachindividualone.we   vemadetheproblemmucheasierforourselves,whichiswhysomeonemightopttobe   naive   whendoingbayesianclassi   cation.12eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture3,9/22/2016instructor:johnpaisley   uptothispointwe   vebeendiscussingproblemsthataresimpleenoughthatwecancalculateallposteriordistributionsinclosedform.thatis,we   veassumedthatgivenamodelx   p(x|  )andprior     p(  ),wecananalyticallycalculatetheposteriorusingbayesrule,p(  |x)=p(x|  )p(  )rp(x|  )p(  )d  .(1)sincewehavede   nedthemodelandpriors,thejointlikelihoodterminthenumeratorisalwaysknown.therefore,wespeci   callyhaveassumedthatthenormalizingconstantp(x)=zp(x|  )p(  )d  (2)isanintegralwecananalyticallysolve.   inpractice,thisistheexceptionandnottheruleformachinelearningapplications.again,sincewewillalwaysbeabletowriteananalyticformforp(x|  )p(  )becausewede   neit,thisisequivalenttosayingthattheintegralinthedenominatorisusuallyintractable.   whyshouldwecare?weknowthatp(  |x)=1zp(x,  )forsomenumberz.thisjustscalesthejointlikelihoodfunctionp(x,  )upordown.imaginewewanttocalcualtetheexpecta-tionofsomefunctionoveritsposterior,ep(  |x)[f(  )]=rf(  )p(  |x)d  .thisisequalto1zrf(  )p(x,  )d  .whatifthisisanintegralwecancalculate?theproblemhereisthatzcanbeexpectedtovaryoversuchalargerangethat,withoutknowingzwearenoclosertoknowinganythingaboutep(  |x)[f(  )].   inthislecturewewilldiscusstwomethodsforapproximatingposteriordistributions.the   rstmethodiscalledthelaplaceapproximationandisusefulwhenthenumberofmodelvariablesisrelativelysmall.thesecondmethodiscalledgibbssampling,whichisaninstanceofawidevarietyofposteriorid136algorithmscollectivelyreferredtoasmarkovchainmontecarlo(mcmc)methods.gibbssamplingismuchmorewidelyusedthanthelaplaceapproximationandisstillusefulinthecaseofmodelsthathaveverymanyvariables.1example:logisticregression   logisticregressionisoneinstanceoflinearregressionappliedtotheclassi   cationproblem.torecallthesetup,wehavedataintheformoflabeledpairsd={(xi,yi)}ni=1wherex   rdandy   {0,1}.thegoalistousethisinformationtohelpuspredictthelabelofanewygivenanewx.   model:aswithlinearregression,weassumethattheoutputdependsonthelinearrelationshipbetweenxandaweightvectorw   rd.thistimetherelationshipisyiind   bernoulli(  (xtiw)),  (xtiw)=extiw1+extiw(3)lastlecturewesawanothermodelingapproachtobinaryclassi   cationusingabayesclassi   er.thereisno   correct   choiceofmodel,onlymoreandlessappropriatemodelsfortheproblemathand;allmodelsmakeasimplifyingapproximationabouttheunderlyingfunctiongeneratingofthedata   the   rstonebeingthatsuchafunctionexists.to   ndoutwhichmodelis   better,   youcantrybothonyourdataandseewhichperformsbetterinpractice.   prior:weassumeapriordistributiononwoftheformw   normal(0,ci).(4)again,thisisachoicethatwemakeandadifferentpriorcanbede   ned,perhapsonethatmakesstrongerassumptionsaboutwhattypesofvectorsweshouldlearnintheposterior.forexample,apriorthatisonlyde   nedonrd+wouldrequirethattheposteriorbede   nedonrd+aswell(simplylookatbayesruletoseethis).withthenormalpriorthereisnosuchrequirement.   posterior:theposteriordistributiononwisfoundusingbayesrulep(w|x,~y)=qni=1p(yi|xi,w)p(w)rqni=1p(yi|xi,w)p(w)dw(5)whenwepluginp(yi|xi,w)=(cid:16)extiw1+extiw(cid:17)yi(cid:16)1   extiw1+extiw(cid:17)1   yi,wequickly   ndthattheintegralinthedenominatorcan   tbesolved.therefore,wecan   tgettheposteriordistribution.furthermore,imaginethatthereexistedanoraclethatcouldmagicallygaveusthevalueofthesolutiontothisintegral.wewouldstilllikelyhitaroadblocklaterwhentryingtosolveintegrals,forexampletocalculatethepredictivedistribution.thisiswhymanyresearchersliketokeepthingsconjugateiftheycan.whentheycan   ttheyoftenresorttoapproximations.wewillnextdiscussonesimplemethodtoapproximatethisposteriordistribution.laplaceapproximations   returntothegenericmodel:x   p(x|  )and     p(  ).wemakenoassumptionsabouttheformsofeitherofthesedistributions.laplaceapproximationstreattheposteriorasbeingapproximatelygaussian.thatis,thegoalofthelaplaceapproximationistosetp(  |x)   normal(  ,  )(6)forsomegoodsettingof  and  .therearedifferentwaysonemight   ndtheseparametervalues;thelaplaceapproximationisoneapproach.2   first,itturnsoutthatanormalapproximationtotheposteriorcanariseautomaticallyfromaspe-ci   capproximationtothejointlikelihoodp(x,  )=p(x|  )p(  ).thatis,wewillapproximatethenumeratoranddenominatorinbayesrule,afterwhichthenormaldistributionwillpopout.   the   rststepistowritebayesruleslightlydifferentlyp(  |x)=elnp(x,  )relnp(x,  )d  (7)   next,ratherthanfocusonp(x,  ),wefocusonlnp(x,  ).noticethatinmoreabstracttermswecanjustthinkofthisasafunctionof  .theinterpretationofthisfunctionasthelogofthejointlikelihoodissimplyaddedlater.wewantanapproximationoflnp(x,  )thatisagoodone,butalsogeneralenoughthatwedon   tneedtoworryaboutthefactthatwedon   tactuallyknowwhatthisfunctionisyetsincewehaven   tde   nedanythingotherthanabstractid203distributions.   thelaplaceapproximationusesa2ndordertaylorexpansionasanapproximation.recallthatwecanapproximateafunctionf(  )withits2ndordertaylorexpansionasfollows:f(  )   f(  0)+(       0)t   f(  0)+12(       0)t   2f(  0)(       0)(8)thepoint  0isanyarbitrarilychosenpointinthedomainoff(i.e.,chosenamongvaluesthat  isallowedtotake).theshorthand   f(  0)meansweevaluatethegradientoff(  0)at  0.forourproblemf(  )=lnp(x,  ).   thepoint  0iscriticalsincethe2ndordertaylorapproximationisveryaccuratearoundthispoint,andmuchlessaccurateaswemoveawayfromit.therefore,wemustdecideagoodsettingfor  0.   forthelaplaceapproximationweareableto   ndagoodpoint,andalsoaconvenientone.speci   cally,weset  0=  map=argmax  p(  |x)=argmax  p(x|  )p(  )p(x)=argmax  lnp(x,  )(9)thisiscalledthemaximumaposteriorisolution.noticethatitisthepointthatisthemostprobable(or,hasthehighestdensity)accordingtotheposteriordistribution.however,thewholereasonwe   reonthisendeavoristhatwedon   tknowtheposteriori.fortunatelythemaximumpointofp(  |x)isthesameasforp(x,  );theconstantp(x)doesn   tdependon  soitjustscalesthefunctionanddoesn   tchangethepointatwhichit   smaximized.alsonoticethatthelnfunctiondoesn   tchangethelocationofthemaximum,whichisallthatwe   reinterestedin.   asideaboutmapid136:mapid136istheprocessof   nding,forexample,  mapintheproblemabove.it   rstconvertsthejointlikelihoodtothelogjointlikelihood,lnp(x,  ),makingtheobservationthatthemonotonicityofthelogtransformationpreservesalllocalmaximumofp(x,  ).ittheninitializes  anditerativelymodi   es  tomaximizelnp(x,  ).onepervasiveap-proachtothisarethegradientascentmethods.forexample,withsteepestascent,weiterativelyupdate       +       lnp(x,  )untilitreachesalocalmaximumoflnp(x,  ).     [0,1]scalesthegradientandisusuallysmall.3   andsoforthelaplaceapproximation,the   rststepistorunaniterativealgorithmto   nd  map.then,usingthisas  0inthe2ndordertaylorexpansionwenoticeanicesimpli   cationoccurs,lnp(x,  )   lnp(x,  map)+(       map)t   lnp(x,  map)|{z}=0+12(       map)t   2lnp(x,  map)(       map)(10)the   rstordertermcancelsoutbecausethegradientoflnp(x,  )equalszeroatthelocationofitsmaximumpoint,whichispreciselywhatwegetwhenwerunthemapalgorithmto   nd  map.   directlyplugginginthisapproximation,p(x,  )   p(x,  map)e   12(       map)t(      2lnp(x,  map))(       map)rp(x,  map)e   12(       map)t(      2lnp(x,  map))(       map)d  (11)thetermsp(x,  map)canceloutandweareleftwithsomethingwecanimmediatelyrecognizeasamultivariatenormaldistribution.inparticular,weseethatp(x,  )   normal(  map,(      2lnp(x,  map))   1)(12)wesetthemeanofourposteriorapproximationtothemapsolution  mapandthecovariancetotheinverseofthenegativeofthehessianoflnp(x,  )evaluatedat  map.therefore,afterrunningthemapalgorithm,wecalculatethematrixofsecondderivativesbyhandandplug  =  mapintotheresultingmatrix-organizedsetoffunctionsof  .weusethismatrixtoapproximatethecovariance.noticethat,fromacomputationalperspective,themeasureofuncertaintythatwegetontopofmapfromanapproximateposteriordistributioncomesalmostforfreewhendim(  )issmall.logisticregressionrevisited   wenextderivethelaplaceapproximationforthelogisticregressionproblem.we   regoingtoapproximatep(w|~y,x)   n(  ,  ).we   rstwritethejointlikelihood,p(~y,w|x)   nyi=1  (xtiw)yi(1     (xtiw))1   yie   12cwtw,  (xtiw)=extiw1+extiw(13)weinitializew(0)=0andupdatew(t)=w(t   1)+     wlnp(~y,w|x)|w(t   1)usinggradientascent.thederivativeisdirectlycalculatedtobe   wlnp(~y,w|x)=nxi=1(yi     (xtiw))xi   1cw(14)wesimplyplugthemostrecentvalueforwintothisfunctiontogetthedirectioninwhichtostep.whenthealgorithmconvergestoapoint   closeenough   tothemaximumvalue,foundbymonitoringhowmuchlnp(~y,w|x)isincreasingwitheachstep,wetakethe   nalvalueofw(t)atthisiteration(callitt)andsetwmaptobethisvectorandthereforethemeanoftheapproximateposteriordistributionofw.so  =wmap   w(t).to   ndthecovariance,wedirectlycalculatethat        1=   2wlnp(~y,w|x)=   nxi=1  (xtiw)(1     (xtiw))xixti   1ci(15)4weplugthevalueofwmapintothisfunctionandinvertthenegativeofittoobtainthethecovari-ancematrixofthenormaldistributionusedtoapproximatep(w|~y,x).anothermodelingexample   let   slookatanotherexampleofamodelwherewecan   tcalculatetheposterioranalytically,butforwhichthelaplaceapproximationisn   tafeasibleoption.   setup:wearegivenasetofmeasurementsd={rij}wheretheindexpair(i,j)      .thatis,weleti=1,...,nandj=1,...,m,butnotevery(i,j)ismeasured(i.e.,in   ).forexample,rijcouldbetheratinggivenbyuseritoobjectj(thinknet   ix,yelp,amazon,etc.),inwhichcase|   |(cid:28)nm.wewanttocomeupwithamodeltopredictthoserij6   d.   model:atypicalmodelforthisusesmatrixfactorization(wewillnotdirectlyusethematrixnotationhere)andalowrankassumption.here,weassumeeachuseriisrepresentedbyavectorui   rdandeachobjectjbyavectorvj   rd.thelowrankassumptioncomesfromd(cid:28)min(n,m).thenweletrij   normal(utivj,  2)forall(i,j)      (16)comment:thismodeldoesnotseemidealforallcasessinceitassumesrij   r.forexample,inthecontextofuser/objectratingsalarmbellsshouldbegoingoffbecausewearemodelinga   nitesetofpositiveintegersascontinuous.inthiscasewhererij6   rwearemakingasimplifyingmodelingassumption.thiswon   tleadtoanybugsduringid136time,andthejusti   cationforitisintheperformance.still,wewilllaterseehowafewsimplemodi   cationscanmakeusmorecomfortablewiththemodelingassumptionswhilestillbeingveryclosetothismodel.   priors:wewillassumethatuiiid   normal(0,ci)andvjiid   normal(0,ci).   posterior:letucontainallui,vcontainallviandrcontainallmeasuredrij.usingbayesrule,wewanttocalculatep(u,v|r)=p(r|u,v)p(u,v)r      rp(r|u,v)p(u,v)du1      dundv1      dvm(17)becauseofthede   nitionswemadeinthemodelandthepriors,conditionalindependenceletsuswritep(r|u,v)=q(i,j)      p(rij|ui,vj)andp(u,v)=qip(ui)qjp(vj).   theproblemisthatthedenominatorisintractableandsowecan   tsolvebayesrule.justlikebeforewehitaroadblock.however,unlikebeforeitseemsthatlaplacecan   thelpushere.eventhoughwecould   ndalocaloptimalsolutionforumapandvmaptoapproximatealarge,nd+mdlengthmeanvector,thecovariancematrixobtainedfromthehessianwouldbe(nd+md)  (nd+md).thiscaneasilyexceedonebillionvaluestoestimate!   stepback:let   stakeastepbackandtalkaboutwhatourobjectivestypicallyarewithbayesruleincontextslikethese.letp(u,v|r)betheposteriordistributiononthemodelvariables.whatwereallywantisthepredictivedistributionp(rij|r)forsome(i,j)6      .or,atleast,wewouldliketocalculatetheexpectatione[rij|r]underthisposteriorwhereweintegrateoutallofouruncertaintyinthemodelvariables.fornowlet   sjustfocusontheexpectation.5   wedevelopthisfurtherinthecontextoftheaboveproblemtomakeitconcrete,butobservethatthisisaverygeneralprobleminbayesianmodeling.usinganotationalshorthandforthisproblem,theexpectationofanewrijunderthepredictivedistributionise[rij|r]=zrijp(rij|r)drij(18)=zrijzzp(rij|u,v)p(u,v|r)dudvdrij(19)=zz(cid:20)zrijp(rij|u,v)drij(cid:21)|{z}=utivjp(u,v|r)dudv(20)noticethatwe   veswappedintegralsabove.sincep(rij|u,v)=normal(utivj,  2),thebrack-etedtermisequaltoutivj.therefore,weultimatelywante[utivj|r]=rr(utivj)p(u,v|r)dudvwheretheexpectationisundertheposteriordistributionp(u,v|r).sincewedon   tknowthisdistribution,weareforcedtoseekotheroptions.aswewillsee,oneoftheseoptionsemploysmontecarlointegration.   montecarlointegration:mcintegrationisaverygeneraltechnique.imaginewewanttocal-culateep(x)[f(x)]butrunintoamathematicalproblemindoingso.wecanapproximatethisexpectationby   rstsamplingx1,...,xniid   p(x)andthenapproximatingep(x)[f(x)]   1nnxn=1f(xn)(21)asn      thisapproximationconvergestothetrueexpectation.   soifwecould   ndawaytogeti.i.d.samples(u(1),v(1)),...,(u(n),v(n))iid   p(u,v|r)(22)wecouldsaye[utivj|r]   1nnxn=1hu(n)i,v(n)ji(23)   infact,havingaccesstothesesampleswouldallowustocalculatemuchmorethananapprox-imationtoe[rij|r].forexample,ifwewantedameasureofcon   denceinthisprediction,wecouldcalculatethevarianceunderthepredictivedistribution,var(rij|r)=e[r2ij|r]   e[rij|r]2.usingsimilarreasoningasabove,wewould   ndthatvar(rij|r)     2+1nnxn=1(cid:16)hu(n)i,v(n)ji(cid:17)2    1nnxn=1hu(n)i,v(n)ji!2(24)manyothercalculationsofinterestunderthepredictivedistributioncanalsobeobtainedthisway.therefore,alotcanbedoneifwecansomehowgetaccesstoi.i.d.samplesfromtheposteriordistributiononthemodelp(u,v|r).   amajorprocedurefor(approximately)gettingthesesamplesiscalledmarkovchainmontecarlo(mcmc)sampling.we   llnextdiscussoneinstanceofthisgeneraltechnique,andarguablytheeasiest,calledgibbssampling.6gibbssampling   wewillpresenttherecipeforgibbssampling.unfortunatelyadiscussiononthecorrectnessoftherecipeisbeyondthescopeofthisclass.everystatisticsdepartmentwillhavecoursesthatfocusheavilyonmcmcmethods,andtotrulyappreciatemcmcandit   stheoreticalcorrectness,it   sworthwhiletotakeoneofthoseclasses.thisisincontrasttovariationalid136methods,whicharegenerallynottaughtinstatisticsdepartmentsandwillbediscussedmorefullylaterinthiscourse.variationalid136isadevelopmentofmachinelearningresearch.   tousesimplerandmoregeneralnotation,imaginewehaveamodelwithtwovariables,  1and  2.wehavedatafromthismodel,x   p(x|  1,  2)andindependentpriorsp(  1)andp(  2).we   ndthatwecan   tcalculatep(  1,  2|x)becausethenormalizingconstantisanintractableintegral.thisisthetypicalstorythusfar.   however,gibbssamplingmakestheaddedassumptionthatwecancalculatep(  1|x,  2)andp(  2|x,  1)usingbayesrule.inotherwords,ifwehadaccesstoallofthemodelvariablesexceptforoneofthem,thenwecouldcalculatetheposteriordistributionofthisoneunknownvariablegiventhedataandallothervariables.   gibbssamplingusesthispropertyofthetractabilityofallconditionalposteriordistributionstogetsamplesfromtheunknownfullposteriordistributionofallmodelvariables.below,wewritethegibbssamplingprocedureforthisgenericmodel.gibbssamplingalgorithmforthetoyproblem1.randomlyinitialize  (0)1andsample  (0)2   p(  2|x,  (0)1)2.forstept=1,...,t(a)sample  (t)1   p(  1|x,  (t   1)2)(b)sample  (t)2   p(  2|x,  (t)1)   inotherwords,we   rstinitializeallthevariablesinsomeway.step1aboveisonepossibleinitializationmethod.then,weiteratethrougheachvariableandupdateitbysamplingfromitsposteriordistributionconditionedonthecurrentvaluesofeverythingels.mcmctheoryprovesthatweget(approximate)samplesfromp(  1,  2|x)thisway,andgibbssamplingmcmcrequiresustobeabletocalculatetheconditionalposteriordistributions.   discussion:first,noticethattheconditionalposteriorusesthemostrecentvaluesforallvariables.therefore,whensamplingaparticular  iduringiterationt,somevariablesweconditiononwillusetheirvalueatiterationt   1(becausetheyhaven   tbeenupdatedyetduringiterationt),whileotherswillusetheirnewvalueatiterationt(becausetheyweresampledbeforesampling  iduringthecurrentiteration).whatgibbssamplingdoesnotdoispartitionthevariablesintotwosetsandsamplethenewsetatsteptconditionedontheoldsetfromt   1.7   togivesomemcmcbackgroundandhowitisruninpractice(inthecontextofthetoymodel):1.mcmcisaspecialcaseofamarkovchainwherethestationarydistributionisthedesiredfullposteriordistribution.therefore,weareonlyguaranteedtogetonesamplefromthefullposteriorinthein   nitelimit.(itisnecessarytotakeaclassdevotedtothesemethodstoappreciatethisfully.)2.inpractice,one   rstrunsmcmc(here,thegibbssampler)foralargenumberofsteps,letthatnumberbet1,andthenapproximatesthe   rstsamplefromtheposteriordistributionas(  (t1)1,  (t1)2).the   rstt1   1stepsarecalledthe   burn-in   phaseandisintendedtominimizetheimpactoftheinitialization,whichlikelydoesnotstartthechainoffwithagoodsettingforthemodelvariables.the   rstt1   1iterations      x   thisbadinitialization.3.nextwewanttogetasecondsamplefromtheposterior.however,rememberthatweneedthesamplestobei.i.d.formontecarlointegration.clearlyanytwoadjacentsamples(  (t)1,  (t)2)and(  (t+1)1,  (t+1)2)arenotindependentofeachother,sincethevaluesattwereusedtogetthevaluesatt+1.therefore,justlikewiththeburn-inphase,werunthechainforseveralmoreiterationsuntilstept2sothat(  (t1)1,  (t1)2)and(  (t2)1,  (t2)2)arealmosttotallyuncorrelated.thiscontinuesagainuntilstept3,thent4,etc.usually,ti   ti   1fori>1ispickedtobeaconstantnumbermuchlessthant1.4.goodnumbersfort1andti   ti   1arenotimmediatelyobvious.mcmctextbooksgiveusefulheuristics.inmachinelearningapplicationstheseareusuallypickedarbitrarilyinpracticeandareusuallydeterminedmorebythedesirednumberofsampleswithinapre-allottedrunningtimethanbythestatisticalpropertiesofthemarkovchain.5.wethenmaketheapproximationthat(  (t1)1,  (t1)2),(  (t2)1,  (t2)2),...,(  (ts)1,  (ts)2)iid   p(  1,  2|x)(25)noticethatwe   resimplythrowingawaytheupdatesformanyoftheiterations.bywritingouttheintegralformoftheexpectationbelow,weseewecanusethesesamplesformontecarlointegrationtoapproximatee[f(  x)|x]   1ssxs=1e[f(  x)|  (ts)1,  (ts)2](26)togivethefullderivation(letting  =(  1,  2):e[f(  x)|x]=zf(  x)p(  x|x)d  x=zf(  x)zp(  x|  )p(  |x)d  d  x   zf(  x)1ssxs=1p(  x|  (s))d  x=1ssxs=1zf(  x)p(  x|  (s))d  xthislasttermequals1spss=1e[f(  x)|  (s)1,  (s)2]andwejustre-indexthesamples.6.thereasonthatmcmc,despiteitsprecisetheory,isstillonlyanapproximatemethodisthefactthatt1andti   ti   1are   nite.therefore,atstept1wearenotsamplingfromthestationarydistributionofthemarkovchain(rememberthatthe   stationarydistribution   isthedesiredposteriordistribution),andthesamplesatstepstiandti   1arenot100%uncorrelatedwitheachother,andthereforenotindependent.nevertheless,mcmcoftenworksextremelywellinpractice.87.muchofthedevelopmentofmcmctechniquesnotcoveredinthisclass   inadditiontoaccountingformodelsinwhichwedon   thaveallconditionalposteriors   areonwaystosamplesuchthatthevaluesoft1andti   ti   1canbereducedwithoutsacri   cingthequalityofthesamples(i.e.,theirapproximateindependenceandclosenesstotheposteriordistribution).   returningtotheoriginalmodelwewerediscussing,wewantp(u,v|r)whereuisthesetofn   user   vectorsandvthesetofm   object   vectors,andristhesetofmeasuredvalues{rij:(i,j)      }oftencorrespondingtoaratinggivenbyuseritoobjectj.   wecan   t   ndthisposteriorexactly.anextstepistocheckifwecancalculatealltheconditionalposteriorsexactly.ifso,wecanapproximatelysamplefromp(u,v|r)usinggibbssampling.   wewillcheckforoneui.clearlythisthenestablishesthecaseforallui,andbysymmetrywecanperformthesameexactderivationforallvj.theconditionalposteriorofuiisp(ui|v,r)   yj:(i,j)      p(rij|ui,vj)p(ui)(27)1.noticewiththisthatwedecidedtowritethelikelihoodintermsofrijonlyandnotvj.2.also,noticethatonthelhsweconditiononallvandr.3.ontherhsweusethestructureofthemodelwede   ned:weonlyneedtoconsidertherijfora   xediandoverthosejsuchthatrij   d.also,thelikelihoodoftheserijareindependentgivenuiandv.foraparticularrij,conditioningonallvis   netowriteoutintheabstract,butwhenweactuallywriteoutthedistributionwewillseethat,accordingtoourmodelassumption,rijonlydependsonuiandvj,andnootheruorv.4.therefore,p(rij|ui,v)=p(rij|ui,vj).conditioningonmorethanisnecessaryisjustsuper   uousandmakesourparsingofthedistributionsandthedependenciestheyinduceamongthemodelvariableslesstransparent   itisgoodpracticetoonlyconditiononwhatisnecessaryaccordingtothemodelandpriorde   nitions.   usingthelikelihoodandpriorde   nedforthisproblem,wehavep(ui|v,r)   yj:(i,j)      normal(rij|utivj,  2)normal(ui|0,ci)(28)wehaveactuallyalreadycalculatedthislastlectureforthebayesianlinearregressionproblem.infact,fromtheperspectiveofuiit   sidenticallythesameproblem!fromthatcalculation,weknowthatp(ui|v,r)=normal(  ui,  ui)(29)  ui=      1ci+1  2xj:(i,j)      vjvtj         1  ui=  ui      1  2xj:(i,j)      rijvj         bysymmetry,wegetasimilarposteriordistributionforeachvj.theresultinggibbssamplingalgorithmforthismodelisthefollowing.9gibbssamplingalgorithmforthematrixfactorizationmodel1.randomlyinitializeeachu(0)iandv(0)j2.forstept=1,...,t(a)fori=1,...,nsampleanewuifromitsconditionalposteriordistribution,u(t)i   normal(  (t)ui,  (t)ui)whereatiterationtweusetheparameters  (t)ui=      1ci+1  2xj:(i,j)      v(t   1)j(v(t   1)j)t         1  (t)ui=  (t)ui      1  2xj:(i,j)      rijv(t   1)j      (b)forj=1,...,msampleanewvjfromitsconditionalposteriordistribution,v(t)j   normal(  (t)vj,  (t)vj)whereatiterationtweusetheparameters  (t)vj=      1ci+1  2xi:(i,j)      u(t)i(u(t)i)t         1  (t)vj=  (t)vj      1  2xi:(i,j)      riju(t)i         noticethat,becausewechoosetoupdateallui   rstineachiteration,theirconditionalposteriorsusethevaluesofvjinthepreviousiteration   thosearethemostrecentvaluesforvj.whenwethenupdateallvj,weusethevaluesofuiinthecurrentiterationbecausethesearethemostrecentvalues.thismightgivetheimpressionthattheorderinwhichweupdateeachvariablewillmakeadifference.however,orderdoesnotmatter.wecouldhavechosentoupdateallvj   rstduringeachiteration.orwecouldalternatebetweenuiandvjinanarbitraryway.allthatmattersisthat,whencalculatingtheconditionalposterior,weconditiononthemostrecentvaluesforallvariables.10eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture4,10/6/2016instructor:johnpaisley   sofar,we   vebeentalkingaboutposteriordistributionswhere:1.thepriorisconjugatetothelikelihood.inthiscasewecancalculatetheposteriordistri-butionexactlybygivingthedistributionnameanditsparameters.2.wecanmakeasimpleapproximationusingthelaplace   smethod.inthiscase,weapprox-imatetheposteriorofthemodelvariableswithamultivariategaussianandusethemapsolutionandthehessianofthelogjointlikelihoodevaluatedatthemapsolution.3.wecancalculateallconditionalposteriordistributionsofthemodelvariablesandapprox-imatelysamplei.i.d.fromtheposteriordistributionusingmcmcgibbssampling.   inmachinelearningapplications,the   rstapproachisrelativelyraresincethemodel   sareusuallytoocomplex.thesecondapproachisalsonotverycommonsincethereareusuallytoomanyvariablestoestimateafullcovariancematrixforthem.thethirdapproachiscommonlyused,butitsuffersfrompoorscalabilitytolargedatasets,whichiswhatmachinelearningproblemstypicallyencounter.arecentfocusofresearchinterestison   xingthisproblem.   thereisafourthmajortechniqueusedwidelyinthemachinelearningcommunitycalledvari-ationalid136.unlikemcmcmethods,whichsamplenewvaluesofthemodelvariablesineachiteration,variationalmethodssetupanobjectivefunction(notunlikemap)andoptimizethisobjectivefunctionoverparameters.aswewillseeinfuturelectures,theseparameterscor-respondtoparametersofdistributionsoverthemodelvariables.variationalmethodstryto   ndparameterstothesedistributionssuchthattheresultisacloseapproximationtothedesired,butintractableposteriordistribution.   thiswillbemademorepreciseinthenextfewlectures.however,tolaythegroundworkforunderstandingvariationalid136(vi)itisnecessarytogoovertheexpectation-maximization(em)algorithminitsfullgenerality.thegoalwillbetopresentviasaverysimplebutinterest-ingmodi   cationofem.   therefore,fortherestofthislecturewewillreturntotheworldoflearningpointestimatesofmodelvariables,insteadoftryingtolearntheirposteriordistributions.1maximumaposteriori(map)id136   recallthegeneralmodelingsetup:model:x   p(x|  )prior:     p(  )   weassumethatwecan   tcalculatetheposteriorp(  |x)exactly,soinsteadweusemapid136to   ndthevalueof  thatisamaximumofp(  |x).  map=argmax  lnp(x,  )(1)(whenlnp(x,  )isnon-convex,wecanusuallyonlyhopeto   ndalocalmaximum.thisistypicallythecase.)   mapid136isanoptimizationproblemwithabayesianinterpretationoftheobjectivefunc-tionandwhatisbeingoptimized.   ultimately,wewantto   ndthepoint  thatmaximizes,oratleastlocallymaximizes,theobjectivefunctionlnp(x,  ).   equivalently,wewanttosearchfora  suchthat     lnp(x,  )=0,sincethederivativeatanylocalmaximumwillequalzero.wealsoneedto   ndthispointinawaythatassuresuswehaven   tfoundalocalminimum,sincethegradientherewillbezeroaswell.   sometimeswecandothisinclosedform;wecantakethederivativeandsolvefor  .inthiscasetheproblemissolvedandnoiterativeoptimizationalgorithmisnecessary.   oftenwecan   t   ndasolutionfor  totheequation     lnp(x,  )=0.inthiscase,thetypicalapproachistousegradientmethods,whereweiterativelyupdate  accordingtotherule  0     +  b     lnp(x,  ).(2)thevalueof  >0isastepsizeandthematrixbispositivesemi-de   nite.whenb=i,theidentitymatrix,thisissteepestascent.whenbistheinverseofthenegativehessianoflnp(x,  ),thisisnewton   smethod.   whatweshouldset  andbtoarenotsolvedproblems.thisispartofthe   art   ofoptimizationandthereareseveralwaysof   guringoutwhattheseshouldbe,andtheymaychangewitheachupdateof  totakeintoaccountpropertiesoftheobjectivefunctionlnp(x,  )atthecurrentpoint.   inasense,wewanttohaveclosed-formsolutions.wewanttosaythat,forsuch-and-suchvariable,thebestsettingforitisequaltosomethingwecanwriteoutexactly,ratherthansomethingwestumid7ponafterbeingtoldwhichwaytomoveby50differentgradients(thenumber50isjustforexample).   theemalgorithmisaverycleverwaytodothis.beforediscussingemingeneral,let   s   rstlookatonemodelforwhichwemightgiveuphopeifwewantedtodomapbydirectlydoinggradientascentonthelogjointlikelihoodasdescribedabove.2probitregression   setup:thesetupisthesameasforlogisticregression:wehaveadataset,d={(xi,yi)}ni=1wherex   rdisafeaturevectorandy   {0,1}isitsclasslabel.wewanttousethisinformationtohelppredictthevalueofyforanewx.   modelandprior:theprobitmodelissimilartothelogisticregressionmodel.weassumethatthelabelsarebernoullirandomvariablesthatdependonthedotproductofthecorrespondingfeatureswithaweightvector,whichweassumehasagaussianprior,yi   bernoulli(  (xtiw/  )),w   normal(0,     1i)(3)thevalue  >0isaparameterwecanchange.aswiththelogisticregressionmodel,theprobitregressionmodelisadiscriminativeclassi   erbecauseitconditionsonxthroughoutinsteadofmodelingitwithadistribution.however,wherelogisticregressionusesthesigmoidfunctiontomapxtiwtoanumberbetween0and1,probitregressionusesthecumulativedistributionfunction(cdf)ofastandardnormaldistribution,  (xtiw/  )=zxtiw/        (2  )   12e   12s2ds(4)noticethat,aswiththesigmoidfunctiondiscussedinanearlierlecture,asxtiwincreasesto+   ,thefunction  (xtiw/  )increasesto1.asxtiwdecreasesto      ,thefunction  (xtiw/  )decreasesto0.(infact,itshouldbeclearthatanyfunctionthathasthepropertiesofacdfisacandidateforpluggingintothisbernoullidistribution.)   mapid136:ideally,wewoulddoposteriorid136forw.however,applyingbayesrulewequickly   ndthatwerunintothesameproblemasforlogisticregression:thenormalizingconstantisnotatractableintegral.anextstepwouldbetolearnthemapsolutionforw,wmap=argmaxwlnp(~y,w|x)(5)=argmaxwlnp(w)+nxi=1lnp(yi|w,xi)=argmaxw     2wtw+nxi=1yiln  (xtiw/  )+(1   yi)ln(1     (xtiw/  ))thisrequiresustocalculate   wlnp(~y,w|x).oneofthetermsinthisgradientis   wln  (xtiw/  )=   wlnzxtiw/        (2  )   12e   12s2ds(6)takingthefullgradientoflnp(~y,w|x),wecanseethat,notonlywouldwenotbeabletosolveforw,butit   snotevenclearwhattheanalyticsolutionto   wln  (xtiw/  )is!therefore,wecan   tevengetaclosedformvectorrepresentationforlnp(~y,w|x)thattellsuswhichdirectiontomoveintoupdatew.   sothequestionis,isprobitregressionevenhardertoworkwiththatlogisticregression?isittotallyhopeless?wewillseethattheemalgorithmcanbeusedtooptimizelnp(~y,w|x)inacleverwaythatinvolvesclosedformupdatesandresultsinanalgorithmnotmorecomplicated(andarguablycleanerandnicer)thangradientascentforlogisticregression.3settinguptheexpectation-maximization(em)algorithm   inthefollowingdiscussionoftheemalgorithm,wereturntothegeneralmodelsetting,wherex   p(x|  ),     p(  ).again,thegoalistodomapid136,wherewemaximizelnp(x,  )over  .wewillthenderiveanemalgorithmforprobitregressiontomakeitmorespeci   c.   emstartswithanassumption:assumethereissomeothervariable  thatwecanintroducetothemodelsothatthemarginaldistributionisunchanged.thatisp(x,  )=zp(x,  ,  )d     notethatwecanalwaysmanagetodothis.thepurposeforemwillbetopickavariable  thatisuseful.   inthenextlecturewewillseethat,whencomingatitfromtheotherdirection,thiscomesdirectlyoutofthemodelitself.thatis,inthislecturewearestartingwithp(x,  )and   ndingthatit   sproblematic,leadingusdownthisempath.oftenwestartwithamorecomplexmodelde   nitionp(x,  ,  )andthendecidetooptimizethemarginaldistributionofthisp(x,  ).inthiscaseweknow  becausewede   neditinthelargermodel.   again,onecasestartswithp(x,  )andexpandsit(thehardcase),andonecasestartswithp(x,  ,  )andcollapsesit(theeasycase).traditionallyanintroductionofemstartswiththehardcase.   thenextstepsarealldirectedtowards   ndinganequivalentrepresentationforlnp(x,  )thatinvolveslnp(x,  ,  ).thereasonwhythisnewrepresentationshouldhelpusinanywaytooptimizelnp(x,  )moreeasilywillcomelater.fornow,weareonlymanipulatingequations.   the   rststepistouseabasicruleofid203p(x,  )p(  |x,  )=p(x,  ,  )(7)andtakinglogarithmsandreorganizing,wethereforehavelnp(x,  )=lnp(x,  ,  )   lnp(  |x,  )(8)noticethatthelhsofthisequationistheoriginalobjectivefunctionwewanttomaximize.wecannowinterpretthisasthemarginallogjointlikelihoodoftheextendedlogjointlikelihoodontherhs.alsosubtractedontherhsistheposteriordistributionof  .inpracticewewill   ndthiswithbayesrule.fromthiswecanguess(andbecorrect)thatifp(  |x,  )isnotsolvable,theemalgorithmwe   reheadingtowardsisn   tgoingtomakeoptimizinglnp(x,  )easier.   thenextsteptowardtheem   masterequation   istointroduceaid203distributionq(  )suchthatqisde   nedonthesamesupportas  .thatis,if     r,thenqisde   nedonr.if     {1,2,3,4,5}thenqisa5-dimensionalid203vectoronthoseintegers.fornow,thisistheonlyrequirementwemakeonq(  ).later,emwilltelluswhatweshouldsetq(  )to.4   wethenuseq(  )inthefollowingsequenceofequalities:q(  )lnp(x,  )=q(  )lnp(x,  ,  )   q(  )lnp(  |x,  )(9)comment:wesimplymultiplythelhsandrhsbythesamething.itdoesn   tmatterthatthisthingisafunction.foreachvalueof  theequalityholds.zq(  )lnp(x,  )d  =zq(  )lnp(x,  ,  )d     zq(  )lnp(  |x,  )d  (10)comment:anintegralcanbethoughtofasanin   nitesimalsummation.foreachin   nitesimalbitweaddonthelhs,weaddanin   nitesimalbitontherhsthatequalsit.lnp(x,  )=zq(  )lnp(x,  ,  )d     zq(  )lnq(  )d  (11)   zq(  )lnp(  |x,  )d  +zq(  )lnq(  )d  comment:onthelhs,wenoticethatlnp(x,  )doesn   tinvolve  inanyway.thereforeit   saconstantfromtheperspectiveoftheintegralandcanbebroughtoutfront.theresultingintegralisofaid203distribution.eventhoughwehaven   tde   nedq(  )yet,weknowit   sintegralhastoequalto,hencethelhs.fortherhswehaveaddedandsubtractedthesamething,leavingtheresultunchanged.however,wedothisbecausetheresultwillgiveusinsightsastohowtodesigntheemalgorithmandproveitworks.lnp(x,  )=zq(  )lnp(x,  ,  )q(  )d  +zq(  )lnq(  )p(  |x,  )d  (12)comment:thislastequalityistheemmasterequationwehavebeenaimingtoarriveat.thisequationwilltellus(1)whatweshoulddo,and(2)whyitwillwork.   noticethatthegoalhasbeenmorethanjusttoarriveatanequalityforlnp(x,  ).wealreadyhadthatwithlnp(x,  )=lnp(x,  ,  )   lnp(  |x,  ).however,thisequalitydoesn   timmediatelyhelpus.wewillseethatthe   nalequationabovedoesgiveusapotentiallyusefulalgorithm.   let   slookattheemequationterm-by-term.1.lnp(x,  ):thisissimplytheobjectivefunctionwewanttooptimize.2.l(  ):=rq(  )lnp(x,  ,  )q(  )d  :thisisafunctiononlyof  sinceweintegrate  out.ofcourse,thefunctionlweendupwithdoesdependonwhatq(  )is.3.kl(qkp):=rq(  )lnq(  )p(  |x,  )d  :thistermiscalledthekullback-leiblerdivergence.itisveryimportantandworthlookingatmoreclosely.kullback-leibler(kl)divergence   thekl-divergenceisafunctionoftwoid203distributions.thatis,theinputtokl(qkp)includesid203distributionsq(x)andp(x)suchthattheyarede   nedonthesamesupport(i.e.,thesamevaluesforxcanbeinputtobothofthem).theoutputisanumber.5   thekl-divergenceisasimilaritymeasurebetweenqandpinthatkl(qkp)   0forallqandpandkl(qkp)=0onlyinthecasewhereq(x)=p(x)forallx   thatis,whenqandpareexactlythesamedistribution.   thekl-divergenceisnottechnicallyadistancemeasurebecause,forthattobethecase,wewouldneedthatforanythreedistributionsq,pandf,kl(qkf)+kl(fkp)   kl(qkp)(calledthetriangleinequality),andthiscanbeshowntonotalwaysbetrue.   wecanshowthatkl(qkp)   0byusingtheconcavityofln(  ).becauseln(  )isconcave,byjensen   sinequalitywecansaythatforanydistributionq(x)onx,eq[lnx]   lneq[x].thisinequalityitselfrequiresaproof,whichwillbegiveninanyconvexoptimizationclass.fornowwewillonlyusethispropertyofthelnfunction.first,noticethat0=ln1=lnzp(x)dx=lnzq(x)p(x)q(x)dx(13)thelastinthisseeminglytrivialsequenceofequalitiescanbeinterpretedaslneq[p(x)q(x)].fromjensen   sinequalityandtheconcavityofln,0=lneqhp(x)q(x)i   eq(cid:20)lnp(x)q(x)(cid:21)=zq(x)lnp(x)q(x)dx(14)andweimmediatelyhavethenon-negativityofklbecause0   zq(x)lnp(x)q(x)dx   0   zq(x)lnq(x)p(x)dx:=kl(qkp)discussionontheemequality   beforepresentingtheemalgorithmandthenshowingwhyitworks,wemaketwoobservationsabouttheequalitylnp(x,  )=zq(  )lnp(x,  ,  )q(  )d  |{z}=l(  )+zq(  )lnq(  )p(  |x,  )d  |{z}=kl(qkp)1.forany   xedvalueof  ,changingq(  )doesn   tchangewhattherhsaddsuptobecausethelhsonlychangeswith  .allthatischangingwithq(  )isthebreakdownofhowmuchthe   rstandsecondtermoftherhscontributetothe   xedtotal.2.sincekl(qkp)   0,theterml(  )   lnp(x,  )andweonlyhavel(  )=lnp(x,  )whenq(  )=p(  |x,  ).3.onemightbetemptedtoask:ifweknowp(  |x,  ),canwejustde   neq(  ):=p(  |x,  )andplugbackin?theanswerisyes,butthiswilldefeatthepurposeofem.noticethatifwepluginthisvalueforq(  ),wewillgetoutthatlnp(x,  )=zp(  |x,  )lnp(x,  ,  )p(  |x,  )d  we   vesimplyfoundanotherwaytowritelnp(x,  )asafunctiononlyof  ,andifthere   snoeasysolutionfor  forthelhs,therewon   tbeonefortherhseither.thetrickistokeepq(  )asaseparateobjectfrom  ,anduseoneofthemwhenupdatingtheother.6theemalgorithm   usingtherhsoftheemequalityonecandeviseameansforiterativelyupdating  ,andthenprovethatitismonotonicallyincreasinglnp(x,  ).thatis,intheiterativealgorithmbelow,wewillgetoutasequence  1,  2,  3,....we   rstfocusonhowtogetthissequence,thenwewillshowthat,usingthisprocedure,thegeneratedsequencehasthepropertythatlnp(x,  1)   lnp(x,  2)   lnp(x,  3)         (15)   theemalgorithmistraditionallybrokenintotwosteps,an   e   (expectation)stepandan   m   (maximization)step.eachiterationofthealgorithmconsistsofoneeandonemstep,whicharethenrepeatedinthenextiteration.e-stepatiterationt:setqt(  )=p(  |x,  t   1)andcalculatelt(  )=zqt(  )lnp(x,  ,  )d     zqt(  )lnqt(  )d  (16)noticethatqt(  )istheconditionalposteriorusingthevalueof  fromthepreviousiteration.also,noticethatthesecondterminlt(  )issimplyaconstantw.r.t.  .m-stepatiterationt:treatlt(  )asafunctionof  and   ndthevalueof  thatmaximizesit,  t=argmax  lt(  )(17)becausethesecondterminlt(  )doesn   tfactorintothismaximization,inpracticeitisonlynecessarytocalculaterqt(  )lnp(x,  ,  )d  andmaximizethisover  .   beforeweanalyzewhatthisisdoing,therearetwocrucialassumptionsunderlyingthisalgorithmthatwouldmakeitpreferabletodirectlyoptimizinglnp(x,  )usinggradientmethods:1.first,weassumewecancalculatep(  |x,  )inclosedformusingbayesrule.ifwecan   tthenwe   realreadystuckwhentryingtoupdatel(  ),whichrequiresthisdistribution.2.second,inthem-stepwehavetooptimizeoverlt(  ),whichisafunctionof  .theorig-inalthingwewanttooptimizeover,lnp(x,  ),isalsoafunctionof  .ifoptimizinglt(  )isnoeasierthanlnp(x,  ),thenthere   snotmuchpointtothis.thatis,ifwecan   tsolve     lt(  )=0for  analytically,thenwe   rebackwherewestarted.whilethefollowingstatementmightnotbetrue100%ofthetime(butican   tthinkofacounter-exampleoffhand),inbig-picturetermsifyouneedtousegradientmethodstooptimizelt(  ),youmightaswelljustusegradientmethodstooptimizelnp(x,  )instead.   therefore,justlikegibbssamplingrequiredanadditionalassumptionaboutthemodelforittobeuseful(thattheconditionalposteriordistributionsofallvariablesareinclosedform),emalsomakesassumptionsaboutthemodelbeforeclaimingtobeuseful(#1and#2above).ofcourse,emwilltechnicallyworkif#2aboveisn   tsatis   ed,whichissomethingtoatleastkeepinmind.7analysisoftheemalgorithm   thelastpartofthisdiscussiononemwillshowthatthesequence  1,  2,...wegetfromthisprocedureismonotonicallyincreasinglnp(x,  ).first,recallthatlt(  )=zqt(  )lnp(x,  ,  )qt(  )d  ,kl(qt(  )kp(  |x,  ))=zqt(  )lnqt(  )p(  |x,  )d  thefollowingsequenceofinequalitiesshowsthatlnp(x,  t   1)   lnp(x,  t),followedbyourelaborationoneachline.inthetransitionfromiterationt   1totwehavethatlnp(x,  t   1)=lt(  t   1)+kl(qt(  )kp(  |x,  t   1))(18)=lt(  t   1)(19)   lt(  t)(20)   lt(  t)+kl(qt(  )kp(  |x,  t))(21)=lnp(x,  t)(22)1.the   rstlineissimplytheemmasterequationusingq(  )   qt(  )=p(  |x,  t   1)andevaluatingthefunctionsat  =  t   1.2.sinceqt(  )=p(  |x,  t   1),thekl-divergenceequalszero,sowecanremoveit.thesetwolinesconstitutewhatwedoforthee-step.3.weknowthatlt(  t   1)   lt(  t)because  t=argmax  lt(  ).thisisthem-step.4.thenextquestionishowlt(  t)relatestolnp(x,  t).inthediscussionabovewehavealreadybeenabletoshowthatl(  )   lnp(x,  ).however,toseethisagainexplicitly,weaddastrategicallychosennon-negativenumbertolt(  t).speci   cally,weaddthekl-divergencekl(qt(  )kp(  |x,  t)).whenwedothis,weseethatthesecondtolastlineisagainsimplytheemequationforlnp(x,  t).   alsoworthemphasizinghereisthatkl(qt(  )kp(  |x,  t))>0becauseqt(  )6=p(  |x,  t)sinceqt(  )=p(  |x,  t   1)andwecanassume  t   16=  t(otherwisethealgorithmhasconverged).therefore,wecanreturntothee-stepbyupdatingq(  )atiterationt+1toaccountforthenewvalueof  .hencethereisanaturalloopwecaniteratebackandforthbetween,whereweupdateq(  )andthenupdate  .theaboveinequalitiesshowthatanysinglecompletionofthiscyclewill   ndanew  thatisbetterthantheoldoneintermsofmaximizinglnp(x,  ).   the   nalquestioniswhetherthesequence  1,  2,...isconvergingtoalocaloptimalsolutionoflnp(x,  ).itiseasytothinkhowwecouldhavelnp(x,  t)   lnp(x,  t+1)forallt,butalsohave     notbeatthetopofoneofthe   hills   oflnp(x,  ).itcanbeshownthatemdoesconvergetooneofthesepeaks,buttheproofissigni   cantlymorecomplicatedandsoweskipitinthisclass.suf   ceittosaythatemwillgivethemapsolution(oralocaloptimalofit)foranyinterestingmodelyouwillcomeacross.theemalgorithmforprobitregression   next,wederiveanemalgorithmfortheprobitregressionmodel.inthismodeltheweightswcorrespondto  .we   rstneedto   ndagood  thatwillmakethingsworkoutnicely.it   sworth8dwellingonthisforawhilebeforejumpingtothealgorithm.eventhoughthisquali   esasastraightforwardapplicationofem,thisisagoodexampleofhowthegenericviewtakenabovedoesn   tquitemakederivinganemalgorithmforaspeci   cproblemanautomaticprocedure.   forthismodel,thejointlikelihoodfactorizesasp(~y,w|x)=p(w)nyi=1p(yi|w,xi)(23)   withem,ourgoalisto   ndarandomvariable,  suchthatzp(~y,w,  |x)d  =p(~y,w|x)(24)   actually,herewewillseehowourabovesimpli   edversionofem,wherewehadone  andone  ,canbegeneralized.instead,let  =(  1,...,  n).wepickthesesuchthatzp(~y,w,  |x)d  =nyi=1zp(yi,  i,w|x)d  i=p(~y,w|x)(25)therefore,weassumethatthevariables  iareindependentfromeachotherandeach(xi,yi)pairhasa  iassociatedwithitinsomeway.   noticethatbyassumingthatthe  iareindependentofeachotherandonlydependenton(xi,yi)andw,theposterioronthevector  =(  1,...,  n)factorizesaswellp(  |~y,w,x)=qni=1p(yi|  i,w,xi)p(  i|w,xi)qni=1rp(yi|  i,w,xi)p(  i|w,xi)d  i=nyi=1p(yi|  i,w,xi)p(  i|w,xi)rp(yi|  i,w,xi)p(  i|w,xi)d  i=nyi=1p(  i|yi,w,xi)(26)   howdoesthisimpacttheemequation?wehavethatlnp(~y,w|x)=lnp(w)+nxi=1lnp(yi|w,xi)(27)forasinglelnp(yi,w|xi)wehavefromthederivationabovethatlnp(yi,w|xi)=zq(  i)lnp(yi,  i|w,xi)q(  i)d  i+zq(  i)lnq(  i)p(  i|yi,w,xi)d  i(28)andsosummingthelhsandrhsoveriwehavethatlnp(~y,w|x)=lnp(w)+nxi=1zq(  i)lnp(yi,  i|w,xi)q(  i)d  i+nxi=1zq(  i)lnq(  i)p(  i|yi,w,xi)d  i(29)9   tobelaborthepoint,wecouldhavegottentothislastemequationfromtheotherdirection,anditcanbeworthwhiletoseeitfromthisperspectiveaswellw.r.t.choosingq.let  bethelatentvariableweintroduce   possiblyavector.fromemweknowthatlnp(~y,w|x)=zq(  )lnp(~y,  ,w|x)q(  )d  +zq(  )lnq(  )p(  |~y,w,x)d  (30)   next,assumethatwepick  =(  1,...,  n)suchthatp(~y,  ,w|x)=qni=1p(yi,  i,w|xi).thenweshowedabovethatp(  |~y,w,x)=qni=1p(  i|yi,w,xi).fromtheemalgorithm,weknowthatweneedtousethisposteriortoselectq,q(  )=p(  |~y,w,x)=nyi=1p(  i|yi,w,xi)(31)thisimpliesafactorizationoftheq(  )distribution(whichresultsintheaboveemequation)q(  )=nyi=1q(  i)andq(  i)=p(  i|yi,w,xi)(32)   therefore,ourgoalisthreefold:1.de   ne  isuchthatzp(yi,  i|w,xi)d  i=p(yi|w,xi)=  (cid:16)xtiw  (cid:17)yi(cid:20)1     (cid:16)xtiw  (cid:17)(cid:21)1   yi2.deriveaclosed-formposteriordistributionp(  i|yi,xi,w)usingbayesrule.3.calculatetheexpectationl(w)and   ndthatwecananalytically   ndthevalueofwthatmaximizesit.   ingeneral,once#1isdone,#2and#3followimmediately(oronthe   ipside,weimmediatelyrealizethatthe  wepickedwon   tworkandweshouldtryagain).it   sstep#1thatrequirestheinsightsthatcomefromexperienceworkingwithmodels,andthatcan   tbereducedtoasetofinstructionslike#2and#3can.therefore,#1isgoingtoappearlikeit   scomingoutofnowherebelow,while#2and#3willbemorestraightforward.   step1:considerthefollowingexpandedmodelyi=1(  i>0),  i   normal(xtiw,  2)(33)noticethateventhoughwehaveanindicatorfunctionforyi,wecanstillviewthisasa(deter-ministic)id203distribution:p(yi=1|  i)=1(  i>0).inotherwords,yiisn   trandomgiven  i,butitmakesperfectmathematicalsensetowritethejointlikelihoodp(yi=1,  i|w,xi)=p(yi=1|  i)p(  i|w,xi)=1(  i>0)(2    2)   12e   12  2(  i   xtiw)2(34)10foryi=0,weusetheindicator1(  i   0)instead.nextweneedtocalculatethemarginaldistributionp(yi|w,xi).clearlyzp(yi=1,  i|w,xi)d  i=z         1(  i>0)(2    2)   12e   12  2(  i   xtiw)2=z   0(2    2)   12e   12  2(  i   xtiw)2=p(  i>0)(35)however,rememberthatweneedtoshowthatthisequals  (xtiw/  )=rxtiw/        (2  )   12e   12s2ds.todothis,we   rstobservethatwecandrawarandomvariable  i   normal(xtiw,  2)asfollows  i=xtiw+  s,s   normal(0,1)(36)therefore,theid203p(  i>0)=p(xtiw+  s>0)=p(s>   xtiw/  )the   nalstepistorecognizethatp(s>   xtiw/  )=p(s   xtiw/  )sincesisastandardnormaldistributionsymmetricaroundzero.therefore,zp(yi=1,  i|w,xi)d  i=p(  i>0)=p(s   xtiw/  )=  (xtiw/  )(37)   wehavethereforefoundahierarchicalexpansionoftheprobitregressionmodel,yi=1(  i>0),  i   normal(xtiw,  2),w   normal(0,     1i)ifweintegrateoutallthe  iinthismodel,wereturntotheoriginalprobitregressionmodel.be-foremovingon,it   sworthdiscussingthisalittlemore.first,there   snothinginherently   right   or   correct   aboutaprobitmodel.becausewepickedthatasourdesiredmodel,wehadtodosomeworktogettothishierarchicalrepresentationforem.however,theprobitmodelwasonlypickedbecauseit   makessense   todoit.thepointiammakingisthat,inmyopinion,theabovemodelmakesalotofsensetoo   wecouldhavemadethismodelourstartingpoint.inthatcase,wecouldhavedonemaponwandthevector  .or,wecouldhavedecidedtointegrateoutalluncertaintyin  anddomaponlyforw.asshownbelow,emwouldthengiveusconditionalposteriordistributionsonthe  iratherthanapointestimate.ingeneral,themorevariablesyoucanintegrateoutthebetter(onereasonisthatthemodelwillavoidover-   tting).comingatemfromthisperspective,theproblemismucheasiersincewealreadyknowthe   hidden   variable  tointegrateout   afterall,itwaspartofthede   nitionofthemodelinthiscase.   step2:we   vefoundalatentvariable  ithatgivesthecorrectmarginaldistribution.nextweneedtocalculatetheposteriorp(  i|yi,xi,w).bybayesrule,p(  i|yi,xi,w)=p(yi|  i)p(  i|xi,w)rp(yi|  i)p(  i|xi,w)d  i=1{sign(  i)=2yi   1}e   12(  i   xtiw)2r         1{sign(  i)=2yi   1}e   12(  i   xtiw)2d  i(38)11unfortunatelytheindicatordoesn   tlooknice,butallitissayingisthat  imustbepositiveifyi=1andmustbenegativeifyi=0.thisdistributioniscalledatruncatednormaldistribution.   ifyi=1,thenthetruncatednormaldistributiontn1(xtiw,  2)isde   nedtobethepartofnormal(xtiw,  2)de   nedonr+re-normalizedtogiveaid203distribution.inotherwords,it   sthedistributionofagaussianrandomvariableconditionedonknowledgethatitispositive.thereverseholdswhenyi=0,inwhichcasewecanwritetn0(xtiw,  2)de   nedonr   .   step3:finally,weneedtocalculatel(w)=lnp(w)+nxi=1eq[lnp(yi,  i|w,xi)]+constant(39)sincethejointdistributionincludingtheextra  variablesisp(~y,  ,w|x)=p(w)nyi=1p(yi|  i)p(  i|xi,w)(40)wehavethatl(w)=     2wtw+nxi=1eq[ln1{sign(  i)=2yi   1}]|{z}=0   12  2eq[(  i   xtiw)2]+const.(41)oneoftheexpectationsalwaysequalszero,sinceq(  i)=tnyi(xtiw,  2),andsoitisonlyintegratingovervaluesof  iforwhich1{sign(  i)=2yi   1}=1.also,ifweexpandthesquareoftherightmostterm,wecanputanythingnotinvolvingwintotheconstant,sincewewanttomaximizel(w)overw.asaresult,wewanttomaximizel(w)=     2wtw   nxi=112  2(wtxixtiw   2wtxieq[  i])+constant(42)solvingfor   wl(w)=0,we   ndthatw=argmaxwl(w)   w=   i+nxi=1xixti/  2!   1 nxi=1xieq[  i]/  2!(43)wejustneedtoknowwhateq[  i]isundertheconditionalposteriorq(  i)=tnyi(xtiw,  2).lookingthisupinatextbookoronwikipedia,we   ndthateq[  i]=                           xtiw+      0(   xtiw/  )1     (   xtiw/  )ifyi=1xtiw+         0(   xtiw/  )  (   xtiw/  )ifyi=0(44)thefunction  0(s)istheid203densityfunction(pdf)ofanormal(0,1)distributioneval-uatedats.  (s),asbefore,isthecdfofanormal(0,1)distributionevaluatedats.thesecanbequicklyevaluatedbycallingabuilt-infunction.12   noticethat,eventhoughittookseveralstepstogettoa   nalemalgorithm,wehaveeverythingwewant:1.anexpressionfortheconditionalposteriordistributionp(  i|yi,w,xi)asaknowndistri-bution(eventhoughit   sanatypicaldistribution,it   sstilloneweknowhowtotaketheexpectationwithrespectto,whichisallthatmattershere)2.aclosedformexpressionforupdatingwthatwecanevaluatequicklyincodewithouthavingtouseiterativegradientmethods.   itmighttakeafewreadingsoftheabovetoappreciatealltheynuancesoftheemalgorithmforprobitregressionandwhyit   scorrect(i.e.,maximizeslnp(~y,w|x)overw).however,the   nalalgorithmcanbesummarizedveryeasily.anemalgorithmforprobitregression1.initializew0toavectorofallzeros.2.foriterationt=1,...,t(a)e-step:calculatethevectoreqt[  ]=(cid:0)eqt[  1],...,eqt[  n](cid:1),whereeqt[  i]=                           xtiwt   1+      0(   xtiwt   1/  )1     (   xtiwt   1/  )ifyi=1xtiwt   1+         0(   xtiwt   1/  )  (   xtiwt   1/  )ifyi=0(b)m-step:updatethevectorwusingtheexpectationsaboveinthefollowingequationwt=   i+nxi=1xixti/  2!   1 nxi=1xieqt[  i]/  2!(c)calculatelnp(~y,wt|x)usingtheequationlnp(~y,wt|x)=d2ln(cid:16)  2  (cid:17)     2wttwt+nxi=1yiln  (xtiwt/  )+nxi=1(1   yi)ln(1     (xtiwt/  ))   part2ccanbeusedtoassessconvergenceandthereforedeterminewhattshouldbe.practicallyspeaking,itcanalsobeusedtomakesureyourimplementationiscorrect,sinceweknowfromtheearlierproofthatlnp(~y,wt|x)mustbemonotonicallyincreasingint.ifyou   ndthatthisisnotthecasewithyourimplementation,thenyoucanbesurethereisabugsomewhere.13eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture5,10/13/2016instructor:johnpaisley   lastweekwetalkedaboutusingtheemalgorithmformapid136.today,wewillapplyemtoasequenceofmodelstoseewhereembreaksdownandvariationalid136(vi)takesover.thepurposeistodiscusseminmoredetailandalsoseehowitcloselyrelatestovaria-tionalmethods.variationalid136providesageneralwaytoapproximatethefullposteriordistributionofallmodelvariables,andisanewtechniquetoaddtothearsenal,alongwiththelaplaceapproximationandmcmcmethods(ofwhichweonlydiscussedgibbssampling).the   coremodel      wearemoreinterestedinthegeneralemandviid136techniques,butithinkthey   reeasiertodifferentiatethroughaneasymodelingexample.therefore,wewillbuildonthebasiclinearregressionmodeldiscussedinanearlierlecture.ourdevelopmentofthismodelwillnotbethemainthingofinterest.instead,wewillbemoreinterestedinwhatwecandowitheachversionofthismodelinthecontextofemandvi.   rememberthatwe   regivenpairsd={(xi,yi)}ni=1wherex   rdandy   randwemodelthisdataasyiind   normal(xtiw,     1),w   normal(0,     1i)(1)thereisnopriordistributiononx,andsoweareinthe   discriminative   setting.thisisincontrasttothebayesclassi   erwherewehadpriorsonxandsowereinthe   generative   setting.   posteriorcalculation:earlier,wesawhowwecouldcalculatetheposteriordistributionofthismodelinclosedform,p(w|x,y)=p(y|w,x)p(w)p(y|x)=normal(  ,  )(2)where  =(cid:16)  i+  nxi=1xixti(cid:17)   1,  =  (cid:16)nxi=1  yixi(cid:17).(thistime,welettheindex-freexandyrepresentallxiandyi,respectively.)1   therefore,forthissimplemodeltheid136problemisveryeasy.noiterativealgorithmsareneededandwecandirectlygotothefullposteriorofallmodelvariables(inthiscase,onlyw)inonestep.wenextdevelopthismodelinawaywherethisisnotpossible.   wenoticethattherearetwomodelparametersthatneedtobesetandwilllikelyhaveasigni   cantimpactonp(w|y,x).thatis,eventhoughwehaveamathematicallycorrectanswerforthispos-teriordistribution,itisonlycorrectundertheassumptionswemakeaboutthevaluesof  and  .(andasemphasizedbefore,ontheassumptionofthemodeltobeginwith,whichisasimplifyingassumptionaboutthedata-generatingprocessesunderlyingareal-worldphenomenon.)   therefore,eventhoughwehavesolvedforp(w|y,x),wecan   tclaimthatwe   vesolvedtheproblemofmodeling(x,y)withalinearregressionmodel   weprobablycanneverclaimthat.   since  correspondstotheinversenoisevariance,itarguablyhasthegreaterimpactonourpredictions,solet   saddressthisone   rst:   thedefaultsolutioninthewidermachinelearningcommunityistouse   cross-validation.   inshort,thatmeanswetryabunchofvaluesfor  andseewhichoneisbestaccordingtosomeproblemwe   reinterestedin.   inthebayesianmodelingworld,thedefaultsolutionistoputapriordistributionon  andtrytolearnitusingposteriorid136.coremodel(version2.0)   weexpandthecoremodelhierarchicallybyputtingapriordistributionon  .outofconveniencewepickaconjugategammaprior,yiind   normal(xtiw,     1),w   normal(0,     1i),     gamma(a,b)(3)   aswasthecasewithw,byaddingaprioron  ,wearecommittingourselvestooneofthefollowing:1.trytolearnthefullposteriorof  andw(exactlyorapproximately)2.learnapointestimateofthevariablesviamapid1363.integrateout(ormarginalize)somevariablestolearntheothers(e.g.withpointestimate)   let   sconsidereachofthese:1.thefullposteriordistributionof  andwisp(w,  |y,x)=p(y|w,  ,x)p(w)p(  )rrp(y|w,  ,x)p(w)p(  )dwd  (4)whenwetrytocalculatethenormalizingconstant,weseethatoneintegralcanbeper-formed(eitherover  orw),butthismakestheotherintegralintractable.inthiscase,theinteractionsbetweenwand  inthemodelaresuchthatwecan   t   ndthefullposterior.2(thisisnotalwaysthecase,e.g.,inthe   rsthomework.)toapproximatethisposterior,wecoulduselaplaceormcmcsampling(gibbssamplingwillworkinthiscase).how-ever,sincewe   remoreinterestedinintroducingnewid136techniquesthaninsolvingspeci   cmodelsnow,let   sconsiderotheroptions.2.wecouldalsosimplydomapoverwand  ,w,  =argmaxw,  lnp(y,w,  |x)(5)=argmaxw,  lnp(y|w,  ,x)+lnp(w)+lnp(  )wecantakethederivativeofthiswithrespecttowor  andsolveforeachvariableinclosedform(butnotbothatonce).thiswouldleadtoa   coordinateascent   algorithm,whereweiteratebetweenmaximizingw.r.t.wor  holdingtheotherone   xed.however,thisinsomesenseisunsatisfyingbecausewe   regivingupallmeasuresofun-certaintyandlearningapointestimateofallvariables.inanycase,iarguethatitshouldbeunsatisfyinginlightofoption#3.3.anotheroptionistointegrateout  .thatis,tocalculatethemarginallikelihoodp(y,w|x)=zp(y,w,  |x)d  (6)wehaveafewoptionsafterdoingthis.the   rstonetotryistodoposteriorid136forp(w|y,x)usingbayesrule.however,noticethatadifferentmodelisimpliedhere.whenwewritep(w|y,x)   p(y|w,x)p(w),weseethatthetermp(y|w,x)isn   tagaussianany-moresinceit   sfoundbyintegratingout  .thatis,p(y|w,x)=rp(y,  |w,x)d  ,whichgivesastudent-tdistribution,notagaussianlikeinthecoremodel.1therefore,thepriorandlikelihoodaren   tconjugateanymore.anotheroptionwouldbetomaximizep(y,w|x)overwusingmap.however,usingthestudent-tmarginalp(y|w,x),weseethat   wlnp(y|w,x)p(w)=0doesnothaveaclosedformsolutionforw.therefore,wewouldhavetousegradientmethodsifwewantedtodirectlymaximizep(y,w|x)overw.however,thissetupisreminiscentofem...   let   snowfocusonoption#3.wewanttomaximizep(y,w|x)=rp(y,w,  |x)d  overw.doingsodirectlyworkingwithlnp(y,w|x)requiresgradientmethods,whichissomethingwemightwanttoavoid.(thisisnotablanketstatementandoptimizationresearcherswouldprobablydisagree.butifwehaveanoptionthatgivesclosedformupdates,wewouldratherusethat.)   rememberfromourdiscussionontheemalgorithmthatwewanttomaximizeajointlikelihood,e.g.,p(y,w|x),butwewanttoavoidgradientmethods.tothisend,weneedto   ndahiddenvariable,e.g.,  ,suchthatp(y,w|x)=rp(y,w,  |x)d  .ourinitialdiscussionstartedwithde   ningthemodelp(y,w|x)andthetrickypartwas   ndingthe  .inthiscase,thelatentvariableisactuallypartoftheoriginalmodelde   nitionandwearethen   ndingapointestimateofthemarginaldistributionofthatmodel.1inthatmodel,  wasaparametersowedidn   tbothertowritep(y|w,x,  ),butthat   stechnicallywhatp(y|w,x)correspondstointhecoremodel,whereasnowp(y|w,x)correspondstotheaboveintegralover  .31.the   rstdirectionwashard:startwithadesiredmarginaldistributionandintroduceavariablethatgivesthatmarginaldistributionafterintegratingitout.2.thisnewdirectioniseasy:startwiththeexpandedmodelasade   nitionandthentrytomaximizeoverthemarginal.wecareaboutthedistributionsinthelargermodel,notthedistributionthatcomesfrommarginalizing.   whateverp(y,w|x)=rp(y,w,  |x)d  endsupbeingiswhatitendsupbeing.   we   rejustgoingtomaximizeit,possiblywithoutevenknowingwhatitis,sinceemdoesn   trequireustoeversolvethisintegralinordertooptimizethemarginaloverw.emforthecoremodel(version2.0)   therefore,we   regoingto   ndapointestimateofwtomaximizethemarginaldistributionp(y,w|x)=rp(y,w,  |x)d  ofthesecondversionofthecoremodel.we   lldosobytreat-ing  astheextravariableinanemsetting.theemequationinthiscaseislnp(y,w|x)=zq(  )lnp(y,w,  |x)q(  )d  |{z}l(w)+zq(  )lnq(  )p(  |y,w,x)d  |{z}kl(qkp)(7)e-stepcanwe   ndp(  |y,w,x)?p(  |y,w,x)   nyi=1p(yi|  ,w,xi)p(  )(8)     n2e     2pni=1(yi   xtiw)2|{z}productofnormallikelihoods    a   1e   b  |{z}gammaprior=gamma(cid:16)a+n2,b+12nxi=1(yi   xtiw)2(cid:17)(9)sowecansetqt(  )=p(  |y,wt   1,x)atiterationt.wethencalculatetheexpectation,lt(w)=eq[lnp(y,  |w,x)p(w)]   eq[lnq(  )](10)=   eqt[  ]2nxi=1(yi   xtiw)2     2wtw+constantw.r.t.wm-stepwecan   ndqandcompletethee-stepbycalculatingananalyticfunctionl(w).next,weneedtoseeifwecanmaximizel(w)inclosedform.ifnot,thenwe   relikelynobetteroffthanwewerewithlnp(y,w|x).differentiating   wlt(w)andsettingtozero,we   ndthatwt=(cid:16)  i+eqt[  ]nxi=1xixti(cid:17)   1(cid:16)nxi=1eqt[  ]yixi(cid:17)(11)theexpectationof  isofagammarandomvariablewiththeparametersfromthee-step,soeqt[  ]=a+n2b+12pni=1(yi   xtiwt   1)2(12)4   noticethatifweplugthisexpressionforeqt[  ]directlyintotheupdateofwt,theemalgorithmisgivingusawayofiterativelyupdatingwusingthepreviousvalue.   alsonoticethat,whilethisisa   pure   emproblemalongthelinesofwhatwediscussedlasttime,theinterpretationherefeelsslightlydifferent:1.lastweek,thelatentvariableweintroducedwasjustasteppingstonetogetthepointestimatewewanted.wedidn   tmotivatethatnewvariableoritsqdistributionasbeingsomethinginterestingitself.2.inthismodel,thelatentvariable  hasaclearinterpretationasrelatingtotheobservationnoise.thiswasoriginallyanimportantmodelparameterthatwedecidedtoputapriordistributionon.therefore,wehaveaclearpictureofwhatthe   introduced   variable  meanshere,sinceitwasintroducedatthepointofthemodelformulationandnotatthepointofid136.therefore,wecanthinkofq(  )=p(  |y,w,x)asbeinginterestinginitsownrightsinceit   stheconditionalposteriordistributionofanimportantmodelvariable.3.therefore,usingemforthisproblemwemakeacompromise:wecan   tgetthefullposte-riorp(w,  |y,x),butwedon   twanttomakepointestimatesforthesetwovariableseither,sowesettleforapointestimateofwandaconditionalposteriorof  .4.alsonoticethatwecouldhavedonethereverse:wecouldhavelearnedapointestimateof  andaconditionalposteriorq(w)distribution.inthiscase,wewouldbedoingmapforp(y,  |x)=rp(y,w,  |x)dw.forthisspeci   cmodel,theemalgorithmwouldworkperfectlywellsinceq(w)=p(w|y,x,  )isamultivariategaussian,l(  )isanalyticandmaximizedinclosedform.however,wisclearlynotjustasteppingstonetolearn  !infact,wecouldinterpretemhereasallowingustocalculatetheposteriorp(w|y,x)oftheoriginal   coremodel   whileadditionallyhelpingussettheparameter  .coremodel(version3.0)   ifweweredoingemforversion2.0ofthecoremodel,wewouldprobablywanttomaximizepoint-wiseover  andlearnq(w)asdiscussedinpoint#4above.however,thisdiscussionisdirectedprimarilytowardsconnectingemwithviandsothedevelopmentinthisnextversionwouldnotworkoutthewayiwantitto.   we   veintegratedout  ,butwhatabout  ?wecanagainupdatethemodelbyaddingaconve-nientlychosenpriorhere,yiind   normal(xtiw,     1),w   normal(0,     1i),     gamma(a,b),     gamma(e,f)   again,wecantrytolearnthismodelusinglaplace,ormcmcsampling(gibbssamplingworkshere),orpointestimatesofw,  and  ,orpointestimatesofsomevariablesandmarginalizationoverothers...   let   sagainlookintomaximizingwoverthethemarginaldistributionp(y,w|x)=zzp(y,w,  ,  |x)d  d  (13)5emforthecoremodel(version3.0)   wenowhavetwolatentvariables,butagainwecanwritelnp(y,w|x)=zzq(  ,  )lnp(y,w,  ,  |x)q(  ,  )d  d  +zzq(  ,  )lnq(  ,  )p(  ,  |y,w,x)d  d  (14)   sidecomment:noticethatthelefthandsideagaincontainslnp(y,w|x),buttherighthandsideisdifferent.doesthismeanthatemforversions2.0and3.0areequivalent?theanswerisemphaticallyno.thisissimplybecausezp(y,w,  |x)|{z}model#2d  6=zzp(y,w,  ,  |x)|{z}model#3d  d  (15)eventhoughbothofthesecanbewrittenasp(y,w|x),theyareofdifferentmodels.thereisalwaysanassumedmodelunderlyingajointlikelihoodandsimplywritingp(y,w|x)isnotenoughinformationtoknowwhatitis.   therefore,theemalgorithmwecanderivefordoingmapofwinthismodelisoveradifferentobjectivefunctionthanthepreviousmodel:ifweweretoactuallycalculatethemarginaldistri-butionsp(y,w|x)forversion2.0and3.0,wewouldseethattheyaredifferentfunctions.e-stepaccordingtotherulesofem,weneedtosetq(  ,  )=p(  ,  |y,w,x).usingbayesrule,p(  ,  |y,w,x)=p(y|w,  )p(  )p(w|  )p(  )rrp(y|w,  )p(  )p(w|  )p(  )d  d  =p(y,  |w)rp(y,  |w,x)d  |{z}=p(  |y,w,x)  p(w,  )rp(w,  )d  |{z}=p(  |w)(16)theconditionalposteriorof  isfoundexactlythesameway,andtheconditionalposteriorof  isfoundinasimilarway,p(  |y,w,x)=gamma(cid:16)a+n2,b+12nxi=1(yi   xtiw)2(cid:17),p(  |w)=gamma(cid:16)e+d2,f+12wtw(cid:17)m-stepusingthesameexactmethodasforthepreviousversion,wecancomputel(w)andmaximizeoverwto   ndthatwt=(cid:16)eqt[  ]i+eqt[  ]nxi=1xixti(cid:17)   1(cid:16)nxi=1eqt[  ]yixi(cid:17)(17)thedifferenceisthatwehaveexpectationsoverboth  and  sincethesearethevariablesbeingmarginalized.6   wenoticethatmarginalizingtwovariablesworksoutbecausewecanstillcalculatetheirfullconditionalposteriors.inthiscase,theposteriorhastheformq(  ,  )=p(  ,  |y,w,x)=p(  |y,w,x)p(  |w)(18)andsowesaytheposterior   factorizes.   therefore,qfactorizesaswellq(  ,  )=q(  )q(  )(19)whereq(  )=p(  |y,w,x)andq(  )=p(  |w).whenwecalculateexpectationsoverbothqdistributions,wecanfocusonlyontherelevantones.thatis,eq[  ]=zz  q(  )q(  )d    =z  q(  )d  (20)technically,duringthee-stepweareperformingthe   rstintegral.however,because  and  areconditionallyindependentgivenw,wesimplyintegrateout  togivetheexpectationof  restrictedtoitsconditionalposterior.thisisnottherule.itisaby-productofthefactthatp(  ,  |y,w,x)factorizesthewayitdoes.inaway,thisfactorizationofthe(conditional)poste-riormakeslifeeasier   somethingtokeepinmindwhenwediscussvariationalid136next.   aswiththepreviousversion,theoutputofthisemalgorithmisapointestimateofwandconditionalposteriorsontheothervariables.we   veagainmadeacompromisebetweenthedesiredfullposteriorandapointestimateofeverything.   beforediscussinghowvariationalmethodsallowustoapproximatethefullposteriorofallvari-ables,it   sworthquicklypointingoutanotherwaytoapproximatethefullposteriordistributionbasedonwhatwe   vedone.thiscanfurtherhighlighthowthereisnosinglesolutiontoapproxi-mateposteriorid136(e.g.,laplace,mcmc,vi,etc.)   theoutputofthisemalgorithmareconditionalposteriordistributionson  and  andamapestimateofw.ifwewantedtogetsomeapproximationoftheposteriordistributionofw,noticethatwecoulddoalaplaceapproximationonlnp(y,w|x)atthemapestimate.wenowhavethis!therefore,weonlyneedtocalculatethehessian   2wlnp(y,w|x)andevaluateitatwmapinordertogetagaussianapproximationoftheposteriorofw.   inthiscase,wewillbeapproximatingtheposteriordistributionp(  ,  ,w|y,x)   q(  )q(  )q(w)(21)whereq(  )=p(  |y,x,wmap),q(  )=p(  |wmap),(22)q(w)=normal(wmap,(      2wlnp(y,wmap,|x))   1)noticethattodothiswewillneedtoactuallycalculatep(y,w|x),whilewedidn   tneedtoinordertooptimizeitoverwusingem.   thisapproachofapproximatingthefullposteriorwithafactorizeddistributionoveritsvariableswillappearagainasamajorcomponentofvariationalid136.intheaboveapproachtheredoesn   tappeartobeasingle,uni   edobjectivefunctionthatweoptimizeinordertogettheseq.wesimplycombinetwotechniques:twooftheqarefoundwithemandthelastonewithlaplace,andwmapiswhatconnectsthem.variationalid136learnsthisfactorizedqusingasingleobjectivefunctionthatiscloselyrelatedtoem.7fromemtovariationalid136   let   strytobeclever.wewereabletogetconditionalposteriorson  and  byintegratingthemoutandrunningem.canweaddwtothis?thatis,whatdoesitmeantodoemforthemarginaldistributionp(y|x)=zzzp(y,w,  ,  )d  d  dw(23)first,wenoticethattherearen   tanyfreeparametersinthemarginaldistributionontheleftside.still,thisisamarginaldistributionofsomething(thedata),andsowecanwritelnp(y|x)=zzzq(  ,  ,w)lnp(y,w,  ,  |x)q(  ,  ,w)d  d  dw+zzzq(  ,  ,w)lnq(  ,  ,w)p(  ,  ,w|y,x)d  d  dw(24)   however,thisscenariofeelsdifferent.thereisnothingtooptimizeoverthelefthandside,sothereisnom-stepthatcanbeperformed.asforthee-step,weneedtosetq(  ,  ,w)=p(  ,  ,w|y,x)(25)   finallywecanseethebigproblem.tryingtoworkthrougheminthisinstancerequiresustocalculatethefullposteriordistributionofallmodelvariables.thiswastheproblemtobeginwithandso,eventhoughthereisnom-steptoperform,wecan   tgettothatpointanywaybecausewecan   tcompletethee-step.beforemovingon,ithinkthefollowingdigressionisworthwhile.   digression:asidecommentrelatingemtotheoriginalcoremodel.thisispurelyadigression.atthispointwe   velaidenoughgroundworkthatwecandosome-thingpotentiallyusefulveryfast.wesawwiththecoremodel,yiind   normal(xtiw,     1),w   normal(0,     1i)thatwecouldcalculatetheposteriorinclosedformandthatitwasagaussian.forthismodel,wecanalsowritethemarginallikelihoodp(y|x)=rp(y,w|x)dwandsetupanem-likeequality,lnp(y|x)=zq(w)lnp(y,w|x)q(w)dw+zq(w)lnq(w)p(w|y,x)dwagainthereisnothingtooptimizeoveronthelefthandside.however,wecansolveforq(w)inthiscasebecausethemodelissimpleenoughthatweknowp(w|y,x).therefore,p(y|x)=expnzp(w|y,x)lnp(y,w|x)p(w|y,x)dwoactuallywedidn   tneedemtomakethisstatement(bayesruleisenough),butgiventhecontextofourdiscussionatthispoint,ithoughtiwouldmentionit.   backtothemaindiscussion:wehavenothingtooptimizeinthemarginaldistributionp(y|x)andwecan   tupdateq(  ,  ,w)asemtellsuswemustbecausewedon   tknowthefullposterior8distributionofthesemodelvariables(whichwastheproblemtobeginwith).thequestioniswhethertheequation:lnp(y|x)=zq(  ,  ,w)lnp(y,w,  ,  |x)q(  ,  ,w)d  d  dw+zq(  ,  ,w)lnq(  ,  ,w)p(  ,  ,w|y,x)d  d  dwtellsusanythinginteresting.theanswerisyes,anditwillleadtoanewapproximateposteriorid136techniquecalledvariationalid136(vi).i   llrefertothisastheviequationbelow.   beforewediscussthethreetermsinthisequation,rememberhowwesetuptheemproblem:   whenwewerearrivingatthisequationwesimplysaidthatqissomedistributiononthemodelvariablewewantedtointegrateoutinthemarginallikelihood.thatis,thisequationistrueforeveryqwecande   ne,solongasitisde   nedonthecorrectspacecorrespondingtowhatvaluesthevariablescantake.   then,becausewewantedtomaximizethelhs(backwhenithadsomethingwecouldoptimizeover,thatis),emtoldusouroneoptionforsettingqinordertobeabletomodifyourpointestimatesuchthatwecanguaranteethatwe   remonotonicallyincreasingthemarginallikelihood.   however,intheviequationscenariothereisnothingtomaximizeonthelhs.therefore,thisstrictrequirementmadebyemonwhatwecansetqtoequalseemsirrelevant.tomakethetran-sitionfromemtovi,westill   ndthisequalitytobeuseful.however,weshiftourperspective:withemwewerefocusedonthelhsandonmakingsurewewerealwaysimprovingitviatherhs.withviwearemoreinterestedinwhatisgoingonwiththeqdistributions,andwhetherthefreedomwehavetopickqcanbeuseful.   inlightofthisnew   viperspective,   let   sbreakdownthethreetermsintheviequation,keepinginmindthatwearenotgoingtobesostrictaboutthedistributionq(  ,  ,w)thatwechoose:   lnp(y|x):thisisthemarginallikelihoodofthedatagiventhemodel.wedon   tknowwhatthisis,andinfactit   sbecauseofthisthatwe   rediscussinganyofthesethingstobeingwith.that   sbecausethetargetposteriorisp(  ,  ,w|y,x)=p(y,  ,  ,w|x)p(y|x)(26)andsincewecan   tsolvetheintegralinthedenominatorto   ndp(y|x),wehavetouseanid136algorithm.therefore,wecan   twriteoutananalyticexpressionforlnp(y|x).however,wedoknowonecrucialfact:lnp(y|x)isconstant.thatis,giventhemodelde   nitionandthedata,therearenootherdegreesoffreedomforp(y|x),andsowhatevervalueittakes,it   ssomethingthatwillneverchangenomatterwhatwedoontherhsoftheviequation.   next,considerthefarrightterm.thisiskl(qkp)whereqissomedistributionwe   regoingtopick.thisisthekl-divergencebetweenourchosenqandthefullposterior.again,wedon   tactuallyknowwhatthisis,sowecan   tgiveanumberforklinthiscase.however,weagainknowtwocrucialfactsaboutthekl-divergence:itisalwaysnon-negativeandonlyequalszerowhenq=p.therefore,thistermisasimilaritymeasurebetweenourchosenqdistributionandthetrue,fullposteriorthatwewant.9   finallythereisthemiddleterm.fortunately,thisissomethingwecancalculate!   thatis,providedthatwepickadistributionforq(  ,  ,w)thathelpsustowardthisend.thisisbecausewehavede   nedthemodel,sowehavede   nedhowtowritethefunctionp(y,w,  ,  |x).wejustneedtocalculatetheintegrals.   ourgoalistopickaq(  ,  ,w)thatisclosetotheposteriordistributionp(  ,  ,w|y,x).wesawpreviouslyhowthelaplaceapproximationtriestodothisbylettingqbeamultivariategaussian.however,wedidn   treallyhaveanobjectivefunctionweweretryingtooptimizethere.ratherweweresolvingasecondordertaylorapproximationproblem.   anaturalmeasureofclosenessbetweenq(  ,  ,w)andp(  ,  ,w|y,x)isthekl-divergence.sincethismeasureisnotsymmetric(i.e.,kl(qkp)6=kl(pkq)),wehavetwooptions.varia-tionalid136optimizeskl(qkp)usingtheviequation,whichweequivalentlywriteaslnp(y|x)=eq[lnp(y,w,  ,  |x)]   eq[lnq(  ,  ,w)]|{z}   l      variationalobjectivefunction   +kl(qkp(  ,  ,w|y,x))   howcanweoptimizekl(qkp)usingthisequation?afterall,wedon   tknowtwoofthethreeterms,includingthekltermwe   renowsettingouttooptimize.thekeyinsightisthatwedon   tneedtoknowtheseterms:thelhsisconstantandthekldivergenceisnon-negative.therefore,l+klmustadduptothesamenumberforeverypossibleqthatwede   ne.by   ndingaqthatmaximizesl,weareequivalently   ndingaqthatminimizesthekldivergencebetweenitandthetargetposteriordistributionbecausekl   0.   therefore,asmentioned,viispurelyinterestedintheqdistribution.andinthiscasewehaveagoodreasontointerpretthisqdistributionasanapproximationtothefullposteriordistribution.variationalid136forthecoremodel(version3.0)   thisleadsustoavariationalid136algorithmforthelastmodelwehavebeendiscussing.thisentailsthefollowingsteps:1.weneedtode   neaqdistributionfamilyfor  ,  andw.comparethiswitheminwhichweweretoldwhattosetqequalto.2.wethenneedtoconstructthevariationalobjectivefunctionl=eq[lnp(y,w,  ,  |x)]   eq[lnq(  ,  ,w)](27)afterdoingthis,w,  and  willbegonesincethey   reintegratedout.allthatwillremainaretheparametersofq.3.wede   nethedistributionfamily,meaningwedon   tactuallysaywhatit   sparametersare.howdoweknowwhattosetthesetoo?ourgoalistomaximizelovertheseparametersbecausetheviequationtellsusthatdoingsowillminimizethekl-divergencebetweenqandthetargetposterior.therefore,asitsnameimplies,wetreatlasanobjectivefunctiontobemaximizesovertheparametersofq.10   intheemalgorithm,lwasafunctionoveramodelvariable   theoneinthemarginallikelihoodweweredoingmapid136for.theparametersoftheqdistributionwerealwaysknownbecausewesetthemtotheconditionalposteriordistribution.now,it   stheparametersofqthatareunknownandlisafunctionoftheseparameters.therefore,eventhoughit   sthesamelinemandvi,lisafunctionofdifferentthingsinthesetwomethods.thisisanothersubtledifferencebetweenviandemthatreallymakesthemtwodistinctid136techniques.   andsoitremainstode   newhatq(  ,  ,w)actuallyis.weneedtopickarecognizabledistributionsowecancarryouttheintegralandoptimizeoveritsparameters.inallcasesthisentailssomesortofsimpli   cation.byfarthemostcommonisthe   mean-   eld   assumption(aphysicstermwheretheseideasoriginatefrom).inthisapproachwewriteq(  ,  ,w)=q(  )q(  )q(w)(28)andpickindividualdistributionsforeachvariable.whydidwesplitacrossthesethree?basicallybecausethesewerethethreevariable   units   oftheprior(noticethatwisavector,sowedon   tnecessarilysplitacrosseveryvariable,althoughwecertainlycouldde   neq(w)=qjq(wj)).   inthenextlecturewewillseehowto   ndtheoptimaldistributionfamilyofeachq.however,wearefreetode   nethemhoweverwewantandthealgorithmwillstillwork,andsowede   nethemtobeinthesamefamilyastheprior.(thesealsohappentobetheoptimalchoices.)q(  )=gamma(a0,b0),q(  )=gamma(e0,f0),q(w)=normal(  0,  0)(29)wejustdon   tknowwhata0,b0,e0,f0,  0and  0are.to   ndthiswe   rstcalculatel.   beforedoingthat,whydowepickthisqdistributionandwhatisitdoing?   wepickit,likehowwepickmanythings,purelyoutofconvenience.it   seasytode   neadistributiononavariablethatisinthesamefamilyastheprior.bywayofcomparison,ifwewantedtopicka2-dimensionaldistributionq(  ,  )thatwasn   tfactorizable,whatcouldwepick?(whatmultivariatenon-negativedistributionsaretherereadilyathand?themultivariategaussianisn   toneofthemsinceitisonallrd,notrd+.)likewise,howcanwede   neadistribution,e.g.,q(  ,w)?whatdistributionsaretherethatwecaneasilywritedownwhereonedimensionisinr+andtheothersinrd?it   sjusteasiertopickdistributionsindividuallyforeachvariable.   theotherconsiderationisthatweneedtobeabletocalculatel.pickingcomplicateddistributionsonlargesetsofvariablesmayproduceanexpectation(i.e.,anintegral)thatisnotsolvableandthenwe   restuckwithanobjectivewecan   tevenwriteout,letaloneeasilyoptimize.   bymakingthisfactorization,weareassumingthatallvariablesareindependentintheposterior.inthecoremodelversion2.0,wesawthat  and  areconditionallyindependentgivenw,whichletuswriteq(  ,  )=q(  )q(  ).inthatcasethatwastrue,butnowthisisnolongertrue.thatis,foreverypossiblechoiceoftheindividualqdistributions,wewillalwayshaveq(  )q(  )q(w)6=p(  ,  ,w|y,x)(30)therefore,thekl-divergencewillalwaysbe>0.11   sonowthatwe   vede   nedq,weneedtocalculatel.nextweekwewillseehowwecanupdateqinsomesituationswithoutactuallygoingthroughl.thatis,thereissometimesatrickthatcanbedonewherewecangostraighttotheparameterupdateswithoutcalculatingl.   however,thedefaultistocalculatel.unfortunatelythisisnotapleasanttask,butit   sthemostfailsafeapproach.thevariationalobjectiveisl=eq[lnp(y,w,  ,  |x)]   eq[lnq(  ,  ,w)]=   eq[  ]2nxi=1eq[(yi   xtiw)2]+12eq[ln  ]   eq[  ]2eq[wtw]+d2eq[ln  ]+(a   1)eq[ln  ]   beq[  ]+(e   1)eq[ln  ]   feq[  ]+constant   eq[lnq(  )]   eq[lnq(  )]   eq[lnq(w)](31)   noticethatthelastlineisthesumoftheentropiesofeachindividualqdistribution,whichisaresultofthefactorization.attheveryleastweneedtopickqsothatwecancalculateitsid178.   the   rsttwolinescontaintheexpectedlogjointlikelihood.noticethataconvenientresultofthefactorizationisthatqassumesallvariablesareindependent.thereforewehavesimpli   cationssuchaseqh  2nxi=1(yi   xtiw)2i=eq[  ]2nxi=1eq[(yi   xtiw)2](32)andtheexpectationsuseonlythepartofqrelevanttothevariablebeingintegratedover.thismakescalculatinglmucheasier.   wewillstopherefornow.thefunctionlwillbenastylooking.however,aftercalculatingitwecantakederivativesandoptimizeovera0,b0,e0,f0,  0and  0.it   snotobviousat   rstsight,butaftersettingtherespectivederivativestozeroandsolving,the   nalalgorithmwillactuallybeverysimpleandintuitivelysatisfying(inmyopinion).we   lldiscussthislater.   fornow,theimportanttake-homemessageisthatbymaximizingloverthesesixparameters,weare   ndingapointestimateofq(  )q(  )q(w)suchthatitisanapproximationofp(  ,  ,w|y,x).thatis,wegetapointestimateofaid203distributionthatapproximatestheposterior.12eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture6,10/20/2016instructor:johnpaisleyvariationalid136review(simplenotation)   forthisfastreview,wecompressthisintoasimplenotation.wehavedataxgeneratedfromamodelwithparameters  .theseparametersarethemselvesgeneratedfromapriordistribution(andsocalled   variables   fromnowon),givingthehierarchicalrepresentationx   p(x|  ),     p(  )(1)   wewanttheposteriordistributionp(  |x)=p(x|  )p(  )p(x),(2)butit   sintractable.thiscanbebecausethepriorandlikelihoodtermarenotconjugate.justascommon,thevariables  couldactuallybealargesetofvariables,thedataxacomplicatedsetofdata,andthedistributionsp(x|  )andp(  )quitecomplicatedinthedependencystructuretheyinduceonthesevariables.   weapproximatep(  |x)withadistributionq(  )andconstructtheequalitylnp(x)|{z}constant=zq(  )lnp(x,  )q(  )d  |{z}variationalobjectivel+zq(  )lnq(  )p(  |x)d  |{z}kl-divergence   0(3)   we   vediscussedhowwecan   tactuallycomputethe   rstandlasttermsingeneral,butthatbymaximizinglwithrespecttotheparametersofq(  ),weareminimizingthekl-divergencebetweenq(  )andp(  |x),whichislikeadistancemeasurebetweenthetwo.   therefore,wecanviewq(  )asanapproximationofp(  |x).thekeythingisthatwemustbeabletocalculatel=zq(  )lnp(x,  )d     zq(  )lnq(  )d  (4)fortunatelyweoftencandothissincewede   nethejointlikelihoodp(x,  )andthedistributionq(  ).1   infact,theaboveoversimpli   estheproblemsomewhat.inreality,wehaveasetofparameters  =(  1,...,  m)which,alongwiththeirpriordistributions,de   nesajointlikelihoodonthedatap(x,  1,...,  m)(5)noticethat,aswritten,ihaven   tgivenyouenoughinformationtoknowhowp(x,  1,...,  m)shouldfactorizeintoaproductofconditionaldistributions.thefollowingdiscussiondoesn   tchangeatallbasedonthisfactorization,soiwilljustkeepusingp(x,  1,...,  m).   question:howshouldwepickq(  1,...,  m)?answer:lastweekwediscussedfactorizingqusinga   mean-   eld   assumption.   mean-   eld   assumption   the   mean-   eld   assumptiongetsitsnamefromphysics,wherethesetechniqueswere   rstdevelopedinthecontextofaproblemwherethisnamemakessense.itconsistsofthefollowingsteps:1.split  intogroups,usually(butnotalways)accordingtothe   units   inwhichtheyaredrawnfromtheprior.herewe   vealreadyassumedthattobe  1,...,  m.noticethatwecouldhave  1   rdand  2   r,soit   snotcorrecttothinkofeach  iasbeinginthesamespace.2.de   neq(  i|  i)forvariables  i.thatis,qiisthedistributionfamilyde   nedfor  iand  iisitsparameters.oftenyouwilljustsee   q(  i)   forshort,butfornowwe   llwriteitinthismorecomplicatedwaytokeepitmoretransparent.3.letq(  1,...,  m)=qmi=1q(  i|  i)bethedistributionwechoosetoapproximatethepos-teriorp(  1,...,  m|x)   usingthisqdistribution,thevariationalobjectiveisthencomputedasl=z(cid:16)myi=1q(  i|  i)(cid:17)lnp(x,  1,...,  m)d  1      d  m   mxi=1zq(  i|  i)lnq(  i|  i)d  i(6)   assumingthatwecancalculatealloftheseintegrals,theresultisafunctionl(  1,...,  m)thatwetrytomaximizeoverall  i.example(directmethod)   wewillrefertotheprocessofexplicitlyde   ningeachqiandcalculatinglasthe   directmethod.   weshowanexampleofthisfromthemodeldiscussedlastweek.   wehavedatad={(xi,yi)}ni=1withx   rdandy   randthemodelyi   normal(xtiw,     1),w   normal(0,     1i),     gamma(a,b)(7)2   basedonthede   nitionofthismodel,thejointlikelihoodfactorizesasp(y,w,  |x)=p(  )p(w)nyi=1p(yi|xi,w,  )(8)   toapproximatethefullposterior,wepickthedistributionq(w,  )=q(  )q(w)=gamma(  |a0,b0)normal(w|  0,  0)(9)   wethecalculatethevariationalobjectivefunctionl(a0,b0,  0,  0)=z   0zrdq(w,  )lnp(y,w,  |x)q(w,  )dwd  =zq(  )lnp(  )d  +zq(w)lnp(w)dw+nxi=1zzq(  )q(w)lnp(yi|xi,w,  )dwd     zq(  )lnq(  )d     zq(w)lnq(w)dw(10)   whenwewriteoutthedistributionsinvolved,weseethatthiswillrequireustocalculatee[  ],e[ln  ],e[w]ande[wwt].thiscanbelookedupwhennecessary.ifweweretoactuallysolvetheseintegrals,wewouldgetsomethingthatlookslikethis:l(a0,b0,  0,  0)=(a   1)(  (a0)   lnb0)   ba0b0+constant     2(  0t  0+tr(  0))+constant+n2(  (a0)   lnb0)   nxi=112a0b0(cid:16)(yi   xti  0)2+xti  0xi(cid:17)+constant+a0   lnb0+ln  (a0)+(1   a0)  (a0)+12ln|  0|+constant(11)theonlyreasonthisisbeingwrittenouthereistogiveanideaofhowcomplicatedthevariationalobjectivefunctioncanlook(rememberhowsimplethemodelis).it   sjustheretomakethings100%concrete,butyoudon   thavetoactuallyparsethisifyoudon   twantto.ofcourseinpracticeitmightbeunavoidablethatyouhavetocalculateatthislevelofdetail.   inthisfunction,  (  )isaspecialfunctioncalledthedigammafunction,andnoticethatthepriorsparametersareincludedwithoutthe0.   generallyspeaking,whenoptimizinglwewanttoupdateallparametersofasingleqidistribu-tionatatime.therefore,wewanttoupdatethepair(a0,b0)togetherand(  0,  0)together.   we   lldiscussafastermethodlater,butthefail-safemethodistodirectlycalculatel(a0,b0,  0,  0)andthensolve   twoequations,twounknowns   problem   l/   a0=0and   l/   b0=0for(a0,b0)andsimilarlyfor(  0,  0).3pickingandthensolvingq(  i|  i)   wereturntothegeneralproblemwherewehavedataxandmodelvariables  1,...,  m.afterde   ninghowq(  1,...,  m)factorizes,thetwokeyproblemsare1.pickingthedistributionfamilyofq(  i|  i)(e.g.,gaussian,gamma,etc.)2.findingawaytoupdatetheparameters  ito   ndthebestq(  i|  i)initsfamily   previously,wesolved#1arbitrarilyand#2throughbruteforcecalculationofl.wehavetwonaturalquestionsthatariseatthispoint:q1:isthereawaytopickabestfamilyforeachqi?(we   llusetheshorthandqi   q(  i|  i).)q2:isthereafasterwaytosolvefor  ithatbypassescalculatingl?theanswertobothofthesequestionsis   yes   (atleastinprinciple)andwecangetthesolutionstobothquestionsatonce!wewillnowshowhowtodothis.   generalsetup:exactlyasbefore,wehaveajointlikelihoodp(x,  1,...,  m)andweapproximatetheposteriordistributionp(  1,...,  m|x)   qmi=1q(  i|  i).   let   snowfocusontheqdistributionforaspeci   cvariable,sayq(  i|  i).ourgoalistothinkaboutthevariationalobjectivefunctiononlyinthecontextofthisqidistributionandseewhatittellsus,l=zq(  i|  i)"z(cid:16)yj6=iq(  j|  j)(cid:17)lnp(x,  1,...,  m)d  j6=i#|{z}=eqj6=i[lnp(x,  1,...,  m)]d  i(12)   zq(  i|  i)lnq(  i|  i)d  i   xj6=izq(  j|  j)lnq(  j|  j)d  j(13)   asindicated,thetermeqj6=i[lnp(x,  1,...,  m)]istheexpectationoverall  jforj6=i.there-fore,theresultofthisexpectationisafunctionof  iandof  jforallj6=i.   wehaven   tde   nedanyotherqjyeteither,butwe   llseethatthisdoesn   tmatterforchoosingthefamilyofqi.   thenextstepistomanipulatehowwewritethevariationalobjectivefunction.first,l=zq(  i|  i)eqj6=i[lnp(x,  1,...,  m)]d  i   zq(  i|  i)lnq(  i|  i)d  i+const.w.r.t.  i=zq(  i|  i)lneeqj6=i[lnp(x,  1,...,  m)]q(  i|  i)d  i+const.w.r.t.  i(14)   nextweaddandsubtractlnzandde   nez=zeeqj6=i[lnp(x,  1,...,  m)]d  i(15)4thisgivesl=zq(  i|  i)ln1zeeqj6=i[lnp(x,  1,...,  m)]q(  i|  i)d  i+lnz+const.w.r.t.  i(16)   whatis1zeeqj6=i[lnp(x,  1,...,  m)]?noticethatwecanthinkofitasaid203distributionon  i.aswewillnowsee,thisisveryrelevant,sinceourgoalistopickq(  i|  i)suchthat,ifweonlyfocusonthistermandignoreallotherqj,wemaximizezq(  i|  i)ln1zeeqj6=i[lnp(x,  1,...,  m)]q(  i|  i)d  i=   kl(cid:0)qik1zeeqj6=i[lnp(x,  1,...,  m)](cid:1)(17)   rememberthatkl   0andso   kl   0.weknowwhenklisminimized,soweequivalentlyknowwhenthenegativeklismaximized,thatis,wenowknowweshouldsetq(  i|  i)=1zeeqj6=i[lnp(x,  1,...,  m)](18)   noticeafewthingsaboutthis:1.thisgivestheoptimalfamilyforq(  i|  i).itdoesn   tmatterwhattheotherqjareortheir  jparameters.2.thisalsogivestheoptimalparameters  iofq(  i|  i)forgivensettingsoftheotherqj.ifweknowallotherqjexceptforqi,there   snothingunknowninthisoptimalq(  i|  i).putanotherway,wehaven   tjustwrittenoutadistributionfamilyabove,we   vewrittenoutaspeci   cdistributionincludingallitsparameters.3.ifwedon   tknowwhatthefamilyoftheotherqjare(e.g.,gaussian,gamma,etc.)westillcansaythefamilyofq(  i|  i).thatis,theaboveequationwillallowustosaysomethinglike   qishouldbede   nedtobeagammadistribution.   4.sinceourchoiceofiwasarbitrary,wethereforeknowwhateveryqishouldbesettobylookingattheformofexp{eqj6=i[lnp(x,  1,...,  m)]}foreachi.therefore,thisisthe   rststepinvariationalid136:toiteratebetweeneach  ianddecidewhatqidistributionshouldbede   nedforit.5.wecanwritethisabstractly,butiftheintegralisintractable(inthenumeratororthede-nominator),thenthisdoesusnogood.doesthisworkingeneral?theansweris   yes,   foralargeclassofmodels(tobediscussedinalaterlecture).   inwords,thissaysthatto   ndtheoptimalqdistributionfor  i:1.takethelogofthejointlikelihood2.taketheexpectationofallvariablesusingtheirrespectiveqdistributionexceptforq(  i|  i)3.exponentiatetheresultandnormalize5generalvariationalid136algorithm   givendataxandajointlikelihoodp(x,  1,...,  m)   foriterationt=1,2,...1.formodelvariableindexi=1,...,msetq(  i|  i)=eeqj6=i[lnp(x,  1,...,  m)]reeqj6=i[lnp(x,  1,...,  m)]d  i2.evaluatethevariationalobjectivefunctionusingtheupdatedqlt=eq[lnp(x,  1,...,  m)]   mxi=1eqi[lnq(  i|  i)]3.ifthemarginalincreaseinltcomparedwithlt   1is   small,   terminate,otherwisecontinuetothenextiteration.   aswithgibbssampling,wealwaysusethemostrecentparametersforallq(  j|  j)whenupdatingqi.let   slookataconcreteexample.example(optimalmethod)   wereturntotheoriginalexampleyi   normal(xtiw,     1),w   normal(0,     1i),     gamma(a,b)(19)   wewanttoapproximatetheposteriorp(w,  |y,x)withq(  ,w)usingvariationalid136andamean-   eldassumption.therefore,the   rststepistochoosethefactorizationofq.againwechoosep(  ,w|y,x)   q(  ,w)   q(  )q(w)   next,wewanttolearnwhatdistributionfamilyweshouldsetq(  )andq(w)tobe.forexample,shouldq(  )begaussian?shoulditbegamma?poisson?etc.   q(  ):weknowfromthegeneralapproachthatwecan   ndq(  )asfollows:q(  )   exp(cid:8)eq(w)[lnp(y|x,  ,w)+lnp(  )+lnp(w)](cid:9)   exp(cid:8)eq(w)[lnp(y|x,  ,w)]+lnp(  )(cid:9)(20)noticethatwecanremoveanytermsnotinvolving  ,thereforelnp(w)isremoved.also,theexpectationisonlyoverw,thereforetheexpectationdoesn   timpactlnp(  ).then,q(  )   exp(cid:8)pni=1eq(w)[lnp(yi|xi,  ,w)](cid:9)p(  )   hnyi=1  12e     2eq(w)[(yi   xtiw)2]i  a   1e   b  (21)6   sincewehaven   tde   nedq(w)yet,wecan   ttakethisexpectation.however,noticethatwehaveenoughinformationtobeabletosaythatq(  )=gamma(  |a0,b0),a0=a+n2,b0=b+12nxi=1eq(w)[(yi   xtiw)2](22)   q(w):next,weperformsimilaroperationsto   ndtheoptimalq(w),q(w)   exp(cid:8)eq(  )[lnp(y|x,  ,w)+lnp(  )+lnp(w)](cid:9)   exp(cid:8)eq(  )[lnp(y|x,  ,w)]+lnp(w)(cid:9)(23)againwecanremoveanytermsnotinvolvingw,andsolnp(  )isremoved.also,sincetheex-pectationisonlyover  ,wedon   thaveanexpectationforthetermlnp(w).again,wecontinue:q(w)   exp(cid:8)pni=1eq(  )[lnp(yi|xi,  ,w)](cid:9)p(w)   hnyi=1e12e[ln  ]e   (eq(  )[  ]/2)(yi   xtiw)2ie     2wtw(24)firstnoticethatwecansimplyignoree12e[ln  ]becauseitwillbecanceledoutwhenwecomputethenormalizingconstant.   wealreadysawthistypeofproportionalitybeforewhenwecalculatedtheposteriorofwinthebayesianlinearregressionproblem.asaresult,weknowthatq(w)=normal(w|  0,  0)(25)  0=(cid:16)  i+eq(  )[  ]nxi=1xixti(cid:17)   1,  0=  0(cid:16)eq(  )[  ]nxi=1yixi(cid:17)   noticethatbycyclingthrougheachparameterandlearningitsqdistribution,wealsolearnwhattheexpectationsarewhenlearningotherqdistributions.thatis,whenwefoundthatq(  )wasagammadistribution,weweren   tabletosaywhattheexpectationwithrespecttoq(w)wasbecausewedidn   tknowwhatdistributiontouseforq(w).nowweknowit   sgaussianandsowecanretroactivelysolvefortheexpectation.   similarly,becausewe   rstfoundthatq(  )wasagammadistribution,weknowwhattheex-pectationisthatweshouldusewhenweupdateq(w).thisisthegeneralpattern.we   rst   ndwhatthedistributionsareforeachqi,andafterwehavethemall,wewillknowwhatalltheexpectationswillbe.   forexample,forthisproblemeq(  )[  ]=a0/b0(26)eq(w)[(yi   xtiw)2]=(yi   xti  0)2+xti  0xi(27)   let   slookatthe   nalvariationalid136algorithmforthisproblem.7vialgorithmforbayesianlinearregressionwithunknownnoiseprecisioninputs:dataandde   nitionsq(  )=gamma(  |a0,b0)andq(w)=normal(w|  0,  0)output:valuesfora0,b0,  0and  01.initializea00,b00,  00and  00insomeway2.foriterationt=1,...,t   updateq(  )bysettinga0t=a+n2b0t=b+12nxi=1(yi   xti  0t   1)2+xti  0t   1xi   updateq(w)bysetting  0t=(cid:16)  i+a0tb0tnxi=1xixti(cid:17)   1  0t=  0t(cid:16)a0tb0tnxi=1yixi(cid:17)   evaluatel(a0t,b0t,  0t,  0t)toassessconvergence(i.e.,decidet).   noticethatthisisexactlythesolutionwewouldhavefoundifwesolvedthesystemofequations   l(a0,b0,  0t   1,  0t   1)   a0=0,   l(a0,b0,  0t   1,  0t   1)   b0=0to   nda0tandb0t.wealsowould   ndthisupdatefor  0tand  0tbysolvingthesystem     0l(a0t,b0t,  0,  0)=0,     0l(a0t,b0t,  0,  0)=0   youcanalsoseethedifferencebetweenviforthismodelandemformaximizinglnp(y,w|x)using  asthemarginalizedvariable,whichwediscussedearlier.inem,theupdateforthepointestimateiswt=  0tusing  0tfromabove.whenwewantedtoupdateq(  ),weagainhadagammadistribution,butbecausetherewasnodistributiononw,wesimplypluggedinwtwhere  0tappearsandremove  0t(again,becausethereisnouncertaintyaboutw).   comment:sometimesresearcherswillde   neaq   distribution   thatisapointmass   thatis,theywillletq(w)=  w0forexample.forthisdistribution,p(w=w0)=1.whenwe   integrate   usingthisq(w)tocalculatel,we   ndthatwesimplyreplacewwithw0anddoapointestimateofw0(andignorethetechnicalissuethattheid178of  w0is      ).thisissometimesdonewhentheintegralusinganyotherq(w)isintractable,whichisnotthecasehere.8vialgorithmforprobitregression   recallthesetup(slightlymodi   ed):wehavedatad={(xi,yi)}ni=1wherex   rdandy   {   1,+1}(previously{0,1},butnoticethatit   snodifferencebelow).includingahiddenvariable  iforeach(xi,yi)pair,theprobitregressionmodelisyi=sign(  i),  i   normal(xtiw,  2),w   normal(0,     1i)(28)   previously,wederivedanemalgorithmformaximizingthemarginaldistributionp(y,w|x)underthismodel.let   snowlookatavariationalid136algorithm.   theunknownsthistimearewandthevector  =(  1,...,  n).wesetupthevariationalid136equationlnp(y|x)=zq(w,  )lnp(y,w,  |x)q(w,  )dwd  +zq(w,  )lnq(w,  )p(w,  |y,x)dwd  (29)   fromthissetup,wecanseethatwe   redesigningq(w,  )toapproximatethefullposteriorofbothwandtheextravariables  together.wepickthefactorizationq(w,  )=q(w)nyi=1q(  i)(30)the   rstproblemusingour   optimalapproach   isto   ndoutwhatthesedistributionsshouldbe.first,noticethatthejointlikelihoodfactorizesasp(y,w,  |x)=p(w)nyi=1p(yi|  i)p(  i|w,xi)(31)the   distribution   p(yi|  i)hasnorandomnessinit:p(yi|  i)=1{yi=sign(  i)}.   q(  i):followingtherules,wetakethelogofp(y,w,  |x)andtheexpectationwithrespecttoq(w)andq(  j)forj6=i,andthenexponentiate.asaresult,wecanwritethatq(  i)   expneq(w)[lnp(w)]+xj6=ieq(  j)[lnp(yj|  j)+lnp(  j|w,xj)]o  expnlnp(yi|  i)+eq(w)[lnp(  i|w,xi)o(32)noticethatthe   rstlinedoesn   tcontainanythinghavingtodowith  i,thereforeit   saconstantasfarastheproportionalityisconcernedandcanbeignored.asaresult,q(  i)   1{yi=sign(  i)}exp{   12  2eq(w)[(  i   xtiw)2]}   1{yi=sign(  i)}exp{   12  2[(  i   xtieq(w)[w])2+xtieq(w)[wwt]xi]}   1{yi=sign(  i)}exp{   12  2(  i   xtieq(w)[w])2}exp{   12  2xtieq(w)[wwt]xi}   1{yi=sign(  i)}exp{   12  2(  i   xtieq(w)[w])2}(33)9thisisatruncatednormalonthehalfofrde   nedbyyi.therefore,theoptimalq(  i)isq(  i)=tnyi(  0  i,  2),  0  i=xtieq(w)[w](34)   q(w):weusethesameapproachto   ndtheoptimalqdistributionofw,q(w)   expnlnp(w)+nxi=1eq(  i)[lnp(yj|  j)+lnp(  j|w,xj)]o   expnlnp(w)+nxi=1eq(  i)[lnp(  j|w,xj)]o   e     2wtwnyi=1e   12  2(eq(  i)[  i]   xtiw)2(35)thisisexactlythebayesianlinearregressionproblemwehavediscussed.thedifferenceisthatbecausewedon   tobserve  i,weendupusingtheexpectationofthislatentvariaid7singitsq(  i)distribution.asaresult,q(w)=normal(  0,  0)(36)  0=(cid:16)  i+1  2nxi=1xixti(cid:17)   1,  0=  0(cid:16)1  2nxi=1eq(  i)[  i]xi(cid:17)   nowthatwe   vede   nedtheqdistributions,wecanwriteouttherelevantexpectations,eq[  i]=                                 xti  0  i+      0(   xti  0  i/  )1     (   xti  0  i/  )ifyi=+1xti  0  i+         0(   xti  0  i/  )  (   xti  0  i/  )ifyi=   1eq(w)[w]=  0   asbefore,  isthecdfofastandardnormaland  0isitspdf.   thisgivesthefollowingtwostepsthatcanbeiterated,   atiterationt1.update  0  iforeachiusingthe  0fromiterationt   1asinequation(34)2.update  0and  0usingall  0  ijustupdatedinstep1asinequation(36)   assessconvergencebyevaluatingthevariationalobjectivel(  0,  0,  0  1,...,  0  n)usingthevaluesofthesevariationalparametersfromiterationt.   asa(painful)exercise,youcantryderivingthisvariationalid136algorithmusingthe   directmethod   of   rstcalculatinglandthentakingderivatives,settingtozeroandsolving.thiswillmakeitveryeasytoappreciatehowmucheasierthisoptimalapproachis.also,ifweweren   tableto   ndtheoptimalqdistributions   rst,andinsteadde   nedsomeotherdistributionforq(  i),thedirectmethodmostlikelywouldleadtoadeadend.10eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture7,10/27/2016instructor:johnpaisley   let   slookatanotherexampleofastandardmodelthatiseasilylearnedwithvariationalid136.latentdirichletallocation(lda)   ldaisabayesianapproachtotopicmodelingandoneofthefundamentalmodelsinmachinelearning.itispopularbecauseithasawiderangeofapplicationsandiseasytodevelopandbuildintolargersystems.setup:wehavediscretegroupeddata,xd={xd1,...,xd,nd},wheredindexesgroupnumber.eachxdi   {1,...,v},meaningeachobservationtakesoneofvvaluesandxdisaparticularcollectionofvaluesdisjointfromallotherxd0.example:theclassicexamplewhereldaisusedisindocumentmodeling.we   llassumewe   reworkinginthatcontextfromnowon.   inthiscase,dwouldindexaparticulardocument(e.g.,ablogpost,anarticleinanewspa-per,etc.)andvwouldbethesizeofthevocabularyofwords.   thevaluexdiistheindexoftheithwordinthedthdocument.forexample,xdi=7241mightmeanthattheithwordindocumentdis   government,   meaningthatthenumber7241mapstotheword   government   inavocabularythatwehaveconstructed.   whenwesaythe   ithwordinthedthdocument,   thisdoesn   thavetorefertotheliteralorder.ldadoesn   tmodelwordorder,soanyre-ordering(andthusre-indexing)ofthewordswillbeviewedinexactlythesamewaybylda.   also,asanaside,thereissigni   cantpre-processingpriortogettingeachxd.thevo-cabularysizevandthewordscomprisingthatvocabularyisselectedinadvance,withoverlycommonwordslike   the   removedandwordsthatareveryrarealsoremoved.thispre-processingwillhaveanimpactonperformance,butwewillassumethatthistaskhasalreadybeendone.1topicmodeling:theideabehindtopicmodelingistomodeltheddocumentsx1,...,xdasbeinggeneratedfromasetofk   topics,     1,...,  k.everydocumentsharesthesetopics,buthasitsowndocument-speci   cmodelvariablethatdictateshowtheyareused.     k:av-dimensionalid203distributiononthevwordsfortopick     d:ak-dimensionalid203distributiononthetopicsfordocumentdmodel:giventhetopics,  1,...,  kandthedistribution  donthemfordocumentd,generatealldataindocumentdasfollows:cdi   discrete(  d),xdi   discrete(  cdi).(1)noticethatwehaveintroducedanadditionallatentvariabletothemodel:   cdipicksoutthetopicthattheithwordindocumentdbelongsto.noticethatifxdi=xdi0(i.e.,thesamewordappearsmultipletimesinadocument)it   snotnecessarilythecasethatcdi=cdi0.thesamewordcanhavesigni   cantid203inmultipletopics.   xdiisgeneratedusingthetopicindexedbycdi,andhencethesubscripton  .priors:wedon   tknow  dor  k(orcdiforthatmatter,butweknowit   sdistributiongiven  d).therefore,weneedtoputpriordistributionsonthem.therearemanydistributionswecouldpick.ldausesthefollowing:  diid   dirichlet(  ),  kiid   dirichlet(  )(2)since  dand  kareall   niteid203vectorsusedinadiscrete(orfromanotherperspective,multinomial)distribution,ldausesaconditionallyconjugatedirichletpriorforeachofthem.thismakesvariationalid136veryeasy.apriordiscussionontheposterior:beforewegetintothevariationalid136algorithmforlda,whatdowehopeto   nd?whatwill  kand  dtellusthat   suseful?     k:ifwelookattheapproximateposteriordistributionof  kandseewhichdimensionsofitareexpectedtobelarge,thenthehighid203wordsshouldallrelatetoacoherenttheme.forexample,thethreemostprobablewordsmightbe   government,      politics,   and   congress.   wewouldthencallthisa   politics   topic.inpapersontopicmodeling,youwilloftenseelistsofwords.thisisexactlywhatisbeingdone:eachlistisshowingthe5or10mostprobablewords(dimensions)accordingtoaspeci   ctopic(  k).     d:thiswilltellusthefractionofeachtopicappearinginaparticulardocument.becausewecanassignmeaningtoeach  kafterthefact,wecanthensay,e.g.,   thisdocumentis75%politicsand25%technology,   etc.,because  d,1=0.75and  1isthe   politics   topic.   ingeneral,both  kand  dwillbehighlysparse,meaningonlyafractionofvalueswillbesigni   cantlynonzero.2posteriorcalculation:weusebayesruletotrytocalculatetheposterior,p(  ,  ,c|x)   p(x|  ,  ,c)p(  ,  ,c)   p(x|  ,  ,c)p(c|  ,  )p(  ,  )   p(x|  ,c)p(c|  )p(  )p(  )(3)the   rsttwolinesaresimplytruestatementsaboutbayesruleandhowprobabilitiesfactorize.thelastlinetakesintoconsiderationthedependencystructureofldatoremoveunnecessaryconditioning.bytheassumedindependencestructure,thisfurtherbreaksdownasfollows:p(  ,  ,c|x)   hdyd=1ndyi=1p(xdi|  ,cdi)p(cdi|  d)ihkyk=1p(  k)ihdyd=1p(  d)i(4)   notsurprisingly,thiscan   tbenormalizedandsoweneedanid136algorithmtoapproximatetheposterior.noticethattherearequiteafewvariableswewanttolearnwiththismodel.forexample,foreachwordxdiinthedataset,thereisanassociatedtopicindicatorcdithatweneedtolearn.thereforethenumberofvariablesismuchlargerthanwhatwe   vediscussedbefore.variationalid136forlda   wewilluseourpreviousdiscussionofthe   optimalmethod   forvariationalid136toapprox-imatetheposterioroftheldamodel.   step1:usingthemean-   eldassumption,weneedtopickafactorizationofq(  ,  ,c)   p(  ,  ,c|x).wesplitthesevariablesaccordingtohowtheyaregeneratedintheprior.thismakeslearningqmucheasier.q(  ,  ,c)=hkyk=1q(  k)ihdyd=1q(  d)ihdyd=1ndyi=1q(cdi)i(5)step2:nextweneedtoselectthedistributionfamilyforeachq.forthismodelwewillbeableto   ndtheoptimaldistributions.rememberfromourpreviousdiscussionthat,foragivenvariable,wecan   ndtheoptimalqdistributionasfollows:1.takethelogofthecompletejointlikelihood2.taketheexpectationofthisusingallotherqdistributionsexcepttheoneofinterest3.exponentiatetheresultandnormalizeoverthevariableofinterest   thepotentialconcernwiththiswasthatwedon   tknowanyoftheqdistributionstobeginwith,sowhenwetaketheexpectationwithrespectto   allother   q,wedon   tknowwhattheseexpectationsare!however,recallthatwedon   tneedtoknowtheactualvaluesoftheexpectationsinorderto   ndthefamilyoftheqdistributionbeingconsidered.therefore,bycompletingoneloopofthisprocedurewereachthepointwherewecangobackandcalculatetheexpectationsthatweleftunde   nedpreviously.   however,beforewecandothis,weneedtoknowhowtowritethejointlikelihoodforlda.jointlikelihoodoflda3   theposteriorp(  ,  ,c|x)   p(x,  ,  ,c),whichiswhatwecalculatedbefore,p(x,  ,  ,c)=hdyd=1ndyi=1p(xdi|  ,cdi)p(cdi|  d)ihkyk=1p(  k)ihdyd=1p(  d)i(6)   theseprobabilitiesarealleasytowrite,butwerunintoaproblemwiththetermp(xdi|  ,cdi)=vyv=1  cdi(v)1(xdi=v)(7)noticethatthisfunctionpicksoutthecorrectdimensionof  accordingtothevaluethatxditakes.however,it   shardtodovariationalid136forcdiaswritten.   thisisanotherexamplewherenotationcanhelpmakethingseasiertoderive.noticethatp(xdi|  ,cdi)=vyv=1  cdi(v)1(xdi=v)=kyk=1hvyv=1  k(v)1(xdi=v)i1(cdi=k)(8)inbothcases,cdipicksoutthecorrecttopicvector  k,whilexdipicksoutthecorrectdimensionoftheselectedvector.   tokeepthenotationclean,wewritep(x,  ,  ,c)=hdyd=1ndyi=1kyk=1(cid:0)p(xdi|  k)  dk(cid:1)1(cdi=k)ihkyk=1p(  k)ihdyd=1p(  d)i(9)noticethatwedirectlyuse  dk=p(cdi=k|  d),butleavetherestinp(  )notation.   therefore,thelogofthejointlikelihoodcanbewrittenaslnp(x,  ,  ,c)=dxd=1ndxi=1kxk=11(cdi=k)lnp(xdi|  k)+1(cdi=k)ln  dk+kxk=1lnp(  k)+dxd=1lnp(  d)(10)   thelogjointlikelihoodisakeyequationandshouldneverbeoutofsightwhenderivingvaria-tionalid136algorithms.   asageneralrule,whenwewantto   ndaqdistribution,wecanthrowawayanythingnotinvolvingthevariablebeingupdated.thiswillmakeworkingwiththelogjointlikelihoodlessintimidatingandrequiremuchlesswriting.4   toemphasizethispoint,let   slookatamade-uptoyexample.toyexample   imagineamodelwithjointlikelihoodp(x,c,a,b)wherexisthedataandc,a,baremodelvariables.thenusingthefactorizationq(c,a,b)=q(c)q(a)q(b)wehavethatq(c)   ee   q[lnp(x,c,a,b)]weusetheshorthandnotation      q   toindicateallqdistributionsexceptfortheonebeingcon-sideredatthemoment.   whatiflnp(x,c,a,b)=ax+bc+xc2+ab?thisisacompletelymadeuplogjointlikelihoodanddoesn   tactuallycorrespondtoanything.however,thisisjusttohighlightthepoint.inthiscase,q(c)   ee[a]x+e[b]c+xc2+e[a]e[b]   ee[a]x+e[a]e[b]ee[b]c+xc2   ee[b]c+xc2   thislastlineisbecauseq(c)=ee[a]x+e[a]e[b]ee[b]c+xc2ree[a]x+e[a]e[b]ee[b]c+xc2dc=ee[b]c+xc2ree[b]c+xc2dc   asasidecomment,wecanwritee[ab]=e[a]e[b]becausetheexpectationusesq(a,b)=q(a)q(b),sothey   reindependent.   thetake-homemessagehereisthat,whenupdatingaqdistributionforaparticularmodelvari-able,weonlyneedtolookatthetermsinthelogjointlikelihoodthatinvolvethisvariable.   thelogjointlikelihoodwillbethesumofmanythings,andanythingnotinvolvingthisvariablecanbeabsorbedinthenormalizingconstantandsoignored.forthismodel,thatmeanswecansimplyignoreax+abinthelogjointlikelihoodwhenwe   ndq(c).   thisisjustatoyexampletodrivehomeapoint.butforcomplicatedmodels,thiswayofsimplifyingthingscanmakederivingthevialgorithmseemlikeamuchlessdauntingtask.   forquickreference,inderivingthevialgorithmforlda,wedoingthiswiththejointlikelihoodlnp(x,  ,  ,c)=dxd=1ndxi=1kxk=11(cdi=k)lnp(xdi|  k)+1(cdi=k)ln  dk+kxk=1lnp(  k)+dxd=1lnp(  d)(11)5q(cdi):indicatorofwhichtopicwordxdicamefrom   to   ndthisqdistribution,wecanfocusonlyontermsinthelogjointlikelihoodinvolvingcdi.therefore,q(cdi)   epkk=11(cdi=k)(cid:0)e   q[lnp(xdi|  k)]+e   q[ln  dk](cid:1)   kyk=1hee   q[lnp(xdi|  k)]+e   q[ln  dk]i1(cdi=k)(12)   wewanttonormalizethisovercdi.sincecdi   {1,...,k},theintegralinthedenominatorturnsintoasum,q(cdi)=qkk=1hee   q[lnp(xdi|  k)]+e   q[ln  dk]i1(cdi=k)pkcdi=1qkk=1hee   q[lnp(xdi|  k)]+e   q[ln  dk]i1(cdi=k)(13)   thisisanotherwayofwritingq(cdi)=kyk=1"ee   q[lnp(xdi|  k)]+e   q[ln  dk]pkj=1ee   q[lnp(xdi|  j)]+e   q[ln  d,j]#1(cdi=k)(14)   noticethatthisissimplyadiscretedistribution,q(cdi)=discrete(  di),  di(k)=ee   q[lnp(xdi|  k)]+e   q[ln  dk]pkj=1ee   q[lnp(xdi|  j)]+e   q[ln  d,j](15)   comparethiswithgibbssampling.rememberthatwesamplefromtheconditionalposteriordistribution,p(cdi=k|xdi,  ,  d)=p(xdi|  k)  dkpkj=1p(xdi|  j)  d,jwhere  and  darethemostrecentsamplesofthesevariables.   forvariationalid136,weswapp(xdi|  k)   ee   q[lnp(xdi|  k)]and  dk   ee   q[ln  dk]ratherthansampling,wethensimplykeepthis   approximateconditionalposterior   astheqdistributionforcdi.   noticethatwhilea=elna,usingexpectations,e[a]6=ee[lna].   wedon   tknowwhate[ln  dk]ande[lnp(xdi|  k)]=e[ln  k,xdi]areyet,butthatdoesn   tchangewhattheoptimalformofq(cdi)is.noticethat,ifwecanusethesamelogicto   ndq(  k)andq(  d),thenafteronecyclethroughthemodelvariableswewillbeabletocomebacktoq(cdi)andexplicitlycalculatetheseexpectations.   alsonoticethat,sincedandiarearbitrary,wehavesolvedq(cdi)foralldandi.6q(  d):thedistributionontopicsfordocumentd   to   ndq(  d),wefocusonlyontermsinthelogjointlikelihoodinvolvingthisvariableq(  d)   epi,ke   q[1(cdi=k)]ln  dk+lnp(  d)   kyk=1  pndi=1e   q[1(cdi=k)]+     1dk(16)   theterm     1comesfromthedirichletpriorp(  d).wewanttonormalizethisover  dsubjectto  dk   0andpk  dk=1.thiswasaproblemonthe   rsthomework,wherewesawthatthesolutionisq(  d)=dirichlet(  d1,...,  dk),  dk=  +ndxi=1e   q[1(cdi=k)]|{z}  di(k)(17)   theexpectationofanindicatorofaneventissimplytheid203ofthatevent.therefore,e   q[1(cdi=k)]=kxj=1q(cdi=j)1(cdi=k)=q(cdi=k)(18)sincewepreviouslycalculatedthisqdistribution,weareabletosolvethisexpectationanduse  di(k)asde   nedaboveforq(cdi).   noticethattheparametersofthedirichletusetheexpectedhistogramoftheallocationsofallwordsfromdocumentd.therefore,if75%ofwordsareexpectedtocomefromtopic1,thenq(  d)willre   ectthisbyhavingeq[  d,1]   0.75.however,qwillalsocaptureanapproximationoftheuncertaintyofthisvalueunderitsposteriordistribution.   againcomparewithgibbssampling,wherewehavetheconditionalposteriorp(  d|cd)   hndyi=1kyk=1  1(cdi=k)dki|{z}p(cd|  d)hkyk=1       1dki|{z}p(  d)(19)   inthiscase,p(  d|cd)=dirichlet(  d1,...,  dk),  dk=  +ndxi=11(cdi=k)wherecdiisthemostrecentsampleofthisvariable.inthiscase,weusetheempiricalhis-togram(ratherthantheexpectedhistogram)constructedfromthemostrecentsamples.gibbssamplingthensamplesanewvector  dfromthisdistribution,whilevariationalid136keepstheapproximateconditionalposteriorastheapproximationtothefullposteriorofthisvariable.   again,becausedisarbitrary,we   vesolvedforall  d.7q(  k):thetopics   finally,welearntheqdistributionsforthetopicsthemselves.followingthesameprocedureasforq(  d)andq(cdi),wehaveq(  k)   epd,ie   q[1(cdi=kk)]lnp(xdi|  k)+lnp(  k)   p(  k)dyd=1ndyi=1p(xdi|  k)e   q[1(cdi=k)](20)   sincep(xdi|  k)=qvv=1  1(xdi=v)kv,q(  k)   vyv=1  pd,ie   q[1(cdi=k)]1(xdi=v)+     1kv(21)   normalizingovertheid203vector  k,q(  k)=dirichlet(  k,1,...,  k,v),  k,v=dxd=1ndxi=1e   q[1(cdi=k)]1(xdi=v)+  (22)   onceagain,wesete   q[1(cdi=k)]=  di(k)asde   nedintheupdateofq(cdi).   wecaninterpretpdd=1pndi=1  di(k)1(xdi=v)asfollows:1.  di(k):theid203thatwordxdicamefromtopickaccordingtoq(cdi)2.1(xdi=v):anindicatorofwhattheithwordindocumentdcorrespondsto   soifv      government   thenthissumistheexpectedtotalnumberoftimesweseetheword   government   comefromtopickgiventhemodel   sqdistributionsatthecurrentiteration.   becausewesolvedforq(cdi)   rst,andtheupdatesofq(  d)andq(  k)onlyusedq(cdi),wewereabletoinputthecorrectvariationalparameterstoupdatetheselasttwodistributions.finally,weneedtoaddresshowtosolvetheexpectationsinq(cdi)inordertoupdatethisdistribution,andthereforeexplicitlysaywhateach  diequalsintheupdatesofq(  d)andq(  k).   thatis,wehavethequestion,howdoweactuallycalculatee   q[ln  dk]ande   q[ln  kv]?   thequickansweris,sinceweknowtheseexpectationsarebothwithrespecttodirichletdistribu-tionson  and  ,wecangotowikipediaandlookupthesolution.however,wecanalsoderivethisusingaclevertechnique.   todoso,wewill   ndthatunderstandingpropertiesofexponentialfamilydistributionsgivesageneralwayto   ndmanyinterestingexpectations.8exponentialfamilydistributions   wetakeaquickdetourtodiscussexponentialfamilydistributions.almostalldistributionswehave(andwill)discussareinthe   exponentialfamily,   forexample,gaussian,beta,dirichlet,gamma,poisson,multinomial,etc.distributions.   thismeanstheycanbewrittenintheformp(x|  )=h(x)e  tt(x)   a(  )(23)wherethesetermsarecalledasfollows:1.  isthenaturalparametervector2.t(x)isthesuf   cientstatisticvector3.h(x)isthebasemeasure(afunctionofx)4.a(  )isthelog-normalizer(afunctionof  )   q:isthereageneralwayto   nde[t(x)]?   a:yes,thederivationfollows:sincezp(x|  )dx=1        zp(x|  )dx=0(24)wehavethat     zp(x|  )dx=0(25)   z(t(x)        a(  ))p(x|  )dx=0(26)   zt(x)p(x|  )dx=z     a(  )p(x|  )dx(27)   e[t(x)]=     a(  )(28)   wewill   ndthat,inmanycases,theexpectationswewanttotakeinvariationalid136areofasuf   cientstatistic.weshowthisforthedirichletexample.   dirichletexample:thedensityofthedirichletdistributioncanbeputintotheexponentialfamilyformasfollowsp(x|  1,...,  k)=  (pi  i)qi  (  i)kyi=1x  i   1i(29)=(cid:16)kyi=1x   1i(cid:17)epki=1  ilnxi   (cid:0)piln  (  i)   ln  (pi  i)(cid:1)(30)9   matchingtermswiththegenericexponentialfamilydistributionform,wehave   h(x)=qix   1i   t(x)=[lnx1,...,lnxk]t     =[  1,...,  k]t   a(  )=piln  (  i)   ln  (pi  i)therefore,e[t(x)]=     a(  )impliesthat,foreachi,e[lnxi]=   a/     i   e[lnxi]=   ln  (  i)     i      ln  (pj  j)     i(31)thesederivativesappearoftenenoughthattheyhavebeengivenasymbol,  (  ),calledadigammafunction.therefore,e[lnxi]=  (  i)     (pj  j).thiscanbeevaluatedinlanguagessuchasmatlabusingabuilt-infunction.   the   nalvariationalid136algorithmforldaisgivenbelow.variationalid136forlatentdirichletallocation(lda)1.de   neq(cdi)=discrete(  di),q(  d)=dirichlet(  d)andq(  k)=dirichlet(  k).initializeeach  (0)di,  (0)dand  (0)kinsomeway.2.foriterationt=1,...,t(a)foreachdandi,set  (t)di(k)=e  (  (t   1)k,xdi)     (pv  (t   1)k,v)+  (  (t   1)d,k)     (pj  (t   1)d,j)pkm=1e  (  (t   1)m,xdi)     (pv  (t   1)m,v)+  (  (t   1)d,m)     (pj  (t   1)d,j)(b)foreachdandk=1,...,k,set  (t)dk=  +ndxi=1  (t)di(k)(c)foreachkandv=1,...,v,set  (t)kv=  +dxd=1ndxi=1  (t)di(k)1(xdi=v)3.usingallvariationalparameterupdatesafteriterationt,evaluatelt=eq[lnp(x,c,  ,  )]   xd,ieq[lnq(cdi)]   xdeq[lnq(  d)]   xkeq[lnq(  k)]toassessconvergence.thisfunctionmustbemonotonicallyincreasingasafunctionoft.10eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture8,11/3/2016instructor:johnpaisley   wenextlookatscalablevariationalid136inthecontextofconjugateexponentialfamilymodels.we   rstreviewthesetypesofmodels.conjugateexponentialfamily(cef)models   generalsetup:wehaveamodelofdataxwithvariables  1,...,  m.incombinationwiththeirpriors,thisde   nesajointlikelihoodp(x,  1,...,  m).   amajorclassofmodelsarecalled   conjugateexponentialfamily   (cef)models.thefeaturesofthistypeofmodelare:1.alldistributionsonvariablesanddatade   nedinthemodelareintheexponentialfamily.2.ifwepickonevariable,  i,it   sconditionalposteriorp(  i|x,     i)isinthesamefamilyasthepriordistributionon  i.thatis,allpriorsareconjugatetothelikelihoodoftheirvariable.   example:let   sreturnthetheearliertoymodel,yi   n(xtiw,     1),w   n(0,     1i),     gam(a,b),     gam(c,d)checks:1.thegaussianandgammadistributionsareexponentialfamilydistributions2.p(w|   )isgaussian;p(  |   )isgamma;p(  |   )isgammasoitisacefmodel.   thereasonwecareaboutcefmodelsisthat(1)theyarecommonlyusedinmachinelearning,(2)wecanmakegeneralstatementsaboutvariationalid136forthesetypesofmodels.   frombishop(slightlymodi   ed),wehaveaconjugatelikelihood-priorpairiftheycanbewrittenasfollows:p(x|  )=h(x)g(  )e  tt(x),p(  |  ,  )=f(  ,  )g(  )  e  t  (1)1becausewegavenamestothetermsinp(x|  )lasttime,andweareclaimingthatbothp(x|  )andp(  |  ,  )areexponentialfamilydistributionsweshouldmapthenamestop(  |  ,  ).inthesetwodistributions,thebasemeasuresareh(x)andg(  )  respectively.thenaturalparametersare  and  respectively.thelognormalizersarelng(  )andlnf(  ,  )respectively.andthesuf   cientstatisticsaret(x)and  respectively.however,thisisn   ttheperspectivewetakewiththeprior.withoutbelaboringthepointtoomuchfurther.noticethatthelognormalizerofp(x|  )isthesameasthelogbasemeasureofp(  |  ,  ).we   llmoveontoanexamplenow.   togiveasimple,moregeneralexample,letx   p(x|  1,  2),  1   p(  1|  3),  2   p(  2),  3   p(  3)(2)let   sfocuson  1.usingaslightlynewnotationforthisproblem,wehavelikelihood:p(x,  2|  1)=h(x,  2)e  (  1)tt(x,  2)   na(  (  1))conjugateprior:p(  1|  3)=f(  (  3),  )e  (  1)t  (  3)     a(  (  1))posterior:p(  1|x,  2,  3)   p(x|  1,  2)p(  1|  3)   e  (  1)t(t(x,  2)+  (  3))   (n+  )a(  (  1))(3)thenotation  (  1)simplyaccountsforthefactthat  1asde   nedbyusmightnotcorrespondtothenaturalparameterform.   however,noticethatifwewanttomakethisadistributionon  1,it   sthesameformastheprior.thatis,wecansaythattheposteriordistributionp(  1|x,  2,  3)=f(  0,  0)e  (  1)t  0     0a(  (  1))(4)  0=t(x,  2)+  (  3),  0=n+     howdoesthisrelatetovariationalid136?recallthattheoptimalq(  1)isfoundby:1.takingthelogofthejointlikelihood2.takingtheexpectationwrtallqdistributionsexceptforq(  1)3.exponentiatingtheresult4.normalizingitasafunctionof  1   inacefmodelthisoptimalqdistributionalwaysisinthesamefamilyasthepriorandhasexpectationsoverthetermsintheexponent.inthecontextoftheaboveabstractmodel,wecanalreadyseethatq(  1)   exp{  (  1)t(eq[t(x,  2)]+eq[  (  3)])   (n+  )a(  (  1))}(5)2thatis,q(  1)=f(  0,  0)e  (  1)t  0     0a(  (  1))(6)  0=e[t(x,  2)]+e[  (  3)],  0=n+     let   sbealittlelessabstract(butnottotallyconcrete)andlookatthisinthecontextofamodelframework.mixedmembershipmodels(andlatentfactormodels)   mixedmembershipmodelscanbebrokendownintothefollowingcomponents.(latentfactormodelscanaswell.)1.groupeddatax1,...,xdthedataisnaturallygrouped,andmodeledasagroup.here,thegroupisnotprecise,butcanbethoughtofasbeingmorecomplexthanasimplepointinrd.forexample,eachgroupcouldbeaperson,andthedatacouldbevariousvitalstatisticsaboutthatperson.thegroupcouldalsobeanaudiosignalfromwhichisextractedalargesetoftime-varyingfeatures.intopicmodeling,thegroupisthedocument,andthedataisthesetofwordsmakingupthatdocument.2.globalvariables  thesearethemodelvariablesthatdirectlyimpacteverygroupofdata.forexample,inldatheyarethesetoftopicsfromwhicheverywordinthedatasetisdrawn.3.localvariablesz1,...,zdthesemirrorthegroupeddata:modelvariablesinzionlyinteractwithdataingroupxi.giventheglobalvariables,ziinnowayimpactsxi0fori06=i.thevariablesinzicanbecomplicatedandhavetheirownhierarchicaldependencies.forexample,inldazi={  i,ci1,...,cini},where  iisthedistributionontopicsandcijistheindicatorofthetopicforthejthwordinxi.thec   saregeneratedfromadistributionparameterizedby  .   mixedmembershipmodelsandlatentfactormodelsaredistinguishedinthewayzand  combinetogeneratexprobabilistically,butthesedetailsaren   tnecessaryforthefollowingdiscussion.ldaisthemostcommonmixedmembershipmodelfordiscretedata.   animportantpropertyofthistypeofmodelisthatthejointlikelihoodfactorizesinaniceway,p(x,z,  )=p(  )dyi=1p(xi,zi|  )(cid:16)=p(  )dyi=1p(xi|zi,  )p(zi)(cid:17)(7)forexample,forldathisisp(x,z,  )=(cid:16)ykp(  k)(cid:17)dyi=1p(  i)qnij=1p(xij|  cij)p(cij|  i)3   forthegenericmodel,itfollowsthatthelogjointlikelihoodislnp(x,z,  )=dxi=1lnp(xi,zi|  )+lnp(  )(8)   sincewecan   t   ndtheexactposteriordistributionofallthemodelvariables,weapproximatewithq(z,  )usingvariationalid136.wepickthefactorizationq(z,  )=q(  )dyi=1q(zi)(9)thesefactorizationscanthensub-factorize.forexample,withlda,q(zi)=q(  i)qjq(cij)(usingthede   nitionofzifromearlier).however,atthelevelatwhichwearetalkingaboutthismodelframework,wemustleavethefactorizationaswrittenabove.   thevariationalobjectivefunctionisthencomputed,l=dxi=1eq(cid:20)lnp(xi,zi|  )q(zi)(cid:21)+eq(cid:20)lnp(  )q(  )(cid:21)(10)   onetypicalwayforoptimizingthisobjectivefunctioniscalled   batch   variationalid136.thisalgorithmisasfollows:batchvariationalid1361.fori=1,...,d,optimizeq(zi)2.optimizeq(  )3.repeat   thegeneralcodingstructurethereforeinvolvesaninnerlooptooptimizetheqdistributionforeachgroupofdata.thentheqdistributionfortheglobalvariablesisoptimized,usuallyveryquicklyusingthestatisticscollectedduringtheinnerloop.thisisdoneinanouterloop,whichcountshowmanyiterationswerun.   forlda,sinceq(zi)=q(  i)qjq(cij),theinnerloopitselfmayuseanalgorithmwhereweiteratebackandforthseveraltimesbetweenupdatingq(  i)andupdatingeachq(cij).   thiscanleadtoscalabilityissues.forexample,whatifdismassive?whatifwehavemillionsofdocumentswewanttodotopicmodelingon?eveniflearningeachq(zi)isfast(e.g.,afractionofasecond),thatfractionmultipliedseveralmilliontimescanleadtoanalgorithmthattakesover24hourstorunasingleiteration!   lookingatthestructureofthemodel,wecanobviouslyspeedthisupwithparallelization.thatis,instep1ofthealgorithm,wecanbreakthegroupsintosubsetsandsendthatinformationoutto4differentprocessors.forexample,ifwehave100computers,eachcomputercanberesponsibleforoptimizing1%oftheq(zi).thiscanmakethealgorithmrunabout100timesfaster.   anotherwaytospeedupvariationalid136inthiscontextistousetechniquesfromoptimiza-tion.noticethatlisanobjectivefunctionlikeanyother,justwithabayesianinterpretationoftheresult.sincelformixedmembershipmodelssuchasldacanbewrittenasasumovergroups,wecanusestochasticoptimizationtooptimizeit.   wewilltalkaboutstochasticoptimizationnextataveryhighlevel.it   sthetypeoftechniquethatneedstobestudiesmorefullyinanoptimizationclass,sincethereisalotthatisknownaboutit.wewilljuststatetheprocedurehereandanalyzeitinthecontextofvariationalid136forcefmodels.   onethingtonoticeinthefollowingisthatitisnotaneither/orchoicebetweenstochasticin-ferenceandparallelization.thestochasticalgorithmdescribedbelowcanbeparallelizedveryeasilytoprovideamassivespeed-upofid136.stochasticallyoptimizingl   sincethelocalvariablesfactorize,wegetasumovergroupsinthevariationalobjective.again,thismeanswecangenericallywritel=dxi=1eq(cid:20)lnp(xi,zi|  )q(zi)(cid:21)+eq(cid:20)lnp(  )q(  )(cid:21)(11)   thistypeofobjectivefunctionisperfectfor   stochasticoptimization.   operationallyspeaking,tostochasticallyoptimizel,wedothefollowingateachiteration:stochasticvariationalid1361.randomlyselectasubsetoflocaldata,st   {1,...,d}2.constructthescaledvariationalobjectivefunctionlt=d|st|xi   steq(cid:20)lnp(xi,zi|  )q(zi)(cid:21)+eq(cid:20)lnp(  )q(  )(cid:21)(12)3.optimizeeachq(zi)inltonly4.updatetheparametersofq(  |  )usingagradientstep(assume  isthenaturalparameter,butitdoesn   thavetobe)q(  |  )     t=  t   1+  tmt     lt(13)5.repeat5   comments:1.wecanshowthate[lt]=l.rewriteltaslt=d|st|dxi=11(d   st)eq(cid:20)lnp(xi,zi|  )q(zi)(cid:21)+eq(cid:20)lnp(  )q(  )(cid:21)letst   p(st).thisisthedistributionofasubsetofsize|st|sampledfrom{1,...,d}uniformlywithoutreplacement.thenep[lt]=d|st|dxi=1eq[1(d   st)]|{z}p(d   st)eq(cid:20)lnp(xi,zi|  )q(zi)(cid:21)+eq(cid:20)lnp(  )q(  )(cid:21)weneedtocalculatep(d   st),whichistheid203thatthedthitemispickedtobeinsetst.wecanusecombinatoricstocalculatethis.sincestisuniformlycreated,eachsetisequallyprobable.sincethereare(cid:0)d|st|(cid:1)differentsets,theyeachhaveid203(cid:0)d|st|(cid:1)   1.also,weneedtocounthowmanyofthepossiblesetscontaind.knowingthatdisintheset,thereare(cid:0)d   1|st|   1(cid:1)possiblesetsofsize|st|   1thatwecouldadddtocreateasetofsize|st|thatcontainsd.thereforetheresultfollowsfromthefactthatp(d   st)=(cid:18)d   1|st|   1(cid:19)(cid:18)d|st|(cid:19)   1=|st|d.sincethestochasticupdateis  t=  t   1+  tmt     lt,wehaveshownthatep[  t]=  t   1+  tmtep[     lt]=  t   1+  tmt     linotherwords,thestochasticgradientisunbiased.incombinationwiththesecondcom-ment,thisisimportantforprovingthatwhenstochasticoptimizationconverges,it   ndsalocaloptimalsolutionofl,containingallofthedata.2.wemusthavep   t=1  t=   andp   t=1  2t<   forthismethodtoprovablyconvergetoalocaloptimalsolutionofl.often,peoplewillchoose  t=1(t0+t)  with     (12,1],sincethiscanbeshowntosatisfytheserequirements.example:stochasticid136forlda   beforewelookmoreindetailinto  t=  t   1+  tmt     ltingeneralforcefmodels,let   slookatthe   nalalgorithmwewillgetforlda.   thelocalvariablesare:1.q(  d):distributionontopicsfordocumentd2.q(cdj):wordallocationforwordjindocumentd   theglobalvariablesare:61.q(  k):topics(distributionsonwords)fork=1,...,k.wepickq(  k)=dirichlet(  k1,...,  kv)sviforlda1.pickarandomsubsetofdocuments2.learnq(cdj)anq(  d)foreachdocumentbyiteratingbetweenupdateingtheseqseveraltimes3.update  kasfollows:(a)de   ne  (k)d=pndj=1eq[1(cdi=k)]exdi(eiisvectorofallzerosexcept1indimi)(b)set  (t)k=(1     t)  (t   1)k+  t(cid:16)  +d|st|pd   st  (k)d(cid:17)   noticethatsviforldatakestheoldparametersandaveragesthemwiththenewparameterscalculatedonlyoverthesubsetchosenforthecurrentiteration.astheiterationsincrease,thenewinformationisweightedlessandlesssince  tisdecreasingtozero.   contrastthiswiththe   batch   id136algorithm1.instep1,weinsteadpickedallthedocuments2.instep3b,weinsteadset  (t)k=  +pdd=1  (k)d.equivalently,wejustset  t=1becausewefullyweightallthenewinformation.   let   sreturntothegenericcefhierarchytoseemoreofthedifferencebetweenbatchandstochas-ticvariationalid136.abstractstochasticvariationalid136   we   reinthegeneralmixedmembershipsettingdescribedabove.also,let  beanaturalparam-eterfornotationconvenience.   likelihood:p(x,z|  )=dyi=1p(xi,zi|  )="dyi=1h(xi,zi)#e  tpdi=1t(xi,zi)   da(  )(14)   prior:p(  )=f(  ,  )e  t       a(  )(15)   theoptimalapproximateposteriorisinsamefamilyastheprior:q(  )=f(  0,  0)e  t  0     0a(  )(16)7   next,wespeci   callyworkwiththevariationalobjectivefunction.before,wedidn   tbotherwiththisbecausewehadthe   trick   whichallowedustogostraighttotheanswer.inotherwords,wecould   nd(  0,  0)suchthat   l=0withoutexplicitlycalculatingl.   focusingontheentiredataset,inthegradientapproachwewanttoset(cid:20)  0  0(cid:21)   (cid:20)  0  0(cid:21)+  tmt   (  0,  0)l(17)therefore,weneedtoexplicitlycalculatel.   ifweonlyfocusonthetermsinlinvolving  ,wecansaythatthefullvariationalobjectivefunctionisl  =dxi=1eq[lnp(xi,zi|  )]+eq[lnp(  )]   eq[lnq(  )](18)   andplugginginthedistributionsabove,wehavel  =eq[  ]t dxi=1e[t(xi,zi)]+       0!   eq[a(  )](d+       0)+lnf(  0,  0)+const.(19)theconstantiswithrespectto  0and  0.   nextweneedtocalculateeq[  ]andeq[a(  )]usingtheqdistributionof  .wehavealreadyseenhowwecandothiswithexponentialfamilydistributions.thatis,ifwesolvetheequalitiesz     0q(  )d  =0,z        0q(  )d  =0(20)wewill   ndthateq[  ]=        0lnf(  0,  0),eq[a(  )]=   lnf(  0,  0)     0(21)   therefore,l  =   [     0lnf(  0,  0)]t dxi=1e[t(xi,zi)]+       0!      lnf(  0,  0)     0(d+       0)   lnf(  0,  0)+const.(22)   recallthatwewanttoset   (  0,  0)l  ="     0l          0l  #=08   usingtheequationforl  above,wecancalculatethat     0l  =      2  0lnf(  0,  0) dxi=1eq[t(xi,zi)]+       0!+     0lnf(  0,  0)      2lnf(  0,  0)     0     (d+       0)        0lnf(  0,  0)(23)        0l  =      2lnf(  0,  0)     0     0t dxi=1eq[t(xi,zi)]+       0!      2lnf(  0,  0)     02(d+       0)+   lnf(  0,  0)     0      lnf(  0,  0)     0(24)   noticethatinbothequations,therearetwotermsthatcancelout.aswritten,thisisatwo-equationstwo-unknownssituation.however,itmakestheproblemmuchclearertowriteitasfollows   (  0,  0)l  ="     0l          0l  #=   "   2  0lnf(  0,  0)   2lnf(  0,  0)     0        2lnf(  0,  0)     0     0t   2lnf(  0,  0)     02#"pdi=1eq[t(xi,zi)]+       0d+       0#   again,allwe   vedoneiswritetheprevioustwoequationsinmatrixvectorform.   thegoalistosettheresultingvectortozero.however,noticethat,sincethepreconditioningmatrixisnegativede   nite,theonlywaywecanmakethismatrix-vectorproductequalzeroisbysettingtherightvectortozero.thismeanswewantto   ndvaluesof  0and  0suchthattherightvectoriszero.wecansimplyreadthisfromthevector:  0=  +dxi=1e[t(xi,zi)],  0=  +d(25)   thisisexactlytheupdatefortheseparametersthatwederivedbeforebyfollowingthe   log   expectation   exponential   id172   rule.itisgoodtoseeviaanotherroutethatthisisthecorrectupdatesfortheseparameters.however,thereasonwearecalculatingthesethingsnowistoseehowstochasticvariationalid136modi   esthis.   recallthatforaparametertoavariationalqdistribution,  ,wewanttoset  t=  t   1+  tmt     lt,whereltiscalculatedusingthesub-sampledsetofgroups.usingtheaboveno-tation,thatmeansthat:(aside:inthematrixbelowthegradientsoffwithrespectto  0and  0areevaluatedatxi0t   1and  0t   1)(cid:20)  0t  0t(cid:21)=(cid:20)  0t   1  0t   1(cid:21)     tmt"   2  0lnf(  0,  0)   2lnf(  0,  0)     0        2lnf(  0,  0)     0     0t   2lnf(  0,  0)     02#"d|st|pi   ste[t(xi,zi)]+       0t   1d+       0t   1#(26)   wecouldsimplyleaveithereandpickmt=i.however,ifwepickmtinacleverway,wecangetavery   clean   update.weneedtopickmttobesomepositivede   nitematrix.for9example,steepestascentusesmt=i.newton   smethodsetsmt=   (   2lt)   1.forstochasticvariationalid136,wesetmt=   "   2  0lnf(  0,  0)   2lnf(  0,  0)     0        2lnf(  0,  0)     0     0t   2lnf(  0,  0)     02#   1(27)thisisalsoevaluatedatthevaluesatiterationt   1.thispreconditioningmatrixcanbeshowntobeequivalenttomt=eq[   2lnq(  )]evaluatedatparametersofqatiterationt   1.thisgradientisknownasthenaturalgradient.thisisa   good   gradientdirectionforreasonsthathavebeenanalyzed(andarebeyondthescopeofthisclass).therefore,wepickthismtnotonlyforconvenience,butalsomotivatedbythenaturalgradientmethod.   whenweusethisvalueofmt,weseethattheupdatefor  0tisalwaystosetitequaltod+  .for  0titis  0t=(1     t)  0t   1+  t(cid:16)  +d|st|xi   ste[t(xi,zi)](cid:17)(28)   weseethattheupdateisaweightedaverageofthenewsuf   cientstatisticswiththeoldvalue.comparethiswiththeupdateforstochasticldaabovetoseethatthisresultgeneralizesthepatternweobservedthere.   alsonoticethatthefunctionofthescalingparameteristotreateachgroupofdataasthoughitappearsd/|st|timesinthedataset.thisisbecauseweonlylookat|st|/dfractionofdata   ifwelookat1%ofthedata,wetreateachobservation(group)asifitappeared100timestokeepthesizeofthedatasetconstant.10eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture9,11/10/2016instructor:johnpaisleyid91withthegaussianmixturemodel(gmm)   wenextlookatafundamentalprobleminmachinelearning   id91data.therearemanytypesofdatawemightwanttocluster.wewillfocusonthegaussianmixturemodelfordataid91,whichrestrictsthedatatobeinrd.   data:thatis,we   regivenasetofvectors{x1,...,xn}whereeachxi   rd.   id91:themaingoalofid91istopartitionthedataintogroups(clusters)suchthatdatawithinthesameclusterare   similar   anddataindifferentclustersare   different.   intuitively,wecanthinkofthegaussianmixturemodelasmeasuringclosenessbydistance,soeachclusterde   nesaregioninaspaceinthesamewaywemightthinkofaregioninaphysicallocation.   id116isonewayofid91data,andarguablethemostfundamental,whichiswhyibringituphere.it   snotaprobabilisticmethodandsowewon   tdiscussit,butit   simportanttoknow.inthislecture,wewillfocusontheprobabilisticapproachtoid91usingagaussianmixturemodel.   gaussianmixturemodelsassumeaid203densityofthedatathatisaweightedsumofkgaussiandistributions.thatis,wemodelthedistributionofeachdatapointxi   rdasfollows:xiiid   p(x|  ,  ,  )=kxj=1  jnormal(x|  j,     1j)(1)   thenumberofgaussians,k,issetinadvance.therearewaystotrytoassesswhatitshouldbe,butfornowwejustassumewehavepickedasetting.themodelparametersare,     :aid203distributionongaussians.   (  j,  j):themeanandprecisionofthejthgaussian   itmightnotbeclearhowtogeneratexifromthisdistributionunlikeasinglegaussian.intuitively,youcanthinkthat,inadatasetofsizen,roughly  jfractionofthatdatawillcomefromthejthgaussianwithmean  jandprecision(inversecovariance)  j.1   fornow,let   snotthinkaboutpriorson  ,  ,  .thelikelihoodofthedatagiven  ,  ,  isp(x1,...,xn|  ,  ,  )=nyi=1 kxj=1  j|  j|12(2  )d2e   12(xi     j)t  j(xi     j)!(2)   ifweweretotrytomaximizethislikelihoodover  ,  ,  (i.e.,perform   maximumlikelihood   ),wewouldnotmakeitveryfarbeforewerealizethatwecan   tdothissimplyandinclosedform.thatis,noderivativeofthislikelihoodcanbesettozeroandsolvedforaparameterofinterest.wethereforewouldneedtoresorttogradientmethodsifwewanttodirectlymaximizelnp(x1,...,xn|  ,  ,  ).   however,recallthatthisisthesettingwhereemcanhelp,andwewillindeedseethatthat   sthecaseforthegmm.   beforediscussingemforthegmm,it   simportantheretopointoutthattheemalgorithmdoesnotequal   maximumlikelihood   or   maximumaposteriori.   emisoftenpresented(butnotinthisclass!)asamaximumlikelihoodtechniqueandit   seasytocon   atethetwo.we   vebeendiscussingemformapid136.inthecontextofthegmm:1.maximumlikelihood   argmaxlnp(x|  ,  ,  )2.maximumaposteriori   argmaxlnp(x,  ,  ,  )=argmaxlnp(x|  ,  ,  )+lnp(  ,  ,  )   emissimplytheprocedureofadding   hiddendata   orextralatentvariablestothissuchthatthemarginaliscorrect.thatis,addingcsuchthatrp(x,c|  ,  ,  )dc=p(x|  ,  ,  )orrp(x,c,  ,  ,  )dc=p(x,  ,  ,  )asthecasemaybe,andthenusingthesectogetclosedformupdatesfor  ,  ,and  .therefore,peoplewillsometimessay   maximumlikelihoodem   or   map-em   tobemorespeci   c.we   lljustsay   em,   butnoticethatuptonowwe   vebeendoingmap-em,whereastodaywewillfocusonml-em.ml-emforthegmm   sincethegmmisalwaysderivedinthemaximumlikelihoodframeworkforem,we   lldothesamehereandnotde   nepriordistributionson  ,  or  .we   llthendiscussvariationalid136wherewewillneedtointroducepriors.   theloglikelihoodlnp(x|  ,  ,  )isaproblembecausethesum-log-sumformdoesnotleadtoclosedformupdatesofparameters.toavoidgradientmethods,weuseem,whichmeanswemustintroduceanadditionalvariabletothemodel.westartfromtheendandde   nealargermodel,thenshowthatithasthecorrectmarginal.   model(expanded):foreachxi   rd,given  ,  and  ,generateci   discrete(  ),xi|ci   normal(  ci,     1ci)(3)   noticethatthisissimilartolda,whereweusedanindicatortosaywhichtopicawordcamefrom.thefunctionofthisindicatorisidenticalhere.thevalueofci   {1,...,k}sayswhich2ofthekgaussiansgeneratedxi,asisevidentbecausecipicksouttheparametersofthecorre-spondinggaussian.marginaldistribution   wenowneedtocheckthatthisexpandedmodelhasthecorrectmarginaldistribution.wehavetodothisbecausewesetouttomaximizenyi=1p(xi|  ,  ,  )=nyi=1nxj=1  jnormal(xi|  j,     1j)(4)   thenewextendedmodelhasthelikelihoodnyi=1p(xi,ci|  ,  ,  )=nyi=1p(xi|ci,  ,  )p(ci|  )=nyi=1kyj=1(cid:0)  jnormal(xi|  j,     1j)(cid:1)1(ci=j)(5)   noticethatindicatorsareusedtopickoutthecorrecttermlikeinlda.thenextquestionis,whatisp(x|  ,  ,  )=kxc1=1      kxcn=1nyi=1p(xi,ci|  ,  ,  )=nyi=1kxj=1p(xi,ci=j|  ,  ,  )(6)   itmightseemtrivial,butweliterallyjustpluginandsumnyi=1kxj=1p(xi|ci=j,  ,  )|{z}=normal(  j,     1j)p(ci=j|  )|{z}=  j(7)   thisisexactlythemarginalwewant,sowefoundourextravariable.derivinganemalgorithmforthegmm   westartwherewehavetostart,withtheemequationlnp(x|  ,  ,  )=xcq(c)lnp(x,c|  ,  ,  )q(c)+xcq(c)lnq(c)p(c|x,  ,  ,  )(8)   wemakeafewcommentsaboutthisequality:1.theextravariablesc=(c1,...,cn)arediscrete.thereforethereisnointegralbecausecisn   tinacontinuousspace.sumsareusedindiscretesettingsandintegralsincontinuoussettings,butthepathtotheequalityisidentical.inourpreviousderivationofthisequation,wecanliterallyreplaceeveryrwitha  andremovethein   nitesimals(e.g.,dc)32.thesumisoverallpossiblevaluesofthevectorc,whereeachentryci   {1,...,k}.therefore,aswritten,thereareknvectorswehavetosumover.ofcourse,thisisimpos-sibleforalmostanyproblem(knismassive!)soifwecan   tsimplifytheproblem,thenemisuseless.3.despitethecombinatorialissue,thisisthetrue,correctequation.wehavetostartfromhereandtrytosimplify.unlikevariationalmethodswherewemakeapproximationswithrespecttoq(c)   i.e.,wede   neq(c)=qiq(ci)   inemwemustsetq(c)=p(c|x,  ,  ,  )andsothetrueconditionalposterioronallcidictateswhatwecandoaboutq(c).4.nextwewillshowthatthetrueconditionalposteriorfactorizes,andsothereforeq(c)mustfactorizeaswell(i.e.,there   snoapproximationinfactorizingq(c)asabove).theposteriortellsusthatthisisthecase,andsonoapproximationsarebeingmadew.r.t.q(c).5.also,asasidecomment,noticethatevenwhendoingmaximumlikelihoodwherenopriorsareassumedonthemodel,posteriorcalculationusingbayesrulestillfactorsintotheproblem.therefore,eventhoughwe   renotdoingbayesianmodelinghere,bayesianideasarestillcriticalformakingprogressonthisproblembecausewewanttouseemandemneedsbayesrule.   werecallthestepsoftheemalgorithmandthenworkthrougheachstep.e-step1.setq(c)=p(c|x,  ,  ,  )2.calculatepcq(c)lnp(x,c|  ,  ,  )m-step3.maximize#2over  ,  and     e-step(step1):usebayesruletocalculatetheconditionalposteriordistributionp(c|x,  ,  ,  )   p(x|c,  ,  )p(c|  )   nyi=1p(xi|ci,  ,  )p(ci|  )(9)   thisconditionalindependenceremovesthekncombinatorialissue.wecandividethisbyanumberz=qni=1zitonormalizetheentirefunctionofcbynormalizingeachindividualprior-likelihoodpair,p(c|x,  ,  ,  )=1znyi=1p(xi|ci,  ,  )p(ci|  )=nyi=1p(xi|ci,  ,  )p(ci|  )zi=nyi=1p(ci|xi,  ,  ,  )(10)4wherep(ci=k|xi,  ,  ,  )=p(xi|ci=k,  ,  )p(ci=k|  )pkj=1p(xi|ci=j,  ,  )p(ci=j|  )=  knormal(xi|  k,     1k)pkj=1  jnormal(xi|  j,     1j)(11)   sotosummarizeq(c)=p(c|x,  ,  ,  )=nyi=1p(ci|xi,  ,  ,  )|{z}   q(ci)=nyi=1q(ci)(12)   weusenotationq(ci=j)=  i(j).whenimplementing,we   rstset  i(j)     jnormal(xi|  j,     1j)forj=1,...,kandthennormalizethek-dimensionalvector  ibydividingitbyitssum.   e-step(step2):theexpectationweneedtotakeisl(  ,  ,  )=nxi=1eq(c)[lnp(xi,ci|  ,  ,  )]+const.(13)theconstantiswithrespectto  ,  ,  .   becauseeach(xi,ci)areconditionallyindependentofeachother.theexpectationoverq(c)reducestoanexpectationoverq(ci).it   sworththinkingaboutthis.eachofthenexpectationsaboveisovertheqdistributionontheentirevectorc=(c1,...,cn).however,becausetheithtermonlycontainsciinit,thesumoverci0i06=iinq(c)causeseachq(ci0)todisappearbecauseitsumstoone.we   rethenleftwith:l(  ,  ,  )=nxi=1eq(ci)[lnp(xi,ci|  ,  ,  )]+const.(14)   noticethatthistakescareofthecombinatorialproblem.wenowonlyneedtosumoverk  ntermsinsteadofoverknterms.   continuing,l(  ,  ,  )=nxi=1kxj=1q(ci=j)[lnp(xi|ci=j,  ,  )+lnp(ci=j|  )]+const.(15)=nxi=1kxj=1  i(j)(cid:18)12ln|  j|   12(xi     j)t  j(xi     j)+ln  j(cid:19)+const.   (theconstantinthesecondlinecontainsextratermsinadditiontowhat   sinthe   rstline.)5   m-step(step3):finally,wemaximizel(  ,  ,  )overitsparameters.wehopewecandothisbytakingderivativesandsettingtozero.ifnot,thenemprobablyhasn   thelpedus.a)     jl=0     jl=   nxi=1  i(j)(  j  j     jxj)=0(16)     j=1njnxi=1  i(j)xi,nj=nxi=1  i(j)(17)noticethatthisisaweightedaverageofallthepointsinthedataset.theweightisdeter-minedbytheconditionalposteriorid203ofeachpointcomingfromthejthclusterusingtheparametersfromthepreviousiteration.b)     jl=0     jl=nxi=1  i(j)(12     1j   12(xi     j)(xi     j)t)=0(18)        1j=1njnxi=1  i(j)(xi     j)(xi     j)t,nj=nxi=1  i(j)(19)weusetheequalities     ln|  |=     1andxt  x=trace(  xxt)        xt  x=xxt.justlikethemean  jweseetheinverseof  j(i.e.,thecovariance)isequaltoaweightedversionoftheempiricalcovariance.forthisstep,weusethemostrecentupdatefor  j.c)     l=0subjectto  j   0andpkj=1  j=1.thissteprequireslagrangemultiplierstoensuretheseconstraints.wewon   treviewthetechniqueoflagrangemultiplierssinceitwouldbetoomuchofadigression,butit   simportanttoknowingeneral.thesolutionis  j=njn,nj=nxi=1  i(j)(20)theupdateofthepriorid203thatapointcomesfromclusterjisthefractionofpointsweexpecttoseefromclusterjunderthecurrentposteriordistribution.   noticewedon   tneedtoiterateover  ,  ,  tomaximizel.   weareleftwiththefollowingemalgorithm.6amaximumlikelihoodemalgorithmforthegaussianmixturemodelinput:datax1,...,xn,x   rd.numberofclustersk.output:gmmparameters  ,  ,  andclusterassignmentdistributions  i1.initialize  (0)andeach(  (0)j,  (0)j)insomeway.2.atiterationt,(a)e-step:fori=1,...,nandj=1,...,kset  (t)i(j)=  (t   1)jnormal(xi|  (t   1)j,(  (t   1)j)   1)pkk=1  (t   1)knormal(xi|  (t   1)k,(  (t   1)k)   1)3.m-step:setn(t)j=nxi=1  (t)i(j)  (t)j=1n(t)jnxi=1  (t)i(j)xi  (t)j= 1n(t)jnxi=1  (t)i(j)(xi     (t)j)(xi     (t)j)t!   1  (t)j=n(t)jn4.calculateft=lnp(x|  (t),  (t),  (t))toassessconvergenceoffasafunctionoft.7variationalid136(vi)   wenextlookatviforthegmm.wethereforeneedtode   nepriorsfor  ,  and  .   priors:wehavemanyoptionsforpriors,butwechoose(outofconvenience)     dirichlet(  ),  j   normal(0,ci),  j   wishart(a,b)(21)   thewishartdistribution   sincethere   sagoodchancethewishartdistributionisnew,we   llreviewitbrie   yhere.thewishartdistributionisaid203distributiononpositivede   nitematrices.itistheconjugatepriorfortheprecision,  j,ofamultivariategaussian,whichiswhywepickit.forad  dmatrix  ,thedensityfunctionisp(  |a,b)=|  |a   d   12e   12trace(b  )2ad2|b|   a2  d(a/2)(22)theparameterrequirementsarea>d   1andbisapositivede   nited  dmatrix.thefunction  d(a/2)isacomplicatedfunctionofthegammafunction,  d(a/2)=  d(d   1)4dyj=1  (cid:18)a2+1   j2(cid:19)   theimportantexpectationsforviareofsuf   cientstatisticsofthewishartdistribution.sincethewishartdistributionisanexponentialfamilydistribution,wecanusethetechniquediscussedinapreviouslectureto   ndthate[  ]=ab   1,e[ln|  |]=dln2   ln|b|+dxj=1  (a/2+(1   j)/2)(23)  (  )isthedigammafunctiondiscussedpreviously.it   ssomethingyouwouldcallabuilt-infunctiontoevaluateinyourcode.   it   sworthtryingthepreviouslydiscussedexponentialfamilytrickoutfore[ln|  |]toseehoweasilywecanderivethisnastyexpectation.   jointandlogjointlikelihood:thejointlikelihoodandlogjointlikelihoodcanbewrittenasp(x,c,  ,  ,  )=hnyi=1kyj=1p(xi|ci=j,  ,  )p(ci=j|  )ihkyj=1p(  j)p(  j)ip(  )(24)therefore,usingtheindicatorfunctionrepresentationforci,lnp(x,c,  ,  ,  )=nxi=1kxj=11(ci=j)[lnp(xi|  j,  j)+ln  j]+kxj=1[lnp(  j)+lnp(  j)]+lnp(  )(25)8   posteriorcalculation:sincewecan   tcalculatep(  ,  ,  ,c|x)weuseavariationalapproximationwithmean-   eldfactorizationp(  ,  ,  ,c|x)   q(  ,  ,  ,c)=q(  )"kyj=1q(  j)q(  j)#"nyi=1q(ci)#(26)   nextweneedtode   nethedistributionfamilyofeachq.weknowhowtodothis,butnoticethatwecanmakeacefargument.alldistributionsareexponentialfamilydistributions(gaussian,wishart,dirichlet,multinomial)andwecanverifythattheconditionalposteriorofeachmodelvariableisinthesamefamilyastheprior,hencethereisconditionalconjugacy.themodelisaconjugateexponentialfamilymodelandsoeachqdistributionshouldbesettothesamefamilyastheprior.   westillneedtocalculatehowtoupdatetheirparameters,sothecefargumenthasn   treallyreducedtheamountofworkwehavetodo.however,wecansayattheoutsetthat   q(  )=dirichlet(  01,...,  0k)   q(  j)=normal(m0j,  0j)   q(  j)=wishart(a0j,b0j)   q(ci)=multinomial(  i)   thevariationalid136algorithmwillthenconsistofiteratingthrougheachoftheseqdistri-butionsandmodifyingtheirparametervaluestomaketheirproductclosertothetrueposteriordistributionaccordingtotheirkldivergence.   sincethisisacefmodel,wecanusethe   log-expectation-exponentiate-normalize   approachwe   vebeendiscussingoverthepastseverallectures.   q(ci=j):q(ci=j)   ee[lnp(xi,ci=j|  ,  j,  j)]   ee[lnp(xi|  j,  j)]+e[lnp(ci=j|  )]   e12e[ln|  j|]   12e[(xi     j)t  j(xi     j)]+e[ln  j](27)theexpectationisover  ,  jand  j.theyaretakenusingtheqdistributionsonthem,soe[ln  j]=  (  0j)     (pk  0k)multiplyingoutweseethate[(xi     j)t  j(xi     j)]=(xi   e[  j])te[  j](xi   e[  j])+trace(e[  j]  0j)wediscussede[ln|  j|]above.  0jisfromq(  j)ande[  j]=m0j.9   q(  ):q(  )   epni=1e[lnp(cj|  )]+lnp(  )   epni=1pkj=1  i(j)ln  j+pki=1(     1)ln  j   kyj=1    +pni=1  i(j)   1j(28)soweset  0j=  +nj,nj=nxi=1  i(j)   q(  j):q(  j)   epni=1e[lnp(xi|ci=j,  j,  j)]+lnp(  j)   e   12pni=1  i(j)(xi     j)te[  j](xi     j)   12c  tj  j   e   12pni=1(xi     j)t(  i(j)e[  j])(xi     j)   12c  tj  j(29)normalizingthisover  ,theresultisagaussiandistributionq(  j)=normal(m0j,  0j)where  0j=(cid:0)c   1i+nje[  j](cid:1)   1,m0j=  0j e[  j]nxi=1  i(j)xi!,nj=nxi=1  i(j)(30)   q(  j):q(  j)   epni=1e[lnp(xi|ci=j,  j,  j)]+lnp(  j)   |  j|nj+a   d   12e   12pni=1  i(j)[(xi   e[  j])t  j(xi   e[  j])+trace(  j  0j)]   12trace(b  j)(31)wecanseethishasawishartform,q(  j)=wishart(a0j,b0j)wherea0j=a+nj,b0j=b+nxi=1  i(j)(cid:2)(xi+e[  j])(xi   e[  j])t+  0j(cid:3)(32)andagainnj=pni=1  i(j).weusedthefactthat  i(j)(xi   e[  j])t  j(xi   e[  j])=tr(  i(j)(xi   e[  j])(xi   e[  j])t  j)   inalloftheseaboveequations,theexpectationsaretakenusingtheqdistributions,soforexamplee[  j]=m0jusingthemostrecentvalueofm0j.   theresultisthefollowingalgorithm.iremovethe0belowbecausethenotationalreadylooksverycomplicated.igiveiterationindexestotheseparameterstoemphasizetheiterativedependence,whichmakesitlookcomplicated.10avariationalid136algorithmforthegaussianmixturemodelinput:datax1,...,xn,x   rd.numberofclustersk.output:parametersforq(  ),q(  j),q(  j)andq(ci).1.initialize(  (0)1,...,  (0)k),(m(0)j,  (0)j),(a(0)j,b(0)j)insomeway.2.atiterationt,(a)updateq(ci)fori=1,...,n.thisoneiscomplicatedsowebreakitdown.set  (t)i(j)=e12t1(j)   12t2(j)   12t3(j)+t4(j)pkk=1e12t1(k)   12t2(k)   12t3(k)+t4(k)wheret1(j)=dxk=1   1   k+a(t   1)j2!   ln|b(t   1)j|t2(j)=(xi   m(t   1)j)t(a(t   1)j(b(t   1)j)   1)(xi   m(t   1)j)t3(j)=trace(a(t   1)j(b(t   1)j)   1  (t   1)j),t4(j)=  (  (t   1)j)     (xk  (t   1)k)tip:someofthesevaluescanbecalculatedonceandreusedforeachq(ci).(b)setn(t)j=pni=1  (t)i(j)forj=1,...,k(c)updateq(  )bysetting  (t)j=  +n(t)jforj=1,...,k.(d)updateq(  j)forj=1,...,kbysetting  (t)j=(cid:16)c   1i+n(t)ja(t   1)j(b(t   1)j)   1(cid:17)   1,m(t)j=  (t)j a(t   1)j(b(t   1)j)   1nxi=1  (t)i(j)xi!(e)updateq(  j)forj=1,...,kbysettinga(t)j=a+n(t)j,b(t)j=b+nxi=1  (t)i(j)h(xi   m(t)j)(xi   m(t)j)t+  (t)ji(f)calculatethevariationalobjectivefunctiontoassessconvergenceasafunctionofiteration:l=e[lnp(x,c,  ,  ,  )]   e[lnq]11eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture10,11/17/2016instructor:johnpaisleymixturemodelswithdirichletpriors   review:wehavedatax={x1,...,xn}.wewillassumex   rd,butthefollowingdiscussiondoesn   trequireit.wewanttouseamixturemodeltopartitionthedataintoclustersandlearnthestatisticalpropertiesofeachcluster.   mixturemodel:letp(x|  )beaid203distributiononthesupportofx,forexample,adistributiononrd.letp(  )beaprioron  .amixturemodelcanbeusedtogeneratedataasfollows:   model:ci   discrete(  ),xi   p(x|  ci)   priors:     dirichlet(  ,...,  |{z}ktimes),  jiid   p(  )   gaussianmixturemodel(gmm):lasttimewesawhowthegmmresultsfrom  j={  j,  j},p(x|  j)=normal(  j,     1j)andp(  )=p(  ,  )=normal(  |m,(c  )   1)wishart(  |a,b).   akeyquestionis,howmanyclustersarethere?uptonowwe   vebeenpresettingktoavalueandassumingweknowwhatthatvalueshouldbe.whileoneapproachistodocross-validation,theapproachwewilldiscussnextusesbayesiannonparametrics.   abayesiannonparametric(bnp)priorisapriordistributionlikeanyother,exceptitinvolves   in   nity   inawaythatrequiresextratheoreticalanalysis.wewon   tdiscussthetheory,butonlybeinterestedinthepracticalaspectsofabnppriorcalleda   dirichletprocess.      dirichletprocesses(high-level):imagineweusedtheprior     dirichlet(  k,...,  k|{z}ktimes)(1)theonlydifferenceisthatwedivideeachparameter  (somenon-negative,presetnumber)bythedimensionalityof  (k).wethenhavekpossibleclustersasbefore,butweletk      .theresultiscalledadirichletprocess.1   again,therearetwoaspectstothinkingaboutthis:1.oneistheoretical,inwhichit   snecessarytomakesurethingsremainwell-de   nedandwherewewanttounderstandwhat  lookslikeinthiscase,etc.thereisnoconcernaboutthefactthatwehaveanin   nitenumberof  jsittingaround,andthat  isin   nitedimensional,sowecan   tactuallydothisinpractice.2.thus,thesecondaspectispractical.thisessentiallyaddressesthequestionofhowwecandoid136foramodelwherethereareanin   nitenumberofparameters.weclearlycan   tkeepanin   nitenumberofthingsinmemory,sothisissueboilsdownto   ndingequivalentrepresentationsthatwecanusethatdon   trequirethisin   nitenumberofobjects.   weonlyfocusonthepracticalanddiscussanequivalentrepresentation.provingequivalenceisitselfatheoreticalquestion,so#1and#2aren   tmutuallyexclusive.we   llgivesomehintsaswellaboutthetheoreticalideasasitrelatestothegibbssamplingalgorithmwederivenext.   wewon   tdiscussthetheoryindepth,butthefollowingcanhelpwithsomeintuitionaboutwhat  lookslikeask      andwhyitisuseful.1.first,thingsremainwell-de   ned.second,thenumberofdimensionsof  thathaveprob-abilitygreaterthansomesmallnumberwillbesmall.virtuallyalldimensionswillhavevanishinglysmallprobabilitiesandtechnicallyanin   nitenumberwillhaveid203equaltozero.therefore,eventhoughthereareanin   nitenumberofclusters,fora   niteamountofdataonlyasmallnumber(e.g.,10)willactuallybeused.2.sincethenumberisvariableandlearnedaspartofid136,thedirichletprocessinsomesenseuncoversthe   correct   numberofclusters.(theuseof   correct   isnotatallpartofthetheory!)theposteriordistributionwillreturnanumberof   occupied   clustersthatisdependentonthedatasetused.3.ultimately,thetheoryisonlyusedtoarguewhythispriorwillbeusefulforourpurposesofnotpresettingk,andnotwhyit   sthe   correct   waytolearnk.4.dirichletprocessesaresometimesreferredtoassparsity-promotingpriorsforthisreason.   again,ourconcernispractical,soweareinterestedinhowtoinfer(  ,  1,...,  k,c1,...,cn)ask      .   wewillshowhowmarginalization(of  )cansavethedayinanmcmcgibbssamplingsetup.beforedoingthis,noticethedifferenceinadvance:   inem,weused   de-marginalization   tohelp.thatis,wewantedtomaximizeamarginaldistributionoversomevariablesandwedidthisbyaddingnewvariablessuchthattheyproducedthecorrectmarginal.   inthislecture,wewillhavedif   cultydoingid136overallthemodelvariables,sowewillintegrateoutoneofthempermanentlyandonlysampletheremainingones.   generallyspeaking,whileemusesonedirection,mcmc   ndsbothdirectionsuseful.2   beforewederivethegenericgibbssamplingalgorithmforthemixturemodelwherek      ,let   sreviewthegibbssamplerforthefullmodelwith  froma   nitedirichletpriorwithrepeatedparameter  /k(andsok<   ).   recallthegibbssamplingprocedure(inthecontextofthismodel)1.foreachiteration:(a)sampleeachcigiventhemostrecent  ,  andotherc   i(b)sampleeach  igiventhemostrecentc,  and     i(c)sample  giventhemostrecentcand     weusethesubscript   itoindicateallvariableindexesexceptfortheithone.wesampleeachvariablefromtheirconditionalposteriors.theseare:gibbssamplingforthe   nitedirichletmixturemodel   sampleci:theconditionalposteriorp(ci|  ,  ,c   i,x)doesnotdependonc   jandx   i.thusp(ci=j|  ,xi,  )   p(xi|  ,ci=j)p(ci=j|  )=p(xi|  j)  jpk   =1p(xi|     )     (2)andsowecalculatehowlikelyeachclusterisforthedatapointconsideredxi,andpre-weightthisbehowlikelytheclusterisingeneral.thisgivesakdimensionalvectorthatwenormalizeandthenuseastheparameterofadiscretedistributiontosampleanewindexvalueforci.   sample  j:theconditionalposteriorp(  j|     j,c,  ,x)onlydependsonthedatainxthathasbeenassignedtoclusterj.wecanwritethisas,p(  j|x,c)   p(x|  j,c)p(  j)   "nyi=1p(xi|  j)1(ci=j)#p(  j)(3)thisisproblem-speci   candthepriorisoftenchosentobeconjugatetothelikelihoodsowecancalculatetheposteriorinclosedformandsamplefromit.   sample  :thisisasimpledirichletprior   multinomiallikelihoodsetting.theconditionalposteriorof  onlydependsonc.therefore,p(  |c)   p(c|  )p(  )   "nyi=1p(ci|  )#p(  )   "nyi=1kyj=1  1(ci=j)j#"kyj=1    k   1j#   kyj=1    k+pni=11(ci=j)   1j(4)3de   nenj=pni=11(ci=j).thentheposteriorisp(  |c)=dirichlet(  k+n1,...,  k+nk)(5)   nowweneedtoaddresshowtosample  andeach  jask      .thiswillbedonebyintegrat-ingout  ,whichwillproduceanewmethodforsamplingeachci.discussiononsampling     wewillintegrateout  inthenextsection.thereforewewanttosampleeach  jfromitscondi-tionalposteriorgiventhat  isintegratedout.inthiscase,theconditionalposteriordistributionhasexactlythesameformasbeforep(  j|x,c,     j)   p(x|  j,c)p(  j)   "nyi=1p(xi|  j)1(ci=j)#p(  j)(6)   therefore,theposteriordistributioniscalculatedexactlythesameandthen  jcanbesampledfromitsconditionalposterior.however,nowsincethereareanin   nitenumberof  j(sincej   {1,2,3,...},wecan   tactuallydothis).wecanbreakthisprocessdownintotwocategories:1.nj>0.therecanonlybea   nitenumberofjsuchthatthisisthecase(sincetheamountofdatais   nite).therefore,wede   nitelyneedtosampletheseindicesfromtheirconditionalposterior.2.nj=0.thereareanin   nitenumberofthese.wecan   tactuallysamplethem,however,acrucialobservationisthat,forajsuchthatnj=0,1(ci=j)=0foralli.   therefore,p(  j|x,c,     j)   p(  j)forthesej.thatis,theconditionalposteriorisequaltotheprior(thereisnodatainclusterj,sothereisnoprior-to-posteriorupdate!).   soeventhoughwecan   tsampleanin   nitenumberoftimesfromthisdistribution,wedoknowthedistributionwewouldsamplefromifwecould.   inthiscase,wewouldsampleanin   nitenumberofi.i.d.samplesfromthepriorp(  ).thisfactwillcomeinhandylater.discussionondealingwith  byintegratingitout   thesolutiontoworkingwithanin   nitedimensional  istointegrateitout(ormarginalizeit)fromthemodel.noticethatthisistheoppositesolutionasem,whereweexpandthemodelbyaddingmorevariables.   inotherwords,beforeweweregibbssamplingfromp(  ,c,  |x).nowwewanttogibbssamplefromp(c,  |x)=zp(  ,c,  |x)d  =p(x|c,  )p(  )p(x)zp(c|  )p(  )d  (7)4   wealreadyshowedhowtosample  inthismodel(exactlythesameasbefore).nextweneedtoshowhowtosampleeachci.however,beforedoingthatwebrie   ydiscusswhatthedifferencebetweenthesetwogibbssamplersisinmoredetail.   imagineweobtainedalargesetofsamplesofthesetofmodelvariables{  (t),c(t),  (t)}usinggibbssamplingofp(  ,c,  |x).thesamplesarethistripletformany(say1000)differentvaluesoftasdiscussedmoregenerallyinanearlierlecture.   then,imaginethatwethrewawayeach  (t)andonlykept{c(t),  (t)}   statistically,thesetof   thinned   sampleswewouldhavewouldbeidenticaltoifwesam-pledthispairfromthemarginalizedmodelp(c,  |x).   therefore,ifwedirectlysamplethispairofvariablesfromthemarginalp(c,  |x),wearenotsamplingfromadifferentmodel,butonlysamplingasubsetofvariablesfromthesameoriginalmodel.   often(suchasinthiscase)thissubsetofvariablesissuf   cienttogiveusinformationaboutallwemightcareabout   afterall,  (t)wouldonlylooklikethehistogramofc(t).samplingci   wegobacktoa   nitek,dosomecalculations,andthenletk      .   sinceweintegrateout  ,samplingciturnsouttobedifferentfrombefore.bybayesrule,p(ci=j|x,  ,c   i)   p(x|ci=j,  )|{z}=p(xi|  j)asbeforep(ci=j|c   i)|{z}=?(8)   thederivationofp(ci=j|c   i)isthemainnewcalculationweneed.theresultisveryinter-pretable,butrequiresthefollowingderivation,p(ci=j|c   i)=zp(ci=j|  )p(  |c   i)d  =r  j  dirichlet(  |  k+n(   i)1,...,  k+n(   i)k)d        (cid:16)n(   i)j=xs6=i1(cs=j)(cid:17)=z  j  (  +n   1)q     (  k+n(   i)   )ky   =1    k+n(   i)      1   d  =  (  +n   1)q     (  k+n(   i)   )z    k+n(   i)j+1   1jy   6=j    k+n(   i)      1   d  |{z}=  (  k+n(   i)j+1)q   6=j  (  k+n(   i)   )  (  +n)=  (  +n   1)  (  +n)  (  k+n(   i)j+1)  (  k+n(   i)j)(9)5   wesolvedtheintegralbyrecognizingthatthisisthenormalizingconstantforadirichletdistri-butionwiththeparameterizationindicatedintheequation.   tocompletethecalculation,weusethepropertythat  (y)=(y   1)  (y   1).applyingthisequalityto  (  +n)and  (  k+n(   i)j+1)intheequationabove,we   ndthatp(ci=j|c   j)=  k+n(   i)j  +n   1(10)   asaresult,p(ci=j|x,  ,c   i)   p(xi|  j)   k+n(   i)j  +n   1!(11)lettingk         theproblemisthatask      ,moreandmoren(   i)j=0.whenn(   i)j>0,there   snoproblem.wediscussthesetwocasesbelow.   casen(   i)j>0:inthelimitk      ,p(ci=j|x,  ,c   i)   p(xi|  j)n(   i)j  +n   1(12)   casen(   i)j=0:inthelimitk      ,p(ci=j|x,  ,c   i)   0(13)   doesthismeanthatciislimitedtotheexistingclusters?no!insteadofaskingtheid203ci=jforaparticularjinthecasewheren(   i)j=0,weaskp(ci=new|x,  ,c   i)=xj:n(   i)j=0p(ci=j|x,  ,c   i)(14)   limk      xj:n(   i)j=0p(xi|  j)  /k  +n   1(15)   limk          +n   1xj:n(   i)j=0p(xi|  j)k(16)   whatisthislastlimit?rememberthat  jiid   p(  )forthein   nitenumberofjsuchthatn(   i)j=0.wecansaythatask      thismontecarlointegralconvergestothetrueintegralitapproximateswhenkis   nite.therefore,limk      xj:n(   i)j=0p(xi|  j)k=e[p(xi|  )]=zp(xi|  )p(  )d  (17)6   asideaboutmontecarlointegration:imaginewewanttocalculatee[p(xi|  )]=rp(xi|  )p(  )d  .ifthisintegralisintractable,approximateonewayistosample  jiid   p(  )forj=1,...,ktimesandapproximatee[p(xi|  )]   sk:=1kpkj=1p(xi|  j).thenlimk      sk=e[p(xi|  )].next,whathappensifwethrowawaya   nitenumberoftheevaluationsp(xi|  j)butstilldividebyk?inthiscase,nothingchangesask      andtheconvergenceisstilltothesameintegral.   tosummarize,wehaveshownthatci=                     jw.p.   p(xi|  j)n(   i)j  +n   1ifn(   i)j>0anewindexw.p.       +n   1rp(xi|  )p(  )d     finally,ifwepickanewindex,thequestionis,whatindexdidwepick?theansweris   whocares   !?intheend,thereisa1-to-1mappingbetweenthenewindexj0andnewvariable  j0.soweonlyneedtopickthenewvariable.thisisnottrivialtoprovehowtodo,butthesolutionis,ifci=j0andj0isanewindex(i.e.,n(   i)j0=0),then  j0|{ci=j0}   p(  |xi)(18)   thisgivesageneralalgorithmforsamplingfromtheposteriorofadirichletprocessmixturemodel.speci   cproblemsthatneedtobeaddresseddependingonwhattypeofmixtureitis(e.g.,gaussianmixture)arecalculatingtheposteriorof  givenxandthemarginalrp(x|  )p(  )d  .amarginalizedsamplingmethodforthedirichletprocessmixturemodel   initializeinsomeway,e.g.,setallci=1fori=1,...,n,andsample  1   p(  ).   atiterationt,re-indexclusterstogofrom1tok(t   1),wherek(t   1)isthenumberofoccupiedclustersafterthepreviousiteration.sampleallvariablesbelowusingthemostrecentvaluesoftheothervariables.1.fori=1,...,n(a)foralljsuchthatn(   i)j>0,set    i(j)=p(xi|  j)n(   i)j/(  +n   1)(b)foranewvaluej0,set    i(j0)=    +n   1zp(xi|  )p(  )d  (c)normalize    iandsampletheindexcifromadiscretedistributionwiththisparameter.(d)ifci=j0,generate  j0   p(  |xi)2.forj=1,...,k(t)generate  j   p(  |{xi:ci=j})(kt=#non-zeroclustersthatarere-indexedaftercompletingstep1)7comments:1.thereisalotofbookkeepingwiththisalgorithmthatcanveryeasilyleadtocodingissues.2.n(   i)jneedstobeupdatedaftereachciissampled.thismeansthatanewclusterispossiblycreated,ordestroyed,aftereachiinstep1.itiscreatedifcipicksanewj0.3.aclusterisdestroyedifthepreviousvalueofciwastheonlyinstanceofthisvalueinthedataset.whenwecheckforthenewvalueofci,weerasethevaluepreviouslyassignedtoit,inwhichcasetheclusternolongerexistsbecausexiwastheonlyobservationinthatcluster.then,ifcijoinsanexistingclusters,thenumberofclustersisreducedbyone.ifcistillcreatesanewclusterj0,anewvalueof  j0mustbegeneratedandsothe  variableisstillchanged.4.n(   1)jneedstoalwaysbeuptodate.whenanewciissampled,thesecountsneedtobeupdated.5.thereforethe   newvaluej0   instep1bcanbechanging(e.g.,foronei,j0=10,forthenextiperhapsj0=11ifthepreviouscipickedj0whenj0=10)6.keepingindexingcorrectisalsocrucialtogettingthingstowork.aftereachiteration,itwouldbegoodtore-indexclusterstostartat1andincreaseone-by-onefromthere.generativeprocess   we   vederivedagibbssamplerforthemarginaldirichletprocess.howdowerepresentthegenerativeprocessthatcorrespondstothismodel?1.generateanin   nitesequence  1,  2,  3,...,where  jiid   p(  ).2.forthenthobservation,de   nekn   1=#unique(c1,...,cn   1)anddrawcn   xi=1n   11  +n   1  ci+    +n   1  (kn   1+1)3.generatedataxn   p(x|  cn).   canwesaysomethingabouttheassumptionsofthisprior?onethingswecansayiswhatknlookslike.thisisthenumberofuniqueclustersinadatasetofsizengeneratedfromthismodel.first,weknowthatkn=nxi=11(ci6=cj<i).whatise[kn]?e[kn]=nxi=1e[1(ci6=cj<i)]=nxi=1pr(ci6=cj<i)wecanjustlookatthegenerativeprocess:weknowthatpni=1pr(ci6=cj<i)=    +n   1,soe[kn]=nxi=1pr(ci6=cj<i)=nxi=1    +n   1     ln(  +n   1).8   onepotentiallyconcerningaspectofthisprioristhatcionlydependsonc1:i   1togeneratedata,butwhenwedoid136weconditiononc   i.isthealgorithmwrong?ifnot,howdowereconcilethisapparentcontradiction?   theanswerrelatestotheconceptofexchangeability.tothisend,weaskwhatisp(c1,...,cn)=p(c1)nyi=2p(ci|c1,...,ci   1)wecanagainreadtheseprobabilitiesfromthegenerativeprocess.letn(n)k=pni=11(ci=k).thenp(c1,...,cn)=  knqknk=1n(n)k!qni=1  +i   1thecrucialobservationisthatp(c1,...,cn)isindependentoftheordering.thatis,ifwede   neapermutationoftheintegers,  (i)   {1,...,n}fori=1,...,n,thenp(c1,...,cn)=p(c  (1),...,c  (n))exchangeabilityisaheavily-researchedtheoreticalareaofid203.forourpurposes,wesimplyconcludethatwecanreorderthedatahoweverwelikeandtreateachciasifitwerethelastonewhensampling.(itmightbetemptingtoconcludethatwecouldalsonotreorderciandonlysampleconditionedonthepreviousvalues.however,thevalueofciwillalsoimpacttheprobabilitiesofallfuturec.ratherthan   gureouthowtowritetheconditionalposteriorforastrictordering,wejusttreateachciasifitwerethelastoneandsoit   sobviouswhatthecondi-tionalpriorshouldbewhenusingbayesrule.)dirichletprocessgaussianmixturemodel   forthismixturemodel,p(  )=p(  |  )p(  ),where     wishart(a,b),  |     normal(m,(c  )   1)(19)   invariationalid136wedidn   tneedthislinkedpriorbecausewewereusinganapproximationthatallowedfortractablecalculations.withgibbssampling,weneedtobemorerigorouslycorrectandsoinordertocalculatethemarginaldistributionp(x),weneedtoconnectthepriors.   imaginethatx1,...,xswereassignedtoclusterj,thenp(  j,  j|x1,...,xs)=normal(  j|m0j,(c0j  j)   1)wishart(  j|a0j,b0j)(20)wherem0j=cs+cm+1s+csxi=1xi,c0j=s+ca0j=a+s,b0j=b+sxi=1(xi     x)(xi     x)t+sas+1(  x   m)(  x   m)twede   ne  x=1spsi=1xi.9   whenwewanttosampleanewclusterinwhichxiistheonlyobservation,wecanalsousethisposteriordistribution.wejustdon   thaveanysumsands=1.noticethatoneofthetermsinb0jendsupequalingzero.   nextweneedtocalculatethemarginaldistributionunderthepriorp(x)=zzp(x|  ,  )p(  ,  )d  d  (21)   again,thepriorp(  ,  )=p(  |  )p(  )wherep(  |  )=normal(m,(c  )   1),p(  )=wishart(a,b)   usingthispriorandalikelihoodp(x|  ,  )=normal(  ,     1),themarginalisp(x)=(cid:18)c  (1+c)(cid:19)d2|b+c1+c(x   m)(x   m)t|   a+12|b|   a2  d(cid:0)a+12(cid:1)  d(cid:0)a2(cid:1)(22)   andwhere  d(y)=  d(d   1)4dyj=1  (cid:18)y+1   j2(cid:19)   again,disthedimensionalityofx.   whenimplementingthis,theremightbescalingissues.forexample,inthefractionx1/x2,thetermsx1andx2mightbetoolargeforacomputer(i.e.,   roundedup   to   ),buttheirfractionissomething   reasonablysized   .thisisparticularlyrelevantforthefraction  d(cid:0)a+12(cid:1)  d(cid:0)a2(cid:1)   animplementationtrickistorepresentthisas  d(cid:0)a+12(cid:1)  d(cid:0)a2(cid:1)=epdj=1[ln  (a+12+1   j2)   ln  (a2+1   j2)]   by   rstcalculatingthevalueintheexponent,wecanworkwithsmallerterms(becauseofthenaturallog)andtheycancanceleachothersuchthatthesumissmall.theexponentthenreturnsthis   nalvaluetothecorrect   space   .thistrickisoftenusefulwhenimplementingmachinelearningalgorithmsthatrunintoscalingissues(whenweknowthattheyshouldn   t).10eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture11,12/1/2016instructor:johnpaisley   wenextlookatamodelforsequentialdata.inthiscase,weassumewehaveasinglesequence(x1,...,xt)whereeachxt   {1,...,v}.wewanttomodelthesequentialinformationinthisdata.   forsimplicity,wewillassumewehaveonlyonesequencefornow.however,inrealityweoftenhavemanysequences.i   lladdressthesimplemodi   cationsthatcanbemadetoaccountforthisattheend.fornow,assumetisverylargesowecanhaveplentyofsequentialinformationinthissinglesequence.   oftenthedataisinrd.thatwouldrequireaslightlydifferentmodelingapproachthanwhatwewilldiscussbelow.incidentally,discrete-valuedsequencesoftenarisefromwhatareoriginallycontinuous-valuedsequences.forexample,youcantaketheoriginaldatainrdandquantizeitby   rstrunningaid91algorithm(id116istheusualone)andthenmappingxt   rdtoavaluext   {1,...,v},wherethevalueindicateswhichofthevclusterstheoriginalxtwasmappedto.   therefore,thefollowingdiscussingondiscretesequencesisusefulformanydifferenttypesofsequentialdatathataren   tinitiallydiscrete-valued.   wewillnextdiscussamodelforx=(x1,...,xt)calleda(discrete)hiddenmarkovmodel.hiddenmarkovmodel(id48)   firstthehigh-leveldescription.weassociateeachobservationxtinthesequencexwithahidden   state   st.therefore,thereisasequences=(s1,...,st)thatcorrespondstoxwhereeachelementinsgivesthestateforthecorrespondingelementinx.fornow,the   state   isatotallyabstractthing,muchlikethe   cluster   wasanabstractconceptinthegmm.   oftenonecanhopeinadvancetolearnstatesthatcorrespondtosomethingmeaningful(e.g.,machineis   working   or   notworking   ),butforthislecturetakeastatetobesomethingthatisanabstractmathematicalconcept.   therearethreevariablesthatde   neak-statediscreteid48:11.  :ak-dimensionalid203vectorusedtogenerates12.a:ak  kmarkovtransitionmatrix.aijistheid203oftransitioningtostatejgiventhatthecurrentstateisi3.b:ak  vemissionmatrix.bivistheid203ofobservingx=vgivenitscorrespondingstates=i.   model:usingthesemodelvariables,wegeneratedatafromtheid48asfollows.again,wehaveasequenceofdatax=(x1,...,xt)wherext   {1,...,v}.ak-statediscreteid48modelsthissequenceasfollows.   generatestatesequence:s1   discrete(  ),st|st   1   discrete(ast   1,:)   generateobservationsequence:xt|st   discrete(bst,:)   eachrowinaisaid203distributionthatdecideswhatthenextstatewillbe.therefore,st   1   {1,...,k}simplypicksoutthecorrectrowofatouse,whichiswhatthenotationisindicating.similarlywithb,thecurrentstatestpicksouttheid203distribution(i.e.,rowofb)tousetogeneratext.   inthissense,astatecorrespondstoaid203distributiononanobservation.it   sworththink-ingabouthowthisrelatestothemixturemodel.eachrowofbisadata-leveldistributionmuchlikeagaussianwasforthegmm.eachrowofaislikeamixingdistributiononwhich   cluster   (or   state   here)topicktogenerateanobservation.theid48islikeamixturemodelwherethe   clusters   arenotchanging,butthedistributionontheclustersischangingaccordingtoa   rst-ordermarkovprocess.maximumlikelihoodexpectation-maximization(ml-em)   inmostofwhatfollows,ourgoalisto   ndapointestimateofh={  ,a,b}thatmaximizesthemarginallikelihoodlnp(x|h)=lnxsp(x,s|h)(1)   comments:1.fromnowon,we   llusehfor  ,a,bwhentheyareallconditionedon.2.thesumisoverallt-lengthsequencess.sincetherearektpossiblesequences,thissumcan   tbedirectlydone.3.however,withemweneveractuallyneedtocalculatethemarginalexplicitly.weonlyneedtobeabletowriteoutthejointlikelihoodoverxands.4.wementionthatthesumcanbeperformedinacleverwayusingoutputsfromeitherthe   forward   orthe   backward   algorithms,bothofwhichwewilldiscusslater.2   jointlikelihood:asusual,wehavetostartherebeforewecandoanythingelse.p(x,s|h)=p(x|s,b)p(s|a,  )(2)="tyt=1p(xt|st,b)#"p(s1|  )tyt=2p(st|st   1,a)#="tyt=1kyk=1vyv=1b1(st=k)1(xt=v)kv#"kyk=1  1(s1=k)k#"tyt=2kyi=1kyj=1a1(st   1=i,st=j)ij#   comments:1.p(x|s,b):giventhestatesequences,xonlydependsonb.also,ifiknowwhichstateeachobservationcamefrom,thentheobservationsareallindependentofeachother.thisisaresultofthemodelde   nition.sowecanwritethislikelihoodasaproduct.2.p(s|  ,a):bythechainruleofid203p(s|  ,a)=p(s1|  ,a)tyt=2p(st|s1,...,st   1,  ,a)thisisalwaystrue.bythe   rst-ordermarkovproperty(i.e.,themodelde   nition)wecanfurthersaythatp(st|s1,...,st   1,  ,a)=p(st|st   1,  ,a)andalsosimplifythecondition-ingbyremoving  oraasrequired.3.noticethatwe   veusedindicatorsagaintopickoutthecorrectentriesin  ,aandb.thisisbasicallyalwaysusefulfordiscretevariables/data.   emsteps:recallthethreemainemsteps(the   rsttwoarethe   e   andthethirdisthe   m   step)1.setq(s)=p(s|x,h)2.calculatel=eq[lnp(x,s|h)]3.maximizeloverh={  ,a,b}   the   rststepalreadyposesaproblem.recallfromthegmmthatwehadnodif   cultylearningq(c)wherecwasthevectorofclusterindicators(cn=jmeansobservationxncamefromclusterj;refertothepreviouslecturenotesformoredetails).thisisbecauseconditionedonthegmmmodelvariables  ,andthedatax,c1,...,cnwereconditionallyindependent.therefore,p(c|x,  )=qnn=1p(cn|xn,  )andthusq(c)=qnn=1q(cn)whereq(cn)=p(cn|xn,  ).   theconditionalposteriorp(s|x,h)6=qtt=1p(st|xt,h),andsowecan   teasilysolveforq(s)inthisway.   therearektdifferentsequencesthatscouldbe.inprincipalwewouldthereforehavetocalculatep(s|x,h)   p(x|s,h)p(s|h)foreachofthese.eventhoughktis   nite   andsoweknowthatp(s|x,h)isjustamultinomialthatcanbecalculatedinprincipal   inpracticektiswaytoolargetoenumerateallvalues.3   we   llseethatwedon   tactuallyhavetocalculateq(s)foreachandeverysequences.however,wedon   tknowthatyet.sincewecan   ndq(s)inprincipal,wewilladdressstep1abovebysimplydeclaringthatwe   vecalculatedit.   step1ofem:   wehavesetq(s)=p(s|x,h).   let   spretendthisisthecasetotryandmakesomeprogress(andseeifweactuallydon   tneedthistobethecase).   step2ofem:nexttaketheexpectation,eq[lnp(x,s|h)].l=txt=1kxk=1vxv=1eq[1(st=k)]1(xt=v)lnbkv   eq[lnp(x|s,b)]+kxk=1eq[1(s1=k)]ln  k   eq[lnp(s1|  )]+txt=2kxi=1kxj=1eq[1(st   1=i,st=j)]lnaij   eq[lnp(s2,...,st|a)](3)   therearetwokeyexpectationshere.1.eq[1(st=k)]:theexpectationofanindicatorofaneventistheid203ofthatevent,eq[1(st=k)]=q(st=k).toseethis,xsq(s)1(st=k)=kxs1=1      kxst   1=1kxst+1=1      kxst=1q(s1,...,st   1,st=k,st+1,...,st)=p(st=k|x,h)   q(st=k)(4)whatisgoingonhere?thisexpectationisjustthemarginaldistributionofq(s)whereweintegrate(i.e.,sum)overeveryvalueofst0exceptstwhichwesetequaltok.inotherwords,insteadofneedingtoknowtheentireposteriordistributionofasequences,wearehereonlyaskingfortheposteriorid203thatst=kgivenxandhwithoutregardtowhatanyothervalueofthesequenceisequalto.2.eq[1(st   1=i,st=j)]:byusingexactlythesamereasoningasabove,eq[1(st   1=i,st=j)]=p(st   1=i,st=j|x,h)   q(st   1=i,st=j)(5)again,weonlycareabouttheposteriorid203ofaspeci   ctransitionataspeci   ctime,notcaringaboutanythingelsegoingoninthesequence.   westilldon   tknowwhattheseprobabilitiesare,orhowtocalculatethem.however,wehaveshownthatinordertocompletethee-step,weneverneedtoknowtheposteriordistributionoftheentiresequence.weonlyneedtoknowmarginalposteriordistributionsonisolatedportionsofthatsequence.4   thatis,wedon   tneedtocalculatep(s|x,h),onlyp(st=k|x,h)andp(st   1=i,st=j|x,h)forallt,k,iandj.thisisstillnon-trivial,butwe   llseethatthereisanalgorithmthatlet   susgetthesevaluesquicklycalledtheforward-backwardalgorithm.   step3ofem:let   spretendwehavep(st=k|x,h)andp(st   1=i,st=j|x,h)alreadycalculatedandquicklytakecareofthem-step.usinglagrangemultiplierstoensurethattheupdatesfor  ,aandbareid203distributions,wecanmaximizel=xt,k,vq(st=k)1(xt=v)lnbkv+xkq(s1=k)ln  k+xt>1,i,jq(st   1=i,st=j)lnaij(6)asfollows:  k=q(s1=k)(7)aij=ptt=2q(st   1=i,st=j)pkk=1ptt=2q(st   1=i,st=k)(8)bkv=ptt=1q(st=k)1(xt=v)pvw=1ptt=1q(st=k)1(xt=w)(9)calculatingq(st=k)andq(st   1=i,st=j)   sowenowjustneedto   ndthesetwomarginaldistributions.wewilldothisinatwo-steppro-cedure,   rstde   ningthemintermsofquantitiesthatagainwewishwehad,andthenpresentinganalgorithmto   ndthosequantities.   calculateq(st=k):forthisproblemwewanttosetq(st=k)=p(st=k|x,h)(10)   p(xt+1,...,xt|st=k,h)|{z}     t(k)p(st=k|x1,...,xt,h)|{z}     t(k)   wemakethede   nitions:     t(k)=p(xt+1,...,xt|st=k,h),theid203ofseeingeverythingtocomeaftersteptgiventhatweareinstatekatstept.     t(k)=p(st=k|x1,...,xt,h),theposteriorid203ofbeinginstatekatsteptgivenallthedataobservedupto,andincluding,thattimepoint.   wehavesimplyusedbayesruleandde   nethetwotermsinthelikelihoodandprioras  and  .wecouldhaveusedbayesruleinotherways,forexampleusingxt+2andlaterin  andincludingxt+1in  .thereasonwedon   tisbecausewewouldn   tbeabletogetthingstoworkoutthatway.ultimately,thegoalistousetherulesofid203tokeepre-writingthingsuntilwereachaformwherewecanactuallystartplugginginnumbersand   ndsolutions.5   againwe   vepushedthesolutionofftoalaterpoint,butnoticethat,ifwecould   ndawaytocalculate  t(k)and  t(k),wecouldthensetq(st=k)=  t(k)  t(k)pkj=1  t(j)  t(j)(11)   calculateq(st   1=i,st=j):forthisoneweagainusebayesrule,followedbyadditionalfac-torizations,q(st   1=i,st=j)=p(st   1=i,st=j|x,h)(12)   p(xt,...,xt|st   1=i,st=j,h)p(st   1=i,st=j|x1,...,xt   1,h)   p(xt,...,xt|st=j,h)p(st=j|st   1=i,h)p(st   1=i|x1,...,xt   1,h)   p(xt+1,...,xt|st=j,h)|{z}     t(j)p(xt|st=j,h)|{z}bj,xt  p(st=j|st   1=i,h)|{z}aijp(st   1=i|x1,...,xt   1,h)|{z}     t   1(i)   again,wehaveusedbayesruleinsuchawaythatwecanmakeprogresstowardssolvingforthisposteriordistribution.noticethattwoofthetermsaredirectlyfromtheid48variables.wecandirectlyplugthesevaluesinforthemostrecentiterationofthealgorithm.wedon   tknowtheothertwoprobabilities,however,noticethattheyareexactlywhatweneedtoknowforq(st=k).therefore,wecanusethesamede   nitionsandwhateveralgorithmwedeveloptosolveq(st=k),wecanusethesameresulttosolveq(st   1=i,st=j).   imaginingthatwehave  and  ,wecanthensetq(st   1=i,st=j)=  t(j)bj,xtaij  t   1(i)pkr=1pks=1  t(r)br,xtasr  t   1(s)(13)   thinkofq(st   1=i,st=j)asthe(i,j)-thelementinak  kmatrixthatgivestheid203ofthistransition.sincetherecanonlybeonetransition,thesumofprobabilitiesinthismatrixhastoequalone.thedenominatorabovemakesthisbethecase.theforward-backwardalgorithm   thealgorithmthatgivesus  tiscalledthe   forwardalgorithm   whilethatwhichgives  tisthe   backwardalgorithm.   theyarerecursivealgorithms,meaningthattolearn  tweneed  t   1,andtolearn  tweneed  t+1(hencethedirectionsinthename).sincewecan   nd  1and  t,wecansolveforall  tand  t.   forwardalgorithm:herewewanttolearnthek-dimensionalvector  tattimet,where  t(k)=p(st=k|x1,...,xt,h).todothis,we   rstwritethisasthemarginalofajointid203distributionandthenusebayesruleonthejointdistribution,6p(st=k|x1,...,xt,h)|{z}  t(k)=kxj=1p(st=k,st   1=j|x1,...,xt)(14)   kxj=1p(xt|st=k,st   1=j,h)p(st=k,st   1=j|x1,...,xt   1,h)   kxj=1p(xt|st=k,h)|{z}bk,xtp(st=k|st   1=j,h)|{z}ajkp(st   1=j|x1,...,xt   1,h)|{z}     t   1(j)   therefore,wecanset    t(k)=bk,xtkxj=1ajk  t   1(j)(15)  t(k)=    t(k)pkj=1    t(j)(16)   wealsonoticethatwecansolveq(s1=k)=p(s1=k|x1,h)=  1(k)exactlyusingbayesrule,  1(k)=  kbk,x1pkj=1  jbj,x1(17)thismeansweknowthestartingpointfor  1andcansolveallsubsequent  t.   backwardalgorithm:wenowwanttolearnthek-dimensionalvector  tattimet,where  t(k)=p(xt+1,...,xt|st=k,h).weagainwritethisasthemarginalofajointid203distribution.however,thistimewewon   tneedbayesrule.p(xt+1,...,xt|st=k,h)|{z}  t(k)=kxj=1p(xt+1,...,xt,st+1=j|st=k,h)(18)=kxj=1p(xt+1,...,xt|st+1=j,h)p(st+1=j|st=k,h)=kxj=1p(xt+2,...,xt|st+1=j,h)|{z}     t+1(j)p(xt+1|st+1=j,h)|{z}bj,xt+1p(st+1=j|st=k,h)|{z}akj   noticethatthistimethereisnoproportionality.wecanjustset  t(k)=kxj=1  t+1(j)bj,xt+1akj(19)7   forthe   rstvalue,wecanthenset  t(j)=1foralljthensolveforthepreviousvalues.noticethatthesenumberscanbecomeverysmallastbecomeslessandlessleadingtocomputerprecisionissues.afterupdatingeach  t,youcannormalizethisvector.noticethatthisre-scalingwillnotchangeanyotherupdatedvalues.   logmarginallikelihood:toevaluateconvergence,weneedtocalculatelnp(x1,...,xt|h)=lnxsp(x,s|...,h)(20)   recallthatsincetherearektdifferentsequencesstosumover,wesimplyaren   tgoingtocalculatethismarginaldirectly.   however,usingthechainruleofid203wecanmakeprogress.thatis,p(x1,...,xt|h)=p(x1|h)tyt=2p(xt|x1,...,xt   1,h)(21)   noticethatp(xt|x1,...,xt   1,h)isactuallysomethingwe   vecalculatedintheforwardalgorithm,p(xt|x1,...,xt   1,h)=kxk=1    t(k)(22)   weseethisbysimplyplugginginwhatwede   nedtobe    t(k),kxk=1    t(k)=kxk=1kxj=1p(xt|st=k,h)p(st=k|st   1=j,h)p(st   1=j|x1,...,xt   1,h)=p(xt|x1,...,xt   1,h)(23)   therefore,lnp(x1,...,xt|h)=txt=1lnkxk=1    t(k)(24)   wecanliterallycomputethisvalue   inpassing   duringtheforwardalgorithm.multiplesequences   finally,weshow(butdon   tderive)howtomodifythealgorithmwhenwehavemultiplese-quencesx(1),...,x(n)ofpossiblyvaryinglengtht1,...,tn.hopefullythisisastraightforwardexercisetoderiveonyourownbynow.   forward-backward:first,giventheid48variablesh={  ,a,b}forward-backwardisrunindependentlyoneachsequencetoobtainvalues  (n)t(k)and  (n)t(k)forthenthsequence.   q(s(n)t=k)andq(s(n)t   1=i,s(n)t=j):forsequencenumbern,theseprobabilitiesarecomputedusing  (n)t(k)and  (n)t(k)and  ,a,bexactlyasbefore.8   updating  ,aandb:simplysumoverthesequencesandnormalize  k=1nnxn=1q(s(n)1=k)(25)aij=pnn=1ptnt=2q(s(n)t   1=i,s(n)t=j)pnn=1pkk=1ptnt=2q(s(n)t   1=i,s(n)t=k)(26)bkv=pnn=1ptnt=1q(s(n)t=k)1(x(n)t=v)pnn=1pvw=1ptnt=1q(s(n)t=k)1(x(n)t=w)(27)map-emid136   inthelastpart,wewillbrie   yseehowsmallmodi   cationscanbemadetoderivemap-emandvariationalid136algorithms.inbothcasesweneedtode   nepriorsontheid48parameters,  ,aandb.typicallyconjugacymotivatesthesetobeak,:   dirichlet(  ),bk,:   dirichlet(  ),     dirichlet(  ).(28)   formap-em,wesimplyaddlnp(h)=lnp(  )qkp(ak,:)p(bk,:)totheemobjective.inthiscasetheupdatetop(s)isidenticaltothepreviousoneandtheonlydifferenceisintheupdatesto  ,aandbabove,wheretermsareaddedinthenumeratoranddenominator.   idon   twritethoseupdatesdown,butpointoutthattherearemany   1   sinvolved,oneinthenumeratorandkoftheminthedenominator(soa   kinthedenominator)resultingfromthedirichletprior.thisindicatesthatformap-emtobeguaranteedtobewell-de   ned,weneedtosetalldirichletparameters   1.themapsolutiongivesasigni   cantlydifferentmeaningtothedirichletparametersthanwhatweapriorithink(wheresparsityrequiresthemtobe<1).   mapformultinomialparameterswithdirichletpriorsingeneralisaproblem.fortunatelyviiseasyinthiscaseand   xesthisinterpretabilityproblem.i   llfocusonvibelow.variationalid136   wewanttoapproximatetheposteriorofthediscreteid48withdirichletpriorsusingvariationalid136.wetherefore   rstneedtode   neafactorizedqdistribution.wewillusedq(s,  ,a,b)=q(s)q(  )kyk=1q(ak,:)q(bk,:).(29)   noticethatwearen   tfactorizingoverthevaluesinthesequences.however,thesameexactexpectationsofidentitiesshowuphereaswell,sowecanfollowthesamereasoningtoarriveatthesamealgorithmasforml-em(veryslightlymodi   edasdescribedbelow).9   let   spretendweknowq(s).thenthefollowingstandard(bynow)stepscanbefollowed:q(ak,:)   expntxt=2kxj=1q(st   1=k,st=j)lnak,j+(     1)lnak,jo=dirichlet(cid:16)h  +txt=2q(st   1=k,st=j)ikj=1(cid:17)(30)   wherethe[]notationindicatesavectorrangingovervaluesofj.similarly,q(bk,:)=dirichlet(cid:16)h  +txt=1q(st=k)1(xt=v)ivv=1(cid:17)(31)q(  )=dirichlet(cid:16)h  +q(s1=k)ikk=1(cid:17)(32)   iftherearemultiplesequences,thesvariableshaveasequenceindexandallqdistributionsaboveincludeanextrasummationoverthesequenceindex.   whataboutq(s)?wewon   tgointoalldetails,butsimplypointoutthatthefollowingchangescanbemadetotheforward-backwardalgorithm.aij   eeq[lnaij],bkv   eeq[lnbkv],  i   eeq[ln  i](33)   thisfollowsthefamiliarpattern.wealsorecognizethatallexpectationsareusingdirichletqdistributions,soareoftheform  (  )     (p  ).itdoesn   timpactthealgorithmthatthesemodi   cationsdon   tsumtooneandsoaren   tid203distributionsanymore   they   resimplypluggedin.   finally,tocalculatethevariationalobjectivefunctionl,wenotethatl=zq(  )lnp(  )q(  )d  +kxi=1zq(ai,:)lnp(ai,:)q(ai,:)dai,:+kxk=1zq(bk,:)lnp(bk,:)q(bk,:)dbk,:+txt=1lnkxk=1    t(k)(34)   noticethattheterminvolving    t(k)isjustlikebefore,onlywe   vereplacedawitheeq[lna],etc.,whencalculatingit.theotherthreeterms,arethenegativekl-divergencesbetweentheapproximateposteriorqandpriorpofeachvariable.   finally,tocalculatelwhentherearemultiplesequences,theterminvolving    thasanadditionalsequenceindexandasecondsumisdoneoverthisindex.the   rstlineisunchanged.10eecse6720bayesianmodelsformachinelearningcolumbiauniversity,fall2016lecture12,12/8/2016instructor:johnpaisleynon-negativematrixfactorization   goal:wehaveam  ndatamatrixxwherexij   0.wewanttoapproximatethiswithaproductoftwonon-negativematrices,   w:am  kmatrix,wij   0   v:ak  nmatrix,vkj   0   xij   (wv)ij=pkk=1wikvkj   thequestions(asusual)aretwo-fold:1.whatobjectivedoweusetomeasureapproximationquality?2.whatalgorithmdoweruntolearnwandv?   lee&seung   snmfpaperisamajor(non-bayesian)stepinthisdirection.theyproposetwoobjectives:squarederror:argminw,vxi,j(xij   (wv)ij)2(1)divergencepenalty:argminw,vxi,jxijlnxij(wv)ij   xij+(wv)ij(2)   thekeycontributionistheirfastmultiplicativeupdaterulesforoptimizingtheseobjectivefunc-tionsoverwandv.   forexample,thealgorithmforthedivergencepenaltyistoupdatewandvasfollowsvkj   vkjpmi=1wikxik/(wv)ijpmi=1wik(3)wik   wikpnj=1vkjxij/(wv)ijpnj=1vkj(4)1   thenmfpapershowsthatthesetwoupdatesproducenewvaluesofwandvthataremono-tonicallydecreasingthedivergencepenalty.similarupdatesarederivedforthesquarederrorpenalty.   thispaperisworthstudyingverycloselyforitsownsake.wediscussitheretomakeconnectionswithbayesianmethods.forexample,whatisthisdivergencepenaltydoing?imaginewehadthefollowingmodel.   model:wehaveam  nmatrixxofnon-negativevalues.wemodelthisasxij   poisson kxk=1wikvkj!(5)wherewikandvkjarenon-negativemodelvariables.   recallthepoissondistribution:x   {0,1,2,...}ispoissondistributedwithparameter  ifp(x=x|  )=  xx!e     ,ex=var(x)=  (6)   jointlikelihood:asusual,writethisout   rstbeforedoinganythingelsep(x|w,v)=myi=1nyj=1poisson(xij|(wv)ij)=myi=1nyj=1(wv)xijijxij!e   (wv)ij(7)   maximumlikelihood:next,considermaximumlikelihoodforthismodel.wewantto   ndargmaxw,vlnp(x|w,v)=xi,jxijln(wv)ij   (wv)ij+constantwrtw,v(8)   noticethatthisobjectiveissimplythenegativeofthedivergencepenaltyofnmf.sincethenmfalgoritid48onotonicallyincreasesthisobjective,thisalgorithmperformsmaximumlikelihoodforthepoissonfactorizationmodeldescribedabove.seethenmfpaperfortheoriginalproof.we   llnextshowthatthemultiplicativeupdatesareequivalenttotheemalgorithm.   thisisanotherperspectiveofthenmfalgorithmforthedivergencepenalty.it   snottheonetheytakeinthepaper,andit   snotrequiredforsomeonetotakeiteither.however,we   llseethatbythinkingintermsofid203models,wecanintroducepriorstructureanduseotheropti-mization(i.e.,id136)algorithmstoapproximatetheposteriordistributionofthesevariables.   imaginederivingagradientalgorithmforequation(8).thiswouldbeverydif   cult.thismotivatedtheusefulnessofthemultiplicativeupdatesoftheoriginalnmfpaper.2maximumlikelihoodemforpoissonmatrixfactorization   ourgoalistoderiveanemalgorithmforthepoissonlikelihoodmodeldescribedabovethatwillhavesimpleandclosedformupdates.therefore,weneedto   ndanappropriatelatentvariabletoaddthathasthecorrectmarginaldistribution.tothisend,wedigressintoapropertyaboutsumsofpoissondistributedrandomvariables.   lety(k)   poisson(  k)independentlyfork=1,...,k.thende   nex=pkk=1y(k).itfollowsthatthemarginaldistributionofxisx   poisson(pkk=1  k).proof:arandomvariableywithdistributionp(y)isuniquelyidenti   edbyitsmomentgener-atingfunction,ep[ety].forapoisson(  )distributionep[ety]=   xy=0ety  yy!e     =e     e  et   xy=0(  et)yy!e     et=e     (1   et)(9)thesumequals1becauseitisoverapoisson(  et)distribution.recognizingthaty(k)aregeneratedindependently,thefollowingcompletestheproof,e[etx]=e[etpkk=1y(k)]=kyk=1e[ety(k)]=kyk=1e     k(1   et)=e   (pkk=1  k)(1   et)(10)theproofiscompletebecausewecalculatedthesemomentgeneratingfunctionsinthecontextofy(k)andfoundthattheirsumhasexactlythesamegeneratingfunctionasapoisson(pkk=1  k)randomvariable.thereforewecansaythatpkk=1y(k)hasthissamedistribution,whichwewillshowisthecorrectmarginaldistribution.   extendedmodel:foreachelement(i,j)inx,wenowusethegenerativemodely(k)ij   poisson(wikvkj),xij|~yij   1(cid:16)xij=kxk=1y(k)ij(cid:17)(11)noticethatthe   distribution   onxijputsallofitsid203massontheeventxij=pkk=1y(k)ij.therefore,there   snothingrandom,butwestillcansaywhatp(x|y)is.alsonoticethatthemarginalofxij   i.e.,thedistributiononxijnotconditionedony(soweintegrateyout)   isxij   poisson(pkk=1wikvkj)asrequired.   jointlikelihood:wenowhavethatthejointlikelihoodincludingtheextravariablesyisp(x,y|w,v)=myi=1nyj=1p(xij|~yij)kyk=1p(y(k)ij|wik,vkj)(12)   emequation:weusethejointlikelihoodtosetuptheemequationlnp(x|w,v)=xyq(y)lnp(x,y|w,v)q(y)+xyq(y)lnq(y)p(y|x,w,v)(13)theqdistributionisonallvaluesy(k)ijfori=1,...,m,j=1,...,nandk=1,...,k.hopefullywecansimplifythis,otherwisewewon   tgetveryfar.3   e-step(part1):the   rstpartofthee-stepistosetq(y)=p(y|x,w,v).wesimplyusebayesruleandseewhatprogresswecanmake.p(y|x,w,v)   p(x|y,w,v)p(y|w,v)   p(x|y)p(y|w,v)   myi=1nyj=1p(xij|~yij)p(~yij|w,v)(14)wehaveusedtheconditionalindependencede   nedbythemodeltowritethisexpression.noticethatforeach(i,j)pair,wehavea   mini   bayesruleembeddedinthisproblem.thatisp(y|x,w,v)=myi=1nyj=1p(xij|~yij)p(~yij|w,v)p(xij|w,v)=myi=1nyj=1p(~yij|xij,w,v)(15)thusweknowthatq(y)=qi,jq(~yij)andq(~yij)=p(~yij|xij,w,v).wejustneedtoseeifwecancalculatethisforone(i,j)pair.   inwords,thisissayingthatifsomeoneelsegeneratesy(k)ij   poisson(wikvkj),calculatesxij=pkk=1y(k)ijandthenshowsmexijandwandv,whatismyposteriorbeliefabout~yij?   again,wesolvebayesruleforthissub-problem.theresultisoneofthefavoritesofid203,hereunfortunatelyderivedusingbogged-downnotationfromthisproblem.p(~yij|xij,w,v)=p(xij|~yij)p(~yij|w,v)p(xij|w,v)=1(xij=pkk=1y(k)ij)qkk=1(wikvkj)y(k)ijy(k)ij!e   wikvkj(wv)xijijxij!e   (wv)ij(16)inthenumerator,we   vemultipliedtheindicatordistributiononxijwiththeproductofkin-dependentpoissondistributionson~yij.inthedenominator,weusethefactthatthemarginaldistributiononxijispoisson(pkk=1wikvkj),whichweprovedabove.wesimplywriteoutthisdistributionhere.the   nalkeystepistosimplifythis,p(~yij|xij,w,v)=1(cid:16)xij=kxk=1y(k)ij(cid:17)xij!qkk=1y(k)ij!(cid:18)wikvkjpkl=1wilvlj|{z}     ij(k)(cid:19)y(k)ij=multinomial(xij,  ij)(17)   theconditionalposterioronthekpoissonrandomvariablesinthevector~yijgiventhattheirsummustequalxijisamultinomialdistributionwithid203distributionequaltothenor-malizationofthekparametersinthepoissonprioron~yij.whilethiswouldn   tbeobviousapriori,it   sanintuitivelyreasonableresultandaniceonetoo.noticethattheindicatorinfrontissuper   uoussincebyde   nitionofthemultinomialdistributionwiththeseparametersthissummustbetrue.wecanthereforeignoreit,butnoticethatithadtobetheretostartthecalculation.4   e-step2:nextwecalculatetheexpectationusingthisq(y)distribution.l=xyq(y)lnp(x,y|w,v)(18)=mxi=1nxj=1neq[lnp(xij|~yij)]+kxk=1eq[lnp(y(k)ij|wik,vkj)]o=mxi=1nxj=1kxk=1eq[y(k)ij]ln(wikvkj)   wikvkj+constant   intheseequalities,wewereabletogetridofeq[lnp(xij|~yij)]=eq[ln1(xij=pkk=1y(k)ij)]becauseq(~yij)issuchthatxij=pkk=1y(k)ijwithid203equaltoone.thereforetheex-pectationisentirelyoverln1=0.(thankfully!ifanyoftheq(~yij)hadnonzeroid203ofxij6=pkk=1y(k)ijthenl=      andwecouldn   tproceed).   noticethatgivenq(~yij)=multinomial(xij,  ij),wecanseteq[y(k)ij]=xij  ij(k).   m-step:finallywetakederivativeswithrespecttowikandvkjandsettozero.   wikl(w,v)=0=nxj=1xij  ij(k)wik   nxj=1vkj   wik=pnj=1xij  ij(k)pnj=1vkj(19)   vkjl(w,v)=0=mxi=1xij  ij(k)vkj   mxi=1wik   vkj=pmi=1xij  ij(k)pmi=1wik(20)   nowrecallingthat  ij(k)=wikvkj/(wv)ij,weseethattheupdatesareidenticaltothemulti-plicativeupdatesforthenmfalgorithmusingthedivergencepenalty.noticethatthevaluesofwandvin  are   xedatthevaluesofthepreviousiterationforallupdatesinthisstep.alsonoticethattheseupdatesdonotmaximizelifdoneonlyonce.however,theydoincreasel,whichisallthatisrequired.wecouldcontinuetoiteratebetweenupdatingwandvfora   xed  ,orjustmakeoneupdatetoeachandthenupdate  .   theemalgorithmalsocontainsalittlemoreinformationthanthemultiplicativealgorithmfornmfdiscussedatthebeginningofthislecture.(butit   snotobviousat   rstglanceifthisextrainformationisusefulatall.ifitisitwouldbeforcomputationalef   ciency.)aswrittenintheoriginalnmfalgorithm,itappearsthatallvaluesofwneedtobethemostrecentoneswhenupdatingvandvice-versaforupdatingw.justlookingatequations(3)and(4)aboveisn   t5enoughtosayotherwise.however,weknowfromemthatthefunctionsofwandvinthenumeratorcorrespondto  andsowecankeepthoseattheoldvaluesandonlyupdatethede-nominators.therefore,onecoulditeratetheoriginalnmfalgorithmseveraltimesonlyupdatingthesumsinthedenominatorsandholdingallothervalues   xed,andseparatelyupdatewandvinthenumeratorandmultipliedoutfronteveryfewiterations.variationalid136forpoissonmatrixfactorization   wehavemotivatedabayesianapproachtothisproblem.next,weagainde   nethemodel,thistimealongwithitspriors,andthendiscussavariationalid136algorithmforapproximatingtheposterior.thisalgorithmwillintroducenewchallengesthatwehaven   tfacedbefore,andwe   llworkthroughapossiblesolutionthatcanbemademoregeneral.   model:wehavethematrixxandmodelitwithak-ranknon-negativefactorizationwvsuchthatxij   poisson((wv)ij)(21)   priors:weusegammapriorsforwandvasfollows:wik   gamma(a,b),vkj   gamma(c,d)(22)   posterior:weapproximatetheintractableposteriorp(w,v|x)withq(w,v)usingvariationalid136.weusethefactorizationq(w,v)="myi=1kyk=1q(wik)#"nyj=1kyk=1q(vkj)#(23)   problem:werunintoanotherproblemthistimeinthatthingsarestillintractable.thevariationalobjectivefunctionisl=eq[lnp(x|w,v)]+eq[lnp(w)p(v)]   eq[lnq](24)=xi,jxijeqhlnkxk=1wikvkji|{z}=???   kxk=1eq[wikvkj]+eq[lnp(w)p(v)]   eq[lnq]+const.   forthismodelwecan   tevencalculatel,sohowdowetakederivativestoupdatevariationalparametersforeachq?the   optimalmethod   forlearningeachqhasthesameproblem,sincetheproblematicexpectationstillneedstobetaken.   therefore,wedon   thaveananalyticobjectivefunctiontoworkwith.thisisaverycommonproblemandhasafewsolutions.onetrickistoreplacetheproblemfunctionwithanotherfunctionthatapproximatesit(bylowerboundingit)andistractable.   sincewewanttomaximizel,wethereforelowerboundtheproblemfunction.   let   sworkoutthisproblemforthiscaseusingmoreabstractnotation.6   problemfunction:eqlnpkzk,q=qkq(zk)   thefunctionlnisconcave.recallthatforaconcavefunctionofarandomvariable,f(z),f(ez)   ef(z)(25)   noticethatthefunctioneqlnpkzk   lnpkeqzk.therefore,wecan   tlowerboundtheproblemfunctionthisway   insteadit   sanupperbound.theexpectationineqlnpkzkisnottheexpectationcorrespondingtothefunctionfabove.   instead,weintroduceabrandnewdiscretek-dimensionalid203vector  =[  (1),...,  (k)]andwritethetrivialequalitylnxkzk=lnxk  (k)zk  (k)(26)   thisisa   trick   inthat  doesn   tnecessarilyhaveamodelingpurpose.we   rejustusingmathtoouradvantage.(however,wecouldalsomotivatethisinawaysimilartothepreviousemalgorithm,whereweintroducedauxiliaryvariables.)   wehavewrittenlnpkzk=lne[z/  ],wherethistimetheexpectationiswithrespecttothedistribution  .sincelnisconcavelnxkzk=lne[z/  ]   e[lnz/  ]=kxk=1  (k)lnzk  (k)(27)   therefore,inthevariationalobjectivefunctioneqlnkxk=1zk   kxk=1  (k)eqlnzk   kxk=1  (k)ln  (k)(28)   noticethattheproblemshouldbe   xednowsincetheseareexpectationsweusuallycantake.pickingthelowerboundaswedidispartofthe   art   ofthistechnique.thereareotherlowerboundswecoulduse.somearenogoodbecausetheydon   tresultinanalyticexpectations.somemightbebetterthanthisoneinthattheyaretighter   theyapproximatetheoriginalfunctionmoreclosely.thisspeci   clowerboundisprobablynottheonlyoption.   thenextquestionishowdoweset  =[  (1),...,  (k)]?wewanttoset  sothattheboundisasgoodofanapproximationaspossible.   thereforewewanttomaximizethislowerboundover  .usinglagrangemultipliers,wecan   ndthatthelowerboundismaximizedwhen  (k)=eeqlnzkpk   =1eeqlny   (29)7   atthispoint,wehavetwopathswecangodown:1.plugthis  (k)backintothelower-boundedobjectivefunction2.keep  (k)asanauxiliaryvariableandusethisupdateforit   path1:pluggingthisvalueof  (k)backinandsimplifying,we   ndeqlnkxk=1zk   lnkxk=1eeqlnzk(30)   thisisthetightestpossiblelowerboundofeqlnpkk=1zkwhenwelimitourselvestoselectingfromthefamilypkk=1  (k)eqlnzk   pkk=1  (k)ln  (k)   ifwegodownthispath,thentheoriginalproblemismodi   edtol   xi,jxijlnkxk=1eeqlnwik+eqlnvkj   kxk=1eq[wik]eq[vkj]+eq[lnp(w)p(v)]   eq[lnq](31)   the   rsttermistheonlyplacewhereanapproximationisbeingmade.thispathhassomeprosandcons.   pros:wehaveaclosedformobjectivethatisthetightestpossibleapproximationgiventhelowerboundweuse.(we   llseethatpath2actuallyhasthissamepro.)   cons:wecan   tusetheoptimalmethodto   ndq.weneedtotakederivativesandusegradientmethods.thatisok,butliketheemstory,ifwecouldavoiddoingthisitwouldbepreferable.   path2:thesecondoptionistokeep  asanauxiliaryvariablethatwealsooptimizeover.sincethereisafunctioneq[lnpkwikvkj]foreach(i,j)pair,weintroduceavector  ijforeachofthesetolowerboundit.thisisbecausethebestlowerboundwillbedifferentforeach(i,j).lowerboundingthemindividuallyratherthanusingoneshared  willresultinamuchbetterapproximation(itlikelywouldn   tworkwellwithasingle  sincetheoverallapproximationwillbebad).   therefore,welowerboundthevariationalobjectivefunctionasl   xi,jkxk=1xij[  ij(k)eq[lnwik+lnvkj]     ij(k)ln  ij(k)](32)   kxk=1eq[wik]eq[vkj]+eq[lnp(w)p(v)]   eq[lnq]   theadvantageofthisfunctionisthatwecannowusetheoptimalmethodfor   ndingeachq.also,noticethatatthepointofconvergence,noneoftheparameterschangeanymore.therefore,  ij(k)willequalitsoptimalvalue.therefore,path2   ndsalocaloptimalofthesamefunction8thatpath1does.itjustdoesitbytakingafewmoresteps.wehadaverysimilarsituationwithemandit   sworthindependentlythinkingmoreaboutthesimilarities.   letln  p(x,w,v)bethelog-jointdistributionusingthelowerboundinsteadofthetrueobjective,ln  p(x,w,v)=xi,jkxk=1xij[  ij(k)(lnwik+lnvkj)     ij(k)ln  ij(k)]   kxk=1wikvkj+eq[lnp(w)p(v)]   usingtheoptimalmethodfor   ndingqwiththislowerbound,wehavethefollowingalgorithm.   findingq(wik):bythetypicalroute,wehaveq(wik)   ee   q[ln  p(x,w,v)](33)   wa+pnj=1xij  ij(k)   1ike   (b+pnj=1eq[vkj])wiktherefore,q(wik)=gamma a+nxj=1xij  ij(k),b+nxj=1eq[vkj]!(34)   findingq(vkj):bysymmetrywecanquickly   ndthatq(vkj)=gamma c+mxi=1xij  ij(k),b+mxi=1eq[wik]!(35)   optimizing  ij(k):afterupdatingeachq(wik)andq(vkj),set  ij(k)=eeq[lnwik]+eq[lnvkj]pk   =1eeq[lnwi   ]+eq[lnv   j](36)   toassessconvergence,wethenevaluatethelowerboundofthevariationalobjectivefunction,sincethisiswhatwearetryingtomaximize.wehopethattheqdistributionsthenare   aboutasgoodas   whatwewouldhavegottenoptimizingldirectly,sincethepeaksandvalleysoftheobjectivefunctionlshouldoverlapsigni   cantlywiththoseofthelowerboundweuse.comparisonwithem   withem,updating  ij(k)didn   tinvolveexpectationssincewehaveapointestimateofwandv.noticethatifweremovetheseexpectationsabove,theupdateto  ij(k)isidenticaltoem.   usingthedistributionsq(wik)andq(vkj),compareeq[wik]andeq[vkj]withtheupdatesofem.wecanseeaclosesimilaritybetweenthetwo,onlynowwehaveafullid203distributionontheseterms.thereforethismodelcanbeconsideredasthebayesianapproachtonmfwithadivergencepenalty,ratherthanjustbeingmotivatedbyit.   thisindicatesthatourlowerboundisdoingsomethingsimilar(orperhapsequivalent)tointro-ducinglatentrandomvariablesy(k)ijtothemodel.9