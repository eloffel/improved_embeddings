from	deep	learning	of	disentangled	

representa4ons	to	higher-level	

cogni4on	

yoshua	bengio,	october	24th,	2017	

university	of	amsterdam	

	

s@ll	far	from	human-level	ai	

       industrial	successes	mostly	based	on	
supervised	learning	

       learning	super   cial	clues,	not	generalizing	
well	enough	outside	of	training	contexts,	easy	
to	fool	trained	networks:		
      current	models	cheat	by	picking	on	surface	
regulari@es	

the	need	for	predic@ve	
causal	modeling:	rare	&	

dangerous	states	

       example:	autonomous	vehicles	in	

near-accident	situa@ons	

       current	supervised	learning	may	

not	handle	well	these	cases	
because	they	are	too	rare	(not	
enough	data)	

      
      

it	would	be	even	worse	with	current	rl	(sta@s@cal	ine   ciency)	
long-term	objec@ve:	develop	bewer	predic@ve	models	of	the	world	able	to	
generalize	in	completely	unseen	scenarios,	but	it	does	not	seem	reasonable	
to	model	the	sequence	of	future	states	in	all	their	details	

       human	drivers:	no	need	to	die	a	thousand	deaths	

deep	genera@ve	models	&	gans	
       simula@on	
       filling-in	missing	data	
       genera@ng	text	from	speech,	speech	from	text,	
       inpain@ng,	denoising,	super-resolu4on	
       art,	etc.	

image	2	image	

ledig	et	al.	2017	

4	

deep	unsupervised	learning	takes	o   	with	

gans	(nips   2014)		

       progress	in	unsupervised	genera@ve	neural	
nets	allows	them	to	synthesize	a	diversity	
images,	sounds	and	text	imita@ng	
unlabeled	images,	sounds	or	text	

random	
vector	

generator	
network	

fake	
image	

discriminator	

network	

random	
index	

training	

set	

real	
image	

5	

text	2	image,	coloriza@on	

zhang	et	al.	2017	

lucy	li	

6	

introduction

image	2	image	

image     image

isola	et	al.	2016	

gans

7	

what   s missing? 

humans	outperform	machines	at	

autonomous	learning	

       humans	are	very	good	at	
unsupervised	learning,	e.g.	
a	2	year	old	knows	intui@ve	
physics	

       babies	construct	an	

approximate	but	su   ciently	
reliable	model	of	physics,	
how	do	they	manage	that?	
note	that	they	interact	
with	the	world,	not	just	
observe	it.	

what   s missing? 

learning	mul@ple	levels	of	abstrac@on	
       the	big	payo   	of	deep	learning	is	to	allow	learning	

higher	levels	of	abstrac@on	

       higher-level	abstrac@ons	disentangle	the	

factors	of	varia4on,	which	allows	much	easier	
generaliza@on	and	transfer	

11	

how	to	discover	good	representa@ons	
       how	to	discover	abstrac@ons?		
       what	is	a	good	representa@on?	
       need	clues	to	disentangle	the	underlying	

factors	
      spa@al	&	temporal	scales	
      marginal	independence	
      controllable	factors	

12	

ac@ng	to	guide		

representa@on	learning	

&	disentangling	

       some	factors	(e.g.	objects)	correspond	to	

   independently	controllable   	aspects	of	the	world	

       can	only	be	discovered	by	ac>ng	in	the	world	

independently controllable features

2.2 policy selectivity

discrete case objective function

which make such policies possible.

speci   cally, we will look for policies which can separately in   uence one of the dimensions of h, and we will prefer representations
which make such policies possible.

independently controllable factors 
(emmanuel bengio, valentin thomas, joelle pineau, doina 
precup, yoshua bengio, 2017) 
(valentin thomas, jules pondard, emmanuel bengio, marc 
sarfati, philippe beaudoin, marie-jean meurs, joelle pineau, 
doina precup, yoshua bengio, 2017) 
       jointly	train	for	each	aspect	(factor)	

it is common to assume that n     d. this causes f and g to perform id84 of x, i.e. compression, since there
procedure to uncover principal factors of variation of the data on which they are trained. however, this does not necessarily imply that
is a dimension bottleneck through which information about the input data must pass. often, this bottleneck forces the optimization
the different dimensions of h = f (x) are individually meaningful. in fact, note that for any bijective function r, we could obtain the
procedure to uncover principal factors of variation of the data on which they are trained. however, this does not necessarily imply that
same reconstruction error by replacing f by r   f and g by r 1   g, so we should not expect any form of disentangling of the factors
the different dimensions of h = f(x) are individually meaningful. in fact, note that for any bijective function r, we could obtain the
of variation unless some additional constraints or penalties are imposed on h. this motivates the approach we are about to present.
same reconstruction error by replacing f by r   f and g by r 1   g, so we should not expect any form of disentangling of the factors
speci   cally, we will look for policies which can separately in   uence one of the dimensions of h, and we will prefer representations
in order to quantify the change in fk when actions are taken according to    k, we de   ne the selectivity of a feature as:
of variation unless some additional constraints or penalties are imposed on h. this motivates the approach we are about to present.
which make such policies possible.
consider the following simple scenario: we train an autoencoder f, g producing n latent features, fk, k = 1, . . . n. in tandem with
speci   cally, we will look for policies which can separately in   uence one of the dimensions of h, and we will prefer representations
these features we train n policies, denoted    k. autoencoders can learn relatively arbitrary feature representations, but we would like
proposition
sel(s, a, k) = es0   p a
which make such policies possible.
       a	policy									(which	tries	to	selec@vely	change	just	that	factor)	
these features to correspond to controllable factors in the learner   s environment. speci   cally, we would like policy    k to cause a
   latent properties    can be controlled independently from other
2.2 policy selectivity
       a	representa@on	(which	maps	state	to	value	of	factor)	
   things    in the environment
change only in fk and not in any other features. we think of fk and    k as a feature-policy pair.
where s,s0 are successive raw state representations (e.g. pixels), a is the action, p a
ss0 is the environment   s transition distribution from
s to s0 under action a. the id172 by the change in all features means that the selectivity of fk is maximal when only that
discrete case,   2 {1, .., n}, de   ne selectivity:
2.2 policy selectivity
consider the following simple scenario: we train an autoencoder f, g producing n latent features, fk, k = 1, . . . n. in tandem with
single feature changes as a result of some action.
these features we train n policies, denoted    k. autoencoders can learn relatively arbitrary feature representations, but we would like
by having an objective that maximizes selectivity and minimizes the autoencoder objective, we can ensure that the features learned
consider the following simple scenario: we train an autoencoder f, g producing n latent features, fk, k = 1, . . . n. in tandem with
these features to correspond to controllable factors in the learner   s environment. speci   cally, we would like policy    k to cause a
can both reconstruct the data and recover independently controllable factors. hence, we de   ne the following objective, which can be
these features we train n policies, denoted    k. autoencoders can learn relatively arbitrary feature representations, but we would like
change only in fk and not in any other features. we think of fk and    k as a feature-policy pair.
minimized via stochastic id119:
these features to correspond to controllable factors in the learner   s environment. speci   cally, we would like policy    k to cause a
change only in fk and not in any other features. we think of fk and    k as a feature-policy pair.

consider the following simple scenario: we train an autoencoder f, g producing n latent features, fk, k = 1, . . . n. in tandem with
these features we train n policies, denoted    k. autoencoders can learn relatively arbitrary feature representations, but we would like
these features to correspond to controllable factors in the learner   s environment. speci   cally, we would like policy    k to cause a
change only in fk and not in any other features. we think of fk and    k as a feature-policy pair.

pk0 |fk0(s0)   fk0(s)| 
pk0 |fk0 (st+1)   fk0 (st )|35
e(st ,at ,st+1)24   k (at|st )
es[xa
   xk
{z
|

nxk=1
2||s   g(f (s))||2
2]
}

       op@mize	both	policy								and	representa@on									to	minimize	

here one can think of log sel(s, a, k) as the reward signal rk(s, a) of a control problem, and the expected reward ea      k [rk] is
maximized by    nding the optimal set of policies    k.

fk (st+1)   fk (st )
1

note: this is an objective, not a constraint (e.g. whitney et al., 2016)

   k(a|s) log sel(s, a, k)]

|fk(s0)   fk(s)|

disentanglement objective
independently controllable features

ss0   

e. bengio, j. pondard, m. sarfati, v. thomas et al.

reconstruction error

{z

es[ 1

|

}

1

1

1

14	

	

predict the effect of actions in 
attribute space 

independently controllable features

given	ini@al	state	and	set	of	ac@ons,	predict	new	awribute	values	
predict e   ect of a cause
and	the	corresponding	reconstructed	images	

encoder

(0.4, 13.1)

h|{z}

15	

decoder

  h0|{z}

( 4.6,  1.9)

= h + dhright

(5,  5)

| {z }

+ 2    dhdown
( 10,  10)

{z

|

}

given two states, recover the causal 
actions leading from one to the other 

recover the cause

independently controllable features

encoder

(0.4, 13.1)

h1|{z}

encoder

h2|{z}

(5.9,  11.6)

16	

dh = (5.5,  24.8)     2    dhdown + 3    dhright

e. bengio, j. pondard, m. sarfati, v. thomas et al.

independently controllable features

independently controllable features

from discrete to continuous

continuous set of attributes: attribute 
embeddings = variable name 

principle

we map controllable factors to embeddings   instead of
coordinates k (one policy network). discovers by itself the
relevant number of features.
  = g (h, z) 2 rn is now generated from h = f (s), z     n (0, 1):

e(st ,at ,st+1) e 

    (at|st)

26664

a f (st+1)   f (st),   

 0=g (ht ,z0)   |a f (st+1)   f (st),  0 | 

e

37775

how much the value of property   changed relatively to other
properties.
17	

e. bengio, j. pondard, m. sarfati, v. thomas et al.

independently controllable features

9/17

continuous attributes model: 
computational graph 

independently controllable features

computation graph
total	objec@ve	=	autoencoder	loss	+	selec@vity	loss	

zt     u

 

g

f

  st

ht

st

lae

env

a(ht+1   ht,  )

lsel

ht+1

st+1

    (  |st)

at

env

o   -policy a2c/pg method on a batch of  

18	

what   s wrong with 
our unsupervised 
training objectives? 

abstraction challenge for unsupervised 
learning 
       why	is	modeling	p(acous@cs)	so	much	worse	than	modeling	

p(acous@cs	|	phonemes)	p(phonemes)?	

       why	are	our	current	models	not	able	to	   gure	out	phonemes	

and	model	their	distribu@on	separately?	

       may	have	to	do	with	the	di   erent	@me	scales	and	objec@ve	

func@on	at	the	wrong	level	of	abstrac@on:		
       log-likelihood	focuses	most	of	its	value	on	the	vast	majority	of	bits	

characterizing	the	acous4c	details	(instead	of	the	higher-level	linguis4c	
structure)	

       it	would	be	good	to	just	predict	the	future	in	in	abstract	space	rather	

than	in	the	pixel	space	

the consciousness prior 
bengio 2017, arxiv:1709.08568 
       conscious	thoughts	are	very	low-dimensional	objects	compared	

to	the	full	state	of	the	(unconscious)	brain	

       yet	they	have	unexpected	predic@ve	value	or	usefulness	

  	strong	constraint	or	prior	on	the	underlying	representa@on	

       thought:	composi@on	of	few	selected	factors	/	concepts	

at	the	highest	level	of	abstrac@on	of	our	brain	

       richer	than	but	closely	associated	with	short	verbal	

expression	such	as	a	sentence	or	phrase,	a	rule	or	fact	
(link	to	classical	symbolic	ai	&	knowledge	representa@on)	

21	

how to select a few 
relevant abstract 
concepts in a 

thought? 

on the relation between abstraction 
and attention 
       awen@on	allows	to	focus	on	a	few	elements	out	of	a	large	set	
       sok-awen@on	allows	this	process	to	be	trainable	with	gradient-

based	op@miza@on	and	backprop	

awen@on	focuses	on	a	few	
appropriate	abstract	or	concrete	
elements	of	mental	
representa@on		

23	

the attention revolution in deep learning 
       aken4on	mechanisms	exploit	gating	units,	have	unlocked	a	

breakthrough	in	machine	transla@on:		

	

	neural	machine	transla@on	

(iclr   2015)	

sokmax	over	lower		
loca@ons	condi@oned	
on	context	at	lower	and	
higher	loca@ons		

	
	
	
	
	
	
	
	

       now	in	google	translate:		
id165	
transla@on	

24	

higher-level	

lower-level	

current	
neural	net	
transla@on	

human	
transla@on	

human	
evalua@on	

the consciousness prior 
bengio 2017, arxiv:1709.08568 
       2	levels	of	representa@on:		

       high-dimensional	abstract	representa@on	space	(all	known	
concepts	and	factors)	h	
       low-dimensional	conscious	thought	c,	extracted	from	h	

       example:	c	is	a	predic@on	about	some	future	event,	

involves	current	variables	and	their	values,	and	a	
predic@on	about	a	future	variable	

       predictor	needs	to	refer	to	a	predicted	variable	by	name	

(e.g.	embedding)	so	as	to	be	able	to	separate	the	name	
from	the	value	and	recover	the	predic@on	when	a	future	
event	makes	the	variable	observed	(at	a	di   erent	value).	

25	

the consciousness prior 
bengio 2017, arxiv:1709.08568 
       conscious	predic@on	over	awended	variables	a	(sok	awen@on)	

awen@on	weights	

predicted	value	

earlier	conscious	
state	

	
       how	to	train	the	awen@on	mechanism	which	
				selects	which	variables	to	predict?	
      
					connec@on	to	classical	symbolic	ai	

(predicted	variables,	condi@oning	variables)	=	rule	

26	

representing agents, objects, policies 

the	controllable	factors	idea	works	on	a	small	scale:	ongoing	work	
to	expand	this	to	more	complex	environments	
	
       expand	to	sequences	of	ac@ons	and	use	them	to	de   ne	op@ons	
       no@on	of	objects	and	awributes	naturally	falls	out	
       extension	to	non-sta@c	set	of	objects:	types	&	instances	
       objects	are	groups	of	controllable	features:	by	whom?	agents	

       factors	controlled	by	other	agents;	mirror	neurons	

       because	the	set	of	objects	may	be	unbounded,	we	need	to	learn	
to	represent	policies	themselves,	and	the	de   ni@on	of	an	object	
is	bound	to	the	policies	associated	with	it	(for	using	it	and	
changing	its	awributes)	

27	

research program towards higher-
level deep learning 
       build	a	series	of	gradually	more	complex	environments	

       design	training	frameworks	for	learning	agents	to	discover	the	
no@on	of	objects,	awributes,	types,	other	agents,	rela@ons,	the	
combinatorial	space	of	policies,	etc(cid:1)	

       associate	all	these	concepts	to	linguis@c	construc@ons:	learn	

seman@cally	grounded	linguis@c	expressions.	
       words	are	clues	of	higher-level	abstrac@ons	
       language	is	an	input/output	modality	

28	

ongoing research:  
dl for ai 
neural nets    cognition 
       learn	more	abstract	representa@ons	which	capture	causality	
      
independently	controllable	factors:	some	abstract	factors	are	
controllable	aspects	of	the	environment,	disentangled	

       jointly	learn	condi4onal	exploratory	policies	with	intrinsic	

rewards	+	associated	factors	

       naturally	gives	rise	to	the	no@on	of		

	objects,	akributes	&	agents	

       natural	language	&	consciousness	prior:	other	
					clue	about	abstract	representa@ons	
       unsupervised	rl	research,	performed	in	simulated	environments	

29	

the future of deep ai 

       scien@   c	progress	is	slow	and	con@nuous,	but	social	and	

economic	impact	can	be	disrup@ve	

      

       many	fundamental	research	ques@ons	are	in	front	of	us,	with	
much	uncertainty	about	when	we	will	crack	them,	but	we	will	
importance	of	con@nued	investment	in	basic	&	exploratory	ai	
research,	for	both	prac@cal	(recruitment)	short-term	and	long-
term	reasons		

       let	us	con@nue	to	keep	the	   eld	open	and	   uid,	be	mindful	of	

social	impacts,	and	make	sure	ai	will	bloom	for	the	bene   t	of	all	

30	

montreal	ins4tute	for	
learning	algorithms	

