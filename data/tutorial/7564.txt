3

basics of bayesian statistics

suppose a woman believes she may be pregnant after a single sexual encounter,
but she is unsure. so, she takes a pregnancy test that is known to be 90%
accurate   meaning it gives positive results to positive cases 90% of the time   
and the test produces a positive result.1 ultimately, she would like to know the
id203 she is pregnant, given a positive test (p(preg| test +)); however,
what she knows is the id203 of obtaining a positive test result if she is
pregnant (p(test +| preg)), and she knows the result of the test.
in a similar type of problem, suppose a 30-year-old man has a positive
blood test for a prostate cancer marker (psa). assume this test is also ap-
proximately 90% accurate. once again, in this situation, the individual would
like to know the id203 that he has prostate cancer, given the positive
test, but the information at hand is simply the id203 of testing positive
if he has prostate cancer, coupled with the knowledge that he tested positive.
bayes    theorem o   ers a way to reverse conditional probabilities and,
hence, provides a way to answer these questions. in this chapter, i    rst show
how bayes    theorem can be applied to answer these questions, but then i
expand the discussion to show how the theorem can be applied to id203
distributions to answer the type of questions that social scientists commonly
ask. for that, i return to the polling data described in the previous chapter.

3.1 bayes    theorem for point probabilities

bayes    original theorem applied to point probabilities. the basic theorem
states simply:

p(b|a) =

p(a|b)p(b)

p(a)

.

(3.1)

1 in fact, most pregnancy tests today have a higher accuracy rate, but the accuracy

rate depends on the proper use of the test as well as other factors.

48

3 basics of bayesian statistics

in english, the theorem says that a id155 for event b
given event a is equal to the id155 of event a given event
b, multiplied by the marginal id203 for event b and divided by the
marginal id203 for event a.

proof: from the id203 rules introduced in chapter 2, we know that
p(a, b) = p(a|b)p(b). similarly, we can state that p(b, a) = p(b|a)p(a).
obviously, p(a, b) = p(b, a), so we can set the right sides of each of these
equations equal to each other to obtain:

p(b|a)p(a) = p(a|b)p(b).

dividing both sides by p(a) leaves us with equation 3.1.

the left side of equation 3.1 is the id155 in which we
are interested, whereas the right side consists of three components. p(a|b)
is the id155 we are interested in reversing. p(b) is the un-
conditional (marginal) id203 of the event of interest. finally, p(a) is the
marginal id203 of event a. this quantity is computed as the sum of
the id155 of a under all possible events bi in the sample
space: either the woman is pregnant or she is not. stated mathematically for
a discrete sample space:

p(a) = xbi   sb

p(a | bi)p(bi).

returning to the pregnancy example to make the theorem more concrete,
suppose that, in addition to the 90% accuracy rate, we also know that the
test gives false-positive results 50% of the time. in other words, in cases in
which a woman is not pregnant, she will test positive 50% of the time. thus,
there are two possible events bi: b1 = preg and b2 = not preg. additionally,
given the accuracy and false-positive rates, we know the conditional probabil-
ities of obtaining a positive test under these events: p(test +|preg) = .9 and
p(test +|not preg) = .5. with this information, combined with some    prior   
information concerning the id203 of becoming pregnant from a single
sexual encounter, bayes    theorem provides a prescription for determining the
id203 of interest.

the    prior    information we need, p(b)     p(preg), is the marginal probabil-
ity of being pregnant, not knowing anything beyond the fact that the woman
has had a single sexual encounter. this information is considered prior infor-
mation, because it is relevant information that exists prior to the test. we may
know from previous research that, without any additional information (e.g.,
concerning date of last menstrual cycle), the id203 of conception for any
single sexual encounter is approximately 15%. (in a similar fashion, concerning
the prostate cancer scenario, we may know that the prostate cancer incidence
rate for 30-year-olds is .00001   see exercises). with this information, we can
determine p(b | a)     p(preg|test +) as:

3.1 bayes    theorem for point probabilities

49

p(preg | test +) =

p(test + | preg)p(preg)

p(test + | preg)p(preg) + p(test + | not preg)p(not preg)

.

filling in the known information yields:

p(preg | test +) =

(.90)(.15)

(.90)(.15) + (.50)(.85)

=

.135

.135 + .425

= .241.

thus, the id203 the woman is pregnant, given the positive test, is only
.241. using bayesian terminology, this id203 is called a    posterior prob-
ability,    because it is the estimated id203 of being pregnant obtained
after observing the data (the positive test). the posterior id203 is quite
small, which is surprising, given a test with so-called 90%    accuracy.    how-
ever, a few things a   ect this id203. first is the relatively low id203
of becoming pregnant from a single sexual encounter (.15). second is the ex-
tremely high id203 of a false-positive test (.50), especially given the high
id203 of not becoming pregnant from a single sexual encounter (p = .85)
(see exercises).

if the woman is aware of the test   s limitations, she may choose to repeat the
test. now, she can use the    updated    id203 of being pregnant (p = .241)
as the new p(b); that is, the prior id203 for being pregnant has now been
updated to re   ect the results of the    rst test. if she repeats the test and again
observes a positive result, her new    posterior id203    of being pregnant
is:

p(preg | test +) =

(.90)(.241)

(.90)(.241) + (.50)(.759)

=

.135

.135 + .425

= .364.

this result is still not very convincing evidence that she is pregnant, but if she
repeats the test again and    nds a positive result, her id203 increases to
.507 (for general interest, subsequent positive tests yield the following prob-
abilities: test 4 = .649, test 5 = .769, test 6 = .857, test 7 = .915, test 8 =
.951, test 9 = .972, test 10 = .984).

this process of repeating the test and recomputing the id203 of in-
terest is the basic process of concern in bayesian statistics. from a bayesian
perspective, we begin with some prior id203 for some event, and we up-
date this prior id203 with new information to obtain a posterior prob-
ability. the posterior id203 can then be used as a prior id203 in
a subsequent analysis. from a bayesian point of view, this is an appropriate
strategy for conducting scienti   c research: we continue to gather data to eval-
uate a particular scienti   c hypothesis; we do not begin anew (ignorant) each
time we attempt to answer a hypothesis, because previous research provides
us with a priori information concerning the merit of the hypothesis.

50

3 basics of bayesian statistics

3.2 bayes    theorem applied to id203 distributions

bayes    theorem, and indeed, its repeated application in cases such as the ex-
ample above, is beyond mathematical dispute. however, bayesian statistics
typically involves using id203 distributions rather than point probabili-
ties for the quantities in the theorem. in the pregnancy example, we assumed
the prior id203 for pregnancy was a known quantity of exactly .15. how-
ever, it is unreasonable to believe that this id203 of .15 is in fact this
precise. a cursory glance at various websites, for example, reveals a wide range
for this id203, depending on a woman   s age, the date of her last men-
strual cycle, her use of contraception, etc. perhaps even more importantly,
even if these factors were not relevant in determining the prior id203
for being pregnant, our knowledge of this prior id203 is not likely to be
perfect because it is simply derived from previous samples and is not a known
and    xed population quantity (which is precisely why di   erent sources may
give di   erent estimates of this prior id203!). from a bayesian perspec-
tive, then, we may replace this value of .15 with a distribution for the prior
pregnancy id203 that captures our prior uncertainty about its true value.
the inclusion of a prior id203 distribution ultimately produces a poste-
rior id203 that is also no longer a single quantity; instead, the posterior
becomes a id203 distribution as well. this distribution combines the
information from the positive test with the prior id203 distribution to
provide an updated distribution concerning our knowledge of the id203
the woman is pregnant.

put generally, the goal of bayesian statistics is to represent prior uncer-
tainty about model parameters with a id203 distribution and to update
this prior uncertainty with current data to produce a posterior id203 dis-
tribution for the parameter that contains less uncertainty. this perspective
implies a subjective view of id203   id203 represents uncertainty   
and it contrasts with the classical perspective. from the bayesian perspective,
any quantity for which the true value is uncertain, including model param-
eters, can be represented with id203 distributions. from the classical
perspective, however, it is unacceptable to place id203 distributions on
parameters, because parameters are assumed to be    xed quantities: only the
data are random, and thus, id203 distributions can only be used to rep-
resent the data.

bayes    theorem, expressed in terms of id203 distributions, appears

as:

f (  |data) =

f (data|  )f (  )

f (data)

,

(3.2)

where f (  |data) is the posterior distribution for the parameter   , f (data|  )
is the sampling density for the data   which is proportional to the likeli-
hood function, only di   ering by a constant that makes it a proper density
function   f (  ) is the prior distribution for the parameter, and f (data) is the

3.2 bayes    theorem applied to id203 distributions

51

marginal id203 of the data. for a continuous sample space, this marginal
id203 is computed as:

f (data) =z f (data|  )f (  )d  ,

the integral of the sampling density multiplied by the prior over the sample
space for   . this quantity is sometimes called the    marginal likelihood    for the
data and acts as a normalizing constant to make the posterior density proper
(but see raftery 1995 for an important use of this marginal likelihood). be-
cause this denominator simply scales the posterior density to make it a proper
density, and because the sampling density is proportional to the likelihood
function, bayes    theorem for id203 distributions is often stated as:

posterior     likelihood    prior,

(3.3)

where the symbol           means    is proportional to.   

3.2.1 proportionality

as equation 3.3 shows, the posterior density is proportional to the likelihood
function for the data (given the model parameters) multiplied by the prior for
the parameters. the prior distribution is often   but not always   normalized
so that it is a true density function for the parameter. the likelihood function,
however, as we saw in the previous chapter, is not itself a density; instead, it is
a product of densities and thus lacks a normalizing constant to make it a true
density function. consider, for example, the bernoulli versus binomial speci-
   cations of the likelihood function for the dichotomous voting data. first, the
bernoulli speci   cation lacked the combinatorial expression to make the like-
lihood function a true density function for either the data or the parameter.
second, although the binomial representation for the likelihood function con-
stituted a true density function, it only constituted a true density for the data
and not for the parameter p. thus, when the prior distribution for a parameter
is multiplied by the likelihood function, the result is also not a proper density
function. indeed, equation 3.3 will be    o       by the denominator on the right
side of equation 3.2, in addition to whatever normalizing constant is needed
to equalize the likelihood function and the sampling density p(data |   ).
fortunately, the fact that the posterior density is only proportional to the
product of the likelihood function and prior is not generally a problem in
bayesian analysis, as the remainder of the book will demonstrate. however,
a note is in order regarding what proportionality actually means. in brief, if
a is proportional to b, then a and b only di   er by a multiplicative constant.
how does this translate to id203 distributions? first, we need to keep in
mind that, in a bayesian analysis, model parameters are considered random
quantities, whereas the data, having been already observed, are considered
   xed quantities. this view is completely opposite that assumed under the

52

3 basics of bayesian statistics

classical approach. second, we need to recall from chapter 2 that potential
density functions often need to have a normalizing constant included to make
them proper density functions, but we also need to recall that this normalzing
constant only has the e   ect of scaling the density   it does not fundamentally
change the relative frequencies of di   erent values of the random variable.
as we saw in chapter 2, the normalizing constant is sometimes simply a
true constant   a number   but sometimes the constant involves the random
variable(s) themselves.

as a general rule, when considering a univariate density, any term, say
q (no matter how complicated), that can be factored away from the random
variable in the density   so that all the term(s) involving the random variable
are simply multiples of q   can be considered an irrelevant proportionality
constant and can be eliminated from the density without a   ecting the results.
in theory, this rule is fairly straightforward, but it is often di   cult to apply
for two key reasons. first, it is sometimes di   cult to see whether a term can
be factored out. for example, consider the following function for   :

f (  ) = e     +q.

it may not be immediately clear that q here is an arbitrary constant with
respect to   , but it is. this function can be rewritten as:

f (  ) = e         eq,

using the algebraic rule that ea+b = eaeb. thus, if we are considering f (  )
as a density function for   , eq would be an arbitrary constant and could be
removed without a   ecting id136 about   . thus, we could state without
loss of information that:

f (  )     e     .

in fact, this particular function, without q, is an exponential density for   
with parameter    = 1 (see the end of this chapter). with q, it is proportional
to an exponential density; it simply needs a normalizing constant of e   q so
that the function integrates to 1 over the sample space s = {   :    > 0}:

z    

0

e     +q d   =    

1

e      q + eq = eq.

thus, given that this function integrates to eq, e   q renormalizes the integral
to 1.

a second di   culty with this rule is that multivariate densities sometimes
make it di   cult to determine what is an irrelevant constant and what is not.
with id150, as we will discuss in the next chapter and throughout
the remainder of the book, we generally break down multivariate densities into
univariate conditional densities. when we do this, we can consider all terms
not involving the random variable to which the conditional density applies to

3.3 bayes    theorem with distributions: a voting example

53

be proportionality constants. i will show this shortly in the last example in
this chapter.

3.3 bayes    theorem with distributions: a voting
example

to make the notion of bayes    theorem applied to id203 distributions
concrete, consider the polling data from the previous chapter. in the previous
chapter, we attempted to determine whether john f. kerry would win the
popular vote in ohio, using the most recent id98/usatoday/gallup polling
data. when we have a sample of data, such as potential votes for and against a
candidate, and we assume they arise from a particular id203 distribution,
the construction of a likelihood function gives us the joint id203 of the
events, conditional on the parameter of interest: p(data|parameter). in the
election polling example, we maximized this likelihood function to obtain a
value for the parameter of interest   the proportion of kerry voters in ohio   
that maximized the id203 of obtaining the polling data we did. that
estimated proportion (let   s call it k to minimize confusion) was .521. we
then determined how uncertain we were about our    nding that k = .521.
to be more precise, we determined under some assumptions how far k may
reasonably be from .521 and still produce the polling data we observed.

this process of maximizing the likelihood function ultimately simply tells
us how probable the data are under di   erent values for k   indeed, that is
precisely what a likelihood function is    but our ultimate question is really
whether kerry will win, given the polling data. thus, our question of interest
is    what is p(k > .5),    but the likelihood function gives us p(poll data| k)   
that is, the id203 of the data given di   erent values of k.
in order to answer the question of interest, we need to apply bayes    the-
orem in order to obtain a posterior distribution for k and then evaluate
p(k > .5) using this distribution. bayes    theorem says:

f (k|poll data)     f (poll data|k)f (k),

or verbally: the posterior distribution for k, given the sample data, is propor-
tional to the id203 of the sample data, given k, multiplied by the prior
id203 for k. f (poll data|k) is the likelihood function (or sampling den-
sity for the data). as we discussed in the previous chapter, it can be viewed
as a binomial distribution with x = 556    successes    (votes for kerry) and
n     x = 511    failures    (votes for bush), with n = 1, 067 total votes between
the two candidates. thus,

f (poll data|k)     k 556(1     k)511.

what remains to be speci   ed to complete the bayesian development of the
model is a prior id203 distribution for k. the important question is:
how do we do construct a prior?

54

3 basics of bayesian statistics

3.3.1 speci   cation of a prior: the beta distribution

speci   cation of an appropriate prior distribution for a parameter is the most
substantial aspect of a bayesian analysis that di   erentiates it from a classi-
cal analysis. in the pregnancy example, the prior id203 for pregnancy
was said to be a point estimate of .15. however, as we discussed earlier, that
speci   cation did not consider that that prior id203 is not known with
complete certainty. thus, if we wanted to be more realistic in our estimate of
the posterior id203 of pregnancy, we could compute the posterior prob-
ability under di   erent values for the prior id203 to obtain a collection
of possible posterior probabilities that we could then consider and compare
to determine which estimated posterior id203 we thought was more rea-
sonable. more e   ciently, we could replace the point estimate of .15 with a
id203 distribution that represented (1) the plausible values of the prior
id203 of pregnancy and (2) their relative merit. for example, we may
give considerable prior weight to the value .15 with diminishing weight to
values of the prior id203 that are far from .15.

similarly, in the polling data example, we can use a distribution to repre-
sent our prior knowledge and uncertainty regarding k. an appropriate prior
distribution for an unknown proportion such as k is a beta distribution. the
pdf of the beta distribution is:

f (k |   ,   ) =

   (   +   )
   (  )   (  )

k      1(1     k)     1,

where    (a) is the gamma function applied to a and 0 < k < 1.2 the param-
eters    and    can be thought of as prior    successes    and    failures,    respec-
tively. the mean and variance of a beta distribution are determined by these
parameters:

e(k |   ,   ) =

  

   +   

    

and

var(k |   ,   ) =

(   +   )2(   +    + 1)

.

this distribution looks similar to the binomial distribution we have already
discussed. the key di   erence is that, whereas the random variable is x and the
key parameter is k in the binomial distribution, the random variable is k and
the parameters are    and    in the beta distribution. keep in mind, however,
from a bayesian perspective, all unknown quantities can be considered random
variables.

2 the gamma function is the generalization of the factorial to nonintegers. for
0 xa   1 e   x dx. most soft-
integers,    (a) = (a     1)!. for nonintegers,    (a) = r    
ware packages will compute this function, but it is often unnecessary in practice,
because it tends to be part of the normalizing constant in most problems.

3.3 bayes    theorem with distributions: a voting example

55

how do we choose    and    for our prior distribution? the answer to this
question depends on at least two factors. first, how much information prior
to this poll do we have about the parameter k? second, how much stock
do we want to put into this prior information? these are questions that all
bayesian analyses must face, but contrary to the view that this is a limitation
of bayesian statistics, the incorporation of prior information can actually be
an advantage and provides us considerable    exibility. if we have little or no
prior information, or we want to put very little stock in the information we
have, we can choose values for    and    that reduce the distribution to a
uniform distribution. for example, if we let    = 1 and    = 1, we get

f (p|   = 1,    = 1)     k 1   1=0(1     k)1   1=0 = 1,

which is proportional to a uniform distribution on the allowable interval for
k ([0,1]). that is, the prior distribution is    at, not producing greater a priori
weight for any value of k over another. thus, the prior distribution will have
little e   ect on the posterior distribution. for this reason, this type of prior is
called    noninformative.   3

at the opposite extreme, if we have considerable prior information and we
want it to weigh heavily relative to the current data, we can use large values of
   and   . a little algebraic manipulation of the formula for the variance reveals
that, as    and    increase, the variance decreases, which makes sense, because
adding additional prior information ought to reduce our uncertainty about the
parameter. thus, adding more prior successes and failures (increasing both
parameters) reduces prior uncertainty about the parameter of interest (k).
finally, if we have considerable prior information but we do not wish for it to
weigh heavily in the posterior distribution, we can choose moderate values of
the parameters that yield a mean that is consistent with the previous research
but that also produce a variance around that mean that is broad.

figure 3.1 displays some beta distributions with di   erent values of    and
   in order to clarify these ideas. all three displayed beta distributions have
a mean of .5, but they each have di   erent variances as a result of having   
and    parameters of di   erent magnitude. the most-peaked beta distribution
has parameters    =    = 50. the least-peaked distribution is actually    at   
uniform   with parameters    =    = 1. as with the binomial distribution, the
beta distribution becomes skewed if    and    are unequal, but the basic idea
is the same: the larger the parameters, the more prior information and the
narrower the density.

returning to the voting example, id98/usatoday/gallup had conducted
three previous polls, the results of which could be treated as prior information.

3 virtually all priors, despite sometimes being called    noninformative,    impart
some information to the posterior distribution. another way to say this is that
claiming ignorance is, in fact, providing some information! however,    at priors
generally have little weight in a   ecting posterior id136, and so they are called
noninformative. see box and tiao 1973; gelman et al. 1995; and lee 1989.

56

3 basics of bayesian statistics

y
c
n
e
u
q
e
r
f

0
1

8

6

4

2

0

beta(50,50)

beta(5,5)

beta(1,1)

0.0

0.2

0.4

0.6

0.8

1.0

k

fig. 3.1. three beta distributions with mean   /(   +   ) = .5.

these additional polling data are shown in table 3.1.4 if we consider these
previous polls to provide us prior knowledge about the election, then our prior
information consists of 1,008 (339 + 325 + 344) votes for bush and 942 votes
for kerry (346 + 312 + 284) out of a total of 1,950 votes.

this prior information can be included by using a beta distribution with

parameters    = 942 and    = 1008:

f (k |   ,   )     k 942   1(1     k)1008   1.

4 the data appear to show some trending, in the sense that the proportion stating
that they would vote for bush declined across time, whereas the proportion stating
that they would vote for kerry increased. this fact may suggest consideration
of a more complex model than discussed here. nonetheless, given a margin of
error of   4% for each of these additional polls, it is unclear whether the trend
is meaningful. in other words, we could simply consider these polls as repeated
samples from the same, unchanging population. indeed, the website shows the
results of 22 polls taken by various organizations, and no trending is apparent in
the proportions from late september on.

3.3 bayes    theorem with distributions: a voting example

57

table 3.1. id98/usatoday/gallup 2004 presidential election polls.

date
oct 17-20
sep 25-28
sep 4-7
total

n
706
664
661
2,031

% for bush

48%
49%
52%

    n
339
325
344
1,008

% for kerry

49%
47%
43%

    n
346
312
284
942

note: proportions and candidate-speci   c sample sizes may not add to 100% of total
sample n, because proportions opting for third-party candidates have been excluded.

after combining this prior with the binomial likelihood for the current sample,
we obtain the following posterior density for k:

p(k |   ,   , x)     k 556(1     k)511k 941(1     k)1007 = k 1497(1     k)1518.

this posterior density is also a beta density, with parameters    = 1498 and
   = 1519, and highlights the important concept of    conjugacy    in bayesian
statistics. when the prior and likelihood are of such a form that the poste-
rior distribution follows the same form as the prior, the prior and likelihood
are said to be conjugate. historically, conjugacy has been very important to
bayesians, because, prior to the development of the methods discussed in this
book, using conjugate priors/likelihoods with known forms ensured that the
posterior would be a known distribution that could be easily evaluated to
answer the scienti   c question of interest.

figure 3.2 shows the prior, likelihood, and posterior densities. the likeli-
hood function has been normalized as a proper density for k, rather than x.
the    gure shows that the posterior density is a compromise between the prior
distribution and the likelihood (current data). the prior is on the left side of
the    gure; the likelihood is on the right side; and the posterior is between,
but closer to the prior. the reason the posterior is closer to the prior is that
the prior contained more information than the likelihood: there were 1,950
previously sampled persons and only 1,067 in the current sample.5

with the posterior density determined, we now can summarize our up-
dated knowledge about k, the proportion of voters in ohio who will vote for
kerry, and answer our question of interest: what is the id203 that kerry
would win ohio? a number of summaries are possible, given that we have a
posterior distribution with a known form (a beta density). first, the mean
of k is 1498/(1498 + 1519) = .497, and the median is also .497 (found using
the qbeta function in r). the variance of this beta distribution is .00008283
(standard deviation=.0091). if we are willing to assume that this beta distri-
bution is approximately normal, then we could construct a 95% interval based
on a normal approximation and conclude that the proportion of ohio voters

5 this movement of the posterior distribution away from the prior and toward the

likelihood is sometimes called    bayesian shrinkage    (see gelman et al. 1995).

58

3 basics of bayesian statistics

posterior

prior

(normalized)

likelihood

)

k

(
f

0
6

0
5

0
4

0
3

0
2

0
1

0

0.40

0.45

0.50

k

0.55

0.60

fig. 3.2. prior, likelihood, and posterior for polling data example: the likelihood
function has been normalized as a density for the parameter k.

who would vote for kerry falls between .479 and .515 (.497  1.96  .0091). this
interval is called a    credible interval,    a    posterior id203 interval,    or a
   id203 interval,    and it has a simpler interpretation than the classical
con   dence interval. using this interval, we can say simply that the proportion
k falls in this interval with id203 .95.

if, on the other hand, we are not willing to assume that this posterior
density is approximately normal, we can directly compute a 95% id203
interval by selecting the lower and upper values of this beta density that
produce the desired interval. that is, we can determine the values of this beta
density below which 2.5% of the distribution falls and above which 2.5% of
the distribution falls. these values are .479 and .514, which are quite close to
those under the normal approximation.

these results suggest that, even with the prior information, the election
may have been too close to call, given that the interval estimate for k captures
.5. however, the substantive question   what is the id203 that kerry
would win   can also be answered within the bayesian framework. this prob-
ability is the id203 that kerry will get more than half of the votes, which

3.3 bayes    theorem with distributions: a voting example

59

is simply the id203 that k > .5. this id203 can be directly com-
puted from the beta distribution as the integral of this density from .5 to 1
(the mass of the curve to the right of .5; see figure 3.3). the result is .351,
which means that kerry did not have a favorable chance to win ohio, given
the complete polling data.

0
4

0
3

p(k<.5)=.649

p(k>.5)=.351

)

k

(
f

0
2

0
1

0

0.40

0.45

0.50

k

0.55

0.60

fig. 3.3. posterior for polling data example: a vertical line at k = .5 is included to
show the area needed to be computed to estimate the id203 that kerry would
win ohio.

in fact, kerry did not win ohio; he obtained 48.9% of the votes cast for
either kerry or bush. the classical analysis did not yield this conclusion: it
simply suggested that the results were too close to call. the bayesian anal-
ysis, on the other hand, while recognizing that the election would be close,
suggested that there was not a very high id203 that kerry would win.
the price that had to be paid for reaching this conclusion, however, was (1)
we had to be willing to specify a prior id203 for k, and (2) we had to
be willing to treat the parameter of interest as a random, and not a    xed,
quantity.

60

3 basics of bayesian statistics

3.3.2 an alternative model for the polling data: a gamma prior/
poisson likelihood approach

in this section, i repeat the analysis from the previous section. however, in-
stead of considering the problem as a binomial problem with the proportion
parameter p, i consider the problem as a poisson distribution problem with
rate parameter   . as we discussed in the previous chapter, the poisson dis-
tribution is a distribution for count variables; we can consider an individual   s
potential vote for kerry as a discrete count that takes values of either 0 or 1.
from that perspective, the likelihood function for the 1,067 sample members
in the most recent survey prior to the election is:

l(  |y ) =

  yie     

yi!

=

1067

yi=1

  p1067

i=1 yie   1067  

,

i=1 yi!

q1067

where yi is the 0 (bush) or 1 (kerry) vote of the ith individual.

as in the binomial example, we would probably like to include the previ-
ous survey data in our prior distribution. a conjugate prior for the poisson
distribution is a gamma distribution. the pdf of the gamma distribution is as
follows. if x     gamma(  ,   ), then:

f (x) =

    
   (  )

x     1e     x.

the parameters    and    in the gamma distribution are shape and inverse-
scale parameters, respectively. the mean of a gamma distribution is   /  , and
the variance is   /  2. figure 3.4 shows four di   erent gamma distributions. as
the plot shows, the distribution is very    exible: slight changes in the    and
   parameters   which can take any non-negative value   yield highly variable
shapes and scales for the density.

for the moment, we will leave    and    unspeci   ed in our voting model so
that we can see how they enter into the posterior distribution. if we combine
this gamma prior with the likelihood function, we obtain:

p(   | y )    (cid:18)     

   (  )(cid:19)        1e        

1

i=1 yi!!   p1067
q1067

i=1 yie   1067  .

this expression can be simpli   ed by combining like terms and excluding the
arbitrary proportionality constants (the terms in parentheses, which do not
include   ) to obtain:

p(   | y)       p1067

i=1 yi+     1e   (1067+  )  .

given that each yi is either a 0 (vote for bush) or 1 (vote for kerry),p1067

i=1 yi
is simply the count of votes for kerry in the current sample (=556). thus,
just as in the binomial example, the parameters    and      at least in this

3.3 bayes    theorem with distributions: a voting example

61

g(1,1)
g(10,1)
g(1,.1)
g(20,2

)
x
(
f

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0

5

10

x

15

20

fig. 3.4. some examples of the gamma distribution.

particular model   appear to capture prior    successes    and    failures.    specif-
ically,    is the count of prior    successes,    and    is the total number of prior
observations. the mean of the gamma distribution (  /  ) also supports this
conclusion. thus, as in the beta prior/binomial likelihood example, if we want
to incorporate the data from previous survey into the prior distribution, we
can set    = 942 and    = 942 + 1008 = 1950 to obtain the following posterior:

p(   | y )       556+942   1e   (1067+1950)   =   1497e   3017  .

thus, the posterior density is also a gamma density with parameters    =
1498 and    = 3017. because the gamma density is a known density, we can
immediately compute the posterior mean and standard deviation for   :      =
.497;        = .0128. if we wish to construct a 95% id203/credible interval
for   , and we are willing to make a normal approximation given the large
sample size, we can construct the interval as .497    1.96    .0128. this result
gives us an interval estimate of [.472, .522] for   . on the other hand, if we
wish to compute the interval directly using integration of the gamma density
(i.e., the cdf for the gamma distribution), we obtain an interval of [.472, .522].

62

3 basics of bayesian statistics

in this case, the normal-theory interval and the analytically derived interval
are the same when rounded to three decimal places.

how does this posterior id136 compare with that obtained using the

beta prior/binomial likelihood approach? the means for k in the beta/binomial
approach and for    in the gamma/poisson approach are identical. the inter-
vals are also quite comparable, but the interval in this latter approach is
wider   about 42% wider. if we wish to determine the id203 that kerry
would win ohio, we simply need to compute p(   > .5), which equals .390.
thus, under this model, kerry had a id203 of winning of .390, which is
still an unfavorable result, although it is a slightly greater id203 than
the beta/binomial result of .351.

which model is to be preferred? in this case, the substantive conclusion
we reached was comparable for the two models: kerry was unlikely to win
ohio. so, it does not matter which model we choose. the fact that the two
models produced comparable results is reassuring, because the conclusion does
not appear to be very sensitive to choice of model. ultimately, however, we
should probably place greater emphasis on the beta/binomial model, because
the poisson distribution is a distribution for counts, and our data, which
consisted of dichotomous outcomes, really does not    t the bill. consider the
parameter   : there is no guarantee with the gamma/poisson setup that    will
be less than 1. this lack of limit could certainly be problematic if we had less
data, or if the underlying proportion favoring kerry were closer to 1. in such
a case, the upper bound on the interval for    may have exceeded 1, and our
results would therefore be suspect. in this particular case, however, we had
enough data and prior information that ultimately made the interval width
very narrow, and so the bounding problem was not an issue. nonetheless, the
beta/binomial setup is a more natural model for the voting data.

3.4 a normal prior   normal likelihood example with   
known

2

the normal distribution is one of the most common distributions used in
statistics by social scientists, in part because many social phenomena in fact
follow a normal distribution. thus, it is not uncommon for a social scientist
to use a normal distribution as the basis for a likelihood function for a set of
data. here, i develop a normal distribution problem, but for the sake of keeping
this example general for use in later chapters, i used a contrived scenario and
keep the mathematics fairly general. the purpose at this point is simply to
illustrate a bayesian approach with a multivariate posterior distribution.6

6 the normal distribution involves two parameters: the mean (  ) and variance (  2).
when considered as a density for x, it is univariate, but when a normal likelihood
and some prior for the parameters are combined, the result is a joint posterior
distribution for    and   2, which makes the posterior a multivariate density.

3.4 a normal prior   normal likelihood example with   2 known

63

suppose that we have a class of 30 students who have recently taken a
midterm exam, and the mean grade was   x = 75 with a standard deviation of
   = 10. note that for now we have assumed that the variance is known, hence,
the use of    rather than s. we have taught the course repeatedly, semester
after semester, and past test means have given us an overall mean    of 70, but
the class means have varied from class to class, giving us a standard deviation
for the class means of    = 5. that is,    re   ects how much our class means have
varied and does not directly re   ect the variability of individual test scores.
we will discuss this more in depth momentarily.

our goal is ultimately to update our knowledge of   , the unobservable
population mean test score with the new test grade data. in other words, we
wish to    nd f (  |x). bayes    theorem tells us that:
f (  |x)     f (x|  )f (  ),

where f (x|  ) is the likelihood function for the current data, and f (  ) is the
prior for the test mean. (at the moment, i am omitting   2 from the notation).
if we assume the current test scores are normally distributed with a mean
equal to    and variance   2, then our likelihood function for x is:

f (x|  )     l(  |x) =

n

yi=1

1

   2    2

exp(cid:26)   

(xi       )2
2  2 (cid:27) .

furthermore, our previous test results have provided us with an overall mean
of 70, but we are uncertain about      s actual value, given that class means
vary semester by semester (giving us    = 5). so our prior distribution for   
is:

f (  ) =

1

   2     2

exp(cid:26)   

(       m )2
2   2 (cid:27) ,

where in this expression,    is the random variable, with m as the prior mean
(=70), and    2 (=25) re   ects the variation of    around m .

our posterior is the product of the likelihood and prior, which gives us:

f (  |x)    

1

      2  2

exp(cid:26)   (       m )2

2   2

+    pn

i=1(xi       )2
2  2

(cid:27) .

this posterior can be reexpressed as a normal distribution for   , but it takes
some algebra in order to see this. first, since the terms outside the exponential
are simply normalizing constants with respect to   , we can drop them and
work with the terms inside the exponential function. second, let   s expand
the quadratic components and the summations. for the sake of simplicty, i
temporarily drop the exponential function in this expression:

(   1/2)(cid:20)   2     2  m + m 2

   2

+ p x2     2n  x   + n  2

  2

(cid:21) .

64

3 basics of bayesian statistics

using this expression, any term that does not include    can be viewed as
a proportionality constant, can be factored out of the exponent, and can be
dropped (recall that ea+b = eaeb). after obtaining common denominators for
the remaining terms by cross-multiplying by each of the individual denomi-
nators and dropping proportionality constants, we are left with:

(   1/2)(cid:20)   2  2     2  2  m     2   2n  x   +    2n  2

  2   2

(cid:21) .

from here, we need to combine terms involving   2 and those involving   :

(   1/2)(cid:20) (n   2 +   2)  2     2(  2m +    2n  x)  

  2   2

(cid:21) .

dividing the numerator and denominator of this fraction by the (n   2 +   2)
in front of   2 yields:

finally, all we need to do is to complete the square in    and discard any
remaining constants to obtain:

  2     2   (  2m +n   2   x)

(n   2+  2)

  2   2

(n   2+  2)

(n   2+  2) (cid:17)2
(cid:16)       (  2m +n   2   x)

  2   2

(n   2+  2)

(   1/2)   
   

(   1/2)   
      

.

   
   

.

   
      

this result shows that our updated    is normally distributed with mean
(  2m +    2n  x)/(n   2 +   2) and variance (  2   2)/(n   2 +   2). notice how the
posterior mean is a weighted combination of the prior mean and the sample
mean. the prior mean is multiplied by the known variance of test scores in the
sample,   2, whereas the sample mean   x is multiplied by n and by the prior
variance    2. this shows    rst that the sample mean will tend to have more
weight than the prior mean (because of the n multiple), but also that the
prior and sample variances a   ect the weighting of the means. if the sample
variance is large, then the prior mean has considerable weight in the poste-
rior; if the prior variance is large, the sample mean has considerable weight in
the posterior. if the two quantities are equal (  2 =    2), then the calculation
reduces to (m + n  x)/(n + 1), which means that the prior mean will only have
a weight of 1/(n + 1) in the posterior.

in this particular example, our posterior mean would be:

(100    70) + (25    30    75)/(30    25 + 100) = 74.4.

thus, our result is clearly more heavily in   uenced by the sample data than
by the prior. one thing that must be kept in mind but is easily forgotten is
that our updated variance parameter (which is 20   the standard deviation is

3.4 a normal prior   normal likelihood example with   2 known

65

therefore 4.47) re   ects our uncertainty about   . this estimate is smaller than
both the prior variance and the sample variance, and it is much closer to    2
than to   2. why? again, this quantity re   ects how much    varies (or, put
another way, how much uncertainty we have in knowing m , the true value
of   ) and not how much we know about any particular sample. thus, the
fact that our sample standard deviation was 10 does not play a large role in
changing our minds about uncertainty in   , especially given that the sample
mean was not that di   erent from the prior mean. in other words, our sample
mean is su   ciently close to our prior mean    so that we are unconvinced that
the variance of    around m should be larger than it was. indeed, the data
convince us that our prior variance should actually be smaller, because the
current sample mean is well within the range around m implied by our prior
value for    .

3.4.1 extending the normal distribution example

the natural extension of the previous example in which the variance   2 was
considered known is to consider the more realistic case in which the variance is
not known. recall that, ultimately in the previous example, we were interested
in the quantity      the overall mean test score. previous data had given us an
estimate of   , but we were still uncertain about its value, and thus, we used
   to represent our uncertainty in   . we considered   2 to be a known quantity
(10). in reality, we typically do not know   2 any more than we know   , and
thus we have two quantities of interest that we should be updating with new
information. a full id203 model for    and   2 would look like:

f (  ,   2|x)     f (x|  ,   2)f (  ,   2).

this model is similar to the one in the example above, but we have now
explicitly noted that   2 is also an unknown quantity, by including it in the
prior distribution. therefore, we now need to specify a joint prior for both   
and   2, and not just a prior for   . if we assume    and   2 are independent   
and this is a reasonable assumption as we mentioned in the previous chapter;
there   s no reason the two parameters need be related   then we can consider
p(  ,   2) = p(  )p(  2) and establish separate priors for each.

in the example above, we established the prior for    to be        n (m,    2),
where m was the prior mean (70) and    2 was the measure of uncertainty
we had in   . we did not, however, specify a prior for   2, but we used   2 to
update our knowledge of    .7

how do we specify a prior distribution for    and   2 in a more general case?
unlike in the previous example, we often do not have prior information about
these parameters, and so we often wish to develop noninformative priors for

7 recall from the clt that   x     n (  ,   2/n); thus   2 and    2 are related:   2/n
should be an estimate for    2, and so treating   2 as    xed yields an updated    2
that depends heavily on the new sample data.

66

3 basics of bayesian statistics

them. there are several ways to do this in the normal distribution problem,
but two of the most common approaches lead to the same prior. one approach
is to assign a uniform prior over the real line for    and the same uniform prior
for log(  2). we assign a uniform prior on log(  2) because   2 is a nonega-
tive quantity, and the transformation to log(  2) stretches this new parameter
across the real line. if we transform the uniform prior on log(  2) into a density
for   2, we obtain p(  2)     1/  2.8 thus, our joint prior is: p(  ,   2)     1/  2.
a second way to obtain this prior is to give    and   2 proper prior distribu-
tions (not uniform over the real line, which is improper). if we continue with
the assumption that        n (m,    2), we can choose values of m and    2 that
yield a    at distribution. for example, if we let        n (0, 10000), we have a
very    at prior for   . we can also choose a relatively noninformative prior for
  2 by    rst noting that variance parameters follow an inverse gamma distri-
bution (see the next section) and then choosing values for the inverse gamma
distribution that produce a noninformative prior. if   2     ig(a, b), the pdf
appears as:

f (  2|a, b)     (  2)   (a+1)e     /(  2).

in the limit, if we let the parameters a and b approach 0, a noninformative
prior is obtained as 1/  2. strictly speaking, however, if a and b are 0, the
distribution is improper, but we can let both parameters approach 0. we can
then use this as our prior for   2 (that is,   2     ig(0, 0); p(  2)     1/  2). there
are other ways to arrive at this choice for the prior distribution for    and   ,
but i will not address them here (see gelman et al. 1995).

the resulting posterior for    and   2, if we assume a joint prior of 1/  2 for

these parameters, is:

f (  ,   2|x)    

1
  2

1

   2    2

exp(cid:26)   

(xi       )2
2  2 (cid:27) .

(3.4)

n

yi=1

unlike in the previous example, however, this is a joint posterior density
for two parameters rather than one. yet we can determine the conditional
posterior distributions for both parameters, using the rule discussed in the
previous chapter that, generally, f (x|y)     f (x, y).
determining the form for the posterior density for    follows the same logic
as in the previous section. first, we carry out the product over all observations.
next, we expand the quadratic, eliminate terms that are constant with respect
to    and rearrange the terms with the   2 term    rst. doing so yields:

8 this transformation of variables involves a jacobian, as discussed in the previous
chapter. let m = log(  2), and let p(m)     constant. then p(  2)     constant    j,
where j is the jacobian of the transformation from m to   2. the jacobian is then
dm/d  2 = 1/  2. see degroot (1986) for a fuller exposition of this process, and
see any introductory calculus book for a general discussion of transformations of
variables. see gelman et al. 1995 for further discussion of this prior.

3.4 a normal prior   normal likelihood example with   2 known

67

f (  |x,   2)     exp(cid:26)   

n  2     2n  x  

2  2

(cid:27) .

next, to isolate   2, we can divide the numerator and denominator by n.
finally, we can complete the square in    to    nd:

f (  |x,   2)     exp(cid:26)   

(         x)2
2  2/n (cid:27) .

this result shows us that the conditional distribution for   |x,   2     n (  x,   2
n ),
which should look familiar. that is, this is a similar result to what the central
limit theorem in classical statistics claims regarding the sampling distribu-
tion for   x.

what about the posterior distribution for   2? there are at least two ways
to approach this derivation. first, we could consider the conditional distribu-
tion for   2|  , x. if we take this approach, then we again begin with the full
posterior density, but we now must consider all terms that involve   2. if we
carry out the multiplication in the posterior density and combine like terms,
we obtain:

f (  ,   2)    

1

(  2)n/2+1 exp(cid:26)   p(xi       )2

2  2

(cid:27) .

referring back to the above description of the inverse gamma distribution, it
is clear that, if    is considered    xed, the conditional posterior density for   2

is inverse gamma with parameters a = n/2 and b =p(xi       )2/2.

a second way to approach this problem is to consider that the joint pos-
terior density for    and   2 can be factored using the id155
rule as:

f (  ,   2|x) = f (  |  2, x)f (  2|x).

the    rst term on the right-hand side we have already considered in the pre-
vious example with   2 considered to be a known,    xed quantity. the latter
term, however, is the marginal posterior density for   2. technically, an exact
expression for it can be found by integrating the joint posterior density over

   (i.e., r f (  ,   2)d  .) (see gelman et al. 1995). alternatively, we can    nd an
expression proportional to it by factoring equation 3.4. we know that the
distribution for   |  2, x is proportional to a normal density with mean   x and
variance   2/n. thus, if we factor this term out of the posterior, what is left
is proportional to the marginal density for   2.

in order to factor the posterior,    rst, expand the quadratic again to obtain:

1

(  2)n/2+1 exp(cid:26)   p x2

i     2n  x   + n  2

2  2

(cid:27) .

next, rearrange terms to put   2    rst, and divide the numerator and denomi-
nator by n. once again, complete the square to obtain:

68

3 basics of bayesian statistics

1

(  2)n/2+1 exp(cid:26)   

(         x)2 +p x2

2  2/n

i /n       x2

(cid:27) .

we can now separate the two parts of the exponential to obtain:

1
  

exp(cid:26)   

(         x)2
2  2/n (cid:27)   

1

(  2)n/2 exp(cid:26)p x2

i     n  x2
2  2

(cid:27) .

the    rst term is the conditional posterior for   . the latter term is proportional
to the marginal posterior density for   2. the numerator in the exponential is

the numerator for the computational version of the sample variance, p(xi    
  x)2, and so, the result is recognizable as an inverse gamma distribution with
parameters a = (n     1)/2 and b = (n     1)var(x)/2.

3.5 some useful prior distributions

thus far, we have discussed the use of a beta prior for proportion parameter
p combined with a binomial likelihood function, a gamma prior for a poisson
rate parameter   , a normal prior for a mean parameter combined with a
normal likelihood function for the case in which the variance parameter   2
was assumed to be known, and a reference prior of 1/  2   a special case of an
inverse gamma distribution   for a normal likelihood function for the case in
which neither    nor   2 were assumed to be known. in this section, i discuss a
few additional distributions that are commonly used as priors for parameters
in social science models. these distributions are commonly used as priors,
because they are conjugate for certain sampling densities/likelihood functions.
speci   cally, i discuss the dirichlet, the inverse gamma (in some more depth),
and the wishart and inverse wishart distributions.

one thing that must be kept in mind when considering distributions as
priors and/or sampling densities is what symbols in the density are parameters
versus what symbols are the random variables. for example, take the binomial
distribution discussed in chapter 2. in the binomial mass function, the ran-
dom variable is represented by x, whereas the parameter is represented by
p. however, in the beta distribution, the random variable is represented by
p and the parameters are    and   . from a bayesian perspective, parameters
are random variables or at least can be treated as such. thus, what is im-
portant to realize is that we may need to change notation in the pdf so that
we maintain the appropriate notation for representing the prior distribution
for the parameter(s). for example, if we used    to represent the parameter p
in the binomial likelihood function, while p is used as the random variable in
the beta distribution, the two distributions, when multiplied together, would
contain p,   , and x, and it would be unclear how    and p were related. in fact,
in the beta-binomial setup,    = p, but we need to make sure our notation is
clear so that that can be immediately seen.

3.5 some useful prior distributions

69

3.5.1 the dirichlet distribution

just as the multinomial distribution is a multivariate extension of the bi-
nomial distribution, the dirichlet distribution is a multivariate generaliza-
tion of the beta distribution. if x is a k-dimensional vector and x    
dirichlet(  1,   2, . . . ,   k), then:

f (x) =

   (  1 + . . . +   k)
   (  1) . . .    (  k)

x  1   1
1

. . . x  k   1

k

.

just as the beta distribution is a conjugate prior for the binomial distribution,
the dirichlet is a conjugate prior for the multinomial distribution. we can see
this result clearly, if we combine a dirichlet distribution as a prior with a
multinomial distribution likelihood:

f (p1 . . . pk|x)     f (x|p1 . . . pk)f (p1 . . . pk)

    multinomial(x|p1 . . . pk)dirichlet(p1 . . . pk|  1 . . .   k)
    dirichlet(p1 . . . pk|  1 + x1,   2 + x2, . . . ,   k + xk)
    p  1+x1   1

. . . p  k+xk   1

p  2+x2   1
2

k

1

.

notice how here, as we discussed at the beginning of the section, the vector x
in the original speci   cation of the dirichlet pdf has been changed to a vector
p. in this speci   cation, p is the random variable in the dirichlet distribution,
whereas   1 . . .   k are the parameters representing prior counts of outcomes in
each of the k possible outcome categories.

also observe how the resulting dirichlet posterior distribution looks just
like the resulting beta posterior distribution, only with more possible out-
comes.

3.5.2 the inverse gamma distribution

we have already discussed the gamma distribution in the poisson/gamma
example, and we have brie   y discussed the inverse gamma distribution. if
1/x     gamma(  ,   ), then x     ig(  ,   ). the density function for the inverse
gamma distribution is:

f (x) =

    
   (  )

x   (  +1)e     /x,

with x > 0. just as in the gamma distribution, the parameters    and    a   ect
the shape and scale of the curve (respectively), and both must be greater than
0 to make the density proper.

as discussed earlier, the inverse gamma distribution is used as a conju-
gate prior for the variance in a normal model. if the normal distribution is
parameterized with a precision parameter rather than with a variance param-
eter, where the precision parameter is simply the inverse of the variance, the

70

3 basics of bayesian statistics

gamma distribution is appropriate as a conjugate prior distribution for the
precision parameter. in a normal model, if an inverse gamma distribution is
used as the prior for the variance, the marginal distribution for the mean is a
t distribution.

the gamma and inverse gamma distributions are general distributions;
other distributions arise by    xing the parameters to speci   c values. for ex-
ample, if    is set to 1, the exponential distribution results:

f (x) = (1/  )e   x/  ,

or, more commonly f (x) =   e     x, where    is an inverse scale parameter.
under this parameterization,   inverse scale = 1/  scale.

if    is set to v/2, where v is the degrees of freedom, and    is set to 1/2, the
chi-square distribution results. setting the parameters equal to the same value
in the inverse-gamma distribution yields an inverse-chi-square distribution.

3.5.3 wishart and inverse wishart distributions

the wishart and inverse wishart distributions are complex in appearance;
they are multivariate generalizations of the gamma and inverse gamma dis-
tributions, respectively. thus, just as the inverse gamma is a conjugate prior
density for the variance in a univariate normal model, the inverse wishart
is a conjugate prior density for the variance-covariance matrix in a multi-
variate normal model. with an inverse wishart distribution for the variance-
covariance matrix in a multivariate normal model, the marginal distribution
for the mean vector is multivariate t.

if x     wishart(s), where s is a scale matrix of dimension d, then

f (x)    | x |(v   d   1)/2 exp(cid:26)   

1
2

tr(s   1x)(cid:27) ,

where v is the degrees of freedom.

if x     inverse wishart(s   1), then:

f (x)    | x |   (v+d+1)/2 exp(cid:26)   

1
2

tr(sx    1)(cid:27) .

the assumption for both the wishart and inverse wishart distributions is
that x and s are both positive de   nite; that is, zt xz > 0 and zt sz > 0 for
any non-zero vector z of length d.

3.6 criticism against bayesian statistics

as we have seen in the examples, the development of a bayesian model re-
quires the inclusion of a prior distribution for the parameters in the model.
the notion of using prior research or other information to inform a current

3.6 criticism against bayesian statistics

71

analysis and to produce an updated prior for subsequent use seems quite rea-
sonable, if not very appropriate, for the advancement of research toward a
more re   ned knowledge of the parameters that govern social processes. how-
ever, the bayesian approach to updating knowledge of parameters has been
criticized on philosophical grounds for more than a century, providing one
reason its adoption has been relatively limited in mainstream social science
research.

what is in philosophical dispute between bayesians and classical statisti-
cians includes: (1) whether data and hypotheses (which are simply statements
about parameters of distributions9) can hold the same status as random vari-
ables, and (2) whether the use of a prior id203 injects too much subjec-
tivity into the modeling process.

the    rst standard argument presented against the bayesian approach is
that, because parameters are    xed, it is unreasonable to place a id203
distribution on them (they simply are what they are). more formally, pa-
rameters and data cannot share the same sample space. however, recall that
the bayesian perspective on id203 is that id203 is a subjective ap-
proach to uncertainty. whether a parameter is indeed    xed, to a bayesian, is
irrelevant, because we are still uncertain about its true value. thus, impos-
ing a id203 distribution over a parameter space is reasonable, because
it provides a method to re   ect our uncertainty about the parameter   s true
value.

bayesians argue that doing so has some signi   cant advantages. first, as
we have seen, bayesian interval estimates have a clearer and more direct inter-
pretation than classical con   dence intervals. that is, we can directly conclude
that a parameter falls in some interval with some id203. this is a com-
mon but incorrect interpretation of classical con   dence intervals, which simply
re   ect the id203 of obtaining an interval estimate that contains the pa-
rameter of interest under repeated sampling. second, the bayesian approach
can naturally incorporate the    ndings of previous research with the prior,
whereas the classical approach to statistics really has no coherent means of
using previous results in current analyses beyond assisting with the speci   ca-
tion of a hypothesis. that is, the bayesian approach formalizes the process of
hypothesis construction by incorporating it as part of the model. third, the
bayesian approach more easily allows more detailed summaries concerning
parameters. instead of simply obtaining a maximum likelihood estimate and
standard error, we have an entire distribution that can be summarized using
various measures (e.g., mean, median, mode, and interquartile range).

9 an alternative representation of bayes    theorem is p(hypothesis | data)    
p(data | hypothesis)    p(hypothesis), which shows that, from a bayesian per-
spective, we can place a id203 (distribution) on a scienti   c hypothesis. see
je   reys 1961 for a detailed discussion of the theory of    inverse id203,    which
describes the bayesian approach in these terms.

72

3 basics of bayesian statistics

the second general argument that has been advanced against bayesian
analysis is that incorporating a prior injects too much subjectivity into statis-
tical modeling. the bayesian response to this argument is multifaceted. first,
all statistics is subjective. the choice of sampling density (likelihood) to use
in a speci   c project is a subjective determination. for example, when faced
with an ordinal outcome, some choose to use a normal likelihood function,
leading to the ordinary least squares (ols) regression model. others choose a
binomial likelihood with a link function, leading to an ordinal logit or probit
regression model. these are subjective choices.

second, the choice of cut-point (  ) at which to declare a result    statisti-
cally signi   cant    in a classical sense is a purely subjective determination. also,
similarly, the decision to declare a statistically signi   cant result substantively
meaningful is a subjective decision.

a third response to the subjectivity criticism is that priors tend to be
overwhelmed by data, especially in social science research. the prior distribu-
tion generally contributes to the posterior once, whereas data enter into the
likelihood function multiple times. as n        , the prior   s in   uence on the
posterior often becomes negligible.
fourth, priors can be quite noninformative, obviating the need for large
quantities of data to    outweigh    them. in other words, a prior can be made
to contribute little information to the posterior. that is, given that the pos-
terior density is simply a weighted likelihood function, where the weighting
is imposed by the prior, we can simply choose a prior distribution for the
parameters that assigns approximately equal weight to all possible values of
the parameters. the simplest noninformative prior that is often used is thus a
uniform prior. use of this prior yields a posterior density that is proportional
to the likelihood function. in that case, the mode of the likelihood function
(the maximum likelihood estimate) is the same as the bayesian maximum a
posteriori (map) estimate, and the substantive conclusions reached by both
approaches may be similar, only di   ering in interpretation.

in defense of the classical criticism, although uniform densities for param-
eters are often used as priors, transformation from one parameterization of
a parameter to another may yield an informative prior. however, alternative
approaches have been developed for generating noninformative priors, includ-
ing the development of je   reys priors and other priors. these noninformative
priors tend to be based on the information matrix and are invariant under pa-
rameter transformation. an in-depth discussion of such priors is beyond the
scope of this book, given the goal of a general introduction to estimation. for
more details, see gelman et al. (1995) or see gill (2002) for a more in-depth
discussion of the history of the use and construction of noninformative priors.
a fourth response is that the in   uence of priors can be evaluated after
modeling the data to determine whether posterior id136 is reasonable. ul-
timately, the results of any statistical analysis, whether bayesian or classical,
must be subjectively evaluated to determine whether they are reasonable, and
so, the use of informative priors cannot introduce any more subjectivity than

3.7 conclusions

73

could be included via other means in any analysis. another response along
these lines is that we can use priors to our advantage to examine how pow-
erful the data are at invalidating the prior. for example, we may establish
a conservative prior for a regression coe   cient that claims that the a priori
id203 for a regression coe   cient is heavily concentrated around 0 (i.e.,
the covariate has no e   ect on the outcome). we can then examine the strength
of the data in rejecting this prior, providing a conservative test of a covariate   s
e   ect.

in general, the historical criticisms of bayesian statistics are philosophical
in nature and cannot be conclusively adjudicated. instead, the rise in the
use of bayesian statistics over the last few decades has largely occurred for
pragmatic reasons, including (1) that many contemporary research questions
readily lend themselves to a bayesian approach, and (2) that the development
of sampling methods used to estimate model parameters has increased their
ease of use. the remaining chapters attempt to demonstrate these points.

3.7 conclusions

in this chapter, we have developed the basics of the bayesian approach to
statistical id136. first, we derived bayes    theorem from the id203
rules developed in the previous chapter, and we applied bayes    theorem to
problems requiring point estimates for probabilities. we then extended the
bayesian approach to handle prior distributions for parameters rather than
simply point estimates for prior probabilties. the result was that our posterior
id203 became a distribution, rather than a point estimate. next, we dis-
cussed how to summarize posterior id203 distributions, and we demon-
strated how to do so using several common examples. finally, we discussed
some common criticisms of the bayesian approach that have been advanced
over the last century, and we reviewed some common bayesian responses to
them. although the material presented in this chapter is su   cient for gaining
a basic understanding of the bayesian approach to statistics, i recommend
several additional sources for more in-depth coverage. i recommend lee 1989
for an extremely thorough but accessible exposition of the bayesian paradigm,
and i recommend box and tiao (1973) for a more advanced exposition.

in the next chapter, we will continue exploring the bayesian approach
to posterior summarization and id136, but we will ultimately focus on
multivariate posterior distributions   the most common type of posterior dis-
tribution found in social science research   where the multivariate posterior
distribution may not be as easy to summarize directly as the univariate pos-
terior densities shown in this chapter.

74

3 basics of bayesian statistics

3.8 exercises

1. in your own words, state what bayes    theorem for point probabilities ac-
tually does. for example, refer to chapter 2 where i de   ned conditional
id203, and use the same sort of discussion to describe how the the-
orem works.

2. the pregnancy example was completely contrived. in fact, most pregnancy
tests today do not have such high rates of false positives. the    accuracy
rate    is usually determined by computing the percent of correct answers
the test gives; that is, the combined percent of positive results for positive
cases and negative results for negative cases (versus false positives and
false negatives). recompute the posterior id203 for being pregnant
based on an accuracy rate of 90% de   ned in this manner. assume that
false positives and false negatives occur equally frequently under this 90%
rate. what changes in the calculation?

3. determine the posterior id203 that a 30-year-old male has prostate
cancer, given (1) a positive psa test result; (2) a 90% accuracy rate (as
de   ned in the pregnancy example), coupled with a 90% false positive rate;
and (3) a prior id203 of .00001 for a 30-year-old male having prostate
cancer. based on the result, why might a physician consider not testing a
30-year-old male using the psa test?

4. find and plot the posterior distribution for a binomial likelihood with
x = 5 successes out of n = 10 trials using at least three di   erent beta prior
distributions. does the prior make a large di   erence in the outcome   
when?

5. find and plot the posterior distribution for a normal distribution likeli-
hood with a sample mean   x = 100 and variance var(x) = 144 (assume
n = 169) using at least three di   erent normal priors for the mean. when
does the prior make the largest di   erence in the outcome   when the prior
mean varies substantially from the sample mean, or when the prior vari-
ance is small or large?

6. reconsider the pregnancy example from the beginning of the chapter. i
showed the posterior probabilities for the second through the tenth sub-
sequent tests. reproduce these results, using the posterior obtained from
the kth test as the prior for the (k + 1)st test. next, assume the original
prior (p = .15) and assume the 10 tests were taken simultaneously and
all yielded a positive result. what is the posterior id203 for preg-
nancy? finally, reconduct the pregnancy example with the 10 positive
tests treated simultaneously as the current data, and use a beta prior
distribution. interpret the results.

7. in the 2004 u.s. presidential election, surveys throughout the fall con-
stantly reversed the projected victor. as each survey was conducted, would
it have been appropriate to incorporate the results of previous surveys as
priors and treat the current survey as new data to update the prior in
a bayesian fashion? if so, do you think a more consistent picture of the

3.8 exercises

75

winner would have emerged before the election? if a bayesian approach
would not have been appropriate, why not?

8. give two simple examples showing a case in which a prior distribution

would not be overwhelmed by data, regardless of the sample size.

9. show how the multinomial likelihood and dirichlet prior are simply a

multivariate generalization of the binomial likelihood and beta prior.

10. show how the wishart distribution reduces to the gamma distribution

when the number of dimensions of the random variable is 1.

11. i said throughout the chapter that the inverse gamma distribution was the
appropriate distribution for a variance parameter. it could be said that
variance parameter could be considered to be distributed as an inverse
chi-square random variable. both of these statements are true. how?

12. why can a prior distribution that equals a constant be considered pro-

portional to a uniform distribution?

