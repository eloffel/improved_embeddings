the mathematics of deep learning

iccv tutorial, santiago de chile, december 12, 2015   

joan bruna (berkeley), raja giryes (duke), guillermo sapiro (duke), rene vidal (johns hopkins)

   
motivations and goals of the tutorial
    motivation: deep networks have led to dramatic 

improvements in performance for many tasks, but the 
mathematical reasons for this success remain unclear.  

    goal: review very recent work that aims at understanding 

the mathematical reasons for the success of deep networks. 

    what we will do: study theoretical questions such as 

    what properties of images are being captured/exploited by dnns? 
    can we ensure that the learned representations are globally optimal? 
    can we ensure that the learned representations are stable? 

    what we will not do: show x% improvement in performance 

for a particular application. 

tutorial schedule
    14:00-14.30: introduction 

    14:30-15.15: global optimality in deep learning (ren   vidal) 

    15:15-16.00: coffee break  

    16:00-16:45: scattering convolutional networks (joan bruna) 

    16:45-17:30: stability of deep networks (raja giryes) 

    17.30-18:00: questions and discussion

disclaimer

what do we mean by    deep learning    in this tutorial?

disclaimer

what do we mean by    deep learning    in this tutorial?

   a class of signal representations that are hierarchical:

   the optimization procedure by which these representations 
are learnt from data end-to-end. 

   gure from raja giryes

early hierarchical feature models for vision

    hubel & wiesel [60s] simple & complex cells architecture:

    fukushima   s neocognitron [70s]

   gures from yann lecun   s cvpr   15 plenary

early hierarchical feature models for vision

    yann lecun   s early convnets [80s]:

    used for character recognition
    trained with back propagation.

   gures from yann lecun   s cvpr   15 plenary

deep learning pre-2012

   despite its very competitive performance, deep learning 
architectures were not widespread before 2012.
    state-of-the-art in handwritten pattern recognition [lecun et al.    89, 
ciresan et al,    07, etc] 

   gures from yann lecun   s cvpr   15 plenary

deep learning pre-2012

   despite its very competitive performance, deep learning 
architectures were not widespread before 2012.
    face detection [vaillant et al   93,   94 ; osadchy et al,    03,    04,    07]

(yann   s family)

deep learning pre-2012

   despite its very competitive performance, deep learning 
architectures were not widespread before 2012.
    scene parsing [farabet et al,    12,   13]

   gures from yann lecun   s cvpr   15 plenary

deep learning pre-2012

   despite its very competitive performance, deep learning 
architectures were not widespread before 2012.
    scene parsing [farabet et al,    12,   13]

   gures from yann lecun   s cvpr   15 plenary

deep learning pre-2012

   despite its very competitive performance, deep learning 
architectures were not widespread before 2012.

    too many parameters to learn from few labeled examples.
       i know my features are better for this task   .
    non-id76? no, thanks. 
    black-box model, no interpretability. 

deep learning golden age in vision

    2012-2014 id163 results: 

id98

non-id98

    2015 results: msra under 3.5% error. (using a id98 with 150 layers!)

   gures from yann lecun   s cvpr   15 plenary

puzzling questions
   what made this result possible?

    larger training sets (1.2 million, high-resolution training samples, 1000 
object categories)
    better hardware (gpu)
    better learning id173 (dropout)

   is this just for a particular dataset?

   is this just for a particular task?

   why are these architectures so ef   cient? 

is it just for a particular dataset?

    no. nowadays id98s hold the state-of-the-art on virtually any object 
classi   cation task.

   gures from yann lecun   s nips   15 tutorial

is it just for a particular task?

    no. id98 architectures also obtain state-of-the-art performance on many 
other tasks:

object localization 
[r-id98, hypercolumns, overfeat, etc.]

pose estimation [thomson et al, cvpr   15]

   gures from yann lecun   s cvpr   15 plenary

is it just for a particular task?

    no. id98 architectures also obtain state-of-the-art performance on other 
tasks:

   semantic segmentation [pinhero, collobert, dollar, iccv   15]

   gures from yann lecun   s cvpr   15 plenary

is it just for a particular task?

    no. id98 architectures also obtain state-of-the-art performance on other 
tasks:

   generative models for natural images [radford, metz & chintala,   15]

is it just for a particular task?

    no. id98 architectures also obtain state-of-the-art performance on other 
tasks:

   generative models for natural images [radford, metz & chintala,   15]

is it just for a particular task?

    no. id98 architectures also obtain state-of-the-art performance on other 
tasks:

   related work [kulkarni et al   15, dosovitsky et al    14]

is it just for a particular task?

    no. id98 architectures also obtain state-of-the-art performance on other 
tasks:

   image captioning [vinyals et al   14, karpathy et al    14, etc]

   optical flow estimation [zontar    15]

   image super-resolution [msr   14]

   convolutional deep learning models thus appear to 
capture high level image properties more ef   ciently than 
previous models.
    highly expressive representations capturing complex geometrical and 
statistical patterns.
    excellent generalization:    beating    the curse of dimensionality

   convolutional deep learning models thus appear to 
capture high level image properties more ef   ciently than 
previous models.

    which architectural choices might explain this advantage 
mathematically?
    role of non-linearities?
    role of convolutions?
    role of depth?
    interplay with geometrical, class-speci   c invariants?

   convolutional deep learning models thus appear to 
capture high level image properties more ef   ciently than 
previous models.

    which architectural choices might explain this advantage 
mathematically?

   which optimization choices might explain this advantage?

    presence of local minima or saddle points?
    equivalence of local solutions?
    role of stochastic optimization?

deep learning approximation theory

    deep networks de   ne a class of    universal approximators   : cybenko and 
hornik characterization: 

theorem [c   89, h   91] let    () be a bounded, non-constant continuous func-
tion. let im denote the m-dimensional hypercube, and c(im) denote the space
of continuous functions on im. given any f 2 c(im) and     > 0, there exists
n > 0 and vi, wi, bi, i = 1 . . . , n such that

f (x) =xi   n

vi   (wt

i x + bi) satis   es

sup

x2im |f (x)   f (x)| <     .

deep learning approximation theory

    deep networks de   ne a class of    universal approximators   : cybenko and 
hornik characterization:

theorem [c   89, h   91] let    () be a bounded, non-constant continuous func-
tion. let im denote the m-dimensional hypercube, and c(im) denote the space
of continuous functions on im. given any f 2 c(im) and     > 0, there exists
n > 0 and vi, wi, bi, i = 1 . . . , n such that

f (x) =xi   n

vi   (wt

i x + bi) satis   es

sup

x2im |f (x)   f (x)| <     .

    it guarantees that even a single hidden-layer network can represent any 
classi   cation problem in which the boundary is locally linear (smooth).
    it does not inform us about good/bad architectures.
    or how they relate to the optimization.

deep learning estimation theory

theorem [barron   92] the mean integrated square error between the esti-

mated network   f and the target function f is bounded by

o  c2
n ! + o    n m

k

f

log k    ,

where k is the number of training points, n is the number of neurons, m is the
input dimension, and cf measures the global smoothness of f .

deep learning estimation theory

theorem [barron   92] the mean integrated square error between the esti-

mated network   f and the target function f is bounded by

o  c2
n ! + o    n m

k

f

log k    ,

where k is the number of training points, n is the number of neurons, m is the
input dimension, and cf measures the global smoothness of f .

    combines approximation and estimation error. 
    does not explain why online/stochastic optimization works better than batch 
id172.
    does not relate generalization error with choice of architecture.

non-id76

    [choromaska et al, aistats   15] (also [dauphin et al, icml   15] ) use tools 
from statistical physics to explain the behavior of stochastic gradient methods 
when training deep neural networks. 

non-id76

    [choromaska et al, aistats   15] (also [dauphin et al, icml   15] ) use tools 
from statistical physics to explain the behavior of stochastic gradient methods 
when training deep neural networks. 

    offers a macroscopic explanation of why sgd    works   .
    gives a characterization of the network depth.
    strong model simpli   cations, no convolutional speci   cation.

tutorial outline
    part i: global optimality in deep learning (ren   vidal) 

chapter 4. generalized factorizations

critical points of non-convex function

guarantees of our framework

(a)

(c)

(b)

(d)

(e)

(f)

(g)

(h)

(i)

scattering recovery

original

o(log n)

single-layer recovery

        

    

    part ii: signal recovery from scattering convolutional 

figure 4.1: left: example critical points of a non-convex function (shown in red).
(a) saddle plateau (b,d) global minima (c,e,g) local maxima (f,h) local minima (i
networks (joan bruna) 
- right panel) saddle point. right: guaranteed properties of our framework. from
any initialization a non-increasing path exists to a global minimum. from points on
a    at plateau a simple method exists to    nd the edge of the plateau (green points).

guaranteed to    nd a global minimum of the non-convex factorization problem if from

points). taken together, these results will imply a theoretical meta-algorithm that is

one layer stable embedding

plateaus (a,c) for which there is no local descent direction1, there is a simple method
o((log n )2)
    part iii: on the stability of deep networks (raja giryes)
to    nd the edge of the plateau from which there will be a descent direction (green

    
    (        )          
               
theorem 1: there exists an algorithm      such that
           (    (        )) <                =        3
(cid:190)after      layers we have an error             3

any point one can either    nd a local descent direction or verify the non-existence of a

part i: global optimality in deep learning
    key questions 

    how to deal with the non-convexity of the learning problem? 
    do learning methods get trapped in local minima? 
    why many local solutions seem to give about equally good results? 
    why using rectified linear rectified units instead of other nonlinearities? 

    key results 

    deep learning is a positively homogeneous factorization problem 
    with proper id173, local minima are global 
chapter 4. generalized factorizations
    if network large enough, global minima can be found by local descent

critical points of non-convex function

guarantees of our framework

(a)

(c)

(b)

(d)

(e)

(f)

(g)

(h)

(i)

figure 4.1: left: example critical points of a non-convex function (shown in red).
(a) saddle plateau (b,d) global minima (c,e,g) local maxima (f,h) local minima (i
- right panel) saddle point. right: guaranteed properties of our framework. from

part ii: scattering convolutional networks
    key questions 

    what is the importance of "deep" and "convolutional" in id98 

architectures? 

    what statistical properties of images are being captured/exploited by 

deep networks? 

original

    key results 

single-layer recovery

scattering recovery

o(log n )

o((log n )2)

    scattering coefficients are stable encodings of geometry and texture 
    layers in a id98 encode complex, class-specific geometry.

part iii: on the stability of deep networks
    key questions 

        

    

perturbations to the output of the network? 

    can i recover the input from the output?  

    stability: do small perturbations to the input image cause small 
one layer stable embedding

    
    (        )          
               
theorem 1: there exists an algorithm      such that
           (    (        )) <                =        3
(cid:190)after      layers we have an error             3

    gaussian mean width is a good measure of data complexity.  
    dnn keep important information of the data. 
    deep learning can be viewed as metric learning problem. 

[plan & vershynin, 2013, giryes, sapiro & bronstein 2015].

    key results 

(cid:190)stands in line with [mahendran and vedaldi, 2015].

tutorial schedule
    14:00-14.30: introduction 

    14:30-15.15: global optimality in deep learning (ren   vidal) 

    15:15-16.00: coffee break  

    16:00-16:45: scattering convolutional networks (joan bruna) 

    16:45-17:30: stability of deep networks (raja giryes) 

    17.30-18:00: questions and discussion

