uai 2017 australia

tutorial on 

deep generative models

shakir mohamed and danilo rezende

@shakir_za     @deepspiker

abstract

this tutorial will be a review of recent advances in deep generative models. generative models 
have  a  long  history  at  uai  and  recent  methods  have  combined  the  generality  of  probabilistic 
reasoning  with  the  scalability  of  deep  learning  to  develop  learning  algorithms  that  have  been 
applied  to  a  wide  variety  of  problems  giving  state-of-the-art  results  in  image  generation, 
text-to-speech  synthesis,  and  image  captioning,  amongst  many  others.  advances  in  deep 
generative models are at the forefront of deep learning research because of the promise they 
offer  for  allowing  data-efficient  learning,  and  for  model-based  reinforcement  learning.  at  the 
end  of  this  tutorial,  audience  member  will  have  a  full  understanding  of  the  latest  advances  in 
generative  modelling  covering  three  of  the  active  types  of  models:  markov  models,  latent 
variable models and implicit models, and how these models can be scaled to high-dimensional 
data.  the  tutorial  will  expose  many  questions  that  remain  in  this  area,  and  for  which  there 
remains a great deal of opportunity from members of the uai community.

beyond classification

move beyond associating 

inputs to outputs

understand and imagine 
how the world evolves

recognise objects in the 
world and their factors of 

variation

establish concepts as useful 

for reasoning and 
decision making

detect surprising events in 

the world

anticipate and generate 
rich plans for the future

what is a generative model?

a model that allows 

us to learn a 

simulator of data

models that allow for 
(conditional) density 

estimation

characteristics are:

- probabilistic models of data that allow 

for uncertainty to be captured.

- data distribution p(x) is targeted.
- high-dimensional outputs.

approaches for 

unsupervised learning 

of data

why generative models?

why generative models

generative models have a 

role in many problems.

drug design and response prediction

proposing candidate molecules and for improving prediction through semi-supervised learning.

g  mez-bombarelli, et al. 2016

locating celestial bodies

generative models for applications in astronomy and high-energy physics. 

regier et al., 2015

image super-resolution

photo-realistic single image super-resolution

ledig et al., 2016

text-to-id133

generating audio conditioned on text

oord et al., 2016

image and content generation

generating images and video content.

draw

pixel id56

ali

gregor et al., 2015, oord et al., 2016, dumoulin et al., 2016

communication and compression

hierarchical compression of images and other data.

original images

compression rate: 0.2bits/dimension

jpeg

jpeg-2000

rvae v1

rvae v2

gregor et al., 2016

one-shot generalisation

rapid generalisation of novel concepts

rezende et al., 2016

visual concept learning

understanding the factors of variation and invariances.

higgins et al., 2017

future simulation

simulate future trajectories of environments based on actions for planning

atari simulation

robot arm simulation

chiappa et al, 2017, kalchbrenner et al., 2017

scene understanding

understanding the components of scenes and their interactions

wu et al., 2017

probabilistic deep learning

two streams of machine learning

deep learning

probabilistic reasoning

+ rich non-linear models for 

classification and sequence 
prediction.

+ scalable learning using stochastic 

approximation and conceptually 
simple.

+ easily composable with other 

gradient-based methods.

- only point estimates.
- hard to score models, do selection 

and complexity penalisation.

- mainly conjugate and linear models.
- potentially intractable id136, 

computationally expensive or long 
simulation time.

+ unified framework for model 

building, id136, prediction and 
decision making.

+ explicit accounting for uncertainty 

and variability of outcomes.

+ robust to overfitting; tools for model 

selection and composition.

complementary strengths, making it natural to combine them

thinking about machine learning

3. algorithms

1. models

2. learning principles

types of generative models

fully-observed models
model observed data directly without 
introducing any new unobserved local 
variables. 

latent variable models
introduce an unobserved random variable for every 
observed data point to explain hidden causes.

    prescribed models: use observer likelihoods and 

assume observation noise.
implicit models: likelihood-free models.

   

1. models

spectrum of fully-observed models

building generative models

equivalent ways of representing the same dag

fully-observed models

all conditional probabilities described by deep networks.

+ can directly encode how observed 

points are related.

+ any data type can be used
+

for directed id114: 
parameter learning simple
log-likelihood is directly 
computable, no approximation 
needed. 

+

+ easy to scale-up to large models, 
many optimisation tools available.

- order sensitive.
-

for undirected models, parameter 
learning difficult: need to compute 
normalising constants.

- generation can be slow: iterate 

through elements sequentially, or 
using a markov chain.

spectrum of latent variable models

building generative models

building generative models

id114 + computational graphs (aka nnets)

latent variable models

-

inversion process to 
determine latents 
corresponding to a input is 
difficult in general

- difficult to compute 

marginalised likelihood 
requiring approximations.

- not easy to specify rich 

approximations for latent 
posterior distribution.

+ easy sampling. 
+ easy way to include 
hierarchy and depth.

+ easy to encode structure 
+ avoids order dependency 

assumptions: 
marginalisation induces 
dependencies.

+ provide compression and 

+

representation.
scoring, model comparison 
and selection possible using 
the marginalised likelihood.

introduce an unobserved local 

random variables that represents 

hidden causes.

choice of learning principles

for a given model, there are many competing id136 methods.

    exact methods (conjugacy, enumeration)
    numerical integration (quadrature)
    generalised method of moments
    maximum likelihood (ml)
    maximum a posteriori (map)
    laplace approximation
    integrated nested laplace approximations (inla)
    expectation maximisation (em)
    monte carlo methods (mcmc, smc, abc)
    contrastive estimation (nce)
    cavity methods (ep)
    variational methods

2. learning principles

combining models and id136

3. algorithms

a given model and learning principle can be implemented in many ways.

convolutional neural network 
+ penalised maximum likelihood

implicit generative model 

+ two-sample testing

    optimisation methods 

   

(sgd,  adagrad)
regularisation (l1, l2, 
batchnorm, dropout)

    method-of-moments
    approximate bayesian computation 

(abc)

    generative adversarial network (gan)

latent variable model 
+ variational id136

vem algorithm
expectation propagation

   
   
    approximate message passing
   

variational auto-encoders (vae)

restricted id82 

+ maximum likelihood

   
contrastive divergence
   
persistent cd
   
parallel tempering
    natural gradients

id136 questions?

objective

quantity of interest

prediction

planning

parameter estimation

experimental design

hypothesis testing

approximate id136

latent variable models

methods for approximate id136

    laplace approximations

    importance sampling

    variational approximations

    perturbative corrections

    other methods: mcmc, langevin, hmc, adaptive mcmc

laplace approximation

other names

saddle-point approximation, 
delta-method

importance sampling 

importance weights

monte-carlo

pointwise free-energy

important property

importance sampling provides a bound in expectation 

z

x

x

variational id136

variational id136

reconstruction

regularizer

perturbative corrections

design choices

computation graphs, renderers, simulators and environments

choice of model

variational optimisation
- variational em
- stochastic vem
- monte carlo gradient 

estimators

approximate posteriors

- mean-field
- structured approx
- aux. variable methods

variational em algorithm

fixed-point iterations between variational and model parameters

e

m

e

m

amortised id136

introduce a parametric family of conditional densities

rezende et al., 2015

variational auto-encoders

simplest instantiation of a vae

deep latent gaussian model p(x,z)

gaussian recognition model q(z)

we then optimise the free-energy wrt model and variational parameters

kingma and welling, 2014, rezende et al., 2014

richer vaes

draw: recurrent/dependent priors

recurrent/dependent id136 networks

air: structured priors

volumetric and 
sequence data

semi-supervised learning

applications of 
generative models

summary so far

probabilistic 
deep learning

types of 

generative models

variational principles

amortised id136

end of first half

stochastic optimisation

classical id136 approach

e

m

compute expectations then m-step gradients

stochastic id136 approach

gradient is of the parameters of the distribution w.r.t. which the expectation is taken.

in general, we won   t know the expectations.

stochastic gradient estimators

score-function estimator: 

differentiate the density q(z|x)

pathwise gradient estimator: 
differentiate the function f(z)

fu, 2006

typical problem areas:

id23 and control

    generative models and id136
   
    operations research and inventory control
    monte carlo simulation
   
   

finance and asset pricing
sensitivity estimation

score function estimators

gradient reweighted by the value of the function

other names:

   
   
   

likelihood-ratio trick
radon-nikodym derivative
reinforce and policy 
gradients

    automated id136
   
black-box id136

when to use:

function is not differentiable.

   
    distribution q is easy to sample 

from.

    density q is known and 

differentiable.

reparameterisation

find an invertible function g(.) that expresses z as a 

transformation of a base distribution .

kingma and welling, 2014, rezende et al., 2014

pathwise derivative estimator

other names:

reparameterisation trick
stochastic id26
perturbation analysis

   
   
   
    affine-independent id136
    doubly stochastic estimation
    hierarchical non-centred 

parameterisations.

when to use

function f is differentiable

   
    density q can be described using a simpler 

base distribution: inverse cdf, location-scale 
transform, or other co-ordinate transform.
easy to sample from base distribution.

   

gaussian stochastic gradients

first-order gradient

second-order gradient

we can develop low-variance estimators by exploiting knowledge 

of the distributions involved when we know them

rezende et al., 2014

beyond the mean field

mean field approximations

key part of variational id136 is choice of approximate posterior distribution q.

mean-field posterior approximations

deep latent 

gaussian model

mean-field or fully-factorised posterior is usually not sufficient

real-world posterior distributions

deep latent 

gaussian model

complex dependencies    non-gaussian distributions    multiple modes

richer families of posteriors

two high-level goals: 

build richer approximate posterior distributions.

   
    maintain computational efficiency and scalability.

same as the problem of specifying a model of the data itself

structured approximations

families of approximate posteriors

covariance models

normalising flows

auxiliary variable models

normalising flows
exploit the rule for change of variables:
    begin with an initial distribution 
    apply a sequence of k invertible transforms

rezende and mohamed, 2015

distribution flows through a sequence of invertible transforms

normalising flows

normalising flows

choice of transformation

begin with a fully-factorised 

gaussian and improve by 

change of variables.

triangular jacobians allow for 

computational efficiency.

linear time computation of the determinant and its gradient.

rezende and mohamed, 2015; dinh et al, 2016, kingma et al, 2016

normalising flows on non-euclidean manifolds

gemici et al., 2016

normalising flows on non-euclidean manifolds

learning in 
implicit generative models

learning by comparison

for some models, we only have access to an 

unnormalised id203, partial knowledge of the 

distribution, or a simulator of data.

we compare the estimated 
distribution q(x) to the true 

distribution p*(x) using samples.

mohamed and lakshminarayanan, 2017.

learning by comparison

comparison

use a hypothesis test or comparison to 
build an auxiliary model to indicate how 
data simulated from the model differs 

from observed data. 

estimation

adjust model parameters to better match 
the data distribution using the comparison.

density ratios and classification

bayes    
rule

real data

simulated data

density 
ratio

combine data

assign labels

equivalence  

sugiyama et al, 2012

density ratios and classification

conditional

bayes    substitution

class id203

computing a density ratio is equivalent to class id203 estimation.

unsupervised-as-supervised learning

scoring function

bernoulli loss

alternating 
optimisation

    use when we have differentiable 

   

simulators and models
can form the loss using any proper 
scoring rule.

other names and places:

    unsupervised and supervised learning
   
   
    id3

continuously updating id136
classifier abc

friedman et al. 2001

id3

alternating optimisation

comparison loss

(alt) generative loss

goodfellow et al. 2014

integral id203 metrics

f sometimes referred to as a 

test function, witness function or a critic. 

many choices of f available: classifiers or 
functions in specified spaces.

wasserstein

total 
variation

max mean discrepancy

cramer

generative models and rl

probabilistic policy learning

policy gradient update:
    uniform prior on actions
    score-function gradient estimator (aka reinforce)

other algorithms:

relative id178 policy search

   
    generative adversarial imitation learning
   

reinforced variational id136

other names and instantiations:

   
   
   

planning-as-id136 
variational mdps
path-integral control

the future

applications of 
generative models

probabilistic 
deep learning

types of 

generative models

rich distributions

variational principles

amortised id136

stochastic optimisation

learning by comparison

challenges
    scalability to large images, videos, multiple data modalities.
    evaluation of generative models.
    robust conditional models.
    discrete latent variables.
    support-coverage in models, mode-collapse.
    calibration.
    parameter uncertainty.
    principles of likelihood-free id136.

uai 2017 australia

tutorial on 

deep generative models

shakir mohamed and danilo rezende

@shakir_za     @deepspiker

references: applications

   

   

   

   

   

   

   
   

   

   

frey, brendan j., and geoffrey e. hinton. "variational learning in nonlinear gaussian belief networks." neural computation 11, no. 1 
(1999): 193-213.   
eslami, s. m., heess, n., weber, t., tassa, y., kavukcuoglu, k., and hinton, g. e. attend, infer, repeat: fast scene understanding 
with generative models. nips (2016).   
rezende, danilo jimenez, shakir mohamed, ivo danihelka, karol gregor, and daan wierstra. "one-shot generalization in deep 
generative models." icml (2016).   
kingma, diederik p., shakir mohamed, danilo jimenez rezende, and max welling. "semi-supervised learning with deep 
generative models." in advances in neural information processing systems, pp. 3581-3589. 2014.
higgins, irina, loic matthey, xavier glorot, arka pal, benigno uria, charles blundell, shakir mohamed, and alexander lerchner. 
"early visual concept learning with unsupervised deep learning." arxiv preprint arxiv:1606.05579 (2016).
bellemare, marc g., sriram srinivasan, georg ostrovski, tom schaul, david saxton, and remi munos. "unifying count-based 
exploration and intrinsic motivation." arxiv preprint arxiv:1606.01868 (2016).
odena, augustus. "semi-supervised learning with id3." arxiv preprint arxiv:1606.01583 (2016).
springenberg, jost tobias. "unsupervised and semi-supervised learning with categorical id3." 
arxiv preprint arxiv:1511.06390 (2015).
alexander (sasha) vezhnevets, mnih, volodymyr, john agapiou, simon osindero, alex graves, oriol vinyals, and koray 
kavukcuoglu. "strategic attentive writer for learning macro-actions." arxiv preprint arxiv:1606.04695 (2016).
gregor, karol, ivo danihelka, alex graves, danilo jimenez rezende, and daan wierstra. "draw: a recurrent neural network for 
image generation." arxiv preprint arxiv:1502.04623 (2015).

references: applications

   

   
   

   

   

   

   

   

   
   

g  mez-bombarelli r, duvenaud d, hern  ndez-lobato jm, aguilera-iparraguirre j, hirzel td, adams rp, aspuru-guzik a. 
automatic chemical design using a data-driven continuous representation of molecules. arxiv preprint arxiv:1610.02415. 2016.
rampasek l, goldenberg a. dr. vae: drug response variational autoencoder. arxiv preprint arxiv:1706.08203. 2017 jun 26.
regier j, miller a, mcauliffe j, adams r, hoffman m, lang d, schlegel d, prabhat m. celeste: variational id136 for a 
generative model of astronomical images. in international conference on machine learning 2015 jun 1 (pp. 2095-2103).
ledig c, theis l, husz  r f, caballero j, cunningham a, acosta a, aitken a, tejani a, totz j, wang z, shi w. photo-realistic single 
image super-resolution using a generative adversarial network. arxiv preprint arxiv:1609.04802. 2016 sep 15.
oord av, dieleman s, zen h, simonyan k, vinyals o, graves a, kalchbrenner n, senior a, kavukcuoglu k. wavenet: a generative 
model for raw audio. arxiv preprint arxiv:1609.03499. 2016 sep 12.
dumoulin v, belghazi i, poole b, lamb a, arjovsky m, mastropietro o, courville a. adversarially learned id136. arxiv preprint 
arxiv:1606.00704. 2016 jun 2.
gregor k, besse f, rezende dj, danihelka i, wierstra d. towards conceptual compression. in advances in neural information 
processing systems 2016 (pp. 3549-3557).
higgins i, matthey l, glorot x, pal a, uria b, blundell c, mohamed s, lerchner a. early visual concept learning with unsupervised 
deep learning. arxiv preprint arxiv:1606.05579. 2016 jun 17.
chiappa s, racaniere s, wierstra d, mohamed s. recurrent environment simulators. arxiv preprint arxiv:1704.02254. 2017 apr 7.
kalchbrenner n, oord av, simonyan k, danihelka i, vinyals o, graves a, kavukcuoglu k. video pixel networks. arxiv preprint 
arxiv:1610.00527. 2016 oct 3.

    wu j, tenenbaum jb, kohli p. neural scene de-rendering., cvpr 2017

references: fully-observed models

   

   
   
   

oord, aaron van den, nal kalchbrenner, and koray kavukcuoglu. "pixel recurrent neural networks." arxiv preprint 
arxiv:1601.06759 (2016).
larochelle, hugo, and iain murray. "the neural autoregressive distribution estimator." in aistats, vol. 1, p. 2. 2011.
uria, benigno, iain murray, and hugo larochelle. "a deep and tractable density estimator." in icml, pp. 467-475. 2014.
veness, joel, kee siong ng, marcus hutter, and michael bowling. "context tree switching." in 2012 data compression 
conference, pp. 327-336. ieee, 2012.
rue, havard, and leonhard held. gaussian markov random fields: theory and applications. crc press, 2005.

   
    wainwright, martin j., and michael i. jordan. "id114, exponential families, and variational id136." foundations and 

trends   in machine learning 1, no. 1-2 (2008): 1-305.

references: latent variable models

   
   

   

   

   

   

   
   

   

   

   

   
   

hyv  rinen, a., karhunen, j., & oja, e. (2004). independent component analysis (vol. 46). john wiley & sons.
gregor, karol, ivo danihelka, andriy mnih, charles blundell, and daan wierstra. "deep autoregressive networks." arxiv preprint 
arxiv:1310.8499 (2013).
ghahramani, zoubin, and thomas l. griffiths. "infinite latent feature models and the indian buffet process." in advances in neural 
information processing systems, pp. 475-482. 2005.
teh, yee whye, michael i. jordan, matthew j. beal, and david m. blei. "hierarchical dirichlet processes." journal of the american 
statistical association (2012).
adams, ryan prescott, hanna m. wallach, and zoubin ghahramani. "learning the structure of deep sparse id114." in 
aistats, pp. 1-8. 2010.
lawrence, neil d. "gaussian process latent variable models for visualisation of high dimensional data." advances in neural 
information processing systems 16.3 (2004): 329-336.
damianou, andreas c., and neil d. lawrence. "deep gaussian processes." in aistats, pp. 207-215. 2013.
mattos, c  sar lincoln c., zhenwen dai, andreas damianou, jeremy forth, guilherme a. barreto, and neil d. lawrence. 
"recurrent gaussian processes." arxiv preprint arxiv:1511.06644 (2015).
salakhutdinov, ruslan, andriy mnih, and geoffrey hinton. "restricted id82s for id185." in 
proceedings of the 24th international conference on machine learning, pp. 791-798. acm, 2007.
saul, lawrence k., tommi jaakkola, and michael i. jordan. "mean field theory for sigmoid belief networks." journal of artificial 
intelligence research 4, no. 1 (1996): 61-76.
frey, brendan j., and geoffrey e. hinton. "variational learning in nonlinear gaussian belief networks." neural computation 11, no. 1 
(1999): 193-213.
durk kingma and max welling. "auto-encoding id58." iclr (2014).   
burda y, grosse r, salakhutdinov r. importance weighted autoencoders. arxiv preprint arxiv:1509.00519. 2015 sep 1.

references: latent variable models (cont)

   
   

ranganath, rajesh, sean gerrish, and david m. blei. "black box variational id136." in aistats, pp. 814-822. 2014.
mnih, andriy, and karol gregor. "neural variational id136 and learning in belief networks." arxiv preprint arxiv:1402.0030 
(2014).
l  zaro-gredilla, miguel. "doubly stochastic id58 for non-conjugate id136." (2014).

   
    wingate, david, and theophane weber. "automated variational id136 in probabilistic programming." arxiv preprint 

   

   

arxiv:1301.1299 (2013).
paisley, john, david blei, and michael jordan. "id58ian id136 with stochastic search." arxiv preprint 
arxiv:1206.6430 (2012).
barber d, de van laar p. variational cumulant expansions for intractable distributions. journal of artificial intelligence research. 
1999;10:435-55.

references: stochastic gradients

   

   
   
   
   
   

   
   
   
   
   

   

   
   

   

   

pierre l   ecuyer, note: on the interchange of derivative and expectation for likelihood ratio derivative estimators, management 
science, 1995   
peter w glynn, likelihood ratio gradient estimation for stochastic systems, communications of the acm, 1990   
michael c fu, gradient estimation, handbooks in operations research and management science, 2006   
ronald j williams, simple statistical gradient-following algorithms for connectionist id23, machine learning, 1992   
paul glasserman, monte carlo methods in financial engineering, 2003   
omiros papaspiliopoulos, gareth o roberts, martin skold, a general framework for the parametrization of id187, 
statistical science, 2007   
michael c fu, gradient estimation, handbooks in operations research and management science, 2006   
rajesh ranganath, sean gerrish, and david m. blei. "black box variational id136." in aistats, pp. 814-822. 2014.   
andriy mnih, and karol gregor. "neural variational id136 and learning in belief networks." arxiv preprint arxiv:1402.0030 (2014).
michalis titsias and miguel l  zaro-gredilla. "doubly stochastic id58 for non-conjugate id136." (2014).   
david wingate and theophane weber. "automated variational id136 in probabilistic programming." arxiv preprint arxiv:1301.1299 
(2013).   
john paisley, david blei, and michael jordan. "id58ian id136 with stochastic search." arxiv preprint arxiv:1206.6430 
(2012).   
durk kingma and max welling. "auto-encoding id58." iclr (2014).   
danilo jimenez rezende, shakir mohamed, daan wierstra. "stochastic id26 and approximate id136 in deep 
generative models." icml (2014).
papaspiliopoulos o, roberts go, sk  ld m. a general framework for the parametrization of id187. statistical science. 
2007 feb 1:59-73.
fan k, wang z, beck j, kwok j, heller ka. fast second order stochastic id26 for variational id136. inadvances in 
neural information processing systems 2015 (pp. 1387-1395).

references: amortised id136

   

   

   

   
   

   

   

dayan, peter, geoffrey e. hinton, radford m. neal, and richard s. zemel. "the helmholtz machine." neural computation 7, no. 5 
(1995): 889-904.   
gershman, samuel j., and noah d. goodman. "amortized id136 in probabilistic reasoning." in proceedings of the 36th annual 
conference of the cognitive science society. 2014.   
danilo jimenez rezende, shakir mohamed, daan wierstra. "stochastic id26 and approximate id136 in deep 
generative models." icml (2014).
durk kingma and max welling. "auto-encoding id58." iclr (2014).   \
heess, nicolas, daniel tarlow, and john winn. "learning to pass expectation propagation messages." in advances in neural 
information processing systems, pp. 3219-3227. 2013.   
jitkrittum, wittawat, arthur gretton, nicolas heess, s. m. eslami, balaji lakshminarayanan, dino sejdinovic, and zolt     an szab    s. 
"kernel-based just-in-time learning for passing expectation propagation messages." arxiv preprint arxiv:1503.02551 (2015).   
korattikara, anoop, vivek rathod, kevin murphy, and max welling. "bayesian dark knowledge." arxiv preprint arxiv:1506.04416 
(2015).

references: structured mean field 

   

   

   

   
   

   

jaakkola, t. s., and jordan, m. i. (1998). improving the mean field approximation via the use of mixture distributions. in learning in 
id114 (pp. 163-173). springer netherlands.   
saul, l.k. and jordan, m.i., 1996. exploiting tractable substructures in intractable networks. advances in neural information 
processing systems, pp.486-492.   
gregor, karol, ivo danihelka, alex graves, danilo jimenez rezende, and daan wierstra. "draw: a recurrent neural network for 
image generation." icml (2015).   
gershman, s., hoffman, m. and blei, d., 2012. nonparametric variational id136. arxiv preprint arxiv:1206.4665.
felix v. agakov, and david barber. "an auxiliary variational method." nips (2004).   rajesh ranganath, dustin tran, and david m. 
blei. "hierarchical variational models." icml (2016).   lars maal  e et al. "auxiliary deep generative models." icml (2016).   tim 
salimans, durk kingma, max welling. "id115 and variational id136: bridging the gap. in international 
conference on machine learning." icml (2015).
maal  e l, s  nderby ck, s  nderby sk, winther o. auxiliary deep generative models. arxiv preprint arxiv:1602.05473. 2016 feb 
17.

references: normalising flows

   

   
   

   

tabak, e. g., and cristina v. turner. "a family of nonparametric density estimation algorithms." communications on pure and 
applied mathematics 66, no. 2 (2013): 145-164.   
rezende, danilo jimenez, and shakir mohamed. "variational id136 with normalizing flows." icml (2015).   
kingma, d.p., salimans, t. and welling, m., 2016. improving variational id136 with inverse autoregressive flow. arxiv preprint 
arxiv:1606.04934.   
dinh, l., sohl-dickstein, j. and bengio, s., 2016. density estimation using real nvp. arxiv preprint arxiv:1605.08803.

references: other variational objectives

   
   
   
   

   

yuri burda, roger grosse, ruslan salakhutidinov. "importance weighted autoencoders." iclr (2015).   
yingzhen li, richard e. turner. "r  nyi divergence variational id136." nips (2016).   
guillaume and balaji lakshminarayanan. "approximate id136 with the variational holder bound." arxiv (2015).   
jos   miguel hern  ndez-lobato, yingzhen li, daniel hern  ndez-lobato, thang bui, and richard e. turner. black-box 
  -divergence minimization. icml (2016).   
rajesh ranganath, jaan altosaar, dustin tran, david m. blei. operator variational id136. nips (2016).

references: discrete latent variable models

   

   

   
   
   

 radford neal. "learning stochastic feedforward networks." tech. rep. crg-tr-90-7: department of computer science, 
university of toronto (1990).   
lawrence k. saul, tommi jaakkola, and michael i. jordan. "mean field theory for sigmoid belief networks." journal of artificial 
intelligence research 4, no. 1 (1996): 61-76.   
karol gregor, ivo danihelka, andriy mnih, charles blundell, and daan wierstra. "deep autoregressive networks." icml (2014).   
rajesh ranganath, linpeng tang, laurent charlin, and david m. blei. "deep exponential families." aistats (2015).   
rajesh ranganath, dustin tran, and david m. blei. "hierarchical variational models." icml (2016).

references: implicit generative models

   
   

   

   

   

   

   

   
   
   

   
   

   

   
   

borgwardt, karsten m., and zoubin ghahramani. "bayesian two-sample tests." arxiv preprint arxiv:0906.4032 (2009).
gutmann, michael, and aapo hyv  rinen. "noise-contrastive estimation: a new estimation principle for unnormalized statistical 
models." aistats. vol. 1. no. 2. 2010.
tsuboi, yuta, hisashi kashima, shohei hido, steffen bickel, and masashi sugiyama. "direct density ratio estimation for 
large-scale covariate shift adaptation." information and media technologies 4, no. 2 (2009): 529-546.
sugiyama, masashi, taiji suzuki, and takafumi kanamori. density ratio estimation in machine learning. cambridge university 
press, 2012.
goodfellow, ian, jean pouget-abadie, mehdi mirza, bing xu, david warde-farley, sherjil ozair, aaron courville, and yoshua 
bengio. "generative adversarial nets." in advances in neural information processing systems, pp. 2672-2680. 2014.
verrelst, herman, johan suykens, joos vandewalle, and bart de moor. "bayesian learning and the fokker-planck machine." in 
proceedings of the international workshop on advanced black-box techniques for nonlinear modeling, leuven, belgium, pp. 
55-61. 1998.
devroye, luc. "random variate generation in one line of code." in proceedings of the 28th conference on winter simulation, pp. 
265-272. ieee computer society, 1996.
mohamed s, lakshminarayanan b. learning in implicit generative models. arxiv preprint arxiv:1610.03483. 2016 oct 11.
gutmann mu, dutta r, kaski s, corander j. likelihood-free id136 via classification. statistics and computing. 2017 mar 13:1-5.
beaumont ma, zhang w, balding dj. approximate bayesian computation in population genetics. genetics. 2002 dec 
1;162(4):2025-35.
arjovsky m, chintala s, bottou l. wasserstein gan. icml 2017.
nowozin s, cseke b, tomioka r. f-gan: training generative neural samplers using variational divergence minimization. in 
advances in neural information processing systems 2016 (pp. 271-279).
bellemare mg, danihelka i, dabney w, mohamed s, lakshminarayanan b, hoyer s, munos r. the cramer distance as a solution 
to biased wasserstein gradients. arxiv preprint arxiv:1705.10743. 2017 may 30.
dumoulin v, belghazi i, poole b, lamb a, arjovsky m, mastropietro o, courville a. adversarially learned id136.
friedman j, hastie t, tibshirani r. the elements of statistical learning. new york: springer series in statistics; 2001.

references: prob. id23

   

   

   

kappen hj. path integrals and symmetry breaking for optimal control theory. journal of statistical mechanics: theory and 
experiment. 2005 nov 30;2005(11):p11011.
rawlik k, toussaint m, vijayakumar s. approximate id136 and stochastic optimal control. arxiv preprint arxiv:1009.3958. 2010 
sep 20.
toussaint m. robot trajectory optimization using approximate id136. inproceedings of the 26th annual international 
conference on machine learning 2009 jun 14 (pp. 1049-1056). acm.

    weber t, heess n, eslami a, schulman j, wingate d, silver d. reinforced variational id136. in advances in neural information 

   
   
   

   

processing systems (nips) workshops 2015.
rajeswaran a, lowrey k, todorov e, kakade s. towards generalization and simplicity in continuous control.
peters j, m  lling k, altun y. relative id178 policy search. in aaai 2010 jul 11 (pp. 1607-1612).
furmston t, barber d. variational methods for id23. in proceedings of the thirteenth international conference 
on artificial intelligence and statistics 2010 mar 31 (pp. 241-248).
ho j, ermon s. generative adversarial imitation learning. in advances in neural information processing systems 2016 (pp. 
4565-4573

