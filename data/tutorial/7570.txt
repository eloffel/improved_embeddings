opportunities and challenges in deep 

learning for information retrieval 

hang li 

noah   s ark lab, huawei technologies 

talk outline 

    introduction to huawei noah   s ark lab 
    deep learning     new opportunities for information 

retrieval 

    three useful deep learning tools 
    information retrieval tasks 

    id162 
    retrieval-based id53 
    generation-based id53 
    id53  from knowledge base 
    id53 from database 

    discussions and concluding remarks 

noah   s ark lab is research lab working on 

intelligent  

mobile 
devices 

data mining  

& 

artificial 

intelligence 

intelligent 
telecommunication 
networks 

intelligent enterprise 

intelligent telecommunication networks 

software-defined 

network 

network 

maintenance 

network 

planning and 
optimization 

intelligent mobile devices 

information 

recommendation 

information 
extraction 

personal 

information 
management 

machine 

translation 

natural language 
dialogue(question 

answering) 

intelligent enterprise 

supply chain 
management 

customer 

relationship 
management 

information and 

knowledge 
management 

human resources 

management 

communication 

talk outline 

    introduction to huawei noah   s ark lab 
    deep learning     new opportunities for information 

retrieval 

    three useful deep learning tools 
    information retrieval tasks 

    id162 
    retrieval-based id53 
    generation-based id53 
    id53 from database 
    id53  from knowledge base 

    discussions and concluding remarks 

overview of information retrieval 

content: 
documents, 
images, 
knowledge 

information 

retrieval 
system 

information and 
knowledge base 

intent: 
key words, 
question 

query 

relevant 

result 

key questions:  how to represent intent and 
content, how to match intent and content 

key is matching 

intent 

content 

matching 

   indexing:  for efficient retrieval 
   ranking:  when there are multiple result 
   generation:  when only return single answer 

approach in traditional ir 

query: 
star wars the force awakens reviews 

document: 

star wars: episode vii 

three decades after the defeat of 
the galactic empire, a new threat 

arises.   

    representing query and document as word vectors 
    calculating cosine similarity between them 

||||||||,),(dqdqdqfvsm                                          001   q                                    101   d),(dqfapproach in modern ir 

query: 
star wars the force awakens reviews 

(star wars) 
(the force awakens)  
(reviews) 

document: 

star wars: episode vii 

three decades after the defeat of 
the galactic empire, a new threat 

arises.   

    conducting query and document understanding 
    representing query and document as multiple feature vectors 
    calculating multiple matching scores between query and document 
    training ranker with matching scores as features using learning to rank 

qmqvvq   1dndvvd   1),(dqf   examples of query document mismatch 

query 

document 

term 
matching 

semantic 
matching 

seattle best hotel  

pool schedule 

natural logarithm 
transformation  

seattle best 
hotels 

swimmingpool 
schedule 

logarithm 
transformation 

no 

no 

yes 

yes 

partial 

yes 

china kong  

china hong kong  partial 

no 

why are windows so 
expensive  

why are macs so 
expensive 

partial 

no 

13 

hard problems in ir and nlp 

how far is sun from earth? 

id53 

how tall is yao ming? 

id53 from 
relational database 

the average distance between the 
sun and the earth is about 
92,935,700 miles. 

name 

height 

weight 

yao ming  2.29m 

134kg 

liu xiang 

1.89m 

85kg 

a dog catching a ball 

id162 

key questions:  how to represent intent and 
content, how to match intent and content 

representation and matching are key 

problems in ir 

intent 

matching 

content 

deep learning 

recent progress: deep learning enables representation 
learning and matching in ir 

talk outline 

    introduction to huawei noah   s ark lab 
    deep learning     new opportunities for information 

retrieval 

    three useful deep learning tools 
    information retrieval tasks 

    id162 
    retrieval-based id53 
    generation-based id53 
    id53  from knowledge base 
    id53 from database 

    discussions and concluding remarks 

representation of sentence meaning 

new finding    
this is possible 

mary loves john 

mary is loved by john 

john loves mary 

using high-dimensional real-valued vectors 
to represent the meaning of sentences 

word representation: neural id27 

(mikolov et al., 2013) 

1 

2 

10 

2.5 

5 

1 

1 

id105 

7 

1 

1 

2 

1.5 

1 

3 

2 

id27 
or id97 

)()(),(logcpwpcwp1w2w3w1c2c3c4c5c1w2w3w1t2t3ttwcm   wmconvolutional neural network (id98) 

(hu et al., 2014) 

concatenation 

the cat sat on the mat 

the cat  

sat on 

the cat sat  

       

sat on  

the mat 

on the mat 

max pooling 

the cat  

cat sat  

cat sat  

sat on  

sat on 

on the 

on the 

the mat 

the cat sat  

cat sat on 

sat on the 

on the mat 

convolution 

    the                cat             sat                 on              the                mat 

recurrent neural network (id56) 

(mikolov et al. 2010) 

    on sequence of words 
    variable length 
    long dependency: lstm or gru 

the cat sat on the mat 

the                           cat                           sat                                      .                      mat 

),(1tttxhfh      tx1   thtalk outline 

    introduction to huawei noah   s ark lab 
    deep learning     new opportunities for information 

retrieval 

    three useful deep learning tools 
    information retrieval tasks 

    id162 
    retrieval-based id53 
    generation-based id53 
    id53  from knowledge base 
    id53 from database 

    discussions and concluding remarks 

dl for nlp @noah lab 

zhengdong lu 

xin jiang 

lin ma 

lifeng shang 

zhaopeng tu 

id162 

id162 

    scenario 

    image search on 

smartphone 

    key: matching 

queries to images 

    technology 

    deep model for 

matching text and 
image 
 

image representation 

matching 

model 

query representation 

index of 
images 

find the picture that i had 
dinner with my friends at an 
italian restaurant in hong 
kong 

deep match model for image and text 
    represent text and image as vectors and then match 

the two vectors 

    word-level matching, phrase-level matching, 

sentence-level matching 

    our model (id98) work better than state of the art 

models (id56) 
 

 mlp 

id98 

id98 

a dog is catching a ball 

ma et al.,  iccv 2015 

word-level matching model 

 mlp 

       

  

  

  

  
  
  

  
  

id98 

   a            dog          is      catching    a           ball 

    adding image vector to word vectors 

sentence-level matching 

 mlp 

id98 

       

  

  

  

  
  
  

  
  

   a            dog        is     catching       a           ball 

    combing image vector with sentence vector 

demo 

experimental result 

flickr 30k images 

our id98 model outperforms all existing models using id56 

retrieval based question 

answering 

retrieval-based id53 

repository of question 
answer pairs 

retrieval 
system 

learning 
system 

u:  how far is huawei headquarter 
from hong kong science park?  
p:  about 30km 
u:  how can i get there? 
p:  you can first take mtr train to lo 
ma chow and then take a taxi 
 
 

id53 system 
- retrieval based approach 

retrieved 
questions and  
answers 

 
matching 

matched 
answers 

ranked 
answers 

 

ranking 

 

retrieval 

best  
answer 

index of 

questions and 

answers 

 

matching 
models 

 
ranking 
model 

question 

online 

offline 

deep match id98 
- architecture i 

    first represent two sentences as vectors, and then 

match the vectors 

 mlp 

       

  

  

  

  
  
  

  
  

       

  

  

  

  
  
  

  
  

hu et al.,  nips 2014 

deep match id98  
- architecture ii 

    represent and match two sentences simultaneously 
    two dimensional model 

sentence x 

1d   
convolution 

max-
pooling 

2d   
convolution 

more 2d  
convolution &  
pooling 

mlp 

 

 

y
e
c
n
e
t
n
e
s

    

matching  
degree 

34 

retrieval based approach:   

accuracy = 70% 

                           
              
 
                               
 
                            
          
 
 
                     ~       

it is very hot in shanghai 
today, just like singapore . 
 
it is unusually hot. 
 
i want to go to mountain 
wudang, it there anybody 
going together with me? 
 
haha, i want to go with you, 
handsome boy 

using 5 million weibo data 

generation based question 

answering 

generation-based id53 

generation 

system 

learning 
system 

u:  how far is huawei headquarter 
from hong kong science park?  
p:  about 30km 
u:  how can i get there? 
p:  you can first take mtr train to lo 
ma chow and then take a taxi 
 
 

natural language dialogue system 

- generation based approach 

answer 

decoder 

context 
generator 

encoder 

    encoding questions to 

intermediate 
representations 

    decoding intermediate 

representations to 
answers 

    recurrent neural 

network (id56)  

question  

shang et al.,  acl 2015 

txxx   21   xtyyy   21   ychencoder 

local encoder 

global encoder 

    

    

1x2xtxtx1h1   ththth1   tstcdecoder 

    

    

1c2ctctc1s1   tststs1y1   tytytygeneration based approach 

accuracy = 76% 

                         
occupy central is finally over. 
 
  
 
 
                            
will lujiazui (finance district in 
shanghai) be the next? 
 
 
 
                         
i want to buy a samsung phone 
 
 
 
 
                                  
 let us support our national brands 
 
 

vs. accuracy of translation approach = 26% 
      accuracy of retrieval based approach = 70% 

demo 

id53 from 

knowledge base 

id53 from knowledge base 

q: how tall is yao ming? 
a: he is 2.29m tall and is visible from space. 
(yao ming, height, 2.29m) 
 
q: which country was beethoven from? 
a: he was born in what is now germany. 
(ludwig van beethoven, place of 
birth, germany) 

learning system 

knowledge base 

(yao-ming, spouse, ye-li) 
(yao-ming, born, shanghai) 
(yao-ming,  height, 2.29m) 
        
(ludwig van beethoven, place of 
birth, germany) 
        
 

q: how tall is liu xiang? 

id53 

a: he is 1.89m tall 

system 

   

   

interpreter:  creates 
representation of 
question using id56 
enquirer:  retrieves top k 
triples with highest 
matching scores using 
id98 model 

    generator: generates 

answer based on 
question and retrieved 
triples using attention-
based id56 

    attention model:  

controls generation of 
answer 

genqa 

attention 

model 

he is 2.29m tall 

generator 

enquirer 

short term memory 

interpreter 

long term 
memory 

(knowledge base) 

how tall is yao ming? 

key idea:   
    generation of answer based on question and 
retrieved result 
    combination of neural processing and symbolic 
processing 

yin et al.  2015 

enquirer: retrieval and matching 

question 
and its embedding 

matching 

(how, tall, is, liu, xiang) 

retrieved top k triples 
and their embeddings 

< liu xiang, height, 1.90m> 
 
< yao ming, height, 2.26m> 
        
 
<liu xiang, birth place, shanghai> 

    retaining both symbolic representations and vector representations 
    using question words to retrieve top k triples 
    calculating matching scores between question and triples using id98 
model 
    finding best matched triples 

generator: answer generation 

he 
    

    

    

is 

tall 

2.29m 

    

< yao ming, height, 2.29m> 

how 

tall 

is 

yao 

ming 

? 

    generating answer using attention mechanism 
    at each position, a classifier decides whether to 
generate  a word  or use the object of top triple 

2s3s3c03   z13   z3z3y3y2yoexperimental results 

accuracy = 52% 

 

60k triples and 700k qa pairs for training 

id53 from 

relational database 

id53 from relational database 

q: how many people participated in the 
game in beijing? 
a: 4,200 
sql: select #_participants, where city=beijing 
q: when was the latest game hosted? 
a: 2012 
sql: argmax(city, year) 

year 

relational database 

city 

#_days 

#_medals 

2000 

2004 

2008 

2012 

sydney 

athens 

beijing 

london 

20 

35 

30 

40 

2,000 

1,500 

2,500 

2,300 

id53 

system 

a:  athens 

learning system 

q: which city 
hosted the longest 
olympic game 
before the game in 
beijing? 

neural enquirer 

    query encoder:  encoding query 
    table encoder: encoding entries in table 
    five executors: executing query against table 

yin et al.  2015 

query encoder and table encoder 

query encoder 

table encoder 

query representation 

entry representation 

table representation 

id56 

dnn 

    

query 

field  value 

    creating query embedding using id56 
    creating entry table embedding for each entry using 
dnn 

executors 

select #_participants where city = beijing 

    five layers, except last layer, each layer has reader, 
annotator, and memory  
    reader fetches important representation for each 
row,  e.g.,  city=beijing 
    annotator encodes result representation for each 
row, e.g.,  row where city=beijing 

experimental results 

    experiments on synthetic data 
    outperforms semantic parser 

talk outline 

    introduction to huawei noah   s ark lab 
    deep learning     new opportunities for information 

retrieval 

    three useful deep learning tools 
    information retrieval tasks 

    id162 
    retrieval-based id53 
    generation-based id53 
    id53  from knowledge base 
    id53 from database 

    discussions and concluding remarks 

discussions 

    key is to combine symbolic processing and 

neural processing 

    advantage of symbolic processing:  direct, 

effective, and easy to control 

    advantage of neural processing: flexible, 

robust, and completely data-driven 

    challenge:  difficult to make the combination 

summary 

    matching is key for information retrieval 
    deep learning provides new opportunities for ir 
    can learn better representations for matching 
    information retrieval tasks 

    id162 
    retrieval-based id53 
    generation-based id53 
    id53  from knowledge base 
    id53 from database key question:  how to 

combine symbolic processing and neural processing 

    future relies on combination of symbolic processing 

and neural processing 

references 

    baotian hu, zhengdong lu, hang li, qingcai chen. convolutional neural 

network architectures for matching natural language sentences. nips'14, 
2042-2050, 2014.  
lifeng shang, zhengdong lu, hang li. neural responding machine for short 
text conversation. acl-ijcnlp'15, 2015.  
lin ma, zhengodng lu, lifeng shang, hang li . multimodal convolutional 
neural networks for matching image and sentence, iccv   15, 2015. 

   

   

    pengcheng yin, zhengdong lu, hang li, ben kao. neural enquirer: learning 

   

to query tables. arxiv, 2015 
jun yin, xin jiang, zhengdong lu, lifeng shang, hang li, xiaoming li. neural 
generative id53. arxiv, 2015. 
 

thank you! 

