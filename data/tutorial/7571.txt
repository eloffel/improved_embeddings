deep learning systems at scale

soumith chintala
facebook ai research

ibm supercomputing pic, t.j. watson research center
8th june, 2016

talk overview

1.deep learning
2. algorithms 
- convolution nets 
- recurrent nets
3. hardware 
- cpus 
- gpus 

- nvidia 
- amd 
- other 

- xeon phi

4. scale & optimizations 
- multi-core cpu 
- single gpu 
- single machine 
- multi-machine

5. computing paradigms 
- collectives / mpi 

- torch, ca   e 

- graph compilation 

- theano, mxnet, tensorflow,  
- ca   e2, purine

deep learning

image intelligence: classi   cation

image intelligence : detection

image intelligence : detection

image intelligence

https://code.facebook.com/posts/accessibility/

video intelligence

image and video generation
predicting the future

natural language understanding

    language translation 
    reading, writing and answering questions 
    chatbots / personal assistants

algorithms

convolution neural networks 
(convnets)

2d convolution

convolution neural networks 
(convnets)

2d convolution

convnet

convolution neural networks 
(convnets)

convnet

convolution neural networks 
(convnets)

recurrent neural networks

recurrent neural networks

unfolded 

in 
time

recurrent neural networks

lstm cell

hardware

hardware

- cpus 
- gpus 
- nvidia 
- amd 
- other

cpus

- multi-core cpu evaluations 
- limitations 
- 1 or 2 cpus per machine 
- poor theoretical (and practical) peak

gpu-powered convnets

gpu-powered convnets

gpu-powered convnets

alex khrizevsky

other hardware

    intel xeon phi 
    nervana unnamed 
    google tpu 
    graphcore 
    teradeep 
    other custom chips (yunji chen et. al.)

hardware

big sur
open compute for deep learning

    serviceability 
    thermal ef   ciency 
    performance

big sur
open compute for deep learning

hot swappable fan modules 

gpu removal using 2 
thumb screws

swap pci-e topologies  
with incredible ease 

removable 
motherboard tray 

removable gpu 
baseboard

cables to change 
topologies 

rails for in-rack 
servicing

2.5    drive carriers 

big sur
pci-e topologies     matter!

big sur
pci-e topologies     matter!

scale

scale

- multicore cpu 
- single gpu 
- single machine multi-gpu 
- multi-machine multi-gpu

basic optimizations
for convnets
    convolutions, gemm take ~90% of wall-time 
    faster convolutions = faster research 
    parallelize over the mini-batch of input samples 
    asynchronous hogwild training 
    trivially easy to implement and use 
    linear scaling guaranteed (wall-time)

multicore cpu optimizations

    fast convolutions and gemm 
    simd and threading 
    fft and winograd transforms 
    parallelization over mini-batch 
    asynchronous hogwild training

single gpu optimizations

    fast convolutions and gemm 
    fft and winograd transforms 
    parallelization over mini-batch

single gpu optimizations
fft based convolutions

single gpu optimizations
winograd transform based convolutions

single gpu optimizations

winograd transform based convolutions

single gpu optimizations

    the standard in deep learning: 

nvidia gpus + cuda + cudnn 

multi-gpu optimizations
    multiple gpus on single machine

multi-gpu optimizations

    data parallel

multi-gpu optimizations

    model parallel

multi-gpu optimizations

    pipeline-parallel

multi-gpu optimizations

    bottleneck: interconnects 
    reduce transfer precision 
    faster interconnects 
    di   erent pci-e topologies

multi-machine training

    multi-machine sgd

send gradients

multi-machine training

    multi-machine sgd

send weights

multi-machine training

    elastic averaging sgd! (sixin zhang, anna choromanska, yann lecun)

multi-machine training

    elastic averaging sgd!

train synchronously 
occasionally, check with master 
dont go too far from everyone else

multi-machine training

    elastic averaging sgd!

train synchronously 
occasionally, check with neighbors 
dont go too far from everyone else

multi-machine training

    elastic averaging sgd! 
    empirical speedup of squareroot(n) 
    n = number of nodes 
    no communication overhead with pre-fetching 
    128 gpus (32 clients * 4 gpus) 
    sharded parameters over 64 cpu servers 
    tau = 10, prefetch = 5 
    zero overhead

multi-machine training

    elastic averaging sgd! 
    fun fact: trained alexnet in 5 epochs of id163 data 
    good success in training vision and text networks

computing paradigms

computing paradigms

    collectives / mpi 
    torch, ca   e 
    graph compilation 
    theano, mxnet, tensorflow,  
    ca   e2, purine

computing paradigms
e   cient collectives + imperative programs
    data / model / pipeline parallel seems su   cient 
    allreduce and other mpi collectives 
    torch (nn / autograd / distlearn) 
    ca   e

computing paradigms
computational graph toolkits
    intel cnc, ca   e2, tensorflow, mxnet, theano 
    graph placement hints + execution 
    dsls to write the computation graphs

computation graph

de   ne graph in a dsl

send to engine 
to be executed

computation 

engine

computing paradigms
computational graph toolkits

computation engine

optimizer

placement

engine

comms
nodes

silver bullet
imperative language + graph compiler
    best of both worlds 
    hard problem of automatic graph placement 
    limited heuristic-driven success

