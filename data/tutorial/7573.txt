deep id23

david silver, google deepmind

id23: ai = rl

rl is a general-purpose framework for arti   cial intelligence

(cid:73) rl is for an agent with the capacity to act
(cid:73) each action in   uences the agent   s future state
(cid:73) success is measured by a scalar reward signal

rl in a nutshell:

(cid:73) select actions to maximise future reward

we seek a single agent which can solve any human-level task

(cid:73) the essence of an intelligent agent

agent and environment

(cid:73) at each step t the agent:

(cid:73) receives state st
(cid:73) receives scalar reward rt
(cid:73) executes action at

(cid:73) the environment:

(cid:73) receives action at
(cid:73) emits state st
(cid:73) emits scalar reward rt

staterewardactionatrtstexamples of rl

(cid:73) control physical systems: walk,    y, drive, swim, ...
(cid:73) interact with users: retain customers, personalise channel,

optimise user experience, ...

(cid:73) solve logistical problems: scheduling, bandwidth allocation,

elevator control, cognitive radio, power optimisation, ..

(cid:73) play games: chess, checkers, go, atari games, ...
(cid:73) learn sequential algorithms: attention, memory, conditional

computation, activations, ...

policies and value functions

(cid:73) policy    is a behaviour function selecting actions given states

a =   (s)

(cid:73) value function q   (s, a) is expected total reward

from state s and action a under policy   

q   (s, a) = e(cid:2)rt+1 +   rt+2 +   2rt+3 + ... | s, a(cid:3)

   how good is action a in state s?   

approaches to id23

policy-based rl

(cid:73) search directly for the optimal policy      
(cid:73) this is the policy achieving maximum future reward

value-based rl

(cid:73) estimate the optimal value function q   (s, a)
(cid:73) this is the maximum value achievable under any policy

model-based rl

(cid:73) build a transition model of the environment
(cid:73) plan (e.g. by lookahead) using model

deep id23

(cid:73) can we apply deep learning to rl?
(cid:73) use deep network to represent value function / policy / model
(cid:73) optimise value function / policy /model end-to-end
(cid:73) using stochastic id119

bellman equation

(cid:73) value function can be unrolled recursively

q   (s, a) = e(cid:2)rt+1 +   rt+2 +   2rt+3 + ... | s, a(cid:3)

= es(cid:48)(cid:2)r +   q   (s(cid:48), a(cid:48)) | s, a(cid:3)

(cid:73) optimal value function q   (s, a) can be unrolled recursively

q   (s, a) = es(cid:48)

r +    max

a(cid:48) q   (s(cid:48), a(cid:48)) | s, a

(cid:73) value iteration algorithms solve the bellman equation
a(cid:48) qi (s(cid:48), a(cid:48)) | s, a

qi+1(s, a) = es(cid:48)

r +    max

(cid:20)

(cid:20)

(cid:21)

(cid:21)

deep id24

(cid:73) represent value function by deep q-network with weights w

q(s, a, w )     q   (s, a)

(cid:73) de   ne objective function by mean-squared error in q-values

l(w ) = e

    q(s, a, w )

            
            r +    max
(cid:124)
(cid:123)(cid:122)
(cid:125)
a(cid:48) q(s(cid:48), a(cid:48), w )
(cid:20)(cid:18)

target

(cid:73) leading to the following id24 gradient

   l(w )
   w

= e

r +    max

a(cid:48) q(s(cid:48), a(cid:48), w )     q(s, a, w )
(cid:73) optimise objective end-to-end by sgd, using    l(w )
   w

            2            
(cid:19)    q(s, a, w )

(cid:21)

   w

stability issues with deep rl

naive id24 oscillates or diverges with neural nets

1. data is sequential

(cid:73) successive samples are correlated, non-iid

2. policy changes rapidly with slight changes to q-values

(cid:73) policy may oscillate
(cid:73) distribution of data can swing from one extreme to another

3. scale of rewards and q-values is unknown
(cid:73) naive id24 gradients can be large

unstable when backpropagated

deep q-networks

id25 provides a stable solution to deep value-based rl

1. use experience replay

(cid:73) break correlations in data, bring us back to iid setting
(cid:73) learn from all past policies

2. freeze target q-network

(cid:73) avoid oscillations
(cid:73) break correlations between q-network and target

3. clip rewards or normalize network adaptively to sensible range

(cid:73) robust gradients

stable deep rl (1): experience replay

to remove correlations, build data-set from agent   s own experience

(cid:73) take action at according to  -greedy policy
(cid:73) store transition (st, at, rt+1, st+1) in replay memory d
(cid:73) sample random mini-batch of transitions (s, a, r , s(cid:48)) from d
(cid:73) optimise mse between q-network and id24 targets, e.g.

l(w ) = es,a,r ,s(cid:48)   d

r +    max

a(cid:48) q(s(cid:48), a(cid:48), w )     q(s, a, w )

(cid:19)2(cid:35)

(cid:34)(cid:18)

stable deep rl (2): fixed target q-network

to avoid oscillations,    x parameters used in id24 target

(cid:73) compute id24 targets w.r.t. old,    xed parameters w   

r +    max

a(cid:48) q(s(cid:48), a(cid:48), w   )

(cid:73) optimise mse between q-network and id24 targets

(cid:34)(cid:18)

(cid:19)2(cid:35)
a(cid:48) q(s(cid:48), a(cid:48), w   )     q(s, a, w )

l(w ) = es,a,r ,s(cid:48)   d

r +    max

(cid:73) periodically update    xed parameters w        w

stable deep rl (3): reward/value range

(cid:73) id25 clips the rewards to [   1, +1]
(cid:73) this prevents q-values from becoming too large
(cid:73) ensures gradients are well-conditioned
(cid:73) can   t tell di   erence between small and large rewards

id23 in atari

staterewardactionatrtstid25 in atari

(cid:73) end-to-end learning of values q(s, a) from pixels s
(cid:73) input state s is stack of raw pixels from last 4 frames
(cid:73) output is q(s, a) for 18 joystick/button positions
(cid:73) reward is change in score for that step

network architecture and hyperparameters    xed across all games
[mnih et al.]

id25 results in atari

id25 demo

how much does id25 help?

id24

id24 id24
+ replay

+ target q
10
142
2868
1003
373

3
29
1453
276
302

241
831
4103
823
826

breakout
enduro
river raid
seaquest
space invaders

id25

id24
+ replay
+ target q
317
1006
7447
2894
1089

normalized id25

(cid:73) normalized id25 uses true (unclipped) reward signal
(cid:73) network outputs a scalar value in    stable    range,

u(s, a, w )     [   1, +1]

(cid:73) output is scaled and translated into q-values,

q(s, a, w ,   ,   ) =   u(s, a, w ) +   
(cid:73)   ,    are adapted to ensure u(s, a, w )     [   1, +1]
(cid:73) network parameters w are adjusted to keep q-values constant

  1u(s, a, w1) +   1 =   2u(s, a, w2) +   2

demo: normalized id25 in pacman

gorila (google id23 architecture)

(cid:73) parallel acting: generate new interactions
(cid:73) distributed replay memory: save interactions
(cid:73) parallel learning: compute gradients from replayed interactions
(cid:73) distributed neural network: update network from gradients

id25 losstarget qnetworklearnerq(s,a;   )gradientwrt lossrq networkmaxa    q(s   ,a   ;      )shard k-1shard k+1shard kparameter servergradientsyncsync(s,a)sync everyglobal n stepss   store(s,a,r,s   )bundledmodereplaymemoryq networkactorenvironmentargmaxa q(s,a;   )sstable deep rl (4): parallel updates

vanilla id25 is unstable when applied in parallel. we use:

(cid:73) reject stale gradients
(cid:73) reject outlier gradients g >    + k  
(cid:73) adagrad optimisation

gorila results

using 100 parallel actors and learners

(cid:73) gorila signi   cantly outperformed vanilla id25

(cid:73) on 41 out of 49 atari games

(cid:73) gorila achieved x2 score of vanilla id25

(cid:73) on 22 out of 49 atari games

(cid:73) gorila matched vanilla id25 results 10x faster

(cid:73) on 38 out of 49 atari games

gorila id25 results in atari: time to beat id25

012345601020304050highestbeatingtime (days)gamesdeterministic policy gradient for continuous actions

(cid:73) represent deterministic policy by deep network a =   (s, u)

with weights u

(cid:73) de   ne objective function as total discounted reward

(cid:73) optimise objective end-to-end by sgd

j(u) = e(cid:2)r1 +   r2 +   2r3 + ...(cid:3)
(cid:21)

(cid:20)    q   (s, a)

     (s, u)

   j(u)

= es

   u

   a

   u

(cid:73) update policy in the direction that most improves q
(cid:73) i.e. backpropagate critic through actor

deterministic actor-critic

use two networks: an actor and a critic

(cid:73) critic estimates value of current policy by id24

(cid:20)(cid:18)

   l(w )
   w

= e

r +   q(s(cid:48),   (s(cid:48)), w )     q(s, a, w )

(cid:73) actor updates policy in direction that improves q

(cid:20)    q(s, a, w )

   j(u)

   u

= es

     (s, u)

   a

   u

(cid:21)

(cid:19)    q(s, a, w )
(cid:21)

   w

deterministic deep actor-critic

(cid:73) naive actor-critic oscillates or diverges with neural nets
(cid:73) ddac provides a stable solution

1. use experience replay for both actor and critic
2. use target q-network to avoid oscillations

   l(w )
   w
   j(u)

   u

= es,a,r ,s(cid:48)   d

= es,a,r ,s(cid:48)   d

(cid:20)(cid:18)
(cid:20)    q(s, a, w )

(cid:21)

     (s, u)

   a

   u

r +   q(s(cid:48),   (s(cid:48)), w   )     q(s, a, w )

(cid:19)    q(s, a, w )

(cid:21)

   w

ddac for continuous control

(cid:73) end-to-end learning of control policy from raw pixels s
(cid:73) input state s is stack of raw pixels from last 4 frames
(cid:73) two separate convnets are used for q and   
(cid:73) physics are simulated in mujoco

[lillicrap et al.]

q(s,a)  (s)addac demo

model-based rl

learn a transition model of the environment

p(r , s(cid:48) | s, a)

plan using the transition model

(cid:73) e.g. lookahead using transition model to    nd optimal actions

rightleftrightrightleftleftdeep models

(cid:73) represent transition model p(r , s(cid:48) | s, a) by deep network
(cid:73) de   ne objective function measuring goodness of model
(cid:73) e.g. number of bits to reconstruct next state
(cid:73) optimise objective by sgd

(gregor et al.)

darn demo

challenges of model-based rl

compounding errors

(cid:73) errors in the transition model compound over the trajectory
(cid:73) by the end of a long trajectory, rewards can be totally wrong
(cid:73) model-based rl has failed (so far) in atari

deep networks of value/policy can    plan    implicitly

(cid:73) each layer of network performs arbitrary computational step
(cid:73) n-layer network can    lookahead    n steps
(cid:73) are transition models required at all?

deep learning in go

monte-carlo search

(cid:73) monte-carlo search (mcts) simulates future trajectories
(cid:73) builds large lookahead search tree with millions of positions
(cid:73) state-of-the-art 19    19 go programs use mcts
(cid:73) e.g. first strong go program mogo

(gelly et al.)

convolutional networks

(cid:73) 12-layer convnet trained to predict expert moves
(cid:73) raw convnet (looking at 1 position, no search at all)
(cid:73) equals performance of mogo with 105 position search tree

program
human 6-dan
12-layer convnet
8-layer convnet*
prior state-of-the-art

accuracy
    52%
55%
44%
31-39%

*clarke & storkey

(maddison et al.)

program
gnugo
mogo (100k)
pachi (10k)
pachi (100k)

winning rate
97%
46%
47%
11%

conclusion

(cid:73) rl provides a general-purpose framework for ai
(cid:73) rl problems can be solved by end-to-end deep learning
(cid:73) a single agent can now solve many challenging tasks
(cid:73) id23 + deep learning = ai

questions?

   the only stupid question is the one you never ask    -rich sutton

