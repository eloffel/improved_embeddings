tutorial: deep id23

david silver, google deepmind

outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

id23 in a nutshell

rl is a general-purpose framework for decision-making

i rl is for an agent with the capacity to act
i each action in   uences the agent   s future state
i success is measured by a scalar reward signal
i goal: select actions to maximise future reward

deep learning in a nutshell

dl is a general-purpose framework for representation learning

i given an objective
i learn representation that is required to achieve objective
i directly from raw inputs
i using minimal domain knowledge

deep id23: ai = rl + dl

we seek a single agent which can solve any human-level task

i rl de   nes the objective
i dl gives the mechanism
i rl + dl = general intelligence

examples of deep rl @deepmind

i play games: atari, poker, go, ...
i explore worlds: 3d worlds, labyrinth, ...
i control physical systems: manipulate, walk, swim, ...
i interact with users: recommend, optimise, personalise, ...

outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

deep representations

i a deep representation is a composition of many functions

x

/ h1

w1

/ ...

...

/ hn

/ y

/ l

wn

i its gradient can be backpropagated by the chain rule

@l
@x

@h1
@x

@l
@h1

@h2
@h1

@h1
@w1

@l
@w1

...

...

@hn
@hn 1

@l
@hn

@y
@hn

@l
@y

@hn
@wn

@l
@wn

/
/
/
/
/
o
o
o
o
o
o
   
   
o
o
o
o
   
   
o
o
deep neural network

a deep neural network is typically composed of:

i linear transformations

hk+1 = whk

i non-linear id180

hk+2 = f (hk+1)

i a id168 on the output, e.g.
i mean-squared error l = ||y      y||2
i log likelihood l = log p [y   ]

training neural networks by stochastic id119
!"#$%"&'(%")*+,#'-'!"#$%%" (%")*+,#'.+/0+,#

(cid:1) !"#$%&'('%$&#()&*+$,*$#&&-&$$$$."'%"$
'*$%-/0'*,('-*$.'("$("#$1)*%('-*$
i sample gradient of expected loss l(w) = e [l]
%,*$0#$)+#4$(-$
,22&-3'/,(-&
@l(w)
@l
%&#,(#$,*$#&&-&$1)*%('-*$$$$$$$$$$$$$

@w     e    @l

@w  =

@w

i adjust w down the sampled gradient

(cid:1) !"#$2,&(',5$4'11#&#*(',5$-1$("'+$#&&-&$

 w /

1)*%('-*$$$$$$$$$$$$$$$$6$("#$7&,4'#*($
%,*$*-.$0#$)+#4$(-$)24,(#$("#$
'*(#&*,5$8,&',05#+$'*$("#$1)*%('-*$
,22&-3'/,(-& 9,*4$%&'('%:;$$$$$$

@l
@w

<&,4'#*($4#+%#*($=>?

weight sharing

recurrent neural network shares weights between time-steps

yt

/ ht

xt

...

w

yt+1

ht+1

...

w

xt+1

convolutional neural network shares weights between local regions

w1

w1

h1

w2

w2

h2

x

/
/
/
o
o
/
/
o
o
?
?
o
o
=
=
o
o
outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

many faces of id23

computer science

engineering

neuroscience

machine 
learning

optimal 
control

reward
system

reinforcement 

learning

operations 
research

classical/operant

conditioning

mathematics

rationality/
game theory

psychology

economics

agent and environment

observation

ot

action
at

reward

rt

i at each step t the agent:

i executes action at
i receives observation ot
i receives scalar reward rt

i the environment:

i receives action at
i emits observation ot+1
i emits scalar reward rt+1

state

i experience is a sequence of observations, actions, rewards

o1, r1, a1, ..., at 1, ot, rt

i the state is a summary of experience

st = f (o1, r1, a1, ..., at 1, ot, rt)

i in a fully observed environment

st = f (ot)

major components of an rl agent

i an rl agent may include one or more of these components:

i policy: agent   s behaviour function
i value function: how good is each state and/or action
i model: agent   s representation of the environment

policy

i a policy is the agent   s behaviour
i it is a map from state to action:
i deterministic policy: a =    (s)
i stochastic policy:    (a|s) = p [a|s]

value function

i a value function is a prediction of future reward

i    how much reward will i get from action a in state s?   

i q-value function gives expected total reward

i from state s and action a
i under policy    
i with discount factor  

q   (s, a) = e   rt+1 +  rt+2 +  2rt+3 + ... | s, a   

value function

i a value function is a prediction of future reward

i    how much reward will i get from action a in state s?   

i q-value function gives expected total reward

i from state s and action a
i under policy    
i with discount factor  

i value functions decompose into a bellman equation

q   (s, a) = e   rt+1 +  rt+2 +  2rt+3 + ... | s, a   
q   (s, a) = es0,a0   r +  q   (s0, a0) | s, a   

optimal value functions

i an optimal value function is the maximum achievable value

q   (s, a) = max

   

q   (s, a) = q      (s, a)

optimal value functions

i an optimal value function is the maximum achievable value

q   (s, a) = max

   

q   (s, a) = q      (s, a)

i once we have q    we can act optimally,

      (s) = argmax

a

q   (s, a)

optimal value functions

i an optimal value function is the maximum achievable value

q   (s, a) = max

   

q   (s, a) = q      (s, a)

i once we have q    we can act optimally,

      (s) = argmax

a

q   (s, a)

i optimal value maximises over all decisions. informally:

q   (s, a) = rt+1 +   max
at+1
= rt+1 +   max
at+1

rt+2 +  2 max
at+2
q   (st+1, at+1)

rt+3 + ...

optimal value functions

i an optimal value function is the maximum achievable value

q   (s, a) = max

   

q   (s, a) = q      (s, a)

i once we have q    we can act optimally,

      (s) = argmax

a

q   (s, a)

i optimal value maximises over all decisions. informally:

q   (s, a) = rt+1 +   max
at+1
= rt+1 +   max
at+1

rt+2 +  2 max
at+2
q   (st+1, at+1)

rt+3 + ...

i formally, optimal values decompose into a bellman equation

q   (s, a) = es0   r +   max

a0

q   (s0, a0) | s, a 

value function demo

model

observation

ot

action
at

reward

rt

model

observation

ot

action
at

reward

rt

i model is learnt from experience
i acts as proxy for environment
i planner interacts with model
i e.g. using lookahead search

approaches to id23

value-based rl

i estimate the optimal value function q   (s, a)
i this is the maximum value achievable under any policy

policy-based rl

i search directly for the optimal policy       
i this is the policy achieving maximum future reward

model-based rl

i build a model of the environment
i plan (e.g. by lookahead) using model

deep id23

i use deep neural networks to represent

i value function
i policy
i model

i optimise id168 by stochastic id119

outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

q-networks

represent value function by q-network with weights w

q(s, a, w)     q   (s, a)

q(s,a,w)

q(s,a1,w)

   

q(s,am,w)

w

s

a

w

s

id24

i optimal q-values should obey bellman equation

q   (s, a) = es0   r +   max

a0

q   (s0, a0) | s, a 

i treat right-hand side r +   max

q(s0, a0, w) as a target

a0

i minimise mse loss by stochastic id119

l =   r +   max

a0

q(s0, a0, w)   q(s, a, w)   2

id24

i optimal q-values should obey bellman equation

q   (s, a) = es0   r +   max

a0

q   (s0, a0) | s, a 

i treat right-hand side r +   max

q(s0, a0, w) as a target

a0

i minimise mse loss by stochastic id119

l =   r +   max

a0

q(s0, a0, w)   q(s, a, w)   2

i converges to q    using table lookup representation

id24

i optimal q-values should obey bellman equation

q   (s, a) = es0   r +   max

a0

q   (s0, a0) | s, a 

i treat right-hand side r +   max

q(s0, a0, w) as a target

a0

i minimise mse loss by stochastic id119

l =   r +   max

a0

q(s0, a0, w)   q(s, a, w)   2

i converges to q    using table lookup representation
i but diverges using neural networks due to:

i correlations between samples
i non-stationary targets

deep q-networks (id25): experience replay

to remove correlations, build data-set from agent   s own experience

s1, a1, r2, s2
s2, a2, r3, s3
s3, a3, r4, s4

...

! s, a, r , s0

st, at, rt+1, st+1 ! st, at, rt+1, st+1
sample experiences from data-set and apply update

l =   r +   max

a0

q(s0, a0, w )   q(s, a, w)   2

to deal with non-stationarity, target parameters w  are held    xed

deep id23 in atari

state
st

action
at

reward

rt

id25 in atari

i end-to-end learning of values q(s, a) from pixels s
i input state s is stack of raw pixels from last 4 frames
i output is q(s, a) for 18 joystick/button positions
i reward is change in score for that step

network architecture and hyperparameters    xed across all games

id25 results in atari

id25ataridemoid25paperwww.nature.com/articles/nature14236id25sourcecode:sites.google.com/a/deepmind.com/id25/nature.com/nature26 february 2015   10vol. 518, no. 7540epidemiologyshare data in outbreaksforge open access to sequences and more page 477cosmologya giant in the early universea supermassive black hole at a redshift of 6.3pages 490 & 512quantum physicsteleportation for twotransferring two properties of a single photon pages 491 & 516innovations inthe microbiomeself-taught ai software attains human-level performance in video games  pages 486 & 529the international weekly journal of scienceimprovements since nature id25

i double id25: remove upward bias caused by max

a

q(s, a, w)

i current q-network w is used to select actions
i older q-network w  is used to evaluate actions

l =   r +  q(s0, argmax

a0

q(s0, a0, w), w )   q(s, a, w)   2

improvements since nature id25

i double id25: remove upward bias caused by max

a

q(s, a, w)

i current q-network w is used to select actions
i older q-network w  is used to evaluate actions

l =   r +  q(s0, argmax

a0

q(s0, a0, w), w )   q(s, a, w)   2

i prioritised replay: weight experience according to surprise
i store experience in priority queue according to id25 error

   r +   max

a0

q(s0, a0, w )   q(s, a, w )   

improvements since nature id25

i double id25: remove upward bias caused by max

a

q(s, a, w)

i current q-network w is used to select actions
i older q-network w  is used to evaluate actions

l =   r +  q(s0, argmax

a0

q(s0, a0, w), w )   q(s, a, w)   2

i prioritised replay: weight experience according to surprise
i store experience in priority queue according to id25 error

   r +   max

a0

q(s0, a0, w )   q(s, a, w )   

i duelling network: split q-network into two channels

i action-independent value function v (s, v )
i action-dependent advantage function a(s, a, w)

q(s, a) = v (s, v ) + a(s, a, w)

improvements since nature id25

i double id25: remove upward bias caused by max

a

q(s, a, w)

i current q-network w is used to select actions
i older q-network w  is used to evaluate actions

l =   r +  q(s0, argmax

a0

q(s0, a0, w), w )   q(s, a, w)   2

i prioritised replay: weight experience according to surprise
i store experience in priority queue according to id25 error

   r +   max

a0

q(s0, a0, w )   q(s, a, w )   

i duelling network: split q-network into two channels

i action-independent value function v (s, v )
i action-dependent advantage function a(s, a, w)

q(s, a) = v (s, v ) + a(s, a, w)

combined algorithm: 3x mean atari score vs nature id25

gorila (general id23 architecture)

i 10x faster than nature id25 on 38 out of 49 atari games
i applied to recommender systems within google

asynchronous id23

i exploits multithreading of standard cpu
i execute many instances of agent in parallel
i network parameters shared between threads
i parallelism decorrelates data

i viable alternative to experience replay

i similar speedup to gorila - on a single machine!

outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

deep policy networks

i represent policy by deep network with weights u

a =    (a|s, u) or a =    (s, u)

i de   ne objective function as total discounted reward

l(u) = e   r1 +  r2 +  2r3 + ... |    (  , u)   

i optimise objective end-to-end by sgd
i i.e. adjust policy parameters u to achieve more reward

policy gradients

how to make high-value actions more likely:

i the gradient of a stochastic policy    (a|s, u) is given by

@l(u)

@u

= e    @log    (a|s, u)

@u

q   (s, a) 

policy gradients

how to make high-value actions more likely:

i the gradient of a stochastic policy    (a|s, u) is given by

@l(u)

@u

= e    @log    (a|s, u)

@u

q   (s, a) 

i the gradient of a deterministic policy a =    (s) is given by

@l(u)

@u

= e    @q   (s, a)

@a

@a

@u 

i if a is continuous and q is di   erentiable

actor-critic algorithm

i estimate value function q(s, a, w)     q   (s, a)
i update policy parameters u by stochastic gradient ascent

or

@l
@u

=

@log    (a|s, u)

@u

q(s, a, w)

@l
@u

=

@q(s, a, w)

@a

@a
@u

asynchronous advantage actor-critic (a3c)

i estimate state-value function

v (s, v)     e [rt+1 +  rt+2 + ...|s]

i q-value estimated by an n-step sample

qt = rt+1 +  rt+2... +  n 1rt+n +  nv (st+n, v)

asynchronous advantage actor-critic (a3c)

i estimate state-value function

v (s, v)     e [rt+1 +  rt+2 + ...|s]

i q-value estimated by an n-step sample

qt = rt+1 +  rt+2... +  n 1rt+n +  nv (st+n, v)

i actor is updated towards target

@lu
@u

=

@log    (at|st, u)

@u

(qt   v (st, v))

i critic is updated to minimise mse w.r.t. target

lv = (qt   v (st, v))2

asynchronous advantage actor-critic (a3c)

i estimate state-value function

v (s, v)     e [rt+1 +  rt+2 + ...|s]

i q-value estimated by an n-step sample

qt = rt+1 +  rt+2... +  n 1rt+n +  nv (st+n, v)

i actor is updated towards target

@lu
@u

=

@log    (at|st, u)

@u

(qt   v (st, v))

i critic is updated to minimise mse w.r.t. target

lv = (qt   v (st, v))2

i 4x mean atari score vs nature id25

deep id23 in labyrinth

a3c in labyrinth

  (a|st-1)

v(st-1)

  (a|st)

v(st)

  (a|st+1)

v(st-1)

st-1

deep id23 in labyrinth

st

st+1

deep id23 in labyrinth

deep id23 in labyrinth

ot-1

ot

ot+1

i end-to-end learning of softmax policy    (a|st) from pixels
i observations ot are raw pixels from current frame
i state st = f (o1, ..., ot) is a recurrent neural network (lstm)
i outputs both value v (s) and softmax over actions    (a|s)
i task is to collect apples (+1 reward) and escape (+10 reward)

a3c labyrinth demo

demo:
www.youtube.com/watch?v=nmr5mjcfzcw&feature=youtu.be

labyrinth source code (coming soon):
sites.google.com/a/deepmind.com/labyrinth/

deep id23 with continuous actions

how can we deal with high-dimensional continuous action spaces?

i can   t easily compute max

a

q(s, a)

i actor-critic algorithms learn without max

i q-values are di   erentiable w.r.t a

i deterministic policy gradients exploit knowledge of @q
@a

deep dpg

dpg is the continuous analogue of id25

i experience replay: build data-set from agent   s experience
i critic estimates value of current policy by id25

lw =   r +  q(s0,    (s0, u ), w )   q(s, a, w)   2

to deal with non-stationarity, targets u , w  are held    xed

i actor updates policy in direction that improves q

@lu
@u

=

@q(s, a, w)

@a

@a
@u

i in other words critic provides id168 for actor

dpg in simulated physics

i physics domains are simulated in mujoco
i end-to-end learning of control policy from raw pixels s
i input state s is stack of raw pixels from last 4 frames
i two separate convnets are used for q and    
i policy     is adjusted in direction that most improves q

a

q(s,a)

  (s)

dpg in simulated physics demo

i demo: dpg from pixels

a3c in simulated physics demo

i asynchronous rl is viable alternative to experience replay
i train a hierarchical, recurrent locomotion controller
i retrain controller on more challenging tasks

fictitious self-play (fsp)

can deep rl    nd nash equilibria in multi-agent games?

i q-network learns    best response    to opponent policies

i by applying id25 with experience replay
i c.f.    ctitious play

i policy network    (a|s, u) learns an average of best responses

@l
@u

=

@log    (a|s, u)

@u

i actions a sample mix of policy network and best response

neural fsp in texas hold   em poker

i heads-up limit texas hold   em
i nfsp with raw inputs only (no prior knowledge of poker)
i vs smooct (3x medal winner 2015, handcrafted knowlege)

-800-700-600-500-400-300-200-100 0 100 0 5e+06 1e+07 1.5e+07 2e+07 2.5e+07 3e+07 3.5e+07mbb/hiterationssmooctnfsp, best response strategynfsp, greedy-average strategynfsp, average strategyoutline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

learning models of the environment

i demo: generative model of atari
i challenging to plan due to compounding errors

i errors in the transition model compound over the trajectory
i planning trajectories di   er from executed trajectories
i at end of long, unusual trajectory, rewards are totally wrong

deep id23 in go

what if we have a perfect model? e.g. game rules are known

alphago paper:
www.nature.com/articles/nature16961

alphago resources:
deepmind.com/alphago/

the international weekly journal of science

at last     a  computer program that 
can beat a champion go player  page 484

all systems go

conservation

songbirds
   la carte
illegal harvest of millions
of mediterranean birds

page 452

research ethics

safeguard 

transparency 
don   t let openness backfire 

on individuals

page 459

nature.com/nature

28 january 2016    10
vol. 529, no. 7587

popular science

when genes 
got    selfish   
dawkins   s  calling
card forty years on

page 462

cover 28 january 2016.indd   1

20/01/2016   15:40

conclusion

i general, stable and scalable rl is now possible
i using deep networks to represent value, policy, model
i successful in atari, labyrinth, physics, poker, go
i using a variety of deep rl paradigms

