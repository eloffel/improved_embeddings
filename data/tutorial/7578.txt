adversarial examples

presentation by ian goodfellow

deep learning summer school 
montreal 
august 9, 2015

google proprietary

in this presentation   .

-

-

-

   intriguing properties of neural networks.    szegedy et 
al., iclr 2014. 
   explaining and harnessing adversarial examples.    
goodfellow et al., iclr 2014. 
   distributional smoothing by virtual adversarial 
examples.    miyato et al arxiv 2015. 

google proprietary

universal engineering machine (model-based optimization)
make new 
inventions by 
finding input that 
maximizes model   s 
predicted 
performance

training data extrapolation

google proprietary

deep neural networks are as good as humans at...

...recognizing objects 
and faces   .

(szegedy et al, 2014)

(taigmen et al, 2013)

...solving captchas 
and reading addresses...

(goodfellow et al, 2013)

(goodfellow et al, 2013)

and other tasks...

google proprietary

do neural networks    understand    these tasks?
- john searle   s    chinese room    thought experiment 

         ? ->                      

- what happens for a sentence not in the instruction 

book? 

                      -> 

google proprietary

turning objects into    airplanes   

google proprietary

attacking a linear model

google proprietary

clever hans

(   clever hans, 
clever 
algorithms   , bob 
sturm)

google proprietary

adversarial examples from overfitting

o

x

x

o

x

x

o

o

o

google proprietary

adversarial examples from underfitting

o

o

x

o

x

x

x

o

o

google proprietary

different kinds of low capacity

different kinds of low capacity

linear model: overconfident when extrapolating

rbf: no opinion in most places

google proprietary

google proprietary

modern deep nets are very (piecewise) linear
modern deep nets are very (piecewise) linear

rectified linear unit 
rectified linear unit

maxout 
maxout

carefully tuned sigmoid
carefully tuned sigmoid

lstm
lstm

google proprietary

google proprietary

a thin manifold of accuracy

x
a
m

t
f

o
s
 

o

t
 
t

n
e
m
u
g
r
a

google proprietary

not every class change is a mistake

clean example perturbation

corrupted 
example

perturbation changes the true 
class

random perturbation does not 
change the class

perturbation changes the input 
to    rubbish class   

google proprietary

all three perturbations have l2 norm 3.96
this is actually small. we typically use 7!

guaranteeing that class changes are mistakes

corrupted 
example

clean example perturbation

each pixel has some precision 
below which changes can   t be 
measured. 

-> 

constrain the max norm 

and we can guarantee that we do 
not change the class. 

to make adversarial examples for 
other kinds of data, we need to 
define different invariant 
constraint regions.

max norm constraint 
blocks previous attempt 
at changing the class

random perturbations 
still do not change the 
class

max norm constraint 
blocks change to 
rubbish class

all three perturbations have l2 norm 3.96, same as on prev slide. they now have max norm .14142. 
this is accomplished by taking .14142 * sign(perturbation) and choosing the sign randomly for pixels 
that were not perturbed before.

google proprietary

the fast gradient sign method

google proprietary

linear adversarial examples

google proprietary

high-dimensional linear models

clean examples

adversarial examples

weights

signs of weights

google proprietary

higher-dimensional linear models

(andrej karpathy,    breaking linear classifiers on id163   )

google proprietary

rbfs behave more intuitively far from the data

google proprietary

easy to optimize = easy to perturb

do we need to move past gradient-based 
optimization to overcome adversarial 
examples?

google proprietary

ubiquitous hallucinations

google proprietary

methods based on expensive search, strong hand-designed 
priors

(nguyen et al 2015)

(olah 2015)

google proprietary

cross-model, cross-dataset generalization

google proprietary

cross-model, cross-dataset generalization

neural net -> nearest neighbor: 25.3% error rate 
smoothed nearest neighbor -> nearest neighbor: 47.2% error rate 
     (a non-differentiable model doesn   t provide much 
       protection, it just requires the attacker to work 
       indirectly) 
adversarially trained neural net -> nearest neighbor: 22.15% error rate 
(adversarially trained neural net -> self: 18% error rate) 
maxout net -> relu net: 99.4% error rate 
    agree on wrong class 85% of the time 
maxout net -> tanh net: 99.3% error rate 
maxout net -> softmax regression: 88.9% error rate 
    agree on wrong class 67% of the time 
maxout net -> shallow rbf: 36.8% error rate 
    agree on class 43% of the time 

google proprietary

adversarial examples in the human visual system

(circles are concentric but 
appear intertwining)

(pinna and gregory, 2002)

google proprietary

failed defenses

- defenses that fail due to cross-model transfer: 

- ensembles 
- voting after multiple saccades 

- other failed defenses: 

- noise resistance 
- generative modeling / unsupervised pretraining 
- denoise the input with an autoencoder (gu and 

rigazio, 2014) 

-defenses that solve the adversarial task only if they break 
the clean task performance: 

- weight decay (l1 or l2) 
- jacobian id173 (like double backprop) 
- deep rbf network

google proprietary

limiting total 
variation 
(weight 
constraints)

usually underfits before 
it solves the adversarial 
example problem.

limiting sensitivity 
to infinitesimal 
perturbation 
(double backprop, 
cae)
-very hard to make the 
derivative close to 0 
-only provides constraint 
very near training examples, 
so does not solve adversarial examples.

limiting sensitivity 
to finite perturbation 
(adversarial 
training)

-easy to fit because slope is not 
constrained 
-constrains function over a 
wide area

google proprietary

generative modeling cannot solve the problem

both these two 
class mixture 
models implement 
the same marginal 
over x, with totally 
different posteriors 
over the classes. 
the likelihood 
criterion can   t 
prefer one to the 
other, and in many 
cases will prefer 
the bad one.

google proprietary

security implications

- must consider existence of adversarial examples when 

deciding whether to use machine learning 

- attackers can shut down a system that detects and 

refuses to process adversarial examples 

- attackers can control the output of a naive system 
- attacks can resemble regular data, or can appear to be 

unstructured noise, or can be structured but unusual 

- attacker does not need access to your model, 

parameters, or training set 

google proprietary

universal approximator theorem

universal approximator theorem

neural nets can represent either function:

neural nets can represent either function:

maximum likelihood doesn   t cause them to 
learn the right function. but we can fix that...

maximum likelihood doesn   t cause them to 
learn the right function. but we can fix that...

google proprietary

google proprietary

training on adversarial examples

0.0782% error  on mnist

google proprietary

weaknesses persist

google proprietary

more weaknesses

google proprietary

pertubation   s effect on class distributions

google proprietary

pertubation   s effect after adversarial training

google proprietary

virtual adversarial training

-

-
-
-

penalize full kl divergence between predictions on 
clean and adversarial point 
does not need y 
semi-supervised learning 
mnist results:

0.64% test error (statistically tied with state of the art) 

100 examples: 
vae -> 3.33% error 
virtual adversarial -> 2.12% 
ladder network -> 1.13%

google proprietary

clearing up common misconceptions

-

inputs that the model processes incorrectly are 
ubiquitous, not rare, and occur most often in half-spaces 
rather than pockets 

- adversarial examples are not specific to deep learning 
- deep learning is uniquely able to overcome adversarial 

examples, due to the universal approximator theorem 

- an attacker does not need access to a model or its 

training set 

- common off-the-shelf id173 techniques like 
model averaging and unsupervised learning do not 
automatically solve the problem 

google proprietary

please use evidence, not speculation

-

-

it   s common to say that obviously some technique will fix 
adversarial examples, and then just assume it will work without 
testing it 
it   s common to say in the introduction to some new paper on 
regularizing neural nets that this id173 research is justified 
because of adversarial examples 

- usually this is wrong 
- please actually test your method on adversarial examples and 

- consider doing this even if you   re not primarily concerned with 

report the results 

adversarial examples 

google proprietary

recommended adversarial example benchmark

-
-

-

-

fix epsilon 
compute the error rate on test data perturbed by the fast gradient 
sign method 
report the error rate, epsilon, and the version of the model used 
for both forward and back-prop 
alternative variant: design your own fixed-size perturbation 
scheme and report the error rate and size. for example, rotation 
by some angle. 

google proprietary

alternative adversarial example benchmark

-
-
-
-
-

use l-bfgs or other optimizer 
search for minimum size misclassified perturbation 
report average size 
report exhaustive detail to make the optimizer reproducible 
downsides: computation cost, difficulty of reproducing, hard to 
guarantee the perturbations will really be mistakes 

google proprietary

recommended fooling image / rubbish class benchmark

-
-
-
-

-
-

-

fix epsilon 
fit a gaussian to the training inputs 
draw samples from the gaussian 
perturb them toward a specific positive class with the fast gradient 
sign method 
report the rate at which you achieved this positive class 
report the rate at which the model believed any specific non-
rubbish class was present (id203 of that class being present 
exceeds 0.5) 
report epsilon

google proprietary

conclusion

- many modern ml algorithms 

- get the right answer for the wrong reason on 

naturally occurring inputs 

- get very wrong answers on almost all inputs 
- adversarial training can expand the very narrow 

manifold of accuracy 

- deep learning is on course to overcome adversarial 

examples, but maybe only by expanding our    instruction 
book    

- measure your model   s error rate on fast gradient sign 

method adversarial examples and report it!

google proprietary

