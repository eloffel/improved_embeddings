how to train a gan
soumith chintala 
facebook ai research 

emily denton, martin arjovsky, michael mathieu 
new york university

timeline - the stability of gans

timeline - the stability of gans

goodfellow et. al.    id3   

2014

timeline - the stability of gans

denton et. al.    deep generative image models using a laplacian pyramid of adversarial networks   

2015

timeline - the stability of gans

model architecture generator

denton et. al.    deep generative image models using a laplacian pyramid of adversarial networks   

2015

timeline - the stability of gans

model architecture generator

visual inspection

denton et. al.    deep generative image models using a laplacian pyramid of adversarial networks   

2015

timeline - the stability of gans

model architecture generator

visual inspection

countless failed stability hacks

denton et. al.    deep generative image models using a laplacian pyramid of adversarial networks   

2015

timeline - the stability of gans

model architecture generator

visual inspection

countless failed stability hacks

hope

denton et. al.    deep generative image models using a laplacian pyramid of adversarial networks   

2015

timeline - the stability of gans

countless hours    nding stable models

stable upto 64x64
mode dropping

under   tting

radford et. al.    unsupervised representation learning with deep convolutional id3   

2015

timeline - the stability of gans

more heuristics
more stability

salimans et. al.    improved techniques for training gans   

2015

comparison to classi   cation convnets

   throw things at the wall and see what sticks 
   intuition is poorer 
   no objective validation

#1: normalize the inputs

   normalize the images between -1 and 1 
   tanh as the last layer of the generator output

#2: modi   ed id168

   in papers people write  min (log 1-d), but in practice folks practically 
use max log d 
-  because the    rst formulation has vanishing gradients early on 
- goodfellow et. al (2014) 
   in practice: 
-flip labels when training generator: real = fake, fake = real

#3: use spherical z
   interpolation via great circle 
   tom white    sampling generative networks    
- https://arxiv.org/abs/1609.04468

#4: batchnorm

   di   erent batches for real and fake 
   when batchnorm is not an option use instance norm

real

generated

   
   
   

discriminator

#5: avoid sparse gradients: relu, maxpool

   the stability of the gan game su   ers 
   leakyrelu (both g and d) 
   downsampling: average pooling, conv2d + stride 
   upsampling: pixelshu   e, convtranspose2d + stride 
- pixelshu   e: https://arxiv.org/abs/1609.05158

#6: soft and noisy labels

   label smoothing 
   making the labels the noisy a bit for the discriminator, sometimes 
-salimans et. al. 2016

#7: dcgans / hybrid models

   dcgan when you can 

   if you cant use dcgans and no model is stable,  
   use a hybrid model :  kl + gan or vae + gan

#8: stability tricks from rl

   experience replay 
   things that work for deep deterministic policy gradients 
   see pfau & vinyals (2016)

#9: optimizer: adam

   optim.adam rules! 
- see radford et. al. 2015 
   [mmathieu] use sgd for discriminator and adam for generator

#10: track failures early

   d loss = 0 
   check norms of gradients: if they are over 100, things are screwing up 
   when things are working, d loss has low variance and goes down over 
time vs having huge variance and spiking 
   if loss of generator steadily decreases, then it's fooling d with garbage

#11: dont balance via loss statistics

   dont try to    nd a (number of g / number of d) schedule to uncollapse 
training 

   while lossd > x: 
- train d 
   while lossg > x: 
- train g

#12: if you have labels, use them

   if you have labels available, training the discriminator to also classify the 
samples: auxillary gans

#13: add noise to inputs, decay over time

   add some arti   cial noise to inputs to d (arjovsky et. al., huszar, 2016) 
-http://www.id136.vc/instance-noise-a-trick-for-stabilising-gan-training/ 
-https://openreview.net/forum?id=hk4_qw5xe 
   adding gaussian noise to every layer of generator (zhao et. al. ebgan) 
    improved gans: openai code also has it (commented out)

#14: train discriminator more (sometimes)

   especially when you have noise 
   hard to    nd a schedule of number of d iterations vs g iterations

#15: batch discrimination

   mixed results

conditional gans

#16: discrete variables

   use an embedding layer 
   add as additional channels to images 
   keep embedding dimensionality low and upsample to match image 
channel size

conclusion

   model stability is improving 
   theory is improving 
   hacks are a stop-gap

