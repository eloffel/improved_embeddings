hardware architectures for 

deep neural networks 

isca tutorial 
june 24, 2017 

website: http://eyeriss.mit.edu/tutorial.html  

1 

2 

speakers and contributors 

joel emer 

vivienne sze 

professor 

mit 

senior distinguished  
research scientist 

nvidia 
professor 

mit 

yu-hsin chen 
phd candidate 

tien-ju yang 
phd candidate 

mit 

mit 

2 

outline 

       overview of deep neural networks 
       dnn development resources 
       survey of dnn hardware 
       dnn accelerators 
       dnn model and hardware co-design 

3 

participant takeaways 

       understand the key design considerations for 

dnns  

       be able to evaluate different implementations of 
dnn with benchmarks and comparison metrics  

       understand the tradeoffs between various 

architectures and platforms 

       assess the utility of various optimization 

approaches 

       understand recent implementation trends and 

opportunities 

4 

resources 

       eyeriss project: http://eyeriss.mit.edu  

       tutorial slides 
       benchmarking 
       energy modeling 
       mailing list for updates 

       http://mailman.mit.edu/mailman/listinfo/eems-news  

       paper based on today   s tutorial: 

       v. sze, y.-h. chen, t-j. yang, j. emer,    efficient processing 
of deep neural networks: a tutorial and survey   , arxiv, 2017 

5 

background of  
deep neural networks 

6 

artificial intelligence 

artificial intelligence 

   the science and engineering of creating 
intelligent machines    
 

          - john mccarthy, 1956 

 

7 

ai and machine learning 

artificial intelligence 
machine learning 

   field of study that gives computers the ability 
to learn without being explicitly programmed    
     arthur samuel, 1959 

8 

brain-inspired machine learning 

artificial intelligence 
machine learning 

brain-inspired 

an  algorithm  that  takes  its  basic 
functionality  from  our  understanding 
of how the brain operates 

9 

how does the brain work? 

       the basic computational unit of the brain is a neuron 

   86b neurons in the brain 

       neurons are connected with nearly 1014     1015 synapses 
       neurons  receive  input  signal  from  dendrites  and  produce 
output  signal  along axon,  which  interact  with  the  dendrites  of 
other neurons via synaptic weights 

       synaptic weights     learnable & control influence strength 

image source: stanford 

10 

spiking-based machine learning 

artificial intelligence 
machine learning 

brain-inspired 

spiking 

11 

spiking architecture 

       brain-inspired 
       integrate and fire 
       example: ibm truenorth 

[merolla et al., science 2014; esser et al., pnas 2016] 
http://www.research.ibm.com/articles/brain-chip.shtml 

12 

machine learning with neural networks 

artificial intelligence 
machine learning 

brain-inspired 

spiking 

 

neural 
networks 

13 

neural networks: weighted sum 

image source: stanford 

14 

many weighted sums 

image source: stanford 

15 

deep learning 

artificial intelligence 
machine learning 

brain-inspired 

spiking 

 

neural 
networks 

deep 

learning 

16 

what is deep learning? 

image 

   volvo 
xc90    

image source: [lee et al., comm. acm 2011] 

17 

why is deep learning hot now? 

big data 
availability 

gpu 

acceleration 

new ml 

techniques 

350m images 
uploaded per 
day 
2.5 petabytes 
of customer 
data hourly 

300 hours of 
video uploaded 
every minute 

18 

id163 challenge 

image classification task: 

 1.2m training images     1000 object categories 

 id164 task: 

 456k training images     200 object categories 

19 

id163: image classification task 

30 

25 

20 

15 

10 

5 

0 

top 5 classification error (%) 
large error rate reduction 
due to deep id98 

2010 

2011 

2012 

2013 

2014 

2015 

human 

hand-crafted feature- 

based designs 

deep id98-based designs 

[russakovsky et al., ijcv 2015] 

20 

gpu usage for id163 challenge 

21 

established applications 

       image 

o    classification: image to object class 
o    recognition: same as classification (except for faces) 
o    detection: assigning bounding boxes to objects 
o    segmentation: assigning object class to every pixel 

       speech & language 

o    id103: audio to text 
o    translation 
o    natural language processing: text to meaning 
o    audio generation: text to audio 

       games 

22 

deep learning on games 

google deepmind alphago 

23 

emerging applications 
       medical (cancer detection, pre-natal) 

       finance (trading, energy forecasting, risk) 

       infrastructure (structure safety and traffic) 

       weather forecasting and id37 

http://www.nextplatform.com/2016/09/14/next-wave-deep-learning-applications/ 

24 

deep learning for self-driving cars 

25 

opportunities 

from ee times     september 27, 2016 
 
   today  the  job  of  training  machine  learning  models  is 
limited  by  compute,  if  we  had  faster  processors  we   d 
run bigger models   in practice we train on a reasonable 
subset of data that can finish in a matter of months. we 
could use improvements of several orders of magnitude 
    100x or greater.    

       greg diamos, senior researcher, svail, baidu 

26 

overview of  
deep neural networks 

27 

dnn timeline 

       1940s: neural networks were proposed 
       1960s: deep neural networks were proposed 
       1989: neural network for recognizing digits (lenet) 
       1990s: hardware for shallow neural nets 

       example: intel etann (1992) 

       2011: breakthrough dnn-based id103 

       microsoft real-time speech translation  

       2012: dnns for vision supplanting traditional ml 

       alexnet for image classification 

       2014+: rise of dnn accelerator research 

       examples: neuflow, diannao, etc. 

28 

publications at architecture conferences 

       micro, isca, hpca, asplos 

29 

so many neural networks! 

http://www.asimovinstitute.org/neural-network-zoo/ 

30 

dnn terminology 101 

neurons 

image source: stanford 

31 

dnn terminology 101 

synapses 

image source: stanford 

32 

dnn terminology 101 

each synapse has a weight for neuron activation 

yj = activation wij    xi

3
   
i=1

   
   
   

   
   
   

x1 

x2 

x3 

w11 

w34 

y1 

y2 

y3 

y4 

image source: stanford 

33 

dnn terminology 101 

weight sharing: multiple synapses use the same weight value 

yj = activation wij    xi

3
   
i=1

   
   
   

   
   
   

x1 

x2 

x3 

w11 

w34 

y1 

y2 

y3 

y4 

image source: stanford 

34 

dnn terminology 101 

l1 neuron inputs 
e.g. image pixels 

layer 1 

l1 neuron outputs 
a.k.a. activations 

image source: stanford 

35 

dnn terminology 101 

l2 input  
activations 

layer 2 

l2 output  
activations 

image source: stanford 

36 

dnn terminology 101 

fully-connected: all i/p neurons connected to all o/p neurons 
sparsely-connected 

image source: stanford 

37 

dnn terminology 101 

feed forward 

feedback 

image source: stanford 

38 

popular types of dnns 

       fully-connected nn 

       feed forward, a.k.a. multilayer id88 (mlp) 

       convolutional nn (id98) 

       feed forward, sparsely-connected w/ weight sharing 

       recurrent nn (id56)  

       feedback 

       long short-term memory (lstm) 

       feedback + storage 

39 

id136 vs. training  

       training: determine weights 

       supervised:  

       training set has inputs and outputs, i.e., labeled 

       unsupervised:  

       training set is unlabeled 

       semi-supervised:  

       training set is partially labeled  

       reinforcement: 

       output assessed via rewards and punishments 

       id136: apply weights to determine output  

40 

deep convolutional neural networks 

modern deep id98: 5     1000 layers 

conv 
layer 

low-level 
features 

    

high-level 
features 

conv 
layer 

fc 
layer 

classes 

1     3 layers 

41 

deep convolutional neural networks 

conv 
layer 

low-level 
features 

    

high-level 
features 

conv 
layer 

fc 
layer 

classes 

convolution  activation 

  	

42 

deep convolutional neural networks 

conv 
layer 

low-level 
features 

    

high-level 
features 

conv 
layer 

fc 
layer 

classes 

fully 

connected 

activation 

  	

43 

deep convolutional neural networks 

optional layers in between  

conv and/or fc layers 

conv 
layer 

norm 
layer 

pool 
layer 

conv 
layer 

high-level 
features 

fc 
layer 

classes 

id172 

pooling 

44 

deep convolutional neural networks 

conv 
layer 

norm 
layer 

pool 
layer 

conv 
layer 

high-level 
features 

fc 
layer 

classes 

  
  

  
  

  
  

convolutions  account  for  more 
than 90% of overall computation, 
dominating runtime and energy 
consumption 

45 

convolution (conv) layer 

a plane of input activations 

a.k.a. input feature map (fmap) 

filter (weights) 

r 

s 

h 

w 

46 

convolution (conv) layer 

input fmap 

h 

filter (weights) 

r 

s 

w 

element-wise 
multiplication 

47 

convolution (conv) layer 

input fmap 

output fmap 

filter (weights) 

r 

s 

h 

e 

element-wise 
multiplication 

w 

f 
partial sum (psum) 

accumulation 

an output  
activation 

48 

convolution (conv) layer 

input fmap 

output fmap 

filter (weights) 

r 

s 

h 

w 

an output  
activation 

e 

f 

sliding window processing 

49 

convolution (conv) layer 

filter 

c 

r 

s 

input fmap 
c 

h 

w 

many input channels (c) 

output fmap 

e 

f 

50 

convolution (conv) layer 

many 

filters (m) 
c 

r 

 

1 
s 
   
c 

input fmap 
c 

h 

w 

output fmap 

e 

m 

f 
many 

output channels (m) 

r 

m 
s 

51 

convolution (conv) layer 

filters 

c 

r 

 

s 
   
c 

r 

s 

many 

input fmaps (n) 

c 

h 

1 

 

w 
   

c 

h 

n 

  

  

w 

  

  

many 

output fmaps (n) 

m 

e 

1 

 

f 
   

e 

n 

f 

52 

id98 decoder ring 

       n     number of input fmaps/output fmaps (batch size) 
       c     number of 2-d input fmaps /filters (channels) 
       h     height of input fmap (activations)  
       w     width of input fmap (activations) 
       r     height of 2-d filter (weights) 
       s     width of 2-d filter (weights) 
       m     number of 2-d output fmaps (channels) 
       e     height of output fmap  (activations) 
       f     width of output fmap (activations) 

53 

conv layer tensor computation 

output fmaps (o) 

input fmaps (i)  

biases (b) 

filter weights (w) 

54 

conv layer implementation 

na  ve 7-layer for-loop implementation: 

for each output fmap value 

for	(n=0;	n<n;	n++)	{	
				for	(m=0;	m<m;	m++)	{	
								for	(x=0;	x<f;	x++)	{	
												for	(y=0;	y<e;	y++)	{	
	
																o[n][m][x][y]	=	b[m];	
																for	(i=0;	i<r;	i++)	{	
convolve  
																				for	(j=0;	j<s;	j++)	{	
a window 
																								for	(k=0;	k<c;	k++)	{	
and apply 
																												o[n][m][x][y]	+=	i[n][k][ux+i][uy+j]	  	w[m][k][i][j];	
																								}	
activation 
																				}	
																}	
	
																o[n][m][x][y]	=	activation(o[n][m][x][y]);	
												}																	
								}	
				}	
}	

55 

traditional id180 

sigmoid 

hyperbolic tangent 
1 

0 

-1 

-1 

0 

1 

0 

1 

1 

0 

-1 

-1 

y=1/(1+e-x)	

y=(ex-e-x)/(ex+e-x)	

image source: caffe tutorial 

56 

modern id180 

rectified linear unit 

(relu) 

leaky relu 

exponential lu 

1 

0 

1 

0 

1 

0 

-1 

-1 

0 

-1 
-1 

1 

0 

1 

-1 

y=max(0,x)	

y=max(  x,x)	
   = small const. (e.g. 0.1) 

0 

-1 

1 
x   0	
				x,							
y=	
				  (ex-1),	
x<0	

image source: caffe tutorial 

57 

fully-connected (fc) layer 

       height and width of output fmaps are 1 (e = f = 1) 
       filters as large as input fmaps (r = h, s = w) 
      

implementation: id127 

filters 
chw 

input fmaps 

output fmaps 

n 

n 

m 

chw 

   

m = 

58 

fc layer     from conv layer pov 

filters 

input fmaps 
c 

c 

h 

 

w 
   
c 

h 

w 

h 

1 

 

w 
   

c 

h 

n 

  
  

  

  

  

w 

  

  

output fmaps 

m 

1 

1 

 

1 
   

  

1 

1 

n 

59 

pooling (pool) layer 

       reduce resolution of each channel independently 
       overlapping or non-overlapping    depending on stride 

increases translation-invariance and noise-resilience  

image source: caffe tutorial 

60 

pool layer implementation 

na  ve 6-layer for-loop max-pooling implementation: 

for each pooled value 

for	(n=0;	n<n;	n++)	{	
				for	(m=0;	m<m;	m++)	{	
								for	(x=0;	x<f;	x++)	{	
												for	(y=0;	y<e;	y++)	{	
	
																max	=	-inf;		
																for	(i=0;	i<r;	i++)	{	
																				for	(j=0;	j<s;	j++)	{	
																								if	(i[n][m][ux+i][uy+j]	>	max)	{	
																												max	=	i[n][m][ux+i][uy+j];	
																								}	
																				}	
																}	
	
																o[n][m][x][y]	=	max;	
												}																	
								}	
				}	
}	

find the max  
with in a window 

61 

id172 (norm) layer 

       batch id172 (bn) 

       normalize  activations  towards  mean=0  and  std. 

dev.=1 based on the statistics of the training dataset 
       put in between conv/fc and activation function 

conv 
layer 

convolution 

activation 

bn 

  	

believed to be key to getting high accuracy and  
faster training on very deep neural networks. 

[ioffe et al., icml 2015] 

62 

bn layer implementation 

       the normalized value is further scaled and shifted, the 

parameters of which are learned from training 

data mean 

learned scale factor 

data std. dev. 

learned shift factor 

small const. to avoid 
numerical problems 

63 

id172 (norm) layer 

       local response id172 (lrn) 

       tries to mimic the inhibition scheme in the brain 

now deprecated! 
image source: caffe tutorial 

64 

relevant components for tutorial 

       typical operations that we will discuss: 

       convolution (conv) 
       fully-connected (fc) 
       max pooling 
       relu 

65 

survey of dnn 
development resources 

isca tutorial (2017) 

website: http://eyeriss.mit.edu/tutorial.html  

joel emer, vivienne sze, yu-hsin chen 

1 

popular dnns 

       lenet (1998) 
       alexnet (2012) 
       overfeat (2013) 
       vggnet (2014) 
       googlenet (2014) 
       resnet (2015) 

 
)
r
o
r
r
e
 
5
 
p
o
t
(
 

y
c
a
r
u
c
c
a

18 
16 
14 
12 
10 
8 
6 
4 
2 
0 

id163: large scale visual 

recognition challenge (ilsvrc) 

alexnet	

overfeat	

vggnet	

googlenet	

resnet	

	
i

a
f
i
r
a
c

l

2012 

2013 

2014 

2015  human 

[o. russakovsky et al., ijcv 2015] 

2 

lenet-5  

conv layers: 2 
fully connected layers: 2 
weights: 60k 
macs: 341k 
sigmoid used for non-linearity 

digit classification! 

six  

5x5 filters 

2x2  

average 
pooling 

six  

5x5 filters 

2x2  

average 
pooling 

[y. lecun et al, proceedings of the ieee, 1998] 

3 

alexnet 

conv layers: 5 
fully connected layers: 3 
weights: 61m 
macs: 724m 
relu used for non-linearity 

ilscvr12 winner 

uses local response id172 (lrn) 

[krizhevsky et al., nips, 2012] 

1000	
scores	

	

	
)
1
1
x
1
1
(
	
v
n
o
c

y
t
i
r
a
e
n
i
l
-
n
o
n

	
)

n
r
l
(
	

m
r
o
n

	
l

o
o
p
	
x
a
m

l1	

	

	
)
5
x
5
(
	
v
n
o
c

y
t
i
r
a
e
n
i
l
-
n
o
n

	
)

n
r
l
(
	

m
r
o
n

	

g
n

i
l

o
o
p
	
x
a
m

	

l3	

l2	

	

	
)
3
x
3
(
	
v
n
o
c

y
t
i
r
a
e
n
i
l
-
n
o
n

	
)
3
x
3
(
	
v
n
o
c

	

y
t
i
r
a
e
n
i
l
-
n
o
n

l4	

	

	
)
3
x
3
(
	
v
n
o
c

y
t
i
r
a
e
n
i
l
-
n
o
n

	

g
n

i
l

o
o
p
	
x
a
m

	

l5	

	
t
c
e
n
n
o
c
y
l
l

	

	

y
t
i
r
a
e
n
i
l
-
n
o
n

u
f

l6	

	
t
c
e
n
n
o
c
y
l
l

	

	

y
t
i
r
a
e
n
i
l
-
n
o
n

u
f

l7	

	
t
c
e
n
n
o
c
y
l
l

	

	

y
t
i
r
a
e
n
i
l
-
n
o
n

u
f

224x224	
input	
image	

#	of	weights	

35k	

307k	

885k	

664k	

442k	

37.7m	 16.8m	 4.1m	

4 

5 

large sizes with varying shapes 

alexnet	convolu7onal	layer	con   gura7ons	

layer  filter size (rxs)  # filters (m)  # channels (c)  stride 

1 
2 
3 
4 
5 

11x11 
5x5 
3x3 
3x3 
3x3 

96 
256 
384 
384 
256 
layer	2	

3 
48 
256 
192 
192 

4 
1 
1 
1 
1 

layer	1	

layer	3	

34k	params	
105m	macs	

307k	params	
224m	macs	

885k	params	
150m	macs	

[krizhevsky et al., nips, 2012] 

5 

vgg-16 

conv layers: 13 
fully connected layers: 3 
weights: 138m 
macs: 15.5g 

also, 19 layer version 

reduce # of weights 

more layers    deeper! 

image source: http://www.cs.toronto.edu/~frossard/post/vgg16/ 

[simonyan et al., arxiv 2014, iclr 2015] 

6 

googlenet (v1) 

conv layers: 21 (depth), 57 (total) 
fully connected layers: 1 
weights: 7.0m 
macs: 1.43g 

also, v2, v3 and v4 
ilsvrc14 winner 

parallel filters of different size has the effect of 

processing image at different scales 

inception  
module 

1x1    bottleneck    to 
reduce number of 
weights 

[szegedy et al., arxiv 2014, cvpr 2015] 

7 

googlenet (v1) 

conv layers: 21 (depth), 57 (total) 
fully connected layers: 1 
weights: 7.0m 
macs: 1.43g 

also, v2, v3 and v4 
ilsvrc14 winner 

9 inception layers 

3 conv layers 

1 fc layer 

[szegedy et al., arxiv 2014, cvpr 2015] 

8 

resnet-50 

conv layers: 49 
fully connected layers: 1 
weights: 25.5m 
macs: 3.9g 

x	

short cut module 

3x3 conv 

relu 

learns  
residual  
f(x)=h(x)-x 

f(x)	
h(x)	=	f(x)	+	x	

+ 

3x3 conv 

also, 34,152 and 1202 layer versions 
ilsvrc15 winner 
resnet-34 

x	

iden%ty	

1 conv layer 

16 short  
cut layers 

relu 

helps address the vanishing gradient 

challenge for training very deep networks 

[he et al., arxiv 2015, cvpr 2016] 

1 fc layer 

9 

revolution of depth 

image source: http://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf  

10 

summary of popular dnns 

metrics 

lenet-5 

alexnet 

vgg-16 

top-5 error 
input size 
# of conv layers  
filter sizes 
# of channels 
# of filters 
stride 
# of weights 
# of macs 
# of fc layers 
# of weights 
# of macs 
total weights 
total macs 

n/a 

16.4 

7.4 

28x28 

227x227 

224x224 

2 
5 
1, 6 
6, 16 

1 

2.6k 
283k 

2 
58k 
58k 
60k 
341k 

5 

3, 5,11 
3 - 256 
96 - 384 

16 
3 

3 - 512 
64 - 512 

1, 4 
2.3m 
666m 

3 

58.6m 
58.6m 
61m 
724m 

1 

14.7m 
15.3g 

3 

124m 
124m 
138m 
15.5g 

googlenet  

(v1) 
6.7 

224x224 
21 (depth) 
1, 3 , 5, 7 
3 - 1024 
64 - 384 

resnet-50 

5.3 

224x224 

49 

1, 3, 7 
3 - 2048 
64 - 2048 

1, 2 
6.0m 
1.43g 

1 
1m 
1m 
7m 
1.43g 

1, 2 
23.5m 
3.86g 

1 
2m 
2m 
25.5m 
3.9g 

conv layers increasingly important! 

11 

summary of popular dnns 

       alexnet 

       first id98 winner of ilsvrc 
       uses lrn (deprecated after this) 

       vgg-16 

       goes deeper (16+ layers) 
       uses only 3x3 filters (stack for larger filters) 

       googlenet (v1) 

       reduces weights with inception and only one fc layer 
       inception: 1x1 and dag (parallel connections) 
       batch id172 

       resnet 

       goes deeper (24+ layers) 
       shortcut connections 

12 

frameworks 

* 

* 

berkeley / bvlc 
(c, c++, python, matlab) 

google 
(c++, python) 

u. montreal 
(python) 

facebook / nyu 
(c, c++, lua) 

also, cntk, mxnet, etc. 
more at: https://developer.nvidia.com/deep-learning-frameworks  

* lightweight mobile versions (caffe2go, tensorflow mobile) 

13 

example: layers in caffe 

convolution layer 
!
layer {!
  name: "conv1"!
  type: "convolution"!
  bottom: "data"!
  top: "conv1"!
 ...!
  convolution_param {!
    num_output: 20!
    kernel_size: 5!
    stride: 1!
...!

non-linearity 
!
layer {!
  name: "relu1"!
  type: "relu"!
  bottom: "conv1"!
  top: "conv1"!
}!

pooling layer 
!
layer {!
  name: "pool1"!
  type: "pooling"!
  bottom: "conv1"!
  top: "pool1"!
  pooling_param {!
    pool: max!
    kernel_size: 2!
    stride: 2 ...!

http://caffe.berkeleyvision.org/tutorial/layers.html 

14 

benefits of frameworks 

       rapid development 
       sharing models 
       workload profiling 
       network hardware co-design 

15 

image classification datasets  

       image classification/recognition 

       given an entire image    select 1 of n classes 
       no localization (detection) 

 

datasets affect difficulty of task 

16 

image source: stanford cs231n 

mnist 

digit classification 
28x28 pixels (b&w) 
10 classes 
60,000 training 
10,000 testing 

lenet in 1998 
(0.95% error) 
 
 
 
icml 2013 
(0.21% error) 

http://yann.lecun.com/exdb/mnist/  

17 

id163 

object classification 
~256x256 pixels (color) 
1000 classes 
1.3m training 
100,000 testing (50,000 validation) 

image source: http://karpathy.github.io/ 

http://www.image-net.org/challenges/lsvrc/  

18 

id163 

image source: http://karpathy.github.io/ 

image source: krizhevsky et al., nips 2012 

fine grained  
classes 
(120 breeds) 

top-5 error 
winner 2012  
(16.42% error) 
 
 
 
winner 2016 
(2.99% error) 

http://www.image-net.org/challenges/lsvrc/  

19 

image classification summary 

year 
resolution 
classes 
training 
testing 
accuracy 

mnist 
1998 
28x28 

10 
60k 
10k 

0.21% error  
(icml 2013) 

id163 

2012 

256x256 

1000 
1.3m 
100k 
2.99%  
top-5 error 

(2016 winner) 

http://rodrigob.github.io/are_we_there_yet/build/

classification_datasets_results.html 

20 

next tasks: localization and detection 

[russakovsky et al., ijcv, 2015] 

21 

others popular datasets 

       pascal voc 
       11k images 
       id164 
       20 classes 
       ms coco   

       300k images 
       detection, segmentation 
       recognition in context 

http://host.robots.ox.ac.uk/pascal/voc/  

http://mscoco.org/  

22 

recently introduced datasets 

       google open images (~9m images) 
       https://github.com/openimages/dataset 

       youtube-8m (8m videos) 

       https://research.google.com/youtube8m/  

       audioset (2m sound clips) 

       https://research.google.com/audioset/index.html  

23 

summary 

       development resources presented in this 

section enable us to evaluate hardware using 
the appropriate dnn model and dataset 
       difficult tasks typically require larger models 
       different datasets for different tasks 
       number of datasets growing at a rapid pace 
 

24 

survey of  
dnn hardware 

isca tutorial (2017) 

website: http://eyeriss.mit.edu/tutorial.html  

joel emer, vivienne sze, yu-hsin chen 

1 

cpus are targeting deep learning 

intel knights landing (2016) 

       7 tflops fp32 
       16gb mcdram    400 gb/s 
       245w tdp 
       29 gflops/w (fp32) 
       14nm process 

knights mill: next gen xeon phi    optimized for deep learning     

intel announced the addition of new vector instructions for deep learning 

(avx512-4vnniw and avx512-4fmaps), october 2016 

image source: intel, data source: next platform 

2 

gpus are targeting deep learning 

nvidia pascal gp100 (2016) 

       10/20 tflops fp32/fp16 
       16gb hbm     750 gb/s 
       300w tdp 
       33/67 gflops/w (fp32/fp16) 
       16nm process 
       160gb/s nv link  

source: nvidia 

3 

gpus are targeting deep learning 

nvidia volta gv100 (2017) 

       15 tflops fp32 
       16gb hbm2     900 gb/s 
       300w tdp 
       50 gflops/w (fp32) 
       12nm process 
       300gb/s nv link2  
       tensor core   . 

source: nvidia 

4 

gv100        tensor core    

tensor core   . 
       120 tflops (fp16) 
       400 gflops/w (fp16) 

5 

systems for deep learning 

nvidia dgx-1 (2016) 

       170 tflops 
       8   tesla p100, dual xeon 
       nvlink hybrid cube mesh 
       optimized dl software 
       7 tb ssd cache 
       dual 10gbe, quad ib 100gb 
       3ru     3200w 

source: nvidia 

6 

cloud systems for deep learning 

facebook   s deep learning machine 

       open rack compliant 
       powered by 8 tesla m40 gpus 
       2x faster training for faster deployment 
       2x larger networks for higher accuracy 
 

source: facebook 

7 

socs for deep learning id136 

nvidia tegra - parker 

arm v8 

cpu 

complex 
(2x denver 2 + 4x 

a57) 

coherent hmp 

security 
engines 

4k60 
video 

encoder 

4k60 
video 

decoder 

audio 
engine 

2d 

engine 

display 
engines 

128-bit  
lpddr4 

boot and 
pm proc 

gige 

ethernet 

mac 

image 
proc 
(isp) 

safety 
engine 

i/o 

       gpu: 1.5 teraflops fp16 
       4gb lpddr4 @ 25.6 gb/s 
       15 w tdp  

(1w idle, <10w typical) 
       100 gflops/w (fp16) 
       16nm process 

xavier: next gen tegra to be an    ai supercomputer    

source: nvidia 

8 

mobile socs for deep learning 

samsung exynos (arm mali) 

exynos 8 octa 8890 

       gpu: 0.26 tflops 
       lpddr4 @ 28.7 gb/s 
       14nm process 

source: wikipedia 

9 

fpgas for deep learning 

intel/altera stratix 10 
       10 tflops fp32 
       hbm2 integrated 
       up to 1 ghz 
       14nm process 
       80 gflops/w 
xilinx virtex ultrascale+ 
       dsp: up to 21.2 tmacs 
       dsp: up to 890 mhz 
       up to 500mb on-chip memory 
       16nm process 

10 

kernel  
computation 

11 

fully-connected (fc) layer 

       matrix   vector multiply:  

       multiply all inputs in all channels by a weight and sum 

filters 
chw 

input fmaps 

output fmaps 

1 

1 

   

chw 

= 

m 

m 

12 

fully-connected (fc) layer 

       batching (n) turns operation into a matrix-matrix multiply 

filters 
chw 

input fmaps 

output fmaps 

n 

n 

m 

chw 

   

m = 

13 

fully-connected (fc) layer 

       implementation: id127 (gemm) 

 

       cpu: openblas, intel mkl, etc 
       gpu: cublas, cudnn, etc 

       optimized by tiling to storage hierarchy 

14 

convolution (conv) layer 

       convert to matrix mult. using the toeplitz matrix 

convolution: 

filter 
1 2
3 4

input fmap  output fmap 

* 

1 2 3
4 5 6
7 8 9

= 

1 2
3 4

matrix mult: 

toeplitz matrix 

(w/ redundant data) 
= 

1 2 3 4    

1 2 4 5
2 3 5 6
4 5 7 8
5 6 8 9

1 2 3 4

15 

convolution (conv) layer 

       convert to matrix mult. using the toeplitz matrix 

convolution: 

filter 
1 2
3 4

input fmap  output fmap 

* 

1 2 3
4 5 6
7 8 9

= 

1 2
3 4

matrix mult: 

toeplitz matrix 

(w/ redundant data) 
= 

1 2 3 4    

1 2 4 5
2 3 5 6
4 5 7 8
5 6 8 9

1 2 3 4

data is repeated 

16 

convolution (conv) layer 

       multiple channels and filters 

filter 1 

1 2
3 4

1 2
3 4

filter 2 

1 2
3 4

1 2
3 4
chnl 1  chnl 2 

* 

input fmap 

1 2 3
1 2 3
4 5 6
4 5 6
7 8 9
7 8 9
chnl 1  chnl 2 

output fmap 
= 

1 2
3 4

1 2
3 4

chnl 1 

chnl 2 

17 

convolution (conv) layer 

       multiple channels and filters 

chnl 1 
1 2 3 4
1 2 3 4

chnl 2 
1 2 3 4
1 2 3 4

filter 1 
filter 2 

toeplitz matrix 

(w/ redundant data) 
   
= 

1
2
4
5
1
2
4
5

2
3
5
6
2
3
5
6

4
5
7
8
4
5
7
8

5
6
8
9
5
6
8
9

chnl 1 

chnl 2 

1 2
3 4
1 2 3 4

chnl 1 
chnl 2 

18 

computational  
transforms 

19 

computation transformations 

       goal: bitwise same result, but reduce 

number of operations 

       focuses mostly on compute 

20 

gauss   s multiplication algorithm 

4 multiplications + 3 additions 

3 multiplications + 5 additions 

21 

strassen 

8 multiplications + 4 additions 

p1 = a(f     h) 
p2 = (a + b)h 
p3 = (c + d)e 
p4 = d(g     e) 

p5 = (a + d)(e + h) 
p6 = (b - d)(g + h) 
p7 = (a     c)(e + f) 

7 multiplications + 18 additions 

7 multiplications + 13 additions (for constant b matrix     weights) 

[cong et al., icann, 2014] 

22 

strassen 

       reduce the complexity of id127 

from   (n3) to   (n2.807) by reducing multiplication 

complexity 

na  ve 

n 

strassen 

comes at the price of reduced numerical stability 

and requires significantly more memory 

image source: http://www.stoimen.com/blog/2012/11/26/computer-algorithms-strassens-matrix-multiplication/  

23 

winograd 1d     f(2,3) 

       targeting convolutions instead of matrix multiply 
       notation: f(size of output, filter size) 

input 

filter 

6 multiplications + 4 additions 

=[       0@    1   ] 

4 multiplications + 12 additions + 2 shifts 
4 multiplications + 8 additions (for constant weights) 

[lavin et al., arxiv 2015] 

24 

winograd 2d - f(2x2, 3x3) 

       1d winograd is nested to make 2d winograd 

filter 

g00  g01  g02 
g10  g11  g12 
g20  g21  g22 

* 

input fmap 

output fmap 

= 

y00  y01 
y10  y11 

d00  d01  d02  d03 
d10  d11  d12  d13 
d20  d21  d22  d23 
d30  d31  d32  d33 

original: 
winograd: 

 36 multiplications 
 16 multiplications    2.25 times reduction 

25 

winograd halos 

       winograd works on a small region of output at a 

time, and therefore uses inputs repeatedly 

filter 

g00  g01  g02 
g10  g11  g12 
g20  g21  g22 

input fmap 

d00  d01  d02  d03  d04  d05 
d10  d11  d12  d13  d14  d15 
d20  d21  d22  d23  d24  d25 
d30  d31  d32  d33  d34  d35 

output fmap 

y00  y01 
y10  y11 

y02  y03 
y12  y12 

halo columns 

26 

winograd performance varies 

source: nvidia  

27 

winograd summary 

       winograd is an optimized computation for 

convolutions 
 

       it can significantly reduce multiplies 

       for example, for 3x3 filter by 2.25x 

 

       but, each filter size is a different computation. 

28 

winograd as a transform 

transform inputs 

dot-product 

transform output 

gggt can be precomputed 

[lavin et al., arxiv 2015] 

29 

filter 
input 

fft flow  
input fmap 

output fmap 

filter (weights) 

r 

s 

f
f
t

h 

* 

= 

e 

w 

f
f
t

f 
i
f
f
t

an output  
activation 

fft(w) 

x 

fft(i) 

= 

fft(0) 

30 

fft overview 

       convert filter and input to frequency domain 
to make convolution a simple multiply then 
convert back to time domain. 
 

       convert direct convolution o(no

computation to o(no

2log2no) 

2nf

2) 

       so note that computational benefit of fft 

decreases with decreasing size of filter 

[mathieu et al., arxiv 2013, vasilache et al., arxiv 2014] 

31 

fft costs 

       input and filter matrices are    0-completed   , 

        i.e., expanded to size e+r-1 x f+s-1 

       frequency domain matrices are same 

dimensions as input, but complex.  

       fft often reduces computation, but requires 

much more memory space and bandwidth 

32 

optimization opportunities 

       fft of real matrix is symmetric allowing one 

to save    the computes 

       filters can be pre-computed and stored, but 
convolutional filter in frequency domain is 
much larger than in time domain 

       can reuse frequency domain version of input 

for creating different output channels to 
avoid fft re-computations 

33 

cudnn: speed up with transformations 

source: nvidia  

34 

dnn accelerator 
architectures 

isca tutorial (2017) 

website: http://eyeriss.mit.edu/tutorial.html  

joel emer, vivienne sze, yu-hsin chen 

1 

2 

highly-parallel compute paradigms 

temporal architecture 

(simd/simt) 

spatial architecture 
(dataflow processing) 

memory hierarchy 
register file 

alu 

alu 

alu 

alu 

memory hierarchy 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

control 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

2 

memory access is the bottleneck 

memory read 

mac* 

memory write 

filter weight 
fmap activation 
partial sum 

  
  

alu 
  
  

updated partial sum 

* multiply-and-accumulate 

3 

memory access is the bottleneck 

memory read 

mac* 

memory write 

dram 

  

  

  
  

alu 
  
  

dram 

* multiply-and-accumulate 

worst case: all memory r/w are dram accesses 
       example: 

 alexnet [nips 2012]  has 724m macs  
    2896m dram accesses required 

 

4 

memory access is the bottleneck 

memory read 

mac* 

memory write 

dram 

mem 

  
  

alu 
  
  

mem 

dram 

extra levels of local memory hierarchy 

5 

memory access is the bottleneck 

memory read 
1 

dram 

mem 

mac* 

memory write 

  
  

alu 
  
  

mem 

dram 

extra levels of local memory hierarchy 

opportunities:      data reuse         local accumulation 

1 

6 

types of data reuse in dnn 

convolutional reuse 

conv layers only 
(sliding window) 

filter 

  

input fmap 

  

  

reuse: 

activations 
filter weights 

7 

types of data reuse in dnn 

convolutional reuse 

fmap reuse 

conv layers only 
(sliding window) 

conv and fc layers 

filter 

  

input fmap 

filters 

input fmap 

  

  

1   

  
2 

  
  

reuse: 

activations 
filter weights 

reuse: 

activations 

8 

types of data reuse in dnn 

convolutional reuse 

fmap reuse 

conv layers only 
(sliding window) 

conv and fc layers 

filter 

  

input fmap 

filters 

input fmap 

  

  

1   

  
2 

  
  

filter reuse 

conv and fc layers 

(batch size > 1) 

input fmaps 

filter 

  

  

  

1 

2 

reuse: 

activations 
filter weights 

reuse: 

activations 

reuse: 

filter weights 

9 

memory access is the bottleneck 

memory read 
1 

dram 

mem 

mac* 

memory write 

  
  

alu 
  
  

mem 

dram 

extra levels of local memory hierarchy 

opportunities:      data reuse         local accumulation 

1 

1)    can reduce dram reads of filter/fmap by up to 500  ** 
1 

** alexnet conv layers 

10 

memory access is the bottleneck 

memory read 
1 

dram 

mem 

mac* 

memory write 

  
  

alu 
  
  

2 

mem 

dram 

extra levels of local memory hierarchy 

opportunities:      data reuse         local accumulation 

2 

1 

1)    can reduce dram reads of filter/fmap by up to 500   
1 
2)    partial sum accumulation does not have to access dram 
2 

11 

memory access is the bottleneck 

memory read 
1 

dram 

mem 

mac* 

memory write 

  
  

alu 
  
  

2 

mem 

dram 

extra levels of local memory hierarchy 

opportunities:      data reuse         local accumulation 

2 

1 

1)    can reduce dram reads of filter/fmap by up to 500   
1 
2)    partial sum accumulation does not have to access dram 
2 
       example: 

 dram access in alexnet can be reduced 
 from 2896m to 61m (best case) 

 

12 

spatial architecture for dnn 

dram 

global buffer (100     500 kb) 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

alu 

  
alu 
  

alu 

local memory hierarchy 
       global buffer 
       direct inter-pe network 
       pe-local memory (rf) 

processing 
element (pe) 
  
reg file 

0.5     1.0 kb 

control 
  

13 

low-cost local data access 

dram 

global 
buffer 

pe  pe 

pe 

alu 

fetch data to run  
a mac here 

rf 

0.5     1.0 kb 
pe 

noc: 200     1000 pes 
100     500 kb 
buffer 
dram 

normalized energy cost* 

1   (reference) 
1   
2   

6   

alu 
alu 

alu 

alu 

alu 

200   

* measured from a commercial 65nm process 

14 

low-cost local data access 

how to exploit     data reuse and     local accumulation 
with limited low-cost local storage? 

2 

1 

rf 

0.5     1.0 kb 
pe 

noc: 200     1000 pes 
100     500 kb 
buffer 
dram 

normalized energy cost* 

1   (reference) 
1   
2   

6   

alu 
alu 

alu 

alu 

alu 

200   

* measured from a commercial 65nm process 

15 

low-cost local data access 

how to exploit     data reuse and     local accumulation 
with limited low-cost local storage? 

2 

1 

specialized processing dataflow required! 

rf 

0.5     1.0 kb 
pe 

noc: 200     1000 pes 
100     500 kb 
buffer 
dram 

normalized energy cost* 

1   (reference) 
1   
2   

6   

alu 
alu 

alu 

alu 

alu 

200   

* measured from a commercial 65nm process 

16 

dataflow taxonomy 

       weight stationary (ws) 
       output stationary (os) 
       no local reuse (nlr) 

[chen et al., isca 2016] 

17 

weight stationary (ws) 

psum 

  

  

  
w0 

weight 

global buffer 

activation 
  

  

  

  

  

  

  

  

  

w1 

w2 

w3 

w4 

w5 

w6 

  
w7 

pe 

       minimize weight read energy consumption 

       maximize convolutional and filter reuse of weights 
       broadcast  activations  and  accumulate  psums 

spatially across the pe array. 

  

18 

ws example: nn-x (neuflow) 

a 3  3 2d convolution engine 

activations 

weights 

  

  

  

[farabet et al., iccv 2009] 

19 

psums 

output stationary (os) 

  

  
activation 

  

  
p0 

psum 

p1 

  

p2 

global buffer 
weight 
  

  

  

p3 

p4 

  

p5 

  

p6 

  

  

  
p7 

pe 

       minimize partial sum r/w energy consumption 

       maximize local accumulation 

       broadcast/multicast  filter  weights  and  reuse 

activations spatially across the pe array

  

20 

os example: shidiannao 

top-level architecture 

pe architecture 
weights 

activations 

psums 

[du et al., isca 2015] 

21 

no local reuse (nlr) 

weight 
activation 

  

global buffer 

  

  

  

pe 

psum 

       use a large global buffer as shared storage 
       reduce dram access energy consumption 

       multicast activations, single-cast weights, and 
accumulate psums spatially across the pe array 

  

22 

nlr example: ucla 

psums 

activations 

weights 

[zhang et al., fpga 2015] 

23 

nlr example: tpu 

top-level architecture 

matrix multiply unit 

weights 

psums 

activations 

[jouppi et al., isca 2017] 

24 

taxonomy: more examples 

       weight stationary (ws) 
[chakradhar, isca 2010]  [nn-x (neuflow), cvprw 2014] 
[park, isscc 2015]  [isaac, isca 2016]  [prime, isca 2016] 

       output stationary (os) 
[peemen, iccd 2013] 
[gupta, icml 2015] 

[shidiannao, isca 2015] 

[moons, vlsi 2016] 

       no local reuse (nlr) 
[diannao, asplos 2014]  [dadiannao, micro 2014] 
[zhang, fpga 2015] 

[tpu, isca 2017] 

25 

energy efficiency comparison 
       same total area 
       alexnet conv layers 

       256 pes 
       batch size = 16 

/

p
o
y
g
r
e
normalized  
n
energy/mac 
e

 
.

m
r
o
n

2

1.5

1

0.5

0

variants of os 

ws 
ws

osa 
osa

osb 
osc 
osb osc
dataflows
id98 dataflows 

nlr 
nlr

rs

[chen et al., isca 2016] 

26 

energy efficiency comparison 
       same total area 
       alexnet conv layers 

       256 pes 
       batch size = 16 

/

p
o
y
g
r
e
normalized  
n
energy/mac 
e

 
.

m
r
o
n

2

1.5

1

0.5

0

variants of os 

ws 
ws

osa 
osa

osb 
osc 
osb osc
dataflows
id98 dataflows 

nlr 
nlr

row 
rs

stationary 

[chen et al., isca 2016] 

27 

energy-efficient dataflow: 
row stationary (rs) 

       maximize reuse and accumulation at rf 
       optimize  for  overall  energy  efficiency 

instead for only a certain data type 

[chen et al., isca 2016] 

28 

row stationary: energy-efficient dataflow 

29 

filter 

input fmap 

output fmap 

* 

= 

29 

30 

1d row convolution in pe 

input fmap 
a  b  c  d  e 

partial sums 

a  b  c 

= 

filter 
a  b  c 

* 

reg file 

pe 

c 

b  a 

e 

d  c 

b 

a 

  
  

  

30 

31 

1d row convolution in pe 

input fmap 
a  b  c  d  e 

partial sums 

a  b  c 

= 

filter 
a  b  c 

* 

reg file 
c 

b  a 

e  d 

c 

b  a 

a 

pe 

  
  

  

31 

32 

1d row convolution in pe 

input fmap 
a  b  c  d  e 

partial sums 

a  b  c 

= 

filter 
a  b  c 

* 

reg file 
c 

b  a 

e 

d 

c  b 

b 

  
  

pe 

  

a 

32 

33 

1d row convolution in pe 

input fmap 
a  b  c  d  e 

partial sums 

a  b  c 

= 

filter 
a  b  c 

* 

reg file 
c 

b  a 

e 

d  c 

c 

pe 

  
  

  

b  a 

33 

34 

1d row convolution in pe 

       maximize row convolutional reuse in rf 

       keep a filter row and fmap sliding window in rf 

       maximize row psum accumulation in rf 

reg file 
c 

b  a 

e 

d  c 

c 

pe 

  
  

  

b  a 

34 

35 

2d convolution in pe array 

pe 1 

row 1 

* 

row 1 

* 

= 

35 

36 

2d convolution in pe array 

row 1 

pe 1 

row 1 

* 

row 1 

pe 2 

row 2 

* 

row 2 

pe 3 

row 3 

* 

row 3 

* 

= 

36 

37 

2d convolution in pe array 

row 1 

row 2 

pe 1 

pe 4 

row 1 

* 

row 1 

row 1 

* 

row 2 

pe 2 

pe 5 

row 2 

* 

row 2 

row 2 

* 

row 3 

pe 3 

pe 6 

row 3 

* 

row 3 

row 3 

* 

row 4 

* 

= 

* 

= 

37 

38 

2d convolution in pe array 

row 1 

row 2 

row 3 

pe 1 

pe 4 

pe 7 

row 1 

* 

row 1 

row 1 

* 

row 2 

row 1 

* 

row 3 

pe 2 

pe 5 

pe 8 

row 2 

* 

row 2 

row 2 

* 

row 3 

row 2 

* 

row 4 

pe 3 

pe 6 

pe 9 

row 3 

* 

row 3 

row 3 

* 

row 4 

row 3 

* 

row 5 

* 

= 

* 

= 

* 

= 

38 

39 

convolutional reuse maximized 

row 1 

row 2 

row 3 

pe 1 

pe 4 

pe 7 

row 1 

* 

row 1 

row 1 

* 

row 2 

row 1 

* 

row 3 

pe 2 

pe 5 

pe 8 

row 2 

* 

row 2 

row 2 

* 

row 3 

row 2 

* 

row 4 

pe 3 

pe 6 

pe 9 

row 3 

* 

row 3 

row 3 

* 

row 4 

row 3 

* 

row 5 

filter rows are reused across pes horizontally 

39 

40 

convolutional reuse maximized 

row 1 

row 2 

row 3 

pe 1 

pe 4 

pe 7 

row 1 

* 

row 1 

row 1 

* 

row 2 

row 1 

* 

row 3 

pe 2 

pe 5 

pe 8 

row 2 

* 

row 2 

row 2 

* 

row 3 

row 2 

* 

row 4 

pe 3 

pe 6 

pe 9 

row 3 

* 

row 3 

row 3 

* 

row 4 

row 3 

* 

row 5 

fmap rows are reused across pes diagonally 

40 

maximize 2d accumulation in pe array 

41 

row 1 

row 2 

row 3 

pe 1 

pe 4 

pe 7 

row 1 

* 

row 1 

row 1 

* 

row 2 

row 1 

* 

row 3 

pe 2 

pe 5 

pe 8 

row 2 

* 

row 2 

row 2 

* 

row 3 

row 2 

* 

row 4 

pe 3 

pe 6 

pe 9 

row 3 

* 

row 3 

row 3 

* 

row 4 

row 3 

* 

row 5 

partial sums accumulate across pes vertically 

41 

dimensions beyond 2d convolution 

1  multiple fmaps 

2  multiple filters 

3  multiple channels 

42 

 filter reuse in pe 

1  multiple fmaps 

2 multiple filters 

3 multiple channels 

c

r

r

c

c

h

h

h

h

channel 1 

filter 1 
row 1 

filter 1 
channel 1  row 1 

fmap 1 
row 1 

psum 1 
row 1 

= 

fmap 2 
row 1 

psum 2 
row 1 

= 

* 

* 

43 

 filter reuse in pe 

1  multiple fmaps 

2 multiple filters 

3 multiple channels 

c

r

r

c

c

h

h

h

h

channel 1 

filter 1 
row 1 

* 

fmap 1 
row 1 

psum 1 
row 1 

= 

filter 1 
channel 1  row 1 

fmap 2 
row 1 

psum 2 
row 1 

= 

* 

share the same filter row 

44 

 filter reuse in pe 

1  multiple fmaps 

2 multiple filters 

3 multiple channels 

c

r

r

c

c

h

h

h

h

channel 1 

filter 1 
row 1 

* 

fmap 1 
row 1 

psum 1 
row 1 

= 

filter 1 
channel 1  row 1 

fmap 2 
row 1 

psum 2 
row 1 

= 

* 

share the same filter row 

processing in pe: concatenate fmap rows 

channel 1 

filter 1 
row 1 

* 

fmap 1 & 2 

row 1 

row 1 

psum 1 & 2 
row 1  row 1 

= 

45 

fmap reuse in pe 

1 multiple fmaps 

2  multiple filters 

3 multiple channels 

c

c

r

r

r

r

c

h

h

channel 1 

filter 1 
row 1 

filter 2 
channel 1  row 1 

fmap 1 
row 1 

psum 1 
row 1 

= 

fmap 1 
row 1 

psum 2 
row 1 

= 

* 

* 

46 

fmap reuse in pe 

1 multiple fmaps 

2  multiple filters 

3 multiple channels 

c

c

r

r

r

r

c

h

h

channel 1 

filter 1 
row 1 

* 

fmap 1 
row 1 

psum 1 
row 1 

= 

filter 2 
channel 1  row 1 

fmap 1 
row 1 

psum 2 
row 1 

= 

* 

share the same fmap row 

47 

fmap reuse in pe 

1 multiple fmaps 

2  multiple filters 

3 multiple channels 

c

c

r

r

r

r

c

h

h

channel 1 

filter 1 
row 1 

* 

fmap 1 
row 1 

psum 1 
row 1 

= 

filter 2 
channel 1  row 1 

fmap 1 
row 1 

psum 2 
row 1 

= 

* 

share the same fmap row 

processing in pe: interleave filter rows 

channel 1 

filter 1 & 2 

fmap 1 
row 1 

= 

* 

psum 1 & 2 

48 

channel accumulation in pe 

1 multiple fmaps 

2 multiple filters 

3  multiple channels 

c

r

r

c

h

h

channel 1 

filter 1 
row 1 

filter 1 
channel 2  row 1 

fmap 1 
row 1 

psum 1 
row 1 

= 

fmap 1 
row 1 

psum 1 
row 1 

= 

* 

* 

49 

channel accumulation in pe 

1 multiple fmaps 

2 multiple filters 

3  multiple channels 

c

r

r

c

h

h

channel 1 

filter 1 
row 1 

* 

fmap 1 
row 1 

psum 1 
row 1 

= 

filter 1 
channel 2  row 1 

fmap 1 
row 1 

psum 1 
row 1 

= 

* 
+ 

row 1 

accumulate psums 

row 1 

=  row 1 

50 

channel accumulation in pe 

1 multiple fmaps 

2 multiple filters 

3  multiple channels 

c

r

r

c

h

h

channel 1 

filter 1 
row 1 

filter 1 
channel 2  row 1 

* 

* 

fmap 1 
row 1 

psum 1 
row 1 

= 

fmap 1 
row 1 

psum 1 
row 1 

= 

accumulate psums 

processing in pe: interleave channels 

channel 1 & 2 

filter 1 

* 

fmap 1 

psum 
row 1 

= 

51 

52 

dnn processing     the full picture 

multiple fmaps: 

multiple filters: 

filter 1

*

filter 1

image 1 & 2
fmap 

psum 1 & 2

=

filter 1 & 2

image 1
fmap 

psum 1 & 2

=
image 1
fmap 

psum

*
*

multiple channels: 
map rows from multiple fmaps, filters and channels to same pe 

=

to exploit other forms of reuse and local accumulation 

52 

optimal mapping in row stationary 
id98 configurations 

c

r

1

r
   
c

r

m
r

c

h

1

h
   

c

h

n

h

m

e

1

e
   

e

n

e

hardware resources 

global buffer

alu

alu

alu

alu

alu

alu

alu

alu

alu

alu

alu

alu

alu

alu

alu

alu

  
optimization 

compiler 
(mapper) 

  

row stationary mapping 

row 1

row 2

row 3

*

*

*

row 1

row 2

pe

pe

pe

row 1

row 2

row 3

row 3

*

*

*

row 2

row 3

pe

pe

pe

row 1

row 2

row 4

row 3

pe

pe

pe

row 3

row 4

row 5

*

*

*

multiple fmaps:

multiple filters:

filter 1

*

filter 1 & 2

multiple channels:

filter 1

image 1 & 2
fmap

psum 1 & 2

=

image 1
fmap

=
image 1
fmap

*
*

psum 1 & 2

psum

=

[chen et al., isca 2016] 

53 

computer architecture analogy 

compilation 

dnn shape and size 

(program) 

execution 

processed 

data 

  
mapper 
  

(compiler) 

dataflow,     
(architecture) 

implementation  

details 
(  arch) 

  

  
dnn accelerator 
  

(processor) 

  

mapping 
(binary) 

input 
data 

[chen et al., micro top-picks 2017] 

54 

dataflow 
simulation results 

55 

56 

evaluate	reuse	in	di   erent	data   ows	

       weight	sta7onary	

       output	sta7onary	

       minimize	movement	of	   lter	weights	

       minimize	movement	of	par5al	sums	

       no	pe	local	storage.	maximize	global	bu   er	size.	

       no	local	reuse	

       row	sta7onary	
	
evaluation setup 
       same total area 
       256 pes 
       alexnet 
       batch size = 16 

normalized energy cost*

rf

pe

buffer

dram

alu
alu

alu

alu

alu

1   (reference)
1  
2  

6  

200  

56 

variants of output stationary 

osa 

osb 

osc 

parallel  
output region 

m 

e 

# output channels 
# output activations 

notes 

e 

single 
multiple 

targeting 

conv layers 

m 

e 

e 

multiple 
multiple 

m 

e 

e 

multiple 
single 

targeting 
fc layers 

57 

dataflow comparison: conv layers 

psums 
weights 
activations 

2

1.5

normalized 
energy/mac 

1

0.5

0

ws 

osa  osb  osc  nlr 

id98 dataflows 

rs 

rs optimizes for the best overall energy efficiency 

[chen et al., isca 2016] 

58 

dataflow comparison: conv layers 

2

1.5

normalized 
energy/mac 

1

0.5

0

alu 
rf 
noc 
buffer 
dram 

ws 

osa  osb  osc  nlr 

id98 dataflows 

rs 

rs uses 1.4       2.5   lower energy than other dataflows 

[chen et al., isca 2016] 

59 

dataflow comparison: fc layers 

psums 
weights 
activations 

normalized 
energy/mac 

2

1.5

1

0.5

0

ws 

osa  osb  osc  nlr 

id98 dataflows 

rs 

rs uses at least 1.3   lower energy than other dataflows 

[chen et al., isca 2016] 

60 

row stationary: layer breakdown 

2.0e10	

1.5e10	

1.0e10	

0.5e10	

normalized 

energy 

(1 mac = 1) 

alu 
rf 
noc 
buffer 
dram 

0	

l1 

l3 

l4 
l2 
conv layers 

l5 

l6 

l7 

l8 

fc layers 

[chen et al., isca 2016] 

61 

row stationary: layer breakdown 

2.0e10	

1.5e10	

1.0e10	

0.5e10	

normalized 

energy 

(1 mac = 1) 

alu 
rf 
noc 
buffer 
dram 

0	

l1 

l3 

l4 
l2 
conv layers 
rf dominates 

l5 

l6 

l7 

l8 

fc layers 

[chen et al., isca 2016] 

62 

row stationary: layer breakdown 

2.0e10	

1.5e10	

1.0e10	

0.5e10	

normalized 

energy 

(1 mac = 1) 

alu 
rf 
noc 
buffer 
dram 

0	

l1 

l3 

l4 
l2 
conv layers 
rf dominates 

l5 

l6 

l7 

l8 

fc layers 
dram dominates 

[chen et al., isca 2016] 

63 

row stationary: layer breakdown 

2.0e10	

1.5e10	

1.0e10	

0.5e10	

normalized 

energy 

(1 mac = 1) 

total energy 
20% 

80% 

alu 
rf 
noc 
buffer 
dram 

0	

l1 

l3 

l4 
l2 
conv layers 

l5 

l6 

l7 

l8 

fc layers 

conv layers dominate energy consumption! 

64 

hardware architecture 
for rs dataflow 

[chen et al., isscc 2016] 

65 

66 

eyeriss dnn accelerator 

link clock  core clock  

  
  

filter 

input fmap 
decomp 
output fmap 
comp 
relu 

global 
buffer 
sram 

 

108kb 

filt 

fmap 

psum 

psum 

   

 

dnn accelerator 
14  12 pe array 

    

    

    

    

   

 

66 

64 bits 

off-chip dram 

[chen et al., isscc 2016] 

data delivery with on-chip network 

did98 accelerator 
14  12 pe array 

link clock  core clock  

data delivery patterns 

filter 

  
  

filter  
delivery 

input image 
decomp 
output image 
fmap 
relu 
comp 
delivery 

buffer 
sram 

 

108kb 

filt 

fmap 

psum 

psum 

   

 

how to accommodate different shapes with fixed pe array? 

off-chip dram 

64 bits 

    

    

    

    

   

 

67 

logical to physical mappings 
folding 
replication 
27 
13 
.. 
.. 
.. 
.. 
.. 
.. 

alexnet 
layer 2 

3 

5 

.
.
 

.
.
 

.
.
 

.
.
 

.
.
 

alexnet 
layer 3-5 

.
.
 

14 
13 
13 
13 
13 

12 

3 
3 
3 
3 

14 

14 

13 

12 

5 

5 

physical pe array 

physical pe array 

68 

logical to physical mappings 
folding 
replication 
27 
13 
.. 
.. 
.. 
.. 
.. 
.. 

alexnet 
layer 2 

3 

5 

.
.
 

.
.
 

.
.
 

.
.
 

.
.
 

alexnet 
layer 3-5 

.
.
 

14 
13 
13 
13 
13 

12 

3 
3 
3 
3 

14 

14 

13 

12 

5 

5 

unused pes 

are 

clock gated 

physical pe array 

physical pe array 

69 

data delivery with on-chip network 

link clock  core clock  

data delivery patterns 

filter 

  
  

filter  
delivery 

input image 
decomp 
output image 
image 
relu 
comp 
delivery 

buffer 
sram 

 

108kb 

filt 

img 

psum 

psum 

   

 

did98 accelerator 
14  12 pe array 

    

    

    

    

   

 

compared to broadcast, multicast saves >80% of noc energy 

off-chip dram 

64 bits 

70 

chip spec & measurement results 

technology  tsmc 65nm lp 1p9m 

on-chip buffer  108 kb 

# of pes  168 

scratch pad / pe  0.5 kb 
core frequency  100     250 mhz 

peak performance  33.6     84.0 gops 
word bit-width  16-bit fixed-point 
filter width: 1     32 
filter height: 1     12 
num. filters: 1     1024 
num. channels: 1     1024 
horz. stride: 1   12 
vert. stride: 1, 2, 4 

natively supported 
dnn shapes 

4000   m 

global 
buffer 

spatial array 
(168 pes) 

4
0
0
0
  
m

 

 

to	support	2.66	gmacs	[8	billion	16-bit	inputs	(16gb)	and	2.7	billion	
outputs	(5.4gb)],	only	requires	208.5mb	(bu   er)	and	15.4mb	(dram)			

[chen et al., isscc 2016] 

71 

summary of dnn dataflows 

       weight stationary 

       minimize movement of filter weights 
       popular with processing-in-memory architectures 

       output stationary 

       minimize movement of partial sums 
       different variants optimized for conv or fc layers 

       no pe local storage    maximize global buffer size 

       no local reuse 

 

       row stationary 

       adapt to the nn shape and hardware constraints 
       optimized for overall system energy efficiency 

 

72 

fused layer 
       dataflow across multiple layers 

[alwani et al., micro 2016] 

73 

advanced technology 
opportunities 

isca tutorial (2017) 

website: http://eyeriss.mit.edu/tutorial.html  

joel emer, vivienne sze, yu-hsin chen 

1 

advanced storage technology 

       embedded dram (edram) 

       increase on-chip storage capacity 

       3d stacked dram  

       e.g. hybrid memory cube memory (hmc), high 

bandwidth memory (hbm) 

       increase memory bandwidth  

2 

edram (dadiannao) 

       advantages of edram 

       2.85x higher density than sram 
       321x more energy-efficient than dram (ddr3) 

       store weights in edram (36mb) 

       target fully connected layers since dominated by weights 
 

16 parallel 

tiles 

[chen et al., dadiannao, micro 2014] 

3 

stacked dram (neurocube) 

       neurocube on hyper memory cube logic die  

       6.25x higher bw than ddr3 

       hmc (16 ch x 10gb/s) > ddr3 bw (2 ch x 12.8gb/s) 

       computation closer to memory (reduce energy) 
 

[kim et al., neurocube, isca 2016] 

4 

stacked dram (tetris) 
       explores the use of hmc with the eyeriss spatial 

architecture and row stationary dataflow 

       allocates more area to the computation (pe array) than 
on-chip memory (global buffer) to exploit the low energy 
and high throughput properties of the hmc 
       1.5x energy reduction, 4.1x higher throughput vs. 2-d dram 

eyeriss 
design 

[gao et al., tetris, asplos 2017] 

5 

analog computation 

       conductance = weight 
       voltage = input 
       current = voltage    conductance  
       sum currents for addition 

v1 

v2 

output = weight    input

   

g1 

i1 = v1  g1 

g2 

i2 = v2  g2 

input = v1, v2,     
filter weights = g1, g2,     (conductance) 

i = i1 + i2  
= v1  g1 + v2  g2 

weight stationary dataflow 

figure source:  isaac, isca 2016 

6 

memristor computation 

use memristors as programmable 

weights (resistance) 

       advantages 

       high density (< 10nm x 10nm size*) 

       ~30x smaller than sram** 
       1.5x smaller than dram** 

       non-volatile 
       operates at low voltage 
       computation within memory (in situ) 

       reduce data movement 

*[govoreanu et al., iedm 2011], **itrs 2013 

7 

memristor 

[chi et al., isca 2016] 

8 

challenges with memristors 

       limited precision  
       a/d and d/a conversion 
       array size and routing 

       wire dominates energy for array size of 1k    1k 
       ir drop along wire can degrade read accuracy 

       write/programming energy 
       multiple pulses can be costly 

       variations & yield 

       device-to-device, cycle-to-cycle 
       non-linear conductance across range  

[eryilmaz et al., isqed 2016] 

9 

isaac 

       edram using memristors  
       16-bit dot-product operation 

       8 x 2-bits per memristors 
       1-bit per cycle computation 

v1 

v2 

g1 

i1 = v1.g1 

g2 

i2 = v2.g2 

i = i1 + i2 =v1.g1 + v2.g2 

s&h 

s&h 

s&h 

s&h 

s&h 

s&h 

s&h 

s&h 

adc 

shift & add 

[shafiee et al., isca 2016] 

10 

isaac 

eight 128x128 
arrays per ima 

 

12 imas per tile 

[shafiee et al., isca 2016] 

11 

prime 

       bit precision for each 256x256 reram array 

       3-bit input, 4-bit weight (2x for 6-bit input and 8-bit weight) 
       dynamic fixed point (6-bit output) 

       reconfigurable to be main memory or accelerator 

       4-bit mlc computation; 1-bit slc for storage 

[chi et al., isca 2016] 

12 

fabricated memristor crossbar 

       transistor-free metal-oxide 

12x12 crossbar 
       a single-layer id88 

(linear classification)  

       3x3 binary image 
       10 inputs x 3 outputs x 2 
differential weights = 60 
memristors 

[prezioso et al., nature 2015] 

13 

optical neural network 

id127 in the optical domain 

the photodetection rate is 100   ghz 

 

   in principle, such a system can be at least 

two orders of magnitude faster than 
electronic neural networks (which are 

restricted to a ghz clock rate)    

[shen et al., nature photonics 2017] 

14 

dnn model and  
hardware co-design  

isca tutorial (2017) 

website: http://eyeriss.mit.edu/tutorial.html  

joel emer, vivienne sze, yu-hsin chen, tien-ju yang 

1 

approaches 

       reduce size of operands for storage/compute 

       floating point    fixed point 
       bit-width reduction 
       non-linear quantization 

 

       reduce number of operations for storage/compute 

       exploit activation statistics (compression) 
       network pruning 
       compact network architectures 

2 

cost of operations 

relative energy cost 

operation: 

8b add 
16b add 
32b add 
16b fp add 
32b fp add 
8b mult 
32b mult 
16b fp mult 
32b fp mult 
32b sram read (8kb) 
32b dram read 

energy 
(pj) 
0.03 
0.05 
0.1 
0.4 
0.9 
0.2 
3.1 
1.1 
3.7 
5 
640 

relative area cost 

area 
(  m2) 
36 
67 
137 
1360 
4184 
282 
3495 
1640 
7700 
n/a 
n/a 

[horowitz,    computing   s energy problem (and what we can do about it)   , isscc 2014]  

1 

10  102  103  104 

1 

10  102  103 

3 

number representation 

fp32 
 
 
fp16 
 
 
int32 
 
 
int16 
 
 
int8 

1 
s 

8 
e 

1  5 
s  e 

10 
m 

1 
s 

1 
s 

7 
1 
s  m 

15 
m 

31 
m 

23 
m 

range 

accuracy 

10-38     1038  

.000006% 

6x10-5 - 6x104  

.05% 

0     2x109 

0     6x104 

0     127 

   

   

   

image source: b. dally 

4 

floating point    fixed point 

floating point 

sign 

exponent (8-bits) 

mantissa (23-bits) 

32-bit float 

1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 

-1.42122425 x 10-13 

s = 1 

e = 70 

m = 20482 

fixed point 

sign 

mantissa (7-bits) 

0 1 1 0 0 1 1 0 

integer  
(4-bits) 
s = 0 

fractional 
(3-bits) 

m=102 

8-bit  
fixed 

12.75 

5 

n-bit precision 

for no loss in precision, m is determined based on largest 
filter size (in the range of 10 to 16 bits for popular dnns) 

2n+m-bits 

2n-bits 

+ 

accumulate 

quantize 
to n-bits 

output 
(n-bits) 

weight  
(n-bits) 

activation  
(n-bits) 

n x n 
multiply 

6 

dynamic fixed point 

floating point 

sign 

exponent (8-bits) 

mantissa (23-bits) 

32-bit float 

1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 

-1.42122425 x 10-13 

s = 1 

e = 70 

m = 20482 

fixed point 

sign 

mantissa (7-bits) 

sign 

mantissa (7-bits) 

8-bit  

dynamic  

fixed 

12.75 

0 1 1 0 0 1 1 0 

integer  
([7-f ]-bits) 
s = 0 

fractional 
(f-bits) 
f = 3 

m=102 

8-bit  

dynamic  

fixed 

0 

1 1 0 0 1 1 0 

fractional 
(f-bits) 
m=102 

f = 9 

0.19921875 

s = 0 

allow f to vary based on data type and layer 

7 

impact on accuracy 

top-1 accuracy 
on of caffenet 
on id163 

w/o fine tuning 

[gysel et al., ristretto, iclr 2016] 

8 

avoiding dynamic fixed point 

batch id172    centers    dynamic range 

alexnet 
(layer 6) 

image source: moons 

et al, wacv 2016 

   centered    dynamic ranges might reduce need for 

dynamic fixed point 

9 

nvidia pascal 

   new half-precision, 16-bit 
floating point instructions 
deliver over 21 teraflops for 
unprecedented training 
performance. with 47 tops 
(tera-operations per second) 
of performance, new 8-bit 
integer instructions in pascal 
allow ai algorithms to deliver 
real-time responsiveness for 
deep learning id136.     
 
    nvidia.com (april 2016) 

10 

google   s tensor processing unit (tpu) 

    with its tpu google has 
seemingly focused on delivering 
the data really quickly by cutting 
down on precision. specifically, 
it doesn   t rely on floating point 
precision like a gpu  
   . 
instead the chip uses integer 
math   tpu used 8-bit integer.    
 
- next platform (may 19, 2016) 

[jouppi et al., isca 2017] 

11 

precision varies from layer to layer 

[judd et al., arxiv 2016] 

[moons et al., wacv 2016] 

12 

bitwidth scaling (speed) 

bit-serial processing: reduce bit-width    skip cycles 

speed up of 2.24x vs. 16-bit fixed 

[judd et al., stripes, cal 2016] 

13 

bitwidth scaling (power) 

reduce bit-width    
shorter critical path 
   reduce voltage 

power reduction of 
2.56x vs. 16-bit fixed 
on alexnet layer 2 

[moons et al., vlsi 2016] 

14 

binary nets 

       binary connect (bc) 

       weights {-1,1}, activations 32-bit float 
       mac    addition/subtraction 
       accuracy loss: 19% on alexnet 
 

[courbariaux, nips 2015] 

       binarized neural networks (bnn) 

       weights {-1,1}, activations {-1,1} 
       mac    xnor 
       accuracy loss: 29.8% on alexnet 
 

[courbariaux, arxiv 2016] 

binary filters 

15 

scale the weights and activations  

       binary weight nets (bwn) 

       weights {-  ,   }    except first and last layers are 32-bit float 
       activations: 32-bit float 
          determined by the l1-norm of all weights in a layer 
       accuracy loss: 0.8% on alexnet 

       xnor-net 

hardware needs to support 
both activation precisions 

       weights {-  ,   } 
       activations {-  i,   i}    except first and last layers are 32-bit float 
         i determined by the l1-norm of all activations across channels 
       accuracy loss: 11% on alexnet 
 
 

scale factors (  ,   i) can change per layer or position in filter 

for given position i of the input feature map  

[rastegari et al., bwn & xnor-net, eccv 2016] 

16 

 

xnor-net 

[rastegari et al., bwn & xnor-net, eccv 2016] 

17 

ternary nets 

       allow for weights to be zero 

       increase sparsity, but also increase number of bits (2-bits) 
 

       ternary weight nets (twn) 

[li et al., arxiv 2016] 

       weights {-w, 0, w}    except first and last layers are 32-bit float 
       activations: 32-bit float 
       accuracy loss: 3.7% on alexnet 

       trained ternary quantization (ttq) 

[zhu et al., iclr 2017] 
       weights {-w1, 0, w2}    except first and last layers are 32-bit float 
       activations: 32-bit float 
       accuracy loss: 0.6% on alexnet 

18 

non-linear quantization 

       precision refers to the number of levels  

       number of bits = log2 (number of levels) 

       quantization: mapping data to a smaller set of levels 

       linear, e.g., fixed-point 
       non-linear 

       computed 
       table lookup 

objective: reduce size to improve speed and/or reduce energy 

while preserving accuracy 

19 

computed non-linear quantization  

 

log domain quantization 

product =  x * w 

product = x << w 

[lee et al., lognet, icassp 2017] 

20 

log domain computation 

only activation 
in log domain 

both weights 
and activations 
in log domain 

max, bitshifts, adds/subs 

[miyashita et al., arxiv 2016] 

21 

log domain quantization 

       weights: 5-bits for conv, 4-bit for fc; activations: 4-bits 
       accuracy loss: 3.2% on alexnet 
 

shift and add 

ws 

[miyashita et al., arxiv 2016], 
[lee et al., lognet, icassp 2017] 

22 

reduce precision overview 

       learned mapping of data to quantization levels   

(e.g., id116) 

implement with 
look up table 

[han et al., iclr 2016] 

       additional properties 

       fixed or variable (across data types, layers, channels, etc.) 

23 

non-linear quantization table lookup 

trained quantization: find k weights via id116 id91 
 to reduce number of unique weights per layer (weight sharing) 

example: alexnet (no accuracy loss) 
256 unique weights for conv layer 

16 unique weights for fc layer 

smaller weight 

memory 
weight  
memory 
crsm x 
log2u-bits 

weight  
index 

(log2u-bits) 

overhead 
weight 
decoder/
dequant 
u x 16b 

weight  
(16-bits) 

input 

activation  
(16-bits) 

does not reduce 
precision of mac 
  
  

mac 

  
  

output 

activation 
(16-bits) 

consequences: narrow weight memory and second access from (small) table 

[han et al., deep compression, iclr 2016] 

24 

summary of reduce precision 

weights  
(# of bits) 
8 
8 
2* 

activations 
(# of bits) 
10 
8 
32 

accuracy loss vs. 
32-bit float (%) 
0.4 
0.6 
3.7 

category 

method 

dynamic fixed 
point 

reduce weight 

reduce weight 
and activation 

non-linear 

2* 

w/o fine-tuning 
w/ fine-tuning 
ternary weights 
networks (twn) 
trained ternary 
quantization (ttq) 
binary connect (bc)  1 
binary weight net 
1* 
(bwn) 
binarized neural net 
(bnn) 
xnor-net 
lognet 
weight sharing 

1 

32 

32 
32 

1 

1* 
1 
5(conv), 4(fc)  4 
8(conv), 4(fc)  16 

0.6 

19.2 
0.8 

29.8 

11 
3.2 
0 

* first and last layers are 32-bit float 

full list @ [sze et al., arxiv, 2017] 

25 

reduce number of ops and weights 

       exploit activation statistics 
       network pruning 
       compact network architectures 
       knowledge distillation 
 

26 

sparsity in fmaps 

many zeros in output fmaps after relu 

relu 

9  -1  -3 
1  -5  5 
-2  6  -1 

9  0  0 
1  0  5 
0  6  0 

# of activations 

# of non-zero activations 

(normalized) 

1 
0.8 
0.6 
0.4 
0.2 
0 

1 

2 

3 

conv layer 

4 

5 

27 

i/o compression in eyeriss 

link clock  core clock  

filter 

filt 

did98 accelerator 
14  12 pe array 

  
  

input image 
decomp 
output image 
  
comp 
relu 

    

img 

run-length compression (rlc)  
buffer 
example: 
sram 
input:   0, 0, 12, 0, 0, 0, 0, 53, 0, 0, 22,     
108kb 
run level run level run level term 
output (64b): 
psum 
   
   
2  12  4 
5b  16b 

22  0 
1b 

psum 

    

    

53 

2 

 

 

 

5b  16b 5b  16b 
    

64 bits 

off-chip dram 

[chen et al., isscc 2016] 

28 

compression reduces dram bw 

	
)
b
m

dram  
access  
(mb)  

(
	
s
s
e
c
c
a
m
a
r
d

	

6 
6	
5	
4 
4	
3	
2 
2	
1	
0 
0	

uncompressed	

1.2   

1.4   

compressed	

1.7   

1.8   

1.9   

2 
2	

5 
1 
5	
1	
alexnet conv layer 

4 
4	
alexnet	conv	layer	

3 
3	

uncompressed 
fmaps + weights 

rle compressed 
fmaps + weights 

simple rlc within 5% - 10% of theoretical id178 limit 

[chen et al., isscc 2016] 

29 

img 

filt 

input 
psum 

data	ga&ng	/	zero	skipping	in	eyeriss	

  

== 0 

  

image 

scratch pad 
(12x16b reg) 

zero 
buffer 
filter  

  
  
(225x16b sram) 

scratch pad 

  

  

  

enable 

  

   

skip mac and mem reads  
when image data is zero. 
reduce pe power by 45% 
2-stage 
pipelined  
multiplier 

accumulate 
input psum 

output 
psum   

  

0 

1 

partial sum 
scratch pad 
(24x16b reg) 

0 

1 
0 
    

reset 

[chen et al., isscc 2016] 

30 

cnvlutin 

       process convolution layers 
       built on top of dadiannao (4.49% area overhead) 
       speed up of 1.37x (1.52x with activation pruning) 

[albericio et al., isca 2016] 

31 

pruning activations 

remove small activation values 

speed up 11% (id163) 

reduce power 2x (mnist) 

cnvlutin 

minerva 

[albericio et al., isca 2016] 

[reagen et al., isca 2016] 

32 

pruning     make weights sparse 

       optimal brain damage 
1.    choose a reasonable network 

architecture 

2.    train network until reasonable 

solution obtained 

3.    compute the second derivative 

for each weight 

4.    compute saliencies (i.e. impact 

on training error) for each weight 

5.    sort weights by saliency and 
delete low-saliency weights 
iterate to step 2 

6.   

retraining 

[lecun et al., nips 1989] 

33 

pruning     make weights sparse 

prune based on magnitude of weights 

(cid:69)(cid:72)(cid:73)(cid:82)(cid:85)(cid:72)(cid:3)(cid:83)(cid:85)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74)

(cid:68)(cid:73)(cid:87)(cid:72)(cid:85)(cid:3)(cid:83)(cid:85)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74)

(cid:83)(cid:85)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)
(cid:86)(cid:92)(cid:81)(cid:68)(cid:83)(cid:86)(cid:72)(cid:86)

(cid:83)(cid:85)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)
(cid:81)(cid:72)(cid:88)(cid:85)(cid:82)(cid:81)(cid:86)

(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:3)(cid:38)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)

(cid:51)(cid:85)(cid:88)(cid:81)(cid:72)(cid:3)(cid:38)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)

(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:3)(cid:58)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:86)

example: alexnet 
weight reduction: conv layers 2.7x, fc layers 9.9x 
(most reduction on fully connected layers) 
overall: 9x weight reduction, 3x mac reduction 

[han et al., nips 2015] 

34 

speed up of weight pruning on cpu/gpu 

on fully connected layers only 

average speed up of 3.2x on gpu, 3x on cpu, 5x on mgpu 

intel core i7 5930k: mkl cblas gemv, mkl spblas csrmv 
nvidia geforce gtx titan x: cublas gemv, cusparse csrmv 
nvidia tegra k1: cublas gemv, cusparse csrmv 
 
batch size = 1 

[han et al., nips 2015] 

35 

key metrics for embedded dnn 

       accuracy    measured on dataset 
       speed    number of macs 
       storage footprint    number of weights 
       energy    ? 

36 

energy-aware pruning 

       # of weights alone is not a good metric for 

energy  
       example (alexnet): 

       # of weights (fc layer) > # of weights (conv layer)  
       energy (fc layer) < energy (conv layer) 

       use energy evaluation method to estimate dnn 

energy 
       account for data movement 
 

[yang et al., cvpr 2017] 

37 

energy-evaluation methodology 

id98 shape configuration 
(# of channels, # of filters, etc.) 

hardware energy costs of each 
mac and memory access 

# acc. at mem. level 1 
# acc. at mem. level 2 

   

# acc. at mem. level n 

 

# of macs 

  

  
  
  

memory 
accesses 
optimization 

# of macs 
calculation 

  

id98 weights and input data 
[0.3, 0, -0.4, 0.7, 0, 0, 0.1,    ] 

edata 

ecomp 

energy 

l1 l2 l3 

    

id98 energy consumption  

evaluation tool available at http://eyeriss.mit.edu/energy.html  

38 

key observations 

       number of weights alone is not a good metric for energy 
       all data types should be considered  
 

computa&on	

10%	

input	feature	map	

25%	

energy	consump&on	

of	googlenet	

weights	

22%	

output	feature	map	

43%	

[yang et al., cvpr 2017] 

39 

energy consumption of existing dnns 

	

y
c
a
r
u
c
c
a
5
-
p
o
t

	

93%	
91%	
89%	
87%	
85%	
83%	
81%	
79%	
77%	

5e+08	

resnet-50	

googlenet	

vgg-16	

alexnet	

squeezenet	

5e+09	

5e+10	

normalized	energy	consump&on	

original	dnn	

deeper id98s with fewer weights do not necessarily consume less 

energy than shallower id98s with more weights 

[yang et al., cvpr 2017] 

40 

	

y
c
a
r
u
c
c
a
5
-
p
o
t

	

magnitude-based weight pruning 

93%	
91%	
89%	
87%	
85%	
83%	
81%	
79%	
77%	

5e+08	

resnet-50	

googlenet	

vgg-16	

squeezenet	

alexnet	

alexnet	

squeezenet	

5e+09	

5e+10	

original	dnn	

normalized	energy	consump&on	
magnitude-based	pruning	[6]	

[han	et	al.,	nips	2015]	

reduce number of weights by removing small magnitude weights 

41 

energy-aware pruning 

resnet-50	

googlenet	

vgg-16	

googlenet	

1.74x 

squeezenet	

alexnet	

alexnet	

squeezenet	

alexnet	

squeezenet	

	

y
c
a
r
u
c
c
a
5
-
p
o
t

	

93%	
91%	
89%	
87%	
85%	
83%	
81%	
79%	
77%	

5e+08	

5e+09	

5e+10	

normalized	energy	consump&on	

original	dnn	 magnitude-based	pruning	[6]	 energy-aware	pruning	(this	work)	
remove weights from layers in order of highest to lowest energy 

3.7x reduction in alexnet / 1.6x reduction in googlenet 

dnn models available at http://eyeriss.mit.edu/energy.html  

42 

energy estimation tool 

website: https://energyestimation.mit.edu/  

input dnn configuration file 

output dnn energy breakdown across layers 

[yang et al., cvpr 2017] 

43 

compression of weights & activations 
       compress weights and activations between dram  

and accelerator 

       variable length / huffman coding 

example: 

value: 16   b0     compressed code: {1   b0} 
value: 16   bx     compressed code: {1   b1, 16   bx} 

       tested on alexnet    2   overall bw reduction 

[moons et al., vlsi 2016; han et al., iclr 2016] 

44 

sparse matrix-vector dsp 
       use csc rather than csr for spmxv 
compressed sparse row (csr)  

compressed sparse column (csc)  

n 

m 

reduce memory bandwidth (when not m >> n) 

for dnn, m = # of filters, n = # of weights per filter 

[dorrance et al., fpga 2014] 

45 

eie: a sparse id202 engine 
       process fully connected layers (after deep compression) 
       store weights column-wise in run length format 
       read relative column when input is non-zero 

supports fully connected layers only 

input 

~a  0 a1

0

   

a3  

 
p e0
 
p e1
weights 
p e2
 
p e3
 

0

w0,0w0,1 0 w0,3
0
0 w1,2 0
0 w2,1 0 w2,3
0
0
0
0 w4,2w4,3
0
0
0
w5,0 0
0 w6,3
0
0
0 w7,1 0
0

0bbbbbbbbbbbbb@

=

1ccccccccccccca

output 

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

0bbbbbbbbbbbbb@

1ccccccccccccca

relu)

b0
b1
 b2
b3
 b4
b5
b6
 b7
[han et al., isca 2016] 

1ccccccccccccca

dequantize weight 

keep track of location 

output stationary dataflow  

46 

sparse id98 (sid98) 
supports convolutional layers 

densely packed 

storage of weights 

and activations 

all-to all 

multiplication of 

weights and activations 

mechanism to add to 
scattered partial sums  

a
b
c
d
e
f 

x
y
z

= 

x
y
z
x
y
z

a * 
a * 
a * 
b * 
b * 
b * 
   

 

scatter 
network 

accumulate muls 

pe frontend 

pe backend 

input stationary dataflow  
[parashar et al., isca 2017] 

47 

structured/coarse-grained pruning  
       scalpel 

       prune to match the underlying data-parallel hardware 

organization for speed up 

  

example: 2-way simd 

dense weights 

sparse weights 

[yu et al., isca 2017] 

48 

compact network architectures 

       break large convolutional layers into a series 

of smaller convolutional layers 
       fewer weights, but same effective receptive field 
 

       before training: network architecture design 
 
       after training: decompose trained filters 

49 

network architecture design 

build network with series of small filters 

googlenet/inception v3 

5x5 filter 

5x1 filter 

apply sequentially 

decompose 

1x5 filter 

separable  

filters 

vgg-16 

5x5 filter 

decompose 

two 3x3 filters 

apply sequentially 

50 

network architecture design 

reduce size and computation with 1x1 filter (bottleneck) 

figure source: 
stanford cs231n 

used in network in network(nin) and googlenet 

[lin et al., arxiv 2013 / iclr 2014] 

[szegedy et al., arxiv 2014 / cvpr 2015] 

51 

network architecture design 

reduce size and computation with 1x1 filter (bottleneck) 

figure source: 
stanford cs231n 

used in network in network(nin) and googlenet 

[lin et al., arxiv 2013 / iclr 2014] 

[szegedy et al., arxiv 2014 / cvpr 2015] 

52 

network architecture design 

reduce size and computation with 1x1 filter (bottleneck) 

figure source: 
stanford cs231n 

used in network in network(nin) and googlenet 

[lin et al., arxiv 2013 / iclr 2014] 

[szegedy et al., arxiv 2014 / cvpr 2015] 

53 

bottleneck in popular dnn models 

resnet 

googlenet 

compress 

expand 

compress 

54 

squeezenet 

reduce weights by reducing number of input 

channels by    squeezing    with 1x1 
50x fewer weights than alexnet 

(no accuracy loss) 

fire module 

[f.n. iandola et al., arxiv, 2016]] 

55 

energy consumption of existing dnns 

	

y
c
a
r
u
c
c
a
5
-
p
o
t

	

93%	
91%	
89%	
87%	
85%	
83%	
81%	
79%	
77%	

5e+08	

resnet-50	

googlenet	

vgg-16	

alexnet	

squeezenet	

5e+09	

5e+10	

normalized	energy	consump&on	

original	dnn	

deeper id98s with fewer weights do not necessarily consume less 

energy than shallower id98s with more weights 

[yang et al., cvpr 2017] 

56 

decompose trained filters 

after training, perform low-rank approximation by applying tensor 
decomposition to weight kernel; then fine-tune weights for accuracy 

r = canonical rank 

[lebedev et al., iclr 2015] 

57 

decompose trained filters 

visualization of filters 

original  approx. 

       speed up by 1.6     2.7x on cpu/gpu for conv1, 

conv2 layers 

       reduce size by 5 - 13x for fc layer  
       < 1% drop in accuracy 

[denton et al., nips 2014] 

58 

decompose trained filters on phone 

tucker decomposition 

[kim et al., iclr 2016] 

59 

knowledge distillation 

(cid:86)(cid:70)(cid:82)(cid:85)(cid:72)(cid:86) 

class  
probabilities 

 
(cid:38)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:91) 
x
a
m
dnn a 
(teacher)  s

o

t
f

(cid:38)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:91) 
dnn b 
(teacher) 

 
x
a
m

t
f

o
s

try to match 

(cid:54)(cid:76)(cid:80)(cid:83)(cid:79)(cid:72) dnn 

(student) 

 
x
a
m

t
f
o
s

[bucilu et al., kdd 2006],[hinton et al., arxiv 2015]  

60 

benchmarking metrics  
for dnn hardware 

isca tutorial (2017) 

website: http://eyeriss.mit.edu/tutorial.html  

joel emer, vivienne sze, yu-hsin chen 

1 

metrics overview 
       how can we compare designs? 
       target metrics 

       accuracy 
       power 
       throughput 
       cost  

       additional factors 

       external memory bandwidth  
       required on-chip storage 
       utilization of cores 

2 

download benchmarking data 

       input (http://image-net.org/)  

       sample subset from id163 validation dataset 

 

       widely accepted state-of-the-art dnns  

(model zoo: http://caffe.berkeleyvision.org/) 
       alexnet 
       vgg-16 
       googlenet-v1 
       resnet-50 

3 

metrics for dnn algorithm 

       accuracy 
       network architecture  

       # layers, filter size, # of filters, # of channels 

       # of weights (storage capacity) 
       number of non-zero (nz) weights  

       # of macs (operations) 

       number of non-zero (nz) macs 

4 

metrics of dnn algorithms 

alexnet 

19.8 

227x227 

5 

3, 5,11 
3 - 256 
96 - 384 

1, 4 
2.3m 
666m 

vgg-16 

googlenet (v1) 

resnet-50 

8.80 

224x224 

16 
3 

3 - 512 
64 - 512 

1 

14.7m 
15.3g 

10.7 

224x224 

21 

1, 3 , 5, 7 
3 - 1024 
64 - 384 

1, 2 
6.0m 
1.43g 

7.02 

224x224 

49 

1, 3, 7 
3 - 2048 
64 - 2048 

1, 2 
23.5m 
3.86g 

3 

3 

1 
2m 
2m 
25.5m 
3.9g 
*single crop results: https://github.com/jcjohnson/id98-benchmarks  

1 
1m 
1m 
7m 
1.43g 

58.6m 
58.6m 
61m 
724m 

124m 
124m 
138m 
15.5g 

5 

metrics 
accuracy (top-5 error)* 
input 
# of conv layers 
filter sizes 
# of channels 
# of filters 
stride 
# of weights 
# of macs 
# of fc layers 
# of weights 
# of macs 
total weights 
total macs 

metrics of dnn algorithms 

metrics 
accuracy (top-5 error)* 
# of conv layers 
# of weights 
# of macs 
# of nz macs** 
# of fc layers 
# of weights 
# of macs 
# of nz macs** 
total weights 
total macs 
# of nz macs** 

alexnet 

vgg-16 

googlenet (v1) 

resnet-50 

19.8 
5 

2.3m 
666m 
394m 

3 

58.6m 
58.6m 
14.4m 
61m 
724m 
409m 

8.80 
16 

14.7m 
15.3g 
7.3g 

3 

124m 
124m 
17.7m 
138m 
15.5g 
7.3g 

10.7 
21 
6.0m 
1.43g 
806m 

1 
1m 
1m 
639k 
7m 
1.43g 
806m 

7.02 
49 

23.5m 
3.86g 
1.5g 

1 
2m 
2m 
1.8m 
25.5m 
3.9g 
1.5g 

*single crop results: https://github.com/jcjohnson/id98-benchmarks  
**# of nz macs computed based on 50,000 validation images 

6 

metrics of dnn algorithms 

metrics 
accuracy (top-5 error) 
# of conv layers 
# of weights 
# of macs 
# of nz weights 
# of nz macs 
# of fc layers 
# of weights 
# of macs 
# of nz weights 
# of nz macs 
total weights 
total macs 
# of nz weights 
# of nz macs 

alexnet 

alexnet (sparse) 

19.8 
5 

2.3m 
666m 
2.3m 
394m 

3 

58.6m 
58.6m 
58.6m 
14.4m 
61m 
724m 
61m 
409m 

19.8 
5 

2.3m 
666m 
863k 
207m 

3 

58.6m 
58.6m 
5.9m 
2.1m 
61m 
724m 
6.8m 
209m 

# of nz macs computed based on 50,000 validation images 

7 

metrics for dnn hardware 

       measure energy and dram access relative to 

number of non-zero macs and bit-width of macs 
       account for impact of sparsity in weights and activations  
       normalize dram access based on operand size 

       energy efficiency of design 

       pj/(non-zero weight & activation) 

       external memory bandwidth 

       dram operand access/(non-zero weight & activation) 

       area efficiency 

       total chip mm2/multi (also include process technology) 
       accounts for on-chip memory 

8 

asic benchmark (e.g. eyeriss) 

asic specs 
65nm lp tsmc (1.0v) 
process technology 
200 
clock frequency (mhz) 
168 
number of multipliers 
total core area (mm2) /total # of multiplier 
0.073 
total on-chip memory (kb) / total # of multiplier  1.14 
measured or simulated 
if simulated, syn or pnr? which corner? 

measured 
n/a 

9 

asic benchmark (e.g. eyeriss) 

layer by layer breakdown for alexnet conv layers 

metric 
batch size 
bit/operand 
energy/ 
non-zero macs 
(weight & act) 
dram access/
non-zero macs 
runtime  
power 

units 
# 
# 

l1 

l2 

l3 

l4 

l5  overall* 

4 
16 

pj/mac 

16.5 

18.2 

29.5 

41.6 

32.3 

21.7 

operands/
mac 
ms 
mw 

0.006  0.003  0.007  0.010  0.008 

0.005 

20.9 
332 

41.9 
288 

23.6 
266 

18.4 
235 

10.5 
236 

115.3 
278 

* weighted average of conv layers 

10 

website to summarize results 
       http://eyeriss.mit.edu/benchmarking.html  
       send results or feedback to: eyeriss@mit.edu  

asic specs 
process technology 

clock frequency 
(mhz) 
number of multipliers 
core area (mm2) /
multiplier 
on-chip memory 
(kb) / multiplier 
measured or 
simulated 
if simulated, syn or 
pnr? which corner? 

input 
65nm lp 
tsmc (1.0v) 
200 

168 
0.073 

1.14 

measured 

n/a 

units 
metric 
text 
name of id98 
# 
# of images tested 
# 
bits per operand 
# 
batch size 
# of non zero macs  # 
ms 
runtime  
utilization vs. peak  % 
power 
energy/non-zero  
macs 
dram access/non-
zero macs 

mw 
pj/mac 

operands
/mac 

input 
alexnet 
100 
16 
4 
409m 
115.3 
41 
278 
21.7 

0.005 

11 

implementation-specific metrics 

different devices may have implementation-specific metrics 

example: fpgas 

metric 
device  
utilization 

dsp 
bram 
lut 
ff 
performance density 

alexnet 
units 
xilinx virtex-7 xc7v690t 
text 
2,240 
# 
1,024 
# 
186,251 
# 
205,704 
# 
gops/slice  8.12e-04 

12 

tutorial summary 

       dnns are a critical component in the ai revolution, delivering 
record breaking accuracy on many important ai tasks for a wide 
range of applications; however, it comes at the cost of high 
computational complexity 

       efficient processing of dnns is an important area of research with 

many promising opportunities for innovation at various levels of 
hardware design, including algorithm co-design 

       when considering different dnn solutions it is important to evaluate 

      

with the appropriate workload in term of both input and model, 
and recognize that they are evolving rapidly. 
it   s important to consider a comprehensive set of metrics when 
evaluating different dnn solutions: accuracy, speed, energy, and 
cost 

1 

resources 

       eyeriss project: http://eyeriss.mit.edu  

       tutorial slides 
       benchmarking 
       energy modeling 
       mailing list for updates 

       http://mailman.mit.edu/mailman/listinfo/eems-news  

       paper based on today   s tutorial: 

       v. sze, y.-h. chen, t-j. yang, j. emer,    efficient processing 
of deep neural networks: a tutorial and survey   , arxiv, 2017 

2 

references 

isca tutorial (2017) 

website: http://eyeriss.mit.edu/tutorial.html  

joel emer, vivienne sze, yu-hsin chen, tien-ju yang 

1 

references (alphabetical by author) 

      
      
      

      

      

      

      
      

      

      

      

albericio, jorge, et al. "cnvlutin: ineffectual-neuron-free deep neural network computing," isca, 2016. 
alwani, manoj, et al., "fused layer id98 accelerators," micro, 2016 
chakradhar, srimat, et al., "a dynamically configurable coprocessor for convolutional neural networks," 
isca, 2010 
chen, tianshi, et al., "diannao: a small-footprint high-throughput accelerator for ubiquitous machine-
learning," asplos, 2014 
chen, yu-hsin, et al. "eyeriss: an energy-efficient reconfigurable accelerator for deep convolutional 
neural networks,    isscc, 2016. 
chen, yu-hsin, joel emer, and vivienne sze. "eyeriss: a spatial architecture for energy-efficient dataflow 
for convolutional neural networks,    isca, 2016. 
chen, yunji, et al. "dadiannao: a machine-learning supercomputer,    micro, 2014. 
chi, ping, et al. "prime: a novel processing-in-memory architecture for neural network computation in 
reram-based main memory," isca 2016. 
cong, jason, and bingjun xiao. "minimizing computation in convolutional neural networks." international 
conference on id158s. springer international publishing, 2014. 
courbariaux, matthieu, and yoshua bengio. "binarynet: training deep neural networks with weights and 
activations constrained to+ 1 or-1." arxiv preprint arxiv:1602.02830 (2016). 
courbariaux, matthieu, yoshua bengio, and jean-pierre david. "binaryconnect: training deep neural 
networks with binary weights during propagations," nips, 2015. 

2 

references (alphabetical by author) 

      
      

      

      
      
      

      

      

      

      
      

      

dean, jeffrey, et al., "large scale distributed deep networks," nips, 2012 
denton, emily l., et al. "exploiting linear structure within convolutional networks for efficient 
evaluation," nips, 2014. 
dorrance, richard, fengbo ren, and dejan markovi  . "a scalable sparse matrix-vector multiplication 
kernel for energy-efficient sparse-blas on fpgas." proceedings of the 2014 acm/sigda international 
symposium on field-programmable gate arrays. acm, 2014. 
du, zidong, et al., "shidiannao: shifting vision processing closer to the sensor," isca, 2015 
eryilmaz, sukru burc, et al. "neuromorphic architectures with electronic synapses.    isqed, 2016. 
esser, steven k., et al., "convolutional networks for fast, energy-efficient neuromorphic computing," 
pnas 2016 
farabet, clement, et al., "an fpga-based stream processor for embedded real-time vision with 
convolutional networks," iccv 2009 
gokhale, vinatak, et al., "a 240 g-ops/s mobile coprocessor for deep neural networks," cvpr workshop, 
2014 
govoreanu, b., et al. "10   10nm 2 hf/hfo x crossbar resistive ram with excellent performance, reliability 
and low-energy operation,    iedm, 2011. 
gupta, suyog, et al., "deep learning with limited numerical precision," icml, 2015 
gysel, philipp, mohammad motamedi, and soheil ghiasi. "hardware-oriented approximation of 
convolutional neural networks." arxiv preprint arxiv:1604.03168 (2016). 
han, song, et al. "eie: efficient id136 engine on compressed deep neural network," isca, 2016. 

3 

references (alphabetical by author) 

      
      

      
      
      

      

      

      
      

      

      

      

han, song, et al. "learning both weights and connections for efficient neural network,    nips, 2015. 
han, song, huizi mao, and william j. dally. "deep compression: compressing deep neural network with 
pruning, trained quantization and huffman coding," iclr, 2016. 
he, kaiming, et al. "deep residual learning for image recognition," cvpr, 2016. 
horowitz, mark.    computing's energy problem (and what we can do about it),    isscc, 2014. 
iandola, forrest n., et al. "squeezenet: alexnet-level accuracy with 50x fewer parameters and< 1mb model 
size," iclr, 2017. 
ioffe, sergey, and szegedy, christian, "batch id172: accelerating deep network training by 
reducing internal covariate shift," icml, 2015 
jermyn, michael, et al., "neural networks improve brain cancer detection with raman spectroscopy in the 
presence of operating room light artifacts," journal of biomedical optics, 2016 
jouppi, norman p., et al. "in-datacenter performance analysis of a tensor processing unit,    isca, 2017. 
judd, patrick, et al. "reduced-precision strategies for bounded memory in deep neural nets." arxiv 
preprint arxiv:1511.05236 (2015). 
judd, patrick, jorge albericio, and andreas moshovos. "stripes: bit-serial deep neural network 
computing." ieee computer architecture letters (2016). 
kim, duckhwan, et al. "neurocube: a programmable digital neuromorphic architecture with high-density 
3d memory." isca 2016 
kim, yong-deok, et al. "compression of deep convolutional neural networks for fast and low power mobile 
applications." iclr 2016 

4 

references (alphabetical by author) 

      

      

      

      
      
      

      

      

      
      

      

      
      

krizhevsky, alex, ilya sutskever, and geoffrey e. hinton. "id163 classification with deep convolutional 
neural networks," nips, 2012. 
lavin, andrew, and gray, scott, "fast algorithms for convolutional neural networks," arxiv preprint  arxiv:
1509.09308 (2015) 
lecun, yann, et al. "gradient-based learning applied to document recognition." proceedings of the ieee 
86.11 (1998): 2278-2324. 
lecun, yann, et al. "optimal brain damage," nips, 1989. 
lin, min, qiang chen, and shuicheng yan. "network in network." arxiv preprint arxiv:1312.4400 (2013). 
mathieu, michael, mikael henaff, and yann lecun. "fast training of convolutional networks through ffts." 
arxiv preprint arxiv:1312.5851 (2013). 
merola, paul a., et al. "artificial brains. a million spiking-neuron integrated circuit with a scalable 
communication network and interface," science, 2014 
moons, bert, and marian verhelst. "a 0.3   2.6 tops/w precision-scalable processor for real-time large-scale 
convnets." symposium on vlsi circuits (vlsi-circuits), 2016. 
moons, bert, et al. "energy-efficient convnets through approximate computing,    wacv, 2016. 
parashar, angshuman, et al. "sid98: an accelerator for compressed-sparse convolutional neural 
networks." isca, 2017. 
park, seongwook, et al., "a 1.93tops/w scalable deep learning/id136 processor with tetra-parallel 
mimd architecture for big-data applications," isscc, 2015 
peemen, maurice, et al., "memory-centric accelerator design for convolutional neural networks," iccd, 2013 
prezioso, mirko, et al. "training and operation of an integrated neuromorphic network based on metal-oxide 
memristors." nature 521.7550 (2015): 61-64. 

5 

references (alphabetical by author) 

      

      

      

      

      

      

      
      

      
      

      

      
      

 

rastegari, mohammad, et al. "xnor-net: id163 classification using binary convolutional neural 
networks,    eccv, 2016 
reagen, brandon, et al. "minerva: enabling low-power, highly-accurate deep neural network accelerators,    
isca, 2016. 
rhu, minsoo, et al., "vdnn: virtualized deep neural networks for scalable, memory-efficient neural network 
design," micro, 2016 
russakovsky, olga, et al. "id163 large scale visual recognition challenge." international journal of 
id161 115.3 (2015): 211-252. 
sermanet, pierre, et al. "overfeat: integrated recognition, localization and detection using convolutional 
networks,    cvpr, 2014. 
shafiee, ali, et al. "isaac: a convolutional neural network accelerator with in-situ analog arithmetic in 
crossbars." proc. isca. 2016. 
shen, yichen, et al. "deep learning with coherent nanophotonic circuits." nature photonics, 2017. 
simonyan, karen, and andrew zisserman. "very deep convolutional networks for large-scale image 
recognition." arxiv preprint arxiv:1409.1556 (2014). 
szegedy, christian, et al. "going deeper with convolutions,    cvpr, 2015. 
vasilache, nicolas, et al. "fast convolutional nets with fbfft: a gpu performance evaluation." arxiv preprint 
arxiv:1412.7580 (2014). 
yang, tien-ju, et al. "designing energy-efficient convolutional neural networks using energy-aware 
pruning," cvpr, 2017 
yu, jiecao, et al. "scalpel: customizing dnn pruning to the underlying hardware parallelism." isca, 2017. 
zhang, chen, et al., "optimizing fpga-based accelerator design for deep convolutional neural networks," 
fpga, 2015 

6 

