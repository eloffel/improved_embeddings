how to do good research, 

get it published in 

sigkdd and get it cited!

eamonn keogh 

computer science & engineering department 

university of california - riverside 

riverside, ca 92521 
eamonn@cs.ucr.edu

disclaimers i
disclaimers i

     i don   t have a magic bullet for publishing in sigkdd
    this is simply my best effort to the community, especially 

young faculty, grad students and    outsiders   .

     for every piece of advice where i tell you    you 
should do this    or    you should never do this      
    you will be able to find counterexamples, including ones 

that won best paper awards etc.

     i will be critiquing some published papers (including 

some of my own), however i mean no offence. 
    of course, these are published papers, so the authors 

could legitimately say i am wrong.

disclaimers ii
disclaimers ii

     these slides are meant to be presented, and then 

studied offline. to allow them to be self-contained 
like this, i had to break my rule about keeping the 
number of words to a minimum.

     you have a pdf copy of these slides, if you want a 

powerpoint version, email me.

     i plan to continually update these slides, so if you 

have any feedback/suggestions/criticisms please let 
me know.     

disclaimers iii
disclaimers iii

     many of the positive examples are mine, making 

this tutorial seem self indulgent and vain.

     i did this simply because   

    i know what reviewers said for my papers.

    i know the reasoning behind the decisions in my papers.

    i know when earlier versions of my papers got rejected, 

and why, and how this was fixed. 

disclaimers iiii
disclaimers iiii

     many of the ideas i will share are very simple, you 

might find them insultingly simple.

     nevertheless at least half of papers submitted to 
sigkdd have at least one of these simple flaws. 

the following people offered advice
the following people offered advice

     geoff webb
     frans coenen
     cathy blake
     michael pazzani
     lane desborough
     stephen north
     fabian moerchen
     ankur jain
     themis palpanas
    
     howard j. hamilton
     mark last
     chen li
     magnus lie hetland
     david jensen
     chris clifton
     oded goldreich

jeff scargle

johannes fuernkranz

     michalis vlachos
     claudia bauzer medeiros
     chunsheng yang
     xindong wu
     lee giles 
    
     vineet chaoji
     stephen few
     wolfgang jank
     claudia perlich
     mitsunori ogihara
     hui xiong
     chris drummond 
     charles ling 
     charles elkan
    
     saeed salem

jieping ye

     tina eliassi-rad
     parthasarathy srinivasan
     mohammad hasan
     vibhu mittal
     chris giannella
     frank vahid
     carla brodley
     ansaf salleb-aouissi
     tomas skopal
     frans coenen
     sang-hee lee 
     michael carey
     vijay atluri
     shashi shekhar
    
     hui yang

jennifer windom

my students: jessica lin, chotirat ratanamahatana, li wei ,xiaopeng xi, dragomir yankov, lexiang 
ye, xiaoyue (elaine) wang , jin-wien shieh, abdullah mueen, qiang zhu, bilson campana

these people are not responsible for any controversial or incorrect claims made here

outline
outline

     the review process
     writing a sigkdd paper

     finding problems/data

     framing problems
     solving problems

     tips for writing

     motivating your work
     clear writing
     clear figures 

     the top ten reasons papers get rejected

     with solutions 

the curious case of srikanth
the curious case of 

srikanth krishnamurthy
krishnamurthy

     in 2004 srikanth   s student submitted a paper to mobicom
     deciding to change the title, the student resubmitted the 

paper, accidentally submitting it as a new paper

     one version of the paper scored 1,2 and 3, and was rejected, 

the other version scored a 3,4 and 5, and was accepted!

     this    natural    experiments suggests that the reviewing 

process is random, is it really that bad?

a look at the 
30 papers were 
accepted
reviewing 
statistics for 
a recent 
sigkdd

(i cannot say what year)

mean number of reviews 3.02

6

5

4

3

2

1

104 papers 
accepted

0

0

50

100

150

200

mean and standard deviation 
among review scores for 
papers submitted to recent 
sigkdd

paper id
250

300

350

400

450

500

    papers accepted after a discussion, not solely based on the mean score. 
    these are final scores, after reviewer discussions. 
    the variance in reviewer scores is much larger than the differences in 
the mean score, for papers on the boundary between accept and reject. 
     in order to halve the standard deviation we must quadruple the 
number of reviews.

conference 
30 papers were 
reviewing is an 
accepted
imperfect system.
we must learn to live 
with rejection.
all we can do is try 
to make sure that 
our paper lands as 
far left as possible 

6

5

4

3

2

1

104 papers 
accepted

0

0

50

100

150

200

mean and standard deviation 
among review scores for 
papers submitted to recent 
sigkdd

paper id
250

300

350

400

450

500

    at least three papers with a score of 3.67 (or lower) must have been 
accepted. but there were a total of 41 papers that had a score of 3.67.
    that means there exist at least 38 papers that were rejected, that had 
the same or better numeric score as some papers that were accepted.
    bottom line: with very high id203, multiple papers will be 
rejected in favor of less worthy papers.

a sobering 
30 papers were 
accepted
experiment 

6

5

4

3

2

1

0

0

50

100

150

200

paper id
250

300

350

400

450

500

    suppose i add one reasonable review to each paper.
    a reasonable review is one that is drawn uniformly from the range of 
one less than the lowest score to one higher than the highest score.
    if we do this, then on average, 14.1 papers move across the 
accept/reject borderline. this suggests a very brittle system.

but the good 
30 papers were 
accepted
news is   
most of us only 
need to 
improve a little 
to improve our 
odds a lot.

6

5

4

3

2

1

104 papers 
accepted

0

0

50

100

150

200

mean and standard deviation 
among review scores for 
papers submitted to recent 
sigkdd

paper id
250

300

350

400

450

500

    suppose you are one of the 41 groups in the green (light) area. if you 
can convince just one reviewer to increase their ranking by just one 
point, you go from near certain reject to near certain accept.
    suppose you are one of the 140 groups in the blue (bold) area. if you 
can convince just one reviewer to increase their ranking by just one 
point, you go from near certain reject to a good chance at accept.  

idealized algorithm for writing a paper
idealized algorithm for writing a paper

and during

before and 

during research)
research)

        find problem/data
find problem/data
start writing  ((yesyes, start writing 
        start writing  
, start writing before
        do research/solve problem
do research/solve problem
        finish 95% draft
finish 95% draft
        send preview to mock reviewers 
send preview to mock reviewers 
send preview to the rival authors (virtually or literally)
        send preview to the rival authors 
(virtually or literally)
        revise using checklist.
revise using checklist.
        submit
submit

o
n
e
 
m
o
n
t
h
 
b
e
f
o
r
e
 
d
e
a
d
l
i
n
e

what makes a good research problem?
what makes a good research problem?

        it is important:

        you can get real data

it is important: if you can solve it, you can make money, 
if you can solve it, you can make money, 
or save lives, or help children learn a new language, or...
or save lives, or help children learn a new language, or...
you can get real data: doing dna analysis of the loch 
: doing dna analysis of the loch 
ness monster would be interesting, but      
ness monster would be interesting, but
you can make incremental progress: some problems are 
        you can make incremental progress
: some problems are 
allall--oror--nothing. such problems may be too risky for young 
nothing. such problems may be too risky for young 
scientists.
scientists.
there is a clear metric for success: some problems fulfill 
: some problems fulfill 
the criteria above, but it is hard to know when you are 
the criteria above, but it is hard to know when you are 
making progress on them.
making progress on them.

        there is a clear metric for success

finding problems/finding data
finding problems/finding data

        finding a good problem can be the hardest part 
finding a good problem can be the hardest part 
of the whole process.
of the whole process.
        once you have a problem, you will need data
once you have a problem, you will need data      
        as i shall show in the next few slides, finding 
as i shall show in the next few slides, finding 
problems and finding data are best integrated.
problems and finding data are best integrated.

        however, the obvious way to find problems is 
however, the obvious way to find problems is 
lots of papers, both in sigkdd 
of papers, both in sigkdd 
the best, read lots
the best, read 
and elsewhere.
and elsewhere.

domain experts as a source of problems
domain experts as a source of problems

        data miners are almost unique in that they can 
data miners are almost unique in that they can 
work with almost any scientist or business   
work with almost any scientist or business   
i have worked with anthropologists, 
        i have worked with anthropologists, 
nematologists, archaeologists, astronomers, 
nematologists, archaeologists, astronomers, 
entomologists, cardiologists, herpetologists, 
entomologists, cardiologists, herpetologists, 
electroencephalographers, geneticists, space 
, geneticists, space 
electroencephalographers
vehicle technicians etc
vehicle technicians etc
        such collaborations can be a rich source of 
such collaborations can be a rich source of 
interesting problems. 
interesting problems. 

        domain experts can help with the 

working with domain experts i
working with domain experts i
        getting problems from domain experts might come 
getting problems from domain experts might come 
with some bonuses
with some bonuses
domain experts can help with the motivation
..insects cause 40 billion dollars of damage to crops each year....
       
..insects cause 40 billion dollars of damage to crops each year
gnosis..
..compiling a dictionary of such patterns would help doctors diagnosis..
       
..compiling a dictionary of such patterns would help doctors dia
petroglyphs are one of the earliest expressions of abstract thinking, and a true hallmark... 
king, and a true hallmark... 
        petroglyphs are one of the earliest expressions of abstract thin
        domain experts sometimes have funding/internships etc
domain experts sometimes have funding/internships etc
        coco--authoring with domain experts can give you credibility.
authoring with domain experts can give you credibility.

motivation for the paper 
for the paper 

sigkdd 09

working with domain experts ii
working with domain experts ii

if i had asked my 

customers what they 

wanted, they would have 

said a faster horse

henry ford

     ford focused not on stated need but on latent need. 
     in working with domain experts, don   t just ask them 

what they want. instead, try to learn enough about their 
domain to understand their latent needs.

     in general, domain experts have little idea about what is 

hard/easy for computer scientists. 

working with domain experts iii
working with domain experts iii

concrete example:

     i once had a biologist spend an hour asking me about 

sampling/estimation. she wanted to estimate a quantity.
     after an hour i realized that we did not have to estimate 

it, we could compute an exact answer!

     the exact computation did take three days, but it had 

taken several years to gather the data. 

     understand the latent need. 

finding research problems
finding research problems

     suppose you think idea x is very good
     can you extend x by   

    making it more accurate (statistically significantly more accurate)
    making it faster (usually an order of magnitude, or no one cares) 
    making it an anytime algorithm
    making it an online (streaming) algorithm
    making it work for a different data type (including uncertain data)
    making it work on low powered devices
    explaining why it works so well
    making it work for distributed systems
    applying it in a novel setting (industrial/government track)
    removing a parameter/assumption
    making it disk-aware (if it is currently a main memory algorithm)
    making it simpler

finding research problems (examples)
finding research problems (examples)

    
    

suppose you think idea x is a very good
can you extend x by   

     making it more accurate (statistically significantly more accurate)
     making it faster (usually an order of magnitude, or no one cares)
     making it an anytime algorithm
     making it an online (streaming) algorithm
     making it work for a different data type (including uncertain data)
     making it work on low powered devices
    
     making it work for distributed systems
    
    
     making it disk-aware (if it is currently a main memory algorithm) 

applying it in a novel setting (industrial/government track)
removing a parameter/assumption

explaining why it works so well

     the nearest neighbor algorithm is very useful. i wondered if we 

could make it an anytime

algorithm   .  icdm06 [b].

     motif discovery is very useful for dna, would it be useful for time 

series?  sigkdd03 [c]

     the bottom-up algorithm is very useful for batch data, could we 

make it work in an online

setting?   icdm01 [d]

     chaos game visualization of dna is very useful, would it be useful 

for other kinds of data?  sdm05 [a]

 

 

[a] kumar, n.,  lolla  n.,  keogh, e.,  lonardi, s. , ratanamahatana, c. a. and wei, l. (2005). time-series bitmaps: icdm 2006
[b] ueno, xi, keogh, lee. anytime classification using the nearest neighbor algorithm with applications to stream mining. icdm  2006.
[c] chiu, b. keogh, e., & lonardi, s. (2003). probabilistic discovery of time series motifs. sigkdd 2003
[d] keogh, e., chu, s., hart, d. & pazzani, m. an online algorithm for segmenting time series. icdm 2001

finding research problems 
finding research problems 

    
    

suppose you think idea x is a very good
can you extend x by   

     making it more accurate (statistically significantly more accurate)
     making it faster (usually an order of magnitude, or no one cares)
     making it an anytime algorithm
     making it an online (streaming) algorithm
     making it work for a different data type (including uncertain data)
     making it work on low powered devices
    
     making it work for distributed systems
    
    
     making it disk-aware (if it is currently a main memory algorithm) 

applying it in a novel setting (industrial/government track)
removing a parameter/assumption

explaining why it works so well

     some people have suggested that this method can lead to 

incremental, boring, low-risk papers   
     perhaps, but there are 104 papers in sigkdd this year, they are 

not all going to be groundbreaking.

     sometimes ideas that seem incremental at first blush may turn out 

to be very exciting as you explore the problem. 

     an early career person might eventually go on to do high risk 
research, after they have a    cushion    of two or three lower-risk 
sigkdd papers.

framing research problems i
framing research problems i

as a reviewer, i am often frustrated by how many people don   t have 
a clear problem statement in the abstract (or the entire paper!)
can you write a research statement for your paper in a single sentence? 

    x is good for y (in the context of z).
    x can be extended to achieve y (in the context of z).
    the adoption of x facilitates y (for data in z format).
    an x approach to the problem of y mitigates the need for z.
(an anytime algorithm approach to the problem of nearest neighbor 
classification mitigates the need for high performance hardware) (ueno et al. icdm 06)

if i, as a reviewer, cannot form such a sentence for your paper 
after reading just the abstract, then your paper is usually doomed.

i hate it when a paper under review does 
not give a concise definition of the problem 

tina eliassi-rad

see talk by frans coenen on this topic
http://www.csc.liv.ac.uk/~frans/seminars/doingaphdseminarai2007.pdf

framing research problems ii
framing research problems ii

your research statement should be falsifiable

a real paper claims:

to the best of our knowledge, this is most 
sophisticated subsequence matching solution 
mentioned in the literature.

is there a way that we could show this is not true?

falsifiability (or refutability) is the logical possibility that an claim can be shown false by 
falsifiability (or refutability) is the logical possibility that an claim can be shown false by 
an observation or a physical experiment. that something is    falsifiable    does not mean it is 
an observation or a physical experiment. that something is    falsifiable    does not mean it is 
false; rather, that if it is false, then this can be shown by observation or experiment
false; rather, that if it is false, then this can be shown by observation or experiment 

falsifiability is the demarcation between 

science and nonscience

karl popper

framing research problems iii
framing research problems iii

examples of falsifiable claims:

    quicksort is faster than bubblesort.  (this may needed expanding, if the lists are.. )
    the x function lower bounds the dtw distance.
    the l2 distance measure generally outperforms l1 measure

(this needs some work (under what conditions etc), but it is falsifiable )

examples of unfalsifiable claims:

    we can approximately cluster dna with dft.

    any random arrangement of dna could be considered a    id91   .

    we present an alterative approach through fourier harmonic 
projections to enhance the visualization. the experimental results 
demonstrate significant improvement of the visualizations.

    since    enhance    and    improvement     are subjective and vague, this is unfalsifiable. note 
that it could be made falsifiable. consider: 

    we improve the mean time to find an embedded pattern by a factor of ten.
    we enhanced the separability of  weekdays and weekends, as measured by..

from the problem to the data
from the problem to the data

    at this point we have a concrete, falsifiable research problem 
    now is the time to get data!
by    now   , i mean months before the deadline. i have one of the largest collections of free datasets in 
the world. each year i am amazed at how many emails i get a few days before the sigkdd deadline 
that asks    we want to submit a paper to sigkdd, do you have any datasets that..    
    interesting, real (large, when appropriate) datasets greatly 
increase your papers chances.
    having good data will also help do better research, by 
preventing you from converging on unrealistic solutions. 
    early experience with real data can feed back into the finding 
and framing the research question stage.

    given the above, we are going to spend some time considering data..

is it ok to make data?
is it ok to make data?

there is a huge difference between   
we wrote a matlab script to create random trajectories

and   
we glued tiny radio 
transmitters to the backs 
of mormon crickets and 
tracked the trajectories

photo by jaime holguin

why is synthetic data so bad?
why is synthetic data so bad?

suppose you say    here are the 
results on our synthetic dataset:   

our 
method

accuracy

95%

their 
method

80%

this is good right? after all, you 
are doing much better than your 
rival.

why is synthetic data so bad?
why is synthetic data so bad?

suppose you say    here are the 
results on our synthetic dataset:   

our 
method
95%

their 
method
80%

accuracy

but as far as i know, you might 
have created ten versions of your 
dataset, but only reported one!
even if you did not do this 
consciously, you may have done it 
unconsciously.
at best, your making of your test 
data is a huge conflict of interest.

our 
method
80%
75%
90%
95%
85%

their 
method

85%
85%
90%
80%
95%

accuracy

accuracy

accuracy

accuracy

accuracy

why is synthetic data so bad?
why is synthetic data so bad?

note that is does not really make a difference if you have real 
data but you modify it somehow, it is still synthetic data..
a paper has a section heading:   results on two real data sets
but then we read   
we add some noises to a small number of shapes in both 
data sets to manually create some anomalies.

is this still real data? the answer is no, even if they authors 
had explained how they added noise (which they don   t).

note that there are probably a handful of circumstances were taking real data, doing an 
experiment, tweaking the data and repeating the experiment is genuinely illuminating.

synthetic data can lead to a contradiction
synthetic data can lead to a contradiction

     avoid the contradiction of claiming that the problem is 

very important, but there is no real data.

     if the problem is as important as you claim, a reviewer 

would wonder why there is no real data. 

     i encounter this contradiction very frequently, here is a 

real example:   
     early in the paper: the ability to process large 

datasets becomes more and more important   

     later in the paper: ..because of the lack of 

publicly available large datasets   

steady 
pointing

hand moving to 
shoulder level

hand moving 
down to grasp gun

hand moving 
above holster

hand at rest

0

10

20

30

40

50

60

70

80

90

in 2003, i spent two full days recording 
in 2003, i spent two full days recording 
a video dataset. the data consisted of 
a video dataset. the data consisted of 
my student chotirat (ann) 
my student chotirat (ann) 
ratanamahatana performing actions 
ratanamahatana performing actions 
in front of a green screen. 
in front of a green screen. 

was this a waste of two days?
was this a waste of two days?

i want to convince you 
that the effort it takes to 
find or create real data is 
worthwhile.

sdm 04

vldb 04

sigkdd 04

sdm 05

i have used this data in at least a dozen 
i have used this data in at least a dozen 
papers, and one dataset derived from it, the 
papers, and one dataset derived from it, the 
gun/nogun problem, has been used in 
gun/nogun problem, has been used in 
well over 100 other papers (all of which 
well over 100 other papers (all of which 
reference my work!)
reference my work!) 

sigkdd 09

spending the time to make/obtain/clean 
spending the time to make/obtain/clean 
good datasets will pay off in the long run
good datasets will pay off in the long run

 

the vast majority of papers on 
the vast majority of papers on 
shape mining use the mpeg- 
shape mining use the mpeg- 
7 dataset.
7 dataset. 

visually, they are telling us  : 
visually, they are telling us  : 
   i can tell the difference 
   i can tell the difference 
between mickey mouse and 
between mickey mouse and 
spoon   .
spoon   . 
the problem is not that i think 
the problem is not that i think 
this easy, the problem is i just 
this easy, the problem is i just 
don   t care.
don   t care. 
show me data i care about
show me data i care about

real data motivates your 
clever algorithms: part i

this figure tells me    if i rotate 
my hand drawn apples, then i 
will need to have a rotation 
invariant algorithm to find 
them   

in contrast, this figure tells me 
   even in this important 
domain, where tens of 
millions of dollars are spent 
each year, the robots that 
handle the wings cannot 
guarantee that they can 
present them in the same 
orientation each time. 
therefore i will need to have 
a rotation invariant algorithm    

figure 3: shapes of natural objects can be from different views 
of the same object, shapes can be rotated, scaled, skewed

figure 5: two sample wing images from a collection of 
drosophila images. note that the rotation of images can vary 
even in such a structured domain

real data motivates your 
clever algorithms: part ii

this figure tells me    if i use 
photoshop to take a chunk 
out of a drawing of an apple, 
then i will need an occlusion 
resistant algorithm to match it 
back to the original   

in contrast, this figure tells me 
   in this important domain of 
cultural artifacts it is common 
to have objects which are 
effectively occluded by 
breakage. therefore i will 
need to have an occlusion 
resistant algorithm    

figure 15: project points are frequently found with broken 
tips or tangs. such objects require lcss to find 
meaningful matches to complete specimens.

here is a great example. this 
here is a great example. this 
paper is not technically deep.
paper is not technically deep. 

however, instead of 
however, instead of 
classifying synthetic shapes, 
classifying synthetic shapes, 
they have a very cool problem 
they have a very cool problem 
(fish counting/classification) 
(fish counting/classification) 
and they made an effort to 
and they made an effort to 
create a very interesting 
create a very interesting 
dataset. 
dataset. 

show me data someone 
show me data someone 
cares about
cares about

 

how big does my dataset need to be?
how big does my dataset need to be?
it depends   
suppose you are proposing an algorithm for mining neanderthal bones. 

there are only a few hundred specimens known, and it is very 
unlikely that number will double in our lifetime. so you could 
reasonably test on a synthetic* dataset with a mere 1,000 objects.

however   
suppose you are proposing an algorithm for mining portuguese web 

pages (there are billions) or some new biometric (there may soon be 
millions). you do have an obligation to test on large datasets.

it is increasing difficult to excuse data mining papers testing on small 

datasets. data is typically free, cpu cycles are essentially free, a 
terabyte of storage costs less than $100   

*in this case, the    synthetic    could be easer to obtain monkey bones etc.

where do i get good data? 
where do i get good data? 

     from your domain expert collaborators:
     from formal data mining archives:

     the uci knowledge discovery in databases archive.
     the ucr time series and shape archive.

     from general archives: 

     chart-o-matic
     nasa ges disc
     from creating it:

     glue tiny radio transmitters to the backs of mormon crickets   
     by a wii, and hire a asl interpreter to   

     remember there is no excuse for not getting real data.

solving problems
solving problems

     now we have a problem and data, all we need to do is to 

solve the problem.

     techniques for solving problems depend on your skill 
set/background and the problem itself, however i will 
quickly suggest some simple general techniques.

     before we see these techniques, let me suggest you avoid 
complex solutions. this is because complex solutions... 

        are less likely to generalize to datasets.
        are much easer to overfit with.
        are harder to explain well. 
        are difficult to reproduce by others.
        are less likely to be cited.

unjustified complexity i
unjustified complexity i

from a recent paper: 
this forecasting model integrates a case based reasoning 
(cbr) technique, a fuzzy decision tree (fdt), and 
id107 (ga) to construct a decision-making 
system based on historical data and technical indexes.

    even if you believe the results. did the improvement 
come from the cbr, the fdt, the ga, or from the 
combination of two things, or the combination of all three? 
    in total, there are more than 15 parameters   
    how reproducible do you think this is?

unjustified complexity ii
unjustified complexity ii

    there may be problems that really require very 
complex solutions, but they seem rare. see [a].
    your paper is implicitly claiming    this is the 
simplest way to get results this good   . 
    make that claim explicit, and carefully justify the 
complexity of your approach. 

[a] r.c. holte, very simple classification rules perform well on most commonly used datasets, machine learning 11 (1) (1993). this 

paper shows that one-level id90 do very well most of the time.

j. shieh and e. keogh isax: indexing and mining terabyte sized time series. sigkdd 2008. this paper shows that the simple 

euclidean distance is competitive to much more complex distance measures, once the datasets are reasonably large.

unjustified complexity iii
unjustified complexity iii

paradoxically and wrongly, sometimes if the paper 
used an excessively complicated algorithm, it is 

more likely that it would be accepted

charles elkan
if your idea is simple, don   t try to hid that fact with 
unnecessary padding (although unfortunately, that does seem 
to work sometimes). instead, sell the simplicity.
      it reinforces our claim that our methods are very simple
solution this 

implement.. ..before explaining our simple
problem      we can objectively discover the anomaly using the 
simple

algorithm       sigkdd04

to 

 

 

simplicity is a strength, not a weakness, acknowledge it and 
claim it as an advantage. 

 

solving research problems
solving research problems

    problem relaxation:
    looking to other fields for solutions:

we don   t have time to look at all 
we don   t have time to look at all 
ways of solving problems, so lets just 
ways of solving problems, so lets just 
look at two examples in detail. 
look at two examples in detail. 

if there is a problem you can't solve, then there 

is an easier problem you can solve: find it. 

can you find a problem analogous to your problem and solve that?
george polya
can you vary or change your problem to create a new problem (or set of problems) whose solution(s) 
will help you solve your original problem?
can you find a subproblem or side problem whose solution will help you solve your problem?
can you find a problem related to yours that has been solved and use it to solve your problem?
can you decompose the problem and    recombine its elements in some new manner   ? (divide and conquer)
can you solve your problem by deriving a generalization from some examples?
can you find a problem more general than your problem?
can you start with the goal and work backwards to something you already know?
can you draw a picture of the problem?
can you find a problem more specialized?

problem relaxation: if you cannot solve the problem, make it 
easier and then try to solve the easy version. 

    if you can solve the easier problem    publish it if it is worthy, then revisit 
the original problem to see if what you have learned helps.
    if you cannot solve the easier problem   make it even easier and try again.

example: suppose you want to maintain the closest pair of real- 
valued points in a sliding window over a stream, in worst-case 
linear time and in constant space1. suppose you find you cannot 
make progress on this   

could you solve it if you..

    relax to amortized instead of worst-case linear time.
    assume the data is discrete, instead of real.
    assume you have infinite space.
    assume that there can never be ties.

1i am not suggesting this is an meaningful problem to work on, it is just a teaching example

problem relaxation: concrete example, petroglyph mining

i want to build a tool 
that can find and 
extract petroglyphs 
from an image, 
quickly search for 
similar ones, do 
classification and 
id91 etc 

bighorn sheep petroglyph
click here for pictures 
of similar petroglyphs.
click here for similar 
images within walking 
distance.

the extraction and segmentation is really hard, for 
example the cracks in the rock are extracted as features.
i need to be scale, offset, and rotation invariant, but 
rotation invariance is really hard to achieve in this 
domain. 

what should i do?       (continued next slide)

problem relaxation: concrete example, petroglyph mining

    let us relax the difficult segmentation and 
extraction problem, after all, there are thousands of 
segmented petroglyphs online in old books   
    let us relax rotation invariance problem, after all, 
for some objects (people, animals) the orientation is 
usually fixed.
    given the relaxed version of the problem, can we 
make progress? yes! is it worth publishing? yes!
    note that i am not saying we should give up now. 
we should still tried to solve the harder problem. 
what we have learned solving the easier version 
might help when we revisit it.
    in the meantime, we have a paper and a little more 
confidence.

note that we must acknowledge the assumptions/limitations in the paper 

sigkdd 2009

looking to other fields for solutions: concrete example, 
finding repeated patterns in time series

    in 2002 i became interested in the idea of finding repeated patterns 
in time series, which is a computationally demanding problem.
    after making no progress on the problem, i started to look to other 
fields, in particular computational biology, which has a similar 
problem of dna motifs..
    as happens tompa & buhler had just published a clever algorithm 
for dna motif finding. we adapted their idea for time series, and 
published in sigkdd 2002   

tompa, m. & buhler, j. (2001). finding motifs using random projections. 5th int   l conference on computational molecular biology. pp 67-74. 

looking to other fields for solutions

you never can tell were good 
ideas will come from. the 
solution to a problem on anytime 
classification came from looking 
at bee foraging strategies. 

bumblebees can choose wisely or rapidly, but not both at once.. lars chittka, 
adrian g. dyer, fiola bock, anna dornhaus, nature vol.424, 24 jul 2003, p.388

    we data miners can often be inspired by biologists, data compression 
experts, information retrieval experts, cartographers, biometricians, 
code breakers etc.
    read widely, give talks about your problems (not solutions), 
collaborate, and ask for advice (on blogs, newsgroups etc)          

eliminate simple ideas
eliminate simple ideas

when trying to solve a problem, you should begin 
by eliminating simple ideas. there are two reasons 
why:

    it may be the case that that simple ideas really 
work very well, this happens much more often 
than you might think.

    your paper is making the implicit claim    this 
is the simplest way to get results this good   . you 
need to convince the reviewer that this is true, to 
do this, start by convincing yourself.

eliminate simple ideas: case study i (a)
eliminate simple ideas: case study i (a)

vegetation greenness measure

tomato
cotton

5

10

15

20

25

190

180

170

160

150

140

130

120

110

100

0

but there is a problem   .

in 2009 i was approached by a group to work on 
the classification of crop types in central valley 
california using landsat satellite imagery to 
support pesticide exposure assessment in 
disease. 

they came to me because they could not get 
dtw to work well..

at first glance this is a dream problem
    important domain
    different amounts of variability in each class
    i could see the need to invent a mechanism to 
allow partial rotation invariant  dynamic 
time warping (i could almost smell the best 
paper award!)

eliminate simple ideas: case study i (b)
eliminate simple ideas: case study i (b)

vegetation greenness measure

tomato
cotton

5

10

15

20

25

190

180

170

160

150

140

130

120

110

100

0

it is possible to get perfect 
accuracy with a single line 
of matlab! 
in particular this line:    sum(x) > 2700

lesson learned: sometimes really simple ideas 
work very well. they might be more difficult or 
impossible to publish, but oh well.
we should always be thinking in the back of our 
minds, is there a simpler way to do this?
when writing, we must convince the reviewer 
this is the simplest way to get results this good

>> sum(x)
ans = 

2845       2843       2734       2831       2875

2625       2642       2642       2490       2525

>> sum(x) > 2700
ans =     1     1     1     1     1

0     0     0     0     0

 

 

eliminate simple ideas: case study ii
eliminate simple ideas: case study ii

a paper sent to sigmod  4 or 5 years ago tackled the problem of generating 
the most typical time series in a large collection. 
the paper used a complex method using wavelets, transition probabilities, multi- 
resolution properties etc.
the quality of the most typical time series was measured by comparing it to every 
time series in the collection, and the smaller the average distance to everything, 
the better. 

sigmod submission paper algorithm
(a few hundred lines of code, learns model 
from data)
   
x =  dwt(a + somefun(b)) 
typical_time_series  = x + z

reviewers algorithm
(does not look at the data, and 
takes exactly one line of code)
typical_time_series  = zeros(64)

under their metric of success, it is clear to the reviewer (without doing any 
experiments) that a constant line is the optimal answer for any dataset! 
we should always be thinking in the back of our minds, is there a simpler way to do this?
when writing, we must convince the reviewer this is the simplest way to get results this good

the importance of being cynical
the importance of being cynical

in 1515 albrecht d  rer drew a rhino from a  
sketch and written description. the drawing is 
remarkably accurate, except that there is a 
spurious horn on the shoulder.

this extra horn appears on every european 
reproduction of a rhino for the next 300 years.

d  rer's rhinoceros (1515)

it it ain'tain't necessarily so
necessarily so
     not every statement in the literature is true.
     implications of this:

    research opportunities exist, confirming or refuting 

   known facts    (or more likely, investigating under what conditions they are true)

    we must be careful not to assume that it is not worth 

trying x, since x is    known    not to work, or y is 
   known    to be better than x

     in the next few slides we will see some examples

if you would be a real seeker after 
truth, it is necessary that you doubt, 

as far as possible, all things.

     in kdd 2000 i said    euclidean distance can be an 
extremely brittle distance measure    please note the    can   ! 
     this has been taken as gospel by many researchers

distance measure   yu et al. 07

     however, euclidean distance can be an extremely brittle.. xiao et al. 04
     it is an extremely brittle
     the euclidean distance, yields a brittle
     to overcome the brittleness of the euclidean distance measure    wu 04
     therefore, euclidean distance is a brittle
distance measure santosh 07
distance measure tuzcu 04
    

that the euclidean distance is a very brittle

metric.. adams et al 04

 

 

 

is this really true?
based on comparisons to 12 state- 
of-the-art measures on 40 different 
datasets, it is true on some small 
datasets, but there is no published 
evidence it is true on any large 
dataset (ding et al vldb 08)

 
r
o
r
r
e
n
n
1

 

 
 
e
l
p
m
a
s
-
f
o
-
t
u
o

t
e
s
a
t
a
d

 
t
a
p
-
2
 
n
o
 
 
e
t
a
r

0.5

0

0

 

true for some 
small datasets

almost certainly 
not true for any 
large dataset

euclidean 
dtw

1000

2000

3000

4000

5000

6000

increasingly large training sets

a sigmod best paper says..
a sigmod best paper says..

our empirical results indicate that chebyshev approximation can deliver a 
3- to 5-fold reduction on the dimensionality of the index space. for 
instance, it only takes 4 to 6 chebyshev coefficients to deliver the same 
pruning power produced by 20 apca coefficients

is this really true?
no, actually chebyshev 
approximation is slightly 
worse that other techniques 
(ding et al vldb 08) 

20

15

10

5

0

64

apca light blue, cheb dark blue

64

128

256

8

4

32

dimensionality

16

the good results were 
due to a coding bug.. 
.. thus it is clear that the 
c++ version contained a 
bug. we apologize for any 
inconvenience caused (note 
on authors page)

128

sequence length

256

this is a problem, because many researchers have assumed it is true, and used chebyshev 
polynomials without even considering other techniques. for example.. 

(we use chebyshev polynomial approximation) because it is very accurate, and incurs low 
storage, which has proven very useful for similarity search.  ni and ravishankar 07

in most cases, do not
because someone claims this.

 

assume the problem is solved, or that algorithm x is the best, just 

a sigkdd (r--up) best paper says..
up) best paper says..
a sigkdd (r

(my id141) you can slide a window across a time series, place all exacted 
subsequences in a matrix, and then cluster them with id116. the resulting 
cluster centers then represent the typical patterns in that time series. 

is this really true?
no, if you cluster the data as described above the output is independent of the input 
(random number generators are the only algorithms that are supposed to have this property). 
the first paper to point this out (keogh et al 2003) met with tremendous resistance 
at first, but has been since confirmed in dozens of papers.

this is a problem, dozens of people wrote papers on making it faster/better, without realizing it 
does not work at all! at least two groups published multiple papers on this:
    exploiting efficient parallelism for mining rules in time series data. sarker et al 05
    parallel algorithms for mining association rules in time series data. sarker  et al 03
    mining association rules from multi-stream time series data on multiprocessor systems. sarker et al 05 
    efficient parallelism for mining sequential rules in time series. sarker et al 06 
    parallel mining of sequential rules from temporal multi-stream time series data. sarker et al 06 

in most cases, do not
because someone claims this.

 

assume the problem is solved, or that algorithm x is the best, just 

miscellaneous examples
miscellaneous examples

voodoo correlations in social neuroscience. vul, e, harris, c, winkielman, p & pashler, 
h.. perspectives on psychological science. here social neuroscientists criticized for overstating links between brain activity and emotion. 
this is an wonderful paper.

why most published research findings are false. j.p. ioannidis.  plos med 2 (2005), 
p. e124.

publication bias: the    file-drawer problem    in scientific id136. scargle, j. d. 
(2000), journal for scientific exploration 14 (1): 91   106

classifier technology and the illusion of progress. hand, d. j.
statistical science 2006, vol. 21, no. 1, 1-15

everything you know about dynamic time warping is wrong. ratanamahatana, c. 
a. and keogh. e. (2004). tdm 04

magical thinking in data mining: lessons from coil challenge 2000 
charles elkan

how many scientists fabricate and falsify research? a systematic review and 
meta-analysis of survey data. fanelli d, 2009 plos one4(5)

nonnon--existent problems 
existent problems 

a final point before break.

it is important that the problem you are working on is 
a real problem. 

it may be hard to believe, but many people attempt 
(and occasionally succeed) to publish papers on 
problems that don   t exist!

lets us quickly spend 6 slides to see an example.

solving problems that don      t exist  i
t exist  i
solving problems that don
   this picture shows the visual intuition 
of the euclidean distance between two 
time series of the same length

d(q,c)

    suppose the time series are of different 
lengths? 
    we can just make 
one shorter or the 
other one longer..
c_new = resample(c, length(q), length(c))

q

c

it takes one line 
of matlab code

solving problems that don      t exist  ii
t exist  ii
solving problems that don
but more than 2 dozen group have claimed that this 
is    wrong    for some reason, and written papers on 
how to compare two time series of different lengths 
(without simply making them the same length) 

      (we need to be able) handle sequences of different lengths    
pods 2005
      (we need to be able to find) sequences with similar patterns 
to be found even when they are of different lengths    information 
systems 2004
      (our method) can be used to measure similarity between 
sequences of different lengths    ideas2003

solving problems that don      t exist  iii
t exist  iii
solving problems that don

but an extensive literature search (by me), through 
more than 500 papers dating back to the 1960   s 
failed to produce any theoretical or empirical 
results to suggest that simply making the sequences 
have the same length has any detrimental effect in 
classification, id91, query by content or any 
other application.
let us test this!

solving problems that don      t exist  iiii
t exist  iiii
solving problems that don
for all publicly available time series datasets 
which have naturally different lengths, let us 
compare the 1-nearest neighbor classification rate 
in two ways:

    after simply re-normalizing lengths (one line of matlab, 
no parameters)
    using the ideas introduced in these papers to to 
support different length comparisons (various complicated 
ideas, some parameters to tweak) we tested the four most referenced ideas, and 
only report the best of the four.

solving problems that don      t exist  v
t exist  v
solving problems that don

the face, leaf, asl and trace datasets are the only publicly available 
classification datasets that come in different lengths, lets try all of them 
dataset resample to same 
dataset

working with different 

length

lengths

trace
leaves
asl
face

0.00
4.07
14.3
2.68
a two-tailed t-test with 0.05 significance level for each dataset 

0.00
4.01
14.3
2.68

indicates that there is no statistically significant difference between 

the accuracy of the two sets of experiments.

solving problems that don      t exist  vi
t exist  vi
solving problems that don
a least two dozen groups assumed that comparing different 
length sequences was a non-trivial problem worthy of 
research and publication.

but there was and still is to this day, zero evidence to support 
this!

and there is strong evidence to suggest this is not true. 

there are two implications of this:

    make sure the problem you are solving exists!
    make sure you convince the reviewer it exists.

coffee break

part ii of
part ii of

how to do good 
how to do good 
research, get it 
research, get it 
published in 
published in 
sigkdd and 
sigkdd and 
get it cited
get it cited

eamonn keogh
eamonn keogh

writing the paper
writing the paper

there are three rules for writing 

the novel   

w. somerset maugham

..unfortunately, no one knows 

what they are.

writing the paper
writing the paper

samuel johnson

what is written without 
effort is in general read 

without pleasure

indicate the gap: what need   s to be done?

introduce the topic and define (informally at this stage) terminology

introduce formal definitions.
introduce your novel algorithm/representation/data structure etc. 

     make a working title
    
     motivation: emphasize why is the topic important
     relate to current knowledge: what   s been done
    
     formally pose research questions
     explain any necessary background material.
    
    
     describe experimental set-up, explain what the experiments will show
     describe the datasets
     summarize results with figures/tables
     discuss results 
     explain conflicting results, unexpected findings and discrepancies with other research
     state limitations of the study
     state importance of findings
     announce directions for further research
     acknowledgements
     references

adapted from hengl, t. and gould, m., 2002. rules of thumb for writing research articles.

a useful principle
a useful principle
steve krug has a wonderful book about web 
design, which also has some useful ideas for 
writing papers.

a fundamental principle is captured in the title: 
don   t make the reviewer of your paper think!
1) if they are forced to think, they may resent being forced to 
make the effort. the are literally not being paid to think.
2) if you let the reader think, they may think wrong!

with very careful writing, great organization, and self explaining 
figures, you can (and should) remove most of the effort for the 
reviewer

a useful principle
a useful principle
a simple concrete example:

this requires a lot of thought 
to see that 2ddw is better 

than euclidian distance

this does not

2ddw 
distance

euclidean 
distance

figure 3: two pairs of faces clustered 
using 2ddw (top) and euclidean 
distance (bottom)

keogh      s maxim
s maxim
keogh
i firmly believe in the following:
if you can save the reviewer one 
minute of their time, by spending 
one extra hour of your time, then 
you have an obligation to do so.

keogh      s maxim can be derived from first principles
s maxim can be derived from first principles
keogh

the author sends about oneone paper to sigkdd
       the author sends about 
paper to sigkdd
the reviewer must review about tenten papers for sigkdd
       the reviewer must review about 
papers for sigkdd

       the benefit for the author in getting a paper into sigkdd is ha
the benefit for the author in getting a paper into sigkdd is hard to 
rd to 
quantify, but could be tens of thousands of dollars (if you get tenure, if 
tenure, if 
quantify, but could be tens of thousands of dollars (if you get 
you get that job in google      ).).
you get that job in google
the benefit for a reviewer is close to zero, they don      t get paid.
       the benefit for a reviewer is close to zero, they don
t get paid.
therefore: the author has the responsibly to do allall the work to make 
the work to make 
therefore: the author has the responsibly to do 
the reviewers task as easy as possible.
the reviewers task as easy as possible.

remember, each report was prepared without 

charge by someone whose time you could not buy

alan jay smith

a. j. smith,    the task of the referee    ieee computer, vol. 23, no. 4, pp. 65-71, april 1990.

an example of keogh      s maxim
s maxim
an example of keogh

       we wrote a paper for sigkdd 2009
we wrote a paper for sigkdd 2009
       our mock reviewers had a hard time 
our mock reviewers had a hard time 
understanding a step, where a template 
understanding a step, where a template 
must be rotated. they all eventually got 
must be rotated. they all eventually got 
it, it just took them some effort.
it, it just took them some effort.
       we rewrote some of the text, and 
we rewrote some of the text, and 
added in a figure that explicitly shows 
added in a figure that explicitly shows 
the template been rotated 
the template been rotated 
       we retested the section on the same, 
we retested the section on the same, 
and new mock reviewers, it worked 
and new mock reviewers, it worked 
much better.
much better.
       we spent 2 or 3 hours to save the 
we spent 2 or 3 hours to save the 
reviewers tens of seconds.
reviewers tens of seconds.

first draft
first draft

new draft
new draft

i have often said reviewers make an 
initial impression on the first page 
and don   t change 80% of the time

mike pazzani

this idea, that first impressions tend to be hard to change, 
has a formal name in psychology, anchoring.

others have claimed that anchoring
others have claimed that 
by reviewers 
by reviewers 

anchoring is used 
is used 

xindong wu

another strategy people seem to use intuitively and unconsciously
to simplify the task of making judgments is called anchoring. some natural 
starting point is used as a first approximation to the desired judgment. 
this starting point is then adjusted, based on the results of additional information 
or analysis. typically, however, the starting point serves as an anchor that reduces 
the amount of adjustment, so the final estimate remains closer to the starting point 
than it ought to be.
richards j. heuer, jr.  psychology of intelligence analysis (cia)
what might be the    natural starting point    for a sigkdd reviewer making 
a judgment on your paper?
hopefully it is not the author or institution:    people from cmu tend to do 
good work, lets have a look at this      ,    this guys last paper was junk..   
i believe that the title, abstract and introduction form an anchor. if these 
are excellent, then the reviewer reads on assuming this is a good paper, 
and she is looking for things to confirm this.
however, if they are poor, the reviewer is just going to scan the paper to 
confirm what she already knows,   this is junk   

i don   t have any studies to support this for reviewing papers. i am making this claim based on my experience and feedback (the title is the most important part of the paper. jeff 
scargle). however there are dozens of studies to support the idea of anchoring when people make judgments about buying cars, stocks, personal injury amounts in court cases etc.

the first page as an anchor
anchor
the first page as an 

the introduction acts as an anchor. by the end 
of the introduction the reviewer must know. 

jennifer windom

if possible, an 
interesting figure on the 
first page helps 

     what is the problem?
     why is it interesting and important?
     why is it hard? why do naive approaches fail?
     why hasn't it been solved before? (or, what's 

wrong with previous proposed solutions?)

     what are the key components of my approach and 

results? also include any specific limitations.
     a final paragraph or subsection:    summary of 

contributions   . it should list the major 
contributions in bullet form, mentioning in which 
sections they can be found. this material doubles 
as an outline of the rest of the paper, saving space 
and eliminating redundancy. 

this advice is taken almost verbatim from jennifer.

reproducibility
reproducibility

reproducibility is one of the main 
principles of the scientific method, and 
refers to the ability of a test or 
experiment to be accurately 
reproduced, or replicated, by someone 
else working independently.

reproducibility
reproducibility

     in a    bake-off    paper veltkamp and latecki attempted 
to reproduce the accuracy claims of 15 shape matching 
papers but discovered to their dismay that they could 
not match the claimed accuracy for any approach.

     a recent paper in vldb showed a similar thing for 

time series distance measures.

the vast body of results being generated by 
current computational science practice suffer a 
large and growing credibility gap: it is impossible 

to believe most of the computational results 

shown in conferences and papers

david donoho

properties and performance of shape similarity measures. remco c. veltkamp and longin jan latecki. ifcs 2006 
querying and mining of time series data: experimental comparison of representations and distance measures. ding, trajcevski, scheuermann, wang & keogh. vldb 2008
fifteen years of reproducible research in computational harmonic analysis- donoho et al.

two types of non--reproducibility
reproducibility
two types of non
    explicit: the authors don   t give you the data, or 
they don   t tell you the parameter settings.

    implicit: the work is so complex that it would 
take you weeks to attempts to reproduce the results, 
or you are forced to buy expensive software/ 
hardware/data to attempt reproduction.
or, the authors do give distribute data/code, but it 
is not annotated or is so complex as to be an 
unnecessary large burden to work with. 

we approximated collections of time 
we approximated collections of time 
series, using algorithms 
series, using algorithms 
agglomerativehistogram and 
agglomerativehistogram and 
fixedwindowhistogram and utilized 
fixedwindowhistogram and utilized 
the techniques of keogh et. al., in the 
the techniques of keogh et. al., in the 
problem of querying collections of 
problem of querying collections of 
time series based on similarity. our 
time series based on similarity. our 
results, indicate that the histogram 
results, indicate that the histogram 
approximations resulting from our 
approximations resulting from our 
algorithms are far superior than those 
algorithms are far superior than those 
resulting from the apca algorithm of 
resulting from the apca algorithm of 
keogh et. al.,the superior quality of 
keogh et. al.,the superior quality of 
our histograms is reflected in these 
our histograms is reflected in these 
problems by reducing the number of 
problems by reducing the number of 
false positives during time series 
false positives during time series 
similarity indexing, while remaining 
similarity indexing, while remaining 
competitive in terms of the time 
competitive in terms of the time 
required to approximate the time 
required to approximate the time 
series.
series. 

explicit non reproducibility
this paper appeared in icde02. the 
   experiment    is shown in its entirety, 
there are no extra figures or details.
which collections? how 
which collections? how 
large? what kind of data?
large? what kind of data? 
how are the queries selected?
how are the queries selected?
what results?
what results?

superior by how much?, 
superior by how much?, 
as measured how?
as measured how? 

how competitive?, as 
how competitive?, as 
measured how?
measured how? 

we approximated collections of time 
we approximated collections of time 
series, using algorithms 
series, using algorithms 
agglomerativehistogram and 
agglomerativehistogram and 
fixedwindowhistogram and utilized 
fixedwindowhistogram and utilized 
the techniques of keogh et. al., in the 
the techniques of keogh et. al., in the 
problem of querying collections of 
problem of querying collections of 
time series based on similarity. our 
time series based on similarity. our 
results, indicate that the histogram 
results, indicate that the histogram 
approximations resulting from our 
approximations resulting from our 
algorithms are far superior than those 
algorithms are far superior than those 
resulting from the apca algorithm of 
resulting from the apca algorithm of 
keogh et. al.,the superior quality of 
keogh et. al.,the superior quality of 
our histograms is reflected in these 
our histograms is reflected in these 
problems by reducing the number of 
problems by reducing the number of 
false positives during time series 
false positives during time series 
similarity indexing, while remaining 
similarity indexing, while remaining 
competitive in terms of the time 
competitive in terms of the time 
required to approximate the time 
required to approximate the time 
series.
series. 

i got a collection of opera 
i got a collection of opera 
arias as sung by luciano
arias as sung by luciano 
pavarotti, i compared his 
pavarotti, i compared his 
recordings to my own 
recordings to my own 
renditions of the songs.
renditions of the songs. 
my results, indicate that 
my results, indicate that 
my performances are far 
my performances are far 
superior to those by 
superior to those by 
pavarotti. the superior 
pavarotti. the superior 
quality of my 
quality of my 
performance is reflected 
performance is reflected 
in my mastery of the 
in my mastery of the 
highest notes of a tenor's 
highest notes of a tenor's 
range, while remaining 
range, while remaining 
competitive in terms of 
competitive in terms of 
the time required to 
the time required to 
prepare for a 
prepare for a 
performance. 
performance. 

implicit non reproducibility

from a recent paper: 

this forecasting model integrates a case based reasoning (cbr) 
technique, a fuzzy decision tree (fdt), and id107 
(ga) to construct a decision-making system based on historical 
data and technical indexes.

    in order to begin reproduce this work, we have to implement a case 
based reasoning system and a fuzzy decision tree and a genetic 
algorithm.
    with rare exceptions, people don   t spend a month reproducing 
someone else's results, so this is effectively non-reproducible.
    note that it is not the extraordinary complexity of the work that 
makes this non-reproducible (although it does not help), if the authors 
had put free high quality code and data online   

why reproducibility?
why reproducibility?
    we could talk about reproducibility as the 
cornerstone of scientific method and an obligation to 
the community, to your funders etc. however this 
tutorial is about getting papers published.
    having highly reproducible research will greatly 
help your chances of getting your paper accepted.
    explicit efforts in reproducibility instill confidence 
in the reviewers that your work is correct.
   explicit efforts in reproducibility will give the (true) 
appearance of value. 

as a bonus, reproducibility will increase your number of citations.

how to ensure reproducibility
how to ensure reproducibility
    explicitly state all parameters and settings in your paper.
    build a webpage with annotated data and code and point to it

(use an anonymous hosting service if necessary for double blind reviewing)

    it is too easy to fool yourself into thinking your work is 
reproducible when it is not. someone other than you should 
test the reproducibly of the paper. 

(from the paper)

for blind review conferences, you can create a 
gmail account, place all data there, and put 
the account info in the paper.   

how to ensure reproducibility
how to ensure reproducibility
in the next few slides i will quickly dismiss commonly 
heard objections to reproducible research (with thanks to david donoho)

    i can   t share my data for privacy reasons.

    reproducibility takes too much time and effort.

    strangers will use your code/data to compete with you.

    no one else does it. i won   t get any credit for it.

but i can      t share my data for privacy reasons
but i can

t share my data for privacy reasons      

     my first reaction when i see this is to think it may 

not be true. if you a going to claim this, prove it.

     can you also get a dataset that you can release?

     can you make a dataset that you can publicly 

release, which is about the same size, cardinality, 
distribution as the private dataset, then test on both 
in you paper, and release the synthetic one?  

reproducibility takes too much time and effort
reproducibility takes too much time and effort
     first of all, this has not been my personal experience.
     reproducibility can save time. when your conference 

paper gets invited to a journal a year later, and you need to 
do more experiments, you will find it much easier to pick 
up were you left off.

     forcing grad students/collaborators to do reproducible 

research makes them much easier to work with. 

strangers will use your code/data to compete with you
strangers will use your code/data to compete with you
     but competition means    strangers will read your papers 

and try to learn from them and try to do even better   . if you 
prefer obscurity, why are you publishing?

     other people using your code/data is something that funding 

agencies and tenure committees love to see.

sometimes the competition is undone by their carelessness. below (center) is a figure from a 
paper that uses my publicly available datasets. the alleged shapes in their paper are clearly 
not the real shapes (confusion of cartesian and polar coordinates?). this is good example of 
the importance of the    send preview to the rival authors   . this would have avoided 
publishing such an embarrassing mistake. 

alleged arrowhead and diatoms

actual arrowhead

actual diatoms

no one else does it. i won      t get any credit for it
t get any credit for it
no one else does it. i won

     it is true that not everyone does it, but that just 
means that you have a way to stand above the 
competition. 

     a review of my sigkdd 2004 paper said (my 

id141, i have lost the original email).

the results seem to good to be true, but i had 
my grad student download the code and 
data and check the results, it really does 
work as well as they claim.

parameters (are bad)
parameters (are bad)

    the most common cause of  implicit non reproducibility
algorithm with many parameters.
    parameter-laden algorithms can seem (and often are) ad-hoc and brittle.
    parameter-laden algorithms decrease reviewer confidence.
    for every parameter in your method, you must show, by logic, reason 
or experiment, that either   

is a 
 

    there is some way to set a good value for the parameter.
    the exact value of the parameter makes little difference.

with four parameters i 
can fit an elephant, and 
with five i can make him 

wiggle his trunk

john von neumann

unjustified choices (are bad)
unjustified choices (are bad)

   it is important to explain/justify every choice, even if 
it was an arbitrary choice.
    for example, this line frustrated me: of the 300 users with 
enough number of sessions within the year, we randomly 
picked 100 users to study.   why 100? would we have gotten similar results with 200?
    bad: we used single linkage id91...why single linkage, why not group average or wards?
    good: we experimented with single/group/complete linkage, but found 
this choice made little difference, we therefore report only   
    better: we experimented with single/group/complete linkage, but found 
this choice little difference, we therefore report only single linkage in 
this paper, however the interested reader can view the tech report [a] 
to see all variants of id91.

important words/phrases i
important words/phrases i

     optimal: does not mean    very good   

    we picked the optimal value for x... no! (unless you can prove it)
    we picked a value for x that produced the best..

     proved: does not mean    demonstrated   

    with experiments we proved that our.. no! (experiments rarely prove things)
    with experiments we offer evidence that our..

     significant: there is a danger of confusing the 

informal statement and the statistical claim
    our idea is significantly better than smiths
    our idea is statistically significantly better than smiths, at a 

confidence level of   

important words/phrases ii
important words/phrases ii
     complexity: has an overloaded meaning in computer science 

     the x algorithms complexity means it is not a good solution (complex= intricate )
     the x algorithms time complexity is o(n6) meaning it is not a good solution

     it is easy to see: first, this is a clich  . second, are you sure it is easy?

     it is easy to see that p = np

     actual: almost always has no meaning in a sentence 

     it is an actual b-tree   ->  it is a b-tree
     there are actually 5 ways to hash a string -> there are 5 ways to hash a string

     theoretically: almost always has no meaning in a sentence

     theoretically we could have jam or jelly on our toast.

     etc

: only use it if the remaining items on the list are obvious.

     we named the buckets for the 7 colors of the rainbow, red, orange, yellow etc.
     we measure performance factors such as stability, scalability, etc.     no! 

 

important words/phrases iii
important words/phrases iii

     correlated:

in informal speech it is a synonym for    related   

     celsius and  fahrenheit are correlated.    (clearly correct, perfect linear correlation)
     the tightness of lower bounds is correlated with pruning power. no!

 

     (data) mined 

     don   t say    we mined the data      , if you can say    we clustered the data..    or 

   we classified the data       etc

important words/phrases iiii
important words/phrases iiii

     in this paper:

where else? we are reading this paper

 

from a single sigmod paper

     in this paper, we attempt to approximate.. 
     thus, in this paper, we explore how to use..
     in this paper, our focus is on indexing large collections..
     in this paper, we seek to approximate and index.. 
     thus, in this paper, we explore how to use the.. 
     the indexing proposed in this paper
     figure 1 summarizes all the symbols used in this paper   
 
     in this paper, we use euclidean distance..
     the results to be presented in this paper
     a key result to be proven later in this paper
is that the..
 
     in this paper, we adopt the euclidean distance function..
     in

we explore how to apply

do not..

paper

 

belongs to the class of..

this
 

 

 

dht is used

dabtau

dht is used

and again
and again
and again
and again
and again
and again
dht is finally defined!

it is very important that you 
it is very important that you 
dabtau or your readers 
dabtau or your readers 
may be confused. 
may be confused. 
(define acronyms before they are used)
(define acronyms before they are used)

but anyone that reviews for this conference will surely know what the acronym means!
don   t be so sure, your reviewer may be a first-year, non-native english-speaking grad student 
that got 15 papers dumped on his desk 3 days before the reviewing deadline.

you can only assume this for acronyms where we have forgotten the original words, like laser, 
radar, scuba. remember our principle, don   t make the reviewer think. 

use use allall the space available
the space available

some reviewer is going to look at this 
empty space and say..
they could have had an additional 
experiment
they could have had more discussion 
of related work
they could have referenced more of 
my papers
etc
the best way to write a great 9 page 
paper, is to write a good 12 or 13 page 
paper and carefully pare it down.

you can use color in the text
you can use color in the text

in the example to the right, color helps 
emphasize that the order in which bits 
are added/removed to a representation.  
in the example below, color links 
numbers in the text with numbers in a 
figure. 
bear in mind that the reader may not 
see the color version, so you cannot 
rely on color.

sigkdd 2009

sigkdd 2008

people have 
been using 
color this way 
for well over 
a 1,000 years

avoid weak language i
avoid weak language i

compare
..with a dynamic series, it might fail to give 

accurate results.

with..
..with a dynamic series, it has been shown by [7] to 

give inaccurate results. (give a concrete reference)

or..
..with a dynamic series, it will give inaccurate 

results, as we show in section 7. (show me numbers)

avoid weak language ii
avoid weak language ii

compare
in this paper, we attempt to approximate and index 

a d-dimensional spatio-temporal trajectory.. 

with   
in this paper, we approximate and index a d- 

dimensional spatio-temporal trajectory.. 

or   
in this paper, we show, for the first time, how to 
approximate and index a d-dimensional spatio- 
temporal trajectory..

avoid weak language iii
avoid weak language iii

the paper is aiming to detect and retrieve videos of the same scene   

are you aiming at doing this, or have you done it? why not say   

in this work, we introduce a novel algorithm to detect and retrieve videos..

the dtw algorithm tries to find the path, minimizing the cost..

the dtw does not try to do this, it does this.

the dtw algorithm finds the path, minimizing the cost..

monitoring aggregate queries in real-time over distributed streaming environments 
appears to be a great challenge.

appears to be, or is? why not say   

monitoring aggregate queries in real-time over distributed streaming environments is 
known to be a great challenge [1,2].

avoid overstating
avoid overstating

don   t say:

we have shown our algorithm is better than a decision tree.

if you really mean   

we have shown our algorithm can be better than decision 
trees, when the data is correlated.

or..

on the iris and stock dataset, we have shown that our 
algorithm is more accurate, in future work we plan to discover 
the conditions under which our...

use the active voice
use the active voice
we can see that   
we can see that   

it can be seen that   
it can be seen that   
   seen    by whom?
   seen    by whom?

experiments were conducted   
experiments were conducted   

the data was collected by us.
the data was collected by us.

we conducted experiments...
we conducted experiments...

take responsibility
take responsibility

we collected the data.
we collected the data.

active voice is often shorter
active voice is often shorter

the active voice is usually more 
direct and vigorous than the passive

william strunk, jr

avoid implicit pointers
avoid implicit pointers

consider the following sentence: 

   we used dft. it has circular convolution 
property but not the unique eigenvectors 
property. this allows us to      

what does the    this    refer to? 

    the use of dft?
    the convolution property? 
    the unique eigenvectors property?

check every occurrence of the words    it   ,    this   , 
   these    etc. are they used in an unambiguous way?

avoid nonreferential use of "this", 

"that", "these", "it", and so on.

jeffrey d. ullman

many papers read like this:
many papers read like this:

we invented a new problem, and guess what, we can solve it!

this paper proposes a new trajectory id91 scheme for objects moving on 
this paper proposes a new trajectory id91 scheme for objects moving on 
road networks. a trajectory on road networks can be defined as a sequence of 
road networks. a trajectory on road networks can be defined as a sequence of 
road segments a moving object has passed by. we first propose a similarity 
road segments a moving object has passed by. we first propose a similarity 
measurement scheme that judges the degree of similarity by considering the total 
measurement scheme that judges the degree of similarity by considering the total 
length of matched road segments. then, we propose a new id91 algorithm 
length of matched road segments. then, we propose a new id91 algorithm 
based on such similarity measurement criteria by modifying and adjusting the 
based on such similarity measurement criteria by modifying and adjusting the 
fastmap and hierarchical id91 schemes. to evaluate the performance of the 
fastmap and hierarchical id91 schemes. to evaluate the performance of the 
proposed id91 scheme, we also develop a trajectory generator considering 
proposed id91 scheme, we also develop a trajectory generator considering 
the fact that most objects tend to move from the starting point to the destination 
the fact that most objects tend to move from the starting point to the destination 
point along their shortest path. the performance result shows that our scheme has 
point along their shortest path. the performance result shows that our scheme has 
the accuracy of over 95%.
the accuracy of over 95%. 

when the authors invent the definition of the data, and they invent 
the problem, and they invent the error metric, and they make their 
own data, can we be surprised if they have high accuracy? 

motivating your work
motivating your work

if there is a different way 
to solve your problem, 
and you do not address 
this, your reviewers might 
think you are hiding 
something
you should very 
explicitly say why the 
other ideas will not work. 
even if it is obvious to 
you, it might not be 
obvious to the reviewer.
another way to handle 
this might be to simply 
code up the other way and 
compare to it.

motivation
motivation

for reasons i don   t understand, sigkdd papers rarely quote other papers. quoting other 
papers can allow the writing of more forceful arguments   

this is much better than..
paper [20] notes that rotation is hard to deal with.

this is much better than..
that paper says time warping is too slow.

motivation
motivation

martin wattenberg had a beautiful 
paper in infovis 2002 that showed the 
repeated structure in strings   

if i had reviewed it, i would have 
rejected it, noting it had already been 
done, in 1120!

bach, goldberg variations

it is very important to convince 
the reviewers that your work is 
original.
    do a detailed literature search.
    use mock reviewers.
    explain why your work is 
different (see avoid 

laundry list       citations)
citations)

avoid       laundry list

de musica: leaf from boethius' treatise on music. diagram is decorated with 
the animal form of a beast. alexander turnbull library, wellington, new 
zealand

avoid       laundry list
avoid 

laundry list       citations i
citations i

in some of my early papers, i misspelled  davood rafiei   s name refiei. this 
spelling mistake now shows up in dozens of papers by others   

    
    
    
    
    

finding similarity in time series data by method of time weighted ..
similarity search in time series databases using ..
financial time series indexing based on low resolution    
similarity search in time series data using time weighted    
data reduction and noise filtering for predicting times    

   time series data analysis and pre-process on large    
   g id203-based method and its    
   a review on time series representation for similarity-based    
   financial time series indexing based on low resolution    
   a new design of multiple classifier system and its application to   

this (along with other facts omitted here) suggests that some people copy 
   classic    references, without having read them.
in other cases i have seen papers that claim    we introduce a novel 
algorithm x   , when in fact an essentially identical algorithm appears in one 
of the papers they have referenced (but probably not read).
read your references!
duplicate previous work, explicitly address this in your paper.

if what you are doing appears to contradict or 

 

a classic is something that 

everybody wants to have read and 

nobody wants to read

avoid       laundry list
avoid 

laundry list       citations ii
citations ii

one of carla brodley   s pet peeves is laundry list citations:
   paper a says "blah blah" about paper b, so in my paper i 
say the same thing, but cite paper b, and i did not read 
paper b to form my own opinion. (and in some cases did 
not even read paper b at all....)   
the problem with this is:

     you look like you are lazy.
     you look like you cannot form your own opinion.
     if paper a is wrong about paper b, and you echo the errors, 

carla brodley

you look na  ve. 

i dislike broad reference bundles such as there has been plenty of related work [a,s,d,f,g,c,h] 
often related work sections are little more than annotated bibliographies.        

claudia perlich
chris drummond 

a common logic error in evaluating algorithms: part i
a common logic error in evaluating algorithms: part i

here the authors test the rival 
algorithm, dtw, which has no 
parameters, and achieved an error 
rate of 0.127.
they then test 64 variations of 
their own approach, and since 
there exists at least one 
combination that is lower than 
0.127, they claim that their 
algorithm    performs better   

note that in this case the error is explicit, 
because the authors published the table. 
however in many case the authors just 
publish the result    we got 0.100   , and it is less 
clear that the problem exists.  

   comparing the error rates of dtw (0.127) and 
those of table 3, we observe that xxx performs 
better   

table 3: error rates using xxx on time 
series histograms with equal bin size

a common logic error in evaluating algorithms: part ii
a common logic error in evaluating algorithms: part ii

to see why this is a flaw, consider this:
    we want to find the fastest 100m runner, between india and china. 
    india does a set of trails, finds its best man, anil, and anil turns up 
expecting a race.
    china ask joe to run by himself. although mystified, he obliging 
does so, and clocks 9.75 seconds.
    china then tells all 1.4 billion chinese people to run 100m. 
    the best of all 1.4 billion runs was jin, who clocked 9.70 seconds.
    china declares itself the winner!
is this fair? of course not, but this is exactly what the previous slide does.

keep in mind that you should never look at the test set. 
this may sound obvious, but i cannot longer count the 
number of papers that i had to reject because of this.

johannes fuernkranz

0.8933    0.9600
0.9733    0.9600
0.9867    0.9733
0.9333    0.9467
0.9200    0.9600
0.9200    0.9467
0.9600    1.0000
0.9600    0.9467
0.9467    0.9733
0.9200    0.9600
0.9067    0.9600
0.9067    0.9733
0.9600    0.9867
0.9600    0.9733
0.9200    0.9333
0.9200    0.9333
0.9600    0.9600
0.9467    0.9733
0.9467    0.9600
0.8933    0.9600
0.9200    0.9733
0.9200    0.9200
0.9467    0.9333
0.9200    0.9600
0.9333    0.9733
0.9333    0.9867
0.9867    0.9867
0.9200    0.9733
0.9733    0.9733
0.9333    0.9733
0.9067    0.9333
0.9467    0.9600
0.9333    0.9200
0.9467    0.9467
0.9333    0.9333
0.9600    0.9867
0.9733    0.9867
0.9333    0.9467
0.9600    0.9867
0.9467    0.9600
0.9600    0.9867
0.9733    0.9733
0.9467    0.9867
0.9600    0.9600
0.9467    0.9467
0.9467    0.9600
0.9600    0.9733
0.9333    0.9733
0.9467    0.9733
0.9200    0.9600

always put some variance estimate on 

performance measures (do everything 10 times and 
give me the variance of whatever you are reporting) 

claudia perlich

suppose i want to know if euclidean distance or l1 distance is 
suppose i want to know if euclidean distance or l1 distance is 
best on the cbf problem (with 150 objects), using 1nn   
best on the cbf problem (with 150 objects), using 1nn    

bad: do one test

a littler better: 
do 50 tests, and 
report mean

better: do 50 
tests, report mean 
and variance

1

0.98

0.96

0.94

0.92

0.9

y
c
a
r
u
c
c
a

1

0.98

0.96

0.94

0.92

0.9

y
c
a
r
u
c
c
a

red bar at 
plus/minus one std

1

0.98

0.96

0.94

0.92

0.9

y
c
a
r
u
c
c
a

1

0.98

0.96

0.94

0.92

0.9

y
c
a
r
u
c
c
a

much better: do 
50 tests, report 
confidence 

euclidean

l1

euclidean

l1

euclidean

l1

euclidean

l1

variance estimate on performance measures

suppose i want to know if american males are taller than 
chinese males. i randomly sample 16 of each, although it 
happens that i get yao ming in the sample   
plotting just the mean heights is very deceptive here.   

229.00  166.26
170.31  167.08
163.61  166.60
179.06  161.40
170.52  175.32
164.91  173.31
168.69  180.39
164.99  182.37
184.31  177.39
189.76  167.75
170.95  179.81
168.47  174.83
164.25  171.04
178.09  177.40
178.53  166.41
166.31  180.62
mean
175.74     173.00
std
16.15      6.45

 

m
c
n

 

i
 
t

i

h
g
e
h

230

220

210

200

190

180

170

160

 

m
c
n

 

i
 
t

i

h
g
e
h

230

220

210

200

190

180

170

160

china

us

china

us

be fair to the strawmen
be fair to the 

strawmen/rivals
/rivals
in a kdd paper, this figure is the main proof of utility for a new ew 
in a kdd paper, this figure is the main proof of utility for a n
idea. a query is suppose to match to location 250 in the target 
idea. a query is suppose to match to location 250 in the target 
sequence. their approach doesdoes, euclidean distance 
does not      ..
sequence. their approach 

, euclidean distance does not

query

sid48 (larger is better match)

euclidean distance (smaller is better match)

the authors would not share this data, citing confidentiality 
the authors would not share this data, citing confidentiality 
(even though the entire dataset is plotted in the paper)  so we 
(even though the entire dataset is plotted in the paper)  so we 
cannot reproduce their experiments       or can we?  
or can we?  
cannot reproduce their experiments

i wrote a program to extract the data from the pdf file      
i wrote a program to extract the data from the pdf file

query

sid48 (larger is better match)

euclidean distance (smaller is better match)

if we simply normalize the data (as dozens of papers 
if we simply normalize the data (as dozens of papers 
explicitly point out) the best match for euclidean distance is 
explicitly point out) the best match for euclidean distance is 
atat       location 250!
location 250!
so this paper introduces a method 
so this paper introduces a method 
which is: 
which is:
1) very hard to implement 
1) very hard to implement
2) computationally demanding 
2) computationally demanding
3) requires lots of parameters 
3) requires lots of parameters
to do the same job as 2 lines of parameter 
to do the same job as 2 lines of parameter 
free code.
free code.

be fair to the strawmen
strawmen/rivals
/rivals
be fair to the 
be fair to the strawmen/rivals

rmse: z-norm

200
0

100

150

200

250

300

350

400

300

250

350

400

450

50

because the experiments are not
because the experiments are not
reproducible, no one has noticed this. 
reproducible, no one has noticed this. 
 
 
several authors wrote follow-up papers, 
several authors wrote follow-up papers, 
simply assuming the utility of this work.
simply assuming the utility of this work. 

2

1.5

1

0.5

0

0

50

100

150

200

250

300

350

400

(normalized) euclidean distance

plagiarism 
plagiarism 

can be 
can be 
obvious..
obvious..

2006 paper

1

k < m + n    

element of w is defined as wk = (i, j)k, w = w1,w2, ,wk, ,wk

suppose we have two time series x1 and x2, of length t1 and t2 
respectively, where:
to align two sequences using dtw we construct an t1-by-t2 matrix 
where the (i-th, jth)
element of the matrix contains the distance d(x1i,x2j) between the 
two points x1i and x2j (with euclidean distance, d(x1i, x2j) =(x1i   
x2j)2 ). each matrix element (i,j) corresponds to the alignment 
between the points x1i and x2j
a warping path w, is a contiguous (in the sense stated below) set of 
matrix elements that de fines a mapping between x1 and x2. the k-
th
max(t1, t2)    
the warping path is typically subject to several constraints.
    boundary conditions: w1 = (1,1) and wk
this requires the warping path to start and finish in diagonally
opposite corner cells of the matrix.
    continuity: given wk = (a,b) then wk  1 = (a0, b0) where a   a0    
and b   b0    
to adjacent cells(including
    monotonicity: given wk = (a,b) then wk  1 = (a0, b0) where a    
1 and b    
b0    
spaced in time.
there are exponentially many warping paths that satisfy the above 
conditions, however we are interested only in the path which 
minimizes the warping cost,
the k in the denominator is used to compensate for the fact that
warping paths may have different 

1. this restricts the allowable steps in the warping path 

0. this forces the points in w to be monotonically 

 
diagonally adjacent cells).

= (t1, t2), simply stated, 

a0    

1 

 

 

 

 

 

 

 

1999 paper

 

 

-

and cj

and cj.

matrix 

so we have:

element of w is 

max(m,n) (cid:31)   k < m+n-1

a warping path w, is a 

(with euclidean distance, 

mapping between q and c. the kth
 

cj)2 ). each matrix element (i,j) corresponds to the 

suppose we have two time series q and c, of length n and m 
respectively, where:
to align two sequences using dtw we construct an n-by-m
where the (ith, jth) element of the matrix contains the distance 
d(qi,cj) between the two points qi
d(qi,cj) = (qi
alignment between the points qi
contiguous (in the sense stated below) set of matrix elements that 
definesa
defined as wk = (i,j)k
 
 
w = w1, w2,    ,wk,   ,wk
the warping path is typically subject to several constraints.
(cid:31)   boundary conditions: w1 = (1,1) and wk = (m,n), simply stated, 
this requires the warping path to start and finish in diagonally
opposite corner cells of the matrix.
(cid:31)   continuity: given wk = (a,b) then wk-1 = (a   ,b   ) where a   a' (cid:31)1 and 
b-b' (cid:31)   1.
this restricts the allowable steps in the warping path to adjacent 
cells (including diagonally adjacent cells).
 
(cid:31) monotonicity: given wk = (a,b) then wk-1 = (a',b') where a   a'   
and b-b'   
 
in time.
there are exponentially many warping paths that satisfy the above 
conditions, however we are interested only in the path which 
 
minimizes the warping cost:
the k in the denominator is used to compensate for the fact that 
warping paths may have different

0.this forces the points in w to be monotonically spaced 

0 

 

 

 

 

 

 

 

 

 

 

 

 

..or it can be subtle. i think the below is an example of plagiarism, but the 
rism, but the 
..or it can be subtle. i think the below is an example of plagia
2005 authors do not.
2005 authors do not.

 

et al    refers to the

2005 paper: as with most data mining problems, data representation
is one of 
the major elements to reach an efficient and effective solution. ... pioneered by 
pavlidis
idea of representing a time series of length n using 
k straight lines
2001 paper:
data is the key to efficient and effective solutions.... 
pavlidis   
k straight lines
 

pioneered by 
 
refers to the approximation of a time series t, of length n, with 

computer science problems, representation

 
as with most

of the 

 

 

 

 

figures also get plagiarized
figures also get plagiarized

this particular figure 
gets stolen a lot.

here by two medical 
doctors

here in a chinese 
publication ( the author 
did flip the figure upside 
down!)

here in a portuguese 
publication..

one page of a ten page paper. all the 
figures are taken without 
acknowledgement from keogh   s tutorial 

what happens if you plagiarize?
what happens if you plagiarize?

the best thing that can happen is 
the paper gets rejected by a 
reviewer that spots the problem.

if the paper gets published, there is 
an excellent chance that the 
original author will find out, at that 
point, they own you.

withdrawn articles in press are 

note to users:
proofs of articles which have been peer 
reviewed and initially accepted, but have since 
been withdrawn..

 

making good figures
making good figures

     i personally feel that making good figures is very 

important to a papers chance of acceptance. 

     the first thing reviewers often do with a paper is 

scan through it, so images act as an anchor.

     in some cases a picture really is worth a thousand 

words.

see papers of michail vlachos, it is clear that he agonizes over every detail in his beautiful figures.
see the books of edward tufte.
see stephen few   s books/blog (www.perceptualedge.com)

3

2

3

1

1

1

1

1

1

4

5

3

1

6

fig. 1. a sample sequence graph. the line 
thickness encodes relative id178

fig. 1. sequence graph example

what's wrong with this figure? let me count the ways   
what's wrong with this figure? let me count the ways   
none of the arrows line up with the    circles   . the    circles    are all different sizes and aspect ratios, the 
none of the arrows line up with the    circles   . the    circles    are all different sizes and aspect ratios, the 
(normally invisible) white bounding box around the numbers breaks the arrows in many places. the 
(normally invisible) white bounding box around the numbers breaks the arrows in many places. the 
figure captions has almost no information. circles are not aligned   
figure captions has almost no information. circles are not aligned    
on the right is my redrawing of the figure with powerpoint. it took me 300 seconds
on the right is my redrawing of the figure with powerpoint. it took me 300 seconds
this figure is an insult to reviewers. it says,    we expect you to spend an unpaid hour to 
this figure is an insult to reviewers. it says,    we expect you to spend an unpaid hour to 
review our paper, but we don   t think it worthwhile to spend 5 minutes to make clear figures   
review our paper, but we don   t think it worthwhile to spend 5 minutes to make clear figures    

fig. 1. sequence graph example

note that there are figures drawn seven hundred years 
note that there are figures drawn seven hundred years 
ago that have much better symmetry and layout. 
ago that have much better symmetry and layout. 

diaconus, and others, various saints lives: netherlands, s. or france, n. w.; 2nd quarter of the 13th century
diaconus, and others, various saints lives: netherlands, s. or france, n. w.; 2nd quarter of the 13th century 

peter damian, paulus
peter damian, paulus

lets us see some more examples of poor figures, then see some principles that can help 

 

 

this figure wastes 80% 
this figure wastes 80% 
of the space it takes up. 
of the space it takes up. 

in any case, it could be 
in any case, it could be 
replace by a short 
replace by a short 
english sentence:       we we 
english sentence: 
found that for 
found that for 
selectivity ranging 
selectivity ranging 
from 0 to 0.05, the four 
from 0 to 0.05, the four 
methods did not differ 
methods did not differ 
by more than 5%      
by more than 5%

why did they bother 
why did they bother 
with the legend, since 
with the legend, since 
you can      t tell the four 
t tell the four 
you can
lines apart anyway?
lines apart anyway?

this figure wastes 
almost a quarter of a 
page. 

the ordering on the x- 
axis is arbitrary, so the 
figure could be 
replaced with the 
sentence    we found 
the average 
performance was 198 
with a standard 
deviation of 11.2   . 

the paper in question 
had 5 similar plots, 
wasting an entire page.

the figure below takes up 1/6 of a page, but it only reports 
the figure below takes up 1/6 of a page, but it only reports 
3 numbers.
3 numbers.

the figure below takes up 1/6 of a page, but it only reports 
the figure below takes up 1/6 of a page, but it only reports 
2 numbers!
2 numbers!

actually, it really only reports one number! only the relative times really matter, so 
imes really matter, so 
actually, it really only reports one number! only the relative t
we found that ftw is 1007 times faster than the exact 
they could have written       we found that ftw is 1007 times faster than the exact 
they could have written 
calculation, independent of the sequence length      ..
calculation, independent of the sequence length

both figures below describe the classification of time series motions
both figures below describe the classification of time series mo
redesign by keogh
redesign by keogh

tions      

it is not obvious from this figure which 
it is not obvious from this figure which 
algorithm is best. the caption has almost 
algorithm is best. the caption has almost 
zero information
zero information
you need to read the text very carefully to 
you need to read the text very carefully to 
understand the figure
understand the figure

at a glance we can see that the accuracy is 
at a glance we can see that the accuracy is 
very high. we can also see that dtw 
very high. we can also see that dtw 
tends to win when the... 
tends to win when the... 

the data is plotted in figure 5. note that any correctly 
classified motions must appear in the upper left (gray) 
triangle.
1

in this region our 
algorithm wins 

in this region 
dtw wins 

1

0

0

figure 5. each of our 100 motions plotted as a point in 2 
dimensions. the x value is set to the distance to the nearest 
neighbor from the same class, and the y value is set to the 
distance to the  nearest neighbor from any other class.

both figures below describe the performance of 4 algorithms on indexing of time series of different lengths
both figures below describe the performance of 4 algorithms on i

ndexing of time series of different lengths      

this figure takes 1/2 of a page.
this figure takes 1/2 of a page.

this figure takes 1/6 of a page.
this figure takes 1/6 of a page.

this should be a bar chart, the four items are unrelated
this should be a bar chart, the four items are unrelated

(in any case this should probably be a table, not a figure)
(in any case this should probably be a table, not a figure)

principles to make good figures
principles to make good figures

    think about the point you want to make, should it be done with 
words, a table, or a figure. if a figure, what kind?
    color helps 
    linking helps 
    direct labeling helps 
    meaningful captions helps
    minimalism helps 
(omit needless elements)
    finally, taking great care, taking pride in your work, helps 

(but you cannot depend on it)

(sometimes called brushing)

direct labeling
helps 

it removes one 
level of 
indirection, and 
allows the figures 
to be self 
explaining

(see edward tufte: visual 
explanations, chapter 4)

d

c

e

b

a

figure 10. stills from a video sequence; the right hand is 
tracked, and converted into a time series: a) hand at rest: b) 
hand moving above holster. c) hand moving down to grasp 
gun. d hand moving to shoulder level, e) aiming gun. 

linking helps interpretability i

what is linking?
linking is connecting the same data in two views 
by using the same color (or thickness etc). in the 
figures below, color links the data in the pie 
chart, with data in the scatterplot.

fish

fowl

neither

both

50
45
40
35
30

25
20
15
10
5
0

0

10

20

30

40

50

60

how did we get from here

to here? 

it is not clear from the above figure.

see next slide for a suggested fix.

linking helps interpretability ii

in this figure, the color of 
the arrows inside the fish 
link to the colors of the 
arrows on the time series. 

this tells us 
exactly how we 
go from a shape 
to a time series.

note that there are other links, 
for example in ii, you can tell 
which fish is which based on 
color or link thickness linking.  

minimalism helps: in this 
case, numbers on the x-axis 
do not mean anything, so 
they are deleted. 

1

ebel

abel

drop1

drop2

false alarm rate 

1

 

 
e
t
a
r
n
o
i
t
c
e
t
e
d

0
0

    don   t cover the data with the labels! 
you are implicitly saying    the results 
are not that important   .
    do we need all the numbers to annotate 
the x and y axis? 
    can we remove the text    with 
ranking   ?

direct labeling helps

note that the line thicknesses 
differ by powers of 2, so even 
in a b/w printout you can tell 
the four lines apart.
minimalism helps: delete the    with ranking   , 
the x-axis numbers, the grid   

covering the data with the 
labels is a common sin

these two images, which are both use to discuss an anomaly detection algorithm, illustrate 
many of the points discussed in previous slides.

color helps - direct labeling helps - meaningful captions help

the images should be as self contained as possible, to avoid forcing the reader to look back to 
the text for clarification multiple times.  

note that while figure 6 
use color to highlight the 
anomaly, it also uses the 
line thickness (hard to 
see in powerpoint) thus 
this figure works also 
well in b/w printouts

thinking about the point you want to make, helps

2ddw 
distance

euclidean 
distance

figure 3: two pairs of faces clustered using 
2ddw (top) and euclidean distance (bottom)

from looking at this figure, we are suppose to 
tell that 2ddw produces more intuitive results 
than euclidean distance. 
i have a lot of experience with these types of 
things, and high motivation, but it still took me 4 
or 5 minutes to see this. 
do you think the reviewer will spend that amount 
of time on a single figure?

looking at this figure, we can tell that 2ddw 
produces more intuitive results than euclidean 
distance in 2 or 3 seconds.

paradoxically, this figure has less information  
(hierarchical id91 is lossy relative to a 
distance matrix) but communicates a lot more 
knowledge.

contrast these two figures, both of which attempt to show that 
contrast these two figures, both of which attempt to show that 
petroglyphs can be clustered meaningfully.
petroglyphs can be clustered meaningfully.

    thinking about the   , helps
    color helps 
    direct labeling helps
    meaningful captions helps

to figure out the utility 
of the similarity 
measures in this paper, 
you need to look at text 
and two figures, 
spanning four pages.

sigkdd 09

using the labels 
   method1    method2    
etc, gives a level of 
indirection. we have to 
keep referring back to 
the text (on a different 
page) to understand the 
content.
direct labeling helps

sequential 
sparsification

redesigned by keogh

the four significant digits are 
ludicrous on a data set with 
300 objects.

linear 

wavelet 

sequential 
sparsification
0.77
0.71
0.66
0.71

length
sparsification
128
0.77
256
0.86
512
0.94
0.86
avg
table 3: similarity results for cbf trials

quadratic 
sparsification
0.95
0.94
0.95
0.95

sparsification
0.95
0.95
0.94
0.95

raw 
data
0.77
0.74
0.77
0.76

this paper offers 7 
significant digits in the 
results on a dataset a few 
thousand items

this paper offers 9 significant 
digits in the results on a 
dataset a few hundred items

spurious digits are not just unnecessary, they are a lie! 
they imply a precision that you do not have. at best they 
make you look like an amateur.   

pseudo code

as with real code, it is probably better to break 
very long puesdocode into several shorter units 

the most common problems with figures
the most common problems with figures
1.  too many patterns on bars
2.  use of both different symbols and different lines
3.  too many shades of gray on bars
4.  lines too thin (or thick)
5.  use of three-dimensional bars for only two variables
6.  lettering too small and font difficult to read
7.  symbols too small or difficult to distinguish
8.  redundant title printed on graph
9.  use of gray symbols or lines
10.key outside the graph
11.unnecessary numbers in the axis
12.multiple colors map to the same shade of gray
13. unnecessary shading in background
14. using bitmap graphics (instead of vector graphics)
15. general carelessness

eileen k schofield: quality of graphs in scientific journals: an exploratory 
study.  science editor, 25 (2), 39-41 

eamonn keogh: my pet peeves 

1. too many patterns on bars

here the problem is compounded 
by the tiny size of the key. the 
area of each key-box is about 
2mm2

the key drawn 
to scale.

5. use of three-dimensional bars for only two variables

why is this chart in 3d?

isax

dct

0.7
0.6

0.5

0.4
0.3

0.2

0.1
0

1920

1440

960

480

length of time series

1920

b
l
t

10

8

6

4

1440

960

480

3d is fine when needed

6. lettering too small and font difficult to read

here the 
font size on 
the legend 
and key is 
about 1mm. 
(coin for scale)

hand

all the 
problems 
are trivial to 
fix 

 
)
c
e
s
m

(
 
e
m
t

i

full body

tree levels

)

%

(
 
y
c
n
e
i
c
i
f
f
e
g
n
i
n
u
r
p

 

 

s
r
o
t
c
e
v
d
n
o
c
e
s

s
r
o
t
c
e
v
 
t
s
r
i
f
hand

e
s
u
m

m

fs mfs
body
arms

10. key outside the graph

here the problem is not that 
the key is in text format 
(although it does not help). 
the problem is the distance 
between the key and the 
data.

data

key

11. unnecessary numbers in the axis

do we really need every 
integer from zero to 25 in this 
chart? (if    yes   , then make a table, not a figure)

in this version, i can still find, 
say    23   , by locating 20 and 
counting three check marks.

this problem is more common in the x-axis

12. multiple colors map to the 

same shade of gray

this image works fine in color   

in b/w however, multiples colors 
map to the same shades of gray.

note that we can easily represent upto 5 things with 
shades of gray. we can also directly label bars. 

1
 
l
e
v
e
l

2
 
l
e
v
e
l

3
 
l
e
v
e
l

4
 
l
e
v
e
l

5
 
l
e
v
e
l

6
 
l
e
v
e
l

level 1

level 2

level 3

level 4

level 5

level 6

13. unnecessary shading in background
all the other problems (multiple colors map to the same shade of gray, etc) 
are compounded by having a shaded background.

14 using bitmap graphics

below is a particularly bad 
example, compounded by a tiny 
font size, however even the best 
bitmaps look amateurish and can 
hard to read.
use vector graphics.

bitmap graphics often have 
compression artifacts, 
resulting in noise around 
sharp lines.

15 general carelessness

original 

why did the authors of 
this graphic not spend 
the 30 seconds it took to 
fix this problem?

such careless figures are 
an insult to reviewers. 

fixed  

0 200

400

600

800

1000

1200

1400

1600

1800

2000

top ten avoidable 
top ten avoidable 
reasons papers get 
reasons papers get 

rejected, with 
rejected, with 

solutions
solutions

to catch a thief, you must think like a thief
old french proverb

to convince a reviewer, you must think like a 
reviewer

always write your paper imagining the most cynical 
reviewer looking over your shoulder. this reviewer does not 
particularly like you, does not have a lot of time to spend on 
your paper, and does not think you are working in an 
interesting area. but he will listen to reason.

this paper is out of scope for sigkdd
this paper is out of scope for sigkdd

     in some cases, your paper may really be 

irretrievably out of scope, so send it elsewhere.

     solution

    did you read and reference sigkdd papers? 
    did you frame the problem as a kdd problem? 
    did you test on well known sigkdd datasets?
    did you use the common sigkdd id74? 
    did you use sigkdd formatting? (   look and feel   )
    can you write an explicit section that says: at first blush this 
problem might seem like a signal processing problem, but note that..

the experiments are not reproducible
the experiments are not reproducible

     this is becoming more and more common as a reason 
for rejection and some conferences now have official 
standards for reproducibility

     solution

    create a webpage with all the data and the paper itself.
    do the following sanity check. assume you lose all files. 

using just the webpage, can you recreate all the experiments 
in your paper? (it is easy to fool yourself here, really really think about this, or have a grad student actually attempt it). 

    forcing yourself to do this will eliminate 99% of the problems

this is too similar to your last paper
this is too similar to your last paper

     if you really are trying to    double-dip    then this 

is a justifiable reject.

     solution

    did you reference your previous work? 
    did you explicitly spend at least a paragraph explaining how 

you are extending that work (or, are different to that work).
    are you reusing all your introduction text and figures etc. it 

might be worth the effort to redo them.

    if your last paper measured, say, accuracy on dataset x, and 
this paper is also about improving accuracy, did you compare 
to your last work on x? (note that this does not exclude you from additional datasets/rival 
methods, but if you don   t compare to your previous work, you look like you are hiding something)

you did not acknowledge this weakness
you did not acknowledge this weakness

     this looks like you either don   t know it is a weakness 

(you are an idiot) or you are pretending it is not a 
weakness (you are a liar).

     solution

    explicitly acknowledge the weaknesses, and explain why the 

work is still useful (and, if possible, how it might be fixed)  
   while our algorithm only works for discrete data, as we noted 
in section 4, there are commercially important problems in 
the discrete domain. we further believe that we may be able 
to mitigate this weakness by considering      

you unfairly diminish others work
you unfairly diminish others work

     compare:

       in her inspiring paper smith shows.... we extend her 

foundation by mitigating the need for...   

       smith   s idea is slow and clumsy.... we fixed it.   

     some reviewers noted that they would not explicitly tell the authors 

that they felt their papers was unfairly critical/dismissive (such 
subjective feedback takes time to write), but it would temper how they 
felt about the paper.

     solution

    send a preview to the rival authors:    dear sue, we are trying to 

extend your idea and we wanted to make sure that we represented your work 
correctly and fairly, would you mind taking a look at this preview      

there is a easier way to solve this problem. 
there is a easier way to solve this problem.

you did not compare to the x algorithm
you did not compare to the x algorithm

     solution

    include simple strawmen (   while we do not expect the hamming distance 

to work well for the reasons we discussed, we include it for completeness   )

    write an explicit explanation as to why other methods 

won   t work (see below). but don   t just say    smith says the 
hamming distance is not good, so we didn   t try it   

you do not reference this related work.. 
you do not reference this related work
this idea is already known, see lee 1978
this idea is already known, see lee 1978

    solution

    do a detailed literature search.
    if the related literature is huge, write a longer tech report 

and say in your paper    the related work in this area is vast, we refer 
the interested reader to our tech-report for a more detailed survey   

    give a draft of your paper to mock-reviewers ahead of time.
    even if you have accidentally rediscovered a known result, 
you might be able to fix this if you know ahead of time. for 
example    in our paper we reintroduced an obscure result 
from cartography to data mining and show      

(in ten years i have rejected 4 papers that rediscovered the douglas-peuker algorithm.)

you have too many parameters/magic 
you have too many parameters/magic 

numbers/arbitrary choices
numbers/arbitrary choices

    solution

    for every parameter, either:

     show how you can set the value (by theory or experiment)
     show your idea is not sensitive to the exact values

    explain every choice. 

     if your choice was arbitrary, state that explicitly. we used single 

linkage in all our experiments, we also tried average, group and wards 
linkage, but found it made almost no difference, so we omitted those results 
for brevity.

     if your choice was not arbitrary, justify it. we chose dct instead of 

the more traditional dft for three reasons, which are   

not an interesting or important problem. 
not an interesting or important problem.

why do we care?
why do we care?

    solution

    did you test on real data?
    did you have a domain expert collaborator help with 
motivation?
    did you explicitly state why this is an important problem?
    can you estimate value?         in this case switching from motif 
8 to motif 5 gives us a nearly $40,000 in annual savings! patnaiky 
et al. sigkdd 2009   
     note that estimated value does not have to be in dollars, it 
could be in crimes solved, lives saved etc  

the writing is generally careless. 
the writing is generally careless.

there are many typos, unclear figures
there are many typos, unclear figures

this may seem unfair if your paper has a good idea, but 

reviewing carelessly written papers is frustrating. many 
reviewers will assume that you put as much care into the 
experiments as you did with the presentation.  

    solution

    finish writing well ahead of time, pay someone to check 
the writing. 
    use mock reviewers.
    take pride in your work!

tutorial summary
tutorial summary

     publishing in top tier venues such as sigkdd can 

seem daunting, and can be frustrating   

     but you can do it!

     taking a systematic approach, and being self- 

critical at every stage will help you chances 
greatly.

     having an external critical eye (mock-reviewers) 

will also help you chances greatly.

the end

appendix a:

why mock reviewers can help

a mock reviewer might have spotted 
that    upward shift    was misspelled, or 
that    negro    is not a good choice of 
words, or   

appendix b:

be concrete

sax is a kind of statistical algorithm   

no, sax is a data representation

finally, dynamic time warping metric was   
the same dynamic time warping metric was used to compare clusters   
    or dynamic time warping metric and to retrieve the last sensor data   

no, dynamic time warping is a measure, not a metric  

appendix c:

the owner of a small company needed to get rid of an old boiler 
that his company had replaced with a shiny new one. not wanting 
to pay disposal fees, and thinking that someone else could use it, he 
dragged it out onto the street and put a    free    sign on it. to his 
dismay, a week later it was still there. he was about to call a 
disposal company when his foreman said    i can get rid of it in one 
day   . 

the foreman replaced the    free    sign with one that said    for sale, 
$1,500   . that night, the boiler was stolen.

the moral? imply value for your paper.

