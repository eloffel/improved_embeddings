maximum likelihood (ml),  

expectation maximization (em) 

 
 

pieter abbeel 

uc berkeley eecs 

 
 
 
 

many slides adapted from thrun, burgard and fox, probabilistic robotics 
 
texpoint fonts used in emf.  

 

outline 

n       maximum likelihood (ml) 

n       priors, and maximum a posteriori (map) 

n       cross-validation 

n       expectation maximization (em) 

thumbtack 

n       let    = p(up),  1-   = p(down) 

n       how to determine    ? 

n       empirical estimate:  8 up, 2 down       

n      

http://web.me.com/todd6ton/site/classroom_blog/entries/2009/10/7_a_thumbtack_experiment.html 

maximum likelihood 

n          = p(up),  1-   = p(down) 

n       observe: 

n       likelihood of the observation sequence depends on   : 

n       maximum likelihood finds  

 

         extrema at    = 0,    = 1,    = 0.8 
         inspection of each extremum yields    ml = 0.8  

maximum likelihood 

n       more generally, consider binary-valued random variable with    = p(1), 1-   = 

p(0), assume we observe n1 ones, and n0 zeros 
n       likelihood: 
n       derivative: 

n       hence we have for the extrema: 

n       n1/(n0+n1) is the maximum 

n       = empirical counts.  

log-likelihood 

n       the function 

 

 

         

is a monotonically increasing function of x 

n       hence for any (positive-valued) function f: 

n      

in practice often more convenient to optimize the log-
likelihood rather than the likelihood itself 

n       example:  

 

 

log-likelihood            likelihood 

n       reconsider thumbtacks: 8 up, 2 down 

n       likelihood 

n      

log-likelihood 

not concave 

concave 

n       definition: a function f is concave if and only 

n       concave functions are generally easier to maximize then 

non-concave functions  

concavity and convexity 

f is concave if and only 

f is convex if and only 

 

 

 

 

 

 

 

 

x1 

x2 

   x2+(1-  )x2 

 

 

 

 

 

 

 

 

x1 

x2 

   x2+(1-  )x2 

   easy    to maximize 

   easy    to minimize 

ml for multinomial 

n       consider having received samples 

ml for fully observed id48 

n       given samples 

n       dynamics model: 

n       observation model:   

 

 

 

 

      independent ml problems for each        and each  

ml for exponential distribution 

source: wikipedia 

n       consider having received samples 

n       3.1, 8.2, 1.7 

ll 

ml for exponential distribution 

source: wikipedia 

n       consider having received samples 

n         

uniform 

n       consider having received samples 

n         

ml for gaussian 

n       consider having received samples 

n         

ml for conditional gaussian 

equivalently: 

 

 

more generally: 

ml for conditional gaussian 

ml for conditional multivariate gaussian 

aside: key identities for derivation on 
previous slide 

ml estimation in fully observed 
linear gaussian bayes filter setting 
n       consider the linear gaussian setting: 

n       fully observed, i.e., given 

n             two separate ml estimation problems for conditional 

multivariate gaussian:  

n       1: 

n       2:    

priors --- thumbtack 

n       let    = p(up),  1-   = p(down) 

n       how to determine    ? 

n       ml estimate:  5 up, 0 down       

n       laplace estimate: add a fake count of 1 for each outcome 

priors --- thumbtack 

n       alternatively, consider    to be random variable 
n       prior p(  ) /   (1-  ) 
n       measurements: p( x |    ) 

n       posterior: 

n       maximum a posterior (map) estimation  

n       = find    that maximizes the posterior     
       

priors --- beta distribution 

figure source: wikipedia 

priors --- dirichlet distribution 

n       generalizes beta distribution 
n       map estimate corresponds to adding fake counts n1,    , nk 

map for mean of univariate gaussian 

n       assume variance known.  (can be extended to also find map for variance.) 

n       prior:  

map for univariate conditional linear 
gaussian 
n       assume variance known.  (can be extended to also find map 

for variance.) 

n       prior:  

[interpret!] 

map for univariate conditional linear 
gaussian: example 

true --- 
samples . 
ml --- 
map --- 

cross validation 

n       choice of prior will heavily influence quality of result 

n       fine-tune choice of prior through cross-validation: 

n       1. split data into    training    set and    validation    set 
n       2. for a range of priors,  

n       train: compute   map on training set 
n       cross-validate: evaluate performance on validation set by evaluating 

the likelihood of the validation data under   map just found 

n       3. choose prior with highest validation score  

n       for this prior, compute   map on (training+validation) set 

n       typical training / validation splits: 

n       1-fold: 70/30, random split 

n       10-fold: partition into 10 sets, average performance for each of the sets being the 

validation set and the other 9 being the training set 

outline 

n       maximum likelihood (ml) 

n       priors, and maximum a posteriori (map) 

n       cross-validation 
n       expectation maximization (em) 

mixture of gaussians 

n       generally: 

n       example:  

n       ml objective: given data z(1),    , z(m) 

n       setting derivatives w.r.t.   ,   ,    equal to zero does not enable to solve 

for their ml estimates in closed form 

we can evaluate function       we can in principle perform local optimization.  in this lecture:    em    algorithm, 
which is typically used to efficiently optimize the objective (locally)  

expectation maximization (em) 

n       example: 

n       model: 

n       goal:  

n       given data z(1),    , z(m)  (but no x(i) observed) 
n       find maximum likelihood estimates of   1,   2 

n       em basic idea: if x(i) were known       two easy-to-solve separate ml 

problems 

n       em iterates over 

n       e-step: for i=1,   ,m   fill in missing data x(i) according to what is most 

likely given the current model    

n       m-step: run ml for completed data, which gives new model    

em derivation 
n       em solves a maximum likelihood problem of the form: 

  
  : parameters of the probabilistic model we try to find 
x: unobserved variables 
z: observed variables 
 
  

jensen   s inequality 

jensen   s inequality 

illustration:  
p(x=x1) = 1-  ,  
p(x=x2) =    

x1 

x2 

e[x] =    x2+(1-  )x2 

em derivation (ctd) 

jensen   s inequality: equality holds when  
 
function.  this is achieved for   

 

    is an affine  

em algorithm: iterate 

 1. e-step: compute 

  

 2. m-step: compute  

  

m-step optimization can be done efficiently in most cases 
e-step is usually the more expensive step 
it does not fill in the missing data x with hard values, but finds a distribution q(x) 

em derivation (ctd) 

n       m-step objective is upper-
bounded by true objective 

n       m-step objective is equal 

to true objective at 
current parameter 
estimate 

 
n             improvement in true objective is at least as large as 

improvement in m-step objective 

em 1-d example --- 2 iterations 

n       estimate 1-d mixture of two gaussians with unit variance: 

n         

n       one parameter    ;   1 =    - 7.5,   2 =   +7.5 

em for mixture of gaussians 

n       x ~ multinomial distribution, p(x=k ;   ) =   k 
n       z ~ n(  k,   k) 
n       observed: z(1), z(2),    , z(m) 
 

em for mixture of gaussians 

n       e-step: 

n       m-step: 

ml objective id48 

n       given samples 

n       dynamics model: 

n       observation model:   

n       ml objective: 

 

 

 

         no simple decomposition into independent ml problems for    

each        and each  

         no closed form solution found by setting derivatives equal to zero 

em for id48 --- m-step 

n      

  

             and    computed from    soft    counts  

em for id48 --- e-step 

n       no need to find conditional full joint  

n       run smoother to find: 

ml objective for linear gaussians 

n       linear gaussian setting: 

n       given 

n       ml objective: 

n       em-derivation: same as id48 

em for linear gaussians --- e-step 

n       forward: 

n       backward: 

em for linear gaussians --- m-step 

[updates for a, b, c, d. todo: fill in once found/derived.] 

em for linear gaussians --- the 
log-likelihood  

n       when running em, it can be good to keep track of the log-

likelihood score --- it is supposed to increase every iteration 

em for extended kalman filter setting 

n       as the linearization is only an approximation, when 

performing the updates, we might end up with parameters 
that result in a lower (rather than higher) log-likelihood 
score 

n             solution: instead of updating the parameters to the newly 
estimated ones, interpolate between the previous parameters 
and the newly estimated ones.  perform a    line-search    to 
find the setting that achieves the highest log-likelihood score 

