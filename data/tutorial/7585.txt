topic models

david m. blei

department of computer science

princeton university

september 1, 2009

d. blei

topic models

the problem with information

as more information becomes
available, it becomes more di   cult
to access what we are looking for.

we need new tools to help us
organize, search, and understand
these vast amounts of information.

d. blei

topic models

www.betaversion.org/~stefano/linotype/news/26/id96

id96 provides methods for automatically organizing,
understanding, searching, and summarizing large electronic archives.

1 uncover the hidden topical patterns that pervade the collection.

2 annotate the documents according to those topics.

3 use the annotations to organize, summarize, and search the texts.

d. blei

topic models

candida hofer discover topics from a corpus

d. blei

topic models

   genetics      evolution      disease      computers   humanevolutiondiseasecomputergenomeevolutionaryhostmodelsdnaspeciesbacteriainformationgeneticorganismsdiseasesdatagenesliferesistancecomputerssequenceoriginbacterialsystemgenebiologynewnetworkmoleculargroupsstrainssystemssequencingphylogeneticcontrolmodelmaplivinginfectiousparallelinformationdiversitymalariamethodsgenetiid19roupparasitenetworksmappingnewparasitessoftwareprojecttwounitednewsequencescommontuberculosissimulationsmodel the evolution of topics over time

d. blei

topic models

1880190019201940196019802000ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo1880190019201940196019802000ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooorelativitylaserforcenerveoxygenneuron"theoretical physics""neuroscience"model connections between topics

d. blei

topic models

wild typemutantmutationsmutantsmutationplantsplantgenegenesarabidopsisp53cell cycleactivitycyclinregulationamino acidscdnasequenceisolatedproteingenediseasemutationsfamiliesmutationrnadnarna polymerasecleavagesitecellscellexpressioncell linesbone marrowunited stateswomenuniversitiesstudentseducationsciencescientistssaysresearchpeopleresearchfundingsupportnihprogramsurfacetipimagesampledevicelaseropticallightelectronsquantummaterialsorganicpolymerpolymersmoleculesvolcanicdepositsmagmaeruptionvolcanismmantlecrustupper mantlemeteoritesratiosearthquakeearthquakesfaultimagesdataancientfoundimpactmillion years agoafricaclimateoceanicechangesclimate changecellsproteinsresearchersproteinfoundpatientsdiseasetreatmentdrugsclinicalgeneticpopulationpopulationsdifferencesvariationfossil recordbirdsfossilsdinosaursfossilsequencesequencesgenomednasequencingbacteriabacterialhostresistanceparasitedevelopmentembryosdrosophilagenesexpressionspeciesforestforestspopulationsecosystemssynapsesltpglutamatesynapticneuronsneuronsstimulusmotorvisualcorticalozoneatmosphericmeasurementsstratosphereconcentrationssunsolar windearthplanetsplanetco2carboncarbon dioxidemethanewaterreceptorreceptorsligandligandsapoptosisproteinsproteinbindingdomaindomainsactivatedtyrosine phosphorylationactivationphosphorylationkinasemagneticmagnetic    eldspinsuperconductivitysuperconductingphysicistsparticlesphysicsparticleexperimentsurfaceliquidsurfaces   uidmodelreactionreactionsmoleculemoleculestransition stateenzymeenzymesironactive sitereductionpressurehigh pressurepressurescoreinner corebrainmemorysubjectslefttaskcomputerprobleminformationcomputersproblemsstarsastronomersuniversegalaxiesgalaxyvirushivaidsinfectionvirusesmiceantigent cellsantigensimmune responseannotate images

d. blei

topic models

automaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53skywatertreemountainpeopleautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53scotlandwaterflowerhillstreeautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53skywaterbuildingpeoplewaterautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53fishwateroceantreecoralautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53peoplemarketpatterntextiledisplayautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53birdsnesttreebranchleavesid96 topics

from a machine learning perspective, id96 is a case study in
applying hierarchical bayesian models to grouped data, like documents or
images. id96 research touches on

    directed id114
    conjugate priors and nonconjugate priors
    time series modeling
    modeling with graphs
    hierarchical bayesian methods
    fast approximate posterior id136 (mcmc, variational methods)
    exploratory data analysis
    model selection and nonparametric bayesian methods
    mixed membership models

d. blei

topic models

id44 (lda)

1 introduction to lda

2 the posterior distribution for lda

approximate posterior id136

1 id150

2 variational id136

3 comparison/theory/advice

other topic models

1 topic models for prediction: relational and supervised topic models

2 the logistic normal: dynamic and correlated topic models

3    in   nite    topic models, i.e., the hierarchical dirichlet process

interpreting and evaluating topic models

d. blei

topic models

id44

d. blei

topic models

probabilistic modeling

1 treat data as observations that arise from a generative probabilistic

process that includes hidden variables

    for documents, the hidden variables re   ect the thematic

structure of the collection.

2 infer the hidden structure using posterior id136

    what are the topics that describe this collection?

3 situate new data into the estimated model.

    how does this query or new document    t into the estimated

topic structure?

d. blei

topic models

intuition behind lda

simple intuition: documents exhibit multiple topics.

d. blei

topic models

generative model

    each document is a random mixture of corpus-wide topics
    each word is drawn from one of those topics

d. blei

topic models

gene     0.04dna      0.02genetic  0.01.,,life     0.02evolve   0.01organism 0.01.,,brain    0.04neuron   0.02nerve    0.01...data     0.02number   0.02computer 0.01.,,topicsdocumentstopic proportions andassignmentsthe posterior distribution

    in reality, we only observe the documents
    our goal is to infer the underlying topic structure

d. blei

topic models

topicsdocumentstopic proportions andassignmentsid114 (aside)

    nodes are random variables
    edges denote possible dependence
    observed variables are shaded
    plates denote replicated structure

d. blei

topic models

      yx1x2xnxnyn   id114 (aside)

    structure of the graph de   nes the pattern of conditional dependence

between the ensemble of random variables

    e.g., this graph corresponds to

p(y , x1, . . . , xn ) = p(y )

n(cid:89)n=1

p(xn | y )

d. blei

topic models

      yx1x2xnxnyn   id44

each piece of the structure is a random variable.

d. blei

topic models

  dzd,nwd,nndk  k    dirichletparameterper-documenttopic proportionsper-wordtopic assignmentobservedwordtopicstopichyperparameterthe dirichlet distribution

    the dirichlet distribution is an exponential family distribution over

the simplex, i.e., positive vectors that sum to one

p(   | (cid:126)  ) =

   ((cid:80)i   i )
(cid:81)i   (  i ) (cid:89)i

    i   1
i

.

    the dirichlet is conjugate to the multinomial. given a multinomial

observation, the posterior distribution of    is a dirichlet.

    the parameter    controls the mean shape and sparsity of   .
    the topic proportions are a k dimensional dirichlet.

the topics are a v dimensional dirichlet.

d. blei

topic models

the dirichlet distribution

(from wikipedia)

d. blei

topic models

id44

    lda is a mixed membership model (erosheva, 2004) that builds on

the work of deerwester et al. (1990) and hofmann (1999).

    for document collections and other grouped data, this might be

more appropriate than a simple    nite mixture.

    the same model was independently invented for population genetics

analysis (pritchard et al., 2000).

d. blei

topic models

  dzd,nwd,nndk  k    id44

    from a collection of documents, infer
    per-word topic assignment zd,n
    per-document topic proportions   d
    per-corpus topic distributions   k

    use posterior expectations to perform the task at hand, e.g.,

information retrieval, document similarity, etc.

d. blei

topic models

  dzd,nwd,nndk  k    id44

approximate posterior id136 algorithms

    mean    eld variational methods (blei et al., 2001, 2003)
    expectation propagation (minka and la   erty, 2002)
    collapsed id150 (gri   ths and steyvers, 2002)
    collapsed variational id136 (teh et al., 2006)

for comparison, see mukherjee and blei (2009) and asuncion et al. (2009).

d. blei

topic models

  dzd,nwd,nndk  k    example id136

    data: the ocr   ed collection of science from 1990   2000

    17k documents
    11m words
    20k unique terms (stop words and rare words removed)

    model: 100-topic lda model using variational id136.

d. blei

topic models

example id136

d. blei

topic models

18162636465666768696topicsid2030.00.10.20.30.4example id136

d. blei

topic models

   genetics      evolution      disease      computers   humanevolutiondiseasecomputergenomeevolutionaryhostmodelsdnaspeciesbacteriainformationgeneticorganismsdiseasesdatagenesliferesistancecomputerssequenceoriginbacterialsystemgenebiologynewnetworkmoleculargroupsstrainssystemssequencingphylogeneticcontrolmodelmaplivinginfectiousparallelinformationdiversitymalariamethodsgenetiid19roupparasitenetworksmappingnewparasitessoftwareprojecttwounitednewsequencescommontuberculosissimulationsexample id136 (ii)

d. blei

topic models

example id136 (ii)

d. blei

topic models

problemmodelselectionspeciesproblemsratemaleforestmathematicalconstantmalesecologynumberdistributionfemales   shnewtimesexecologicalmathematicsnumberspeciesconservationuniversitysizefemalediversitytwovaluesevolutionpopulation   rstvaluepopulationsnaturalnumbersaveragepopulationecosystemsworkratessexualpopulationstimedatabehaviorendangeredmathematiciansdensityevolutionarytropicalchaosmeasuredgeneticforestschaoticmodelsreproductiveecosystemused to explore and browse document collections

d. blei

topic models

measuredaveragerangevaluesdifferentsizethreecalculatedtwolowsequenceregionpcridenti   edfragments twogenesthreecdnaanalysis  residuesbindingdomainshelixcysregionsstructureterminusterminalsitecomputermethodsnumbertwoprincipledesignaccessprocessingadvantageimportant0.000.100.20top ten similar documentsexhaustive matching of the entire protein sequence databasehow big is the universe of exons?counting and discounting the universe of exonsdetecting subtle sequence signals: a id150 strategy for multiple alignmentancient conserved regions in new gene sequences and the protein databasesa method to identify protein sequences that fold into a known three- dimensional structuretesting the exon theory of genes: the evidence from protein structurepredicting coiled coils from protein sequencesgenome sequence of the nematode c. elegans: a platform for investigating biologytop words from the top topics (by term score)expected topic proportionsabstract with the most likely topic assignmentswhy does lda    work   ?

why does the lda posterior put    topical    words together?

    word probabilities are maximized by dividing the words among the

topics. (more terms means more mass to be spread around.)

    in a mixture, this is enough to    nd clusters of co-occurring words.
    in lda, the dirichlet on the topic proportions can encourage
sparsity, i.e., a document is penalized for using many topics.

    loosely, this can be thought of as softening the strict de   nition of

   co-occurrence    in a mixture model.

    this    exibility leads to sets of terms that more tightly co-occur.

d. blei

topic models

lda is modular, general, useful

    lda can be embedded in more complicated models, embodying

further intuitions about the structure of the texts.

    e.g., syntax; authorship; word sense; dynamics; correlation;

hierarchies; nonparametric bayes

d. blei

topic models

dynamictopicmodelsways,andquantitativeresultsthatdemonstrategreaterpre-dictiveaccuracywhencomparedwithstatictopicmodels.2.dynamictopicmodelswhiletraditionaltimeseriesmodelinghasfocusedoncon-tinuousdata,topicmodelsaredesignedforcategoricaldata.ourapproachistousestatespacemodelsonthenat-uralparameterspaceoftheunderlyingtopicmultinomials,aswellasonthenaturalparametersforthelogisticnor-maldistributionsusedformodelingthedocument-speci   ctopicproportions.first,wereviewtheunderlyingstatisticalassumptionsofastatictopicmodel,suchaslatentdirichletallocation(lda)(bleietal.,2003).let  1:kbektopics,eachofwhichisadistributionovera   xedvocabulary.inastatictopicmodel,eachdocumentisassumeddrawnfromthefollowinggenerativeprocess:1.choosetopicproportions  fromadistributionoverthe(k   1)-simplex,suchasadirichlet.2.foreachword:(a)chooseatopicassignmentz   mult(  ).(b)chooseawordw   mult(  z).thisprocessimplicitlyassumesthatthedocumentsaredrawnexchangeablyfromthesamesetoftopics.formanycollections,however,theorderofthedocumentsre   ectsanevolvingsetoftopics.inadynamictopicmodel,wesupposethatthedataisdividedbytimeslice,forexamplebyyear.wemodelthedocumentsofeachslicewithak-componenttopicmodel,wherethetopicsassociatedwithslicetevolvefromthetopicsassociatedwithslicet   1.forak-componentmodelwithvterms,let  t,kdenotethev-vectorofnaturalparametersfortopickinslicet.theusualrepresentationofamultinomialdistributionisbyitsmeanparameterization.ifwedenotethemeanparam-eterofav-dimensionalmultinomialby  ,theithcom-ponentofthenaturalparameterisgivenbythemapping  i=log(  i/  v).intypicallanguagemodelingapplica-tions,dirichletdistributionsareusedtomodeluncertaintyaboutthedistributionsoverwords.however,thedirichletisnotamenabletosequentialmodeling.instead,wechainthenaturalparametersofeachtopic  t,kinastatespacemodelthatevolveswithgaussiannoise;thesimplestver-sionofsuchamodelis  t,k|  t   1,k   n(  t   1,k,  2i).(1)ourapproachisthustomodelsequencesofcompositionalrandomvariablesbychaininggaussiandistributionsinadynamicmodelandmappingtheemittedvaluestothesim-plex.thisisanextensionofthelogisticnormaldistribu-aaa      zzz            wwwnnnkfigure1.graphicalrepresentationofadynamictopicmodel(forthreetimeslices).eachtopic   snaturalparameters  t,kevolveovertime,togetherwiththemeanparameters  tofthelogisticnormaldistributionforthetopicproportions.tion(aitchison,1982)totime-seriessimplexdata(westandharrison,1997).inlda,thedocument-speci   ctopicproportions  aredrawnfromadirichletdistribution.inthedynamictopicmodel,weusealogisticnormalwithmean  toexpressuncertaintyoverproportions.thesequentialstructurebe-tweenmodelsisagaincapturedwithasimpledynamicmodel  t|  t   1   n(  t   1,  2i).(2)forsimplicity,wedonotmodelthedynamicsoftopiccor-relation,aswasdoneforstaticmodelsbybleiandlafferty(2006).bychainingtogethertopicsandtopicproportiondistribu-tions,wehavesequentiallytiedacollectionoftopicmod-els.thegenerativeprocessforslicetofasequentialcorpusisthusasfollows:1.drawtopics  t|  t   1   n(  t   1,  2i).2.draw  t|  t   1   n(  t   1,  2i).3.foreachdocument:(a)draw     n(  t,a2i)(b)foreachword:i.drawz   mult(  (  )).ii.drawwt,d,n   mult(  (  t,z)).notethat  mapsthemultinomialnaturalparameterstothemeanparameters,  (  k,t)w=exp(  k,t,w)pwexp(  k,t,w).thegraphicalmodelforthisgenerativeprocessisshowninfigure1.whenthehorizontalarrowsareremoved,break-ingthetimedynamics,thegraphicalmodelreducestoasetofindependenttopicmodels.withtimedynamics,thekthdctu    nd    di        figure5:modelingcommunitywithtopicssidertheconditionalid203p(c,u,z|  ),aword  as-sociatesthreevariables:community,userandtopic.ourinterpretationofthesemanticmeaningofp(c,u,z|  )istheid203thatword  isgeneratedbyuseruundertopicz,incommunityc.unfortunately,thisconditionalid203cannotbecom-puteddirectly.togetp(c,u,z|  ),wehave:p(c,u,z|  )=p(c,u,z,  )  c,u,zp(c,u,z,  )(3)considerthedenominatorineq.3,summingoverallc,uandzmakesthecomputationimpracticalintermsofef-   ciency.inaddition,asshownin[7],thesummingdoesn   tfactorize,whichmakesthemanipulationofdenominatordi   cult.inthefollowingsection,wewillshowhowanapproximateapproachofgibbssamplingwillprovideso-lutionstosuchproblems.afasteralgorithmenf-gibbssamplingwillalsobeintroduced.4.semanticcommunitydiscovery:thealgorithmsinthissection,we   rstintroducethegibbssamplingalgorithm.thenweaddresstheproblemofsemanticcom-munitydiscoverybyadaptinggibbssamplingframeworktoourmodels.finally,wecombinetwopowerfulideas:gibbssamplingandid178   lteringtoimprovee   ciencyandperformance,yieldinganewalgorithm:enf-gibbssampling.4.1gibbssamplinggibbssamplingisanalgorithmtoapproximatethejointdistributionofmultiplevariablesbydrawingasequenceofsamples.asaspecialcaseofthemetropolis-hastingsalgorithm[18],gibbssamplingisamarkovchainmontecarloalgorithmandusuallyapplieswhentheconditionalid203distributionofeachvariablecanbeevaluated.ratherthanexplicitlyparameterizingthedistributionsforvariables,gibbssamplingintegratesouttheparametersandestimatesthecorrespondingposteriorid203.gibbssamplingwas   rstintroducedtoestimatethetopic-wordmodelin[7].ingibbssampling,amarkovchainisformed,thetransitionbetweensuccessivestatesofwhichissimulatedbyrepeatedlydrawingatopicforeachob-servedwordfromitsconditionalid203onallothervariables.intheauthor-topicmodel,thealgorithmgoesoveralldocumentswordbyword.foreachword  i,thetopicziandtheauthorxiresponsibleforthiswordareassignedbasedontheposteriorid203conditionedonallothervariables:p(zi,xi|  i,z   i,x   i,w   i,ad).ziandxidenotethetopicandauthorassignedto  i,whilez   iandx   iareallotherassignmentsoftopicandauthorex-cludingcurrentinstance.w   irepresentsotherobservedwordsinthedocumentsetandadistheobservedauthorsetforthisdocument.akeyissueinusinggibbssamplingfordistributionapproximationistheevaluationofconditionalposteriorid203.inauthor-topicmodel,giventtopicsandvwords,p(zi,xi|  i,z   i,x   i,w   i,ad)isestimatedby:p(zi=j,xi=k|  i=m,z   i,x   i,w   i,ad)   (4)p(  i=m|xi=k)p(xi=k|zi=j)   (5)cwtmj+    m!cwtm!j+v  catkj+    j!catkj!+t  (6)wherem""=mandj""=j,  and  arepriorparametersforwordandtopicdirichlets,cwtmjrepresentsthenumberoftimesthatword  i=misassignedtotopiczi=j,catkjrepresentsthenumberoftimesthatauthorxi=kisassignedtotopicj.thetransformationfromeq.4toeq.5dropsthevari-ables,z   i,x   i,w   i,ad,becauseeachinstanceof  iisassumedindependentoftheotherwordsinamessage.4.2semanticcommunitydiscoverybyapplyingthegibbssampling,wecandiscoverthese-manticcommunitiesbyusingthecutmodels.considertheconditionalid203p(c,u,z|  ),wherethreevari-ablesinthemodel,community,user4andtopic,areasso-ciatedbyaword  .thesemanticmeaningofp(c,u,z|  )istheid203that  belongstouseruundertopicz,incommunityc.byestimationofp(c,u,z|  ),wecanla-belacommunitywithsemantictags(topics)inadditiontothea   liatedusers.theproblemofsemanticcommunitydiscoveryisthusreducedtotheestimationofp(c,u,z|  ).(1)/*initialization*/(2)foreachemaild(3)foreachword  iind(4)assign  itorandomcommunity,topicanduser;(5)/*userinthelistobservedfromd*/(6)/*markovchainconvergence*/(7)i   0;(8)i   desirednumberofiterations;(9)whilei<i(10)foreachemaild(11)foreach  i   d(12)estimatep(ci,ui,zi|  i),u     d;(13)(p,q,r)   argmax(p(cp,uq,zr|  i));(14)/*assigncommunityp,userq,topicrto  i*/(15)recordassignment  (cp,uq,zr,  i);(16)i++;figure6:gibbssamplingforcutmodels4notewedenoteuserwithuinourmodelsinsteadofxasinpreviouswork.177zwd!"#$tdnzwd!0"#$tdn%2"x&   1"((a)(b)figure1:graphicalmodelsfor(a)thestandardldatopicmodel(left)and(b)theproposedspecialwordstopicmodelwithabackgrounddistribution(swb)(right).aregeneratedbydrawingatopictfromthedocument-topicdistributionp(z|  d)andthendrawingawordwfromthetopic-worddistributionp(w|z=t,  t).asshowningrif   thsandsteyvers(2004)thetopicassignmentszforeachwordtokeninthecorpuscanbeef   cientlysampledviagibbssampling(aftermarginalizingover  and  ).pointestimatesforthe  and  distributionscanbecomputedconditionedonaparticularsample,andpredictivedistributionscanbeobtainedbyaveragingovermultiplesamples.wewillrefertotheproposedmodelasthespecialwordstopicmodelwithbackgrounddistribution(swb)(figure1(b)).swbhasasimilargeneralstructuretotheldamodel(figure1(a))butwithadditionalmachinerytohandlespecialwordsandbackgroundwords.inparticular,associatedwitheachwordtokenisalatentrandomvariablex,takingvaluex=0ifthewordwisgeneratedviathetopicroute,valuex=1ifthewordisgeneratedasaspecialword(forthatdocument)andvaluex=2ifthewordisgeneratedfromabackgrounddistributionspeci   cforthecorpus.thevariablexactsasaswitch:ifx=0,thepreviouslydescribedstandardtopicmechanismisusedtogeneratetheword,whereasifx=1orx=2,wordsaresampledfromadocument-speci   cmultinomial  oracorpusspeci   cmultinomial   (withsymmetricdirichletpriorsparametrizedby  1and  2)respectively.xissampledfromadocument-speci   cmultinomial  ,whichinturnhasasymmetricdirichletprior,  .onecouldalsouseahierarchicalbayesianapproachtointroduceanotherlevelofuncertaintyaboutthedirichletpriors(e.g.,seeblei,ng,andjordan,2003)   wehavenotinvestigatedthisoption,primarilyforcomputationalreasons.inallourexperiments,weset  =0.1,  0=  2=0.01,  1=0.0001and  =0.3   allweaksymmetricpriors.theconditionalid203ofawordwgivenadocumentdcanbewrittenas:p(w|d)=p(x=0|d)t!t=1p(w|z=t)p(z=t|d)+p(x=1|d)p!(w|d)+p(x=2|d)p!!(w)wherep!(w|d)isthespecialworddistributionfordocumentd,andp!!(w)isthebackgroundworddistributionforthecorpus.notethatwhencomparedtothestandardtopicmodeltheswbmodelcanexplainwordsinthreedifferentways,viatopics,viaaspecialworddistribution,orviaaback-groundworddistribution.giventhegraphicalmodelabove,itisrelativelystraightforwardtoderivegibbssamplingequationsthatallowjointsamplingoftheziandxilatentvariablesforeachwordtokenwi,forxi=0:p(xi=0,zi=t|w,x   i,z   i,  ,  0,  )   nd0,   i+  nd,   i+3    ctdtd,   i+  "t!ctdt!d,   i+t    cwtwt,   i+  0"w!cwtw!t,   i+w  0andforxi=1:p(xi=1|w,x   i,z   i,  1,  )   nd1,   i+  nd,   i+3    cwdwd,   i+  1"w!cwdw!d,   i+w  1mccallum,wang,&corrada-emmanuel!"zwid44(lda)[blei, ng, jordan, 2003]ndxzwauthor-topic model(at)[rosen-zvi, grif   ths, steyvers, smyth 2004]nd"#!$ta#$txzwauthor-recipient-topic model(art)[this paper]nd"#!$ta,azwauthor model(multi-label mixture model)[mccallum 1999]nd#$aadadrdadddddfigure1:threerelatedmodels,andtheartmodel.inallmodels,eachobservedword,w,isgeneratedfromamultinomialworddistribution,  z,speci   ctoaparticulartopic/author,z,howevertopicsareselecteddi   erentlyineachofthemodels.inlda,thetopicissampledfromaper-documenttopicdistribution,  ,whichinturnissampledfromadirichletovertopics.intheauthormodel,thereisonetopicassociatedwitheachauthor(orcategory),andauthorsaresampleduniformly.intheauthor-topicmodel,thetopicissampledfromaper-authormultinomialdistribution,  ,andauthorsaresampleduniformlyfromtheobservedlistofthedocument   sauthors.intheauthor-recipient-topicmodel,thereisaseparatetopic-distributionforeachauthor-recipientpair,andtheselectionoftopic-distributionisdeterminedfromtheobservedauthor,andbyuniformlysam-plingarecipientfromthesetofrecipientsforthedocument.itsgenerativeprocessforeachdocumentd,asetofauthors,ad,isobserved.togenerateeachword,anauthorxischosenuniformlyfromthisset,thenatopiczisselectedfromatopicdistribution  xthatisspeci   ctotheauthor,andthenawordwisgeneratedfromatopic-speci   cmultinomialdistribution  z.however,asdescribedpreviously,noneofthesemodelsissuitableformodelingmessagedata.anemailmessagehasonesenderandingeneralmorethanonerecipients.wecouldtreatboththesenderandtherecipientsas   authors   ofthemessage,andthenemploytheatmodel,butthisdoesnotdistinguishtheauthorandtherecipientsofthemessage,whichisundesirableinmanyreal-worldsituations.amanagermaysendemailtoasecretaryandviceversa,butthenatureoftherequestsandlanguageusedmaybequitedi   erent.evenmoredramatically,considerthelargequantityofjunkemailthatwereceive;modelingthetopicsofthesemessagesasundistinguishedfromthetopicswewriteaboutasauthorswouldbeextremelyconfoundingandundesirablesincetheydonotre   ectourexpertiseorroles.alternativelywecouldstillemploytheatmodelbyignoringtherecipientinformationofemailandtreatingeachemaildocumentasifitonlyhasoneauthor.however,inthiscase(whichissimilartotheldamodel)wearelosingallinformationabouttherecipients,andtheconnectionsbetweenpeopleimpliedbythesender-recipientrelationships.252networkneuralnetworksoutput...imageimagesobjectobjects...supportvectorid166...kernelinwithforon...usedtrainedobtaineddescribed...0.50.40.10.70.90.80.2neuralnetworktrainedwithid166imagesimageoutputnetworkforused imageskernelwithobtaineddescribedwithobjectsssss1423wwww1423zzzz!1423(a)(b)figure1:thecompositemodel.(a)graphicalmodel.(b)generatingphrases.  (z),eachclassc!=1isassociatedwithadistributionoverwords  (c),eachdocumentdhasadistributionovertopics  (d),andtransitionsbetweenclassesci   1andcifollowadistribution  (si   1).adocumentisgeneratedviathefollowingprocedure:1.sample  (d)fromadirichlet(  )prior2.foreachwordwiindocumentd(a)drawzifrom  (d)(b)drawcifrom  (ci   1)(c)ifci=1,thendrawwifrom  (zi),elsedrawwifrom  (ci)figure1(b)providesanintuitiverepresentationofhowphrasesaregeneratedbythecom-positemodel.the   gureshowsathreeclassid48.twoclassesaresimplemultinomialdistributionsoverwords.thethirdisatopicmodel,containingthreetopics.transitionsbetweenclassesareshownwitharrows,annotatedwithtransitionprobabilities.thetop-icsinthesemanticclassalsohaveprobabilities,usedtochooseatopicwhentheid48transitionstothesemanticclass.phrasesaregeneratedbyfollowingapaththroughthemodel,choosingawordfromthedistributionassociatedwitheachsyntacticclass,andatopicfollowedbyawordfromthedistributionassociatedwiththattopicfortheseman-ticclass.sentenceswiththesamesyntaxbutdifferentcontentwouldbegeneratedifthetopicdistributionweredifferent.thegenerativemodelthusactslikeitisplayingagameof   madlibs   :thesemanticcomponentprovidesalistoftopicalwords(showninblack)whichareslottedintotemplatesgeneratedbythesyntacticcomponent(showningray).2.2id136theemalgorithmcanbeappliedtothegraphicalmodelshowninfigure1,treatingthedocumentdistributions  ,thetopicsandclasses  ,andthetransitionprobabilities  asparameters.however,emproducespoorresultswithtopicmodels,whichhavemanypa-rametersandmanylocalmaxima.consequently,recentworkhasfocusedonapproximateid136algorithms[6,8].wewillusemarkovchainmontecarlo(mcmc;see[9])toperformfullbayesianid136inthismodel,samplingfromaposteriordistributionoverassignmentsofwordstoclassesandtopics.weassumethatthedocument-speci   cdistributionsovertopics,  ,aredrawnfromadirichlet(  )distribution,thetopicdistributions  (z)aredrawnfromadirichlet(  )dis-tribution,therowsofthetransitionmatrixfortheid48aredrawnfromadirichlet(  )distribution,theclassdistributions  (c)aredrawnfromadirichlet(  )distribution,andalldirichletdistributionsaresymmetric.weusegibbssamplingtodrawiterativelyatopicassignmentziandclassassignmentciforeachwordwiinthecorpus(see[8,9]).giventhewordsw,theclassassignmentsc,theothertopicassignmentsz   i,andthehyperparameters,eachziisdrawnfrom:p(zi|z   i,c,w)   p(zi|z   i)p(wi|z,c,w   i)   !n(di)zi+  (n(di)zi+  )n(zi)wi+  n(zi)+w  ci!=1ci=1constraintsofwordalignment,i.e.,words   close-in-source   areusuallyalignedtowords   close-in-target   ,underdocument-speci   ctopicalassignment.toincorporatesuchconstituents,weintegratethestrengthsofbothid48andbitam,andproposeahiddenmarkovbilingualtopic-admixturemodel,orhm-bitam,forwordalignmenttoleveragebothlocalityconstraintsandtopicalcontextunderlyingparalleldocument-pairs.inthehm-bitamframework,onecanestimatetopic-speci   cword-to-wordtranslationlexicons(lexicalmappings),aswellasthemonolingualtopic-speci   cword-frequenciesforbothlanguages,basedonparalleldocument-pairs.theresultingmodeloffersaprincipledwayofinferringoptimaltranslationfromagivensourcelanguageinacontext-dependentfashion.wereportanextensiveempiricalanalysisofhm-bitam,incomparisonwithrelatedmethods.weshowourmodel   sef-fectivenessontheword-alignmenttask;wealsodemonstratetwoapplicationaspectswhichwereuntouchedin[10]:theutilityofhm-bitamforbilingualtopicexploration,anditsapplicationforimprovingtranslationqualities.2revisitid48forsmtansmtsystemcanbeformulatedasanoisy-channelmodel[2]:e   =argmaxep(e|f)=argmaxep(f|e)p(e),(1)whereatranslationcorrespondstosearchingforthetargetsentencee   whichexplainsthesourcesentencefbest.thekeycomponentisp(f|e),thetranslationmodel;p(e)ismonolinguallanguagemodel.inthispaper,wegeneralizep(f|e)withtopic-admixturemodels.anid48implementsthe   proximity-bias   assumption   thatwords   close-in-source   arealignedtowords   close-in-target   ,whichiseffectiveforimprovingwordalignmentaccuracies,especiallyforlinguisticallycloselanguage-pairs[8].following[8],tomodelword-to-wordtranslation,weintroducethemappingj   aj,whichassignsafrenchwordfjinpositionjtoanenglishwordeiinpositioni=ajdenotedaseaj.each(ordered)frenchwordfjisanobservation,anditisgeneratedbyanid48statede   nedas[eaj,aj],wherethealignmentindicatorajforpositionjisconsideredtohaveadependencyonthepreviousalignmentaj   1.thusa   rst-orderid48foranalignmentbetweene   e1:iandf   f1:jisde   nedas:p(f1:j|e1:i)=!a1:jj"j=1p(fj|eaj)p(aj|aj   1),(2)wherep(aj|aj   1)isthestatetransitionid203;jandiaresentencelengthsofthefrenchandenglishsentences,respectively.thetransitionmodelenforcestheproximity-bias.anadditionalpseudoword   null   isusedatthebeginningofenglishsentencesforid48tostartwith.theid48implementedingiza++[5]isusedasourbaseline,whichincludesre   nementssuchasspecialtreatmentofajumptoanullword.agraphicalmodelrepresentationforsuchanid48isillustratedinfigure1(a).ti,i!fm,3fm,2fm,1fjm,nmam,3am,2am,1ajm,nem,iim,nb=p(f|e)nm  zm,n  mfm,3fm,2fm,1bkfjm,nnmmam,3am,2am,1em,iim,nti,i!  kkkajm,n(a)id48forwordalignment(b)hm-bitamfigure1:thegraphicalmodelrepresentationsof(a)id48,and(b)hm-bitam,forparallelcorpora.circlesrepresentrandomvariables,hexagonsdenoteparameters,andobservedvariablesareshaded.2lda is modular, general, useful

    the data generating distribution can be changed.
    e.g., images, social networks, music, purchase histories, computer

code, genetic data, click-through data; ...

d. blei

topic models

dynamictopicmodelsways,andquantitativeresultsthatdemonstrategreaterpre-dictiveaccuracywhencomparedwithstatictopicmodels.2.dynamictopicmodelswhiletraditionaltimeseriesmodelinghasfocusedoncon-tinuousdata,topicmodelsaredesignedforcategoricaldata.ourapproachistousestatespacemodelsonthenat-uralparameterspaceoftheunderlyingtopicmultinomials,aswellasonthenaturalparametersforthelogisticnor-maldistributionsusedformodelingthedocument-speci   ctopicproportions.first,wereviewtheunderlyingstatisticalassumptionsofastatictopicmodel,suchaslatentdirichletallocation(lda)(bleietal.,2003).let  1:kbektopics,eachofwhichisadistributionovera   xedvocabulary.inastatictopicmodel,eachdocumentisassumeddrawnfromthefollowinggenerativeprocess:1.choosetopicproportions  fromadistributionoverthe(k   1)-simplex,suchasadirichlet.2.foreachword:(a)chooseatopicassignmentz   mult(  ).(b)chooseawordw   mult(  z).thisprocessimplicitlyassumesthatthedocumentsaredrawnexchangeablyfromthesamesetoftopics.formanycollections,however,theorderofthedocumentsre   ectsanevolvingsetoftopics.inadynamictopicmodel,wesupposethatthedataisdividedbytimeslice,forexamplebyyear.wemodelthedocumentsofeachslicewithak-componenttopicmodel,wherethetopicsassociatedwithslicetevolvefromthetopicsassociatedwithslicet   1.forak-componentmodelwithvterms,let  t,kdenotethev-vectorofnaturalparametersfortopickinslicet.theusualrepresentationofamultinomialdistributionisbyitsmeanparameterization.ifwedenotethemeanparam-eterofav-dimensionalmultinomialby  ,theithcom-ponentofthenaturalparameterisgivenbythemapping  i=log(  i/  v).intypicallanguagemodelingapplica-tions,dirichletdistributionsareusedtomodeluncertaintyaboutthedistributionsoverwords.however,thedirichletisnotamenabletosequentialmodeling.instead,wechainthenaturalparametersofeachtopic  t,kinastatespacemodelthatevolveswithgaussiannoise;thesimplestver-sionofsuchamodelis  t,k|  t   1,k   n(  t   1,k,  2i).(1)ourapproachisthustomodelsequencesofcompositionalrandomvariablesbychaininggaussiandistributionsinadynamicmodelandmappingtheemittedvaluestothesim-plex.thisisanextensionofthelogisticnormaldistribu-aaa      zzz            wwwnnnkfigure1.graphicalrepresentationofadynamictopicmodel(forthreetimeslices).eachtopic   snaturalparameters  t,kevolveovertime,togetherwiththemeanparameters  tofthelogisticnormaldistributionforthetopicproportions.tion(aitchison,1982)totime-seriessimplexdata(westandharrison,1997).inlda,thedocument-speci   ctopicproportions  aredrawnfromadirichletdistribution.inthedynamictopicmodel,weusealogisticnormalwithmean  toexpressuncertaintyoverproportions.thesequentialstructurebe-tweenmodelsisagaincapturedwithasimpledynamicmodel  t|  t   1   n(  t   1,  2i).(2)forsimplicity,wedonotmodelthedynamicsoftopiccor-relation,aswasdoneforstaticmodelsbybleiandlafferty(2006).bychainingtogethertopicsandtopicproportiondistribu-tions,wehavesequentiallytiedacollectionoftopicmod-els.thegenerativeprocessforslicetofasequentialcorpusisthusasfollows:1.drawtopics  t|  t   1   n(  t   1,  2i).2.draw  t|  t   1   n(  t   1,  2i).3.foreachdocument:(a)draw     n(  t,a2i)(b)foreachword:i.drawz   mult(  (  )).ii.drawwt,d,n   mult(  (  t,z)).notethat  mapsthemultinomialnaturalparameterstothemeanparameters,  (  k,t)w=exp(  k,t,w)pwexp(  k,t,w).thegraphicalmodelforthisgenerativeprocessisshowninfigure1.whenthehorizontalarrowsareremoved,break-ingthetimedynamics,thegraphicalmodelreducestoasetofindependenttopicmodels.withtimedynamics,thekthdctu    nd    di        figure5:modelingcommunitywithtopicssidertheconditionalid203p(c,u,z|  ),aword  as-sociatesthreevariables:community,userandtopic.ourinterpretationofthesemanticmeaningofp(c,u,z|  )istheid203thatword  isgeneratedbyuseruundertopicz,incommunityc.unfortunately,thisconditionalid203cannotbecom-puteddirectly.togetp(c,u,z|  ),wehave:p(c,u,z|  )=p(c,u,z,  )  c,u,zp(c,u,z,  )(3)considerthedenominatorineq.3,summingoverallc,uandzmakesthecomputationimpracticalintermsofef-   ciency.inaddition,asshownin[7],thesummingdoesn   tfactorize,whichmakesthemanipulationofdenominatordi   cult.inthefollowingsection,wewillshowhowanapproximateapproachofgibbssamplingwillprovideso-lutionstosuchproblems.afasteralgorithmenf-gibbssamplingwillalsobeintroduced.4.semanticcommunitydiscovery:thealgorithmsinthissection,we   rstintroducethegibbssamplingalgorithm.thenweaddresstheproblemofsemanticcom-munitydiscoverybyadaptinggibbssamplingframeworktoourmodels.finally,wecombinetwopowerfulideas:gibbssamplingandid178   lteringtoimprovee   ciencyandperformance,yieldinganewalgorithm:enf-gibbssampling.4.1gibbssamplinggibbssamplingisanalgorithmtoapproximatethejointdistributionofmultiplevariablesbydrawingasequenceofsamples.asaspecialcaseofthemetropolis-hastingsalgorithm[18],gibbssamplingisamarkovchainmontecarloalgorithmandusuallyapplieswhentheconditionalid203distributionofeachvariablecanbeevaluated.ratherthanexplicitlyparameterizingthedistributionsforvariables,gibbssamplingintegratesouttheparametersandestimatesthecorrespondingposteriorid203.gibbssamplingwas   rstintroducedtoestimatethetopic-wordmodelin[7].ingibbssampling,amarkovchainisformed,thetransitionbetweensuccessivestatesofwhichissimulatedbyrepeatedlydrawingatopicforeachob-servedwordfromitsconditionalid203onallothervariables.intheauthor-topicmodel,thealgorithmgoesoveralldocumentswordbyword.foreachword  i,thetopicziandtheauthorxiresponsibleforthiswordareassignedbasedontheposteriorid203conditionedonallothervariables:p(zi,xi|  i,z   i,x   i,w   i,ad).ziandxidenotethetopicandauthorassignedto  i,whilez   iandx   iareallotherassignmentsoftopicandauthorex-cludingcurrentinstance.w   irepresentsotherobservedwordsinthedocumentsetandadistheobservedauthorsetforthisdocument.akeyissueinusinggibbssamplingfordistributionapproximationistheevaluationofconditionalposteriorid203.inauthor-topicmodel,giventtopicsandvwords,p(zi,xi|  i,z   i,x   i,w   i,ad)isestimatedby:p(zi=j,xi=k|  i=m,z   i,x   i,w   i,ad)   (4)p(  i=m|xi=k)p(xi=k|zi=j)   (5)cwtmj+    m!cwtm!j+v  catkj+    j!catkj!+t  (6)wherem""=mandj""=j,  and  arepriorparametersforwordandtopicdirichlets,cwtmjrepresentsthenumberoftimesthatword  i=misassignedtotopiczi=j,catkjrepresentsthenumberoftimesthatauthorxi=kisassignedtotopicj.thetransformationfromeq.4toeq.5dropsthevari-ables,z   i,x   i,w   i,ad,becauseeachinstanceof  iisassumedindependentoftheotherwordsinamessage.4.2semanticcommunitydiscoverybyapplyingthegibbssampling,wecandiscoverthese-manticcommunitiesbyusingthecutmodels.considertheconditionalid203p(c,u,z|  ),wherethreevari-ablesinthemodel,community,user4andtopic,areasso-ciatedbyaword  .thesemanticmeaningofp(c,u,z|  )istheid203that  belongstouseruundertopicz,incommunityc.byestimationofp(c,u,z|  ),wecanla-belacommunitywithsemantictags(topics)inadditiontothea   liatedusers.theproblemofsemanticcommunitydiscoveryisthusreducedtotheestimationofp(c,u,z|  ).(1)/*initialization*/(2)foreachemaild(3)foreachword  iind(4)assign  itorandomcommunity,topicanduser;(5)/*userinthelistobservedfromd*/(6)/*markovchainconvergence*/(7)i   0;(8)i   desirednumberofiterations;(9)whilei<i(10)foreachemaild(11)foreach  i   d(12)estimatep(ci,ui,zi|  i),u     d;(13)(p,q,r)   argmax(p(cp,uq,zr|  i));(14)/*assigncommunityp,userq,topicrto  i*/(15)recordassignment  (cp,uq,zr,  i);(16)i++;figure6:gibbssamplingforcutmodels4notewedenoteuserwithuinourmodelsinsteadofxasinpreviouswork.177zwd!"#$tdnzwd!0"#$tdn%2"x&   1"((a)(b)figure1:graphicalmodelsfor(a)thestandardldatopicmodel(left)and(b)theproposedspecialwordstopicmodelwithabackgrounddistribution(swb)(right).aregeneratedbydrawingatopictfromthedocument-topicdistributionp(z|  d)andthendrawingawordwfromthetopic-worddistributionp(w|z=t,  t).asshowningrif   thsandsteyvers(2004)thetopicassignmentszforeachwordtokeninthecorpuscanbeef   cientlysampledviagibbssampling(aftermarginalizingover  and  ).pointestimatesforthe  and  distributionscanbecomputedconditionedonaparticularsample,andpredictivedistributionscanbeobtainedbyaveragingovermultiplesamples.wewillrefertotheproposedmodelasthespecialwordstopicmodelwithbackgrounddistribution(swb)(figure1(b)).swbhasasimilargeneralstructuretotheldamodel(figure1(a))butwithadditionalmachinerytohandlespecialwordsandbackgroundwords.inparticular,associatedwitheachwordtokenisalatentrandomvariablex,takingvaluex=0ifthewordwisgeneratedviathetopicroute,valuex=1ifthewordisgeneratedasaspecialword(forthatdocument)andvaluex=2ifthewordisgeneratedfromabackgrounddistributionspeci   cforthecorpus.thevariablexactsasaswitch:ifx=0,thepreviouslydescribedstandardtopicmechanismisusedtogeneratetheword,whereasifx=1orx=2,wordsaresampledfromadocument-speci   cmultinomial  oracorpusspeci   cmultinomial   (withsymmetricdirichletpriorsparametrizedby  1and  2)respectively.xissampledfromadocument-speci   cmultinomial  ,whichinturnhasasymmetricdirichletprior,  .onecouldalsouseahierarchicalbayesianapproachtointroduceanotherlevelofuncertaintyaboutthedirichletpriors(e.g.,seeblei,ng,andjordan,2003)   wehavenotinvestigatedthisoption,primarilyforcomputationalreasons.inallourexperiments,weset  =0.1,  0=  2=0.01,  1=0.0001and  =0.3   allweaksymmetricpriors.theconditionalid203ofawordwgivenadocumentdcanbewrittenas:p(w|d)=p(x=0|d)t!t=1p(w|z=t)p(z=t|d)+p(x=1|d)p!(w|d)+p(x=2|d)p!!(w)wherep!(w|d)isthespecialworddistributionfordocumentd,andp!!(w)isthebackgroundworddistributionforthecorpus.notethatwhencomparedtothestandardtopicmodeltheswbmodelcanexplainwordsinthreedifferentways,viatopics,viaaspecialworddistribution,orviaaback-groundworddistribution.giventhegraphicalmodelabove,itisrelativelystraightforwardtoderivegibbssamplingequationsthatallowjointsamplingoftheziandxilatentvariablesforeachwordtokenwi,forxi=0:p(xi=0,zi=t|w,x   i,z   i,  ,  0,  )   nd0,   i+  nd,   i+3    ctdtd,   i+  "t!ctdt!d,   i+t    cwtwt,   i+  0"w!cwtw!t,   i+w  0andforxi=1:p(xi=1|w,x   i,z   i,  1,  )   nd1,   i+  nd,   i+3    cwdwd,   i+  1"w!cwdw!d,   i+w  1mccallum,wang,&corrada-emmanuel!"zwid44(lda)[blei, ng, jordan, 2003]ndxzwauthor-topic model(at)[rosen-zvi, grif   ths, steyvers, smyth 2004]nd"#!$ta#$txzwauthor-recipient-topic model(art)[this paper]nd"#!$ta,azwauthor model(multi-label mixture model)[mccallum 1999]nd#$aadadrdadddddfigure1:threerelatedmodels,andtheartmodel.inallmodels,eachobservedword,w,isgeneratedfromamultinomialworddistribution,  z,speci   ctoaparticulartopic/author,z,howevertopicsareselecteddi   erentlyineachofthemodels.inlda,thetopicissampledfromaper-documenttopicdistribution,  ,whichinturnissampledfromadirichletovertopics.intheauthormodel,thereisonetopicassociatedwitheachauthor(orcategory),andauthorsaresampleduniformly.intheauthor-topicmodel,thetopicissampledfromaper-authormultinomialdistribution,  ,andauthorsaresampleduniformlyfromtheobservedlistofthedocument   sauthors.intheauthor-recipient-topicmodel,thereisaseparatetopic-distributionforeachauthor-recipientpair,andtheselectionoftopic-distributionisdeterminedfromtheobservedauthor,andbyuniformlysam-plingarecipientfromthesetofrecipientsforthedocument.itsgenerativeprocessforeachdocumentd,asetofauthors,ad,isobserved.togenerateeachword,anauthorxischosenuniformlyfromthisset,thenatopiczisselectedfromatopicdistribution  xthatisspeci   ctotheauthor,andthenawordwisgeneratedfromatopic-speci   cmultinomialdistribution  z.however,asdescribedpreviously,noneofthesemodelsissuitableformodelingmessagedata.anemailmessagehasonesenderandingeneralmorethanonerecipients.wecouldtreatboththesenderandtherecipientsas   authors   ofthemessage,andthenemploytheatmodel,butthisdoesnotdistinguishtheauthorandtherecipientsofthemessage,whichisundesirableinmanyreal-worldsituations.amanagermaysendemailtoasecretaryandviceversa,butthenatureoftherequestsandlanguageusedmaybequitedi   erent.evenmoredramatically,considerthelargequantityofjunkemailthatwereceive;modelingthetopicsofthesemessagesasundistinguishedfromthetopicswewriteaboutasauthorswouldbeextremelyconfoundingandundesirablesincetheydonotre   ectourexpertiseorroles.alternativelywecouldstillemploytheatmodelbyignoringtherecipientinformationofemailandtreatingeachemaildocumentasifitonlyhasoneauthor.however,inthiscase(whichissimilartotheldamodel)wearelosingallinformationabouttherecipients,andtheconnectionsbetweenpeopleimpliedbythesender-recipientrelationships.252networkneuralnetworksoutput...imageimagesobjectobjects...supportvectorid166...kernelinwithforon...usedtrainedobtaineddescribed...0.50.40.10.70.90.80.2neuralnetworktrainedwithid166imagesimageoutputnetworkforused imageskernelwithobtaineddescribedwithobjectsssss1423wwww1423zzzz!1423(a)(b)figure1:thecompositemodel.(a)graphicalmodel.(b)generatingphrases.  (z),eachclassc!=1isassociatedwithadistributionoverwords  (c),eachdocumentdhasadistributionovertopics  (d),andtransitionsbetweenclassesci   1andcifollowadistribution  (si   1).adocumentisgeneratedviathefollowingprocedure:1.sample  (d)fromadirichlet(  )prior2.foreachwordwiindocumentd(a)drawzifrom  (d)(b)drawcifrom  (ci   1)(c)ifci=1,thendrawwifrom  (zi),elsedrawwifrom  (ci)figure1(b)providesanintuitiverepresentationofhowphrasesaregeneratedbythecom-positemodel.the   gureshowsathreeclassid48.twoclassesaresimplemultinomialdistributionsoverwords.thethirdisatopicmodel,containingthreetopics.transitionsbetweenclassesareshownwitharrows,annotatedwithtransitionprobabilities.thetop-icsinthesemanticclassalsohaveprobabilities,usedtochooseatopicwhentheid48transitionstothesemanticclass.phrasesaregeneratedbyfollowingapaththroughthemodel,choosingawordfromthedistributionassociatedwitheachsyntacticclass,andatopicfollowedbyawordfromthedistributionassociatedwiththattopicfortheseman-ticclass.sentenceswiththesamesyntaxbutdifferentcontentwouldbegeneratedifthetopicdistributionweredifferent.thegenerativemodelthusactslikeitisplayingagameof   madlibs   :thesemanticcomponentprovidesalistoftopicalwords(showninblack)whichareslottedintotemplatesgeneratedbythesyntacticcomponent(showningray).2.2id136theemalgorithmcanbeappliedtothegraphicalmodelshowninfigure1,treatingthedocumentdistributions  ,thetopicsandclasses  ,andthetransitionprobabilities  asparameters.however,emproducespoorresultswithtopicmodels,whichhavemanypa-rametersandmanylocalmaxima.consequently,recentworkhasfocusedonapproximateid136algorithms[6,8].wewillusemarkovchainmontecarlo(mcmc;see[9])toperformfullbayesianid136inthismodel,samplingfromaposteriordistributionoverassignmentsofwordstoclassesandtopics.weassumethatthedocument-speci   cdistributionsovertopics,  ,aredrawnfromadirichlet(  )distribution,thetopicdistributions  (z)aredrawnfromadirichlet(  )dis-tribution,therowsofthetransitionmatrixfortheid48aredrawnfromadirichlet(  )distribution,theclassdistributions  (c)aredrawnfromadirichlet(  )distribution,andalldirichletdistributionsaresymmetric.weusegibbssamplingtodrawiterativelyatopicassignmentziandclassassignmentciforeachwordwiinthecorpus(see[8,9]).giventhewordsw,theclassassignmentsc,theothertopicassignmentsz   i,andthehyperparameters,eachziisdrawnfrom:p(zi|z   i,c,w)   p(zi|z   i)p(wi|z,c,w   i)   !n(di)zi+  (n(di)zi+  )n(zi)wi+  n(zi)+w  ci!=1ci=1constraintsofwordalignment,i.e.,words   close-in-source   areusuallyalignedtowords   close-in-target   ,underdocument-speci   ctopicalassignment.toincorporatesuchconstituents,weintegratethestrengthsofbothid48andbitam,andproposeahiddenmarkovbilingualtopic-admixturemodel,orhm-bitam,forwordalignmenttoleveragebothlocalityconstraintsandtopicalcontextunderlyingparalleldocument-pairs.inthehm-bitamframework,onecanestimatetopic-speci   cword-to-wordtranslationlexicons(lexicalmappings),aswellasthemonolingualtopic-speci   cword-frequenciesforbothlanguages,basedonparalleldocument-pairs.theresultingmodeloffersaprincipledwayofinferringoptimaltranslationfromagivensourcelanguageinacontext-dependentfashion.wereportanextensiveempiricalanalysisofhm-bitam,incomparisonwithrelatedmethods.weshowourmodel   sef-fectivenessontheword-alignmenttask;wealsodemonstratetwoapplicationaspectswhichwereuntouchedin[10]:theutilityofhm-bitamforbilingualtopicexploration,anditsapplicationforimprovingtranslationqualities.2revisitid48forsmtansmtsystemcanbeformulatedasanoisy-channelmodel[2]:e   =argmaxep(e|f)=argmaxep(f|e)p(e),(1)whereatranslationcorrespondstosearchingforthetargetsentencee   whichexplainsthesourcesentencefbest.thekeycomponentisp(f|e),thetranslationmodel;p(e)ismonolinguallanguagemodel.inthispaper,wegeneralizep(f|e)withtopic-admixturemodels.anid48implementsthe   proximity-bias   assumption   thatwords   close-in-source   arealignedtowords   close-in-target   ,whichiseffectiveforimprovingwordalignmentaccuracies,especiallyforlinguisticallycloselanguage-pairs[8].following[8],tomodelword-to-wordtranslation,weintroducethemappingj   aj,whichassignsafrenchwordfjinpositionjtoanenglishwordeiinpositioni=ajdenotedaseaj.each(ordered)frenchwordfjisanobservation,anditisgeneratedbyanid48statede   nedas[eaj,aj],wherethealignmentindicatorajforpositionjisconsideredtohaveadependencyonthepreviousalignmentaj   1.thusa   rst-orderid48foranalignmentbetweene   e1:iandf   f1:jisde   nedas:p(f1:j|e1:i)=!a1:jj"j=1p(fj|eaj)p(aj|aj   1),(2)wherep(aj|aj   1)isthestatetransitionid203;jandiaresentencelengthsofthefrenchandenglishsentences,respectively.thetransitionmodelenforcestheproximity-bias.anadditionalpseudoword   null   isusedatthebeginningofenglishsentencesforid48tostartwith.theid48implementedingiza++[5]isusedasourbaseline,whichincludesre   nementssuchasspecialtreatmentofajumptoanullword.agraphicalmodelrepresentationforsuchanid48isillustratedinfigure1(a).ti,i!fm,3fm,2fm,1fjm,nmam,3am,2am,1ajm,nem,iim,nb=p(f|e)nm  zm,n  mfm,3fm,2fm,1bkfjm,nnmmam,3am,2am,1em,iim,nti,i!  kkkajm,n(a)id48forwordalignment(b)hm-bitamfigure1:thegraphicalmodelrepresentationsof(a)id48,and(b)hm-bitam,forparallelcorpora.circlesrepresentrandomvariables,hexagonsdenoteparameters,andobservedvariablesareshaded.2lda is modular, general, useful

    the posterior can be used in creative ways
    e.g., ir, collaborative    ltering, document similarity,

visualizing interdisciplinary documents

d. blei

topic models

dynamictopicmodelsways,andquantitativeresultsthatdemonstrategreaterpre-dictiveaccuracywhencomparedwithstatictopicmodels.2.dynamictopicmodelswhiletraditionaltimeseriesmodelinghasfocusedoncon-tinuousdata,topicmodelsaredesignedforcategoricaldata.ourapproachistousestatespacemodelsonthenat-uralparameterspaceoftheunderlyingtopicmultinomials,aswellasonthenaturalparametersforthelogisticnor-maldistributionsusedformodelingthedocument-speci   ctopicproportions.first,wereviewtheunderlyingstatisticalassumptionsofastatictopicmodel,suchaslatentdirichletallocation(lda)(bleietal.,2003).let  1:kbektopics,eachofwhichisadistributionovera   xedvocabulary.inastatictopicmodel,eachdocumentisassumeddrawnfromthefollowinggenerativeprocess:1.choosetopicproportions  fromadistributionoverthe(k   1)-simplex,suchasadirichlet.2.foreachword:(a)chooseatopicassignmentz   mult(  ).(b)chooseawordw   mult(  z).thisprocessimplicitlyassumesthatthedocumentsaredrawnexchangeablyfromthesamesetoftopics.formanycollections,however,theorderofthedocumentsre   ectsanevolvingsetoftopics.inadynamictopicmodel,wesupposethatthedataisdividedbytimeslice,forexamplebyyear.wemodelthedocumentsofeachslicewithak-componenttopicmodel,wherethetopicsassociatedwithslicetevolvefromthetopicsassociatedwithslicet   1.forak-componentmodelwithvterms,let  t,kdenotethev-vectorofnaturalparametersfortopickinslicet.theusualrepresentationofamultinomialdistributionisbyitsmeanparameterization.ifwedenotethemeanparam-eterofav-dimensionalmultinomialby  ,theithcom-ponentofthenaturalparameterisgivenbythemapping  i=log(  i/  v).intypicallanguagemodelingapplica-tions,dirichletdistributionsareusedtomodeluncertaintyaboutthedistributionsoverwords.however,thedirichletisnotamenabletosequentialmodeling.instead,wechainthenaturalparametersofeachtopic  t,kinastatespacemodelthatevolveswithgaussiannoise;thesimplestver-sionofsuchamodelis  t,k|  t   1,k   n(  t   1,k,  2i).(1)ourapproachisthustomodelsequencesofcompositionalrandomvariablesbychaininggaussiandistributionsinadynamicmodelandmappingtheemittedvaluestothesim-plex.thisisanextensionofthelogisticnormaldistribu-aaa      zzz            wwwnnnkfigure1.graphicalrepresentationofadynamictopicmodel(forthreetimeslices).eachtopic   snaturalparameters  t,kevolveovertime,togetherwiththemeanparameters  tofthelogisticnormaldistributionforthetopicproportions.tion(aitchison,1982)totime-seriessimplexdata(westandharrison,1997).inlda,thedocument-speci   ctopicproportions  aredrawnfromadirichletdistribution.inthedynamictopicmodel,weusealogisticnormalwithmean  toexpressuncertaintyoverproportions.thesequentialstructurebe-tweenmodelsisagaincapturedwithasimpledynamicmodel  t|  t   1   n(  t   1,  2i).(2)forsimplicity,wedonotmodelthedynamicsoftopiccor-relation,aswasdoneforstaticmodelsbybleiandlafferty(2006).bychainingtogethertopicsandtopicproportiondistribu-tions,wehavesequentiallytiedacollectionoftopicmod-els.thegenerativeprocessforslicetofasequentialcorpusisthusasfollows:1.drawtopics  t|  t   1   n(  t   1,  2i).2.draw  t|  t   1   n(  t   1,  2i).3.foreachdocument:(a)draw     n(  t,a2i)(b)foreachword:i.drawz   mult(  (  )).ii.drawwt,d,n   mult(  (  t,z)).notethat  mapsthemultinomialnaturalparameterstothemeanparameters,  (  k,t)w=exp(  k,t,w)pwexp(  k,t,w).thegraphicalmodelforthisgenerativeprocessisshowninfigure1.whenthehorizontalarrowsareremoved,break-ingthetimedynamics,thegraphicalmodelreducestoasetofindependenttopicmodels.withtimedynamics,thekthdctu    nd    di        figure5:modelingcommunitywithtopicssidertheconditionalid203p(c,u,z|  ),aword  as-sociatesthreevariables:community,userandtopic.ourinterpretationofthesemanticmeaningofp(c,u,z|  )istheid203thatword  isgeneratedbyuseruundertopicz,incommunityc.unfortunately,thisconditionalid203cannotbecom-puteddirectly.togetp(c,u,z|  ),wehave:p(c,u,z|  )=p(c,u,z,  )  c,u,zp(c,u,z,  )(3)considerthedenominatorineq.3,summingoverallc,uandzmakesthecomputationimpracticalintermsofef-   ciency.inaddition,asshownin[7],thesummingdoesn   tfactorize,whichmakesthemanipulationofdenominatordi   cult.inthefollowingsection,wewillshowhowanapproximateapproachofgibbssamplingwillprovideso-lutionstosuchproblems.afasteralgorithmenf-gibbssamplingwillalsobeintroduced.4.semanticcommunitydiscovery:thealgorithmsinthissection,we   rstintroducethegibbssamplingalgorithm.thenweaddresstheproblemofsemanticcom-munitydiscoverybyadaptinggibbssamplingframeworktoourmodels.finally,wecombinetwopowerfulideas:gibbssamplingandid178   lteringtoimprovee   ciencyandperformance,yieldinganewalgorithm:enf-gibbssampling.4.1gibbssamplinggibbssamplingisanalgorithmtoapproximatethejointdistributionofmultiplevariablesbydrawingasequenceofsamples.asaspecialcaseofthemetropolis-hastingsalgorithm[18],gibbssamplingisamarkovchainmontecarloalgorithmandusuallyapplieswhentheconditionalid203distributionofeachvariablecanbeevaluated.ratherthanexplicitlyparameterizingthedistributionsforvariables,gibbssamplingintegratesouttheparametersandestimatesthecorrespondingposteriorid203.gibbssamplingwas   rstintroducedtoestimatethetopic-wordmodelin[7].ingibbssampling,amarkovchainisformed,thetransitionbetweensuccessivestatesofwhichissimulatedbyrepeatedlydrawingatopicforeachob-servedwordfromitsconditionalid203onallothervariables.intheauthor-topicmodel,thealgorithmgoesoveralldocumentswordbyword.foreachword  i,thetopicziandtheauthorxiresponsibleforthiswordareassignedbasedontheposteriorid203conditionedonallothervariables:p(zi,xi|  i,z   i,x   i,w   i,ad).ziandxidenotethetopicandauthorassignedto  i,whilez   iandx   iareallotherassignmentsoftopicandauthorex-cludingcurrentinstance.w   irepresentsotherobservedwordsinthedocumentsetandadistheobservedauthorsetforthisdocument.akeyissueinusinggibbssamplingfordistributionapproximationistheevaluationofconditionalposteriorid203.inauthor-topicmodel,giventtopicsandvwords,p(zi,xi|  i,z   i,x   i,w   i,ad)isestimatedby:p(zi=j,xi=k|  i=m,z   i,x   i,w   i,ad)   (4)p(  i=m|xi=k)p(xi=k|zi=j)   (5)cwtmj+    m!cwtm!j+v  catkj+    j!catkj!+t  (6)wherem""=mandj""=j,  and  arepriorparametersforwordandtopicdirichlets,cwtmjrepresentsthenumberoftimesthatword  i=misassignedtotopiczi=j,catkjrepresentsthenumberoftimesthatauthorxi=kisassignedtotopicj.thetransformationfromeq.4toeq.5dropsthevari-ables,z   i,x   i,w   i,ad,becauseeachinstanceof  iisassumedindependentoftheotherwordsinamessage.4.2semanticcommunitydiscoverybyapplyingthegibbssampling,wecandiscoverthese-manticcommunitiesbyusingthecutmodels.considertheconditionalid203p(c,u,z|  ),wherethreevari-ablesinthemodel,community,user4andtopic,areasso-ciatedbyaword  .thesemanticmeaningofp(c,u,z|  )istheid203that  belongstouseruundertopicz,incommunityc.byestimationofp(c,u,z|  ),wecanla-belacommunitywithsemantictags(topics)inadditiontothea   liatedusers.theproblemofsemanticcommunitydiscoveryisthusreducedtotheestimationofp(c,u,z|  ).(1)/*initialization*/(2)foreachemaild(3)foreachword  iind(4)assign  itorandomcommunity,topicanduser;(5)/*userinthelistobservedfromd*/(6)/*markovchainconvergence*/(7)i   0;(8)i   desirednumberofiterations;(9)whilei<i(10)foreachemaild(11)foreach  i   d(12)estimatep(ci,ui,zi|  i),u     d;(13)(p,q,r)   argmax(p(cp,uq,zr|  i));(14)/*assigncommunityp,userq,topicrto  i*/(15)recordassignment  (cp,uq,zr,  i);(16)i++;figure6:gibbssamplingforcutmodels4notewedenoteuserwithuinourmodelsinsteadofxasinpreviouswork.177zwd!"#$tdnzwd!0"#$tdn%2"x&   1"((a)(b)figure1:graphicalmodelsfor(a)thestandardldatopicmodel(left)and(b)theproposedspecialwordstopicmodelwithabackgrounddistribution(swb)(right).aregeneratedbydrawingatopictfromthedocument-topicdistributionp(z|  d)andthendrawingawordwfromthetopic-worddistributionp(w|z=t,  t).asshowningrif   thsandsteyvers(2004)thetopicassignmentszforeachwordtokeninthecorpuscanbeef   cientlysampledviagibbssampling(aftermarginalizingover  and  ).pointestimatesforthe  and  distributionscanbecomputedconditionedonaparticularsample,andpredictivedistributionscanbeobtainedbyaveragingovermultiplesamples.wewillrefertotheproposedmodelasthespecialwordstopicmodelwithbackgrounddistribution(swb)(figure1(b)).swbhasasimilargeneralstructuretotheldamodel(figure1(a))butwithadditionalmachinerytohandlespecialwordsandbackgroundwords.inparticular,associatedwitheachwordtokenisalatentrandomvariablex,takingvaluex=0ifthewordwisgeneratedviathetopicroute,valuex=1ifthewordisgeneratedasaspecialword(forthatdocument)andvaluex=2ifthewordisgeneratedfromabackgrounddistributionspeci   cforthecorpus.thevariablexactsasaswitch:ifx=0,thepreviouslydescribedstandardtopicmechanismisusedtogeneratetheword,whereasifx=1orx=2,wordsaresampledfromadocument-speci   cmultinomial  oracorpusspeci   cmultinomial   (withsymmetricdirichletpriorsparametrizedby  1and  2)respectively.xissampledfromadocument-speci   cmultinomial  ,whichinturnhasasymmetricdirichletprior,  .onecouldalsouseahierarchicalbayesianapproachtointroduceanotherlevelofuncertaintyaboutthedirichletpriors(e.g.,seeblei,ng,andjordan,2003)   wehavenotinvestigatedthisoption,primarilyforcomputationalreasons.inallourexperiments,weset  =0.1,  0=  2=0.01,  1=0.0001and  =0.3   allweaksymmetricpriors.theconditionalid203ofawordwgivenadocumentdcanbewrittenas:p(w|d)=p(x=0|d)t!t=1p(w|z=t)p(z=t|d)+p(x=1|d)p!(w|d)+p(x=2|d)p!!(w)wherep!(w|d)isthespecialworddistributionfordocumentd,andp!!(w)isthebackgroundworddistributionforthecorpus.notethatwhencomparedtothestandardtopicmodeltheswbmodelcanexplainwordsinthreedifferentways,viatopics,viaaspecialworddistribution,orviaaback-groundworddistribution.giventhegraphicalmodelabove,itisrelativelystraightforwardtoderivegibbssamplingequationsthatallowjointsamplingoftheziandxilatentvariablesforeachwordtokenwi,forxi=0:p(xi=0,zi=t|w,x   i,z   i,  ,  0,  )   nd0,   i+  nd,   i+3    ctdtd,   i+  "t!ctdt!d,   i+t    cwtwt,   i+  0"w!cwtw!t,   i+w  0andforxi=1:p(xi=1|w,x   i,z   i,  1,  )   nd1,   i+  nd,   i+3    cwdwd,   i+  1"w!cwdw!d,   i+w  1mccallum,wang,&corrada-emmanuel!"zwid44(lda)[blei, ng, jordan, 2003]ndxzwauthor-topic model(at)[rosen-zvi, grif   ths, steyvers, smyth 2004]nd"#!$ta#$txzwauthor-recipient-topic model(art)[this paper]nd"#!$ta,azwauthor model(multi-label mixture model)[mccallum 1999]nd#$aadadrdadddddfigure1:threerelatedmodels,andtheartmodel.inallmodels,eachobservedword,w,isgeneratedfromamultinomialworddistribution,  z,speci   ctoaparticulartopic/author,z,howevertopicsareselecteddi   erentlyineachofthemodels.inlda,thetopicissampledfromaper-documenttopicdistribution,  ,whichinturnissampledfromadirichletovertopics.intheauthormodel,thereisonetopicassociatedwitheachauthor(orcategory),andauthorsaresampleduniformly.intheauthor-topicmodel,thetopicissampledfromaper-authormultinomialdistribution,  ,andauthorsaresampleduniformlyfromtheobservedlistofthedocument   sauthors.intheauthor-recipient-topicmodel,thereisaseparatetopic-distributionforeachauthor-recipientpair,andtheselectionoftopic-distributionisdeterminedfromtheobservedauthor,andbyuniformlysam-plingarecipientfromthesetofrecipientsforthedocument.itsgenerativeprocessforeachdocumentd,asetofauthors,ad,isobserved.togenerateeachword,anauthorxischosenuniformlyfromthisset,thenatopiczisselectedfromatopicdistribution  xthatisspeci   ctotheauthor,andthenawordwisgeneratedfromatopic-speci   cmultinomialdistribution  z.however,asdescribedpreviously,noneofthesemodelsissuitableformodelingmessagedata.anemailmessagehasonesenderandingeneralmorethanonerecipients.wecouldtreatboththesenderandtherecipientsas   authors   ofthemessage,andthenemploytheatmodel,butthisdoesnotdistinguishtheauthorandtherecipientsofthemessage,whichisundesirableinmanyreal-worldsituations.amanagermaysendemailtoasecretaryandviceversa,butthenatureoftherequestsandlanguageusedmaybequitedi   erent.evenmoredramatically,considerthelargequantityofjunkemailthatwereceive;modelingthetopicsofthesemessagesasundistinguishedfromthetopicswewriteaboutasauthorswouldbeextremelyconfoundingandundesirablesincetheydonotre   ectourexpertiseorroles.alternativelywecouldstillemploytheatmodelbyignoringtherecipientinformationofemailandtreatingeachemaildocumentasifitonlyhasoneauthor.however,inthiscase(whichissimilartotheldamodel)wearelosingallinformationabouttherecipients,andtheconnectionsbetweenpeopleimpliedbythesender-recipientrelationships.252networkneuralnetworksoutput...imageimagesobjectobjects...supportvectorid166...kernelinwithforon...usedtrainedobtaineddescribed...0.50.40.10.70.90.80.2neuralnetworktrainedwithid166imagesimageoutputnetworkforused imageskernelwithobtaineddescribedwithobjectsssss1423wwww1423zzzz!1423(a)(b)figure1:thecompositemodel.(a)graphicalmodel.(b)generatingphrases.  (z),eachclassc!=1isassociatedwithadistributionoverwords  (c),eachdocumentdhasadistributionovertopics  (d),andtransitionsbetweenclassesci   1andcifollowadistribution  (si   1).adocumentisgeneratedviathefollowingprocedure:1.sample  (d)fromadirichlet(  )prior2.foreachwordwiindocumentd(a)drawzifrom  (d)(b)drawcifrom  (ci   1)(c)ifci=1,thendrawwifrom  (zi),elsedrawwifrom  (ci)figure1(b)providesanintuitiverepresentationofhowphrasesaregeneratedbythecom-positemodel.the   gureshowsathreeclassid48.twoclassesaresimplemultinomialdistributionsoverwords.thethirdisatopicmodel,containingthreetopics.transitionsbetweenclassesareshownwitharrows,annotatedwithtransitionprobabilities.thetop-icsinthesemanticclassalsohaveprobabilities,usedtochooseatopicwhentheid48transitionstothesemanticclass.phrasesaregeneratedbyfollowingapaththroughthemodel,choosingawordfromthedistributionassociatedwitheachsyntacticclass,andatopicfollowedbyawordfromthedistributionassociatedwiththattopicfortheseman-ticclass.sentenceswiththesamesyntaxbutdifferentcontentwouldbegeneratedifthetopicdistributionweredifferent.thegenerativemodelthusactslikeitisplayingagameof   madlibs   :thesemanticcomponentprovidesalistoftopicalwords(showninblack)whichareslottedintotemplatesgeneratedbythesyntacticcomponent(showningray).2.2id136theemalgorithmcanbeappliedtothegraphicalmodelshowninfigure1,treatingthedocumentdistributions  ,thetopicsandclasses  ,andthetransitionprobabilities  asparameters.however,emproducespoorresultswithtopicmodels,whichhavemanypa-rametersandmanylocalmaxima.consequently,recentworkhasfocusedonapproximateid136algorithms[6,8].wewillusemarkovchainmontecarlo(mcmc;see[9])toperformfullbayesianid136inthismodel,samplingfromaposteriordistributionoverassignmentsofwordstoclassesandtopics.weassumethatthedocument-speci   cdistributionsovertopics,  ,aredrawnfromadirichlet(  )distribution,thetopicdistributions  (z)aredrawnfromadirichlet(  )dis-tribution,therowsofthetransitionmatrixfortheid48aredrawnfromadirichlet(  )distribution,theclassdistributions  (c)aredrawnfromadirichlet(  )distribution,andalldirichletdistributionsaresymmetric.weusegibbssamplingtodrawiterativelyatopicassignmentziandclassassignmentciforeachwordwiinthecorpus(see[8,9]).giventhewordsw,theclassassignmentsc,theothertopicassignmentsz   i,andthehyperparameters,eachziisdrawnfrom:p(zi|z   i,c,w)   p(zi|z   i)p(wi|z,c,w   i)   !n(di)zi+  (n(di)zi+  )n(zi)wi+  n(zi)+w  ci!=1ci=1constraintsofwordalignment,i.e.,words   close-in-source   areusuallyalignedtowords   close-in-target   ,underdocument-speci   ctopicalassignment.toincorporatesuchconstituents,weintegratethestrengthsofbothid48andbitam,andproposeahiddenmarkovbilingualtopic-admixturemodel,orhm-bitam,forwordalignmenttoleveragebothlocalityconstraintsandtopicalcontextunderlyingparalleldocument-pairs.inthehm-bitamframework,onecanestimatetopic-speci   cword-to-wordtranslationlexicons(lexicalmappings),aswellasthemonolingualtopic-speci   cword-frequenciesforbothlanguages,basedonparalleldocument-pairs.theresultingmodeloffersaprincipledwayofinferringoptimaltranslationfromagivensourcelanguageinacontext-dependentfashion.wereportanextensiveempiricalanalysisofhm-bitam,incomparisonwithrelatedmethods.weshowourmodel   sef-fectivenessontheword-alignmenttask;wealsodemonstratetwoapplicationaspectswhichwereuntouchedin[10]:theutilityofhm-bitamforbilingualtopicexploration,anditsapplicationforimprovingtranslationqualities.2revisitid48forsmtansmtsystemcanbeformulatedasanoisy-channelmodel[2]:e   =argmaxep(e|f)=argmaxep(f|e)p(e),(1)whereatranslationcorrespondstosearchingforthetargetsentencee   whichexplainsthesourcesentencefbest.thekeycomponentisp(f|e),thetranslationmodel;p(e)ismonolinguallanguagemodel.inthispaper,wegeneralizep(f|e)withtopic-admixturemodels.anid48implementsthe   proximity-bias   assumption   thatwords   close-in-source   arealignedtowords   close-in-target   ,whichiseffectiveforimprovingwordalignmentaccuracies,especiallyforlinguisticallycloselanguage-pairs[8].following[8],tomodelword-to-wordtranslation,weintroducethemappingj   aj,whichassignsafrenchwordfjinpositionjtoanenglishwordeiinpositioni=ajdenotedaseaj.each(ordered)frenchwordfjisanobservation,anditisgeneratedbyanid48statede   nedas[eaj,aj],wherethealignmentindicatorajforpositionjisconsideredtohaveadependencyonthepreviousalignmentaj   1.thusa   rst-orderid48foranalignmentbetweene   e1:iandf   f1:jisde   nedas:p(f1:j|e1:i)=!a1:jj"j=1p(fj|eaj)p(aj|aj   1),(2)wherep(aj|aj   1)isthestatetransitionid203;jandiaresentencelengthsofthefrenchandenglishsentences,respectively.thetransitionmodelenforcestheproximity-bias.anadditionalpseudoword   null   isusedatthebeginningofenglishsentencesforid48tostartwith.theid48implementedingiza++[5]isusedasourbaseline,whichincludesre   nementssuchasspecialtreatmentofajumptoanullword.agraphicalmodelrepresentationforsuchanid48isillustratedinfigure1(a).ti,i!fm,3fm,2fm,1fjm,nmam,3am,2am,1ajm,nem,iim,nb=p(f|e)nm  zm,n  mfm,3fm,2fm,1bkfjm,nnmmam,3am,2am,1em,iim,nti,i!  kkkajm,n(a)id48forwordalignment(b)hm-bitamfigure1:thegraphicalmodelrepresentationsof(a)id48,and(b)hm-bitam,forparallelcorpora.circlesrepresentrandomvariables,hexagonsdenoteparameters,andobservedvariablesareshaded.2approximate posterior id136

d. blei

topic models

posterior distribution for lda

    for now, assume the topics   1:k are    xed.

the per-document posterior is

p(   |   )(cid:81)n
(cid:82)   p(   |   )(cid:81)n

n=1(cid:80)k

n=1 p(zn |   )p(wn | zn,   1:k )

z=1 p(zn |   )p(wn | zn,   1:k )

    this is intractable to compute
    it is a    multiple hypergeometric function    (see dickey, 1983)
    can be seen as sum of n k (tractable) dirichlet integral terms

d. blei

topic models

posterior distribution for lda

we appeal to approximate posterior id136 of the posterior,

p(   |   )(cid:81)n
(cid:82)   p(   |   )(cid:81)n

n=1(cid:80)k

n=1 p(zn |   )p(wn | zn,   1:k )

z=1 p(zn |   )p(wn | zn,   1:k )

    id150
    variational methods
    particle    ltering

d. blei

topic models

  dzd,nwd,nndk  k    id150

    de   ne a markov chain whose stationary distribution is the

posterior of interest

    collect independent samples from that stationary distribution;

approximate the posterior with them

    in id150, the space of the mc is the space of possible

con   gurations of the hidden variables.

    the chain is run by iteratively sampling from the conditional
distribution of each hidden variable given observations and the
current state of the other hidden variables

    once a chain has    burned in,    collect samples at a lag to

approximate the posterior.

d. blei

topic models

id150 for lda

de   ne n(z1:n ) to be the counts vector. a simple gibbs sampler is

where

   | w1:n , z1:n     dir(   + n(z1:n ))
zi | z   i , w1:n     mult(  (z   i , wi ))

  (z   i , wi )     (   + n(z1:n ))p(wi |   1:k )

d. blei

topic models

  dzd,nwd,nndk  k    id150 for lda

    the topic proportions    can be integrated out.
    a collapsed gibbs sampler draws from

p(zi | z   i , w1:n )     p(wi |   1:k )(cid:81)k

k=1   (nk (z   i )),

where nk (z   i ) is the number of times we   ve seen topic k in the
collection of topic assignments z   i .

    integrating out variables leads to a faster mixing chain.

d. blei

topic models

  dzd,nwd,nndk  k    variational id136 (in general)

    variational methods are a deterministic alternative to mcmc.
    let x1:n be observations and z1:m be latent variables
    our goal is to compute the posterior distribution
p(z1:m , x1:n )

p(z1:m | x1:n ) =

(cid:82) p(z1:m , x1:n )dz1:m

    for many interesting distributions, the marginal likelihood of the

observations is di   cult to e   ciently compute

d. blei

topic models

variational id136

    use jensen   s inequality to bound the log prob of the observations:

log p(x1:n ) = log(cid:90) p(z1:m , x1:n )dz1:m

= log(cid:90) p(z1:m , x1:n )
    eq   [log p(z1:m , x1:n )]     eq   [log q  (z1:m )]

q  (z1:m )
q  (z1:m )

dz1:m

    we have introduced a distribution of the latent variables with free

variational parameters   .

    we optimize those parameters to tighten this bound.
    this is the same as    nding the member of the family q   that is

closest in kl divergence to p(z1:m | x1:n ).

d. blei

topic models

mean-   eld variational id136

    complexity of optimization is determined by the factorization of q  
    in mean    eld variational id136 we choose q   to be fully factored

q  (z1:m ) =

q  m(zm).

m(cid:89)m=1

    the latent variables are independent.

    each is governed by its own variational parameter   m.

    in the true posterior they can exhibit dependence

(often, this is what makes exact id136 di   cult).

d. blei

topic models

mfvi and conditional exponential families

    suppose the distribution of each latent variable conditional on the
observations and other latent variables is in the exponential family:

p(zm | z   m, x) = hm(zm) exp{gm(z   m, x)t zm     am(gi (z   m, x))}

    assume q   is fully factorized, and each factor is in the same

exponential family:

q  m(zm) = hm(zm) exp{  t

m zm     am(  m)}

d. blei

topic models

mfvi and conditional exponential families

    variational id136 is the following coordinate ascent algorithm

  m = eq   [gm(z   m, x)]

    notice the relationship to id150.
    (you will hear much more about this from minka and winn.)

d. blei

topic models

variational id136

    alternative to mcmc; replace sampling with optimization.
    deterministic approximation to posterior distribution.
    uses established optimization methods

(block coordinate ascent; newton-raphson; interior-point).

    faster, more scalable than mcmc for large problems.
    biased, whereas mcmc is not.
    emerging as a useful framework for fully bayesian and empirical

bayesian id136 problems.

d. blei

topic models

variational id136 for lda

    the mean    eld variational distribution is

q(  , z1:n |   ,   1:n ) = q(   |   )(cid:81)n

n=1 q(zn |   )

    this is a family of distributions over the latent variables, where all
variables are independent and governed by their own parameters.

    in the true posterior, the latent variables are not independent.

d. blei

topic models

  dzd,nwd,nndk  k    variational id136 for lda

the variational paramters are:

  

dirichlet parameters

  1:n multinomial parameters for k-dim variables

there is a separate variational dirichlet distribution for each document;
there is a separate multinomial distribution for each word in each docu-
ment. (contrast this to the model.)

d. blei

topic models

  dzd,nwd,nndk  k    variational id136 for lda

coordinate ascent on the variational objective,

where

n=1   n

   =    +(cid:80)n
  n     exp{e[log   ] + log   .,wn},
e[log   i ] =   (  i )       ((cid:80)j   j ).

d. blei

topic models

  dzd,nwd,nndk  k    estimating the topics

maximum likelihood: expectation-maximization

    e-step: use variational or mcmc to approximate the per-document

posterior

    m-step: find id113 of   1:k from expected counts

bayesian topics

    put a dirichlet prior on the topics (usually exchangeable)

note/warning: this controls the sparsity of the topics

    collapsed id150 is still possible   we only need to keep

track of the topic assignments.

    variational: use a variational dirichlet for each topic

d. blei

topic models

id136 comparison

    conventional wisdom says that:

    gibbs is easiest to implement
    variational can be faster, especially when dealing with

nonconjugate priors (more on that later)

    there are other options:

    collapsed variational id136
    parallelized id136 for large corpora
    particle    lters for on-line id136

    an icml paper examining these issues is asuncion et al. (2009).

d. blei

topic models

jonathan chang   s r implementation

result <-

lda.collapsed.gibbs.sampler(cora.documents,

k, ## num clusters
cora.vocab, ## vocabulary
100, ## num iterations
0.1, ## topic dirichlet
0.1) ## prop dirichlet

see http://www.pleasescoopme.com/

d. blei

topic models

jonathan chang   s r implementation

d. blei

topic models

proportiontopicdecision.learning.tree.trees.classificationnetwork.time.networks.algorithm.dataplanning.visual.model.memory.systeid113arning.networks.neural.system.reinforcementdesign.logic.search.learning.systemslearning.search.crossover.algorithm.complexitymodels.networks.bayesian.data.hiddenbelief.model.theory.distribution.markovgenetic.search.optimization.evolutionary.functionresearch.reasoning.grant.science.supporteddecision.learning.tree.trees.classificationnetwork.time.networks.algorithm.dataplanning.visual.model.memory.systeid113arning.networks.neural.system.reinforcementdesign.logic.search.learning.systemslearning.search.crossover.algorithm.complexitymodels.networks.bayesian.data.hiddenbelief.model.theory.distribution.markovgenetic.search.optimization.evolutionary.functionresearch.reasoning.grant.science.supported160.00.20.40.60.81.0270.00.20.40.60.81.0380.00.20.40.60.81.0490.00.20.40.60.81.05100.00.20.40.60.81.0document12345678910supervised and relational topic models

d. blei

topic models

supervised topic models

    but lda is an unsupervised model. how can we build a topic model

that is good at the task we care about?

    many data are paired with response variables.
    user reviews paired with a number of stars
    web pages paired with a number of    diggs   
    documents paired with links to other documents
    images paired with a category

    supervised topic models are topic models of documents and

responses,    t to    nd topics predictive of the response.

d. blei

topic models

supervised lda

1 draw topic proportions    |        dir(  ).
2 for each word

3 draw response variable y | z1:n ,   ,   2     n(cid:0)  (cid:62)  z,   2(cid:1), where

    draw topic assignment zn |        mult(  ).
    draw word wn | zn,   1:k     mult(  zn).
  z = (1/n)(cid:80)n

n=1 zn.

topic models

d. blei

  dzd,nwd,nndk  k  yd  ,  2supervised lda

    the response variable y is drawn after the document because it

depends on z1:n , an assumption of partial exchangeability.

    consequently, y is necessarily conditioned on the words.
    in a sense, this blends generative and discriminative modeling.

d. blei

topic models

  dzd,nwd,nndk  k  yd  ,  2supervised lda

    given a set of document-response pairs,    t the model parameters by

maximum likelihood.

    given a new document, compute a prediction of its response.
    both of these activities hinge on variational id136.

d. blei

topic models

  dzd,nwd,nndk  k  yd  ,  2variational id136 in slda

    our goal is to compute the posterior distribution

p(  , z1:n | w1:n ) =

p(  , z1:n , w1:n )

,(cid:82)   p(  , z1:n , w1:n )

(cid:80)z1:n

    we approximate by minimizing the kl divergence to a simpler

family of distributions,

q      = arg min
q   q

d. blei

kl(q||p)

topic models

  dzd,nwd,nndk  k  yd  ,  2variational id136 in slda

equivalently, maximize the jensen   s bound

log p(w1:n , y )    

e[log p(   |   )] +(cid:80)n
+e[log p(y | z1:n ,   ,   2)] + h(q)

n=1 e[log p(zn |   )] +(cid:80)n

n=1 e[log p(wn | zn,   1:k )]

d. blei

topic models

  dzd,nwd,nndk  k  yd  ,  2variational id136 in slda

the distinguishing term is

e[log p(y | z1:n )] =    
we use the fully-factorized variational distribution

log(cid:0)2    2(cid:1)    

1
2

y 2     2y   (cid:62)e(cid:2)   z(cid:3) +   (cid:62)e(cid:2)   z   z(cid:62)(cid:3)   

2  2

q(  , z1:n |   ,   1:n ) = q(   |   )(cid:81)n

n=1 q(zn |   n),

d. blei

topic models

  dzd,nwd,nndk  k  yd  ,  2variational id136 in slda

    the expectations are

  n

1
n

n(cid:88)n=1
e(cid:2)   z(cid:3) =      :=
n 2(cid:16)(cid:80)n
e(cid:2)   z   z(cid:62)(cid:3) =
n=1(cid:80)m(cid:54)=n   n  (cid:62)m +(cid:80)n

1

    leads to an easy coordinate ascent algorithm.

n=1 diag{  n}(cid:17) .

d. blei

topic models

  dzd,nwd,nndk  k  yd  ,  2id113

    the m-step is an id113 under expected su   cient statistics.
    de   ne

    y = y1:d is the response vector
    a is the d    k matrix whose rows are   z(cid:62)d .

    id113 of the coe   cients solve the expected normal equations

e(cid:2)a(cid:62)a(cid:3)   = e[a](cid:62)y

    the id113 of the variance is

   

    new    (cid:16)e(cid:2)a(cid:62)a(cid:3)(cid:17)   1

e[a](cid:62)y

    2

new     (1/d){y(cid:62)y     y(cid:62)e[a](cid:16)e(cid:2)a(cid:62)a(cid:3)(cid:17)   1

e[a](cid:62)y}

d. blei

topic models

prediction

    we have    t slda parameters to a corpus, using variational em.
    we have a new document w1:n with unknown response value.
    first, run variational id136 in the unsupervised lda model, to

obtain    and   1:n for the new document.
(lda     integrating unobserved y out of slda.)

    predict y using slda expected value:

e(cid:2)y | w1:n ,   ,   1:k ,   ,   2(cid:3)       (cid:62)eq(cid:2)   z(cid:3) =   (cid:62)     .

d. blei

topic models

example: movie reviews

    10-topic slda model on movie reviews (pang and lee, 2005).
    response: number of stars associated with each review
    each component of coe   cient vector    is associated with a topic.

d. blei

topic models

bothmotionsimpleperfectfascinatingpowercomplexhowevercinematographyscreenplayperformancespictureseffectivepicturehistheircharactermanywhileperformancebetween   30   20   1001020                              morehasthan   lmsdirectorwillcharactersonefromtherewhichwhomuchwhatawfulfeaturingroutinedryofferedcharlieparisnotaboutmovieallwouldtheyitshavelikeyouwasjustsomeoutbadguyswatchableitsnotonemovieleastproblemunfortunatelysupposedworse   atdullpredictive r2

(slda is red.)

d. blei

topic models

llllllllllllllllllll51015202530354045500.00.10.20.30.40.5number of topicspredictive r2held out likelihood

(slda is red.)

d. blei

topic models

llllllllllllllllllll5101520253035404550   6.42   6.41   6.40   6.39   6.38   6.37number of topicsper   word held out log likelihoodpredictive r2 on digg

(slda is red.)

d. blei

topic models

llllllllllllll241020300.000.020.040.060.080.100.12number of topicspredictive r2held out likelihood on digg

(slda is red.)

d. blei

topic models

llllllllllllll24102030   8.6   8.5   8.4   8.3   8.2   8.1   8.0number of topicsper   word held out log likelihooddiverse response types with glms

    want to work with response variables that don   t live in the reals.

    binary / multiclass classi   cation
    count data
    waiting time

    model the response response with a generalized linear model

p(y |   ,   ) = h(y ,   ) exp(cid:26)   y     a(  )

  

(cid:27) ,

where    =   (cid:62)  z.

    complicates id136, but allows for    exible modeling.

d. blei

topic models

example: multi-class classi   cation

slda for image classi   cation (with chong wang, cvpr 2009)

d. blei

topic models

756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863cvpr#318cvpr#318cvpr2009submission#318.confidentialreviewcopy.donotdistribute. highway  car, sign, road      coast (highway)  car, sand beach, tree   inside city  buildings, car, sidewalk     street (inside city)  window, tree, building occluded   tall building  trees, buildings occluded, window    inside city (tall building)  tree, car, sidewalk   street   tree, car, sidewalk     highway (street)  car, window, tree   forest  tree trunk, trees,  ground grass     mountain (forest)  snowy mountain, tree trunk   coast  sand beach, cloud    open country (coast)  sea water, buildings   mountain  snowy mountain,  sea water, field    highway (mountain)  tree, snowy mountain  open country  cars, field,  sand beach    coast (open country)  tree, field, sea water incorrect classification (correct class) with predicted annotations  correct classification with predicted annotations figure4.exampleresultsfromthelabelmedataset.foreachclass,leftsidecontainsexampleswithcorrectclassi   cationandpredictedannotations,whilerightsidecontainswrongones(theclasslabelinthebracketistherightone)withthepredictedannotations.theitalicwordsindicatetheclasslabel,whilethenormalwordsareassociatedpredictedannotations.[28]j.vogelandb.schiele.asemantictypicalitymeasurefornaturalscenecategorization.indagm-symposium,2004.5[29]y.wangands.gong.conditionalrandom   eldfornaturalscenecategorization.inbmvc,2007.5[30]z.-h.zhouandm.-l.zhang.multi-instancemulti-labellearningwithapplicationtosceneclassi   cation.innips,2006.58756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863cvpr#318cvpr#318cvpr2009submission#318.confidentialreviewcopy.donotdistribute. highway  car, sign, road      coast (highway)  car, sand beach, tree   inside city  buildings, car, sidewalk     street (inside city)  window, tree, building occluded   tall building  trees, buildings occluded, window    inside city (tall building)  tree, car, sidewalk   street   tree, car, sidewalk     highway (street)  car, window, tree   forest  tree trunk, trees,  ground grass     mountain (forest)  snowy mountain, tree trunk   coast  sand beach, cloud    open country (coast)  sea water, buildings   mountain  snowy mountain,  sea water, field    highway (mountain)  tree, snowy mountain  open country  cars, field,  sand beach    coast (open country)  tree, field, sea water incorrect classification (correct class) with predicted annotations  correct classification with predicted annotations figure4.exampleresultsfromthelabelmedataset.foreachclass,leftsidecontainsexampleswithcorrectclassi   cationandpredictedannotations,whilerightsidecontainswrongones(theclasslabelinthebracketistherightone)withthepredictedannotations.theitalicwordsindicatetheclasslabel,whilethenormalwordsareassociatedpredictedannotations.[28]j.vogelandb.schiele.asemantictypicalitymeasurefornaturalscenecategorization.indagm-symposium,2004.5[29]y.wangands.gong.conditionalrandom   eldfornaturalscenecategorization.inbmvc,2007.5[30]z.-h.zhouandm.-l.zhang.multi-instancemulti-labellearningwithapplicationtosceneclassi   cation.innips,2006.58756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863cvpr#318cvpr#318cvpr2009submission#318.confidentialreviewcopy.donotdistribute. highway  car, sign, road      coast (highway)  car, sand beach, tree   inside city  buildings, car, sidewalk     street (inside city)  window, tree, building occluded   tall building  trees, buildings occluded, window    inside city (tall building)  tree, car, sidewalk   street   tree, car, sidewalk     highway (street)  car, window, tree   forest  tree trunk, trees,  ground grass     mountain (forest)  snowy mountain, tree trunk   coast  sand beach, cloud    open country (coast)  sea water, buildings   mountain  snowy mountain,  sea water, field    highway (mountain)  tree, snowy mountain  open country  cars, field,  sand beach    coast (open country)  tree, field, sea water incorrect classification (correct class) with predicted annotations  correct classification with predicted annotations figure4.exampleresultsfromthelabelmedataset.foreachclass,leftsidecontainsexampleswithcorrectclassi   cationandpredictedannotations,whilerightsidecontainswrongones(theclasslabelinthebracketistherightone)withthepredictedannotations.theitalicwordsindicatetheclasslabel,whilethenormalwordsareassociatedpredictedannotations.[28]j.vogelandb.schiele.asemantictypicalitymeasurefornaturalscenecategorization.indagm-symposium,2004.5[29]y.wangands.gong.conditionalrandom   eldfornaturalscenecategorization.inbmvc,2007.5[30]z.-h.zhouandm.-l.zhang.multi-instancemulti-labellearningwithapplicationtosceneclassi   cation.innips,2006.58756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863cvpr#318cvpr#318cvpr2009submission#318.confidentialreviewcopy.donotdistribute. highway  car, sign, road      coast (highway)  car, sand beach, tree   inside city  buildings, car, sidewalk     street (inside city)  window, tree, building occluded   tall building  trees, buildings occluded, window    inside city (tall building)  tree, car, sidewalk   street   tree, car, sidewalk     highway (street)  car, window, tree   forest  tree trunk, trees,  ground grass     mountain (forest)  snowy mountain, tree trunk   coast  sand beach, cloud    open country (coast)  sea water, buildings   mountain  snowy mountain,  sea water, field    highway (mountain)  tree, snowy mountain  open country  cars, field,  sand beach    coast (open country)  tree, field, sea water incorrect classification (correct class) with predicted annotations  correct classification with predicted annotations figure4.exampleresultsfromthelabelmedataset.foreachclass,leftsidecontainsexampleswithcorrectclassi   cationandpredictedannotations,whilerightsidecontainswrongones(theclasslabelinthebracketistherightone)withthepredictedannotations.theitalicwordsindicatetheclasslabel,whilethenormalwordsareassociatedpredictedannotations.[28]j.vogelandb.schiele.asemantictypicalitymeasurefornaturalscenecategorization.indagm-symposium,2004.5[29]y.wangands.gong.conditionalrandom   eldfornaturalscenecategorization.inbmvc,2007.5[30]z.-h.zhouandm.-l.zhang.multi-instancemulti-labellearningwithapplicationtosceneclassi   cation.innips,2006.58540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647cvpr#318cvpr#318cvpr2009submission#318.confidentialreviewcopy.donotdistribute.204060801001200.640.660.680.70.720.740.760.78topicsaverage accuracyimage classification on the labelme dataset  multi!class slda with annotationsmulti!class sldafei!fei and perona, 2005bosch et al., 2006204060801001200.560.580.60.620.640.66topicsaverage accuracyimage classification on the uiuc!sport datasetfigure2.comparisonsofaverageaccuracyoverallclassesbasedon5randomtrain/testsubsets.multi-classsldawithannotationsandmulti-classslda(redcurvesincolor)arebothourmodels.left.accuracyasafunctionofthenumberoftopicsonthelabelmedataset.right.accuracyasafunctionofthenumberoftopicsontheuiuc-sportdataset.3.multi-classslda:thisisthemulti-classsldamodel,describedinthispaper.4.multi-classsldawithannotations:thisismulti-classsldawithannotations,describedinthispaper.notealltestingisperformedonunlabeledandunannotatedimages.theresultsareillustratedinthegraphsoffigure2andintheconfusionmatricesoffigure3.2ourmodels   multi-classsldaandmulti-classsldawithannotations   per-formbetterthantheotherapproaches.theyreducetheerroroffei-feiandperona,2005byatleast10%onbothdatasets,andevenmoreforboschetal.,2006.thisdemon-stratesthatmulti-classsldaisabetterclassi   er,andthatjointmodelingdoesnotnegativelyaffectclassi   cationac-curacywhenannotationinformationisavailable.infact,itusuallyincreasestheaccuracy.observethatthemodelof[5],unsupervisedldacom-binedwithknn,givestheworstperformanceofthesemethods.thishighlightsthedifferencebetween   ndingtopicsthatarepredictive,asourmodelsdo,and   ndingtopicsinanunsupervisedway.theaccuracyofunsuper-visedldamightbeincreasedbyusingsomeoftheothervisualfeaturessuggestedby[5].here,werestrictourselvestosiftfeaturesinordertocomparemodels,ratherthanfeaturesets.asthenumberoftopicsincreases,themulti-classsldamodels(withandwithoutannotation)donotover   tuntilaround100topics,whilefei-feiandperona,2005beginstoover   tat40topics.thissuggeststhatmulti-classslda,whichcombinesaspectsofbothgenerativeanddiscrimina-tiveclassi   cation,canhandlemorelatentfeaturesthana2otherthanthetopicmodelslisted,wealsotestedanid166-basedap-proachusingsiftimagefeatures.theid166yieldedmuchworseperfor-mancethanthetopicmodels(47%forthelabelmedata,and20%fortheuiuc-sportdata).thesearenotmarkedontheplots.purelygenerativeapproach.ononehand,alargenumberoftopicsincreasesthepossibilityofover   tting;ontheotherhand,itprovidesmorelatentfeaturesforbuildingtheclas-si   er.imageannotation.inthecaseofmulti-classsldawithannotations,wecanusethesametrainedmodelforimageannotation.weemphasizethatourmodelsaredesignedforsimultaneousclassi   cationandannotation.forimagean-notation,wecomparefollowingtwomethods,1.bleiandjordan,2003:thisisthecorr-ldamodelfrom[2],trainedonannotatedimages.2.multi-classsldawithannotations:thisisexactlythesamemodeltrainedforimageclassi   cationinthepre-vioussection.intestingannotation,weobserveonlyimages.tomeasureimageannotationperformance,weuseanevaluationmeasurefrominformationretrieval.speci   -cally,weexaminethetop-nf-measure3,denotedasf-measure@n,wherewesetn=5.we   ndthatmulti-classsldawithannotationsperformsslightlybetterthancorr-ldaoverallthenumbersoftopicstested(about1%relativeimprovement).forexample,consideringmodelswith100topics,thelabelmef-measuresare38.2%(corr-lda)and38.7%(multi-classsldawithannotations);onuiuc-sport,theyare34.7%(corr-lda)and35.0%(multi-classsldawithannotations).theseresultsdemonstratethatourmodelscanperformclassi   cationandannotationwiththesamelatentspace.withasingletrainedmodel,we   ndtheannotationper-formancethatiscompetitivewiththestate-of-the-art,andclassi   cationperformancethatissuperior.3f-measureisde   nedas2   precision   recall/(precision+recall).6# of componentssupervised topic models

    slda enables model-based regression where the predictor    variable   

is a text document.

    it can easily be used wherever lda is used in an unsupervised

fashion (e.g., images, genes, music).

    slda is a supervised dimension-reduction technique, whereas lda

performs unsupervised dimension reduction.

    lda + regression compared to slda is like principal components

regression compared to partial least squares.

d. blei

topic models

relational topic models

    many data sets contain connected observations.
    for example:

    id191 of documents
    hyperlinked networks of web-pages.
    friend-connected social network pro   les

d. blei

topic models

52478430248775288112321222299135418541855896359224381364791096401196861201959153914717217796591121921489885178378286208156923431270218129022322723616172541176256634264196321951377303426209131316425348013353445851244229126171627229012753751027396167824472583106169212079601238201216442042381418179212846515241165219715682593169854768321371637255720336321020436442449474649263623005395416031047722660806112111388318371335902964966981167311401481143212531590106099299410011010165115781039104013441345134813551420108914831188167416802272128515921234130413171426169514651743194422592213we address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts...irrelevant features and the subset selection problemin many domains, an appropriate inductive bias is the min-features bias, which prefers consistent hypotheses definable over as few features as possible...learning with many irrelevant featuresin this introduction, we define the term bias as it is used in machine learning systems. we motivate the importance of automated methods for evaluating...evaluation and selection of biases in machine learningthe inductive learning problem consists of learning a concept given examples and nonexamples of the concept. to perform this learning task, inductive learning algorithms bias their learning method...utilizing prior concepts for learningthe problem of learning decision rules for sequential tasks is addressed, focusing on the problem of learning tactical plans from a simple flight simulator where a plane must avoid a missile...improving tactical plans with id107evolutionary learning methods have been found to be useful in several areas in the development of intelligent robots. in the approach described here, evolutionary...an evolutionary approach to learning in robotsnavigation through obstacles such as mine fields is an important capability for autonomous underwater vehicles. one way to produce robust behavior...using a genetic algorithm to learn strategies for collision avoidance and local navigation..............................relational topic models

    research has focused on    nding communities and patterns in the

link-structure of these networks (kemp et al. 2004, ho    et al.,
2002, hofman and wiggins 2007, airoldi et al. 2008).

    by adapting supervised id96, we can build a good model

of content and structure.

    rtms    nd related hidden structure in both types of data.

d. blei

topic models

52478430248775288112321222299135418541855896359224381364791096401196861201959153914717217796591121921489885178378286208156923431270218129022322723616172541176256634264196321951377303426209131316425348013353445851244229126171627229012753751027396167824472583106169212079601238201216442042381418179212846515241165219715682593169854768321371637255720336321020436442449474649263623005395416031047722660806112111388318371335902964966981167311401481143212531590106099299410011010165115781039104013441345134813551420108914831188167416802272128515921234130413171426169514651743194422592213we address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts...irrelevant features and the subset selection problemin many domains, an appropriate inductive bias is the min-features bias, which prefers consistent hypotheses definable over as few features as possible...learning with many irrelevant featuresin this introduction, we define the term bias as it is used in machine learning systems. we motivate the importance of automated methods for evaluating...evaluation and selection of biases in machine learningthe inductive learning problem consists of learning a concept given examples and nonexamples of the concept. to perform this learning task, inductive learning algorithms bias their learning method...utilizing prior concepts for learningthe problem of learning decision rules for sequential tasks is addressed, focusing on the problem of learning tactical plans from a simple flight simulator where a plane must avoid a missile...improving tactical plans with id107evolutionary learning methods have been found to be useful in several areas in the development of intelligent robots. in the approach described here, evolutionary...an evolutionary approach to learning in robotsnavigation through obstacles such as mine fields is an important capability for autonomous underwater vehicles. one way to produce robust behavior...using a genetic algorithm to learn strategies for collision avoidance and local navigation..............................relational topic models

    binary response variable with each pair of documents
    adapt variational em algorithm for slda with binary glm response

model (with di   erent link id203 functions).

    allows predictions that are out of reach for traditional models.

d. blei

topic models

  nd  dwd,nzd,nk  kyd,d'  nd'  d'wd',nzd',npredictive performance of one type given the other

cora corpus (mccallum et al., 2000)

d. blei

topic models

510152025!3600!3550!3500!3450coranumber of topicsword log likelihood510152025!14000!13500!13000coranumber of topicslink log likelihoodrtm,,  !!""rtm,,  !!elda + regression       mixed!membershipunigram/bernoullipredictive performance of one type given the other

webkb corpus (craven et al., 1998)

d. blei

topic models

510152025!3350!3300!3250!3200webkbnumber of topicslink log likelihood510152025!1145!1140!1135!1130webkbnumber of topicsword log likelihoodrtm,,  !!""rtm,,  !!elda + regression       mixed!membershipunigram/bernoullipredictive performance of one type given the other

pnas corpus (courtesy of jstor)

d. blei

topic models

510152025!4750!4700!4650!4600!4550!4500!4450pnasnumber of topicslink log likelihood510152025!2970!2960!2950!2940pnasnumber of topicsword log likelihoodrtm,,  !!""rtm,,  !!elda + regression       mixed!membershipunigram/bernoullipredicting links from documents

given a new document, which documents is it likely to link to?

d. blei

topic models

16j.changandd.bleitable2topeightlinkpredictionsmadebyrtm(  e)andlda+regressionfortwodocuments(italicized)fromcora.themodelswere   twith10topics.boldfacedtitlesindicateactualdocumentscitedbyorcitingeachdocument.overthewholecorpus,rtmimprovesprecisionoverlda+regressionby80%whenevaluatedonthe   rst20documentsretrieved.markovchainmontecarloconvergencediagnostics:acomparativereviewminorizationconditionsandconvergenceratesformarkovchainmontecarlortm(  e)ratesofconvergenceofthehastingsandmetropolisalgorithmspossiblebiasesinducedbymcmcconvergencediagnosticsboundingconvergencetimeofthegibbssamplerinbayesianimagerestorationselfregenerativemarkovchainmontecarloauxiliaryvariablemethodsformarkovchainmontecarlowithapplicationsrateofconvergenceofthegibbssamplerbygaussianapproximationdiagnosingconvergenceofmarkovchainmontecarloalgorithmsexactboundfortheconvergenceofmetropolischainslda+regressionselfregenerativemarkovchainmontecarlominorizationconditionsandconvergenceratesformarkovchainmontecarlogibbs-markovmodelsauxiliaryvariablemethodsformarkovchainmontecarlowithapplicationsmarkovchainmontecarlomodeldeterminationforhierarchicalandgraphicalmodelsmediatinginstrumentalvariablesaqualitativeframeworkforprobabilisticid136adaptationforselfregenerativemcmccompetitiveenvironmentsevolvebettersolutionsforcomplextaskscoevolvinghighlevelrepresentationsrtm(  e)asurveyofevolutionarystrategiesgeneticalgorithmsinsearch,optimizationandmachinelearningstronglytypedgeneticprogramminginevolvingcooperationstrategiessolvingcombinatorialproblemsusingevolutionaryalgorithmsapromisinggeneticalgorithmapproachtojob-shopscheduling...evolutionarymoduleacquisitionanempiricalinvestigationofmulti-parentrecombinationoperators...anewalgorithmfordnasequenceassemblylda+regressionidenti   cationofproteincodingregionsingenomicdnasolvingcombinatorialproblemsusingevolutionaryalgorithmsapromisinggeneticalgorithmapproachtojob-shopscheduling...ageneticalgorithmforpassivemanagementtheperformanceofageneticalgorithmonachaoticobjectivefunctionadaptiveglobaloptimizationwithlocalsearchmutationratesasadaptationstable2illustratessuggestedcitationsusingrtm(  e)andlda+regres-sionaspredictivemodels.thesesuggestionswerecomputedfromamodel   tononeofthefoldsofthecoradata.thetopresultsillustratesuggestedlinksfor   markovchainmontecarloconvergencediagnostics:acomparativere-predicting links from documents

given a new document, which documents is it likely to link to?

d. blei

topic models

16j.changandd.bleitable2topeightlinkpredictionsmadebyrtm(  e)andlda+regressionfortwodocuments(italicized)fromcora.themodelswere   twith10topics.boldfacedtitlesindicateactualdocumentscitedbyorcitingeachdocument.overthewholecorpus,rtmimprovesprecisionoverlda+regressionby80%whenevaluatedonthe   rst20documentsretrieved.markovchainmontecarloconvergencediagnostics:acomparativereviewminorizationconditionsandconvergenceratesformarkovchainmontecarlortm(  e)ratesofconvergenceofthehastingsandmetropolisalgorithmspossiblebiasesinducedbymcmcconvergencediagnosticsboundingconvergencetimeofthegibbssamplerinbayesianimagerestorationselfregenerativemarkovchainmontecarloauxiliaryvariablemethodsformarkovchainmontecarlowithapplicationsrateofconvergenceofthegibbssamplerbygaussianapproximationdiagnosingconvergenceofmarkovchainmontecarloalgorithmsexactboundfortheconvergenceofmetropolischainslda+regressionselfregenerativemarkovchainmontecarlominorizationconditionsandconvergenceratesformarkovchainmontecarlogibbs-markovmodelsauxiliaryvariablemethodsformarkovchainmontecarlowithapplicationsmarkovchainmontecarlomodeldeterminationforhierarchicalandgraphicalmodelsmediatinginstrumentalvariablesaqualitativeframeworkforprobabilisticid136adaptationforselfregenerativemcmccompetitiveenvironmentsevolvebettersolutionsforcomplextaskscoevolvinghighlevelrepresentationsrtm(  e)asurveyofevolutionarystrategiesgeneticalgorithmsinsearch,optimizationandmachinelearningstronglytypedgeneticprogramminginevolvingcooperationstrategiessolvingcombinatorialproblemsusingevolutionaryalgorithmsapromisinggeneticalgorithmapproachtojob-shopscheduling...evolutionarymoduleacquisitionanempiricalinvestigationofmulti-parentrecombinationoperators...anewalgorithmfordnasequenceassemblylda+regressionidenti   cationofproteincodingregionsingenomicdnasolvingcombinatorialproblemsusingevolutionaryalgorithmsapromisinggeneticalgorithmapproachtojob-shopscheduling...ageneticalgorithmforpassivemanagementtheperformanceofageneticalgorithmonachaoticobjectivefunctionadaptiveglobaloptimizationwithlocalsearchmutationratesasadaptationstable2illustratessuggestedcitationsusingrtm(  e)andlda+regres-sionaspredictivemodels.thesesuggestionswerecomputedfromamodel   tononeofthefoldsofthecoradata.thetopresultsillustratesuggestedlinksfor   markovchainmontecarloconvergencediagnostics:acomparativere-spatially consistent topics

    for exploratory tasks, rtms can be used to    guide    the topics
    documents are geographically-tagged news articles from yahoo!

links are the adjacency matrix of states

    rtm    nds spatially consistent topics.

d. blei

topic models

18j.changandd.bleitopic 1topic 2topic 3topic 4topic 5topic 1topic 2topic 3topic 4topic 5fig5.acomparisonbetweenrtm(left)andlda(right)oftopicdistributionsonlocalnewsdata.eachcolor/rowdepictsasingletopic.eachstate   scolorintensityindicatesthemagnitudeofthattopic   scomponent.thecorrespondingwordsassociatedwitheachtopicaregivenintable3.whereaslda   ndsgeographicallydi   usetopics,rtm,bymodelingspatialconnectivity,   ndscoherentregions.18j.changandd.bleitopic 1topic 2topic 3topic 4topic 5topic 1topic 2topic 3topic 4topic 5fig5.acomparisonbetweenrtm(left)andlda(right)oftopicdistributionsonlocalnewsdata.eachcolor/rowdepictsasingletopic.eachstate   scolorintensityindicatesthemagnitudeofthattopic   scomponent.thecorrespondingwordsassociatedwitheachtopicaregivenintable3.whereaslda   ndsgeographicallydi   usetopics,rtm,bymodelingspatialconnectivity,   ndscoherentregions.18j.changandd.bleitopic 1topic 2topic 3topic 4topic 5topic 1topic 2topic 3topic 4topic 5fig5.acomparisonbetweenrtm(left)andlda(right)oftopicdistributionsonlocalnewsdata.eachcolor/rowdepictsasingletopic.eachstate   scolorintensityindicatesthemagnitudeofthattopic   scomponent.thecorrespondingwordsassociatedwitheachtopicaregivenintable3.whereaslda   ndsgeographicallydi   usetopics,rtm,bymodelingspatialconnectivity,   ndscoherentregions.18j.changandd.bleitopic 1topic 2topic 3topic 4topic 5topic 1topic 2topic 3topic 4topic 5fig5.acomparisonbetweenrtm(left)andlda(right)oftopicdistributionsonlocalnewsdata.eachcolor/rowdepictsasingletopic.eachstate   scolorintensityindicatesthemagnitudeofthattopic   scomponent.thecorrespondingwordsassociatedwitheachtopicaregivenintable3.whereaslda   ndsgeographicallydi   usetopics,rtm,bymodelingspatialconnectivity,   ndscoherentregions.18j.changandd.bleitopic 1topic 2topic 3topic 4topic 5topic 1topic 2topic 3topic 4topic 5fig5.acomparisonbetweenrtm(left)andlda(right)oftopicdistributionsonlocalnewsdata.eachcolor/rowdepictsasingletopic.eachstate   scolorintensityindicatesthemagnitudeofthattopic   scomponent.thecorrespondingwordsassociatedwitheachtopicaregivenintable3.whereaslda   ndsgeographicallydi   usetopics,rtm,bymodelingspatialconnectivity,   ndscoherentregions.relational topic models

    relational id96 allows us to analyze connected
documents, or other data for which the mixed-membership
assumptions are appropriate.

    traditional models cannot predict with new and unlinked data.
    rtms allow for such predictions

    links given the new words of a document
    words given the links of a new document

d. blei

topic models

used in exploratory tools of document collections

d. blei

topic models

