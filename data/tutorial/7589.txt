id203: review 

 

pieter abbeel 

uc berkeley eecs 

 

 
 
 
 
 
many slides adapted from thrun, burgard and fox, probabilistic robotics 
 
 
 texpoint fonts used in emf.  

why id203 in robotics? 

n       often state of robot and state of its environment are 

unknown and only noisy sensors available 
n       id203 provides a framework to fuse sensory 

information 

         result: id203 distribution over possible states of 

robot and environment 

n       dynamics is often stochastic, hence can   t optimize for a 
particular outcome, but only optimize to obtain a good 
distribution over outcomes   
n       id203 provides a framework to reason in this setting 
         result: ability to find good control policies for stochastic 

dynamics and environments 

 

example 1: helicopter 

n       state: position, orientation, velocity, angular rate 
n       sensors:  

n       gps : noisy estimate of position (sometimes also velocity) 
n       inertial sensing unit: noisy measurements from  

(i)    3-axis gyro [=angular rate sensor],  
(ii)    3-axis accelerometer [=measures acceleration + 

gravity; e.g., measures (0,0,0) in free-fall], 

(iii)    3-axis magnetometer 

n       dynamics: 

n       noise from: wind, unmodeled dynamics in engine, servos, 

blades 

example 2: mobile robot inside building 

n       state: position and heading 
n       sensors: 

n       odometry (=sensing motion of actuators): e.g., wheel 

encoders  

n       laser range finder:  

n       measures time of flight of a laser beam between 

departure and return  

n       return is typically happening when hitting a surface 
that reflects the beam back to where it came from 

n       dynamics: 

n       noise from: wheel slippage, unmodeled variation in floor 

axioms of id203 theory 

 
n      

n      

n      

  
 

  

   
  

pr(

a
0
1)
   
   
pr(!) =1
pr(a !b) = pr(a) +pr(b) "pr(a #b)

pr(!) = 0

pr(a) denotes id203 that the outcome    is an 
  

element of the set of possible outcomes a. a is often 
called an event.  same for b. 

   is the set of all possible outcomes. 
   is the empty set. 

5 

a closer look at axiom 3 

pr(a !b) = pr(a) +pr(b) "pr(a #b)

!

a

a !b

b

6 

using the axioms 

pr(a !(" \ a)) = pr(a) +pr(" \ a) #pr(a $(" \ a))

pr(")
1

pr(" \ a)

=
=
=

pr(a) +pr(" \ a) #pr(!)
pr(a) +pr(" \ a) #0

1#pr(a)

7 

discrete random variables 
x2 
x4 

x1 

n       x denotes a random variable. 

!

x3 

n       x can take on a countable number of values in {x1, x2, 

   , xn}. 

n       p(x=xi), or p(xi), is the id203 that the random 

variable x takes on value xi.  
. 

n       p( ) is called id203 mass function. 

 

n       e.g., x models the outcome of a coin flip, x1 = head, x2 = 

tail, p( x1 ) = 0.5 , p( x2 ) = 0.5  

8 

continuous random variables 

n       x takes on values in the continuum. 

n       p(x=x), or p(x), is a id203 density function. 

 

pr(

x

   

ba
,(

))

b

   =

a

dxxp
)(

p(x) 

n       e.g. 

x 

9 

joint and id155 

n       p(x=x and y=y) = p(x,y) 

n       if x and y are independent then  

 

 p(x,y) = p(x) p(y) 

n       p(x | y) is the id203 of x given y 

 
 

 p(x | y) = p(x,y) / p(y) 
 p(x,y)   = p(x | y) p(y) 

n       if x and y are independent then 

 

 p(x | y) = p(x) 

n       same for id203 densities, just p       p 

10 

law of total id203, marginals 

discrete case 

continuous case 

xp
1)(
=

   

x

xp
)(

xp
)(

   =

y

   =

y

xp
)(

dx

=1

   

yxp
,(

)

xp
)(

   =

yxp
),(

dy

ypyxp
(
)(

)

|

xp
)(

   =

ypyxp
|(
)()

dy

11 

bayes formula 

ypyxp
(
)(

)

|

xpxyp
(
)(

)

|

=

yxp
,(

)

=

   

yxp
(

)

=

xpxyp
(
)(

|
)
yp
)(

=

 
likelihood
   
evidence

prior

12 

id172 
|
)
yp
)(
1
   

yxp

=

)

(

  

=

yp
)(

=

xpxyp
)(

(

xpxyp
)(

)

(

|

=

  
1
|

xpxyp
)()

(

   

x

algorithm: 

x
   

:

aux

=

yx
|

  

=

1
aux

yx
|

   

x

xpxyp
(
)(

)

|

yxpx
:

(

|

   

)

aux

  

=

yx
|

13 

conditioning 

n       law of total id203: 

xp
)(

xp
)(

yxp
(

)

=

=

=

   
   
   

zxp
),(

dz

zpzxp
(
)(

)

|

dz

yzpzyxp
(

),

(

|

|

)

dz

14 

bayes rule with background knowledge 

zyxp
(
),

|

=

zxpzxyp
(
)|

|

),
(
zyp
(
)|

15 

conditional independence 

 

zypzxpzyxp
,(
)|

)|

(

(

)

=

 equivalent to 

    

  and 

yzxpzxp
(
,

(

)

|

=

)

xzypzyp
(
),

(

)

|

=

16 

simple example of state estimation 

n       suppose a robot obtains measurement z 
n       what is p(open|z)? 

17 

causal vs. diagnostic reasoning 

n       p(open|z) is diagnostic. 

n       p(z|open) is causal. 

n       often causal knowledge is easier to obtain. 

n       bayes rule allows us to use causal knowledge: 

count frequencies! 

p

(

open

|

z

)

=

zp
(

|

p
open
)
zp
)(

(

open

)

18 

example 

 p(z|  open) = 0.3 

n       p(z|open) = 0.6  
n       p(open) = p(  open) = 0.5 
p(z |open)p(open)

p(open | z) =

p(z)

p

(

open

|

z

)

=

zp
(

|

p

(

open

|

z

)

=

open
p
)
5.06.0
   
+

5.03.05.06.0

   

   

open
)
+
2
3

=

67.0

=

zp
(
|
open
(

p
)
zp
(

open
(
|
  

)
open

)

p

(

  

open

)

       z raises the id203 that the door is open. 

19 

combining evidence 

n       suppose our robot obtains another observation z2. 

n       how can we integrate this new information? 

n       more generally, how can we estimate 

p(x| z1...zn )? 

20 

recursive bayesian updating 

zxp
(
|
,
1

   

,

z
n

)

=

n

|

zxzp
(
,
,
,
   
1
zp
(
n

   

z
)
n
1
z
|
,
   
1

zxp
(
|
,
1
z
,
)
n
1

   

   

,

z
n

)
1

   

markov assumption: zn is independent of z1,...,zn-1 if we 
know x. 

p(x | z1,   ,zn) =

p(zn | x) p(x | z1,   ,zn ! 1)

p(zn | z1,   ,zn ! 1)

=!p(zn | x) p(x | z1,   ,zn ! 1)

=!1...n

#
%
$

"
i=1...n

p(zi | x)

&
(p(x)
'

21 

example: second measurement  

n       p(z2|open) = 0.5 
n       p(open|z1)=2/3 
 
p

open

z

)

(

,

|

=

z
1

2

|

zp
open
(
2
1
2
2
3
2
3
   +   
3
5

1
3

   

1
2

=

 p(z2|  open) = 0.6 

p
)
(
zp
(

open
|
  

z
|
)
1
open

2

zp
(
2
open

|
|

open
z
+
1

)

)

p

(

=

5
8

.0

625

=

       z2 lowers the id203 that the door is open. 

)

p

(

  

open

|

z
1

)

22 

a typical pitfall 

n       two possible locations x1 and x2 
n       p(x1)=0.99  
n       p(z|x2)=0.09 p(z|x1)=0.07  

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

)
d

 
|
 

x
 
(
p

5

10

15

20
number of integrations

25

30

p(x2 | d)
p(x1 | d)

35

40

45

50

23 

