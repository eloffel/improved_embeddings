the problem(s) 
with neural chatbots

ryan lowe
mcgill university, openai

dialogue systems

why work on dialogue systems?

    many commercial applications

    creating a    general-purpose communicating agent   

    an agent that can communicate with humans on many topics, to 

exchange knowledge and complete a variety of tasks in its environment.

    language is a natural communication interface between humans 

and machines

modular dialogue systems

    traditional system consists of 

modules

    each module optimized with  

separate objective function

    achieves fairly good performance with small amounts of data

problem: require supervised data for each module    

does not scale well to general domains!

end-to-end dialogue systems

    a single model trained directly

on conversational data

    uses a single objective 

function, usually maximum 
likelihood on next response

    significant recent work using neural networks to predict the next 

response. (ritter et al., 2011; sordoni et al., 2015; shang et al., 2015)  

why might this work?

advantages of end-to-end systems:

1) does not require feature engineering (only architecture 

engineering).

2) can be transferred to different domains.

3) does not require supervised data for each module!
(collecting this data is difficult at large scales)

early results     vinyals & le (2015)

current results (vhred)

<first_speaker> what do you mean?
<second_speaker> what are you talking about?
<first_speaker> i 'm not a big fan of cats , dogs , dogs 
, dogs , dogs , dogs , dogs , dogs , dogs , dogs , dogs , 
dogs , dogs , dogs , dogs , dogs , dogs , dogs , dogs , 
dogs , dogs , dogs , dogs , dogs , dogs , dogs , dogs , 
dogs , dogs , dogs , dogs , dogs.

what went wrong?

problem #1: data

dialogue datasets

    building general-purpose dialogue systems requires lots of data

    the best datasets are proprietary

    we need large (>500k dialogues), open-source datasets to 

make progress

ubuntu dialogue 
corpus

    large dataset of ~1 million 
tech support dialogues

    scraped from ubuntu irc 

channel

    2-person dialogues extracted 

from chat stream

lowe*, pow*, serban, pineau.    the ubuntu dialogue corpus: a large dataset 
for research in unstructured multi-turn dialogue systems.    sigdial, 2015.

ubuntu dialogue corpus

pros:
    hard
    large
    open-source
    related to many real-

cons:
    too hard?
    not perfectly disentangled
    requires external knowledge 

to solve

world technical problems

    ideally suited for task-oriented 

setting, but no reward signal 
in dataset

large-scale dialogue datasets

    ubuntu dialogue corpus (lowe et al., 2015)
    twitter corpus (ritter et al., 2011)
    movie dialog dataset (dodge et al. 2016)
    reddit 
       

survey paper covering existing datasets: 
serban, lowe, charlin, pineau.    a survey of available corpora for building 
data-driven dialogue systems.    arxiv:1512.05742, 2015.

problem #2: model architecture

recurrent neural networks

    augment neural networks with self-loops
    leads to the formation of a hidden state st that evolves over 

time:

ht = f(whhht-1 + wihxt)

    used to model sequences (e.g. natural language)

source: colah.github.io

sequence-to-sequence learning

    use an id56 encoder to map an 
input sequence to a fixed-length 
vector

    use an id56 decoder (with 

different parameters) to map the 
vector to the target sequence

(cho et al., 2014; sustkever et al., 2014)

main goal

build models with right inductive biases to effectively represent 
dialogue data 

judge model quality by quality of generated responses

some problems: generic responses

    most models trained to predict most likely 

next utterance given context

    but some utterances are likely given any 

context!

    neural models often generate    i don   t 

know   , or    i   m not sure    to most contexts

(li et al., 2016)

more problems

    strong constraint on generation process: only source of 

variation is at the output

    when the model lacks capacity, it is encouraged to mostly 

capture short-term dependencies

    want to explicitly model variations at    higher level    

representations (e.g. topic, tone, sentiment, etc.)

variational encoder-
decoder (vhred)

    augment hred with 

gaussian latent variable z
    z can capture high-level 
utterance features (e.g. 
topic, tone)

    when generating first

sample latent variable, 
then use it to condition 
generation

serban, sordoni, lowe, charlin, pineau, courville, bengio. 
   a hierarchical latent variable encoder-decoder model for 
generating dialogues.    arxiv:1605.06069, 2016.

variational encoder-decoder 
(vhred)

    inspired by vae (kingma & welling, 2014; rezende et al., 2014): 

train model with backprop using reparameterization trick

    prior mean and variance are learned conditioned on previous 

utterance representation. posterior mean and variance also 
conditioned on representation of target utterance.

    at training time, sample from posterior. at test time, sample from 

prior.

    developed concurrently with bowman et al. (2016)

    use word-dropping and kl annealing tricks

quantitative results

cherry-picked results

future work

    many interesting areas to be investigated:

    modifying the id168
    adversarial training
    id23
    learning from human interaction
    ...

problem #3: evaluation

dialogue evaluation

    hard to know if we   re making progress in building dialogue models

    important to define     wrong metrics can lead to spurious research

    human evaluation is effective, but slow and expensive     want to 

have an automatic evaluation metric

    lack of reliable metrics means researchers only compare to 

their own previously implemented models

comparison of ground-truth utterance

context

hey, want to 
go to the 
movies tonight?

generated 
response

nah, let   s do 
something 
active.

reference 
response
yeah, the film 
about turing 
looks great!

score

comparison of ground-truth utterance

    word-overlap metrics:

    id7, meteor, id8

    look at the number of overlapping 

id165s between the generated 
and reference responses

    correlate poorly with humans in 

dialogue

generated 
response

yes, let   s go 
see that movie 
about turing!

reference 
response
nah, i   d rather 
stay at home, 
thanks.

score

correlation 
study

    created 100 questions each for twitter and ubuntu datasets (20 

contexts with responses from 5    diverse models   )

    25 volunteers from cs department at mcgill

    asked to judge response quality on a scale from 1 to 5

    compared human ratings with ratings from automatic evaluation 

metrics

models for response variety

1) randomly selected response

2) retrieval models:

    response with smallest tf-idf cosine distance
    response selected by dual encoder (de) model

3) generative models:

    hierarchical recurrent encoder-decoder (hred)

4) human-written response (not ground truth)

goal (inter-annotator)

reality (id7)

reality (id8 & meteor)

correlation results

original paper (liu et al., 2016):

after removing pre-processing 
artifacts (<speaker> token):

word-overlap metrics are poor substitute for human evaluations

learning to 
evaluate

a dialogue response is probably good if it is rated highly by 
humans.

    collect a labelled dataset of human scores of responses

    build a model that learns to predict human scores of response 

quality (adem)

    condition response score on the reference response and the 

context

context-conditional evaluation

context

hey, want to 
go to the 
movies tonight?

generated 
response

nah, let   s do 
something 
active.

reference 
response
yeah, the film 
about turing 
looks great!

score

context-conditional evaluation

context

hey, want to 
go to the 
movies tonight?

seen any good 
movies 
recently?

generated 
response

nah, let   s do 
something 
active.

reference 
response
yeah, the film 
about turing 
looks great!

score

dialogue response score should also depend on context!

evaluation dataset

conducted 2 rounds of amt studies to get 
evaluation on twitter

study 1: ask workers to generate next 
sentence of a conversation

study 2: ask workers to evaluate responses 
from various models (human, tfidf, 
hred, de)

evaluation dataset

    our simplifying assumption is that dialogue 

response quality measured by 
   appropriateness   

    in our experiments, other measures 

(   topicality   ,    informativeness   , etc.) either 
had little inter-annotator agreement, or 
correlated strongly with    appropriateness   

adem

    given: context c, model response r, reference response r (with 

^

embeddings c, r, r), compute score as:

^

where m, n are parameter matrices,   ,    are constants.

    trained to minimize squared error:

adem

adem pre-training

    want model that can 

learn from limited 
data (since collection 
is expensive)

    pre-train id56 

encoder of adem 
using vhred

length correlation

problem: humans favour shorter responses, and adem can 
trivially use this for better performance (length gets 0.27 
correlation with human score)

solution: bin training set examples by length, re-weight samples 
such that each length bin has same average score

utterance-level results

system-level results

results     generalization 

how useful is this?

    moderately. need to collect more data for better 

generalization

    only considers single utterances, rather than a whole dialogue

    what about other aspects of dialogue quality?

adversarial evaluation

    rather than imitating human scores, train 

a model to distinguish between real and 
generated responses (kannan et al, 2016; li et 
al., 2017)

    similar to discriminator in a gan

    combines well with adem     want 

dialogue responses that are appropriate, 
and similar to human responses

problem #4: entire premise?

learning from static datasets

    will training solely from static datasets lead to a    general-purpose communicating 

agent   ?

    probably not. in this setting, we are primarily learning the statistical structure of 

language

    but we also want to learn the function of language, and ground the learned 

language in the agent   s observations

    an alternative approach: have simulated agents in physical environments learn to 

communicate to solve tasks in that environment (gauthier & mordatch, 2016)

multi-agent language learning

primary collaborators

joelle pineau

iulian v. serban

mike noseworthy

mcgill

u. montreal

mcgill

chia-wei liu

mcgill

laurent charlin

hec montreal

nicolas angelard-gontier

mcgill

nissan pow

mcgill

aaron courville

u. montreal

yoshua bengio

u. montreal

references

bowman, s. r., vilnis, l., vinyals, o., dai, a. m., jozefowicz, r., & bengio, s.    generating sentences from a continuous space.    coling, 2016.
cho, k., van merri  nboer, b., gulcehre, c., bahdanau, d., bougares, f., schwenk, h., & bengio, y.    learning phrase representations using id56 
encoder-decoder for id151.    emnlp, 2014.

kannan, vinyals.    adversarial evaluation of dialogue models.    nips workshop on adversarial training, 2016.

kingma & welling.    auto-encoding id58.    iclr, 2014.

li, monroe, shi, ritter, jurafsky.    adversarial learning for neural dialogue generation.    2017.

liu, lowe, serban, noseworthy, charlin, pineau.    how not to evaluate your dialogue system: a study of unsupervised id74 for 
dialogue response generation.    emnlp, 2016.

lowe, noseworthy, serban, angelard-gontier, bengio, pineau.    towards an automatic turing test: learning to evaluate dialogue responses.    
2016.

lowe, pow, serban, pineau.    the ubuntu dialogue corpus: a large dataset for research in unstructured dialogue systems.    sigdial, 2015.

papineni, k., roukos, s., ward, t., & zhu, w. j.    id7: a method for automatic evaluation of machine translation   . acl, 2002.

ranzato, m. a., chopra, s., auli, m., & zaremba, w.    sequence level training with recurrent neural networks.    iclr, 2015.

rezende, d. j., mohamed, s., & wierstra, d.    stochastic id26 and approximate id136 in deep generative models.    icml, 2014.

serban, lowe, charlin, pineau.    a survey of available corpora for building data-driven dialogue systems.    2016.

serban, sordoni, lowe, pineau, courville, bengio.    a hierarchical latent variable encoder-decoder model for generating dialogues.    aaai, 2017.

sutskever, vinyals, le.    sequence-to-sequence learning with neural networks.    nips, 2014.

sutton & barto.    id23: an introduction.    1998.

thank you!

quantitative vhred 
results

vhred results

length bias of word overlap 
metrics

where does adem do better?

