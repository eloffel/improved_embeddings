an overview of deep learning frameworks 
& an introduction to

adam paszke, sam gross, soumith chintala, francisco massa, adam 
lerer, james bradbury, gregory chanan, trevor killeen, zeming lin, 
natalia gimelshein, christian sarofeen, alban desmaison, andreas 
kopf, edward yang, zach devito, martin raison, alykhan tejani, sasank 
chilamkurthy  

overview of the talk

deep learning 
frameworks

computation 
graph toolkits

pytorch

declarative

imperative

implementation

advantages & disadvantages

deep learning frameworks

deep learning frameworks
in addition to computation graph toolkits 
- provide gradient computation 
- gradient of one variable w.r.t. any variable in graph

mm

mm

add

tanh

deep learning frameworks
in addition to computation graph toolkits 
- provide gradient computation 
- gradient of one variable w.r.t. any variable in graph 

-for example, can compute:

mm

mm

add

tanh

deep learning frameworks
in addition to computation graph toolkits 
- provide gradient computation 
- gradient of one variable w.r.t. any variable in graph 

-for example, can compute: 

-and then you usually do sgd step:

mm

mm

add

tanh

deep learning frameworks
in addition to computation graph toolkits 

   provide integration with high 
performance dl libraries like cudnn

mm

mm

add

tanh

computation graph toolkits

computation graph toolkits
declarative toolkits

computation graph toolkits
declarative toolkits 
    declare a computation 
    with placeholder variables 
    compile it 
    run it in a session

computation graph toolkits
declarative toolkits 
    declare a computation 
    with placeholder variables 
    compile it 
    run it in a session

computation graph toolkits
declarative toolkits 
    declare a computation 
    with placeholder variables 
    compile it 
    run it in a session

a separate turing-complete  

virtual machine

computation graph toolkits
declarative toolkits 
    declare a computation 
    with placeholder variables 
    compile it 
    run it in a session

can handle loops, conditionals 

(if, scan, while, etc.)

computation graph toolkits
declarative toolkits 
    declare a computation 
    with placeholder variables 
    compile it 
    run it in a session

has it's own  
execution engine

computation graph toolkits
declarative toolkits 
    declare a computation 
    with placeholder variables 
    compile it 
    run it in a session

has it's own compiler 
- fuse operations 
- reuse memory 
- do optimizations

computation graph toolkits
declarative toolkits 
    declare a computation 
    with placeholder variables 
    compile it 
    run it in a session

graph can be serialized easily

computation graph toolkits
declarative toolkits 
    declare a computation 
    with placeholder variables 
    compile it 
    run it in a session

own virtual machine

computation graph toolkits
declarative toolkits 
    declare a computation 
    with placeholder variables 
    compile it 
    run it in a session

own virtual machine 

- separate debugging tools 

computation graph toolkits
declarative toolkits 
    declare a computation 
    with placeholder variables 
    compile it 
    run it in a session

own virtual machine 

- separate debugging tools 
- non-linear thinking for user

imperative toolkits

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run

hips autograd

dynet

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run!

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run!

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine 
   debugging is easy

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine 
   debugging is easy

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine 
   debugging is easy

print(x) 
y = foo(x) 
print(y)

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine 
   debugging is easy

print("hello") 

y = foo(x) 

print("hello2")

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine 
   debugging is easy

print("hello") 

y = foo(x) 

print("hello2")

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine 
   debugging is easy 
   linear program    ow 
   linear thinking for user

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine 
   debugging is easy 
cannot compile program
   linear program    ow 
   linear thinking for user

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine 
   debugging is easy 
cannot compile program
   linear program    ow 
cannot optimize
   linear thinking for user

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine 
   debugging is easy 
cannot compile program
   linear program    ow 
cannot optimize
   linear thinking for user
cannot do static analysis

computation graph toolkits
imperative toolkits 
    run a computation 
   computation is run! 
   no separate execution engine 
   debugging is easy 
cannot compile program
   linear program    ow 
cannot optimize
   linear thinking for user
cannot do static analysis

(more on this later)

pytorch

what is pytorch?

automatic di   erentiation 

engine

ndarray library  
with gpu support

gradient based  

optimization package

utilities 

(data loading, etc.)

deep learning

id23

numpy-alternative

ndarray library

   np.ndarray <-> torch.tensor 
   200+ operations, similar to numpy 
   very fast acceleration on nvidia gpus 

ndarray library

numpy

pytorch

ndarray / tensor library

ndarray / tensor library

ndarray / tensor library

ndarray / tensor library

numpy bridge 

numpy bridge 

zero memory-copy 

very e   cient

numpy bridge 

numpy bridge 

seaid113ss gpu tensors

automatic di   erentiation engine

for deep learning and id23

pytorch autograd
from torch.autograd import variable 

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t())

mm

mm

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h

mm

mm

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h

mm

mm

add

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh()

mm

mm

add

tanh

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh() 
next_h.backward(torch.ones(1, 20))

mm

mm

add

tanh

neural networks

neural networks

neural networks

optimization package
sgd, adagrad, rmsprop, lbfgs, etc.

ml work   ows

pain points

core philosophy 

(of pytorch)

upcoming 
features

what do they look 

like in  

the deep learning space

how does pytorch  
deal with them?

ml work   ows

ml work   ows

idea / theory

design experiments

preprocess data /  
setup environments

publish / ship

training & 
validation

implement models

work items in practice

writing  

dataset loaders

building models

implementing  
training loop

checkpointing 

models

interfacing with 
environments

building optimizers

dealing with  

gpus

building 
baselines

work items in practice

writing  

dataset loaders

building models

implementing  
training loop

checkpointing 

models

python + pytorch - an environment to do all of this

interfacing with 
environments

building optimizers

dealing with  

gpus

building 
baselines

writing data loaders

    every dataset is slightly di   erently formatted

writing data loaders

    every dataset is slightly di   erently formatted 
    have to be preprocessed and normalized di   erently

writing data loaders

    every dataset is slightly di   erently formatted 
    have to be preprocessed and normalized di   erently 
    need a multithreaded data loader to feed gpus fast enough

writing data loaders
pytorch solution: 
    share data loaders across the community! 

writing data loaders
pytorch solution: 
    share data loaders across the community! 

writing data loaders
pytorch solution: 
    use regular python to write datasets:  

leverage existing python code 

writing data loaders
pytorch solution: 
    use regular python to write datasets:  

leverage existing python code 
example: parlai 

writing data loaders
pytorch solution: 
   code in practice

writing data loaders
pytorch solution: 
   code in practice

research 
work   ows

pain points

core philisophy

upcoming 
features

writing data loaders
pytorch solution: 
   code in practice

code for writing 

a new  

pytorch dataset

writing data loaders
pytorch solution: 
   code in practice

code for writing 

a new  

pytorch dataset

writing data loaders
pytorch solution: 
   code in practice

code for writing 

a new  

pytorch dataset

under the hood of data loaders
pytorch solution: 
    torch.multiprocessing 

- a minor fork of python multiprocessing 
- custom pickler 
- tensors are automatically shared across processes 

interfacing with environments

cars

video games

internet

interfacing with environments

cars

video games

pretty much every environment provides a python api

interfacing with environments

cars
natively interact with the environment directly

video games

debugging
    pytorch is a python extension

debugging
    pytorch is a python extension 
    use your favorite python debugger

debugging
    pytorch is a python extension 
    use your favorite python debugger

debugging
    pytorch is a python extension 
    use your favorite python debugger 
    use the most popular debugger:

debugging
    pytorch is a python extension 
    use your favorite python debugger 
    use the most popular debugger: 
          print(foo)

identifying bottlenecks
    pytorch is a python extension 
    use your favorite python pro   ler

identifying bottlenecks
    pytorch is a python extension 
    use your favorite python pro   ler: line_pro   ler

compilation time
    pytorch is written for the impatient

compilation time
    pytorch is written for the impatient 
    absolutely no compilation time when writing your scripts 
whatsoever

compilation time
    pytorch is written for the impatient 
    absolutely no compilation time when writing your scripts 
whatsoever 
    all core kernels are pre-compiled

ecosystem
    use the entire python ecosystem at your will

ecosystem
    use the entire python ecosystem at your will 
    including scipy, scikit-learn, etc.

ecosystem
    use the entire python ecosystem at your will 
    including scipy, scikit-learn, etc.

ecosystem
    use the entire python ecosystem at your will 
    including scipy, scikit-learn, etc.

ecosystem
    a shared model-zoo:

ecosystem
    a shared model-zoo:

linear style of programming
    pytorch is an imperative / eager computational toolkit

linear style of programming
    pytorch is an imperative / eager computational toolkit

linear style of programming
    pytorch is an imperative / eager computational toolkit 
- not unique to pytorch

linear style of programming
    pytorch is an imperative / eager computational toolkit 
- not unique to pytorch 

- chainer, dynet, mxnet-imperative, tensorflow-imperative, tensorflow-eager, etc.

linear style of programming
    pytorch is an imperative / eager computational toolkit 
- not unique to pytorch 

- chainer, dynet, mxnet-gluon, tensorflow-imperative, tensorflow-eager, etc. 

- least overhead, designed with this in mind 

- max of 20 to 30 microseconds overhead per node creation 
- vs several milliseconds / seconds in other options

the philosophy of pytorch

the philosophy of pytorch

    stay out of the way 
    cater to the impatient 
    promote linear code-   ow 
    full interop with the python ecosystem 
    be as fast as anything else

recent and upcoming features

distributed pytorch
    mpi style distributed communication 
    broadcast tensors to other nodes 
    reduce tensors among nodes 
   - for example: sum gradients among all nodes 

higher order derivatives
    grad(grad(grad(grad(grad(grad(torch.norm(x)))))))

higher order derivatives
    grad(grad(grad(grad(grad(grad(torch.norm(x))))))) 
    useful to implement crazy ideas

broadcasting and advanced indexing
    numpy-style broadcasting 
    numpy-style indexing that covers advanced cases

jit compilation (upcoming)
   a full tracing jit used to cache and compile subgraphs

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

jitfoo = torch.jit.traced(foo)

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

jitfoo = torch.jit.traced(foo)

y  = jitfoo(x)

tracing jit

trace

sum(x) -> t1

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

jitfoo = torch.jit.traced(foo)

y  = jitfoo(x)

tracing jit

trace

sum(x) -> t1

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

jitfoo = torch.jit.traced(foo)

y  = jitfoo(x)

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

trace

sum(x) -> t1

add(x, 1) -> retval

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

jitfoo = torch.jit.traced(foo)

y  = jitfoo(x)

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

trace

sum(x) -> t1

add(x, 1) -> retval

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

jitfoo = torch.jit.traced(foo)

y  = jitfoo(x)  # 0, 1, 2, 3

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

trace

sum(x) -> t1

add(x, 1) -> retval

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

jitfoo = torch.jit.traced(foo)

y  = jitfoo(x)  # 0, 1, 2, 3
y2 = jitfoo(x2)

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

trace

sum(x) -> t1

add(x, 1) -> retval

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

jitfoo = torch.jit.traced(foo)

y  = jitfoo(x)  # 0, 1, 2, 3
y2 = jitfoo(x2)

tracing jit

def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

trace

sum(x) -> t1

add(x, 1) -> retval

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 3, 4, 5, 6

jitfoo = torch.jit.traced(foo)

y  = jitfoo(x)  # 0, 1, 2, 3
y2 = jitfoo(x2) # 2, 3, 4, 5

tracing jit

@torch.jit.trace
def foo(x):
    sum = x.sum().data[0]
    if sum < 5:
     return x + 1
    else:
     return x + 2

x = torch.tensor([-1, 0, 1, 2])
x2 = torch.tensor([1, 2, 3, 4])

y  = foo(x)  # 0, 1, 2, 3
y2 = foo(x2) # 2, 3, 4, 5

compilation bene   ts

kernel fusion

out-of-order 

execution

automatic 

work placement

relu

batchnorm

conv2d

1

2

3

3

1

2

node 1 
cpu

node 0 
gpu 0

node 0 
gpu 1

node 1 
gpu 1

node 1 
gpu 0

node 0 
gpu 0

jit compilation (upcoming)
   a full tracing jit used to cache and compile subgraphs 
   fuse operations on the    y and generate optimized code code

jit compilation (upcoming)
   a full tracing jit used to cache and compile subgraphs 
   fuse operations on the    y and generate optimized code code 
   use tracer to ship models to other frameworks such as ca   e2, 
tensorflow and pure c++ runtimes

with     from

http://pytorch.org 
released jan 2017 
200,000+ downloads 
1500+ community repos 
11,500+ user posts 
290 contributors 

