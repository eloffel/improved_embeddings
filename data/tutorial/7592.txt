a framework for  
new-generation ai research

soumith chintala, adam paszke, sam gross & team 
facebook ai research

paradigm shifts in ai research

today's ai

    active research & 

future ai

tools for ai 

keeping up with change

today's ai

densecap by justin johnson & group  

https://github.com/jcjohnson/densecap

today's ai

future ai

tools for ai 

today's ai

deepmask by pedro pinhero & group

today's ai

future ai

tools for ai 

today's ai
machine translation

today's ai

future ai

tools for ai 

today's ai

text classi   cation (id31 etc.) 

text embeddings 
graph embeddings 
machine translation 

ads ranking

today's ai

future ai

tools for ai 

today's ai

train model

data

model

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

objective

today's ai

future ai

tools for ai 

today's ai

train model

data

model

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

objective

deploy & use

new 
data

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

prediction

today's ai

future ai

tools for ai 

today's ai

static datasets + static model structure
train model
objective

c
o
n
v
2
d

r
e
l
u

b
a
t
c
h
n
o
r
m

data

deploy & use

new 
data

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

prediction

today's ai

future ai

tools for ai 

today's ai

static datasets + static model structure
train model
objective

c
o
n
v
2
d

r
e
l
u

b
a
t
c
h
n
o
r
m

data

o   ine learning

deploy & use

new 
data

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

prediction

today's ai

future ai

tools for ai 

current ai research / future ai
self-driving cars

today's ai

future ai

tools for ai 

current ai research / future ai
agents trained in many environments

cars

video games

internet

today's ai

future ai

tools for ai 

current ai research / future ai
dynamic neural networks

self-adding new memory or layers 

changing evaluation path based on inputs

today's ai

future ai

tools for ai 

current ai research / future ai

live 
data

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

prediction

continued online learning

today's ai

future ai

tools for ai 

current ai research / future ai

sample-1

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

r

e

l

u

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

conv2d

batchnorm

b

atc

h

n

o
r

m

c

o

n

v

2

d

conv2d

batchnorm

relu

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

c

o

n

v

2

d

b

a

t

c

h

n

o

r

m

relu

r

e

l

u

prediction

data-dependent change in model structure

today's ai

future ai

tools for ai 

current ai research / future ai

sample-2

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

n

o
r

m

c

o

n

v

2

d

conv2d

batchnorm

relu

r

e

l

u

b

atc

h

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

r

e

l

u

b

a
t
c

h

n

o
r

m

c

conv2d

batchnorm

relu

o

n

v

2

d

prediction

data-dependent change in model structure

today's ai

future ai

tools for ai 

current ai research / future ai

sample

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

c
o
n
v
2
d

c
o
n
v
2
d

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

r
e
l
u

r
e
l
u

r
e
l
u

c
o
n
v
2
d

c
o
n
v
2
d

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

r
e
l
u

r
e
l
u

r
e
l
u

prediction

change in model-capacity at runtime

today's ai

future ai

tools for ai 

current ai research / future ai

sample

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

r
e
l
u

c
o
n
v
2
d

c
o
n
v
2
d

c
o
n
v
2
d

c
o
n
v
2
d

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

r
e
l
u

r
e
l
u

r
e
l
u

r
e
l
u

r
e
l
u

c
o
n
v
2
d

c
o
n
v
2
d

c
o
n
v
2
d

c
o
n
v
2
d

c
o
n
v
2
d

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

b
a
t
c
h
n
o
r
m

r
e
l
u

r
e
l
u

r
e
l
u

r
e
l
u

r
e
l
u

prediction

change in model-capacity at runtime

today's ai

future ai

tools for ai 

the need for a dynamic framework

   interop with many dynamic environments 
- connecting to car sensors should be as easy as training on a dataset 
- connect to environments such as openai universe 
   dynamic neural networks 
- change behavior and structure of neural network at runtime 
   minimal abstractions 
- more complex ai systems means harder to debug without a simple api

today's ai

future ai

tools for ai 

tools for ai research and deployment
many machine learning tools and deep learning frameworks

today's ai

future ai

tools for ai 

tools for ai research and deployment

static graph frameworks 

dynamic graph frameworks 

de   ne-by-run

de   ne-and-run

today's ai

future ai

tools for ai 

dynamic graph frameworks

   model is constructed on the    y at runtime 
   change behavior, structure of model 
   imperative style of programming

today's ai

future ai

tools for ai 

overview

automatic di   erentiation 

engine

ndarray library  
with gpu support

gradient based  

optimization package

deep learning

id23

numpy-alternative

ndarray library

with gpu support

ndarray library

   np.ndarray <-> torch.tensor 
   200+ operations, similar to numpy 
   very fast acceleration on nvidia gpus 

ndarray library

numpy

pytorch

ndarray / tensor library

ndarray / tensor library

ndarray / tensor library

ndarray / tensor library

numpy bridge 

numpy bridge 

zero memory-copy 

very e   cient

numpy bridge 

numpy bridge 

seaid113ss gpu tensors

seaid113ss gpu tensors

a full suite of high performance  

tensor operations on gpu

gpus are fast

   buy $700 nvidia 1080ti 
   100x faster matrix multiply 
   10x faster operations in general on matrices 

automatic di   erentiation engine

for deep learning and id23

deep learning frameworks

- provide gradient computation 
- gradient of one variable w.r.t. any variable in graph

mm

mm

add

tanh

deep learning frameworks

- provide gradient computation 
- gradient of one variable w.r.t. any variable in graph

d(i2h)/d(w_h)

mm

mm

add

tanh

deep learning frameworks

- provide gradient computation 
- gradient of one variable w.r.t. any variable in graph 
- provide integration with high 
performance dl libraries like cudnn

mm

mm

d(h2h)/d(w_h)

add

tanh

pytorch autograd
from torch.autograd import variable 

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t())

mm

mm

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h

mm

mm

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h

mm

mm

add

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh()

mm

mm

add

tanh

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh() 
next_h.backward(torch.ones(1, 20))

mm

mm

add

tanh

side by side: tensorflow and pytorch

high performance

   integration of: 
   cudnn v6 
   nccl 
   intel mkl 

   200+ operations, similar to numpy 
   very fast acceleration on nvidia gpus 

upcoming feature:   
distributed pytorch

planned feature: 
jit compilation

compilation bene   ts

kernel fusion

out-of-order 

execution

automatic 

work placement

relu

batchnorm

conv2d

1

2

3

3

1

2

node 1 
cpu

node 0 
gpu 0

node 0 
gpu 1

node 1 
gpu 1

node 1 
gpu 0

node 0 
gpu 0

jit compilation
   possible in de   ne-by-run frameworks 
   the key idea is deferred or lazy evaluation 
- y = x + 2 
- z = y * y 
- # nothing is executed yet, but the graph is being constructed 
- print(z) # now the entire graph is executed: z = (x+2) * (x+2) 
   we can do just in time compilation on the graph before execution

lazy evaluation
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh() 
next_h.backward(torch.ones(1, 20))

mm

mm

add

tanh

lazy evaluation
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh() 

add
graph built but not actually executed
tanh

mm

mm

lazy evaluation
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh() 
print(next_h)

mm

mm

add

tanh

data accessed. execute graph.

lazy evaluation
   a little bit of time between building and executing graph 
- use it to compile the graph just-in-time

mm

mm

add

tanh

jit compilation
   fuse and optimize operations

mm

mm

fuse operations. ex:

add

tanh

jit compilation
   cache subgraphs

i've seen this part of the graph before 
mm
let me pull up the compiled version 
from cache

mm

add

tanh

jit compilation
   possible in dynamic frameworks 
   the key idea is deferred or lazy evaluation 
- y = x + 2 
- z = y * y 
- # nothing is executed yet, but the graph is being constructed 
- print(z) # now the entire graph is executed: z = (x+2) * (x+2) 
   we can do just in time compilation on the graph before execution 
   we can cache repeating patterns in subsets of the graph 
- to avoid recompilation 
   compiler is very di   erent from ahead-of-time compiler 
- fast compilation 
- compile traces rather than full graph 

summary

- fast ndarray library with gpu support 
- build the latest neural networks and do gradient based 
learning using the autograd and neural network package 
- large community of people, many companies using and 
contributing 

http://pytorch.org 
released jan 18th 
58,000+ downloads 
250+ community repos 
6100+ user posts 
524k+ forum views 

with     from

you?

