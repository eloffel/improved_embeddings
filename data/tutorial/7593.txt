researcher edition
adam paszke, sam gross, soumith chintala, francisco massa, adam 
lerer, james bradbury, gregory chanan, trevor killeen, zeming lin, 
natalia gimelshein, alban desmaison, andreas kopf, edward yang, zach 
devito, martin raison, alykhan tejani, sasank chilamkurthy & team 

what is pytorch?

what is pytorch?

automatic di   erentiation 

engine

ndarray library  
with gpu support

gradient based  

optimization package

utilities 

(data loading, etc.)

deep learning

id23

numpy-alternative

ndarray library

   np.ndarray <-> torch.tensor 
   200+ operations, similar to numpy 
   very fast acceleration on nvidia gpus 

ndarray library

numpy

pytorch

ndarray / tensor library

ndarray / tensor library

ndarray / tensor library

ndarray / tensor library

numpy bridge 

numpy bridge 

zero memory-copy 

very e   cient

numpy bridge 

numpy bridge 

seaid113ss gpu tensors

automatic di   erentiation engine

for deep learning and id23

pytorch autograd
from torch.autograd import variable 

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t())

mm

mm

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h

mm

mm

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h

mm

mm

add

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh()

mm

mm

add

tanh

pytorch autograd
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh() 
next_h.backward(torch.ones(1, 20))

mm

mm

add

tanh

neural networks

optimization package
sgd, adagrad, rmsprop, lbfgs, etc.

research 
work   ows

pain points

core philosophy 

(of pytorch)

upcoming 
features

what do they look 

like in  

the deep learning space

how does pytorch  
deal with them?

research work   ows

research work   ow

idea / theory

design experiments

pick datasets /  
environments

add results  
to paper

training & 
validation

implement models

research work   ow

literature review

idea / theory

design experiments

pick datasets /  
environments

add results  
to paper

training & 
validation

implement models

research work   ow

i'm watching you

literature review

idea / theory

design experiments

pick datasets /  
environments

add results  
to paper

training & 
validation

implement models

work items in practice

writing  

dataset loaders

building models

implementing  
training loop

checkpointing 

models

interfacing with 
environments

building optimizers

dealing with  

gpus

building 
baselines

work items in practice

writing  

dataset loaders

building models

implementing  
training loop

checkpointing 

models

python + pytorch - an environment to do all of this

interfacing with 
environments

building optimizers

dealing with  

gpus

building 
baselines

writing data loaders

    every dataset is slightly di   erently formatted

writing data loaders

    every dataset is slightly di   erently formatted 
    have to be preprocessed and normalized di   erently

writing data loaders

    every dataset is slightly di   erently formatted 
    have to be preprocessed and normalized di   erently 
    need a multithreaded data loader to feed gpus fast enough

writing data loaders
pytorch solution: 
    share data loaders across the community! 

writing data loaders
pytorch solution: 
    share data loaders across the community! 

writing data loaders
pytorch solution: 
    use regular python to write datasets:  

leverage existing python code 

writing data loaders
pytorch solution: 
    use regular python to write datasets:  

leverage existing python code 
example: parlai 

writing data loaders
pytorch solution: 
   code in practice

writing data loaders
pytorch solution: 
   code in practice

research 
work   ows

pain points

core philisophy

upcoming 
features

writing data loaders
pytorch solution: 
   code in practice

research 
work   ows

pain points

core philisophy

upcoming 
features

interfacing with environments

cars

video games

internet

interfacing with environments

cars

video games

pretty much every environment provides a python api

interfacing with environments

cars
natively interact with the environment directly

video games

debugging
    pytorch is a python extension

debugging
    pytorch is a python extension 
    use your favorite python debugger

debugging
    pytorch is a python extension 
    use your favorite python debugger

debugging
    pytorch is a python extension 
    use your favorite python debugger 
    use the most popular debugger:

debugging
    pytorch is a python extension 
    use your favorite python debugger 
    use the most popular debugger: 
          print(foo)

identifying bottlenecks
    pytorch is a python extension 
    use your favorite python pro   ler

identifying bottlenecks
    pytorch is a python extension 
    use your favorite python pro   ler: line_pro   ler

compilation time
    pytorch is written for the impatient

compilation time
    pytorch is written for the impatient 
    absolutely no compilation time when writing your scripts 
whatsoever

compilation time
    pytorch is written for the impatient 
    absolutely no compilation time when writing your scripts 
whatsoever 
    all core kernels are pre-compiled

ecosystem
    use the entire python ecosystem at your will

ecosystem
    use the entire python ecosystem at your will 
    including scipy, scikit-learn, etc.

ecosystem
    use the entire python ecosystem at your will 
    including scipy, scikit-learn, etc.

ecosystem
    use the entire python ecosystem at your will 
    including scipy, scikit-learn, etc.

ecosystem
    a shared model-zoo:

ecosystem
    a shared model-zoo:

linear style of programming
    pytorch is an imperative / eager computational toolkit

linear style of programming
    pytorch is an imperative / eager computational toolkit

linear style of programming
    pytorch is an imperative / eager computational toolkit 
- not unique to pytorch

linear style of programming
    pytorch is an imperative / eager computational toolkit 
- not unique to pytorch 

- chainer, dynet, mxnet-imperative, tensorflow-imperative, tensorflow-eager, etc.

linear style of programming
    pytorch is an imperative / eager computational toolkit 
- not unique to pytorch 

- chainer, dynet, mxnet-imperative, tensorflow-imperative, tensorflow-eager, etc. 

- least overhead, designed with this in mind 

- 20 to 30 microseconds overhead per node creation 
- vs several milliseconds / seconds in other options

go through an example

the philosophy of pytorch

the philosophy of pytorch

    stay out of the way 
    cater to the impatient 
    promote linear code-   ow 
    full interop with the python ecosystem 
    be as fast as anything else

upcoming features

distributed pytorch
    mpi style distributed communication 
    broadcast tensors to other nodes 
    reduce tensors among nodes 
   - for example: sum gradients among all nodes 

higher order derivatives
    grad(grad(grad(grad(grad(grad(torch.norm(x)))))))

higher order derivatives
    grad(grad(grad(grad(grad(grad(torch.norm(x))))))) 
    useful to implement crazy ideas

broadcasting and advanced indexing
    numpy-style broadcasting 
    numpy-style indexing that covers advanced cases

jit compilation
   possible in imperative frameworks 
   the key idea is deferred or lazy evaluation 
- y = x + 2 
- z = y * y 
- # nothing is executed yet, but the graph is being constructed 
- print(z) # now the entire graph is executed: z = (x+2) * (x+2) 
   we can do just in time compilation on the graph before execution

computation 

graphs

deep learning 
frameworks

imperative 
frameworks

jit 

compilation

lazy evaluation
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh() 

mm

mm

add

tanh

computation 

graphs

deep learning 
frameworks

imperative 
frameworks

jit 

compilation

lazy evaluation
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh() 

add
graph built but not actually executed
tanh

mm

mm

computation 

graphs

deep learning 
frameworks

imperative 
frameworks

jit 

compilation

lazy evaluation
from torch.autograd import variable 
x = variable(torch.randn(1, 10)) 
prev_h = variable(torch.randn(1, 20)) 
w_h = variable(torch.randn(20, 20)) 
w_x = variable(torch.randn(20, 10)) 
i2h = torch.mm(w_x, x.t()) 
h2h = torch.mm(w_h, prev_h.t()) 
next_h = i2h + h2h 
next_h = next_h.tanh() 
print(next_h)

mm

mm

add

tanh

data accessed. execute graph.

computation 

graphs

deep learning 
frameworks

imperative 
frameworks

jit 

compilation

lazy evaluation
   a little bit of time between building and executing graph 
- use it to compile the graph just-in-time

mm

mm

add

tanh

computation 

graphs

deep learning 
frameworks

imperative 
frameworks

jit 

compilation

jit compilation
   fuse and optimize operations

mm

mm

fuse operations. ex:

add

tanh

computation 

graphs

deep learning 
frameworks

imperative 
frameworks

jit 

compilation

jit compilation
   cache subgraphs

i've seen this part of the graph before 
mm
let me pull up the compiled version 
from cache

mm

add

tanh

computation 

graphs

deep learning 
frameworks

imperative 
frameworks

jit 

compilation

compilation bene   ts

kernel fusion

out-of-order 

execution

automatic 

work placement

relu

batchnorm

conv2d

1

2

3

3

1

2

node 1 
cpu

node 0 
gpu 0

node 0 
gpu 1

node 1 
gpu 1

node 1 
gpu 0

node 0 
gpu 0

with     from

http://pytorch.org 
released jan 18th 
100,000+ downloads 
950+ community repos 
8200+ user posts 
245 contributors 

