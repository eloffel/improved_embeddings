c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

gaussian processes for machine learning

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

adaptive computation and machine learning
thomas dietterich, editor
christopher bishop, david heckerman, michael jordan, and michael kearns, associate editors

bioinformatics: the machine learning approach,
pierre baldi and s  ren brunak

id23: an introduction,
richard s. sutton and andrew g. barto

id114 for machine learning and digital communication,
brendan j. frey

learning in id114,
michael i. jordan

causation, prediction, and search, second edition,
peter spirtes, clark glymour, and richard scheines

principles of data mining,
david hand, heikki mannila, and padhraic smyth

bioinformatics: the machine learning approach, second edition,
pierre baldi and s  ren brunak

learning kernel classi   ers: theory and algorithms,
ralf herbrich

learning with kernels: support vector machines, id173, optimization, and beyond,
bernhard sch  olkopf and alexander j. smola

introduction to machine learning,
ethem alpaydin

gaussian processes for machine learning,
carl edward rasmussen and christopher k. i. williams

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

gaussian processes for machine learning

carl edward rasmussen
christopher k. i. williams

the mit press
cambridge, massachusetts
london, england

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

c(cid:13) 2006 massachusetts institute of technology

all rights reserved. no part of this book may be reproduced in any form by any electronic or mechanical
means (including photocopying, recording, or information storage and retrieval) without permission in
writing from the publisher.

mit press books may be purchased at special quantity discounts for business or sales promotional use.
for information, please email special sales@mitpress.mit.edu or write to special sales department,
the mit press, 55 hayward street, cambridge, ma 02142.

typeset by the authors using latex 2  .
this book was printed and bound in the united states of america.

library of congress cataloging-in-publication data

rasmussen, carl edward.

gaussian processes for machine learning / carl edward rasmussen, christopher k. i. williams.

p. cm.    (adaptive computation and machine learning)

includes bibliographical references and indexes.
isbn 0-262-18253-x
1. gaussian processes   data processing. 2. machine learning   mathematical models.
i. williams, christopher k. i. ii. title. iii. series.

qa274.4.r37 2006
519.2'3   dc22

10 9 8 7 6

5 4 3

2

2005053433

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

the actual science of logic is conversant at present only with things either
certain, impossible, or entirely doubtful, none of which (fortunately) we have to
reason on. therefore the true logic for this world is the calculus of probabilities,
which takes account of the magnitude of the id203 which is, or ought to
be, in a reasonable man   s mind.

    james clerk maxwell [1850]

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

contents

series foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xi
preface
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii
symbols and notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii

1 introduction

1.1 a pictorial introduction to bayesian modelling . . . . . . . . . . . . . . .
1.2 roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 regression

2.1 weight-space view . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.1 the standard linear model . . . . . . . . . . . . . . . . . . . . . .
2.1.2 projections of inputs into feature space . . . . . . . . . . . . . . .
2.2 function-space view . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 varying the hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 decision theory for regression . . . . . . . . . . . . . . . . . . . . . . . .
2.5 an example application . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.6 smoothing, weight functions and equivalent kernels
. . . . . . . . . . .
incorporating explicit basis functions . . . . . . . . . . . . . . . . . . . .
2.7.1 marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8 history and related work . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9 exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

    2.7

1
3
5

7
7
8
11
13
19
21
22
24
27
29
29
30

3 classi   cation

3.1 classi   cation problems

33
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
3.1.1 decision theory for classi   cation . . . . . . . . . . . . . . . . . .
35
3.2 linear models for classi   cation . . . . . . . . . . . . . . . . . . . . . . . .
37
3.3 gaussian process classi   cation . . . . . . . . . . . . . . . . . . . . . . . .
39
3.4 the laplace approximation for the binary gp classi   er . . . . . . . . . .
41
3.4.1 posterior
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.4.2 predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.4.3
implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.4.4 marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . . .
47
    3.5 multi-class laplace approximation . . . . . . . . . . . . . . . . . . . . . .
48
implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.6 expectation propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.6.1 predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.6.2 marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.6.3
implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.7 experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.7.1 a toy problem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.7.2 one-dimensional example
. . . . . . . . . . . . . . . . . . . . . .
62
3.7.3 binary handwritten digit classi   cation example . . . . . . . . . .
63
3.7.4
10-class handwritten digit classi   cation example . . . . . . . . .
70
3.8 discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
   sections marked by an asterisk contain advanced material that may be omitted on a    rst reading.

3.5.1

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

viii

contents

    3.9 appendix: moment derivations . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.10 exercises

74
75

4.1 preliminaries

4 covariance functions
   

4.2 examples of covariance functions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.1 mean square continuity and di   erentiability . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
4.2.1
stationary covariance functions . . . . . . . . . . . . . . . . . . .
4.2.2 dot product covariance functions . . . . . . . . . . . . . . . . . .
4.2.3 other non-stationary covariance functions . . . . . . . . . . . . .
4.2.4 making new kernels from old . . . . . . . . . . . . . . . . . . . .
4.3 eigenfunction analysis of kernels . . . . . . . . . . . . . . . . . . . . . . .
4.3.1 an analytic example . . . . . . . . . . . . . . . . . . . . . . . . .
4.3.2 numerical approximation of eigenfunctions . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .

79
79
81
81
82
89
90
94
96
97
98
99
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

string kernels
4.4.1
4.4.2 fisher kernels

4.4 kernels for non-vectorial inputs

4.5 exercises

   

5 model selection and adaptation of hyperparameters

105
5.1 the model selection problem . . . . . . . . . . . . . . . . . . . . . . . . . 106
5.2 bayesian model selection . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
5.3 cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.4 model selection for gp regression . . . . . . . . . . . . . . . . . . . . . . 112
5.4.1 marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . 112
5.4.2 cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.4.3 examples and discussion . . . . . . . . . . . . . . . . . . . . . . . 118
5.5 model selection for gp classi   cation . . . . . . . . . . . . . . . . . . . . . 124
5.5.1 derivatives of the marginal likelihood for laplace   s approximation 125
5.5.2 derivatives of the marginal likelihood for ep . . . . . . . . . . . . 127
5.5.3 cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.5.4 example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128

5.6 exercises

   
   

6 relationships between gps and other models

129
6.1 reproducing kernel hilbert spaces . . . . . . . . . . . . . . . . . . . . . . 129
6.2 id173 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
6.2.1 id173 de   ned by di   erential operators . . . . . . . . . . 133
6.2.2 obtaining the regularized solution . . . . . . . . . . . . . . . . . . 135
6.2.3 the relationship of the id173 view to gaussian process

   

   
    6.4 support vector machines

prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
6.3 spline models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
6.3.1 a 1-d gaussian process spline construction . . . . . . . . . . . . . 138
. . . . . . . . . . . . . . . . . . . . . . . . . . . 141
support vector classi   cation . . . . . . . . . . . . . . . . . . . . . 141
support vector regression . . . . . . . . . . . . . . . . . . . . . . 145
    6.5 least-squares classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . . 146
6.5.1 probabilistic least-squares classi   cation . . . . . . . . . . . . . . . 147

6.4.1
6.4.2

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

contents

ix

    6.6 relevance vector machines

6.7 exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . 149
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150

7.1.1

some speci   c examples of equivalent kernels

7 theoretical perspectives
7.1 the equivalent kernel

151
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
. . . . . . . . . . . 153
    7.2 asymptotic analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
7.2.1 consistency
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
7.2.2 equivalence and orthogonality . . . . . . . . . . . . . . . . . . . . 157
    7.3 average-case learning curves . . . . . . . . . . . . . . . . . . . . . . . . . 159
    7.4 pac-bayesian analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
7.4.1 the pac framework . . . . . . . . . . . . . . . . . . . . . . . . . . 162
7.4.2 pac-bayesian analysis
. . . . . . . . . . . . . . . . . . . . . . . . 163
7.4.3 pac-bayesian analysis of gp classi   cation . . . . . . . . . . . . . 164
7.5 comparison with other supervised learning methods . . . . . . . . . . . 165
    7.6 appendix: learning curve for the ornstein-uhlenbeck process . . . . . . 168
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169

7.7 exercises

8 approximation methods for large datasets

171
8.1 reduced-rank approximations of the gram matrix . . . . . . . . . . . . . 171
8.2 greedy approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
8.3 approximations for gpr with fixed hyperparameters . . . . . . . . . . . 175
8.3.1
subset of regressors . . . . . . . . . . . . . . . . . . . . . . . . . . 175
8.3.2 the nystr  om method . . . . . . . . . . . . . . . . . . . . . . . . . 177
8.3.3
. . . . . . . . . . . . . . . . . . . . . . . . . 177
8.3.4 projected process approximation . . . . . . . . . . . . . . . . . . . 178
8.3.5 bayesian committee machine . . . . . . . . . . . . . . . . . . . . . 180
8.3.6
. . . . . . . . . . . . . . . . . 181
8.3.7 comparison of approximate gpr methods . . . . . . . . . . . . . 182
8.4 approximations for gpc with fixed hyperparameters . . . . . . . . . . . 185
. . . . . . . . 185

    8.5 approximating the marginal likelihood and its derivatives
    8.6 appendix: equivalence of sr and gpr using the nystr  om approximate

iterative solution of linear systems

subset of datapoints

kernel

8.7 exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187

9 further issues and conclusions

189
9.1 multiple outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
9.2 noise models with dependencies . . . . . . . . . . . . . . . . . . . . . . . 190
9.3 non-gaussian likelihoods . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
9.4 derivative observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
9.5 prediction with uncertain inputs . . . . . . . . . . . . . . . . . . . . . . . 192
9.6 mixtures of gaussian processes . . . . . . . . . . . . . . . . . . . . . . . . 192
9.7 global optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
9.8 evaluation of integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
9.9 student   s t process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
9.10 invariances
9.11 latent variable models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
9.12 conclusions and future directions . . . . . . . . . . . . . . . . . . . . . . 196

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

x

contents

appendix a mathematical background

199
a.1 joint, marginal and id155 . . . . . . . . . . . . . . . . . 199
a.2 gaussian identities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
a.3 matrix identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
a.3.1 matrix derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
a.3.2 matrix norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
a.4 cholesky decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
a.5 id178 and id181 . . . . . . . . . . . . . . . . . . 203
a.6 limits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
a.7 measure and integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
a.7.1 lp spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
a.8 fourier transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
a.9 convexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206

appendix b gaussian markov processes

b.1 fourier analysis

b.2 continuous-time gaussian markov processes

207
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
b.1.1 sampling and periodization . . . . . . . . . . . . . . . . . . . . . . 209
. . . . . . . . . . . . . . . . 211
b.2.1 continuous-time gmps on r . . . . . . . . . . . . . . . . . . . . . 211
b.2.2 the solution of the corresponding sde on the circle . . . . . . . 213
. . . . . . . . . . . . . . . . . . 214
b.3.1 discrete-time gmps on z . . . . . . . . . . . . . . . . . . . . . . . 214
b.3.2 the solution of the corresponding di   erence equation on pn . . 215

b.3 discrete-time gaussian markov processes

b.4 the relationship between discrete-time and sampled continuous-time

gmps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
b.5 markov processes in higher dimensions . . . . . . . . . . . . . . . . . . . 218

appendix c datasets and code

bibliography

author index

subject index

221

223

239

245

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

series foreword

the goal of building systems that can adapt to their environments and learn
from their experience has attracted researchers from many    elds, including com-
puter science, engineering, mathematics, physics, neuroscience, and cognitive
science. out of this research has come a wide variety of learning techniques that
have the potential to transform many scienti   c and industrial    elds. recently,
several research communities have converged on a common set of issues sur-
rounding supervised, unsupervised, and id23 problems. the
mit press series on adaptive computation and machine learning seeks to
unify the many diverse strands of machine learning research and to foster high
quality research and innovative applications.

one of the most active directions in machine learning has been the de-
velopment of practical bayesian methods for challenging learning problems.
gaussian processes for machine learning presents one of the most important
bayesian machine learning approaches based on a particularly e   ective method
for placing a prior distribution over the space of functions. carl edward ras-
mussen and chris williams are two of the pioneers in this area, and their book
describes the mathematical foundations and practical application of gaussian
processes in regression and classi   cation tasks. they also show how gaussian
processes can be interpreted as a bayesian version of the well-known support
vector machine methods. students and researchers who study this book will be
able to apply gaussian process methods in creative ways to solve a wide range
of problems in science and engineering.

thomas dietterich

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

preface

over the last decade there has been an explosion of work in the    kernel ma-
chines    area of machine learning. probably the best known example of this is
work on support vector machines, but during this period there has also been
much activity concerning the application of gaussian process models to ma-
chine learning tasks. the goal of this book is to provide a systematic and uni-
   ed treatment of this area. gaussian processes provide a principled, practical,
probabilistic approach to learning in kernel machines. this gives advantages
with respect to the interpretation of model predictions and provides a well-
founded framework for learning and model selection. theoretical and practical
developments of over the last decade have made gaussian processes a serious
competitor for real supervised learning applications.

roughly speaking a stochastic process is a generalization of a id203
distribution (which describes a    nite-dimensional random variable) to func-
tions. by focussing on processes which are gaussian, it turns out that the
computations required for id136 and learning become relatively easy. thus,
the supervised learning problems in machine learning which can be thought of
as learning a function from examples can be cast directly into the gaussian
process framework.

our interest in gaussian process (gp) models in the context of machine
learning was aroused in 1994, while we were both graduate students in geo   
hinton   s neural networks lab at the university of toronto. this was a time
when the    eld of neural networks was becoming mature and the many con-
nections to statistical physics, probabilistic models and statistics became well
known, and the    rst kernel-based learning algorithms were becoming popular.
in retrospect it is clear that the time was ripe for the application of gaussian
processes to machine learning problems.

many researchers were realizing that neural networks were not so easy to
apply in practice, due to the many decisions which needed to be made: what
architecture, what id180, what learning rate, etc., and the lack of
a principled framework to answer these questions. the probabilistic framework
was pursued using approximations by mackay [1992b] and using markov chain
monte carlo (mcmc) methods by neal [1996]. neal was also a graduate stu-
dent in the same lab, and in his thesis he sought to demonstrate that using the
bayesian formalism, one does not necessarily have problems with    over   tting   
when the models get large, and one should pursue the limit of large models.
while his own work was focused on sophisticated markov chain methods for
id136 in large    nite networks, he did point out that some of his networks
became gaussian processes in the limit of in   nite size, and    there may be sim-
pler ways to do id136 in this case.   

it is perhaps interesting to mention a slightly wider historical perspective.
the main reason why neural networks became popular was that they allowed
the use of adaptive basis functions, as opposed to the well known linear models.
the adaptive basis functions, or hidden units, could    learn    hidden features

kernel machines

gaussian process

gaussian processes
in machine learning

neural networks

large neural networks
    gaussian processes

adaptive basis functions

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

xiv

many    xed basis
functions

useful representations

supervised learning
in statistics

statistics and
machine learning

data and models

algorithms and
predictions

bridging the gap

preface

useful for the modelling problem at hand. however, this adaptivity came at the
cost of a lot of practical problems. later, with the advancement of the    kernel
era   , it was realized that the limitation of    xed basis functions is not a big
restriction if only one has enough of them, i.e. typically in   nitely many, and
one is careful to control problems of over   tting by using priors or id173.
the resulting models are much easier to handle than the adaptive basis function
models, but have similar expressive power.

thus, one could claim that (as far a machine learning is concerned) the
adaptive basis functions were merely a decade-long digression, and we are now
back to where we came from. this view is perhaps reasonable if we think of
models for solving practical learning problems, although mackay [2003, ch. 45],
for example, raises concerns by asking    did we throw out the baby with the bath
water?   , as the kernel view does not give us any hidden representations, telling
us what the useful features are for solving a particular problem. as we will
argue in the book, one answer may be to learn more sophisticated covariance
functions, and the    hidden    properties of the problem are to be found here.
an important area of future developments for gp models is the use of more
expressive covariance functions.

supervised learning problems have been studied for more than a century
in statistics, and a large body of well-established theory has been developed.
more recently, with the advance of a   ordable, fast computation, the machine
learning community has addressed increasingly large and complex problems.

much of the basic theory and many algorithms are shared between the
statistics and machine learning community. the primary di   erences are perhaps
the types of the problems attacked, and the goal of learning. at the risk of
oversimpli   cation, one could say that in statistics a prime focus is often in
understanding the data and relationships in terms of models giving approximate
summaries such as linear relations or independencies. in contrast, the goals in
machine learning are primarily to make predictions as accurately as possible and
to understand the behaviour of learning algorithms. these di   ering objectives
have led to di   erent developments in the two    elds: for example, neural network
algorithms have been used extensively as black-box function approximators in
machine learning, but to many statisticians they are less than satisfactory,
because of the di   culties in interpreting such models.

gaussian process models in some sense bring together work in the two com-
munities. as we will see, gaussian processes are mathematically equivalent to
many well known models, including bayesian linear models, spline models, large
neural networks (under suitable conditions), and are closely related to others,
such as support vector machines. under the gaussian process viewpoint, the
models may be easier to handle and interpret than their conventional coun-
terparts, such as e.g. neural networks. in the statistics community gaussian
processes have also been discussed many times, although it would probably be
excessive to claim that their use is widespread except for certain speci   c appli-
cations such as spatial models in meteorology and geology, and the analysis of
computer experiments. a rich theory also exists for gaussian process models

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

preface

in the time series analysis literature; some pointers to this literature are given
in appendix b.

the book is primarily intended for graduate students and researchers in
machine learning at departments of computer science, statistics and applied
mathematics. as prerequisites we require a good basic grounding in calculus,
id202 and id203 theory as would be obtained by graduates in nu-
merate disciplines such as electrical engineering, physics and computer science.
for preparation in calculus and id202 any good university-level text-
book on mathematics for physics or engineering such as arfken [1985] would
be    ne. for id203 theory some familiarity with multivariate distributions
(especially the gaussian) and id155 is required. some back-
ground mathematical material is also provided in appendix a.

the main focus of the book is to present clearly and concisely an overview
of the main ideas of gaussian processes in a machine learning context. we have
also covered a wide range of connections to existing models in the literature,
and cover approximate id136 for faster practical algorithms. we have pre-
sented detailed algorithms for many methods to aid the practitioner. software
implementations are available from the website for the book, see appendix c.
we have also included a small set of exercises in each chapter; we hope these
will help in gaining a deeper understanding of the material.

in order limit the size of the volume, we have had to omit some topics, such
as, for example, id115 methods for id136. one of the
most di   cult things to decide when writing a book is what sections not to write.
within sections, we have often chosen to describe one algorithm in particular
in depth, and mention related work only in passing. although this causes the
omission of some material, we feel it is the best approach for a monograph, and
hope that the reader will gain a general understanding so as to be able to push
further into the growing literature of gp models.

the book has a natural split into two parts, with the chapters up to and
including chapter 5 covering core material, and the remaining sections covering
the connections to other methods, fast approximations, and more specialized
properties. some sections are marked by an asterisk. these sections may be
omitted on a    rst reading, and are not pre-requisites for later (un-starred)
material.

we wish to express our considerable gratitude to the many people with
whom we have interacted during the writing of this book. in particular moray
allan, david barber, peter bartlett, miguel carreira-perpi  n  an, marcus gal-
lagher, manfred opper, anton schwaighofer, matthias seeger, hanna wallach,
joe whittaker, and andrew zisserman all read parts of the book and provided
valuable feedback. dilan g  or  ur, malte kuss, iain murray, joaquin qui  nonero-
candela, leif rasmussen and sam roweis were especially heroic and provided
comments on the whole manuscript. we thank chris bishop, miguel carreira-
perpi  n  an, nando de freitas, zoubin ghahramani, peter gr  unwald, mike jor-
dan, john kent, radford neal, joaquin qui  nonero-candela, ryan rifkin, ste-
fan schaal, anton schwaighofer, matthias seeger, peter sollich, ingo steinwart,

xv

intended audience

focus

scope

book organization

   

acknowledgements

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

xvi

preface

errata

looking ahead

amos storkey, volker tresp, sethu vijayakumar, grace wahba, joe whittaker
and tong zhang for valuable discussions on speci   c issues. we also thank bob
prior and the sta    at mit press for their support during the writing of the
book. we thank the gatsby computational neuroscience unit (ucl) and neil
lawrence at the department of computer science, university of she   eld for
hosting our visits and kindly providing space for us to work, and the depart-
ment of computer science at the university of toronto for computer support.
thanks to john and fiona for their hospitality on numerous occasions. some
of the diagrams in this book have been inspired by similar diagrams appearing
in published work, as follows: figure 3.5, sch  olkopf and smola [2002]; fig-
ure 5.2, mackay [1992b]. cer gratefully acknowledges    nancial support from
the german research foundation (dfg). ckiw thanks the school of infor-
matics, university of edinburgh for granting him sabbatical leave for the period
october 2003-march 2004.

finally, we reserve our deepest appreciation for our wives agnes and bar-
bara, and children ezra, kate, miro and ruth for their patience and under-
standing while the book was being written.

despite our best e   orts it is inevitable that some errors will make it through
to the printed version of the book. errata will be made available via the book   s
website at

http://www.gaussianprocess.org/gpml

we have found the joint writing of this book an excellent experience. although
hard at times, we are con   dent that the end result is much better than either
one of us could have written alone.

now, ten years after their    rst introduction into the machine learning com-
munity, gaussian processes are receiving growing attention. although gps
have been known for a long time in the statistics and geostatistics    elds, and
their use can perhaps be traced back as far as the end of the 19th century, their
application to real problems is still in its early phases. this contrasts somewhat
the application of the non-probabilistic analogue of the gp, the support vec-
tor machine, which was taken up more quickly by practitioners. perhaps this
has to do with the probabilistic mind-set needed to understand gps, which is
not so generally appreciated. perhaps it is due to the need for computational
short-cuts to implement id136 for large datasets. or it could be due to the
lack of a self-contained introduction to this exciting    eld   with this volume, we
hope to contribute to the momentum gained by gaussian processes in machine
learning.

carl edward rasmussen and chris williams
t  ubingen and edinburgh, summer 2005

second printing: we thank baback moghaddam, mikhail parakhin, leif ras-
mussen, benjamin sobotta, kevin s. van horn and aki vehtari for reporting
errors in the    rst printing which have now been corrected.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

symbols and notation

matrices are capitalized and vectors are in bold type. we do not generally distinguish between proba-
bilities and id203 densities. a subscript asterisk, such as in x   , indicates reference to a test set
quantity. a superscript asterisk denotes complex conjugate.

symbol
\
,
c=
|k|
|y|
hf, gih
kfkh
y>
   
   
    or    f
      
0 or 0n
1 or 1n
c
cholesky(a)
cov(f   )
d
d
diag(w)
diag(w )
  pq
e or eq(x)[z(x)]
f(x) or f
f   
  f   
gp

h(x) or h(x)
h or h(x)
i or in
j  (z)
k(x, x0)
k or k(x, x)
k   
k(x   ) or k   
kf or k

meaning
left matrix divide: a\b is the vector x which solves ax = b
an equality which acts as a de   nition
equality up to an additive constant
determinant of k matrix

euclidean length of vector y, i.e.(cid:0)p

(cid:1)1/2

i y2
i

rkhs inner product
rkhs norm
the transpose of vector y
proportional to; e.g. p(x|y)     f(x, y) means that p(x|y) is equal to f(x, y) times
a factor which is independent of x
distributed according to; example: x     n (  ,   2)
partial derivatives (w.r.t. f)
the (hessian) matrix of second derivatives
vector of all 0   s (of length n)
vector of all 1   s (of length n)
number of classes in a classi   cation problem
cholesky decomposition: l is a lower triangular matrix such that ll> = a
gaussian process posterior covariance
dimension of input space x
data set: d = {(xi, yi)|i = 1, . . . , n}
(vector argument) a diagonal matrix containing the elements of vector w
(matrix argument) a vector containing the diagonal elements of matrix w
kronecker delta,   pq = 1 i    p = q and 0 otherwise
expectation; expectation of z(x) when x     q(x)
gaussian process (or vector of) latent function values, f = (f(x1), . . . , f(xn))>
gaussian process (posterior) prediction (random variable)
gaussian process posterior mean
gaussian process with mean function m(x) and covariance function k(x, x0)
either    xed basis function (or set of basis functions) or weight function
set of basis functions evaluated at all training points
the identity matrix (of size n)
bessel function of the    rst kind
covariance (or kernel) function evaluated at x and x0
n    n covariance (or gram) matrix
n    n    matrix k(x, x   ), the covariance between training and test cases
vector, short for k(x, x   ), when there is only a single test case
covariance matrix for the (noise free) f values

gaussian process: f     gp(cid:0)m(x), k(x, x0)(cid:1), the function f is distributed as a

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

xviii

symbol

ky

k  (z)
l(a, b)
log(z)
log2(z)
    or    d
  (z)
m(x)
  
n (  ,   ) or n (x|  ,   )
n (x)
n and n   
n
nh
n
o(  )

o
y|x and p(y|x)
pn
  (xi) or   (x)
  (z)
  (x)
    (x   )
    (x   )
r
rl(f) or rl(c)
  rl(l|x   )
rc
s(s)
  (z)
  2
f
  2
n
  
tr(a)
tl
v or vq(x)[z(x)]
x
x
x   
xi
xdi
z

symbols and notation

meaning
covariance matrix for the (noisy) y values; for independent homoscedastic noise,
ky = kf +   2
ni
modi   ed bessel function
id168, the loss of predicting b, when a is true; note argument order
natural logarithm (base e)
logarithm to the base 2
characteristic length-scale (for input dimension d)

logistic function,   (z) = 1/(cid:0)1 + exp(   z)(cid:1)

the mean function of a gaussian process
a measure (see section a.7)
(the variable x has a) gaussian (normal) distribution with mean vector    and
covariance matrix   
short for unit gaussian x     n (0, i)
number of training (and test) cases
dimension of feature space
number of hidden units in a neural network
the natural numbers, the positive integers
big oh; for functions f and g on n, we write f(n) = o(g(n)) if the ratio
f(n)/g(n) remains bounded as n        
either matrix of all zeros or di   erential operator
conditional random variable y given x and its id203 (density)
the regular n-polygon
feature map of input xi (or input set x)

cumulative unit gaussian:   (z) = (2  )   1/2r z

       exp(   t2/2)dt

the sigmoid of the latent value:   (x) =   (f(x)) (stochastic if f(x) is stochastic)
map prediction:    evaluated at   f(x   ).
mean prediction: expected value of   (x   ). note, in general that     (x   ) 6=     (x   )
the real numbers
the risk or expected loss for f, or classi   er c (averaged w.r.t. inputs and outputs)
expected loss for predicting l, averaged w.r.t. the model   s pred. distr. at x   
decision region for class c
power spectrum
any sigmoid function, e.g. logistic   (z), cumulative gaussian   (z), etc.
variance of the (noise free) signal
noise variance
vector of hyperparameters (parameters of the covariance function)
trace of (square) matrix a
the circle with circumference l
variance; variance of z(x) when x     q(x)
input space and also the index set for the stochastic process
d    n matrix of the training inputs {xi}n
matrix of test inputs
the ith training input
the dth coordinate of the ith training input xi
the integers . . . ,   2,    1, 0, 1, 2, . . .

i=1: the design matrix

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

chapter 1

introduction

in this book we will be concerned with supervised learning, which is the problem
of learning input-output mappings from empirical data (the training dataset).
depending on the characteristics of the output, this problem is known as either
regression, for continuous outputs, or classi   cation, when outputs are discrete.
a well known example is the classi   cation of images of handwritten digits.
the training set consists of small digitized images, together with a classi   cation
from 0, . . . , 9, normally provided by a human. the goal is to learn a mapping
from image to classi   cation label, which can then be used on new, unseen
images. supervised learning is an attractive way to attempt to tackle this
problem, since it is not easy to specify accurately the characteristics of, say, the
handwritten digit 4.

an example of a regression problem can be found in robotics, where we wish
to learn the inverse dynamics of a robot arm. here the task is to map from
the state of the arm (given by the positions, velocities and accelerations of the
joints) to the corresponding torques on the joints. such a model can then be
used to compute the torques needed to move the arm along a given trajectory.
another example would be in a chemical plant, where we might wish to predict
the yield as a function of process parameters such as temperature, pressure,
amount of catalyst etc.

in general we denote the input as x, and the output (or target) as y. the
input is usually represented as a vector x as there are in general many input
variables   in the handwritten digit recognition example one may have a 256-
dimensional input obtained from a raster scan of a 16    16 image, and in the
robot arm example there are three input measurements for each joint in the
arm. the target y may either be continuous (as in the regression case) or
discrete (as in the classi   cation case). we have a dataset d of n observations,
d = {(xi, yi)|i = 1, . . . , n}.

given this training data we wish to make predictions for new inputs x   
that we have not seen in the training set. thus it is clear that the problem
at hand is inductive; we need to move from the    nite training data d to a

digit classi   cation

robotic control

the dataset

training is inductive

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2

introduction

two approaches

gaussian process

consistency

tractability

function f that makes predictions for all possible input values. to do this we
must make assumptions about the characteristics of the underlying function,
as otherwise any function which is consistent with the training data would be
equally valid. a wide variety of methods have been proposed to deal with the
supervised learning problem; here we describe two common approaches. the
   rst is to restrict the class of functions that we consider, for example by only
considering linear functions of the input. the second approach is (speaking
rather loosely) to give a prior id203 to every possible function, where
higher probabilities are given to functions that we consider to be more likely, for
example because they are smoother than other functions.1 the    rst approach
has an obvious problem in that we have to decide upon the richness of the class
of functions considered; if we are using a model based on a certain class of
functions (e.g. linear functions) and the target function is not well modelled by
this class, then the predictions will be poor. one may be tempted to increase the
   exibility of the class of functions, but this runs into the danger of over   tting,
where we can obtain a good    t to the training data, but perform badly when
making test predictions.

the second approach appears to have a serious problem, in that surely
there are an uncountably in   nite set of possible functions, and how are we
going to compute with this set in    nite time? this is where the gaussian
process comes to our rescue. a gaussian process is a generalization of the
gaussian id203 distribution. whereas a id203 distribution describes
random variables which are scalars or vectors (for multivariate distributions),
a stochastic process governs the properties of functions. leaving mathematical
sophistication aside, one can loosely think of a function as a very long vector,
each entry in the vector specifying the function value f(x) at a particular input
x. it turns out, that although this idea is a little na    ve, it is surprisingly close
what we need. indeed, the question of how we deal computationally with these
in   nite dimensional objects has the most pleasant resolution imaginable: if you
ask only for the properties of the function at a    nite number of points, then
id136 in the gaussian process will give you the same answer if you ignore the
in   nitely many other points, as if you would have taken them all into account!
and these answers are consistent with answers to any other    nite queries you
may have. one of the main attractions of the gaussian process framework is
precisely that it unites a sophisticated and consistent view with computational
tractability.

it should come as no surprise that these ideas have been around for some
time, although they are perhaps not as well known as they might be. indeed,
many models that are commonly employed in both machine learning and statis-
tics are in fact special cases of, or restricted kinds of gaussian processes. in this
volume, we aim to give a systematic and uni   ed treatment of the area, showing
connections to related models.

1these two approaches may be regarded as imposing a restriction bias and a preference

bias respectively; see e.g. mitchell [1997].

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

1.1 a pictorial introduction to bayesian modelling

3

(a), prior

(b), posterior

figure 1.1: panel (a) shows four samples drawn from the prior distribution. panel
(b) shows the situation after two datapoints have been observed. the mean prediction
is shown as the solid line and four samples from the posterior are shown as dashed
lines. in both plots the shaded region denotes twice the standard deviation at each
input value x.

1.1 a pictorial introduction to bayesian mod-

elling

in this section we give graphical illustrations of how the second (bayesian)
method works on some simple regression and classi   cation examples.

we    rst consider a simple 1-d regression problem, mapping from an input
x to an output f(x). in figure 1.1(a) we show a number of sample functions
drawn at random from the prior distribution over functions speci   ed by a par-
ticular gaussian process which favours smooth functions. this prior is taken
to represent our prior beliefs over the kinds of functions we expect to observe,
before seeing any data. in the absence of knowledge to the contrary we have
assumed that the average value over the sample functions at each x is zero.
although the speci   c random functions drawn in figure 1.1(a) do not have a
mean of zero, the mean of f(x) values for any    xed x would become zero, in-
dependent of x as we kept on drawing more functions. at any value of x we
can also characterize the variability of the sample functions by computing the
variance at that point. the shaded region denotes twice the pointwise standard
deviation; in this case we used a gaussian process which speci   es that the prior
variance does not depend on x.

suppose that we are then given a dataset d = {(x1, y1), (x2, y2)} consist-
ing of two observations, and we wish now to only consider functions that pass
though these two data points exactly. (it is also possible to give higher pref-
erence to functions that merely pass    close    to the datapoints.) this situation
is illustrated in figure 1.1(b). the dashed lines show sample functions which
are consistent with d, and the solid line depicts the mean value of such func-
tions. notice how the uncertainty is reduced close to the observations. the
combination of the prior and the data leads to the posterior distribution over
functions.

regression

random functions

mean function

pointwise variance

functions that agree
with observations

posterior over functions

00.51   2   1012input, xf(x)00.51   2   1012input, xf(x)c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4

non-parametric

id136

prior speci   cation

covariance function

modelling and
interpreting

classi   cation

squashing function

introduction

if more datapoints were added one would see the mean function adjust itself
to pass through these points, and that the posterior uncertainty would reduce
close to the observations. notice, that since the gaussian process is not a
parametric model, we do not have to worry about whether it is possible for the
model to    t the data (as would be the case if e.g. you tried a linear model on
strongly non-linear data). even when a lot of observations have been added,
there may still be some    exibility left in the functions. one way to imagine the
reduction of    exibility in the distribution of functions as the data arrives is to
draw many random functions from the prior, and reject the ones which do not
agree with the observations. while this is a perfectly valid way to do id136,
it is impractical for most purposes   the exact analytical computations required
to quantify these properties will be detailed in the next chapter.

the speci   cation of the prior is important, because it    xes the properties of
the functions considered for id136. above we brie   y touched on the mean
and pointwise variance of the functions. however, other characteristics can also
be speci   ed and manipulated. note that the functions in figure 1.1(a) are
smooth and stationary (informally, stationarity means that the functions look
similar at all x locations). these are properties which are induced by the co-
variance function of the gaussian process; many other covariance functions are
possible. suppose, that for a particular application, we think that the functions
in figure 1.1(a) vary too rapidly (i.e. that their characteristic length-scale is
too short). slower variation is achieved by simply adjusting parameters of the
covariance function. the problem of learning in gaussian processes is exactly
the problem of    nding suitable properties for the covariance function. note,
that this gives us a model of the data, and characteristics (such a smoothness,
characteristic length-scale, etc.) which we can interpret.

we now turn to the classi   cation case, and consider the binary (or two-
class) classi   cation problem. an example of this is classifying objects detected
in astronomical sky surveys into stars or galaxies. our data has the label +1 for
stars and    1 for galaxies, and our task will be to predict   (x), the id203
that an example with input vector x is a star, using as inputs some features
that describe each object. obviously   (x) should lie in the interval [0, 1]. a
gaussian process prior over functions does not restrict the output to lie in this
interval, as can be seen from figure 1.1(a). the approach that we shall adopt
is to squash the prior function f pointwise through a response function which
restricts the output to lie in [0, 1]. a common choice for this function is the
logistic function   (z) = (1 + exp(   z))   1, illustrated in figure 1.2(b). thus the
prior over f induces a prior over probabilistic classi   cations   .

this set up is illustrated in figure 1.2 for a 2-d input space.

in panel
(a) we see a sample drawn from the prior over functions f which is squashed
through the logistic function (panel (b)). a dataset is shown in panel (c), where
the white and black circles denote classes +1 and    1 respectively. as in the
regression case the e   ect of the data is to downweight in the posterior those
functions that are incompatible with the data. a contour plot of the posterior
mean for   (x) is shown in panel (d). in this example we have chosen a short
characteristic length-scale for the process so that it can vary fairly rapidly; in

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

1.2 roadmap

5

(a)

(b)

(c)

(d)

figure 1.2: panel (a) shows a sample from prior distribution on f in a 2-d input
space. panel (b) is a plot of the logistic function   (z). panel (c) shows the location
of the data points, where the open circles denote the class label +1, and closed circles
denote the class label    1. panel (d) shows a contour plot of the mean predictive
id203 as a function of x; the decision boundaries between the two classes are
shown by the thicker lines.

this case notice that all of the training points are correctly classi   ed, including
the two    outliers    in the ne and sw corners. by choosing a di   erent length-
scale we can change this behaviour, as illustrated in section 3.7.1.

1.2 roadmap

the book has a natural split into two parts, with the chapters up to and includ-
ing chapter 5 covering core material, and the remaining chapters covering the
connections to other methods, fast approximations, and more specialized prop-
erties. some sections are marked by an asterisk. these sections may be omitted
on a    rst reading, and are not pre-requisites for later (un-starred) material.

   50501logistic function(cid:176)(cid:176)(cid:176)   (cid:176)(cid:176)(cid:176)(cid:176)(cid:176)   (cid:176)                  (cid:176)      0.250.50.50.50.750.25c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6

regression

classi   cation

covariance functions

learning

connections

theory

fast approximations

introduction

chapter 2 contains the de   nition of gaussian processes, in particular for the
use in regression. it also discusses the computations needed to make predic-
tions for regression. under the assumption of gaussian observation noise the
computations needed to make predictions are tractable and are dominated by
the inversion of a n    n matrix. in a short experimental section, the gaussian
process model is applied to a robotics task.

chapter 3 considers the classi   cation problem for both binary and multi-
class cases. the use of a non-linear response function means that exact compu-
tation of the predictions is no longer possible analytically. we discuss a number
of approximation schemes, include detailed algorithms for their implementation
and discuss some experimental comparisons.

as discussed above, the key factor that controls the properties of a gaussian
process is the covariance function. much of the work on machine learning so far,
has used a very limited set of covariance functions, possibly limiting the power
of the resulting models. in chapter 4 we discuss a number of valid covariance
functions and their properties and provide some guidelines on how to combine
covariance functions into new ones, tailored to speci   c needs.

many covariance functions have adjustable parameters, such as the char-
acteristic length-scale and variance illustrated in figure 1.1. chapter 5 de-
scribes how such parameters can be inferred or learned from the data, based on
either bayesian methods (using the marginal likelihood) or methods of cross-
validation. explicit algorithms are provided for some schemes, and some simple
practical examples are demonstrated.

gaussian process predictors are an example of a class of methods known as
kernel machines; they are distinguished by the probabilistic viewpoint taken.
in chapter 6 we discuss other kernel machines such as support vector machines
(id166s), splines, least-squares classi   ers and relevance vector machines (rvms),
and their relationships to gaussian process prediction.

in chapter 7 we discuss a number of more theoretical issues relating to
gaussian process methods including asymptotic analysis, average-case learning
curves and the pac-bayesian framework.

one issue with gaussian process prediction methods is that their basic com-
plexity is o(n3), due to the inversion of a n  n matrix. for large datasets this is
prohibitive (in both time and space) and so a number of approximation methods
have been developed, as described in chapter 8.

the main focus of the book is on the core supervised learning problems of
regression and classi   cation.
in chapter 9 we discuss some rather less standard
settings that gps have been used in, and complete the main part of the book
with some conclusions.

appendix a gives some mathematical background, while appendix b deals
speci   cally with gaussian markov processes. appendix c gives details of how
to access the data and programs that were used to make the some of the    gures
and run the experiments described in the book.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

chapter 2

regression

supervised learning can be divided into regression and classi   cation problems.
whereas the outputs for classi   cation are discrete class labels, regression is
concerned with the prediction of continuous quantities. for example, in a    -
nancial application, one may attempt to predict the price of a commodity as
a function of interest rates, currency exchange rates, availability and demand.
in this chapter we describe gaussian process methods for regression problems;
classi   cation problems are discussed in chapter 3.

there are several ways to interpret gaussian process (gp) regression models.
one can think of a gaussian process as de   ning a distribution over functions,
and id136 taking place directly in the space of functions, the function-space
view. although this view is appealing it may initially be di   cult to grasp,
so we start our exposition in section 2.1 with the equivalent weight-space view
which may be more familiar and accessible to many, and continue in section
2.2 with the function-space view. gaussian processes often have characteristics
that can be changed by setting certain parameters and in section 2.3 we discuss
how the properties change as these parameters are varied. the predictions
from a gp model take the form of a full predictive distribution; in section 2.4
we discuss how to combine a id168 with the predictive distributions
using decision theory to make point predictions in an optimal way. a practical
comparative example involving the learning of the inverse dynamics of a robot
arm is presented in section 2.5. we give some theoretical analysis of gaussian
process regression in section 2.6, and discuss how to incorporate explicit basis
functions into the models in section 2.7. as much of the material in this chapter
can be considered fairly standard, we postpone most references to the historical
overview in section 2.8.

2.1 weight-space view

the simple id75 model where the output is a linear combination of
the inputs has been studied and used extensively. its main virtues are simplic-

two equivalent views

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

8

regression

ity of implementation and interpretability. its main drawback is that it only
allows a limited    exibility; if the relationship between input and output can-
not reasonably be approximated by a linear function, the model will give poor
predictions.

in this section we    rst discuss the bayesian treatment of the linear model.
we then make a simple enhancement to this class of models by projecting the
inputs into a high-dimensional feature space and applying the linear model
there. we show that in some feature spaces one can apply the    kernel trick    to
carry out computations implicitly in the high dimensional space; this last step
leads to computational savings when the dimensionality of the feature space is
large compared to the number of data points.

we have a training set d of n observations, d = {(xi, yi) | i = 1, . . . , n},
where x denotes an input vector (covariates) of dimension d and y denotes
a scalar output or target (dependent variable); the column vector inputs for
all n cases are aggregated in the d    n design matrix 1 x, and the targets
are collected in the vector y, so we can write d = (x, y). in the regression
setting the targets are real values. we are interested in making id136s about
the relationship between inputs and targets, i.e. the conditional distribution of
the targets given the inputs (but we are not interested in modelling the input
distribution itself).

2.1.1 the standard linear model

we will review the bayesian analysis of the standard id75 model
with gaussian noise

f(x) = x>w,

y = f(x) +   ,

(2.1)

where x is the input vector, w is a vector of weights (parameters) of the linear
model, f is the function value and y is the observed target value. often a bias
weight or o   set is included, but as this can be implemented by augmenting the
input vector x with an additional element whose value is always one, we do not
explicitly include it in our notation. we have assumed that the observed values
y di   er from the function values f(x) by additive noise, and we will further
assume that this noise follows an independent, identically distributed gaussian
distribution with zero mean and variance   2
n
       n (0,   2
n).

(2.2)

training set

design matrix

bias, o   set

likelihood

this noise assumption together with the model directly gives rise to the likeli-
hood, the id203 density of the observations given the parameters, which is

1in statistics texts the design matrix is usually taken to be the transpose of our de   nition,
but our choice is deliberate and has the advantage that a data point is a standard (column)
vector.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.1 weight-space view

factored over cases in the training set (because of the independence assumption)
to give

ny

i=1

p(yi|xi, w) =

ny
exp(cid:0)     1

i=1

1   
2    n

exp(cid:0)    (yi     x>

(cid:1)
|y     x>w|2(cid:1) = n (x>w,   2

i w)2

2  2
n

1
(2    2
n)n/2

2  2
n

(2.3)

ni),

p(y|x, w) =

=

where |z| denotes the euclidean length of vector z. in the bayesian formalism
we need to specify a prior over the parameters, expressing our beliefs about the
parameters before we look at the observations. we put a zero mean gaussian
prior with covariance matrix   p on the weights
w     n (0,   p).

(2.4)

the r  ole and properties of this prior will be discussed in section 2.2; for now
we will continue the derivation with the prior as speci   ed.

id136 in the bayesian linear model is based on the posterior distribution

over the weights, computed by bayes    rule, (see eq. (a.3))2

posterior =

likelihood    prior
marginal likelihood ,

p(w|y, x) = p(y|x, w)p(w)

p(y|x)

,

(2.5)

where the normalizing constant, also known as the marginal likelihood (see page
19), is independent of the weights and given by

p(y|x) =

p(y|x, w)p(w) dw.

(2.6)

z

9

prior

posterior

marginal likelihood

the posterior in eq. (2.5) combines the likelihood and the prior, and captures
everything we know about the parameters. writing only the terms from the
likelihood and prior which depend on the weights, and    completing the square   
we obtain

p(w|x, y)     exp(cid:0)    1
    exp(cid:0)    1

(y     x>w)>(y     x>w)(cid:1) exp(cid:0)    1
(cid:1)(w       w)(cid:1),
(w       w)>(cid:0) 1

xx> +      1

p w(cid:1)

w>     1

(2.7)

2  2
n

2

p

2

  2
n

where   w =      2
posterior distribution as gaussian with mean   w and covariance matrix a   1

p )   1xy, and we recognize the form of the

n xx> +      1

n (     2

p(w|x, y)     n (   w =

a   1xy, a   1),

1
  2
n

(2.8)

n xx> +      1

where a =      2
p . notice that for this model (and indeed for any
gaussian posterior) the mean of the posterior distribution p(w|y, x) is also
its mode, which is also called the maximum a posteriori (map) estimate of
2often bayes    rule is stated as p(a|b) = p(b|a)p(a)/p(b); here we use it in a form where we
additionally condition everywhere on the inputs x (but neglect this extra conditioning for
the prior which is independent of the inputs).

map estimate

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

10

regression

(a)

(b)

(c)

(d)

figure 2.1: example of bayesian linear model f (x) = w1 + w2x with intercept
w1 and slope parameter w2. panel (a) shows the contours of the prior distribution
p(w)     n (0, i), eq. (2.4). panel (b) shows three training points marked by crosses.
panel (c) shows contours of the likelihood p(y|x, w) eq. (2.3), assuming a noise level of
  n = 1; note that the slope is much more    well determined    than the intercept. panel
(d) shows the posterior, p(w|x, y) eq. (2.7); comparing the maximum of the posterior
to the likelihood, we see that the intercept has been shrunk towards zero whereas the
more    well determined    slope is almost unchanged. all contour plots give the 1 and
2 standard deviation equi-id203 contours. superimposed on the data in panel
(b) are the predictive mean plus/minus two standard deviations of the (noise-free)
predictive distribution p(f   |x   , x, y), eq. (2.9).

w. in a non-bayesian setting the negative log prior is sometimes thought of
as a penalty term, and the map point is known as the penalized maximum
likelihood estimate of the weights, and this may cause some confusion between
the two approaches. note, however, that in the bayesian setting the map
estimate plays no special r  ole.3 the penalized maximum likelihood procedure

3in this case, due to symmetries in the model and posterior, it happens that the mean
of the predictive distribution is the same as the prediction at the mean of the posterior.
however, this is not the case in general.

intercept, w1slope, w2   2   1012   2   1012   505   505input, xoutput, yintercept, w1slope, w2   2   1012   2   1012intercept, w1slope, w2   2   1012   2   1012c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

11

ridge regression

predictive distribution

feature space

polynomial regression

linear in the parameters

2.1 weight-space view

is known in this case as ridge regression [hoerl and kennard, 1970] because of
the e   ect of the quadratic penalty term 1

p w from the log prior.

2 w>     1

to make predictions for a test case we average over all possible parameter
values, weighted by their posterior id203. this is in contrast to non-
bayesian schemes, where a single parameter is typically chosen by some crite-
rion. thus the predictive distribution for f    , f(x   ) at x    is given by averaging
the output of all possible linear models w.r.t. the gaussian posterior

p(f   |x   , x, y) =

p(f   |x   , w)p(w|x, y) dw

    a   1x   (cid:1).

    a   1xy, x>
x>

(2.9)

z
= n(cid:0) 1

  2
n

the predictive distribution is again gaussian, with a mean given by the poste-
rior mean of the weights from eq. (2.8) multiplied by the test input, as one would
expect from symmetry considerations. the predictive variance is a quadratic
form of the test input with the posterior covariance matrix, showing that the
predictive uncertainties grow with the magnitude of the test input, as one would
expect for a linear model.

an example of bayesian id75 is given in figure 2.1. here we
have chosen a 1-d input space so that the weight-space is two-dimensional and
can be easily visualized. contours of the gaussian prior are shown in panel (a).
the data are depicted as crosses in panel (b). this gives rise to the likelihood
shown in panel (c) and the posterior distribution in panel (d). the predictive
distribution and its error bars are also marked in panel (b).

2.1.2 projections of inputs into feature space

in the previous section we reviewed the bayesian linear model which su   ers
from limited expressiveness. a very simple idea to overcome this problem is to
   rst project the inputs into some high dimensional space using a set of basis
functions and then apply the linear model in this space instead of directly on
the inputs themselves. for example, a scalar input x could be projected into
the space of powers of x:   (x) = (1, x, x2, x3, . . .)> to implement polynomial
regression. as long as the projections are    xed functions (i.e. independent of
the parameters w) the model is still linear in the parameters, and therefore
analytically tractable.4 this idea is also used in classi   cation, where a dataset
which is not linearly separable in the original data space may become linearly
separable in a high dimensional feature space, see section 3.3. application of
this idea begs the question of how to choose the basis functions? as we shall
demonstrate (in chapter 5), the gaussian process formalism allows us to answer
this question. for now, we assume that the basis functions are given.

speci   cally, we introduce the function   (x) which maps a d-dimensional
input vector x into an n dimensional feature space. further let the matrix

4models with adaptive basis functions, such as e.g. multilayer id88s, may at    rst
seem like a useful extension, but they are much harder to treat, except in the limit of an
in   nite number of hidden units, see section 4.2.3.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

12

regression

explicit feature space
formulation

alternative formulation

computational load

kernel

kernel trick

  (x) be the aggregation of columns   (x) for all cases in the training set. now
the model is

(2.10)
where the vector of parameters now has length n. the analysis for this model
is analogous to the standard linear model, except that everywhere   (x) is
substituted for x. thus the predictive distribution becomes

f(x) =   (x)>w,

f   |x   , x, y     n(cid:0) 1

  (x   )>a   1  y,   (x   )>a   1  (x   )(cid:1)

(2.11)

  2
n

with    =   (x) and a =      2
p . to make predictions using this
equation we need to invert the a matrix of size n    n which may not be
convenient if n, the dimension of the feature space, is large. however, we can
rewrite the equation in the following way

n     > +      1

>
ni)   1y,
      p  (k +   2
      p            
>
>
      p  (k +   2

  

ni)   1  >  p     (cid:1),

(2.12)

f   |x   , x, y     n(cid:0)  

ni) =      2

n   (  >  p   +   2

where we have used the shorthand   (x   ) =       and de   ned k =   >  p  .
to show this for the mean,    rst note that using the de   nitions of a and k
we have      2
n   (k +   2
ni) = a  p  . now multiplying
through by a   1 from left and (k +   2
n a   1   =
ni)   1, showing the equivalence of the mean expressions in eq. (2.11)
  p  (k +   2
and eq. (2.12). for the variance we use the matrix inversion lemma, eq. (a.9),
setting z   1 =   p, w    1 =   2
in eq. (2.12) we
need to invert matrices of size n    n which is more convenient when n < n.
geometrically, note that n datapoints can span at most n dimensions in the
feature space.

ni)   1 from the right gives      2

ni and v = u =    therein.

>
      p  , or   

notice that in eq. (2.12) the feature space always enters in the form of
>
  >  p  ,   
      p     ; thus the entries of these matrices are invariably of
the form   (x)>  p  (x0) where x and x0 are in either the training or the test sets.
let us de   ne k(x, x0) =   (x)>  p  (x0). for reasons that will become clear later
we call k(  ,  ) a covariance function or kernel. notice that   (x)>  p  (x0) is an
inner product (with respect to   p). as   p is positive de   nite we can de   ne   1/2
so that (  1/2
)2 =   p; for example if the svd (singular value decomposition)
of   p = u du>, where d is diagonal, then one form for   1/2
is u d1/2u>.
then de   ning   (x) =   1/2
p   (x) we obtain a simple dot product representation
k(x, x0) =   (x)      (x0).

p

p

p

if an algorithm is de   ned solely in terms of inner products in input space
then it can be lifted into feature space by replacing occurrences of those inner
products by k(x, x0); this is sometimes called the kernel trick. this technique is
particularly valuable in situations where it is more convenient to compute the
kernel than the feature vectors themselves. as we will see in the coming sections,
this often leads to considering the kernel as the object of primary interest, and
its corresponding feature space as having secondary practical importance.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.2 function-space view

13

2.2 function-space view

an alternative and equivalent way of reaching identical results to the previous
section is possible by considering id136 directly in function space. we use
a gaussian process (gp) to describe a distribution over functions. formally:

de   nition 2.1 a gaussian process is a collection of random variables, any
(cid:3)
   nite number of which have a joint gaussian distribution.

gaussian process

a gaussian process is completely speci   ed by its mean function and co-
variance function. we de   ne mean function m(x) and the covariance function
k(x, x0) of a real process f(x) as

covariance and
mean function

m(x) = e[f(x)],

k(x, x0) = e[(f(x)     m(x))(f(x0)     m(x0))],

and will write the gaussian process as

f(x)     gp(cid:0)m(x), k(x, x0)(cid:1).

(2.13)

(2.14)

usually, for notational simplicity we will take the mean function to be zero,
although this need not be done, see section 2.7.

in our case the random variables represent the value of the function f(x)
at location x. often, gaussian processes are de   ned over time, i.e. where the
index set of the random variables is time. this is not (normally) the case in
our use of gps; here the index set x is the set of possible inputs, which could
be more general, e.g. rd. for notational convenience we use the (arbitrary)
enumeration of the cases in the training set to identify the random variables
such that fi , f(xi) is the random variable corresponding to the case (xi, yi)
as would be expected.

a gaussian process is de   ned as a collection of random variables. thus, the
de   nition automatically implies a consistency requirement, which is also some-
times known as the marginalization property. this property simply means
that if the gp e.g. speci   es (y1, y2)     n (  ,   ), then it must also specify
y1     n (  1,   11) where   11 is the relevant submatrix of   , see eq. (a.6).
in other words, examination of a larger set of variables does not change the
distribution of the smaller set. notice that the consistency requirement is au-
tomatically ful   lled if the covariance function speci   es entries of the covariance
matrix.5 the de   nition does not exclude gaussian processes with    nite index
sets (which would be simply gaussian distributions), but these are not partic-
ularly interesting for our purposes.

5note, however, that if you instead speci   ed e.g. a function for the entries of the inverse
covariance matrix, then the marginalization property would no longer be ful   lled, and one
could not think of this as a consistent collection of random variables   this would not qualify
as a gaussian process.

index set    
input domain

marginalization
property

   nite index set

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

14

bayesian linear model
is a gaussian process

basis functions

smoothness

characteristic
length-scale

regression

a simple example of a gaussian process can be obtained from our bayesian
id75 model f(x) =   (x)>w with prior w     n (0,   p). we have for
the mean and covariance

e[f(x)] =   (x)>e[w] = 0,

e[f(x)f(x0)] =   (x)>e[ww>]  (x0) =   (x)>  p  (x0).

(2.15)

thus f(x) and f(x0) are jointly gaussian with zero mean and covariance given
by   (x)>  p  (x0). indeed, the function values f(x1), . . . , f(xn) corresponding
to any number of input points n are jointly gaussian, although if n < n then
this gaussian is singular (as the joint covariance matrix will be of rank n).

in this chapter our running example of a covariance function will be the
squared exponential 6 (se) covariance function; other covariance functions are
discussed in chapter 4. the covariance function speci   es the covariance between
pairs of random variables

cov(cid:0)f(xp), f(xq)(cid:1) = k(xp, xq) = exp(cid:0)    1

2|xp     xq|2(cid:1).

(2.16)

note, that the covariance between the outputs is written as a function of the
inputs. for this particular covariance function, we see that the covariance is
almost unity between variables whose corresponding inputs are very close, and
decreases as their distance in the input space increases.

it can be shown (see section 4.3.1) that the squared exponential covariance
function corresponds to a bayesian id75 model with an in   nite
number of basis functions. indeed for every positive de   nite covariance function
k(  ,  ), there exists a (possibly in   nite) expansion in terms of basis functions
(see mercer   s theorem in section 4.3). we can also obtain the se covariance
function from the linear combination of an in   nite number of gaussian-shaped
basis functions, see eq. (4.13) and eq. (4.30).

the speci   cation of the covariance function implies a distribution over func-
tions. to see this, we can draw samples from the distribution of functions evalu-
ated at any number of points; in detail, we choose a number of input points,7 x   
and write out the corresponding covariance matrix using eq. (2.16) elementwise.
then we generate a random gaussian vector with this covariance matrix

f        n(cid:0)0, k(x   , x   )(cid:1),

(2.17)

and plot the generated values as a function of the inputs. figure 2.2(a) shows
three such samples. the generation of multivariate gaussian samples is de-
scribed in section a.2.

in the example in figure 2.2 the input values were equidistant, but this
need not be the case. notice that    informally    the functions look smooth.
in fact the squared exponential covariance function is in   nitely di   erentiable,
leading to the process being in   nitely mean-square di   erentiable (see section
4.1). we also see that the functions seem to have a characteristic length-scale,

6sometimes this covariance function is called the radial basis function (rbf) or gaussian;

here we prefer squared exponential.

7technically, these input points play the r  ole of test inputs and therefore carry a subscript

asterisk; this will become clearer later when both training and test points are involved.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.2 function-space view

15

(a), prior

(b), posterior

figure 2.2: panel (a) shows three functions drawn at random from a gp prior;
the dots indicate values of y actually generated; the two other functions have (less
correctly) been drawn as lines by joining a large number of evaluated points. panel (b)
shows three random functions drawn from the posterior, i.e. the prior conditioned on
the    ve noise free observations indicated. in both plots the shaded area represents the
pointwise mean plus and minus two times the standard deviation for each input value
(corresponding to the 95% con   dence region), for the prior and posterior respectively.

which informally can be thought of as roughly the distance you have to move in
input space before the function value can change signi   cantly, see section 4.2.1.
for eq. (2.16) the characteristic length-scale is around one unit. by replacing
|xp   xq| by |xp   xq|/    in eq. (2.16) for some positive constant     we could change
the characteristic length-scale of the process. also, the overall variance of the
random function can be controlled by a positive pre-factor before the exp in
eq. (2.16). we will discuss more about how such factors a   ect the predictions
in section 2.3, and say more about how to set such scale parameters in chapter
5.

prediction with noise-free observations

we are usually not primarily interested in drawing random functions from the
prior, but want to incorporate the knowledge that the training data provides
about the function. initially, we will consider the simple special case where the
observations are noise free, that is we know {(xi, fi)|i = 1, . . . , n}. the joint
distribution of the training outputs, f, and the test outputs f    according to the
prior is

(cid:20) k(x, x) k(x, x   )

(cid:20) f

(cid:21)(cid:19)

(cid:18)

(cid:21)

.

(2.18)

    n

0,

f   

k(x   , x) k(x   , x   )

magnitude

joint prior

if there are n training points and n    test points then k(x, x   ) denotes the
n    n    matrix of the covariances evaluated at all pairs of training and test
points, and similarly for the other entries k(x, x), k(x   , x   ) and k(x   , x).
to get the posterior distribution over functions we need to restrict this joint
prior distribution to contain only those functions which agree with the observed
data points. graphically in figure 2.2 you may think of generating functions
from the prior, and rejecting the ones that disagree with the observations, al-

graphical rejection

   505   2   1012input, xoutput, f(x)   505   2   1012input, xoutput, f(x)c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

16

regression

noise-free predictive
distribution

predictive distribution

though this strategy would not be computationally very e   cient. fortunately,
in probabilistic terms this operation is extremely simple, corresponding to con-
ditioning the joint gaussian prior distribution on the observations (see section
a.2 for further details) to give

f   |x   , x, f     n(cid:0)k(x   , x)k(x, x)   1f ,

k(x   , x   )     k(x   , x)k(x, x)   1k(x, x   )(cid:1).

(2.19)

function values f    (corresponding to test inputs x   ) can be sampled from the
joint posterior distribution by evaluating the mean and covariance matrix from
eq. (2.19) and generating samples according to the method described in section
a.2.

figure 2.2(b) shows the results of these computations given the    ve data-
points marked with + symbols. notice that it is trivial to extend these compu-
tations to multidimensional inputs     one simply needs to change the evaluation
of the covariance function in accordance with eq. (2.16), although the resulting
functions may be harder to display graphically.

prediction using noisy observations

it is typical for more realistic modelling situations that we do not have access
to function values themselves, but only noisy versions thereof y = f(x) +   .8
assuming additive independent identically distributed gaussian noise    with
variance   2

n, the prior on the noisy observations becomes

cov(yp, yq) = k(xp, xq) +   2

n  pq or cov(y) = k(x, x) +   2

ni,

(2.20)

where   pq is a kronecker delta which is one i    p = q and zero otherwise. it
follows from the independence9 assumption about the noise, that a diagonal
matrix10 is added, in comparison to the noise free case, eq. (2.16). introducing
the noise term in eq. (2.18) we can write the joint distribution of the observed
target values and the function values at the test locations under the prior as

(cid:21)

(cid:20) y

f   

(cid:18)

(cid:20) k(x, x) +   2

k(x   , x)

    n

0,

ni k(x, x   )
k(x   , x   )

.

(2.21)

(cid:21)(cid:19)

deriving the conditional distribution corresponding to eq. (2.19) we arrive at
the key predictive equations for gaussian process regression

f   |x, y, x        n(cid:0)  f   , cov(f   )(cid:1), where

  f    , e[f   |x, y, x   ] = k(x   , x)[k(x, x) +   2

cov(f   ) = k(x   , x   )     k(x   , x)[k(x, x) +   2

ni(cid:3)   1

ni]   1y,
k(x, x   ).

(2.22)
(2.23)
(2.24)

8there are some situations where it is reasonable to assume that the observations are

noise-free, for example for computer simulations, see e.g. sacks et al. [1989].

9more complicated noise models with non-trivial covariance structure can also be handled,

see section 9.2.

10notice that the kronecker delta is on the index of the cases, not the value of the input;
for the signal part of the covariance function the input value is the index set to the random
variables describing the function, for the noise part it is the identity of the point.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.2 function-space view

    
    
f1             

f   
6 6 6 6

y1
6 6

y   
6

observations

gaussian    eld

yc
6

        
fc     

6 6 6

inputs

x1

x2

x   

xc

figure 2.3: graphical model (chain graph) for a gp for regression. squares rep-
resent observed variables and circles represent unknowns. the thick horizontal bar
represents a set of fully connected nodes. note that an observation yi is conditionally
independent of all other nodes given the corresponding latent variable, fi. because of
the marginalization property of gps addition of further inputs, x, latent variables, f ,
and unobserved targets, y   , does not change the distribution of any other variables.

notice that we now have exact correspondence with the weight space view in
eq. (2.12) when identifying k(c, d) =   (c)>  p  (d), where c, d stand for ei-
ther x or x   . for any set of basis functions, we can compute the corresponding
covariance function as k(xp, xq) =   (xp)>  p  (xq); conversely, for every (posi-
tive de   nite) covariance function k, there exists a (possibly in   nite) expansion
in terms of basis functions, see section 4.3.

the expressions involving k(x, x), k(x, x   ) and k(x   , x   ) etc. can look
rather unwieldy, so we now introduce a compact form of the notation setting
k = k(x, x) and k    = k(x, x   ). in the case that there is only one test
point x    we write k(x   ) = k    to denote the vector of covariances between the
test point and the n training points. using this compact notation and for a
single test point x   , equations 2.23 and 2.24 reduce to

  f    = k>

    (k +   2

ni)   1y,

v[f   ] = k(x   , x   )     k>

    (k +   2

ni)   1k   .

(2.25)
(2.26)

let us examine the predictive distribution as given by equations 2.25 and 2.26.
note    rst that the mean prediction eq. (2.25) is a linear combination of obser-
vations y; this is sometimes referred to as a linear predictor. another way to
look at this equation is to see it as a linear combination of n id81s,
each one centered on a training point, by writing

nx

17

correspondence with
weight-space view

compact notation

predictive distribution

linear predictor

  f(x   ) =

  ik(xi, x   )

(2.27)

i=1

ni)   1y. the fact that the mean prediction for f(x   ) can be
where    = (k +   2
written as eq. (2.27) despite the fact that the gp can be represented in terms
of a (possibly in   nite) number of basis functions is one manifestation of the
representer theorem; see section 6.2 for more on this point. we can understand
this result intuitively because although the gp de   nes a joint gaussian dis-
tribution over all of the y variables, one for each point in the index set x , for

representer theorem

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

18

regression

(a), posterior

(b), posterior covariance

figure 2.4: panel (a) is identical to figure 2.2(b) showing three random functions
drawn from the posterior. panel (b) shows the posterior co-variance between f (x) and
f (x0) for the same data for three di   erent values of x0. note, that the covariance at
close points is high, falling to zero at the training points (where there is no variance,
since it is a noise-free process), then becomes negative, etc. this happens because if
the smooth function happens to be less than the mean on one side of the data point,
it tends to exceed the mean on the other side, causing a reversal of the sign of the
covariance at the data points. note for contrast that the prior covariance is simply
of gaussian shape and never negative.

making predictions at x    we only care about the (n+1)-dimensional distribution
de   ned by the n training points and the test point. as a gaussian distribu-
tion is marginalized by just taking the relevant block of the joint covariance
matrix (see section a.2) it is clear that conditioning this (n+1)-dimensional
distribution on the observations gives us the desired result. a graphical model
representation of a gp is given in figure 2.3.

note also that the variance in eq. (2.24) does not depend on the observed
targets, but only on the inputs; this is a property of the gaussian distribution.
the variance is the di   erence between two terms: the    rst term k(x   , x   ) is
simply the prior covariance; from that is subtracted a (positive) term, repre-
senting the information the observations gives us about the function. we can
very simply compute the predictive distribution of test targets y    by adding
ni to the variance in the expression for cov(f   ).
  2

the predictive distribution for the gp model gives more than just pointwise
errorbars of the simpli   ed eq. (2.26). although not stated explicitly, eq. (2.24)
holds unchanged when x    denotes multiple test inputs; in this case the co-
variance of the test targets are computed (whose diagonal elements are the
pointwise variances). in fact, eq. (2.23) is the mean function and eq. (2.24) the
covariance function of the (gaussian) posterior process; recall the de   nition
of gaussian process from page 13. the posterior covariance in illustrated in
figure 2.4(b).

it will be useful (particularly for chapter 5) to introduce the marginal likeli-
hood (or evidence) p(y|x) at this point. the marginal likelihood is the integral

noisy predictions

joint predictions

posterior process

marginal likelihood

   505   2   1012input, xoutput, f(x)   505   0.200.20.40.6input, xpost. covariance, cov(f(x),f(x   ))x   =   2x   =1x   =3c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.3 varying the hyperparameters

19

o

input: x (inputs), y (targets), k (covariance function),   2

n (noise level),

x    (test input)

ni)

o

predictive mean eq. (2.25)

2 y>      p

2: l := cholesky(k +   2
   := l>\(l\y)
4:   f    := k>
      
v := l\k   
predictive variance eq. (2.26)
6: v[f   ] := k(x   , x   )     v>v
log p(y|x) :=     1
eq. (2.30)
8: return:   f    (mean), v[f   ] (variance), log p(y|x) (log marginal likelihood)
algorithm 2.1: predictions and log marginal likelihood for gaussian process regres-
sion. the implementation addresses the matrix inversion required by eq. (2.25) and
(2.26) using cholesky factorization, see section a.4. for multiple test cases lines
4-6 are repeated. the log determinant required in eq. (2.30) is computed from the
cholesky factor (for large n it may not be possible to represent the determinant itself).
the computational complexity is n3/6 for the cholesky decomposition in line 2, and
n2/2 for solving triangular systems in line 3 and (for each test case) in line 5.

i log lii     n

2 log 2  

of the likelihood times the prior
p(y|x) =

z

p(y|f , x)p(f|x) df .

(2.28)

the term marginal likelihood refers to the marginalization over the function
values f. under the gaussian process model the prior is gaussian, f|x    
n (0, k), or

log p(f|x) =     1

2 log |k|     n
2 log 2  ,
(2.29)
and the likelihood is a factorized gaussian y|f     n (f ,   2
ni) so we can make use
of equations a.7 and a.8 to perform the integration yielding the log marginal
likelihood

2 f>k   1f     1

2 y>(k +   2

log p(y|x) =     1

ni)   1y     1

2 log |k +   2

ni|     n

2 log 2  .

this result can also be obtained directly by observing that y     n (0, k +   2

(2.30)
ni).
a practical implementation of gaussian process regression (gpr) is shown
in algorithm 2.1. the algorithm uses cholesky decomposition, instead of di-
rectly inverting the matrix, since it is faster and numerically more stable, see
section a.4. the algorithm returns the predictive mean and variance for noise
free test data   to compute the predictive distribution for noisy test data y   ,
simply add the noise variance   2

n to the predictive variance of f   .

2.3 varying the hyperparameters

typically the covariance functions that we use will have some free parameters.
for example, the squared-exponential covariance function in one dimension has
the following form

f exp(cid:0)    1

2   2 (xp     xq)2(cid:1) +   2

n  pq.

ky(xp, xq) =   2

(2.31)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

20

regression

(a),     = 1

hyperparameters

(b),     = 0.3

(c),     = 3

figure 2.5: (a) data is generated from a gp with hyperparameters (   ,   f ,   n) =
(1, 1, 0.1), as shown by the + symbols. using gaussian process prediction with these
hyperparameters we obtain a 95% con   dence region for the underlying function f
(shown in grey). panels (b) and (c) again show the 95% con   dence region, but this
time for hyperparameter values (0.3, 1.08, 0.00005) and (3.0, 1.16, 0.89) respectively.

the covariance is denoted ky as it is for the noisy targets y rather than for the
underlying function f. observe that the length-scale    , the signal variance   2
f
and the noise variance   2
n can be varied. in general we call the free parameters
hyperparameters.11

in chapter 5 we will consider various methods for determining the hyperpa-
rameters from training data. however, in this section our aim is more simply to
explore the e   ects of varying the hyperparameters on gp prediction. consider
the data shown by + signs in figure 2.5(a). this was generated from a gp
with the se kernel with (   ,   f ,   n) = (1, 1, 0.1). the    gure also shows the 2
standard-deviation error bars for the predictions obtained using these values of
the hyperparameters, as per eq. (2.24). notice how the error bars get larger
for input values that are distant from any training points. indeed if the x-axis

11we refer to the parameters of the covariance function as hyperparameters to emphasize
that they are parameters of a non-parametric model; in accordance with the weight-space
view, section 2.1, the parameters (weights) of the underlying parametric model have been
integrated out.

   505   3   2   10123input, xoutput, y   505   3   2   10123input, xoutput, y   505   3   2   10123input, xoutput, yc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.4 decision theory for regression

21

were extended one would see the error bars re   ect the prior standard deviation
of the process   f away from the data.

if we set the length-scale shorter so that     = 0.3 and kept the other pa-
rameters the same, then generating from this process we would expect to see
plots like those in figure 2.5(a) except that the x-axis should be rescaled by a
factor of 0.3; equivalently if the same x-axis was kept as in figure 2.5(a) then
a sample function would look much more wiggly.

if we make predictions with a process with     = 0.3 on the data generated
from the     = 1 process then we obtain the result in figure 2.5(b). the remaining
two parameters were set by optimizing the marginal likelihood, as explained in
chapter 5. in this case the noise parameter is reduced to   n = 0.00005 as the
greater    exibility of the    signal    means that the noise level can be reduced.
this can be observed at the two datapoints near x = 2.5 in the plots. in figure
2.5(a) (    = 1) these are essentially explained as a similar function value with
di   ering noise. however, in figure 2.5(b) (    = 0.3) the noise level is very low,
so these two points have to be explained by a sharp variation in the value of
the underlying function f. notice also that the short length-scale means that
the error bars in figure 2.5(b) grow rapidly away from the datapoints.

in contrast, we can set the length-scale longer, for example to     = 3, as shown
in figure 2.5(c). again the remaining two parameters were set by optimizing the
marginal likelihood. in this case the noise level has been increased to   n = 0.89
and we see that the data is now explained by a slowly varying function with a
lot of noise.

of course we can take the position of a quickly-varying signal with low noise,
or a slowly-varying signal with high noise to extremes; the former would give rise
to a white-noise process model for the signal, while the latter would give rise to a
constant signal with added white noise. under both these models the datapoints
produced should look like white noise. however, studying figure 2.5(a) we see
that white noise is not a convincing model of the data, as the sequence of y   s does
not alternate su   ciently quickly but has correlations due to the variability of
the underlying function. of course this is relatively easy to see in one dimension,
but methods such as the marginal likelihood discussed in chapter 5 generalize
to higher dimensions and allow us to score the various models. in this case the
marginal likelihood gives a clear preference for (   ,   f ,   n) = (1, 1, 0.1) over the
other two alternatives.

2.4 decision theory for regression

in the previous sections we have shown how to compute predictive distributions
for the outputs y    corresponding to the novel test input x   . the predictive dis-
tribution is gaussian with mean and variance given by eq. (2.25) and eq. (2.26).
in practical applications, however, we are often forced to make a decision about
how to act, i.e. we need a point-like prediction which is optimal in some sense.
to this end we need a id168, l(ytrue, yguess), which speci   es the loss (or

too short length-scale

too long length-scale

model comparison

optimal predictions
id168

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

22

regression

non-bayesian paradigm
bayesian paradigm

penalty) incurred by guessing the value yguess when the true value is ytrue. for
example, the id168 could equal the absolute deviation between the guess
and the truth.

notice that we computed the predictive distribution without reference to
the id168. in non-bayesian paradigms, the model is typically trained
by minimizing the empirical risk (or loss). in contrast, in the bayesian setting
there is a clear separation between the likelihood function (used for training, in
addition to the prior) and the id168. the likelihood function describes
how the noisy measurements are assumed to deviate from the underlying noise-
free function. the id168, on the other hand, captures the consequences
of making a speci   c choice, given an actual true state. the likelihood and loss
function need not have anything in common.12

expected loss, risk

absolute error loss
squared error loss

robot arm

our goal is to make the point prediction yguess which incurs the smallest loss,
but how can we achieve that when we don   t know ytrue? instead, we minimize
the expected loss or risk, by averaging w.r.t. our model   s opinion as to what the
truth might be

  rl(yguess|x   ) =

l(y   , yguess)p(y   |x   ,d) dy   .

(2.32)

z

thus our best guess, in the sense that it minimizes the expected loss, is

yoptimal|x    = argmin

yguess

  rl(yguess|x   ).

(2.33)

in general the value of yguess that minimizes the risk for the id168 |yguess   
y   | is the median of p(y   |x   ,d), while for the squared loss (yguess     y   )2 it is
the mean of this distribution. when the predictive distribution is gaussian
the mean and the median coincide, and indeed for any symmetric id168
and symmetric predictive distribution we always get yguess as the mean of the
predictive distribution. however, in many practical problems the id168s
can be asymmetric, e.g. in safety critical applications, and point predictions
may be computed directly from eq. (2.32) and eq. (2.33). a comprehensive
treatment of decision theory can be found in berger [1985].

2.5 an example application

in this section we use gaussian process regression to learn the inverse dynamics
of a seven degrees-of-freedom sarcos anthropomorphic robot arm. the task
is to map from a 21-dimensional input space (7 joint positions, 7 joint velocities,
7 joint accelerations) to the corresponding 7 joint torques. this task has pre-
viously been used to study regression algorithms by vijayakumar and schaal
[2000], vijayakumar et al. [2002] and vijayakumar et al. [2005].13 following

12beware of fallacious arguments like: a gaussian likelihood implies a squared error loss

function.

13we thank sethu vijayakumar for providing us with the data.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.5 an example application

this previous work we present results below on just one of the seven mappings,
from the 21 input variables to the    rst of the seven torques.

one might ask why it is necessary to learn this mapping; indeed there exist
physics-based rigid-body-dynamics models which allow us to obtain the torques
from the position, velocity and acceleration variables. however, the real robot
arm is actuated hydraulically and is rather lightweight and compliant, so the
assumptions of the rigid-body-dynamics model are violated (as we see below).
it is worth noting that the rigid-body-dynamics model is nonlinear, involving
trigonometric functions and squares of the input variables.

an inverse dynamics model can be used in the following manner: a planning
module decides on a trajectory that takes the robot from its start to goal states,
and this speci   es the desired positions, velocities and accelerations at each time.
the inverse dynamics model is used to compute the torques needed to achieve
this trajectory and errors are corrected using a feedback controller.

the dataset consists of 48,933 input-output pairs, of which 44,484 were used
as a training set and the remaining 4,449 were used as a test set. the inputs
were linearly rescaled to have zero mean and unit variance on the training set.
the outputs were centered so as to have zero mean on the training set.

given a prediction method, we can evaluate the quality of predictions in
several ways. perhaps the simplest is the squared error loss, where we compute
the squared residual (y          f(x   ))2 between the mean prediction and the target
at each test point. this can be summarized by the mean squared error (mse),
by averaging over the test set. however, this quantity is sensitive to the overall
scale of the target values, so it makes sense to normalize by the variance of the
targets of the test cases to obtain the standardized mean squared error (smse).
this causes the trivial method of guessing the mean of the training targets to
have a smse of approximately 1.

additionally if we produce a predictive distribution at each test input we
can evaluate the negative log id203 of the target under the model.14 as
gpr produces a gaussian predictive density, one obtains

    log p(y   |d, x   ) =

1
2

log(2    2   ) +

(y          f(x   ))2

2  2   

,

(2.34)

where the predictive variance   2    for gpr is computed as   2    = v(f   ) +   2
n,
where v(f   ) is given by eq. (2.26); we must include the noise variance   2
n as we
are predicting the noisy target y   . this loss can be standardized by subtracting
the loss that would be obtained under the trivial model which predicts using
a gaussian with the mean and variance of the training data. we denote this
the standardized log loss (sll). the mean sll is denoted msll. thus the
msll will be approximately zero for simple methods and negative for better
methods.

a number of models were tested on the data. a id75 (lr) model
provides a simple baseline for the smse. by estimating the noise level from the

14 it makes sense to use the negative log id203 so as to obtain a loss, not a utility.

23

why learning?

mse

smse

msll

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

24

regression

method
lr
rbd
lwpr
gpr

smse msll
0.075
-1.29
0.104
0.040
0.011

-2.25

   
   

table 2.1: test results on the inverse dynamics problem for a number of di   erent
methods. the           denotes a missing entry, caused by two methods not producing full
predictive distributions, so msll could not be evaluated.

residuals on the training set one can also obtain a predictive variance and thus
get a msll value for lr. the rigid-body-dynamics (rbd) model has a number
of free parameters; these were estimated by vijayakumar et al. [2005] using a
least-squares    tting procedure. we also give results for the locally weighted
projection regression (lwpr) method of vijayakumar et al. [2005] which is an
on-line method that cycles through the dataset multiple times. for the gp
models it is computationally expensive to make use of all 44,484 training cases
due to the o(n3) scaling of the basic algorithm. in chapter 8 we present several
di   erent approximate gp methods for large datasets. the result given in table
2.1 was obtained with the subset of regressors (sr) approximation with a subset
size of 4096. this result is taken from table 8.1, which gives full results of the
various approximation methods applied to the inverse dynamics problem. the
squared exponential covariance function was used with a separate length-scale
parameter for each of the 21 input dimensions, plus the signal and noise variance
n. these parameters were set by optimizing the marginal
parameters   2
likelihood eq. (2.30) on a subset of the data (see also chapter 5).

f and   2

the results for the various methods are presented in table 2.1. notice that
the problem is quite non-linear, so the id75 model does poorly in
comparison to non-linear methods.15 the non-linear method lwpr improves
over id75, but is outperformed by gpr.

2.6 smoothing, weight functions and equiva-

lent kernels

gaussian process regression aims to reconstruct the underlying signal f by
removing the contaminating noise   . to do this it computes a weighted average
of the noisy observations y as   f(x   ) = k(x   )>(k +  2
ni)   1y; as   f(x   ) is a linear
combination of the y values, gaussian process regression is a linear smoother
(see hastie and tibshirani [1990, sec. 2.8] for further details). in this section
we study smoothing    rst in terms of a matrix analysis of the predictions at the
training points, and then in terms of the equivalent kernel.

15it is perhaps surprising that rbd does worse than id75. however, stefan
schaal (pers. comm., 2004) states that the rbd parameters were optimized on a very large
dataset, of which the training data used here is subset, and if the rbd model were optimized
w.r.t. this training set one might well expect it to outperform id75.

linear smoother

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.6 smoothing, weight functions and equivalent kernels

25

the predicted mean values   f at the training points are given by

ni)   1y.

  f = k(k +   2

let k have the eigendecomposition k = pn
eigenvectors are mutually orthogonal. let y =pn

i , where   i is the ith
eigenvalue and ui is the corresponding eigenvector. as k is real and sym-
metric positive semide   nite, its eigenvalues are real and non-negative, and its
i=1  iui for some coe   cients
  i = u>

i=1  iuiu>

(2.35)

i y. then

  f =

  i  i

  i +   2
n

ui.

(2.36)

nx

i=1

n) (cid:28) 1 then the component in y along ui is e   ectively
notice that if   i/(  i +   2
eliminated. for most covariance functions that are used in practice the eigen-
values are larger for more slowly varying eigenvectors (e.g. fewer zero-crossings)
so that this means that high-frequency components in y are smoothed out.
the e   ective number of parameters or degrees of freedom of the smoother is
n), see hastie and tibshirani
de   ned as tr(k(k +   2
[1990, sec. 3.5]. notice that this counts the number of eigenvectors which are
not eliminated.

ni)   1) =pn

i=1   i/(  i +   2

we can de   ne a vector of functions h(x   ) = (k +   2

ni)   1k(x   ). thus we
have   f(x   ) = h(x   )>y, making it clear that the mean prediction at a point
x    is a linear combination of the target values y. for a    xed test point x   ,
h(x   ) gives the vector of weights applied to targets y. h(x   ) is called the weight
function [silverman, 1984]. as gaussian process regression is a linear smoother,
the weight function does not depend on y. note the di   erence between a
linear model, where the prediction is a linear combination of the inputs, and a
linear smoother, where the prediction is a linear combination of the training set
targets.

understanding the form of the weight function is made complicated by the
matrix inversion of k+  2
ni and the fact that k depends on the speci   c locations
of the n datapoints. idealizing the situation one can consider the observations
to be    smeared out    in x-space at some density of observations. in this case
analytic tools can be brought to bear on the problem, as shown in section 7.1.
by analogy to kernel smoothing, silverman [1984] called the idealized weight
function the equivalent kernel; see also girosi et al. [1995, sec. 2.1].

a kernel smoother centres a id8116    on x    and then computes
  i =   (|xi     x   |/   ) for each data point (xi, yi), where     is a length-scale. the
gaussian is a commonly used id81. the prediction for f(x   ) is
j=1   j. this is also known

i=1wiyi where wi =   i/pn

computed as   f(x   ) =pn

as the nadaraya-watson estimator, see e.g. scott [1992, sec. 8.1].

the weight function and equivalent kernel for a gaussian process are illus-
trated in figure 2.6 for a one-dimensional input variable x. we have used the
squared exponential covariance function and have set the length-scale     = 0.0632
(so that    2 = 0.004). there are n = 50 training points spaced randomly along

16note that this id81 does not need to be a valid covariance function.

eigendecomposition

degrees of freedom

weight function

equivalent kernel

kernel smoother

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

26

regression

(a)

(c)

(b)

(d)

figure 2.6: panels (a)-(c) show the weight function h(x   ) (dots) corresponding to
the n = 50 training points, the equivalent kernel (solid) and the original squared
exponential kernel (dashed). panel (d) shows the equivalent kernels for two di   erent
data densities. see text for further details. the small cross at the test point is to
scale in all four plots.

the x-axis. figures 2.6(a) and 2.6(b) show the weight function and equivalent
kernel for x    = 0.5 and x    = 0.05 respectively, for   2
n = 0.1. figure 2.6(c) is also
for x    = 0.5 but uses   2
n = 10. in each case the dots correspond to the weight
function h(x   ) and the solid line is the equivalent kernel, whose construction is
explained below. the dashed line shows a squared exponential kernel centered
on the test point, scaled to have the same height as the maximum value in the
equivalent kernel. figure 2.6(d) shows the variation in the equivalent kernel as
a function of n, the number of datapoints in the unit interval.

many interesting observations can be made from these plots. observe that
the equivalent kernel has (in general) a shape quite di   erent to the original se
kernel. in figure 2.6(a) the equivalent kernel is clearly oscillatory (with negative
sidelobes) and has a higher spatial frequency than the original kernel. figure
2.6(b) shows similar behaviour although due to edge e   ects the equivalent kernel
is truncated relative to that in figure 2.6(a). in figure 2.6(c) we see that at
higher noise levels the negative sidelobes are reduced and the width of the
equivalent kernel is similar to the original kernel. also note that the overall
height of the equivalent kernel in (c) is reduced compared to that in (a) and

00.20.40.60.8100.200.20.40.60.8100.200.20.40.60.8100.0500.20.40.60.8100.110250c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.7 incorporating explicit basis functions

27

(b)   it averages over a wider area. the more oscillatory equivalent kernel for
lower noise levels can be understood in terms of the eigenanalysis above; at
higher noise levels only the large    (slowly varying) components of y remain,
while for smaller noise levels the more oscillatory components are also retained.
in figure 2.6(d) we have plotted the equivalent kernel for n = 10 and n =
250 datapoints in [0, 1]; notice how the width of the equivalent kernel decreases
as n increases. we discuss this behaviour further in section 7.1.

the plots of equivalent kernels in figure 2.6 were made by using a dense
grid of ngrid points on [0, 1] and then computing the smoother matrix k(k +
gridi)   1. each row of this matrix is the equivalent kernel at the appropriate
  2
location. however, in order to get the scaling right one has to set   2
grid =
nngrid/n; for ngrid > n this means that the e   ective variance at each of the
  2
ngrid points is larger, but as there are correspondingly more points this e   ect
cancels out. this can be understood by imagining the situation if there were
ngrid/n independent gaussian observations with variance   2
grid at a single x-
position; this would be equivalent to one gaussian observation with variance
n. in e   ect the n observations have been smoothed out uniformly along the
  2
interval. the form of the equivalent kernel can be obtained analytically if we
go to the continuum limit and look to smooth a noisy function. the relevant
theory and some example equivalent kernels are given in section 7.1.

2.7

incorporating explicit basis functions

   

it is common but by no means necessary to consider gps with a zero mean func-
tion. note that this is not necessarily a drastic limitation, since the mean of the
posterior process is not con   ned to be zero. yet there are several reasons why
one might wish to explicitly model a mean function, including interpretability
of the model, convenience of expressing prior information and a number of an-
alytical limits which we will need in subsequent chapters. the use of explicit
basis functions is a way to specify a non-zero mean over functions, but as we
will see in this section, one can also use them to achieve other interesting e   ects.
using a    xed (deterministic) mean function m(x) is trivial: simply apply
the usual zero mean gp to the di   erence between the observations and the
   xed mean function. with

f(x)     gp(cid:0)m(x), k(x, x0)(cid:1),

the predictive mean becomes

  f    = m(x   ) + k(x   , x)k   1

y (y     m(x)),

where ky = k +   2
eq. (2.24).

ni, and the predictive variance remains unchanged from

however, in practice it can often be di   cult to specify a    xed mean function.
in many cases it may be more convenient to specify a few    xed basis functions,

(2.37)

(2.38)

   xed mean function

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

28

stochastic mean
function

polynomial regression

whose coe   cients,   , are to be inferred from the data. consider

g(x) = f(x) + h(x)>  , where f(x)     gp(cid:0)0, k(x, x0)(cid:1),

(2.39)

regression

here f(x) is a zero mean gp, h(x) are a set of    xed basis functions, and    are
additional parameters. this formulation expresses that the data is close to a
global linear model with the residuals being modelled by a gp. this idea was
explored explicitly as early as 1975 by blight and ott [1975], who used the gp
to model the residuals from a polynomial regression, i.e. h(x) = (1, x, x2, . . .).
when    tting the model, one could optimize over the parameters    jointly with
the hyperparameters of the covariance function. alternatively, if we take the
prior on    to be gaussian,        n (b, b), we can also integrate out these
parameters. following o   hagan [1978] we obtain another gp

g(x)     gp(cid:0)h(x)>b, k(x, x0) + h(x)>bh(x0)(cid:1),

(2.40)

now with an added contribution in the covariance function caused by the un-
certainty in the parameters of the mean. predictions are made by plugging
the mean and covariance functions of g(x) into eq. (2.39) and eq. (2.24). after
rearranging, we obtain
  g(x   ) = h>
cov(g   ) = cov(f   ) + r>(b   1 + hk   1

y (y     h>     ) =   f(x   ) + r>     ,

         + k>

y h>)   1r,

    k   1

(2.41)

y h>)   1(hk   1

where the h matrix collects the h(x) vectors for all training (and h    all test)
cases,      = (b   1 + hk   1
y k   .
notice the nice interpretation of the mean expression, eq. (2.41) top line:      is
the mean of the global linear model parameters, being a compromise between
the data term and prior, and the predictive mean is simply the mean linear
output plus what the gp model predicts from the residuals. the covariance is
the sum of the usual covariance term and a new non-negative contribution.

y y + b   1b), and r = h        hk   1

exploring the limit of the above expressions as the prior on the    param-
eter becomes vague, b   1     o (where o is the matrix of zeros), we obtain a
predictive distribution which is independent of b

  g(x   ) =   f(x   ) + r>     ,
cov(g   ) = cov(f   ) + r>(hk   1

y h>)   1r,

(2.42)

y h>)   1hk   1

where the limiting      = (hk   1
y y. notice that predictions under
the limit b   1     o should not be implemented na    vely by plugging the modi   ed
covariance function from eq. (2.40) into the standard prediction equations, since
the entries of the covariance function tend to in   nity, thus making it unsuitable
for numerical implementation. instead eq. (2.42) must be used. even if the
non-limiting case is of interest, eq. (2.41) is numerically preferable to a direct
implementation based on eq. (2.40), since the global linear part will often add
some very large eigenvalues to the covariance matrix, a   ecting its condition
number.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.8 history and related work

2.7.1 marginal likelihood

in this short section we brie   y discuss the marginal likelihood for the model
with a gaussian prior        n (b, b) on the explicit parameters from eq. (2.40),
as this will be useful later, particularly in section 6.3.1. we can express the
marginal likelihood from eq. (2.30) as

29

log p(y|x, b, b) =     1
    1

2(h>b     y)>(ky + h>bh)   1(h>b     y)
2 log |ky + h>bh|     n

2 log 2  ,

(2.43)

where we have included the explicit mean. we are interested in exploring the
limit where b   1     o, i.e. when the prior is vague. in this limit the mean of the
prior is irrelevant (as was the case in eq. (2.42)), so without loss of generality
(for the limiting case) we assume for now that the mean is zero, b = 0, giving

log p(y|x, b=0, b) =     1
    1
y h> and c = k   1

2 y>k   1
y y + 1
2 log |ky|     1

where a = b   1 + hk   1
the matrix inversion lemma, eq. (a.9) and eq. (a.10).

2 y>cy
2 log |b|     1
y h>a   1hk   1

y

2 log |a|     n

2 log 2  ,

(2.44)

and we have used

we now explore the behaviour of the log marginal likelihood in the limit of
vague priors on   . in this limit the variances of the gaussian in the directions
spanned by columns of h> will become in   nite, and it is clear that this will
require special treatment. the log marginal likelihood consists of three terms:
a quadratic form in y, a log determinant term, and a term involving log 2  .
performing an eigendecomposition of the covariance matrix we see that the
contributions to quadratic form term from the in   nite-variance directions will
be zero. however, the log determinant term will tend to minus in   nity. the
standard solution [wahba, 1985, ansley and kohn, 1985] in this case is to
project y onto the directions orthogonal to the span of h> and compute the
marginal likelihood in this subspace. let the rank of h> be m. then as
shown in ansley and kohn [1985] this means that we must discard the terms
    1
2 log |b|     m
log p(y|x) =     1
where a = hk   1

2 log 2   from eq. (2.44) to give
2 y>cy     1
y h>a   1hk   1
y .

y h> and c = k   1

2 log |a|     n   m

2 log |ky|     1

2 y>k   1

y y + 1

log 2  ,

(2.45)

2

2.8 history and related work

prediction with gaussian processes is certainly not a very recent topic, espe-
cially for time series analysis; the basic theory goes back at least as far as the
work of wiener [1949] and kolmogorov [1941] in the 1940   s. indeed lauritzen
[1981] discusses relevant work by the danish astronomer t. n. thiele dating
from 1880.

time series

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

30

geostatistics

kriging

computer experiments

machine learning

regression

gaussian process prediction is also well known in the geostatistics    eld (see,
e.g. matheron, 1973; journel and huijbregts, 1978) where it is known as krig-
ing,17 and in meteorology [thompson, 1956, daley, 1991] although this litera-
ture naturally has focussed mostly on two- and three-dimensional input spaces.
whittle [1963, sec. 5.4] also suggests the use of such methods for spatial pre-
diction. ripley [1981] and cressie [1993] provide useful overviews of gaussian
process prediction in spatial statistics.

gradually it was realized that gaussian process prediction could be used in
a general regression context. for example o   hagan [1978] presents the general
theory as given in our equations 2.23 and 2.24, and applies it to a number of
one-dimensional regression problems. sacks et al. [1989] describe gpr in the
context of computer experiments (where the observations y are noise free) and
discuss a number of interesting directions such as the optimization of parameters
in the covariance function (see our chapter 5) and experimental design (i.e. the
choice of x-points that provide most information on f). the authors describe
a number of computer simulations that were modelled, including an example
where the response variable was the clock asynchronization in a circuit and the
inputs were six transistor widths. santner et al. [2003] is a recent book on the
use of gps for the design and analysis of computer experiments.

williams and rasmussen [1996] described gaussian process regression in
a machine learning context, and described optimization of the parameters in
the covariance function, see also rasmussen [1996]. they were inspired to use
gaussian process by the connection to in   nite neural networks as described in
section 4.2.3 and in neal [1996]. the    kernelization    of linear ridge regression
described above is also known as kernel ridge regression see e.g. saunders et al.
[1998].

relationships between gaussian process prediction and id173 the-
ory, splines, support vector machines (id166s) and relevance vector machines
(rvms) are discussed in chapter 6.

2.9 exercises

1. replicate the generation of random functions from figure 2.2. use a
regular (or random) grid of scalar inputs and the covariance function from
eq. (2.16). hints on how to generate random samples from multi-variate
gaussian distributions are given in section a.2.
invent some training
data points, and make random draws from the resulting gp posterior
using eq. (2.19).

2. in eq. (2.11) we saw that the predictive variance at x    under the feature
space regression model was var(f(x   )) =   (x   )>a   1  (x   ). show that
cov(f(x   ), f(x0
   ). check that this is compatible with
the expression given in eq. (2.24).

   )) =   (x   )>a   1  (x0

17matheron named the method after the south african mining engineer d. g. krige.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

2.9 exercises

31

3. the wiener process is de   ned for x     0 and has f(0) = 0. (see sec-
tion b.2.1 for further details.)
it has mean zero and a non-stationary
covariance function k(x, x0) = min(x, x0). if we condition on the wiener
process passing through f(1) = 0 we obtain a process known as the brow-
nian bridge (or tied-down wiener process). show that this process has
covariance k(x, x0) = min(x, x0)    xx0 for 0     x, x0     1 and mean 0. write
a computer program to draw samples from this process at a    nite grid of
x points in [0, 1].

4. let varn(f(x   )) be the predictive variance of a gaussian process regres-
sion model at x    given a dataset of size n. the corresponding predictive
variance using a dataset of only the    rst n     1 training points is de-
noted varn   1(f(x   )). show that varn(f(x   ))     varn   1(f(x   )), i.e. that
the predictive variance at x    cannot increase as more training data is ob-
tained. one way to approach this problem is to use the partitioned matrix
equations given in section a.3 to decompose varn(f(x   )) = k(x   , x   )    
k>
ni)   1k   . an alternative information theoretic argument is given
    (k +  2
in williams and vivarelli [2000]. note that while this conclusion is true
for gaussian process priors and gaussian noise models it does not hold
generally, see barber and saad [1996].

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

chapter 3

classi   cation

in chapter 2 we have considered regression problems, where the targets are
real valued. another important class of problems is classi   cation 1 problems,
where we wish to assign an input pattern x to one of c classes, c1, . . . ,cc.
practical examples of classi   cation problems are handwritten digit recognition
(where we wish to classify a digitized image of a handwritten digit into one of
ten classes 0-9), and the classi   cation of objects detected in astronomical sky
surveys into stars or galaxies. (information on the distribution of galaxies in
the universe is important for theories of the early universe.) these examples
nicely illustrate that classi   cation problems can either be binary (or two-class,
c = 2) or multi-class (c > 2).

we will focus attention on probabilistic classi   cation, where test predictions
take the form of class probabilities; this contrasts with methods which provide
only a guess at the class label, and this distinction is analogous to the di   erence
between predictive distributions and point predictions in the regression setting.
since generalization to test cases inherently involves some level of uncertainty,
it seems natural to attempt to make predictions in a way that re   ects these
uncertainties. in a practical application one may well seek a class guess, which
can be obtained as the solution to a decision problem, involving the predictive
probabilities as well as a speci   cation of the consequences of making speci   c
predictions (the id168).

both classi   cation and regression can be viewed as function approximation
problems. unfortunately, the solution of classi   cation problems using gaussian
processes is rather more demanding than for the regression problems considered
in chapter 2. this is because we assumed in the previous chapter that the
likelihood function was gaussian; a gaussian process prior combined with a
gaussian likelihood gives rise to a posterior gaussian process over functions,
and everything remains analytically tractable. for classi   cation models, where
the targets are discrete class labels, the gaussian likelihood is inappropriate;2

1in the statistics literature classi   cation is often called discrimination.
2one may choose to ignore the discreteness of the target values, and use a regression
treatment, where all targets happen to be say   1 for binary classi   cation. this is known as

binary, multi-class

probabilistic
classi   cation

non-gaussian likelihood

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

34

classi   cation

in this chapter we treat methods of approximate id136 for classi   cation,
where exact id136 is not feasible.3

section 3.1 provides a general discussion of classi   cation problems, and de-
scribes the generative and discriminative approaches to these problems.
in
section 2.1 we saw how gaussian process regression (gpr) can be obtained
in section 3.2 we describe an analogue of
by generalizing id75.
id75 in the classi   cation case, id28.
in section 3.3
id28 is generalized to yield gaussian process classi   cation (gpc)
using again the ideas behind the generalization of id75 to gpr.
for gpr the combination of a gp prior with a gaussian likelihood gives rise
to a posterior which is again a gaussian process. in the classi   cation case the
likelihood is non-gaussian but the posterior process can be approximated by a
gp. the laplace approximation for gpc is described in section 3.4 (for binary
classi   cation) and in section 3.5 (for multi-class classi   cation), and the expecta-
tion propagation algorithm (for binary classi   cation) is described in section 3.6.
both of these methods make use of a gaussian approximation to the posterior.
experimental results for gpc are given in section 3.7, and a discussion of these
results is provided in section 3.8.

3.1 classi   cation problems

generative approach

discriminative approach

generative model
example

the natural starting point for discussing approaches to classi   cation is the
joint id203 p(y, x), where y denotes the class label. using bayes    theorem
this joint id203 can be decomposed either as p(y)p(x|y) or as p(x)p(y|x).
this gives rise to two di   erent approaches to classi   cation problems. the    rst,
which we call the generative approach, models the class-conditional distribu-
tions p(x|y) for y = c1, . . . ,cc and also the prior probabilities of each class,
and then computes the posterior id203 for each class using

p(y|x) =

pc
p(y)p(x|y)
c=1 p(cc)p(x|cc)

.

(3.1)

the alternative approach, which we call the discriminative approach, focusses
on modelling p(y|x) directly. dawid [1976] calls the generative and discrimina-
tive approaches the sampling and diagnostic paradigms, respectively.

to turn both the generative and discriminative approaches into practical
methods we will need to create models for either p(x|y), or p(y|x) respectively.4
these could either be of parametric form, or non-parametric models such as
those based on nearest neighbours. for the generative case a simple, com-

least-squares classi   cation, see section 6.5.

3note, that the important distinction is between gaussian and non-gaussian likelihoods;
regression with a non-gaussian likelihood requires a similar treatment, but since classi   cation
de   nes an important conceptual and application area, we have chosen to treat it in a separate
chapter; for non-gaussian likelihoods in general, see section 9.3.

4for the generative approach id136 for p(y) is generally straightforward, being esti-
mation of a binomial id203 in the binary case, or a multinomial id203 in the
multi-class case.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.1 classi   cation problems

35

discriminative model
example

response function

probit regression

generative or
discriminative?

missing values

mon choice would be to model the class-conditional densities with gaussians:
p(x|cc) = n (  c,   c). a bayesian treatment can be obtained by placing appro-
priate priors on the mean and covariance of each of the gaussians. however,
note that this gaussian model makes a strong assumption on the form of class-
conditional density and if this is inappropriate the model may perform poorly.
for the binary discriminative case one simple idea is to turn the output of a
regression model into a class id203 using a response function (the inverse
of a link function), which    squashes    its argument, which can lie in the domain
(      ,   ), into the range [0, 1], guaranteeing a valid probabilistic interpretation.

one example is the linear id28 model

p(c1|x) =   (x>w), where   (z) =

1

1 + exp(   z) ,

(3.2)

standard normal distribution   (z) =r z

which combines the linear model with the logistic response function. another
common choice of response function is the cumulative density function of a
       n (x|0, 1)dx. this approach is known
as probit regression. just as we gave a bayesian approach to id75 in
chapter 2 we can take a parallel approach to id28, as discussed in
section 3.2. as in the regression case, this model is an important step towards
the gaussian process classi   er.

given that there are the generative and discriminative approaches, which
one should we prefer? this is perhaps the biggest question in classi   cation,
and we do not believe that there is a right answer, as both ways of writing the
joint p(y, x) are correct. however, it is possible to identify some strengths and
weaknesses of the two approaches. the discriminative approach is appealing
in that it is directly modelling what we want, p(y|x). also, density estimation
for the class-conditional distributions is a hard problem, particularly when x is
high dimensional, so if we are just interested in classi   cation then the generative
approach may mean that we are trying to solve a harder problem than we need
to. however, to deal with missing input values, outliers and unlabelled data
points in a principled fashion it is very helpful to have access to p(x), and
this can be obtained from marginalizing out the class label y from the joint
y p(y)p(x|y) in the generative approach. a further factor in the
choice of a generative or discriminative approach could also be which one is
most conducive to the incorporation of any prior information which is available.
see ripley [1996, sec. 2.1] for further discussion of these issues. the gaussian
process classi   ers developed in this chapter are discriminative.

as p(x) = p

3.1.1 decision theory for classi   cation
the classi   ers described above provide predictive probabilities p(y   |x   ) for a
test input x   . however, sometimes one actually needs to make a decision and
to do this we need to consider decision theory. decision theory for the regres-
sion problem was considered in section 2.4; here we discuss decision theory for
classi   cation problems. a comprehensive treatment of decision theory can be
found in berger [1985].

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

36

loss, risk

zero-one loss

asymmetric loss

bayes classi   er

decision regions

reject option

risk minimization

classi   cation

given x is rl(c0|x) =p

let l(c, c0) be the loss incurred by making decision c0 if the true class is cc.
usually l(c, c) = 0 for all c. the expected loss5 (or risk) of taking decision c0
c l(c, c0)p(cc|x) and the optimal decision c    is the one
that minimizes rl(c0|x). one common choice of id168 is the zero-one
loss, where a penalty of one unit is paid for an incorrect classi   cation, and 0
for a correct one. in this case the optimal decision rule is to choose the class cc
that maximizes6 p(cc|x), as this minimizes the expected error at x. however,
the zero-one loss is not always appropriate. a classic example of this is the
di   erence in loss of failing to spot a disease when carrying out a medical test
compared to the cost of a false positive on the test, so that l(c, c0) 6= l(c0, c).
the optimal classi   er (using zero-one loss) is known as the bayes classi-
   er. by this construction the feature space is divided into decision regions
r1, . . . ,rc such that a pattern falling in decision region rc is assigned to class
cc. (there can be more than one decision region corresponding to a single class.)
the boundaries between the decision regions are known as decision surfaces or
decision boundaries.

one would expect misclassi   cation errors to occur in regions where the max-
imum class id203 maxj p(cj|x) is relatively low. this could be due to
either a region of strong overlap between classes, or lack of training examples
within this region. thus one sensible strategy is to add a reject option so that
if maxj p(cj|x)        for a threshold    in (0, 1) then we go ahead and classify
the pattern, otherwise we reject it and leave the classi   cation task to a more
sophisticated system. for multi-class classi   cation we could alternatively re-
quire the gap between the most probable and the second most probable class to
exceed   , and otherwise reject. as    is varied from 0 to 1 one obtains an error-
reject curve, plotting the percentage of patterns classi   ed incorrectly against
the percentage rejected. typically the error rate will fall as the rejection rate
increases. hansen et al. [1997] provide an analysis of the error-reject trade-o   .
we have focused above on the probabilistic approach to classi   cation, which
involves a two-stage approach of    rst computing a posterior distribution over
functions and then combining this with the id168 to produce a decision.
however, it is worth noting that some authors argue that if our goal is to
eventually make a decision then we should aim to approximate the classi   cation
function that minimizes the risk (expected loss), which is de   ned as

z

l(cid:0)y, c(x)(cid:1)p(y, x) dydx,

rl(c) =

(3.3)

where p(y, x) is the joint distribution of inputs and targets and c(x) is a clas-
si   cation function that assigns an input pattern x to one of c classes (see
pn
e.g. vapnik [1995, ch. 1]). as p(y, x) is unknown, in this approach one often
then seeks to minimize an objective function which includes the empirical risk
i=1 l(yi, c(xi)) as well as a id173 term. while this is a reasonable

5in economics one usually talks of maximizing expected utility rather than minimizing
expected loss; loss is negative utility. this suggests that statisticians are pessimists while
economists are optimists.

6if more than one class has equal posterior id203 then ties can be broken arbitrarily.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.2 linear models for classi   cation

37

method, we note that the probabilistic approach allows the same id136 stage
to be re-used with di   erent id168s, it can help us to incorporate prior
knowledge on the function and/or noise model, and has the advantage of giving
probabilistic predictions which can be helpful e.g. for the reject option.

3.2 linear models for classi   cation

in this section we brie   y review linear models for binary classi   cation, which
form the foundation of gaussian process classi   cation models in the next sec-
tion. we follow the id166 literature and use the labels y = +1 and y =    1 to
distinguish the two classes, although for the multi-class case in section 3.5 we
use 0/1 labels. the likelihood is

p(y =+1|x, w) =   (x>w),

(3.4)

given the weight vector w and   (z) can be any sigmo  function. when using
the logistic,   (z) =   (z) from eq. (3.2), the model is usually called simply logistic
regression, but to emphasize the parallels to id75 we prefer the term
linear id28. when using the cumulative gaussian   (z) =   (z),
we call the model linear probit regression.

as the id203 of the two classes must sum to 1, we have p(y =   1|x, w) =
1     p(y = +1|x, w). thus for a data point (xi, yi) the likelihood is given by
i w) if yi =    1. for symmetric likelihood
i w) if yi = +1, and 1       (x>
  (x>
functions, such as the logistic or probit where   (   z) = 1       (z), this can be
written more concisely as

p(yi|xi, w) =   (yifi),

(3.5)

log(cid:0)p(y = +1|x)/p(y =   1|x)(cid:1) we see that the id28 model can be

where fi , f(xi) = x>
i w. de   ning the logit transformation as logit(x) =
written as logit(x) = x>w. the logit(x) function is also called the log odds
ratio. generalized linear modelling [mccullagh and nelder, 1983] deals with
the issue of extending linear models to non-gaussian data scenarios; the logit
transformation is the canonical link function for binary data and this choice
simpli   es the algebra and algorithms.

given a dataset d = {(xi, yi)|i = 1, . . . , n}, we assume that the labels are
generated independently, conditional on f(x). using the same gaussian prior
w     n (0,   p) as for regression in eq. (2.4) we then obtain the un-normalized
log posterior

log p(w|x, y) c=    1
2

w>     1

p w +

log   (yifi).

(3.6)

nx

i=1

in the id75 case with gaussian noise the posterior was gaussian
with mean and covariance as given in eq. (2.8). for classi   cation the posterior
7a sigmoid function is a monotonically increasing function mapping from r to [0, 1]. it

derives its name from being shaped like a letter s.

linear id28

linear probit regression

concise notation

logit

log odds ratio

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

38

concavity

unique maximum

irls algorithm

properties of maximum
likelihood

classi   cation

does not have a simple analytic form. however, it is easy to show that for
some sigmoid functions, such as the logistic and cumulative gaussian, the log
likelihood is a concave function of w for    xed d. as the quadratic penalty on
w is also concave then the log posterior is a concave function, which means
that it is relatively easy to    nd its unique maximum. the concavity can also be
derived from the fact that the hessian of log p(w|x, y) is negative de   nite (see
section a.9 for further details). the standard algorithm for    nding the maxi-
mum is newton   s method, which in this context is usually called the iteratively
reweighted least squares (irls) algorithm, as described e.g. in mccullagh and
nelder [1983]. however, note that minka [2003] provides evidence that other
optimization methods (e.g. conjugate gradient ascent) may be faster than irls.
notice that a maximum likelihood treatment (corresponding to an unpe-
nalized version of eq. (3.6)) may result in some undesirable outcomes. if the
dataset is linearly separable (i.e. if there exists a hyperplane which separates the
positive and negative examples) then maximizing the (unpenalized) likelihood
will cause |w| to tend to in   nity, however, this will still give predictions in [0, 1]
for p(y = +1|x, w), although these predictions will be    hard    (i.e. zero or one).
if the problem is ill-conditioned, e.g. due to duplicate (or linearly dependent)
input dimensions, there will be no unique solution.

predictions

softmax
multiple logistic

as an example, consider linear id28 in the case where x-space
is two dimensional and there is no bias weight so that w is also two-dimensional.
the prior in weight space is gaussian and for simplicity we have set   p = i.
contours of the prior p(w) are illustrated in figure 3.1(a). if we have a data set
d as shown in figure 3.1(b) then this induces a posterior distribution in weight
space as shown in figure 3.1(c). notice that the posterior is non-gaussian
and unimodal, as expected. the dataset is not linearly separable but a weight
vector in the direction (1, 1)> is clearly a reasonable choice, as the posterior
distribution shows. to make predictions based the training set d for a test
point x    we have

p(y    =+1|x   ,d) =

p(y    =+1|w, x   )p(w|d) dw,

(3.7)

z

integrating the prediction p(y    =+1|w, x   ) =   (x>
    w) over the posterior distri-
bution of weights. this leads to contours of the predictive distribution as shown
in figure 3.1(d). notice how the contours are bent, re   ecting the integration
of many di   erent but plausible w   s.

in the multi-class case we use the multiple logistic (or softmax) function

p(y = cc|x, w ) =

p
exp(x>wc)
c0 exp(x>wc0) ,

(3.8)

pn

where wc is the weight vector for class c, and all weight vectors are col-
lected into the matrix w . the corresponding log likelihood is of the form
i wc0))]. as in the binary case the log

i wc     log(p

pc
c=1   c,yi[x>

c0 exp(x>

i=1

likelihood is a concave function of w .

it is interesting to note that in a generative approach where the class-
conditional distributions p(x|y) are gaussian with the same covariance matrix,

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.3 gaussian process classi   cation

39

(a)

(b)

(c)

(d)

figure 3.1: linear id28: panel (a) shows contours of the prior distri-
bution p(w) = n (0, i). panel (b) shows the dataset, with circles indicating class +1
and crosses denoting class    1. panel (c) shows contours of the posterior distribution
p(w|d). panel (d) shows contours of the predictive distribution p(y    = +1|x   ).

p(y|x) has the form given by eq. (3.4) and eq. (3.8) for the two- and multi-class
cases respectively (when the constant function 1 is included in x).

3.3 gaussian process classi   cation

for binary classi   cation the basic idea behind gaussian process prediction
is very simple   we place a gp prior over the latent function f(x) and then
   squash    this through the logistic function to obtain a prior on   (x) , p(y =
+1|x) =   (f(x)). note that    is a deterministic function of f, and since f
is stochastic, so is   . this construction is illustrated in figure 3.2 for a one-
it is a natural generalization of the linear logistic
dimensional input space.

latent function

   2   1012   2   1012w1w2   505   505x1x2   2   1012   2   1012w1w2   505   505x1x20.10.50.9c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

40

classi   cation

(a)

(b)

figure 3.2: panel (a) shows a sample latent function f (x) drawn from a gaussian
process as a function of x. panel (b) shows the result of squashing this sample func-
tion through the logistic logit function,   (z) = (1 + exp(   z))   1 to obtain the class
id203   (x) =   (f (x)).

regression model and parallels the development from id75 to gp
regression that we explored in section 2.1. speci   cally, we replace the linear
f(x) function from the linear logistic model in eq. (3.6) by a gaussian process,
and correspondingly the gaussian prior on the weights by a gp prior.

the latent function f plays the r  ole of a nuisance function: we do not
observe values of f itself (we observe only the inputs x and the class labels y)
and we are not particularly interested in the values of f, but rather in   , in
particular for test cases   (x   ). the purpose of f is solely to allow a convenient
formulation of the model, and the computational goal pursued in the coming
sections will be to remove (integrate out) f.

we have tacitly assumed that the latent gaussian process is noise-free, and
combined it with smooth likelihood functions, such as the logistic or probit.
however, one can equivalently think of adding independent noise to the latent
process in combination with a step-function likelihood. in particular, assuming
gaussian noise and a step-function likelihood is exactly equivalent to a noise-
free8 latent process and probit likelihood, see exercise 3.10.1.

nuisance function

noise-free latent process

id136 is naturally divided into two steps:    rst computing the distribution

of the latent variable corresponding to a test case

p(f   |x, y, x   ) =

p(f   |x, x   , f)p(f|x, y) df ,

(3.9)

where p(f|x, y) = p(y|f)p(f|x)/p(y|x) is the posterior over the latent vari-
ables, and subsequently using this distribution over the latent f    to produce a
probabilistic prediction

        , p(y    =+1|x, y, x   ) =

  (f   )p(f   |x, y, x   ) df   .

(3.10)

z

z

8this equivalence explains why no numerical problems arise from considering a noise-free
process if care is taken with the implementation, see also comment at the end of section 3.4.3.

   4   2024input, xlatent function, f(x)01input, xclass id203,   (x)c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.4 the laplace approximation for the binary gp classi   er

41

in the regression case (with gaussian likelihood) computation of predictions was
straightforward as the relevant integrals were gaussian and could be computed
analytically.
in classi   cation the non-gaussian likelihood in eq. (3.9) makes
the integral analytically intractable. similarly, eq. (3.10) can be intractable
analytically for certain sigmoid functions, although in the binary case it is
only a one-dimensional integral so simple numerical techniques are generally
adequate.

thus we need to use either analytic approximations of integrals, or solutions
based on monte carlo sampling. in the coming sections, we describe two ana-
lytic approximations which both approximate the non-gaussian joint posterior
with a gaussian one: the    rst is the straightforward laplace approximation
method [williams and barber, 1998], and the second is the more sophisticated
expectation propagation (ep) method due to minka [2001]. (the cavity tap ap-
proximation of opper and winther [2000] is closely related to the ep method.)
a number of other approximations have also been suggested, see e.g. gibbs and
mackay [2000], jaakkola and haussler [1999], and seeger [2000]. neal [1999]
describes the use of id115 (mcmc) approximations. all
of these methods will typically scale as o(n3); for large datasets there has been
much work on further approximations to reduce computation time, as discussed
in chapter 8.

the laplace approximation for the binary case is described in section 3.4,
and for the multi-class case in section 3.5. the ep method for binary clas-
si   cation is described in section 3.6. relationships between gaussian process
classi   ers and other techniques such as spline classi   ers, support vector ma-
chines and least-squares classi   cation are discussed in sections 6.3, 6.4 and 6.5
respectively.

3.4 the laplace approximation for the binary

gp classi   er

laplace   s method utilizes a gaussian approximation q(f|x, y) to the poste-
rior p(f|x, y) in the integral (3.9). doing a second order taylor expansion
of log p(f|x, y) around the maximum of the posterior, we obtain a gaussian
approximation

q(f|x, y) = n (f|  f , a   1)     exp(cid:0)    1

2(f       f)>a(f       f)(cid:1),

(3.11)
where   f = argmaxf p(f|x, y) and a =           log p(f|x, y)|f =  f is the hessian of
the negative log posterior at that point.

the structure of the rest of this section is as follows: in section 3.4.1 we
describe how to    nd   f and a. section 3.4.2 explains how to make predictions
having obtained q(f|y), and section 3.4.3 gives more implementation details
for the laplace gp classi   er. the laplace approximation for the marginal
likelihood is described in section 3.4.4.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

42

classi   cation

(a), logistic

(b), probit

figure 3.3: the log likelihood and its derivatives for a single case as a function of
zi = yifi, for (a) the logistic, and (b) the cumulative gaussian likelihood. the two
likelihood functions are fairly similar, the main qualitative di   erence being that for
large negative arguments the log logistic behaves linearly whereas the log cumulative
gaussian has a quadratic penalty. both likelihoods are log concave.

3.4.1 posterior
by bayes    rule the posterior over the latent variables is given by p(f|x, y) =
p(y|f)p(f|x)/p(y|x), but as p(y|x) is independent of f, we need only consider
the un-normalized posterior when maximizing w.r.t. f. taking the logarithm
and introducing expression eq. (2.29) for the gp prior gives

  (f) , log p(y|f) + log p(f|x)

= log p(y|f)     1
2

f>k   1f     1
2

log |k|     n
2

log 2  .

(3.12)

di   erentiating eq. (3.12) w.r.t. f we obtain

     (f) =     log p(y|f)     k   1f ,
        (f) =        log p(y|f)     k   1 =    w     k   1,

(3.13)
(3.14)
where w ,           log p(y|f) is diagonal, since the likelihood factorizes over
cases (the distribution for yi depends only on fi, not on fj6=i). note, that if the
likelihood p(y|f) is log concave, the diagonal elements of w are non-negative,
and the hessian in eq. (3.14) is negative de   nite, so that   (f) is concave and
has a unique maximum (see section a.9 for further details).

there are many possible functional forms of the likelihood, which gives the
target class id203 as a function of the latent variable f. two commonly
used likelihood functions are the logistic, and the cumulative gaussian, see
figure 3.3. the expressions for the log likelihood for these likelihood functions
and their    rst and second derivatives w.r.t. the latent variable are given in the

un-normalized posterior

log likelihoods
and their derivatives

   202   3   2   101latent times target, zi=yifi    log likelihood, log p(yi|fi)log likelihood1st derivative2nd derivative   202   6   4   202latent times target, zi=yifi    log likelihood, log p(yi|fi)log likelihood1st derivative2nd derivativec. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.4 the laplace approximation for the binary gp classi   er

43

following table:

   2
log p(yi|fi)
   f 2
i
     i(1       i)

log p(yi|fi)

    log(cid:0)1 + exp(   yifi)(cid:1)

   
   fi

log p(yi|fi)
ti       i
yin (fi)
  (yifi)

(3.15)

(3.16)

log   (yifi)

where we have de   ned   i = p(yi = 1|fi) and t = (y + 1)/2. at the maximum
of   (f) we have

    n (fi)2
  (yifi)2     yifin (fi)
      = 0 =      f = k(cid:0)    log p(y|  f)(cid:1),

(3.17)
as a self-consistent equation for   f (but since     log p(y|  f) is a non-linear function
of   f, eq. (3.17) cannot be solved directly). to    nd the maximum of    we use
newton   s method, with the iteration

  (yifi)

f new = f     (        )   1      = f + (k   1 + w )   1(    log p(y|f)     k   1f)

= (k   1 + w )   1(cid:0)w f +     log p(y|f)(cid:1).

(3.18)

to gain more intuition about this update, let us consider what happens to
datapoints that are well-explained under f so that     log p(yi|fi)/   fi and wii
are close to zero for these points. as an approximation, break f into two
subvectors, f1 that corresponds to points that are not well-explained, and f2 to
those that are. then it is easy to show (see exercise 3.10.4) that

= k11(i11 + w11k11)   1(cid:0)w11f1 +     log p(y1|f1)(cid:1),

(3.19)

f new
1
f new
2

= k21k   1

11 f new

1

,

where k21 denotes the n2    n1 block of k containing the covariance between
is computed by ignoring
the two groups of points, etc. this means that f new
using the
entirely the well-explained points, and f new
usual gp prediction methods (i.e. treating these points like test points). of
course, if the predictions of f new
fail to match the targets correctly they would
cease to be well-explained and so be updated on the next iteration.

is predicted from f new

2

1

2

1

having found the maximum posterior   f, we can now specify the laplace
approximation to the posterior as a gaussian with mean   f and covariance matrix
given by the negative inverse hessian of    from eq. (3.14)

q(f|x, y) = n(cid:0)  f , (k   1 + w )   1(cid:1).

(3.20)

one problem with the laplace approximation is that it is essentially un-
controlled, in that the hessian (evaluated at   f) may give a poor approximation
to the true shape of the posterior. the peak could be much broader or nar-
rower than the hessian indicates, or it could be a skew peak, while the laplace
approximation assumes it has elliptical contours.

newton   s method

intuition on in   uence of
well-explained points

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

44

classi   cation

3.4.2 predictions

latent mean

the posterior mean for f    under the laplace approximation can be expressed
by combining the gp predictive mean eq. (2.25) with eq. (3.17) into

eq[f   |x, y, x   ] = k(x   )>k   1  f = k(x   )>    log p(y|  f).

(3.21)

compare this with the exact mean, given by opper and winther [2000] as

ep[f   |x, y, x   ] =

=

e[f   |f , x, x   ]p(f|x, y)df
k(x   )>k   1f p(f|x, y)df = k(x   )>k   1e[f|x, y],

(3.22)

z
z

where we have used the fact that for a gp e[f   |f , x, x   ] = k(x   )>k   1f and
have let e[f|x, y] denote the posterior mean of f given x and y. notice the
similarity between the middle expression of eq. (3.21) and eq. (3.22), where the
exact (intractable) average e[f|x, y] has been replaced with the modal value
  f = eq[f|x, y].

a simple observation from eq. (3.21) is that positive training examples will
give rise to a positive coe   cient for their id81 (as    i log p(yi|fi) > 0
in this case), while negative examples will give rise to a negative coe   cient;
this is analogous to the solution to the support vector machine, see eq. (6.34).
also note that training points which have    i log p(yi|fi)     0 (i.e. that are
well-explained under   f) do not contribute strongly to predictions at novel test
points; this is similar to the behaviour of non-support vectors in the support
vector machine (see section 6.4).

we can also compute vq[f   |x, y], the variance of f   |x, y under the gaussian

approximation. this comprises of two terms, i.e.

vq[f   |x, y, x   ] = ep(f   |x,x   ,f )[(f        e[f   |x, x   , f])2]

+ eq(f|x,y)[(e[f   |x, x   , f]     e[f   |x, y, x   ])2].

(3.23)

the    rst term is due to the variance of f    if we condition on a particular value
of f, and is given by k(x   , x   )     k(x   )>k   1k(x   ), cf. eq. (2.19). the second
term in eq. (3.23) is due to the fact that e[f   |x, x   , f] = k(x   )>k   1f depends
on f and thus there is an additional term of k(x   )>k   1 cov(f|x, y)k   1k(x   ).
under the gaussian approximation cov(f|x, y) = (k   1 + w )   1, and thus
    k   1(k   1 + w )   1k   1k   

vq[f   |x, y, x   ] = k(x   , x   )   k>
= k(x   , x   )   k>

    k   1k    + k>
    (k + w    1)   1k   ,

(3.24)

where the last line is obtained using the matrix inversion lemma eq. (a.9).
given the mean and variance of f   , we make predictions by computing

            eq[     |x, y, x   ] =

  (f   )q(f   |x, y, x   ) df   ,

(3.25)

z

sign of kernel
coe   cients

latent variance

averaged predictive
id203

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.4 the laplace approximation for the binary gp classi   er

45

map prediction

identical binary
decisions

where q(f   |x, y, x   ) is gaussian with mean and variance given by equations
3.21 and 3.24 respectively. notice that because of the non-linear form of the
sigmoid the predictive id203 from eq. (3.25) is di   erent from the sigmoid
of the expectation of f:         =   (eq[f   |y]). we will call the latter the map
prediction to distinguish it from the averaged predictions from eq. (3.25).

in fact, as shown in bishop [1995, sec. 10.3], the predicted test labels
given by choosing the class of highest id203 obtained by averaged and
map predictions are identical for binary 9 classi   cation. to see this, note
that the decision boundary using the the map value eq[f   |x, y, x   ] corre-
sponds to   (eq[f   |x, y, x   ]) = 1/2 or eq[f   |x, y, x   ] = 0. the decision bound-
ary of the averaged prediction, eq[     |x, y, x   ] = 1/2, also corresponds to
eq[f   |x, y, x   ] = 0. this follows from the fact that   (f   )     1/2 is antisym-
metric while q(f   |x, y, x   ) is symmetric.

thus if we are concerned only about the most probable classi   cation, it is
not necessary to compute predictions using eq. (3.25). however, as soon as we
also need a con   dence in the prediction (e.g. if we are concerned about a reject
option) we need eq[     |x, y, x   ]. if   (z) is the cumulative gaussian function
then eq. (3.25) can be computed analytically, as shown in section 3.9. on
the other hand if    is the logistic function then we need to resort to sampling
methods or analytical approximations to compute this one-dimensional integral.
one attractive method is to note that the logistic function   (z) is the c.d.f.
(cumulative density function) corresponding to the p.d.f. (id203 density
function) p(z) = sech2(z/2)/4; this is known as the logistic or sech-squared
distribution, see johnson et al. [1995, ch. 23]. then by approximating p(z) as a
mixture of gaussians, one can approximate   (z) by a linear combination of error
functions. this approximation was used by williams and barber [1998, app. a]
and wood and kohn [1998]. another approximation suggested in mackay
[1992d] is               (  (f   |y)   f   ), where   2(f   |y) = (1 +   vq[f   |x, y, x   ]/8)   1.
the e   ect of the latent predictive variance is, as the approximation suggests,
to    soften    the prediction that would be obtained using the map prediction
        =   (   f   ), i.e. to move it towards 1/2.

3.4.3

implementation

we give implementations for    nding the laplace approximation in algorithm
3.1 and for making predictions in algorithm 3.2. care is taken to avoid numer-
ically unstable computations while minimizing the computational e   ort; both
can be achieved simultaneously. it turns out that several of the desired terms
can be expressed in terms of the symmetric positive de   nite matrix

b = i + w

(3.26)
computation of which costs only o(n2), since w is diagonal. the b matrix has
eigenvalues bounded below by 1 and bounded above by 1 + n maxij(kij)/4, so
for many covariance functions b is guaranteed to be well-conditioned, and it is

1
2 kw

1
2 ,

9for multi-class predictions discussed in section 3.5 the situation is more complicated.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

46

classi   cation

)

input: k (covariance matrix), y (  1 targets), p(y|f) (likelihood function)
initialization
newton iteration
eval. w e.g. using eq. (3.15) or (3.16)

1
2 kw

1

2 )

b = i + w

1
2 kw

1
2

2: f := 0
repeat
4: w :=           log p(y|f)
l := cholesky(i + w
b := w f +     log p(y|f)
a := b     w
2 l>\(l\(w
f := ka

6:

1

1

8:

2 kb))

i log lii

objective:     1

2 a>f + log p(y|f)    p

eq. (3.18) using eq. (3.27)
2 a>f + log p(y|f)
until convergence
10: log q(y|x,   ) :=     1
eq. (3.32)
return:   f := f (post. mode), log q(y|x,   ) (approx. log marg. likelihood)
algorithm 3.1: mode-   nding for binary laplace gpc. commonly used convergence
criteria depend on the di   erence in successive values of the objective function   (f )
from eq. (3.12), the magnitude of the gradient vector      (f ) from eq. (3.13) and/or the
magnitude of the di   erence in successive values of f . in a practical implementation
one needs to secure against divergence by checking that each iteration leads to an
increase in the objective (and trying a smaller step size if not). the computational
complexity is dominated by the cholesky decomposition in line 5 which takes n3/6
operations (times the number of newton iterations), all other operations are at most
quadratic in n.

thus numerically safe to compute its cholesky decomposition ll> = b, which
is useful in computing terms involving b   1 and |b|.

the mode-   nding procedure uses the newton iteration given in eq. (3.18),
involving the matrix (k   1 +w )   1. using the matrix inversion lemma eq. (a.9)
we get

(k   1 + w )   1 = k     kw

(3.27)
where b is given in eq. (3.26). the advantage is that whereas k may have
eigenvalues arbitrarily close to zero (and thus be numerically unstable to invert),
we can safely work with b. in addition, algorithm 3.1 keeps the vector a =
k   1f in addition to f, as this allows evaluation of the part of the objective
  (f) in eq. (3.12) which depends on f without explicit reference to k   1 (again
to avoid possible numerical problems).

2 b   1w

1
2 k,

1

similarly, for the computation of the predictive variance vq[f   |y] from eq. (3.24)

we need to evaluate a quadratic form involving the matrix (k + w    1)   1. re-
writing this as

(k + w    1)   1 = w

1

2 w     1

2 (k + w    1)   1w     1

2 w

1

2 = w

1

2 b   1w

1
2

(3.28)

achieves numerical stability (as opposed to inverting w itself, which may have
arbitrarily small eigenvalues). thus the predictive variance from eq. (3.24) can
be computed as

vq[f   |y] = k(x   , x   )     k(x   )>w

1

2 (ll>)   1w

1

2 k(x   )

= k(x   , x   )     v>v, where v = l\(w

1

2 k(x   )),

(3.29)

which was also used by seeger [2003, p. 27].

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.4 the laplace approximation for the binary gp classi   er

47

input:   f (mode), x (inputs), y (  1 targets), k (covariance function),

p(y|f) (likelihood function), x    test input

2: w :=           log p(y|  f)
1
l := cholesky(i + w
2 kw
4:   f    := k(x   )>    log p(y|  f)
6: v[f   ] := k(x   , x   )     v>v

v := l\(cid:0)w
        :=r   (z)n (z|   f   , v[f   ])dz

2 k(x   )(cid:1)

1

1

2 )

o

b = i + w

1
2 kw

1
2

eq. (3.21)

eq. (3.24) using eq. (3.29)
eq. (3.25)

8: return:         (predictive class id203 (for class 1))
algorithm 3.2: predictions for binary laplace gpc. the posterior mode   f (which
can be computed using algorithm 3.1) is input. for multiple test inputs lines 4    7 are
applied to each test input. computational complexity is n3/6 operations once (line
3) plus n2 operations per test case (line 5). the one-dimensional integral in line 7
can be done analytically for cumulative gaussian likelihood, otherwise it is computed
using an approximation or numerical quadrature.

in practice we compute the cholesky decomposition ll> = b during the
newton steps in algorithm 3.1, which can be re-used to compute the predictive
variance by doing backsubstitution with l as discussed above.
in addition,
l may again be re-used to compute |in + w
2| = |b| (needed for the

computation of the marginal likelihood eq. (3.32)) as log |b| = 2p log lii. to

save computation, one could use an incomplete cholesky factorization in the
newton steps, as suggested by fine and scheinberg [2002].

1
2 kw

1

sometimes it is suggested that it can be useful to replace k by k + i where
  is a small constant, to improve the numerical conditioning10 of k. however,
by taking care with the implementation details as above this should not be
necessary.

3.4.4 marginal likelihood

incomplete cholesky
factorization

z

it will also be useful (particularly for chapter 5) to compute the laplace ap-
proximation of the marginal likelihood p(y|x). (for the regression case with
gaussian noise the marginal likelihood can again be calculated analytically, see
eq. (2.30).) we have

p(y|x) =

p(y|f)p(f|x) df =

(3.30)
using a taylor expansion of   (f) locally around   f we obtain   (f)       (  f)    
2(f      f)>a(f      f) and thus an approximation q(y|x) to the marginal likelihood
1
as

p(y|x)     q(y|x) = exp(cid:0)  (  f)(cid:1)z

exp(cid:0)    1

2(f       f)>a(f       f)(cid:1) df .

(3.31)

z

exp(cid:0)  (f)(cid:1) df .

10neal [1999] refers to this as adding    jitter    in the context of id115
(mcmc) based id136; in his work the latent variables f are explicitly represented in
the markov chain which makes addition of jitter di   cult to avoid. within the analytical
approximations of the distribution of f considered here, jitter is unnecessary.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

48

classi   cation

this gaussian integral can be evaluated analytically to obtain an approximation
to the log marginal likelihood

log q(y|x,   ) =     1

  f>k   1  f + log p(y|  f)     1

(3.32)
where |b| = |k|    |k   1 + w| = |in + w
2|, and    is a vector of hyper-
parameters of the covariance function (which have previously been suppressed
from the notation for brevity).

1
2 kw

2 log |b|,

2

1

   

3.5 multi-class laplace approximation

(cid:1)>

f = (cid:0)f 1

our presentation follows williams and barber [1998]. we    rst introduce the
vector of latent function values at all n training points and for all c classes

.

(3.33)

n, f 2

1 , . . . , f 2

n, . . . , f c

1 , . . . , f c
n

1 , . . . , f 1
thus f has length cn.
in the following we will generally refer to quantities
pertaining to a particular class with superscript c, and a particular case by
subscript i (as usual); thus e.g. the vector of c latents for a particular case is
fi. however, as an exception, vectors or matrices formed from the covariance
function for class c will have a subscript c. the prior over f has the form
f     n (0, k). as we have assumed that the c latent processes are uncorrelated,
the covariance matrix k is block diagonal in the matrices k1, . . . , kc. each
individual matrix kc expresses the correlations of the latent function values
within the class c. note that the covariance functions pertaining to the di   erent
classes can be di   erent. let y be a vector of the same length as f which for
each i = 1, . . . , n has an entry of 1 for the class which is the label for example
i and 0 for the other c     1 entries.

let   c

i denote output of the softmax at training point i, i.e.

p(yc

i|fi) =   c

i =

p

i )
exp(f c
i ) .
c0 exp(f c0

(3.34)

then    is a vector of the same length as f with entries   c
analogue of eq. (3.12) is the log of the un-normalized posterior

i . the multi-class

2 f>k   1f +y>f    nx

log(cid:0) cx

  (f) ,     1

(cid:1)    1

exp f c
i

2 log |k|    cn

2 log 2  . (3.35)

as in the binary case we seek the map value   f of p(f|x, y). by di   erentiating
eq. (3.35) w.r.t. f we obtain

i=1

c=1

(3.36)
thus at the maximum we have   f = k(y         ). di   erentiating again, and using

      =    k   1f + y       .

exp(f j

i ) =   c

i   cc0 +   c

i   c0
i ,

(3.37)

logx

j

       2
   f c

i    f c0

i

softmax

un-normalized posterior

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.5 multi-class laplace approximation

49

we obtain11

         =    k   1     w, where w , diag(  )         >,

(3.38)
where    is a cn  n matrix obtained by stacking vertically the diagonal matrices
diag(  c), and   c is the subvector of    pertaining to class c. as in the binary case
notice that             is positive de   nite, thus   (f) is concave and the maximum
is unique (see also exercise 3.10.2).

predictive
distribution for f   

z

as in the binary case we use newton   s method to search for the mode of   ,

giving

f new = (k   1 + w )   1(w f + y       ).

(3.39)
this update if coded na    vely would take o(c 3n3) as matrices of size cn have to
be inverted. however, as described in section 3.5.1, we can utilize the structure
of w to bring down the computational load to o(cn3).

the laplace approximation gives us a gaussian approximation q(f|x, y) to
the posterior p(f|x, y). to make predictions at a test point x    we need to com-
pute the posterior distribution q(f   |x, y, x   ) where f(x   ) , f    = (f 1    , . . . , f c    )>.
in general we have

q(f   |x, y, x   ) =

p(f   |x, x   , f)q(f|x, y) df .

(3.40)
as p(f   |x, x   , f) and q(f|x, y) are both gaussian, q(f   |x, y, x   ) will also be
gaussian and we need only compute its mean and covariance. the predictive
mean for class c is given by

eq[f c(x   )|x, y, x   ] = kc(x   )>k   1

c

  f c = kc(x   )>(yc         c),

(3.41)

where kc(x   ) is the vector of covariances between the test point and each of
the training points for the cth covariance function, and   f c is the subvector of
  f pertaining to class c. the last equality comes from using eq. (3.36) at the
maximum   f. note the close correspondence to eq. (3.21). this can be put into
a vector form eq[f   |y] = q>

    (y         ) by de   ning the cn    c matrix
k1(x   )

0

               

q    =

k2(x   )

...
0

0
0
...

. . .
. . .
...
. . . kc(x   )

0
...
0

                .

using a similar argument to eq. (3.23) we obtain

covq(f   |x, y, x   ) =    + q>

    k   1(k   1 + w )   1k   1q   

= diag(k(x   , x   ))     q>

    (k + w    1)   1q   ,

where    is a diagonal c    c matrix with   cc = kc(x   , x   )    k>
c (x   )k   1
and k(x   , x   ) is a vector of covariances, whose c   th element is kc(x   , x   ).

c kc(x   ),

11there is a sign error in equation 23 of williams and barber [1998] but not in their

implementation.

(3.42)

(3.43)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

50

classi   cation

input: k (covariance matrix), y (0/1 targets)

4:

6:

2: f := 0
repeat

initialization
newton iteration
compute    and    from f with eq. (3.34) and defn. of    under eq. (3.38)
for c := 1 . . . c do

c )
e is block diag. d

1

2 (icn + d

1

1
2 kd

2 )   1d
2 log determinant

1
2

compute 1

1
2
c kcd

1
2

zc :=p

l := cholesky(in + d
c l>\(l\d
ec := d
c )
i log lii

1
2

1
2

8:

10: m := cholesky(p

c ec)

end for
b := (d         >)f + y       
c := ekb
a := b    c + erm>\(m\(r>c))
f := ka

12:

14:

b = w f + y        from eq. (3.39)

)
2 a>f + y>f +p
i log(cid:0)p
c)(cid:1)    p

c exp(f i

i log(cid:0)p

eq. (3.39) using eq. (3.45) and (3.47)

c)(cid:1)

2 a>f + y>f +p

objective:     1

c exp(f i
until convergence
16: log q(y|x,   ) :=     1
eq. (3.44)
return:   f := f (post. mode), log q(y|x,   ) (approx. log marg. likelihood)
algorithm 3.3: mode-   nding for multi-class laplace gpc, where d = diag(  ), r
is a matrix of stacked identity matrices and a subscript c on a block diagonal matrix
indicates the n    n submatrix pertaining to class c. the computational complexity
is dominated by the cholesky decomposition in lines 6 and 10 and the forward and
backward substitutions in line 7 with total complexity o((c + 1)n3) (times the num-
ber of newton iterations), all other operations are at most o(cn2) when exploiting
diagonal and block diagonal structures. the memory requirement is o(cn2). for
comments on convergence criteria for line 15 and avoiding divergence, refer to the
caption of algorithm 3.1 on page 46.

c zc

we now need to consider the predictive distribution q(     |y) which is ob-
tained by softmaxing the gaussian q(f   |y). in the binary case we saw that the
predicted classi   cation could be obtained by thresholding the mean value of the
gaussian. in the multi-class case one does need to take the variability around
the mean into account as it can a   ect the overall classi   cation (see exercise
3.10.3). one simple way (which will be used in algorithm 3.4) to estimate
the mean prediction eq[     |y]
is to draw samples from the gaussian q(f   |y),
softmax them and then average.

marginal likelihood

the laplace approximation to the marginal likelihood can be obtained in

the same way as for the binary case, yielding
log p(y|x,   )     log q(y|x,   )

  f>k   1  f + y>  f     nx

=     1

2

(cid:16) cx

log

exp   f c

i

(cid:17)     1

2 log |icn + w

(3.44)
2|.

1

1
2 kw

i=1

c=1

as for the inversion of k   1 + w , the determinant term can be computed e   -
ciently by exploiting the structure of w , see section 3.5.1.

in this section we have described the laplace approximation for multi-class
classi   cation. however, there has also been some work on ep-type methods for
the multi-class case, see seeger and jordan [2004].

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.5 multi-class laplace approximation

51

input: k (covariance matrix),   f (posterior mode), x    (test input)

2: compute    and    from   f using eq. (3.34) and defn. of    under eq. (3.38)

for c := 1 . . . c do

1
2
c kcd

1
2

c )

4:

1
2

l := cholesky(in + d
c l>\(l\d
ec := d
c )

m := cholesky(p

1
2

6: end for

8: for c := 1 . . . c do

c ec)
  c    := (yc       c)>kc   
b := eckc   
c := ec(r(m>\(m\(r>b))))
for c0 := 1 . . . c do

  cc0 := c>kc0
   

end for
  cc :=   cc + kc(x   , x   )     b>kc   

10:

12:

14:

16: end for
      := 0

e is block diag. d

1

2 (icn + d

1
2 kd

1

2 )   1d

1
2

latent test mean from eq. (3.41)

)

latent test covariance from eq. (3.43)

18: for i := 1 : s do
f        n (     ,   )

      :=       + exp(f c   )/p

20:

initialize monte carlo loop to estimate
predictive class probabilities using s samples
sample latent values from joint gaussian posterior
c0 exp(f c0
accumulate id203 eq. (3.34)
    )

22:         :=      /s

end for
normalize mc estimate of prediction vector
return: eq(f )[  (f(x   ))|x   , x, y] :=         (predicted class id203 vector)
algorithm 3.4: predictions for multi-class laplace gpc, where d = diag(  ), r is
a matrix of stacked identity matrices and a subscript c on a block diagonal matrix
indicates the n    n submatrix pertaining to class c. the computational complexity
is dominated by the cholesky decomposition in lines 4 and 7 with a total complexity
o((c + 1)n3), the memory requirement is o(cn2). for multiple test cases repeat
from line 8 for each test case (in practice, for multiple test cases one may reorder the
computations in lines 8-16 to avoid referring to all ec matrices repeatedly).

3.5.1

implementation

the implementation follows closely the implementation for the binary case de-
tailed in section 3.4.3, with the slight complications that k is now a block
diagonal matrix of size cn    cn and the w matrix is no longer diagonal, see
eq. (3.38). care has to be taken to exploit the structure of these matrices to
reduce the computational burden.

the newton iteration from eq. (3.39) requires the inversion of k   1 + w ,

which we    rst re-write as

(k   1 + w )   1 = k     k(k + w    1)   1k,

(3.45)

using the matrix inversion lemma, eq. (a.9). in the following the inversion of
the above matrix k + w    1 is our main concern. first, however, we apply the

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

52

classi   cation

matrix inversion lemma, eq. (a.9) to the w matrix:12

w    1 = (d         >)   1 = d   1     r(i     r>dr)   1r>

(3.46)

= d   1     ro   1r>,

where d = diag(  ), r = d   1   is a cn   n matrix of stacked in unit matrices,
c dc = in and o is
the zero matrix. introducing the above in k + w    1 and applying the matrix
inversion lemma, eq. (a.9) again we have
(k + w    1)   1 = (k + d   1     ro   1r>)   1

we use the fact that    normalizes over classes: r>dr =p
= e     er(o + r>er)   1r>e = e     er(p

(3.47)
c ec)   1r>e.
2 is a block diagonal matrix
c ec. the newton iterations can now be computed by inserting
eq. (3.47) and (3.45) in eq. (3.39), as detailed in algorithm 3.3. the predictions
use an equivalent route to compute the gaussian posterior, and the    nal step
of deriving predictive class probabilities is done by monte carlo, as shown in
algorithm 3.4.

and r>er =p

where e = (k + d   1)   1 = d

2 )   1d

2 (i + d

1
2 kd

1

1

1

3.6 expectation propagation

the expectation propagation (ep) algorithm [minka, 2001] is a general approxi-
mation tool with a wide range of applications. in this section we present only its
application to the speci   c case of a gp model for binary classi   cation. we note
that opper and winther [2000] presented a similar method for binary gpc
based on the    xed-point equations of the thouless-anderson-palmer (tap)
type of mean-   eld approximation from statistical physics. the    xed points for
the two methods are the same, although the precise details of the two algorithms
are di   erent. the ep algorithm naturally lends itself to sparse approximations,
which will not be discussed in detail here, but touched upon in section 8.4.

the object of central importance is the posterior distribution over the latent
variables, p(f|x, y). in the following notation we suppress the explicit depen-
dence on hyperparameters, see section 3.6.2 for their treatment. the posterior
is given by bayes    rule, as the product of a id172 term, the prior and
the likelihood

p(f|x, y) =

p(f|x)

(3.48)
where the prior p(f|x) is gaussian and we have utilized the fact that the likeli-
hood factorizes over the training cases. the id172 term is the marginal
likelihood

i=1

p(yi|fi),

ny

z = p(y|x) =

p(f|x)

p(yi|fi) df .

(3.49)

1
z

z

ny

12readers who are disturbed by our sloppy treatment of the inverse of singular matrices
are invited to insert the matrix (1       )in between    and   > in eq. (3.46) and verify that
eq. (3.47) coincides with the limit        0.

i=1

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.6 expectation propagation

53

so far, everything is exactly as in the regression case discussed in chapter 2.
however, in the case of classi   cation the likelihood p(yi|fi) is not gaussian,
a property that was used heavily in arriving at analytical solutions for the
regression framework. in this section we use the probit likelihood (see page 35)
for binary classi   cation

(3.50)
and this makes the posterior in eq. (3.48) analytically intractable. to overcome
this hurdle in the ep framework we approximate the likelihood by a local like-
lihood approximation 13 in the form of an un-normalized gaussian function in
the latent variable fi

p(yi|fi) =   (fiyi),

p(yi|fi)     ti(fi|   zi,     i,     2

i ) ,   zin (fi|    i,     2
i ),

(3.51)

site parameters

which de   nes the site parameters   zi,     i and     2
i . remember that the notation
n is used for a normalized gaussian distribution. notice that we are approxi-
mating the likelihood, i.e. a id203 distribution which normalizes over the
targets yi, by an un-normalized gaussian distribution over the latent variables
fi. this is reasonable, because we are interested in how the likelihood behaves
as a function of the latent fi. in the regression setting we utilized the gaussian
shape of the likelihood, but more to the point, the gaussian distribution for
the outputs yi also implied a gaussian shape as a function of the latent vari-
able fi. in order to compute the posterior we are of course primarily interested
in how the likelihood behaves as a function of fi.14 the property that the
likelihood should normalize over yi (for any value of fi) is not simultaneously
achievable with the desideratum of gaussian dependence on fi; in the ep ap-
proximation we abandon exact id172 for tractability. the product of
the (independent) local likelihoods ti is

ny

i ) = n (    ,     )y

ti(fi|   zi,     i,     2

i=1

i

  zi,

(3.52)

ny

where      is the vector of     i and      is diagonal with     ii =     2
the posterior p(f|x, y) by q(f|x, y)

i . we approximate

q(f|x, y) , 1
zep
with    =          1     , and    = (k   1 +        1)   1,

ti(fi|   zi,     i,     2

p(f|x)

i ) = n (  ,   ),

i=1

(3.53)

where we have used eq. (a.7) to compute the product (and by de   nition, we
know that the distribution must normalize correctly over f). notice, that we use
the tilde-parameters      and      (and   z) for the local likelihood approximations,

13note, that although each likelihood approximation is local, the posterior approximation
produced by the ep algorithm is global because the latent variables are coupled through the
prior.

14however, for computing the marginal likelihood id172 becomes crucial, see section

3.6.2.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

54

classi   cation

and plain    and    for the parameters of the approximate posterior. the nor-
malizing term of eq. (3.53), zep = q(y|x), is the ep algorithm   s approximation
to the normalizing term z from eq. (3.48) and eq. (3.49).

kl divergence

how do we choose the parameters of the local approximating distributions
ti? one of the most obvious ideas would be to minimize the kullback-leibler
(kl) divergence (see section a.5) between the posterior and its approximation:

kl(cid:0)p(f|x, y)||q(f|x, y)(cid:1). direct minimization of this kl divergence for the
choose to minimize the reversed kl divergence kl(cid:0)q(f|x, y)||p(f|x, y)(cid:1) with

joint distribution on f turns out to be intractable.
(one can alternatively
respect to the distribution q(f|x, y); this has been used to carry out variational
id136 for gpc, see, e.g. seeger [2000].)

instead, the key idea in the ep algorithm is to update the individual ti ap-
proximations sequentially. conceptually this is done by iterating the following
four steps: we start from some current approximate posterior, from which we
leave out the current ti, giving rise to a marginal cavity distribution. secondly,
we combine the cavity distribution with the exact likelihood p(yi|fi) to get the
desired (non-gaussian) marginal. thirdly, we choose a gaussian approximation
to the non-gaussian marginal, and in the    nal step we compute the ti which
makes the posterior have the desired marginal from step three. these four steps
are iterated until convergence.

in more detail, we optimize the ti approximations sequentially, using the
approximation so far for all the other variables. in particular the approximate
posterior for fi contains three kinds of terms:

1. the prior p(f|x)
2. the local approximate likelihoods tj for all cases j 6= i
3. the exact likelihood for case i, p(yi|fi) =   (yifi)

our goal is to combine these sources of information and choose parameters of ti
such that the marginal posterior is as accurate as possible. we will    rst combine
the prior and the local likelihood approximations into the cavity distribution

q   i(fi)    

tj(fj|   zj,     j,     2

j )dfj,

(3.54)

z

p(f|x)y

j6=i

and subsequently combine this with the exact likelihood for case i. concep-
tually, one can think of the combination of prior and the n     1 approximate
likelihoods in eq. (3.54) in two ways, either by explicitly multiplying out the
terms, or (equivalently) by removing approximate likelihood i from the approx-
imate posterior in eq. (3.53). here we will follow the latter approach. the
marginal for fi from q(f|x, y) is obtained by using eq. (a.6) in eq. (3.53) to
give

(3.55)
i =   ii. this marginal eq. (3.55) contains one approximate term

q(fi|x, y) = n (fi|  i,   2
i ),

where   2

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.6 expectation propagation

55

(namely ti)    too many   , so we need to divide it by ti to get the cavity dis-
tribution

cavity distribution

q   i(fi) , n (fi|     i,   2   i),
i   i            2

where      i =   2   i(     2

i     i), and   2   i = (     2

i            2

i

(3.56)

)   1.

note that the cavity distribution and its parameters carry the subscript    i,
indicating that they include all cases except number i. the easiest way to
verify eq. (3.56) is to multiply the cavity distribution by the local likelihood
approximation ti from eq. (3.51) using eq. (a.7) to recover the marginal in
eq. (3.55). notice that despite the appearance of eq. (3.56), the cavity mean
and variance are (of course) not dependent on     i and     2

i , see exercise 3.10.5.

to proceed, we need to    nd the new (un-normalized) gaussian marginal
which best approximates the product of the cavity distribution and the exact
likelihood

(3.57)
it is well known that when q(x) is gaussian, the distribution q(x) which min-

imizes kl(cid:0)p(x)||q(x)(cid:1) is the one whose    rst and second moments match that

i )     q   i(fi)p(yi|fi).

  q(fi) ,   zin (    i,     2

of p(x), see eq. (a.24). as   q(fi) is un-normalized we choose additionally to
impose the condition that the zero-th moments (normalizing constants) should
match when choosing the parameters of   q(fi) to match the right hand side of
eq. (3.57). this process is illustrated in figure 3.4.

the derivation of the moments is somewhat lengthy, so we have moved the

details to section 3.9. the desired posterior marginal moments are

  zi =   (zi),
i =   2   i       4   in (zi)
(1 +   2   i)  (zi)

    i =      i +
n (zi)
  (zi)

zi +

(cid:16)

    2

yi  2   in (zi)
   
(cid:17)
  (zi)
1 +   2   i

where

,

,

(3.58)

   

zi = yi     i
1 +   2   i

.

the    nal step is to compute the parameters of the approximation ti which
achieves a match with the desired moments. in particular, the product of the
cavity distribution and the local approximation must have the desired moments,
leading to

    i =     2
  zi =   zi

p
i     i          2   i      i),
i (       2
   
2  

i exp(cid:0) 1

  2   i +     2

i = (       2
    2

i          2   i )   1,
2(     i         i)2/(  2   i +     2

i )(cid:1),

(3.59)

which is easily veri   ed by multiplying the cavity distribution by the local ap-
proximation using eq. (a.7) to obtain eq. (3.58). note that the desired marginal
posterior variance     2
i given by eq. (3.58) is guaranteed to be smaller than the
cavity variance, such that     2

i > 0 is always satis   ed.15

this completes the update for a local likelihood approximation ti. we then
have to update the approximate posterior using eq. (3.53), but since only a

15in cases where the likelihood is log concave, one can show that     2

i > 0, but for a general

likelihood there may be no such guarantee.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

56

classi   cation

(a)

(b)

figure 3.4: approximating a single likelihood term by a gaussian. panel (a) dash-
dotted: the exact likelihood,   (fi) (the corresponding target being yi = 1) as a
function of the latent fi, dotted: gaussian cavity distribution n (fi|     i = 1,   2   i = 9),
solid: posterior, dashed: posterior approximation. panel (b) shows an enlargement of
panel (a).

single site has changed one can do this with a computationally e   cient rank-
one update, see section 3.6.3. the ep algorithm is used iteratively, updating
each local approximation in turn. it is clear that several passes over the data
are required, since an update of one local approximation potentially in   uences
all of the approximate marginal posteriors.

3.6.1 predictions

the procedure for making predictions in the ep framework closely resembles
the algorithm for the laplace approximation in section 3.4.2. ep gives a gaus-
sian approximation to the posterior distribution, eq. (3.53). the approximate
predictive mean for the latent variable f    becomes

eq[f   |x, y, x   ] = k>

    k   1   = k>
= k>

    k   1(k   1 +        1)   1        1     
    (k +     )   1     .

(3.60)

the approximate latent predictive variance is analogous to the derivation from
eq. (3.23) and eq. (3.24), with      playing the r  ole of w

vq[f   |x, y, x   ] = k(x   , x   )     k>

    (k +     )   1k   .

(3.61)

q(y    = 1|x, y, x   ) = eq[     |x, y, x   ] =

the approximate predictive distribution for the binary target becomes
  (f   )q(f   |x, y, x   ) df   ,

(3.62)
where q(f   |x, y, x   ) is the approximate latent predictive gaussian with mean
and variance given by eq. (3.60) and eq. (3.61). this integral is readily evaluated
using eq. (3.80), giving the predictive id203

z

q(y    = 1|x, y, x   ) =   

k>
    (k +     )   1     

1 + k(x   , x   )     k>

    (k +     )   1k   

.

(3.63)

(cid:16)

p

(cid:17)

   5051000.20.40.60.81likelihoodcavityposteriorapproximation   5051000.020.040.060.080.10.120.14c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.6 expectation propagation

3.6.2 marginal likelihood

57

the ep approximation to the marginal likelihood can be found from the nor-
malization of eq. (3.53)

z

ny

zep = q(y|x) =

p(f|x)

ti(fi|   zi,     i,     2

i ) df .

(3.64)

using eq. (a.7) and eq. (a.8) in an analogous way to the treatment of the
regression setting in equations (2.28) and (2.30) we arrive at

i=1

log(zep|  ) =    1
2
   

log   (cid:0) yi     i

log |k +     |     1
nx
2

(cid:1) +

nx

+

i=1

1 +   2   i

1
2

i=1

    >(k +     )   1     

log(  2   i +     2

i ) +

nx

i=1

(3.65)

(     i         i)2
i ) ,
2(  2   i +     2

where    denotes the hyperparameters of the covariance function. this expres-
sion has a nice intuitive interpretation: the    rst two terms are the marginal
likelihood for a regression model for     , each component of which has inde-
pendent gaussian noise of variance     ii (as      is diagonal), cf. eq. (2.30). the
remaining three terms come from the id172 constants   zi. the    rst
of these penalizes the cavity (or leave-one-out) distributions for not agreeing
with the classi   cation labels, see eq. (3.82). in other words, we can see that
the marginal likelihood combines two desiderata, (1) the means of the local
likelihood approximations should be well predicted by a gp, and (2) the corre-
sponding latent function, when ignoring a particular training example, should
be able to predict the corresponding classi   cation label well.

3.6.3

implementation

the implementation for the ep algorithm follows the derivation in the previous
section closely, except that care has to be taken to achieve numerical stability,
in similar ways to the considerations for laplace   s method in section 3.4.3.
in addition, we wish to be able to speci   cally handle the case were some site
variances     2
i may tend to in   nity; this corresponds to ignoring the corresponding
likelihood terms, and can form the basis of sparse approximations, touched upon
in section 8.4. in this limit, everything remains well-de   ned, although this is
not obvious e.g. from looking at eq. (3.65). it turns out to be slightly more
convenient to use natural parameters     i,     i and      i,      i for the site and cavity
parameters
    i =        2
i ,
rather than     2
importance is

     i =      i     i (3.66)
i ,     i and   2   i,      i themselves. the symmetric matrix of central

     i =      2   i ,

  s = diag(     ),

     =   s     ,

(3.67)
which plays a r  ole equivalent to eq. (3.26). expressions involving the inverse of
b are computed via cholesky factorization, which is numerically stable since

b = i +   s

2 k   s

1
2 ,

1

natural parameters

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

58

classi   cation

input: k (covariance matrix), y (  1 targets)

2:      := 0,      := 0,    := k,    := 0

initialization and eq. (3.53)

repeat

for i := 1 to n do
i         i
i   i         i
i          i         i and     i :=     i +        
o
i     i          i

     i :=      2
     i :=      2
compute the marginal moments     i and     2
        :=        2
i
    i :=        2

   :=       (cid:0)(       )   1 +   ii

(cid:1)   1si s>

i

o

compute approximate cavity para-
meters      i and      i using eq. (3.56)
using eq. (3.58)
update site parameters
    i and     i using eq. (3.59)
update    and    by eq. (3.70) and
eq. (3.53). si is column i of   

   :=       

o

)

end for
l := cholesky(in +   s
2 )
v := l>\   s
   := k     v >v and    :=       

2 k   s

1
2 k

1

1

re-compute the approximate
posterior parameters    and   
using eq. (3.53) and eq. (3.68)

4:

6:

8:

10:

12:

14:

16: until convergence
compute log zep

using eq. (3.65), (3.73) and (3.74) and the existing l
18: return:     ,      (natural site param.), log zep (approx. log marg. likelihood)
algorithm 3.5: expectation propagation for binary classi   cation. the targets y are
used only in line 7. in lines 13-15 the parameters of the approximate posterior are
re-computed (although they already exist); this is done because of the large number of
rank-one updates in line 10 which would eventually cause loss of numerical precision
in   . the computational complexity is dominated by the rank-one updates in line
10, which takes o(n2) per variable, i.e. o(n3) for an entire sweep over all variables.
similarly re-computing    in lines 13-15 is o(n3).

the eigenvalues of b are bounded below by one. the parameters of the gaussian
approximate posterior from eq. (3.53) are computed as
   = (k   1 +   s)   1 = k     k(k +   s   1)   1k = k     k   s

2 k. (3.68)

2 b   1   s

1

1

after updating the parameters of a site, we need to update the approximate
posterior eq. (3.53) taking the new site parameters into account. for the inverse
covariance matrix of the approximate posterior we have from eq. (3.53)
     1 = k   1 +   s, and thus      1
)eie>
i , (3.69)
where ei is a unit vector in direction i, and we have used that   s = diag(     ).
using the matrix inversion lemma eq. (a.9), on eq. (3.69) we obtain the new   

new = k   1 +   sold + (     new

i          old

i

  new =   old    

i          old
     new
i          old
1 + (     new

i

i

)  old
ii

sis>
i ,

(3.70)

in time o(n2), where si is the i   th column of   old. the posterior mean is then
calculated from eq. (3.53).

in the ep algorithm each site is updated in turn, and several passes over all
sites are required. pseudocode for the ep-gpc algorithm is given in algorithm

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.6 expectation propagation

59

input:     ,      (natural site param.), x (inputs), y (  1 targets),

1

2 )

1

1

1

z :=   s

2 k   s
2 k     ))

2 l>\(l\(   s

v := l\(cid:0)   s

2: l := cholesky(in +   s
4:   f    := k(x   )>(         z)
6: v[f   ] := k(x   , x   )     v>v
1 + v[f   ])

2 k(x   )(cid:1)

        :=   (   f   /

   

1

k (covariance function), x    test input
2 k   s
eq. (3.60) using eq. (3.71)

b = in +   s

1
2

1

o
o

eq. (3.61) using eq. (3.72)
eq. (3.63)

8: return:         (predictive class id203 (for class 1))
algorithm 3.6: predictions for expectation propagation. the natural site parameters
     and      of the posterior (which can be computed using algorithm 3.5) are input. for
multiple test inputs lines 4-7 are applied to each test input. computational complexity
is n3/6 + n2 operations once (line 2 and 3) plus n2 operations per test case (line
5), although the cholesky decomposition in line 2 could be avoided by storing it in
algorithm 3.5. note the close similarity to algorithm 3.2 on page 47.

3.5. there is no formal guarantee of convergence, but several authors have
reported that ep for gaussian process models works relatively well.16

for the predictive distribution, we get the mean from eq. (3.60) which is

evaluated using
eq[f   |x, y, x   ] = k>

    (k +   s   1)   1   s   1      = k>
= k>

    (i       s
and the predictive variance from eq. (3.61) similarly by

   (cid:0)i     (k +   s   1)   1k(cid:1)    

1

2 b   1   s

1

2 k)    ,

vq[f   |x, y, x   ] = k(x   , x   )     k>
    (k +   s   1)   1k   
= k(x   , x   )     k>
      s
2 k   .

2 b   1   s

1

1

(3.71)

(3.72)

pseudocode for making predictions using ep is given in algorithm 3.6.

finally, we need to evaluate the approximate log marginal likelihood from
eq. (3.65). there are several terms which need careful consideration, principally
due to the fact the     i values may be arbitrarily small (and cannot safely be
inverted). we start with the fourth and    rst terms of eq. (3.65)
x
2 log |   s   1(i +   st    1)|     1

2 log |t    1+   s   1|     1

log(1+    i     1   i )    x

2 log |k +     | = 1
= 1
2

2 log |   s   1b|
log lii,

(3.73)

1

i

i

where t is a diagonal matrix of cavity precisions tii =      i =      2   i and l is the
cholesky factorization of b. in eq. (3.73) we have factored out the matrix   s   1
from both determinants, and the terms cancel. continuing with the part of the

16it has been conjectured (but not proven) by l. csat  o (personal communication) that ep

is guaranteed to converge if the likelihood is log concave.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

60

classi   cation

   fth term from eq. (3.65) which is quadratic in      together with the second term

1

2     >(t    1 +   s   1)   1          1

2     >(k +     )   1     

2     >   s   1(cid:0)(t    1 +   s   1)   1     (k +   s   1)   1(cid:1)   s   1     
2     >(cid:0)(k   1 +   s)   1     (t +   s)   1(cid:1)    
2     >(cid:0)k     k   s

2 k     (t +   s)   1(cid:1)    ,

2 b   1   s

1

1

= 1
= 1
= 1

(3.74)

where in eq. (3.74) we apply the matrix inversion lemma eq. (a.9) to both
parenthesis to be inverted. the remainder of the    fth term in eq. (3.65) is
evaluated using the identity

1

2   >

   i(t    1 +   s   1)   1(     i     2    ) = 1

2   >

   it (   s + t )   1(   s     i     2    ),

(3.75)

where      i is the vector of cavity means      i. the third term in eq. (3.65)
requires in no special treatment and can be evaluated as written.

3.7 experiments

in this section we present the results of applying the algorithms for gp clas-
si   cation discussed in the previous sections to several data sets. the purpose
is    rstly to illustrate the behaviour of the methods and secondly to gain some
insights into how good the performance is compared to some other commonly-
used machine learning methods for classi   cation.

section 3.7.1 illustrates the action of a gp classi   er on a toy binary pre-
diction problem with a 2-d input space, and shows the e   ect of varying the
length-scale     in the se covariance function. in section 3.7.2 we illustrate and
compare the behaviour of the two approximate gp methods on a simple one-
dimensional binary task. in section 3.7.3 we present results for a binary gp
classi   er on a handwritten digit classi   cation task, and study the e   ect of vary-
ing the kernel parameters. in section 3.7.4 we carry out a similar study using
a multi-class gp classi   er to classify digits from all ten classes 0-9. in section
3.8 we discuss the methods from both experimental and theoretical viewpoints.

3.7.1 a toy problem

figure 3.5 illustrates the operation of a gaussian process classi   er on a binary
problem using the squared exponential kernel with a variable length-scale and
the logistic response function. the laplace approximation was used to make
the plots. the data points lie within the square [0, 1]2, as shown in panel (a).
notice in particular the lone white point amongst the black points in the ne
corner, and the lone black point amongst the white points in the sw corner.

in panel (b) the length-scale is     = 0.1, a relatively short value. in this case
the latent function is free to vary relatively quickly and so the classi   cations

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.7 experiments

61

(a)

(b)

(c)

(d)

figure 3.5: panel (a) shows the location of the data points in the two-dimensional
space [0, 1]2. the two classes are labelled as open circles (+1) and closed circles (-
1). panels (b)-(d) show contour plots of the predictive id203 eq[  (x   )|y] for
signal variance   2
f = 9 and length-scales     of 0.1, 0.2 and 0.3 respectively. the de-
cision boundaries between the two classes are shown by the thicker black lines. the
maximum value attained is 0.84, and the minimum is 0.19.

provided by thresholding the predictive id203 eq[  (x   )|y] at 0.5 agrees
with the training labels at all data points. in contrast, in panel (d) the length-
scale is set to     = 0.3. now the latent function must vary more smoothly, and
so the two lone points are misclassi   ed. panel (c) was obtained with     = 0.2.
as would be expected, the decision boundaries are more complex for shorter
length-scales. methods for setting the hyperparameters based on the data are
discussed in chapter 5.

(cid:176)(cid:176)(cid:176)   (cid:176)(cid:176)(cid:176)(cid:176)(cid:176)   (cid:176)                  (cid:176)      0.250.50.50.50.750.250.30.50.70.30.50.7c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

62

classi   cation

(a)

(b)

figure 3.6: one-dimensional toy classi   cation dataset: panel (a) shows the dataset,
where points from class +1 have been plotted at    = 1 and class    1 at    = 0, together
with the predictive id203 for laplace   s method and the ep approximation. also
shown is the id203 p(y = +1|x) of the data generating process. panel (b) shows
the corresponding distribution of the latent function f (x), showing curves for the
mean, and   2 standard deviations, corresponding to 95% con   dence regions.

3.7.2 one-dimensional example

although laplace   s method and the ep approximation often give similar re-
sults, we here present a simple one-dimensional problem which highlights some
of the di   erences between the methods. the data, shown in figure 3.6(a),
consists of 60 data points in three groups, generated from a mixture of three
gaussians, centered on    6 (20 points), 0 (30 points) and 2 (10 points), where
the middle component has label    1 and the two other components label +1; all
components have standard deviation 0.8; thus the two left-most components
are well separated, whereas the two right-most components overlap.

both approximation methods are shown with the same value of the hyperpa-
rameters,     = 2.6 and   f = 7.0, chosen to maximize the approximate marginal
likelihood for laplace   s method. notice in figure 3.6 that there is a consid-
erable di   erence in the value of the predictive id203 for negative inputs.
the laplace approximation seems overly cautious, given the very clear separa-
tion of the data. this e   ect can be explained as a consequence of the intuition
that the in   uence of    well-explained data points    is e   ectively reduced, see the
discussion around eq. (3.19). because the points in the left hand cluster are
relatively well-explained by the model, they don   t contribute as strongly to the
posterior, and thus the predictive id203 never gets very close to 1. notice
in figure 3.6(b) the 95% con   dence region for the latent function for laplace   s
method actually includes functions that are negative at x =    6, which does
not seem appropriate. for the positive examples centered around x = 2 on the
right-hand side of figure 3.6(b), this e   ect is not visible, because the points
around the transition between the classes at x = 1 are not so    well-explained   ;
this is because the points near the boundary are competing against the points
from the other class, attempting to pull the latent function in opposite di-
rections. consequently, the datapoints in this region all contribute strongly.

   8   6   4   202400.20.40.60.81input, xpredictive id203,   *class +1class    1laplaceepp(y|x)   8   6   4   2024   10   5051015input, xlatent function, f(x)laplaceepc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.7 experiments

63

another sign of this e   ect is that the uncertainty in the latent function, which
is closely related to the    e   ective    local density of the data, is very small in
the region around x = 1; the small uncertainty reveals a high e   ective density,
which is caused by all data points in the region contributing with full weight. it
should be emphasized that the example was arti   cially constructed speci   cally
to highlight this e   ect.

finally, figure 3.6 also shows clearly the e   ects of uncertainty in the latent
function on eq[     |y]. in the region between x = 2 to x = 4, the latent mean
in panel (b) increases slightly, but the predictive id203 decreases in this
region in panel (a). this is caused by the increase in uncertainty for the latent
function; when the widely varying functions are squashed through the non-
linearity it is possible for both classes to get high id203, and the average
prediction becomes less extreme.

3.7.3 binary handwritten digit classi   cation example

handwritten digit and character recognition are popular real-world tasks for
testing and benchmarking classi   ers, with obvious application e.g. in postal
services. in this section we consider the discrimination of images of the digit
3 from images of the digit 5 as an example of binary classi   cation; the speci   c
choice was guided by the experience that this is probably one of the most
di   cult binary subtasks. 10-class classi   cation of the digits 0-9 is described in
the following section.

we use the us postal service (usps) database of handwritten digits which
consists of 9298 segmented 16    16 greyscale images normalized so that the
intensity of the pixels lies in [   1, 1]. the data was originally split into a training
set of 7291 cases and a testset of the remaining 2007 cases, and has often been
used in this con   guration. unfortunately, the data in the two partitions was
collected in slightly di   erent ways, such that the data in the two sets did not
stem from the same distribution.17 since the basic underlying assumption for
most machine learning algorithms is that the distribution of the training and
test data should be identical, the original data partitions are not really suitable
as a test bed for learning algorithms, the interpretation of the results being
hampered by the change in distribution. secondly, the original test set was
rather small, sometimes making it di   cult to di   erentiate the performance of
di   erent algorithms. to overcome these two problems, we decided to pool the
two partitions and randomly split the data into two identically sized partitions
of 4649 cases each. a side-e   ect is that it is not trivial to compare to results
obtained using the original partitions. all experiments reported here use the
repartitioned data. the binary 3s vs. 5s data has 767 training cases, divided
406/361 on 3s vs. 5s, while the test set has 773 cases split 418/355.

usps dataset

usps repartitioned

we present results of both laplace   s method and ep using identical ex-
perimental setups. the squared exponential covariance function k(x, x0) =

squared exponential
covariance function

17it is well known e.g. that the original test partition had more di   cult cases than the

training set.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

64

classi   cation

(a)

(b)

(c)

(d)

figure 3.7: binary laplace approximation: 3s vs. 5s discrimination using the usps
data. panel (a) shows a contour plot of the log marginal likelihood as a function of
log(   ) and log(  f ). the marginal likelihood has an optimum at log(   ) = 2.85 and
log(  f ) = 2.35, with an optimum value of log p(y|x,   ) =    99. panel (b) shows a
contour plot of the amount of information (in excess of a simple base-line model, see
text) about the test cases in bits as a function of the same variables. the statistical
uncertainty (because of the    nite number of test cases) is about   0.03 bits (95%
con   dence interval). panel (c) shows a histogram of the latent means for the training
and test sets respectively at the values of the hyperparameters with optimal marginal
likelihood (from panel (a)). panel (d) shows the number of test errors (out of 773)
when predicting using the sign of the latent mean.

f exp(   |x     x0|2/2   2) was used, so there are two free parameters, namely   f
  2
(the process standard deviation, which controls its vertical scaling), and the
length-scale     (which controls the input length-scale). let    = (log(   ), log(  f ))
denote the vector of hyperparameters. we    rst present the results of laplace   s
method in figure 3.7 and discuss these at some length. we then brie   y compare
these with the results of the ep method in figure 3.8.

hyperparameters

2345012345log lengthscale, log(l)log magnitude, log(  f)log marginal likelihood   100   105   115   115   130   150   200   2002345012345log lengthscale, log(l)log magnitude, log(  f)information about test targets in bits0.250.250.50.50.70.70.80.80.84   50501020latent means, ffrequencytraining set latent means   50501020latent means, ffrequencytest set latent means2345012345log lengthscale, log(l)log magnitude, log(  f)21191919191919191918181819191919191919191919192220181819191817171717171818181818181818181818232321181818171616161515161616161616161616161529262220181717161616161717171717171717171717173329262423191817151616161616161615151515151515343430282625232320191817171717171717171818181835343430302927242322212019181818181818181919193635343432303027262322222221212021202020202020393636343532323130292725242423242322222222222341393736353632323130302625252526262626252424244240393836353632323131292627252526272828282828454240393836363632323131292726252528282829292951454240393836363632323131292726262830293030306051454240393836363632333131292728272830293030896051454240393836373632333131282828293030293088605145424039383637363233313128292826303029886051454240393836373632333131282928293030number of test misclassificationsc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.7 experiments

65

(a)

(b)

(c)

(d)

figure 3.8: the ep algorithm on 3s vs. 5s digit discrimination task from the usps
data. panel (a) shows a contour plot of the log marginal likelihood as a function of
the hyperparameters log(   ) and log(  f ). the marginal likelihood has an optimum
at log(   ) = 2.6 at the maximum value of log(  f ), but the log marginal likelihood is
essentially    at as a function of log(  f ) in this region, so a good point is at log(  f ) =
4.1, where the log marginal likelihood has a value of    90. panel (b) shows a contour
plot of the amount of information (in excess of the baseline model) about the test cases
in bits as a function of the same variables. zero bits corresponds to no information
and one bit to perfect binary generalization. the 773 test cases allows the information
to be determined within   0.035 bits. panel (c) shows a histogram of the latent means
for the training and test sets respectively at the values of the hyperparameters with
optimal marginal likelihood (from panel a). panel (d) shows the number of test errors
(out of 773) when predicting using the sign of the latent mean.

in figure 3.7(a) we show a contour plot of the approximate log marginal
likelihood (lml) log q(y|x,   ) as a function of log(   ) and log(  f ), obtained
from runs on a grid of 17 evenly-spaced values of log(   ) and 23 evenly-spaced
values of log(  f ). notice that there is a maximum of the marginal likelihood

laplace results

2345012345log lengthscale, log(l)log magnitude, log(  f)log marginal likelihood   92   95   100   105   105   115   130   160   160   200   2002345012345log lengthscale, log(l)log magnitude, log(  f)information about test targets in bits0.250.50.70.70.80.80.840.840.860.880.89   100   5005001020latent means, ffrequencytraining set latent means   100   5005001020latent means, ffrequencytest set latent means2345012345log lengthscale, log(l)log magnitude, log(  f)21201919191919191919191919191919191919191919192220191819191919191919191919191919191919191919242321191817171717171717171717171717171717171729262322201818181818181818181818181818181818183330252523211917171617181818181818181818181818343530292625242120191919181818181818181818181835343431292927262423232120202020212121212121213635343432313131302523232324242424242525252525393636343432323230302724232625252626262626262641393736353632323230292924242626272727272727274240393736353632323131292926232627272727272727454240393836363632333131292826242526282828282751454240393836363632333131292826252828282827286051454240393836363632333131292827272829282727896051454240393836363632333131292827272929282788605145424039383636363233313129282727292828876051454240393836363632333131292828272928number of test misclassificationsc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

66

classi   cation

test log predictive
id203

base-line method

interpretation of
information score

error rate

near log(   ) = 2.85 and log(  f ) = 2.35. as will be explained in chapter 5, we
would expect that hyperparameters that yield a high marginal likelihood would
give rise to good predictions. notice that an increase of 1 unit on the log scale
means that the id203 is 2.7 times larger, so the marginal likelihood in
figure 3.7(a) is fairly well peaked.

there are at least two ways we can measure the quality of predictions at the
test points. the    rst is the test log predictive id203 log2 p(y   |x   ,d,   ).
in figure 3.7(b) we plot the average over the test set of the test log predictive
id203 for the same range of hyperparameters. we express this as the
amount of information in bits about the targets, by using log to the base 2.
further, we o   -set the value by subtracting the amount of information that a
simple base-line method would achieve. as a base-line model we use the best
possible model which does not use the inputs; in this case, this model would
just produce a predictive distribution re   ecting the frequency of the two classes
in the training set, i.e.

    418/773 log2(406/767)     355/773 log2(361/767) = 0.9956 bits,

(3.76)

essentially 1 bit. (if the classes had been perfectly balanced, and the training
and test partitions also exactly balanced, we would arrive at exactly 1 bit.)
thus, our scaled information score used in figure 3.7(b) would be zero for a
method that did random guessing and 1 bit for a method which did perfect
classi   cation (with complete con   dence). the information score measures how
much information the model was able to extract from the inputs about the
identity of the output. note that this is not the mutual information between
the model output and the test targets, but rather the kullback-leibler (kl)
divergence between them. figure 3.7 shows that there is a good qualitative
agreement between the marginal likelihood and the test information, compare
panels (a) and (b).

the second (and perhaps most commonly used) method for measuring the
quality of the predictions is to compute the number of test errors made when
using the predictions. this is done by computing eq[     |y] (see eq. (3.25)) for
each test point, thresholding at 1/2 to get    hard    predictions and counting the
number of errors. figure 3.7(d) shows the number of errors produced for each
entry in the 17    23 grid of values for the hyperparameters. the general trend
in this table is that the number of errors is lowest in the top left-hand corner
and increases as one moves right and downwards. the number of errors rises
dramatically in the far bottom righthand corner. however, note in general that
the number of errors is quite small (there are 773 cases in the test set).

the qualitative di   erences between the two evaluation criteria depicted in
figure 3.7 panels (b) and (d) may at    rst sight seem alarming. and although
panels (a) and (b) show similar trends, one may worry about using (a) to select
the hyperparameters, if one is interested in minimizing the test misclassi   cation
rate. indeed a full understanding of all aspects of these plots is quite involved,
but as the following discussion suggests, we can explain the major trends.

first, bear in mind that the e   ect of increasing     is to make the kernel
function broader, so we might expect to observe e   ects like those in figure 3.5

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.7 experiments

67

where large widths give rise to a lack of    exibility. keeping     constant, the
e   ect of increasing   f is to increase the magnitude of the values obtained for
  f. by itself this would lead to    harder    predictions (i.e. predictive probabilities
closer to 0 or 1), but we have to bear in mind that the variances associated
will also increase and this increased uncertainty for the latent variables tends
to    soften    the predictive probabilities, i.e. move them closer to 1/2.

the most marked di   erence between figure 3.7(b) and (d) is the behaviour
in the the top left corner, where classi   cation error rate remains small, but
the test information and marginal likelihood are both poor. in the left hand
side of the plots, the length scale     is very short. this causes most points to
be deemed    far away    from most other points. in this regime the prediction
is dominated by the class-label of the nearest neighbours, and for the task at
hand, this happens to give a low misclassi   cation rate. in this parameter region
the test latent variables f    are very close to zero, corresponding to probabilities
very close to 1/2. consequently, the predictive probabilities carry almost no
information about the targets. in the top left corner, the predictive probabilities
for all 773 test cases lie in the interval [0.48, 0.53]. notice that a large amount
of information implies a high degree of correct classi   cation, but not vice versa.
at the optimal marginal likelihood values of the hyperparameters, there are 21
misclassi   cations, which is slightly higher that the minimum number attained
which is 15 errors.

in exercise 3.10.6 readers are encouraged to investigate further the behaviour
of   f and the predictive probabilities etc. as functions of log(   ) and log(  f ) for
themselves.

in figure 3.8 we show the results on the same experiment, using the ep
method. the    ndings are qualitatively similar, but there are signi   cant dif-
ferences. in panel (a) the approximate log marginal likelihood has a di   erent
shape than for laplace   s method, and the maximum of the log marginal likeli-
hood is about 9 units on a natural log scale larger (i.e. the marginal id203
is exp(9)     8000 times higher). also note that the marginal likelihood has a
ridge (for log     = 2.6) that extends into large values of log   f . for these very
large latent amplitudes (see also panel (c)) the probit likelihood function is well
approximated by a step function (since it transitions from low to high values
in the domain [   3, 3]). once we are in this regime, it is of course irrelevant
exactly how large the magnitude is, thus the ridge. notice, however, that this
does not imply that the prediction will always be    hard   , since the variance of
the latent function also grows.

figure 3.8 shows a good qualitative agreement between the approximate
log marginal likelihood and the test information, compare panels (a) and (b).
the best value of the test information is signi   cantly higher for ep than for
laplace   s method. the classi   cation error rates in panel (d) show a fairly
similar behaviour to that of laplace   s method. in figure 3.8(c) we show the
latent means for training and test cases. these show a clear separation on
the training set, and much larger magnitudes than for laplace   s method. the
absolute values of the entries in f    are quite large, often well in excess of 50,
which may suggest very    hard    predictions (probabilities close to zero or one),

ep results

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

68

classi   cation

(a)

(b)

figure 3.9: map vs. averaged predictions for the ep algorithm for the 3   s vs. 5   s
digit discrimination using the usps data. the optimal values of the hyperparameters
from figure 3.8(a) log(   ) = 2.6 and log(  f ) = 4.1 are used. the map predictions
  (eq[f   |y]) are    hard   , mostly being very close to zero or one. on the other hand,
the averaged predictions eq[     |y] from eq. (3.25) are a lot less extreme. in panel (a)
the 21 cases that were misclassi   ed are indicated by crosses (correctly classi   ed cases
are shown by points). note that only 4 of the 21 misclassi   ed points have con   dent
predictions (i.e. outside [0.1, 0.9]). notice that all points fall in the triangles below
and above the horizontal line, con   rming that averaging does not change the    most
probable    class, and that it always makes the probabilities less extreme (i.e. closer to
1/2). panel (b) shows histograms of averaged and map predictions, where we have
truncated values over 30.

since the sigmoid saturates for smaller arguments. however, when taking the
uncertainties in the latent variables into account, and computing the predictions
using averaging as in eq. (3.25) the predictive probabilities are    softened   . in
figure 3.9 we can verify that the averaged predictive probabilities are much less
extreme than the map predictions.

in order to evaluate the performance of the two approximate methods for
gp classi   cation, we compared to a linear probit model, a support vector ma-
chine, a least-squares classi   er and a nearest neighbour approach, all of which
are commonly used in the machine learning community. in figure 3.10 we show
error-reject curves for both misclassi   cation rate and the test information mea-
sure. the error-reject curve shows how the performance develops as a function
of the fraction of test cases that is being rejected. to compute these, we    rst
modify the methods that do not naturally produce probabilistic predictions to
do so, as described below. based on the predictive probabilities, we reject test
cases for which the maximum predictive id203 is smaller than a threshold.
varying the threshold produces the error-reject curve.

the gp classi   ers applied in figure 3.10 used the hyperparameters which
optimized the approximate marginal likelihood for each of the two methods.
for the gp classi   ers there were two free parameters   f and    . the linear pro-
bit model (linear logistic models are probably more common, but we chose the
probit here, since the other likelihood based methods all used probit) can be

error-reject curve

linear probit model

00.20.40.60.8100.20.40.60.81  * map  * averaged00.20.40.60.810510152025  * mapfrequency00.20.40.60.810510152025  * averagedfrequencyc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.7 experiments

69

(a)

(b)

figure 3.10: panel (a) shows the error-reject curve and panel (b) the amount of
information about the test cases as a function of the rejection rate. the probabilistic
one nearest neighbour (p1nn) method has much worse performance than the other
methods. gaussian processes with ep behaves similarly to id166   s although the clas-
si   cation rate for id166 for low rejection rates seems to be a little better. laplace   s
method is worse than ep and id166. the gp least squares classi   er (lsc) described
in section 6.5 performs the best.

implemented as gp model using laplace   s method, which is equivalent to (al-
though not computationally as e   cient as) iteratively reweighted least squares
(irls). the covariance function k(x, x0) =   2x>x0 has a single hyperparam-
eter,   , which was set by maximizing the log marginal likelihood. this gives
log p(y|x,   ) =    105, at    = 2.0, thus the marginal likelihood for the linear
covariance function is about 6 units on a natural log scale lower than the max-
imum log marginal likelihood for the laplace approximation using the squared
exponential covariance function.

the support vector machine (id166) classi   er (see section 6.4 for further de-
tails on the id166) used the same se kernel as the gp classi   ers. for the id166
the r  ole of     is identical, and the trade-o    parameter c in the id166 formulation
f . we carried out 5-fold cross validation
(see eq. (6.37)) plays a similar r  ole to   2
on a grid in parameter space to identify the best combination of parameters
w.r.t. the error rate; this turned out to be at c = 1,     = 10. our experiments
were conducted using the id166torch software [collobert and bengio, 2001].
in order to compute probabilistic predictions, we squashed the test-activities
through a cumulative gaussian, using the methods proposed by platt [2000]:
we made a parameterized linear transformation of the test-activities and fed
this through the cumulative gaussian.18 the parameters of the linear trans-
formation were chosen to maximize the log predictive id203, evaluated on
the hold-out sets of the 5-fold cross validation.

the probabilistic one nearest neighbour (p1nn) method is a simple nat-
ural extension to the classical one nearest neighbour method which provides
probabilistic predictions.
it computes the leave-one-out (loo) one nearest
neighbour prediction on the training set, and records the fraction of cases   
where the loo predictions were correct. on test cases, the method then pre-

18platt [2000] used a logistic whereas we use a cumulative gaussian.

support vector machine

probabilistic
one nearest neighbour

00.10.20.300.010.020.03rejection ratemisclassification rateeplaplaceid166p1nnlsclin probit00.20.40.60.810.850.90.951rejection ratetest information, bitseplaplaceid166p1nnlsclin probitc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

70

classi   cation

dicts the one nearest neighbour class with id203   , and the other class
with id203 1      . rejections are based on thresholding on the distance to
the nearest neighbour.

the least-squares classi   er (lsc) is described in section 6.5. in order to
produce probabilistic predictions, the method of platt [2000] was used (as de-
scribed above for the id166) using the predictive means only (the predictive
variances were ignored19), except that instead of the 5-fold cross validation,
leave-one-out cross-validation (loo-cv) was used, and the kernel parameters
were also set using loo-cv.

figure 3.10 shows that the three best methods are the ep approximation for
gpc, the id166 and the least-squares classi   er (lsc). presenting both the error
rates and the test information helps to highlight di   erences which may not be
apparent from a single plot alone. for example, laplace   s method and ep seem
very similar on error rates, but quite di   erent in test information. notice also,
that the error-reject curve itself reveals interesting di   erences, e.g. notice that
although the p1nn method has an error rate comparable to other methods at
zero rejections, things don   t improve very much when rejections are allowed.
refer to section 3.8 for more discussion of the results.

3.7.4

10-class handwritten digit classi   cation example

we apply the multi-class laplace approximation developed in section 3.5 to the
10-class handwritten digit classi   cation problem from the (repartitioned) usps
dataset, having n = 4649 training cases and n    = 4649 cases for testing, see
page 63. we used a squared exponential covariance function with two hyper-
parameters: a single signal amplitude   f , common to all 10 latent functions,
and a single length-scale parameter    , common to all 10 latent functions and
common to all 256 input dimensions.

the behaviour of the method was investigated on a grid of values for the
hyperparameters, see figure 3.11. note that the correspondence between the
log marginal likelihood and the test information is not as close as for laplace   s
method for binary classi   cation in figure 3.7 on page 64. the maximum value
of the log marginal likelihood attained is -1018, and for the hyperparameters
corresponding to this point the error rate is 3.1% and the test information
2.67 bits. as with the binary classi   cation problem, the test information is
standardized by subtracting o    the negative id178 (information) of the targets
which is    3.27 bits. the classi   cation error rate in figure 3.11(c) shows a clear
minimum, and this is also attained at a shorter length-scale than where the
marginal likelihood and test information have their maxima. this e   ect was
also seen in the experiments on binary classi   cation.

to gain some insight into the level of performance we compared these re-
sults with those obtained with the probabilistic one nearest neighbour method
p1nn, a multiple id28 model and a id166. the p1nn    rst uses an

19of course, one could also have tried a variant where the full latent predictive distribution

was averaged over, but we did not do that here.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.7 experiments

71

(a)

(b)

(c)

figure 3.11: 10-way digit classi   cation using the laplace approximation. panel
(a) shows the approximate log marginal likelihood, reaching a maximum value of
log p(y|x,   ) =    1018 at log     = 2.35 and log   f = 2.6.
in panel (b) information
about the test cases is shown. the maximum possible amount of information about
the test targets, corresponding to perfect classi   cation, would be 3.27 bits (the id178
of the targets). at the point of maximum marginal likelihood, the test information is
2.67 bits. in panel (c) the test set misclassi   cation rate is shown in percent. at the
point of maximum marginal likelihood the test error rate is 3.1%.

internal leave-one-out assessment on the training set to estimate its probabil-
ity of being correct,   . for the test set it then predicts the nearest neighbour
with id203    and all other classes with equal id203 (1       )/9. we
obtained    = 0.967, a test information of 2.98 bits and a test set classi   cation
error rate of 3.0%.

we also compare to multiple linear id28. one way to imple-
ment this method is to view it as a gaussian process with a linear covariance

2345012345log lengthscale, log(l)log magnitude, log(  f)log marginal likelihood   1050   1100   1200   1300   1500   2000   3000   3000   30002345012345log lengthscale, log(l)log magnitude, log(  f)information about the test targets in bits11222.52.52.82.82.952.992345012345log lengthscale, log(l)log magnitude, log(  f)test set misclassification percentage2.62.72.833.3451010c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

72

classi   cation

function, although it is equivalent and computationally more e   cient to do the
laplace approximation over the    weights    of the linear model. in our case there
are 10  257 weights (256 inputs and one bias), whereas there are 10  4696 latent
function values in the gp. the linear covariance function k(x, x0) =   2x>x0 has
a single hyperparameter    (used for all 10 latent functions). optimizing the log
marginal likelihood w.r.t.    gives log p(y|x,   ) =    1339 at    = 1.45. using this
value for the hyperparameter, the test information is 2.95 bits and the test set
error rate is 5.7%.

finally, a support vector machine (id166) classi   er was trained using the
same se kernel as the gaussian process classi   ers. (see section 6.4 for further
details on the id166.) as in the binary id166 case there were two free parameters
    (the length-scale of the kernel), and the trade-o    parameter c (see eq. (6.37)),
f . we carried out 5-fold cross-validation on a grid
which plays a similar r  ole to   2
in parameter space to identify the best combination of parameters w.r.t. the
error rate; this turned out to be at c = 1,     = 5. our experiments were
conducted using the id166torch software [collobert and bengio, 2001], which
implements multi-class id166 classi   cation using the one-versus-rest method de-
scribed in section 6.5. the test set error rate for the id166 is 2.2%; we did not
attempt to evaluate the test information for the multi-class id166.

3.8 discussion

in the previous section we presented several sets of experiments comparing the
two approximate methods for id136 in gpc models, and comparing them to
other commonly-used supervised learning methods. in this section we discuss
the results and attempt to relate them to the properties of the models.

for the binary examples from figures 3.7 and 3.8, we saw that the two ap-
proximations showed quite di   erent qualitative behaviour of the approximated
log marginal likelihood, although the exact marginal likelihood is of course iden-
tical. the ep approximation gave a higher maximum value of the log marginal
likelihood (by about 9 units on the log scale) and the test information was
somewhat better than for laplace   s method, although the test set error rates
were comparable. however, although this experiment seems to favour the ep
approximation, it is interesting to know how close these approximations are to
the exact (analytically intractable) solutions. in figure 3.12 we show the results
of running a sophisticated id115 method called annealed
importance sampling [neal, 2001] carried out by kuss and rasmussen [2005].
the usps dataset for these experiments was identical to the one used in fig-
ures 3.7 and 3.8, so the results are directly comparable.
it is seen that the
mcmc results indicate that the ep method achieves a very high level of accu-
racy, i.e. that the di   erence between ep and laplace   s method is caused almost
exclusively by approximation errors in laplace   s method.

the main reason for the inaccuracy of laplace   s method is that the high
dimensional posterior is skew, and that the symmetric approximation centered
on the mode is not characterizing the posterior volume very well. the posterior

monte carlo results

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.8 discussion

73

(a)

(b)

figure 3.12: the log marginal likelihood, panel (a), and test information, panel
(b), for the usps 3   s vs. 5   s binary classi   cation task computed using markov chain
monte carlo (mcmc). comparing this to the laplace approximation figure 3.7 and
figure 3.8 shows that the ep approximation is surprisingly accurate. the slight
wiggliness of the contour lines are caused by    nite sample e   ects in the mcmc runs.

is a combination of the (correlated) gaussian prior centered on the origin and
the likelihood terms which (softly) cut o    half-spaces which do not agree with
the training set labels. therefore the posterior looks like a correlated gaussian
restricted to the orthant which agrees with the labels. its mode will be located
close to the origin in that orthant, and it will decrease rapidly in the direction
towards the origin due to con   icts from the likelihood terms, and decrease only
slowly in the opposite direction (because of the prior). seen in this light it is
not surprising that the laplace approximation is somewhat inaccurate. this
explanation is corroborated further by kuss and rasmussen [2005].

it should be noted that all the methods compared on the binary digits clas-
si   cation task except for the linear probit model are using the squared distance
between the digitized digit images measured directly in the image space as the
sole input to the algorithm. this distance measure is not very well suited for
the digit discrimination task   for example, two similar images that are slight
translations of each other may have a huge squared distance, although of course
identical labels. one of the strengths of the gp formalism is that one can use
prior distributions over (latent, in this case) functions, and do id136 based
on these. if however, the prior over functions depends only on one particular as-
pect of the data (the squared distance in image space) which is not so well suited
for discrimination, then the prior used is also not very appropriate. it would be
more interesting to design covariance functions (parameterized by hyperparame-
ters) which are more appropriate for the digit discrimination task, e.g. re   ecting
on the known invariances in the images, such as the    tangent-distance    ideas
from simard et al. [1992]; see also sch  olkopf and smola [2002, ch. 11] and section
9.10. the results shown here follow the common approach of using a generic

suitablility of the
covariance function

2345012345log lengthscale, log(l)log magnitude, log(  f)log marginal likelihood   92   95   100   105   105   115   130   160   160   200   2002345012345log lengthscale, log(l)log magnitude, log(  f)information about test targets in bits0.250.50.70.70.80.80.840.840.860.880.89c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

74

classi   cation

covariance function with a minimum of hyperparameters, but this doesn   t allow
us to incorporate much prior information about the problem. for an example
in the gp framework for doing id136 about multiple hyperparameters with
more complex covariance functions which provide clearly interpretable infor-
mation about the data, see the carbon dioxide modelling problem discussed on
page 118.

   

3.9 appendix: moment derivations

or in matrix notation

zv>0 =

=

consider the integral of a cumulative gaussian,   , with respect to a gaussian

z    

      

z =

zv>0 =

initially for the special case v >0. writing out in full, substituting z = y     x +
       m and w = x        and interchanging the order of the integrals

z x

      

n (y) dy,

(3.77)

(cid:1) dy dx
    (x       )2
(cid:1) dw dz,
2  2
(cid:21)(cid:20) w

2  2

1
v2
1
v2

z

(3.78)

(cid:21)(cid:1) dw dz

dw dz,

(3.79)

=

v

1

1

      

      

2v2

2    v

  (cid:0) x     m
(cid:1)n (x|  ,   2) dx, where   (x) =
z    
z x
exp(cid:0)    (y     m)2
z    
z      m
exp(cid:0)    (z + w)2
2v2     w2
(cid:21)>(cid:20) 1
(cid:20) w
z      m
z    
exp(cid:0)    1
z      m
z    
i(cid:12)(cid:12)(cid:12) 0,
n(cid:16)h w
h   2
z      m

     2
v2 +   2

v2 + 1
  2

i(cid:17)

     2

2    v

2    v

      

      

      

      

      

      

1
v2

1

2

z

z

(cid:17)

(cid:16)    

dz =   (cid:0)        m   

(cid:1),

v2 +   2

i.e. an (incomplete) integral over a joint gaussian. the inner integral corre-
sponds to marginalizing over w (see eq. (a.6)), yielding

(3.80)
which assumed v > 0. if v is negative, we can substitute the symmetry   (   z) =
1       (z) into eq. (3.77) to get

z2

      

exp

zv>0 =

2(v2 +   2)

1p2  (v2 +   2)
(cid:1) =   (cid:0)           m   
zv<0 = 1       (cid:0)        m   
(cid:1)n (x|  ,   2) dx =   (z), where z =
(cid:1)n (x|  ,   2),
q(x) = z   1  (cid:0) x     m

v2 +   2
collecting the two cases, eq. (3.80) and eq. (3.81) we arrive at
   

z =
for general v 6= 0. we wish to compute the moments of

  (cid:0) x     m

v2 +   2

z

v

v

(cid:1).

(3.81)

, (3.82)

(3.83)

       m
1 +   2/v2

v

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.10 exercises

75

where z is given in eq. (3.82). perhaps the easiest way to do this is to di   er-
entiate w.r.t.    on both sides of eq. (3.82)

z x       
  2   (cid:0) x     m
x  (cid:0) x     m

v

(cid:1)n (x|  ,   2) dx =    

v

(cid:1)n (x|  ,   2) dx       z

  2 =

   z
     

=

z

1
  2

  (z)       
n (z)
1 +   2/v2

     
   

v

(3.84)

,

where we have used      (z)/      = n (z)   z/     . we recognize the    rst term in
the integral in the top line of eq. (3.84) as z/  2 times the    rst moment of q
which we are seeking. multiplying through by   2/z and rearranging we obtain

   rst moment

eq[x] =    +

  2n (z)
   

  (z)v

1 +   2/v2

.

(3.85)

similarly, the second moment can be obtained by di   erentiating eq. (3.82) twice

z h x2
  4     2  x
       eq[x2] = 2  eq[x]       2 +   2       4zn (z)

  (cid:0) x     m

  4 +   2

  4     1

i

  2

v

(cid:1)n (x|  ,   2) dx =     zn (z)

v2 +   2

   2z
     2 =

(3.86)

  (z)(v2 +   2) ,

second moment

where the    rst and second terms of the integral in the top line of eq. (3.86) are
multiples of the    rst and second moments. the second central moment after
reintroducing eq. (3.85) into eq. (3.86) and simplifying is given by

(cid:2)(x   eq[x])2(cid:3) = eq[x2]   eq[x]2 =   2   

eq

(cid:16)

(cid:17)

  4n (z)

(v2 +   2)  (z)

z +

n (z)
  (z)

. (3.87)

3.10 exercises

1. for binary gpc, show the equivalence of using a noise-free latent process
combined with a probit likelihood and a latent process with gaussian
noise combined with a step-function likelihood. hint: introduce explicitly
additional noisy latent variables   fi, which di   er from fi by gaussian noise.
write down the step function likelihood for a single case as a function of
  fi, integrate out the noisy variable, to arrive at the probit likelihood as a
function of the noise-free process.

2. consider a multinomial random variable y having c states, with yc = 1 if
the variable is in state c, and 0 otherwise. state c occurs with id203
  c. show that cov(y) = e[(y       )(y       )>] = diag(  )         >. ob-
serve that cov(y), being a covariance matrix, must necessarily be positive
semide   nite. using this fact show that the matrix w = diag(  )         >
from eq. (3.38) is positive semide   nite. by showing that the vector of all
ones is an eigenvector of cov(y) with eigenvalue zero, verify that the ma-
trix is indeed positive semide   nite, and not positive de   nite. (see section
4.1 for de   nitions of positive semide   nite and positive de   nite matrices.)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

76

classi   cation

figure 3.13: the decision regions for the three-class softmax function in z2-z3 space.

3. consider the 3-class softmax function

p(cc) =

exp(fc)

exp(f1) + exp(f2) + exp(f3) ,

where c = 1, 2, 3 and f1, f2, f3 are the corresponding activations. to
more easily visualize the decision boundaries, let z2 = f2     f1 and z3 =
f3     f1. thus

p(c1) =

1

1 + exp(z2) + exp(z3) ,

(3.88)

and similarly for the other classes. the decision boundary relating to
p(c1) > 1/3 is the curve exp(z2) + exp(z3) = 2. the decision regions for
the three classes are illustrated in figure 3.13. let f = (f1, f2, f3)> have
a gaussian distribution centered on the origin, and let   (f) = softmax(f).

we now consider the e   ect of this distribution on      =r   (f)p(f) df. for

a gaussian with given covariance structure this integral is easily approxi-
mated by drawing samples from p(f). show that the classi   cation can be
made to fall into any of the three categories depending on the covariance
matrix. thus, by considering displacements of the mean of the gaussian
by   from the origin into each of the three regions we have shown that
overall classi   cation depends not only on the mean of the gaussian but
also on its covariance. show that this conclusion is still valid when it is
recalled that z is derived from f as z = t f where

(cid:18) 1

0

(cid:19)

,

0    1
1    1

t =

so that cov(z) = t cov(f)t >.

4. consider the update equation for f new given by eq. (3.18) when some of
the training points are well-explained under f so that ti       i and wii     0

r1r2r3   303   303z2z3c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

3.10 exercises

77

for these points. break f into two subvectors, f1 that corresponds to
points that are not well-explained, and f2 to those that are. re-write
(k   1 + w )   1 from eq. (3.18) as k(i + w k)   1 and let k be partitioned
as k11, k12, k21, k22 and similarly for the other matrices. using the
partitioned matrix inverse equations (see section a.3) show that

= k11(i11 + w11k11)   1(cid:0)w11f1 +     log p(y1|f1)(cid:1),

(3.89)

f new
1
f new
2

= k21k   1

11 f new

1

.

see section 3.4.1 for the consequences of this result.

5. show that the expressions in eq. (3.56) for the cavity mean      i and vari-
ance   2   i do not depend on the approximate likelihood terms     i and     2
i
for the corresponding case, despite the appearance of eq. (3.56).

6. consider the usps 3s vs. 5s prediction problem discussed in section 3.7.3.
use the implementation of the laplace binary gpc provided to investi-
gate how   f and the predictive probabilities etc. vary as functions of log(   )
and log(  f ).

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

chapter 4

covariance functions

we have seen that a covariance function is the crucial ingredient in a gaussian
process predictor, as it encodes our assumptions about the function which we
wish to learn. from a slightly di   erent viewpoint it is clear that in supervised
learning the notion of similarity between data points is crucial; it is a basic
assumption that points with inputs x which are close are likely to have similar
target values y, and thus training points that are near to a test point should
be informative about the prediction at that point. under the gaussian process
view it is the covariance function that de   nes nearness or similarity.

an arbitrary function of input pairs x and x0 will not, in general, be a valid
covariance function.1 the purpose of this chapter is to give examples of some
commonly-used covariance functions and to examine their properties. section
4.1 de   nes a number of basic terms relating to covariance functions. section 4.2
gives examples of stationary, dot-product, and other non-stationary covariance
functions, and also gives some ways to make new ones from old. section 4.3
introduces the important topic of eigenfunction analysis of covariance functions,
and states mercer   s theorem which allows us to express the covariance function
(under certain conditions) in terms of its eigenfunctions and eigenvalues. the
covariance functions given in section 4.2 are valid when the input domain x is
a subset of rd. in section 4.4 we describe ways to de   ne covariance functions
when the input domain is over structured objects such as strings and trees.

4.1 preliminaries
a stationary covariance function is a function of x     x0. thus it is invariant
to translations in the input space.2 for example the squared exponential co-

1to be a valid covariance function it must be positive semide   nite, see eq. (4.2).
2in stochastic process theory a process which has constant mean and whose covariance
function is invariant to translations is called weakly stationary. a process is strictly sta-
tionary if all of its    nite dimensional distributions are invariant to translations [papoulis,
1991, sec. 10.1].

similarity

valid covariance
functions

stationarity

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

80

isotropy

dot product covariance

kernel

gram matrix
covariance matrix

positive semide   nite

covariance functions

variance function given in equation 2.16 is stationary. if further the covariance
function is a function only of |x     x0| then it is called isotropic; it is thus in-
variant to all rigid motions. for example the squared exponential covariance
function given in equation 2.16 is isotropic. as k is now only a function of
r = |x     x0| these are also known as radial basis functions (rbfs).

if a covariance function depends only on x and x0 through x    x0 we call it
a dot product covariance function. a simple example is the covariance function
0 + x    x0 which can be obtained from id75 by putting
k(x, x0) =   2
n (0, 1) priors on the coe   cients of xd (d = 1, . . . , d) and a prior of n (0,   2
0)
on the bias (or constant function) 1, see eq. (2.15). another important example
0 + x    x0)p where p is a
is the inhomogeneous polynomial kernel k(x, x0) = (  2
positive integer. dot product covariance functions are invariant to a rotation
of the coordinates about the origin, but not translations.

a general name for a function k of two arguments mapping a pair of inputs
x     x , x0     x into r is a kernel. this term arises in the theory of integral
operators, where the operator tk is de   ned as

(tkf)(x) =

k(x, x0)f(x0) d  (x0),

(4.1)

z

x

where    denotes a measure; see section a.7 for further explanation of this point.3
a real kernel is said to be symmetric if k(x, x0) = k(x0, x); clearly covariance
functions must be symmetric from the de   nition.

given a set of input points {xi|i = 1, . . . , n} we can compute the gram
matrix k whose entries are kij = k(xi, xj). if k is a covariance function we
call the matrix k the covariance matrix.

a real n    n matrix k which satis   es q(v) = v>kv     0 for all vectors
v     rn is called positive semide   nite (psd). if q(v) = 0 only when v = 0
the matrix is positive de   nite. q(v) is called a quadratic form. a symmetric
matrix is psd if and only if all of its eigenvalues are non-negative. a gram
matrix corresponding to a general id81 need not be psd, but the
gram matrix corresponding to a covariance function is psd.

a kernel is said to be positive semide   nite if

k(x, x0)f(x)f(x0) d  (x) d  (x0)     0,

(4.2)

z

for all f     l2(x ,   ). equivalently a id81 which gives rise to psd
gram matrices for any choice of n     n and d is positive semide   nite. to
see this let f be the weighted sum of delta functions at each xi. since such
functions are limits of functions in l2(x ,   ) eq. (4.2) implies that the gram
matrix corresponding to any d is psd.

for a one-dimensional gaussian process one way to understand the charac-
teristic length-scale of the process (if this exists) is in terms of the number of
upcrossings of a level u. adler [1981, theorem 4.1.1] states that the expected

3informally speaking, readers will usually be able to substitute dx or p(x)dx for d  (x).

upcrossing rate

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4.2 examples of covariance functions

81

number of upcrossings e[nu] of the level u on the unit interval by a zero-mean,
stationary, almost surely continuous gaussian process is given by

s   k00(0)

k(0)

(cid:16)    u2

2k(0)

(cid:17)

e[nu] =

1
2  

exp

.

(4.3)

if k00(0) does not exist (so that the process is not mean square di   erentiable)
then if such a process has a zero at x0 then it will almost surely have an in   nite
number of zeros in the arbitrarily small interval (x0, x0 +   ) [blake and lindsey,
1973, p. 303].

4.1.1 mean square continuity and di   erentiability

   

we now describe mean square continuity and di   erentiability of stochastic pro-
cesses, following adler [1981, sec. 2.2]. let x1, x2, . . . be a sequence of points
and x    be a    xed point in rd such that |xk     x   |     0 as k        . then a
process f(x) is continuous in mean square at x    if e[|f(xk)     f(x   )|2]     0 as mean square continuity
k        . if this holds for all x        a where a is a subset of rd then f(x) is said
to be continuous in mean square (ms) over a. a random    eld is continuous in
mean square at x    if and only if its covariance function k(x, x0) is continuous
at the point x = x0 = x   . for stationary covariance functions this reduces
to checking continuity at k(0). note that ms continuity does not necessarily
imply sample function continuity; for a discussion of sample function continuity
and di   erentiability see adler [1981, ch. 3].

the mean square derivative of f(x) in the ith direction is de   ned as

mean square
di   erentiability

   f(x)
   xi

= l. i. m
h   0

f(x + hei)     f(x)

h

,

(4.4)

when the limit exists, where l.i.m denotes the limit in mean square and ei
is the unit vector in the ith direction. the covariance function of    f(x)/   xi
is given by    2k(x, x0)/   xi   x0
i. these de   nitions can be extended to higher
order derivatives. for stationary processes, if the 2kth-order partial derivative
   2kk(x)/   2xi1 . . .    2xik exists and is    nite at x = 0 then the kth order partial
derivative    kf(x)/   xi1 . . . xik exists for all x     rd as a mean square limit.
notice that it is the properties of the kernel k around 0 that determine the
smoothness properties (ms di   erentiability) of a stationary process.

4.2 examples of covariance functions
in this section we consider covariance functions where the input domain x is
a subset of the vector space rd. more general input spaces are considered in
section 4.4. we start in section 4.2.1 with stationary covariance functions, then
consider dot-product covariance functions in section 4.2.2 and other varieties
of non-stationary covariance functions in section 4.2.3. we give an overview
of some commonly used covariance functions in table 4.1 and in section 4.2.4

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

82

covariance functions

we describe general methods for constructing new kernels from old. there
exist several other good overviews of covariance functions, see e.g. abrahamsen
[1997].

4.2.1 stationary covariance functions

in this section (and section 4.3) it will be convenient to allow kernels to be a map
from x     x , x0     x into c (rather than r). if a zero-mean process f is complex-
valued, then the covariance function is de   ned as k(x, x0) = e[f(x)f   (x0)],
where     denotes complex conjugation.

a stationary covariance function is a function of    = x     x0. sometimes in

this case we will write k as a function of a single argument, i.e. k(   ).

the covariance function of a stationary process can be represented as the

fourier transform of a positive    nite measure.

bochner   s theorem

theorem 4.1 (bochner   s theorem) a complex-valued function k on rd is the
covariance function of a weakly stationary mean square continuous complex-
valued random process on rd if and only if it can be represented as

where    is a positive    nite measure.

k(   ) =

rd

e2  is     d  (s)

(4.5)

(cid:3)

z

spectral density
power spectrum

the statement of bochner   s theorem is quoted from stein [1999, p. 24]; a proof
can be found in gihman and skorohod [1974, p. 208]. if    has a density s(s)
then s is known as the spectral density or power spectrum corresponding to k.
the construction given by eq. (4.5) puts non-negative power into each fre-
quency s; this is analogous to the requirement that the prior covariance matrix
  p on the weights in equation 2.4 be non-negative de   nite.

z

z

in the case that the spectral density s(s) exists, the covariance function and
the spectral density are fourier duals of each other as shown in eq. (4.6);4 this
is known as the wiener-khintchine theorem, see, e.g. chat   eld [1989]

k(   ) =

notice that the variance of the process is k(0) =r s(s) ds so the power spectrum

s(s) =

k(   )e   2  is     d   .

s(s)e2  is     ds,

(4.6)

must be integrable to de   ne a valid gaussian process.

to gain some intuition for the de   nition of the power spectrum given in
eq. (4.6) it is important to realize that the complex exponentials e2  is  x are
eigenfunctions of a stationary kernel with respect to lebesgue measure (see
section 4.3 for further details). thus s(s) is, loosely speaking, the amount of
power allocated on average to the eigenfunction e2  is  x with frequency s. s(s)
must eventually decay su   ciently fast as |s|         so that it is integrable; the

4see appendix a.8 for details of fourier transforms.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4.2 examples of covariance functions

83

rate of this decay of the power spectrum gives important information about
the smoothness of the associated stochastic process. for example it can deter-
mine the mean-square di   erentiability of the process (see section 4.3 for further
details).

if the covariance function is isotropic (so that it is a function of r, where
r = |  |) then it can be shown that s(s) is a function of s , |s| only [adler,
1981, theorem 2.5.2]. in this case the integrals in eq. (4.6) can be simpli   ed
by changing to spherical polar coordinates and integrating out the angular
variables (see e.g. bracewell, 1986, ch. 12) to obtain

z    
z    

0

0

k(r) =

s(s) =

2  

rd/2   1

2  

sd/2   1

s(s)jd/2   1(2  rs)sd/2 ds,

k(r)jd/2   1(2  rs)rd/2 dr,

(4.7)

(4.8)

where jd/2   1 is a bessel function of order d/2   1. note that the dependence on
the dimensionality d in equation 4.7 means that the same isotropic functional
form of the spectral density can give rise to di   erent isotropic covariance func-
tions in di   erent dimensions. similarly, if we start with a particular isotropic
covariance function k(r) the form of spectral density will in general depend on
d (see, e.g. the mat  ern class spectral density given in eq. (4.15)) and in fact
k(r) may not be valid for all d. a necessary condition for the spectral density

to exist is thatr rd   1|k(r)| dr <    ; see stein [1999, sec. 2.10] for more details.

we now give some examples of commonly-used isotropic covariance func-
tions. the covariance functions are given in a normalized form where k(0) = 1;
we can multiply k by a (positive) constant   2
f to get any desired process vari-
ance.

squared exponential covariance function

the squared exponential (se) covariance function has already been introduced
in chapter 2, eq. (2.16) and has the form

kse(r) = exp

,

(4.9)

with parameter     de   ning the characteristic length-scale. using eq. (4.3) we
see that the mean number of level-zero upcrossings for a se process in 1-
d is (2     )   1, which con   rms the r  ole of     as a length-scale. this covari-
ance function is in   nitely di   erentiable, which means that the gp with this
covariance function has mean square derivatives of all orders, and is thus
very smooth. the spectral density of the se covariance function is s(s) =
(2     2)d/2 exp(   2  2   2s2). stein [1999] argues that such strong smoothness
assumptions are unrealistic for modelling many physical processes, and rec-
ommends the mat  ern class (see below). however, the squared exponential is
probably the most widely-used kernel within the kernel machines    eld.

(cid:17)

(cid:16)    r2

2   2

squared exponential

characteristic
length-scale

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

84

covariance functions

in   nitely divisible

the se kernel is in   nitely divisible in that (k(r))t is a valid kernel for all

t > 0; the e   ect of raising k to the power of t is simply to rescale    .

in   nite network
construction for se
covariance function

mat  ern class

nx

c=1

lim
n      

  2
p
n

nx

c=1

z cmax

cmin

we now digress brie   y, to show that the squared exponential covariance
function can also be obtained by expanding the input x into a feature space
de   ned by gaussian-shaped basis functions centered densely in x-space. for
simplicity of exposition we consider scalar inputs with basis functions

  c(x) = exp(cid:0)    (x     c)2

(cid:1),

2   2

(4.10)

where c denotes the centre of the basis function. from sections 2.1 and 2.2 we
recall that with a gaussian prior on the weights w     n (0,   2
pi), this gives rise
to a gp with covariance function

k(xp, xq) =   2
p

  c(xp)  c(xq).

(4.11)

now, allowing an in   nite number of basis functions centered everywhere on an
interval (and scaling down the variance of the prior on the weights with the
number of basis functions) we obtain the limit

  c(xp)  c(xq) =   2
p

  c(xp)  c(xq)dc.

(4.12)

plugging in the gaussian-shaped basis functions eq. (4.10) and letting the in-
tegration limits go to in   nity we obtain

z    
exp(cid:0)    (xp     c)2
p exp(cid:0)    (xp     xq)2

2   2
   
2(

2   )2

      

       2

(cid:1)dc

(cid:1) exp(cid:0)    (xq     c)2
(cid:1),

2   2

k(xp, xq) =   2
p
   

=

   
which we recognize as a squared exponential covariance function with a
2
times longer length-scale. the derivation is adapted from mackay [1998]. it
is straightforward to generalize this construction to multivariate x. see also
eq. (4.30) for a similar construction where the centres of the basis functions are
sampled from a gaussian distribution; the constructions are equivalent when
the variance of this gaussian tends to in   nity.

(4.13)

the mat  ern class of covariance functions

the mat  ern class of covariance functions is given by

kmatern(r) =

21     
  (  )

(cid:16)   

(cid:17)  

2  r
   

k  

(cid:16)   

(cid:17)

,

2  r
   

(4.14)

with positive parameters    and    , where k   is a modi   ed bessel function
[abramowitz and stegun, 1965, sec. 9.6]. this covariance function has a spectral
density

(cid:16)2  
   2 + 4  2s2(cid:17)   (  +d/2)

(4.15)

s(s) =

2d  d/2  (   + d/2)(2  )  

  (  )   2  

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4.2 examples of covariance functions

85

(a)

(b)

figure 4.1: panel (a): covariance functions, and (b): random functions drawn from
gaussian processes with mat  ern covariance functions, eq. (4.14), for di   erent values of
  , with     = 1. the sample functions on the right were obtained using a discretization
of the x-axis of 2000 equally-spaced points.

in d dimensions. note that the scaling is chosen so that for            we obtain
the se covariance function e   r2/2   2, see eq. (a.25). stein [1999] named this the
mat  ern class after the work of mat  ern [1960]. for the mat  ern class the process
f(x) is k-times ms di   erentiable if and only if    > k. the mat  ern covariance
functions become especially simple when    is half-integer:    = p + 1/2, where
p is a non-negative integer. in this case the covariance function is a product
of an exponential and a polynomial of order p, the general expression can be
derived from [abramowitz and stegun, 1965, eq. 10.2.15], giving

(cid:16)   

   

(cid:17)   (p + 1)

px

  (2p + 1)

i=0

2  r
   

(cid:16)   

(cid:17)p   i

(p + i)!
i!(p     i)!

8  r
   

.

(4.16)

k  =p+1/2(r) = exp

it is possible that the most interesting cases for machine learning are    = 3/2
and    = 5/2, for which

(cid:16)
(cid:16)

(cid:17)

   

3r
   
   
5r
   

exp

+

5r2
3   2

(cid:16)   
(cid:17)

   

(cid:17)
(cid:16)   

3r
   

exp

,

   

5r
   

k  =3/2(r) =

k  =5/2(r) =

1 +

1 +

(cid:17)

,

(4.17)

since for    = 1/2 the process becomes very rough (see below), and for        7/2,
in the absence of explicit prior knowledge about the existence of higher order
derivatives, it is probably very hard from    nite noisy training examples to
distinguish between values of        7/2 (or even to distinguish between    nite
values of    and           , the smooth squared exponential, in this case). for
example a value of    = 5/2 was used in [cornford et al., 2002].

ornstein-uhlenbeck process and exponential covariance function

the special case obtained by setting    = 1/2 in the mat  ern class gives the
exponential covariance function k(r) = exp(   r/   ). the corresponding process

exponential

012300.20.40.60.81input distance, rcovariance, k(r)  =1/2  =2           505   202input, xoutput, f(x)c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

86

covariance functions

(a)

(b)

figure 4.2: panel (a) covariance functions, and (b) random functions drawn from
gaussian processes with the   -exponential covariance function eq. (4.18), for di   erent
values of   , with     = 1. the sample functions are only di   erentiable when    = 2 (the
se case). the sample functions on the right were obtained using a discretization of
the x-axis of 2000 equally-spaced points.

is ms continuous but not ms di   erentiable. in d = 1 this is the covariance
function of the ornstein-uhlenbeck (ou) process. the ou process [uhlenbeck
and ornstein, 1930] was introduced as a mathematical model of the velocity
of a particle undergoing brownian motion. more generally in d = 1 setting
   + 1/2 = p for integer p gives rise to a particular form of a continuous-time
ar(p) gaussian process; for further details see section b.2.1. the form of the
mat  ern covariance function and samples drawn from it for    = 1/2,    = 2 and
           are illustrated in figure 4.1.

the   -exponential covariance function

the   -exponential family of covariance functions, which includes both the ex-
ponential and squared exponential, is given by

k(r) = exp(cid:0)     (r/   )  (cid:1) for 0 <        2.

(4.18)

although this function has a similar number of parameters to the mat  ern class,
it is (as stein [1999] notes) in a sense less    exible. this is because the corre-
sponding process is not ms di   erentiable except when    = 2 (when it is in-
   nitely ms di   erentiable). the covariance function and random samples from
the process are shown in figure 4.2. a proof of the positive de   niteness of this
covariance function can be found in schoenberg [1938].

rational quadratic covariance function

the rational quadratic (rq) covariance function

(cid:16)

(cid:17)     

krq(r) =

1 + r2
2     2

(4.19)

ornstein-uhlenbeck
process

  -exponential

rational quadratic

012300.20.40.60.81input distancecovariance  =1  =1.5  =2   505   3   2   10123input, xoutput, f(x)c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4.2 examples of covariance functions

87

(a)

(b)

figure 4.3: panel (a) covariance functions, and (b) random functions drawn from
gaussian processes with rational quadratic covariance functions, eq. (4.20), for di   er-
ent values of    with     = 1. the sample functions on the right were obtained using a
discretization of the x-axis of 2000 equally-spaced points.

with   ,     > 0 can be seen as a scale mixture (an in   nite sum) of squared
exponential (se) covariance functions with di   erent characteristic length-scales
(sums of covariance functions are also a valid covariance, see section 4.2.4).
parameterizing now in terms of inverse squared length scales,    =       2, and
putting a gamma distribution on p(  |  ,   )             1 exp(        /  ),5 we can add
up the contributions through the following integral

krq(r) =

   

p(  |  ,   )kse(r|  ) d  

(cid:17)

(cid:16)        

  

(cid:16)       r2

2

(cid:17)

d       (cid:16)

1 + r2
2     2

(cid:17)     

,

(4.20)

exp

        1 exp

z
z

scale mixture

where we have set      1 =    2. the rational quadratic is also discussed by mat  ern
[1960, p. 17] using a slightly di   erent parameterization; in our notation the limit
of the rq covariance for            (see eq. (a.25)) is the se covariance function
with characteristic length-scale    , eq. (4.9). figure 4.3 illustrates the behaviour
for di   erent values of   ; note that the process is in   nitely ms di   erentiable for
every    in contrast to the mat  ern covariance function in figure 4.1.

the previous example is a special case of kernels which can be written as
superpositions of se kernels with a distribution p(   ) of length-scales    , k(r) =

r exp(   r2/2   2)p(   ) d   . this is in fact the most general representation for an

isotropic kernel which de   nes a valid covariance function in any dimension d,
see [stein, 1999, sec. 2.10].

piecewise polynomial covariance functions with compact support

a family of piecewise polynomial functions with compact support provide an-
other interesting class of covariance functions. compact support means that

5 note that there are several common ways to parameterize the gamma distribution   our

choice is convenient here:    is the    shape    and    is the mean.

piecewise polynomial
covariance functions
with compact support

012300.20.40.60.81input distancecovariance  =1/2  =2           505   3   2   10123input, xoutput, f(x)c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

88

covariance functions

(a)

(b)

figure 4.4: panel (a): covariance functions, and (b): random functions drawn from
gaussian processes with piecewise polynomial covariance functions with compact sup-
port from eq. (4.21), with speci   ed parameters.

positive de   niteness

restricted dimension

the covariance between points become exactly zero when their distance exceeds
a certain threshold. this means that the covariance matrix will become sparse
by construction, leading to the possibility of computational advantages.6 the
challenge in designing these functions is how to guarantee positive de   nite-
ness. multiple algorithms for deriving such covariance functions are discussed
by wendland [2005, ch. 9]. these functions are usually not positive de   nite
for all input dimensions, but their validity is restricted up to some maximum
dimension d. below we give examples of covariance functions kppd,q(r) which
are positive de   nite in rd

kppd,0(r) = (1     r)j
+,
kppd,1(r) = (1     r)j+1
kppd,2(r) = (1     r)j+2
kppd,3(r) = (1     r)j+3

+

+

+

2 c + q + 1,

where j = b d

(cid:0)(j + 1)r + 1(cid:1),
(cid:0)(j2 + 4j + 3)r2 + (3j + 6)r + 3(cid:1)/3,
(cid:0)(j3 + 9j2 + 23j + 15)r3+
(6j2 + 36j + 45)r2 + (15j + 45)r + 15(cid:1)/15.

(4.21)

the properties of three of these covariance functions are illustrated in fig-
ure 4.4. these covariance functions are 2q-times continuously di   erentiable,
and thus the corresponding processes are q-times mean-square di   erentiable,
see section 4.1.1.
it is interesting to ask to what extent one could use the
compactly-supported covariance functions described above in place of the other
covariance functions mentioned in this section, while obtaining id136s that
are similar. one advantage of the compact support is that it gives rise to spar-
sity of the gram matrix which could be exploited, for example, when using
iterative solutions to gpr problem, see section 8.3.6.

6if the product of the inverse covariance matrix with a vector (needed e.g. for prediction)
is computed using a conjugate gradient algorithm, then products of the covariance matrix
with vectors are the basic computational unit, and these can obviously be carried out much
faster if the matrix is sparse.

00.20.40.60.8100.20.40.60.81input distance, rcovariance, k(r)d=1, q=1d=3, q=1d=1, q=2   2   1012   202input, xoutput, f(x)c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4.2 examples of covariance functions

89

further properties of stationary covariance functions

the covariance functions given above decay monotonically with r and are always
positive. however, this is not a necessary condition for a covariance function.
for example yaglom [1987] shows that k(r) = c(  r)     j  (  r) is a valid covari-
ance function for        (d     2)/2 and    > 0; this function has the form of a
damped oscillation.

anisotropic versions of these isotropic covariance functions can be created
by setting r2(x, x0) = (x     x0)>m(x     x0) for some positive semide   nite m.
if m is diagonal this implements the use of di   erent length-scales on di   erent
dimensions   for further discussion of automatic relevance determination see
section 5.1. general m   s have been considered by mat  ern [1960, p. 19], poggio
and girosi [1990] and also in vivarelli and williams [1999]; in the latter work a
low-rank m was used to implement a linear id84 step from
the input space to lower-dimensional feature space. more generally, one could
assume the form

(4.22)
where    is a d    k matrix whose columns de   ne k directions of high relevance,
and    is a diagonal matrix (with positive entries), capturing the (usual) axis-
aligned relevances, see also figure 5.1 on page 107. thus m has a factor analysis
form. for appropriate choices of k this may represent a good trade-o    between
   exibility and required number of parameters.

k(x), the kernel kt(x) =p

stationary kernels can also be de   ned on a periodic domain, and can be
readily constructed from stationary kernels on r. given a stationary kernel
m   z k(x + ml) is periodic with period l, as shown in

m =     > +   

section b.2.2 and sch  olkopf and smola [2002, eq. 4.42].

anisotropy

factor analysis distance

periodization

if   2

4.2.2 dot product covariance functions
0 + x    x0 can
as we have already mentioned above the kernel k(x, x0) =   2
0 = 0 we call this the homogeneous
be obtained from id75.
linear kernel, otherwise it is inhomogeneous. of course this can be generalized
0 + x>  px0 by using a general covariance matrix   p on the
to k(x, x0) =   2
components of x, as described in eq. (2.4).7 it is also the case that k(x, x0) =
0 + x>  px0)p is a valid covariance function for positive integer p, because of
(  2
the general result that a positive-integer power of a given covariance function is
also a valid covariance function, as described in section 4.2.4. however, it is also
interesting to show an explicit feature space construction for the polynomial
covariance function. we consider the homogeneous polynomial case as the
inhomogeneous case can simply be obtained by considering x to be extended

7indeed the bias term could also be included in the general expression.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

90

covariance functions

by concatenating a constant. we write

(cid:16) dx

d=1

(cid:17)p

(cid:16) dx

d1=1

=

(cid:17)      (cid:16) dx

dp=1

(cid:17)

xdpx0

dp

xd1x0

d1

xdx0

d

(xd1        xdp)(x0

d1        x0

dp

) ,   (x)      (x0).

(4.23)

k(x, x0) = (x    x0)p =

dx

       dx

d1=1

dp=1

=

d appears in the monomial, under the constraint that pd

notice that this sum apparently contains dp terms but in fact it is less than this
as the order of the indices in the monomial xd1        xdp is unimportant, e.g. for
p = 2, x1x2 and x2x1 are the same monomial. we can remove the redundancy
by de   ning a vector m whose entry md speci   es the number of times index
i=1 mi = p. thus
  m(x), the feature corresponding to vector m is proportional to the monomial
m1!...md! (where as usual we de   ne
xm1
1
0! = 1), giving the feature map

d . the degeneracy of   m(x) is

. . . xmd

p!

  m(x) =

p!

m1!       md! xm1

1

       xmd
d .

(4.24)

   

2x1x2)>. dot-
for example, for p = 2 in d = 2, we have   (x) = (x2
product kernels are sometimes used in a normalized form given by eq. (4.35).
for regression problems the polynomial kernel is a rather strange choice as
the prior variance grows rapidly with |x| for |x| > 1. however, such kernels
have proved e   ective in high-dimensional classi   cation problems (e.g. take x
to be a vectorized binary image) where the input data are binary or greyscale
normalized to [   1, 1] on each dimension [sch  olkopf and smola, 2002, sec. 7.8].

1, x2
2,

r

4.2.3 other non-stationary covariance functions

above we have seen examples of non-stationary dot product kernels. however,
there are also other interesting kernels which are not of this form. in this section
we    rst describe the covariance function belonging to a particular type of neural
network; this construction is due to neal [1996].

consider a network which takes an input x, has one hidden layer with nh
units and then linearly combines the outputs of the hidden units with a bias b
to obtain f(x). the mapping can be written

f(x) = b +

vjh(x; uj),

(4.25)

j=1

where the vjs are the hidden-to-output weights and h(x; u) is the hidden unit
transfer function (which we shall assume is bounded) which depends on the
input-to-hidden weights u. for example, we could choose h(x; u) = tanh(x   u).
this architecture is important because it has been shown by hornik [1993] that
networks with one hidden layer are universal approximators as the number of

nhx

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4.2 examples of covariance functions

91

hidden units tends to in   nity, for a wide class of transfer functions (but exclud-
ing polynomials). let b and the v   s have independent zero-mean distributions
v, respectively, and let the weights uj for each hidden unit
of variance   2
be independently and identically distributed. denoting all weights by w, we
obtain (following neal [1996])

b and   2

ew[f(x)] = 0
ew[f(x)f(x0)] =   2

b +x

eu[h(x; uj)h(x0; uj)]

  2
v

j

=   2

b + nh   2

v

eu[h(x; u)h(x0; u)],

(4.26)
(4.27)

(4.28)

where eq. (4.28) follows because all of the hidden units are identically dis-
tributed. the    nal term in equation 4.28 becomes   2eu[h(x; u)h(x0; u)] by
letting   2

v scale as   2/nh.

the sum in eq. (4.27) is over nh identically and independently distributed
random variables. as the transfer function is bounded, all moments of the
distribution will be bounded and hence the central limit theorem can be applied,
showing that the stochastic process will converge to a gaussian process in the
limit as nh        .

  r z

by evaluating eu[h(x; u)h(x0; u)] we can obtain the covariance function of
the neural network. for example if we choose the error function h(z) = erf(z) =
   
j=1ujxj) and
2/
choose u     n (0,   ) then we obtain [williams, 1998]
2  x>    x0

dt as the transfer function, let h(x; u) = erf(u0 +pd
(cid:17)

0 e   t2

p(1 + 2  x>    x)(1 + 2  x0>    x0)

sin   1(cid:16)

knn(x, x0) =

(4.29)

2
  

,

where   x = (1, x1, . . . , xd)> is an augmented input vector. this is a true    neural
network    covariance function. the    sigmoid    kernel k(x, x0) = tanh(a + bx   x0)
has sometimes been proposed, but in fact this kernel is never positive de   -
nite and is thus not a valid covariance function, see, e.g. sch  olkopf and smola
[2002, p. 113]. figure 4.5 shows a plot of the neural network covariance function
and samples from the prior. we have set    = diag(  2
0,   2). samples from a gp
with this covariance function can be viewed as superpositions of the functions
erf(u0 +ux), where   2
0 controls the variance of u0 (and thus the amount of o   set
of these functions from the origin), and   2 controls u and thus the scaling on
the x-axis. in figure 4.5(b) we observe that the sample functions with larger   
vary more quickly. notice that the samples display the non-stationarity of the
covariance function in that for large values of +x or    x they should tend to a
constant value, consistent with the construction as a superposition of sigmoid
functions.

another interesting construction is to set h(x; u) = exp(   |x     u|2/2  2
g),
where   g sets the scale of this gaussian basis function. with u     n (0,   2
ui)

neural network
covariance function

modulated squared
exponential

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

92

covariance functions

(a), covariance

(b), sample functions

figure 4.5: panel (a): a plot of the covariance function knn(x, x0) for   0 = 10,    = 10.
panel (b): samples drawn from the neural network covariance function with   0 = 2
and    as shown in the legend. the samples were obtained using a discretization of
the x-axis of 500 equally-spaced points.

(4.30)

,

we obtain
kg(x, x0) =

z
(cid:16)    x>x

(cid:16)    |x     u|2
(cid:17)

2  2
g

exp

exp

    |x0     u|2
(cid:16)    |x     x0|2
(cid:17)
2  2
g

(cid:17)
    u>u
(cid:16)    x0>x0
2  2
u

du

(cid:17)

exp

2  2
m
s = 2  2

2  2
s
u and   2

2  2
m
u +   2

1
(2    2
u)d/2

(cid:16)   e

(cid:17)d

exp

=

  u
e = 2/  2

g/  2

u,   2

g +   4

g + 1/  2

m = 2  2
where 1/  2
g. this is
u         (while scaling
in general a non-stationary covariance function, but if   2
  2 appropriately) we recover the squared exponential kg(x, x0)     exp(   |x    
x0|2/4  2
u, kg(x, x0) comprises a squared exponen-
tial covariance function modulated by the gaussian decay envelope function
exp(   x>x/2  2
m), cf. the vertical rescaling construction de-
scribed in section 4.2.4.

g). for a    nite value of   2

m) exp(   x0>x0/2  2

one way to introduce non-stationarity is to introduce an arbitrary non-linear
mapping (or warping) u(x) of the input x and then use a stationary covariance
function in u-space. note that x and u need not have the same dimensionality as
each other. this approach was used by sampson and guttorp [1992] to model
patterns of solar radiation in southwestern british columbia using gaussian
processes.

another interesting example of this warping construction is given in mackay

[1998] where the one-dimensional input variable x is mapped to the two-dimensional
u(x) = (cos(x), sin(x)) to give rise to a periodic random function of x. if we
use the squared exponential kernel in u-space, then

(cid:16)    2 sin2(cid:0) x   x0

2

(cid:1)

(cid:17)

   2

k(x, x0) = exp

as (cos(x)     cos(x0))2 + (sin(x)     sin(x0))2 = 4 sin2( x   x0

2

(4.31)

,

).

warping

periodic random
function

   404   404input, xinput, x      0.5   0.5000.50.50.950.95   404   101input, xoutput, f(x)   = 10   = 3   = 1c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4.2 examples of covariance functions

93

(a)

(b)

figure 4.6: panel (a) shows the chosen length-scale function    (x). panel (b) shows
three samples from the gp prior using gibbs    covariance function eq. (4.32). this
   gure is based on fig. 3.9 in gibbs [1997].

varying length-scale

we have described above how to make an anisotropic covariance function
by scaling di   erent dimensions di   erently. however, we are not free to make
these length-scales    d be functions of x, as this will not in general produce a
valid covariance function. gibbs [1997] derived the covariance function

k(x, x0) =

dy

d=1

(cid:16) 2   d(x)   d(x0)

d(x) +    2
   2

d(x0)

(cid:17)1/2

(cid:16)    dx

d=1

exp

(cid:17)

(xd     x0
d(x) +    2
   2

d)2
d(x0)

,

(4.32)

where each    i(x) is an arbitrary positive function of x. note that k(x, x) = 1
for all x. this covariance function is obtained by considering a grid of n
gaussian basis functions with centres cj and a corresponding length-scale on
input dimension d which varies as a positive function    d(cj). taking the limit
as n         the sum turns into an integral and after some algebra eq. (4.32) is
obtained.

an example of a variable length-scale function and samples from the prior
corresponding to eq. (4.32) are shown in figure 4.6. notice that as the length-
scale gets shorter the sample functions vary more rapidly as one would expect.
the large length-scale regions on either side of the short length-scale region can
be quite strongly correlated. if one tries the converse experiment by creating
a length-scale function    (x) which has a longer length-scale region between
two shorter ones then the behaviour may not be quite what is expected; on
initially transitioning into the long length-scale region the covariance drops o   
quite sharply due to the prefactor in eq. (4.32), before stabilizing to a slower
variation. see gibbs [1997, sec. 3.10.3] for further details. exercises 4.5.4 and
4.5.5 invite you to investigate this further.

paciorek and schervish [2004] have generalized gibbs    construction to obtain
non-stationary versions of arbitrary isotropic covariance functions. let ks be a

0123400.511.5lengthscale l(x)input, x01234   101input, xoutput, f(x)c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

94

covariance functions

covariance function
constant
linear
polynomial
squared exponential

mat  ern
exponential
  -exponential
rational quadratic
neural network

d

expression
  2
0

pd

d=1   2

dxdx0
(x    x0 +   2
0)p
exp(    r2
2   2 )

(cid:16)   
    )  (cid:1)
exp(cid:0)     ( r
sin   1(cid:0)

2     2 )     
(1 + r2
   

2     1  (  )
exp(    r
    )

1

(cid:17)  

(cid:17)

(cid:16)   

2  
    r

k  

2  
    r

2  x>    x0

(1+2  x>    x)(1+2  x0>    x0)

s nd
   

       
       
       
       
       
   

(cid:1)

table 4.1: summary of several commonly-used covariance functions. the covariances
are written either as a function of x and x0, or as a function of r = |x     x0|. two
columns marked    s    and    nd    indicate whether the covariance functions are stationary
and nondegenerate respectively. degenerate covariance functions have    nite rank, see
section 4.3 for more discussion of this issue.

stationary, isotropic covariance function that is valid in every euclidean space
rd for d = 1, 2, . . .. let   (x) be a d    d matrix-valued function which
is positive de   nite for all x, and let   i ,   (xi).
(the set of gibbs       i(x)
functions de   ne a diagonal   (x).) then de   ne the quadratic form

qij = (xi     xj)>((  i +   j)/2)   1(xi     xj).

paciorek and schervish [2004] show that

kns(xi, xj) = 2d/2|  i|1/4|  j|1/4|  i +   j|   1/2ks(pqij),

(4.33)

(4.34)

wiener process

is a valid non-stationary covariance function.

in chapter 2 we described the id75 model in feature space f(x) =
  (x)>w. o   hagan [1978] suggested making w a function of x to allow for
di   erent values of w to be appropriate in di   erent regions. thus he put a
gaussian process prior on w of the form cov(w(x), w(x0)) = w0kw(x, x0) for
some positive de   nite matrix w0, giving rise to a prior on f(x) with covariance
kf (x, x0) =   (x)>w0  (x0)kw(x, x0).

finally we note that the wiener process with covariance function k(x, x0) =
min(x, x0) is a fundamental non-stationary process. see section b.2.1 and texts
such as grimmett and stirzaker [1992, ch. 13] for further details.

4.2.4 making new kernels from old

in the previous sections we have developed many covariance functions some of
which are summarized in table 4.1. in this section we show how to combine or
modify existing covariance functions to make new ones.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

95

sum

product

vertical rescaling

convolution

direct sum
tensor product

additive model

functional anova

4.2 examples of covariance functions

the sum of two kernels is a kernel. proof: consider the random process
f(x) = f1(x) + f2(x), where f1(x) and f2(x) are independent. then k(x, x0) =
k1(x, x0) + k2(x, x0). this construction can be used e.g. to add together kernels
with di   erent characteristic length-scales.

the product of two kernels is a kernel. proof: consider the random process
f(x) = f1(x)f2(x), where f1(x) and f2(x) are independent. then k(x, x0) =
k1(x, x0)k2(x, x0).8 a simple extension of this argument means that kp(x, x0) is
a valid covariance function for p     n.

let a(x) be a given deterministic function and consider g(x) = a(x)f(x)
where f(x) is a random process. then cov(g(x), g(x0)) = a(x)k(x, x0)a(x0).
such a construction can be used to normalize kernels by choosing a(x) =
k   1/2(x, x) (assuming k(x, x) > 0    x), so that

  k(x, x0) =

pk(x, x)pk(x0, x0)

k(x, x0)

.

(4.35)

this ensures that   k(x, x) = 1 for all x.

we can also obtain a new process by convolution (or blurring). consider

an arbitrary    xed kernel h(x, z) and the map g(x) = r h(x, z)f(z) dz. then
clearly cov(g(x), g(x0)) =r h(x, z)k(z, z0)h(x0, z0) dz dz0.

i=1fi(xi),

if k(x1, x0

1) + k2(x2, x0

i=1fi(xi) +p

shirani, 1990] has the form f(x) = c +pd
f(x) = c +pd
form k(x, x0) =pd

2) are covariance functions over di   erent spaces x1
1) and k(x2, x0
and x2, then the direct sum k(x, x0) = k1(x1, x0
2) and the tensor
1)k2(x2, x0
product k(x, x0) = k1(x1, x0
2) are also covariance functions (de   ned
on the product space x1  x2), by virtue of the sum and product constructions.
the direct sum construction can be further generalized. consider a func-
tion f(x), where x is d-dimensional. an additive model [hastie and tib-
i.e. a linear combina-
tion of functions of one variable.
if the individual fi   s are taken to be in-
dependent stochastic processes, then the covariance function of f will have the
form of a direct sum. if we now admit interactions of two variables, so that
ij,j<i fij(xi, xj) and the various fi   s and fij   s are
independent stochastic processes, then the covariance function will have the
j). indeed this pro-
cess can be extended further to provide a functional anova9 decomposition,
ranging from a simple additive model up to full interaction of all d input vari-
ables. (the sum can also be truncated at some stage.) wahba [1990, ch. 10]
and stitson et al. [1999] suggest using tensor products for kernels with inter-
actions so that in the example above kij(xi, xj; x0
j) would have the form
ki(xi; x0
j). note that if d is large then the large number of pairwise
(or higher-order) terms may be problematic; plate [1999] has investigated using
a combination of additive gp models plus a general covariance function that
permits full interactions.

pi   1
j=1 kij(xi, xj; x0

i) +pd

i=1ki(xi, x0

i)kj(xj; x0

i, x0

i, x0

i=2

8if f1 and f2 are gaussian processes then the product f will not in general be a gaussian

process, but there exists a gp with this covariance function.

9anova stands for analysis of variance, a statistical technique that analyzes the interac-

tions between various attributes.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

96

covariance functions

4.3 eigenfunction analysis of kernels

we    rst de   ne eigenvalues and eigenfunctions and discuss mercer   s theorem
which allows us to express the kernel (under certain conditions) in terms of these
quantities. section 4.3.1 gives the analytical solution of the eigenproblem for the
se kernel under a gaussian measure. section 4.3.2 discusses how to compute
approximate eigenfunctions numerically for cases where the exact solution is
not known.

it turns out that gaussian process regression can be viewed as bayesian
id75 with a possibly in   nite number of basis functions, as discussed
in chapter 2. one possible basis set is the eigenfunctions of the covariance
function. a function   (  ) that obeys the integral equation

k(x, x0)  (x) d  (x) =     (x0),

(4.36)

z

eigenvalue,
eigenfunction

mercer   s theorem

is called an eigenfunction of kernel k with eigenvalue    with respect to measure10
  . the two measures of particular interest to us will be (i) lebesgue measure
over a compact subset c of rd, or (ii) when there is a density p(x) so that
d  (x) can be written p(x)dx.

in general there are an in   nite number of eigenfunctions, which we label
  1(x),   2(x), . . . we assume the ordering is chosen such that   1       2     . . ..
the eigenfunctions are orthogonal with respect to    and can be chosen to be

normalized so thatr   i(x)  j(x) d  (x) =   ij where   ij is the kronecker delta.

mercer   s theorem (see, e.g. k  onig, 1986) allows us to express the kernel k

in terms of the eigenvalues and eigenfunctions.
theorem 4.2 (mercer   s theorem). let (x ,   ) be a    nite measure space and
k     l   (x 2,   2) be a kernel such that tk : l2(x ,   )     l2(x ,   ) is positive
de   nite (see eq. (4.2)). let   i     l2(x ,   ) be the normalized eigenfunctions of
tk associated with the eigenvalues   i > 0. then:

1. the eigenvalues {  i}   
2.

i=1 are absolutely summable

   x

k(x, x0) =

  i  i(x)     

i (x0),

(4.37)

holds   2 almost everywhere, where the series converges absolutely and
(cid:3)
uniformly   2 almost everywhere.

i=1

this decomposition is just the in   nite-dimensional analogue of the diagonaliza-
tion of a hermitian matrix. note that the sum may terminate at some value
n     n (i.e. the eigenvalues beyond n are zero), or the sum may be in   nite.
we have the following de   nition [press et al., 1992, p. 794]

10for further explanation of measure see appendix a.7.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4.3 eigenfunction analysis of kernels

97

de   nition 4.1 a degenerate kernel has only a    nite number of non-zero eigen-
(cid:3)
values.

a degenerate kernel is also said to have    nite rank. if a kernel is not degenerate
it is said to be nondegenerate. as an example a n-dimensional id75
model in feature space (see eq. (2.10)) gives rise to a degenerate kernel with at
most n non-zero eigenvalues. (of course if the measure only puts weight on a
   nite number of points n in x-space then the eigendecomposition is simply that
of a n    n matrix, even if the kernel is nondegenerate.)

degenerate,
nondegenerate
kernel

the statement of mercer   s theorem above referred to a    nite measure   .
if we replace this with lebesgue measure and consider a stationary covariance
function, then directly from bochner   s theorem eq. (4.5) we obtain

k(x     x0) =

e2  is  (x   x0) d  (s) =

d  (s).

(4.38)

z

rd

e2  is  x(cid:16)

e2  is  x0(cid:17)   

z

rd

the complex exponentials e2  is  x are the eigenfunctions of a stationary kernel
w.r.t. lebesgue measure. note the similarity to eq. (4.37) except that the
summation has been replaced by an integral.

the rate of decay of the eigenvalues gives important information about the
smoothness of the kernel. for example ritter et al. [1995] showed that in 1-d
with    uniform on [0, 1], processes which are r-times mean-square di   erentiable
have   i     i   (2r+2) asymptotically. this makes sense as    rougher    processes
have more power at high frequencies, and so their eigenvalue spectrum decays
more slowly. the same phenomenon can be read o    from the power spectrum
of the mat  ern class as given in eq. (4.15).

hawkins [1989] gives the exact eigenvalue spectrum for the ou process on
[0, 1]. widom [1963; 1964] gives an asymptotic analysis of the eigenvalues of
stationary kernels taking into account the e   ect of the density d  (x) = p(x)dx;
bach and jordan [2002, table 3] use these results to show the e   ect of varying
p(x) for the se kernel. an exact eigenanalysis of the se kernel under the
gaussian density is given in the next section.

4.3.1 an analytic example

   

for the case that p(x) is a gaussian and for the squared-exponential kernel
k(x, x0) = exp(   (x   x0)2/2   2), there are analytic results for the eigenvalues and
eigenfunctions, as given by zhu et al. [1998, sec. 4]. putting p(x) = n (x|0,   2)
we    nd that the eigenvalues   k and eigenfunctions   k (for convenience let k =
0, 1, . . . ) are given by

r2a

  k =

  k(x) = exp(cid:0)    (c     a)x2(cid:1)hk

bk,

a

(cid:0)   

2cx(cid:1),

(4.39)

(4.40)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

98

covariance functions

figure 4.7: the    rst 3 eigenfunctions of the squared exponential kernel w.r.t. a
gaussian density. the value of k = 0, 1, 2 is equal to the number of zero-crossings
of the function. the dashed line is proportional to the density p(x).

c =p

where hk(x) = (   1)k exp(x2) dk
(see gradshteyn and ryzhik [1980, sec. 8.95]), a   1 = 4  2, b   1 = 2   2 and

dxk exp(   x2) is the kth order hermite polynomial

a2 + 2ab,

a = a + b + c,

b = b/a.

(4.41)

hints on the proof of this result are given in exercise 4.5.9. a plot of the    rst
three eigenfunctions for a = 1 and b = 3 is shown in figure 4.7.

the result for the eigenvalues and eigenfunctions is readily generalized to
the multivariate case when the kernel and gaussian density are products of
the univariate expressions, as the eigenfunctions and eigenvalues will simply
be products too. for the case that a and b are equal on all d dimensions,
the degeneracy of the eigenvalue ( 2a

(cid:1) which is o(kd   1). as
(cid:1)   th eigenvalue has a value given by

(cid:1) we see that the(cid:0)k+d

a )d/2bk is(cid:0)k+d   1

(cid:1) =(cid:0)k+d

(cid:0)j+d   1

pk

d   1

a )d/2bk, and this can be used to determine the rate of decay of the spectrum.
( 2a

d   1

j=0

d

d

4.3.2 numerical approximation of eigenfunctions

the standard numerical method for approximating the eigenfunctions and eigen-
values of eq. (4.36) is to use a numerical routine to approximate the integral
(see, e.g. baker [1977, ch. 3]). for example letting d  (x) = p(x)dx in eq. (4.36)
one could use the approximation

  i  i(x0) =

k(x, x0)p(x)  i(x) dx     1
n

k(xl, x0)  i(xl),

(4.42)

z

nx

l=1

   202   0.200.20.4c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4.4 kernels for non-vectorial inputs

99

where the xl   s are sampled from p(x). plugging in x0 = xl for l = 1, . . . , n into
eq. (4.42) we obtain the matrix eigenproblem

kui =   mat

i ui,

(4.43)

i

   

n(ui)j where the

where k is the n   n gram matrix with entries kij = k(xi, xj),   mat
is the ith
i ui = 1). we have   i(xj)        
matrix eigenvalue and ui is the corresponding eigenvector (normalized so that
u>
n factor arises from the
di   ering id172s of the eigenvector and eigenfunction. thus 1
is
an obvious estimator for   i for i = 1, . . . , n. for    xed n one would expect that
the larger eigenvalues would be better estimated than the smaller ones. the
theory of the numerical solution of eigenvalue problems shows that for a    xed i,
will converge to   i in the limit that n         [baker, 1977, theorem 3.4].
1
n   mat
it is also possible to study the convergence further; for example it is quite
easy using the properties of principal components analysis (pca) in feature
space to show that for any l, 1     l     n, en[ 1
i=1  i and
en[ 1
i=l+1  i, where en denotes expectation with respect to
samples of size n drawn from p(x). for further details see shawe-taylor and
williams [2003].

]     pl

]    pn

pn

i=l+1  mat

pl

i=1  mat

n   mat

n

n

i

i

i

i

nystr  om method

kernel pca

the nystr  om method for approximating the ith eigenfunction (see baker

[1977] and press et al. [1992, section 18.1]) is given by

  i(x0)    

k(x0)>ui,

(4.44)

   

n
  mat

i

where k(x0)> = (k(x1, x0), . . . , k(xn, x0)), which is obtained from eq. (4.42) by
dividing both sides by   i. equation 4.44 extends the approximation   i(xj)    
   
n(ui)j from the sample points x1, . . . , xn to all x.

there is an interesting relationship between the kernel pca method of
sch  olkopf et al. [1998] and the eigenfunction expansion discussed above. the
eigenfunction expansion has (at least potentially) an in   nite number of non-
zero eigenvalues. in contrast, the kernel pca algorithm operates on the n    n
matrix k and yields n eigenvalues and eigenvectors. eq. (4.42) clari   es the
relationship between the two. however, note that eq. (4.44) is identical (up to
scaling factors) to sch  olkopf et al. [1998, eq. 4.1] which describes the projection
of a new point x0 onto the ith eigenvector in the kernel pca feature space.

4.4 kernels for non-vectorial inputs

so far in this chapter we have assumed that the input x is a vector, measuring
the values of a number of attributes (or features). however, for some learning
problems the inputs are not vectors, but structured objects such as strings,
trees or general graphs. for example, we may have a biological problem where
we want to classify proteins (represented as strings of amino acid symbols).11

11proteins are initially made up of 20 di   erent amino acids, of which a few may later be

modi   ed bringing the total number up to 26 or 30.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

100

covariance functions

or our input may be parse-trees derived from a linguistic analysis. or we may
wish to represent chemical compounds as labelled graphs, with vertices denoting
atoms and edges denoting bonds.

to follow the discriminative approach we need to extract some features from
the input objects and build a predictor using these features. (for a classi   cation
problem, the alternative generative approach would construct class-conditional
models over the objects themselves.) below we describe two approaches to
this feature extraction problem and the e   cient computation of kernels from
them: in section 4.4.1 we cover string kernels, and in section 4.4.2 we describe
fisher kernels. there exist other proposals for constructing kernels for strings,
for example watkins [2000] describes the use of pair id48
(id48s that generate output symbols for two strings conditional on the hidden
state) for this purpose.

4.4.1 string kernels
we start by de   ning some notation for strings. let a be a    nite alphabet of
characters. the concatenation of strings x and y is written xy and |x| denotes
the length of string x. the string s is a substring of x if we can write x = usv
for some (possibly empty) u, s and v.

let   s(x) denote the number of times that substring s appears in string x.

then we de   ne the kernel between two strings x and x0 as

k(x, x0) = x

s   a   

ws  s(x)  s(x0),

(4.45)

where ws is a non-negative weight for substring s. for example, we could set
ws =   |s|, where 0 <    < 1, so that shorter substrings get more weight than
longer ones.

a number of interesting special cases are contained in the de   nition 4.45:
    setting ws = 0 for |s| > 1 gives the bag-of-characters kernel. this takes
the feature vector for a string x to be the number of times that each
character in a appears in x.

    in text analysis we may wish to consider the frequencies of word occur-
rence. if we require s to be bordered by whitespace then a    bag-of-words   
representation is obtained. although this is a very simple model of text
(which ignores word order) it can be surprisingly e   ective for document
classi   cation and retrieval tasks, see e.g. hand et al. [2001, sec. 14.3].
the weights can be set di   erently for di   erent words, e.g. using the    term
frequency inverse document frequency    (tf-idf) weighting scheme de-
veloped in the information retrieval area [salton and buckley, 1988].

    if we only consider substrings of length k, then we obtain the k-spectrum

kernel [leslie et al., 2003].

bag-of-characters

bag-of-words

k-spectrum kernel

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

4.4 kernels for non-vectorial inputs

101

importantly, there are e   cient methods using su   x trees that can compute
a string kernel k(x, x0) in time linear in |x| +|x0| (with some restrictions on the
weights {ws}) [leslie et al., 2003, vishwanathan and smola, 2003].

work on string kernels was started by watkins [1999] and haussler [1999].
there are many further developments of the methods we have described above;
for example lodhi et al. [2001] go beyond substrings to consider subsequences
of x which are not necessarily contiguous, and leslie et al. [2003] describe
mismatch string kernels which allow substrings s and s0 of x and x0 respectively
to match if there are at most m mismatches between them. we expect further
developments in this area, tailoring (or engineering) the string kernels to have
properties that make sense in a particular domain.

the idea of string kernels, where we consider matches of substrings, can
easily be extended to trees, e.g. by looking at matches of subtrees [collins and
du   y, 2002].

leslie et al. [2003] have applied string kernels to the classi   cation of protein
domains into scop12 superfamilies. the results obtained were signi   cantly
better than methods based on either psi-blast13 searches or a generative
hidden markov model classi   er. similar results were obtained by jaakkola et al.
[2000] using a fisher kernel (described in the next section). saunders et al.
[2003] have also described the use of string kernels on the problem of classifying
natural language newswire stories from the reuters-2157814 database into ten
classes.

4.4.2 fisher kernels

score vector

as explained above, our problem is that the input x is a structured object of
arbitrary size e.g. a string, and we wish to extract features from it. the fisher
kernel (introduced by jaakkola et al., 2000) does this by taking a generative
model p(x|  ), where    is a vector of parameters, and computing the feature
vector     (x) =       log p(x|  ).     (x) is sometimes called the score vector.

in string x. then a markov model gives p(x|  ) = p(x1|  )q|x|   1

take, for example, a markov model for strings. let xk be the kth symbol
i=1 p(xi+1|xi, a),
where    = (  , a). here (  )j gives the id203 that x1 will be the jth symbol
in the alphabet a, and a is a |a|    |a| stochastic matrix, with ajk giving the
id203 that p(xi+1 = k|xi = j). given such a model it is straightforward
to compute the score vector for a given x.

it is also possible to consider other generative models p(x|  ). for example
we might try a kth-order markov model where xi is predicted by the preceding
k symbols. see leslie et al. [2003] and saunders et al. [2003] for an interesting
discussion of the similarities of the features used in the k-spectrum kernel and
the score vector derived from an order k     1 markov model; see also exercise
12structural classi   cation of proteins database, http://scop.mrc-lmb.cam.ac.uk/scop/.
13position-speci   c iterative basic local alignment search tool, see

http://www.ncbi.nlm.nih.gov/education/blastinfo/psi1.html.

14http://www.daviddlewis.com/resources/testcollections/reuters21578/.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

102

covariance functions

fisher information
matrix

fisher kernel

top kernel

4.5.12. another interesting choice is to use a hidden markov model (id48) as
the generative model, as discussed by jaakkola et al. [2000]. see also exercise
4.5.11 for a linear kernel derived from an isotropic gaussian model for x     rd.
we de   ne a kernel k(x, x0) based on the score vectors for x and x0. one

simple choice is to set

k(x, x0) =   

>
   (x)m   1    (x0),

(4.46)

where m is a strictly positive de   nite matrix. alternatively we might use the
squared exponential kernel k(x, x0) = exp(     |    (x)       (x0)|2) for some    > 0.
the structure of p(x|  ) as    varies has been studied extensively in informa-
tion geometry (see, e.g. amari, 1985). it can be shown that the manifold of
log p(x|  ) is riemannian with a metric tensor which is the inverse of the fisher
information matrix f , where

f = ex[    (x)  

>
   (x)].

(4.47)

setting m = f in eq. (4.46) gives the fisher kernel . if f is di   cult to compute
then one might resort to setting m = i. the advantage of using the fisher
information matrix is that it makes arc length on the manifold invariant to
reparameterizations of   .

the fisher kernel uses a class-independent model p(x|  ). tsuda et al.
[2002] have developed the tangent of posterior odds (top) kernel based on
     (log p(y = +1|x,   )   log p(y =    1|x,   )), which makes use of class-conditional
distributions for the c+ and c    classes.

4.5 exercises

1. the ou process with covariance function k(x     x0) = exp(   |x     x0|/   )
is the unique stationary    rst-order markovian gaussian process (see ap-
pendix b for further details). consider training inputs x1 < x2 . . . <
xn   1 < xn on r with corresponding function values f = (f(x1), . . . , f(xn))>.
let xl denote the nearest training input to the left of a test point x   , and
similarly let xu denote the nearest training input to the right of x   . then
the markovian property means that p(f(x   )|f) = p(f(x   )|f(xl), f(xu)).
demonstrate this by choosing some x-points on the line and computing
the predictive distribution p(f(x   )|f) using eq. (2.19), and observing that
non-zero contributions only arise from xl and xu. note that this only
occurs in the noise-free case; if one allows the training points to be cor-
rupted by noise (equations 2.23 and 2.24) then all points will contribute
in general.

2. computer exercise: write code to draw samples from the neural network
covariance function, eq. (4.29) in 1-d and 2-d. consider the cases when
var(u0) is either 0 or non-zero. explain the form of the plots obtained
when var(u0) = 0.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

103

4.5 exercises

3. consider the random process f(x) = erf(u0 +pd

i=1ujxj), where u    
n (0,   ). show that this non-linear transform of a process with an inho-
mogeneous linear covariance function has the same covariance function as
the erf neural network. however, note that this process is not a gaussian
process. draw samples from the given process and compare them to your
results from exercise 4.5.2.

4. derive gibbs    non-stationary covariance function, eq. (4.32).

5. computer exercise: write code to draw samples from gibbs    non-stationary
covariance function eq. (4.32) in 1-d and 2-d. investigate various forms of
length-scale function    (x).

6. show that the se process is in   nitely ms di   erentiable and that the ou

process is not ms di   erentiable.

7. prove that the eigenfunctions of a symmetric kernel are orthogonal w.r.t. the

measure   .

8. let   k(x, x0) = p1/2(x)k(x, x0)p1/2(x0), and assume p(x) > 0 for all x.
    i(x0) has the same

show that the eigenproblem r   k(x, x0)     i(x)dx =     i
eigenvalues asr k(x, x0)p(x)  i(x)dx =   i  i(x0), and that the eigenfunc-

tions are related by     i(x) = p1/2(x)  i(x). also give the matrix version
of this problem (hint: introduce a diagonal matrix p to take the r  ole of
p(x)). the signi   cance of this connection is that it can be easier to    nd
eigenvalues of symmetric matrices than general matrices.

9. apply the construction in the previous exercise to the eigenproblem for
the se kernel and gaussian density given in section 4.3.1, with p(x) =
exp(   ax2) exp(   b(x   x0)2) exp(   a(x0)2). using equation 7.374.8 in grad-
shteyn and ryzhik [1980]:

p2a/   exp(   2ax2). thus consider the modi   ed kernel given by   k(x, x0) =
z    
(cid:17)

exp(cid:0)    (x     y)2(cid:1)hn(  x) dx =

  (1       2)n/2hn

(cid:16)

   

  y

,

(1       2)1/2

      

   
verify that     k(x) = exp(   cx2)hk(
and 4.40.

2cx), and thus con   rm equations 4.39

10. computer exercise: the analytic form of the eigenvalues and eigenfunc-
tions for the se kernel and gaussian density are given in section 4.3.1.
compare these exact results to those obtained by the nystr  om approxi-
mation for various values of n and choice of samples.

11. let x     n (  ,   2i). consider the fisher kernel derived from this model
with respect to variation of    (i.e. regard   2 as a constant). show that:

(cid:12)(cid:12)(cid:12)(cid:12)  =0

    log p(x|  )

     

=

x
  2

and that f =      2i. thus the fisher kernel for this model with    = 0 is
the linear kernel k(x, x0) = 1

  2 x    x0.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

104

covariance functions

12. consider a k     1 order markov model for strings on a    nite alphabet. let
this model have parameters   t|s1,...,sk   1 denoting the id203 p(xi =
t|xi   1 = s1, . . . , xk   1 = sk   1). of course as these are probabilities they
t0   t0|s1,...,sk   1 = 1. enforcing this constraint

obey the constraint that p

can be achieved automatically by setting

  t|s1,...,sk   1 =

p

  t,s1,...,sk   1
t0   t0,s1,...,sk   1

,

where the   t,s1,...,sk   1 parameters are now independent, as suggested in
[jaakkola et al., 2000]. the current parameter values are denoted   0.
let the current values of   0
t0,s1,...,sk   1 = 1,
i.e. that   0

show that log p(x|  ) =p nt,s1,...,sk   1 log   t|s1,...,sk   1 where nt,s1,...,sk   1 is

t,s1,...,sk   1 be set so that p

t,s1,...,sk   1 =   0

the number of instances of the substring sk   1 . . . s1t in x. thus, following
leslie et al. [2003], show that

t|s1,...,sk   1

t0  0

.

(cid:12)(cid:12)(cid:12)(cid:12)  =  0

    log p(x|  )
     t,s1,...,sk   1

=

nt,s1,...,sk   1
  0
t|s1,...,sk   1

    ns1,...,sk   1,

t|s1,...,sk   1

where ns1,...,sk   1 is the number of instances of the substring sk   1 . . . s1 in
x. as ns1,...,sk   1  0
is the expected number of occurrences of the
string sk   1 . . . s1t given the count ns1,...,sk   1, the fisher score captures the
degree to which this string is over- or under-represented relative to the
model. for the k-spectrum kernel the relevant feature is   sk   1...,s1,t(x) =
nt,s1,...,sk   1.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

chapter 5

model selection and
adaptation of
hyperparameters

in chapters 2 and 3 we have seen how to do regression and classi   cation using
a gaussian process with a given    xed covariance function. however, in many
practical applications, it may not be easy to specify all aspects of the covari-
ance function with con   dence. while some properties such as stationarity of
the covariance function may be easy to determine from the context, we typically
have only rather vague information about other properties, such as the value
of free (hyper-) parameters, e.g. length-scales. in chapter 4 several examples
of covariance functions were presented, many of which have large numbers of
parameters.
in addition, the exact form and possible free parameters of the
likelihood function may also not be known in advance. thus in order to turn
gaussian processes into powerful practical tools it is essential to develop meth-
ods that address the model selection problem. we interpret the model selection
problem rather broadly, to include all aspects of the model including the dis-
crete choice of the functional form for the covariance function as well as values
for any hyperparameters.

in section 5.1 we outline the model selection problem. in the following sec-
tions di   erent methodologies are presented: in section 5.2 bayesian principles
are covered, and in section 5.3 cross-validation is discussed, in particular the
leave-one-out estimator. in the remaining two sections the di   erent methodolo-
gies are applied speci   cally to learning in gp models, for regression in section
5.4 and classi   cation in section 5.5.

model selection

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

106

model selection and adaptation of hyperparameters

5.1 the model selection problem

in order for a model to be a practical tool in an application, one needs to make
decisions about the details of its speci   cation. some properties may be easy to
specify, while we typically have only vague information available about other
aspects. we use the term model selection to cover both discrete choices and the
setting of continuous (hyper-) parameters of the covariance functions. in fact,
model selection can help both to re   ne the predictions of the model, and give
a valuable interpretation to the user about the properties of the data, e.g. that
a non-stationary covariance function may be preferred over a stationary one.

a multitude of possible families of covariance functions exists, including
squared exponential, polynomial, neural network, etc., see section 4.2 for an
overview. each of these families typically have a number of free hyperparameters
whose values also need to be determined. choosing a covariance function for a
particular application thus comprises both setting of hyperparameters within a
family, and comparing across di   erent families. both of these problems will be
treated by the same methods, so there is no need to distinguish between them,
and we will use the term    model selection    to cover both meanings. we will
refer to the selection of a covariance function and its parameters as training of
a gaussian process.1 in the following paragraphs we give example choices of
parameterizations of distance measures for stationary covariance functions.

enable interpretation

hyperparameters

training

covariance functions such as the squared exponential can be parameterized

in terms of hyperparameters. for example

f exp(cid:0)    1

(xp     xq)>m(xp     xq)(cid:1) +   2

k(xp, xq) =   2

(5.1)
where    = ({m},   2
n)> is a vector containing all the hyperparameters,2 and
{m} denotes the parameters in the symmetric matrix m. possible choices for
the matrix m include

f ,   2

n  pq,

2

characteristic
length-scale
automatic relevance
determination

m1 =       2i,

m2 = diag(   )   2,

m3 =     > + diag(   )   2,

(5.2)
where     is a vector of positive values, and    is a d    k matrix, k < d. the
properties of functions with these covariance functions depend on the values of
the hyperparameters. for many covariance functions it is easy to interpret the
meaning of the hyperparameters, which is of great importance when trying to
understand your data. for the squared exponential covariance function eq. (5.1)
with distance measure m2 from eq. (5.2), the    1, . . . ,    d hyperparameters play
the r  ole of characteristic length-scales; loosely speaking, how far do you need
to move (along a particular axis) in input space for the function values to be-
come uncorrelated. such a covariance function implements automatic relevance
determination (ard) [neal, 1996], since the inverse of the length-scale deter-
mines how relevant an input is: if the length-scale has a very large value, the

1this contrasts the use of the word in the id166 literature, where    training    usually refers

to    nding the support vectors for a    xed kernel.

2sometimes the noise level parameter,   2

n is not considered a hyperparameter; however it
plays an analogous role and is treated in the same way, so we simply consider it a hyperpa-
rameter.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.1 the model selection problem

107

(a)

(b)

(c)

figure 5.1: functions with two dimensional input drawn at random from noise free
squared exponential covariance function gaussian processes, corresponding to the
three di   erent distance measures in eq. (5.2) respectively. the parameters were: (a)
    = 1, (b)     = (1, 3)>, and (c)    = (1,   1)>,     = (6, 6)>. in panel (a) the two inputs
are equally important, while in (b) the function varies less rapidly as a function of x2
than x1. in (c) the    column gives the direction of most rapid variation .

covariance will become almost independent of that input, e   ectively removing
it from the id136. ard has been used successfully for removing irrelevant
input by several authors, e.g. williams and rasmussen [1996]. we call the pa-
rameterization of m3 in eq. (5.2) the factor analysis distance due to the analogy
with the (unsupervised) factor analysis model which seeks to explain the data
through a low rank plus diagonal decomposition. for high dimensional datasets
the k columns of the    matrix could identify a few directions in the input space
with specially high    relevance   , and their lengths give the inverse characteristic
length-scale for those directions.

in figure 5.1 we show functions drawn at random from squared exponential
covariance function gaussian processes, for di   erent choices of m.
in panel
(a) we get an isotropic behaviour. in panel (b) the characteristic length-scale
is di   erent along the two input axes; the function varies rapidly as a function
of x1, but less rapidly as a function of x2. in panel (c) the direction of most
rapid variation is perpendicular to the direction (1, 1). as this    gure illustrates,

factor analysis distance

   202   202   2   1012input x1input x2output y   202   202   2   1012input x1input x2output y   202   202   2   1012input x1input x2output yc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

108

model selection and adaptation of hyperparameters

there is plenty of scope for variation even inside a single family of covariance
functions. our task is, based on a set of training data, to make id136s about
the form and parameters of the covariance function, or equivalently, about the
relationships in the data.

it should be clear from the above example that model selection is essentially
open ended. even for the squared exponential covariance function, there is a
huge variety of possible distance measures. however, this should not be a cause
for despair, rather seen as a possibility to learn. it requires, however, a sys-
tematic and practical approach to model selection. in a nutshell we need to be
able to compare two (or more) methods di   ering in values of particular param-
eters, or the shape of the covariance function, or compare a gaussian process
model to any other kind of model. although there are endless variations in the
suggestions for model selection in the literature three general principles cover
most: (1) compute the id203 of the model given the data, (2) estimate
the generalization error and (3) bound the generalization error. we use the
term generalization error to mean the average error on unseen test examples
(from the same distribution as the training cases). note that the training error
is usually a poor proxy for the generalization error, since the model may    t
the noise in the training set (over-   t), leading to low training error but poor
generalization performance.

in the next section we describe the bayesian view on model selection, which
involves the computation of the id203 of the model given the data, based
on the marginal likelihood.
in section 5.3 we cover cross-validation, which
estimates the generalization performance. these two paradigms are applied
to gaussian process models in the remainder of this chapter. the probably
approximately correct (pac) framework is an example of a bound on the gen-
eralization error, and is covered in section 7.4.2.

5.2 bayesian model selection

in this section we give a short outline description of the main ideas in bayesian
model selection. the discussion will be general, but focusses on issues which will
be relevant for the speci   c treatment of gaussian process models for regression
in section 5.4 and classi   cation in section 5.5.

it is common to use a hierarchical speci   cation of models. at the lowest level
are the parameters, w. for example, the parameters could be the parameters
in a linear model, or the weights in a neural network model. at the second level
are hyperparameters    which control the distribution of the parameters at the
bottom level. for example the    weight decay    term in a neural network, or the
   ridge    term in ridge regression are hyperparameters. at the top level we may
have a (discrete) set of possible model structures, hi, under consideration.

we will    rst give a    mechanistic    description of the computations needed
for bayesian id136, and continue with a discussion providing the intuition
about what is going on. id136 takes place one level at a time, by applying

id187

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.2 bayesian model selection

the rules of id203 theory, see e.g. mackay [1992b] for this framework and
mackay [1992a] for the context of neural networks. at the bottom level, the
posterior over the parameters is given by bayes    rule

p(w|y, x,   ,hi) = p(y|x, w,hi)p(w|  ,hi)

p(y|x,   ,hi)

,

(5.3)

109

level 1 id136

where p(y|x, w,hi) is the likelihood and p(w|  ,hi) is the parameter prior.
the prior encodes as a id203 distribution our knowledge about the pa-
rameters prior to seeing the data.
if we have only vague prior information
about the parameters, then the prior distribution is chosen to be broad to
re   ect this. the posterior combines the information from the prior and the
data (through the likelihood). the normalizing constant in the denominator of
eq. (5.3) p(y|x,   ,hi) is independent of the parameters, and called the marginal
likelihood (or evidence), and is given by

p(y|x,   ,hi) =

p(y|x, w,hi)p(w|  ,hi) dw.

(5.4)

at the next level, we analogously express the posterior over the hyperparam-
eters, where the marginal likelihood from the    rst level plays the r  ole of the
likelihood

p(  |y, x,hi) = p(y|x,   ,hi)p(  |hi)

(5.5)
where p(  |hi) is the hyper-prior (the prior for the hyperparameters). the
normalizing constant is given by

p(y|x,hi)

,

z

z

p(y|x,hi) =

p(y|x,   ,hi)p(  |hi)d  .

(5.6)

level 2 id136

at the top level, we compute the posterior for the model

level 3 id136

,

(5.7)

p(y|x)

where p(y|x) = p

p(hi|y, x) = p(y|x,hi)p(hi)
i p(y|x,hi)p(hi). we note that the implementation of
bayesian id136 calls for the evaluation of several integrals. depending on the
details of the models, these integrals may or may not be analytically tractable
and in general one may have to resort to analytical approximations or markov
chain monte carlo (mcmc) methods. in practice, especially the evaluation
of the integral in eq. (5.6) may be di   cult, and as an approximation one may
shy away from using the hyperparameter posterior in eq. (5.5), and instead
maximize the marginal likelihood in eq. (5.4) w.r.t. the hyperparameters,   .
this approximation is known as type ii maximum likelihood (ml-ii). of course,
one should be careful with such an optimization step, since it opens up the
possibility of over   tting, especially if there are many hyperparameters. the
integral in eq. (5.6) can then be approximated using a local expansion around
the maximum (the laplace approximation). this approximation will be good
if the posterior for    is fairly well peaked, which is more often the case for the

ml-ii

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

110

model selection and adaptation of hyperparameters

figure 5.2: the marginal likelihood p(y|x,hi) is the id203 of the data, given
the model. the number of data points n and the inputs x are    xed, and not shown.
the horizontal axis is an idealized representation of all possible vectors of targets y.
the marginal likelihood for models of three di   erent complexities are shown. note,
that since the marginal likelihood is a id203 distribution, it must normalize
to unity. for a particular dataset indicated by y and a dotted line, the marginal
likelihood prefers a model of intermediate complexity over too simple or too complex
alternatives.

hyperparameters than for the parameters themselves, see mackay [1999] for an
illuminating discussion. the prior over models hi in eq. (5.7) is often taken to
be    at, so that a priori we do not favour one model over another. in this case,
the id203 for the model is proportional to the expression from eq. (5.6).
it is primarily the marginal likelihood from eq. (5.4) involving the integral
over the parameter space which distinguishes the bayesian scheme of id136
from other schemes based on optimization.
it is a property of the marginal
likelihood that it automatically incorporates a trade-o    between model    t and
model complexity. this is the reason why the marginal likelihood is valuable
in solving the model selection problem.

in figure 5.2 we show a schematic of the behaviour of the marginal likelihood
for three di   erent model complexities. let the number of data points n and
the inputs x be    xed; the horizontal axis is an idealized representation of all
possible vectors of targets y, and the vertical axis plots the marginal likelihood
p(y|x,hi). a simple model can only account for a limited range of possible sets
of target values, but since the marginal likelihood is a id203 distribution
over y it must normalize to unity, and therefore the data sets which the model
does account for have a large value of the marginal likelihood. conversely for
a complex model:
it is capable of accounting for a wider range of data sets,
and consequently the marginal likelihood doesn   t attain such large values as
for the simple model. for example, the simple model could be a linear model,
and the complex model a large neural network. the    gure illustrates why the
marginal likelihood doesn   t simply favour the models that    t the training data
the best. this e   ect is called occam   s razor after william of occam 1285-1349,
whose principle:    plurality should not be assumed without necessity    he used
to encourage simplicity in explanations. see also rasmussen and ghahramani
[2001] for an investigation into occam   s razor in statistical models.

occam   s razor

ymarginal likelihood, p(y|x,hi)all possible data setssimpleintermediatecomplexc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.3 cross-validation

notice that the trade-o    between data-   t and model complexity is automatic;
there is no need to set a parameter externally to    x the trade-o   . do not confuse
the automatic occam   s razor principle with the use of priors in the bayesian
method. even if the priors are       at    over complexity, the marginal likelihood
will still tend to favour the least complex model able to explain the data. thus,
a model complexity which is well suited to the data can be selected using the
marginal likelihood.

in the preceding paragraphs we have thought of the speci   cation of a model
as the model structure as well as the parameters of the priors, etc.
if it is
unclear how to set some of the parameters of the prior, one can treat these as
hyperparameters, and do model selection to determine how to set them. at
the same time it should be emphasized that the priors correspond to (proba-
bilistic) assumptions about the data. if the priors are grossly at odds with the
distribution of the data, id136 will still take place under the assumptions
encoded by the prior, see the step-function example in section 5.4.3. to avoid
this situation, one should be careful not to employ priors which are too narrow,
ruling out reasonable explanations of the data.3

5.3 cross-validation

in this section we consider how to use methods of cross-validation (cv) for
model selection. the basic idea is to split the training set into two disjoint sets,
one which is actually used for training, and the other, the validation set, which
is used to monitor performance. the performance on the validation set is used
as a proxy for the generalization error and model selection is carried out using
this measure.

in practice a drawback of hold-out method is that only a fraction of the
full data set can be used for training, and that if the validation set it small,
the performance estimate obtained may have large variance. to minimize these
problems, cv is almost always used in the k-fold cross-validation setting: the
data is split into k disjoint, equally sized subsets; validation is done on a single
subset and training is done using the union of the remaining k     1 subsets, the
entire procedure being repeated k times, each time with a di   erent subset for
validation. thus, a large fraction of the data can be used for training, and all
cases appear as validation cases. the price is that k models must be trained
instead of one. typical values for k are in the range 3 to 10.

an extreme case of k-fold cross-validation is obtained for k = n, the number
of training cases, also known as leave-one-out cross-validation (loo-cv). of-
ten the computational cost of loo-cv (   training    n models) is prohibitive, but
in certain cases, such as gaussian process regression, there are computational
shortcuts.

3this is known as cromwell   s dictum [lindley, 1985] after oliver cromwell who on august
5th, 1650 wrote to the synod of the church of scotland:    i beseech you, in the bowels of
christ, consider it possible that you are mistaken.   

111

automatic trade-o   

cross-validation

k-fold cross-validation

leave-one-out
cross-validation
(loo-cv)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

112

model selection and adaptation of hyperparameters

other id168s

model parameters

cross-validation can be used with any id168. although the squared
error loss is by far the most common for regression, there is no reason not to
allow other id168s. for probabilistic models such as gaussian processes
it is natural to consider also cross-validation using the negative log probabil-
ity loss. craven and wahba [1979] describe a variant of cross-validation using
squared error known as generalized cross-validation which gives di   erent weight-
ings to di   erent datapoints so as to achieve certain invariance properites. see
wahba [1990, sec. 4.3] for further details.

5.4 model selection for gp regression

we apply bayesian id136 in section 5.4.1 and cross-validation in section 5.4.2
to gaussian process regression with gaussian noise. we conclude in section
5.4.3 with some more detailed examples of how one can use the model selection
principles to tailor covariance functions.

5.4.1 marginal likelihood

bayesian principles provide a persuasive and consistent framework for id136.
unfortunately, for most interesting models for machine learning, the required
computations (integrals over parameter space) are analytically intractable, and
good approximations are not easily derived. gaussian process regression mod-
els with gaussian noise are a rare exception: integrals over the parameters are
analytically tractable and at the same time the models are very    exible. in this
section we    rst apply the general bayesian id136 principles from section
5.2 to the speci   c gaussian process model, in the simpli   ed form where hy-
perparameters are optimized over. we derive the expressions for the marginal
likelihood and interpret these.

since a gaussian process model is a non-parametric model, it may not be
immediately obvious what the parameters of the model are. generally, one
may regard the noise-free latent function values at the training inputs f as the
parameters. the more training cases there are, the more parameters. using
the weight-space view, developed in section 2.1, one may equivalently think
of the parameters as being the weights of the linear model which uses the
basis-functions   , which can be chosen as the eigenfunctions of the covariance
function. of course, we have seen that this view is inconvenient for nondegen-
erate covariance functions, since these would then have an in   nite number of
weights.

we proceed by applying eq. (5.3) and eq. (5.4) for the 1st level of id136   
which we    nd that we have already done back in chapter 2! the predictive dis-
tribution from eq. (5.3) is given for the weight-space view in eq. (2.11) and
eq. (2.12) and equivalently for the function-space view in eq. (2.22). the
marginal likelihood (or evidence) from eq. (5.4) was computed in eq. (2.30),

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.4 model selection for gp regression

113

(a)

(b)

figure 5.3: panel (a) shows a decomposition of the log marginal likelihood into
its constituents: data-   t and complexity penalty, as a function of the characteristic
length-scale. the training data is drawn from a gaussian process with se covariance
function and parameters (   ,   f ,   n) = (1, 1, 0.1), the same as in figure 2.5, and we are
   tting only the length-scale parameter     (the two other parameters have been set in
accordance with the generating process). panel (b) shows the log marginal likelihood
as a function of the characteristic length-scale for di   erent sizes of training sets. also
shown, are the 95% con   dence intervals for the posterior length-scales.

and we re-state the result here

log p(y|x,   ) =    1
2

y>k   1

y y     1
2

log |ky|     n
2

log 2  ,

(5.8)

where ky = kf +   2
ni is the covariance matrix for the noisy targets y (and kf
is the covariance matrix for the noise-free latent f), and we now explicitly write
the marginal likelihood conditioned on the hyperparameters (the parameters of
the covariance function)   . from this perspective it becomes clear why we call
eq. (5.8) the log marginal likelihood, since it is obtained through marginaliza-
tion over the latent function. otherwise, if one thinks entirely in terms of the
function-space view, the term    marginal    may appear a bit mysterious, and
similarly the    hyper    from the    parameters of the covariance function.4

the three terms of the marginal likelihood in eq. (5.8) have readily inter-
pretable r  oles: the only term involving the observed targets is the data-   t
   y>k   1
y y/2; log |ky|/2 is the complexity penalty depending only on the co-
variance function and the inputs and n log(2  )/2 is a id172 constant.
in figure 5.3(a) we illustrate this breakdown of the log marginal likelihood.
the data-   t decreases monotonically with the length-scale, since the model be-
comes less and less    exible. the negative complexity penalty increases with the
length-scale, because the model gets less complex with growing length-scale.
the marginal likelihood itself peaks at a value close to 1. for length-scales
somewhat longer than 1, the marginal likelihood decreases rapidly (note the

4another reason that we like to stick to the term    marginal likelihood    is that it is the
likelihood of a non-parametric model, i.e. a model which requires access to all the training
data when making predictions; this contrasts the situation for a parametric model, which
   absorbs    the information from the training data into its (posterior) parameter (distribution).
this di   erence makes the two    likelihoods    behave quite di   erently as a function of   .

marginal likelihood

interpretation

100   100   80   60   40   2002040log id203characteristic lengthscaleminus complexity penaltydata fitmarginal likelihood100   100   80   60   40   20020characteristic lengthscalelog marginal likelihood95% conf int  82155c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

114

model selection and adaptation of hyperparameters

figure 5.4: contour plot showing the log marginal likelihood as a function of the
characteristic length-scale and the noise level, for the same data as in figure 2.5 and
figure 5.3. the signal variance hyperparameter was set to   2
f = 1. the optimum is
close to the parameters used when generating the data. note, the two ridges, one
for small noise and length-scale     = 0.4 and another for long length-scale and noise
  2

n = 1. the contour lines spaced 2 units apart in log id203 density.

log scale!), due to the poor ability of the model to explain the data, compare to
figure 2.5(c). for smaller length-scales the marginal likelihood decreases some-
what more slowly, corresponding to models that do accommodate the data,
but waste predictive mass at regions far away from the underlying function,
compare to figure 2.5(b).

in figure 5.3(b) the dependence of the log marginal likelihood on the charac-
teristic length-scale is shown for di   erent numbers of training cases. generally,
the more data, the more peaked the marginal likelihood. for very small numbers
of training data points the slope of the log marginal likelihood is very shallow
as when only a little data has been observed, both very short and intermediate
values of the length-scale are consistent with the data. with more data, the
complexity term gets more severe, and discourages too short length-scales.

marginal likelihood
gradient

to set the hyperparameters by maximizing the marginal likelihood, we seek
the partial derivatives of the marginal likelihood w.r.t. the hyperparameters.
using eq. (5.8) and eq. (a.14-a.15) we obtain
k   1y     1
2
(    >     k   1)    k
     j

tr(cid:0)k   1    k
(cid:17)

where    = k   1y.

y>k   1    k
     j

log p(y|x,   ) =

   
     j

1
2
1
2

(5.9)

(cid:16)

(cid:1)

     j

tr

=

the complexity of computing the marginal likelihood in eq. (5.8) is dominated
by the need to invert the k matrix (the log determinant of k is easily com-
puted as a by-product of the inverse). standard methods for matrix inversion of
positive de   nite symmetric matrices require time o(n3) for inversion of an n by
n matrix. once k   1 is known, the computation of the derivatives in eq. (5.9)
requires only time o(n2) per hyperparameter.5 thus, the computational over-
5note that matrix-by-matrix products in eq. (5.9) should not be computed directly: in the
   rst term, do the vector-by-id127s    rst; in the trace term, compute only the
diagonal terms of the product.

10010110   1100characteristic lengthscalenoise standard deviationc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.4 model selection for gp regression

115

head of computing derivatives is small, so using a gradient based optimizer is
advantageous.

estimation of    by optimzation of the marginal likelihood has a long history
in spatial statistics, see e.g. mardia and marshall [1984]. as n increases, one
would hope that the data becomes increasingly informative about   . however,
it is necessary to contrast what stein [1999, sec. 3.3] calls    xed-domain asymp-
totics (where one gets increasingly dense observations within some region) with
increasing-domain asymptotics (where the size of the observation region grows
with n). increasing-domain asymptotics are a natural choice in a time-series
context but    xed-domain asymptotics seem more natural in spatial (and ma-
chine learning) settings. for further discussion see stein [1999, sec. 6.4].

figure 5.4 shows an example of the log marginal likelihood as a function
of the characteristic length-scale and the noise standard deviation hyperpa-
rameters for the squared exponential covariance function, see eq. (5.1). the
signal variance   2
f was set to 1.0. the marginal likelihood has a clear maximum
around the hyperparameter values which were used in the gaussian process
from which the data was generated. note that for long length-scales and a
noise level of   2
n = 1, the marginal likelihood becomes almost independent of
the length-scale; this is caused by the model explaining everything as noise,
and no longer needing the signal covariance. similarly, for small noise and a
length-scale of     = 0.4, the marginal likelihood becomes almost independent of
the noise level; this is caused by the ability of the model to exactly interpolate
the data at this short length-scale. we note that although the model in this
hyperparameter region explains all the data-points exactly, this model is still
disfavoured by the marginal likelihood, see figure 5.2.

there is no guarantee that the marginal likelihood does not su   er from mul-
tiple local optima. practical experience with simple covariance functions seem
to indicate that local maxima are not a devastating problem, but certainly they
do exist. in fact, every local maximum corresponds to a particular interpre-
tation of the data. in figure 5.5 an example with two local optima is shown,
together with the corresponding (noise free) predictions of the model at each
of the two local optima. one optimum corresponds to a relatively complicated
model with low noise, whereas the other corresponds to a much simpler model
with more noise. with only 7 data points, it is not possible for the model to
con   dently reject either of the two possibilities. the numerical value of the
marginal likelihood for the more complex model is about 60% higher than for
the simple model. according to the bayesian formalism, one ought to weight
predictions from alternative explanations according to their posterior probabil-
ities. in practice, with data sets of much larger sizes, one often    nds that one
local optimum is orders of magnitude more probable than other local optima,
so averaging together alternative explanations may not be necessary. however,
care should be taken that one doesn   t end up in a bad local optimum.

above we have described how to adapt the parameters of the covariance
function given one dataset. however, it may happen that we are given several
datasets all of which are assumed to share the same hyperparameters; this
is known as id72, see e.g. caruana [1997]. in this case one can

multiple local maxima

id72

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

116

model selection and adaptation of hyperparameters

(a)

(b)

(c)

n (noise standard deviation), where   2

figure 5.5: panel (a) shows the marginal likelihood as a function of the hyperparame-
ters     (length-scale) and   2
f = 1 (signal standard
deviation) for a data set of 7 observations (seen in panels (b) and (c)). there are
two local optima, indicated with    +   : the global optimum has low noise and a short
length-scale; the local optimum has a high noise and a long length scale. in (b) and (c)
the inferred underlying functions (and 95% con   dence intervals) are shown for each
of the two solutions. in fact, the data points were generated by a gaussian process
with (   ,   2

n) = (1, 1, 0.1) in eq. (5.1).

f ,   2

simply sum the log marginal likelihoods of the indiviid78 and optimize
this sum w.r.t. the hyperparameters [minka and picard, 1999].

5.4.2 cross-validation

the predictive log id203 when leaving out training case i is

log p(yi|x, y   i,   ) =    1
2

log   2

i     (yi       i)2

2  2
i

    1
2

log 2  ,

(5.10)

negative log validation
density loss

where the notation y   i means all targets except number i, and   i and   2
i are
computed according to eq. (2.23) and (2.24) respectively, in which the training
sets are taken to be (x   i, y   i). accordingly, the loo log predictive id203
is

lloo(x, y,   ) =

log p(yi|x, y   i,   ),

(5.11)

nx

i=1

10010110   1100characteristic lengthscalenoise standard deviation   505   2   1012input, xoutput, y   505   2   1012input, xoutput, yc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.4 model selection for gp regression

117

pseudo-likelihood

see [geisser and eddy, 1979] for a discussion of this and related approaches.
lloo in eq. (5.11) is sometimes called the log pseudo-likelihood. notice, that
in each of the n loo-cv rotations, id136 in the gaussian process model
(with    xed hyperparameters) essentially consists of computing the inverse co-
variance matrix, to allow predictive mean and variance in eq. (2.23) and (2.24)
to be evaluated (i.e. there is no parameter-   tting, such as there would be in a
parametric model). the key insight is that when repeatedly applying the pre-
diction eq. (2.23) and (2.24), the expressions are almost identical: we need the
inverses of covariance matrices with a single column and row removed in turn.
this can be computed e   ciently from the inverse of the complete covariance
matrix using inversion by partitioning, see eq. (a.11-a.12). a similar insight
has also been used for spline models, see e.g. wahba [1990, sec. 4.2]. the ap-
proach was used for hyperparameter selection in gaussian process models in
sundararajan and keerthi [2001]. the expressions for the loo-cv predictive
mean and variance are

  i = yi     [k   1y]i/[k   1]ii,

and

i = 1/[k   1]ii,
  2

(5.12)

where careful inspection reveals that the mean   i is in fact independent of yi as
indeed it should be. the computational expense of computing these quantities
is o(n3) once for computing the inverse of k plus o(n2) for the entire loo-
cv procedure (when k   1 is known). thus, the computational overhead for
the loo-cv quantities is negligible. plugging these expressions into eq. (5.10)
and (5.11) produces a performance estimator which we can optimize w.r.t. hy-
perparameters to do model selection. in particular, we can compute the partial
derivatives of lloo w.r.t. the hyperparameters (using eq. (a.14)) and use con-
jugate gradient optimization. to this end, we need the partial derivatives of
the loo-cv predictive mean and variances from eq. (5.12) w.r.t. the hyperpa-
rameters

      i[zjk   1]ii

,

     i
     j

=

[zj  ]i
[k   1]ii

[k   1]2
where    = k   1y and zj = k   1    k
obtained by using the chain-rule and eq. (5.13) to give

     j

ii

     2
i
     j

=

[zjk   1]ii
[k   1]2

ii

. the partial derivatives of eq. (5.11) are

,

(5.13)

   lloo

     j

=

=

nx
nx

i=1

i=1

(cid:16)

    log p(yi|x, y   i,   )

(cid:16)

     i
     j
1 +   2

i

+     log p(yi|x, y   i,   )
(cid:17)

     2
i
     j
/[k   1]ii.

[zjk   1]ii

(cid:17)

     2
i

[k   1]ii

     i

  i[zj  ]i     1
2

(5.14)

the computational complexity is o(n3) for computing the inverse of k, and
o(n3) per hyperparameter 6 for the derivative eq. (5.14). thus, the computa-
tional burden of the derivatives is greater for the loo-cv method than for the
method based on marginal likelihood, eq. (5.9).

6computation of the matrix-by-matrix product k   1    k
     j

for each hyperparameter is un-

avoidable.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

118

model selection and adaptation of hyperparameters

loo-cv with squared
error loss

in eq. (5.10) we have used the log of the validation density as a cross-
validation measure of    t (or equivalently, the negative log validation density as
a id168). one could also envisage using other id168s, such as the
commonly used squared error. however, this id168 is only a function
of the predicted mean and ignores the validation set variances. further, since
the mean prediction eq. (2.23) is independent of the scale of the covariances
(i.e. you can multiply the covariance of the signal and noise by an arbitrary
positive constant without changing the mean predictions), one degree of freedom
is left undetermined7 by a loo-cv procedure based on squared error loss (or
any other id168 which depends only on the mean predictions). but, of
course, the full predictive distribution does depend on the scale of the covariance
function. also, computation of the derivatives based on the squared error loss
has similar computational complexity as the negative log validation density loss.
in conclusion, it seems unattractive to use loo-cv based on squared error loss
for hyperparameter selection.

comparing the pseudo-likelihood for the loo-cv methodology with the
marginal likelihood from the previous section, it is interesting to ask under
which circumstances each method might be preferable. their computational
demands are roughly identical. this issue has not been studied much empir-
ically. however, it is interesting to note that the marginal likelihood tells us
the id203 of the observations given the assumptions of the model. this
contrasts with the frequentist loo-cv value, which gives an estimate for the
(log) predictive id203, whether or not the assumptions of the model may
be ful   lled. thus wahba [1990, sec. 4.8] has argued that cv procedures should
be more robust against model mis-speci   cation.

5.4.3 examples and discussion

in the following we give three examples of model selection for regression models.
we    rst describe a 1-d modelling task which illustrates how special covariance
functions can be designed to achieve various useful e   ects, and can be evaluated
using the marginal likelihood. secondly, we make a short reference to the model
selection carried out for the robot arm problem discussed in chapter 2 and again
in chapter 8. finally, we discuss an example where we deliberately choose a
covariance function that is not well-suited for the problem; this is the so-called
mis-speci   ed model scenario.

mauna loa atmospheric carbon dioxide

we will use a modelling problem concerning the concentration of co2 in the
atmosphere to illustrate how the marginal likelihood can be used to set multiple
hyperparameters in hierarchical gaussian process models. a complex covari-
ance function is derived by combining several di   erent kinds of simple covariance
functions, and the resulting model provides an excellent    t to the data as well

7in the special case where we know either the signal or the noise variance there is no

indeterminancy.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.4 model selection for gp regression

119

figure 5.6: the 545 observations of monthly averages of the atmospheric concentra-
tion of co2 made between 1958 and the end of 2003, together with 95% predictive
con   dence region for a gaussian process regression model, 20 years into the future.
rising trend and seasonal variations are clearly visible. note also that the con   dence
interval gets wider the further the predictions are extrapolated.

as insights into its properties by interpretation of the adapted hyperparame-
ters. although the data is one-dimensional, and therefore easy to visualize, a
total of 11 hyperparameters are used, which in practice rules out the use of
cross-validation for setting parameters, except for the gradient-based loo-cv
procedure from the previous section.

the data [keeling and whorf, 2004] consists of monthly average atmospheric
co2 concentrations (in parts per million by volume (ppmv)) derived from in situ
air samples collected at the mauna loa observatory, hawaii, between 1958 and
2003 (with some missing values).8 the data is shown in figure 5.6. our goal is
the model the co2 concentration as a function of time x. several features are
immediately apparent: a long term rising trend, a pronounced seasonal variation
and some smaller irregularities. in the following we suggest contributions to a
combined covariance function which takes care of these individual properties.
this is meant primarily to illustrate the power and    exibility of the gaussian
process framework   it is possible that other choices would be more appropriate
for this data set.

to model the long term smooth rising trend we use a squared exponential
(se) covariance term, with two hyperparameters controlling the amplitude   1
and characteristic length-scale   2

smooth trend

k1(x, x0) =   2

1 exp

.

(5.15)

(cid:16)    (x     x0)2

(cid:17)

2  2
2

note that we just use a smooth trend; actually enforcing the trend a priori to
be increasing is probably not so simple and (hopefully) not desirable. we can
use the periodic covariance function from eq. (4.31) with a period of one year to
model the seasonal variation. however, it is not clear that the seasonal trend is

8the data is available from http://cdiac.esd.ornl.gov/ftp/trends/co2/maunaloa.co2.

seasonal component

1960197019801990200020102020320340360380400420yearco2 concentration, ppmc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

120

model selection and adaptation of hyperparameters

(b)

(a)
figure 5.7: panel (a):
long term trend, dashed, left hand scale, predicted by the
squared exponential contribution; superimposed is the medium term trend, full line,
right hand scale, predicted by the rational quadratic contribution; the vertical dash-
dotted line indicates the upper limit of the training data. panel (b) shows the seasonal
variation over the year for three di   erent years. the concentration peaks in mid may
and has a low in the beginning of october. the seasonal variation is smooth, but
not of exactly sinusoidal shape. the peak-to-peak amplitude increases from about 5.5
ppm in 1958 to about 7 ppm in 2003, but the shape does not change very much. the
characteristic decay length of the periodic component is inferred to be 90 years, so
the seasonal trend changes rather slowly, as also suggested by the gradual progression
between the three years shown.

exactly periodic, so we modify eq. (4.31) by taking the product with a squared
exponential component (using the product construction from section 4.2.4), to
allow a decay away from exact periodicity

(cid:16)    (x     x0)2

2  2
4

(cid:17)

    2 sin2(  (x     x0))

  2
5

,

(5.16)

k2(x, x0) =   2

3 exp

where   3 gives the magnitude,   4 the decay-time for the periodic component,
and   5 the smoothness of the periodic component; the period has been    xed
to one (year). the seasonal component in the data is caused primarily by
di   erent rates of co2 uptake for plants depending on the season, and it is
probably reasonable to assume that this pattern may itself change slowly over
time, partially due to the elevation of the co2 level itself; if this e   ect turns
out not to be relevant, then it can be e   ectively removed at the    tting stage by
allowing   4 to become very large.

to model the (small) medium term irregularities a rational quadratic term

(cid:16)

(cid:17)     8

k3(x, x0) =   2

6

1 +

(x     x0)2
2  8  2
7

,

(5.17)

medium term
irregularities

is used, eq. (4.19)

where   6 is the magnitude,   7 is the typical length-scale and   8 is the shape pa-
rameter determining di   useness of the length-scales, see the discussion on page
87. one could also have used a squared exponential form for this component,
but it turns out that the rational quadratic works better (gives higher marginal
likelihood), probably because it can accommodate several length-scales.

1960197019801990200020102020320340360380400co2 concentration, ppmyear   1   0.500.51co2 concentration, ppmjfmamjjasond   3   2   10123co2 concentration, ppmmonth195819702003c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.4 model selection for gp regression

121

figure 5.8: the time course of the seasonal e   ect, plotted in a months vs. year plot
(with wrap-around continuity between the edges). the labels on the contours are in
ppmv of co2. the training period extends up to the dashed line. note the slow
development: the height of the may peak may have started to recede, but the low in
october may currently (2005) be deepening further. the seasonal e   ects from three
particular years were also plotted in figure 5.7(b).

(cid:16)    (xp     xq)2

(cid:17)

2  2
10

finally we specify a noise model as the sum of a squared exponential con-

tribution and an independent component

k4(xp, xq) =   2

9 exp

+   2

11  pq,

(5.18)

where   9 is the magnitude of the correlated noise component,   10 is its length-
scale and   11 is the magnitude of the independent noise component. noise in
the series could be caused by measurement inaccuracies, and by local short-term
weather phenomena, so it is probably reasonable to assume at least a modest
amount of correlation in time. notice that the correlated noise component, the
   rst term of eq. (5.18), has an identical expression to the long term component
in eq. (5.15). when optimizing the hyperparameters, we will see that one of
these components becomes large with a long length-scale (the long term trend),
while the other remains small with a short length-scale (noise). the fact that
we have chosen to call one of these components    signal    and the other one    noise   
is only a question of interpretation. presumably we are less interested in very
short-term e   ect, and thus call it noise; if on the other hand we were interested
in this e   ect, we would call it signal.

the    nal covariance function is

k(x, x0) = k1(x, x0) + k2(x, x0) + k3(x, x0) + k4(x, x0),

(5.19)
with hyperparameters    = (  1, . . . ,   11)>. we    rst subtract the empirical mean
of the data (341 ppm), and then    t the hyperparameters by optimizing the
marginal likelihood using a conjugate gradient optimizer. to avoid bad local
minima (e.g. caused by swapping r  oles of the rational quadratic and squared
exponential terms) a few random restarts are tried, picking the run with the
best marginal likelihood, which was log p(y|x,   ) =    108.5.

noise terms

parameter estimation

jfmamjjasond1960197019801990200020102020monthyear   3.6   3.3   2.8   2.8   2   2   1   10011222.833.1c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

122

model selection and adaptation of hyperparameters

we now examine and interpret the hyperparameters which optimize the
marginal likelihood. the long term trend has a magnitude of   1 = 66 ppm
and a length scale of   2 = 67 years. the mean predictions inside the range
of the training data and extending for 20 years into the future are depicted in
figure 5.7 (a). in the same plot (with right hand axis) we also show the medium
term e   ects modelled by the rational quadratic component with magnitude
  6 = 0.66 ppm, typical length   7 = 1.2 years and shape   8 = 0.78. the very
small shape value allows for covariance at many di   erent length-scales, which
is also evident in figure 5.7 (a). notice that beyond the edge of the training
data the mean of this contribution smoothly decays to zero, but of course it
still has a contribution to the uncertainty, see figure 5.6.

the hyperparameter values for the decaying periodic contribution are: mag-
nitude   3 = 2.4 ppm, decay-time   4 = 90 years, and the smoothness of the
periodic component is   5 = 1.3. the quite long decay-time shows that the
data have a very close to periodic component in the short term. in figure 5.7
(b) we show the mean periodic contribution for three years corresponding to
the beginning, middle and end of the training data. this component is not
exactly sinusoidal, and it changes its shape slowly over time, most notably the
amplitude is increasing, see figure 5.8.

   

for the noise components, we get the amplitude for the correlated compo-
nent   9 = 0.18 ppm, a length-scale of   10 = 1.6 months and an independent
noise magnitude of   11 = 0.19 ppm. thus, the correlation length for the noise
component is indeed inferred to be short, and the total magnitude of the noise
11 = 0.26 ppm, indicating that the data can be explained very
is just
well by the model. note also in figure 5.6 that the model makes relatively
con   dent predictions, the 95% con   dence region being 16 ppm wide at a 20
year prediction horizon.

9 +   2
  2

in conclusion, we have seen an example of how non-trivial structure can be
inferred by using composite covariance functions, and that the ability to leave
hyperparameters to be determined by the data is useful in practice. of course
a serious treatment of such data would probably require modelling of other
e   ects, such as demographic and economic indicators too. finally, one may
want to use a real time-series approach (not just a regression from time to co2
level as we have done here), to accommodate causality, etc. nevertheless, the
ability of the gaussian process to avoid simple parametric assumptions and still
build in a lot of structure makes it, as we have seen, a very attractive model in
many application domains.

robot arm inverse dynamics

we have discussed the use of gpr for the sarcos robot arm inverse dynamics
problem in section 2.5. this example is also further studied in section 8.3.7
where a variety of approximation methods are compared, because the size of
the training set (44, 484 examples) precludes the use of simple gpr due to its
o(n2) storage and o(n3) time complexity.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.4 model selection for gp regression

123

(a)

(b)

figure 5.9: mis-speci   cation example. fit to 64 datapoints drawn from a step func-
tion with gaussian noise with standard deviation   n = 0.1. the gaussian process
models are using a squared exponential covariance function. panel (a) shows the mean
and 95% con   dence interval for the noisy signal in grey, when the hyperparameters are
chosen to maximize the marginal likelihood. panel (b) shows the resulting model when
the hyperparameters are chosen using leave-one-out cross-validation (loo-cv). note
that the marginal likelihood chooses a high noise level and long length-scale, whereas
loo-cv chooses a smaller noise level and shorter length-scale. it is not immediately
obvious which    t it worse.

one of the techniques considered in section 8.3.7 is the subset of datapoints
(sd) method, where we simply discard some of the data and only make use
of m < n training examples. given a subset of the training data of size m
selected at random, we adjusted the hyperparameters by optimizing either the
marginal likelihood or lloo. as ard was used, this involved adjusting d +
2 = 23 hyperparameters. this process was repeated 10 times with di   erent
random subsets of the data selected for both m = 1024 and m = 2048. the
results show that the predictive accuracy obtained from the two optimization
methods is very similar on both standardized mean squared error (smse) and
mean standardized log loss (msll) criteria, but that the marginal likelihood
optimization is much quicker.

step function example illustrating mis-speci   cation

in this section we discuss the mis-speci   ed model scenario, where we attempt
to learn the hyperparameters for a covariance function which is not very well
suited to the data. the mis-speci   cation arises because the data comes from a
function which has either zero or very low id203 under the gp prior. one
could ask why it is interesting to discuss this scenario, since one should surely
simply avoid choosing such a model in practice. while this is true in theory,
for practical reasons such as the convenience of using standard forms for the
covariance function or because vague prior information, one inevitably ends up
in a situation which resembles some level of mis-speci   cation.

as an example, we use data from a noisy step function and    t a gp model
with a squared exponential covariance function, figure 5.9. there is mis-
speci   cation because it would be very unlikely that samples drawn from a gp

   1   0.500.51   2   1012output, yinput, x   1   0.500.51   2   1012output, yinput, xc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

124

model selection and adaptation of hyperparameters

(a)

(b)

figure 5.10: same data as in figure 5.9. panel (a) shows the result of using a
covariance function which is the sum of two squared-exponential terms. although this
is still a stationary covariance function, it gives rise to a higher marginal likelihood
than for the squared-exponential covariance function in figure 5.9(a), and probably
also a better    t. in panel (b) the neural network covariance function eq. (4.29) was
used, providing a much larger marginal likelihood and a very good    t.

with the stationary se covariance function would look like a step function. for
short length-scales samples can vary quite quickly, but they would tend to vary
rapidly all over, not just near the step. conversely a stationary se covariance
function with a long length-scale could model the    at parts of the step function
but not the rapid transition. note that gibbs    covariance function eq. (4.32)
would be one way to achieve the desired e   ect. it is interesting to note the dif-
ferences between the model optimized with marginal likelihood in figure 5.9(a),
and one optimized with loo-cv in panel (b) of the same    gure. see exercise
5.6.2 for more on how these two criteria weight the in   uence of the prior.

for comparison, we show the predictive distribution for two other covari-
ance functions in figure 5.10. in panel (a) a sum of two squared exponential
terms were used in the covariance. notice that this covariance function is still
stationary, but it is more    exible than a single squared exponential, since it has
two magnitude and two length-scale parameters. the predictive distribution
looks a little bit better, and the value of the log marginal likelihood improves
from    37.7 in figure 5.9(a) to    26.1 in figure 5.10(a). we also tried the neural
network covariance function from eq. (4.29), which is ideally suited to this case,
since it allows saturation at di   erent values in the positive and negative direc-
tions of x. as shown in figure 5.10(b) the predictions are also near perfect,
and the log marginal likelihood is much larger at 50.2.

5.5 model selection for gp classi   cation

in this section we compute the derivatives of the approximate marginal likeli-
hood for the laplace and ep methods for binary classi   cation which are needed
for training. we also give the detailed algorithms for these, and brie   y discuss
the possible use of cross-validation and other methods for training binary gp

   1   0.500.51   2   1012output, yinput, x   1   0.500.51   2   1012output, yinput, xc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.5 model selection for gp classi   cation

125

classi   ers.

5.5.1 derivatives of the marginal likelihood for laplace   s    

approximation

recall from section 3.4.4 that the approximate log marginal likelihood was given
in eq. (3.32) as

log q(y|x,   ) =    1
2

  f>k   1  f + log p(y|  f)     1
2

log |b|,

(5.20)

1

1
2 kw

2 and   f is the maximum of the posterior eq. (3.12)
where b = i + w
found by newton   s method in algorithm 3.1, and w is the diagonal matrix
w =           log p(y|  f). we can now optimize the approximate marginal likeli-
hood q(y|x,   ) w.r.t. the hyperparameters,   . to this end we seek the partial
derivatives of    q(y|x,   )/     j. the covariance matrix k is a function of the hy-
perparameters, but   f and therefore w are also implicitly functions of   , since
when    changes, the optimum of the posterior   f also changes. thus
      fi
     j

=     log q(y|x,   )

    log q(y|x,   )

    log q(y|x,   )

(cid:12)(cid:12)(cid:12)(cid:12)explicit

nx

(5.21)

      fi

     j

     j

+

,

i=1

by the chain rule. using eq. (a.14) and eq. (a.15) the explicit term is given by

(cid:17)

(cid:17)

(cid:16)

(cid:16)

(cid:12)(cid:12)(cid:12)(cid:12)explicit

    log q(y|x,   )

     j

=

1
2

  f>k   1    k
     j

k   1  f    1
2

tr

(w    1+k)   1    k
     j

. (5.22)

when evaluating the remaining term from eq. (5.21), we utilize the fact that
  f is the maximum of the posterior, so that      (f)/   f = 0 at f =   f, where the
(un-normalized) log posterior   (f) is de   ned in eq. (3.12); thus the implicit
derivatives of the two    rst terms of eq. (5.20) vanish, leaving only

    log q(y|x,   )

      fi

=    1
2
=    1
2

    log |b|

=    1
2

      fi

(cid:2)(k   1 + w )   1(cid:3)

tr

(k   1 + w )   1    w
      fi

log p(y|  f).

   3
   f 3
i

ii

(5.23)

=    k
     j

in order to evaluate the derivative      f /     j, we di   erentiate the self-consistent
eq. (3.17)   f = k    log p(y|  f) to obtain
     f
    log p(y|  f),
     j
(5.24)
where we have used the chain rule    /     j =      f /     j       /     f and the identity
       log p(y|  f)/     f =    w . the desired derivatives are obtained by plugging
eq. (5.22-5.24) into eq. (5.21).

= (i +kw )   1    k
     j

    log p(y|  f)+k

       log p(y|  f)

     f
     j

     f

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

126

model selection and adaptation of hyperparameters

eq. (5.24)
eq. (5.21)

(f , a) := mode(cid:0)k, y, p(y|f)(cid:1)

input: x (inputs), y (  1 targets),    (hypers), p(y|f) (likelihood function)
compute covariance matrix from x and   
locate posterior mode using algorithm 3.1

2: compute k
4: w :=           log p(y|f)
l := cholesky(i + w
6: log z :=     1
2 l>\(l\w

1

1

8: c := l\(w

r := w
s2 :=     1

1

2 )

1
2 kw

2 a>f + log p(y|f)    p log(diag(l))
2 diag(cid:0) diag(k)     diag(c>c)(cid:1)   3 log p(y|f)

r = w

2 k)

2 )

1

10: for j := 1 . . . dim(  ) do

solve ll> = b = i + w

1
2 kw

1
2

1

2 (i + w

1
2 kw

1

eq. (5.20)
2 )   1w
eq. (5.23)

1
2

compute derivative matrix from x and   
eq. (5.22)

o

o

12:

14:

c :=    k/     j
2 a>ca     1
s1 := 1
b := c    log p(y|f)
s3 := b     krb
   j log z := s1 + s>
2 s3

2 tr(rc)

16: end for

return: log z (log marginal likelihood),     log z (partial derivatives)

algorithm 5.1: compute the approximate log marginal likelihood and its derivatives
w.r.t. the hyperparameters for binary laplace gpc for use by an optimization routine,
such as conjugate gradient optimization. in line 3 algorithm 3.1 on page 46 is called
to locate the posterior mode.
in line 12 only the diagonal elements of the matrix
product should be computed. in line 15 the notation    j means the partial derivative
w.r.t. the j   th hyperparameter. an actual implementation may also return the value
of f to be used as an initial guess for the subsequent call (as an alternative the zero
initialization in line 2 of algorithm 3.1).

details of the implementation

the implementation of the log marginal likelihood and its partial derivatives
w.r.t. the hyperparameters is shown in algorithm 5.1. it is advantageous to re-
write the equations from the previous section in terms of well-conditioned sym-
metric positive de   nite matrices, whose solutions can be obtained by cholesky
factorization, combining numerical stability with computational speed.

in detail, the matrix of central importance turns out to be
2 )   1w

r = (w    1 + k)   1 = w

2 (i + w

1
2 kw

1

1

1
2 ,

(5.25)

where the right hand side is suitable for numerical evaluation as in line 7 of
algorithm 5.1, reusing the cholesky factor l from the newton scheme above.
remember that w is diagonal so eq. (5.25) does not require any real matrix-by-
matrix products. rewriting eq. (5.22-5.23) is straightforward, and for eq. (5.24)
we apply the matrix inversion lemma (eq. (a.9)) to (i + kw )   1 to obtain
i     kr, which is used in the implementation.

the computational complexity is dominated by the cholesky factorization
in line 5 which takes n3/6 operations per iteration of the newton scheme. in
addition the computation of r in line 7 is also o(n3), all other computations
being at most o(n2) per hyperparameter.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

5.5 model selection for gp classi   cation

127

input: x (inputs), y (  1 targets),    (hyperparameters)

(    ,      , log zep) := ep(cid:0)k, y(cid:1)

2: compute k

2 k   s

4: l := cholesky(i +   s
2 l\(l>\   s

b :=            s

6: r := bb>       s

2 l>\(l\   s
for j := 1 . . . dim(  ) do

2 )
2 k     )
2 )

1

1

1

1

1

1

compute covariance matrix from x and   
run the ep algorithm 3.5
2 k   s
b from under eq. (5.27)
r = bb>       s
2 b   1   s

solve ll> = b = i +   s

1
2

1
2

1

1

8:

2 tr(rc)

10: end for

c :=    k/     j
   j log zep := 1

compute derivative matrix from x and   
eq. (5.27)
return: log zep (log marginal likelihood),     log zep (partial derivatives)
algorithm 5.2: compute the log marginal likelihood and its derivatives w.r.t. the
hyperparameters for ep binary gp classi   cation for use by an optimization routine,
such as conjugate gradient optimization.   s is a diagonal precision matrix with entries
  sii =     i. in line 3 algorithm 3.5 on page 58 is called to compute parameters of the ep
approximation. in line 9 only the diagonal of the matrix product should be computed
and the notation    j means the partial derivative w.r.t. the j   th hyperparameter. the
computational complexity is dominated by the cholesky factorization in line 4 and
the solution in line 6, both of which are o(n3).

5.5.2 derivatives of the marginal likelihood for ep

   

optimization of the ep approximation to the marginal likelihood w.r.t. the
hyperparameters of the covariance function requires evaluation of the partial
derivatives from eq. (3.65). luckily, it turns out that implicit terms in the
derivatives caused by the solution of ep being a function of the hyperparam-
eters is exactly zero. we will not present the proof here, see seeger [2005].
consequently, we only have to take account of the explicit dependencies

(cid:0)     1

    log zep

     j

    >(k +     )   1          1
2

=    
     j
1
    >(k +   s   1)   1    k
2
     j

=

2

(k +   s   1)   1          1
2

log |k +     |(cid:1)

in algorithm 5.2 the derivatives from eq. (5.26) are implemented using

(5.26)

(cid:1).

tr(cid:0)(k +   s   1)   1    k
(cid:17)
2(cid:1)    k

     j

1

,

(5.27)

     j

(cid:16)(cid:0)bb>       s

1

2 b   1s

    log zep

=

1
2

tr

where b = (i       s

     j
2 b   1   s

1

1

2 k)    .

5.5.3 cross-validation

whereas the loo-cv estimates were easily computed for regression through
the use of rank-one updates, it is not so obvious how to generalize this to
classi   cation. opper and winther [2000, sec. 5] use the cavity distributions
of their mean-   eld approach as loo-cv estimates, and one could similarly
use the cavity distributions from the closely-related ep algorithm discussed in

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

128

model selection and adaptation of hyperparameters

section 3.6. although technically the cavity distribution for site i could depend
on the label yi (because the algorithm uses all cases when converging to its
   xed point), this e   ect is probably very small and indeed opper and winther
[2000, sec. 8] report very high precision for these loo-cv estimates. as an
alternative k-fold cv could be used explicitly for some moderate value of k.

other methods for setting hyperparameters

alignment

above we have considered setting hyperparameters by optimizing the marginal
likelihood or cross-validation criteria. however, some other criteria have been
proposed in the literature. for example cristianini et al. [2002] de   ne the
alignment between a gram matrix k and the corresponding +1/    1 vector of
targets y as

a(k, y) =

trices ki so that k =p

(5.28)
where kkkf denotes the frobenius norm of the matrix k, as de   ned in eq. (a.16).
lanckriet et al. [2004] show that if k is a convex combination of gram ma-
i   iki with   i     0 for all i then the optimization of
the alignment score w.r.t. the   i   s can be achieved by solving a semide   nite
programming problem.

,

y>ky
nkkkf

5.5.4 example

for an example of model selection, refer to section 3.7. although the experi-
ments there were done by exhaustively evaluating the marginal likelihood for a
whole grid of hyperparameter values, the techniques described in this chapter
could be used to locate the same solutions more e   ciently.

5.6 exercises

1. the optimization of the marginal likelihood w.r.t. the hyperparameters
is generally not possible in closed form. consider, however, the situation
where one hyperparameter,   0 gives the overall scale of the covariance

ky(x, x0) =   0

  ky(x, x0),

(5.29)
where ky is the covariance function for the noisy targets (i.e. including
noise contributions) and   ky(x, x0) may depend on further hyperparam-
eters,   1,   2, . . .. show that the marginal likelihood can be optimized
w.r.t.   0 in closed form.

p
given by p
2. consider the di   erence between the log marginal likelihood given by:
i log p(yi|{yj, j < i}), and the loo-cv using log id203 which is
i log p(yi|{yj, j 6= i}). from the viewpoint of the marginal
likelihood the loo-cv conditions too much on the data. show that the
expected loo-cv loss is greater than the expected marginal likelihood.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

chapter 6

relationships between gps
and other models

in this chapter we discuss a number of concepts and models that are related to
gaussian process prediction. in section 6.1 we cover reproducing kernel hilbert
spaces (rkhss), which de   ne a hilbert space of su   ciently-smooth functions
corresponding to a given positive semide   nite kernel k.

as we discussed in chapter 1, there are many functions that are consistent
with a given dataset d. we have seen how the gp approach puts a prior
over functions in order to deal with this issue. a related viewpoint is provided
by id173 theory (described in section 6.2) where one seeks a trade-o   
between data-   t and the rkhs norm of function. this is closely related to the
map estimator in gp prediction, and thus omits uncertainty in predictions
and also the marginal likelihood. in section 6.3 we discuss splines, a special
case of id173 which is obtained when the rkhs is de   ned in terms of
di   erential operators of a given order.

there are a number of other families of kernel machines that are related
to gaussian process prediction. in section 6.4 we describe support vector ma-
chines, in section 6.5 we discuss least-squares classi   cation (lsc), and in section
6.6 we cover relevance vector machines (rvms).

6.1 reproducing kernel hilbert spaces

here we present a brief introduction to reproducing kernel hilbert spaces. the
theory was developed by aronszajn [1950]; a more recent treatise is saitoh
[1988]. information can also be found in wahba [1990], sch  olkopf and smola
[2002] and wegman [1982]. the collection of papers edited by weinert [1982]
provides an overview of the uses of rkhss in statistical signal processing.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

130

relationships between gps and other models

we start with a formal de   nition of a rkhs, and then describe two speci   c
bases for a rkhs,    rstly through mercer   s theorem and the eigenfunctions of
k, and secondly through the reproducing kernel map.
de   nition 6.1 (reproducing kernel hilbert space). let h be a hilbert space
of real functions f de   ned on an index set x . then h is called a reproducing
kernel hilbert space endowed with an inner product h  ,  ih (and norm kfkh =

phf, fih) if there exists a function k : x   x     r with the following properties:

1. for every x, k(x, x0) as a function of x0 belongs to h, and
2. k has the reproducing property hf(  ), k(  , x)ih = f(x).

(cid:3)

see e.g. sch  olkopf and smola [2002] and wegman [1982]. note also that as
k(x,  ) and k(x0,  ) are in h we have that hk(x,  ), k(x0,  )ih = k(x, x0).

reproducing property

the rkhs uniquely determines k, and vice versa, as stated in the following

theorem:
theorem 6.1 (moore-aronszajn theorem, aronszajn [1950]). let x be an in-
dex set. then for every positive de   nite function k(  ,  ) on x    x there exists
(cid:3)
a unique rkhs, and vice versa.

the hilbert space l2 (which has the dot product hf, gil2 =r f(x)g(x)dx)
function is the representer of evaluation, i.e. f(x) =r f(x0)  (x   x0)dx0. kernels

contains many non-smooth functions. in l2 (which is not a rkhs) the delta

are the analogues of delta functions within the smoother rkhs. note that the
delta function is not itself in l2; in contrast for a rkhs the kernel k is the
representer of evaluation and is itself in the rkhs.

inner product
hf, gih

the above description is perhaps rather abstract. for our purposes the key
intuition behind the rkhs formalism is that the squared norm kfk2h can be
thought of as a generalization to functions of the n-dimensional quadratic form
f>k   1f we have seen in earlier chapters.

consider a real positive semide   nite kernel k(x, x0) with an eigenfunction
i=1  i  i(x)  i(x0) relative to a measure   . recall from
mercer   s theorem that the eigenfunctions are orthonormal w.r.t.   , i.e. we have

expansion k(x, x0) =pn
r   i(x)  j(x) d  (x) =   ij. we now consider a hilbert space comprised of linear
combinations of the eigenfunctions, i.e. f(x) =pn
functions f(x) and g(x) =pn

i /  i <
   . we assert that the inner product hf, gih in the hilbert space between

i=1fi  i(x) withpn

i=1gi  i(x) is de   ned as

i=1f 2

hf, gih =

.

(6.1)

pn
thus this hilbert space is equipped with a norm kfkh where kfk2h = hf, fih =
i /  i. note that for kfkh to be    nite the sequence of coe   cients {fi}
must decay quickly; e   ectively this imposes a smoothness condition on the
space.

i=1f 2

i=1

nx

figi
  i

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6.1 reproducing kernel hilbert spaces

131

we now need to show that this hilbert space is the rkhs corresponding to
the kernel k, i.e. that it has the reproducing property. this is easily achieved
as

hf(  ), k(  , x)ih =

= f(x).

(6.2)

similarly

nx

fi  i  i(x)

  i

i=1

nx

  i  i(x)  i  i(x0)

hk(x,  ), k(x0,  )ih =

= k(x, x0).

(6.3)

i=1

  i

notice also that k(x,  ) is in the rkhs as it has normpn
linear combinations of the eigenfunctions with the restrictionpn

i=1(  i  i(x))2/  i =
k(x, x) <    . we have now demonstrated that the hilbert space comprised of
i /  i <    
ful   ls the two conditions given in de   nition 6.1. as there is a unique rkhs
associated with k(  ,  ), this hilbert space must be that rkhs.

i=1f 2

the advantage of the abstract formulation of the rkhs is that the eigenbasis
will change as we use di   erent measures    in mercer   s theorem. however, the
rkhs norm is in fact solely a property of the kernel and is invariant under
this change of measure. this can be seen from the fact that the proof of the
rkhs properties above is not dependent on the measure; see also kailath
[1971, sec. ii.b]. a    nite-dimensional example of this measure invariance is
explored in exercise 6.7.1.

notice the analogy between the rkhs norm kfk2h = hf, fih =pn
if we sample the coe   cients fi in the eigenexpansion f(x) =pn

i /  i
and the quadratic form f>k   1f; if we express k and f in terms of the eigen-
vectors of k we obtain exactly the same form (but the sum has only n terms if
f has length n).

i=1f 2

i=1fi  i(x)

from n (0,   i) then

e[kfk2h] =

nx

i=1

e[f 2
i ]
  i

=

nx

i=1

1.

(6.4)

thus if n is in   nite the sample functions are not in h (with id203 1)
as the expected value of the rkhs norm is in   nite; see wahba [1990, p. 5]
and kailath [1971, sec. ii.b] for further details. however, note that although
sample functions of this gaussian process are not in h, the posterior mean after
observing some data will lie in the rkhs, due to the smoothing properties of
averaging.

another view of the rkhs can be obtained from the reproducing kernel

map construction. we consider the space of functions f de   ned as

n

f(x) =

nx

i=1

  ik(x, xi) : n     n, xi     x ,   i     ro

.

(6.5)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

132

regularizer

(kernel) ridge regression

representer theorem

nx

n0x

i=1

j=1

nx

now let g(x) =pn0

relationships between gps and other models

j=1  0

jk(x, x0

j). then we de   ne the inner product

hf, gih =

  i  0

jk(xi, x0
j).

(6.6)

clearly condition 1 of de   nition 6.1 is ful   lled under the reproducing kernel
map construction. we can also demonstrate the reproducing property, as

hk(  , x), f(  )ih =

  ik(x, xi) = f(x).

(6.7)

i=1

6.2 id173

the problem of inferring an underlying function f(x) from a    nite (and possibly
noisy) dataset without any additional assumptions is clearly    ill posed   . for
example, in the noise-free case, any function that passes through the given data
points is acceptable. under a bayesian approach our assumptions are charac-
terized by a prior over functions, and given some data, we obtain a posterior
over functions. the problem of bringing prior assumptions to bear has also
been addressed under the id173 viewpoint, where these assumptions
are encoded in terms of the smoothness of f.

we consider the functional

j[f] =   
2

kfk2h + q(y, f),

(6.8)

where y is the vector of targets we are predicting and f = (f(x1), . . . , f(xn))>
is the corresponding vector of function values, and    is a scaling parameter that
trades o    the two terms. the    rst term is called the regularizer and represents
smoothness assumptions on f as encoded by a suitable rkhs, and the second
term is a data-   t term assessing the quality of the prediction f(xi) for the
observed datum yi, e.g. the negative log likelihood.

indeed, recalling that kfk2h = pn

ridge regression (described in section 2.1) can be seen as a particular case
i /  i where fi is the
of id173.
coe   cient of eigenfunction   i(x), we see that we are penalizing the weighted
squared coe   cients. this is taking place in feature space, rather than simply in
input space, as per the standard formulation of ridge regression (see eq. (2.4)),
so it corresponds to kernel ridge regression.

the representer theorem shows that each minimizer f     h of j[f] has the
i=1   ik(x, xi).1 the representer theorem was    rst stated by
kimeldorf and wahba [1971] for the case of squared error.2 o   sullivan et al.
[1986] showed that the representer theorem could be extended to likelihood

form f(x) = pn

i=1f 2

1if the rkhs contains a null space of unpenalized functions then the given form is correct

modulo a term that lies in this null space. this is explained further in section 6.3.

2schoenberg [1964] proved the representer theorem for the special case of cubic splines and
squared error. this was result extended to general rkhss in kimeldorf and wahba [1971].

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6.2 id173

133

functions arising from generalized linear models. the representer theorem can
be generalized still further, see e.g. sch  olkopf and smola [2002, sec. 4.2]. if the
data-   t term is convex (see section a.9) then there will be a unique minimizer
  f of j[f].

for gaussian process prediction with likelihoods that involve the values of
f at the n training points only (so that q(y, f) is the negative log likelihood
up to some terms not involving f), the analogue of the representer theorem is
obvious. this is because the predictive distribution of f(x   ) , f    at test point

x    given the data y is p(f   |y) =r p(f   |f)p(f|y) df. as derived in eq. (3.22) we
thus e[f   |y] =pn

(6.9)
due to the formulae for the conditional distribution of a multivariate gaussian.

i=1  ik(x   , xi), where    = k   1e[f|y].

e[f   |y] = k(x   )>k   1e[f|y]

have

the id173 approach has a long tradition in inverse problems, dat-
ing back at least as far as tikhonov [1963]; see also tikhonov and arsenin
[1977]. for the application of this approach in the machine learning literature
see e.g. poggio and girosi [1990].

in section 6.2.1 we consider rkhss de   ned in terms of di   erential operators.
in section 6.2.2 we demonstrate how to solve the id173 problem in the
speci   c case of squared error, and in section 6.2.3 we compare and contrast the
id173 approach with the gaussian process viewpoint.

   

null space

dx.

(6.10)

6.2.1 id173 de   ned by di   erential operators
for x     rd de   ne

z x
(cid:16)    2f
(cid:17)2

j1+...+jd=m

(cid:16)    mf(x)
(cid:16)    2f
(cid:17)2

   xj1

(cid:17)2
(cid:17)2i

1 . . . xjd

d

komfk2 =

z h(cid:16)    2f

for example for m = 2 and d = 2

ko2fk2 =

now set kp fk2 =pm

+ 2

   x2
1

(6.11)
m=0 amkomfk2 with non-negative coe   cients am. notice

dx1 dx2.

   x1   x2

   x2
2

+

that kp fk2 is translation and rotation invariant.

in this section we assume that a0 > 0; if this is not the case and ak is
the    rst non-zero coe   cient, then there is a null space of functions that are
unpenalized. for example if k = 2 then constant and linear functions are in the
null space. this case is dealt with in section 6.3.

kp fk2 penalizes f in terms of the variability of its function values and
derivatives up to order m. how does this correspond to the rkhs formulation
of section 6.1? the key is to recognize that the complex exponentials exp(2  is  
x) are eigenfunctions of the di   erential operator if x = rd. in this case

kp fk2 =

am(4  2s    s)m|   f(s)|2ds,

(6.12)

z mx

m=0

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

134

relationships between gps and other models

where   f(s) is the fourier transform of f(x). comparing eq. (6.12) with eq. (6.1)
we see that the kernel has the power spectrum

s(s) =

,

(6.13)

and thus by fourier inversion we obtain the stationary kernel

1

pm
m=0 am(4  2s    s)m
pm
m=0 am(4  2s    s)m

e2  is  x

z

k(x) =

ds.

(6.14)

a slightly di   erent approach to obtaining the kernel is to use calculus of
variations to minimize j[f] with respect to f. the euler-lagrange equation
leads to

nx

f(x) =

  ig(x     xi),

with

mx

i=1

(   1)mam   2mg =   (x     x0),

(6.15)

(6.16)

green   s function
    kernel

m=0

di   erential operatorpm

where g(x, x0) is known as a green   s function. notice that the green   s func-
tion also depends on the boundary conditions. for the case of x = rd by
fourier transforming eq. (6.16) we recognize that g is in fact the kernel k. the
m=0(   1)mam   2m and the integral operator k(  ,  ) are in
fact inverses, as shown by eq. (6.16). see poggio and girosi [1990] for further
details. arfken [1985] provides an introduction to calculus of variations and
green   s functions. rkhss for regularizers de   ned by di   erential operators are
sobolev spaces; see e.g. adams [1975] for further details on sobolev spaces.

we now give two speci   c examples of kernels derived from di   erential oper-

ators.
example 1. set a0 =   2, a1 = 1 and am = 0 for m     2 in d = 1. using
the fourier pair e     |x|     2  /(  2 + 4  2s2) we obtain k(x     x0) = 1
2   e     |x   x0|.
note that this is the covariance function of the ornstein-uhlenbeck process, see
section 4.2.1.
example 2. by setting am =   2m
we obtain

m!2m and using the power series ey =p   

k=0 yk/k!

k(x     x0) =

exp(2  is    (x     x0)) exp(      2
2

(4  2s    s))ds

=

1

(2    2)d/2

exp(    1

2  2 (x     x0)>(x     x0)),

(6.17)

(6.18)

z

as shown by yuille and grzywacz [1989]. this is the squared exponential co-
variance function that we have seen earlier.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6.2 id173

135

6.2.2 obtaining the regularized solution

the representer theorem tells us the general form of the solution to eq. (6.8).
we now consider a speci   c functional

j[f] =

kfk2h +

1
2

1
2  2
n

(yi     f(xi))2,

(6.19)

nx

i=1

which uses a squared error data-   t term (corresponding to the negative log
likelihood of a gaussian noise model with variance   2
n). substituting f(x) =

pn
i=1  ik(x, xi) and using hk(  , xi), k(  , xj)ih = k(xi, xj) we obtain

j[  ] =

=

1
2   >k   +
1
2   >(k +

1
|y     k  |2
2  2
n
k 2)       1
1
  2
  2
n
n

y>k   +

(6.20)

y>y.

1
2  2
n

minimizing j by di   erentiating w.r.t. the vector of coe   cients    we obtain
ni)   1y, so that the prediction for a test point x    is   f(x   ) =
     = (k +   2
k(x   )>(k +   2
ni)   1y. this should look very familiar   it is exactly the form of
the predictive mean obtained in eq. (2.23). in the next section we compare and
contrast the id173 and gp views of the problem.

the solution f(x) =pn

id173 network in poggio and girosi [1990].

i=1  ik(x, xi) that minimizes eq. (6.19) was called a

id173 network

6.2.3 the relationship of the id173 view to gaus-

sian process prediction

the id173 method returns   f = argminf j[f]. for a gaussian process
predictor we obtain a posterior distribution over functions. can we make a
connection between these two views? in fact we shall see in this section that   f
can be viewed as the maximum a posteriori (map) function under the posterior.

following szeliski [1987] and poggio and girosi [1990] we consider

exp (   j[f]) = exp(cid:0)       

kp fk2(cid:1)    exp (   q(y, f)) .

2

(6.21)

the    rst term on the rhs is a gaussian process prior on f, and the second
is proportional to the likelihood. as   f is the minimizer of j[f], it is the map
function.

f. thus we obtain kp fk2     pm

to get some intuition for the gaussian process prior, imagine f(x) being
represented on a grid in x-space, so that f is now an (in   nite dimensional) vector
mdm)f
where dm is an appropriate    nite-di   erence approximation of the di   erential
operator om. observe that this prior term is a quadratic form in f.

m=0 am(dmf)>(dmf) = f>(p

m amd>

to go into more detail concerning the map relationship we consider three
cases: (i) when q(y, f) is quadratic (corresponding to a gaussian likelihood);

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

136

relationships between gps and other models

(ii) when q(y, f) is not quadratic but convex and (iii) when q(y, f) is not
convex.

in case (i) we have seen in chapter 2 that the posterior mean function can
be obtained exactly, and the posterior is gaussian. as the mean of a gaussian
is also its mode this is the map solution. the correspondence between the gp
posterior mean and the solution of the id173 problem   f was made in
kimeldorf and wahba [1970].

in case (ii) we have seen in chapter 3 for classi   cation problems using the
logistic, probit or softmax response functions that q(y, f) is convex. here the
map solution can be found by    nding   f (the map solution to the n-dimensional
problem de   ned at the training points) and then extending it to other x-values
through the posterior mean conditioned on   f.

in case (iii) there will be more than one local minimum of j[f] under the
id173 approach. one could check these minima to    nd the deepest one.
however, in this case the argument for map is rather weak (especially if there
are multiple optima of similar depth) and suggests the need for a fully bayesian
treatment.

while the id173 solution gives a part of the gaussian process solu-

tion, there are the following limitations:

1. it does not characterize the uncertainty in the predictions, nor does it

handle well multimodality in the posterior.

2. the analysis is focussed at approximating the    rst level of bayesian infer-
ence, concerning predictions for f. it is not usually extended to the next
level, e.g. to the computation of the marginal likelihood. the marginal
likelihood is very useful for setting any parameters of the covariance func-
tion, and for model comparison (see chapter 5).

in addition, we    nd the speci   cation of smoothness via the penalties on deriva-
tives to be not very intuitive. the id173 viewpoint can be thought of
as directly specifying the inverse covariance rather than the covariance. as
marginalization is achieved for a gaussian distribution directly from the covari-
ance (and not the inverse covariance) it seems more natural to us to specify
the covariance function. also, while non-stationary covariance functions can
be obtained from the id173 viewpoint, e.g. by replacing the lebesgue
measure in eq. (6.10) with a non-uniform measure   (x), calculation of the cor-
responding covariance function can then be very di   cult.

6.3 spline models

in section 6.2 we discussed regularizers which had a0 > 0 in eq. (6.12). we now
consider the case when a0 = 0; in particular we consider the regularizer to be
of the form komfk2, as de   ned in eq. (6.10). in this case polynomials of degree

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6.3 spline models

137

up to m     1 are in the null space of the id173 operator, in that they
are not penalized at all.

in the case that x = rd we can again use fourier techniques to ob-
tain the green   s function g corresponding to the euler-lagrange equation
(   1)m   2mg(x) =   (x). the result, as shown by duchon [1977] and meinguet
[1979] is

(cid:26)cm,d|x     x0|2m   d log |x     x0|

cm,d|x     x0|2m   d

g(x   x0) =

if 2m > d and d even
otherwise,

(6.22)

where cm,d is a constant (wahba [1990, p. 31] gives the explicit form). note that
the constraint 2m > d has to be imposed to avoid having a green   s function
that is singular at the origin. explicit calculation of the green   s function for
other domains x is sometimes possible; for example see wahba [1990, sec. 2.2]
for splines on the sphere.

because of the null space, a minimizer of the id173 functional has

the form

nx

kx

f(x) =

  ig(x, xi) +

  jhj(x),

(6.23)

i=1

j=1

where h1(x), . . . , hk(x) are polynomials that span the null space. the exact
values of the coe   cients    and    for a speci   c problem can be obtained in
an analogous manner to the derivation in section 6.2.2; in fact the solution is
equivalent to that given in eq. (2.42).

to gain some more insight into the form of the green   s function we consider
the equation (   1)m   2mg(x) =   (x) in fourier space, leading to   g(s) = (4  2s  
s)   m.   g(s) plays a r  ole like that of the power spectrum in eq. (6.13), but notice

thatr   g(s)ds is in   nite, which would imply that the corresponding process has

in   nite variance. the problem is of course that the null space is unpenalized; for
example any arbitrary constant function can be added to f without changing
the regularizer.

tions of f(x) of the form g(x) =pk

because of the null space we have seen that one cannot obtain a simple
connection between the spline solution and a corresponding gaussian process
problem. however, by introducing the notion of an intrinsic random function
(irf) one can de   ne a generalized covariance; see cressie [1993, sec. 5.4] and
stein [1999, section 2.9] for details. the basic idea is to consider linear combina-
i=1 aif(x+  i) for which g(x) is second-order
stationary and where (hj(  1), . . . , hj(  k))a = 0 for j = 1, . . . , k. a careful de-
scription of the equivalence of spline and irf prediction is given in kent and
mardia [1994].

the power-law form of   g(s) = (4  2s  s)   m means that there is no character-
istic length-scale for random functions drawn from this (improper) prior. thus
we obtain the self-similar property characteristic of fractals; for further details
see szeliski [1987] and mandelbrot [1982]. some authors argue that the lack
of a characteristic length-scale is appealing. this may sometimes be the case,
but if we believe there is an appropriate length-scale (or set of length-scales)

irf

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

138

relationships between gps and other models

for a given problem but this is unknown in advance, we would argue that a
hierarchical bayesian formulation of the problem (as described in chapter 5)
would be more appropriate.

splines were originally introduced for one-dimensional interpolation and
smoothing problems, and then generalized to the multivariate setting. schoen-
berg [1964] considered the problem of    nding the function that minimizes

spline interpolation

natural polynomial
spline

smoothing spline

z b

a

(f (m)(x))2 dx,

(6.24)

where f (m) denotes the m   th derivative of f, subject to the interpolation con-
straints f(xi) = fi, xi     (a, b) for i = 1, . . . , n and for f in an appropriate
sobolev space. he showed that the solution is the natural polynomial spline,
which is a piecewise polynomial of order 2m     1 in each interval [xi, xi+1],
i = 1, . . . , n     1, and of order m     1 in the two outermost intervals. the pieces
are joined so that the solution has 2m     2 continuous derivatives. schoen-
berg also proved that the solution to the univariate smoothing problem (see
eq. (6.19)) is a natural polynomial spline. a common choice is m = 2, leading
to the cubic spline. one possible way of writing this solution is

1x

nx

f(x) =

  jxj +

j=0

i=1

  i(x     xi)3

+, where (x)+ =

(cid:26) x if x > 0

0 otherwise.

(6.25)

it turns out that the coe   cients    and    can be computed in time o(n) using
an algorithm due to reinsch; see green and silverman [1994, sec. 2.3.3] for
details.

splines were    rst used in regression problems. however, by using general-
ized linear modelling [mccullagh and nelder, 1983] they can be extended to
classi   cation problems and other non-gaussian likelihoods, as we did for gp
classi   cation in section 3.3. early references in this direction include silverman
[1978] and o   sullivan et al. [1986].

there is a vast literature in relation to splines in both the statistics and
numerical analysis literatures; for entry points see citations in wahba [1990]
and green and silverman [1994].

   

6.3.1 a 1-d gaussian process spline construction

in this section we will further clarify the relationship between splines and gaus-
sian processes by giving a gp construction for the solution of the univariate
cubic spline smoothing problem whose cost functional is

nx

(cid:0)f(xi)     yi

(cid:1)2 +   

z 1

(cid:0)f00(x)(cid:1)2

dx,

(6.26)

i=1

0

where the observed data are {(xi, yi)|i = 1, . . . , n, 0 < x1 <        < xn < 1} and
   is a smoothing parameter controlling the trade-o    between the    rst term, the

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6.3 spline models

139

data-   t, and the second term, the regularizer, or complexity penalty. recall
that the solution is a piecewise polynomial as in eq. (6.25).

following wahba [1978], we consider the random function

1x

j=0

g(x) =

  jxj + f(x)

(6.27)

where        n (0,   2
where

ksp(x, x0) ,

and v = min(x, x0).

0

z 1

   i) and f(x) is a gaussian process with covariance   2

f ksp(x, x0),

(x     u)+(x0     u)+ du =

|x     x0|v2

2

+ v3
3 ,

(6.28)

to complete the analogue of the regularizer in eq. (6.26), we need to remove
any penalty on polynomial terms in the null space by making the prior vague,
          . notice that the covariance has the form of
i.e. by taking the limit   2
contributions from explicit basis functions, h(x) = (1, x)> and a regular covari-
ance function ksp(x, x0), a problem which we have already studied in section 2.7.
          ,
indeed we have computed the limit where the prior becomes vague   2
the result is given in eq. (2.42).

plugging into the mean equation from eq. (2.42), we get the predictive mean

  f(x   ) = k(x   )>k   1

y (y     h>     ) + h(x   )>     ,

(6.29)

n  ij eval-
where ky is the covariance matrix corresponding to   2
uated at the training points, h is the matrix that collects the h(xi) vectors at
all training points, and      = (hk   1
y y is given below eq. (2.42).
it is not di   cult to show that this predictive mean function is a piecewise cu-
bic polynomial, since the elements of k(x   ) are piecewise3 cubic polynomials.
showing that the mean function is a    rst order polynomial in the outer intervals
[0, x1] and [xn, 1] is left as exercise 6.7.3.

y h>)   1hk   1

f ksp(xi, xj) +   2

so far ksp has been produced rather mysteriously    from the hat   ; we now
provide some explanation. shepp [1966] de   ned the l-fold integrated wiener
process as

z 1

(x     u)l

+

0

l!

z(u)du,

wl(x) =

l = 0, 1, . . .

(6.30)
where z(u) denotes the gaussian white noise process with covariance   (u    u0).
note that w0 is the standard wiener process. it is easy to show that ksp(x, x0)
is the covariance of the once-integrated wiener process by writing w1(x) and
w1(x0) using eq. (6.30) and taking the expectation using the covariance of the
white noise process. note that wl is the solution to the stochastic di   erential
equation (sde) x (l+1) = z; see appendix b for further details on sdes. thus
3the pieces are joined at the datapoints, the points where the min(x, x0) from the covari-

ance function is non-di   erentiable.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

140

relationships between gps and other models

(a), spline covariance

(b), squared exponential cov.

figure 6.1: panel (a) shows the application of the spline covariance to a simple
dataset. the full line shows the predictive mean, which is a piecewise cubic polyno-
mial, and the grey area indicates the 95% con   dence area. the two thin dashed and
dash-dotted lines are samples from the posterior. note that the posterior samples
are not as smooth as the mean. for comparison a gp using the squared exponential
covariance function is shown in panel (b). the hyperparameters in both cases were
optimized using the marginal likelihood.

the regularizerr (f00(x))2dx.

for the cubic spline we set l = 1 to obtain the sde x00 = z, corresponding to

we can also give an explicit basis-function construction for the covariance

function ksp. consider the family of random functions given by

n   1x

i=0

n   1x

i=0

fn (x) =

1   
n

  i(x     i
n

)+,

(6.31)

where    is a vector of parameters with        n (0, i). note that the sum has
the form of evenly spaced    ramps    whose magnitudes are given by the entries
in the    vector. thus

e[fn (x)fn (x0)] =

1
n

(x     i
n

)+(x0     i
n

)+.

(6.32)

taking the limit n        , we obtain eq. (6.28), a derivation which is also found
in [vapnik, 1998, sec. 11.6].

notice that the covariance function ksp given in eq. (6.28) corresponds to a
gaussian process which is ms continuous but only once ms di   erentiable. thus
samples from the prior will be quite    rough   , although (as noted in section 6.1)
the posterior mean, eq. (6.25), is smoother.

the constructions above can be generalized to the regularizerr (f (m)(x))2 dx

by replacing (x     u)+ with (x     u)m   1
eq. (6.32), and setting h(x) = (1, x, . . . , xm   1)>.

+ /(m     1)! in eq. (6.28) and similarly in

thus, we can use a gaussian process formulation as an alternative to the
usual spline    tting procedure. note that the trade-o    parameter    from eq. (6.26)

   505   4   3   2   1012input, xoutput, y   505   4   3   2   1012input, xoutput, yc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6.4 support vector machines

141

(a)

(b)

figure 6.2: panel (a) shows a linearly separable binary classi   cation problem, and a
separating hyperplane. panel (b) shows the maximum margin hyperplane.

n/  2

f and   2

f . the hyperparameters   2

n can be set
is now given as the ratio   2
using the techniques from section 5.4.1 by optimizing the marginal likelihood
given in eq. (2.45). kohn and ansley [1987] give details of an o(n) algorithm
(based on kalman    ltering) for the computation of the spline and the marginal
likelihood. in addition to the predictive mean the gp treatment also yields an
explicit estimate of the noise level and predictive error bars. figure 6.1 shows
a simple example. notice that whereas the mean function is a piecewise cubic
polynomial, samples from the posterior are not smooth. in contrast, for the
squared exponential covariance functions shown in panel (b), both the mean
and functions drawn from the posterior are in   nitely di   erentiable.

6.4 support vector machines

   

since the mid 1990   s there has been an explosion of interest in kernel machines,
and in particular the support vector machine (id166). the aim of this section
is to provide a brief introduction to id166s and in particular to compare them
to gaussian process predictors. we consider id166s for classi   cation and re-
gression problems in sections 6.4.1 and 6.4.2 respectively. more comprehensive
treatments can be found in vapnik [1995], cristianini and shawe-taylor [2000]
and sch  olkopf and smola [2002].

6.4.1 support vector classi   cation

for support vector classi   ers, the key notion that we need to introduce is that
of the maximum margin hyperplane for a linear classi   er. then by using the
   kernel trick    this can be lifted into feature space. we consider    rst the sep-
arable case and then the non-separable case. we conclude this section with a
comparison between gp classi   ers and id166s.

xixj               w  x+w0<0w  x+w0>0.               wmarginxixj.c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

142

relationships between gps and other models

the separable case

figure 6.2(a) illustrates the case where the data is linearly separable. for a
linear classi   er with weight vector w and o   set w0, let the decision boundary
be de   ned by w    x + w0 = 0, and let   w = (w, w0). clearly, there is a whole
version space of weight vectors that give rise to the same classi   cation of the
training points. the id166 algorithm chooses a particular weight vector, that
gives rise to the    maximum margin    of separation.

functional margin

geometrical margin

let the training set be pairs of the form (xi, yi) for i = 1, . . . , n, where yi =
  1. for a given weight vector we can compute the quantity     i = yi(w   x + w0),
which is known as the functional margin. notice that     i > 0 if a training point
is correctly classi   ed.

if the equation f(x) = w    x + w0 de   nes a discriminant function (so that
the output is sgn(f(x))), then the hyperplane cw    x + cw0 de   nes the same
discriminant function for any c > 0. thus we have the freedom to choose the
scaling of   w so that mini     i = 1, and in this case   w is known as the canonical
form of the hyperplane.

the geometrical margin is de   ned as   i =     i/|w|. for a training point xi
that is correctly classi   ed this is simply the distance from xi to the hyperplane.
to see this, let c = 1/|w| so that   w = w/|w| is a unit vector in the direction
of w, and   w0 is the corresponding o   set. then   w    x computes the length
of the projection of x onto the direction orthogonal to the hyperplane and
  w  x+   w0 computes the distance to the hyperplane. for training points that are
misclassi   ed the geometrical margin is the negative distance to the hyperplane.
the geometrical margin for a dataset d is de   ned as   d = mini   i. thus
for a canonical separating hyperplane the margin is 1/|w|. we wish to    nd the
maximum margin hyperplane, i.e. the one that maximizes   d.

optimization problem

by considering canonical hyperplanes, we are thus led to the following op-

timization problem to determine the maximum margin hyperplane:

|w|2

1
2

minimize
subject to yi(w    xi + w0)     1

over w, w0

for all i = 1, . . . , n.

(6.33)

it is clear by considering the geometry that for the maximum margin solution
there will be at least one data point in each class for which yi(w  xi+w0) = 1, see
figure 6.2(b). let the hyperplanes that pass through these points be denoted
h+ and h    respectively.

this constrained optimization problem can be set up using lagrange multi-
pliers, and solved using numerical methods for quadratic programming4 (qp)
problems. the form of the solution is

4a quadratic programming problem is an optimization problem where the objective func-

tion is quadratic and the constraints are linear in the unknowns.

i

  iyixi,

(6.34)

w = x

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6.4 support vector machines

143

support vectors

kernel trick

soft margin

where the   i   s are non-negative lagrange multipliers. notice that the solution
is a linear combination of the xi   s.

the key feature of equation 6.34 is that   i is zero for every xi except those
which lie on the hyperplanes h+ or h   ; these points are called the support
vectors. the fact that not all of the training points contribute to the    nal
solution is referred to as the sparsity of the solution. the support vectors
lie closest to the decision boundary. note that if all of the other training
points were removed (or moved around, but not crossing h+ or h   ) the same
maximum margin hyperplane would be found. the quadratic programming
problem for    nding the   i   s is convex, i.e. there are no local minima. notice
the similarity of this to the convexity of the optimization problem for gaussian
process classi   ers, as described in section 3.4.

to make predictions for a new input x    we compute

sgn(w    x    + w0) = sgn

  iyi(xi    x   ) + w0

(cid:16) nx

(cid:17)

.

(6.35)

i=1

in the qp problem and in eq. (6.35) the training points {xi} and the test point
x    enter the computations only in terms of inner products. thus by using the
kernel trick we can replace occurrences of the inner product by the kernel to
obtain the equivalent result in feature space.

the non-separable case

for linear classi   ers in the original x space there will be some datasets that
are not linearly separable. one way to generalize the id166 problem in this
case is to allow violations of the constraint yi(w    xi + w0)     1 but to impose a
penalty when this occurs. this leads to the soft margin support vector machine
problem, the minimization of

nx

i=1

|w|2 + c

1
2

(1     yifi)+

(6.36)

with respect to w and w0, where fi = f(xi) = w    xi + w0 and (z)+ = z if
z > 0 and 0 otherwise. here c > 0 is a parameter that speci   es the relative
importance of the two terms. this id76 problem can again be
solved using qp methods and yields a solution of the form given in eq. (6.34). in
this case the support vectors (those with   i 6= 0) are not only those data points
which lie on the separating hyperplanes, but also those that incur penalties.
this can occur in two ways (i) the data point falls in between h+ and h    but
on the correct side of the decision surface, or (ii) the data point falls on the
wrong side of the decision surface.

in a feature space of dimension n, if n > n then there will always be
separating hyperplane. however, this hyperplane may not give rise to good
generalization performance, especially if some of the labels are incorrect, and
thus the soft margin id166 formulation is often used in practice.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

144

relationships between gps and other models

(a)

(b)

figure 6.3: (a) a comparison of the hinge error, g   and g  . (b) the  -insensitive
error function used in svr.

for both the hard and soft margin id166 qp problems a wide variety of
algorithms have been developed for their solution; see sch  olkopf and smola
[2002, ch. 10] for details. basic interior point methods involve inversions of n  n
matrices and thus scale as o(n3), as with gaussian process prediction. however,
there are other algorithms, such as the sequential minimal optimization (smo)
algorithm due to platt [1999], which often have better scaling in practice.

above we have described id166s for the two-class (binary) classi   cation prob-
lem. there are many ways of generalizing id166s to the multi-class problem,
see sch  olkopf and smola [2002, sec. 7.6] for further details.

comparing support vector and gaussian process classi   ers

for the soft margin classi   er we obtain a solution of the form w = p
(with   i =   iyi) and thus |w|2 =p

i   ixi
i,j   i  j(xi    xj). kernelizing this we obtain
|w|2 =   >k   = f>k   1f, as5 k   = f. thus the soft margin objective
nx
function can be written as

f>k   1f + c

(1     yifi)+.

(6.37)

1
2

1
2

i=1

f>k   1f     nx

i=1

for the binary gp classi   er, to obtain the map value   f of p(f|y) we minimize
the quantity

log p(yi|fi),

(6.38)

cf. eq. (3.12). (the    nal two terms in eq. (3.12) are constant if the kernel is
   xed.)

for log-concave likelihoods (such as those derived from the logistic or pro-
bit response functions) there is a strong similarity between the two optimiza-
tion problems in that they are both convex. let g  (z) , log(1 + e   z), g   =
5here the o   set w0 has been absorbed into the kernel so it is not an explicit extra param-

eter.

   2014012log(1 + exp(   z))   log f(z)max(1   z, 0)zg (z)    0    .c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6.4 support vector machines

145

hinge error function

    log   (z), and ghinge(z) , (1     z)+ where z = yifi. we refer to ghinge as the
hinge error function, due to its shape. as shown in figure 6.3(a) all three data
   t terms are monotonically decreasing functions of z. all three functions tend
to in   nity as z            and decay to zero as z        . the key di   erence is that
the hinge function takes on the value 0 for z     1, while the other two just decay
slowly. it is this    at part of the hinge function that gives rise to the sparsity of
the id166 solution.

thus there is a close correspondence between the map solution of the gp
classi   er and the id166 solution. can this correspondence be made closer by
considering the hinge function as a negative log likelihood? the answer to this
is no [seeger, 2000, sollich, 2002]. if cghinge(z) de   ned a negative log likelihood,
then exp(   cghinge(f)) + exp(   cghinge(   f)) should be a constant independent
of f, but this is not the case. to see this, consider the quantity

  (f; c) =   (c)[exp(   c(1     f)+) + exp(   c(1 + f)+)].

(6.39)

  (c) cannot be chosen so as to make   (f; c) = 1 independent of the value
of f for c > 0. by comparison, for the logistic and probit likelihoods the
analogous expression is equal to 1. sollich [2002] suggests choosing   (c) =
1/(1 + exp(   2c)) which ensures that   (f, c)     1 (with equality only when
f =   1). he also gives an ingenious interpretation (involving a    don   t know   
class to soak up the unassigned id203 mass) that does yield the id166
solution as the map solution to a certain bayesian problem, although we    nd
this construction rather contrived. exercise 6.7.2 invites you to plot   (f; c) as
a function of f for various values of c.

one attraction of the gp classi   er is that it produces an output with a
clear probabilistic interpretation, a prediction for p(y = +1|x). one can try
to interpret the function value f(x) output by the id166 probabilistically, and
platt [2000] suggested that probabilistic predictions can be generated from the
id166 by computing   (af(x) + b) for some constants a, b that are    tted using
some    unbiased version    of the training set (e.g. using cross-validation). one
disadvantage of this rather ad hoc procedure is that unlike the gp classi   ers
it does not take into account the predictive variance of f(x) (cf. eq. (3.25)).
seeger [2003, sec. 4.7.2] shows that better error-reject curves can be obtained
on an experiment using the mnist digit classi   cation problem when the e   ect
of this uncertainty is taken into account.

6.4.2 support vector regression

the id166 was originally introduced for the classi   cation problem, then extended
to deal with the regression case. the key concept is that of the  -insensitive
error function. this is de   ned as

(cid:26) |z|      

0

if |z|      ,
otherwise.

g (z) =

(6.40)

this function is plotted in figure 6.3(b). as in eq. (6.21) we can interpret
exp(   g (z)) as a likelihood model for the regression residuals (c.f. the squared

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

146

relationships between gps and other models

error function corresponding to a gaussian model). however, we note that
this is quite an unusual choice of model for the distribution of residuals and
is basically motivated by the desire to obtain a sparse solution (see below)
as in support vector classi   er.
if   = 0 then the error model is a laplacian
distribution, which corresponds to least absolute values regression (edgeworth
[1887], cited in rousseeuw [1984]); this is a heavier-tailed distribution than the
gaussian and provides some protection against outliers. girosi [1991] showed
that the laplacian distribution can be viewed as a continuous mixture of zero-
mean gaussians with a certain distribution over their variances. pontil et al.
[1998] extended this result by allowing the means to uniformly shift in [    ,  ]
in order to obtain a probabilistic model corresponding to the  -insensitive error
function. see also section 9.3 for work on robusti   cation of the gp regression
problem.

for the id75 case with an  -insensitive error function and a

gaussian prior on w, the map value of w is obtained by minimizing

g (yi     fi)

(6.41)

nx
w.r.t. w. the solution6 is f(x   ) =pn
solution f(x   ) =pn

i=1   ik(xi, x   ).

|w|2 + c

1
2

i=1

i=1   ixi    x    where the coe   cients    are
obtained from a qp problem. the problem can also be kernelized to give the

as for support vector classi   cation, many of the coe   cients   i are zero. the
data points which lie inside the  -   tube    have   i = 0, while those on the edge
or outside have non-zero   i.

   

6.5 least-squares classi   cation

in chapter 3 we have argued that the use of logistic or probit likelihoods pro-
vides the natural route to develop a gp classi   er, and that it is attractive in
that the outputs can be interpreted probabilistically. however, there is an even
simpler approach which treats classi   cation as a regression problem.

our starting point is binary classi   cation using the linear predictor f(x) =
w>x. this is trained using id75 with a target y+ for patterns that
have label +1, and target y    for patterns that have label    1. (targets y+, y   
give slightly more    exibility than just using targets of   1.) as shown in duda
and hart [1973, section 5.8], choosing y+, y    appropriately allows us to obtain
the same solution as fisher   s linear discriminant using the decision criterion
f(x)     0. also, they show that using targets y+ = +1, y    =    1 with the
least-squares error function gives a minimum squared-error approximation to
the bayes discriminant function p(c+|x)    p(c   |x) as n        . following rifkin
and klautau [2004] we call such methods least-squares classi   cation (lsc). note
that under a probabilistic interpretation the squared-error criterion is rather an

6here we have assumed that the constant 1 is included in the input vector x.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6.5 least-squares classi   cation

147

odd choice as it implies a gaussian noise model, yet only two values of the
target (y+ and y   ) are observed.

it is natural to extend the least-squares classi   er using the kernel trick.
this has been suggested by a number of authors including poggio and girosi
[1990] and suykens and vanderwalle [1999]. experimental results reported in
rifkin and klautau [2004] indicate that performance comparable to id166s can
be obtained using kernel lsc (or as they call it the regularized least-squares
classi   er, rlsc).

consider a single random variable y which takes on the value +1 with proba-
bility p and value    1 with id203 1   p. then the value of f which minimizes
the squared error function e = p(f     1)2 + (1     p)(f + 1)2 is   f = 2p     1, which
is a linear rescaling of p to the interval [   1, 1]. (equivalently if the targets are
1 and 0, we obtain   f = p.) hence we observe that lsc will estimate p correctly
in the large data limit. if we now consider not just a single random variable,
but wish to estimate p(c+|x) (or a linear rescaling of it), then as long as the
approximating function f(x) is su   ciently    exible, we would expect that in the
limit n         it would converge to p(c+|x). (for more technical detail on this
issue, see section 7.2.1 on consistency.) hence lsc is quite a sensible procedure
for classi   cation, although note that there is no guarantee that f(x) will be
constrained to lie in the interval [y   , y+].
if we wish to guarantee a proba-
bilistic interpretation, we could    squash    the predictions through a sigmoid, as
suggested for id166s by platt [2000] and described on page 145.

when generalizing from the binary to multi-class situation there is some
freedom as to how to set the problem up. sch  olkopf and smola [2002, sec. 7.6]
identify four methods, namely one-versus-rest (where c binary classi   ers are
trained to classify each class against all the rest), all pairs (where c(c     1)/2
binary classi   ers are trained), error-correcting output coding (where each class
is assigned a binary codeword, and binary classi   ers are trained on each bit
separately), and multi-class objective functions (where the aim is to train c
classi   ers simultaneously rather than creating a number of binary classi   cation
problems). one also needs to specify how the outputs of the various classi   ers
that are trained are combined so as to produce an overall answer. for the
one-versus-rest7 method one simple criterion is to choose the classi   er which
produces the most positive output. rifkin and klautau [2004] performed ex-
tensive experiments and came to the conclusion that the one-versus-rest scheme
using either id166s or rlsc is as accurate as any other method overall, and
has the merit of being conceptually simple and straightforward to implement.

6.5.1 probabilistic least-squares classi   cation

the lsc algorithm discussed above is attractive from a computational point
of view, but to guarantee a valid probabilistic interpretation one may need
to use a separate post-processing stage to    squash    the predictions through a
sigmoid. however, it is not so easy to enforce a probabilistic interpretation

7this method is also sometimes called one-versus-all.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

148

relationships between gps and other models

during the training stage. one possible solution is to combine the ideas of
training using leave-one-out cross-validation, covered in section 5.4.2, with the
use of a (parameterized) sigmoid function, as in platt [2000]. we will call this
method the probabilistic least-squares classi   er (plsc).

in section 5.4.2 we saw how to compute the gaussian leave-one-out (loo)
predictive probabilities, and that training of hyperparameters can be based
on the sum of the log loo probabilities. using this idea, we express the loo
id203 by squashing a linear function of the gaussian predictive id203
through a cumulative gaussian
p(yi|x, y   i,   ) =

  (cid:0)yi(  fi +   )(cid:1)n (fi|  i,   2
(cid:16) yi(    i +   )

(6.42)

i ) dfi

(cid:17)

z

=   

   

,

1 +   2  2
i

   lloo

     j

=

=

nx
nx

i=1

i=1

where the integral is given in eq. (3.82) and the leave-one-out predictive mean   i
and variance   2
i are given in eq. (5.12). the objective function is the sum of the
log loo probabilities, eq. (5.11) which can be used to set the hyperparameters
as well as the two additional parameters of the linear transformation,    and    in
eq. (6.42). introducing the likelihood in eq. (6.42) into the objective eq. (5.11)
and taking derivatives we obtain

    log p(yi|x, y   i,   )

     i
yi     
1 +   2  2
i

n (ri)
  (yiri)
   

     i
     j

(cid:16)      i

     j

+     log p(yi|x, y   i,   )
(cid:17)

     2
i
      (    i +   )
2(1 +   2  2
i )

     2
i
     j

,

     2
i
     j

(6.43)

1 +   2  2
where ri = (    i +   )/
loo parameters      i/     j and      2
linear transformation parameters we have

i and the partial derivatives of the gaussian
i /     j are given in eq. (5.13). finally, for the

nx
nx

i=1

i=1

   lloo

     

   lloo

     

=

=

n (ri)
  (yiri)
n (ri)
  (yiri)

yi   
1 +   2  2
i

yip1 +   2  2

i

  i           2
i
1 +   2  2
i

,

.

(6.44)

these partial derivatives can be used to train the parameters of the gp. there
are several options on how to do predictions, but the most natural would seem
to be to compute predictive mean and variance and squash it through the
sigmoid, parallelling eq. (6.42). applying this model to the usps 3s vs. 5s
binary classi   cation task discussed in section 3.7.3, we get a test set error rate
of 12/773 = 0.0155%, which compares favourably with the results reported for
other methods in figure 3.10. however, the test set information is only 0.77
bits,8 which is very poor.

8the test information is dominated by a single test case, which is predicted con   dently
to belong to the wrong class. visual inspection of the digit reveals that indeed it looks as
though the testset label is wrong for this case. this observation highlights the danger of not
explicitly allowing for data mislabelling in the model for this kind of data.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

6.6 relevance vector machines

149

6.6 relevance vector machines

   

although usually not presented as such, the relevance vector machine (rvm)
introduced by tipping [2001] is actually a special case of a gaussian process.
the covariance function has the form

nx

j=1

k(x, x0) =

  j(x)  j(x0),

1
  j

(6.45)

where   j are hyperparameters and the n basis functions   j(x) are usually, but
not necessarily taken to be gaussian-shaped basis functions centered on each
of the n training data points

(cid:16)    |x     xj|2

(cid:17)

2   2

  j(x) = exp

,

(6.46)

where     is a length-scale hyperparameter controlling the width of the basis
function. notice that this is simply the construction for the covariance function
corresponding to an n-dimensional set of basis functions given in section 2.1.2,
with   p = diag(     1

1 , . . . ,      1
n ).

the covariance function in eq. (6.45) has two interesting properties:    rstly,
it is clear that the feature space corresponding to the covariance function is
   nite dimensional, i.e. the covariance function is degenerate, and secondly the
covariance function has the odd property that it depends on the training data.
this dependency means that the prior over functions depends on the data, a
property which is at odds with a strict bayesian interpretation. although the
usual treatment of the model is still possible, this dependency of the prior on
the data may lead to some surprising e   ects, as discussed below.

training the rvm is analogous to other gp models: optimize the marginal
likelihood w.r.t. the hyperparameters. this optimization often leads to a sig-
ni   cant number of the   j hyperparameters tending towards in   nity, e   ectively
removing, or pruning, the corresponding basis function from the covariance
function in eq. (6.45). the basic idea is that basis functions that are not sig-
ni   cantly contributing to explaining the data should be removed, resulting in
a sparse model. the basis functions that survive are called relevance vectors.
empirically it is often observed that the number of relevance vectors is smaller
than the number of support vectors on the same problem [tipping, 2001].

the original rvm algorithm [tipping, 2001] was not able to exploit the
sparsity very e   ectively during model    tting as it was initialized with all of the
  is set to    nite values, meaning that all of the basis functions contributed to
the model. however, careful analysis of the rvm marginal likelihood by faul
and tipping [2002] showed that one can carry out optimization w.r.t. a single
  i analytically. this has led to the accelerated training algorithm described
in tipping and faul [2003] which starts with an empty model (i.e. all   is set
to in   nity) and adds basis functions sequentially. as the number of relevance
vectors is (usually much) less than the number of training cases it will often
be much faster to train and make predictions using a rvm than a non-sparse

relevance vectors

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

150

relationships between gps and other models

gp. also note that the basis functions can include additional hyperparameters,
e.g. one could use an automatic relevance determination (ard) form of basis
function by using di   erent length-scales on di   erent dimensions in eq. (6.46).
these additional hyperparameters could also be set by optimizing the marginal
likelihood.

the use of a degenerate covariance function which depends on the data
imagine a test point, x   , which lies far away
has some undesirable e   ects.
from the relevance vectors. at x    all basis functions will have values close to
zero, and since no basis function can give any appreciable signal, the predictive
distribution will be a gaussian with a mean close to zero and variance close
to zero (or to the inferred noise level). this behaviour is undesirable, and
could lead to dangerously false conclusions. if the x    is far from the relevance
vectors, then the model shouldn   t be able to draw strong conclusions about
the output (we are extrapolating), but the predictive uncertainty becomes very
small   this is the opposite behaviour of what we would expect from a reasonable
model. here, we have argued that for localized basis functions, the rvm has
undesirable properties, but as argued in rasmussen and qui  nonero-candela
[2005] it is actually the degeneracy of the covariance function which is the
core of the problem. although the work of rasmussen and qui  nonero-candela
[2005] goes some way towards    xing the problem, there is an inherent con   ict:
degeneracy of the covariance function is good for computational reasons, but
bad for modelling reasons.

6.7 exercises

sis vectors f = pn
pn

1. we motivate the fact that the rkhs norm does not depend on the den-
sity p(x) using a    nite-dimensional analogue. consider the n-dimensional
vector f, and let the n   n matrix    be comprised of non-colinear columns
  1, . . . ,   n. then f can be expressed as a linear combination of these ba-
i=1 ci  i =   c for some coe   cients {ci}. let the   s
be eigenvectors of the covariance matrix k w.r.t. a diagonal matrix p
with non-negative entries, so that kp    =     , where    is a diagonal
matrix containing the eigenvalues. note that   >p    = in. show that
i /  i = c>     1c = f>k   1f, and thus observe that f>k   1f can be
expressed as c>     1c for any valid p and corresponding   . hint: you
may    nd it useful to set      = p 1/2  ,   k = p 1/2kp 1/2 etc.

i=1 c2

2. plot eq. (6.39) as a function of f for di   erent values of c. show that
there is no value of c and   (c) which makes   (f; c) equal to 1 for all
values of f. try setting   (c) = 1/(1 + exp(   2c)) as suggested in sollich
[2002] and observe what e   ect this has.

3. show that the predictive mean for the spline covariance gp in eq. (6.29)
is a linear function of x    when x    is located either to the left or to the
right of all training points. hint: consider the eigenvectors corresponding
to the two largest eigenvalues of the training set covariance matrix from
eq. (2.40) in the vague limit.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

chapter 7

theoretical perspectives

this chapter covers a number of more theoretical issues relating to gaussian
processes. in section 2.6 we saw how gpr carries out a linear smoothing of the
datapoints using the weight function. the form of the weight function can be
understood in terms of the equivalent kernel, which is discussed in section 7.1.
as one gets more and more data, one would hope that the gp predictions
would converge to the true underlying predictive distribution. this question
of consistency is reviewed in section 7.2, where we also discuss the concepts of
equivalence and orthogonality of gps.

when the generating process for the data is assumed to be a gp it is particu-
larly easy to obtain results for learning curves which describe how the accuracy
of the predictor increases as a function of n, as described in section 7.3. an
alternative approach to the analysis of generalization error is provided by the
pac-bayesian analysis discussed in section 7.4. here we seek to relate (with
high id203) the error observed on the training set to the generalization
error of the gp predictor.

gaussian processes are just one of the many methods that have been devel-
oped for supervised learning problems. in section 7.5 we compare and contrast
gp predictors with other supervised learning methods.

7.1 the equivalent kernel

in this section we consider regression problems. we have seen in section 6.2
that the posterior mean for gp regression can be obtained as the function which
minimizes the functional

j[f] =

kfk2h +

1
2

1
2  2
n

(cid:0)yi     f(xi)(cid:1)2

nx

i=1

,

(7.1)

where kfkh is the rkhs norm corresponding to kernel k. our goal is now to
understand the behaviour of this solution as n        .

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

152

theoretical perspectives

let   (x, y) be the id203 measure from which the data pairs (xi, yi) are

generated. observe that

eh nx

(cid:0)yi     f(xi)(cid:1)2i

z (cid:0)y     f(x)(cid:1)2

= n

d  (x, y).

(7.2)

z (cid:0)  (x)     f(x)(cid:1)2

i=1

let   (x) = e[y|x] be the regression function corresponding to the id203
then writing y     f = (y       ) + (       f) we obtain

measure   . the variance around   (x) is denoted   2(x) =r (y       (x))2d  (y|x).
z (cid:0)y     f(x)(cid:1)2

z (cid:0)  (x)     f(x)(cid:1)2

  2(x) d  (x),

d  (x, y) =

d  (x) +

(7.3)

z

as the cross term vanishes due to the de   nition of   (x).

as the second term on the right hand side of eq. (7.3) is independent of f,
an idealization of the regression problem consists of minimizing the functional

j  [f] = n
2  2
n

d  (x) +

kfk2h.

1
2

(7.4)

the form of the minimizing solution is most easily understood in terms of the

eigenfunctions {  i(x)} of the kernel k w.r.t. to   (x), wherer   i(x)  j(x)d  (x) =
form a complete orthonormal basis, we write f(x) =p   
i=1   i  i(x), where   i =r   (x)  i(x)d  (x). thus
  (x) =p   
   x

  ij, see section 4.3. assuming that the kernel is nondegenerate so that the   s
i=1 fi  i(x). similarly,

   x

j  [f] = n
2  2
n

i=1

(  i     fi)2 +

1
2

i=1

f 2
i
  i

.

(7.5)

this is readily minimized by di   erentiation w.r.t. each fi to obtain

fi =

  i
  i +   2

n/n

  i.

(7.6)

the generalized fourier seriesp   

n/n     0 as n         so that in this limit we would
notice that the term   2
expect that f(x) will converge to   (x). there are two caveats: (1) we have
assumed that   (x) is su   ciently well-behaved so that it can be represented by
i=1   i  i(x), and (2) we assumed that the kernel
is nondegenerate. if the kernel is degenerate (e.g. a polynomial kernel) then f
should converge to the best   -weighted l2 approximation to    within the span
of the      s. in section 7.2.1 we will say more about rates of convergence of f to
  ; clearly in general this will depend on the smoothness of   , the kernel k and
the measure   (x, y).

from a bayesian perspective what is happening is that the prior on f is
being overwhelmed by the data as n        . looking at eq. (7.6) we also see
n (cid:29) n  i then fi is e   ectively zero. this means that we cannot    nd
that if   2
out about the coe   cients of eigenfunctions with small eigenvalues until we get
su   cient amounts of data. ferrari trecate et al. [1999] demonstrated this by

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

7.1 the equivalent kernel

153

showing that regression performance of a certain nondegenerate gp could be
approximated by taking the    rst m eigenfunctions, where m was chosen so that
  m       2

using the fact that   i = r   (x0)  i(x0)d  (x0) and de   ning   2

n/n. of course as more data is obtained then m has to be increased.

,   2

n/n we

e   

obtain

   x

i=1

f(x) =

  i  i
  i +   2
e   

  i(x) =

z h    x

i=1

i

  i  i(x)  i(x0)

  i +   2
e   

  (x0) d  (x0).

(7.7)

equivalent kernel

wiener    ltering

z

with   f(x0) = r hn(x, x0)y(x)d  (x). notice that in the limit n         (so that

the term in square brackets in eq. (7.7) is the equivalent kernel for the smooth-
ing problem; we denote it by hn(x, x0). notice the similarity to the vector-valued
weight function h(x) de   ned in section 2.6. the di   erence is that there the pre-
diction was obtained as a linear combination of a    nite number of observations
yi with weights given by hi(x) while here we have a noisy function y(x) instead,
e        0) the equivalent kernel tends towards the delta function.
  2
the form of the equivalent kernel given in eq. (7.7) is not very useful in
practice as it requires knowledge of the eigenvalues/functions for the combina-
tion of k and   . however, in the case of stationary kernels we can use fourier
methods to compute the equivalent kernel. consider the functional

j  [f] =   
2  2
n

(y(x)     f(x))2 dx +

kfk2h,

1
2

(7.8)

where    has dimensions of the number of observations per unit of x-space
(length/area/volume etc. as appropriate). using a derivation similar to eq. (7.6)
we obtain

  h(s) =

sf (s)
sf (s) +   2

n/  

=

1
1 + s   1

f (s)  2

n/  

,

(7.9)

where sf (s) is the power spectrum of the kernel k. the term   2
n/   corresponds
to the power spectrum of a white noise process, as the delta function covari-
ance function of white noise corresponds to a constant in the fourier domain.
this analysis is known as wiener    ltering; see, e.g. papoulis [1991, sec. 14-1].
equation (7.9) is the same as eq. (7.6) except that the discrete eigenspectrum
has been replaced by a continuous one.

as can be observed in figure 2.6, the equivalent kernel essentially gives a
weighting to the observations locally around x. thus identifying    with np(x)
we can obtain an approximation to the equivalent kernel for stationary kernels
when the width of the kernel is smaller than the length-scale of variations in
p(x). this form of analysis was used by silverman [1984] for splines in one
dimension.

7.1.1 some speci   c examples of equivalent kernels
we    rst consider the ou process in 1-d. this has k(r) = exp(     |r|) (setting
   = 1/    relative to our previous notation and r = x     x0), and power spectrum

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

154

theoretical perspectives

s(s) = 2  /(4  2s2 +   2). let vn ,   2

n/  . using eq. (7.9) we obtain

  h(s) =

2  

vn(4  2s2 +   2) ,

(7.10)

where   2 =   2 + 2  /vn. this again has the form of fourier transform of an
vn   e     |r|. in
ou covariance function1 and can be inverted to obtain h(r) =   
particular notice that as n increases (and thus vn decreases) the inverse length-
scale    of h(r) increases; asymptotically        n1/2 for large n. this shows that
the width of equivalent kernel for the ou covariance function will scale as n   1/2
asymptotically. similarly the width will scale as p(x)   1/2 asymptotically.

a similar analysis can be carried out for the ar(2) gaussian process in 1-d
(see section b.2) which has a power spectrum     (4  2s2 +   2)   2 (i.e. it is in
the mat  ern class with    = 3/2). in this case we can show (using the fourier
relationships given by papoulis [1991, p. 326]) that the width of the equivalent
kernel scales as n   1/4 asymptotically.

analyzes end-e   ects if the domain of interest is a bounded open interval. for

analysis of the equivalent kernel has also been carried out for spline models.
silverman [1984] gives the explicit form of the equivalent kernel in the case
of a one-dimensional cubic spline (corresponding to the regularizer kp fk2 =
00)2dx). thomas-agnan [1996] gives a general expression for the equivalent

r (f
kernel for the spline regularizer kp fk2 =r (f (m))2dx in one dimension and also
the regularizer kp fk2 =r (   2f)2dx in two dimensions, the equivalent kernel is
sponding to a roughness penalty of r (f (m))2 dx) the width of the equivalent

silverman [1984] has also shown that for splines of order m in 1-d (corre-
kernel will scale as n   1/2m asymptotically. in fact it can be shown that this is
true for splines in d > 1 dimensions too, see exercise 7.7.1.

given in terms of the kelvin function kei (poggio et al. 1985, stein 1991).

another interesting case to consider is the squared exponential kernel, where

s(s) = (2     2)d/2 exp(   2  2   2|s|2). thus

  hse(s) =

1

1 + b exp(2  2   2|s|2) ,

(7.11)

where b =   2
n/  (2     2)d/2. we are unaware of an exact result in this case, but
the following approximation due to sollich and williams [2005] is simple but
e   ective. for large    (i.e. large n) b will be small. thus for small s = |s| we
have that   hse     1, but for large s it is approximately 0. the change takes
place around the point sc where b exp(2  2   2s2
c = log(1/b)/2  2   2.
as exp(2  2   2s2) grows quickly with s, the transition of   hse between 1 and 0
can be expected to be rapid, and thus be well-approximated by a step function.
by using the standard result for the fourier transform of the step function we
obtain

c) = 1, i.e. s2

hse(x) = 2scsinc(2  scx)

(7.12)

1the fact that   h(s) has the same form as sf (s) is particular to the ou covariance function

and is not generally the case.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

7.2 asymptotic analysis

155

for d = 1, where sinc(z) = sin(z)/z. a similar calculation in d > 1 using
eq. (4.7) gives

hse(r) =

jd/2(2  scr).

(7.13)

(cid:16) sc

(cid:17)d/2

r

notice that sc scales as (log(n))1/2 so that the width of the equivalent kernel
will decay very slowly as n increases. notice that the plots in figure 2.6 show
the sinc-type shape, although the sidelobes are not quite as large as would be
predicted by the sinc curve (because the transition is smoother than a step
function in fourier space so there is less    ringing   ).

7.2 asymptotic analysis

   

in this section we consider two asymptotic properties of gaussian processes,
consistency and equivalence/orthogonality.

7.2.1 consistency

in section 7.1 we have analyzed the asymptotics of gp regression and have
seen how the minimizer of the functional eq. (7.4) converges to the regression
function as n        . we now broaden the focus by considering id168s
other than squared loss, and the case where we work directly with eq. (7.1)
rather than the smoothed version eq. (7.4).

the set up is as follows: let l(  ,  ) be a pointwise id168. consider
a procedure that takes training data d and this id168, and returns a
function fd(x). for a measurable function f, the risk (expected loss) is de   ned
as

z

rl(f) =

l(y, f(x)) d  (x, y).

(7.14)

l denote the function that minimizes this risk. for squared loss f   

let f   
l(x) =
e[y|x]. for 0/1 loss with classi   cation problems, we choose f   
l(x) to be the
class c at x such that p(cc|x) > p(cj|x) for all j 6= c (breaking ties arbitrarily).

de   nition 7.1 we will say that a procedure that returns fd is consistent for
a given measure   (x, y) and id168 l if

consistency

rl(fd)     rl(f   
l)

as n        ,

(7.15)

where convergence is assessed in a suitable manner, e.g. in id203. if fd(x)
is consistent for all borel id203 measures   (x, y) then it is said to be uni-
(cid:3)
versally consistent.

ing   f(x   ) =pn

a simple example of a consistent procedure is the kernel regression method.
as described in section 2.6 one obtains a prediction at test point x    by comput-
j=1  j (the nadaraya-watson estima-
tor). let h be the width of the kernel    and d be the dimension of the input

i=1 wiyi where wi =   i/pn

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

156

theoretical perspectives

space. it can be shown that under suitable regularity conditions if h     0 and
nhd         as n         then the procedure is consistent; see e.g. [gy  or    et al.,
2002, theorem 5.1] for the regression case with squared loss and devroye et al.
[1996, theorem 10.1] for the classi   cation case using 0/1 loss. an intuitive
understanding of this result can be obtained by noting that h     0 means that
only datapoints very close to x    will contribute to the prediction (eliminating
bias), while the condition nhd         means that a large number of datapoints
will contribute to the prediction (eliminating noise/variance).

combination of eigenfunctionsp   

it will    rst be useful to consider why we might hope that gpr and gpc
should be universally consistent. as discussed in section 7.1, the key property
is that a non-degenerate kernel will have an in   nite number of eigenfunctions
forming an orthonormal set. thus from generalized fourier analysis a linear
i=1 ci  i(x) should be able to represent a suf-
   ciently well-behaved target function f   
l. however, we have to estimate the
in   nite number of coe   cients {ci} from the noisy observations. this makes it
clear that we are playing a game involving in   nities which needs to be played
with care, and there are some results [diaconis and freedman, 1986, freedman,
1999, gr  unwald and langford, 2004] which show that in certain circumstances
bayesian id136 in in   nite-dimensional objects can be inconsistent.

however, there are some positive recent results on the consistency of gpr
and gpc. choudhuri et al. [2005] show that for the binary classi   cation case
under certain assumptions gpc is consistent. the assumptions include smooth-
ness on the mean and covariance function of the gp, smoothness on e[y|x] and
an assumption that the domain is a bounded subset of rd. their result holds
for the class of response functions which are c.d.f.s of a unimodal symmetric
density; this includes the probit and logistic functions.

for gpr, choi and schervish [2004] show that for a one-dimensional input
space of    nite length under certain assumptions consistency holds. here the
assumptions again include smoothness of the mean and covariance function of
the gp and smoothness of e[y|x]. an additional assumption is that the noise
has a normal or laplacian distribution (with an unknown variance which is
inferred).

there are also some consistency results relating to the functional

j  n[f] =   n
2

kfk2h +

1
n

l(cid:0)yi, f(xi)(cid:1),

nx

i=1

(7.16)

m =pm

where   n     0 as n        . note that to agree with our previous formulations
we would set   n = 1/n, but other decay rates on   n are often considered.

in the splines literature, cox [1984] showed that for regression problems us-
ing the regularizer kfk2
k=0 kokfk2 (using the de   nitions in eq. (6.10))
consistency can be obtained under certain technical conditions. cox and o   sulli-
van [1990] considered a wide range of problems (including regression problems
with squared loss and classi   cation using logistic loss) where the solution is
obtained by minimizing the regularized risk using a spline smoothness term.
l     h (where h is the rkhs corresponding to the spline
they showed that if f   

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

7.2 asymptotic analysis

157

regularizer) then as n         and   n     0 at an appropriate rate, one gets
convergence of fd to f   
l.

more recently, zhang [2004, theorem 4.4] has shown that for the classi   ca-
tion problem with a number of di   erent id168s (including logistic loss,
hinge loss and quadratic loss) and for general rkhss with a nondegenerate
kernel, that if   n     0,   nn         and   (x, y) is su   ciently regular then the
classi   cation error of fd will converge to the bayes optimal error in id203
as n        . similar results have also been obtained by steinwart [2005] with
various rates on the decay of   n depending on the smoothness of the kernel.
bartlett et al. [2003] have characterized the id168s that lead to universal
consistency.

above we have focussed on regression and classi   cation problems. however,
similar analyses can also be given for other problems such as density estimation
and deconvolution; see wahba [1990, chs. 8, 9] for references. also we have
discussed consistency using a    xed decay rate for   n. however, it is also possible
to analyze the asymptotics of methods where   n is set in a data-dependent way,
e.g. by cross-validation;2 see wahba [1990, sec. 4.5] and references therein for
further details.

consistency is evidently a desirable property of supervised learning proce-
dures. however, it is an asymptotic property that does not say very much about
how a given prediction procedure will perform on a particular problem with a
given dataset. for instance, note that we only required rather general prop-
erties of the id81 (e.g. non-degeneracy) for some of the consistency
results. however, the choice of the kernel can make a huge di   erence to how a
procedure performs in practice. some analyses related to this issue are given in
section 7.3.

7.2.2 equivalence and orthogonality

the presentation in this section is based mainly on stein [1999, ch. 4]. for
two id203 measures   0 and   1 de   ned on a measurable space (   ,f),3
  0 is said to be absolutely continuous w.r.t.   1 if for all a     f,   1(a) = 0
implies   0(a) = 0. if   0 is absolutely continuous w.r.t.   1 and   1 is absolutely
continuous w.r.t.   0 the two measures are said to be equivalent, written   0       1.
  0 and   1 are said to be orthogonal, written   0       1, if there exists an a     f
such that   0(a) = 1 and   1(a) = 0. (note that in this case we have   0(ac) = 0
and   1(ac) = 1, where ac is the complement of a.) the dichotomy theorem for
gaussian processes (due to hajek [1958] and, independently, feldman [1958])
states that two gaussian processes are either equivalent or orthogonal.

equivalence and orthogonality for gaussian measures   0,   1 with corre-
sponding id203 densities p0, p1, can be characterized in terms of the

2cross validation is discussed in section 5.3.
3see section a.7 for background on measurable spaces.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

158

theoretical perspectives

z

symmetrized id181 klsym between them, given by

klsym(p0, p1) =

(7.17)
the measures are equivalent if klsym <     and orthogonal otherwise. for two
   nite-dimensional gaussian distributions n (  0, k0) and n (  1, k1) we have
[kullback, 1959, sec. 9.1]

p1(f) df .

(p0(f)     p1(f)) log p0(f)

klsym = 1
+ 1

2 tr(k0     k1)(k   1
2 tr(k   1

1 + k   1

1     k   1
0 )

0 )(  0       1)(  0       1)>.

(7.18)

this expression can be simpli   ed considerably by simultaneously diagonalizing
k0 and k1. two    nite-dimensional gaussian distributions are equivalent if the
null spaces of their covariance matrices coincide, and are orthogonal otherwise.
things can get more interesting if we consider in   nite-dimensional distribu-
tions, i.e. gaussian processes. consider some closed subset r     rd. choose
some    nite number n of x-points in r and let f = (f1, . . . , fn)> denote the
values corresponding to these inputs. we consider the klsym-divergence as
above, but in the limit n        . klsym can now diverge if the rates of decay of
the eigenvalues of the two processes are not the same. for example, consider
zero-mean periodic processes with period 1 where the eigenvalue   i
j indicates
the amount of power in the sin/cos terms of frequency 2  j for process i = 0, 1.
then using eq. (7.18) we have

klsym =

(  0

0       1
0)2
0  1
  0
0

+ 2

(  0

j       1
j)2
  0
j   1
j

(7.19)

   x

j=1

(see also [stein, 1999, p. 119]). some corresponding results for the equiva-
lence or orthogonality of non-periodic gaussian processes are given in stein
[1999, pp. 119-122]. stein (p. 109) gives an example of two equivalent gaussian
processes on r, those with covariance functions exp(   r) and 1/2 exp(   2r). (it
is easy to check that for large s these have the same power spectrum.)

we now turn to the consequences of equivalence for the model selection
problem. suppose that we know that either gp 0 or gp 1 is the correct model.
then if gp 0     gp 1 then it is not possible to determine which model is correct
with id203 1. however, under a bayesian setting all this means is if we
have prior probabilities   0 and   1 = 1      0 on these two hypotheses, then after
observing some data d the posterior probabilities p(gp i|d) (for i = 0, 1) will
not be 0 or 1, but could be heavily skewed to one model or the other.

the other important observation is to consider the predictions made by gp 0
or gp 1. consider the case where gp 0 is the correct model and gp 1     gp 0.
then stein [1999, sec. 4.3] shows that the predictions of gp 1 are asymptotically
optimal, in the sense that the expected relative prediction error between gp 1
and gp 0 tends to 0 as n         under some technical conditions. stein   s corol-
lary 9 (p. 132) shows that this conclusion remains true under additive noise if
the un-noisy gps are equivalent. one caveat about equivalence is although the
predictions of gp 1 are asymptotically optimal when gp 0 is the correct model
and gp 0     gp 1, one would see di   ering predictions for    nite n.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

7.3 average-case learning curves

159

7.3 average-case learning curves

   

in section 7.2 we have discussed the asymptotic properties of gaussian process
predictors and related methods.
in this section we will say more about the
speed of convergence under certain speci   c assumptions. our goal will be to
obtain a learning curve describing the generalization error as a function of the
training set size n. this is an average-case analysis, averaging over the choice
of target functions (drawn from a gp) and over the x locations of the training
points.

in more detail, we    rst consider a target function f drawn from a gaussian
process. n locations are chosen to make observations at, giving rise to the train-
ing set d = (x, y). the yis are (possibly) noisy observations of the underlying
function f. given a id168 l(  ,  ) which measures the di   erence between
the prediction for f and f itself, we obtain an estimator fd for f. below we
will use the squared loss, so that the posterior mean   fd(x) is the estimator.
then the generalization error (given f and d) is given by

egd(f) =

l(f(x   ),   fd(x   ))p(x   ) dx   .

(7.20)

as this is an expected loss it is technically a risk, but the term generalization
error is commonly used.

egd(f) depends on both the choice of f and on x. (note that y depends on
the choice of f, and also on the noise, if present.) the    rst level of averaging
we consider is over functions f drawn from a gp prior, to obtain

z

z

eg(x) =

egd(f)p(f) df.

(7.21)

generalization error

it will turn out that for regression problems with gaussian process priors and
predictors this average can be readily calculated. the second level of averaging
assumes that the x-locations of the training set are drawn i.i.d. from p(x) to
give

z

eg(n) =

eg(x)p(x1) . . . p(xn) dx1 . . . dxn.

(7.22)

a plot of eg(n) against n is known as a learning curve.

learning curve

rather than averaging over x, an alternative is to minimize eg(x) w.r.t. x.
this gives rise to the optimal experimental design problem. we will not say
more about this problem here, but it has been subject to a large amount of
investigation. an early paper on this subject is by ylvisaker [1975]. these
questions have been addressed both in the statistical literature and in theoretical
numerical analysis; for the latter area the book by ritter [2000] provides a useful
overview.

we now proceed to develop the average-case analysis further for the speci   c
case of gp predictors and gp priors for the regression case using squared loss.
let f be drawn from a zero-mean gp with covariance function k0 and noise
level   2
0. similarly the predictor assumes a zero-mean process, but covariance

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

160

theoretical perspectives

1. at a particular test location x   , averaging over

function k1 and noise level   2
f we have
e[(f(x   )     k1(x   )>k   1

1,yy)2]

= e[f 2(x   )]     2k1(x   )>k   1
= k0(x   , x   )     2k1(x   )>k   1

e[f(x   )y] + k1(x   )>k   1
1,yk0(x   ) + k1(x   )>k   1

e[yy>]k   1
1,yk1(x   )

1,y k0,yk   1

1,y

1,y

(7.23)
1,yk1(x   )

where ki,y = ki,f +   2
assumed noise.
the above expression reduces to k0(x   , x   )    k0(x   )>k   1
variance of the gp.

i for i = 0, 1, i.e. the covariance matrix including the
if k1 = k0 so that the predictor is correctly speci   ed then
0,yk0(x   ), the predictive

z
z

averaging the error over p(x   ) we obtain

eg(x) =

=

e[(f(x   )     k1(x   )>k   1
k0(x   , x   )p(x   ) dx        2 tr
k   1
1,y k0,yk   1

z

1,y

(cid:16)

(cid:16)

+ tr

1,yy)2]p(x   ) dx   

z

k1(x   )k1(x)>p(x   ) dx   

.

(cid:17)

k   1

1,y

k0(x   )k1(x   )>p(x   ) dx   

(7.24)

(cid:17)

for some choices of p(x   ) and covariance functions these integrals will be an-
alytically tractable, reducing the computation of eg(x) to a n    n matrix
computation.

to obtain eg(n) we need to perform a    nal level of averaging over x. in
general this is di   cult even if eg(x) can be computed exactly, but it is some-
times possible, e.g. for the noise-free ou process on the real line, see section
7.6.

k1 we use the de   nition k(x, x0) =p

the form of eg(x) can be simpli   ed considerably if we express the covari-
ance functions in terms of their eigenfunction expansions. in the case that k0 =
  i  i(x0). let    be a diagonal matrix of the eigenvalues and    be the n    n
design matrix, as de   ned in section 2.1.2. then from eq. (7.24) we obtain

i   i  i(x)  i(x0) andr k(x, x0)  i(x)p(x) dx =

eg(x) = tr(  )     tr((  2
= tr(     1 +      2

ni +   >    )   1  >  2  )
n     >)   1,

(7.25)

where the second line follows through the use of the matrix inversion lemma
eq. (a.9) (or directly if we use eq. (2.11)), as shown in sollich [1999] or opper
and vivarelli [1999]. using the fact that ex[    >] = ni, a na    ve approximation
would replace     > inside the trace with its expectation; in fact opper and
vivarelli [1999] showed that this gives a lower bound, so that

eg(n)     tr(     1 + n     2

n i)   1 =   2

.

(7.26)

nx

  i

n + n  i
  2

i=1

examining the asymptotics of eq. (7.26), we see that for each eigenvalue where
  i (cid:29)   2
n/n onto the bound on the generalization error. as we saw

n/n we add   2

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

7.4 pac-bayesian analysis

161

p   

in section 7.1, more eigenfunctions    come into play    as n increases, so the rate
of decay of eg(n) is slower than 1/n. sollich [1999] derives a number of more
accurate approximations to the learning curve than eq. (7.26).

for the noiseless case with k1 = k0, there is a simple lower bound eg(n)    
i=n+1   i due to micchelli and wahba [1981]. this bound is obtained by
demonstrating that the optimal n pieces of information are the projections of the
random function f onto the    rst n eigenfunctions. as observations which simply
consist of function evaluations will not in general provide such information this
is a lower bound. plaskota [1996] generalized this result to give a bound on the
learning curve if the observations are noisy.

some asymptotic results for the learning curves are known. for example, in
ritter [2000, sec. v.2] covariance functions obeying sacks-ylvisaker conditions4
of order r in 1-d are considered. he shows that for an optimal sampling of the
input space the generalization error goes as o(n   (2r+1)/(2r+2)) for the noisy
problem. similar rates can also be found in sollich [2002] for random designs.
for the noise-free case ritter [2000, p. 103] gives the rate as o(n   (2r+1)).

one can examine the learning curve not only asymptotically but also for
small n, where typically the curve has a roughly linear decrease with n. williams
and vivarelli [2000] explained this behaviour by observing that the introduction
of a datapoint x1 reduces the variance locally around x1 (assuming a stationary
covariance function). the addition of another datapoint at x2 will also create
a    hole    there, and so on. with only a small number of datapoints it is likely
that these holes will be far apart so their contributions will add, thus explaining
the initial linear trend.

sollich [2002] has also investigated the mismatched case where k0 6= k1.
this can give rise to a rich variety of behaviours in the learning curves, includ-
ing plateaux. stein [1999, chs. 3, 4] has also carried out some analysis of the
mismatched case.

although we have focused on gp regression with squared loss, we note that
malzahn and opper [2002] have developed more general techniques that can be
used to analyze learning curves for other situations such as gp classi   cation.

7.4 pac-bayesian analysis

   

in section 7.3 we gave an average-case analysis of generalization, taking the
average with respect to a gp prior over functions. in this section we present
a di   erent kind of analysis within the probably approximately correct (pac)
framework due to valiant [1984]. seeger [2002; 2003] has presented a pac-
bayesian analysis of generalization in gaussian process classi   ers and we get
to this in a number of stages; we    rst present an introduction to the pac
framework (section 7.4.1), then describe the pac-bayesian approach (section

4roughly speaking, a stochastic process which possesses r ms derivatives but not r + 1
is said to satisfy sacks-ylvisaker conditions of order r; in 1-d this gives rise to a spectrum
  i     i   (2r+2) asymptotically. the ou process obeys sacks-ylvisaker conditions of order 0.

pac

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

162

theoretical perspectives

7.4.2) and then    nally the application to gp classi   cation (section 7.4.3). our
presentation is based mainly on seeger [2003].

7.4.1 the pac framework
consider a    xed measure   (x, y). given a id168 l there exists a function
  (x) which minimizes the expected risk. by running a learning algorithm on
a data set d of size n drawn i.i.d. from   (x, y) we obtain an estimate fd of   
p
which attains an expected risk rl(fd). we are not able to evaluate rl(fd) as
p
we do not know   . however, we do have access to the empirical distribution of
i   (x   xi)  (y   yi) and can compute the empirical
the training set     (x, y) = 1
n
i l(yi, fd(xi)). because the training set had been used to
risk   rl(fd) = 1
compute fd we would expect   rl(fd) to underestimate rl(fd),5 and the aim
of the pac analysis is to provide a bound on rl(fd) based on   rl(fd).

n

a pac bound has the following format

pd{rl(fd)       rl(fd) + gap(fd,d,   )}     1       ,

(7.27)

where pd denotes the id203 distribution of datasets drawn i.i.d. from
  (x, y), and        (0, 1) is called the con   dence parameter. the bound states
that, averaged over draws of the dataset d from   (x, y), rl(fd) does not
exceed the sum of   rl(fd) and the gap term with id203 of at least 1       .
the    accounts for the    probably    in pac, and the    approximately    derives
from the fact that the gap term is positive for all n. it is important to note that
pac analyses are distribution-free, i.e. eq. (7.27) must hold for any measure   .
there are two kinds of pac bounds, depending on whether gap(fd,d,   )
actually depends on the particular sample d (rather than on simple statistics
like n). bounds that do depend on d are called data dependent, and those that
do not are called data independent. the pac-bayesian bounds given below are
data dependent.

has mean m. an estimate of m is given by the sample mean   x = p

it is important to understand the interpretation of a pac bound and to
clarify this we    rst consider a simpler case of statistical id136. we are
given a dataset d = {x1, . . . , xn} drawn i.i.d. from a distribution   (x) that
i xi/n.
under certain assumptions we can obtain (or put bounds on) the sampling
distribution p(  x|m) which relates to the choice of dataset d. however, if we
wish to perform probabilistic id136 for m we need to combine p(  x|m) with
a prior distribution p(m) and use bayes    theorem to obtain the posterior.6
the situation is similar (although somewhat more complex) for pac bounds as
these concern the sampling distribution of the expected and empirical risks of
fd w.r.t. d.

5it is also possible to consider pac analyses of other empirical quantities such as the

cross-validation error (see section 5.3) which do not have this bias.

6in introductory treatments of frequentist statistics the logical hiatus of going from the

sampling distribution to id136 on the parameter of interest is often not well explained.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

7.4 pac-bayesian analysis

163

we might wish to make a conditional statement like

pd{rl(fd)     r + gap(fd,d,   )|   rl(fd) = r}     1       ,

(7.28)
where r is a small value, but such a statement cannot be inferred directly from
the pac bound. this is because the gap might be heavily anti-correlated with
  rl(fd) so that the gap is large when the empirical risk is small.

pac bounds are sometimes used to carry out model selection   given a learn-
ing machine which depends on a (discrete or continuous) parameter vector   ,
one can seek to minimize the generalization bound as a function of   . however,
this procedure may not be well-justi   ed if the generalization bounds are loose.
let the slack denote the di   erence between the value of the bound and the
generalization error. the danger of choosing    to minimize the bound is that
if the slack depends on    then the value of    that minimizes the bound may be
very di   erent from the value of    that minimizes the generalization error. see
seeger [2003, sec. 2.2.4] for further discussion.

7.4.2 pac-bayesian analysis

we now consider a bayesian set up, with a prior distribution p(w) over the pa-
rameters w, and a    posterior    distribution q(w). (strictly speaking the analysis
does not require q(w) to be the posterior distribution, just some other distribu-
tion, but in practice we will consider q to be an (approximate) posterior distri-
bution.) we also limit our discussion to binary classi   cation with labels {   1, 1},
although more general cases can be considered, see seeger [2003, sec. 3.2.2].

r q(f   |w, x   )q(w)dw, and the predictive classi   er outputs sgn(q(f   |x   )    1/2).

the predictive distribution for f    at a test point x    given q(w) is q(f   |x   ) =

the gibbs classi   er has also been studied in learning theory; given a test point
x    one draws a sample   w from q(w) and predicts the label using sgn(f(x   ;   w)).
the main reason for introducing the gibbs classi   er here is that the pac-
bayesian theorems given below apply to gibbs classi   ers.

for a given parameter vector w giving rise to a classi   er c(x; w), the ex-

pected risk and empirical risk are given by

rl(w) =

l(y, c(x; w)) d  (x, y),

  rl(w) =

l(yi, c(xi; w)). (7.29)

as the gibbs classi   er draws samples from q(w) we consider the averaged risks

rl(q) =

rl(w)q(w) dw,

  rl(q) =

  rl(w)q(w) dw.

(7.30)

nx

i=1

1
n

z

z

z

theorem 7.1 (mcallester   s pac-bayesian theorem) for any id203 mea-
sures p and q over w and for any bounded id168 l for which l(y, c(x))    
[0, 1] for any classi   er c and input x we have

n

s

pd

rl(q)       rl(q) +

kl(q||p) + log 1
2n     1

   + log n + 2

    q

o     1       .

(7.31)
(cid:3)

predictive classi   er
gibbs classi   er

mcallester   s
pac-bayesian theorem

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

164

theoretical perspectives

the proof can be found in mcallester [2003]. the kullback-leibler (kl) diver-
gence kl(q||p) is de   ned in section a.5. an example of a id168 which
obeys the conditions of the theorem is the 0/1 loss.

seeger   s pac-
bayesian theorem

for the special case of 0/1 loss, seeger [2002] gives the following tighter

n

bound.
theorem 7.2 (seeger   s pac-bayesian theorem) for any distribution over x   
{   1, +1} and for any id203 measures p and q over w the following bound
holds for i.i.d. samples drawn from the data distribution
(kl(q||p) + log n + 1

(7.32)
(cid:3)
here klber(  ||  ) is the kl divergence between two bernoulli distributions (de-
   ned in eq. (a.22)). thus the theorem bounds (with high id203) the kl
divergence between   rl(q) and rl(q).

klber(   rl(q)||rl(q))     1
n

o     1       .

)     q

pd

  

the pac-bayesian theorems above refer to a gibbs classi   er.

if we are
interested in the predictive classi   er sgn(q(f   |x   )     1/2) then seeger [2002]
shows that if q(f   |x   ) is symmetric about its mean then the expected risk
of the predictive classi   er is less than twice the expected risk of the gibbs
classi   er. however, this result is based on a simple bounding argument and in
practice one would expect that the predictive classi   er will usually give better
performance than the gibbs classi   er. recent work by meir and zhang [2003]
provides some pac bounds directly for bayesian algorithms (like the predictive
classi   er) whose predictions are made on the basis of a data-dependent posterior
distribution.

7.4.3 pac-bayesian analysis of gp classi   cation

to apply this bound to the gaussian process case we need to compute the
kl divergence kl(q||p) between the posterior distribution q(w) and the prior
distribution p(w). although this could be considered w.r.t. the weight vector
w in the eigenfunction expansion, in fact it turns out to be more convenient
to consider the latent function value f(x) at every possible point in the input
space x as the parameter. we divide this (possibly in   nite) vector into two
parts, (1) the values corresponding to the training points x1, . . . , xn, denoted
f, and (2) those at the remaining points in x-space (the test points) f   .

the key observation is that all methods we have described for dealing with
gp classi   cation problems produce a posterior approximation q(f|y) which is
de   ned at the training points. (this is an approximation for laplace   s method
and for ep; mcmc methods sample from the exact posterior.) this posterior
over f is then extended to the test points by setting q(f , f   |y) = q(f|y)p(f   |f).
of course for the prior distribution we have a similar decomposition p(f , f   ) =

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

7.5 comparison with other supervised learning methods

165

p(f)p(f   |f). thus the kl divergence is given by

z
z

kl(q||p) =

=

q(f|y)p(f   |f) log q(f|y)p(f   |f)
q(f|y) log q(f|y)

p(f)p(f   |f) df df   

p(f) df ,

(7.33)

as shown e.g. in seeger [2002]. notice that this has reduced a rather scary
in   nite-dimensional integration to a more manageable n-dimensional integra-
tion; in the case that q(f|y) is gaussian (as for the laplace and ep approxima-
tions), this kl divergence can be computed using eq. (a.23). for the laplace
approximation with p(f) = n (0, k) and q(f|y) = n (  f , a   1) this gives
kl(q||p) = 1

2 tr(cid:0)a   1(k   1     a)(cid:1) + 1

  f>k   1  f . (7.34)

2 log |k| + 1

2 log |a| + 1

2

seeger [2002] has evaluated the quality of the bound produced by the pac-
bayesian method for a laplace gpc on the task of discriminating handwritten
2s and 3s from the mnist handwritten digits database.7 he reserved a test set
of 1000 examples and used training sets of size 500, 1000, 2000, 5000 and 9000.
the classi   cations were replicated ten times using draws of the training sets
from a pool of 12089 examples. we quote example results for n = 5000 where
the training error was 0.0187    0.0016, the test error was 0.0195    0.0011 and
the pac-bayesian bound on the generalization error (evaluated for    = 0.01)
was 0.076    0.002.
(the       gures denote a 95% con   dence interval.) the
classi   cation results are for the gibbs classi   er; for the predictive classi   er the
test error rate was 0.0171   0.0016. thus the generalization error is around 2%,
while the pac bound is 7.6%. many pac bounds struggle to predict error rates
below 100%(!), so this is an impressive and highly non-trivial result. further
details and experiments can be found in seeger [2002].

7.5 comparison with other supervised learn-

ing methods

the focus of this book is on gaussian process methods for supervised learning.
however, there are many other techniques available for supervised learning such
as id75, id28, id90, neural networks, support
vector machines, kernel smoothers, k-nearest neighbour classi   ers, etc., and we
need to consider the relative strengths and weaknesses of these approaches.

supervised learning is an inductive process   given a    nite training set we
wish to infer a function f that makes predictions for all possible input values.
the additional assumptions made by the learning algorithm are known as its
inductive bias (see e.g. mitchell [1997, p. 43]). sometimes these assumptions
are explicit, but for other algorithms (e.g. for decision tree induction) they can
be rather more implicit.

7see http://yann.lecun.com/exdb/mnist.

inductive bias

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

166

theoretical perspectives

however, for all their variety, supervised learning algorithms are based on
the idea that similar input patterns will usually give rise to similar outputs (or
output distributions), and it is the precise notion of similarity that di   erentiates
the algorithms. for example some algorithms may do feature selection and
decide that there are input dimensions that are irrelevant to the predictive task.
some algorithms may construct new features out of those provided and measure
similarity in this derived space. as we have seen, many regression techniques
can be seen as linear smoothers (see section 2.6) and these techniques vary in
the de   nition of the weight function that is used.

one important distinction between di   erent learning algorithms is how they
relate to the question of universal consistency (see section 7.2.1). for example
a id75 model will be inconsistent if the function that minimizes the
risk cannot be represented by a linear function of the inputs. in general a model
with a    nite-dimensional parameter vector will not be universally consistent.
examples of such models are id75 and id28 with a
   nite-dimensional feature vector, and neural networks with a    xed number of
hidden units. in contrast to these parametric models we have non-parametric
models (such as k-nearest neighbour classi   ers, kernel smoothers and gaussian
processes and id166s with nondegenerate kernels) which do not compress the
training data into a    nite-dimensional parameter vector. an intermediate po-
sition is taken by semi-parametric models such as neural networks where the
number of hidden units k is allowed to increase as n increases. in this case uni-
versal consistency results can be obtained [devroye et al., 1996, ch. 30] under
certain technical conditions and growth rates on k.

although universal consistency is a    good thing   , it does not necessarily
mean that we should only consider procedures that have this property; for
example if on a speci   c problem we knew that a id75 model was
consistent for that problem then it would be very natural to use it.

in the 1980   s there was a large surge in interest in arti   cial neural networks
(anns), which are feedforward networks consisting of an input layer, followed
by one or more layers of non-linear transformations of weighted combinations of
the activity from previous layers, and an output layer. one reason for this surge
of interest was the use of the id26 algorithm for training anns.
initial excitement centered around that fact that training non-linear networks
was possible, but later the focus came onto the generalization performance of
anns, and how to deal with questions such as how many layers of hidden
units to use, how many units there should be in each layer, and what type of
non-linearities should be used, etc.

for a particular ann the search for a good set of weights for a given training
set is complicated by the fact that there can be local optima in the optimization
problem; this can cause signi   cant di   culties in practice. in contrast for gaus-
sian process regression and classi   cation the posterior for the latent variables
is convex.

neural networks

bayesian neural
networks

one approach to the problems raised above was to put anns in a bayesian
framework, as developed by mackay [1992a] and neal [1996]. this gives rise

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

7.5 comparison with other supervised learning methods

167

to posterior distributions over weights for a given architecture, and the use of
the marginal likelihood (see section 5.2) for model comparison and selection.
in contrast to gaussian process regression the marginal likelihood for a given
ann model is not analytically tractable, and thus approximation techniques
such as the laplace approximation [mackay, 1992a] and markov chain monte
carlo methods [neal, 1996] have to be used. neal   s observation [1996] that
certain anns with one hidden layer converge to a gaussian process prior over
functions (see section 4.2.3) led us to consider gps as alternatives to anns.

mackay [2003, sec. 45.7] raises an interesting question whether in moving
from neural networks to gaussian processes we have    thrown the baby out with
the bathwater?   . this question arises from his statements that    neural networks
were meant to be intelligent models that discovered features and patterns in
data   , while    gaussian processes are simply smoothing devices   . our answer
to this question is that gps give us a computationally attractive method for
dealing with the smoothing problem for a given kernel, and that issues of feature
discovery etc. can be addressed through methods to select the id81
(see chapter 5 for more details on how to do this). note that using a distance
function r2(x, x0) = (x     x0)>m(x     x0) with m having a low-rank form m =
    > +    as in eq. (4.22), features are described by the columns of   . however,
some of the non-convexity of the neural network optimization problem now
returns, as optimizing the marginal likelihood in terms of the parameters of m
may well have local optima.

as we have seen from chapters 2 and 3 id75 and logistic regres-
sion with gaussian priors on the parameters are a natural starting point for
the development of gaussian process regression and gaussian process classi   -
cation. however, we need to enhance the    exibility of these models, and the
use of non-degenerate kernels opens up the possibility of universal consistency.
kernel smoothers and classi   ers have been described in sections 2.6 and
7.2.1. at a high level there are similarities between gp prediction and these
methods as a kernel is placed on every training example and the prediction is
obtained through a weighted sum of the id81s, but the details of
the prediction and the underlying logic di   er. note that the gp prediction
view gives us much more, e.g. error bars on the predictions and the use of the
marginal likelihood to set parameters in the kernel (see section 5.2). on the
other hand the computational problem that needs to be solved to carry out gp
prediction is more demanding than that for simple kernel-based methods.

kernel smoothers and classi   ers are non-parametric methods, and consis-
tency can often be obtained under conditions where the width h of the kernel
tends to zero while nhd        . the equivalent kernel analysis of gp regression
(section 7.1) shows that there are quite close connections between the kernel
regression method and gpr, but note that the equivalent kernel automatically
reduces its width as n grows; in contrast the decay of h has to be imposed for
kernel regression. also, for some kernel smoothing and classi   cation algorithms
the width of the kernel is increased in areas of low observation density; for ex-
ample this would occur in algorithms that consider the k nearest neighbours of
a test point. again notice from the equivalent kernel analysis that the width

linear and logistic
regression

kernel smoothers and
classi   ers

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

168

theoretical perspectives

id173 networks,
splines, id166s and
rvms

   

of the equivalent kernel is larger in regions of low density, although the exact
dependence on the density will depend on the kernel used.

the similarities and di   erences between gp prediction and id173

networks, splines, id166s and rvms have been discussed in chapter 6.

7.6 appendix: learning curve for the ornstein-

uhlenbeck process

we now consider the calculation of the learning curve for the ou covariance
function k(r) = exp(     |r|) on the interval [0, 1], assuming that the training x   s
are drawn from the uniform distribution u(0, 1). our treatment is based on
williams and vivarelli [2000].8 we    rst calculate eg(x) for a    xed design, and
then integrate over possible designs to obtain eg(n).

in the absence of noise the ou process is markovian (as discussed in ap-
pendix b and exercise 4.5.1). we consider the interval [0, 1] with points x1 <
x2 . . . < xn   1 < xn placed on this interval. also let x0 = 0 and xn+1 = 1. due
to the markovian nature of the process the prediction at a test point x depends
only on the function values of the training points immediately to the left and
right of x. thus in the i-th interval (counting from 0) the bounding points are
xi and xi+1. let this interval have length   i.

using eq. (7.24) we have

z 1

0

z xi+1

nx

i=0

xi

eg(x) =

f (x) dx =
  2

f (x) dx,
  2

(7.35)

where   2
we have in interval i (for i = 1, . . . , n     1) that   2
where k is the 2    2 gram matrix

f (x) is the predictive variance at input x. using the markovian property
f (x) = k(0)     k(x)>k   1k(x)
(cid:19)
(cid:18) k(0)
(cid:18) k(0)    k(  i)

and k(x) is the corresponding vector of length 2. thus

k(  i)
k(0)

(7.36)

k(  i)

k =

(cid:19)

(7.37)

1
   i

   k(  i)

k(0)

,

k   1 =
where    i = k2(0)     k2(  i) and
f (x) = k(0)    1
  2
   i
z xi+1

thus

[k(0)(k2(xi+1   x)+k2(x   xi))   2k(  i)k(x   xi)k(xi+1   x)].
(7.38)

f (x)dx =   ik(0)     2
  2
   i

xi

(i1(  i)     i2(  i))

(7.39)

8cw thanks manfred opper for pointing out that the upper bound developed in williams

and vivarelli [2000] is exact for the noise-free ou process.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

7.7 exercises

where

i1(  ) = k(0)

z   

0

k2(z)dz,

i2(  ) = k(  )

z   

0

k(z)k(       z)dz.

(7.40)

169

z xi+1

xi

z x1

0

for k(r) = exp(     |r|) these equations reduce to i1(  ) = (1     e   2    )/(2  ),

i2(  ) =   e   2     and     = 1     e   2    . thus

f (x)dx =   i     1
  2

  

2  ie   2    i
1     e   2    i

.

+

(7.41)

this calculation is not correct in the    rst and last intervals where only x1
f (x) =

and xn are relevant (respectively). for the 0th interval we have that   2
k(0)     k2(x1     x)/k(0) and thus

z x1

f (x) =   0k(0)     1
  2
k(0)
(1     e   2    0),

=   0     1
2  

0

k2(x1     x)dx

and a similar result holds forr 1

f (x).
  2
putting this all together we obtain

xn

eg(x) = 1     n
  

+

1
2  

(e   2    0 + e   2    n) +

n   1x

i=1

2  ie   2    i
1     e   2    i

.

(7.42)

(7.43)

(7.44)

choosing a regular grid so that   0 =   n = 1/2n and   i = 1/n for i =
1, . . . , n     1 it is straightforward to show (see exercise 7.7.4) that eg scales as
o(n   1), in agreement with the general sacks-ylvisaker result [ritter, 2000, p.
103] when it is recalled that the ou process obeys sacks-ylvisaker conditions
of order 0. a similar calculation is given in plaskota [1996, sec. 3.8.2] for the
wiener process on [0, 1] (note that this is also markovian, but non-stationary).
we have now worked out the generalization error for a    xed design x.
however to compute eg(n) we need to average eg(x) over draws of x from the
uniform distribution. the theory of order statistics david [1970, eq. 2.3.4] tells
us that p(  ) = n(1      )n   1 for all the   i, i = 0, . . . , n. taking the expectation of
eg(x) then turns into the problem of evaluating the one-dimensional integrals

r e   2    p(  )d   and r   e   2    (1     e   2    )   1p(  )d  . exercise 7.7.5 asks you to

compute these integrals numerically.

7.7 exercises

1. consider a spline regularizer with sf (s) = c   1|s|   2m. (as we noted in
section 6.3 this is not strictly a power spectrum as the spline is an im-
proper prior, but it can be used as a power spectrum in eq. (7.9) for the

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

170

theoretical perspectives

z exp(2  is    x)

purposes of this analysis.) the equivalent kernel corresponding to this
spline is given by

h(x) =

(7.45)
n/  . by changing variables in the integration to |t| =

1 +   |s|2m ds,

where    = c  2
  1/2m|s| show that the width of h(x) scales as n   1/2m.

2. equation 7.45 gives the form of the equivalent kernel for a spline regular-
izer. show that h(0) is only    nite if 2m > d. (hint: transform the inte-
gration to polar coordinates.) this observation was made by p. whittle
in the discussion of silverman [1985], and shows the need for the condition
2m > d for spline smoothing.

3. computer exercise: space n + 1 points out evenly along the interval
(   1/2, 1/2). (take n to be even so that one of the sample points falls at 0.)
calculate the weight function (see section 2.6) corresponding to gaussian
process regression with a particular covariance function and noise level,
and plot this for the point x = 0. now compute the equivalent kernel cor-
responding to the covariance function (see, e.g. the examples in section
7.1.1), plot this on the same axes and compare results. hint 1: recall
that the equivalent kernel is de   ned in terms of integration (see eq. (7.7))
so that there will be a scaling factor of 1/(n + 1). hint 2: if you wish to
use large n (say > 1000), use the ngrid method described in section 2.6.

4. consider eg(x) as given in eq. (7.44) and choose a regular grid design x
so that   0 =   n = 1/2n and   i = 1/n for i = 1, . . . , n   1. show that eg(x)
scales as o(n   1) asymptotically. hint: when expanding 1     exp(   2    i),
be sure to extend the expansion to su   cient order.

5. compute numerically the expectation of eg(x) eq. (7.44) over random
designs for the ou process example discussed in section 7.6. make use
of the fact [david, 1970, eq. 2.3.4] that p(  ) = n(1       )n   1 for all the   i,
i = 0, . . . , n. investigate the scaling behaviour of eg(n) w.r.t. n.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

chapter 8

approximation methods for
large datasets

as we have seen in the preceding chapters a signi   cant problem with gaus-
sian process prediction is that it typically scales as o(n3). for large problems
(e.g. n > 10, 000) both storing the gram matrix and solving the associated
linear systems are prohibitive on modern workstations (although this boundary
can be pushed further by using high-performance computers).

an extensive range of proposals have been suggested to deal with this prob-
lem. below we divide these into    ve parts: in section 8.1 we consider reduced-
rank approximations to the gram matrix; in section 8.2 a general strategy for
greedy approximations is described; in section 8.3 we discuss various methods
for approximating the gp regression problem for    xed hyperparameters; in sec-
tion 8.4 we describe various methods for approximating the gp classi   cation
problem for    xed hyperparameters; and in section 8.5 we describe methods to
approximate the marginal likelihood and its derivatives. many (although not
all) of these methods use a subset of size m < n of the training examples.

8.1 reduced-rank approximations of the gram

matrix

in the gp regression problem we need to invert the matrix k +   2
ni (or at least
to solve a linear system (k +   2
ni)v = y for v). if the matrix k has rank q (so
that it can be represented in the form k = qq> where q is an n    q matrix)
then this matrix inversion can be speeded up using the matrix inversion lemma
eq. (a.9) as (qq> +   2
niq + q>q)   1q>. notice that
the inversion of an n    n matrix has now been transformed to the inversion of
a q    q matrix.1

nin)   1 =      2

n in          2

n q(  2

1for numerical reasons this is not the best way to solve such a linear system but it does

illustrate the savings that can be obtained with reduced-rank representations.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

172

approximation methods for large datasets

in the case that the kernel is derived from an explicit feature expansion with
n features, then the gram matrix will have rank min(n, n) so that exploitation
of this structure will be bene   cial if n > n. even if the kernel is non-degenerate
it may happen that it has a fast-decaying eigenspectrum (see e.g. section 4.3.1)
so that a reduced-rank approximation will be accurate.

if k is not of rank < n, we can still consider reduced-rank approximations to
k. the optimal reduced-rank approximation of k w.r.t. the frobenius norm
(see eq. (a.16)) is uq  qu>
q , where   q is the diagonal matrix of the leading
q eigenvalues of k and uq is the matrix of the corresponding orthonormal
eigenvectors [golub and van loan, 1989, theorem 8.1.9]. unfortunately, this
is of limited interest in practice as computing the eigendecomposition is an
o(n3) operation. however, it does suggest that if we can more cheaply obtain
an approximate eigendecomposition then this may give rise to a useful reduced-
rank approximation to k.

(cid:18) kmm

we now consider selecting a subset i of the n datapoints; set i has size
m < n. the remaining n     m datapoints form the set r. (as a mnemonic, i is
for the included datapoints and r is for the remaining points.) we sometimes
call the included set the active set. without loss of generality we assume that
the datapoints are ordered so that set i comes    rst. thus k can be partitioned
as

.

k =

k(n   m)m k(n   m)(n   m)

(8.1)
the top m    n block will also be referred to as kmn and its transpose as knm.
in section 4.3.2 we saw how to approximate the eigenfunctions of a kernel
using the nystr  om method. we can now apply the same idea to approximating
the eigenvalues/vectors of k. we compute the eigenvectors and eigenvalues of
}m
kmm and denote them {  (m)
i=1. these are extended to all n
points using eq. (4.44) to give

i=1 and {u(m)
}m

i

i

km(n   m)

(cid:19)

  (m)
i

,

, n
m

r m

1

    (n)

i

,

  u(n)

i

i = 1, . . . , m

i = 1, . . . , m

(8.2)

(8.3)

knmu(m)

,

i

n

  (m)
i
has been chosen so that |  u(n)

|     1. in general we have
where the scaling of   u(n)
a choice of how many of the approximate eigenvalues/vectors to include in our
)>.

approximation of k; choosing the    rst p we get   k = pp

(  u(n)

i

i

    (n)
i   u(n)

i

i

i=1

below we will set p = m to obtain

nystr  om approximation

  k = knmk   1

mmkmn

(8.4)

using equations 8.2 and 8.3, which we call the nystr  om approximation to k.
computation of   k takes time o(m2n) as the eigendecomposition of kmm is
o(m3) and the computation of each   u(n)
is o(mn). fowlkes et al. [2001] have
applied the nystr  om method to approximate the top few eigenvectors in a
id161 problem where the matrices in question are larger than 106  106
in size.

i

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

8.1 reduced-rank approximations of the gram matrix

173

   

the nystr  om approximation has been applied above to approximate the
elements of k. however, using the approximation for the ith eigenfunction
, where km(x) = (k(x, x1), . . . , k(x, xm))> (a
    i(x) = (
restatement of eq. (4.44) using the current notation) and   i       (m)
/m it is
easy to see that in general we obtain an approximation for the kernel k(x, x0) =

)km(x)>u(m)

m/  (m)

i

i

i

pn
i=1   i  i(x)  i(x0) as
  k(x, x0) =

mx
mx

i=1

  (m)
i
m

    i(x)     i(x0)

=

  (m)
i
m

m
(  (m)
mmkm(x0).
= km(x)>k   1

)2

i=1

i

km(x)>u(m)

i

(u(m)

i

)>km(x0)

(8.5)

(8.6)

(8.7)

clearly eq. (8.4) is obtained by evaluating eq. (8.7) for all pairs of datapoints
in the training set.

by multiplying out eq. (8.4) using kmn = [kmmkm(n   m)] it is easy to
show that kmm =   kmm, km(n   m) =   km(n   m), k(n   m)m =   k(n   m)m, but
that   k(n   m)(n   m) = k(n   m)mk   1
k(n   m)(n   m)       k(n   m)(n   m) is in fact the schur complement of kmm [golub
and van loan, 1989, p. 103]. it is easy to see that k(n   m)(n   m)      k(n   m)(n   m)
is positive semi-de   nite; if a vector f is partitioned as f> = (f>
n   m) and f
has a gaussian distribution with zero mean and covariance k then fn   m|fm
has the schur complement as its covariance matrix, see eq. (a.6).

mmkm(n   m). the di   erence

m, f>

the nystr  om approximation was derived in the above fashion by williams
and seeger [2001] for application to kernel machines. an alternative view which
gives rise to the same approximation is due to smola and sch  olkopf [2000] (and
also sch  olkopf and smola [2002, sec. 10.2]). here the starting point is that we
wish to approximate the kernel centered on point xi as a linear combination of
kernels from the active set, so that

cijk(xj, x) ,   k(xi, x)

(8.8)

k(xi, x)    x

j   i

for some coe   cients {cij} that are to be determined so as to optimize the
approximation. a reasonable criterion to minimize is
kk(xi, x)       k(xi, x)k2h

nx

e(c) =

(8.9)

i=1

= tr k     2 tr(ckmn) + tr(ckmmc>),

(8.10)
where the coe   cients are arranged into a n    m matrix c. minimizing e(c)
w.r.t. c gives copt = knmk   1
mm; thus we obtain the approximation   k =
knmk   1
mmkmn in agreement with eq. (8.4). also, it can be shown that e(copt) =
tr(k       k).

smola and sch  olkopf [2000] suggest a greedy algorithm to choose points to
include into the active set so as to minimize the error criterion. as it takes

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

174

approximation methods for large datasets

o(mn) operations to evaluate the change in e due to including one new dat-
apoint (see exercise 8.7.2) it is infeasible to consider all members of set r for
inclusion on each iteration; instead smola and sch  olkopf [2000] suggest    nd-
ing the best point to include from a randomly chosen subset of set r on each
iteration.

recent work by drineas and mahoney [2005] analyzes a similar algorithm
to the nystr  om approximation, except that they use biased sampling with re-
placement (choosing column i of k with id203     k2
ii) and a pseudoinverse
of the inner m    m matrix. for this algorithm they are able to provide prob-
abilistic bounds on the quality of the approximation. earlier work by frieze
et al. [1998] had developed an approximation to the singular value decomposi-
tion (svd) of a rectangular matrix using a weighted random subsampling of its
rows and columns, and probabilistic error bounds. however, this is rather di   er-
ent from the nystr  om approximation; see drineas and mahoney [2005, sec. 5.2]
for details.

fine and scheinberg [2002] suggest an alternative low-rank approximation
to k using the incomplete cholesky factorization (see golub and van loan
[1989, sec. 10.3.2]). the idea here is that when computing the cholesky de-
composition of k pivots below a certain threshold are skipped.2 if the number
of pivots greater than the threshold is k the incomplete cholesky factorization
takes time o(nk2).

8.2 greedy approximation

many of the methods described below use an active set of training points of size
m selected from the training set of size n > m. we assume that it is impossible
to search for the optimal subset of size m due to combinatorics. the points
in the active set could be selected randomly, but in general we might expect
better performance if the points are selected greedily w.r.t. some criterion. in
the statistics literature greedy approaches are also known as forward selection
strategies.

a general recipe for greedy approximation is given in algorithm 8.1. the
algorithm starts with the active set i being empty, and the set r containing the
indices of all training examples. on each iteration one index is selected from r
and added to i. this is achieved by evaluating some criterion     and selecting
the data point that optimizes this criterion. for some algorithms it can be too
expensive to evaluate     on all points in r, so some working set j     r can be
chosen instead, usually at random from r.

greedy selection methods have been used with the subset of regressors (sr),
subset of datapoints (sd) and the projected process (pp) methods described
below.

2as a technical detail, symmetric permutations of the rows and columns are required to

stabilize the computations.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

8.3 approximations for gpr with fixed hyperparameters

175

input: m, desired size of active set
2: initialization i =    , r = {1, . . . , n}

for j := 1 . . . m do

create working set j     r
compute    j for all j     j
i = argmaxj   j   j
update model to include data from example i
i     i     {i}, r     r\{i}

4:

6:

8:

end for

10: return: i
algorithm 8.1: general framework for greedy subset selection.    j is the criterion
function evaluated on data point j.

8.3 approximations for gpr with fixed hy-

perparameters

we present six approximation schemes for gpr below, namely the subset of
regressors (sr), the nystr  om method, the subset of datapoints (sd), the pro-
jected process (pp) approximation, the bayesian committee machine (bcm)
and the iterative solution of linear systems. section 8.3.7 provides a summary
of these methods and a comparison of their performance on the sarcos data
which was introduced in section 2.5.

8.3.1 subset of regressors

pn
silverman [1985, sec. 6.1] showed that the mean gp predictor can be ob-
tained from a    nite-dimensional generalized id75 model f(x   ) =
i=1   ik(x   , xi) with a prior        n (0, k   1). to see this we use the mean
prediction for id75 model in feature space given by eq. (2.11),
i.e.   f(x   ) =      2
n     >. setting   (x   ) =
k(x   ),    =   > = k and      1
  f(x   ) =      2

n   (x   )>a   1  y with a =      1

p +      2

ni)]   1ky

p = k we obtain
n k>(x   )[     2
= k>(x   )(k +   2

n k(k +   2
ni)   1y,

(8.11)
(8.12)

in agreement with eq. (2.25). note, however, that the predictive (co)variance
of this model is di   erent from full gpr.

a simple approximation to this model is to consider only a subset of regres-

sors, so that

mx

i=1

fsr(x   ) =

  ik(x   , xi),

with

  m     n (0, k   1

mm).

(8.13)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

176

approximation methods for large datasets

again using eq. (2.11) we obtain

  fsr(x   ) = km(x   )>(kmnknm +   2

nkmm)   1kmny,

v[fsr(x   )] =   2

nkm(x   )>(kmnknm +   2

nkmm)   1km(x   ).

(8.14)
(8.15)

sr marginal likelihood

nkmm)   1kmny.

thus the posterior mean for   m is given by
    m = (kmnknm +   2

(8.16)
this method has been proposed, for example, in wahba [1990, chapter 7], and
in poggio and girosi [1990, eq. 25] via the id173 framework. the name
   subset of regressors    (sr) was suggested to us by g. wahba. the computa-
tions for equations 8.14 and 8.15 take time o(m2n) to carry out the necessary
matrix computations. after this the prediction of the mean for a new test point
takes time o(m), and the predictive variance takes o(m2).

under the subset of regressors model we have f     n (0,   k) where   k is

y>(   k +   2

log |   k +   2

nin|     1
2

de   ned as in eq. (8.4). thus the log marginal likelihood under this model is
log psr(y|x) =    1
2
notice that the covariance function de   ned by the sr model has the form
  k(x, x0) = k(x)>k   1
mmk(x0), which is exactly the same as that from the nystr  om
approximation for the covariance function eq. (8.7). in fact if the covariance
function k(x, x0) in the predictive mean and variance equations 2.25 and 2.26
is replaced systematically with   k(x, x0) we obtain equations 8.14 and 8.15, as
shown in appendix 8.6.

nin)   1y     n
2

log(2  ). (8.17)

if the id81 decays to zero for |x|         for    xed x0, then   k(x, x)
will be near zero when x is distant from points in the set i. this will be the case
even when the kernel is stationary so that k(x, x) is independent of x. thus
we might expect that using the approximate kernel will give poor predictions,
especially underestimates of the predictive variance, when x is far from points
in the set i.

ysr   (x   ) = pm

an interesting idea suggested by rasmussen and qui  nonero-candela [2005]
to mitigate this problem is to de   ne the sr model with m + 1 basis func-
tions, where the extra basis function is centered on the test point x   , so that
i=1   ik(x   , xi) +      k(x   , x   ). this model can then be used to
make predictions, and it can be implemented e   ciently using the partitioned
matrix inverse equations a.11 and a.12. the e   ect of the extra basis function
centered on x    is to maintain predictive variance at the test point.

so far we have not said how the subset i should be chosen. one sim-
ple method is to choose it randomly from x, another is to run id91 on
{xi}n
i=1 to obtain centres. alternatively, a number of greedy forward selection
algorithms for i have been proposed. luo and wahba [1997] choose the next
kernel so as to minimize the residual sum of squares (rss) |y    knm  m|2 after
optimizing   m. smola and bartlett [2001] take a similar approach, but choose
as their criterion the quadratic form

|y     knm     m|2 +     >

mkmm     m = y>(   k +   2

nin)   1y,

(8.18)

1
  2
n

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

8.3 approximations for gpr with fixed hyperparameters

177

comparison with rvm

where the right hand side follows using eq. (8.16) and the matrix inversion
lemma. alternatively, qui  nonero-candela [2004] suggests using the approxi-
mate log marginal likelihood log psr(y|x) (see eq. (8.17)) as the selection cri-
terion. in fact the quadratic term from eq. (8.18) is one of the terms comprising
log psr(y|x).

for all these suggestions the complexity of evaluating the criterion on a new
example is o(mn), by making use of partitioned matrix equations. thus it is
likely to be too expensive to consider all points in r on each iteration, and we
are likely to want to consider a smaller working set, as described in algorithm
8.1.

note that the sr model is obtained by selecting some subset of the data-
points of size m in a random or greedy manner. the relevance vector machine
(rvm) described in section 6.6 has a similar    avour in that it automatically
selects (in a greedy fashion) which datapoints to use in its expansion. however,
note one important di   erence which is that the rvm uses a diagonal prior on
the      s, while for the sr method we have   m     n (0, k   1

mm).

8.3.2 the nystr  om method

williams and seeger [2001] suggested approximating the gpr equations by
replacing the matrix k by   k in the mean and variance prediction equations
2.25 and 2.26, and called this the nystr  om method for approximate gpr. notice
that in this proposal the covariance function k is not systematically replaced
by   k, it is only occurrences of the matrix k that are replaced. as for the
sr model the time complexity is o(m2n) to carry out the necessary matrix
computations, and then o(n) for the predictive mean of a test point and o(mn)
for the predictive variance.

experimental evidence in williams et al. [2002] suggests that for large m
the sr and nystr  om methods have similar performance, but for small m the
nystr  om method can be quite poor. also the fact that k is not systematically
replaced by   id116 that embarrassments can occur like the approximated
predictive variance being negative. for these reasons we do not recommend the
nystr  om method over the sr method. however, the nystr  om method can be
e   ective when   m+1, the (m + 1)th eigenvalue of k, is much smaller than   2
n.

8.3.3 subset of datapoints

the subset of regressors method described above approximated the form of the
predictive distribution, and particularly the predictive mean. another simple
approximation to the full-sample gp predictor is to keep the gp predictor,
but only on a smaller subset of size m of the data. although this is clearly
wasteful of data, it can make sense if the predictions obtained with m points
are su   ciently accurate for our needs.

clearly it can make sense to select which points are taken into the active set
i, and typically this is achieved by id192. however, one has to be

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

178

approximation methods for large datasets

wary of the amount of computation that is needed, especially if one considers
each member of r at each iteration.

lawrence et al. [2003] suggest choosing as the next point (or site) for in-
clusion into the active set the one that maximizes the di   erential id178 score
   j , h[p(fj)]     h[pnew(fj)], where h[p(fj)] is the
id178 of the gaus-
sian at site j     r (which is a function of the variance at site j as the poste-
rior is gaussian, see eq. (a.20)), and h[pnew(fj)] is the id178 at this site
once the observation at site j has been included. let the posterior variance
of fj before inclusion be vj. as p(fj|yi , yj)     p(fj|yi)n (yj|fj,   2) we have
j +      2. using the fact that the id178 of a gaussian with
(vnew
variance v is log(2  ev)/2 we obtain

)   1 = v   1

j

ivm

   j = 1

2 log(1 + vj/  2).

(8.19)

   j is a monotonic function of vj so that it is maximized by choosing the site with
the largest variance. lawrence et al. [2003] call their method the informative
vector machine (ivm)

if coded na    vely the complexity of computing the variance at all sites in r
on a single iteration is o(m3 + (n    m)m2) as we need to evaluate eq. (2.26) at
ni can be done once in o(m3)
each site (and the matrix inversion of kmm +   2
then stored). however, as we are incrementally growing the matrices kmm
and km(n   m) in fact the cost is o(mn) per inclusion, leading to an overall
complexity of o(m2n) when using a subset of size m. for example, once a site
has been chosen for inclusion the matrix kmm +   2
ni is grown by including an
extra row and column. the inverse of this expanded matrix can be found using
eq. (a.12) although it would be better practice numerically to use a cholesky
decomposition approach as described in lawrence et al. [2003]. the scheme
evaluates    j over all j     r at each step to choose the inclusion site. this
makes sense when m is small, but as it gets larger it can make sense to select
candidate inclusion sites from a subset of r. lawrence et al. [2003] call this the
randomized greedy selection method and give further ideas on how to choose
the subset.

the di   erential id178 score    j is not the only criterion that can be used for
site selection. for example the information gain criterion kl(pnew(fj)||p(fj))
can also be used (see seeger et al., 2003). the use of greedy selection heuristics
here is similar to the problem of active learning, see e.g. mackay [1992c].

8.3.4 projected process approximation

the sr method has the unattractive feature that it is based on a degenerate
gp, the    nite-dimensional model given in eq. (8.13). the sd method is a non-
degenerate process model but it only makes use of m datapoints. the projected
process (pp) approximation is also a non-degenerate process model but it can
make use of all n datapoints. we call it a projected process approximation
as it represents only m < n latent function values, but computes a likelihood
involving all n datapoints by projecting up the m latent points to n dimensions.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

8.3 approximations for gpr with fixed hyperparameters

179

one problem with the basic gpr algorithm is the fact that the likelihood
term requires us to have f-values for the n training points. however, say we only
represent m of these values explicitly, and denote these as fm. then the remain-
ing f-values in r denoted fn   m have a conditional distribution p(fn   m|fm), the
mean of which is given by e[fn   m|fm] = k(n   m)mk   1
mmfm.3 say we replace the
true likelihood term for the points in r by n (yn   m|e[fn   m|fm],   2
ni). including
also the likelihood contribution of the points in set i we have

q(y|fm) = n (y|knmk   1

ni),
(8.20)
mmfm,   2
which can also be written as q(y|fm) = n (y|e[f|fm],   2
ni). the key feature here
is that we have absorbed the information in all n points of d into the m points
in i.

the form of q(y|fm) in eq. (8.20) might seem rather arbitrary, but in fact
it can be shown that if we consider minimizing kl(q(f|y)||p(f|y)), the kl-
divergence between the approximating distribution q(f|y) and the true posterior
p(f|y) over all q distributions of the form q(f|y)     p(f)r(fm) where r is positive
and depends on fm only, this is the form we obtain. see seeger [2003, lemma 4.1
and sec. c.2.1] for detailed derivations, and also csat  o [2002, sec. 3.3].

to make predictions we    rst have to compute the posterior distribution
mmkmn so that e[f|fm] = p >fm. then

q(fm|y). de   ne the shorthand p = k   1
we have

q(y|fm)     exp(cid:0)     1

(y     p >fm)>(y     p >fm)(cid:1).

2  2
n

q(fm|y)     exp(cid:0)     1

combining this with the prior p(fm)     exp(   f>

mk   1
p p >)fm +
which can be recognized as a gaussian n (  , a) with
a   1 =      2
   =      2

n (  2
n ap y = kmm(  2

mm + p p >) =      2

f>
m(k   1

n k   1

nk   1

nkmm + kmnknm)   1kmny.

mm(  2

mm +

1
  2
n

2

mmfm/2) we obtain

y>p >fm

1
  2
n

(cid:1),

nkmm + kmnknm)k   1
mm,

(8.21)

(8.22)

(8.23)
(8.24)

thus the predictive mean is given by
eq[f(x   )] = km(x   )>k   1
= km(x   )>(  2

(8.25)
(8.26)
which turns out to be just the same as the predictive mean under the sr
model, as given in eq. (8.14). however, the predictive variance is di   erent. the
argument is the same as in eq. (3.23) and yields

mm  
nkmm + kmnknm)   1kmny,

vq[f(x   )] = k(x   , x   )     km(x   )>k   1

+ km(x   )>k   1

mmkm(x   )
mmcov(fm|y)k   1

mmkm(x   )

= k(x   , x   )     km(x   )>k   1

+   2

nkm(x   )>(  2

mmkm(x   )
nkmm + kmnknm)   1km(x   ).

(8.27)

3there is no a priori reason why the m points chosen have to be a subset of the n points
in d   they could be disjoint from the training set. however, for our derivations below we
will consider them to be a subset.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

180

approximation methods for large datasets

notice that predictive variance is the sum of the predictive variance under the
sr model (last term in eq. (8.27)) plus k(x   , x   )    km(x   )>k   1
mmkm(x   ) which
is the predictive variance at x    given fm. thus eq. (8.27) is never smaller than
the sr predictive variance and will become close to k(x   , x   ) when x    is far
away from the points in set i.

as for the sr model it takes time o(m2n) to carry out the necessary matrix
computations. after this the prediction of the mean for a new test point takes
time o(m), and the predictive variance takes o(m2).

we have q(y|fm) = n (y|p >fm,   2

ni) and p(fm) = n (0, kmm). by integrat-
ing out fm we    nd that y     n (0,   k +   2
nin). thus the marginal likelihood
for the projected process approximation is the same as that for the sr model
eq. (8.17).

again the question of how to choose which points go into the set i arises.
csat  o and opper [2002] present a method in which the training examples are
presented sequentially (in an    on-line    fashion). given the current active set i
one can compute the novelty of a new input point; if this is large, then this point
is added to i, otherwise the point is added to r. to be precise, the novelty of
an input x is computed as k(x, x)    km(x)>k   1
mmk(x), which can be recognized
as the predictive variance at x given non-noisy observations at the points in i.
if the active set gets larger than some preset maximum size, then points can
be deleted from i, as speci   ed in section 3.3 of csat  o and opper [2002]. later
work by csat  o et al. [2002] replaced the dependence of the algorithm described
above on the input sequence by an expectation-propagation type algorithm (see
section 3.6).

as an alternative method for selecting the active set, seeger et al. [2003]
suggest using a greedy subset selection method as per algorithm 8.1. com-
putation of the information gain criterion after incorporating a new site takes
o(mn) and is thus too expensive to use as a selection criterion. however, an ap-
proximation to the information gain can be computed cheaply (see seeger et al.
[2003, eq. 3] and seeger [2003, sec. c.4.2] for further details) and this allows the
greedy subset algorithm to be run on all points in r on each iteration.

8.3.5 bayesian committee machine

tresp [2000] introduced the bayesian committee machine (bcm) as a way of
speeding up gaussian process regression. let f    be the vector of function val-
ues at the test locations. under gpr we obtain a predictive gaussian distri-
bution for p(f   |d). for the bcm we split the dataset into p parts d1, . . . ,dp
qp
where di = (xi, yi) and make the approximation that p(y1, . . . , yp|f   , x)    
qp
i=1 p(yi|f   , xi). under this approximation we have
i=1 p(f   |di)
p(yi|f   , xi) = c
pp   1(f   )

q(f   |d1, . . . ,dp)     p(f   )

py

(8.28)

,

i=1

where c is a id172 constant. using the fact that the terms in the
numerator and denomination are all gaussian distributions over f    it is easy

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

8.3 approximations for gpr with fixed hyperparameters

181

px

px

to show (see exercise 8.7.1) that the predictive mean and covariance for f    are
given by

eq[f   |d] = [covq(f   |d)]

[cov(f   |di)]   1e[f   |di],

[covq(f   |d)]   1 =    (p     1)k   1       +

i=1

[cov(f   |di)]   1,

(8.29)

(8.30)

i=1

where k       is the covariance matrix evaluated at the test points. here e[f   |di]
and cov(f   |di) are the mean and covariance of the predictions for f    given di,
as given in eqs. (2.23) and (2.24). note that eq. (8.29) has an interesting form
in that the predictions from each part of the dataset are weighted by the inverse
predictive covariance.

we are free to choose how to partition the dataset d. this has two aspects,
the number of partitions and the assignment of data points to the partitions.
if we wish each partition to have size m, then p = n/m. tresp [2000] used
a random assignment of data points to partitions but schwaighofer and tresp
[2003] recommend that id91 the data (e.g. with p-means id91) can
lead to improved performance. however, note that compared to the greedy
schemes used above id91 does not make use of the target y values, only
the inputs x.

although it is possible to make predictions for any number of test points
n   , this slows the method down as it involves the inversion of n       n    matrices.
schwaighofer and tresp [2003] recommend making test predictions on blocks of
size m so that all matrices are of the same size. in this case the computational
complexity of bcm is o(pm3) = o(m2n) for predicting m test points, or
o(mn) per test point.

the bcm approach is transductive [vapnik, 1995] rather than inductive, in
the sense that the method computes a test-set dependent model making use
of the test set input locations. note also that if we wish to make a prediction
at just one test point, it would be necessary to    hallucinate    some extra test
points as eq. (8.28) generally becomes a better approximation as the number of
test points increases.

8.3.6

iterative solution of linear systems

one straightforward method to speed up gp regression is to note that the lin-
ear system (k +   2
ni)v = y can be solved by an iterative method, for example
conjugate gradients (cg). (see golub and van loan [1989, sec. 10.2] for fur-
ther details on the cg method.) conjugate gradients gives the exact solution
(ignoring round-o    errors) if run for n iterations, but it will give an approxi-
mate solution if terminated earlier, say after k iterations, with time complexity
o(kn2). this method has been suggested by wahba et al. [1995] (in the context
of numerical weather prediction) and by gibbs and mackay [1997] (in the con-
text of general gp regression). cg methods have also been used in the context

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

182

approximation methods for large datasets

method m
sd

sr

pp

bcm

smse
0.0813    0.0198
0.0532    0.0046
0.0398    0.0036
0.0290    0.0013
0.0200    0.0008
0.0351    0.0036
0.0259    0.0014
0.0193    0.0008
0.0150    0.0005
0.0110    0.0004
0.0351    0.0036
0.0259    0.0014
0.0193    0.0008
0.0150    0.0005
0.0110    0.0004
0.0314    0.0046
0.0281    0.0055
0.0180    0.0010
0.0136    0.0007

msll
-1.4291    0.0558
-1.5834    0.0319
-1.7149    0.0293
-1.8611    0.0204
-2.0241    0.0151
-1.6088    0.0984
-1.8185    0.0357
-1.9728    0.0207
-2.1126    0.0185
-2.2474    0.0204
-1.6940    0.0528
-1.8423    0.0286
-1.9823    0.0233
-2.1125    0.0202
-2.2399    0.0160
-1.7066    0.0550
-1.7807    0.0820
-2.0081    0.0321
-2.1364    0.0266

mean runtime (s)
0.8
2.1
6.5
25.0
100.7
11.0
27.0
79.5
284.8
927.6
17.3
41.4
95.1
354.2
964.5
506.4
660.5
1043.2
1920.7

256
512
1024
2048
4096
256
512
1024
2048
4096
256
512
1024
2048
4096
256
512
1024
2048

table 8.1: test results on the inverse dynamics problem for a number of di   erent
methods. ten repetitions were used, the mean loss is shown    one standard deviation.

of laplace gpc, where linear systems are solved repeatedly to obtain the map
solution   f (see sections 3.4 and 3.5 for details).

one way that the cg method can be speeded up is by using an approximate
rather than exact matrix-vector multiplication. for example, recent work by
yang et al. [2005] uses the improved fast gauss transform for this purpose.

8.3.7 comparison of approximate gpr methods

above we have presented six approximation methods for gpr. of these, we
retain only those methods which scale linearly with n, so the iterative solu-
tion of linear systems must be discounted. also we discount the nystr  om ap-
proximation in preference to the sr method, leaving four alternatives: subset
of regressors (sr), subset of data (sd), projected process (pp) and bayesian
committee machine (bcm).

table 8.1 shows results of the four methods on the robot arm inverse dy-
namics problem described in section 2.5 which has d = 21 input variables,
44,484 training examples and 4,449 test examples. as in section 2.5 we used
the squared exponential covariance function with a separate length-scale pa-
rameter for each of the 21 input dimensions.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

8.3 approximations for gpr with fixed hyperparameters

183

method
sd
sr
pp
bcm

storage
o(m2) o(m3)
o(mn) o(m2n)
o(mn) o(m2n)
o(mn)

variance
initialization mean
o(m2)
o(m)
o(m2)
o(m)
o(m2)
o(m)
o(mn) o(mn)

table 8.2: a comparison of the space and time complexity of the four methods
using random selection of subsets. initialization gives the time needed to carry out
preliminary matrix computations before the test point x    is known. mean (resp.
variance) refers to the time needed to compute the predictive mean (variance) at x   .

for the sd method a subset of the training data of size m was selected at
random, and the hyperparameters were set by optimizing the marginal likeli-
hood on this subset. as ard was used, this involved the optimization of d + 2
hyperparameters. this process was repeated 10 times, giving rise to the mean
and standard deviation recorded in table 8.1. for the sr, pp and bcm meth-
ods, the same subsets of the data and hyperparameter vectors were used as had
been obtained from the sd experiments.4 note that the m = 4096 result is not
available for bcm as this gave an out-of-memory error.

these experiments were conducted on a 2.0 ghz twin processor machine
with 3.74 gb of ram. the code for all four methods was written in matlab.5
a summary of the time complexities for the four methods are given in table
8.2. thus for a test set of size n    and using full (mean and variance) predictions
we    nd that the sd method has time complexity o(m3) + o(m2n   ), for the
sr and pp methods it is o(m2n) + o(m2n   ), and for the bcm method it
is o(mnn   ). assuming that n        m these reduce to o(m2n   ), o(m2n) and
o(mnn   ) respectively. these complexities are in broad agreement with the
timings in table 8.1.

the results from table 8.1 are plotted in figure 8.1. as we would expect,
the general trend is that as m increases the smse and msll scores decrease.
notice that it is well worth doing runs with small m so as to obtain a learning
curve with respect to m; this helps in getting a feeling of how useful runs at
large m will be. both in terms of smse and msll we see (not surprisingly)
that sd is inferior to the other methods, all of which have similar performance.
these results were obtained using a random selection of the active set. some
experiments were also carried out using active selection for the sd method
(ivm) and for the sr method but these did not lead to signi   cant improve-
ments in performance. for bcm we also experimented with the use of p-means
id91 instead of random assignment to partitions; again this did not lead
to signi   cant improvements in performance. overall on this dataset our con-

4in the bcm case it was only the hyperparameters that were re-used; the data was parti-

tioned randomly into blocks of size m.

5we thank anton schwaighofer for making his bcm code available to us.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

184

approximation methods for large datasets

(a)

(b)

figure 8.1: panel(a): plot of smse against m. panel(b) shows the msll for the four
methods. the error bars denote one standard deviation. for clarity in both panels
the bcm results are slightly displaced horizontally w.r.t. the sr results.

clusion is that for    xed m sr or pp are the methods of choice, as bcm has
longer running times for similar performance. however, notice that if we com-
pare on runtime, then sd for m = 4096 is competitive with the sr, pp and
bcm results for m = 1024 on both time and performance.

in the above experiments the hyperparameters for all methods were set by
optimizing the marginal likelihood of the sd model of size m. this means that
we get a direct comparison of the di   erent methods using the same hyperparam-
eters and subsets. however, one could alternatively optimize the (approximate)
marginal likelihood for each method (see section 8.5) and then compare results.
notice that the hyperparameters which optimize the approximate marginal like-
lihood may depend on the method. for example figure 5.3(b) shows that
the maximum in the marginal likelihood occurs at shorter length-scales as the
amount of data increases. this e   ect has also been observed by v. tresp and
a. schwaighofer (pers. comm., 2004) when comparing the sd marginal likeli-
hood eq. (8.31) with the full marginal likelihood computed on all n datapoints
eq. (5.8).

schwaighofer and tresp [2003] report some experimental comparisons be-
tween the bcm method and some other approximation methods for a number
of synthetic regression problems. in these experiments they optimized the ker-
nel hyperparameters for each method separately. their results are that for    xed
m bcm performs as well as or better than the other methods. however, these
results depend on factors such as the noise level in the data generating pro-
cess; they report (pers. comm., 2005) that for relatively large noise levels bcm
no longer displays an advantage. based on the evidence currently available
we are unable to provide    rm recommendations for one approximation method
over another; further research is required to understand the factors that a   ect
performance.

25651210242048409600.050.1smsemsdsr and ppbcm256512102420484096   2.2   1.8   1.4msllmsdppsrbcmc. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

8.4 approximations for gpc with fixed hyperparameters

185

8.4 approximations for gpc with fixed hy-

perparameters

the approximation methods for gpc are similar to those for gpr, but need
to deal with the non-gaussian likelihood as well, either by using the laplace
approximation, see section 3.4, or expectation propagation (ep), see section
3.6.
in this section we focus mainly on binary classi   cation tasks, although
some of the methods can also be extended to the multi-class case.

pm
for the subset of regressors (sr) method we again use the model fsr(x   ) =
i=1   ik(x   , xi) with   m     n (0, k   1
mm). the likelihood is non-gaussian but
the optimization problem to    nd the map value of   m is convex and can be
obtained using a newton iteration. using the map value     m and the hessian
at this point we obtain a predictive mean and variance for f(x   ) which can be
fed through the sigmoid function to yield probabilistic predictions. as usual
the question of how to choose a subset of points arises; lin et al. [2000] select
these using a id91 method, while zhu and hastie [2002] propose a forward
selection strategy.

the subset of datapoints (sd) method for gpc was proposed in lawrence
et al. [2003], using an ep-style approximation of the posterior, and the di   er-
ential id178 score (see section 8.3.3) to select new sites for inclusion. note
that the ep approximation lends itself very naturally to sparsi   cation: a sparse
model results when some site precisions (see eq. (3.51)) are zero, making the cor-
responding likelihood term vanish. a computational gain can thus be achieved
by ignoring likelihood terms whose site precisions are very small.

the projected process (pp) approximation can also be used with non-
gaussian likelihoods. csat  o and opper [2002] present an    online    method
where the examples are processed sequentially, while csat  o et al. [2002] give
an expectation-propagation type algorithm where multiple sweeps through the
training data are permitted.

the bayesian committee machine (bcm) has also been generalized to deal
with non-gaussian likelihoods in tresp [2000]. as in the gpr case the dataset
is broken up into blocks, but now approximate id136 is carried out using the
laplace approximation in each block to yield an approximate predictive mean
eq[f   |di] and approximate predictive covariance covq(f   |di). these predictions
are then combined as before using equations 8.29 and 8.30.

8.5 approximating the marginal likelihood and    

its derivatives

we consider approximations    rst for gp regression, and then for gp classi   ca-
tion. for gpr, both the sr and pp methods give rise to the same approximate
marginal likelihood as given in eq. (8.17). for the sd method, a very simple

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

186

approximation methods for large datasets

2 y>

2 log |kmm +   2i|    1

m(kmm +   2i)   1ym    m

approximation (ignoring the datapoints not in the active set) is given by
log psd(ym|xm) =     1

2 log(2  ),
(8.31)
where ym is the subvector of y corresponding to the active set; eq. (8.31) is
simply the log marginal likelihood under the model ym     n (0, kmm +   2i).
for the bcm, a simple approach would be to sum eq. (8.31) evaluated on
each partition of the dataset. this ignores interactions between the partitions.
tresp and schwaighofer (pers. comm., 2004) have suggested a more sophisti-
cated bcm-based method which approximately takes these interactions into
account.

for gpc under the sr approximation, one can simply use the laplace or ep
approximations on the    nite-dimensional model. for sd one can again ignore all
datapoints not in the active set and compute an approximation to log p(ym|xm)
using either laplace or ep. for the projected process (pp) method, seeger
[2003, p. 162] suggests the following lower bound

log p(y|x) = log

p(y|f)p(f) df = log

(cid:16) p(y|f)p(f)

(cid:17)

q(f)

q(f) log

df

(8.32)

z

q(f) p(y|f)p(f)

q(f)

df

q(f) log q(y|f) df     kl(q(f)||p(f))

q(fi) log p(yi|fi) dfi     kl(q(fm)||p(fm)),

z

z

z
z
nx

   

=

=

i=1

where q(f) is a shorthand for q(f|y) and eq. (8.32) follows from the equation
on the previous line using jensen   s inequality. the kl divergence term can
be readily evaluated using eq. (a.23), and the one-dimensional integrals can be
tackled using numerical quadrature.

we are not aware of work on extending the bcm approximations to the

marginal likelihood to gpc.

given the various approximations to the marginal likelihood mentioned
above, we may also want to compute derivatives in order to optimize it. clearly
it will make sense to keep the active set    xed during the optimization, although
note that this clashes with the fact that methods that select the active set
might choose a di   erent set as the covariance function parameters    change.
for the classi   cation case the derivatives can be quite complex due to the fact
that site parameters (such as the map values   f, see section 3.4.1) change as
   changes. (we have already seen an example of this in section 5.5 for the
non-sparse laplace approximation.) seeger [2003, sec. 4.8] describes some ex-
periments comparing sd and pp methods for the optimization of the marginal
likelihood on both regression and classi   cation problems.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

8.6 appendix: equivalence of sr and gpr using the nystr  om approximate kernel 187

8.6 appendix: equivalence of sr and gpr us-    

ing the nystr  om approximate kernel

in section 8.3 we derived the subset of regressors predictors for the mean and
variance, as given in equations 8.14 and 8.15. the aim of this appendix is to
show that these are equivalent to the predictors that are obtained by replacing
k(x, x0) systematically with   k(x, x0) in the gpr prediction equations 2.25 and
2.26.

first for the mean. the gpr predictor is e[f(x   )] = k(x   )>(k +   2

replacing all occurrences of k(x, x0) with   k(x, x0) we obtain

ni)   1y.

ni)   1y

e[   f(x   )] =   k(x   )>(   k +   2
= km(x   )>k   1
=      2
=      2
=      2
= km(x   )>q   1kmny,

n km(x   )>k   1
n km(x   )>k   1
n km(x   )>k   1

mm

mm

mmkmn

mmkmn(knmk   1

ni)   1y

mmkmn +   2

(cid:2)in     knmq   1kmn
(cid:3) y
(cid:2)im     kmnknmq   1(cid:3) kmny
nkmmq   1(cid:3) kmny
(cid:2)  2

(8.33)
(8.34)
(8.35)
(8.36)
(8.37)
(8.38)

nkmm + kmnknm, which agrees with eq. (8.14). equation (8.35)
where q =   2
follows from eq. (8.34) by use of the matrix inversion lemma eq. (a.9) and
nkmm + kmnknm)q   1. for
eq. (8.37) follows from eq. (8.36) using im = (  2
the predictive variance we have
v[   f   ] =   k(x   , x   )       k(x   )>(   k +   2

ni)   1  k(x   )

= km(x   )>k   1
km(x   )>k   1
= km(x   )>k   1

mmkm(x   )   
mmkmn(knmk   1
mmkm(x   )     km(x   )>q   1kmnknmk   1

mmkmn +   2

ni)   1knmk   1

= km(x   )>(cid:2)im     q   1kmnknm

(cid:3) k   1

mmkm(x   )

= km(x   )>q   1  2
=   2

nkmmk   1
nkm(x   )>q   1km(x   ),

mmkm(x   )

(8.39)
(8.40)

mmkm(x   )

mmkm(x   ) (8.41)
(8.42)
(8.43)
(8.44)

in agreement with eq. (8.15). the step between eqs. (8.40) and (8.41) is obtained
from eqs. (8.34) and (8.38) above, and eq. (8.43) follows from eq. (8.42) using
im = (  2

nkmm + kmnknm)q   1.

8.7 exercises

1. verify that the mean and covariance of the bcm predictions (equations
8.29 and 8.30) are correct. if you are stuck, see tresp [2000] for details.
mm show that e(copt) =
tr(k       k), where   k = knmk   1
mmkmn. now consider adding one data-
point into set i, so that kmm grows to k(m+1)(m+1). using eq. (a.12)

2. using eq. (8.10) and the fact that copt = knmk   1

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

188

approximation methods for large datasets

show that the change in e due to adding the extra datapoint can be
computed in time o(mn).
if you need help, see sch  olkopf and smola
[2002, sec. 10.2.2] for further details.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

chapter 9

further issues and
conclusions

in the previous chapters of the book we have concentrated on giving a solid
grounding in the use of gps for regression and classi   cation problems, includ-
ing model selection issues, approximation methods for large datasets, and con-
nections to related models. in this chapter we provide some short descriptions
of other issues relating to gaussian process prediction, with pointers to the
literature for further reading.

so far we have mainly discussed the case when the output target y is a single
label, but in section 9.1 we describe how to deal with the case that there are
multiple output targets. similarly, for the regression problem we have focussed
on i.i.d. gaussian noise; in section 9.2 we relax this condition to allow the
noise process to have correlations. the classi   cation problem is characterized
by a non-gaussian likelihood function; however, there are other non-gaussian
likelihoods of interest, as described in section 9.3.

we may not only have observations of function values, by also on derivatives
of the target function. in section 9.4 we discuss how to make use of this infor-
mation in the gpr framework. also it may happen that there is noise on the
observation of the input variable x; in section 9.5 we explain how this can be
handled. in section 9.6 we mention how more    exible models can be obtained
using mixtures of gaussian process models.

as well as carrying out prediction for test inputs, one might also wish to try
to    nd the global optimum of a function within some compact set. approaches
based on gaussian processes for this problem are described in section 9.7. the
use of gaussian processes to evaluate integrals is covered in section 9.8.

by using a scale mixture of gaussians construction one can obtain a mul-
tivariate student   s t distribution. this construction can be extended to give a
student   s t process, as explained in section 9.9. one key aspect of the bayesian
framework relates to the incorporation of prior knowledge into the problem

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

190

further issues and conclusions

formulation. in some applications we not only have the dataset d but also ad-
ditional information. for example, for an id42 problem
we know that translating the input pattern by one pixel will not change the
label of the pattern. approaches for incorporating this knowledge are discussed
in section 9.10.

in this book we have concentrated on supervised learning problems. how-
ever, gps can be used as components in unsupervised learning models, as de-
scribed in section 9.11. finally, we close with some conclusions and an outlook
to the future in section 9.12.

9.1 multiple outputs

throughout this book we have concentrated on the problem of predicting a
single output variable y from an input x. however, it can happen that one
may wish to predict multiple output variables (or channels) simultaneously.
for example in the robot inverse dynamics problem described in section 2.5
there are really seven torques to be predicted. a simple approach is to model
each output variable as independent from the others and treat them separately.
however, this may lose information and be suboptimal.

one way in which correlation can occur is through a correlated noise process.
even if the output channels are a priori independent, if the noise process is
correlated then this will induce correlations in the posterior processes. such
a situation is easily handled in the gp framework by considering the joint,
block-diagonal, prior over the function values of each channel.

another way that correlation of multiple channels can occur is if the prior
already has this structure. for example in geostatistical situations there may be
correlations between the abundances of di   erent ores, e.g. silver and lead. this
situation requires that the covariance function models not only the correlation
structure of each channel, but also the cross-correlations between channels.
some work on this topic can be found in the geostatistics literature under
the name of cokriging, see e.g. cressie [1993, sec. 3.2.3]. one way to induce
correlations between a number of output channels is to obtain them as linear
combinations of a number of latent channels, as described in teh et al. [2005];
see also micchelli and pontil [2005]. a related approach is taken by boyle and
frean [2005] who introduce correlations between two processes by deriving them
as di   erent convolutions of the same underlying white noise process.

9.2 noise models with dependencies

the noise models used so far have almost exclusively assumed gaussianity and
independence. non-gaussian likelihoods are mentioned in section 9.3 below.
inside the family of gaussian noise models, it is not di   cult to model depen-
dencies. this may be particularly useful in models involving time. we simply
add terms to the noise covariance function with the desired structure, including

cokriging

coloured noise

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

191

arma

9.3 non-gaussian likelihoods

hyperparameters. in fact, we already used this approach for the atmospheric
carbon dioxide modelling task in section 5.4.3. also, murray-smith and girard
[2001] have used an autoregressive moving-average (arma) noise model (see
also eq. (b.51)) in a gp regression task.

9.3 non-gaussian likelihoods

our main focus has been on regression with gaussian noise, and classi   cation
using the logistic or probit response functions. however, gaussian processes can
be used as priors with other likelihood functions. for example, diggle et al.
[1998] were concerned with modelling count data measured geographically using
a poisson likelihood with a spatially varying rate. they achieved this by placing
a gp prior over the log poisson rate.

goldberg et al. [1998] stayed with a gaussian noise model, but introduced
heteroscedasticity, i.e. allowing the noise variance to be a function of x. this
was achieved by placing a gp prior on the log variance function. neal [1997]
robusti   ed gp regression by using a student   s t-distributed noise model rather
than gaussian noise. chu and ghahramani [2005] have described how to use
gps for the ordinal regression problem, where one is given ranked preference
information as the target data.

9.4 derivative observations

since di   erentiation is a linear operator, the derivative of a gaussian process
is another gaussian process. thus we can use gps to make predictions about
derivatives, and also to make id136 based on derivative information.
in
general, we can make id136 based on the joint gaussian distribution of
function values and partial derivatives. a covariance function k(  ,  ) on function
values implies the following (mixed) covariance between function values and
partial derivatives, and between partial derivatives

cov(cid:0)fi,

(cid:1) =    k(xi, xj)

,

   xdj

   fj
   xdj

cov(cid:0)    fi

   xdi

,

   fj
   xej

(cid:1) =    2k(xi, xj)

   xdi   xej

,

(9.1)

see e.g. papoulis [1991, ch. 10] or adler [1981, sec. 2.2]. with n datapoints in
d dimensions, the complete joint distribution of f and its d partial derivatives
involves n(d+1) quantities, but in a typical application we may only have access
to or interest in a subset of these; we simply remove the rows and columns
from the joint matrix which are not needed. observed function values and
derivatives may often have di   erent noise levels, which are incorporated by
adding a diagonal contribution with di   ering hyperparameters. id136 and
predictions are done as usual. this approach was used in the context of learning
in dynamical systems by solak et al. [2003]. in figure 9.1 the posterior process
with and without derivative observations are compared. noise-free derivatives
may be a useful way to enforce known constraints in a modelling problem.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

192

further issues and conclusions

(a)

(b)

figure 9.1: in panel (a) we show four data points in a one dimensional noise-free
regression problem, together with three functions sampled from the posterior and the
95% con   dence region in light grey. in panel (b) the same observations have been
augmented by noise-free derivative information, indicated by small tangent segments
at the data points. the covariance function is the squared exponential with unit
process variance and unit length-scale.

9.5 prediction with uncertain inputs

it can happen that the input values to a prediction problem can be uncer-
tain. for example, for a discrete time series one can perform multi-step-ahead
predictions by iterating one-step-ahead predictions. however, if the one-step-
ahead predictions include uncertainty, then it is necessary to propagate this
uncertainty forward to get the proper multi-step-ahead predictions. one sim-
ple approach is to use sampling methods. alternatively, it may be possible to
use analytical approaches. girard et al. [2003] showed that it is possible to
compute the mean and variance of the output analytically when using the se
covariance function and gaussian input noise.

more generally, the problem of regression with uncertain inputs has been
studied in the statistics literature under the name of errors-in-variables regres-
sion. see dellaportas and stephens [1995] for a bayesian treatment of the
problem and pointers to the literature.

9.6 mixtures of gaussian processes

in chapter 4 we have seen many ideas for making the covariance functions
more    exible. another route is to use a mixture of di   erent gaussian process
models, each one used in some local region of input space. this kind of model
is generally known as a mixture of experts model and is due to jacobs et al.
[1991]. in addition to the local expert models, the model has a manager that
(probabilistically) assigns points to the experts. rasmussen and ghahramani
[2002] used gaussian process models as local experts, and based their manager
on another type of stochastic process: the dirichlet process. id136 in this
model required mcmc methods.

   4   2024   2   1012input, xoutput, y(x)   4   2024   2   1012input, xoutput, y(x)c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

9.7 global optimization

193

9.7 global optimization

often one is faced with the problem of being able to evaluate a continuous
function g(x), and wishing to    nd the global optimum (maximum or minimum)
of this function within some compact set a     rd. there is a very large
literature on the problem of global optimization; see neumaier [2005] for a
useful overview.

given a dataset d = {(xi, g(xi))|i=1, . . . , n}, one appealing approach is to
   t a gp regression model to this data. this will give a mean prediction and
predictive variance for every x     a. jones [2001] examines a number of criteria
that have been suggested for where to make the next function evaluation based
on the predictive mean and variance. one issue with this approach is that one
may need to search to    nd the optimum of the criterion, which may itself be
multimodal optimization problem. however, if evaluations of g are expensive
or time-consuming, it can make sense to work hard on this new optimization
problem.

for historical references and further work in this area see jones [2001] and

ritter [2000, sec. viii.2].

9.8 evaluation of integrals

another interesting and unusual application of gaussian processes is for the
evaluation of the integrals of a deterministic function f. one evaluates the
function at a number of locations, and then one can use a gaussian process as
a posterior over functions. this posterior over functions induces a posterior over
the value of the integral (since each possible function from the posterior would
give rise to a particular value of the integral). for some covariance functions
(e.g. the squared exponential), one can compute the expectation and variance of
the value of the integral analytically. it is perhaps unusual to think of the value
of the integral as being random (as it does have one particular deterministic
value), but it is perfectly in line of bayesian thinking that you treat all kinds
of uncertainty using probabilities. this idea was proposed under the name of
bayes-hermite quadrature by o   hagan [1991], and later under the name of
bayesian monte carlo in rasmussen and ghahramani [2003].

another approach is related to the ideas of global optimization in the section
9.7 above. one can use a gp model of a function to aid an mcmc sampling
procedure, which may be advantageous if the function of interest is computa-
tionally expensive to evaluate. rasmussen [2003] combines hybrid monte carlo
with a gp model of the log of the integrand, and also uses derivatives of the
function (discussed in section 9.4) to get an accurate model of the integrand
with very few evaluations.

combining gps
with mcmc

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

194

scale mixture

noise entanglement

further issues and conclusions

9.9 student   s t process

a student   s t process can be obtained by applying the scale mixture of gaus-
sians construction of a student   s t distribution to a gaussian process [o   hagan,
1991, o   hagan et al., 1999]. we divide the covariances by the scalar    and put
a gamma distribution on    with shape    and mean    so that
      (  )         1 exp

  k(x, x0) =      1k(x, x0),

(cid:16)          

p(  |  ,   ) =

(9.2)

(cid:17)

    

  

,

z

where k is any valid covariance function. now the joint prior distribution of
any    nite number n of function values y becomes
n (y|0,      1ky)p(  |  ,   )d  
  (   + n/2)(2    )   n/2

(cid:17)   (  +n/2)

p(y|  ,   ,   ) =

  y>k   1
y y

(9.3)

(cid:16)

=

  (  )|     1ky|   1/2

1 +

2  

,

which is recognized as the zero mean, multivariate student   s t distribution
with 2   degrees of freedom: p(y|  ,   ,   ) = t (0,      1ky, 2  ). we could state a
de   nition analogous to de   nition 2.1 on page 13 for the gaussian process, and
write

(9.4)
cf. eq. (2.14). the marginal likelihood can be directly evaluated using eq. (9.3),
and training can be achieved using the methods discussed in chapter 5 regarding
   and    as hyperparameters. the predictive distribution for test cases are also
t distributions, the derivation of which is left as an exercise below.

f     t p(0,      1k, 2  ),

notice that the above construction is clear for noise-free processes, but that
the interpretation becomes more complicated if the covariance function k(x, x0)
contains a noise contribution. the noise and signal get entangled by the com-
mon factor   , and the observations can no longer be written as the sum of
independent signal and noise contributions. allowing for independent noise
contributions removes analytic tractability, which may reduce the usefulness of
the t process.
exercise using the scale mixture representation from eq. (9.3) derive the poste-
rior predictive distribution for a student   s t process.
exercise consider the generating process implied by eq. (9.2), and write a pro-
gram to draw functions at random. characterize the di   erence between the
student   s t process and the corresponding gaussian process (obtained in the
limit           ), and explain why the t process is perhaps not as exciting as one
might have hoped.

9.10

invariances

it can happen that the input is apparently in vector form but in fact it has
additional structure. a good example is a pixelated image, where the 2-d array

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

9.10 invariances

195

of pixels can be arranged into a vector (e.g. in raster-scan order). imagine that
the image is of a handwritten digit; then we know that if the image is translated
by one pixel it will remain the same digit. thus we have knowledge of certain
invariances of the input pattern. in this section we describe a number of ways
in which such invariances can be exploited. our discussion is based on sch  olkopf
and smola [2002, ch. 11].

prior knowledge about the problem tells us that certain transformations of
the input would leave the class label invariant   these include simple geometric
transformations such as translations, rotations,1 rescalings, and rather less ob-
vious ones such as line thickness transformations.2 given enough data it should
be possible to learn the correct input-output mapping, but it would make sense
to try to make use of these known invariances to reduce the amount of training
data needed. there are at least three ways in which this prior knowledge has
been used, as described below.

the    rst approach is to generate synthetic training examples by applying
valid transformations to the examples we already have. this is simple but it
does have the disadvantage of creating a larger training set. as kernel-machine
training algorithms typically scale super-linearly with n this can be problematic.
a second approach is to make the predictor invariant to small transforma-
tions of each training case; this method was    rst developed by simard et al.
[1992] for neural networks under the name of    tangent prop   . for a single
training image we consider the manifold of images that are generated as various
transformations are applied to it. this manifold will have a complex structure,
but locally we can approximate it by a tangent space. the idea in    tangent
prop    is that the output should be invariant to perturbations of the training
example in this tangent space. for neural networks it is quite straightforward
to modify the training objective function to penalize deviations from this in-
variance, see simard et al. [1992] for details. section 11.4 in sch  olkopf and
smola [2002] describes some ways in which these ideas can be extended to
kernel machines.

the third approach to dealing with invariances is to develop a representation
of the input which is invariant to some or all of the transformations. for
example, binary images of handwritten digits are sometimes    skeletonized    to
remove the e   ect of line thickness. if an invariant representation can be achieved
for all transformations it is the most desirable, but it can be di   cult or perhaps
impossible to achieve. for example, if a given training pattern can belong to
more than one class (e.g. an ambiguous handwritten digit) then it is clearly not
possible to    nd a new representation which is invariant to transformations yet
leaves the classes distinguishable.

1the digit recognition problem is only invariant to small rotations; we must avoid turning

a 6 into a 9.

2i.e. changing the thickness of the pen we write with within reasonable bounds does not

change the digit we write.

synthetic training
examples

tangent prop

invariant representation

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

196

further issues and conclusions

9.11 latent variable models

gtm

gplvm

our main focus in this book has been on supervised learning. however, gps
have also been used as components for models carrying out non-linear dimen-
sionality reduction, a form of unsupervised learning. the key idea is that data
which is apparently high-dimensional (e.g. a pixelated image) may really lie on
a low-dimensional non-linear manifold which we wish to model.

let z     rl be a latent (or hidden) variable, and let x     rd be a visible
variable. we suppose that our visible data is generated by picking a point in
z-space and mapping this point into the data space through a (possibly non-

linear) mapping, and then optionally adding noise. thus p(x) =r p(x|z)p(z)dz.

if the mapping from z to x is linear and z has a gaussian distribution then
this is the factor analysis model, and the mean and covariance of the gaussian
in x-space can easily be determined. however, if the mapping is non-linear
then the integral cannot be computed exactly. in the generative topographic
mapping (gtm) model [bishop et al., 1998b] the integral was approximated
using a grid of points in z-space. in the original gtm paper the non-linear
mapping was taken to be a linear combination of non-linear basis functions,
but in bishop et al. [1998a] this was replaced by a gaussian process mapping
between the latent and visible spaces.

more recently lawrence [2004] has introduced a rather di   erent model known
as the gaussian process latent variable model (gplvm). instead of having a
prior (and thus a posterior) distribution over the latent space, we consider that
each data point xi is derived from a corresponding latent point zi through
a non-linear mapping (with added noise).
if a gaussian process is used for
this non-linear mapping, then one can easily write down the joint distribution
p(x|z) of the visible variables conditional on the latent variables. optimization
routines can then be used to    nd the locations of the latent points that opti-
mize p(x|z). this has some similarities to the work on regularized principal
manifolds [sch  olkopf and smola, 2002, ch. 17] except that in the gplvm one
integrates out the latent-to-visible mapping rather than optimizing it.

9.12 conclusions and future directions

in this section we brie   y wrap up some of the threads we have developed
throughout the book, and discuss possible future directions of work on gaussian
processes.

in chapter 2 we saw how gaussian process regression is a natural extension
of bayesian id75 to a more    exible class of models. for gaussian
noise the model can be treated analytically, and is simple enough that the gp
model could be often considered as a replacement for the traditional linear
analogue. we have also seen that historically there have been numerous ideas
along the lines of gaussian process models, although they have only gained a
sporadic following.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

9.12 conclusions and future directions

197

one may indeed speculate, why are gps not currently used more widely in
applications? we see three major reasons: (1) firstly, that the application of
gaussian processes requires the handling (inversion) of large matrices. while
these kinds of computations were tedious 20 years ago, and impossible further
in the past, even na    ve implementations su   ce for moderate sized problems on
an anno 2005 pc. (2) another possibility is that most of the historical work
on gps was done using    xed covariance functions, with very little guide as
to how to choose such functions. the choice was to some degree arbitrary,
and the idea that one should be able to infer the structure or parameters of
the covariance function as we discuss in chapter 5 is not so well known. this
is probably a very important step in turning gps into an interesting method
for practitioners. (3) the viewpoint of placing gaussian process priors over
functions is a bayesian one. although the adoption of bayesian methods in the
machine learning community is quite widespread, these ideas have not always
been appreciated more widely in the statistics community.

although modern computers allow simple implementations for up to a few
thousand training cases, the computational constraints are still a signi   cant
limitation for applications where the datasets are signi   cantly larger than this.
in chapter 8 we have given an overview of some of the recent work on approx-
imations for large datasets. although there are many methods and a lot of
work is currently being undertaken, both the theoretical and practical aspects
of these approximations need to be understood better in order to be a useful
tool to the practitioner.

the computations required for the gaussian process classi   cation models
developed in chapter 3 are a lot more involved than for regression. although
the theoretical foundations of gaussian process classi   cation are well developed,
it is not yet clear under which circumstances one would expect the extra work
and approximations associated with treating a full probabilistic latent variable
model to pay o   . the answer may depend heavily on the ability to learn
meaningful covariance functions.

the incorporation of prior knowledge through the choice and parameter-
ization of the covariance function is another prime target for future work on
gps. in chapter 4 we have presented many families of covariance functions with
widely di   ering properties, and in chapter 5 we presented principled methods
for choosing between and adapting covariance functions. particularly in the
machine learning community, there has been a tendency to view gaussian pro-
cesses as a    black box      what exactly goes on in the box is less important, as
long as it gives good predictions. to our mind, we could perhaps learn some-
thing from the statisticians here, and ask how and why the models work. in fact
the hierarchical formulation of the covariance functions with hyperparameters,
the testing of di   erent hypotheses and the adaptation of hyperparameters gives
an excellent opportunity to understand more about the data.

we have attempted to illustrate this line of thinking with the carbon dioxide
prediction example developed at some length in section 5.4.3. although this
problem is comparatively simple and very easy to get an intuitive understanding
of, the principles of trying out di   erent components in the covariance structure

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

198

further issues and conclusions

and adapting their parameters could be used universally. indeed, the use of
the isotropic squared exponential covariance function in the digit classi   cation
examples in chapter 3 is not really a choice which one would expect to provide
very much insight to the classi   cation problem. although some of the results
presented are as good as other current methods in the literature, one could
indeed argue that the use of the squared exponential covariance function for this
task makes little sense, and the low error rate is possibly due to the inherently
low di   culty of the task. there is a need to develop more sensible covariance
functions which allow for the incorporation of prior knowledge and help us to
gain real insight into the data.

going beyond a simple vectorial representation of the input data to take
into account structure in the input domain is also a theme which we see as very
important. examples of this include the invariances described in section 9.10
arising from the structure of images, and the kernels described in section 4.4
which encode structured objects such as strings and trees.

as this brief discussion shows, we see the current level of development of
gaussian process models more as a rich, principled framework for supervised
learning than a fully-developed set of tools for applications. we    nd the gaus-
sian process framework very appealing and are con   dent that the near future
will show many important developments, both in theory, methodology and prac-
tice. we look forward very much to following these developments.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

appendix a

mathematical background

a.1 joint, marginal and id155

let the n (discrete or continuous) random variables y1, . . . , yn have a joint
id203 p(y1, . . . , yn), or p(y) for short.1 technically, one ought to distin-
guish between probabilities (for discrete variables) and id203 densities for
continuous variables. throughout the book we commonly use the term    prob-
ability    to refer to both. let us partition the variables in y into two groups, ya
and yb, where a and b are two disjoint sets whose union is the set {1, . . . , n},
so that p(y) = p(ya, yb). each group may contain one or more variables.

the marginal id203 of ya is given by

z

p(ya) =

p(ya, yb) dyb.

(a.1)

the integral is replaced by a sum if the variables are discrete valued. notice
that if the set a contains more than one variable, then the marginal id203
is itself a joint id203   whether it is referred to as one or the other depends
on the context. if the joint distribution is equal to the product of the marginals,
then the variables are said to be independent, otherwise they are dependent.

the id155 function is de   ned as
p(ya|yb) = p(ya, yb)
p(yb)

,

(a.2)

de   ned for p(yb) > 0, as it is not meaningful to condition on an impossible
event. if ya and yb are independent, then the marginal p(ya) and the condi-
tional p(ya|yb) are equal.

1one can deal with more general cases where the density function does not exist by using

the distribution function.

joint id203

marginal id203

independence

id155

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

200

bayes    rule

gaussian de   nition

conditioning and
marginalizing

products

mathematical background

theorem

using the de   nitions of both p(ya|yb) and p(yb|ya) we obtain bayes   

p(ya|yb) = p(ya)p(yb|ya)

p(yb)

.

(a.3)

since conditional distributions are themselves probabilities, one can use all of
the above also when further conditioning on other variables. for example, in
supervised learning, one often conditions on the inputs throughout, which would
lead e.g. to a version of bayes    rule with additional conditioning on x in all
four probabilities in eq. (a.3); see eq. (2.5) for an example of this.

a.2 gaussian identities

the multivariate gaussian (or normal) distribution has a joint id203 den-
sity given by

p(x|m,   ) = (2  )   d/2|  |   1/2 exp(cid:0)    1

2(x     m)>     1(x     m)(cid:1),

(a.4)

where m is the mean vector (of length d) and    is the (symmetric, positive
de   nite) covariance matrix (of size d    d). as a shorthand we write x    
n (m,   ).

let x and y be jointly gaussian random vectors

(cid:21)

(cid:20) x

y

(cid:20) a

(cid:18)(cid:20)   x

(cid:21)

  y

,

c
c> b

(cid:21)(cid:19)

    n

 (cid:20)   x

(cid:21)

  y

(cid:20)   a

,

  c>

(cid:21)   1!

  c
  b

= n

,

(a.5)

then the marginal distribution of x and the conditional distribution of x given
y are

x     n (  x, a), and x|y     n(cid:0)  x + cb   1(y       y), a     cb   1c>(cid:1)

or x|y     n(cid:0)  x       a   1   c(y       y),   a   1(cid:1).

(a.6)

see, e.g. von mises [1964, sec. 9.3], and eqs. (a.11 - a.13).

the product of two gaussians gives another (un-normalized) gaussian
n (x|a, a)n (x|b, b) = z   1n (x|c, c)

(a.7)

where c = c(a   1a + b   1b) and c = (a   1 + b   1)   1.

notice that the resulting gaussian has a precision (inverse variance) equal to
the sum of the precisions and a mean equal to the convex sum of the means,
weighted by the precisions. the normalizing constant looks itself like a gaussian
(in a or b)

z   1 = (2  )   d/2|a + b|   1/2 exp(cid:0)    1

2(a     b)>(a + b)   1(a     b)(cid:1).

(a.8)

to prove eq. (a.7) simply write out the (lengthy) expressions by introducing
eq. (a.4) and eq. (a.8) into eq. (a.7), and expand the terms inside the exp to

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

a.3 matrix identities

201

generating multivariate
gaussian samples

verify equality. hint: it may be helpful to expand c using the matrix inversion
lemma, eq. (a.9), c = (a   1+b   1)   1 = a   a(a+b)   1a = b   b(a+b)   1b.
to generate samples x     n (m, k) with arbitrary mean m and covariance
matrix k using a scalar gaussian generator (which is readily available in many
programming environments) we proceed as follows:    rst, compute the cholesky
decomposition (also known as the    matrix square root   ) l of the positive def-
inite symmetric covariance matrix k = ll>, where l is a lower triangular
matrix, see section a.4. then generate u     n (0, i) by multiple separate calls
to the scalar gaussian generator. compute x = m + lu, which has the desired
distribution with mean m and covariance le[uu>]l> = ll> = k (by the
independence of the elements of u).

in practice it may be necessary to add a small multiple of the identity
matrix  i to the covariance matrix for numerical reasons. this is because the
eigenvalues of the matrix k can decay very rapidly (see section 4.3.1 for a
closely related analytical result) and without this stabilization the cholesky
decomposition fails. the e   ect on the generated samples is to add additional
independent noise of variance  . from the context   can usually be chosen to
have inconsequential e   ects on the samples, while ensuring numerical stability.

a.3 matrix identities

the matrix inversion lemma, also known as the woodbury, sherman & morri- matrix inversion lemma
son formula (see e.g. press et al. [1992, p. 75]) states that

(z + u w v >)   1 = z   1     z   1u(w    1 + v >z   1u)   1v >z   1,

(a.9)
assuming the relevant inverses all exist. here z is n  n, w is m  m and u and v
are both of size n  m; consequently if z   1 is known, and a low rank (i.e. m < n)
perturbation is made to z as in left hand side of eq. (a.9), considerable speedup
can be achieved. a similar equation exists for determinants

|z + u w v >| = |z| |w| |w    1 + v >z   1u|.

(a.10)

let the invertible n    n matrix a and its inverse a   1 be partitioned into

(cid:18) p q

(cid:19)

r s

(cid:19)

(cid:18)   p

  q
  r   s

a =

,

a   1 =

,

(a.11)

determinants

inversion of a
partitioned matrix

where p and   p are n1    n1 matrices and s and   s are n2    n2 matrices with
n = n1 + n2. the submatrices of a   1 are given in press et al. [1992, p. 77] as

  p = p    1 + p    1qm rp    1
  q =    p    1qm
  r =    m rp    1
  s = m

                      where m = (s     rp    1q)   1,

(a.12)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

202

mathematical background

or equivalently

  p = n
  q =    n qs   1
  r =    s   1rn
  s = s   1 + s   1rn qs   1

                      where n = (p     qs   1r)   1.

a.3.1 matrix derivatives

derivatives of the elements of an inverse matrix:
k   1 =    k   1    k
     

   
     

k   1,

(a.13)

(a.14)

where    k
positive de   nite symmetric matrix we have

      is a matrix of elementwise derivatives. for the log determinant of a

log |k| = tr(cid:0)k   1    k

(cid:1).

     

   
     

(a.15)

derivative of inverse

derivative of log
determinant

a.3.2 matrix norms
the frobenius norm kakf of a n1    n2 matrix a is de   ned as

n1x

n2x

kak2

f =

|aij|2 = tr(aa>),

(a.16)

[golub and van loan, 1989, p. 56].

i=1

j=1

a.4 cholesky decomposition

the cholesky decomposition of a symmetric, positive de   nite matrix a decom-
poses a into a product of a lower triangular matrix l and its transpose

ll> = a,

(a.17)

where l is called the cholesky factor. the cholesky decomposition is useful
for solving linear systems with symmetric, positive de   nite coe   cient matrix
a. to solve ax = b for x,    rst solve the triangular system ly = b by forward
substitution and then the triangular system l>x = y by back substitution.
using the backslash operator, we write the solution as x = l>\(l\b), where
the notation a\b is the vector x which solves ax = b. both the forward and
backward substitution steps require n2/2 operations, when a is of size n    n.
the computation of the cholesky factor l is considered numerically extremely
stable and takes time n3/6, so it is the method of choice when it can be applied.

solving linear systems

computational cost

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

a.5 id178 and id181

note also that the determinant of a positive de   nite symmetric matrix can be
calculated e   ciently by

ny

nx

|a| =

ii, or
l2

log |a| = 2

log lii,

(a.18)

where l is the cholesky factor from a.

i=1

i=1

a.5 id178 and id181

the id178 h[p(x)] of a distribution p(x) is a non-negative measure of the
amount of    uncertainty    in the distribution, and is de   ned as

h[p(x)] =    

p(x) log p(x) dx.

(a.19)

z

the integral is substituted by a sum for discrete variables. id178 is measured
in bits if the log is to the base 2 and in nats in the case of the natural log. the
id178 of a gaussian in d dimensions, measured in nats is

h[n (  ,   )] = 1

(a.20)
the kullback-leibler (kl) divergence (or relative id178) kl(p||q) be-

2 (log 2  e).

tween two distributions p(x) and q(x) is de   ned as
p(x) log p(x)

kl(p||q) =

q(x) dx.

(a.21)

2 log |  | + d
z

it is easy to show that kl(p||q)     0, with equality if p = q (almost everywhere).
for the case of two bernoulli random variables p and q this reduces to

klber(p||q) = p log p
q

+ (1     p) log

(1     p)
(1     q) ,

(a.22)

where we use p and q both as the name and the parameter of the bernoulli
distributions. for two gaussian distributions n (  0,   0) and n (  1,   1) we
have [kullback, 1959, sec. 9.1]

kl(n0||n1) = 1

0 | +

(cid:0)(  0       1)(  0       1)> +   0       1

(cid:1).

consider a general distribution p(x) on rd and a gaussian distribution q(x) =
n (  ,   ). then

kl(p||q) =

2(x       )>     1(x       )p(x) dx +

z

2 log 2   +

p(x) log p(x) dx.

1

1

2 log |  1     1
2 tr      1
z
2 log |  | + d

1

1

(a.23)

(a.24)

203

determinant

id178

divergence of bernoulli
random variables

divergence of gaussians

minimizing kl(p||q)
divergence leads to
moment matching

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

204

mathematical background

equation (a.24) can be minimized w.r.t.    and    by di   erentiating w.r.t. these
parameters and setting the resulting expressions to zero. the optimal q is the
one that matches the    rst and second moments of p.

the kl divergence can be viewed as the extra number of nats needed on
average to code data generated from a source p(x) under the distribution q(x)
as opposed to p(x).

a.6 limits

the limit of a rational quadratic is a squared exponential

(cid:0)1 + x2

(cid:1)      = exp(cid:0)    x2

(cid:1).

2

lim
        

2  

(a.25)

a.7 measure and integration

here we sketch some de   nitions concerning measure and integration; fuller
treatments can be found e.g. in doob [1994] and bartle [1995].

let     be the set of all possible outcomes of an experiment. for example, for
a d-dimensional real-valued variable,     = rd. let f be a   -   eld of subsets
of     which contains all the events in whose occurrences we may be interested.2
then    is a countably additive measure if it is real and non-negative and for
all mutually disjoint sets a1, a2, . . .     f we have

  (cid:0)    [

(cid:1) =

   x

ai

  (ai).

(a.26)

   nite measure
id203 measure
lebesgue measure

i=1

i=1

if   (   ) <     then    is called a    nite measure and if   (   ) = 1 it is called
a id203 measure. the lebesgue measure de   nes a uniform measure over
subsets of euclidean space. here an appropriate   -algebra is the borel   -algebra
bd, where b is the   -algebra generated by the open subsets of r. for example
on the line r the lebesgue measure of the interval (a, b) is b     a.

we now restrict     to be rd and wish to give meaning to integration of a

function f : rd     r with respect to a measure   

z

f(x) d  (x).

(a.27)

we assume that f is measurable, i.e. that for any borel-measurable set a     r,
f   1(a)     bd. there are two cases that will interest us (i) when    is the
lebesgue measure and (ii) when    is a id203 measure. for the    rst case

expression (a.27) reduces to the usual integral notationr f(x)dx.

2the restriction to a   -   eld of subsets is important technically to avoid paradoxes such as
the banach-tarski paradox. informally, we can think of the   -   eld as restricting consideration
to    reasonable    subsets.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

a.8 fourier transforms

205

for a id203 measure    on x, the non-negative function p(x) is called

the density of the measure if for all a     bd we have

  (a) =

p(x) dx.

(a.28)

if such a density exists it is uniquely determined almost everywhere, i.e. except
for sets with measure zero. not all id203 measures have densities   only
distributions that assign zero id203 to individual points in x-space can
have densities.3 if p(x) exists then we have

z

f(x) d  (x) =

f(x)p(x) dx.

(a.29)

z

a

z

if    does not have a density expression (a.27) still has meaning by the standard
construction of the lebesgue integral.

for     = rd the id203 measure    can be related to the distribution
function f : rd     [0, 1] which is de   ned as f (z) =   (x1     z1, . . . xd    
zd). the distribution function is more general than the density as it is always
de   ned for a given id203 measure. a simple example of a random variable
which has a distribution function but no density is obtained by the following
construction: a coin is tossed and with id203 p it comes up heads; if it
comes up heads x is chosen from u(0, 1) (the uniform distribution on [0, 1]),
otherwise (with id203 1    p) x is set to 1/2. this distribution has a    point
mass    (or atom) at x = 1/2.

   point mass    example

a.7.1 lp spaces
let    be a measure on an input set x . for some function f : x     r and
1     p <    , we de   ne

kfklp(x ,  ) , (cid:16)z

(cid:17)1/p

|f(x)|p d  (x)

if the integral exists. for p =     we de   ne

kfkl   (x ,  ) = ess sup
x   x

|f(x)|,

,

(a.30)

(a.31)

where ess sup denotes the essential supremum, i.e. the smallest number that
upper bounds |f(x)| almost everywhere. the function space lp(x ,   ) is de   ned
for any p in 1     p         as the space of functions for which kfklp(x ,  ) <    .

a.8 fourier transforms

for su   ciently well-behaved functions on rd we have

f(x) =

  f(s)e2  is  x ds,

  f(s) =

f(x)e   2  is  x dx,

(a.32)

z    

      

z    

      

3a measure    has a density if and only if it is absolutely continuous with respect to
lebesgue measure on rd, i.e. every set that has lebesgue measure zero also has   -measure
zero.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

206

mathematical background

where   f(s) is called the fourier transform of f(x), see e.g. bracewell [1986].
we refer to the equation on the left as the synthesis equation, and the equation
on the right as the analysis equation. there are other conventions for fourier
transforms, particularly those involving    = 2  s. however, this tends to de-
stroy symmetry between the analysis and synthesis equations so we use the
de   nitions given above.

here we have de   ned fourier transforms for f(x) being a function on rd.
for related transforms for periodic functions, functions de   ned on the integer
lattice and on the regular n-polygon see section b.1.

a.9 convexity

convex sets

convex function

below we state some de   nitions and properties of convex sets and functions
taken from boyd and vandenberghe [2004].

a set c is convex if the line segment between any two points in c lies in c,

i.e. if for any x1, x2     c and for any    with 0            1, we have

(a.33)
a function f : x     r is convex if its domain x is a convex set and if for all
x1, x2     x and    with 0            1, we have:

  x1 + (1       )x2     c.

f(  x1 + (1       )x2)       f(x1) + (1       )f(x2),

(a.34)
where x is a (possibly improper) subset of rd. f is concave if    f is convex.
a function f is convex if and only if its domain x is a convex set and its

hessian is positive semide   nite for all x     x .

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

appendix b

gaussian markov processes

particularly when the index set for a stochastic process is one-dimensional such
as the real line or its discretization onto the integer lattice, it is very interesting
to investigate the properties of gaussian markov processes (gmps). in this
appendix we use x(t) to de   ne a stochastic process with continuous time pa-
rameter t. in the discrete time case the process is denoted . . . , x   1, x0, x1, . . .
etc. we assume that the process has zero mean and is, unless otherwise stated,
stationary.

px

a discrete-time autoregressive (ar) process of order p can be written as

ar process

xt =

akxt   k + b0zt,

(b.1)

k=1

where zt     n (0, 1) and all zt   s are i.i.d. . notice the order-p markov property
that given the history xt   1, xt   2, . . ., xt depends only on the previous p x   s.
this relationship can be conveniently expressed as a graphical model; part of
an ar(2) process is illustrated in figure b.1. the name autoregressive stems
from the fact that xt is predicted from the p previous x   s through a regression
equation. if one stores the current x and the p     1 previous values as a state
vector, then the ar(p) scalar process can be written equivalently as a vector
ar(1) process.

figure b.1: graphical model illustrating an ar(2) process.

moving from the discrete time to the continuous time setting, the question
arises as to how generalize the markov notion used in the discrete-time ar
process to de   ne a continuoous-time ar process. it turns out that the correct
generalization uses the idea of having not only the function value but also p of
its derivatives at time t giving rise to the stochastic di   erential equation (sde)1

sde: stochastic
di   erential equation

. . .. . .c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

208

gaussian markov processes

apx (p)(t) + ap   1x (p   1)(t) + . . . + a0x(t) = b0z(t),

(b.2)
where x (i)(t) denotes the ith derivative of x(t) and z(t) is a white gaus-
sian noise process with covariance   (t     t0). this white noise process can be
considered the derivative of the wiener process. to avoid redundancy in the
coe   cients we assume that ap = 1. a considerable amount of mathemati-
cal machinery is required to make rigorous the meaning of such equations, see
e.g.   ksendal [1985]. as for the discrete-time case, one can write eq. (b.2) as
a    rst-order vector sde by de   ning the state to be x(t) and its    rst p     1
derivatives.

we begin this chapter with a summary of some fourier analysis results in
section b.1. fourier analysis is important to linear time invariant systems such
as equations (b.1) and (b.2) because e2  ist is an eigenfunction of the corre-
sponding di   erence (resp di   erential) operator. we then move on in section
b.2 to discuss continuous-time gaussian markov processes on the real line and
their relationship to the same sde on the circle. in section b.3 we describe
discrete-time gaussian markov processes on the integer lattice and their re-
lationship to the same di   erence equation on the circle.
in section b.4 we
explain the relationship between discrete-time gmps and the discrete sampling
of continuous-time gmps. finally in section b.5 we discuss generalizations
of the markov concept in higher dimensions. much of this material is quite
standard, although the relevant results are often scattered through di   erent
sources, and our aim is to provide a uni   ed treatment. the relationship be-
tween the second-order properties of the sdes on the real line and the circle,
and di   erence equations on the integer lattice and the regular polygon is, to
our knowledge, novel.

b.1 fourier analysis

we follow the treatment given by kamid113r [2000]. we consider fourier analysis
of functions on the real line r, of periodic functions of period l on the circle
tl, of functions de   ned on the integer lattice z, and of functions on pn , the
regular n-polygon, which is a discretization of tl.

for su   ciently well-behaved functions on r we have

f(x) =

  f(s)e2  isx ds,

  f(s) =

f(x)e   2  isx dx.

(b.3)

we refer to the equation on the left as the synthesis equation, and the equation
on the right as the analysis equation.

for functions on tl we obtain the fourier series representations
f(x)e   2  ikx/l dx,

  f[k]e2  ikx/l,

  f[k] =

f(x) =

   x

(b.4)

z    

      

z l

0

1
l

z    

      

k=      

1the ak coe   cients in equations (b.1) and (b.2) are not intended to have a close relation-
ship. an approximate relationship might be established through the use of    nite-di   erence
approximations to derivatives.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

b.1 fourier analysis

209

where   f[k] denotes the coe   cient of e2  ikx/l in the expansion. we use square
] to denote that the argument is discrete, so that xt and x[t] are
brackets [
equivalent notations.

similarly for z we obtain

f[n] =

  f(s)e2  isn/l ds,

  f(s) =

   x

n=      

1
l

f[n]e   2  isn/l.

(b.5)

note that   f(s) is periodic with period l and so is de   ned only for 0     s < l to
avoid aliasing. often this transform is de   ned for the special case l = 1 but the
general case emphasizes the duality between equations (b.4) and (b.5).
finally, for functions on pn we have the discrete fourier transform

f[n] =

  f[k]e2  ikn/n ,

  f[k] =

1
n

f[n]e   2  ikn/n .

(b.6)

z l

0

n   1x

k=0

n   1x

n=0

note that there are other conventions for fourier transforms, particularly those
involving    = 2  s. however, this tends to destroy symmetry between the
analysis and synthesis equations so we use the de   nitions given above.

in the case of stochastic processes, the most important fourier relationship
is between the covariance function and the power spectrum; this is known as
the wiener-khintchine theorem, see e.g. chat   eld [1989].

b.1.1 sampling and periodization
we can obtain relationships between functions and their transforms on r, tl,
z, pn through the notions of sampling and periodization.
de   nition b.1 h-sampling: given a function f on r and a spacing parameter
h > 0, we construct a corresponding discrete function    on z using

(b.7)
(cid:3)
similarly we can discretize a function de   ned on tl onto pn , but in this case
we must take h = l/n so that n steps of size h will equal the period l.

  [n] = f(nh),

n     z.

de   nition b.2 periodization by summation: let f(x) be a function on r that
rapidly approaches 0 as x          . we can sum translates of the function to
produce the l-periodic function

g(x) =

f(x     ml),

(b.8)

for l > 0. analogously, when    is de   ned on z and   [n] rapidly approaches 0
as n           we can construct a function    on pn by n-summation by setting

  [n] =

  [n     mn].

(b.9)
(cid:3)

   x

m=      

   x

m=      

h-sampling

periodization by
summation

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

210

gaussian markov processes

(b.10)

(b.11)

(b.12)

(b.13)

z l

0

   x
   x

z l

z (m+1)l
z l

ml

(cid:16)    x
   x

m=      

m=      

   x

m=      

let   [n] be obtained by h-sampling from f(x), with corresponding fourier

transforms     (s) and   f(s). then we have

z    

      

  [n] = f(nh) =

  f(s)e2  isnh ds,

  [n] =

    (s)e2  isn/l ds.

by breaking up the domain of integration in eq. (b.10) we obtain

  [n] =

=

m=      

m=      

0

  f(s)e2  isnh ds

  f(s0 + ml)e2  inh(s0+ml) ds0,

using the change of variable s0 = s     ml. now set hl = 1 and use e2  inm = 1
for n, m integers to obtain

  [n] =

  f(s + ml)

e2  isn/l ds,

(b.14)

(cid:17)

which implies that

0

    (s) =

  f(s + ml),

(b.15)

p   

with l = 1/h. alternatively setting l = 1 one obtains     (s) = 1
similarly if f is de   ned on tl and   [n] = f( nl

m=         f( s+m
h ).
n ) is obtained by sampling then

h

    [n] =

  f[n + mn].

(b.16)

thus we see that sampling in x-space causes periodization in fourier space.

periodic function g(x) ,p   

now consider the periodization of a function f(x) with x     r to give the l-
m=       f(x   ml). let   g[k] be the fourier coe   cients

of g(x). we obtain

z l
z    

0

      

  g[k] =

=

1
l
1
l

g(x)e   2  ikx/l dx =

1
l
f(x)e   2  ikx/l dx =

z l
   x
(cid:16) k
(cid:17)

0

m=      

  f

1
l

,

l

f(x     ml)e   2  ikx/l dx (b.17)

(b.18)

assuming that f(x) is su   ciently well-behaved that the summation and inte-
gration operations can be exchanged. a similar relationship can be obtained
for the periodization of a function de   ned on z. thus we see that periodization
in x-space gives rise to sampling in fourier space.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

b.2 continuous-time gaussian markov processes

211

b.2 continuous-time gaussian markov processes

we    rst consider continuous-time gaussian markov processes on the real line,
and then relate the covariance function obtained to that for the stationary
solution of the sde on the circle. our treatment of continuous-time gmps on
r follows papoulis [1991, ch. 10].

b.2.1 continuous-time gmps on r

we wish to    nd the power spectrum and covariance function for the stationary
process corresponding to the sde given by eq. (b.2). recall that the covariance
function of a stationary process k(t) and the power spectrum s(s) form a fourier
transform pair.

the fourier transform of the stochastic process x(t) is a stochastic process

z    

      

  x(s) given by

z    

      

  x(s) =

x(t)e   2  ist dt,

x(t) =

  x(s)e2  ist ds,

(b.19)

where the integrals are interpreted as a mean-square limit. let     denote complex
conjugation and h. . .i denote expectation with respect to the stochastic process.
then for a stationary gaussian process we have

h   x(s1)   x   (s2)i =

hx(t)x   (t0)ie   2  is1te2  is2t0

dt dt0

z    
z    

      

z    
dt0e   2  i(s1   s2)t0z    

      

=
= s(s1)  (s1     s2),

      

d   k(  )e   2  is1  

      

(b.20)

(b.21)

the delta function r e   2  istdt =   (s). this shows that   x(s1) and   x(s2) are

(b.22)
using the change of variables    = t     t0 and the integral representation of
uncorrelated for s1 6= s2, i.e. that the fourier basis are eigenfunctions of the
di   erential operator. also from eq. (b.19) we obtain

x (k)(t) =

(2  is)k   x(s)e2  ist ds.

(b.23)

now if we fourier transform eq. (b.2) we obtain

ak(2  is)k   x(s) = b0   z(s),

(b.24)

z    

      

px

k=0

where   z(s) denotes the fourier transform of the white noise. taking the product
of equation b.24 with its complex conjugate and taking expectations we obtain

(cid:20) px

(cid:21)(cid:20) px

(cid:21)

ak(2  is1)k

ak(   2  is2)k

h   x(s1)   x   (s2)i = b2

0h   z(s1)   z   (s2)i.

k=0

k=0

(b.25)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

212

ar(1) process

ar(p) process

gaussian markov processes

let a(z) = pp

spectrum of white noise is 1, we obtain

k=0 akzk. then using eq. (b.22) and the fact that the power

sr(s) =

b2
0

|a(2  is)|2 .

(b.26)

note that the denominator is a polynomial of order p in s2. the relationship
of stationary solutions of pth-order sdes to rational spectral densities can be
traced back at least as far as doob [1944].

above we have assumed that the process is stationary. however, this de-
pends on the coe   cients a0, . . . , ap. to analyze this issue we assume a solution
of the form xt     e  t when the driving term b0 = 0. this leads to the condition
k=0 ak  k must lie in the left

for stationarity that the roots of the polynomialpp

half plane [arat  o, 1982, p. 127].
example: ar(1) process. in this case we have the sde
x0(t) + a0x(t) = b0z(t),

(b.27)

where a0 > 0 for stationarity. this gives rise to the power spectrum

s(s) =

b2
0

(2  is + a0)(   2  is + a0)

=

b2
0

(2  s)2 + a2
0

.

(b.28)

taking the fourier transform we obtain

k(t) = b2
0
2a0

e   a0|t|.

(b.29)

to the power spectrum s(s) = ([pp

this process is known as the ornstein-uhlenbeck (ou) process [uhlenbeck and
ornstein, 1930] and was introduced as a mathematical model of the velocity of
a particle undergoing brownian motion. it can be shown that the ou process
is the unique stationary    rst-order gaussian markov process.
example: ar(p) process.

in general the covariance transform corresponding
k=0 ak(   2  is)k])   1 can be
quite complicated. for example, papoulis [1991, p. 326] gives three forms of
1     4a0 is
the covariance function for the ar(2) process depending on whether a2
greater than, equal to or less than 0. however, if the coe   cients a0, a1, . . . , ap
are chosen in a particular way then one can obtain

k=0 ak(2  is)k][pp

s(s) =

1

(4  2s2 +   2)p

(b.30)

ance function is of the formpp   1

for some   .

it can be shown [stein, 1999, p. 31] that the corresponding covari-
k=0   k|t|ke     |t| for some coe   cients   0, . . . ,   p   1.
2   e     |t| for the ou process. for
4  3 e     |t|(1+  |t|). these are special cases of the mat  ern

for p = 1 we have already seen that k(t) = 1
p = 2 we obtain k(t) = 1
class of covariance functions described in section 4.2.1.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

213

wiener process

b.2 continuous-time gaussian markov processes

example: wiener process. although our derivations have focussed on stationary
gaussian markov processes, there are also several important non-stationary
processes. one of the most important is the wiener process that satis   es the
sde x0(t) = z(t) for t     0 with the initial condition x(0) = 0. this process
has covariance function k(t, s) = min(t, s). an interesting variant of the wiener
process known as the brownian bridge (or tied-down wiener process) is obtained
by conditioning on the wiener process passing through x(1) = 0. this has
covariance k(t, s) = min(t, s)     st for 0     s, t     1. see e.g. grimmett and
stirzaker [1992] for further information on these processes.

markov processes derived from sdes of order p are p    1 times ms di   eren-
tiable. this is easy to see heuristically from eq. (b.2); given that a process gets
rougher the more times it is di   erentiated, eq. (b.2) tells us that x (p)(t) is like
the white noise process, i.e. not ms continuous. so, for example, the ou process
(and also the wiener process) are ms continuous but not ms di   erentiable.

b.2.2 the solution of the corresponding sde on the cir-

cle

the analogous analysis to that on the real line is carried out on tl using

x(t) =

  x[n]e2  int/l,

  x[n] =

1
l

x(t)e   2  int/ldt.

(b.31)

   x

n=      

z l

0

(cid:26) s[n]

0

as x(t) is assumed stationary we obtain an analogous result to eq. (b.22),
i.e. that the fourier coe   cients are independent

px

h   x[m]   x   [n]i =

if m = n
otherwise.

(b.32)

p   
similarly, the covariance function on the cirle is given by k(t   s) = hx(t)x   (s)i =
x (k)(t) =p   
n=       s[n]e2  in(t   s)/l. let   l = 2  /l. then plugging in the expression
n=      (in  l)k   x[n]ein  lt into the sde eq. (b.2) and equating terms

in [n] we obtain

ak(in  l)k   x[n] = b0   z[n].

(b.33)

k=0

as in the real-line case we form the product of equation b.33 with its complex
conjugate and take expectations to give

b2
0

note that st[n] is equal to sr(cid:0) n

|a(in  l)|2 .

(cid:1), i.e. that it is a sampling of sr at intervals

(b.34)

st[n] =

1/l, where sr(s) is the power spectrum of the continuous process on the real
line given in equation b.26. let kt(h) denote the covariance function on the

l

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

214

gaussian markov processes

circle and kr(h) denote the covariance function on the real line for the sde.
then using eq. (b.15) we    nd that

   x

m=      

kt(t) =

kr(t     ml).

(b.35)

e   a0|t|.

1st order sde

example: 1st-order sde. on r for the ou process we have kr(t) = b2
by summing the series (two geometric progressions) we obtain

0
2a0

kt(t) =

b2
0

2a0(1     e   a0l)

(cid:16)

e   a0|t| + e   a0(l   |t|)(cid:17)

= b2
0
2a0

cosh[a0( l

2     |t|)]
sinh( a0l
2 )

(b.36)

for    l     t     l. eq. (b.36) is also given (up to scaling factors) in grenander
et al. [1991, eq. 2.15], where it is obtained by a limiting argument from the
discrete-time gmp on pn, see section b.3.2.

b.3 discrete-time gaussian markov processes

we    rst consider discrete-time gaussian markov processes on z, and then re-
late the covariance function obtained to that of the stationary solution of the
di   erence equation on pn . chat   eld [1989] and diggle [1990] provide good
coverage of discrete-time arma models on z.

b.3.1 discrete-time gmps on z

assuming that the process is stationary the covariance function k[i] denotes
hxtxt+ii    t     z. (note that because of stationarity k[i] = k[   i].)

we    rst use a fourier approach to derive the power spectrum and hence the
covariance function of the ar(p) process. de   ning a0 =    1, we can rewrite

k=0 akxt   k + b0zt = 0. the fourier pair for x[t] is

   x

t=      

  x(s)e2  ist/l ds,

  x(s) =

1
l

x[t]e   2  ist/l.

(b.37)

k=0 akxt   k + b0zt = 0 we obtain

(cid:16) px

ake   i  lsk(cid:17)

  x(s)

+ b0   z(s) = 0,

(b.38)

eq. (b.1) aspp
z l

x[t] =

plugging this intopp

0

k=0

where   l = 2  /l. as above, taking the product of eq. (b.38) with its complex
conjugate and taking expectations we obtain

sz(s) =

b2
0

|a(ei  ls)|2 .

(b.39)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

b.3 discrete-time gaussian markov processes

215

above we have assumed that the process is stationary. however, this de-
pends on the coe   cients a0, . . . , ap. to analyze this issue we assume a solution
of the form xt     zt when the driving term b0 = 0. this leads to the condition
k=0 akzp   k must lie inside

for stationarity that the roots of the polynomial pp

the unit circle. see hannan [1970, theorem 5, p. 19] for further details.

as well as deriving the covariance function from the fourier transform of
the power spectrum it can also be obtained by solving a set of linear equations.
our    rst observation is that xs is independent of zt for s < t. multiplying
equation b.1 through by zt and taking expectations, we obtain hxtzti = b0
and hxt   izti = 0 for i > 0. by multiplying equation b.1 through by xt   j for
j = 0, 1, . . . and taking expectations we obtain the yule-walker equations

yule-walker equations

ar(1) process

k[0] =

k[j] =

aik[i] + b2
0

aik[j     i]

   j > 0.

(b.40)

(b.41)

the    rst p + 1 of these equations form a linear system that can be used to solve
for k[0], . . . , k[p] in terms of b0 and a1, . . . , ap, and eq. (b.41) can be used to
obtain k[j] for j > p recursively.
example: ar(1) process. the simplest example of an ar process is the ar(1)
process de   ned as xt = a1xt   1 + b0zt. this gives rise to the yule-walker
equations

k[0]     a1k[1] = b2

0, and k[1]     a1k[0] = 0.

(b.42)

0/(1     a2

x, where
the linear system for k[0], k[1] can easily be solved to give k[j] = a
1) is the variance of the process. notice that for the process to
x = b2
  2
be stationary we require |a1| < 1. the corresponding power spectrum obtained
from equation b.39 is

|j|
1   2

s(s) =

b2
0

1     2a1 cos(  ls) + a2

1

.

(b.43)

similarly to the continuous case, the covariance function for the discrete-time
ar(2) process has three di   erent forms depending on a2
1 + 4a2. these are
described in diggle [1990, example 3.6].

b.3.2 the solution of the corresponding di   erence equa-

tion on pn

we now consider variables x = x0, x1, . . . , xn   1 arranged around the circle
with n     p. by appropriately modifying eq. (b.1) we obtain

xt =

akxmod(t   k,n ) + b0zt.

(b.44)

px
px

i=1

i=1

px

k=1

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

216

gaussian markov processes

pn   1

the zt   s are i.i.d. and     n (0, 1). thus z = z0, z1, . . . , zn   1 has density
p(z)     exp    1
t . equation (b.44) shows that x and z are related by
a linear transformation and thus

t=0 z 2

2

p(x)     exp

(cid:16)    1

2b2
0

n   1x

(cid:16)

xt     px

t=0

k=1

(cid:17)2(cid:17)

akxmod(t   k,n )

.

(b.45)

this is an n-dimensional multivariate gaussian. for an ar(p) process the
inverse covariance matrix has a circulant structure [davis, 1979] consisting of
a diagonal band (2p + 1) entries wide and appropriate circulant entries in the
corners. thus p(xt|x\ xt) = p(xt|xmod(t   1,n ), . . . , xmod(t   p,n ), xmod(t+1,n ),
. . . , xmod(t+p,n )), which geman and geman [1984] call the    two-sided    markov
property. notice that it is the zeros in the inverse covariance matrix that
indicate the conditional independence structure; see also section b.5.

the properties of eq. (b.44) have been studied by a number of authors,
e.g. whittle [1963] (under the name of circulant processes), kashyap and chel-
lappa [1981] (under the name of circular autoregressive models) and grenander
et al. [1991] (as cyclic markov process).

as above, we de   ne the fourier transform pair

x[n] =

  x[m]e2  inm/n ,

  x[m] =

1
n

x[n]e   2  inm/n .

(b.46)

n   1x

n=0

by similar arguments to those above we obtain

n   1x

m=0

px

ak   x[m](e2  im/n )k + b0   z[m] = 0,

(b.47)

where a0 =    1, and thus

k=0

sp[m] =

b2
0

|a(e2  im/n )|2

.

(b.48)

as in the continuous-time case, we see that sp[m] is obtained by sampling
the power spectrum of the corresponding process on the line, so that sp[m] =

sz(cid:0) ml

n

(cid:1). thus using eq. (b.16) we have
   x

kp[n] =

m=      

kz[n + mn].

(b.49)

ar(1) process

example: ar(1) process. for this process xt = a1xmod(t   1,n) + b0zt, the
diagonal entries in the inverse covariance are (1 + a2
0 and the non-zero o   -
diagonal entries are    a1/b2
0.

1)/b2

by summing the covariance function kz[n] =   2

|n|
1 we obtain

x a

kp[n] =

  2
x

(1     an
1 )

|n|
1 + a

|n   n|
1

(a

)

n = 0, . . . , n     1.

(b.50)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

b.4 the relationship between discrete-time and sampled continuous-time gmps

217

we now illustrate this result for n = 3. in this case the covariance matrix has
diagonal entries of
1).
1)(a1 + a2
the inverse covariance matrix has the structure described above. multiplying
these two matrices together we do indeed obtain the identity matrix.

1) and o   -diagonal entries of

1)(1 + a3

  2
x
(1   a3

  2
x
(1   a3

b.4 the relationship between discrete-time and

sampled continuous-time gmps

we now consider the relationship between continuous-time and discrete-time
gmps. in particular we ask the question, is a regular sampling of a continuous-
time ar(p) process a discrete-time ar(p) process? it turns out that the answer
will, in general, be negative. first we de   ne a generalization of ar processes
known as autoregressive moving-average (arma) processes.

px

qx

i=1

j=0

s(s) =

|b(ei  ls)|2
|a(ei  ls)|2 ,

s(s) =

|b(2  is)|2
|a(2  is)|2

arma processes the ar(p) process de   ned above is a special case of the
more general arma(p, q) process which is de   ned as

xt =

aixt   i +

bjzt   j.

(b.51)

observe that the ar(p) process is in fact also an arma(p, 0) process. a
spectral analysis of equation b.51 similar to that performed in section b.3.1
gives

(b.52)

(b.53)

where b(z) =pq

density of the form

j=0 bjzj. in continuous time a process with a rational spectral

we require q < p as k(0) =r s(s)ds <    .

is known as a arma(p, q) process. for this to de   ne a valid covariance function

discrete-time observation of a continuous-time process let x(t) be
a continuous-time process having covariance function k(t) and power spectrum
s(s). let xh be the discrete-time process obtained by sampling x(t) at interval
h, so that xh[n] = x(nh) for n     z. clearly the covariance function of this
process is given by kh[n] = k(nh). by eq. (b.15) this means that

   x

m=      

sh(s) =

s(s + m
h

)

(b.54)

where sh(s) is de   ned using l = 1/h.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

218

gaussian markov processes

theorem b.1 let x be a continuous-time stationary gaussian process and
xh be the discretization of this process. if x is an arma process then xh
is also an arma process. however, if x is an ar process then xh is not
(cid:3)
necessarily an ar process.

the proof is given in ihara [1993, theorem 2.7.1]. it is easy to see using the
covariance functions given in sections b.2.1 and b.3.1 that the discretization
of a continuous-time ar(1) process is indeed a discrete-time ar(1) process.
however, ihara shows that, in general, the discretization of a continuous-time
ar(2) process is not a discrete-time ar(2) process.

b.5 markov processes in higher dimensions

we have concentrated above on the case where t is one-dimensional. in higher
dimensions it is interesting to ask how the markov property might be general-
ized. let    s be an in   nitely di   erentiable closed surface separating rd into a
bounded part s    and an unbounded part s+. loosely speaking2 a random    eld
x(t) is said to be quasi-markovian if x(t) for t     s    and x(u) for u     s+ are
independent given x(s) for s        s. wong [1971] showed that the only isotropic
quasi-markov gaussian    eld with a continuous covariance function is the degen-
erate case x(t) = x(0), where x(0) is a gaussian variate. however, if instead
of conditioning on the values that the    eld takes on in    s, one conditions on
a somewhat larger set, then gaussian random    elds with non-trivial markov-
type structure can be obtained. for example, random    elds with an inverse
j=1 kj     2p
pseudo-markovian of order p. for example, the d-dimensional tensor-product
i=1 e     i|ti| is pseudo-markovian of order d. for
further discussion of markov properties of random    elds see the appendix in
adler [1981].

power spectrum of the formp
and c(s    s)p    (cid:12)(cid:12)p
of the ou process k(t) = qd

d with k>1 =pd
(cid:12)(cid:12) for some c > 0 are said to be

k ak1,...,kd sk1
1        skd

k>1=2p ak1,...,kd sk1

1        skd

d

if instead of rd we wish to de   ne a markov random    eld (mrf) on a graph-
ical structure (for example the lattice zd) things become more straightforward.
we follow the presentation in jordan [2005]. let g = (x, e) be a graph where
x is a set of nodes that are in one-to-one correspondence with a set of ran-
dom variables, and e be the set of undirected edges of the graph. let c be
the set of all maximal cliques of g. a potential function   c(xc) is a function
on the possible realizations xc of the maximal clique xc. potential functions
are assumed to be (strictly) positive, real-valued functions. the id203
distribution p(x) corresponding to the markov random    eld is given by

y
function) obtained by summing/integratingq

p(x) =

c   c

1
z

where z is a id172 factor (known in statistical physics as the partition
c   c   c(xc) over all possible as-

2for a precise formulation of this de   nition involving   -   elds see adler [1981, p. 256].

  c(xc),

(b.55)

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

b.5 markov processes in higher dimensions

219

signments of values to the nodes x. under this de   nition it is easy to show
that a local markov property holds, i.e. that for any variable x the conditional
distribution of x given all other variables in x depends only on those variables
that are neighbours of x. a useful reference on markov random    elds is winkler
[1995].

a simple example of a gaussian markov random    eld has the form

p(x)     exp

(cid:16)      1

x

x

(xi     xj)2(cid:17)

i       2
x2

i

i,j:j   n (i)

,

(b.56)

where n(i) denotes the set of neighbours of node xi and   1,   2 > 0. on z2
one might choose a four-connected neighbourhood, i.e. those nodes to the north,
south, east and west of a given node.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

appendix c

datasets and code

the datasets used for experiments in this book and implementations of the
algorithms presented are available for download at the website of the book:

http://www.gaussianprocess.org/gpml

the programs are short stand-alone implementations and not part of a larger
package. they are meant to be simple to understand and modify for a desired
purpose. some of the programs allow speci   cation of covariance functions from
a selection provided, or to link in user de   ned covariance code. for some of
the plots, code is provided which produces a similar plot, as this may be a
convenient way of conveying the details.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

bibliography

abrahamsen, p. (1997). a review of gaussian random fields and correlation functions. tech-
http://publications.nr.no/
p. 82

nical report 917, norwegian computing center, oslo, norway.
917 rapport.pdf.

abramowitz, m. and stegun, i. a. (1965). handbook of mathematical functions. dover, new york.
pp. 84, 85

adams, r. (1975). sobolev spaces. academic press, new york.

p. 134

adler, r. j. (1981). the geometry of random fields. wiley, chichester.

pp. 80, 81, 83, 191, 218

amari, s. (1985). di   erential-geometrical methods in statistics. springer-verlag, berlin.

p. 102

ansley, c. f. and kohn, r. (1985). estimation, filtering, and smoothing in state space models with
p. 29

incompletely speci   ed initial conditions. annals of statistics, 13(4):1286   1316.

arat  o, m. (1982). linear stochastic systems with constant coe   cients. springer-verlag, berlin. lecture
p. 212

notes in control and information sciences 45.

arfken, g. (1985). mathematical methods for physicists. academic press, san diego.

pp. xv, 134

aronszajn, n. (1950). theory of reproducing kernels. trans. amer. math. soc., 68:337   404.
pp. 129, 130

bach, f. r. and jordan, m. i. (2002). kernel independent component analysis. journal of machine
p. 97

learning research, 3(1):1   48.

baker, c. t. h. (1977). the numerical treatment of integral equations. clarendon press, oxford.
pp. 98, 99

barber, d. and saad, d. (1996). does extra knowledge necessarily improve generalisation? neural
p. 31

computation, 8:202   214.

bartle, r. g. (1995). the elements of integration and lebesgue measure. wiley, new york.

p. 204

bartlett, p. l., jordan, m. i., and mcauli   e, j. d. (2003). convexity, classi   cation and risk bounds.
technical report 638, department of statistics, university of california, berkeley. available from
http://www.stat.berkeley.edu/tech-reports/638.pdf. accepted for publication in journal of the
american statistical association.
p. 157

berger, j. o. (1985). statistical decision theory and bayesian analysis. springer, new york. second
pp. 22, 35

edition.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

224

bibliography

bishop, c. m. (1995). neural networks for pattern recognition. clarendon press, oxford.

p. 45

bishop, c. m., svensen, m., and williams, c. k. i. (1998a). developments of the generative topographic
p. 196

mapping. neurocomputing, 21:203   224.

bishop, c. m., svensen, m., and williams, c. k. i. (1998b). gtm: the generative topographic
p. 196

mapping. neural computation, 10(1):215   234.

blake, i. f. and lindsey, w. c. (1973). level-crossing problems for random processes. ieee trans
p. 81

id205, 19(3):295   315.

blight, b. j. n. and ott, l. (1975). a bayesian approach to model inadequacy for polynomial regression.
p. 28

biometrika, 62(1):79   88.

boyd, s. and vandenberghe, l. (2004). id76. cambridge university press, cambridge,
p. 206

uk.

boyle, p. and frean, m. (2005). dependent gaussian processes. in saul, l. k., weiss, y., and bottou,
l., editors, advances in neural information processing systems 17, pages 217   224. mit press. p. 190

bracewell, r. n. (1986). the fourier transform and its applications. mcgraw-hill, singapore, inter-
pp. 83, 206

national edition.

caruana, r. (1997). multitask learning. machine learning, 28(1):41   75.

p. 115

chat   eld, c. (1989). the analysis of time series: an introduction. chapman and hall, london, 4th
pp. 82, 209, 214

edition.

choi, t. and schervish, m. j. (2004). posterior consistency in nonparametric regression prob-
lems under gaussian process priors. technical report 809, department of statistics, cmu.
http://www.stat.cmu.edu/tr/tr809/tr809.html.
p. 156

choudhuri, n., ghosal, s., and roy, a. (2005). nonparametric binary regression using a gaussian
p. 156

process prior. unpublished. http://www4.stat.ncsu.edu/   sghosal/papers.html.

chu, w. and ghahramani, z. (2005). gaussian processes for ordinal regression. journal of machine
p. 191

learning research, 6:1019   1041.

collins, m. and du   y, n. (2002). convolution kernels for natural language. in diettrich, t. g., becker,
s., and ghahramani, z., editors, advances in neural information processing systems 14. mit press.
p. 101

collobert, r. and bengio, s. (2001).

gression problems.
   bengio/projects/id166torch.html.

journal of machine learning research, 1:143   160.

id166torch: support vector machines for large-scale re-
http://www.idiap.ch/
pp. 69, 72

cornford, d., nabney, i. t., and williams, c. k. i. (2002). modelling frontal discontinuities in wind
p. 85

fields. journal of nonparameteric statsitics, 14(1-2):43   58.

cox, d. d. (1984). multivariate smoothing spline functions. siam journal on numerical analysis,
p. 156

21(4):789   813.

cox, d. d. and o   sullivan, f. (1990). asymptotic analysis of penalized likelihood and related esti-
p. 156

mators. annals of statistics, 18(4):1676   1695.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

bibliography

225

craven, p. and wahba, g. (1979). smoothing noisy data with spline functions. numer. math., 31:377   
p. 112

403.

cressie, n. a. c. (1993). statistics for spatial data. wiley, new york.

pp. 30, 137, 190

cristianini, n. and shawe-taylor, j. (2000). an introduction to support vector machines. cambridge
p. 141

university press.

cristianini, n., shawe-taylor, j., elissee   , a., and kandola, j. (2002). on kernel-target alignment. in
diettrich, t. g., becker, s., and ghahramani, z., editors, advances in neural information processing
systems 14. mit press.
p. 128

csat  o, l. (2002). gaussian processes   iterative sparse approximations. phd thesis, aston university,
p. 179

uk.

csat  o, l. and opper, m. (2002). sparse on-line gaussian processes. neural computation, 14(3):641   
pp. 180, 185

668.

csat  o, l., opper, m., and winther, o. (2002). tap gibbs free energy, belief propagation and spar-
sity. in diettrich, t. g., becker, s., and ghahramani, z., editors, advances in neural information
processing systems 14, pages 657   663. mit press.
pp. 180, 185

daley, r. (1991). atmospheric data analysis. cambridge university press, cambridge, uk.

p. 30

david, h. a. (1970). order statistics. wiley, new york.

davis, p. j. (1979). circulant matrices. wiley, new york.

dawid, a. p. (1976). properties of diagnostic data distributions. biometrics, 32:647   658.

pp. 169, 170

p. 216

p. 34

dellaportas, p. and stephens, d. a. (1995). bayesian analysis of errors-in-variables regression models.
p. 192

biometrics, 51:1085   1095.

devroye, l., gy  or   , l., and lugosi, g. (1996). a probabilistic theory of pattern recognition. springer,
pp. 156, 166

new york.

diaconis, p. and freedman, d. (1986). on the consistency of bayes estimates. annals of statistics,
p. 156

14(1):1   26.

diggle, p. j. (1990). time series: a biostatistical introduction. clarendon press, oxford. pp. 214, 215

diggle, p. j., tawn, j. a., and moyeed, r. a. (1998). model-based geostatistics (with discussion).
p. 191

applied statistics, 47:299   350.

doob, j. l. (1944). the elementary gaussian processes. annals of mathematical statistics, 15(3):229   
p. 212

282.

doob, j. l. (1994). measure theory. springer-verlag, new york.

p. 204

drineas, p. and mahoney, m. w. (2005). on the nystr  om method for approximating a gram ma-
trix for improved kernel-based learning. technical report yaleu/dcs/tr-1319, yale university.
p. 174
http://cs-www.cs.yale.edu/homes/mmahoney.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

226

bibliography

duchon, j. (1977). splines minimizing rotation-invariant semi-norms in sobolev spaces. in schempp,
w. and zeller, k., editors, constructive theory of functions of several variables, pages 85   100.
springer-verlag.
p. 137

duda, r. o. and hart, p. e. (1973). pattern classi   cation and scene analysis. john wiley, new york.
p. 146

edgeworth, f. y. (1887). on observations relating to several quantities. hermathena, 6:279   285.
p. 146

faul, a. c. and tipping, m. e. (2002). analysis of sparse bayesian learning. in dietterich, t. g.,
becker, s., and ghahramani, z., editors, advances in neural information processing systems 14,
pages 383   389, cambridge, massachussetts. mit press.
p. 149

feldman, j. (1958). equivalence and perpendicularity of gaussian processes. paci   c j. math., 8:699   
p. 157

708. erratum in paci   c j. math. 9, 1295-1296 (1959).

ferrari trecate, g., williams, c. k. i., and opper, m. (1999). finite-dimensional approximation of
gaussian processes. in kearns, m. s., solla, s. a., and cohn, d. a., editors, advances in neural
information processing systems 11, pages 218   224. mit press.
p. 152

fine, s. and scheinberg, k. (2002). e   cient id166 training using low-rank kernel representations.
pp. 47, 174

journal of machine learning research, 2(2):243   264.

fowlkes, c., belongie, s., and malik, j. (2001). e   cient spatiotemporal grouping using the nystr  om
method. in proceedings of the ieee conference on id161 and pattern recognition, cvpr
2001.
p. 172

freedman, d. (1999). on the bernstein-von mises theorem with in   nite-dimensional parameters.
p. 156

annals of statistics, 27(4):1119   1140.

frieze, a., kannan, r., and vempala, s. (1998). fast monte-carlo algorithms for finding low-rank
approximations. in 39th conference on the foundations of computer science, pages 370   378. p. 174

geisser, s. and eddy, w. f. (1979). a predictive approach to model selection. journal of the americal
p. 117

statistical association, 74(365):153   160.

geman, s. and geman, d. (1984). stochastic relaxation, gibbs distributions, and the bayesian restora-
p. 216

tion of images. ieee trans. pattern analysis and machine intellligence, 6(6):721   741.

gibbs, m. n. (1997). bayesian gaussian processes for regression and classi   cation. phd thesis,
p. 93

department of physics, university of cambridge.

gibbs, m. n. and mackay, d. j. c. (1997). e   cient implementation of gaussian processes. unpub-
lished manuscript. cavendish laboratory, cambridge, uk. http://www.id136.phy.cam.ac.uk/
mackay/bayesgp.html.
p. 181

gibbs, m. n. and mackay, d. j. c. (2000). variational gaussian process classi   ers. ieee transactions
p. 41

on neural networks, 11(6):1458   1464.

gihman, i. i. and skorohod, a. v. (1974). the theory of stochastic processes, volume 1. springer
p. 82

verlag, berlin.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

bibliography

227

girard, a., rasmussen, c. e., qui  nonero-candela, j., and murray-smith, r. (2003). gaussian process
priors with uncertain inputs: application to multiple-step ahead time series forecasting. in becker,
s., thrun, s., and obermayer, k., editors, advances in neural information processing systems 15.
mit press.
p. 192

girosi, f. (1991). models of noise and robust estimates. technical report ai memo 1287, mit ai
p. 146

laboratory.

girosi, f., jones, m., and poggio, t. (1995). id173 theory and neural networks architectures.
p. 25

neural computation, 7(2):219   269.

goldberg, p. w., williams, c. k. i., and bishop, c. m. (1998). regression with input-dependent noise:
a gaussian process treatment. in jordan, m. i., kearns, m. j., and solla, s. a., editors, advances
p. 191
in neural information processing systems 10. mit press, cambridge, ma.

golub, g. h. and van loan, c. f. (1989). matrix computations. johns hopkins university press,
pp. 172, 173, 174, 181, 202

baltimore. second edition.

gradshteyn, i. s. and ryzhik, i. m. (1980). tables of integrals, series and products. academic press.
pp. 98, 103

corrected and enlarged edition prepared by a. je   rey.

green, p. j. and silverman, b. w. (1994). nonparametric regression and generalized linear models.
p. 138

chapman and hall, london.

grenander, u., chow, y., and keenan, d. m. (1991). hands: a pattern theoretic study of biological
pp. 214, 216

shapes. springer-verlag, new york.

grimmett, g. r. and stirzaker, d. r. (1992). id203 and random processes. oxford university
pp. 94, 213

press, oxford, england, second edition.

gr  unwald, p. d. and langford, j. (2004). suboptimal behaviour of bayes and mdl in classi   cation
under misspeci   cation. in proc. seventeenth annual conference on computational learning theory
p. 156
(colt 2004).

gy  or   , l., kohler, m., krzy  zak, a., and walk, h. (2002). a distribution-free theory of nonparametric
p. 156

regression. springer, new york.

hajek, j. (1958). on a property of normal distributions of any stochastic process (in russian).
czechoslovak math. j., 8:610   618. translated in selected trans. math. statist. probab. 1 245-252
(1961). also available in collected works of jaroslav hajek, eds. m. hu  skov  a, r. beran, v. dupa  c,
wiley, (1998).
p. 157

hand, d. j., mannila, h., and smyth, p. (2001). principles of data mining. mit press.

hannan, e. j. (1970). multiple time series. wiley, new york.

p. 100

p. 215

hansen, l. k., liisberg, c., and salamon, p. (1997). the error-reject tradeo   . open sys. & information
p. 36

dyn., 4:159   184.

hastie, t. j. and tibshirani, r. j. (1990). generalized additive models. chapman and hall. pp. 24, 25, 95

haussler, d. (1999). convolution kernels on discrete structures. technical report ucsc-crl-99-10,
p. 101

dept of computer science, university of california at santa cruz.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

228

bibliography

hawkins, d. l. (1989). some practical problems in implementing a certain sieve estimator of the
gaussian mean function. communications in statistics   simulation and computation, 18(2):481   
500.
p. 97

hoerl, a. e. and kennard, r. w. (1970). ridge regression: biased estimation for nonorthogonal
p. 11

problems. technometrics, 12(1):55   67.

hornik, k. (1993). some new results on neural network approximation. neural networks, 6(8):1069   
p. 90

1072.

ihara, s. (1993). id205 for continuous systems. world scienti   c, singapore.

p. 218

jaakkola, t. s., diekhans, m., and haussler, d. (2000). a discriminative framework for detecting
pp. 101, 102, 104

remote protein homologies. journal of computational biology, 7:95   114.

jaakkola, t. s. and haussler, d. (1999). probabilistic kernel regression models. in heckerman, d. and
whittaker, j., editors, workshop on arti   cial intelligence and statistics 7. morgan kaufmann. p. 41

jacobs, r. a., jordan, m. i., nowlan, s. j., and hinton, g. e. (1991). adaptive mixtures of local
p. 192

experts. neural computation, 3:79   87.

johnson, n. l., kotz, s., and balakrishnan, n. (1995). continuous univariate distributions volume 2.
p. 45

john wiley and sons, new york, second edition.

jones, d. r. (2001). a taxonomy of global optimization methods based on response surfaces. j.
p. 193

global optimization, 21:345   383.

jordan, m. i. (2005). an introduction to probabilistic id114. draft book.

journel, a. g. and huijbregts, c. j. (1978). mining geostatistics. academic press.

p. 218

p. 30

kailath, t. (1971). rkhs approach to detection and estimation problems   part i: deterministic
p. 131

signals in gaussian noise. ieee trans. id205, 17(5):530   549.

kamid113r, d. w. (2000). a first course in fourier analysis. prentice-hall, upper saddle river, nj.
p. 208

kashyap, r. l. and chellappa, r. (1981). stochastic models for closed boundary analysis: represen-
p. 216

tation and reconstruction. ieee trans. on id205, 27(5):627   637.

keeling, c. d. and whorf, t. p. (2004). atmospheric co2 records from sites in the sio air sampling
network. in trends: a compendium of data on global change. carbon dioxide information analysis
p. 119
center, oak ridge national laboratory, oak ridge, tenn., u.s.a.

kent, j. t. and mardia, k. v. (1994). the link between kriging and thin-plate splines. in kelly,
p. 137

f. p., editor, id203, statsitics and optimization, pages 325   339. wiley.

kimeldorf, g. and wahba, g. (1970). a correspondence between bayesian estimation of stochastic
p. 136

processes and smoothing by splines. annals of mathematical statistics, 41:495   502.

kimeldorf, g. and wahba, g. (1971). some results on tchebyche   an spline functions. j. mathematical
p. 132

analysis and applications, 33(1):82   95.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

bibliography

229

kohn, r. and ansley, c. f. (1987). a new algorithm for spline smoothing based on smoothing a
p. 141

stochastic process. siam j. sci. stat. comput., 8(1):33   48.

kolmogorov, a. n. (1941). interpolation und extrapolation von station  aren zuf  aligen folgen. izv. akad.
p. 29

nauk sssr, 5:3   14.

k  onig, h. (1986). eigenvalue distribution of compact operators. birkh  auser.

kullback, s. (1959). id205 and statistics. dover, new york.

p. 96

pp. 158, 203

kuss, m. and rasmussen, c. e. (2005). assessing approximations for gaussian process classi   cation.
in weiss, y., sch  olkopf, b., and platt, j., editors, advances in neural information processing systems
18. mit press.
pp. 72, 73

lanckriet, g. r. g., cristianini, n., bartlett, p. l., el ghaoui, l., and jordan, m. i. (2004). learning the
kernel matrix with semide   nite programming. journal of machine learning research, 5(1):27   72.
p. 128

lauritzen, s. l. (1981). time series analysis in 1880: a discussion of contributions made by
p. 29

t. n. thiele. international statistical review, 49:319   333.

lawrence, n. (2004). gaussian process latent variable models for visualization of high dimensional
data. in thrun, s., saul, l., and sch  olkopf, b., editors, advances in neural information processing
p. 196
systems 16, pages 329   336. mit press.

lawrence, n., seeger, m., and herbrich, r. (2003). fast sparse gaussian process methods: the infor-
mative vector machine. in becker, s., thrun, s., and obermayer, k., editors, advances in neural
pp. 178, 185
information processing systems 15, pages 625   632. mit press.

leslie, c., eskin, e., weston, j., and sta   ord noble, w. (2003). mismatch string kernels for id166
in becker, s., thrun, s., and obermayer, k., editors, advances in neural
pp. 100, 101, 104

protein classi   cation.
information processing systems 15. mit press.

lin, x., wahba, g., xiang, d., gao, f., klein, r., and klein, b. (2000). smoothing spline anova
models for large data sets with bernoulli observations and the randomized gacv. annals of
statistics, 28:1570   1600.
p. 185

lindley, d. v. (1985). making decisions. john wiley and sons, london, uk, second edition.

p. 111

lodhi, h., shawe-taylor, j., cristianini, n., and watkins, c. j. c. h. (2001). text classi   cation using
string kernels. in leen, t. k., diettrich, t. g., and tresp, v., editors, advances in neural information
p. 101
processing systems 13. mit press.

luo, z. and wahba, g. (1997). hybrid adaptive splines. j. amer. statist. assoc., 92:107   116. p. 176

mackay, d. j. c. (1992a). a practical bayesian framework for id26 networks. neural
pp. 109, 166, 167

computation, 4(3):448   472.

mackay, d. j. c. (1992b). bayesian interpolation. neural computation, 4(3):415   447. pp. xiii, xvi, 109

mackay, d. j. c. (1992c). information-based objective functions for active data selection. neural
p. 178

computation, 4(4):590   604.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

230

bibliography

mackay, d. j. c. (1992d). the evidence framework applied to classi   cation networks. neural com-
p. 45

putation, 4(5):720   736.

mackay, d. j. c. (1998). introduction to gaussian processes. in bishop, c. m., editor, neural networks
pp. 84, 92

and machine learning. springer-verlag.

mackay, d. j. c. (1999). comparison of approximate methods for handling hyperparameters. neural
p. 110

computation, 11(5):1035   1068.

mackay, d. j. c. (2003). id205, id136, and learning algorithms. cambridge univer-
pp. xiv, 167

sity press, cambridge, uk.

malzahn, d. and opper, m. (2002). a variational approach to learning curves. in diettrich, t. g.,
becker, s., and ghahramani, z., editors, advances in neural information processing systems 14. mit
press.
p. 161

mandelbrot, b. b. (1982). the fractal geometry of nature. w. h. freeman, san francisco.

p. 137

mardia, k. v. and marshall, r. j. (1984). id113 for models of residual
p. 115

covariance in spatial regression. biometrika, 71(1):135   146.

mat  ern, b. (1960). spatial variation. meddelanden fr  an statens skogsforskningsinstitut, 49, no.5.
pp. 85, 87, 89

alm  anna f  orlaget, stockholm. second edition (1986), springer-verlag, berlin.

matheron, g. (1973). the intrinsic random functions and their applications. advances in applied
p. 30

id203, 5:439   468.

maxwell, j. c. (1850). letter to lewis campbell; reproduced in l. campbell and w. garrett, the life
p. v

of james clerk maxwell, macmillan, 1881.

mcallester, d. (2003). pac-bayesian stochastic model selection. machine learning, 51(1):5   21. p. 164

mccullagh, p. and nelder, j. (1983). generalized linear models. chapman and hall.

pp. 37, 38, 138

meinguet, j. (1979). multivariate interpolation at arbitrary points made simple. journal of applied
p. 137

mathematics and physics (zamp), 30:292   304.

meir, r. and zhang, t. (2003). generalization error bounds for bayesian mixture algorithms. journal
p. 164

of machine learning research, 4(5):839   860.

micchelli, c. a. and pontil, m. (2005). kernels for id72.

in saul, l. k., weiss, y.,
and bottou, l., editors, advances in neural information processing systems 17, pages 921   928. mit
p. 190
press.

micchelli, c. a. and wahba, g. (1981). design problems for optimal surface interpolation. in ziegler,
p. 161

z., editor, approximation theory and applications, pages 329   348. academic press.

minka, t. p. (2001). a family of algorithms for approximate bayesian id136. phd thesis, mas-
pp. 41, 52

sachusetts institute of technology.

minka, t. p. (2003). a comparison of numerical optimizers for id28.

http://research.microsoft.com/   minka/papers/logreg.

p. 38

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

bibliography

minka, t. p. and picard, r. w. (1999). learning how to learn is learning with point sets.

http://research.microsoft.com/   minka/papers/point-sets.html.

mitchell, t. m. (1997). machine learning. mcgraw-hill, new york.

231

p. 116

pp. 2, 165

murray-smith, r. and girard, a. (2001). gaussian process priors with arma noise models.

in
irish signals and systems conference, pages 147   153, maynooth. http://www.dcs.gla.ac.uk/   rod/
publications/murgir01.pdf.
p. 191

neal, r. m. (1996). bayesian learning for neural networks. springer, new york. lecture notes in
pp. xiii, 30, 90, 91, 106, 166, 167

statistics 118.

neal, r. m. (1997). monte carlo implementation of gaussian process models for bayesian regres-
sion and classi   cation. technical report 9702, department of statistics, university of toronto.
http://www.cs.toronto.edu/   radford.
p. 191

neal, r. m. (1999). regression and classi   cation using gaussian process priors. in bernardo, j. m.,
berger, j. o., dawid, a. p., and smith, a. f. m., editors, bayesian statistics 6, pages 475   501. oxford
pp. 41, 47
university press. (with discussion).

neal, r. m. (2001). annealed importance sampling. statistics and computing, 11:125   139.

p. 72

neumaier, a. (2005). introduction to global optimization. http://www.mat.univie.ac.at/   neum/
p. 193

glopt/intro.html.

o   hagan, a. (1978). curve fitting and optimal design for prediction. journal of the royal statistical
pp. 28, 30, 94

society b, 40:1   42. (with discussion).

o   hagan, a. (1991). bayes-hermite quadrature. journal of statistical planning and id136, 29:245   
pp. 193, 194

260.

o   hagan, a., kennedy, m. c., and oakley, j. e. (1999). uncertainty analysis and other id136 tools
for complex computer codes. in bernardo, j. m., berger, j. o., dawid, a. p., and smith, a. f. m.,
editors, bayesian statistics 6, pages 503   524. oxford university press. (with discussion).
p. 194

  ksendal, b. (1985). stochastic di   erential equations. springer-verlag, berlin.

p. 208

opper, m. and vivarelli, f. (1999). general bounds on bayes errors for regression with gaussian
processes. in kearns, m. s., solla, s. a., and cohn, d. a., editors, advances in neural information
processing systems 11, pages 302   308. mit press.
p. 160

opper, m. and winther, o. (2000). gaussian processes for classi   cation: mean-field algorithms.
pp. 41, 44, 52, 127, 128

neural computation, 12(11):2655   2684.

o   sullivan, f., yandell, b. s., and raynor, w. j. (1986). automatic smoothing of regression functions
in generalized linear models. journal of the american statistical association, 81:96   103. pp. 132, 138

paciorek, c. and schervish, m. j. (2004). nonstationary covariance functions for gaussian process re-
gression. in thrun, s., saul, l., and sch  olkopf, b., editors, advances in neural information processing
pp. 93, 94
systems 16. mit press.

papoulis, a. (1991). id203, random variables, and stochastic processes. mcgraw-hill, new york.
pp. 79, 153, 154, 191, 211, 212

third edition.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

232

bibliography

plaskota, l. (1996). noisy information and computational complexity. cambridge university press,
pp. 161, 169

cambridge.

plate, t. a. (1999). accuarcy versus interpretability in flexible modeling: implementing a tradeo   
p. 95

using gaussian process models. behaviourmetrika, 26(1):29   50.

platt, j. c. (1999). fast training of support vector machines using sequential minimal optimization.
in sch  olkopf, b., burges, c. j. c., and smola, a. j., editors, advances in kernel methods, pages
p. 144
185   208. mit press.

platt, j. c. (2000). probabilities for sv machines.

in smola, a., bartlett, p., sch  olkopf, b.,
and schuurmans, d., editors, advances in large margin classi   ers, pages 61   74. mit press.
pp. 69, 70, 145, 147, 148

poggio, t. and girosi, f. (1990). networks for approximation and learning. proceedings of ieee,
pp. 89, 133, 134, 135, 147, 176

78:1481   1497.

poggio, t., voorhees, h., and yuille, a. (1985). a regularized solution to edge detection. technical
p. 154

report ai memo 833, mit ai laboratory.

pontil, m., mukherjee, s., and girosi, f. (1998). on the noise model of support vector machine
p. 146

regression. technical report ai memo 1651, mit ai laboratory.

press, w. h., teukolsky, s. a., vetterling, w. t., and flannery, b. p. (1992). numerical recipes in c.
pp. 96, 99, 201

cambridge university press, second edition.

qui  nonero-candela, j. (2004). learning with uncertainty   gaussian processes and relevance vector
machines. phd thesis, informatics and mathematical modelling, technical univeristy of denmark.
p. 177

rasmussen, c. e. (1996). evaluation of gaussian processes and other methods for non-linear re-
gression. phd thesis, dept. of computer science, university of toronto. http://www.kyb.mpg.de/
p. 30
publications/pss/ps2304.ps.

rasmussen, c. e. (2003). gaussian processes to speed up hybrid monte carlo for expensive bayesian
integrals. in bernardo, j. m., bayarri, m. j., berger, j. o., dawid, a. p., heckerman, d., smith, a.
f. m., and west, m., editors, bayesian statistics 7, pages 651   659. oxford university press. p. 193

rasmussen, c. e. and ghahramani, z. (2001). occam   s razor. in leen, t., dietterich, t. g., and tresp,
v., editors, advances in neural information processing systems 13, pages 294   300. mit press. p. 110

rasmussen, c. e. and ghahramani, z. (2002).

in
diettrich, t. g., becker, s., and ghahramani, z., editors, advances in neural information processing
systems 14. mit press.
p. 192

in   nite mixtures of gaussian process experts.

rasmussen, c. e. and ghahramani, z. (2003). bayesian monte carlo. in suzanna becker, s. t. and
obermayer, k., editors, advances in neural information processing systems 15, pages 489   496. mit
press.
p. 193

rasmussen, c. e. and qui  nonero-candela, j. (2005). healing the relevance vector machine through
pp. 150, 176

augmentation. in proc. 22nd international conference on machine learning.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

bibliography

233

rifkin, r. and klautau, a. (2004). in defense of one-vs-all classi   cation. journal of machine learning
pp. 146, 147

research, 5:101   141.

ripley, b. (1981). spatial statistics. wiley, new york.

p. 30

ripley, b. (1996). pattern recognition and neural networks. cambridge university press, cambridge,
p. 35

uk.

ritter, k. (2000). average-case analysis of numerical problems. springer verlag. pp. 159, 161, 169, 193

ritter, k., wasilkowski, g. w., and wo  zniakowski, h. (1995). multivariate integration and approxima-
tion of random fields satisfying sacks-ylvisaker conditions. annals of applied id203, 5:518   540.
p. 97

rousseeuw, p. j. (1984). least median of squares regression. journal of the american statistical
p. 146

association, 79:871   880.

sacks, j., welch, w. j., mitchell, t. j., and wynn, h. p. (1989). design and analysis of computer
pp. 16, 30

experiments. statistical science, 4(4):409   435.

saitoh, s. (1988). theory of reproducing kernels and its applications. longman, harlow, england.
p. 129

salton, g. and buckley, c. (1988). term-weighting approaches in automatic text retrieval. information
p. 100

processing and management, 24:513   523.

sampson, p. d. and guttorp, p. (1992). nonparametric estimation of nonstationary covariance struc-
p. 92

ture. journal of the american statistical association, 87:108   119.

santner, t. j., williams, b. j., and notz, w. (2003). the design and analysis of computer experiments.
p. 30

springer, new york.

saunders, c., gammerman, a., and vovk, v. (1998). ridge regression learning algorithm in dual
variables. in shavlik, j., editor, proceedings of the fifteenth international conference on machine
learning (icml 1998). morgan kaufmann.
p. 30

saunders, c., shawe-taylor, j., and vinokourov, a. (2003). string kernels, fisher kernels and finite
state automata. in becker, s., thrun, s., and obermayer, k., editors, advances in neural information
processing systems 15. mit press.
p. 101

schoenberg, i. j. (1938). metric spaces and positive de   nite functions. trans. american mathematical
p. 86

society, 44(3):522   536.

schoenberg, i. j. (1964). spline functions and the problem of graduation. proc. nat. acad. sci. usa,
pp. 132, 138

52:947   950.

sch  olkopf, b. and smola, a. j. (2002). learning with kernels. mit press.

pp. xvi, 73, 89, 90, 91, 129, 130, 133, 141, 144, 147, 173, 188, 195, 196

sch  olkopf, b., smola, a. j., and m  uller, k.-r. (1998). nonlinear component analysis as a kernel
p. 99

eigenvalue problem. neural computation, 10:1299   1319.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

234

bibliography

schwaighofer, a. and tresp, v. (2003). transductive and inductive methods for approximate gaus-
sian process regression. in becker, s., thrun, s., and obermayer, k., editors, advances in neural
information processing systems 15. mit press.
pp. 181, 184

scott, d. w. (1992). multivariate density estimation. wiley, new york.

p. 25

seeger, m. (2000). bayesian model selection for support vector machines, gaussian processes and
other kernel classi   ers. in solla, s. a., leen, t. k., and m  uller, k.-r., editors, advances in neural
pp. 41, 54, 145
information processing systems 12. mit press, cambridge, ma.

seeger, m. (2002). pac-bayesian generalisation error bounds for gaussian process classi   cation.
pp. 161, 164, 165

journal of machine learning research, 3:322   269.

seeger, m. (2003).

bayesian gaussian process models: pac-bayesian generalisation error
bounds and sparse approximations. phd thesis, school of informatics, university of edinburgh.
http://www.cs.berkeley.edu/   mseeger.
pp. 46, 145, 161, 162, 163, 179, 180, 186

seeger, m. (2005). expectation propagation for exponential families. http://www.cs.berkeley.edu/
p. 127

   mseeger/papers/epexpfam.ps.gz.

seeger, m. and jordan, m. i. (2004). sparse gaussian process classi   cation with multiple classes.
p. 50

technical report tr 661, department of statistics, university of california at berkeley.

seeger, m., williams, c. k. i., and lawrence, n. (2003). fast forward selection to speed up sparse
gaussian process regression.
in bishop, c. and frey, b. j., editors, proceedings of the ninth in-
ternational workshop on arti   cial intelligence and statistics. society for arti   cial intelligence and
statistics.
pp. 178, 180

shawe-taylor, j. and williams, c. k. i. (2003). the stability of kernel principal components analysis
and its relation to the process eigenspectrum. in becker, s., thrun, s., and obermayer, k., editors,
advances in neural information processing systems 15. mit press.
p. 99

shepp, l. a. (1966). radon-nikodym derivatives of gaussian measures. annals of mathematical statis-
p. 139

tics, 37(2):321   354.

silverman, b. w. (1978). density ratios, empirical likelihood and cot death. applied statistics,
p. 138

27(1):26   33.

silverman, b. w. (1984). spline smoothing: the equivalent variable kernel method. annals of
pp. 25, 153, 154

statistics, 12(3):898   916.

silverman, b. w. (1985). some aspects of the spline smoothing approach to non-parametric regression
pp. 170, 175

curve fitting (with discussion). j. roy. stat. soc. b, 47(1):1   52.

simard, p., victorri, b., le cun, y., and denker, j. (1992). tangent prop   a formalism for specifying
selected invariances in an adaptive network. in moody, j. e., hanson, s. j., and lippmann, r. p.,
editors, advances in neural information processing systems 4, pages 895   903. morgan kaufmann.
pp. 73, 195

smola, a. j. and bartlett, p. l. (2001). sparse greedy gaussian process regression. in leen, t. k.,
diettrich, t. g., and tresp, v., editors, advances in neural information processing systems 13, pages
p. 176
619   625. mit press.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

bibliography

235

smola, a. j. and sch  olkopf, b. (2000). sparse greedy matrix approximation for machine learning. in
proceedings of the seventeenth international conference on machine learning. morgan kaufmann.
pp. 173, 174

solak, e., murray-smith, r., leithead, w. e., leith, d., and rasmussen, c. e. (2003). derivative
observations in gaussian process models of dynamic systems. in becker, s., s. t. and obermayer,
k., editors, advances in neural information processing systems 15, pages 1033   1040. mit press.
p. 191

sollich, p. (1999). learning curves for gaussian processes. in kearns, m. s., solla, s. a., and cohn,
pp. 160, 161

d. a., editors, neural information processing systems, vol. 11. mit press.

sollich, p. (2002). bayesian methods for support vector machines: evidence and predictive class
pp. 145, 150, 161

probabilities. machine learning, 46:21   52.

sollich, p. and williams, c. k. i. (2005). using the equivalent kernel to understand gaussian process
in saul, l. k., weiss, y., and bottou, l., editors, advances in neural information
p. 154

regression.
processing systems 17. mit press.

stein, m. l. (1991). a kernel approximation to the kriging predictor of a spatial process. ann. inst.
p. 154

statist. math, 43(1):61   75.

stein, m. l. (1999). interpolation of spatial data. springer-verlag, new york.

pp. 82, 83, 85, 86, 87, 115, 137, 157, 158, 161, 212

steinwart, i. (2005). consistency of support vector machines and other regularized kernel classi   ers.
p. 157

ieee trans. on id205, 51(1):128   142.

stitson, m. o., gammerman, a., vapnik, v. n., vovk, v., watkins, c. j. c. h., and weston, j. (1999).
support vector regression with anova decomposition kernels. in sch  olkopf, b., burges, c. j. c.,
and smola, a. j., editors, advances in kernel methods. mit press.
p. 95

sundararajan, s. and keerthi, s. s. (2001). predictive approaches for choosing hyperparameters in
p. 117

gaussian processes. neural computation, 13:1103   1118.

suykens, j. a. k. and vanderwalle, j. (1999). least squares support vector machines. neural processing
p. 147

letters, 9:293   300.

szeliski, r. (1987). id173 uses fractal priors. in proceedings of the 6th national conference
pp. 135, 137

on arti   cial intelligence (aaai-87).

teh, y. w., seeger, m., and jordan, m. i. (2005). semiparametric latent factor models. in cowell,
r. g. and ghahramani, z., editors, proceedings of tenth international workshop on arti   cial inte
lligence and statistics, pages 333   340. society for arti   cial intelligence and statistics.
p. 190

thomas-agnan, c. (1996). computing a family of reproducing kernels for statistical applications.
p. 154

numerical algorithms, 13:21   32.

thompson, p. d. (1956). optimum smoothing of two-dimensional fields. tellus, 8:384   393.

p. 30

tikhonov, a. n. (1963). solution of incorrectly formulated problems and the id173 method.
p. 133

soviet. math. dokl., 5:1035   1038.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

236

bibliography

tikhonov, a. n. and arsenin, v. y. (1977). solutions of ill-posed problems. w. h. winston, washington,
p. 133

d.c.

tipping, m. e. (2001). sparse bayesian learning and the relevance vector machine. journal of machine
p. 149

learning research, 1:211   244.

tipping, m. e. and faul, a. c. (2003). fast marginal likelihood maximisation for sparse bayesian
models. in bishop, c. m. and frey, b. j., editors, proceedings of ninth international workshop on
p. 149
arti   cial intelligence and statistics. society for arti   cial intelligence and statistics.

tresp, v. (2000). a bayesian committee machine. neural computation, 12(11):2719   2741.

pp. 180, 181, 185, 187

tsuda, k., kawanabe, m., r  atsch, g., sonnenburg, s., and m  uller, k.-r. (2002). a new discriminative
p. 102

kernel from probabilistic models. neural computation, 14(10):2397   2414.

uhlenbeck, g. e. and ornstein, l. s. (1930). on the theory of brownian motion. phys. rev., 36:823   841.
pp. 86, 212

valiant, l. g. (1984). a theory of the learnable. communications of the acm, 27(11):1134   1142.
p. 161

vapnik, v. n. (1995). the nature of statistical learning theory. springer verlag, new york.

pp. 36, 141, 181

vapnik, v. n. (1998). statistical learning theory. john wiley and sons.

p. 140

vijayakumar, s., d   souza, a., and schaal, s. (2005). incremental online learning in high dimensions.
pp. 22, 24

accepted for publication in neural computation.

vijayakumar, s., d   souza, a., shibata, t., conradt, j., and schaal, s. (2002). statistical learning for
p. 22

humanoid robots. autonomous robot, 12(1):55   69.

vijayakumar, s. and schaal, s. (2000). lwpr: an o(n) algorithm for incremental real time learning
in high dimensional space. in proc. of the seventeenth international conference on machine learning
(icml 2000), pages 1079   1086.
p. 22

vishwanathan, s. v. n. and smola, a. j. (2003). fast kernels for string and tree matching. in becker,
s., thrun, s., and obermayer, k., editors, advances in neural information processing systems 15.
mit press.
p. 101

vivarelli, f. and williams, c. k. i. (1999). discovering hidden features with gaussian processes
regression. in kearns, m. s., solla, s. a., and cohn, d. a., editors, advances in neural information
processing systems 11. mit press.
p. 89

von mises, r. (1964). mathematical theory of id203 and statistics. academic press.

p. 200

wahba, g. (1978). improper priors, spline smoothing and the problem of guarding against model
p. 139

errors in regression. journal of the royal statistical society b, 40(3):364   372.

wahba, g. (1985). a comparison of gcv and gml for choosing the smoothing parameter in the
p. 29

generalized spline smoothing problem. annals of statistics, 13:1378   1402.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

bibliography

237

wahba, g. (1990).

society for industrial and applied
mathematics, philadelphia, pa. cbms-nsf regional conference series in applied mathematics.
pp. 95, 112, 117, 118, 129, 131, 137, 138, 157, 176

spline models for observational data.

wahba, g., johnson, d. r., gao, f., and gong, j. (1995). adaptive tuning of numerical weather
prediction models: randomized gcv in three-and four-dimensional data assimilation. monthly
weather review, 123:3358   3369.
p. 181

watkins, c. j. c. h. (1999). dynamic alignment kernels. technical report csd-tr-98-11, dept of
p. 101

computer science, royal holloway, university of london.

watkins, c. j. c. h. (2000). dynamic alignment kernels. in smola, a. j., bartlett, p. l., and sch  olkopf,
b., editors, advances in large margin classi   ers, pages 39   50. mit press, cambridge, ma. p. 100

wegman, e. j. (1982). reproducing kernel hilbert spaces. in kotz, s. and johnson, n. l., editors,
pp. 129, 130

encyclopedia of statistical sciences, volume 8, pages 81   84. wiley, new york.

weinert, h. l., editor (1982). reproducing kernel hilbert spaces. hutchinson ross, stroudsburg,
p. 129

pennsylvania.

wendland, h. (2005). scattered data approximation. cambridge monographs on applied and compu-
p. 88

tational mathematics. cambridge university press.

whittle, p. (1963). prediction and regulation by linear least-square methods. english universities
pp. 30, 216

press.

widom, h. (1963). asymptotic behavior of the eigenvalues of certain integral equations. trans. of the
p. 97

american mathematical society, 109(2):278   295.

widom, h. (1964). asymptotic behavior of the eigenvalues of certain integral equations ii. archive
p. 97

for rational mechanics and analysis, 17:215   229.

wiener, n. (1949). extrapolation, interpolation and smoothing of stationary time series. mit press,
p. 29

cambridge, mass.

williams, c. k. i. (1998). computation with in   nite neural networks. neural computation, 10(5):1203   
p. 91

1216.

williams, c. k. i. and barber, d. (1998). bayesian classi   cation with gaussian processes.

transactions on pattern analysis and machine intelligence, 20(12):1342   1351.

ieee
pp. 41, 45, 48, 49

williams, c. k. i. and rasmussen, c. e. (1996). gaussian processes for regression. in touretzky, d. s.,
mozer, m. c., and hasselmo, m. e., editors, advances in neural information processing systems 8,
pages 514   520. mit press.
pp. 30, 107

williams, c. k. i., rasmussen, c. e., schwaighofer, a., and tresp, v. (2002). observations on
the nystr  om method for gaussian process prediction. technical report, university of edinburgh.
http://www.dai.ed.ac.uk/homes/ckiw/online pubs.html.
p. 177

williams, c. k. i. and seeger, m. (2001). using the nystr  om method to speed up kernel machines.
in leen, t. k., diettrich, t. g., and tresp, v., editors, advances in neural information processing
systems 13, pages 682   688. mit press.
pp. 173, 177

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

238

bibliography

williams, c. k. i. and vivarelli, f. (2000). upper and lower bounds on the learning curve for gaussian
pp. 31, 161, 168

proccesses. machine learning, 40:77   102.

winkler, g. (1995). image analysis, random fields and dynamic monte carlo methods. springer,
p. 219

berlin.

wong, e. (1971). stochastic processes in information and dynamical systems. mcgraw-hill, new york.
p. 218

wood, s. and kohn, r. (1998). a bayesian approach to robust binary nonparametric regression. j.
p. 45

american statistical association, 93(441):203   213.

yaglom, a. m. (1987). correlation theory of stationary and related random functions volume i: basic
p. 89

results. springer verlag.

yang, c., duraiswami, r., and david, l. (2005). e   cient kernel machines using the improved fast
gauss transform. in saul, l. k., weiss, y., and bottou, l., editors, advances in neural information
processing systems 17. mit press.
p. 182

ylvisaker, d. (1975). designs on random fields. in srivastava, j. n., editor, a survey of statistical
p. 159

design and linear models, pages 593   608. north-holland.

yuille, a. and grzywacz, n. m. (1989). a mathematical analysis of motion coherence theory. inter-
p. 134

national journal of id161, 3:155   175.

zhang, t. (2004). statistical behaviour and consistency of classi   cation methods based on convex
p. 157

risk minimization (with discussion). annals of statistics, 32(1):56   85.

zhu, h., williams, c. k. i., rohwer, r. j., and morciniec, m. (1998). gaussian regression and optimal
finite dimensional linear models. in bishop, c. m., editor, neural networks and machine learning.
springer-verlag, berlin.
p. 97

zhu, j. and hastie, t. j. (2002). kernel id28 and the import vector machine. in diettrich,
t. g., becker, s., and ghahramani, z., editors, advances in neural information processing systems
14, pages 1081   1088. mit press.
p. 185

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

author index

abrahamsen, p. 82
abramowitz, m. 84, 85
adams, r. 134
adler, r. j. 80, 81, 83, 191, 218
amari, s. 102
ansley, c. f. 29
ansley, c. f., see kohn, r. 141
arat  o, m. 212
arfken, g. xv, 134
aronszajn, n. 129, 130
arsenin, v. y., see tikhonov, a. n. 133

bach, f. r. 97
baker, c. t. h. 98, 99
balakrishnan, n., see johnson, n. l. 45
barber, d. 31
barber, d., see williams, c. k. i. 41, 45, 48, 49
bartle, r. g. 204
bartlett, p. l. 157
bartlett, p. l., see lanckriet, g. r. g. 128
bartlett, p. l., see smola, a. j. 176
belongie, s., see fowlkes, c. 172
bengio, s., see collobert, r. 69, 72
berger, j. o. 22, 35
bishop, c. m. 45, 196
bishop, c. m., see goldberg, p. w. 191
blake, i. f. 81
blight, b. j. n. 28
boyd, s. 206
boyle, p. 190
bracewell, r. n. 83, 206
buckley, c., see salton, g. 100

caruana, r. 115
chat   eld, c. 82, 209, 214
chellappa, r., see kashyap, r. l. 216

choi, t. 156
choudhuri, n. 156
chow, y., see grenander, u. 214, 216
chu, w. 191
collins, m. 101
collobert, r. 69, 72
conradt, j., see vijayakumar, s. 22
cornford, d. 85
cox, d. d. 156
craven, p. 112
cressie, n. a. c. 30, 137, 190
cristianini, n. 128, 141
cristianini, n., see lanckriet, g. r. g. 128
cristianini, n., see lodhi, h. 101
csat  o, l. 179, 180, 185

daley, r. 30
david, h. a. 169, 170
david, l., see yang, c. 182
davis, p. j. 216
dawid, a. p. 34
dellaportas, p. 192
denker, j., see simard, p. 73, 195
devroye, l. 156, 166
diaconis, p. 156
diekhans, m., see jaakkola, t. s. 101, 102, 104
diggle, p. j. 191, 214, 215
doob, j. l. 204, 212
drineas, p. 174
d   souza, a., see vijayakumar, s. 22, 24
duchon, j. 137
duda, r. o. 146
du   y, n., see collins, m. 101
duraiswami, r., see yang, c. 182

eddy, w. f., see geisser, s. 117

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

240

author index

edgeworth, f. y. 146
el ghaoui, l., see lanckriet, g. r. g. 128
elissee   , a., see cristianini, n. 128
eskin, e., see leslie, c. 100, 101, 104

faul, a. c. 149
faul, a. c., see tipping, m. e. 149
feldman, j. 157
ferrari trecate, g. 152
fine, s. 47, 174
flannery, b. p., see press, w. h. 96, 99, 201
fowlkes, c. 172
frean, m., see boyle, p. 190
freedman, d. 156
freedman, d., see diaconis, p. 156
frieze, a. 174

gammerman, a., see saunders, c. 30
gammerman, a., see stitson, m. o. 95
gao, f., see lin, x. 185
gao, f., see wahba, g. 181
geisser, s. 117
geman, d., see geman, s. 216
geman, s. 216
ghahramani, z., see chu, w. 191
ghahramani, z., see rasmussen, c. e. 110, 192,

193

ghosal, s., see choudhuri, n. 156
gibbs, m. n. 41, 93, 181
gihman, i. i. 82
girard, a. 192
girard, a., see murray-smith, r. 191
girosi, f. 25, 146
girosi, f., see poggio, t. 89, 133   135, 147, 176
girosi, f., see pontil, m. 146
goldberg, p. w. 191
golub, g. h. 172   174, 181, 202
gong, j., see wahba, g. 181
gradshteyn, i. s. 98, 103
green, p. j. 138
grenander, u. 214, 216
grimmett, g. r. 94, 213
gr  unwald, p. d. 156
grzywacz, n. m., see yuille, a. 134
guttorp, p., see sampson, p. d. 92
gy  or   , l. 156
gy  or   , l., see devroye, l. 156, 166

hajek, j. 157
hand, d. j. 100
hannan, e. j. 215
hansen, l. k. 36
hart, p. e., see duda, r. o. 146
hastie, t. j. 24, 25, 95
hastie, t. j., see zhu, j. 185
haussler, d. 101
haussler, d., see jaakkola, t. s. 41, 101, 102, 104
hawkins, d. l. 97
herbrich, r., see lawrence, n. 178, 185
hinton, g. e., see jacobs, r. a. 192
hoerl, a. e. 11
hornik, k. 90
huijbregts, c. j., see journel, a. g. 30

ihara, s. 218

jaakkola, t. s. 41, 101, 102, 104
jacobs, r. a. 192
johnson, d. r., see wahba, g. 181
johnson, n. l. 45
jones, d. r. 193
jones, m., see girosi, f. 25
jordan, m. i. 218
jordan, m. i., see bach, f. r. 97
jordan, m. i., see bartlett, p. l. 157
jordan, m. i., see jacobs, r. a. 192
jordan, m. i., see lanckriet, g. r. g. 128
jordan, m. i., see seeger, m. 50
jordan, m. i., see teh, y. w. 190
journel, a. g. 30

kailath, t. 131
kamid113r, d. w. 208
kandola, j., see cristianini, n. 128
kannan, r., see frieze, a. 174
kashyap, r. l. 216
kawanabe, m., see tsuda, k. 102
keeling, c. d. 119
keenan, d. m., see grenander, u. 214, 216
keerthi, s. s., see sundararajan, s. 117
kennard, r. w., see hoerl, a. e. 11
kennedy, m. c., see o   hagan, a. 194
kent, j. t. 137
kimeldorf, g. 132, 136
klautau, a., see rifkin, r. 146, 147
klein, b., see lin, x. 185

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

author index

klein, r., see lin, x. 185
kohler, m., see gy  or   , l. 156
kohn, r. 141
kohn, r., see ansley, c. f. 29
kohn, r., see wood, s. 45
kolmogorov, a. n. 29
k  onig, h. 96
kotz, s., see johnson, n. l. 45
krzy  zak, a., see gy  or   , l. 156
kullback, s. 158, 203
kuss, m. 72, 73

lanckriet, g. r. g. 128
langford, j., see gr  unwald, p. d. 156
lauritzen, s. l. 29
lawrence, n. 178, 185, 196
lawrence, n., see seeger, m. 178, 180
le cun, y., see simard, p. 73, 195
leith, d., see solak, e. 191
leithead, w. e., see solak, e. 191
leslie, c. 100, 101, 104
liisberg, c., see hansen, l. k. 36
lin, x. 185
lindley, d. v. 111
lindsey, w. c., see blake, i. f. 81
lodhi, h. 101
lugosi, g., see devroye, l. 156, 166
luo, z. 176

mackay, d. j. c. xiii, xiv, xvi, 45, 84, 92, 109,

110, 166, 167, 178

mackay, d. j. c., see gibbs, m. n. 41, 181
mahoney, m. w., see drineas, p. 174
malik, j., see fowlkes, c. 172
malzahn, d. 161
mandelbrot, b. b. 137
mannila, h., see hand, d. j. 100
mardia, k. v. 115
mardia, k. v., see kent, j. t. 137
marshall, r. j., see mardia, k. v. 115
mat  ern, b. 85, 87, 89
matheron, g. 30
maxwell, j. c. v
mcallester, d. 164
mcauli   e, j. d., see bartlett, p. l. 157
mccullagh, p. 37, 38, 138
meinguet, j. 137
meir, r. 164

241

micchelli, c. a. 161, 190
minka, t. p. 38, 41, 52, 116
mitchell, t. j., see sacks, j. 16, 30
mitchell, t. m. 2, 165
morciniec, m., see zhu, h. 97
moyeed, r. a., see diggle, p. j. 191
mukherjee, s., see pontil, m. 146
m  uller, k.-r., see sch  olkopf, b. 99
m  uller, k.-r., see tsuda, k. 102
murray-smith, r. 191
murray-smith, r., see girard, a. 192
murray-smith, r., see solak, e. 191

nabney, i. t., see cornford, d. 85
neal, r. m. xiii, 30, 41, 47, 72, 90, 91, 106, 166,

167, 191

nelder, j., see mccullagh, p. 37, 38, 138
neumaier, a. 193
notz, w., see santner, t. j. 30
nowlan, s. j., see jacobs, r. a. 192

oakley, j. e., see o   hagan, a. 194
o   hagan, a. 28, 30, 94, 193, 194
  ksendal, b. 208
opper, m. 41, 44, 52, 127, 128, 160
opper, m., see csat  o, l. 180, 185
opper, m., see ferrari trecate, g. 152
opper, m., see malzahn, d. 161
ornstein, l. s., see uhlenbeck, g. e. 86, 212
o   sullivan, f., see cox, d. d. 156
o   sullivan, f. 132, 138
ott, l., see blight, b. j. n. 28

paciorek, c. 93, 94
papoulis, a. 79, 153, 154, 191, 211, 212
picard, r. w., see minka, t. p. 116
plaskota, l. 161, 169
plate, t. a. 95
platt, j. c. 69, 70, 144, 145, 147, 148
poggio, t. 89, 133   135, 147, 154, 176
poggio, t., see girosi, f. 25
pontil, m. 146
pontil, m., see micchelli, c. a. 190
press, w. h. 96, 99, 201

qui  nonero-candela, j. 177
qui  nonero-candela, j., see girard, a. 192
qui  nonero-candela, j., see rasmussen, c. e. 150,

176

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

242

author index

rasmussen, c. e. 30, 110, 150, 176, 192, 193
rasmussen, c. e., see girard, a. 192
rasmussen, c. e., see kuss, m. 72, 73
rasmussen, c. e., see solak, e. 191
rasmussen, c. e., see williams, c. k. i. 30, 107,

177

r  atsch, g., see tsuda, k. 102
raynor, w. j., see o   sullivan, f. 132, 138
rifkin, r. 146, 147
ripley, b. 30, 35
ritter, k. 97, 159, 161, 169, 193
rohwer, r. j., see zhu, h. 97
rousseeuw, p. j. 146
roy, a., see choudhuri, n. 156
ryzhik, i. m., see gradshteyn, i. s. 98, 103

saad, d., see barber, d. 31
sacks, j. 16, 30
saitoh, s. 129
salamon, p., see hansen, l. k. 36
salton, g. 100
sampson, p. d. 92
santner, t. j. 30
saunders, c. 30, 101
schaal, s., see vijayakumar, s. 22, 24
scheinberg, k., see fine, s. 47, 174
schervish, m. j., see choi, t. 156
schervish, m. j., see paciorek, c. 93, 94
schoenberg, i. j. 86, 132, 138
sch  olkopf, b. xvi, 73, 89   91, 99, 129, 130, 133,

141, 144, 147, 173, 188, 195, 196

sch  olkopf, b., see smola, a. j. 173, 174
schwaighofer, a. 181, 184
schwaighofer, a., see williams, c. k. i. 177
scott, d. w. 25
seeger, m. 41, 46, 50, 54, 127, 145, 161   165,

178   180, 186

seeger, m., see lawrence, n. 178, 185
seeger, m., see teh, y. w. 190
seeger, m., see williams, c. k. i. 173, 177
shawe-taylor, j. 99
shawe-taylor, j., see cristianini, n. 128, 141
shawe-taylor, j., see lodhi, h. 101
shawe-taylor, j., see saunders, c. 101
shepp, l. a. 139
shibata, t., see vijayakumar, s. 22
silverman, b. w. 25, 138, 153, 154, 170, 175
silverman, b. w., see green, p. j. 138

simard, p. 73, 195
skorohod, a. v., see gihman, i. i. 82
smola, a. j. 173, 174, 176
smola, a. j., see sch  olkopf, b. xvi, 73, 89   91, 99,
129, 130, 133, 141, 144, 147, 173, 188, 195, 196

smola, a. j., see vishwanathan, s. v. n. 101
smyth, p., see hand, d. j. 100
solak, e. 191
sollich, p. 145, 150, 154, 160, 161
sonnenburg, s., see tsuda, k. 102
sta   ord noble, w., see leslie, c. 100, 101, 104
stegun, i. a., see abramowitz, m. 84, 85
stein, m. l. 82, 83, 85   87, 115, 137, 154, 157, 158,

161, 212

steinwart, i. 157
stephens, d. a., see dellaportas, p. 192
stirzaker, d. r., see grimmett, g. r. 94, 213
stitson, m. o. 95
sundararajan, s. 117
suykens, j. a. k. 147
svensen, m., see bishop, c. m. 196
szeliski, r. 135, 137

tawn, j. a., see diggle, p. j. 191
teh, y. w. 190
teukolsky, s. a., see press, w. h. 96, 99, 201
thomas-agnan, c. 154
thompson, p. d. 30
tibshirani, r. j., see hastie, t. j. 24, 25, 95
tikhonov, a. n. 133
tipping, m. e. 149
tipping, m. e., see faul, a. c. 149
tresp, v. 180, 181, 185, 187
tresp, v., see schwaighofer, a. 181, 184
tresp, v., see williams, c. k. i. 177
tsuda, k. 102

uhlenbeck, g. e. 86, 212

valiant, l. g. 161
van loan, c. f., see golub, g. h. 172   174, 181,

202

vandenberghe, l., see boyd, s. 206
vanderwalle, j., see suykens, j. a. k. 147
vapnik, v. n. 36, 140, 141, 181
vapnik, v. n., see stitson, m. o. 95
vempala, s., see frieze, a. 174
vetterling, w. t., see press, w. h. 96, 99, 201

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

author index

243

victorri, b., see simard, p. 73, 195
vijayakumar, s. 22, 24
vinokourov, a., see saunders, c. 101
vishwanathan, s. v. n. 101
vivarelli, f. 89
vivarelli, f., see opper, m. 160
vivarelli, f., see williams, c. k. i. 31, 161, 168
von mises, r. 200
voorhees, h., see poggio, t. 154
vovk, v., see saunders, c. 30
vovk, v., see stitson, m. o. 95

wahba, g. 29, 95, 112, 117, 118, 129, 131,

137   139, 157, 176, 181

wahba, g., see craven, p. 112
wahba, g., see kimeldorf, g. 132, 136
wahba, g., see lin, x. 185
wahba, g., see luo, z. 176
wahba, g., see micchelli, c. a. 161
walk, h., see gy  or   , l. 156
wasilkowski, g. w., see ritter, k. 97
watkins, c. j. c. h. 100, 101
watkins, c. j. c. h., see lodhi, h. 101
watkins, c. j. c. h., see stitson, m. o. 95
wegman, e. j. 129, 130
welch, w. j., see sacks, j. 16, 30
wendland, h. 88
weston, j., see leslie, c. 100, 101, 104
weston, j., see stitson, m. o. 95
whittle, p. 30, 216
whorf, t. p., see keeling, c. d. 119
widom, h. 97
wiener, n. 29

williams, b. j., see santner, t. j. 30
williams, c. k. i. 30, 31, 41, 45, 48, 49, 91, 107,

161, 168, 173, 177

williams, c. k. i., see bishop, c. m. 196
williams, c. k. i., see cornford, d. 85
williams, c. k. i., see ferrari trecate, g. 152
williams, c. k. i., see goldberg, p. w. 191
williams, c. k. i., see seeger, m. 178, 180
williams, c. k. i., see shawe-taylor, j. 99
williams, c. k. i., see sollich, p. 154
williams, c. k. i., see vivarelli, f. 89
williams, c. k. i., see zhu, h. 97
winkler, g. 219
winther, o., see csat  o, l. 180, 185
winther, o., see opper, m. 41, 44, 52, 127, 128
wong, e. 218
wood, s. 45
wo  zniakowski, h., see ritter, k. 97
wynn, h. p., see sacks, j. 16, 30

xiang, d., see lin, x. 185

yaglom, a. m. 89
yandell, b. s., see o   sullivan, f. 132, 138
yang, c. 182
ylvisaker, d. 159
yuille, a. 134
yuille, a., see poggio, t. 154

zhang, t. 157
zhang, t., see meir, r. 164
zhu, h. 97
zhu, j. 185

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

subject index

arma process, see autoregressive moving-average

covariance

alignment, 128
anisotropy, 89
ar process, see autoregressive process
ard, see automatic relevance

determination

process

automatic relevance

determination, 106

autoregressive moving-average process, 217

noise model, 191

autoregressive process, 207

bayes classi   er, 36
bayes    theorem, 200
bayesian committee machine, 180
bcm, see bayesian committee machine
bias

inductive, 165

binary classi   cation, 33
bits, 203
blitzkrieging, see fast gaussian processes
bochner   s theorem, 82
brownian bridge, 213
brownian motion, see wiener process

canonical hyperplane, 142
cholesky decomposition, 202
christ, bowels of, 111
classi   cation, 33

binary, 33
least-squares, 146

probabilistic, 148

multi-class, 33
probabilistic, 33

classi   er

gibbs, 163
predictive, 163

cokriging, 190
consistency, 155
convex

function, 206
set, 206

predictive, 18

covariance function, 13, 79, see also kernel

anova, 95
compact support, 87
dot product, 89
exponential, 85
  -exponential, 86
gaussian, see covariance function, squared ex-

ponential

inhomogeneous polynomial, 89
mat  ern, 84
neural network, 91
ornstein-uhlenbeck, 86
covariance
ou,

see

uhlenbeck

periodic, 92, 119
polynomial, 89
piecewise, 87

function, ornstein-

radial basis function, see covariance function,

squared exponential

rational quadratic, 86
rbf, see covariance function, squared expo-

se, see covariance function, squared exponen-

nential

tial

squared exponential, 14, 83

covariance matrix, 80
cromwell   s dictum, 111
cross-validation, 111
generalized, 112
leave-one-out, 111

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

246

subject index

dataset

robot, inverse dynamics, 23
usps, 63

decision region, 36
decision surface, 36
degenerate, see kernel, degenerate
degrees of freedom, 25
derivative observations, 191
dirichlet process, 192
discriminative approach, 34

eigenfunction, 96
eigenvalue, 96
id178, 203
ep, see expectation propagation
 -insensitive error function, 145
equivalent kernel, 25, 151
error

generalization, 159

error function

 -insensitive, 145
hinge, 145

error-reject curve, 36, 68
errors-in-variables regression, 192
evidence, see marginal likelihood
expectation propagation, 52
experimental design

optimal, 159

factor analysis, 89, 107
feature space, 11
fisher information matrix, 102
fisher kernel, 101
fourier transform, 206
fractal, 137

gamma distribution, 87, 194
gaussian distribution, 200
gaussian markov process, 207
gaussian process, 13
gaussian process classi   cation, 34
gaussian process latent variable model, 196
gaussian process regression, 16
generalization error, 108, 159
generative approach, 34
generative topographic mapping, 196
geostatistics, 30
gmp, see gaussian markov process

gp, see gaussian process
gpc, see gaussian process classi   cation
gplvm, see gaussian process latent variable model
gpr, see gaussian process regression
gram matrix, 80
green   s function, 134
gtm, see generative topographic mapping

hidden markov model, 102
hinge error function, 145
hyperparameters, 20, 106
hyperplane

canonical, 142

index set, 13
informative vector machine, 178
integrals, evaluation of, 193
intrinsic random function, 137
invariances, 195
irls, see iteratively reweighted least squares
isotropy, 80
iteratively reweighted least squares, 38
ivm, see informative vector machine

jitter, 47

kernel, 80, see also covariance function

bag-of-characters, 100
degenerate, 94, 97
equivalent, 151
fisher, 101
k-spectrum, 100
nondegenerate, 97
positive de   nite, 80
string, 100
tangent of posterior odds, 102

kernel classi   er, 167
kernel pca, 99
kernel ridge regression, see regression, ridge, kernel
kernel smoother, 25, 167
kernel trick, 12
kriging, 30
id181, 54, 203

laplace approximation, 41
latent variable model, 196
learning curve, 159
learning, supervised, 1

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

subject index

247

least-squares classi   cation, 146
leave-one-out, 148
leave-one-out cross-validation, 111
length-scale, characteristic, 14, 83, 106
likelihood, 8, 37

logistic, 35, 43
multiple-logistic, 38
non-gaussian, 33, 191
probit, 35, 43
linear classi   er, 37
id75, 8
link function, 35
log odds ratio, 37
logistic function, 35
id28, 35
loo, see leave-one-out
loss

negative log id203, 23
squared, 22
zero-one, 36
id168, 21
loss matrix, 36
lsc, see least-squares classi   cation

map, see maximum a posteriori
margin

functional, 142
geometrical, 142

marginal likelihood, 18, 112
marginalization property, 13
id115, 41
markov random    eld, 218
matrix

covariance, 80
fisher information, 102
gram, 80
inversion lemma, 201
loss, 36
partitioned inversion of, 201
positive de   nite, 80
positive semide   nite, 80

maximum a posteriori, see penalized maximum like-

lihood

maximum likelihood

penalized, 10

mcmc, see id115
mean function, 13, 27

mean square continuity, 81
mean square di   erentiability, 81
mean standardized log loss, 23
mean-   eld approximation, 52
measure, 204
mercer   s theorem, 96
mixture of experts, 192
ml-ii, see type ii maximum likelihood
model

non-parametric, 166
parametric, 166
semi-parametric, 166

moore-aronszajn theorem, 130
ms continuity, see mean square continuity
ms di   erentiability, see mean square di   erentiabil-

ity

msll, see mean standardized log loss
multi-class classi   cation, 33
id72, 115
multiple outputs, 190

nadaraya-watson estimator, 25, 155
nats, 203
neural network, 90, 166
newton   s method, 43, 49
noise model, 8, 16
correlated, 190
heteroscedastic, 191

norm

frobenius, 202

null space, 137
nystr  om approximation, 172
nystr  om method, 177

occam   s razor, 110
one-versus-rest, 147
operator, integral, 80
optimal experimental design, 159
outputs, multiple, 190

p1nn, see probabilistic one nearest neighbour
pac-bayesian theorem

mcallester, 163
seeger, 164

penalized maximum likelihood estimate, 10
plsc, see probabilistic least-squares classi   cation
positive de   nite matrix, 80
positive semide   nite matrix, 80

c. e. rasmussen & c. k. i. williams, gaussian processes for machine learning, the mit press, 2006,
c(cid:13) 2006 massachusetts institute of technology. www.gaussianprocess.org/gpml
isbn 026218253x.

248

subject index

stationarity, 79
stochastic di   erential equation, 207
student   s t process, 194
subset of datapoints, 177
subset of regressors, 175
supervised learning, 1
support vector, 143
support vector machine, 141

soft margin, 143

support vector regression, 145
id166, see support vector machine
svr, see support vector regression

tangent of posterior odds kernel, 102
top kernel, see tangent of posterior odds kernel
transduction, 181
type ii maximum likelihood, 109

uncertain inputs, 192
upcrossing rate, 80
usps dataset, 63

weight function, 25
weight vector, 8
wiener process, 213
integrated, 139
tied-down, 213

wiener-khintchine theorem, 82, 209

yule-walker equations, 215

posterior process, 18
pp, see projected process approximation
prediction

classi   cation

averaged, 44
map, 45

probabilistic classi   cation, 33
probabilistic least-squares classi   cation, 147
probabilistic one nearest neighbour, 69
id203

conditional, 199
joint, 199
marginal, 199

probit regression, 35
projected process approximation, 178
pseudo-likelihood, 117

quadratic form, 80
quadratic programming, 142

regression

errors-in-variables, 192
gaussian process, 16
linear, 8
polynomial, 11, 28
ridge, 11, 132

kernel, 30, 132

id173, 132
id173 network, 135
reject option, 36
relative id178, see kullback leibler divergence
relevance vector machine, 149
representer theorem, 132
reproducing kernel hilbert space, 129
response function, 35
ridge regression, see regression, ridge
risk, 22, 36
rkhs, see reproducing kernel hilbert space
rvm, see relevance vector machine

scale mixture, 87, 194
sd, see subset of datapoints
sde, see stochastic di   erential equation
smse, see standardized mean squared error
softmax, 38
splines, 136
sr, see subset of regressors
standardized mean squared error, 23

