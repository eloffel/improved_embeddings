just machine learning

tina eliassi-rad

tina@eliassi.org

@tinaeliassi

http://eliassi.org/safra17.pdf

what	is	
machine	
learning?

https://xkcd.com/1838/

machine	
learning	
emerged	
from	ai

economics 

and 

organizational 

behavior 

computer science 

machine learning 

evolution 

statistics  

animal learning 
(cognitive science, 

psychology, 

neuroscience) 

adaptive control 

theory 

cs, statistics, machine learning

    computer science: how can we build machines 
that solve problems, and which problems are inherently 
tractable/intractable?
    statistics: what can be inferred from data plus a set 
of modeling assumptions, with what reliability?
    machine learning: how can we build computer systems 
that automatically improve with experience, and what are 
the fundamental laws that govern all learning processes?

place of ml within computer science

    the application is too complex for people to manually 
design algorithms     e.g., id161
    the application requires that the software customize 
to its operational environment after it is fielded     e.g., 
id103

(tom mitchell, the discipline of machine learning, july 2006)

    arthur samuel coined 
the term machine 
learning (1959)
    field of study that gives 
computers the ability to 
learn without being 
explicitly programmed 
    the samuel checkers-
playing program

what does    learning    mean?
the well-posed learning problem:
a computer program is said to learn from experience e
w.r.t. some task t and some performance measure p, if
its performance on t, as measured by p, improves with
experience e. -- tom mitchell (1997)

example

    suppose your email program observes which emails you mark 
as spam and which you do not, and based on that information 
learns how to better filter spam.

example

    suppose your email program observes which emails you mark 
as spam and which you do not, and based on that information 
learns how to better filter spam.
    task t: classifying emails as spam or not spam 

example

    suppose your email program observes which emails you mark 
as spam and which you do not, and based on that information 
learns how to better filter spam.
    task t: classifying emails as spam or not spam 
    experience e: observing you label emails as spam or not spam

example

    suppose your email program observes which emails you mark 
as spam and which you do not, and based on that information 
learns how to better filter spam.
    task t: classifying emails as spam or not spam 
    experience e: observing you label emails as spam or not spam
    performance p: the number (or fraction) of emails correctly 
classified as spam/not spam 

some    success    stories
    ibm watson defeats the best human competitors in jeopardy!
    google alphago model defeats euro go campaign
    id103: amazon alexa, apple siri, google go,    
    image recognition
    translation
    fraud detection
    self-driving cars
    id126s: amazon, netflix,    

ethical issues in ai 

top 9 (h/t julia bossman) 

unemployment
wealth inequality

humanity

artificial	stupidity

evil genies
singularity
http://bit.ly/2etvh3x

security

robot	rights

racist/sexist	robots

ethical issues in ai 

top 9 (h/t julia bossman) 

unemployment
wealth inequality

humanity

artificial	stupidity

evil genies
singularity
http://bit.ly/2etvh3x

security

robot	rights

racist/sexist	robots

unemployment
    the end of work?
    the threat of automation 

    manufacturing
    trucking
    loan underwriting

    ai and the future of work: http://futureofwork.mit.edu

wealth inequality
    increased wealth inequality is a likely consequence of the end of work 
    the owners of ai tech/ip will be advantaged
       automated inequality    by nicolas yan (harvard political review, 2016) 

    http://harvardpolitics.com/world/automation/

       the outcome   shared prosperity or increasing inequality   will be 
determined not by technologies but by the choices we make as 
individuals, organizations, and societies. if we fumble that future   if we 
build economies and societies that exclude many people from the cycle of 
prosperity   shame on us.    -- erik brynjolfsson (hbr 2015)

humanity
    altering our behaviors and interactions?
    vying for your attention (for ad $$$)

    click-baiting, fake news, polarization, tech addiction

       you'll be outraged at how easy it was to get you to click on this 
headline    by bryan gardiner (wired, 2015)
    https://www.wired.com/2015/12/psychology-of-clickbait/

    mit conference on digital experimentation: http://bit.ly/2i56mgi

artificial stupidity
    adversarial machine learning

    wikipedia entry: http://bit.ly/2gw908q

    exploitation of stupidity

    poisoning vs. evasion attacks
    black box vs. white box attack

    fooling google's cloud vision api 

evil genies
    unintended consequences due to poorly defined tasks or faulty 
experience/data (garbage in, garbage out)
       customer experience, opaque ai and the risk of unintended 
consequences    by adrian swinscoe (forbes, 2017) 
    http://bit.ly/2zq4np6

singularity
    what if super-intelligence emerges?
    wikipedia entry: http://bit.ly/1tmfhxl

security
    weaponization of ai in both physical and cyber space

    drones, robot soldiers

    wikipedia entry: http://bit.ly/2z7b4at 

robot rights
    can robots have moral status?
    when is a robot a moral agent?    by john p. sullins
(international review of information ethics, 2006)
    http://bit.ly/2z5kauf

    georgia tech robot ethics

    https://ethics.gatech.edu/robot-ethics

ethical issues in ai 

top 9 (h/t julia bossman) 

unemployment

artificial	stupidity

inequality
humanity

evil genies
singularity
http://bit.ly/2etvh3x

security

robot	rights

racist/sexist	robots

racist/sexist robots
       bias in computer systems    by batya friedman and helen nissenbaum
(acm transactions on information systems, 1996)
    https://dl.acm.org/citation.cfm?id=230561

       algorithmic bias in autonomous systems    by david danks and alex 
john london (ijaci 2017)
    http://bit.ly/2zrdbnx

    uc berkeley course on fairness in machine learning

    https://fairmlclass.github.io

    fairness, accountability, and transparency

    fatml conferences: https://www.fatml.org

racist robots in the news

nikon s630

google   s	
speech	

recognition	
has	a	gender	

bias

propublica   s
study	of	
northpointe

software

taytweets:	
microsoft   s	
twitter	bot

science, oct 2017

nips, dec 2016

bias in computer systems
(friedman & nissenbaum, 1996)
    identified three sources of bias

1. preexisting bias 
2. technical bias 
3. emergent bias 

       we conclude by suggesting that freedom from bias should be 
counted among the select set of criteria   including reliability, 
accuracy, and efficiency   according to which the quality of 
systems in use in society should be judged.   

how do computer scientists define fairness?

    probabilistically
    lots of parity (i.e.,    fairness   ) definitions

    decisions should be in some sense probabilistically independent of 
sensitive features values (such as gender, race)
    there are many possible senses

confusion matrix

predicted:	no predicted: yes

actual: no
actual:	yes

tn
fn

fp
tp

    accuracy: how often is the classifier 

    specificity (1     fpr) : when it's actually 

correct? (tp+tn)/total 

    misclassification (a.k.a. error) rate: 
how often is it wrong? (fp+fn)/total 

    true positive rate (tpr, a.k.a. 

sensitivity or recall): when it's actually 
yes, how often does it predict yes? 
tp/actual yes 

    false positive rate (fpr) : when it's 
actually no, how often does it predict 
yes? fp/actual no

no, how often does it predict no? 
tn/actual no 

    precision (a.k.a. positive predictive 

value): when it predicts yes, how often is 
it correct? tp/predicted yes 

    negative predictive value: when it 
predicts no, how often is it correct? 
tn/predicted no

    prevalence: how often does the yes 

condition actually occur in our sample? 
actual yes/total

http://bit.ly/2xapsrz	

lots of parity definitions
(probabilistic definitions of different kinds of fairness)
    demographic parity
    accuracy parity
    true positive parity
    false positive parity
    positive rate parity
    precision parity
    positive predictive value parity
    negative predictive value parity
    predictive value parity
       

for definitions.

see https://fairmlclass.github.io

impossibility results l
    kleinberg, mullainathan, raghavan (2016)
    chouldechova (2016) 
    you can   t have your cake and eat it too

some definitions
    x contains features of an individual (e.g., medical records)

    x incorporates all sorts of measurement biases

    a is a sensitive attribute (e.g., race, gender, ...)

    a is often unknown, ill-defined, misreported, or inferred

    y is the true outcome (a.k.a. the ground truth; e.g., whether patient has 
cancer)
    c is the machine learning algorithm that uses x and a to predict the value 
of y (e.g., predict whether the patient has cancer)

https://fairmlclass.github.io

some simplifying assumptions
    the sensitive attribute a divides the population into two groups 
a (e.g., whites) and b (e.g., non-whites)
    the machine learning algorithm c outputs 0 (e.g., predicts not 
cancer) or 1 (e.g., predicts cancer)
    the true outcome y is 0 (e.g., not cancer) or 1 (e.g., cancer)

impossibility results
    kleinberg, mullainathan, raghavan (2016), chouldechova (2016) 
    assume differing base rates     i.e., pra (y=1)     prb (y=1)     and an 
imperfect machine learning algorithm (c     y), then you can not 
simultaneously achieve
a) precision parity: pra (y=1   c=1) = prb (y=1   c=1).
b) true positive parity: pra(c=1   y=1) = prb (c=1   y=1)
c) false positive parity: pra(c=1   y=0) = prb (c=1   y=0) 

impossibility results
    kleinberg, mullainathan, raghavan (2016), chouldechova (2016) 
    assume differing base rates     i.e., pra (y=1)     prb (y=1)     and an 
imperfect machine learning algorithm (c     y), then you can not 
simultaneously achieve
a) precision parity: pra (y=1   c=1) = prb (y=1   c=1)
b) true positive parity: pra(c=1   y=1) = prb (c=1   y=1)
c) false positive parity: pra(c=1   y=0) = prb (c=1   y=0) 

   equalized odds    -- hardt, price, srebro (2016)

impossibility results

   suppose we want to determine the risk that a person is a carrier for a disease y, 
and suppose that a higher fraction of women than men are carriers. then our 
results imply that in any test designed to estimate the id203 that someone is 
a carrier of y, at least one of the following undesirable properties must hold: (a) the 
test   s id203 estimates are systematically skewed upward or downward for at 
least one gender; or (b) the test assigns a higher average risk estimate to healthy 
people (non-carriers) in one gender than the other; or (c) the test assigns a higher 
average risk estimate to carriers of the disease in one gender than the other. the 
point is that this trade-off among (a), (b), and (c) is not a fact about medicine; it is 
simply a fact about risk estimates when the base rates differ between two groups.    
-- kleinberg, mullainathan, raghavan (2016)

impossibility results

   suppose we want to determine the risk that a person is a carrier for a disease y, 
and suppose that a higher fraction of women than men are carriers. then our 
results imply that in any test designed to estimate the id203 that someone is 
a carrier of y, at least one of the following undesirable properties must hold: (a) the 
test   s id203 estimates are systematically skewed upward or downward for at 
least one gender; or (b) the test assigns a higher average risk estimate to healthy 
people (non-carriers) in one gender than the other; or (c) the test assigns a higher 
average risk estimate to carriers of the disease in one gender than the other. the 
point is that this trade-off among (a), (b), and (c) is not a fact about medicine; it is 
simply a fact about risk estimates when the base rates differ between two groups.    
-- kleinberg, mullainathan, raghavan (2016)

impossibility results

   suppose we want to determine the risk that a person is a carrier for a disease y, 
and suppose that a higher fraction of women than men are carriers. then our 
results imply that in any test designed to estimate the id203 that someone is 
a carrier of y, at least one of the following undesirable properties must hold: (a) the 
test   s id203 estimates are systematically skewed upward or downward for at 
least one gender; or (b) the test assigns a higher average risk estimate to healthy 
people (non-carriers) in one gender than the other; or (c) the test assigns a higher 
average risk estimate to carriers of the disease in one gender than the other. the 
point is that this trade-off among (a), (b), and (c) is not a fact about medicine; it is 
simply a fact about risk estimates when the base rates differ between two groups.    
-- kleinberg, mullainathan, raghavan (2016)

impossibility results

   suppose we want to determine the risk that a person is a carrier for a disease y, 
and suppose that a higher fraction of women than men are carriers. then our 
results imply that in any test designed to estimate the id203 that someone is 
a carrier of y, at least one of the following undesirable properties must hold: (a) the 
test   s id203 estimates are systematically skewed upward or downward for at 
least one gender; or (b) the test assigns a higher average risk estimate to healthy 
people (non-carriers) in one gender than the other; or (c) the test assigns a higher 
average risk estimate to carriers of the disease in one gender than the other. the 
point is that this trade-off among (a), (b), and (c) is not a fact about medicine; it is 
simply a fact about risk estimates when the base rates differ between two groups.    
-- kleinberg, mullainathan, raghavan (2016)

impossibility results

   suppose we want to determine the risk that a person is a carrier for a disease y, 
and suppose that a higher fraction of women than men are carriers. then our 
results imply that in any test designed to estimate the id203 that someone is 
a carrier of y, at least one of the following undesirable properties must hold: (a) the 
test   s id203 estimates are systematically skewed upward or downward for at 
least one gender; or (b) the test assigns a higher average risk estimate to healthy 
people (non-carriers) in one gender than the other; or (c) the test assigns a higher 
average risk estimate to carriers of the disease in one gender than the other. the 
point is that this trade-off among (a), (b), and (c) is not a fact about medicine; it is 
simply a fact about risk estimates when the base rates differ between two groups.    
-- kleinberg, mullainathan, raghavan (2016)

impossibility results

   suppose we want to determine the risk that a person is a carrier for a disease y, 
and suppose that a higher fraction of women than men are carriers. then our 
results imply that in any test designed to estimate the id203 that someone is 
a carrier of y, at least one of the following undesirable properties must hold: (a) the 
test   s id203 estimates are systematically skewed upward or downward for at 
least one gender; or (b) the test assigns a higher average risk estimate to healthy 
people (non-carriers) in one gender than the other; or (c) the test assigns a higher 
average risk estimate to carriers of the disease in one gender than the other. the 
point is that this trade-off among (a), (b), and (c) is not a fact about medicine; it is 
simply a fact about risk estimates when the base rates differ between two groups.    
-- kleinberg, mullainathan, raghavan (2016)

impossibility results

   suppose we want to determine the risk that a person is a carrier for a disease y, 
and suppose that a higher fraction of women than men are carriers. then our 
results imply that in any test designed to estimate the id203 that someone is 
a carrier of y, at least one of the following undesirable properties must hold: (a) the 
test   s id203 estimates are systematically skewed upward or downward for at 
least one gender; or (b) the test assigns a higher average risk estimate to healthy 
people (non-carriers) in one gender than the other; or (c) the test assigns a higher 
average risk estimate to carriers of the disease in one gender than the other. the 
point is that this trade-off among (a), (b), and (c) is not a fact about medicine; it is 
simply a fact about risk estimates when the base rates differ between two groups.    
-- kleinberg, mullainathan, raghavan (2016)

propublica and northpointe
    propublica's main charge was that black defendants experienced 
higher false positive rate
    northpointe's main defense was that their risk assessment 
scores satisfy precision parity: pra (y=1   c=1) = prb (y=1   c=1)
    due to the impossibility results, northpointe   s algorithm cannot 
satisfy    equalized odds    
    disproportionately high false positive rate for blacks
    disproportionately high false negative rate for whites

https://fairmlclass.github.io

group vs. individual fairness 
    fairness through awareness by dwork, hardt, pitassi, 
reingold, zemel (2012)
       people who are similar w.r.t. a specific (classification) task 
should be treated similarity.   
    does not get around the impossibility results
    assuming you have equal base rates, treating everyone equally 
is a good move 

solutions considered from the machine 
learning side
    preprocessing or    massaging    the data to make it less biased 
    learning fair representations: encode data while obfuscating sensitive 
attributes
    penalize the algorithm to encourage it to learn fairly

    during training (e.g., through id173 or constraints) or as a post-
processing step

    allow the sensitive attributes to be used during training, but do not make 
them available to the model during id136 time
    causal id136

    caution: powerful but brittle

solutions considered from the policy side
    regulations     
    the eu has general data protections regulation (gdpr) data 
laws going into effect in 2018
    these laws grant users the right to a logical explanation of how 
an algorithm uses our personal data
    wikipedia entry: http://bit.ly/1lmrnjz

just machine learning in an unjust world?
    racist/sexist humans     e.g., biased judges
    stupid algorithms are already in use     e.g., three-strikes 
laws, mandatory minimum sentencing
    they don   t take enough empirical data into account 
    machine learning can help here

    personalization, context-awareness,    

where do we go from here?
   computers may be intelligent, but they are not wise.
everything they know, we taught them, and we taught
them our biases. they are not going to un-learn them
without transparency and corrective action by humans.   
-- ellora thadaney israni

https://www.nytimes.com/2017/10/26/opinion/algorithm-compas-sentencing-bias.html

an interdisciplinary call for action
    you can   t have all the different kinds of fairness that you might 
want
    recall the impossibility results

    we need to work together across disciplines to reach 
agreement in terms of which kinds of    fairness    we want to 
optimize
    fairness based on explanation?
    fairness based on placement?

thank you
slides at http://eliassi.org/safra17.pdf

