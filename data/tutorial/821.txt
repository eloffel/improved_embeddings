a survey of modern authorship attribution methods 

 

efstathios stamatatos 

dept. of information and communication systems eng. 

karlovassi, samos     83200, greece 

university of the aegean 
stamatatos@aegean.gr 

 

 

abstract 

authorship attribution supported by statistical or computational methods has a long history 
starting from 19th century and marked by the seminal study of mosteller and wallace (1964) 
on the authorship of the disputed federalist papers. during the last decade, this scientific field 
has  been  developed  substantially  taking  advantage  of  research  advances  in  areas  such  as 
machine  learning,  information  retrieval,  and  natural  language  processing.  the  plethora  of 
available electronic texts (e.g., e-mail messages, online forum messages, blogs, source code, 
etc.) indicates a wide variety of applications of this technology provided it is able to handle 
short  and  noisy  text  from  multiple  candidate  authors.  in  this  paper,  a  survey  of  recent 
advances of the automated approaches to attributing authorship is presented examining their 
characteristics for both text representation and text classification. the focus of this survey is 
on computational requirements and settings rather than linguistic or literary issues. we also 
discuss evaluation methodologies and criteria for authorship attribution studies and list open 
questions that will attract future work in this area. 

1. introduction 
the main idea behind statistically or computationally-supported authorship attribution is that 
by  measuring  some  textual  features  we  can  distinguish  between  texts  written  by  different 
authors.  the  first  attempts  to  quantify  the  writing  style  go  back  to  19th  century,  with  the 
pioneering study of mendenhall (1887) on the plays of shakespeare followed  by statistical 
studies in the first half of the 20th century by yule (1938; 1944) and zipf (1932). later, the 
detailed study by mosteller and wallace (1964) on the authorship of    the federalist papers    
(a  series  of  146  political  essays  written  by  john  jay,  alexander  hamilton,  and  james 
madison, twelve of which claimed by both hamilton and madison) was undoubtedly the most 
influential  work  in  authorship  attribution.  their  method  was  based  on  bayesian  statistical 
analysis  of  the  frequencies  of  a  small  set  of  common  words  (e.g.,     and   ,     to   ,  etc.)  and 
produced significant discrimination results between the candidate authors.  

essentially,  the  work  of  mosteller  and  wallace  (1964)  initiated  non-traditional 
authorship attribution studies, as opposed to traditional human expert-based methods. since 
then and until the late 1990s, research in authorship attribution was dominated by attempts to 
define  features  for  quantifying  writing  style,  a  line  of  research  known  as     stylometry    
(holmes, 1994; holmes, 1998). hence, a great variety of measures including sentence length, 
word length, word frequencies, character frequencies, and vocabulary richness functions had 
been  proposed.  rudman  (1998)  estimated  that  nearly  1,000  different  measures  had  been 
proposed that far. the authorship attribution methodologies proposed during that period were 
computer-assisted rather than computer-based, meaning that the aim was rarely at developing 
a  fully-automated  system.  in  certain  cases,  there  were  methods  achieved  impressive 
preliminary  results  and  made  many  people  think  that  the  solution  of  this  problem  was  too 
close.  the  most  characteristic  example  is  the  cusum  (or  qsum)  technique  (morton  & 
michealson,  1990)  that  gained  publicity  and  was  accepted  in  courts  as  expert  evidence. 
however, the research community heavily criticized it and considered it generally unreliable 
(holmes & tweedie, 1995). actually, the main problem of that early period was the lack of 

 

1 

objective evaluation of the proposed methods. in most of the cases, the testing ground was 
literary works of unknown or disputed authorship (e.g., the federalist case), so the estimation 
of  attribution  accuracy  was  not  even  possible.  the  main  methodological  limitations  of  that 
period concerning the evaluation procedure were the following: 

     the  textual  data  were  too  long  (usually  including  entire  books)  and  probably  not 

stylistically homogeneous. 

     the number of candidate authors was too small (usually 2 or 3). 
     the evaluation corpora were not controlled for topic. 
     the  evaluation  of  the  proposed  methods  was  mainly  intuitive  (usually  based  on 

subjective visual inspection of scatterplots). 

     the  comparison  of  different  methods  was  difficult  due  to  lack  of  suitable 

benchmark data. 

since  the  late  1990s,  things  have  changed  in  authorship  attribution  studies.  the  vast 
amount of electronic texts available through internet media (emails, blogs, online forums, etc) 
increased the need for handling this information efficiently. this fact had a significant impact 
in  scientific  areas  such  as  information  retrieval,  machine  learning,  and  natural  language 
processing  (nlp).  the  development  of  these  areas  influenced  authorship  attribution 
technology as described below: 

     information  retrieval  research  developed  efficient  techniques  for  representing  and 

classifying large volumes of text. 

     powerful  machine 

learning  algorithms  became  available 

to  handle  multi-
dimensional  and  sparse  data  allowing  more  expressive  representations.  moreover, 
standard  evaluation  methodologies  have  been  established  to  compare  different 
approaches on the same benchmark data. 

     nlp  research  developed  tools  able  to  analyze  text  efficiently  and  providing  new 

forms of measures for representing the style (e.g., syntax-based features). 

more  importantly,  the  plethora  of  available  electronic  texts  revealed  the  potential  of 
authorship analysis in various applications (madigan, lewis, argamon, fradkin, & ye, 2005) 
in  diverse  areas  including  intelligence  (e.g.,  attribution  of  messages  or  proclamations  to 
known terrorists, linking different messages by authorship) (abbasi & chen, 2005), criminal 
law  (e.g.,  identifying  writers  of  harassing  messages,  verifying  the  authenticity  of  suicide 
notes) and civil law (e.g., copyright disputes) (chaski, 2005; grant, 2007), computer forensics 
(e.g., identifying the authors of source code of malicious software) (frantzeskou, stamatatos, 
gritzalis, & katsikas, 2006), in addition to the traditional application to literary research (e.g., 
attributing anonymous or disputed literary works to known authors) (burrows, 2002; hoover, 
2004a). hence, (roughly) the last decade can be viewed as a new era of authorship analysis 
technology, this time dominated by efforts to develop practical applications dealing with real-
world texts (e.g., e-mails, blogs, online forum messages, source code, etc.) rather than solving 
disputed literary questions. emphasis is now given to the objective evaluation of the proposed 
methods  as  well  as  the  comparison  of  different  methods  based  on  common  benchmark 
corpora  (juola,  2004).  in  addition,  factors  playing  a  crucial  role  in  the  accuracy  of  the 
produced  models  are  examined,  such  as  the  training  text  size  (marton,  wu,  &  hellerstein, 
2005; hirst & feiguina, 2007), the number of candidate authors (koppel, schler, argamon, & 
messeri, 2006), and the distribution of training texts over the candidate authors (stamatatos, 
2008).  

in the typical authorship attribution problem, a text of unknown authorship is assigned to 
one candidate author, given a set of candidate authors for whom text samples of undisputed 
authorship  are  available.  from  a  machine  learning  point-of-view,  this  can  be  viewed  as  a 
multi-class  single-label  text  categorization  task  (sebastiani,  2002).  this  task  is  also  called 
authorship  (or  author)  identification  usually by  researchers  with  a  background  in  computer 
science. several studies focus exclusively on authorship attribution (stamatatos, fakotakis, & 
kokkinakis,  2001;  keselj,  peng,  cercone,  &  thomas,  2003;  zheng,  li,  chen,  &  huang, 

 

2 

2006) while others use it as just another testing ground for text categorization methodologies 
(khmelev & teahan, 2003a; peng, shuurmans, & wang, 2004; marton, et al., 2005; zhang & 
lee,  2006).  beyond  this  problem,  several  other  authorship  analysis  tasks  can  be  defined, 
including the following:  

     author  verification  (i.e.,  to  decide  whether  a  given  text  was  written  by  a  certain 

author or not) (koppel & schler, 2004).  

     plagiarism detection (i.e., finding similarities between two texts) (meyer zu eissen, 

stein, & kulig, 2007; stein & meyer zu eissen, 2007).  

     author  profiling  or  characterization  (i.e.,  extracting  information  about  the  age, 
education, sex, etc. of the author of a given text) (koppel, argamon, & shimoni, 
2002).  

     detection  of  stylistic  inconsistencies  (as  may  happen  in  collaborative  writing) 
(collins,  kaufer,  vlachos,  butler,  &  ishizaki,  2004;  graham,  hirst,  &  marthi, 
2005). 

this paper presents a survey of the research advances in this area during roughly the last 
decade  (earlier  work  is  excellently  reviewed  by  holmes  (1994;  1998))  emphasizing 
computational  requirements  and  settings  rather  than  linguistic  or  literary  issues.  first,  in 
section  2,  a  comprehensive  review  of  the  approaches  to  quantify  the  writing  style  is 
presented. then, in section 3, we focus on the authorship identification problem (as described 
above). we propose the distinction of attribution methodologies according to how they handle 
the training texts, individually or cumulatively (per author), and examine their strengths and 
weaknesses  across  several  factors.  in  section  4,  we  discuss  the  evaluation  criteria  of 
authorship attribution methods while in section 5 the conclusions drawn by this survey are 
summarized and future work directions in open research issues are indicated. 

2. stylometric features 
previous  studies  on  authorship  attribution  proposed  taxonomies  of  features  to  quantify  the 
writing style, the so called style markers, under different labels and criteria (holmes, 1994; 
stamatatos, fakotakis, & kokkinakis, 2000; zheng, et al., 2006). the current review of text 
representation  features  for  stylistic  purposes  is  mainly  focused  on  the  computational 
requirements  for  measuring  them.  first,  lexical  and  character  features  consider  a  text  as  a 
mere sequence of word-tokens or characters, respectively. note that although lexical features 
are more complex than character features, we start with them for the sake of tradition. then, 
syntactic and semantic features require deeper linguistic analysis, while application-specific 
features can only be defined in certain text domains or languages. the basic feature categories 
and the required tools and resources for their measurement are shown in table 1. moreover, 
various feature selection and extraction methods to form the most appropriate feature set for a 
particular corpus are discussed.  

2.1 lexical features 
a simple and natural way to view a text is as a sequence of tokens grouped into sentences, 
each token corresponding to a word, number, or a punctuation mark. the very first attempts 
to  attribute  authorship  were  based  on  simple  measures  such  as  sentence  length  counts  and 
word length counts (mendenhall, 1887). a significant advantage of such features is that they 
can be applied to any language and any corpus with no additional requirements except the 
availability  of  a  tokenizer  (i.e.,  a  tool  to  segment  text  into  tokens).  however,  for  certain 
natural  languages  (e.g.,  chinese)  this  is  not  a  trivial  task.  in  case  of  using  sentential 
information, a tool that detects sentence boundaries should also be available. in certain text 
domains with heavy use of abbreviations or acronyms (e.g., e-mail messages) this procedure 
may introduce considerable noise in the measures. 

the  vocabulary  richness  functions  are  attempts  to  quantify  the  diversity  of  the 
vocabulary of a text. typical examples are the type-token ratio v/n, where v is the size of the 

 

3 

table 1. types of stylometric features together with computational tools and resources 
required for their measurement (brackets indicate optional tools). 

features 

required tools and resources 

lexical 

character 

syntactic 

semantic 

tokenizer, [sentence splitter] 

tokenizer, [stemmer, lemmatizer] 
tokenizer 
tokenizer, orthographic spell checker 

character dictionary 

token-based (word 
length, sentence 
length, etc.) 
vocabulary richness  tokenizer 
word frequencies 
word id165s 
errors 
character types 
(letters, digits, etc.) 
character id165s 
(fixed-length) 
character id165s 
(variable-length) 
compression 
methods 
part-of-speech 

- 

chunks 

sentence and phrase 
structure 
rewrite rules 
frequencies 

errors 

synonyms 

semantic 
dependencies 

functional 

structural 

feature selector 

text compression tool 

tokenizer, sentence splitter, pos tagger 
tokenizer, sentence splitter, [pos tagger], 
text chunker 
tokenizer, sentence splitter, pos tagger, 
text chunker, partial parser 
tokenizer, sentence splitter, pos tagger, 
text chunker, full parser 
tokenizer, sentence splitter, syntactic 
spell checker 
tokenizer, [pos tagger], thesaurus 
tokenizer, sentence splitter, pos tagger, 
text chunker, partial parser, semantic 
parser 
tokenizer, sentence splitter, pos tagger, 
specialized dictionaries 
html parser, specialized parsers 
tokenizer, [stemmer, lemmatizer], 
specialized dictionaries 
tokenizer, [stemmer, lemmatizer], 
specialized dictionaries 

application-specific 

content-specific 

language-specific 

 
 

 

4 

vocabulary (unique tokens) and n is the total number of tokens of the text, and the number of 
hapax legomena (i.e., words occurring once) (de vel, anderson, corney, & mohay, 2001). 
unfortunately,  the  vocabulary  size  heavily  depends  on  text-length  (as  the  text-length 
increases,  the  vocabulary also  increases,  quickly  at  the  beginning  and  then  more  and  more 
slowly). various functions have been proposed to achieve stability over text-length, including 
k (yule, 1944), and r (honore, 1979), with questionable results (tweedie & baayen, 1998). 
hence, such measures are considered unreliable to be used alone.  
the most straightforward approach to represent texts is by vectors of word frequencies. the 
vast majority of authorship attribution studies are (at least partially) based on lexical features 
to represent the style. this is also the traditional bag-of-words text representation followed by 
researchers in topic-based text classification (sebastiani, 2002). that is, the text is considered 
as  a  set  of  words  each  one  having  a  frequency  of  occurrence  disregarding  contextual 
information. however, there is a significant difference in style-based text classification: the 
most common words (articles, prepositions, pronouns, etc.) are found to be among the best 
features to discriminate between authors (burrows, 1987; argamon & levitan, 2005). note 
that such words are usually excluded from the feature set of the topic-based text classification 
methods  since  they  do  not  carry  any  semantic  information  and  they  are  usually  called 
   function     words.  as  a  consequence,  style-based  text  classification  using  lexical  features 
require much lower dimensionality in comparison to topic-based text classification. in other 
words,  much  less  words  are  sufficient  to  perform  authorship  attribution  (a  few  hundred 
words) in comparison to a thematic text categorization task (several thousand words). more 
importantly, function words are used in a largely unconscious manner by the authors and they 
are  topic-independent.  thus,  they  are  able  to  capture  pure  stylistic  choices  of  the  authors 
across different topics. 

the selection of the specific function words that will be used as features is usually based 
on  arbitrary  criteria  and  requires  language-dependent  expertise.  various  sets  of  function 
words have been used for english but limited information was provided about the way they 
have been selected: abbasi and chen (2005) reported a set of 150 function words; argamon, 
saric, and stein (2003) used a set of 303 words; zhao and zobel (2005) used a set of 365 
function words; 480 function words were proposed by koppel and schler (2003); another set 
of 675 words was reported by argamon, whitelaw, chase, hota, garg, and levitan (2007). 

a  simple  and  very  successful  method  to  define  a  lexical  feature  set  for  authorship 
attribution is to extract the most frequent words found in the available corpus (comprising all 
the texts of the candidate authors). then, a decision has to be made about the amount of the 
frequent words that will be used as features. in the earlier studies, sets of at most 100 frequent 
words were considered adequate to represent the style of an author (burrows, 1987; burrows, 
1992). another factor that affects the feature set size is the classification algorithm that will 
be  used  since  many  algorithms  overfit  the  training  data  when  the  dimensionality  of  the 
problem increases. however, the availability of powerful machine learning algorithms able to 
deal  with  thousands  of  features,  like  support  vector  machines  (joachims,  1998),  enabled 
researchers  to  increase  the  feature  set  size  of  this  method.  koppel,  schler,  and  bonchek-
dokow  (2007)  used  the  250  most  frequent  words  while  stamatatos  (2006a)  extracted  the 
1,000 most frequent words. on a larger scale, madigan, et al., (2005) used all the words that 
appear  at  least  twice  in  the  corpus.  note  that  the  first  dozens  of  most  frequent  words  of  a 
corpus are usually dominated by closed class words (articles, prepositions etc.) after a few 
hundred words, open class words (nouns, adjectives, verbs) are the majority. hence, when the 
dimensionality of this representation method increases, some content-specific words may also 
be included in the feature set. 

despite the availability of a tokenizer, word-based features may require additional tools 
for their extraction. this would involve from simple routines like conversion to lowercase to 
more complex tools like stemmers (sanderson & guenter, 2006), lemmatizers (tambouratzis, 
markantonatou, hairetakis, vassiliou, carayannis, & tambouratzis, 2004; gamon, 2004), or 
detectors of common homographic forms (burrows, 2002). another procedure used by van 
halteren  (2007)  is  to  transform  words  into  an  abstract  form.  for  example,  the  dutch  word 
   waarmaken    is transformed to    #l#6+/l/ken   , where the first l indicates low frequency, 6+ 

 

5 

indicates the length of the token, the second l a lowercase token, and    ken    are its last three 
characters. 

the bag-of-words approach provides a simple and efficient solution but disregards word-
order (i.e., contextual) information. for example, the phrases    take on   ,    the second take    and 
   take a bath    would just provide three occurrences of the word    take   . to take advantage of 
contextual information, word id165s (n contiguous words aka word collocations) have been 
proposed  as  textual  features  (peng,  et  al.,  2004;  sanderson  &  guenther,  2006;  coyotl-
morales, villase  or-pineda, montes-y-g  mez, & rosso, 2006). however, the classification 
accuracy  achieved  by  word  id165s  is  not  always  better  than  individual  word  features 
(sanderson  &  guenther,  2006;  coyotl-morales,  et  al.,  2006).  the  dimensionality  of  the 
problem following this approach increases considerably with n to account for all the possible 
combinations between words. moreover, the representation produced by this approach is very 
sparse, since most of the word combinations are not encountered in a given (especially short) 
text making it very difficult to be handled effectively by a classification algorithm. another 
problem with word id165s is that it is quite possible to capture content-specific information 
rather than stylistic information (gamon, 2004). 

from  another point of view, koppel and schler (2003) proposed various writing error 
measures to capture the idiosyncrasies of an author   s style. to that end, they defined a set of 
spelling  errors  (e.g.,  letter  omissions  and  insertions)  and  formatting  errors  (e.g.,  all  caps 
words) and they proposed a methodology to extract such measures automatically using a spell 
checker.  interestingly,  human  experts  mainly  use  similar  observations  in  order  to  attribute 
authorship. however, the availability of accurate spell checkers is still problematic for many 
natural languages. 

2.2 character features 
according to this family of measures, a text is viewed as a mere sequence of characters. that 
way, various character-level measures can be defined, including alphabetic characters count, 
digit  characters  count,  uppercase  and  lowercase  characters  count,  letter  frequencies, 
punctuation  marks  count,  etc.  (de  vel,  et  al.,  2001;  zheng,  et  al.,  2006).  this  type  of 
information is easily available for any natural language and corpus and it has been proven to 
be quite useful to quantify the writing style (grieve, 2007). 

a  more  elaborate,  although  still  computationally  simplistic,  approach  is  to  extract 
frequencies  of  id165s  on  the  character-level.  for  instance,  the  character  4-grams  of  the 
beginning  of  this  paragraph  would  be1:  |a_mo|,  |_mor|,  |more|,  |ore_|,  |re_e|,  etc.  this 
approach is able to capture nuances of style including lexical information (e.g., |_in_|, |text|), 
hints  of  contextual  information  (e.g.,  |in_t|),  use  of  punctuation  and  capitalization,  etc. 
another advantage of this representation is its ability to be tolerant to noise. in cases where 
the  texts  in  question  are  noisy  containing  grammatical  errors  or  making  strange  use  of 
punctuation, as it usually happens in e-mails or online forum messages, the character id165 
representation  is  not  affected  dramatically.  for  example,  the  words     simplistic     and 
   simpilstc    would produce  many common character  trigrams.  on  the other hand, these two 
words would be considered different in a lexically-based representation. note that in style-
based text categorization such errors could be considered personal traits of the author (koppel 
&  schler,  2003).  this  information  is  also  captured  by  character  id165s  (e.g.,  in  the 
uncommon  trigrams  |stc|  and  |tc_|).  finally,  for  oriental  languages  where  the  id121 
procedure  is  quite  hard,  character  id165s  offer  a  suitable  solution  (matsuura  &  kanada, 
2000).  as  can  be  seen  in  table  1,  the  computational  requirements  of  character  id165 
features are minimal. 

note  that,  as  with  words,  the  most  frequent  character  id165s  are  the  most  important 
features  for  stylistic  purposes.  the  procedure  of  extracting  the  most  frequent  id165s  is 
language-independent  and  requires  no  special  tools.  however,  the  dimensionality  of  this 
representation  is  considerably  increased  in  comparison  to  the  word-based  approach 
                                                      
1 the characters    |    and    _    are used to denote id165 boundaries and a single space character, respectively. 

 

6 

(stamatatos,  2006a;  stamatatos,  2006b).  this  happens  because  character  id165s  capture 
redundant  information  (e.g.,  |and_|,  |_and|)  and  many  character  id165s  are  needed  to 
represent a single long word. 

the  application  of  this  approach  to  authorship  attribution  has  been  proven  quite 
successful.  kjell  (1994)  first  used  character  bigrams  and  trigrams  to  discriminate  the 
federalist papers. forsyth and holmes (1996) found that bigrams and character id165s of 
variable-length  performed  better  than  lexical  features  in  several  text  classification  tasks 
including  authorship  attribution.  peng,  shuurmans,  keselj,  &  wang  (2003),  keselj  et  al. 
(2003),  and  stamatatos  (2006b)  reported  very  good  results  using  character  id165 
information.  moreover,  one  of  the  best  performing  algorithms  in  an  authorship  attribution 
competition organized in 2004 was also based on a character id165 representation (juola, 
2004; juola, 2006). likewise, a recent comparison of different lexical and character features 
on the same evaluation corpora (grieve, 2007) showed that character id165s were the most 
effective  measures  (outperformed  in  the  specific  experiments  only  by  a  combination  of 
frequent words and punctuation marks). 

an important issue of the character id165 approach is the definition of n, that is, how 
long should the strings be. a large n would better capture lexical and contextual information 
but it would also better capture thematic information. furthermore, a large n would increase 
substantially  the  dimensionality  of  the  representation  (producing  hundreds  of  thousands  of 
features). on the other hand, a small n (2 or 3) would be able to represent sub-word (syllable-
like) information but it would not be adequate for representing the contextual information. it 
has to be underlined that the selection of the best n value is a language-dependent procedure 
since certain natural languages (e.g., greek, german) tend to have long words in comparison 
to  english.  therefore,  probably  a  larger  n  value  would  be  more  appropriate  for  such 
languages in comparison to the optimal n value for english. the problem of defining a fixed 
value  for  n  can  be  avoided  by  the  extraction  of  id165s  of  variable-length  (forsyth  & 
holmes, 1996; houvardas & stamatatos, 2006). sanderson and guenter (2006) described the 
use of several sequence kernels based on character id165s of variable-length and the best 
results  for  short  english  texts  were  achieved  when  examining  sequences  of  up  to 4-grams. 
moreover,  various  markov  models  of  variable  order  have  been  proposed  for  handling 
character-level information (khmelev & teahan, 2003a; marton, et al., 2005). finally, zhang 
and  lee  (2006)  constructed  a  suffix  tree  representing  all  possible  character  id165s  of 
variable-length and then extracted groups of character id165s as features.  

a  quite  particular  case  of  using  character  information  is  the  compression-based 
approaches (benedetto, caglioti, & loreto, 2002; khmelev & teahan, 2003a; marton, et al., 
2005). the main idea is to use the compression model acquired from one text to compress 
another text, usually based on off-the-shelf compression programs. if the two texts are written 
by the same author, the resulting bit-wise size of the compressed file will be relatively low. 
such methods do not require a concrete representation of text and the classification algorithm 
incorporates the quantification of textual properties. however, the compression models that 
describe the characteristics of the texts are usually based on repetitions of character sequences 
and, as a result, they can capture sub-word and contextual information. in that sense, they can 
be considered as character-based methods.  

2.3 syntactic features 
a more elaborate text representation method is to employ syntactic information. the idea is 
that  authors  tend  to  use  similar  syntactic  patterns  unconsciously.  therefore,  syntactic 
information  is  considered  more  reliable  authorial  fingerprint  in  comparison  to  lexical 
information.  moreover,  the  success  of  function  words  in  representing  style  indicates  the 
usefulness  of  syntactic  information  since  they  are  usually  encountered  in  certain  syntactic 
structures. on the other hand, this type of information requires robust and accurate nlp tools 
able  to  perform  syntactic  analysis  of  texts.  this  fact  means  that  the  syntactic  measure 
extraction is a language-dependent procedure since it relies on the availability of a parser able 

 

7 

to  analyze  a  particular  natural  language  with  relatively  high  accuracy.  moreover,  such 
features will produce noisy datasets due to unavoidable errors made by the parser.  

baayen,  van  halteren,  and  tweedie  (1996)  were  the  first  to  use  syntactic  information 
measures  for  authorship  attribution.  based  on  a  syntactically  annotated  english  corpus, 
comprising a semi-automatically produced full parse tree of each sentence, they were able to 
extract rewrite rule frequencies. each rewrite rule expresses a part of syntactic analysis, for 
instance, the following rewrite rule: 

a:pp (cid:198) p:prep + pc:np 

means  that  an  adverbial  prepositional  phrase  is  constituted  by  a  preposition  followed  by  a 
noun phrase as a prepositional complement. that detailed information describes both what the 
syntactic  class  of  each  word  is  and  how  the  words  are  combined  to  form  phrases  or  other 
structures.  experimental  results  showed  that  this  type  of  measures  performed  better  than 
vocabulary richness and lexical measures. on the other hand, it required a sophisticated and 
accurate  fully-automated  parser  able  to  provide  a  detailed  syntactic  analysis  of  english 
sentences. similarly, gamon (2004) used the output of a syntactic parser to measure rewrite 
rule  frequencies  as  described  above.  although,  the  proposed  syntactic  features  alone 
performed worse than lexical features, the combination of the two improved the results. 

another  attempt  to  exploit  syntactic  information  was  proposed  by  stamatatos,  et  al. 
(2000;  2001).  they  used  a  nlp  tool  able  to  detect  sentence  and  chunk  (i.e.,  phrases) 
boundaries  in  unrestricted  modern  greek  text.  for  example,  the  first  sentence  of  this 
paragraph would be analyzed as following: 

np[another attempt] vp[to exploit] np[syntactic information] vp[was proposed] 
pp[by stamatatos, et al. (2000)]. 

where  np,  vp,  and  pp  stand  for  noun  phrase,  verb  phrase,  and  prepositional  phrase, 
respectively. this type of information is simpler than that used by baayen et al. (1996), since 
there is neither structural analysis within the phrases or the combination of phrases into higher 
structures, but it could be extracted automatically with relatively high accuracy. the extracted 
measures referred to noun phrase counts, verb phrase counts, length of noun phrases, length 
of  verb  phrases,  etc.  more  interesting,  another  type  of  relevant  information  was  also  used 
which stamatatos, et al. (2000; 2001) called analysis-level measures. this type of information 
is  relevant  to  the  particular  architecture  of  that  specific  nlp  tool.  in  more  detail,  that 
particular tool analyzed the text in several steps. the first steps analyzed simple cases while 
the last steps attempted to combine the outcome of the first steps to produce more complex 
results. the analysis-level measures proposed for that tool had to do with the percentage of 
text  each  step  achieved  to  analyze.  essentially  this  is  as  a  type  of  indirect  syntactic 
information and it is tool-specific in addition to language-specific. however, it is a practical 
solution  for  extracting  syntactic  measures  from  unrestricted  text  given  the  availability  of  a 
suitable nlp tool. 

in  a  similar  framework,  tools  that  perform  partial  parsing  can  be  used  to  provide 
syntactic features of varying complexity (luyckx & daelemans, 2005; uzuner & katz, 2005; 
hirst & feiguina, 2007). partial parsing is between text chunking and full parsing and can 
handle unrestricted text with relatively high accuracy. hirst and feiguina (2007) transformed 
the  output  of  a  partial  parser  into  an  ordered  stream  of  syntactic  labels,  for  instance  the 
analysis of the phrase    a simple example    would produce the following stream of labels: 

nx dt jj nn 

in  words,  a  noun  phrase  consisting  of  a  determiner,  an  adjective,  and  a  noun.  then,  they 
extracted measures of bigram frequencies from that stream to represent contextual syntactic 
information and they found this information useful to discriminate the authors of very short 
texts (about 200 words long).  

an  even  simpler  approach  is  to  use  just  a  part-of-speech  (pos)  tagger,  a  tool  that 
assigns  a  tag  of  morpho-syntactic  information  to  each  word-token  based  on  contextual 
information. usually, pos taggers perform quite accurately in unrestricted text and several 

 

8 

researchers have used pos tag frequencies or pos tag id165 frequencies to represent style 
(argamon-engelson,  koppel  &  avneri,  1998;  kukushkina,  polikarpov,  &  khmelev,  2001; 
koppel  &  schler,  2003;  diederich,  kindermann,  leopold,  &  paass,  2003,  gamon,  2004; 
zhao & zobel, 2007). however, pos tag information provides only a hint of the structural 
analysis of sentences since it is not clear how the words are combined to form phrases or how 
the phrases are combined into higher-level structures. 

perhaps the most extensive use of syntactic information was described by van halteren 
(2007). he applied a morpho-syntactic tagger and a syntactic analyzer for dutch to a corpus 
of student essays and extracted unigrams, bigrams, and trigrams of morpho-syntactic tags as 
well as various id165 measures from the application of rewrite rules. as a result, a huge set 
of about 900k features was constructed to quantify syntactic information! 

another  interesting  use  of  syntactic  information  was  proposed  by  koppel  and  schler 
(2003) based on syntactic errors such as sentence fragments, run-on sentences, mismatched 
tense, etc. in order to detect such information they used a commercial spell checker. as with 
orthographic errors, this type of information is similar to that used by human experts when 
they attempt to attribute authorship. unfortunately, the spell checkers are not very accurate 
and koppel and schler (2003) reported they had to modify the output of that tool in order to 
improve the error detection results. 

finally,  karlgren  and  eriksson  (2007)  described  a  preliminary  model  based  on  two 
syntactic features, namely, adverbial expressions and occurrence of clauses within sentences. 
however,  the  quantification  of  these  features  is  not  the  traditional  relative  frequency  of 
occurrence within the text. they used sequence patterns aiming to describe the use of these 
features in consecutive sentences of the text. essentially, this is an attempt to represent the 
distributional  properties  of  the  features  in  the  text,  a  promising  technique  that  can  capture 
important stylistic properties of the author. 

2.4 semantic features 
it  should  be  clear  by  now,  the  more  detailed  the  text  analysis  required  for  extracting 
stylometric features, the less accurate (and the more noisy) the produced measures. nlp tools 
can be applied successfully to low-level tasks, such as sentence splitting, id52, text 
chunking, partial parsing, so relevant features would be measured accurately and the noise in 
the corresponding datasets remains low. on the other hand, more complicated tasks such as 
full  syntactic  parsing,  semantic  analysis,  or  pragmatic  analysis  cannot  yet  be  handled 
adequately by  current  nlp  technology  for  unrestricted  text.  as a  result,  very  few  attempts 
have been made to exploit high-level features for stylometric purposes. 

gamon (2004) used a tool able to produce semantic dependency graphs but he did not 
provide  information  about  the  accuracy  of  this  tool.  two  kinds  of  information  were  then 
extracted:  binary  semantic  features  and  semantic  modification  relations.  the  former 
concerned number and person of nouns, tense and aspect of verbs, etc. the latter described 
the  syntactic  and  semantic  relations  between a  node  of  the  graph  and  its  daughters  (e.g.,  a 
nominal  node  with  a  nominal  modifier  indicating  location).  reported  results  showed  that 
semantic  information  when  combined  with  lexical  and  syntactic  information  improved  the 
classification accuracy. 

mccarthy, lewis, dufty, and mcnamara (2006) described another approach to extract 
semantic  measures.  based  on  id138  (fellbaum,  1998)  they  estimated  information  about 
synonyms  and  hypernyms  of  the  words,  as  well  as  the  identification  of  causal  verbs. 
moreover, they applied latent semantic analysis (deerwester, dumais, furnas, landauer, & 
harshman,  1990)  to  lexical  features  in  order  to  detect  semantic  similarities  between  words 
automatically. however, there was no detailed description of the features and the evaluation 
procedure did not clarify the contribution of semantic information in the classification model. 
perhaps  the  most  important  method  of  exploiting  semantic  information  so  far  was 
described by argamon, et al. (2007). inspired by the theory of systemic functional grammar 
(sfg) (halliday, 1994) they defined a set of functional features that associate certain words 
or phrases with semantic information. in more detail, in sfg the    conjunction    scheme 

 

9 

denotes  how  a  given  clause  expands  on  some  aspect  of  its  preceding  context.  types  of 
expansion  could  be     elaboration     (exemplification  or  refocusing),     extension    
(adding  new information),  or     enhancement     (qualification).  certain  words  or  phrases 
indicate  certain  modalities  of  the     conjunction     scheme.  for  example,  the  word 
   specifically     is  used  to  identify  a     clarification     of  an     elaboration     of  a 
   conjunction    while the phrase    in other words    is used to identify an    apposition    of 
an    elaboration    of a    conjunction   . in order to detect such semantic information, 
they  used  a  lexicon  of  words  and  phrases  produced  semi-automatically  based  on  online 
thesauruses including id138. each entry in the lexicon associated a word or phrase with a 
set of syntactic constraints (in the form of allowed pos tags) and a set of semantic properties. 
the set of functional measures, then, contained measures showing, for instance, how many 
   conjunction   s were expanded to    elaboration   s or how many    elaboration   s 
were elaborated to    clarification   s, etc. however, no information was provided on the 
accuracy of those measures. experiments of authorship identification on a corpus of english 
novels of the 19th century showed that the functional features can improve the classification 
results when combined with traditional function word features. 

2.5 application-specific features 
the  previously  described  lexical,  character,  syntactic,  or  semantic  features  are  application-
independent since  they  can  be  extracted  from  any  textual  data  given  the  availability  of the 
appropriate nlp tools and resources required for their  measurement. beyond that, one can 
define  application-specific  measures  in  order  to  better  represent  the  nuances  of  style  in  a 
given text domain. this section reviews the most important of these measures. 

the  application  of  the  authorship  attribution  technology  in  domains  such  as  e-mail 
messages, and online forum messages revealed the possibility to define structural measures in 
order  to  quantify  the  authorial  style.  structural  measures  include  the  use  of  greetings  and 
farewells in the messages, types of signatures, use of indentation, paragraph length, etc. (de 
vel, et al., 2001; teng, lai, ma, & li, 2004; zheng, et al., 2006; li, zheng, & chen, 2006) 
moreover, provided the texts in question are in html form, measures related to html tag 
distribution (de vel, et al., 2001), font color counts, and font size counts (abbasi & chen, 
2005) can also be defined. apparently, such features can only be defined in given text genres. 
moreover, they are particular important in very short texts where the stylistic properties of the 
textual  content  cannot  be  adequately  represented  using  application-independent  methods. 
however, accurate tools are required for their extraction. zheng, et al. (2006) reported they 
had difficulties to measure accurately their structural features. 

in  general,  the  style  factor  of  a  text  is  considered  orthogonal  to  its  topic.  as  a  result, 
stylometric features attempt to avoid content-specific information to be more reliable in cross-
topic texts. however, in cases all the available texts for all the candidate authors are on the 
same thematic area, carefully selected content-based information may reveal some authorial 
choices. in order to better capture the properties of an author   s style within a particular text 
domain, content-specific keywords can be used. in more detail, given that the texts in question 
deal with certain topics and are of the same genre, one can define certain words frequently 
used within that topic or that genre. for example, in the framework of the analysis of online 
messages from the newsgroup misc.forsale.computers zheng, et al. (2006) defined content-
specific  keywords  such  as     deal   ,     sale   ,  or     obo     (or  best  offer).  the  difference  of  these 
measures  and  the  function  words  discussed  in  section  2.2  is  that  they  carry  semantic 
information and are characteristic of particular topics and genres. it remains unclear how to 
select such features for a given text domain. 

other  types  of  application-specific  features  can  only  be  defined  for  certain  natural 
languages.  for  example,  tambouratzis,  et  al.  (2004)  attempted  to  take  advantage  of  the 
diglossia  phenomenon  in  modern  greek  and  proposed  a  set  of  verbal  endings  which  are 
usually  found  in     katharevousa     and     dimotiki   ,  that  is,  roughly  the  formal  and  informal 
variations  of  modern  greek,  respectively.  although,  such  measures  have  to  be  defined 
manually, they can be very effective when dealing with certain text genres. 

 

10 

2.6 feature selection and extraction 
the feature sets used in authorship attribution studies often combine many types of features. 
in  addition,  some  feature  types,  such  as  lexical  and  character  features,  can  considerably 
increase the dimensionality of the feature set. in such cases, feature selection algorithms can 
be applied to reduce the dimensionality of the representation (forman, 2003). that way the 
classification algorithm is helped to avoid overfitting on the training data.  

in general, the features selected by these methods are examined individually on the basis 
of  discriminating  the  authors  of  a  given  corpus  (forman,  2003).  however,  certain  features 
that seem irrelevant when examined independently may be useful in combination with other 
variables.  in  this  case,  the  performance  of  certain  classification  algorithms  that  can  handle 
high  dimensional  feature  sets  (e.g.,  support  vector  machines)  might  be  diminished  by 
reducing the dimensionality (brank, grobelnik, milic-frayling, & mladenic, 2002). to avoid 
this problem, feature subset selection algorithms examine the discriminatory power of feature 
subsets (kohavi & john, 1997). for example, li, et al. (2006) described the use of a genetic 
algorithm to reduce an initial set of 270 features to an optimal subset for the specific training 
corpus  comprising  134  features.  as  a  result,  the  classification  performance  improved  from 
97.85% (when the full set was used) to 99.01% (when the optimal set was used). 

however, the best features may strongly correlate with one of the authors due to content-
specific rather than stylistic choices (e.g., imagine we have two authors for whom there are 
articles about politics for the one and articles about sports for the other). in other words, the 
features  identified  by  a  feature  selection  algorithm  may  be  too  corpus-dependent  with 
questionable general use. on the other hand, in the seminal work of mosteller and wallace 
(1964)  the  features  were  carefully  selected  based  on  their  universal  properties  to  avoid 
dependency on a specific training corpus. 

the most important criterion for selecting features in authorship attribution tasks is their 
frequency.  in  general,  the  more  frequent  a  feature,  the  more  stylistic  variation  it  captures. 
forsyth and holmes (1996) were the first to compare (character id165) feature sets selected 
by  frequency  with  feature  sets  selected  by  distinctiveness  and  they  found  the  latter  more 
accurate. however, they restricted the size of the extracted feature sets to relatively very low 
level  (96  features).  houvardas  and  stamatatos  (2006)  proposed  an  approach  for  extracting 
character  id165s  of  variable  length  using  frequency  information  only.  the  comparison  of 
this method with information gain, a well-known feature selection algorithm examining the 
discriminatory  power  of  features  individually  (forman,  2003),  showed  that  the  frequency-
based  feature  set  was  more  accurate  for  feature  sets  comprising  up  to  4,000  features. 
similarly,  koppel,  akiva,  and  dagan  (2006)  presented  experiments  comparing  frequency-
based  feature  selection  with  odds-ratio,  another  typical  feature  selection  algorithm  using 
discrimination information (forman, 2003). more important, the frequency information they 
used was not extracted from the training corpus. again, the frequency-based feature subsets 
performed  better  than  those  produced  by  odds-ratio.  when  the  frequency  information  was 
combined with odds-ratio the results were further improved.  

koppel,  akiva,  and  dagan  (2006)  also  proposed  an  additional  important  criterion  for 
feature  selection  in  authorship  attribution,  the  instability  of  features.  given  a  number  of 
variations  of the  same  text,  all  with  the  same  meaning,  the  features  that  remain  practically 
unchanged in all texts are considered stable. in other words, stability may be viewed as the 
availability of    synonyms    for certain language characteristics. for example, words like    and    
and    the    are very stable since there are no alternatives for them. on the other hand, words 
like     benefit     or     over     are  relatively  unstable  since  they  can  be  replaced  by     gain     and 
   above   ,  respectively,  in  certain  situations.  therefore,  instable  features  are  more  likely  to 
indicate stylistic choices of the author. to produce the required variations of the same text, 
koppel,  akiva,  and  dagan  (2006)  used  several  machine  translation  programs  to  generate 
translations from english to another language and then back to english. although the quality 
of the produced texts was obviously low, this procedure was fully-automated. let {d1, d2,   , 
id } a set of variations of the i-th text, all with roughly 
dn} be a set of texts and {

id ,   , m
2

id ,
j

 

11 

the  same  meaning.  for  a  stylometric  feature  c,  let 
variation of the i-th text and     =

c

k

j

i

j

i

. then, the instability of c is defined by: 

ic   be  the  value  of  feature  c  in  the  j-th 
j

   

i

k

i

   
   
   

in

c

   =

1

c

j

i

log

c

j

i

k
log
   

k

i

   

i

   

i

   
j
log

m

   
   
   

 

experiments showed that features selected by the instability criterion alone were not as 
effective as features selected by frequency. however, when the frequency and the instability 
criteria were combined the results were much better. 

another approach to reduce dimensionality is via feature extraction (sebastiani, 2002). 
here, a new set of    synthetic    features is produced by combining the initial set of features. 
the  most  traditional  feature  extraction  technique  in  authorship  attribution  studies  is  the 
principal components analysis which provides linear combinations of the initial features. the 
two most important principal components can, then, be used to represent the texts in a two-
dimensional space (burrows, 1987; burrows, 1992; binongo, 2003). however, the reduction 
of  the  dimensionality  to  a  single  feature  (or  a  couple  of  features)  has  the  consequence  of 
losing  too  much  variation  information.  therefore,  such  simple  features  are  generally 
unreliable to be used alone. another, more elaborate feature extraction method was described 
by zhang and lee (2006). they first built a suffix tree representing all the possible character 
id165s of the texts and then extracted groups of character id165s according to frequency 
and  redundancy  criteria.  the  resulting  key-substring-groups,  each  one  accumulating  many 
character  id165s,  were  the  new  features.  the  application  of  this  method  to  authorship 
attribution and other text classification tasks provided promising results. 

3. attribution methods 
in every authorship identification problem, there is a set of candidate authors, a set of text 
samples of known authorship covering all the candidate authors (training corpus), and a set of 
text samples of unknown authorship (test corpus), each one of them should be attributed to a 
candidate  author.  in  this  survey,  we  distinguish  the  authorship  attribution  approaches 
according to whether they treat each training text individually or cumulatively (per author). in 
more detail, some approaches concatenate all the available training texts per author in one big 
file and extract a cumulative representation of that author   s style (usually called the author   s 
profile) from this concatenated text. that is, the differences between texts written by the same 
author are disregarded. we examine such profile-based approaches2 first since early work in 
authorship attribution has followed this practice (mosteller & wallace, 1964). on the other 
hand, another family of approaches requires multiple training text samples per author in order 
to develop an accurate attribution model. that is, each training text is individually represented 
as  a  separate  instance  of  authorial  style.  such  instance-based approaches3  are  described  in 
section  3.2  while  section  3.3  deals  with  hybrid  approaches  attempting  to  combine 
characteristics of profile-based and instance-based methods. then, in section 3.4 we compare 
these two basic approaches and discuss their strengths and weaknesses across several factors. 
it has to be noted that, in this review, the distinction between profile-based and instance-
based  approaches  is  considered  the  most  basic  property  of  the  attribution  methods  since  it 
largely determines the philosophy of each method (e.g., a classification model of generative 
or  discriminative  nature).  moreover,  it  shows  the  kind  of  writing  style  that  each  method 
attempts  to  handle:  a  general  style  for  each  author  or  a  separate  style  of  each  individual 
document. 
                                                      
2 note that this term should not be confused with author profiling methods (e.g., extracting information 
about the author gender, age, etc.) (koppel, et al., 2003) 
3 note that this term should not be confused with instance-based learning methods (mitchell, 1997). 

 

12 

training texts 
of author a 

+ 

+ 

= 

xa 

text of unknown 

authorship 

attribution 

model 

xu 

+ 

= 

xb 

most likely author 

training texts 
of author b 

fig 1. typical architecture of profile-based approaches. 

3.1 profile-based approaches 
one way to handle the available training texts per author is to concatenate them in one single 
text file. this big file is used to extract the properties of the author   s style. an unseen text is, 
then,  compared  with  each  author  file  and  the  most  likely  author  is  estimated  based  on  a 
distance  measure. it should be stressed that there is no separate representation of each text 
sample  but  only  one  representation  of  a  big  file  per  author.  as  a  result,  the  differences 
between  the  training  texts  by  the  same  author  are  disregarded.  moreover,  the  stylometric 
measures extracted from the concatenated file may be quite different in comparison to each of 
the  original  training  texts.  a  typical  architecture  of  a  profile-based  approach  is  depicted  in 
figure 1. note that x denotes a vector of text representation features. hence, xa is the profile 
of author a and xu is the profile of the unseen text. 

the profile-based approaches have a very simple training process. actually, the training 
phase just comprises the extraction of profiles for the candidate authors. then, the attribution 
model is usually based on a distance function that computes the differences of the profile of 
an  unseen  text  and  the  profile  of  each  author.  let  pr(x)  be  the  profile  of  text  x  and 
d(pr(x),pr(y)) the distance between the profile of text x and the profile of text y. then, the 
most likely author of an unseen text x is given by: 
arg
(min
a   
a

xprd
(

where a is the set of candidate authors and xa is the concatenation of all training texts for 
author  a.  in  the  following,  we  first  describe  how  this  approach  can  be  realized  by  using 
probabilistic  and  compression  models  and,  then,  the  cng  method  and  its  variants  are 
discussed. 

author

xpr
(

x
)(

))

),

=

 

a

 

13 

3.1.1 probabilistic models 
one  of  the  earliest  approaches  to  author  identification  that  is  still  used  in  many  modern 
studies  employ  the  use  of  probabilistic  models  (mosteller  &  wallace,  1964;  clement  & 
sharp,  2003;  peng,  et  al.,  2004;  zhao &  zobel,  2005;  madigan,  et  al.,  2005;  sanderson  & 
guenter,  2006).  such  methods  attempt  to  maximize  the  id203  p(x|a)  for  a  text  x  to 
belong to a candidate author a. then, the attribution model seeks the author that maximizes 
the following similarity metric: 

author

x
)(

=

arg

max
a a   

log

2

axp
(
axp
(

|
|

)
)

 

where  the  conditional  probabilities  are  estimated  by  the  concatenation  xa  of  all  available 
training texts of the author a and the concatenation of all the rest texts, respectively. variants 
of  such  probabilistic  classifiers  (e.g.,  na  ve  bayes)  have  been  studied  in  detain  in  the 
framework  of  topic-based  text  categorization (sebastiani,  2002).  an  extension  of  the  na  ve 
bayes  algorithm  augmented  with  statistical  language  models  was  proposed  by  peng,  et  al. 
(2004) and achieved high performance in authorship attribution experiments. in comparison 
to standard na  ve bayes classifiers, the approach of peng, et al. (2004) allows local markov 
chain  dependencies  in  the  observed  variables  to  capture  contextual  information.  moreover, 
sophisticated smoothing techniques from statistical id38 can be applied to this 
method (the best results for authorship attribution were obtained using absolute smoothing). 
more interesting, this method can be applied to both character and word sequences. actually, 
peng,  et  al  (2004)  achieved  their  best  results  for  authorship  attribution  using  word-level 
models for a specific corpus. however, this was not confirmed in other corpora as well. 

3.1.2 compression models 
the  most  successful  of  the  compression-based  approaches  follow  the  profile-based 
methodology  (kukushkina,  et  al.,  2001;  khmelev  &  teahan,  2003a;  marton,  et  al.,  2005). 
such  methods  do  not  produce  a  concrete  vector  representation  of  the  author   s  profile. 
therefore,  we  can  consider pr(x)=x.  initially,  all  the  available  texts  for  the i-th  author  are 
first concatenated to form  a big file xa and a compression algorithm is  called  to produce  a 
compressed file c(xa). then, the unseen text x is added to each text xa and the compression 
algorithm is called again for each c(xa +x). the difference in bit-wise size of the compressed 
files d(x, xa)=c(xa +x)   c(xa) indicates the similarity of the unseen text with each candidate 
author. essentially, this difference calculates the cross-id178 between the two texts.  several 
off-the-shelf  compression  algorithms  have  been  tested  with  this  approach  including  rar, 
lzw, gzip, bzip2, 7zip, etc. and in most of the cases rar found to be the most accurate 
(kukushkina, et al., 2001; khmelev & teahan, 2003a; marton, et al., 2005). 

it has to be underlined that the prediction by partial matching (ppm) algorithm (teahan 
& harper, 2003) that is used by rar to compress text files works practically the same as the 
method of peng, et al. (2004). however, there is a significant difference with the previously 
described probabilistic method. in particular, in the method of khmelev and teahan (2003a) 
the models describing xa were adaptive with respect to x, that is, the compression algorithm 
was  applied  to  the  text  xa+x,  so  the  compression  model  was  modified  as  it  processed  the 
unseen text. in the method of peng, et al. (2004) the models describing xa were static, that is, 
the id165 markov models were extracted from text xa and then applied to unseen text x and 
no modification of the models was allowed in the latter phase. for that reason, the application 
of the probabilistic method to the classification of an unseen text is faster in comparison to 
this compression-based approach. another advantage of the id38 approach is 
that  it  can  be  applied  to  both  character  and  word  sequences  while  the  ppm  compression 
models are only applied to character sequences.  

3.1.3 cng and variants 
a  profile-based  method  of  particular  interest,  the  common  id165s  (cng)  approach,  was 
described by keselj, et al. (2003). this method used a concrete representation of the author   s 

 

14 

profile.  in  particular,  the  profile  pr(x)  of  a  text  x  was  composed  by  the  l  most  frequent 
character id165s of that text. the following distance is, then, used to estimate the similarity 
between two texts x, and y: 

xprd
(
(

),

ypr
(

))

=

   

xpg
)(
   

   

yp
(

)

(2

gf
)(
   
x
gf
)(
+
x

f
f

y

g
))
(
y
g
)(

   
   
   
   

2

 

   
   
   
   

where g is a character id165 while fx(g) and fy(g) are the relative frequencies of occurrence 
of that id165 in texts x and y, respectively. in words, this measure computes the dissimilarity 
between two profiles by calculating the relative difference between their common id165s. 
all the id165s of the two profiles that are not common contribute a constant value to the 
distance.  the  cng  method  has  two  important  parameters  that  should be  tuned:  the  profile 
size l and the character id165 length n, that is, how many and how long strings constitute 
the profile. keselj, et al. (2003) reported their best results for 1,000   l   5,000 and 3   n   5. this 
basic approach has been applied successfully to various authorship identification experiments 
including the authorship attribution competition organized in 2004 (juola, 2004, juola, 2006). 
an important problem in authorship attribution tasks arises when the distribution of the 
training corpus over the candidate authors is uneven. for example, it is not unusual, especially 
in forensic applications, to have multiple training texts for some candidate authors and very 
few training texts for other authors. moreover, the length of these samples may not allow their 
segmentation into multiple parts to enrich the training instances of certain authors. in machine 
learning terms, this constitutes the class imbalance problem. the majority of the authorship 
attribution approaches studies present experiments based on balanced training sets (i.e., equal 
amount of training text samples for each candidate author) so it is not possible to estimate 
their  accuracy  under  class  imbalance  conditions.  only  a  few  studies  take  this  factor  into 
account (marton, et al., 2005; stamatatos, 2007).  

the cng distance function performs well when the training corpus is relatively balanced 
but  it  fails  in  imbalanced  cases  where  at  least  one  author   s  profile  is  shorter  than  l 
(stamatatos, 2007). for example, if we use l=4,000 and n=3, and the available training texts 
of  a  certain  candidate  author  are  too  short,  then  the  total  amount  of  3-grams  that  can  be 
extracted from that authors    texts may be less than 4,000. the distance function favors that 
author because the union of the profile of the unseen text and the profile of that author will 
result significant less id165s, so the distance between the unseen text and that author would 
be  estimated  as  quite  low  in  comparison  to  the  other  authors.  to  overcome  that  problem, 
frantzeskou,  stamatatos,  gritzalis,  and  katsikas  (2006)  proposed  a  different  and  simpler 
distance,  called  simplified  profile  intersection  (spi),  which  simply  counts  the  amount  of 
common id165s of the two profiles disregarding the rest. the application of this measure to 
author identification of source code provided better results than the original cng distance. 
note  that  in  contrast  to  cng  distance,  spi  is  a  similarity  measure,  meaning  that  the  most 
likely author is the author with the highest spi value. a problem of that distance can arise 
when all the candidate authors except one have very short texts. then, spi metric will favor 
the author with long texts since many more common id165s will be detected in their texts 
and an unseen text. 

another variation of the cng dissimilarity function was proposed by stamatatos (2007):  

xprd
(
(

),

ypr
(

),

npr

(

))

=

   

xpg
)(
   

(2

gf
)(
   
x
gf
)(
+
x

f
f

y

g
))
(
y
g
)(

   
   
   
   

2

   
   
      
   

   
      
   

(2

gf
)(
   
x
gf
)(
+
x

f
f

n

g
))
(
n
g
)(

   
      
   

2

 

where  n  is  the  corpus  norm  (the  concatenation  of  all  available  texts  of  all  the  candidate 
authors) and fn(g) is the relative frequency of occurrence of the id165 g in the corpus norm. 
note that this function is not symmetric as the original cng function. in particular, the first 
argument pr(x) is the profile of the unseen text and the second argument is an author profile. 
that way, only the id165s of the unseen text   s profile contribute to the calculated sum. as a 
result, the problems described earlier with imbalanced corpora are significantly reduced since 
the distance between the unseen text and the candidate authors is always based on the same 
amount of terms. moreover, each term is multiplied by the relative distance of the specific n-
gram frequency from the corpus norm. hence, the more an id165 deviates from its    normal    

 

15 

training texts 
of author a 

xa,1 

xa,2 

xa,3 

xb,1 

xb,2 

training texts 
of author b 

text of unknown 

authorship 

classifier 
training 

attribution 

model 

xu 

most likely author 

fig 2. typical architecture of instance-based approaches. 

frequency, the more contributes to the distance. on the other hand, if the frequency of an n-
gram is found exactly the same as its    normal    frequency, it does not contribute at all at the 
distance value (the norm factor is zero). experiments reported by stamatatos (2007) showed 
that this distance function can better handle cases where limited and imbalanced corpora were 
available  for  training.  furthermore,  it  was  quite  stable  with  respect  to  the  parameter  l. 
however,  in  cases  where  enough  training  texts  were  available,  the  original  cng  method 
produced better results.  

3.2 instance-based approaches 
the majority of the modern authorship identification approaches considers each training text 
sample as a unit that contributes separately to the attribution model. in other words, each text 
sample of known authorship is an instance of the problem in question. a typical architecture 
of such an instance-based approach is shown in figure 2. in detail, each text sample of the 
training  corpus  is  represented  by  a  vector  of  attributes  (x)  following  methods  described  in 
section  2  and  a  classification  algorithm  is  trained  using  the  set  of  instances  of  known 
authorship (training set) in order to develop an attribution model. then, this model will be 
able to estimate the true author of an unseen text.  
it has to be underlined that such classification algorithms require multiple training instances 
per class for extracting a reliable model. therefore, according to instance-based approaches, 
in case we have only one but quite long training text for a particular candidate author (e.g., an 
entire  book),  this  should  be  segmented  into  multiple  parts,  probably  of  equal  length.  from 
another  point  of  view,  when  there  are  multiple  training  text  samples  of  variable-length  per 
author, the training text instance length should be normalized. to that end, the training texts 
per author are segmented to equally-sized samples (sanderson & guenter, 2006). in all these 
cases,  the  text  samples  should  be  long  enough  so  that  the  text  representation  features  can 
represent  adequately  their  style.  various  lengths  of  text  samples  have  been  reported  in  the 

 

16 

literature. sanderson and guenter (2006) produced chunks of 500 characters. koppel, et al. 
(2007)  segmented  the  training  texts  into  chunks  of  about  500  words.  hirst  and  feiguina 
(2007)  conducted  experiments  with  text  blocks  of  varying  length  (i.e.,  200,  500,  and  1000 
words) and they reported significantly reduced accuracy as the text block length decreases. 
therefore, the choice of the training instance text sample is not a trivial process and directly 
affects the performance of the attribution model.  

in what follows, we first describe the vector space models that comprise the majority of 
the instance-based approaches. then, various similarity-based and meta-learning models are 
discussed. 

3.2.1 vector space models 
given that the training texts are represented in a multivariate form, we can consider each text 
as  a  vector  in  a  multivariate  space.  then,  a  variety  of  powerful  statistical  and  machine 
learning  algorithms  can  be  used  to  build  a  classification  model,  including  discriminant 
analysis (stamatatos, et al., 2000; tambouratzis, et al., 2004; chaski, 2005), support vector 
machines (id166) (de vel, et al, 2001; diederich, et al, 2003; teng, et al., 2004; li, et al., 
2006;  sanderson  &  guenter,  2006),  decision  trees  (uzuner  &  katz,  2005;    zhao  &  zobel, 
2005;  zheng,  et  al.,  2006),  neural  networks  (matthews  &  merriam,  1993;  merriam  & 
matthews,  1994;  tweedie,  singh,  &  holmes,  1996;  zheng,  et  al.,  2006;  khosmood  & 
levinson,  2006),  genetic  algorithms  (holmes  &  forsyth,  1995),  memory-based  learners 
(luyckx & daelemans, 2005), classifier ensemble methods (stamatatos, 2006a), etc.  

such algorithms have been studied thoroughly in the framework of (mostly topic-based) 
text categorization research (sebastiani, 2002). therefore, we will not discuss them further. it 
should  be  noted,  though,  that  some  of  these  algorithms  can  effectively  handle  high-
dimensional, noisy, and sparse data, allowing more  expressive representations of texts. for 
example, a id166 model is able to avoid overfitting problems even when several thousands of 
features are used and is considered one of the best solutions of current technology (li, et al., 
2006; stamatatos, 2008). 

the effectiveness of vector space models is usually diminished by the presence of the 
class imbalance problem. recently, stamatatos (2008) proposed an approach to deal with this 
problem  in  the  framework  of  vector  space  instance-based  approaches.  in  more  detail,  the 
training  set  can  be  re-balanced  by  segmenting  the  text  samples  of  a  particular  author 
according to the size of their class (i.e., the length of all texts of that author). that way, many 
short text samples can be produced for minority authors (i.e., the authors for whom only a few 
training  texts  were  available)  while  less  but  longer  texts  can  be  produced  for  the  majority 
authors (i.e., the authors for whom multiple training texts were available). moreover, text re-
sampling  (i.e.,  using  some  text  parts  more  than  one  time)  could  be  used  to  increase  the 
training set of the minority authors. 

3.2.2 similarity-based models 
the main idea of similarity-based models is the calculation of pairwise similarity measures 
between the unseen text and all the training texts and, then, the estimation of the most likely 
author based on a nearest-neighbor algorithm. the most notable approach of this category has 
been proposed by burrows (2002) under the name    delta   . first, this method calculates the z-
distributions of a set of function words (originally, the 150 most frequent words). then, for 
each document, the deviation of each word frequency from the norm is calculated in terms of 
z-score, roughly indicating whether it is used more (positive z-score) or less (negative z-score) 
times than the average. finally, the delta measure indicating the difference between a set of 
(training) texts written by the same author and an unknown text is the mean of the absolute 
differences between the z-scores for the entire function word set in the training texts and the 
corresponding z-scores of the unknown text. the smaller delta measure, the greater stylistic 
similarity  between  the  unknown  text  and  the  candidate  author.  this  method  was  mainly 
evaluated  on  literary  texts  (english  poems  and  novels)  producing  remarkable  results 
(burrows,  2002;  hoover,  2004a).  it  has  been  demonstrated  that  it  is  a  very  effective 

 

17 

attribution  method  for  texts  of  at  least  1,500  words.  for  shorter  texts  the  accuracy  drops 
according  to  length.  however,  even  for  quite  short  texts,  the  correct  author  was  usually 
included in the first five positions of the ranked authors which provides a means for reducing 
the set of candidate authors. 

a  theoretical  understanding  of  the  operation  of  delta  has  been  described  by argamon 
(2008).  in  more  detail,  he  showed  that  delta  can  be  viewed  as  an  axis-weighted  form  of 
nearest-neighbor  classification,  where  the  unknown  text  is  assigned  to  the  nearest  category 
instead of the nearest training text. it was also shown that the distance ranking of candidate 
authors produced by delta is equivalent to id203 ranking under the assumption that word 
frequencies  follow  a  laplace  distribution.  this  view  indicates  many  extensions  and 
generalizations  of  delta,  for  example,  using  gaussian  distributions  of  word  frequencies  in 
place  of  laplace  distributions,  etc.  a  detailed  study  of  variations  of  burrows     delta  was 
presented by hoover (2004a). he found that by using larger sets of frequent words (>500) the 
accuracy  of  the  method  was  increasing.  the  performance  was  also  improved  when  the 
personal pronouns and words for which a single text supplied most of their occurrences were 
eliminated. some variations of the delta score itself were also examined but no significant 
improvement over the original method was achieved (hoover, 2004b).  

another  similarity-based  approach  utilizing  text  compression  models  to  estimate  the 
difference between texts has been described by benedetto, et al. (2002). the training phase of 
this method merely comprises the compression of each training text in separate files using an 
off-the-shelf  algorithm  (gzip).  for  estimating  the  author  of  an  unseen  text,  this  text  is 
concatenated to each training text file and then each resulting file is compressed by the same 
algorithm.  let  c(x)  be  the  bit-wise  size  of  the  compression  of  file  x  while  x+y  is  the 
concatenation of text files x and y. then, the difference c(x+y)-c(x) indicates the similarity of 
a training text x with the unseen text y. finally, a 1-nearest-neighbor decision estimates the 
most likely author.  

this method was strongly criticized by several researchers (goodman, 2002; khmelev & 
teahan,  2003b)  indicating  many  weaknesses.  first,  it  is  too  slow  since  it  has  to  call  the 
compression  algorithm  so  many  times  (as  many  as  the  training  texts).  note  that  in  the 
corresponding  profile-based  approach  of  khmelev  and  teahan  (2003a),  the  compression 
algorithm is called as many times as the candidate authors. hence, the running time will be 
significantly  lower  for  the  profile-based  compression-based  method.  moreover,  various 
authorship identification experiments showed that the compression-based approach following 
the  profile-based  technique  usually  outperforms  the  corresponding  instance-based  method 
(marton, et al., 2005). an important factor that contributes to this direction is that 1-nearest-
neighbor approach is sensitive to noise. however, this problem could be faced by using the k-
nearest-neighbors and a majority vote or a weighted vote scheme. last but not least, gzip is a 
dictionary-based  compression  algorithm  and  uses  a  sliding  window  of  32k  to  build  the 
dictionary. this means that if a training text is long enough the beginning of that document 
will  be  ignored  when  gzip  attempts  to  compress  the  concatenation  of  that  file  with  the 
unseen  text.  comparative  experiments  on  various  corpora  have  shown  that  the  rar 
compression algorithm outperforms gzip in most of the cases (marton, et al., 2005).  

an  alternative  distance  measure  for  the  compression-based  approach  was  proposed  by 
cilibrasi and vitanyi (2005). based on the notion of the kolmogorov complexity they defined 
the normalized compression distance (ncd) between two texts x and y as follows: 

ncd

yx
,(

)

=

xc
(

)

y
+
   
max{

ycxc
min{
(
),
ycxc
),
(
)}(

)}(

 

cilibrasi and vitanyi (2005) used this distance metric and the bzip2 compression algorithm 
to cluster literary works in russian by 4 different authors and reported excellent results. they 
even attempted to cluster the corresponding english translations of those texts with relatively 
good results.  

 

18 

3.2.3 meta-learning models 
in addition to the general purpose classification algorithms described in section 3.2.1, one can 
design more complex algorithms specifically designed for authorship attribution. to this end, 
an existing classification algorithm may serve as a tool in a meta-learning scheme. the most 
interesting approach of this kind is the unmasking method proposed by koppel, et al. (2007) 
originally  for  author  verification.  the  main  difference  with  the  typical  instance-based 
approach shown in figure 2 is that in the unmasking method the training phase does not exist. 
for each unseen text a id166 classifier is built to discriminate it from the training texts of each 
candidate author. so, for n candidate authors koppel, et al. (2007) built n classifiers for each 
unseen text. then, in an iterative procedure, they removed a predefined amount of the most 
important features for each classifier and measured the drop in accuracy. at the beginning, all 
the  classifiers  had  more  or  less  the  same  very  high  accuracy.  after  a  few  iterations,  the 
accuracy of the classifier that discriminates between the unseen text and the true author would 
be  too  low  while  the  accuracy  of  the  other  classifiers  would  remain  relatively  high.  this 
happens because the differences between the unseen text and the other authors are manifold, 
so by removing a few features the accuracy is not affected dramatically. koppel et al. (2007) 
proposed a simple meta-learning method to learn to discriminate the true author automatically 
and reported very good results. this method seems more appropriate when the unknown texts 
are long enough since each unknown text has to be segmented in multiple parts to train the 
id166  classifiers.  this  was  confirmed  by  sanderson and  guenter  (2006)  who examined  the 
unmasking method in long texts (entire books) with high accuracy results while in short texts 
of newspaper articles the results were not encouraging. 

3.3 hybrid approaches 
a method that borrows some elements from both profile-based and instance-based approaches 
was  described  by  van  halteren  (2007).  in  more  detail,  all  the  training  text  samples  were 
represented  separately,  as  it  happens  with  the  instance-based  approaches.  however,  the 
representation vectors for the texts of each author were feature-wisely averaged and produced 
a single profile vector for each author, as it happens with the profile-based approaches. the 
distance of the profile of an unseen text from the profile of each author was, then, calculated 
by a weighted feature-wise function. three weighting parameters had to be tuned empirically: 
one  for  the  difference  between  the  feature  values  of  the  unseen  text  profile  and  the  author 
profile,  one  for  the  feature  importance  for  the  unseen  text,  and  another  for  the  feature 
importance  for  the  particular  author.  a  similar  hybrid  approach  was  also  used  by  grieve 
(2007). 

3.4 comparison 
table 2 shows the results of comparing profile-based and instance-based approaches across 
several  factors.  as  already  underlined,  the  main  difference  is  the  representation  of  training 
texts. the former produce one cumulative representation for all training texts per author while 
the latter produce individual representations for each training text. in certain cases, this is an 
important advantage of profile-based methods. first, when only short texts are available for 
training (e.g., e-mail messages, online forum messages), their concatenation may produce a 
more  reliable  representation  in  comparison  to  individual  representations  of  short  texts. 
furthermore,  when  only  one  long  text  (or  a  few  long  texts)  is  available  for  one  author, 
instance-based approaches require its segmentation to multiple parts. 

on  the  other  hand,  instance-based  approaches  take  advantage  of  powerful  machine 
learning  algorithms  able  to  handle  high-dimensional,  noisy,  and  sparse  data  (e.g.,  id166). 
moreover,  it  is  easy  to  combine  different  kinds  of  stylometric  features  in  an  expressive 
representation. this is more difficult in profile-based approaches that are based on generative 
(e.g.,  bayesian)  models  or  similarity-based  methods  and  usually 
they  can  handle 
homogeneous  feature  sets  (e.g.,  function  words,  character  id165s,  etc.)  an  exception  is 
described  by  van  halteren  (2007)  although  this  is  not  a  pure  profile-based  method.  in 
addition, several stylometric features defined on the text-level, for instance, use of greetings 

 

19 

table 2. comparison of profile-based and instance-based approaches. 
 

instance-based approaches 

text representation 

stylometric features 

profile-based approaches 
one cumulative representation 
for all the training texts per 
author 
difficult to combine different 
features. some (text-level) 
features are not suitable 

classification 

generative (e.g., bayesian) 
models, similarity-based 
methods 

training time cost 

low 

running time cost 

class imbalance 

low (relatively high for 
compression-based methods) 
depends on the length of 
training texts 

each training text is 
represented individually. text 
segmentation may be required. 

different features can be 
combined easily 

discriminative models, 
powerful machine learning 
algorithms (e.g., id166), 
similarity-based methods 
relatively high (low for 
compression-based methods) 
low (very high for 
compression-based methods) 
depends mainly on the amount 
of training texts 

 
and signatures, cannot be easily used by profile-based approaches since the profile attempts to 
represent the general properties of the author   s style rather than the properties of a typical text 
sample by that author. 

another  main  difference  is  the  existence  of  the  training  phase  in  the  instance-based 
approaches  with  the  exception  of  compression-based  models  (benedetto,  et  al.,  2002).  the 
training phase of profile-based approaches is relatively simple comprising just the extraction 
of measures  from training texts. in both cases, the running time cost is low again with the 
exception  of  compression-based  methods.  the  running  time  cost  of  instance-based 
compression methods is analogous to the number of training texts while the running time cost 
for the corresponding profile-based approaches is analogous to the number of the candidate 
authors (marton, et al., 2005). 

in  instance-based  approaches  class  imbalance  depends  on  the  amount  of  training  texts 
per  author.  in  addition,  the  text-length  of  training  texts  may  produce  class  imbalance 
conditions  when  long  texts  are  segmented  into  many  parts.  on  the  other  hand,  the  class 
imbalance problem in profile-based approaches depends only on text-length. hence, we may 
have two candidate authors with exactly the same amount of training text samples. however, 
the first author   s texts are short while the other author   s texts are long. this means that the 
concatenation of the training texts per author will produce two files that differ significantly in 
text length.  

4. evaluation 
the seminal study of mosteller and wallace (1964) was about the disputed authorship of the 
federalist  papers.  this  case  offered  a  well  defined  set  of  candidate  authors,  sets  of  known 
authorship for all the candidate authors, and a set of texts of disputed authorship. moreover, 
all  the  texts  were  of  the  same  genre  and  about  the  same  thematic  area.  hence,  it  was 
considered the ideal testing ground for early authorship attribution studies as well as the first 
fully-automated approaches (holmes & forsyth, 1995; tweedie, et al., 1996). it is also used 
in some modern studies (teahan & harper, 2003; marton, et al., 2005). although appealing, 
this  case  has  a  number  of  important  weaknesses.  more  specifically,  the  set  of  candidate 

 

20 

authors is too small; the texts are relatively long; while the disputed texts may be the result of 
collaborative writing of the candidate authors (collins, et al., 2004).  

a significant part of modern authorship attribution studies apply the proposed techniques 
to literary works of undisputed authorship, including american and english literature (uzuner 
& katz, 2005; mccarthy, et al., 2006; argamon, et al., 2007; koppel, et al., 2007; zhao & 
zobel, 2007), russian literature (kukushkina, et al., 2001; cilibrasi & vitanyi, 2005), italian 
literature (benedetto, et al., 2002), etc. a case of particular difficulty concerns the separation 
of works of the bronte sisters, charlotte and anna, since they share the same characteristics 
(burrows, 1992; koppel, akiva, & dagan, 2006; hirst & feiguina, 2007). the main problem 
when using literary works for evaluating author identification methods is the text-length of 
training and test texts (usually entire books). certain methods can work effectively in long 
texts  but  not  so  well  on  short  or  very  short  texts  (sanderson  &  guenter,  2006;  hirst  & 
feiguina, 2007). to this end, poems provide a more reliable testing ground (burrows, 2002). 
beyond literature, several evaluation corpora for authorship attribution studies have been 
built covering certain text domains such as online newspaper articles (stamatatos, et al., 2000; 
diederich,  et  al.,  2003;  luyckx  &  daelemans,  2005;  sanderson  &  guenter,  2006),  e-mail 
messages (de vel, et al., 2001; koppel & schler, 2003), online forum messages (argamon, et 
al., 2003; abbasi & chen, 2005; zheng, et al., 2006), newswire stories (khmelev & teahan, 
2003a;  zhao  &  zobel,  2005),  blogs  (koppel,  schler,  argamon,  &  messeri,  2006),  etc. 
alternatively,  corpora  built  for  other  purposes  have  also  been  used  in  the  framework  of 
authorship attribution studies including parts of the reuters-21578 corpus (teahan & harper, 
2003;  marton,  et  al.,  2005),  the  reuters  corpus  volume  1  (khmelev  &  teahan,  2003a; 
madigan, et al., 2005; stamatatos, 2007) and the trec corpus  (zhao & zobel, 2005) that 
were initially built for evaluating thematic text categorization tasks. such corpora offer the 
possibility to test methods on cases with many candidate authors and relatively short texts. 

following  the  practice  of  other  text  categorization  tasks,  some  of  these  corpora  have 
been used as a benchmark to compare different methods on exactly the same training and test 
corpus (sebastiani, 2002). one such corpus comprising modern greek newspaper articles was 
introduced by stamatatos, et al. (2000; 2001) and has been later used by peng, et al., (2003), 
keselj,  et  al.  (2003),  peng,  et  al.,  (2004),  zhang  and  lee  (2006),  and  stamatatos  (2006a; 
2006b).  moreover,  in  the  framework  of  an  ad-hoc  authorship  attribution  competition 
organized  in  2004  various  corpora  have  been  collected4  covering  several  natural  languages 
(english,  french,  latin,  dutch,  and  serbian-slavonic)  and  difficulty  levels  (juola,  2004; 
juola, 2006). 

any good evaluation corpus for authorship attribution should be controlled for genre and 
topic. that way, authorship would be the most important discriminatory factor between the 
texts. whilst genre can be easily controlled, the topic factor reveals difficulties. ideally, all the 
texts of the training corpus should be on exactly the same topic for all the candidate authors. 
a few such corpora have been reported. chaski (2001) described a writing sample database 
comprising texts of 92 people on 10 common subjects (e.g., a letter of apology to your best 
friend, a letter to your insurance company, etc.). clement and sharp (2003) reported a corpus 
of  movie  reviews  comprising  5  authors  who  review  the  same  5  movies.  another  corpus 
comprising various genres was described by baayen, van halteren, neijt, and tweedie (2002) 
and was also used by juola and baayen (2005) and van halteren (2007). it consisted of 72 
texts by 8 students of dutch literature on specific topics covering three genres. in more detail, 
each student was asked to write three argumentative nonfiction texts on specific topics (e.g. 
the  unification  of  europe),  three  descriptive  nonfiction  texts  (e.g.,  about  soccer),  and  three 
fiction texts (e.g., a murder story in the university). other factors that should be controlled in 
the ideal evaluation corpus include age, education level, nationality, etc. in order to reduce the 
likelihood  the  stylistic  choices  of  a  given  author  to  be  characteristic  of  a  broad  group  of 
people rather than strictly personal. in addition, all the texts per author should be written in 
the same period to avoid style changes over time (can & patton, 2004). 
                                                      
4 http://www.mathcs.duq.edu/~juola/authorship_contest.html 

 

21 

s
t
x
e
t
 

g
n

i

n

i
a
r
t

25
20
15
10
5
0

25
20
15
10
5
0

25
20
15
10
5
0

1

2

3

4
authors

5

1

2

(a) 

3

4
authors

(b) 

5

1

2

5

3

4
authors

 

(c) 

fig. 3.  different distributions of training and test texts over 5 candidate authors: (a) an 
imbalanced  distribution  of  training  texts,  (b)  an  imbalanced  distribution  of  test  texts 
imitating the distribution of training texts, (c) a balanced distribution of test texts. 
 

a  thorough  evaluation  of  an  authorship  attribution  method  would  require  the 
examination  of  its  performance  under  various  conditions.  the  most  important  evaluation 
parameters are the following: 

     training corpus size, in terms of both the amount and length of training texts. 
     test corpus size (in terms of text length of the unseen texts). 
     number of candidate authors. 
     distribution of the training corpus over the authors (balanced or imbalanced). 
in the case of imbalanced training corpus, an application-dependent methodology should 
be followed to form the most appropriate test corpus. one option is the distribution of test 
corpus  over  the  candidate  authors  to  imitate  the  corresponding  distribution  of  the  training 
corpus (khmelev & teahan, 2003a; madigan, et al., 2005). examples of such training and test 
corpora are shown in figures 3a and 3b, respectively. consequently, a model that learns to 
guess the author of an unseen text taking into account the amount of available training texts 
per author would achieve good performance on such a test corpus. this practice is usually 
followed  in  the  evaluation  of  topic-based  text  categorization  methods.  however,  in  the 
framework  of  authorship  attribution,  it  seems  suitable  only  for  applications  that  aim  at 
filtering  texts  according  to  authorial  information.  another  option  would  be  the  balanced 
distribution of the test corpus over the candidate authors (stamatatos, 2007; stamatatos 2008). 
examples of such training and test corpora are shown in figures 3a and 3c, respectively. as a 
result,  a  model  that  learns  to  guess  the  author  of  an  unseen  text  taking  into  account  the 
amount of available training texts per author will achieve low performance on that balanced 
test  corpus.  this  approach  seems  appropriate  for  the  majority  of  authorship  attribution 
applications, including intelligence, criminal law, or forensics where the availability of texts 
of known authorship should not increase the likelihood of certain candidate authors. that is, 
in  most  cases  it  just  happens  to  have  many  (or  few)  texts  of  known  authorship  for  some 
authors. note also that an imbalanced training corpus is the most likely real-world scenario in 
a given authorship identification application. 

another  important  factor  in  the  evaluation  of  an  authorship  attribution  approach  is  its 
ability to handle more than one natural language. recall from section 2 that many features 
used to represent the stylistic properties are language-dependent. in general, methods using 
character  features  can  be  easily  transferred  to  other  languages.  a  few  studies  present 
experiments in multiple natural languages. peng, et al. (2003), evaluated their method in three 
languages, namely, english, greek, and chinese while keselj, et al. (2003) used english and 
greek corpora. in addition, abassi and chen (2005) and stamatatos (2008) used english and 
arabic corpora while li, et al. (2006) evaluated their approach in english and chinese texts. 

 

22 

beyond  language,  an  attribution  method  should  be  tested  on  a  variety  of  text  genres  (e.g., 
newspaper articles, blogs, literature, etc.) to reveal its ability to handle unrestricted text or just 
certain text domains. 

5. discussion 
rudman (1998) criticized the state of authorship attribution studies saying:    non-traditional 
authorship attribution studies     those employing the computer, statistics, and stylistics     have 
had enough time to pass through any    shake-down    phase and enter one marked by solid, 
scientific, and steadily progressing studies. but after 30 years and 300 publications, they have 
not   . it is a fact that much of redundancy and methodological irregularities still remain in this 
field  partly  due  to  its  interdisciplinary  nature.  however,  during  the  last  decade,  significant 
steps have been taken towards the right direction. from a marginal scientific area dealing only 
with famous cases of disputed or unknown authorship of literary works, authorship attribution 
provides  now  robust  methods  able  to  handle  real-world  texts  with  relatively  high  accuracy 
results. fully-automated approaches can give reliable solutions in a number of applications of 
the internet era (e.g., analysis of e-mails, blogs, online forum messages, etc.) to this end, this 
area has taken advantage of recent advances in information retrieval, machine learning, and 
natural language processing.  

authorship  attribution  can  be  viewed  as  a  typical  text  categorization  task  and actually 
several  researchers  develop  general  text  categorization  techniques  and  evaluate  them  on 
authorship  attribution  together  with  other  tasks,  such  as  topic  identification,  language 
identification, genre detection, etc. (benedetto, et al., 2002; teahan & harper, 2003; peng, et 
al.,  2004;  marton,  et  al.,  2005;  zhang  &  lee,  2006)  however,  there  are  some  important 
characteristics  that  distinguish  authorship  attribution  from  other  text  categorization  tasks. 
first,  in  style-based  text  categorization,  the  most  significant  features  are  the  most  frequent 
ones (houvardas & stamatatos, 2006; koppel, akiva, & dagan, 2006) while in topic-based 
text categorization the best features should be selected based on their discriminatory power 
(forman, 2003). second, in authorship attribution tasks, especially in forensic applications, 
there is  extremely limited training text  material  while in  most text categorization problems 
(e.g., topic identification, genre detection) there is plenty of both labeled and unlabeled (that 
can be  manually labeled) data. hence,  it is crucial for the attribution methods to be robust 
with  a  limited  amount  of  short  texts.  moreover,  in  most  of  the  cases  the  distribution  of 
training  texts  over  the  candidate  authors  is  imbalanced.  in  such  cases,  the  evaluation  of 
authorship  attribution  methods  should  not  follow  the  practice  of  other  text  categorization 
tasks, that is, the test corpus follows the distribution of training corpus (see section 4). on the 
contrary, the test corpus should be balanced. this is the most appropriate evaluation method 
for most of the authorship attribution applications (e.g., intelligence, criminal law, forensics, 
etc.) note that this does not necessarily stand for other style-based text categorization tasks, 
such as genre detection.  

several  crucial  questions  remain  open  for  the authorship  attribution  problem.  perhaps, 
the  most  important  issue  is  the  text-length:  how  long  should  a  text  be  so  that  we  can 
adequately  capture  its  stylistic  properties?  various  studies  have  reported  promising  results 
dealing with short texts (with less than 1,000 words) (sanderson & guenter, 2006; hirst & 
feguina,  2007).  however,  it  is  not  yet  possible  to  define  such  a  text-length  threshold. 
moreover,  it  is  not  yet  clear  whether  other  factors  (beyond  text-length)  also  affect  this 
process. for example, let a and b be two texts of 100 words and 1,000 words, respectively. a 
given authorship attribution tool can easily identify the author of a but not the author of b. 
what are the properties of a that make it an easy case and what makes b so difficult albeit 
much longer than a? on the other hand, what are the minimum requirements in training text 
we need to be able to identify the author of a given text? 

another  important  question  is  how  to  discriminate  between  the  three  basic  factors: 
authorship,  genre,  and  topic.  are  there  specific  stylometric  features  that  can  capture  only 
stylistic, and specifically authorial, information? several features described in section 2 are 
claimed to capture only stylistic information (e.g., function words). however, the application 

 

23 

thematic 

of stylometric features to topic-identification tasks has revealed the potential of these features 
to indicate content information as well (clement & sharp, 2003; mikros & argiri, 2007). it 
seems that low-level features like character id165s are very successful for representing texts 
for  stylistic  purposes  (peng,  et  al.,  2003;  keselj,  et  al.,  2003;  stamatatos,  2006b;  grieve, 
2007).  recall  that  the  compression-based  techniques  operate  also  on  the  character  level. 
however,  these  features  unavoidably  capture  thematic  information  as  well.  is  it  the 
combination  of  stylistic  and 
them  so  powerful 
discriminators?  

that  makes 

information 

more elaborate features, capturing syntactic or semantic information are not yet able to 
represent  adequately  the  stylistic  choices  of  texts.  hence,  they  can  only  be  used  as 
complement in other more powerful features coming from the lexical or the character level. 
perhaps,  the noise  introduced  by  the  nlp  tools in  the  process  of  their  extraction  to  be  the 
crucial  factor  for  their  failure.  it  remains  to  be  seen  whether  nlp  technology  can  provide 
even  more  accurate  and  reliable  tools  to  be  used  for  stylometric  purposes.  moreover, 
distributional  features  (karlgren  &  eriksson,  2007)  should  be  thoroughly  examined,  since 
they can represent detailed sequential patterns of authorial style rather than mere frequencies 
of occurrence. 

the accuracy of current authorship attribution technology depends mainly on the number 
of  candidate  authors,  the  size  of  texts,  and  the  amount  of  training  texts.  however,  this 
technology  is  not  yet  reliable  enough  to  meet  the  court  standards  in  forensic  cases.  an 
important obstacle is that it is not yet possible to explain the differences between the authors    
style.  it  is  possible  to  estimate  the  significance  of  certain  (usually  character  or  lexical) 
features for specific authors. but what we need is a higher level abstract description of the 
authorial  style.  moreover,  in  the  framework  of  forensic  applications,  the  open-set 
classification setting is the most suitable (i.e., the true author is not necessarily included in the 
set  of  candidate  authors).  most  of  the authorship  attribution  studies  consider  the  closed-set 
case (i.e., the true author should be one of the candidate authors). additionally, in the open-
set  case,  apart  of  measuring  the  accuracy  of  the  decisions  of  the  attribution  model,  special 
attention  must  be  paid  to  the  confidence  of  those  decisions  (i.e.,  how  sure  it  is  that  the 
selected  author  is  the  true  author  of  the  text).  another  line  of  research  that  has  not  been 
adequately examined so far is the development of robust attribution techniques that can be 
trained on texts from one genre and applied to texts of another genre by the same authors. 
this  is  quite  useful  especially  for  forensic  applications.  for  instance,  it  is  possible  to  have 
blog  postings  for  training  and  a  harassing  e-mail  message  for  test  or  business  letters  for 
training and a suicide note for test (juola, 2007). 

a significant advance of the authorship attribution technology during the last years was 
the adoption of objective evaluation criteria and the comparison of different methodologies 
using the same benchmark corpora, following the practice of thematic text categorization. a 
crucial issue is to increase the available benchmark corpora so that they cover many natural 
languages  and  text  domains.  it  is  also  very  important  for  the  evaluation  corpora  to  offer 
control over genre, topic and demographic criteria. to that end, it would be extremely useful 
to establish periodic events including competitions of authorship attribution methods (juola, 
2004). such competitions should comprise multiple tasks that cover a variety of problems in 
the  style  of  text  retrieval  conferences5.  this  is  the  fastest  way  to  develop  authorship 
attribution research and provide commercial applications. 

acknowledgement 
the author wishes to thank the anonymous jasist reviewers for their valuable and insightful 
comments. 

                                                      
5 http://trec.nist.gov/ 

 

24 

references  
abbasi, a., & chen, h. (2005). applying authorship analysis to extremist-group web forum messages. 

argamon,  s.  (2008).  interpreting  burrows     delta:  geometric  and  probabilistic  foundations.  literary 

ieee intelligent systems, 20(5), 67-75.  

and linguistic computing, 23(2), 131-147. 

argamon,  s.,  &  levitan,  s.  (2005).  measuring  the  usefulness  of  function  words  for  authorship 
attribution.  in  proceedings  of  the  joint  conference  of  the  association  for  computers  and  the 
humanities and the association for literary and linguistic computing. 

argamon, s., saric, m., & stein, s. (2003). style mining of electronic messages for multiple authorship 

discrimination: first results. in proceedings of the 9th acm sigkdd (pp. 475-480). 

argamon,  s.,  whitelaw,  c.,  chase,  p.,  hota,  s.r.,  garg,  n.,  &  levitan,  s.  (2007).  stylistic  text 
classification using functional lexical features. journal of the american society for information 
science and technology, 58(6), 802-822. 

argamon-engelson,  s.,  koppel,  m.,  &  avneri,  g.  (1998).  style-based  text  categorization:  what 
newspaper  am  i  reading?,  in  proceedings  of  aaai  workshop  on  learning  for  text 
categorization (pp. 1-4). 

baayen, r., van halteren, h., neijt, a., & tweedie, f. (2002). an experiment in authorship attribution. 
in  proceedings  of  jadt  2002:  sixth  international  conference  on  textual  data  statistical 
analysis (pp. 29-37). 

baayen,  r.,  van  halteren,  h.,  &  tweedie,  f.  (1996).  outside  the  cave  of  shadows:  using  syntactic 
annotation to enhance authorship attribution. literary and linguistic computing, 11(3), 121   131. 
benedetto, d., caglioti, e., & loreto, v. (2002). language trees and zipping. physical review letters, 

88(4), 048702. 

binongo,  j.  (2003).  who  wrote  the  15th  book  of  oz?  an  application  of  multivariate  analysis  to 

authorship attribution. chance, 16(2), 9-17. 

brank, j., grobelnik, m., milic-frayling, n., & mladenic, d. (2002). interaction of feature selection 
methods  and  linear  classification  models.  in  proceedings  of  the  icml-02  workshop  on  text 
learning. 

burrows,  j.f.  (1987).  word  patterns  and  story  shapes:  the  statistical  analysis  of  narrative  style. 

literary and linguistic computing, 2, 61-70. 

burrows,  j.f.  (1992).  not  unless  you  ask  nicely:  the  interpretative  nexus  between  analysis  and 

information. literary and linguistic computing, 7(2), 91   109. 

burrows,  j.f.  (2002).     delta   :  a  measure  of  stylistic  difference  and  a  guide  to  likely  authorship. 

literary and linguistic computing, 17(3), 267-287. 

can, f., & patton, j.m. (2004). change of writing style with time. computers and the humanities, 38, 

chaski,  c.e.  (2001).  empirical  evaluations  of  language-based  author  identification  techniques. 

61-82. 

forensic linguistics, 8(1), 1-65. 

chaski, c.e. (2005). who   s at the keyboard? authorship attribution in digital evidence investigations. 

international journal of digital evidence, 4(1). 

cilibrasi r., & vitanyi p.m.b. (2005). id91 by compression. ieee transactions on information 

theory, 51(4), 1523-1545. 

clement,  r.,  &  sharp,  d.  (2003).  ngram  and  bayesian  classification  of  documents  for  topic  and 

authorship. literary and linguistic computing, 18(4), 423-447. 

collins, j., kaufer, d., vlachos, p., butler, b., & ishizaki, s. (2004). detecting collaborations in text: 
comparing the authors    rhetorical language choices in the federalist papers. computers and the 
humanities, 38, 15-36. 

coyotl-morales, r.m., villase  or-pineda, l., montes-y-g  mez, m., & rosso, p. (2006). authorship 
attribution using word sequences. in proceedings of the 11th iberoamerican congress on pattern 
recognition (pp. 844-853) springer. 

deerwester, s., dumais, s., furnas, g.w., landauer, t. k., & harshman r. (1990). indexing by latent 

semantic analysis. journal of the american society for information science 41(6), 391-407. 

diederich,  j.,  kindermann,  j.,  leopold,  e.,  &  paass,  g.  (2003).  authorship  attribution  with  support 

vector machines. applied intelligence, 19(1/2), 109-123. 

fellbaum, c. (1998). id138: an electronic lexical database. cambridge: mit press. 
forman,  g.  (2003).  an  extensive  empirical  study  of  feature  selection  metrics  for  text  classification. 

journal of machine learning research, 3, 1289-1305. 

forsyth,  r.,  &  holmes,  d.  (1996).  feature-finding  for  text  classification.  literary  and  linguistic 

computing, 11(4), 163-174. 

 

25 

frantzeskou, g., stamatatos, e., gritzalis, s., & katsikas, s. (2006). effective identification of source 
code authors using byte-level information. in proceedings of the 28th international conference on 
software engineering (pp. 893-896). 

gamon,  m.  (2004).  linguistic  correlates  of  style:  authorship  classification  with  deep  linguistic 
analysis  features.  in  proceedings  of  the  20th  international  conference  on  computational 
linguistics (pp. 611-617). 

goodman,  j.  (2002).  extended  comment  on  language  trees  and  zipping.  http://arxiv.org/abs/cond-

mat/0202383. 

graham, n., hirst, g., & marthi, b. (2005). segmenting documents by stylistic character. journal of 

natural language engineering, 11(4), 397-415. 

grant,  t.  d.  (2007).  quantifying  evidence  for  forensic  authorship  analysis.  international  journal  of 

grieve,  j.  (2007).  quantitative  authorship  attribution:  an  evaluation  of  techniques.  literary  and 

speech language and the law, 14(1), 1 -25. 

linguistic computing, 22(3), 251-270. 

halliday, m.a.k. (1994). introduction to functional grammar (2nd ed.). london: arnold. 
van halteren, h. (2007). author verification by linguistic profiling: an exploration of the parameter 

space. acm transactions on speech and language processing, 4(1), 1-17. 

holmes, d.i. (1994). authorship attribution. computers and the humanities, 28, 87   106. 
holmes, d.i. (1998). the evolution of stylometry  in humanities scholarship. literary and linguistic 

computing, 13(3), 111-117.  

holmes, d.i., & forsyth, r. (1995). the federelist revisited: new directions in authorship attribution. 

literary and linguistic computing, 10(2), 111-127. 

holmes, d.i., & tweedie, f. j. (1995). forensic stylometry: a review of the cusum controversy. in 

revue informatique et statistique dans les sciences humaines. university of liege (pp. 19-47). 

honore,  a.  (1979).  some  simple  measures  of  richness  of  vocabulary.  association  for  literary  and 

linguistic computing bulletin, 7(2), 172   177. 

hoover, d. (2004a). testing burrows    delta. literary and linguistic computing, 19(4), 453-475. 
hoover, d. (2004b). delta prime? literary and linguistic computing, 19(4), 477-495. 
houvardas,  j.,  &  stamatatos  e.  (2006).  id165  feature  selection  for  authorship  identification.  in 
proceedings  of  the  12th  international  conference  on  artificial  intelligence:  methodology, 
systems, applications, (pp. 77-86), springer. 

joachims, t. (1998). text categorization with support vector machines: learning with many relevant 

features. in proceedings of the 10th european conference on machine learning (pp. 137-142). 

juola, p. (2004). ad-hoc authorship attribution competition. in proceedings of the joint conference of 
the association for computers and the humanities and the association for literary and linguistic 
computing (pp. 175-176). 

juola, p. (2006). authorship attribution for electronic documents. in m. olivier and s. shenoi (eds.) 

advances in digital forensics ii (pp. 119-130) springer. 

juola, p. (2007). future trends in authorship attribution. in p. craiger & s. shenoi (eds.) advances in 

digital forensics iii (pp. 119-132) springer. 

juola,  p.,  &  baayen,  r.  (2005).  a  controlled-corpus  experiment  in  authorship  attribution  by  cross-

id178. literary and linguistic computing, 20, 59-67. 

karlgren, j., & eriksson g. (2007). authors, genre, and linguistic convention. in proceedings of the 
sigir workshop on plagiarism analysis, authorship attribution, and near-duplicate detection 
(pp. 23-28). 

keselj, v., peng, f., cercone, n., & thomas, c. (2003). id165-based author profiles for authorship 
attribution.  in  proceedings  of  the  pacific  association  for  computational  linguistics  (pp.  255-
264). 

khmelev, d.v., & teahan, w.j. (2003a). a repetition based measure for verification of text collections 

and for text categorization. in proceedings of the 26th acm sigir, (pp. 104   110). 

khmelev, d.v., & teahan, w. j. (2003b). comment:    language trees and zipping   . physical review 

letters, 90, 089803. 

khosmood,  f.,  &  levinson,  r.  (2006).  toward  unification  of  source  attribution  processes  and 
techniques.  in  proceedings  of  the  fifth  international  conference  on  machine  learning  and 
cybernetics (pp. 4551-4556). 

kjell,  b.  (1994).  discrimination  of  authorship  using  visualization.  information  processing  and 

kohavi, r., & john, g. (1997). wrappers for feature subset selection. artificial intelligence 97(1-2), 

management, 30(1), 141-150. 

273-324. 

 

26 

koppel, m., akiva, n., & dagan, i. (2006). feature instability as a criterion for selecting potential style 
markers. journal of the american society for information science and technology, 57(11),1519   
1525. 

koppel, m., argamon, s., & shimoni, a.r. (2002). automatically categorizing written texts by author 

gender. literary and linguistic computing, 17(4), pp. 401-412. 

koppel,  m.,  &  schler,  j.  (2003).  exploiting  stylistic  idiosyncrasies  for  authorship  attribution.  in 
proceedings  of  ijcai'03  workshop  on  computational  approaches  to  style  analysis  and 
synthesis (pp. 69-72). 

koppel,  m.,  &  schler,  j.  (2004).  authorship  verification  as  a  one-class  classification  problem.  in 

proceedings of the 21st international conference on machine learning. 

koppel, m., schler, j., argamon, s., & messeri, e. (2006). authorship attribution with thousands of 

candidate authors. in  proceedings of the 29th acm sigir (pp. 659-660). 

koppel,  m.,  schler,  j.,  &  bonchek-dokow,  e.  (2007).  measuring  differentiability:  unmasking 

pseudonymous authors. journal of machine learning research, 8, 1261-1276. 

kukushkina, o.v., polikarpov, a.a., & khmelev, d.v. (2001). using literal and grammatical statistics 

for authorship attribution. problems of information transmission, 37(2), 172-184. 

li, j., zheng, r., & chen, h. (2006). from fingerprint to writeprint. communications of the acm, 

49(4), 76   82. 

luyckx,  k.,  &  daelemans,  w.  (2005).  shallow  text  analysis  and  machine  learning  for  authorship 
attribution.  in  proceedings  of  the  fifteenth  meeting  of  computational  linguistics  in  the 
netherlands. 

madigan, d., genkin, a., lewis, d., argamon, s., fradkin, d., & ye, l. (2005). author identification 

on the large scale. in proceedings of csna-05. 

marton, y., wu, n., & hellerstein, l. (2005). on compression-based text classification. in proceedings 

of the european conference on information retrieval (pp. 300   314) springer. 

matthews, r., & merriam, t. (1993), neural computation in stylometry: an application to the works of 

shakespeare and fletcher. literary and linguistic computing, 8(4), 203-209. 

matsuura,  t.,  &  kanada,  y.  (2000).  extraction  of  authors     characteristics  from  japanese  modern 
sentences  via  id165  distribution.  in  proceedings  of  the  3rd  international  conference  on 
discovery science (pp. 315-319) springer. 

mccarthy, p.m., lewis, g.a., dufty, d.f., & mcnamara, d.s. (2006) analyzing writing styles with 
coh-metrix.  in  proceedings  of  the  florida  artificial  intelligence  research  society  international 
conference (pp. 764-769). 

mendenhall, t. c. (1887). the characteristic curves of composition. science, ix, 237   49. 
merriam, t. & matthews, r. (1994), neural compuation in stylometry ii: an application to the works 

of shakespeare and marlowe. literary and linguistic computing, 9(1), 1-6. 

meyer zu eissen, s., stein, b., & kulig, m. (2007). plagiarism detection without reference collections. 

advances in data analysis (pp. 359-366) springer. 

mikros, g, & argyri, e. (2007). investigating topic influence in authorship attribution. in proceedings 
of  the  international  workshop  on  plagiarism  analysis,  authorship  identification,  and  near-
duplicate detection (pp. 29-35). 

mitchell, t. (1997). machine learning. mcgraw-hill. 
morton,  a.q.,  &  michaelson,  s.  (1990).  the  qsum  plot.  technical  report  csr-3-90,  university  of 

edinburgh. 

wesley. 

mosteller,  f.  &  wallace,  d.l.  (1964).  id136  and  disputed  authorship:  the  federalist.  addison-

peng, f., shuurmans, d., keselj, v., & wang, s. (2003). language independent authorship attribution 
using id186. in proceedings of the 10th conference of the european 
chapter of the association for computational linguistics (pp. 267-274). 

peng,  f.,  shuurmans,  d.,  &  wang,  s.  (2004).  augmenting  naive  bayes  classifiers  with  statistical 

language models. information retrieval journal, 7(1), 317-345. 

rudman,  j.  (1998).  the  state  of  authorship  attribution  studies:  some  problems  and  solutions. 

computers and the humanities, 31, 351-365. 

sanderson, c., & guenter, s. (2006). short text authorship attribution via sequence kernels, markov 
chains and author unmasking: an investigation. in proceedings of the international conference on 
empirical methods in natural language engineering (pp. 482-491). 

sebastiani, f. (2002). machine learning in automated text categorization. acm computing surveys, 

stamatatos, e. (2006a). authorship attribution based on feature set subspacing ensembles. international 

journal on artificial intelligence tools, 15(5), 823-838. 

34(1). 

 

27 

stamatatos, e. (2006b). ensemble-based author identification using character id165s. in proceedings 

of the 3rd international workshop on text-based information retrieval (pp. 41-46). 

stamatatos,  e.  (2007).  author  identification  using  imbalanced  and  limited  training  texts.  in 
proceedings  of  the  4th  international  workshop  on  text-based  information  retrieval  (pp.  237-
241). 

stamatatos,  e.  (2008).  author  identification:  using  text  sampling  to  handle  the  class  imbalance 

problem. information processing and management, 44(2), 790-799. 

stamatatos,  e.,  fakotakis,  n.,  &  kokkinakis,  g.  (2000).  automatic  text  categorization  in  terms  of 

genre and author. computational linguistics, 26(4), 471   495, 2000. 

stamatatos, e., fakotakis, n., & kokkinakis, g. (2001). computer-based authorship attribution without 

lexical measures. computers and the humanities, 35(2), 193-214. 

stein,  b.,  &  meyer  zu  eissen,  s.  (2007).  intrinsic  plagiarism  analysis  with  meta  learning.  in 
proceedings of the sigir workshop on plagiarism analysis, authorship attribution, and near-
duplicate detection (pp.45-50). 

tambouratzis, g., markantonatou, s., hairetakis, n., vassiliou, m., carayannis, g., & tambouratzis, 
d.  (2004).  discriminating  the  registers  and  styles  in  the  modern  greek  language       part  2: 
extending  the  feature  vector  to  optimize  author  discrimination.  literary  and  linguistic 
computing, 19(2), 221-242. 

teahan, w., & harper, d. (2003). using compression-based language models for text categorization. 

in w.b. croft & j. lafferty (eds) id38 and information retrieval, 141   165. 

teng,  g.,  lai,  m.,  ma,  j.,  &  li,  y.  (2004).  e-mail  authorship  mining  based  on  id166  for  computer 
forensic. in proceedings of the international conference on machine learning and cybernetics, 2 
(pp. 1204-1207). 

tweedie, f., & baayen, r. (1998). how variable may a constant be? measures of lexical richness in 

perspective. computers and the humanities, 32(5), 323   352. 

tweedie,  f.,  singh,  s.,  &  holmes,  d.  (1996).  neural  network  applications  in  stylometry:  the 

federalist papers. computers and the humanities, 30(1), 1-10. 

uzuner,  o.,  &  katz,  b.  (2005).  a  comparative  study  of  language  models  for  book  and  author 
recognition.  in  proceedings  of  the  2nd  international  joint  conference  on  natural  language 
processing (pp. 969-980) springer. 

de  vel,  o.,  anderson,  a.,  corney,  m.,  &  mohay,  g.  (2001).  mining  e-mail  content  for  author 

identification forensics. sigmod record, 30(4), 55-64.  

yule, g.u. (1938). on sentence-length as a statistical characteristic of style in prose, with application 

to two cases of disputed authorship. biometrika, 30, 363-390. 

yule, g.u. (1944). the statistical study of literary vocabulary. cambridge university press.  
zhang,  d.,  &  lee,  w.s.  (2006).  extracting  key-substring-group  features  for  text  classification.  in 
proceedings of the 12th annual sigkdd international conference on knowledge discovery and 
data mining (pp. 474-483). 

zhao  y.,  &  zobel,  j.  (2005).  effective  and  scalable  authorship  attribution  using  function  words.  in 

proceedings of the 2nd asia information retrieval symposium. 

zhao  y.,  &  zobel,  j.  (2007).  searching  with  style:  authorship  attribution  in  classic  literature.  in 
proceedings of the thirtieth australasian computer science conference (pp. 59-68) acm press. 
zheng, r., li, j., chen, h., & huang, z. (2006). a framework for authorship identification of online 
messages: writing style features and classification techniques. journal of the american society of 
information science and technology, 57(3), 378-393. 

zipf,  g.k.  (1932).  selected  studies  of  the  principle  of  relative  frequency  in  language.  harvard 

university press, cambridge, ma. 

 

28 

