computational psycholinguistics 

integrating nlp modeling and experimental psycholinguistics to 

investigate real-time human language use 

roger levy

 klinton bicknell

 nathaniel smith 

departments of linguistics & cognitive science 

university of california     san diego 

naacl-hlt 2010 tutorial 

slides available at http://grammar.ucsd.edu/cpl/naacl2010tutorial.html 

basic desiderata 

       realistic models of human sentence 

processing must account for 
       robustness to arbitrary input 
       accurate disambiguation 
       id136 on basis of incomplete input 
(tanenhaus et al 1995, altmann and kamide 
1999, kaiser and trueswell 2004) 
       processing difficulty is differential and 

localized 

incrementality and rationality 

       ambiguity a major problem in language understanding 
       parsing: sentence w     distribution on structures p(t|w)	

       incremental parsing: for a partial sentence w1   i, assign a 
distribution over partial structures that implies p(t|w1   i)   
       lots of evidence that people use diverse information 

sources to perform incremental parsing rationally 

   the boy will eat       

       today: give you an intro to the computational & empirical 

picture 

contents of the day   s talk 

       review of exact incremental id136 techniques for 

pid18s  

       overview of key results in modeling ambiguity resolution 

and expectation-based facilitation in online 
comprehension 

       coffee break 
       rational, probabilistic id136 & speaker choice in 

language production 

       cognitive limitations and approximate id136 in online 

comprehension 

       additional theoretical challenges, bounded rationality, 

input uncertainty 

       summary, open issues, future directions, questions 

pid18s and the earley algorithm

computational psycholinguistics tutorial

naacl 2010

context-free grammars

a context-free grammar (id18) consists of a tuple (n, v , s, r)
such that:

    n is a    nite set of non-terminal symbols;
    v is a    nite set of terminal symbols;
    s is the start symbol;
    r is a    nite set of rules of the form x        where x     n

and    is a sequence of symbols drawn from n     v .

a id18 derivation is the recursive expansion of non-terminal
symbols into a string of terminal symbols by rules in r, starting
with s, and a derivation tree t is the history of those rule
applications.

context-free grammars: an example

let our grammar (the rule-set r) be
s    np vp
np   det n
np   np pp
pp   p np
vp   v
the nonterminal set n is {s, np, vp, det , n, p, v }, the
terminal set v is {the, dog, cat , near , growled}, and our start
symbol s is s.

det    the
n     dog
n     cat
p     near
v     growled

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

vp

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

vp

np

pp

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

vp

np

pp

det

n

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

vp

np

pp

det

n

the

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

vp

np

pp

det

n

the

dog

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

vp

np

pp

det

n

p

np

the

dog

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

vp

np

pp

det

n

p

np

the

dog

near

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

vp

np

pp

det

n

p

np

the

dog

near

det

n

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

vp

np

pp

det

n

p

np

the

dog

near

det

n

the

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

vp

np

pp

det

n

p

np

the

dog

near

det

n

the

cat

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

np

pp

vp

v

det

n

p

np

the

dog

near

det

n

the

cat

context-free grammars: an example ii

s    np vp
np   det n
np   np pp
pp   p np
vp   v

det    the
n     dog
n     cat
p     near
v     growled

here is a derivation and the resulting derivation tree:

s

np

np

pp

vp

v

det

n

p

np

growled

the

dog

near

det

n

the

cat

id140

a probabilistic context-free grammar (pid18) consists of a tuple
(n, v , s, r, p) such that:

    n is a    nite set of non-terminal symbols;
    v is a    nite set of terminal symbols;
    s is the start symbol;
    r is a    nite set of rules of the form x        where x     n

and    is a sequence of symbols drawn from n     v ;

    p is a mapping from r into probabilities, such that for each

x     n,

x

[x      ]   r

p(x       ) = 1

the id203 p(t ) of a derivation tree is simply the product of
the probabilities of each rule application.

example pid18

1
s    np vp
0.8 np    det n
0.2 np    np pp
1
1

pp    p np
vp    v

s

0.2

np

np

0.8
det

p

n

0.5

pp

np

the

dog

near

det

0.8
n

0.5

the

cat

1
det     the
0.5 n     dog
0.5 n     cat
1
1

p     near
v     growled

vp

v

growled

p(t) = 1    0.2    0.8    1    0.5    0.8    1      0.8    1    0.5    1    1

= 0.032

pid18 review (2)

    we just learned how to calculate the id203 of a tree

    the id203 of a string w1      n is the sum of the probabilities of

all trees whose yield is w1      n

    the id203 of a string pre   x w1      i is the sum of the
probabilities of all trees whose yield begins with w1      i

    if we had the probabilities of two string pre   xes w1      i   1 and

w1      i, we could calculate the id155 p(wi |w1      i   1)
as their ratio:

p(wi |w1...i   1) =

p(w1...i )
p(w1...i   1)

id136 over in   nite tree sets

consider the following noun-phrase grammar:

2
3 np     det n
1
3 np     np pp
1 pp     p np

1 det     the
2
3 n     dog
1
3 n     cat
1 p     near

id136 over in   nite tree sets

consider the following noun-phrase grammar:

2
3 np     det n
1
3 np     np pp
1 pp     p np

question: given a sentence starting with

the. . .

1 det     the
2
3 n     dog
1
3 n     cat
1 p     near

what is the id203 that the next word is dog?

id136 over in   nite tree sets

consider the following noun-phrase grammar:

2
3 np     det n
1
3 np     np pp
1 pp     p np

question: given a sentence starting with

the. . .

1 det     the
2
3 n     dog
1
3 n     cat
1 p     near

what is the id203 that the next word is dog?
intuitively, the answers to this question should be

p(dog|the) =

2
3

id136 over in   nite tree sets

consider the following noun-phrase grammar:

2
3 np     det n
1
3 np     np pp
1 pp     p np

question: given a sentence starting with

the. . .

1 det     the
2
3 n     dog
1
3 n     cat
1 p     near

what is the id203 that the next word is dog?
intuitively, the answers to this question should be

p(dog|the) =

2
3

because the second word has to be either dog or cat.

id136 over in   nite tree sets (2)

2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
p     near
1

    we    should    just enumerate the trees that cover the dog . . . ,

id136 over in   nite tree sets (2)

2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
p     near
1

    we    should    just enumerate the trees that cover the dog . . . , and

divide their total id203 by that of the . . .

id136 over in   nite tree sets (2)

2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
p     near
1

    we    should    just enumerate the trees that cover the dog . . . , and

divide their total id203 by that of the . . .

    . . . but there are in   nitely many trees.

id136 over in   nite tree sets (2)

2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
p     near
1

    we    should    just enumerate the trees that cover the dog . . . , and

divide their total id203 by that of the . . .

    . . . but there are in   nitely many trees.

np

np

np

np

. . .

det

n

np

pp

np

pp

np

pp

the

dog

det

n

np

pp

np

pp

the

dog

det

n

np

pp

the

dog

det

n

the

dog

2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
1
p     near

shortcut 1: you can think of a partial tree as marginalizing over all
completions of the partial tree.
it has a corresponding marginal id203 in the pid18.

np

det

the

n

dog

4
9

np

np

np

pp

np

pp

det

the

n

p

np

dog

near

det

n

det

the

n

p

np

dog

near

det

n

the

dog

8
81

the

cat

4
81

2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
1
p     near

shortcut 1: you can think of a partial tree as marginalizing over all
completions of the partial tree.
it has a corresponding marginal id203 in the pid18.

np

det

the

n

dog

np

np

np

pp

np

pp

det

the

n

p

np

dog

near

det

n

det

the

n

p

np

dog

near

det

n

the

dog

8
81

the

cat

4
81

4
9

np

np

pp

det

the

n

p

np

dog

near

det

n

the

12
81

2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
1
p     near

shortcut 1: you can think of a partial tree as marginalizing over all
completions of the partial tree.
it has a corresponding marginal id203 in the pid18.

np

det

the

n

dog

np

np

np

pp

np

pp

det

the

n

p

np

dog

near

det

n

det

the

n

p

np

dog

near

det

n

4
9

np

np

pp

det

the

n

p

np

dog

near

det

n

the

12
81

the

dog

8
81

np

np

pp

det

the

n

dog

4
27

the

cat

4
81

problem 2: there are still an in   nite number of incomplete trees
covering a partial input.
np

np

np

np

pp

np

pp

np

pp

np

pp

np

np

pp

det

the

n

dog

det

the

n

dog

det

the

n

dog

np

pp

det

the

n

dog

4
243

4
9

4
27

4
81

problem 2: there are still an in   nite number of incomplete trees
covering a partial input.
np

np

np

np

pp

np

pp

np

pp

np

pp

np

np

pp

det

the

n

dog

det

the

n

dog

det

the

n

dog

np

pp

det

the

n

dog

4
243

4
9

4
27

4
81

but! these tree probabilities form a geometric series:

p(the dog . . .) =

4
9 +

4
27 +

4
81 +

4
243 +         

problem 2: there are still an in   nite number of incomplete trees
covering a partial input.
np

np

np

np

pp

np

pp

np

pp

np

pp

np

np

pp

det

the

n

dog

det

the

n

dog

det

the

n

dog

np

pp

det

the

n

dog

4
243

4
9

4
27

4
81

but! these tree probabilities form a geometric series:

p(the dog . . .) =

=

4
9 +
4
9

   

x
i=0

4
27 +
1
3  i

`

4
81 +

4
243 +         

problem 2: there are still an in   nite number of incomplete trees
covering a partial input.
np

np

np

np

pp

np

pp

np

pp

np

pp

np

np

pp

det

the

n

dog

det

the

n

dog

det

the

n

dog

np

pp

det

the

n

dog

4
243

4
9

4
27

4
81

but! these tree probabilities form a geometric series:

4
81 +

4
243 +         

p(the dog . . .) =

=

=

4
9 +
4
9

   

x
i=0

4
27 +
1
3  i

`

2
3

problem 2: there are still an in   nite number of incomplete trees
covering a partial input.
np

np

np

np

pp

np

pp

np

pp

np

pp

np

np

pp

det

the

n

dog

det

the

n

dog

det

the

n

dog

np

pp

det

the

n

dog

4
243

4
9

4
27

4
81

but! these tree probabilities form a geometric series:

4
81 +

4
243 +         

p(the dog . . .) =

=

=

4
9 +
4
9

   

x
i=0

4
27 +
1
3  i

`

2
3

. . . which matches the original rule id203

2
3 n     dog

generalizing the geometric series induced by rule
recursion

in general, these in   nite tree sets arise due to left recursion in a
probabilistic grammar

a     b   

b     a   

(stolcke, 1995)

generalizing the geometric series induced by rule
recursion

in general, these in   nite tree sets arise due to left recursion in a
probabilistic grammar

a     b   

b     a   

we can formulate a stochastic left-corner matrix of transitions
between categories:

a

b

. . . k

pl =

a 0.3 0.7         
0
b 0.1 0.1          0.2
...
...
k 0.2 0.1          0.2

...

...

...

(stolcke, 1995)

generalizing the geometric series induced by rule
recursion

in general, these in   nite tree sets arise due to left recursion in a
probabilistic grammar

a     b   

b     a   

we can formulate a stochastic left-corner matrix of transitions
between categories:

a

b

. . . k

pl =

a 0.3 0.7         
0
b 0.1 0.1          0.2
...
...
k 0.2 0.1          0.2

...

...

...

and solve for its closure rl = (i     pl)   1.

(stolcke, 1995)

generalizing the geometric series

1 root     np
2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
p     near
1

    the closure of our left-corner matrix is

rl =

root
np
pp
det
n
p

   

                     

root np pp det n p
0 0
0 0
0 1
0 0
1 0
0 1

3
2
3
2
0
0
0
0

1
0
0
0
0
0

0
0
1
0
0
0

1
1
0
1
0
0

   

                     

generalizing the geometric series

1 root     np
2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
p     near
1

    the closure of our left-corner matrix is

rl =

root
np
pp
det
n
p

   

                     

root np pp det n p
0 0
0 0
0 1
0 0
1 0
0 1

3
2
3
2
0
0
0
0

1
0
0
0
0
0

0
0
1
0
0
0

1
1
0
1
0
0

   

                     

    refer to an entry (x , y ) in this matrix as r(x       l y )

generalizing the geometric series

1 root     np
2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
p     near
1

    the closure of our left-corner matrix is

rl =

root
np
pp
det
n
p

   

                     

root np pp det n p
0 0
0 0
0 1
0 0
1 0
0 1

3
2
3
2
0
0
0
0

1
0
0
0
0
0

0
0
1
0
0
0

1
1
0
1
0
0

   

                     

    refer to an entry (x , y ) in this matrix as r(x       l y )
    note that the 3

2    bonus    accrued for left-recursion of nps
appears in the (root,np) and (np,np) cells of the matrix

generalizing the geometric series

1 root     np
2
3
1
3
1

np     det n
np     np pp
pp     p np

1 det     the
2
n     dog
3
1
n     cat
3
p     near
1

    the closure of our left-corner matrix is

rl =

root
np
pp
det
n
p

   

                     

root np pp det n p
0 0
0 0
0 1
0 0
1 0
0 1

3
2
3
2
0
0
0
0

1
0
0
0
0
0

0
0
1
0
0
0

1
1
0
1
0
0

   

                     

    refer to an entry (x , y ) in this matrix as r(x       l y )
    note that the 3

2    bonus    accrued for left-recursion of nps
appears in the (root,np) and (np,np) cells of the matrix
    we need to do the same with unary chains, constructing a

unary-closure matrix ru.

ef   cient incremental parsing: the probabilistic earley
algorithm

we can use the earley algorithm (earley, 1970) in a probabilistic
incarnation (stolcke, 1995) to deal with these in   nite tree sets.

the (slightly oversimpli   ed) probabilistic earley algorithm has two
fundamental types of operations:

    prediction: if y is a possible goal, and y can lead to z through

a left corner, choose a rule z        and set up    as a new
sequence of possible goals.

    completion: if y is a possible goal, y can lead to z through

unary rewrites, and we encounter a completed z , absorb it and
move on to the next sub-goal in the sequence.

ef   cient incremental parsing: the probabilistic earley
algorithm

    parsing consists of constructing a chart of states (items)

    a state has the following structure:

goal symbol

completed
subgoals

remaining
subgoals

x          
p
q

forward
id203

inside
id203

    the forward id203 is the total id203 of getting from the

root at the start of the sentence through to this state

    the inside id203 is the    bottom-up    id203 of the state

ef   cient incremental parsing: the probabilistic earley
algorithm

id136 rules for probabilistic earley:

    prediction:

x           y   
p

q

a : r(y       l z ) b : z       

z         
b

abp

ef   cient incremental parsing: the probabilistic earley
algorithm

id136 rules for probabilistic earley:

    prediction:

x           y   
p

q

    completion:

x           y   
p

q

a : r(y       l z ) b : z       

z         
b

abp

a : r(y       u z )
x      y       
acq
acp

z         
b

c

ef   cient incremental parsing: probabilistic earley

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

root      np
1

1

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

np   det   n
2
1
3
det   the   
1
1

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

np   det   n
2
1
3
det   the   
1
1

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

n      cat
1
3
n      dog
2
3

1
3

2
3

np   det   n
2
1
3
det   the   
1
1

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

n      cat
1
3
n      dog
2
3

1
3

2
3

np   det   n
2
1
3
det   the   
1
1

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

n      cat
1
3
n      dog
2
3

1
3

2
3

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

n      cat
1
3
n      dog
2
3

1
3

2
3

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

4
9

root   np   
4
9
np   np   pp
4
2
27
9
np   det n   
2
3

4
9

n      cat
1
3
n      dog
2
3

1
3

2
3

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

4
9

root   np   
4
9
np   np   pp
4
2
27
9
np   det n   
2
3

4
9

n      cat
1
3
n      dog
2
3

1
3

2
3

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

4
9

root   np   
4
9
np   np   pp
4
2
27
9
np   det n   
2
3

4
9

p      near
2
1
9
pp      p np
2
1
9

n      cat
1
3
n      dog
2
3

1
3

2
3

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

4
9

root   np   
4
9
np   np   pp
4
2
27
9
np   det n   
2
3

4
9

p      near
2
1
9
pp      p np
2
1
9

n      cat
1
3
n      dog
2
3

1
3

2
3

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

4
9

root   np   
4
9
np   np   pp
4
2
27
9
np   det n   
2
3

4
9

p      near
2
1
9
pp      p np
2
1
9

n      cat
1
3
n      dog
2
3

1
3

2
3

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

pp   p   np
2
1
9
p   near   
2
1
9

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

4
9

root   np   
4
9
np   np   pp
4
2
27
9
np   det n   
2
3

4
9

p      near
2
1
9
pp      p np
2
1
9

n      cat
1
3
n      dog
2
3

1
3

2
3

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

pp   p   np
2
1
9
p   near   
2
1
9

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

4
9

root   np   
4
9
np   np   pp
4
2
27
9
np   det n   
2
3

4
9

p      near
2
1
9
pp      p np
2
1
9

n      cat
1
3
n      dog
2
3

1
3

2
3

det      the
2
1
9
np      det n
3    3
2
np      np pp
3    3
2

1
3

2
3

9    2
2

2
9    1

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

pp   p   np
2
1
9
p   near   
2
1
9

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

4
9

root   np   
4
9
np   np   pp
4
2
27
9
np   det n   
2
3

4
9

p      near
2
1
9
pp      p np
2
1
9

n      cat
1
3
n      dog
2
3

1
3

2
3

det      the
2
1
9
np      det n
3    3
2
np      np pp
3    3
2

1
3

2
3

9    2
2

2
9    1

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

pp   p   np
2
1
9
p   near   
2
1
9

the

dog

near

the

ef   cient incremental parsing: probabilistic earley

2
3

det      the
1
1
np      det n
3    3
2
2
np      np pp
1
3    3
2
root      np
1

1

1
3

4
9

root   np   
4
9
np   np   pp
4
2
27
9
np   det n   
2
3

4
9

p      near
2
1
9
pp      p np
2
1
9

n      cat
1
3
n      dog
2
3

1
3

2
3

det      the
2
1
9
np      det n
3    3
2
np      np pp
3    3
2

1
3

2
3

9    2
2

2
9    1

np   det   n
2
1
3
det   the   
1
1

n   dog   
2
3

2
3

pp   p   np
2
1
9
p   near   
2
1
9

np   det   n
2
2
3
9
det   the   
2
1
9

the

dog

near

the

pre   x probabilities from probabilistic earley

    if you have just processed word wi, then the pre   x id203 of

w1...i can be obtained by summing all forward probabilities of
items that have the form x            wi   

pre   x probabilities from probabilistic earley

    if you have just processed word wi, then the pre   x id203 of

w1...i can be obtained by summing all forward probabilities of
items that have the form x            wi   

    in our example, we see:

p(the) = 1
p(the dog) = 2
3
p(the dog near) = 2
9
p(the dog near the) = 2
9

pre   x probabilities from probabilistic earley

    if you have just processed word wi, then the pre   x id203 of

w1...i can be obtained by summing all forward probabilities of
items that have the form x            wi   

    in our example, we see:

p(the) = 1
p(the dog) = 2
3
p(the dog near) = 2
9
p(the dog near the) = 2
9

    taking the ratios of these pre   x probabilities can give us

conditional word probabilities

probabilistic earley as an    eager    algorithm

    from the inside probabilities of the states on the chart, the
posterior distribution on (incremental) trees can be directly
calculated

    this posterior distribution is precisely the correct result of the

application of bayes    rule

    hence, probabilistic earley is also performing rational

disambiguation

    hale (2001) called this the    eager    property of an incremental

parsing algorithm.

probabilistic earley algorithm: key ideas

    we want to use probabilistic grammars for both disambiguation

and calculating id203 distributions over upcoming events

    in   nitely many trees can be constructed in polynomial time

(

) and space (

)

    the pre   x id203 of the string is calculated in the process

    by taking the log-ratio of two pre   x probabilities, the surprisal of

a word in its context can be calculated

probabilistic earley algorithm: key ideas

    we want to use probabilistic grammars for both disambiguation

and calculating id203 distributions over upcoming events

    in   nitely many trees can be constructed in polynomial time

(o(n3)) and space (

)

    the pre   x id203 of the string is calculated in the process

    by taking the log-ratio of two pre   x probabilities, the surprisal of

a word in its context can be calculated

probabilistic earley algorithm: key ideas

    we want to use probabilistic grammars for both disambiguation

and calculating id203 distributions over upcoming events

    in   nitely many trees can be constructed in polynomial time

(o(n3)) and space (o(n2))

    the pre   x id203 of the string is calculated in the process

    by taking the log-ratio of two pre   x probabilities, the surprisal of

a word in its context can be calculated

other introductions

    you can read about the (non-probabilistic) earley algorithm in

(jurafsky and martin, 2000, chapter 13)

    pre   x probabilities can also be calculated with an extension of

the cky algorithm due to jelinek and lafferty (1991)

references i

altmann, g. t. and kamide, y. (1999). incremental interpretation at
verbs: restricting the domain of subsequent reference. cognition,
73(3):247   264.

attneave, f. (1959). applications of id205 to

psychology: a summary of basic concepts, methods and results.
holt, rinehart and winston.

earley, j. (1970). an ef   cient context-free parsing algorithm.

communications of the acm, 13(2):94   102.

ehrlich, s. f. and rayner, k. (1981). contextual effects on word

perception and eye movements during reading. journal of verbal
learning and verbal behavior, 20:641   655.

grodner, d. and gibson, e. (2005). some consequences of the serial

nature of linguistic input. cognitive science, 29(2):261   290.

hale, j. (2001). a probabilistic earley parser as a psycholinguistic

model. in proceedings of the second meeting of the north
american chapter of the association for computational linguistics,
pages 159   166.

putting the psycho
in psycholinguistics

how can we use these models to 

study human language 

processing?

 

 

syntactic disambiguation

    consider: the women discussed the dogs on 

the beach
    is the discussion on the beach?

 

 

human forced-choice preference (ford et al., 1982)

the women discussed the 

dogs on the beach.

the women kept the dogs 

on the beach.

95%

90%

 

 

jurafsky, 1996

    a product of experts probabilistic parser

    model 1: a pid18
    model 2: a verb valency model (use your corpus to 

count how often each verb appears with each 
possible argument structure)

    p(sentence) = ppid18(sentence)pvalence(sentence)
    note: if one were doing this today, then there 

are better options (e.g., lexicalized pid18)

 

 

human forced-choice preference (ford et al., 1982)
probabilistic parser: pid18 + verb valency (jurafsky, 1996)

the women kept the dogs 

on the beach.

93%

95%

the women discussed the 

dogs on the beach.

52%

90%

 

 

local ambiguity/   garden paths   
    consider: the horse raced past the barn fell.
    (versus: the car driven past the barn crashed.)
    by the end of the sentence, this is actually 

unambiguous... but people still have problems!

    jurafsky suggests: incremental parsing via beam 

search     only keep the top n parses
    (see also crocker & brants, 2000)

       garden path    sentences are ones where the 

correct parse was pruned

    prediction: garden path processing difficulty will 

occur at disambiguating region

 

    ...but how do we check?

 

a methodological digression

    the ideal data for studying language 

comprehension should be:
    incremental
    naturalistic
    unconscious

    how about per-word reading time?

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

the cat sat on the mat.

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

the cat sat on the mat.

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

the cat sat on the mat.

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

the cat sat on the mat.

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

the cat sat on the mat.

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

the cat sat on the mat.

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

the cat sat on the mat.

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

the cat sat on the mat.

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

    cheap and easy to do
    easy to interpret (more or less)
    still not the most natural situation

    eye-tracking

    more naturalistic

 

 

eye-tracking: complications

    we want to know how long it takes the brain to 

process each word

    we can measure where the eyes are pointing
    these are not the same...

 

 

eye movements

    eyes each move in 3 dimensions (up/down, 

left/right, rotational)

    at least 5 distinct systems for controlling them
    but, only one is really relevant to reading: 

saccadic eye movements

 

 

saccadic eye movement

    alternation between:

    fixation: eye is still and gathering sensory 

information (~100-500 ms)

    saccade: extremely fast    jump    to a new position 

(~20-50 ms), little or no sensory information 
available

fixations

 

 

saccades

why saccadic eye movement?
    there is only a small part of our visual field 

where we can see details     the fovea

 

 

why saccadic eye movement?
    there is only a small part of our visual field 

where we can see details     the fovea

 

 

eye-tracking for reading

the cat sat on the mat.

regression

(~15-25% of fixations)

    what do the eyes tell us about the brain?

    just and carpenter, 1980: the eye-mind 

hypothesis: what you're looking at is what you're 
processing

 

 

    seems to work pretty well in practice

per word reading times (at last!)
the cat sat on the mat.

    how long did it take to read on?
    how long did it take to read the?

    first fixation time:
    first pass reading time:      +

for more, start here: rayner, k. (1998). eye movements in reading and 
information processing: 20 years of research. psychological bulletin, 
 
124(3), 372   422.

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

    cheap and easy to do
    easy to interpret (more or less)
    still not the most natural situation

    eye-tracking

    more naturalistic

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

    cheap and easy to do
    easy to interpret (more or less)
    still not the most natural situation

    eye-tracking

    more naturalistic
    therefore, more complicated...

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

    cheap and easy to do
    easy to interpret (more or less)
    still not the most natural situation

    eye-tracking

    more naturalistic
    therefore, more complicated...
    but, we can work around that in practice

 

 

reading time: technicalities

two main types:

    self-paced reading (moving window style)

    cheap and easy to do
    easy to interpret (more or less)
    still not the most natural situation

    eye-tracking

    more naturalistic
    therefore, more complicated...
    but, we can work around that in practice

 

    either way: we get per-word reading times

 

back to garden path sentences!

    prediction of id125 theory: reading time 
should increase at the disambiguating region

the horse raced past the barn fell.

frazier and rayner (1982) tested sentences 
like:

wherever alice walks her dog men follow.
wherever alice walks her dog will follow.

 

 

back to garden path sentences!
90

highly significant

e
m

i

i
t
 
g
n
d
a
e
r
 
s
s
a
p
 
t
s
r
i
f

)
r
e
t
c
a
r
a
h
c
 
r
e
p
 
s
m

(

 

80

70

60

50

40

30

20

10

0

not significant

wherever
wherever
alice walks
alice walks

her dog
her dog

 

men follow
men follow
/ will follow
/ will follow

(frazier & rayner, 1982)

more subtle ambiguities
ambiguity  

    the cop arrested...

makes this slow
    ...by the detective was guilty of taking bribes.

    the crook arrested...

    ...by the detective was guilty of taking bribes.

ambiguity does not 
affect speed here

 

(mcrae et al., 1998)

 

more subtle ambiguities

    probably not a pruning effect
    narayanan & jurafsky (2002): an attention shift 
explanation: incur a reading time cost whenever 
new data causes a switch in the most-probable 
interpretation

    use bayes net model to compute probabilities:

 

 

more subtle ambiguities

this causes a flip 

    the cop arrested...

from mv to rr
    ...by the detective was guilty of taking bribes.

    the crook arrested...

    ...by the detective was guilty of taking bribes.

rr was always 
preferred; no flip

 

 

(mcrae et al., 1998; narayanan & jurafsky, 2002)

so is id203 just for ambiguity?

my brother came inside to play
the children went outside to play

    in general, predictable words are read faster 

than unpredictable words

    a good mathematical model:

 

(hale, 2001)

reading time is 
 

proportional to surprisal

surprisal: example

sentences from konieczny (2000):

how fast do we 
read the verb?

    memory theories: understanding the verb requires 

recalling the subject and other arguments => these 
keep getting harder and harder!
(gibson 1998, 2000; gordon, hendrick, & johnson 2001, 2004; lewis & 
vasishth, 2005; lewis, vasishth, & van dyke, 2006)

    surprisal: the more you've seen, the more you're 
expecting a verb => these get easier and easier!
(levy, 2008)

 

 

520

510

500

490

480

470

460

)
s
m

(
 
e
m

i

i
t
 
g
n
d
a
e
r

surprisal example

16.2

humans
pid18

l
a
s
i
r
p
r
u
s

16

15.8

15.6

15.4

15.2

15

memory theories 

don't make 

precise numerical 
predictions, but 
something like 

this...

450
14.8
no pp short pp long pp

 

(levy, 2008)

 

reading time ~ surprisal

    this seems oddly specific... how do we know?
    dundee eye-tracking corpus: publically available, 

10 participants    50,000 words

    ~425,000 total fixations
    ~200,000 first fixations
    estimate word probabilities with kneser-ney trigram 

model

 

(smith & levy, 2008)

 

reading time ~ surprisal

individual participants

reading time is linearly 
proportional to surprisal

)
s
m

(
 
e
m
t

i

log id203

(base 10)

 

(smith & levy, 2008)

 

unique contribution of id203
to first fixation time
individually by subject
plus 95% confidence intervals 
(bootstrapped)

reading time ~ surprisal

    but why?

    relative id178?

 

 

surprisal as relative id178

relative id178 (= id181): a 

fundamental information-theoretic measure of the distance 
between two generative id203 distributions

intuitively, the penalty paid by encoding one distribution with a 

different one

it turns out that relative id178 over interpretation 

distributions before and after wi =               (surprisal!)

log

1

pi- 1(wi)

surprisal can thus be thought of as cost of
    shifting id203 mass

    

 

(levy, 2008)

 

compare to attention 
shift/reranking cost

in ambiguity resolution

reading time ~ surprisal

    but why?

    relative id178?

 

 

reading time ~ surprisal

    but why?

    relative id178?
    optimal sensory discrimination?

 

 

surprisal as optimal discrimination

the children went outside to 
my brother came inside to 

p(word|context)

=
p(word|visual input)

x

=

p(word|context)

p(word|visual input1)

p(word|visual input2)

x

x

x
...

 

(norris, 2006)

 

less expected words 
require more visual 
evidence to reach 

threshold, which is slow
expected words need minimal 

visual evidence to reach a 

certainty threshold, which is fast

reading time ~ surprisal

    but why?

    relative id178?
    optimal sensory discrimination?

 

 

reading time ~ surprisal

    but why?

    relative id178?
    optimal sensory discrimination?
    optimal preparation?

 

 

surprisal as optimal preparation

come together

 

(smith & levy, 2008)

 

surprisal as optimal preparation

spend 
resources 
preparing here...

come together

 

(smith & levy, 2008)

 

surprisal as optimal preparation

spend 
resources 
preparing here...

come together

...and you might 
read faster here.

    only resources spent preparing for together 

end up being useful

    optimal trade-off: spend more resources 
preparing for words with higher id203
    scale invariance produces log relationship

 

 

(smith & levy, 2008)

reading time ~ surprisal

    but why?

    relative id178?
    optimal sensory discrimination?
    optimal preparation?
    (your theory here)

is that the only puzzle? no...

 

 

memory vs. surprisal: round 2

    the reporter who sent the photographer to...
    the reporter who the photographer sent to...

surprisal predicts 
this will be slow
(because until here, you 
could have been in the 

other sentence)

 

 
(grodner & gibson, 2005; levy, 2008)

memory predicts 
this will be slow
(because we saw the 
reporter a long time 

ago)

...and it's right!

...memory and surprisal?

    demberg & keller (2008)

    used a dependency parser to automatically parse 

the dundee corpus

    then used these dependencies to estimate 

memory/integration costs, and estimated surprisal

    found:

    memory effects on verbs (mostly)

             and

    surprisal effects generally

 

 

conclusion 1
conclusion 1

    probabilistic models explain some otherwise 

mysterious phenomena...

    ...but it's not the end of the story!

conclusion 2

    solving these problems 

will require coffee

 

 

memory constraints: a theoretical puzzle 

       # logically possible analyses grows at best exponentially in 

sentence length 

       exact probabilistic id136 with context-free grammars can 

be done efficiently in o(n3)	


       but    

       requires probabilistic locality, limiting conditioning context 
       human parsing is linear   that is, o(n)   anyway 

       so we must be restricting attention to some subset of 

analyses 

       puzzle: how to choose and manage this subset? 

       previous efforts: k-best id125 (crocker & brants, 2000; roark, 

2001, 2004)  

       here, we   ll explore the id143 as a model of limited-

parallel approximate id136  

levy, reali, & griffiths, 2009, nips 

the id143: general picture 

       sequential monte carlo for incremental observations 
       let xi be observed data, zi be unobserved states 
       suppose that after n-1 observations we have the 

       for parsing: xi are words, zi are incremental structures 
distribution over interpretations p(zn-1|x1   n-1)	

p(zn|x1   n) inductively: 

       after next observation xn, represent the next distribution 

       approximate p(zi|x1   i) by samples 
       sample zn from p(zn|zn-1), and reweight by p(xn|zn)  	


id143 with probabilistic grammars 

    np vp  

s 
np      n  
np      n rrc 
rrc      part n 
vp      v n 
n 
n 

1.0 
0.8 
0.2 
1.0 
1.0 
0.7 
    women 
    sandwiches  0.3 

* 

s 
* 

vp 
* 

* 

* 

v 
* 

* 

n 
* 

np 
* 

* 

* 

n 
* 

v      brought 
v      broke 
v      tripped 
part      brought 
part      broken  
part      tripped 
adv      quickly 

s 
* 

* 

np 
* 

* 

n 
* 

rrc 
* 

* 
* 

part 
* 

0.4 
0.3 
0.3 
0.1 
0.7 
0.2 
1.0 

* 

n 
* 

vp 
* 

* 

v 
* 

women  brought 

sandwiches 

tripped 

women  brought 

sandwiches 

tripped 

0.7 

0.4 

0.3 

0.7 

0.1 

0.3 

0.3 

resampling in the id143 

       with the na  ve id143, id136s are highly dependent 

on initial choices 
       most particles wind up with small weights 
       region of dense posterior poorly explored 

       especially bad for parsing 

       space of possible parses grows (at best) 
   exponentially with input length 

input 

resampling in the id143 

       with the na  ve id143, id136s are highly dependent 

on initial choices 
       most particles wind up with small weights 
       region of dense posterior poorly explored 

       especially bad for parsing 

       space of possible parses grows (at best) 
   exponentially with input length 

       we handle this by 
resampling at each 
input word 

input 

simple garden-path sentences 

the woman brought the sandwich from the kitchen tripped 

main verb (it was the woman who brought the sandwich) 

s

s

np

dt

nn

the

woman

np

vp

dt

nn

vbd

np

the

woman

brought

dt

nn

s

np

vp

dt

nn

vbd

np

pp

the

woman

brought

dt

nn

in

np

the

sandwich

the

sandwich

from

dt

nn

the

kitchen

reduced relative (the woman was brought the sandwich) 

s

np

np

dt

nn

the

woman

s

np

np

vp

dt

nn

vbn

np

the

woman

brought

dt

nn

the

sandwich

s

np

np

vp

dt

nn

vbn

np

pp

the

woman

brought

dt

nn

in

np

the

sandwich

from

dt

nn

the

kitchen

s

np

np

vp

vp

vbd

dt

nn

vbn

np

pp

tripped

the

woman

brought

dt

nn

in

np

the

sandwich

from

dt

nn

       posterior initially misled away from ultimately correct interpretation 
       with finite # of particles, recovery is not always successful 

the

kitchen

solving a puzzle 

 tom heard the gossip about the neighbors wasn   t true. 

a-s  tom heard the gossip wasn   t true. 
a-l
u-s  tom heard that the gossip wasn   t true. 
u-l  tom heard that the gossip about the neighbors wasn   t 

true. 

       previous empirical finding: ambiguity induces difficulty    
          but so does the length of the ambiguous region 
       our linking hypothesis: 
 proportion of parse failures at the disambiguating region 
should increase with sentence difficulty 

frazier & rayner,1982; tabor & hutchins, 2004 

another example (tabor & hutchins 2004) 

as the author wrote the essay the book grew. 
as the author wrote the book grew. 
as the author wrote the essay the book describing babylon grew. 
as the author wrote the book describing babylon grew. 

resampling-induced drift 

informative (p(xi|zi) similar across different zi)	


       in ambiguous region, observed words aren   t strongly 
       but due to resampling, p(zi|xi) will drift 
       one of the interpretations may be lost 
       the longer the ambiguous region, the more likely this is 

)
e
v
i
t
i
s
n
a
r
t
(
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.
0

0
0

.

input

model results 

ambiguity matters    

np/s

np/z

5

4

.
0
3
*
<
,
+
3
8
;

5

/
4

3

/
8
-
8
8
-
6
6
*
8
/
-
8
.
3
2
/
1
0
/
:
0
5
4
.
0
2
0
.
9

!
"

&

(
!

&

'
!

&

%

&
!

#
&
!

!
&
!

5

4

.
0
3
*
<
,
+
3
8
;

5

/
4

3

/
8
-
8
8
-
6
6
*
8
/
-
8
.
3
2
/
1
0
/
:
0
5
4
.
0
2
0
.
9

=!>
?!>
=!@
?!@

!
"

&

(
!

&

'
!

&

%

&
!

#
&
!

!
&
!

=!>
?!>
=!@
?!@

!

"!!

#!!

$!!

%!!

!

"!!

#!!

$!!

%!!

)*+,-./01/23.4567-8

)*+,-./01/23.4567-8

but the length of the ambiguous region also matters! 

human results (offline rating study) 

5

4

.
0
3
*
<
,
+
3
8
;

5

/
4

3

/
8
-
8
8
-
6
6
*
8
/

-
8
.
3
2

/
1

0

/

:
0

5
4
.
0
2
0
.
9

.
0

4

5

3
*
<
,
+
3
8
;

5

/
4

3
/
8
-
8
8
-
6
6
*
8
/

-
8
.
3
2

/
1

0

/

:
0

5
4
.
0
2
0
.
9

np/s

=!>
?!>
=!@
?!@

!

"!!

#!!

$!!

%!!

)*+,-./01/23.4567-8

np/z

=!>
?!>
=!@
?!@

!

&

"

(

&

!

'

&

!

%

&

!

#

&

!

!

&

!

!

&

"

(

&

!

'

&

!

%

&

!

#

&

!

!

&

!

/
$

'
,

#
.
%
-
,
+

*
)
'
(
(
'

%

&
$
#
"
!

;<=8
;<=>

6

5

4

3

2

1

0

!

"!!

#!!

$!!

%!!

)*+,-./01/23.4567-8

7!8

9!8

7!:

9!:

approximate-id136 summary 

       approximate id136   either beam-search or particle-

filter   is a way of giving approximate probabilistic 
processing in linear time 

       loss of analyses accounts better for garden-pathing than 

sheer surprisal does 

       stochasticity in the id143 could account for 

   digging-in    effects 

bounds on rationality? 

       thus far, we have painted a pretty picture of humans of 

rational comprehenders 

       lots of information sources can be usefully brought to 

bear in the difficult task of language comprehension 

       people rationally and incrementally use all the 

information available 

       we have lots of evidence that people do this often 

   put the apple on the towel in the box.      (tanenhaus et al., 1995) 

       but now we   ll challenge this with a new puzzle 

anatomy of ye olde garden path sentence 

       classic example of incrementality in comprehension 
 

   main verb    

  

s 

   reduced relative    

s 

np 

np 

vp 

vp 

 

 the horse raced past the barn fell. 

that was 

(the criminal arrested by the detective confessed.) 

       people fail to understand it most of the time 
       people are likely to misunderstand it   e.g., 

          what   s a barn fell?    
       the horse that raced past the barn fell 
       the horse raced past the barn and fell 

a puzzle for incremental id136 

       try to understand this sentence: 

 (a) the coach smiled at the player tossed the frisbee. 

   and contrast this with: 

 (b) the coach smiled at the player thrown the frisbee. 

 (c) the coach smiled at the player who was thrown the frisbee. 

 (d) the coach smiled at the player who was tossed the frisbee. 

       readers boggle at    tossed    in (a), but not in (b-d) 

rt spike in (a) 

(tabor et al., 2004) 

why is tossed/thrown interesting? 

       as with classic garden-paths, part-of-speech ambiguity leads 

to misinterpretation
       the horse raced past the barn   fell 

  

verb? 

participle? 

       but now context    should    rule out the garden path: 

       the coach smiled at the player tossed    

verb? 

participle? 

       a substantial challenge for rational, incremental models: 

failure to condition on relevant context 

meeting this challenge 

       these effects of merely    locally coherent    chunks within 

a sentence have attracted considerable attention 

       tabor et al., (2004); tabor & hutchins (2004) treat these 

effects within an essentially dynamical-systems tree-
substitution grammar approach 

       hale (2007) treats local coherence effects within a 
guided-search, parallel-space, serial-action parsing 
framework 

       we   ll now discuss two recent models of our own that 

account for these effects in different ways    

          plus new empirical research deriving from the 

modeling work 

weighted    nite-state automata and

intersection with pid18s

roger levy

uc san diego

department of linguistics

1 june 2010

weighted    nite-state automata

a weighted finite-state automaton (wfsa) is a tuple
(q, v , q0, qf , t , w ) such that:
    q is a    nite set of states;
    v is a    nite set of input symbols, including the empty string

  ;

    q0     q is the start state;
    qf     q is the end state;
    t is a set of transitions of the form x : q     q     where x     v

and q, q         q;

    w is a mapping from t to weights.

(there are alternative formulations with multiple    nal states,
starting-state costs, and/or    nal-state costs, but they all have
the same basic properties as what we show here.)

weighted    nite-state automata ii

    the weights in a wfsa can be drawn from any of a

number of semirings (mohri, 1997; goodman, 1999)

    a semiring is a set over which two binary operators are
de   ned:     and    , intuitively behaving more or less like
arithmetic + and   

    here, we   ll consider weights drawn from:

    the non-negative real (or negative-log) semirings,

corresponding to (negative-log) probabilities; or

    the tropical semiring, corresponding to

highest-id203 derivations

x     y
x + y

x     y
semiring
x    y
real
negative-log     log(e   x + e   y ) x + y
tropical
x + y

max(x , y)

weighted    nite-state automata iii i

    a path through a wfsa is a sequence of transitions drawn
from t taking you from the start state q0 to the    nal state qf
    the weight of a path through a wfsa is the    -sum of the

weights of all transitions on the path

    the weight s(w) of a string w is the sum of the weights

of all the paths that generate the string

    note that this can be more than one (or even in   nitely
many) paths even if the wfsa is deterministic, due to   
transition

weighted    nite-state automata iv

    for a real or negative-log semiring wfsa, the total weight
of all accepted strings is called its partition function z

z = x
w

s(w)

    if z is    nite, then the id203 of a string w in a wfsa

is its weight normalized by the partition function:

p(w) =

s(w)

z

    if the normalizing constant is 1 (that is,     identity), then the

wfsa can be said to be a proper probabilistic wfsa

    stronger condition: the wfsa is statewise proper if for

every non-   nal state, sum of outgoing path weights is 1
(   nal state may alternatively have no outgoing paths; see
also smith and johnson, 2007)

weighted    nite-state automata v

    of course, in general there are too many possible paths to

calculate the partition function z exactly by enumeration

    however, there are dynamic programs for computing z

ef   ciently:

    generic single-source algorithm (mohri, 2002;

exponential-time)

    generalized all-pair shortest distance (lehmann, 1977;

o(n3) in principle, sometimes worse in practice; allauzen,
p.c.)

    one can also use    xed-point or other methods to

approximate z (stolcke, 1995; nederhof and satta, 2008)

intersection with pid18s

    a widely known property of fsas is that both fsas and

id18s are closed under intersection with them
(bar-hillel et al., 1964)

    that is, for any two fsas a1, a2, there is some fsa that
accepts only those strings which are accepted by both a1
and a2

    likewise, for any id18 g and any fsa a and, there is some
id18 that accepts only those strings which are accepted by
both a and g

    furthermore, there are constructive procedures for

creating the intersection fsa/id18, and the paths/rules
bear a close resemblance to the original rules

intersection with pid18s ii

    additionally, closure under intersection extends to the

weighted regime

    for any two wfsas a1, a2, there is a wfsa a12 such that

for all strings w, sa12(w) = sa1(w)     sa2(w)

intersection with pid18s ii

    additionally, closure under intersection extends to the

weighted regime

    for any two wfsas a1, a2, there is a wfsa a12 such that

for all strings w, sa12(w) = sa1(w)     sa2(w)

    now consider a pid18 g and a wfsa a in the real

semiring. there is some weighted id18 g    such that for all
strings w, sg   (w) = pg(w)timessa(w)

intersection with pid18s ii

    additionally, closure under intersection extends to the

weighted regime

    for any two wfsas a1, a2, there is a wfsa a12 such that

for all strings w, sa12(w) = sa1(w)     sa2(w)

    now consider a pid18 g and a wfsa a in the real

semiring. there is some weighted id18 g    such that for all
strings w, sg   (w) = pg(w)timessa(w)

    (the same would hold if g and a had probabilities and

weights in the negative-log semiring respectively.)

intersection with pid18s ii

    additionally, closure under intersection extends to the

weighted regime

    for any two wfsas a1, a2, there is a wfsa a12 such that

for all strings w, sa12(w) = sa1(w)     sa2(w)

    now consider a pid18 g and a wfsa a in the real

semiring. there is some weighted id18 g    such that for all
strings w, sg   (w) = pg(w)timessa(w)

    (the same would hold if g and a had probabilities and

weights in the negative-log semiring respectively.)

    furthermore, if z is    nite for each constituent grammar,
then z is    nite in the intersection grammar (n.b. for most
empirically estimated pid18s, z = 1; chi and geman,
1998)

intersection with pid18s ii

    additionally, closure under intersection extends to the

weighted regime

    for any two wfsas a1, a2, there is a wfsa a12 such that

for all strings w, sa12(w) = sa1(w)     sa2(w)

    now consider a pid18 g and a wfsa a in the real

semiring. there is some weighted id18 g    such that for all
strings w, sg   (w) = pg(w)timessa(w)

    (the same would hold if g and a had probabilities and

weights in the negative-log semiring respectively.)

    furthermore, if z is    nite for each constituent grammar,
then z is    nite in the intersection grammar (n.b. for most
empirically estimated pid18s, z = 1; chi and geman,
1998)

    if the z    s are    nite, then gab and g    are thus probabilistic

grammars (abney et al., 1999; chi, 1999; smith and
johnson, 2007)

construction of the weighted id18/fsa intersection

       pid18 phrase    rules: for each rule x     y 1

. . . y n with

id203 p in the pid18, and each state sequence
r0, . . . , rn in the wfsa, put a rule
r1 . . .rn   1 y n

rn

r0xrn    r0 y 1
into g    with weight p

construction of the weighted id18/fsa intersection

       pid18 phrase    rules: for each rule x     y 1

. . . y n with

id203 p in the pid18, and each state sequence
r0, . . . , rn in the wfsa, put a rule
r1 . . .rn   1 y n

rn

r0xrn    r0 y 1
into g    with weight p

       wfsa bottoming-out    rules: for each transition

x : r     r     in the wfsa with weight s, put the rule

r xr         x

into g    with weight s

construction of the weighted id18/fsa intersection

       pid18 phrase    rules: for each rule x     y 1

. . . y n with

id203 p in the pid18, and each state sequence
r0, . . . , rn in the wfsa, put a rule
r1 . . .rn   1 y n

rn

r0xrn    r0 y 1
into g    with weight p

       wfsa bottoming-out    rules: for each transition

x : r     r     in the wfsa with weight s, put the rule

r xr         x

into g    with weight s

    you   re done!

example of pid18/wfsa intersection

    imagine you   ve heard the incomplete sentence

dogs static cats fought. . .

example of pid18/wfsa intersection

    imagine you   ve heard the incomplete sentence

dogs static cats fought. . .

    let   s consider one set of possibilities based purely on

input:

dogs and cats fought. . .
dogs and dogs and cats fought. . .
dogs and cats and cats fought. . .
. . .

example of pid18/wfsa intersection

    imagine you   ve heard the incomplete sentence

dogs static cats fought. . .

    let   s consider one set of possibilities based purely on

input:

dogs and cats fought. . .
dogs and dogs and cats fought. . .
dogs and cats and cats fought. . .
. . .

    we can represent these possibilities succinctly with a

wfsa:

*/0.5

dogs/1

0

1

cats/0.5

2

fought/1

*/1

3

example of pid18/wfsa intersection iii

*/0.5

dogs/1

0

1

cats/0.5

2

fought/1

    how do we interpret this partial input???

*/1

3

example of pid18/wfsa intersection iii

*/0.5

dogs/1

0

1

cats/0.5

2

fought/1

*/1

3

    how do we interpret this partial input???
    grammar-based interpretation: a simple pid18

1 root    s
1 s
   np vp
1 vp    fought

0.3 np     cats
0.3 np     dogs
0.4 np     np conj np
1

conj     and

example of pid18/wfsa intersection iii

*/0.5

dogs/1

0

1

cats/0.5

2

fought/1

*/1

3

    how do we interpret this partial input???
    grammar-based interpretation: a simple pid18

1 root    s
1 s
   np vp
1 vp    fought

0.3 np     cats
0.3 np     dogs
0.4 np     np conj np
1

conj     and

    grammar-based interpretation of partial sentences is just

(weighted) intersection (lang, 1988, 1991; hale, 2006;
levy, 2008)

example of pid18/wfsa intersection iii

*/0.5

dogs/1

0

1

cats/0.5

2

fought/1

*/1

3

    how do we interpret this partial input???
    grammar-based interpretation: a simple pid18

1 root    s
1 s
   np vp
1 vp    fought

0.3 np     cats
0.3 np     dogs
0.4 np     np conj np
1

conj     and

    grammar-based interpretation of partial sentences is just

(weighted) intersection (lang, 1988, 1991; hale, 2006;
levy, 2008)

    we can use a bottom-up agenda-based parser (shieber

et al., 1995) to determine what rules will actually matter in
the grammar

example of pid18/wfsa intersection iii

root    s
s    np vp

root    s
s    np vp
np    np conj np

root    s
s    np vp
vp    fought
np    np conj np
conj    and
np    cats
np    dogs

root    s
s    np vp
np    np conj np

root    s
s    np vp
vp    fought
np    np conj np
conj    and
np    cats
np    dogs

np    dogs

np    cats

vp    fought

dogs

0

cats

1

fought

2

3

example of pid18/wfsa intersection iv

    after pruning away the    useless    rules (those that bottom

out in zero weight), we get the following intersection
weighted id18:

1
1
1
0.3
0.3
0.4
1
0.3
0.3
0.4

   0np1 1vp3

0root3    0s3
0s3
1vp3    1fought3
0np1    0dogs1
1np2    1cats2
0np1    0np1 1conj1 1np1
1conj1    1and1
1np1    1dogs1
1np1    1cats1
1np1    1np1 1conj1 1np1

1
0.5
0.5
0.5
0.5
1
1

0dogs1     dogs
1and1     and
1dogs1     dogs
1cats1     cats
1cats2     cats
2fought3     fought
3*3

    *

    this grammar is improper, but by computing the partition

function of each category we can turn it into a pid18 (chi,
1999; nederhof and satta, 2008)

local coherences under rational comprehension

local coherences under rational comprehension

(2010-06-01)

computational psycholinguistics tutorial

1 / 12

local coherences under rational comprehension

local coherences under rational comprehension

comprehension could still be rational
(cid:73) if we relax some of our assumptions
(cid:73) today: two models that get local coherences in a rational framework

(2010-06-01)

computational psycholinguistics tutorial

1 / 12

local coherences under rational comprehension

local coherences under rational comprehension

comprehension could still be rational
(cid:73) if we relax some of our assumptions
(cid:73) today: two models that get local coherences in a rational framework
(cid:73) model 1: relax assumption that all contextual information is

immediately available

(2010-06-01)

computational psycholinguistics tutorial

1 / 12

local coherences under rational comprehension

local coherences under rational comprehension

comprehension could still be rational
(cid:73) if we relax some of our assumptions
(cid:73) today: two models that get local coherences in a rational framework
(cid:73) model 1: relax assumption that all contextual information is

immediately available

(cid:73) model 2: relax assumption that input is free of noise

(2010-06-01)

computational psycholinguistics tutorial

1 / 12

local coherences under rational comprehension

local coherences under rational comprehension

comprehension could still be rational
(cid:73) if we relax some of our assumptions
(cid:73) today: two models that get local coherences in a rational framework
(cid:73) model 1: relax assumption that all contextual information is

immediately available

(cid:73) model 2: relax assumption that input is free of noise

plan

(cid:73) present each model
(cid:73) experiments showing how they get local coherence e   ects

(2010-06-01)

computational psycholinguistics tutorial

1 / 12

model 1 (bicknell & levy, 2009, naacl)

model 1

goals

(cid:73) propose a new model of local coherence e   ects under rational

sentence comprehension

(cid:73) bottom-up information is rapidly available
(cid:73) but comprehenders ultimately arrive at normative probabilities

(combining bottom-up and top-down information)

(cid:73) show that an implemented version makes the correct predictions for

the tabor et al. data

(2010-06-01)

computational psycholinguistics tutorial

2 / 12

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

the

coach smiled

at

the

player

tossed

a

frisbee

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

d

the

coach smiled

at

the

player

tossed

a

frisbee

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

n

the

coach smiled

at

the

player

tossed

a

frisbee

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

the

coach smiled

at

the

player

tossed

a

frisbee

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

vbd

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

np

d

n

the

coach smiled

at

the

player

tossed

a

frisbee

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

np

d

n

the

coach smiled

at

the

player

tossed

a

frisbee

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

the

coach smiled

at

the

player

tossed

np

d

a

n

frisbee

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

s

np

vp

d

n

vbd

the

coach smiled

at

the

player

tossed

a

frisbee

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

the

coach smiled

at

the

player

tossed

vbd

vp

np

d

a

n

frisbee

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

pp

np

d

n

the

player

tossed

a

frisbee

the

coach smiled

p

at

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

the intuition: new sentence built of familiar parts

model 1

the intuition: parsing as belief update

s

vp

pp

np

rc

np

np

np

d

n

vbd

the

coach

smiled

p

at

d

n

vbn

the

player

tossed

d

a

n

frisbee

(2010-06-01)

computational psycholinguistics tutorial

3 / 12

the intuition: parsing as belief update

model 1

the intuition: parsing as belief update

notice: these likely structures can be incompatible

pp

s

np

np

vp

p

at

d

n

d

n

vbd

the

player

the

player

tossed

(2010-06-01)

computational psycholinguistics tutorial

4 / 12

the intuition: parsing as belief update

model 1

the intuition: parsing as belief update

notice: these likely structures can be incompatible

pp

s

np

np

vp

p

at

d

n

d

n

vbd

the

player

the

player

tossed

(2010-06-01)

computational psycholinguistics tutorial

4 / 12

the intuition: parsing as belief update

model 1

the intuition: parsing as belief update

notice: these likely structures can be incompatible

pp

s

np

np

vp

p

at

d

n

d

n

vbd

the

player

the

player

tossed

what to throw away?

(2010-06-01)

computational psycholinguistics tutorial

4 / 12

the intuition: parsing as belief update

model 1

the intuition: parsing as belief update

notice: these likely structures can be incompatible

pp

s

np

np

vp

p

at

d

n

d

n

vbd

the

player

the

player

tossed

what to throw away?

this is belief update

(2010-06-01)

computational psycholinguistics tutorial

4 / 12

the intuition: parsing as belief update

model 1

the intuition: parsing as belief update

the belief update process

(cid:73) integrating a new word or words w j
i
(cid:73) start: precomputed prior beliefs about structure that w j
(cid:73) end: posterior beliefs about structure w j
i has in context
(cid:73) hypothesis: di   culty increases with the size of the belief update

into a parse is belief update

i will have

(2010-06-01)

computational psycholinguistics tutorial

5 / 12

model 1

the model formally

the model formally

for each syntactic category x :

(cid:73) begin with    bottom-up    prior that w j

i forms the beginning of it

(cid:73) prior: p(x k   j

i

|w j
i )

x

wi . . . wj

(2010-06-01)

computational psycholinguistics tutorial

6 / 12

model 1

the model formally

the model formally

for each syntactic category x :

(cid:73) begin with    bottom-up    prior that w j

i forms the beginning of it

(cid:73) prior: p(x k   j

i

|w j
i )

(cid:73) integrate with    top-down    knowledge to reach a posterior id203

of x spanning w j

i given all words seen

(cid:73) posterior: p(x k   j

i

|w j
0)

s

x

w0 . . . wi   1

wi . . . wj

(2010-06-01)

computational psycholinguistics tutorial

6 / 12

model 1

the model formally

the model formally

for each syntactic category x :

(cid:73) begin with    bottom-up    prior that w j

i forms the beginning of it

(cid:73) prior: p(x k   j

i

|w j
i )

(cid:73) integrate with    top-down    knowledge to reach a posterior id203

of x spanning w j

i given all words seen

(cid:73) posterior: p(x k   j

i

|w j
0)

(cid:73) mij     the amount of modi   cation
required to update prior to posterior
when integrating w j
i

s

x

w0 . . . wi   1

wi . . . wj

(2010-06-01)

computational psycholinguistics tutorial

6 / 12

an intuitive example

model 1

an intuitive example

the coach smiled at the player tossed. . .

          s

.91
np .22

...

         

arti   cial probabilities for

illustrative purposes

prior

the player tossed

(2010-06-01)

computational psycholinguistics tutorial

7 / 12

an intuitive example

model 1

an intuitive example

the coach smiled at the player tossed. . .

          s

.91
np .22

...

         

arti   cial probabilities for

illustrative purposes

prior

the player tossed

s

          s
          s

.01
.01
np .95
np .95

...
...

         
         

posterior

the coach smiled at

the player tossed

(2010-06-01)

computational psycholinguistics tutorial

7 / 12

an intuitive example

model 1

an intuitive example

the coach smiled at the player tossed. . .

          s

.91
np .22

...

         

prior

arti   cial probabilities for

illustrative purposes

mij high

the player tossed

s

          s
          s

.01
.01
np .95
np .95

...
...

         
         

posterior

the coach smiled at

the player tossed

(2010-06-01)

computational psycholinguistics tutorial

7 / 12

an intuitive example

model 1

an intuitive example

the coach smiled at the player thrown . . .

          s

.3
np .9

...

         

arti   cial probabilities for

illustrative purposes

prior

the player thrown

(2010-06-01)

computational psycholinguistics tutorial

8 / 12

an intuitive example

model 1

an intuitive example

the coach smiled at the player thrown . . .

          s

.3
np .9

...

         

arti   cial probabilities for

illustrative purposes

prior

the player thrown

s

          s
          s

.01
.01
np .95
np .95

...
...

         
         

posterior

the coach smiled at

the player thrown

(2010-06-01)

computational psycholinguistics tutorial

8 / 12

an intuitive example

model 1

an intuitive example

the coach smiled at the player thrown . . .

          s

.3
np .9

...

         

prior

arti   cial probabilities for

illustrative purposes

mij low

the player thrown

s

          s
          s

.01
.01
np .95
np .95

...
...

         
         

posterior

the coach smiled at

the player thrown

(2010-06-01)

computational psycholinguistics tutorial

8 / 12

model 1

an intuitive example

model 1

implementation

(cid:73) use probabilistic context-free grammar (pid18)
(cid:73) calculate prior and posterior for each nonterminal category

(cid:73) neither may look obvious, but we can reduce them to forward

probabilities

(cid:73) measure mij as kullback-leibler (k-l) divergence from the prior to

the posterior, summed over categories

(cid:88)

x   n

mij

def=

(cid:123)
(cid:125)(cid:124)
(cid:122)
(cid:16) posterior
|w j
0) ||

p(x k   j

i

(cid:122)

d

(cid:125)(cid:124)

prior
p(x k   j

i

|w j
i )

(cid:123)

(cid:17)

(2010-06-01)

computational psycholinguistics tutorial

9 / 12

experiment

model 1

experiment

see whether this intuition is borne out among tabor et al.   s items

methods

(cid:73) de   ned a small pid18
(cid:73) estimating probabilities from parsed brown corpus
(cid:73) used 12 of tabor et al.   s 20 items (so model knew verb & syntax)
(cid:73) same 4 conditions as tabor et al.:

(cid:73) lexical ambiguity: tossed vs. thrown
(cid:73) relative clause reduction: who was vs.    

(cid:73) used an equal combination of 1-, 2-, and 3-word sizes of w j
i
(cid:73) prediction: mij will be highest for     tossed

(2010-06-01)

computational psycholinguistics tutorial

10 / 12

experiment: results

model 1

experiment

(2010-06-01)

computational psycholinguistics tutorial

11 / 12

llllll051015summed k   l divergence (bits)llllllattheplayertossed/thrownafrisbeelltossedwho was tossedthrownwho was thrownexperiment: results

model 1

experiment

(2010-06-01)

computational psycholinguistics tutorial

11 / 12

llllll051015summed k   l divergence (bits)llllllattheplayertossed/thrownafrisbeelltossedwho was tossedthrownwho was thrownmodel 2: uncertainty about input

    model 1 relaxed assumption that all contextual 

information is immediately available

    model 2 instead relaxes assumption that input is 

perfectly formed and free of noise

    plan:

1.motivate and describe this model
2.show how it predicts local coherence effects

model 2: uncertainty about input

    state of the art models for ambiguity resolution     

probabilistic incremental parsing 

    simplifying assumption: 

    input is clean and perfectly-formed
    no uncertainty about input is admitted

    intuitively seems patently wrong   

    we sometimes misread things
    we can also proofread
    leads to two questions:

1.what might a model of sentence comprehension under 

uncertain input look like?

2.what interesting consequences might such a model have?

today: a first-cut answer

1. what might a model of sentence comprehension under uncertain input 

look like?

2. what interesting consequences might such a model have?

    first: a simple noisy-channel model of rational sentence 

comprehension under uncertain input

    then: show how it can predict local coherence effects
    we use id140 (pid18s) 

and weighted finite-state automata (wfsas) to 
instantiate the model

the noisy-channel model

    say we use a weighted generative grammar g to parse a 

sentence w.  we get a posterior over structures t:

    if we don   t observe a sentence but only a noisy input i:

    posterior over possible sentences:

levy, 2008 (emnlp)

the noisy-channel model (ii)

    this much is familiar from the parsing of speech (hall & 

johnson, 2003, 2004; johnson & charniak, 2004)

    alternative scenario: we know the true sentence w* but 

not observed input i (e.g., the study of reading)

    expected id136s of the comprehender marginalize 

over the input i:

comprehender   s model

true model

representing noisy input

    how can we represent the type of noisy input generated 

by a word sequence?

    probabilistic finite-state automata (pfsas; mohri, 1997) are a 

good model

vocab = a,b,c,d,e,f

input symbol

log-id203

(surprisal)

       word 1 is a or b, and i have no info about word 2   

probabilistic linguistic knowledge

    a generative probabilistic grammar determines beliefs 

about which strings are likely to be seen
    id140 (pid18s; booth, 1969)
    probabilistic minimalist grammars (hale, 2006)
    probabilistic finite-state grammars (mohri, 1997; crocker & brants 2000)

log-id203

(surprisal)

input symbol

    in position 1, {a,b,c,d} equally likely; but in position 2:

    {a,b} are usually followed by e, occasionally by f
    {c,d} are usually followed by f, occasionally by e

combining grammar & uncertain input
    bayes    rule says that the evidence and the prior should 

be combined (multiplied)

    for probabilistic grammars, this combination is the formal 

operation of weighted intersection

input

grammar

+

belief

=

grammar 
affects 
beliefs 
about the 
future

revising beliefs about the past

    when we   re uncertain about the future, grammar + 

partial input can affect beliefs about what will happen 

    with uncertainty of the past, grammar + future input can 

affect beliefs about what has already happened

word 1
{b,c} {?}

grammar

words 1 + 2
{b,c} {f,e}

the noisy-channel model (final)

expected evidence
    for q(w,w*): a wfsa based on levenshtein 

prior

distance between words (kld):

cost(a cat sat)=0

cost(sat a sat cat)=8

result of kld applied to w* = a cat sat 

incremental id136 under uncertain input

    near-neighbors make the    incorrect    analysis    correct   :
any of these changes 
makes tossed a main 
verb!!!

(that?)
(who?)

(and?)
(as?)

(and?)
(that?)
(who?)

the coach smiled at the player tossed the frisbee

    hypothesis: the boggle at    tossed    involves what the 
comprehender wonders whether she might have seen

the core of the intuition

    grammar & input come together to determine two possible 

   paths    through the partial sentence:

(line thickness     id203)

   the player   

tossed
thrown

at
(likely)

the coach smiled   

as/and
(unlikely)

   the player   

tossed
thrown

   

   

tossed is more likely to happen along the bottom path
    this creates a large shift in belief in the tossed condition
thrown is very unlikely to happen along the bottom path
    as a result, there is no corresponding shift in belief

ingredients for the model

prior

expected evidence

    q(w,w*) comes from kld (with minor changes)
    pc(w) comes from a probabilistic grammar
    we need one more ingredient:

    a quantified signal of the alarm induced by word wi about 
changes in beliefs about the past

quantifying alarm about the past

    relative id178 (kl-divergence) is a natural metric of 

change in a id203 distrib. (levy, 2008; itti & baldi, 2005)

    our distribution of interest is probabilities over the 

previous words in the sentence

    call this distribution pi(w[0,j))
conditions on words 0 through i
    the change induced by wi is the error identification 

strings up to but excluding word j

signal eisi, defined as

new distribution

old distribution

error identification signal: example

    measuring change in beliefs about the past:

no change: eis2 = 0

change: eis2 = 0.14

results on local-coherence sentences
    locally coherent:      the coach smiled at the player tossed the frisbee
    locally incoherent:   the coach smiled at the player thrown the frisbee

eis greater for the variant 
humans boggle more on

(all sentences of tabor et al. 2004 with lexical coverage in model)

prediction 2: neighborhood manipulation

    novel prediction: changing the neighborhood of the 

context can change the eis

the coach smiled     at      the player tossed the frisbee

(that?)
(who?)

(and?)
(as?)

(and?)
(that?)
(who?)

the coach smiled toward the player tossed the frisbee

    substituting toward for at should reduce the eis
    in free reading, we should see less tendency to regress 

from tossed when the eis is small

(levy, bicknell, slattery & rayner, 2009)

experimental design

    in a free-reading eye-tracking study, we crossed 

at/toward with tossed/thrown:

the coach smiled   at   the player tossed the frisbee
the coach smiled   at   the player thrown the frisbee
the coach smiled toward the player tossed the frisbee
the coach smiled toward the player thrown the frisbee

    prediction: interaction between preposition & ambiguity 

in some subset of:
    early-measure rts at critical region tossed/thrown
    first-pass regressions out of critical region
    go-past time for critical region
    regressions into at/toward  

model predictions

at   tossed

toward   tossed

toward   thrown

at   thrown

(the coach smiled at/toward the player tossed/thrown the frisbee)

experimental results

at   tossed

the coach smiled at the player tossed   

?

?

toward   tossed

at   thrown

toward   thrown

first-pass

rt

regressions

out

go-past

rt

go-past

regressions

comprehension

accuracy

summary & conclusions

    first model dealing with input uncertainty & implications 

for sentence comprehension

    empirical support from an eye-tracking experiment
syntax id136

    first evidence of fully bidirectional word  

    noise and perceptual uncertainty in the system aren   t 

just afterthoughts
    they introduce fundamentally new kinds of id136s!

what   s missing

introduction

(cid:73) how and why does eis change eye movements?
(cid:73) ideal: make eis-like behavior fall out of a rational model of reading
(cid:73) here: propose a rational framework for eye movements in reading
(cid:73) cf. popular non-rational computational models of eye movements in

reading: e-z reader (reichle, rayner, & pollatsek, 2003) and swift
(engbert et al., 2005)

(2010-06-01)

computational psycholinguistics tutorial

1 / 15

a new framework for eye movements in reading

introduction

a new framework

(2010-06-01)

computational psycholinguistics tutorial

2 / 15

a new framework for eye movements in reading

introduction

a new framework

readers have a diverse range of goals:

(2010-06-01)

computational psycholinguistics tutorial

2 / 15

a new framework for eye movements in reading

introduction

a new framework

readers have a diverse range of goals:

(cid:73) understanding the point of the passage

(2010-06-01)

computational psycholinguistics tutorial

2 / 15

a new framework for eye movements in reading

introduction

a new framework

readers have a diverse range of goals:

(cid:73) understanding the point of the passage
(cid:73) going as fast as possible while getting the comprehension question

(2010-06-01)

computational psycholinguistics tutorial

2 / 15

a new framework for eye movements in reading

introduction

a new framework

readers have a diverse range of goals:

(cid:73) understanding the point of the passage
(cid:73) going as fast as possible while getting the comprehension question

these goals are all accomplished by getting info about the text   s identity

(2010-06-01)

computational psycholinguistics tutorial

2 / 15

a new framework for eye movements in reading

introduction

a new framework

readers have a diverse range of goals:

(cid:73) understanding the point of the passage
(cid:73) going as fast as possible while getting the comprehension question

these goals are all accomplished by getting info about the text   s identity

identifying the text

(cid:73) two sources of information

(cid:73) language knowledge
(cid:73) visual input

(2010-06-01)

computational psycholinguistics tutorial

2 / 15

a new framework for eye movements in reading

introduction

a new framework

readers have a diverse range of goals:

(cid:73) understanding the point of the passage
(cid:73) going as fast as possible while getting the comprehension question

these goals are all accomplished by getting info about the text   s identity

identifying the text

(cid:73) two sources of information

(cid:73) language knowledge (prior)
(cid:73) visual input (likelihood)

(cid:73) normative way to combine these: bayesian id136

(2010-06-01)

computational psycholinguistics tutorial

2 / 15

a new framework for eye movements in reading

introduction

a new framework

readers have a diverse range of goals:

(cid:73) understanding the point of the passage
(cid:73) going as fast as possible while getting the comprehension question

these goals are all accomplished by getting info about the text   s identity

identifying the text

(cid:73) two sources of information

(cid:73) language knowledge (prior)
(cid:73) visual input (likelihood)

(cid:73) normative way to combine these: bayesian id136
(cid:73) prior is known, so eyes move for visual input

(2010-06-01)

computational psycholinguistics tutorial

2 / 15

a new framework for eye movements in reading

introduction

a new framework

readers have a diverse range of goals:

(cid:73) understanding the point of the passage
(cid:73) going as fast as possible while getting the comprehension question

these goals are all accomplished by getting info about the text   s identity

identifying the text

(cid:73) two sources of information

(cid:73) language knowledge (prior)
(cid:73) visual input (likelihood)

(cid:73) normative way to combine these: bayesian id136
(cid:73) prior is known, so eyes move for visual input

central hypothesis: eyes move to obtain visual information about text,
which helps identify it and thus achieve reader goals

(2010-06-01)

computational psycholinguistics tutorial

2 / 15

this bit of the talk

introduction

this bit of the talk

1. present an implemented model of reading within this framework

2. report a simulations with it, demonstrating how model can be    exibly

adapted to di   erent reader goals

(2010-06-01)

computational psycholinguistics tutorial

3 / 15

an implemented model

overview

an implemented model

framework is general

(cid:73) start with prior expectations for the text (language knowledge)
(cid:73) move eyes to get visual input
(cid:73) update beliefs about text identity as visual input arrives

(2010-06-01)

computational psycholinguistics tutorial

4 / 15

an implemented model

overview

an implemented model

framework is general

(cid:73) start with prior expectations for the text (language knowledge)
(cid:73) move eyes to get visual input
(cid:73) update beliefs about text identity as visual input arrives

pieces of a model in this framework

(2010-06-01)

computational psycholinguistics tutorial

4 / 15

an implemented model

overview

an implemented model

framework is general

(cid:73) start with prior expectations for the text (language knowledge)
(cid:73) move eyes to get visual input
(cid:73) update beliefs about text identity as visual input arrives

pieces of a model in this framework

1. formal problem of reading: possible actions a reader can take

(2010-06-01)

computational psycholinguistics tutorial

4 / 15

an implemented model

overview

an implemented model

framework is general

(cid:73) start with prior expectations for the text (language knowledge)
(cid:73) move eyes to get visual input
(cid:73) update beliefs about text identity as visual input arrives

pieces of a model in this framework

1. formal problem of reading: possible actions a reader can take

2. visual input

(2010-06-01)

computational psycholinguistics tutorial

4 / 15

an implemented model

overview

an implemented model

framework is general

(cid:73) start with prior expectations for the text (language knowledge)
(cid:73) move eyes to get visual input
(cid:73) update beliefs about text identity as visual input arrives

pieces of a model in this framework

1. formal problem of reading: possible actions a reader can take

2. visual input

3. behavior policy: how the model decides between the actions

(2010-06-01)

computational psycholinguistics tutorial

4 / 15

an implemented model

overview

an implemented model

framework is general

(cid:73) start with prior expectations for the text (language knowledge)
(cid:73) move eyes to get visual input
(cid:73) update beliefs about text identity as visual input arrives

pieces of a model in this framework

1. formal problem of reading: possible actions a reader can take

2. visual input

3. behavior policy: how the model decides between the actions

next up: describing each of these pieces for our current model

(2010-06-01)

computational psycholinguistics tutorial

4 / 15

an implemented model

formal reading problem

the model: formal reading problem

(2010-06-01)

computational psycholinguistics tutorial

5 / 15

an implemented model

formal reading problem

the model: formal reading problem

assume discrete timesteps and on each:

(cid:73) get visual input about sentence around the    xated character
(cid:73) update beliefs about sentence identity (vision + context)
(cid:73) choose an action

(2010-06-01)

computational psycholinguistics tutorial

5 / 15

an implemented model

formal reading problem

the model: formal reading problem ii

four possible actions

(cid:73) continue    xating current location
(cid:73) initiate a forward saccade to position t
(cid:73) initiate a backward saccade to position t
(cid:73) stop reading the sentence

(2010-06-01)

computational psycholinguistics tutorial

6 / 15

an implemented model

formal reading problem

the model: formal reading problem ii

four possible actions

(cid:73) continue    xating current location (deterministic)
(cid:73) initiate a forward saccade to position t
(cid:73) initiate a backward saccade to position t
(cid:73) stop reading the sentence (deterministic)

(2010-06-01)

computational psycholinguistics tutorial

6 / 15

an implemented model

formal reading problem

the model: formal reading problem ii

four possible actions

(cid:73) continue    xating current location (deterministic)
(cid:73) initiate a forward saccade to position t
(cid:73) initiate a backward saccade to position t
(cid:73) stop reading the sentence (deterministic)

when a saccade is initiated

(cid:73) one timestep of delay (saccade execution lags behind initiation)
(cid:73) then, landing position normally distributed around t

(cid:73) (variance increases with intended distance)

(2010-06-01)

computational psycholinguistics tutorial

6 / 15

an implemented model

visual input

the model: visual input

(2010-06-01)

computational psycholinguistics tutorial

7 / 15

...asaca*tsatatat...1an implemented model

visual input

the model: visual input

(cid:73) acuity curve given by asymmetric gaussian (as in swift)
(cid:73) narrow foveal input (14 highest chars)
(cid:73) wide peripheral input (20 highest chars)

(2010-06-01)

computational psycholinguistics tutorial

7 / 15

...asaca*tsatatat...1an implemented model

visual input

the model: visual input

foveal input

(cid:73) veridical information about word boundaries
(cid:73) noisy information about letter identity (noise related to acuity)
(cid:73) no letter confusability (following norris, 2006)

peripheral input

(cid:73) veridical information about word boundaries only

(2010-06-01)

computational psycholinguistics tutorial

7 / 15

...asaca*tsatatat...1an implemented model

implementation

implementation

noisy visual input

(cid:73) represent characters as 26-d vectors (each: one 1, 25 0s)
(cid:73) visual input     gaussian
(cid:73) mean = true identity
(cid:73) diagonal covariance matrix (magnitude relates to visual acuity)

beliefs as wfsts

(cid:73) grammar (prior) represented with wfst
(cid:73) visual input (likelihood) given by wfst

(cid:73) each arc gives liklihood of character identity at that position

(cid:73) posterior = normalized intersection of grammar and visual input

(2010-06-01)

computational psycholinguistics tutorial

8 / 15

an implemented model

behavior policy

behavior policy: how to choose actions

(2010-06-01)

computational psycholinguistics tutorial

9 / 15

an implemented model

behavior policy

behavior policy: how to choose actions

(cid:73) model   s belief state is a distribution over possible sentences
(cid:73) decisions made based on parameterized policy sensitive to current

belief state

(2010-06-01)

computational psycholinguistics tutorial

9 / 15

an implemented model

behavior policy

behavior policy: how to choose actions

(cid:73) model   s belief state is a distribution over possible sentences
(cid:73) decisions made based on parameterized policy sensitive to current

belief state

(cid:73) policy might be sensitive to many things
(cid:73) expectations about upcoming material
(cid:73) ambiguity in structural analysis
(cid:73) semantics
(cid:73) relevance to expected comprehension question
(cid:73) . . .

(2010-06-01)

computational psycholinguistics tutorial

9 / 15

an implemented model

behavior policy

behavior policy: how to choose actions

(cid:73) model   s belief state is a distribution over possible sentences
(cid:73) decisions made based on parameterized policy sensitive to current

belief state

(cid:73) policy might be sensitive to many things
(cid:73) expectations about upcoming material
(cid:73) ambiguity in structural analysis
(cid:73) semantics
(cid:73) relevance to expected comprehension question
(cid:73) . . .

today   s simple policy

(cid:73) confidence in a character position: id203 of most likely

character under model beliefs

(cid:73) move left to right, bringing up con   dence in each position to   
(cid:73) make regression if con   dence about a previous position falls below   

(cid:73) (if    = 0, never make a regression)

(2010-06-01)

computational psycholinguistics tutorial

9 / 15

experiment motivation

experiment

motivation

(cid:73) one advantage of this framework: actions are goal-directed
(cid:73) thus, we can ask a new question:

(cid:73) how should reading behavior change depending on the reader   s goal?

(2010-06-01)

computational psycholinguistics tutorial

10 / 15

experiment motivation

experiment

motivation

(cid:73) one advantage of this framework: actions are goal-directed
(cid:73) thus, we can ask a new question:

(cid:73) how should reading behavior change depending on the reader   s goal?

speci   cally, using simple policy family and simple goal functions

(cid:73) simple policy family: how should    and    change?
(cid:73) goal functions: relative values of timesteps t vs. accuracy l

(2010-06-01)

computational psycholinguistics tutorial

10 / 15

experiment motivation

experiment

motivation

(cid:73) one advantage of this framework: actions are goal-directed
(cid:73) thus, we can ask a new question:

(cid:73) how should reading behavior change depending on the reader   s goal?

speci   cally, using simple policy family and simple goal functions

(cid:73) simple policy family: how should    and    change?
(cid:73) goal functions: relative values of timesteps t vs. accuracy l

experiment:    nd optimal values of    and    for three goal functions

(2010-06-01)

computational psycholinguistics tutorial

10 / 15

experiment: methods

experiment methods

task

(cid:73) simulate reading on sentences from the schilling et al. (1998) corpus
(cid:73) measure average timesteps t and accuracy l (log prob.)

(2010-06-01)

computational psycholinguistics tutorial

11 / 15

experiment: methods

experiment methods

task

(cid:73) simulate reading on sentences from the schilling et al. (1998) corpus
(cid:73) measure average timesteps t and accuracy l (log prob.)

model

(cid:73) bigram language model trained on british national corpus
(cid:73) implemented with weighted    nite state automata using openfst

(allauzen et al., 2007)

(2010-06-01)

computational psycholinguistics tutorial

11 / 15

experiment: methods ii

experiment methods

formal goal functions

(cid:73) consider linear combinations of l and t :
where        [0, 1] gives weighting of time
(cid:73) we optimize for 3 goals:        {.025, .1, .4}

l(1      )    t   

(2010-06-01)

computational psycholinguistics tutorial

12 / 15

experiment: methods ii

experiment methods

formal goal functions

(cid:73) consider linear combinations of l and t :
where        [0, 1] gives weighting of time
(cid:73) we optimize for 3 goals:        {.025, .1, .4}

l(1      )    t   

optimization

(cid:73) use the pegasus method (ng & jordan, 2000) to estimate

performance for a given [  ,   ]

(cid:73) (essentially, just average performance over sentences from schilling)
(cid:73) then, we use standard hillclimbing techniques to    nd optimum

(2010-06-01)

computational psycholinguistics tutorial

12 / 15

experiment: results

experiment

results

  
.025
.1
.4

  
.90
.36
.18

  
.99
.80
.38

optimization worked

(cid:73)    and    decrease as time valued more

(2010-06-01)

computational psycholinguistics tutorial

13 / 15

experiment: results

experiment

results

  
.025
.1
.4

  
.90
.36
.18

  
.99
.80
.38

timesteps accuracy (prob.)
41.2
25.8
16.4

-0.02 (p     .98)
-0.90 (p     .41)
-4.59 (p     .01)

optimization worked

(cid:73)    and    decrease as time valued more
(cid:73) mean t and l decrease as time valued more

(2010-06-01)

computational psycholinguistics tutorial

13 / 15

conclusion

conclusion

a new framework for eye movements in reading

(cid:73) readers achieve diverse goals by getting info about text identity
(cid:73) text identity is normatively given by bayesian id136 combining

(cid:73) language knowledge (prior)
(cid:73) visual input (likelihood)

(2010-06-01)

computational psycholinguistics tutorial

14 / 15

conclusion

conclusion

a new framework for eye movements in reading

(cid:73) readers achieve diverse goals by getting info about text identity
(cid:73) text identity is normatively given by bayesian id136 combining

(cid:73) language knowledge (prior)
(cid:73) visual input (likelihood)

(cid:73) hypothesis: eye movements are produced to get visual input
(cid:73) see also

(cid:73) reichle & laurent (2006): related id23 approach
(cid:73) nilsson & nivre (2010): predicting eye movements in reading with

id148

(2010-06-01)

computational psycholinguistics tutorial

14 / 15

conclusion

future directions

technical directions

(cid:73) so far, very simple two-parameter behavioral policies
(cid:73) investigating richer, more realistic classes of policies
(cid:73) investigating using richer language models

(2010-06-01)

computational psycholinguistics tutorial

15 / 15

conclusion

future directions

technical directions

(cid:73) so far, very simple two-parameter behavioral policies
(cid:73) investigating richer, more realistic classes of policies
(cid:73) investigating using richer language models

modeling reader goals

(cid:73) exp. demonstrated model   s    exibility to adapt to di   erent reader goals
(cid:73) allows asking new questions:

(2010-06-01)

computational psycholinguistics tutorial

15 / 15

conclusion

future directions

technical directions

(cid:73) so far, very simple two-parameter behavioral policies
(cid:73) investigating richer, more realistic classes of policies
(cid:73) investigating using richer language models

modeling reader goals

(cid:73) exp. demonstrated model   s    exibility to adapt to di   erent reader goals
(cid:73) allows asking new questions: how should reading behavior change as

(cid:73) . . . accuracy is valued more or less relative to time?

(2010-06-01)

computational psycholinguistics tutorial

15 / 15

conclusion

future directions

technical directions

(cid:73) so far, very simple two-parameter behavioral policies
(cid:73) investigating richer, more realistic classes of policies
(cid:73) investigating using richer language models

modeling reader goals

(cid:73) exp. demonstrated model   s    exibility to adapt to di   erent reader goals
(cid:73) allows asking new questions: how should reading behavior change as

(cid:73) . . . accuracy is valued more or less relative to time?
(cid:73) . . . readers learn comprehension questions all ask about direct object?

(2010-06-01)

computational psycholinguistics tutorial

15 / 15

conclusion

future directions

technical directions

(cid:73) so far, very simple two-parameter behavioral policies
(cid:73) investigating richer, more realistic classes of policies
(cid:73) investigating using richer language models

modeling reader goals

(cid:73) exp. demonstrated model   s    exibility to adapt to di   erent reader goals
(cid:73) allows asking new questions: how should reading behavior change as

(cid:73) . . . accuracy is valued more or less relative to time?
(cid:73) . . . readers learn comprehension questions all ask about direct object?
(cid:73) . . . readers learn that the text uses a lot of di   cult words?

(2010-06-01)

computational psycholinguistics tutorial

15 / 15

conclusion

future directions

technical directions

(cid:73) so far, very simple two-parameter behavioral policies
(cid:73) investigating richer, more realistic classes of policies
(cid:73) investigating using richer language models

modeling reader goals

(cid:73) exp. demonstrated model   s    exibility to adapt to di   erent reader goals
(cid:73) allows asking new questions: how should reading behavior change as

(cid:73) . . . accuracy is valued more or less relative to time?
(cid:73) . . . readers learn comprehension questions all ask about direct object?
(cid:73) . . . readers learn that the text uses a lot of di   cult words?
(cid:73) . . . a number of other exciting questions!

(2010-06-01)

computational psycholinguistics tutorial

15 / 15

uncertain input and surprisal

    surprisal theory: the reading time for a 

word is a function of its id203

    and obviously, its id203 is a function 

of its identity...

    ...but we don't know its identity! then 

what?

    maybe the brain makes an educated guess

 
 
this section: (smith & levy, 2010)

average neighborhood surprisal

s = 2
p(cat) = 0.9

s = 5
p(eat) = 0.1

best guess: s = 2 * 0.9 + 5 * 0.1

 

= 2.3

 

average neighborhood surprisal

s = 2
p(cat) = 0.1

s = 5
p(eat) = 0.9

best guess: s = 2 * 0.1 + 5 * 0.9

 

= 4.7

 

model: visual noise

    simplify: assume number of letters is 

known; only their identity is in doubt

    letter confusability norms: engel, 

dougherty, & jones, 1973; geyer, 1977

    one free parameter scales the 

confusability matrix to set overall level of 
noise

    use bayes' rule to combine top-down prior 

from context with bottom-up visual 
information

 

 

 

 

average neighborhood surprisal
    defined as: the average of the surprisal of 

all words in the language in the current 
context, weighted by the id203 that 
each word is, in fact, the currently viewed 
word

    can define average neighborhood x for 

any word property     frequency, 
imageability,    

    prediction: average neighborhood surprisal 
(ans) is a better predictor of reading times 
than raw surprisal (rs)

 

 

results: first fixations

    best-fitting noise level was rather low: 
~66% accuracy at naming single letters
    ans and rs highly correlated: r2 = 0.96
    nevertheless, ans unambiguously pre-

empts rs in regression model:

    ans: t(182155) = -4.2, p < 0.001
    rs: t(182155) = -0.49, n.s.

    prediction: confirmed

 

 

second fixations

    surprisal effect is also seen in the duration 
of re-fixations. we predict that these should 
show a reduced effect of visual noise 
relative to first fixations

    result 1: ans still beats rs:

    ans: t(42010) = -4.2, p < 0.001
    rs: t(42010) = 1.8, p = 0.065

    result 2: noise parameter is ~half that for 

first fixations

 

 

the next mystery...

    can define average neighborhood x for 

any word property     frequency, 
imageability,    

 

 

the next mystery...

    can define average neighborhood x for 

any word property     frequency, 
imageability,     we tried this; it didn't work.
    conclusion?: surprisal-sensitive brain process is 

earlier than frequency-sensitive brain process, has 
less visual data available

    conclusion?: surprisal-sensitive process is more 

complicated, must get started sooner

    conclusion?: visual noise is minimal, frequency 
effect is part of visual processing, and surprisal 
noise is really internal representational noise

 

 

    ...?

remaining challenges and future directions 

       we hope to have convinced you of a few things: 

       the picture of human language comprehension as rational, 
incremental deployment of probabilistic knowledge 
accounts for many empirical phenomena 
       this picture spans both comprehension and production 
       formal tools from computational linguistics are incredibly 
useful in both theory development and data analysis 
       fine-grained modeling of cognitive and environmental 
constraints in language processing is important 
       there are many open issues ripe for exploration 

       now i   ll take a moment to mention a few outstanding 

challenges and directions for future work 

models of memory recall 

       you may remember this challenge for surprisal: 

       subject-extracted relative clause 

the reporter who sent the photographer to... 

       object-extracted relative clause 

the reporter who the photographer sent to... 

       recent work on similarity-based interference seems to 

difficulty occurs here 

identify confusability of preceding nps as a culprit 
       object-extracted rcs aren   t always hard (warren et al., 2002) 

  the reporter who i sent to... 

       direct manipulations show influence of similarity (gordon et al., 2002)  

memory list: (table, sink, truck) 
it was the boat that the guy who lived by the sea fixed/sailed    

hard 

easy 

integrating memory & expectations 
       lewis & vasishth (2005) have an incremental (left-

corner), serial id18 production system implemented in 
act-r 

       new input triggers potentially inaccurate recall of 

previous material 

integrating memory & expectations ii 

       demberg & keller (2009) have 
an incremental tree-adjoining 
grammar model with both 
prediction & verification 
components 

       retrospective verification cost 

gives correct predictions for 
english rcs 

modeling online pragmatic id136 
       let   s go back to the classic visual world experiment 

   put the apple on the towel in the box.      (tanenhaus et al., 1995) 

       what knowledge is required to correctly draw the 

id136 between the context and the correct syntax? 
       grammatical knowledge 
       linking linguistic constituents to referents in the visual field 
       pragmatics: when would a speaker use a postmodifier? 

discourse structure 

       most work analyzing syntactic-level phenomena and 
down has focused on isolated-sentence phenomena 
       but that   s not how language is used naturalistically 
       inter-clause and inter-sentence meaning relations are 

essential 

       this has been well-known in psycholinguistics for a while 

a burglar broke into a bank carrying some dynamite.  he 
planned to blow up a safe.  once inside he saw that there 
was a safe with a new lock and a safe/strongbox with an 
old lock. 

the burglar blew up the safe with the     

       but how to model these effects is open territory 

(altmann & steedman, 1988) 

broad-coverage assessment of models 
       traditional psycholinguistic experiments have tended to 
use isolated sentences with tightly controlled content & 
structure 

       advantages: your world is simpler 
       disadvantages: your world may be mostly imaginary 
       increasing trend toward implementing psycholinguistic 
models with broad coverage and testing on naturalistic 
datasets 

       you   ve seen this already in many cases 

       demberg & keller, 2008; smith & levy, 2008 
       jaeger, 2006, 2010; levy & jaeger, 2007 
       roark et al., 2009 

       knowledge of computational linguistics is essential 

increasing connections between fields 

       computational linguistics and psycholinguistics have had 

many points of contact throughout the years 

       we seem to be on an upswing 

       cuny 2008: special session on computational models 
       acl 2010: there   s a psycholinguistics track! 
       workshop at acl 2010: cognitive modeling and 
computational linguistics (organized by john hale) 

       help us continue this trend! 

