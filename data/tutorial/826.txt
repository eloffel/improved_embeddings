foundations and trends r(cid:1) in
machine learning
vol. 4, no. 4 (2011) 267   373
c(cid:1) 2012 c. sutton and a. mccallum
doi: 10.1561/2200000013

an introduction to conditional

random fields

by charles sutton and andrew mccallum

contents

1 introduction

1.1

implementation details

2 modeling

2.1 graphical modeling
2.2 generative versus discriminative models
2.3 linear-chain crfs
2.4 general crfs
2.5 feature engineering
2.6 examples
2.7 applications of crfs
2.8 notes on terminology

3 overview of algorithms

4 id136

4.1 linear-chain crfs
4.2
4.3

id136 in id114
implementation concerns

268

271

272

272
278
286
290
293
298
306
308

310

313

314
318
328

5 parameter estimation

5.1 maximum likelihood
5.2 stochastic gradient methods
5.3 parallelism
5.4 approximate training
5.5

implementation concerns

6 related work and future directions

6.1 related work
6.2 frontier areas

acknowledgments

references

331

332
341
343
343
350

352

352
359

362

363

foundations and trends r(cid:1) in
machine learning
vol. 4, no. 4 (2011) 267   373
c(cid:1) 2012 c. sutton and a. mccallum
doi: 10.1561/2200000013

an introduction to conditional

random fields

charles sutton1 and andrew mccallum2

1 school of informatics, university of edinburgh, edinburgh, eh8 9ab,

uk, csutton@inf.ed.ac.uk

2 department of computer science, university of massachusetts, amherst,

ma, 01003, usa, mccallum@cs.umass.edu

abstract

many tasks involve predicting a large number of variables that depend
on each other as well as on other observed variables. structured
prediction methods are essentially a combination of classi   cation and
graphical modeling. they combine the ability of id114
to compactly model multivariate data with the ability of classi   ca-
tion methods to perform prediction using large sets of input features.
this survey describes conditional random    elds, a popular probabilistic
method for id170. crfs have seen wide application in
many areas, including natural language processing, id161,
and bioinformatics. we describe methods for id136 and parame-
ter estimation for crfs, including practical issues for implementing
large-scale crfs. we do not assume previous knowledge of graphical
modeling, so this survey is intended to be useful to practitioners in a
wide variety of    elds.

1

introduction

fundamental to many applications is the ability to predict multiple
variables that depend on each other. such applications are as diverse
as classifying regions of an image [49, 61, 69], estimating the score in a
game of go [130], segmenting genes in a strand of dna [7], and syn-
tactic parsing of natural-language text [144]. in such applications, we
wish to predict an output vector y = {y0, y1, . . . , yt} of random vari-
ables given an observed feature vector x. a relatively simple example
from natural-language processing is part-of-speech tagging, in which
each variable ys is the part-of-speech tag of the word at position s, and
the input x is divided into feature vectors {x0,x1, . . . ,xt}. each xs
contains various information about the word at position s, such as its
identity, orthographic features such as pre   xes and su   xes, member-
ship in domain-speci   c lexicons, and information in semantic databases
such as id138.

one approach to this multivariate prediction problem, especially
if our goal is to maximize the number of labels ys that are correctly
classi   ed, is to learn an independent per-position classi   er that maps
x (cid:1)    ys for each s. the di   culty, however, is that the output variables
have complex dependencies. for example, in english adjectives do not

268

269

usually follow nouns, and in id161, neighboring regions in an
image tend to have similar labels. another di   culty is that the output
variables may represent a complex structure such as a parse tree, in
which a choice of what grammar rule to use near the top of the tree
can have a large e   ect on the rest of the tree.

a natural way to represent the manner in which output variables
depend on each other is provided by id114. graphical
models     which include such diverse model families as bayesian net-
works, neural networks, factor graphs, markov random    elds, ising
models, and others     represent a complex distribution over many vari-
ables as a product of local factors on smaller subsets of variables. it
is then possible to describe how a given factorization of the proba-
bility density corresponds to a particular set of conditional indepen-
dence relationships satis   ed by the distribution. this correspondence
makes modeling much more convenient because often our knowledge of
the domain suggests reasonable conditional independence assumptions,
which then determine our choice of factors.

much work in learning with id114, especially in statisti-
cal natural-language processing, has focused on generative models that
explicitly attempt to model a joint id203 distribution p(y,x) over
inputs and outputs. although this approach has advantages, it also
has important limitations. not only can the dimensionality of x be
very large, but the features may have complex dependencies, so con-
structing a id203 distribution over them is di   cult. modeling the
dependencies among inputs can lead to intractable models, but ignoring
them can lead to reduced performance.

a solution to this problem is a discriminative approach, similar to
that taken in classi   ers such as id28. here we model the
conditional distribution p(y|x) directly, which is all that is needed for
classi   cation. this is the approach taken by conditional random    elds
(crfs). crfs are essentially a way of combining the advantages of dis-
criminative classi   cation and graphical modeling, combining the ability
to compactly model multivariate outputs y with the ability to leverage
a large number of input features x for prediction. the advantage to a
conditional model is that dependencies that involve only variables in x
play no role in the conditional model, so that an accurate conditional

270

introduction

model can have much simpler structure than a joint model. the di   er-
ence between generative models and crfs is thus exactly analogous
to the di   erence between the naive bayes and id28 classi-
   ers. indeed, the multinomial id28 model can be seen as
the simplest kind of crf, in which there is only one output variable.
there has been a large amount of interest in applying crfs to
many di   erent problems. successful applications have included text
processing [105, 124, 125], bioinformatics [76, 123], and id161
[49, 61]. although early applications of crfs used linear chains, recent
applications of crfs have also used more general graphical structures.
general graphical structures are useful for predicting complex struc-
tures, such as graphs and trees, and for relaxing the independence
assumption among entities, as in relational learning [142].

this survey describes modeling, id136, and parameter estima-
tion using crfs. we do not assume previous knowledge of graphical
modeling, so this survey is intended to be useful to practitioners in a
wide variety of    elds. we begin by describing modeling issues in crfs
(section 2), including linear-chain crfs, crfs with general graphical
structure, and hidden crfs that include latent variables. we describe
how crfs can be viewed both as a generalization of the well-known
id28 procedure, and as a discriminative analogue of the
hidden markov model.

in the next two sections, we describe id136 (section 4) and
learning (section 5) in crfs. in this context, id136 refers both
to the task of computing the marginal distributions of p(y|x) and to
the related task of computing the maximum id203 assignment
y    = arg maxy p(y|x). with respect to learning, we will focus on the
parameter estimation task, in which p(y|x) is determined by parame-
ters that we will choose in order to best    t a set of training examples
{x(i),y(i)}n
i=1. the id136 and learning procedures are often closely
coupled, because learning usually calls id136 as a subroutine.

finally, we discuss relationships between crfs and other families
including other id170 methods, neural

of models,
networks, and maximum id178 markov models (section 6).

1.1 implementation details

271

1.1

implementation details

throughout this survey, we strive to point out implementation details
that are sometimes elided in the research literature. for example, we
discuss issues relating to feature engineering (section 2.5), avoiding
numerical under   ow during id136 (section 4.3), and the scalability
of crf training on some benchmark problems (section 5.5).

since this is the    rst of our sections on implementation details, it
seems appropriate to mention some of the available implementations of
crfs. at the time of writing, a few popular implementations are:

crf++
mallet
grmm
crfsuite
factorie

http://crfpp.sourceforge.net/
http://mallet.cs.umass.edu/
http://mallet.cs.umass.edu/grmm/
http://www.chokkan.org/software/crfsuite/
http://www.factorie.cc

also, software for markov logic networks (such as alchemy:
http://alchemy.cs.washington.edu/) can be used to build crf models.
alchemy, grmm, and factorie are the only toolkits of which we
are aware that handle arbitrary graphical structure.

2

modeling

in this section, we describe crfs from a modeling perspective, explain-
ing how a crf represents distributions over structured outputs as a
function of a high-dimensional input vector. crfs can be understood
both as an extension of the id28 classi   er to arbitrary
graphical structures, or as a discriminative analog of generative models
of structured data, such as id48.

we begin with a brief

introduction to graphical modeling
(section 2.1) and a description of generative and discriminative models
in nlp (section 2.2). then we will be able to present the for-
mal de   nition of a crf, both for the commonly-used case of linear
chains (section 2.3), and for general graphical structures (section 2.4).
because the accuracy of a crf is strongly dependent on the features
that are used, we also describe some commonly used tricks for engineer-
ing features (section 2.5). finally, we present two examples of appli-
cations of crfs (section 2.6), a broader survey of typical application
areas for crfs (section 2.7).

2.1 graphical modeling

graphical modeling is a powerful framework for representation and
id136 in multivariate id203 distributions. it has proven useful

272

2.1 graphical modeling

273

in diverse areas of stochastic modeling, including coding theory [89],
id161 [41], id99 [103], bayesian statis-
tics [40], and natural-language processing [11, 63].

distributions over many variables can be expensive to represent
na    vely. for example, a table of joint probabilities of n binary vari-
ables requires storing o(2n)    oating-point numbers. the insight of the
graphical modeling perspective is that a distribution over very many
variables can often be represented as a product of local functions that
each depend on a much smaller subset of variables. this factorization
turns out to have a close connection to certain conditional indepen-
dence relationships among the variables     both types of information
being easily summarized by a graph. indeed, this relationship between
factorization, conditional independence, and graph structure comprises
much of the power of the graphical modeling framework: the con-
ditional independence viewpoint is most useful for designing models,
and the factorization viewpoint is most useful for designing id136
algorithms.

in the rest of this section, we introduce id114 from both
the factorization and conditional independence viewpoints, focusing on
those models which are based on undirected graphs. a more detailed
modern treatment of graphical modeling and approximate id136 is
available in a textbook by koller and friedman [57].

2.1.1 undirected models

we consider id203 distributions over sets of random variables y .
we index the variables by integers s     1,2, . . .|y |. every variable ys     y
takes outcomes from a set y, which can be either continuous or discrete,
although we consider only the discrete case in this survey. an arbitrary
assignment to y is denoted by a vector y. given a variable ys     y , the
notation ys denotes the value assigned to ys by y. the notation 1{y=y(cid:2)}
(cid:3)
denotes an indicator function of y which takes the value 1 when y = y
and 0 otherwise. we also require notation for marginalization. for a
   xed-variable assignment ys, we use the summation
y\ys to indicate
a summation over all possible assignments y whose value for variable
ys is equal to ys.

(cid:1)

a(cid:2)

1
z

274 modeling

suppose that we believe that a id203 distribution p of interest
can be represented by a product of factors of the form   a(ya) where
a is an integer index that ranges from 1 to a, the number of factors.
each factor   a depends only on a subset ya     y of the variables. the
value   a(ya) is a non-negative scalar that can be thought of as a mea-
sure of how compatible the values ya are with each other. assignments
that have higher compatibility values will have higher id203. this
factorization can allow us to represent p much more e   ciently, because
the sets ya may be much smaller than the full variable set y .

an undirected graphical model is a family of id203 distribu-
tions that each factorize according to a given collection of factors.
formally, given a collection of subsets {ya}a
a=1 of y , an undirected
graphical model is the set of all distributions that can be written as

p(y) =

  a(ya),

(2.1)
for any choice of factors f = {  a} that have   a(ya)     0 for all ya.
(the factors are also called local functions or compatibility functions.)
we will use the term random    eld to refer to a particular distribution
among those de   ned by an undirected model.

a=1

the constant z is a id172 factor that ensures the distribu-

tion p sums to 1. it is de   ned as

(cid:3)

a(cid:2)

y

a=1

z =

  a(ya).

(2.2)
the quantity z, considered as a function of the set f of factors, is
also called the partition function. notice that the summation in (2.2) is
over the exponentially many possible assignments to y. for this reason,
computing z is intractable in general, but much work exists on how to
approximate it (see section 4).

the reason for the term    graphical model    is that the factoriza-
tion factorization (2.1) can be represented compactly by means of a
graph. a particularly natural formalism for this is provided by factor
graphs [58]. a factor graph is a bipartite graph g = (v, f, e) in which
one set of nodes v = {1,2, . . . ,|y |} indexes the random variables in the
model, and the other set of nodes f = {1,2, . . . , a} indexes the factors.

275
the semantics of the graph is that if a variable node ys for s     v is
connected to a factor node   a for a     f , then ys is one of the argu-
ments of   a. so a factor graph directly describes the way in which a
distribution p decomposes into a product of local functions.

2.1 graphical modeling

we formally de   ne the notion of whether a factor graph    describes   
a given distribution or not. let n(a) be the neighbors of the factor with
index a, i.e., a set of variable indices. then:

de   nition 2.1. a distribution p(y) factorizes according to a factor
graph g if there exists a set of local functions   a such that p can be
written as

p(y) = z

   1

  a(yn(a))

(2.3)

(cid:2)

a   f

a factor graph g describes an undirected model in the same way
that a collection of subsets does. in (2.1), take the collection of subsets
to be the set of neighborhoods {yn(a)|   a     f}. the resulting undi-
rected graphical model from the de   nition in (2.1) is exactly the set of
all distributions that factorize according to g.

for example, figure 2.1 shows an example of a factor graph
over three random variables. in that    gure, the circles are variable
nodes, and the shaded boxes are factor nodes. we have labelled the
nodes according to which variables or factors they index. this fac-
tor graph describes the set of all distributions p over three variables
that can be written as p(y1, y2, y3) =   1(y1, y2)  2(y2, y3)  3(y1, y3) for
all y = (y1, y2, y3).

there is a close connection between the factorization of a graphical
model and the conditional independencies among the variables in its
domain. this connection can be understood by means of a di   erent

fig. 2.1 an example of a factor graph over three variables.

276 modeling

undirected graph, called a markov network, which directly represents
conditional independence relationships in a multivariate distribution.
markov networks are graphs over random variables only, rather than
random variables and factors. now let g be an undirected graph over
integers v = {1,2, . . . ,|y |} that index each random variable of interest.
for a variable index s     v , let n(s) denote its neighbors in g. then we
say that a distribution p is markov with respect to g if it satis   es the
local markov property: for any two variables ys, yt     y , the variable ys
is independent of yt conditioned on its neighbors yn(s). intuitively,
this means that yn(s) on its own contains all of the information that is
useful for predicting ys.

given a factorization of a distribution p as in (2.1), a corresponding
markov network can be constructed by connecting all pairs of variables
that share a local function. it is straightforward to show that p is
markov with respect to this graph, because the conditional distribution
p(ys|yn(s)) that follows from (2.1) is a function only of variables that
appear in the markov blanket.

a markov network has an undesirable ambiguity from the factoriza-
tion perspective. consider the three-node markov network in figure 2.2
(left). any distribution that factorizes as p(y1, y2, y3)     f(y1, y2, y3) for
some positive function f is markov with respect to this graph. how-
ever, we may wish to use a more restricted parameterization, where
p(y1, y2, y3)     f(y1, y2)g(y2, y3)h(y1, y3). this second model family is
a strict subset of the    rst, so in this smaller family we might not
require as much data to accurately estimate the distribution. but
the markov network formalism cannot distinguish between these two

fig. 2.2 a markov network with an ambiguous factorization. both of the factor graphs at
right factorize according to the markov network at left.

parameterizations. in contrast, the factor graph depicts the factoriza-
tion of the model unambiguously.

2.1 graphical modeling

277

2.1.2 directed models

s(cid:2)

whereas the local functions in an undirected model need not have a
direct probabilistic interpretation, a directed graphical model describes
how a distribution factorizes into local id155 distribu-
tions. let g be a directed acyclic graph, in which   (s) are the indices
of the parents of ys in g. a directed graphical model is a family of
distributions that factorize as:

p(ys|y  (s)).

s=1

p(y) =

(2.4)
we refer to the distributions p(ys|y  (s)) as local conditional distribu-
tions. note that   (s) can be empty for variables that have no parents.
in this case p(ys|y  (s)) should be understood as p(ys).

it can be shown by induction that p is properly normalized. directed
models can be thought of as a kind of factor graph in which the indi-
vidual factors are locally normalized in a special fashion so that (a) the
factors equal conditional distributions over subsets of variables, and
(b) the id172 constant z = 1. directed models are often used
as generative models, as we explain in section 2.2.3. an example of a
directed model is the naive bayes model (2.7), which is depicted graph-
ically in figure 2.3 (left). in those    gures, the shaded nodes indicate
variables that are observed in some data set. this is a convention that
we will use throughout this survey.

2.1.3 inputs and outputs

this survey considers the situation in which we know in advance which
variables we will want to predict. the variables in the model will be
partitioned into a set of input variables x that we assume are always
observed, and another set y of output variables that we wish to predict.
for example, an assignment x might represent a vector of each word
that occurs in a sentence, and y a vector of part of speech labels for
each word.

278 modeling

we will be interested in building distributions over the combined
set of variables x     y , and we will extend the previous notation in
order to accommodate this. for example, an undirected model over x
and y would be given by

a(cid:2)

a=1

p(x,y) =

1
z

(cid:3)

(cid:2)

x,y

a   f

  a(xa,ya),

(2.5)

where now each local function   a depends on two subsets of variables
xa     x and ya     y . the id172 constant becomes

z =

  a(xa,ya),

(2.6)

which now involves summing over all assignments both to x and y.

2.2 generative versus discriminative models

in this section we discuss several examples of simple id114
that have been used in natural language processing. although these
examples are well-known, they serve both to clarify the de   nitions in
the previous section, and to illustrate some ideas that will arise again
in our discussion of crfs. we devote special attention to the hidden
markov model (id48), because it is closely related to the linear-chain
crf.

one of the main purposes of this section is to contrast generative
and discriminative models. of the models that we will describe, two are
generative (the naive bayes and id48) and one is discriminative (the
id28 model). generative models are models that describe
how a label vector y can probabilistically    generate    a feature vector x.
discriminative models work in the reverse direction, describing directly
how to take a feature vector x and assign it a label y. in principle, either
type of model can be converted to the other type using bayes   s rule, but
in practice the approaches are distinct, each with potential advantages
as we describe in section 2.2.3.

2.2.1 classi   cation

first we discuss the problem of classi   cation, that is, predicting a single
discrete class variable y given a vector of features x = (x1, x2, . . . , xk).

2.2 generative versus discriminative models

279

k(cid:2)

one simple way to accomplish this is to assume that once the class
label is known, all the features are independent. the resulting classi   er
is called the naive bayes classi   er. it is based on a joint id203
model of the form:

p(y,x) = p(y)

p(xk|y).

(2.7)

k=1

this model can be described by the directed model shown in figure 2.3
(left). we can also write this model as a factor graph, by de   ning a
factor   (y) = p(y), and a factor   k(y, xk) = p(xk|y) for each feature xk.
this factor graph is shown in figure 2.3 (right).

another well-known classi   er that is naturally represented as a
graphical model is id28 (sometimes known as the max-
imum id178 classi   er in the nlp community). this classi   er can
be motivated by the assumption that the log id203, log p(y|x), of
each class is a linear function of x, plus a id172 constant.1 this
leads to the conditional distribution:

exp

  y,jxj

1

p(y|x) =
(cid:1)
y exp{  y +

z(x)

j=1

(cid:1)
j=1   y,jxj} is a normalizing constant, and
where z(x) =
  y is a bias weight that acts like log p(y) in naive bayes. rather than
using one weight vector per class, as in (2.8), we can use a di   erent
notation in which a single set of weights is shared across all the classes.
the trick is to de   ne a set of feature functions that are nonzero only

k

(2.8)

      
     y +

k(cid:3)

      
    ,

y

y

x

x

fig. 2.3 the naive bayes classi   er, as a directed model (left), and as a factor graph (right).

1 by log in this survey we will always mean the natural logarithm.

280 modeling

for a single class. to do this, the feature functions can be de   ned as
fy(cid:2),j(y,x) = 1{y(cid:2)=y}xj for the feature weights and fy(cid:2)(y,x) = 1{y(cid:2)=y} for
the bias weights. now we can use fk to index each feature function fy(cid:2),j,
and   k to index its corresponding weight   y(cid:2),j. using this notational
trick, the id28 model becomes:

p(y|x) =

1

z(x)

exp

  kfk(y,x)

.

(2.9)

(cid:11)

(cid:10)
k(cid:3)

k=1

we introduce this notation because it mirrors the notation for crfs
that we will present later.

2.2.2 sequence models

classi   ers predict only a single class variable, but the true power of
id114 lies in their ability to model many variables that are
interdependent. in this section, we discuss perhaps the simplest form
of dependency, in which the output variables in the graphical model
are arranged in a sequence. to motivate this model, we discuss an
application from natural language processing, the task of named-entity
recognition (ner). ner is the problem of identifying and classifying
proper names in text, including locations, such as china; people, such
as george bush; and organizations, such as the united nations. the
named-entity recognition task is, given a sentence, to segment which
words are part of an entity, and to classify each entity by type (person,
organization, location, and so on). the challenge of this problem is
that many named entity strings are too rare to appear even in a large
training set, and therefore the system must identify them based only
on context.

one approach to ner is to classify each word independently as one
of either person, location, organization, or other (meaning
not an entity). the problem with this approach is that it assumes
that given the input, all of the named-entity labels are independent.
in fact, the named-entity labels of neighboring words are dependent;
for example, while new york is a location, new york times is an
organization. one way to relax this independence assumption is to
arrange the output variables in a linear chain. this is the approach

2.2 generative versus discriminative models

281

taken by the hidden markov model (id48) [111]. an id48 models
a sequence of observations x = {xt}t
t=1 by assuming that there is
an underlying sequence of states y = {yt}t
t=1. we let s be the    nite
set of possible states and o the    nite set of possible observations,
i.e., xt     o and yt     s for all t. in the named-entity example, each
observation xt is the identity of the word at position t, and each state
yt is the named-entity label, that is, one of the entity types person,
location, organization, and other.

to model the joint distribution p(y,x) tractably, an id48 makes
it assumes that each state
two independence assumptions. first,
depends only on its immediate predecessor, that is, each state yt is
independent of all its ancestors y1, y2, . . . , yt   2 given the preceding state
yt   1. second, it also assumes that each observation variable xt depends
only on the current state yt. with these assumptions, we can specify an
id48 using three id203 distributions:    rst, the distribution p(y1)
over initial states; second, the transition distribution p(yt|yt   1); and
   nally, the observation distribution p(xt|yt). that is, the joint proba-
bility of a state sequence y and an observation sequence x factorizes as

p(y,x) =

p(yt|yt   1)p(xt|yt).

(2.10)

t=1

to simplify notation in the above equation, we create a    dummy   
initial state y0 which is clamped to 0 and begins every state sequence.
this allows us to write the initial state distribution p(y1) as p(y1|y0).
id48s have been used for many sequence labeling tasks in natural-
language processing such as part-of-speech tagging, named-entity
recognition, and information extraction.

2.2.3 comparison

both generative models and discriminative models describe distribu-
tions over (y,x), but they work in di   erent directions. a generative
model, such as the naive bayes classi   er and the id48, is a family
of joint distributions that factorizes as p(y,x) = p(y)p(x|y), that is, it
describes how to sample, or    generate,    values for features given the
label. a discriminative model, such as the id28 model, is

t(cid:2)

282 modeling
a family of conditional distributions p(y|x), that is, the classi   cation
rule is modeled directly. in principle, a discriminative model could also
be used to obtain a joint distribution p(y,x) by supplying a marginal
distribution p(x) over the inputs, but this is rarely needed.
the main conceptual di   erence between discriminative and gener-
ative models is that a conditional distribution p(y|x) does not include
a model of p(x), which is not needed for classi   cation anyway. the
di   culty in modeling p(x) is that it often contains many highly depen-
dent features that are di   cult to model. for example, in named-entity
recognition, a naive application of an id48 relies on only one feature,
the word   s identity. but many words, especially proper names, will
not have occurred in the training set, so the word-identity feature is
uninformative. to label unseen words, we would like to exploit other
features of a word, such as its capitalization, its neighboring words, its
pre   xes and su   xes, its membership in predetermined lists of people
and locations, and so on.

the principal advantage of discriminative modeling is that it is
better suited to including rich, overlapping features. to understand
this, consider the family of naive bayes distributions (2.7). this is a
family of joint distributions whose conditionals all take the    logistic
regression form    (2.9). but there are many other joint models, some
with complex dependencies among x, whose conditional distributions
also have the form (2.9). by modeling the conditional distribution
directly, we can remain agnostic about the form of p(x). discrimina-
tive models, such as crfs, make conditional independence assumptions
among y, and assumptions about how the y can depend on x, but do
not make conditonal independence assumptions among x. this point
also be understood graphically. suppose that we have a factor graph
representation for the joint distribution p(y,x). if we then construct a
graph for the conditional distribution p(y|x), any factors that depend
only on x vanish from the graphical structure for the conditional distri-
bution. they are irrelevant to the conditional because they are constant
with respect to y.

to include interdependent features in a generative model, we have
two choices. the    rst choice is to enhance the model to represent depen-
dencies among the inputs, e.g., by adding directed edges among each xt.
but this is often di   cult to do while retaining tractability. for example,

2.2 generative versus discriminative models

283

it is hard to imagine how to model the dependence between the capi-
talization of a word and its su   xes, nor do we particularly wish to do
so, since we always observe the test sentences anyway.

t

(cid:12)

(cid:12)

the second choice is to make simplifying independence assump-
tions, such as the naive bayes assumption. for example, an id48
model with a naive bayes assumption would have the form p(x,y) =
t=1 p(yt|yt   1)
k=1 p(xtk|yt). this idea can sometimes work well. but
it can also be problematic because the independence assumptions can
hurt performance. for example, although the naive bayes classi   er per-
forms well in document classi   cation, it performs worse on average
across a range of applications than id28 [19].

k

furthermore, naive bayes can produce poor id203 estimates.
as an illustrative example, imagine training naive bayes on a two
class problem in which all the features are repeated, that is, given
an original feature vector x = (x1, x2, . . . , xk), we transform it to x(cid:3) =
(x1, x1, x2, x2, . . . , xk, xk) and then run naive bayes. even though no
new information has been added to the data, this transformation will
increase the con   dence of the id203 estimates, by which we mean
that the naive bayes estimates of p(y|x(cid:3)) will tend to be farther from
0.5 than those of p(y|x).

assumptions like naive bayes can be especially problematic when
we generalize to sequence models, because id136 should combine
evidence from di   erent parts of the model. if id203 estimates of
the label at each sequence position are overcon   dent, it can be di   cult
to combine them sensibly.

the di   erence between naive bayes and id28 is due
only to the fact that the    rst is generative and the second discrimi-
native; the two classi   ers are, for discrete input, identical in all other
respects. naive bayes and id28 consider the same hypoth-
esis space, in the sense that any id28 classi   er can be
converted into a naive bayes classi   er with the same decision boundary,
and vice versa. another way of saying this is that the naive bayes model
(2.7) de   nes the same family of distributions as the id28
model (2.9), if we interpret it generatively as

exp{(cid:1)
(cid:1)
  y,  x exp{(cid:1)

k   kfk(y,x)}

k   kfk(  y,   x)} .

p(y,x) =

(2.11)

284 modeling

this means that if the naive bayes model (2.7) is trained to maximize
the conditional likelihood, we recover the same classi   er as from logis-
tic regression. conversely, if the id28 model is interpreted
generatively, as in (2.11), and is trained to maximize the joint likeli-
hood p(y,x), then we recover the same classi   er as from naive bayes.
in the terminology of ng and jordan [98], naive bayes and logistic
regression form a generative-discriminative pair. for a recent theoreti-
cal perspective on generative and discriminative models, see liang and
jordan [72].

in principle, it may not be clear why these approaches should be
so di   erent, because we can always convert between the two methods
using bayes rule. for example, in the naive bayes model, it is easy
to convert the joint p(y)p(x|y) into a conditional distribution p(y|x).
indeed, this conditional has the same form as the id28
model (2.9). and if we managed to obtain a    true    generative model
   (x|y) from which
for the data, that is, a distribution p
the data were actually sampled, then we could simply compute the true
   (y|x), which is exactly the target of the discriminative approach. but
p
it is precisely because we never have the true distribution that the two
approaches are di   erent in practice. estimating p(y)p(x|y)    rst, and
then computing the resulting p(y|x) (the generative approach) yields
a di   erent estimate than estimating p(y|x) directly. in other words,
generative and discriminative models both have the aim of estimating
p(y|x), but they get there in di   erent ways.

   (y,x) = p

   (y)p

one perspective for gaining insight into the di   erence between gen-
erative and discriminative modeling is due to minka [93]. suppose we
have a generative model pg with parameters   . by de   nition, this takes
the form:

pg(y,x;   ) = pg(y;   )pg(x|y;   ).

(2.12)

but we could also rewrite pg using the chain rule of id203 as

pg(y,x;   ) = pg(x;   )pg(y|x;   ),

(2.13)
(cid:1)
where pg(x;   ) and pg(y|x;   ) are computed by id136, i.e., pg(x;   ) =
y pg(y,x;   ) and pg(y|x;   ) = pg(y,x;   )/pg(x;   ).

2.2 generative versus discriminative models

285

now, compare this generative model to a discriminative model
(cid:1)
over the same family of joint distributions. to do this, we de   ne
a prior p(x) over inputs, such that p(x) could have arisen from pg
y pg(y,x|  
(cid:3)),
with some parameter setting. that is, p(x) = pc(x;   
(cid:3) will typically be distinct from the    in (2.13). we combine this
where   
with a conditional distribution pc(y|x;   ) that could also have arisen
from pg, that is, pc(y|x;   ) = pg(y,x;   )/pg(x;   ). then the resulting
distribution is

(cid:3)) =

pc(y,x) = pc(x;   

(cid:3)

)pc(y|x;   ).

(2.14)

by comparing (2.13) with (2.14), it can be seen that the conditional
approach has more freedom to    t the data, because it does not require
(cid:3). intuitively, because the parameters    in (2.13) are used in
that    =   
both the input distribution and the conditional, a good set of param-
eters must represent both well, potentially at the cost of trading o   
accuracy on p(y|x), the distribution we care about, for accuracy on
p(x), which we care less about. on the other hand, the added degree of
freedom brings about an increased risk of over   tting the training data,
and generalizing worse on unseen data.

although so far we have been criticizing generative models, they do
have advantages as well. first, generative models can be more natu-
ral for handling latent variables, partially-labeled data, and unlabelled
data. in the most extreme case, when the data is entirely unlabeled,
generative models can be applied in an unsupervised fashion, whereas
unsupervised learning in discriminative models is less natural and is
still an active area of research.

second, in some cases a generative model can perform better than
a discriminative model, intuitively because the input model p(x) may
have a smoothing e   ect on the conditional. ng and jordan [98] argue
that this e   ect is especially pronounced when the data set is small. for
any particular data set, it is impossible to predict in advance whether
a generative or a discriminative model will perform better. finally,
sometimes either the problem suggests a natural generative model, or
the application requires the ability to predict both future inputs and
future outputs, making a generative model preferable.

286 modeling

because a generative model takes the form p(y,x) = p(y)p(x|y), it
is often natural to represent a generative model by a directed graph
in which in outputs y topologically precede the inputs. similarly, we
will see that it is often natural to represent a discriminative model
by a undirected graph. however, this need not always be the case,
and both undirected generative models, such as the markov random
   eld (2.32), and directed discriminative models, such as the memm
(6.2), are sometimes used. it can also be useful to depict discriminative
models by directed graphs in which the x precede the y.

the relationship between naive bayes and id28 mirrors
the relationship between id48s and linear-chain crfs. just as naive
bayes and id28 are a generative-discriminative pair, there
is a discriminative analogue to the id48, and this analogue is a partic-
ular special case of crf, as we explain in the next section. this analogy
between naive bayes, id28, generative models, and crfs
is depicted in figure 2.4.

fig. 2.4 diagram of the relationship between naive bayes, id28, id48s, linear-
chain crfs, generative models, and general crfs.

2.3 linear-chain crfs

to motivate our introduction of
linear-chain crfs, we begin by
considering the conditional distribution p(y|x) that follows from the
joint distribution p(y,x) of an id48. the key point is that this
conditional distribution is in fact a crf with a particular choice of
feature functions.

2.3 linear-chain crfs

287

first, we rewrite the id48 joint (2.10) in a form that is more

amenable to generalization. this is

      
   

(cid:3)

i,j   s

t(cid:2)
(cid:3)

t=1

exp

(cid:3)

i   s

o   o

p(y,x) =

1
z

+

  ij1{yt=i}1{yt   1=j}

      
    ,

  oi1{yt=i}1{xt=o}

(2.15)

where    = {  ij,   oi} are the real-valued parameters of the distribution
and z is a id172 constant chosen so the distribution sums to
one.2 if we had not added z in the (2.15), then some choices of    would
not yield proper distributions over (y,x), e.g., set all the parameters
to 1.

now the interesting point is that (2.15) describes (almost) exactly
the class (2.10) of id48s. every homogeneous id48 can be written in
the form (2.15) by setting

(cid:3)

= i|y = j)
  ij = log p(y
  oi = log p(x = o|y = i)
z = 1

the other direction is true as well, namely, every distribution that
factorizes as in (2.15) is an id48.3 (this can be shown by construct-
ing the corresponding id48 using the forward   backward algorithm of
section 4.1.) so despite the added    exibility in the parameterization,
we have not added any distributions to the family.

we can write (2.15) more compactly by introducing the concept
of feature functions, just as we did for id28 in (2.9).
each feature function has the form fk(yt, yt   1, xt). in order to dupli-
, x) = 1{y=i}1{y(cid:2)=j}
cate (2.15), there needs to be one feature fij(y, y

(cid:3)

(cid:3)(cid:1)

2 not all choices of    are valid, because the summation de   ning z, that is, z =
(cid:1)
y

i,j   s   ij1{yt=i}1{yt   1=j} +

o   o   oi1{yt=i}1{xt=o}

(cid:2)t
t=1 exp

(cid:1)
x

(cid:1)

(cid:1)

i   s

might not converge. an example of this is a model with one state where   00 > 0. this
issue is typically not an issue for crfs, because in a crf the summation within z is
usually over a    nite set.
3 but not necessarily a homogeneous id48, which is an annoying technicality.

,

(cid:4)

288 modeling

, x) = 1{y=i}1{x=o} for
for each transition (i, j) and one feature fio(y, y
each state-observation pair (i, o). we refer to a feature function gener-
ically as fk, where fk ranges over both all of the fij and all of the fio.
then we can write an id48 as:

(cid:3)

(cid:11)

t(cid:2)

(cid:10)
k(cid:3)

t=1

k=1

p(y,x) =

1
z

exp

  kfk(yt, yt   1, xt)

.

(2.16)

again, equation (2.16) de   nes exactly the same family of distributions
as (2.15), and therefore as the original id48 equation (2.10).
the last step is to write the conditional distribution p(y|x) that
(cid:14) .

(cid:14)
k=1   kfk(yt, yt   1, xt)

results from the id48 (2.16). this is

p(y|x) = p(y,x)

(cid:13)(cid:1)
(cid:13)(cid:1)

(cid:12)
(cid:1)

t
t=1 exp

(cid:12)

=

k

(cid:1)
y(cid:2) p(y(cid:3),x)

k
k=1   kfk(y

(cid:3)
t, y

(cid:3)
t   1, xt)

y(cid:2)

t
t=1 exp

(2.17)
this conditional distribution (2.17) is a particular kind of linear-chain
crf, namely, one that includes features only for the current word   s
identity. but many other linear-chain crfs use richer features of the
input, such as pre   xes and su   xes of the current word, the identity of
surrounding words, and so on. fortunately, this extension requires little
change to our existing notation. we simply allow the feature functions
to be more general than indicator functions. this leads to the general
de   nition of linear-chain crfs:
de   nition 2.2. let y, x be random vectors,    = {  k}     (cid:9)k be a
,xt)}k
parameter vector, and f = {fk(y, y
k=1 be a set of real-valued
feature functions. then a linear-chain conditional random    eld is a
(cid:10)
distribution p(y|x) that takes the form:
k(cid:3)

t(cid:2)

(cid:11)

(cid:3)

exp

  kfk(yt, yt   1,xt)

,

(2.18)

p(y|x) =

1

where z(x) is an input-dependent id172 function

(cid:11)

z(x) =

exp

  kfk(yt, yt   1,xt)

.

(2.19)

z(x)

t=1

(cid:3)

t(cid:2)

k=1

(cid:10)
k(cid:3)

y

t=1

k=1

2.3 linear-chain crfs

289

notice that a linear chain crf can be described as a factor graph

over x and y, i.e.,

p(y|x) =

1

z(x)

t(cid:2)

  t(yt, yt   1,xt)

(2.20)

t=1

(cid:10)
k(cid:3)

(cid:11)

where each local function   t has the special log-linear form:

  t(yt, yt   1,xt) = exp

  kfk(yt, yt   1,xt)

.

(2.21)

k=1

this will be useful when we move to general crfs in the next section.
learn the parameter vector    from data, as

typically we will
described in section 5.
previously we have seen that if the joint p(y,x) factorizes as an
id48, then the associated conditional distribution p(y|x) is a linear-
chain crf. this id48-like crf is pictured in figure 2.5. other types
of linear-chain crfs are also useful, however. for example, in an id48,
a transition from state i to state j receives the same score, log p(yt =
j|yt   1 = i), regardless of the input. in a crf, we can allow the score
of the transition (i, j) to depend on the current observation vector,
simply by adding a feature 1{yt=j}1{yt   1=1}1{xt=o}. a crf with this
kind of transition feature, which is commonly used in text applications,
is pictured in figure 2.6.

in fact, since crfs do not represent dependencies among the vari-
ables x1, . . .xt , we can allow the factors   t to depend on the entire
observation vector x without breaking the linear graphical structure    
allowing us to treat x as a single monolithic variable. as a result, the
feature functions can be written fk(yt, yt   1,x) and have the freedom
to examine all the input variables x together. this fact applies gener-
ally to crfs and is not speci   c to linear chains. a linear-chain crf

y

x

. . .

. . .

fig. 2.5 graphical model of the id48-like linear-chain crf from equation (2.17).

290 modeling

y

x

. . .

. . .

fig. 2.6 graphical model of a linear-chain crf in which the transition factors depend on
the current observation.

y

x

. . .

. . .

fig. 2.7 graphical model of a linear-chain crf in which the transition factors depend on
all of the observations.

with this structure in shown graphically in figure 2.7. in this    gure we
show x = (x1, . . .xt ) as a single large observed node on which all of the
factors depend, rather than showing each of the x1, . . .xt as individual
nodes.

to indicate in the de   nition of linear-chain crf that each feature
function can depend on observations from any time step, we have writ-
ten the observation argument to fk as a vector xt, which should be
understood as containing all the components of the global observations
x that are needed for computing features at time t. for example, if the
crf uses the next word xt+1 as a feature, then the feature vector xt
is assumed to include the identity of word xt+1.

finally, note that the id172 constant z(x) sums over all
possible state sequences, an exponentially large number of terms.
nevertheless, it can be computed e   ciently by the forward   backward
algorithm, as we explain in section 4.1.

2.4 general crfs

now we generalize the previous discussion from a linear-chain to a
general graph, matching the de   nition of a crf originally given in

2.4 general crfs

291

la   erty et al. [63]. conceptually the generalization is straightforward.
we simply move from using a linear-chain factor graph to a more gen-
eral factor graph.

de   nition 2.3. let g be a factor graph over x and y . then (x, y )
is a conditional random    eld if for any value x of x, the distribution
p(y|x) factorizes according to g.

thus, every conditional distribution p(y|x) is a crf for some, per-
haps trivial, factor graph. if f = {  a} is the set of factors in g, then
the conditional distribution for a crf is

p(y|x) =

1

z(x)

a(cid:2)

a=1

  a(ya,xa).

(2.22)

the di   erence between this equation and the general de   nition (2.1) of
an undirected graphical model is that now the id172 constant
z(x) is a function of the input x. because conditioning tends to simplify
a graphical model, z(x) may be computable whereas z might not have
been.

as we have done for id48s and linear chain crfs, it is often use-
ful to require that log   a be linear over a prespeci   ed set of feature
functions, that is,

      
   k(a)(cid:3)

k=1

      
    ,

  a(ya,xa) = exp

  akfak(ya,xa)

(2.23)

where both the feature functions fak and the weights   ak are indexed
by the factor index a to emphasize that each factor has its own set
of weights. in general, each factor is permitted to have a di   erent set
of feature functions as well. notice that if x and y are discrete, then
the log-linear assumption (2.23) is no additional restriction, because we
could choose fak to be indicator functions for each possible assignment
(ya,xa), similarly to the way in which we converted an id48 into a
linear-chain crf.

292 modeling

      
    .

      
    ,

putting together (2.22) and (2.23), the conditional distribution for

a crf with log-linear factors can be written as

(cid:2)

p(y|x) =

1

z(x)

  a   f

      
   k(a)(cid:3)

k=1

exp

  akfak(ya,xa)

(2.24)

in addition, most applied models rely extensively on parameter tying.
for example, in the linear-chain case, typically the same weights are
used for the factors   t(yt, yt   1,xt) at each time step. to denote this,
we partition the factors of g into c = {c1, c2, . . . cp}, where each cp
is a clique template, which is a set of factors sharing a set of feature
functions {fpk(xc,yc)}k(p)
k=1 and a corresponding set of parameters   p    
(cid:9)k(p). a crf that uses clique templates can be written as

p(y|x) =

1

z(x)

  c(xc,yc;   p),

(2.25)

where each templated factor is parameterized as

  c(xc,yc;   p) = exp

  pkfpk(xc,yc)

and the id172 function is

(cid:3)

(cid:2)

z(x) =

  c(xc,yc;   p).

y

cp   c

  c   cp

(2.26)

(2.27)

this notion of clique template speci   es both repeated structure and
parameter tying in the model. for example, in a linear-chain crf,
typically one clique template c0 = {  t(yt, yt   1,xt)}t
t=1 is used for the
entire network, so c = {c0} is a singleton set. if instead we want each
factor   t to have a separate set of parameters for each t, similar to
a non-homogeneous id48, this would be accomplished using t tem-
plates, by taking c = {ct}t

t=1, where ct = {  t(yt, yt   1,xt)}.

one of the most important considerations in de   ning a general crf
lies in specifying the repeated structure and parameter tying. a num-
ber of formalisms have been proposed to specify the clique templates,
which we will mention only brie   y. for example, dynamic conditional

(cid:2)

(cid:2)

cp   c

  c   cp

      
   k(p)(cid:3)
(cid:2)

k=1

2.5 feature engineering

293

random    elds [140] are sequence models which allow multiple labels at
each time step, rather than a single label, in a manner analogous to
dynamic id110s. second, relational markov networks [142]
are a type of general crf in which the graphical structure and param-
eter tying are determined by an sql-like syntax. markov logic net-
works [113, 128] use logical formulae to specify the scopes of local
functions in an undirected model. essentially, there is a set of param-
eters for each    rst-order rule in a knowledge base. the logic portion
of an mln can be viewed as essentially a programming convention for
specifying the repeated structure and parameter tying of an undirected
model. imperatively de   ned factor graphs [87] use the full expressivity
of turing-complete functions to de   ne the clique templates, specifying
both the structure of the model and the su   cient statistics fpk. these
functions have the    exibility to employ advanced programming ideas
including recursion, arbitrary search, lazy evaluation, and memoiza-
tion. the notion of clique template that we present in this survey is
inspired by those in taskar et al. [142], sutton et al. [140], richardson
and domingos [113], and mccallum et al. [87].

2.5 feature engineering

in this section we describe some    tricks of the trade    that involve fea-
ture engineering. although these apply especially to language appli-
cations, they are also useful more generally. the main tradeo    is the
classic one, that using larger features sets can lead to better prediction
accuracy, because the    nal decision boundary can be more    exible, but
on the other hand larger feature sets require more memory to store all
the corresponding parameters, and could have worse prediction accu-
racy due to over   tting.

label-observation features. first, when the label variables are
discrete, the features fpk of a clique template cp are ordinarily chosen
to have a particular form:

fpk(yc,xc) = 1{yc=  yc}qpk(xc).

(2.28)

in other words, each feature is nonzero only for a single output con-
   guration   yc, but as long as that constraint is met, then the feature

294 modeling

value depends only on the input observation. we call features that
have this form label-observation features. essentially, this means that
we can think of our features as depending only on the input xc, but
that we have a separate set of weights for each output con   guration.
this feature representation is also computationally e   cient, because
computing each qpk may involve nontrivial text or image processing,
and it need be evaluated only once for every feature that uses it. to
avoid confusion, we refer to the functions qpk(xc) as observation func-
tions rather than as features. examples of observation functions are
   word xt is capitalized    and    word xt ends in ing.   

unsupported features. the use of label-observation features can
result in a large number of parameters. for example, in the    rst large-
scale application of crfs, sha and pereira [125] use 3.8 million binary
features in their best model. many of these features never occur in the
training data; they are always zero. the reason this happens is that
some observation functions occur only with a small set of labels. for
example, in a task of identifying named entities, the feature    word xt is
with and label yt is city-name    is unlikely to ever have a value of 1 in
the training data. we call these unsupported features. perhaps surpris-
ingly, these features can be useful. this is because they can be given
a negative weight that prevents the spurious label (e.g. city-name)
from being assigned high id203. (decreasing the score of label
sequences that do not occur in the training data will tend to make the
label sequence that does occur more likely, so the parameter estimation
procedures that we describe later do in fact assign negative weights to
such features.) including unsupported features typically results in slight
improvements in accuracy, at the cost of greatly increasing the number
of parameters in the model.

as a simple heuristic for getting some of the bene   ts of unsupported
features with less memory, we have had success with an ad hoc tech-
nique for selecting a small set of unsupported features, which could be
called the    unsupported features trick.    the idea is that many unsup-
ported features are not useful because the model is unlikely to make
mistakes that cause them to be active. for example, the    with    feature
above is unlikely to be useful, because    with    is a common word that
will be strongly associated with the other (not a named entity) label.

2.5 feature engineering

295

(i)
t

to reduce the number of parameters, we would like to include only
those unsupported features that will correct likely mistakes. a simple
way to do this is: first train a crf without any unsupported features,
stopping after a few iterations so that the model is not fully trained.
then add unsupported features for all cliques in which the model does
not already have the correct answer with high con   dence. in the exam-
ple above, we would add the    with    feature if we found a training
(i)
is not
instance i and sequence position t in which x
t
city-name and p(yt = city-name|x(i)

t ) is larger than some   > 0.

is with and y

edge-observation and node-observation features. in order
to reduce the number of features in the model, we can choose to
use label-observation features for certain clique templates but not
for others. the two most common types of label-observation features
are edge-observation features and node-observation features. consider
a linear chain crf that has m observation functions {qm(x)}, for
m     {1,2, . . . m}. if edge-observation features are used, then the fac-
tor for every transition can depend on all of the observation functions,
so that we can use features like    word xt is new, label yt is loca-
tion and label yt   1 is location.    this can lead to a large number
of parameters in the model, which can have disadvantages of memory
usage and propensity to over   tting. one way to reduce the number
of parameters is to use node-observation features instead. with this
style of features, the transition factors can no longer depend on the
observation functions. so we would be allowed features like    label yt
is location and label yt   1 is location    and    word xt is new and
label yt is location,    but we would not be able to use a feature that
depends on xt, yt, and yt   1 all at once. both edge-observation features
and node-observation features are described formally in table 2.1. as
usual, which of these two choices is preferable depends on the individ-
ual problem, such as the number of observation functions and the size
of the data set.

boundary labels. a    nal choice is how to handle labels on the
boundaries, e.g., at the start or end of a sequence or at the edge of
an image. sometimes boundary labels will have di   erent characteris-
tics than other labels. for example, in the middle of a sentence cap-
italization usually indicates a proper noun, but not at the beginning

296 modeling

table 2.1. edge-observation features versus node-observation features.

edge-observation features:

f(yt, yt   1,xt) = qm(xt)1{yt=y}1{yt   1=y(cid:1)}

f(yt,xt) = qm(xt)1{yt=y}

   y, y

(cid:2)     y,   m
   y     y,   m

node-observation features:

f(yt, yt   1,xt) = 1{yt=y}1{yt   1=y(cid:1)}

f(yt,xt) = qm(xt)1{yt=y}

   y, y
(cid:2)     y
   y     y,   m

of the sentence. a simple way to represent this is to begin the label
sequence of each sentence with a special start label, which allows
learning any special characteristics of boundary labels. for example,
if edge-observation features are also used, then a feature like    yt   1 =
start and yt = person and word xt is capitalized,    can represent the
fact that capitalization is not a reliable indicator at the beginning of a
sentence.

feature induction. the    unsupported features trick    described
above is a poor man   s version of feature induction. mccallum [83]
presents a more principled method of feature induction for crfs, in
which the model begins with a number of base features, and the train-
ing procedure adds conjunctions of those features. alternatively, one
can use feature selection. a modern method for feature selection is l1
id173, which we discuss in section 5.1.1. lavergne et al. [65]
   nd that in the most favorable cases l1    nds models in which only 1%
of the full feature set is nonzero, but with comparable performance to a
dense feature setting. they also    nd it useful, after optimizing the l1-
regularized likelihood to    nd a set of nonzero features, to    ne-tune the
weights of the nonzero features only using an l2-regularized objective.
categorical features. if the observations are categorical rather
than ordinal, that is, if they are discrete but have no intrinsic order, it
is important to convert them to binary features. for example, it makes
sense to learn a linear weight on fk(y, xt) when fk is 1 if xt is the word
dog and 0 otherwise, but not when fk is the integer index of word xt
in the text   s vocabulary. thus, in text applications, crf features are

2.5 feature engineering

297

typically binary; in other application areas, such as vision and speech,
they are more commonly real-valued. for real-valued features, it can
help to apply standard tricks such as normalizing the features to have
mean 0 and standard deviation 1 or to bin the features to convert them
to categorical values and then binary features as above.

features from di   erent time steps. although our notation
f(yt, yt   1,xt) for features obscures this, it is usually important for the
features to depend on information not only from the nearest label but
also from neighboring labels as well. an example of such a feature is
   word xt+2 is times and label yt is organization,    which could be
useful for identifying the name of the new york times newspaper. it
can be useful to include conjunctions of features from neighboring time
steps as well, e.g.,    words xt+1 and xt+2 are york times.   

features as backo   . in language applications, it is sometimes
helpful to include redundant factors in the model. for example, in
a linear-chain crf, one may choose to include both edge factors
  t(yt, yt   1,xt) and variable factors   t(yt,xt). although one could
de   ne the same family of distributions using only edge factors, the
redundant node factors provide a bene   t similar to that of backo    in
language modelling, which is useful when the amount of data is small
compared to the number of features. (when there are hundreds of thou-
sands of features, many data sets are small!) it is important to use reg-
ularization (section 5.1.1) when using redundant features because it is
the penalty on large weights that encourages the weight to be spread
across the overlapping features.

features as model combination. another interesting type of
feature can be the results of simpler methods for the same task. for
example, if one already has a simple rule-based system for a task (e.g.,
with rules like    any string of digits between 1900 and 2100 is a year   ),
the predictions of that system can be used as an observation function
for a crf. another example is gazetteer features, which are observa-
tion functions based on prede   ned lists of terms, e.g.,    q(xt) = 1 if xt
appears in a list of city names obtained from wikipedia.   

a more complex example is to use the output of a generative model
as input to the discriminative model. for example, one could use a
feature like ft(y,x) = pid48(yt = y|x), where pid48 denotes the marginal

298 modeling

id203 of label yt = y from an id48 trained on a similar data set.
it is probably not a good idea to train the id48 and the crf-with-
id48-feature on the same data set, because the id48 is expected to
perform very well on its own training set, which could perhaps cause
the crf to rely on it too much. this technique can be useful if the goal
is to improve on a previous system for the same task. bernal et al.[7]
is a good example of doing this in the concept of identifying genes in
dna sequences.

a related idea is to cluster the inputs xt, e.g., cluster all of the
words in the corpus by whatever method you like, and then use the
cluster label for word xt as an additional feature. this sort of feature
was used to good e   ect by miller et al. [90].
input-dependent structure. in a general crf, it is sometimes
useful to allow the structure of the graph for p(y|x) depend on the
input x. a simple example of this is the    skip-chain crf    [37, 117, 133]
which has been used in text applications. the idea behind the skip-
chain crf is that whenever the same word appears twice in the same
sentence, we would like to encourage both occurrences of the word
to have the same label. so we add an edge between the labels of the
corresponding words. this results in a graphical structure over y that
depends on the inputs x.

2.6 examples

in this section, we present more details about two example applications
of crfs. the    rst is a linear-chain crf for a natural language text,
and the second is a grid-structured crf for id161.

2.6.1 named-entity recognition

named-entity recognition is the problem of segmenting and classifying
proper names, such as names of people and organization, in text. since
we have been using this task as a running example throughout the
section, now we will describe a crf for this problem in more detail.

an entity is an individual person, place, or thing in the world,
while a mention is a phrase of text that refers to an entity using a
proper name. the problem of named-entity recognition is in part one

2.6 examples

299

of segmentation because mentions in english are often multi-word, e.g.,
the new york times, the white house. the exact entity types that are
of interest vary across di   erent settings of the problem. for example, in
the conll 2003 shared task [121], entities are people, locations, orga-
nizations, and miscellaneous entities. in biomedical domains [55, 51],
on the other hand, the interest is in representing information from the
molecular biology literature, so entities can include genes, proteins, and
cell lines. for this example, we will consider the conll 2003 shared
task, because it is a good representative example of early applications
of crfs.

the data of the conll 2003 shared task consists of news articles
from english and german. the english articles are newswire articles
from reuters taken from between 1996 and 1997. the english data
consists of a training set of 946 news articles comprising 203,621 tokens,
a development set of 216 articles and 51,362 tokens, and a test set of
231 articles comprising 46,435 tokens. each of the articles have been
manually annotated to indicate the locations and types of all of the
named entities. an example sentence is u.n. o   cial ekeus heads for
baghdad; in this sentence, the token u.n. is labeled as a organization,
ekeus a person, and baghdad a location.

in order to represent this as a sequence labeling problem, we need to
convert the entity annotations into a sequence of labels. this labelling
needs to handle the fact that multi-word phrases (like the new york
times) can refer to a single entity. there is a standard trick for this: we
use one type of label (b-???) for the    rst word of a mention, another
type of label (i-???) for any subsequent words in the mention, and
   nally a label o for words that do not reference any named entity.
this label scheme, which is called bio notation, has the advantages
that it can segment two di   erent entities of the same type that occur
side-by-side, e.g., alice gave bob charlie   s ball, and that the b-???
labels can have di   erent weights than the corresponding i-??? labels.
there are other schemes apart from bio notation for converting the
segmentation problem to an entity labelling problem, but we will not
discuss them here.

in the conll 2003 data set, there are four types of entities: people
(per), organizations (org), locations (loc), and other types of

300 modeling

entities (misc). so we need 9 labels
y ={b-per,i-per,b-loc,i-loc,b-org,i-org,b-misc,i-misc,o}
with this labeling, our example sentence looks like:

t

0
1
2
3
4
5

yt

b-org
o
b-per
o
o
b-loc

xt

u.n.
o   cial
ekeus
heads
for
baghdad

to de   ne a linear chain crf for this problem, we need to choose the
set f of feature functions fk(yt, yt   1,xt). the simplest choice is the
id48-like feature set that we have already described. in this feature
set there are two kinds of features. the    rst kind we will call label   label
features, which are:

ll

ij (yt, yt   1,xt) = 1{yt=i}1{yt   1=j}   i, j     y.

f

(2.29)

f

lw

for this problem, there are 9 di   erent labels, so there are 81 label   label
features. the second kind are label-word features, which are
iv (yt, yt   1,xt) = 1{yt=i}1{xt=v}   i     y, v     v,

(2.30)
where v is the set of all unique words that appear in the corpus. for
the conll 2003 english data set, there are 21,249 such words, so there
are 191,241 label-word features. most of these features will not be very
useful, e.g., they will correspond to words that occur only once in the
training set. to see the relationship to (2.18), our full set of features is
f = {f

|   i, j     y}     {f

|   i     y, v     v}.

a practical ner system will use much more complex features than
this. for example, when predicting the label of the word baghdad, it
can be helpful to know that the previous word was for. to represent
this in our notation, we augment each of the vectors xt to include the
neighboring words, i.e., each vector xt = (xt0, xt1, xt2), where xt1 is the
identity of the word at position t, and xt0 and xt2 are the preceding and
succeeding words, respectively. the beginning and end of the sequence

lw
iv

ll
ij

301
are padded with special (cid:10)start(cid:11) and (cid:10)end(cid:11) tokens. now our example
sentence becomes

2.6 examples

t

0
1
2
3
4
5

yt

b-org
o
b-per
o
o
b-loc

xt

((cid:1)start(cid:2), u.n., o   cial)
(u.n., o   cial, ekeus)
(o   cial, ekeus, heads)
(ekeus, heads, for)
(heads, for, baghdad)
(for, baghdad, (cid:1)end(cid:2))

to construct a crf, we retain the label   label features as before, but
now we have three di   erent kinds of label-word features:

lw0
f
iv
lw1
f
iv
lw2
f
iv

(yt, yt   1,xt) = 1{yt=i}1{xt0=v}    i     y, v     v
(yt, yt   1,xt) = 1{yt=i}1{xt1=v}    i     y, v     v
(yt, yt   1,xt) = 1{yt=i}1{xt2=v}    i     y, v     v.

however, we wish to use still more features than this, which will depend
mostly on the word xt. so we will add label-observation features, as
decribed in section 2.5. to de   ne these, we will de   ne a series of obser-
vation functions qb(x) that take a single word x as input. for each
observation function qb, the corresponding label-observation features
lo
ib have the form:
f
lo

ib (yt, yt   1,xt) = 1{yt=i}qb(xt)    i     y.

(2.31)

f

when mccallum and li [86] applied a crf to this data set, they used a
large set of observation functions, some of which are listed in table 2.2.
all of these are binary functions. the example sentence with this third
feature set is given in table 2.3. to display the feature set, we list the
observation functions which are nonzero for each word.

these features are typical of those that are used in crfs for text
applications, but they should not be taken as the best possible fea-
ture set for this problem. for this particular task, chieu and ng [20]
had an especially good set of features, during the competition was the
best result obtained by a single model (i.e., without using ensemble
methods).

302 modeling

table 2.2. a subset of observation functions qs(x, t) for the conll 2003 engilsh
named-entity data, used by mccallum and li [86].

wt = v
part-of-speech tag for wt is j (as determined by an

automatic tagger)

wt is part of a phrase with syntactic type j (as

determined by an automatic chunker)

   v     v
   pos tags j

w=v
t=j
p=i   j

capitalized
allcaps
endsindot

wt matches [a-z][a-z]+
wt matches [a-z][a-z]+
wt matches [  /.]+.*/.
wt contains a dash
wt matches [a-z]+[a-z]+[a-z]+[a-z]
wt matches [a-z][a-z // .]* // .[a-z // .]*
wt appears in a hand-built list of stop words

acro
stopword
countrycapital wt appears in list of capitals of countries
...
qk(x, t +   ) for all k and        [   1,1]

many other lexicons and id157

table 2.3. the example sentence u.n. o   cial ekeus heads for baghdad converted into
a sequence x1,x2, . . .x5 of feature vectors, using the set of observation functions de   ned
in table 2.2. each row lists the names of the observation functions that are nonzero.

t

1

2

3

4

5

6

yt

b-org

o

b-per

o

o

b-loc

active observation functions

p=i-np@1 w=(cid:6)start(cid:7)@-1 initcap p=i-np t=nnp
t=nn@1 acro endsindot w=o   cial@1 w=u.n.
p=i-np@1 initcap@1 p=i-np t=jj@1
capitalized@1 t=nnp@-1 p=i-np@-1
initcap@-1 t=nn endsindot@-1 acro@-1
w=o   cial w=u.n.@-1w=ekeus@1
p=i-np@1 initcap p=i-np p=i-np@-1
capitalized t=jj t=nn@-1 t=nns@1
w=o   cial@-1 w=heads@1 w=ekeus
p=i-np p=i-np@-1 initcap@-1 stopword@1
t=jj@-1 capitalized@-1 t=in@1 p=i-pp@1
t=nns w=for@1 w=heads w=ekeus@-1
t=nnp@1 p=i-np@1 initcap@1 loc@1
capitalized@1 p=i-np@-1 stopword
countrycapital@1 p=i-pp t=in t=nns@-1
w=for w=baghdad@1 w=heads@-1
initcap p=i-np t=nnp capitalized
stopword@-1 t=.@1 p=o@1 punc@1
w=(cid:6)end(cid:7)@1 countrycapital t=in@-1
p=i-pp@-1 w=for@-1 w=baghdad

2.6.2 image labelling

2.6 examples

303

   

many di   erent crf topologies have been used for id161.
as an example application, we may wish to classify areas of a given
input image according to whether they are foreground or background,
they are a manmade structure or not [61, 62], or whether they are sky,
water, vegetation, etc. [49].

t       

   

   

t +1:2

t , represented as a single vector. that is, x1:

more formally, let the vector x = (x1, x2, . . . xt ) represent an image
t gives
of size
the pixels in row 1, x   
t those in row 2, etc. each individual xi
represents the value of an individual pixel in the image. to keep the
example simple, think of the image as being in black and white, so that
xi will be a real number between 0 and 256 that gives the intensity of
the pixel at location i. (the ideas here extend in a simple way to color
images.) the goal is to predict a vector y = (y1, y2, . . . , yt ), where each
yi gives the label of site i, e.g., +1 if the pixel is a manmade structure,
   1 otherwise.

there is an enormous variety of image features that have been pro-
posed in the id161 literature. as a simple example, given
a pixel at location i, we can compute a histogram of pixel intensities
in a 5    5 box of pixels centered at i, and then include the count of
each bin in the histogram as features. typically more complex features
are used, for example, features that use gradients of the image, texton
features [127], and sift features [77]. importantly, these features do
not depend on the pixel xi alone; most interesting features depend on
a region of pixels, or even the entire image.

a basic characteristic of images is that neighboring pixels tend to
have the same label. one way to incorporate this idea into our model is
to place a prior distribution on y that encourages    smooth    predictions.
the most common such prior in id161 is a grid-structured
undirected graphical model that is typically called a markov random
   eld [10]. an mrf is a generative undirected model with two types of
factors: one type that associates each label yi with its corresponding
pixel xi, and another type that encourages neighboring labels yi and yj
to agree.

304 modeling

more formally, let n de   ne the neighborhood relationship among
pixels, that is, (i, j)     n if any only if xi and xj are neighboring pixels
in the image. often n will be chosen to form a
t grid. an
mrf is a generative model

t       

   

p(y) =

1
z

  (yi, yj)

(cid:2)
t(cid:2)

(i,j)   n

p(y,x) = p(y)

p(xi|yi).

(2.32)

i=1

here    is the factor that encourages smoothness. one common choice is
  (yi, yj) = 1 if yi = yj and    otherwise, where    is a parameter that can
be learned from data and typically    < 1. the motivation behind this
choice of   (yi, yj) is that if    < 1, then e   cient id136 algorithms are
available to maximize log p(y,x). the distribution p(xi|yi) is a class-
conditional distribution over pixel values, for example, a mixture of
gaussians over xi.

a disadvantage of this mrf is that it is di   cult to incorporate fea-
tures over regions of pixels, of the kind discussed earlier, because then
p(x|y) would have complex structure. a conditional model provides a
way to address this di   culty.

the crf that we will describe for this task will be very similar to
the mrf, except that it will allow the factors on both the individual
sites and the edges to depend on arbitrary features of the image. let
q(xi) be a vector of features based on a region of the image around xi,
for example, using color histograms or image gradients as mentioned
above. in addition we will want a feature vector   (xi, xj) that depends
on pairs of sites xi and xj, so that the model can take into account the
similarities and di   erences between xi and xj. one way to do this is to
de   ne   (xi, xj) to be the set of all cross-products between features in
q(xi) and q(xj), i.e., to compute   (xi, xj),    rst compute a matrix via
the outer product q(xi)q(xj)(cid:9) and    attened into into a vector.

we have called the functions q and    features because this is the
terminology most commonly used in id161. but in this survey
we have been using features that depend both on the inputs x and the
labels y. so we will use q and    as observation functions to de   ne

label-observation features in a crf. the resulting label-observation
features are

2.6 examples

305

fm(yi, xi) = 1{yi=m}q(xi)    m     {0,1}

gm,m(cid:2)(yi, yj, xi, xj) = 1{yi=m}1{yj=m(cid:2)}  (xi, xj)    m, m

(cid:3)     {0,1}

f(yi, xi) =

g(yi, yj, xi, xj) =

f0(yi, xi)
f1(yi, xi)

(cid:16)
(cid:15)
   
   
             g00(yi, yj, xi, xj)
            

g01(yi, yj, xi, xj)
g10(yi, yj, xi, xj)
g11(yi, yj, xi, xj)

using label-observation features has the e   ect of allowing the model to
have a separate set of weights for each label.

to make the example concrete, here is a speci   c choice for g and   
that has been used in some prominent applications [14, 119]. consider
the pairwise factor   (yi, yj) in the mrf (2.32). although it is good
that    encourages agreement, the way in which it does so is in   exible.
if the pixels xi and xj have di   erent labels, we expect them to have
di   erent intensities in black and white, because di   erent objects tend
to have di   erent hues. so if we see a label boundary drawn between
two pixels with sharply di   erent intensities, we should be less suprised
than if we see if the boundary drawn between identical-looking pixels.
unfortunately,    imposes the same dissimilarity penalty in both cases,
because the potential does not depend on the pixel values. to    x this
problem, the following choice of features has been proposed [14, 119]

(cid:23)     (xi     xj)2

(cid:24)

  (xi, xj) = exp

g(yi, yj, xi, xj) = 1{yi(cid:10)=yj}  (xi, xj).

putting this all together, the crf model is

p(y|x) =

1

z(x)

exp

(cid:9)

  

f(yi, xi) +

(cid:9)

  

g(yi, yj, xi, xj)

(2.34)
where        (cid:9),        (cid:9)k, and        (cid:9)k
2 are the parameters of the model.
the    rst two terms are analogous to the two types of factors in the

(2.33)

      
    ,

      
    t(cid:3)

i=1

(cid:3)

(i,j)   n

306 modeling

mrf. the    rst term represents the impact of the local evidence around
xi to label yi. using the choice of g described in (2.33), the second term
encourages neighboring labels to be similar, in a manner that depends
on the di   erence in pixel intensity.

notice that this is an instance of the general crf de   nition given
in (2.25), where we have three clique templates, one for each of the
three terms in (2.34).

the di   erence between (2.34) and (2.32) is analogous to the dif-
ference between the linear-chain crf models in figures 2.6 and 2.5:
the pairwise factors now depend on features of the images, rather than
simply on the label identity. as an aside, notice that, just as in the case
of sequences, the conditional distribution p(y|x) that results from the
mrf model (2.32) is a particular kind of crf, in that it will have the
form (2.34) with    = 0.

this simple crf model can be improved in various ways. first, the
feature functions q and    can be more complex, for example, taking
image shape and texture into account [127], or depending on global
characteristics of the image rather than on a local region. additionally,
one could use a more complex graphical structure among the labels than
a grid. for example, one can de   ne factors that depend on regions of
the labels [49, 56]. for a more in-depth survey about how crfs and
other id170 models can be used in id161, see
nowozin and lampert [101].

2.7 applications of crfs

in addition to the example domains in the previous section, crfs have
been applied to a large variety of other domains, including text pro-
cessing, id161, and bioinformatics. one of the    rst large-
scale applications of crfs was by sha and pereira [125], who matched
state-of-the-art performance on segmenting noun phrases in text. since
then, linear-chain crfs have been applied to many problems in natural
language processing, including named-entity recognition [86], feature
induction for ner [83], id66 [125, 138], identifying protein
names in biology abstracts [124], segmenting addresses in web pages
[29], information integration [156],    nding semantic roles in text [118],

2.7 applications of crfs

307

prediction of pitch accents [47], phone classi   cation in speech processing
[48], identifying the sources of opinions [21], word alignment in machine
translation [12], citation extraction from research papers [105], extrac-
tion of information from tables in text documents [106], chinese word
segmentation [104], japanese morphological analysis [59], and many
others.

in bioinformatics, crfs have been applied to rna structural align-
ment [123] and protein structure prediction [76]. semi-markov crfs
[122] are a way to allow more    exible feature functions. in a linear-chain
crf, feature functions f() are restricted to depend only on successive
pairs of labels. in a semi-markov crf, on the other hand, a feature
function can depend on a entire segment of the labelling, that is, a
sequence of successive labels that have the same value. this can be
useful for certain tasks in information extraction and especially bioin-
formatics, where for example, one might want features that depend on
the length of the segment.

general crfs have also been applied to several tasks in nlp. one
promising application is to performing multiple labeling tasks simulta-
neously. for example, sutton et al. [140] show that a two-level dynamic
crf for part-of-speech tagging and noun-phrase chunking performs
better than solving the tasks one at a time. another application is
to multi-label classi   cation, in which each instance can have multiple
class labels. rather than learning an independent classi   er for each
category, ghamrawi and mccallum [42] present a crf that learns
dependencies between the categories, resulting in improved classi   ca-
tion performance. finally, the skip-chain crf [133] is a general crf
that represents long-distance dependencies in information extraction.
another application of general crfs that has used a di   erent struc-
ture been in the problem of proper-noun coreference, that is, of deter-
mining which mentions in a document, such as mr. president and he,
refer to the same underlying entity. mccallum and wellner [88] learn
a distance metric between mentions using a fully-connected crf in
which id136 corresponds to graph partitioning. a similar model has
been used to segment handwritten characters and diagrams [26, 108].
as mentioned in section 2.6.2, crfs have been widely used in
id161, for example, grid-shaped crfs for labeling and

308 modeling

segmenting images [49, 61, 127]. although image labelling is a common
application of crfs in vision, there are other types of probabilistic
structure in the vision problem that are interesting to model. one type
of structure is the relationships between parts of an object. for exam-
ple, quattoni et al. [109] use a tree-structured crf in which the hope
is that the latent variables will recognize characteristic parts of an
object; this was an early example of hidden-variable crfs. an espe-
cially successful example of a discriminative models with variables that
correspond to di   erent parts of an object is felzenszwalb et al. [36].

another type of probabilistic structure in id161 occurs
because similar objects appear in di   erent images. for example,
deselaers et al. [33] present a model that simultaneously performs local-
ization of objects in multiple images. they de   ne a crf in which each
random variable speci   es the boundaries of a bounding box that con-
tains the object, i.e., each random variable corresponds to an entire
image.

an excellent survey on id170 for id161,

including crfs, is nowozin and lampert [101].

vision is one of more common application areas for crfs with more
loopy graphical structure. that said, general crfs have also been used
for global models of natural language [16, 37, 133].

in some applications of crfs, e   cient dynamic programs exist
even though the graphical model is di   cult to specify. for example,
mccallum et al. [84] learn the parameters of a string-edit model in order
to discriminate between matching and nonmatching pairs of strings.
also, there is work on using crfs to learn distributions over the deriva-
tions of a grammar [23, 38, 114, 148].

2.8 notes on terminology

di   erent aspects of the theory of id114 have been developed
independently in di   erent research areas, so many of the concepts in
this section have di   erent names in di   erent areas. for example, undi-
rected models are commonly also referred to markov random    elds,
markov networks, and gibbs distributions. as mentioned, we reserve
the term    graphical model    for a family of distributions de   ned by a

2.8 notes on terminology

309

graph structure;    random    eld    or    distribution    for a single probabil-
ity distribution; and    network    as a term for the graph structure itself.
this choice of terminology is not always consistent in the literature,
partly because it is not ordinarily necessary to be precise in separating
these concepts.

similarly, directed id114 are commonly known as
id110s, but we have avoided this term because of its con-
fusion with the area of bayesian statistics. the term generative model
is an important one that is commonly used in the literature, but is
not usually given a precise de   nition. (we gave our de   nition of it in
section 2.2.).

3

overview of algorithms

the next two sections will discuss id136 and parameter estima-
tion for crfs. parameter estimation is the problem of    nding a set
of parameters    so that the resulting distribution p(y|x,   ) best    ts a
set of training examples d = {x(i),y(i)}n
i=1 for which both the inputs
and outputs are known. intuitively, what we want to accomplish during
parameter estimation is that if we look at any of the training inputs x(i),
the model   s distribution over outputs p(y|x(i),   ) should    look like    the
true output y(i) from the training data.

one way to quantify this intuition is to consider the feature func-
tions that are used to de   ne the model. consider a linear-chain crf.
for each feature fk(yt, yt   1,xt), we would like the total value of fk that
occurs in the data to equal the total value of fk that we would get by
randomly selecting an input sequence x from the training data follow-
ing by sampling y from the conditional model p(y|x,   ). formally this
requirement yields for all fk that

(cid:26)

n(cid:3)

t(cid:3)

i=1

t=1

=

(cid:25)
n(cid:3)

fk

y

(i)
t

t(cid:3)

, y

t

t   1,x(i)
(i)
(cid:3)

fk(y, y

i=1

t=1

y,y(cid:2)

(cid:3)

,x(i)

t )p(yt = y, yt   1 = y

(cid:3)|x(i)).

310

311

remarkably, this system of equations can be obtained as the gradient of
an objective function over parameters. this connection is useful because
once we have an objective function we can optimize it using standard
numerical techniques. the objective function that has this property is
the likelihood

(cid:8)(  ) = p(y(i)|x(i),   )
k(cid:3)

n(cid:3)

t(cid:3)

=

  kfk(y

i=1

t=1

k=1

t )     n(cid:3)

(i)
t   1,x(i)

i=1

(i)
t

, y

log z(x(i)),

which is the id203 of training data under the model, viewed as a
function of the parameter vector.

a standard way of training crfs is maximum likelihood, in which
we seek the parameters     ml = sup   (cid:8)(  ). the intuition behind maximum
likelihood is that     ml is the parameter setting under which the observed
data is most likely. to connect this with the discussion about matching
expectations, take the partial derivative of the likelihood with respect to
some parameter   k and set it to zero. this exactly yields the matching
conditions on the feature expectations that we began with.

although we have illustrated the idea behind maximum likelihood
using linear chain crfs the same ideas carry over to general crfs.
for general crfs, instead of marginals p(yt, yt   1|x,   ) over neighbor-
ing variables in a chain, computing the likelihood gradient requires
marginals p(ya|x,   ) over sets of variables ya in a general graphical
model.

computing the marginal distributions that are required for param-
eter estimation can be computationally challenging. this is the task of
probabilistic id136. in general, the goal of id136 is to compute
predictions over the output y from a given x for a single    xed value
of   . there are two speci   c id136 problems that we will focus on:
    computing marginal distributions p(ya|x,   ) over a subset ya
of output variabels. usually the domain ya of the marginals
consists either of a single variable or of a set of neighboring
variables that share a factor. the algorithms for this problem
will usually also compute as a byproduct the id172
function z(x) which appears in the likelihood.

312 overview of algorithms

    computing the labeling y    = arg maxy p(y|x,   ), which is the
single most likely labeling of a new input x.

the marginals p(ya|x,   ) and the id172 function z(x) are used
for parameter estimation. some parameter estimation methods, like
maximum likelihood when optimized by limited memory bfgs, require
both the marginals and the id172 function. other parameter
estimation methods, like stochastic id119, require only the
marginals. the viterbi assignment y    is used for assigning a sequence
of labels to a new input that was not seen during training.

these inferential tasks can be solved using standard techniques from
id114. in tree-structured models, these quantities can be
computed exactly, while in more general models we typically resort to
approximations.

the next two sections discuss id136 and parameter estimation
in section 4, we
both for linear-chain and general crfs. first,
including exact methods for
discuss id136 methods for crfs,
tree-structured crfs and approximate methods for more general
crfs. in some sense, because a crf is a type of undirected graphical
model, this largely recaps id136 methods for standard graphical
models, but we focus on methods that are most appropriate for crfs.
then, in section 5, we discuss parameter estimation. although the
maximum likelihood procedure is conceptually simple, it can require
expensive computations. we describe both the standard maximum
likelihood procedure, ways to combine maximum likelihood with
approximate id136, and other approximate training methods that
can improve scalability in both the number of training instances and
in the complexity of the graphical model structure of the crf.

4

id136

e   cient id136 is critical for crfs, both during training and for
predicting the labels on new inputs. the are two id136 prob-
lems that arise. first, after we have trained the model, we often
predict the labels of a new input x using the most likely labeling
y    = arg maxy p(y|x). second, as will be seen in section 5, estimation
of the parameters typically requires that we compute the marginal dis-
tribution over subsets of labels, such as over node marginals p(yt|x) and
edge marginals p(yt, yt   1|x). these two id136 problems can be seen
as fundamentally the same operation on two di   erent semirings [1],
that is, to change the marginalization problem to the maximization
problem, we simply substitute maximization for addition.

for discrete variables the marginals could be computed by brute-
force summation, but the time required to do so is exponential in the
size of y . indeed, both id136 problems are intractable for general
graphs, because any propositional satis   ability problem can be easily
represented as a factor graph.

in the case of linear-chain crfs, both id136 tasks can be per-
formed e   ciently and exactly by variants of the standard dynamic-
programming algorithms for id48s. we begin by presenting these
algorithms     the forward   backward algorithm for computing marginal

313

314

id136

distributions and viterbi algorithm for computing the most probable
assignment     in section 4.1. these algorithms are a special case of the
more general belief propagation algorithm for tree-structured graphical
models (section 4.2.2). for more complex models, approximate infer-
ence is necessary.

in one sense, the id136 problem for a crf is no di   erent than
that for any graphical model, so any id136 algorithm for graphical
models can be used, as described in several textbooks [57, 79]. how-
ever, there are two additional issues that need to be kept in mind in
the context of crfs. the    rst issue is that the id136 subroutine is
called repeatedly during parameter estimation (section 5.1.1 explains
why), which can be computationally expensive, so we may wish to trade
o    id136 accuracy for computational e   ciency. the second issue is
that when approximate id136 is used, there can be complex inter-
actions between the id136 procedure and the parameter estimation
procedure. we postpone discussion of these issues to section 5, when
we discuss parameter estimation, but it is worth mentioning them here
because they strongly in   uence the choice of id136 algorithm.

4.1 linear-chain crfs

in this section, we brie   y review the standard id136 algorithms for
id48s, the forward   backward and viterbi algorithms, and describe
how they can be applied to linear-chain crfs. a survey on these
algorithms in the id48 setting is provided by rabiner [111]. both of
these algorithms are special cases of the belief propagation algorithm
described in section 4.2.2, but we discuss the special case of linear
chains in detail both because it may help to make the later discussion
more concrete, and because it is useful in practice.

(cid:12)

first, we introduce notation which will simplify the forward   
backward recursions. an id48 can be viewed as a factor graph
t   t(yt, yt   1, xt) where z = 1, and the factors are de   ned as:
p(y,x) =
  t(j, i, x) def= p(yt = j|yt   1 = i)p(xt = x|yt = j).
(4.1)

if the id48 is viewed as a weighted    nite state machine, then   t(j, i, x)
is the weight on the transition from state i to state j when the current
observation is x.

4.1 linear-chain crfs

315

now, we review the id48 forward algorithm, which is used to
compute the id203 p(x) of the observations. the idea behind
forward   backward is to    rst rewrite the naive summation p(x) =

(cid:1)

y p(x,y) using the distributive law:

(cid:3)
(cid:3)

y

t(cid:2)
(cid:3)

t=1

yt

yt   1

p(x) =

=

  t(yt, yt   1, xt)

  t(yt, yt   1, xt)

(cid:3)

yt   2

  t   1(yt   1, yt   2, xt   1)

(cid:3)

(4.2)
      

yt   3

(4.3)
now we observe that each of the intermediate sums is reused many
times during the computation of the outer sum, and so we can save an
exponential amount of work by caching the inner sums.

this leads to de   ning a set of forward variables   t, each of which is
a vector of size m (where m is the number of states) which represents
the intermediate sums. these are de   ned as:

  t(j) def= p(x(cid:6)1...t(cid:7), yt = j)

(cid:3)

=

  t(j, yt   1, xt)

y(cid:4)1...t   1(cid:5)

  t(cid:2)(yt(cid:2), yt(cid:2)   1, xt(cid:2)),

(4.4)

(4.5)

t   1(cid:2)

t(cid:2)=1

where the summation over y(cid:6)1...t   1(cid:7) ranges over all assignments to the
sequence of random variables y1, y2, . . . , yt   1. the alpha values can be
computed by the recursion
  t(j) =

  t(j, i, xt)  t   1(i),

(cid:3)

(4.6)

i   s

(cid:1)

with initialization   1(j) =   1(j, y0, x1). (recall from (2.10) that y0 is
the    xed initial state of the id48.) it is easy to see that p(x) =
yt   t(yt) by repeatedly substituting the recursion (4.6) to obtain

(4.3). a formal proof would use induction.

the backward recursion is exactly the same, except that in (4.3), we
push in the summations in reverse order. this results in the de   nition
(4.7)

  t(i) def= p(x(cid:6)t+1...t(cid:7)|yt = i)

  t(cid:2)(yt(cid:2), yt(cid:2)   1, xt(cid:2)),

(4.8)

(cid:3)

t(cid:2)

=

y(cid:4)t+1...t(cid:5)

t(cid:2)=t+1

316

id136

and the recursion

(cid:3)

j   s

  t(i) =

  t+1(j, i, xt+1)  t+1(j),

(4.9)

(cid:1)

which is initialized   t(i) = 1. analogously to the forward case, we
can compute p(x) using the backward variables as p(x) =   0(y0) def=
y1   1(y1, y0, x1)  1(y1).
to compute the marginal distributions p(yt   1, yt|x), which will
prove necessary for parameter estimation, we combine the results of
the forward and backward recursions. this can be seen from either the
probabilistic or the factorization perspectives. first, taking a proba-
bilistic viewpoint we can write

p(yt   1, yt|x) = p(x|yt   1, yt)p(yt   1, yt)

p(x)

p(x(cid:6)1...t   1(cid:7), yt   1)p(yt|yt   1)p(xt|yt)p(x(cid:6)t+1...t(cid:7)|yt)

=

p(x)

=

1
p(x)   t   1(yt   1)  t(yt, yt   1, xt)  t(yt),

(4.11)

(4.12)

where in the second line we have used the fact that x(cid:6)1...t   1(cid:7) is indepen-
dent from x(cid:6)t+1...t(cid:7) and from xt given yt   1, yt. equivalently, from the
factorization perspective, we can apply the distributive law to obtain

(4.10)

p(yt   1, yt|x) =

1

p(x)

  t(yt, yt   1, xt)

   
    (cid:3)
   
    (cid:3)

y(cid:4)1...t   2(cid:5)

t   1(cid:2)
t(cid:2)

t(cid:2)=1

y(cid:4)t+1...t(cid:5)

t(cid:2)=t+1

  

  

  t(cid:2)(yt(cid:2), yt(cid:2)   1, xt(cid:2))

   
   

   
    .

  t(cid:2)(yt(cid:2), yt(cid:2)   1, xt(cid:2))

(4.13)

then, by substituting the de   nitions of    and   , we obtain the same
result as before, namely
1
p(x)   t   1(yt   1)  t(yt, yt   1, xt)  t(yt).

p(yt   1, yt|x) =

(4.14)

t   1(cid:2)

t(cid:2)=1

4.1 linear-chain crfs

317

(cid:1)

the factor of 1/p(x) acts as a normalizing constant for this distribution.
we can compute it by using p(x) =   0(y0) or p(x) =

i   s   t(i).

putting this together, the forward   backward algorithm is: first com-
pute   t for all t using (4.6), then compute   t for all t using (4.9), and
then return the marginal distributions computed from (4.14).
finally, to compute the globally most probable assignment y    =
arg maxy p(y|x), we observe that the trick in (4.3) still works if all
the summations are replaced by maximization. this yields the viterbi
recursion. the analogs of the    variables in forward backward are

  t(j) def= max
y(cid:4)1...t   1(cid:5)

  t(j, yt   1, xt)

  t(cid:2)(yt(cid:2), yt(cid:2)   1, xt(cid:2)).

(4.15)

and these can be computed by the analogous recursion

  t(j) = max
i   s

  t(j, i, xt)  t   1(i).

(4.16)

once the    variables have been computed, the maximizing assignment
can be computed by a backwards recursion

y

   
t = arg max
i   s
   
t = arg max
i   s

  t(i)
   
t+1, i, xt+1)  t(i)

y

for t < t

the recursions for   t and y

  t(y
   
t together comprise the viterbi algorithm.
now that we have described the forward   backward and viterbi
algorithms for id48s, the generalization to linear-chain crfs is
straightforward. the forward   backward algorithm for linear-chain
crfs is identical to the id48 version, except that the transition
weights   t(j, i, xt) are de   ned di   erently. we observe that the crf
model (2.18) can be rewritten as:

p(y|x) =

1

z(x)

where we de   ne

  t(yt, yt   1,xt) = exp

  t(yt, yt   1,xt),

(4.17)

(cid:11)

  kfk(yt, yt   1,xt)

.

(4.18)

t(cid:2)
(cid:10)(cid:3)

t=1

k

318

id136

with that de   nition, the algorithms for the forward recursion (4.6), the
backward recursion (4.9), and the viterbi recursion (4.16) can be used
unchanged for linear-chain crfs. only the interpretation is slightly
di   erent. in a crf we no longer have the probabilistic interpretation
that   t(j) = p(x(cid:6)1...t(cid:7), yt = j) that we have for id48s. instead we de   ne
the   ,   , and    variables using the factorization viewpoints, i.e., we
de   ne    as in (4.5),    as in (4.8), and    as in (4.15). also the results
of the forward and backward recursions are z(x) instead of p(x), that
is, z(x) =   0(y0) and z(x) =

(cid:1)

i   s   t(i).

for the marginal distributions, equation (4.14) remains true with

the change that z(x) replaces p(x), that is,

p(yt   1, yt|x) =
p(yt|x) =

1
z(x)   t   1(yt   1)  t(yt, yt   1, xt)  t(yt).
1
z(x)   t(yt)  t(yt).

(4.19)

(4.20)

we mention three more specialized id136 tasks that can also be
solved using direct analogues of the id48 algorithms. first, if we
wish to sample independent draws of y from the posterior p(y|x),
we can use the forward algorithm combined with a backward sam-
pling pass, exactly as in an id48. second, if instead of    nding the
single best assignment arg maxy p(y|x), we wish to    nd the k assign-
ments with highest id203, we can apply standard algorithms from
id48s [129]. finally, sometimes it is useful to compute a marginal
id203 p(ys|x) over a (possibly non-contiguous) set of nodes with
indices s     [1,2, . . . t ]. for example, this is useful for measuring the
model   s con   dence in its predicted labeling over a segment of input.
this marginal id203 can be computed e   ciently using constrained
forward   backward, as described by culotta and mccallum [30].

4.2

id136 in id114

there are a number of exact id136 algorithms for general graph-
ical models. although these algorithms require exponential time in
the worst case, they can still be e   cient for graphs that occur in
practice. the most popular exact algorithm, the junction tree algo-
rithm, successively groups variables until the graph becomes a tree.

4.2 id136 in id114

319

once an equivalent tree has been constructed, its marginals can be
computed using exact id136 algorithms that are speci   c to trees.
however, for certain complex graphs, the junction tree algorithm is
forced to make clusters which are very large, which is why the proce-
dure still requires exponential time in the worst case. for more details
on exact id136, see koller and friedman [57].

because of the complexity of exact id136, an enormous amount
of e   ort has been devoted to approximate id136 algorithms. two
classes of approximate id136 algorithms have received the most
attention: monte carlo algorithms and variational algorithms. monte
carlo algorithms are stochastic algorithms that attempt to approx-
imately produce a sample from the distribution of interest. varia-
tional algorithms are algorithms that convert the id136 problem
into an optimization problem, by attempting to    nd a simple approx-
imation that most closely matches the intractable marginals of inter-
est. generally, monte carlo algorithms are unbiased in the sense that
they are guaranteed to sample from the distribution of interest given
enough computation time, although it is usually impossible in practice
to know when that point has been reached. variational algorithms, on
the other hand, can be much faster, but they tend to be biased, by which
we mean that they tend to have a source of error that is inherent to
the approximation, and cannot be easily lessened by giving them more
computation time. despite this, variational algorithms can be useful
for crfs, because parameter estimation requires performing id136
many times, and so a fast id136 procedure is vital to e   cient train-
ing. for a good reference on mcmc, see robert and casella [116], and
for variational approaches, see wainwright and jordan [150].

the material in this section is in no way speci   c to crfs, but holds
for any distribution that factorizes according to some factor graph,
whether it be a joint distribution p(y) or a conditional distribution
p(y|x) like a crf. to emphasize this, and to lighten the notation, in
this section we will drop the dependence on x and talk about id136
for joint distributions p(y) that factorize according to some factor graph
g = (v, f ), i.e.,

p(y) = z

   1

  a(ya).

(cid:2)

a   f

320

id136

to carry this discussion to crfs, simply replace   a(ya) in the above
equation with   a(ya,xa), and likewise modify z and p(y) to depend
on x. this point is not just notational but also a   ects the design of
implementations: id136 algorithms can be implemented for generic
factor graphs in such a way that the id136 procedure does not know
if it is dealing with an undirected joint distribution p(y), a crf p(y|x),
or even a directed graphical model.

in the remainder of this section, we outline two examples of approx-
imate id136 algorithms, one from each of these two categories. too
much work has been done on approximate id136 for us to attempt
to summarize it here. rather, our aim is to highlight the general issues
that arise when using approximate id136 algorithms within crf
training. in this section, we focus on describing the id136 algo-
rithms themselves, whereas in section 5 we discuss their application to
crfs.

4.2.1 id115

currently the most popular type of monte carlo method for com-
plex models is id115 (mcmc) [116]. rather
than attempting to approximate a marginal distribution p(ys) directly,
mcmc methods generate approximate samples from the joint distribu-
tion p(y). mcmc methods work by constructing a markov chain, whose
state space is the same as that of y , in a careful way so that when the
chain is simulated for a long time, the distribution over states of the
chain is approximately p(ys). suppose that we want to approximate
the expectation of some function f(y) over the distribution p(y). given
a sample y1,y2, . . . ,ym from a markov chain in an mcmc method, we
can approximate this expectation as:

(cid:3)

y

p(y)f(y)     1
m

m(cid:3)

j=1

f(yj).

(4.21)

for example, we will see in the next section that expectations of this
form are used during crf training.

a simple example of an mcmc method is id150. in each
iteration of the id150 algorithm, each variable is resampled

(cid:1)

4.2 id136 in id114

321

individually, keeping all of the other variables    xed. suppose that we
already have a sample yj from iteration j. then to generate the next
sample yj+1,

(1) set yj+1     yj.
(2) for each s     v , resample component ys. sample yj+1

s

from

the distribution p(ys|y\s,x).

(3) return the resulting value of yj+1.

y\ys to indicate a sum-
recall from section 2.1.1 that the summation
mation over all possible assignments y whose value for variable ys is
equal to ys.

the above procedure de   nes a markov chain that can be used to
approximate expectations as in (4.21). in the case of a general factor
graph, the id155 can be computed as

p(ys|y\s) =   

  a(ya),

(4.22)

(cid:2)

a   f

where    is a normalizing constant. (in the following,    will denote a
generic normalizing constant which need not be the same across equa-
tions.) the normalizer    from (4.22) is much easier to compute than
the joint id203 p(y|x), because computing    requires a summa-
tion only over all possible values of ys rather than assignments to the
full vector y.

a major advantage of id150 is that it is simple to imple-
ment. indeed, software packages such as bugs can take a graphical
model as input and automatically compile an appropriate gibbs
sampler [78]. the main disadvantage of id150 is that it can
work poorly if p(y) has strong dependencies, which is often the case in
sequential data. by    works poorly    we mean that it may take many
iterations before the distribution over samples from the markov chain
is close to the desired distribution p(y).

there is an enormous literature on mcmc algorithms. the text-
book by robert and casella [116] provides an overview. however,
mcmc algorithms are not commonly applied in the context of crfs.
perhaps the main reason for this is that as we have mentioned earlier,
parameter estimation by maximum likelihood requires calculating

322

id136

marginals many times. in the most straightforward approach, one
mcmc chain would be run for each training example for each parame-
ter setting that is visited in the course of a id119 algorithm.
since mcmc chains can take thousands of iterations to converge, this
can be computationally prohibitive. one can imagine ways of address-
ing this, such as not running the chain all the way to convergence (see
section 5.4.3).

4.2.2 belief propagation

an important variational id136 algorithm is belief propagation (bp),
which we explain in this section. bp is also of interest because it is a
direct generalization of the exact id136 algorithms for linear-chain
crfs.

suppose that the factor graph g = (v, f ) is a tree, and we wish
to compute the marginal distribution of a variable ys. the intuition
behind bp is that each of the neighboring factors of ys makes a mul-
tiplicative contribution to its marginal, called a message, and each of
these messages can be computed separately because the graph is a tree.
more formally, for every factor index a     n(s), denote by ga = (va, fa)
the subgraph of g that contains ys,   a, and the entire subgraph of g
that is    upstream    of   a. by upstream we mean that va contains all
of the variables that   a separates from ys, and fa all of the factors.
this is depicted in figure 4.1. all of the sets va\ys for all a     n(s) are
mutually disjoint because g is a tree, and similarly for the fa. this
means that we can split up the summation required for the marginal
into a product of independent subproblems as:

p(ys)    

=

=

(cid:3)
(cid:3)
(cid:2)

(cid:2)
(cid:2)
(cid:3)

a   n(s)

a   f

y\ys

y\ys

  a(ya)

(cid:2)
(cid:2)

  b   fa

a   n(s)

yva\ys

  b   fa

  b(yb)

  b(yb).

(4.23)

(4.24)

(4.25)

although the notation somewhat obscures this, notice that the variable
ys is contained in all of the ya, so that it appears on both sides of (4.25).

4.2 id136 in id114

323

fig. 4.1 illustration of how marginal distributions factorize for tree-structured graphs. this
factorization is exploited by the belief propagation algorithm (section 4.2.2).

denote each factor in the above equation by mas, that is,

mas(ys) =

  b(yb).

(4.26)

(cid:3)

(cid:2)

yva\ys

  b   fa

each mas is just the marginal distribution over the variable ys for
the subgraph ga. the marginal distribution of ys in the full graph g
is the product of the marginals in all the subgraphs. metaphorically,
each mas(ys) can be thought of as a message from the factor a to the
variable ys that summarizes the impact of the network upstream of a
on the marginal id203 over ys. in a similar fashion, we can de   ne
messages from variables to factors as

msa(ys) =

  b(yb).

(4.27)

(cid:3)

(cid:2)

yvs

  b   fs

then, from (4.25), we have that the marginal p(ys) is proportional
to the product of all the incoming messages to variable ys. similarly,
factor marginals can be computed as
p(ya)       a(ya)

msa(ya).

(cid:2)

(4.28)

s   n(a)

324

id136

naively computing the messages according to (4.26) is impractical,
because the messages as we have de   ned them require summation over
all possible assignments to yva, and some of the sets va will be large.
fortunately, the messages can also be written using a recursion that
requires only local summation. the recursion is

(cid:2)

t   a\s

  a(ya)

(cid:3)
(cid:2)

ya\ys

mbs(ys).

b   n(s)\a

mas(ys) =

msa(ys) =

mta(yt)

(4.29)

that this recursion matches the explicit de   nition of m can be seen by
repeated substitution, and proven by induction. in a tree, it is possi-
ble to schedule these recursions such that the antecedent messages are
always sent before their dependents, by    rst sending messages from the
root, and so on. this is the belief propagation algorithm [103].

in addition to computing single-variable marginals, we will also
wish to compute factor marginals p(ya) and joint probabilities p(y)
for a given assignment y. (recall that the latter problem is di   cult
because it requires computing the partition function log z.) first, to
compute marginals over factors we can use the same decomposition of
the marginal as for the single-variable case, and get

p(ya) =     a(ya)

msa(ys),

(4.30)

(cid:2)

s   n(a)

where    is a id172 constant. in fact, a similar idea works for
any connected set of variables     not just a set that happens to be
the domain of some factor     although if the set is too large, then
computing    is impractical.

bp can also be used to compute the normalizing constant z.
this can be done directly from the propagation algorithm,
in an
analogous way to the forward   backward algorithm in section 4.1. alter-
natively, there is another way to compute z from only the approx-
imate marginals at the end of the algorithm. in a tree structured
distribution p(y), it can be shown that the joint distribution always

4.2 id136 in id114

325

(4.31)

(4.32)

factorizes as

(cid:2)

s   v

(cid:2)

a

(cid:12)

p(ys)

p(ya)
t   a p(yt) .

p(y) =

for example, in a linear chain this amounts to

p(y) =

p(yt)

p(yt, yt   1)
p(yt)p(yt   1) ,

t(cid:2)

t=1

t(cid:2)
(cid:12)

t=1

which, after cancelling and rearranging terms, is just another way to
t p(yt|yt   1). using this identity, we
write the familiar equation p(y) =
can compute p(y) for any assignment p(y) from the per-variable and
per-factor marginals. this also gives us z = p(y)   1

(cid:12)

a   f   a(ya).

if g is a tree, belief propagation computes the marginal distributions
exactly. indeed, if g is a linear chain, then bp reduces to the forward   
backward algorithm (section 4.1). to see this, refer to figure 4.2. the
   gure shows a three node linear chain along with the bp messages as we
have described them in this section. to see the correspondence to for-
ward backward, the forward message that we denoted   2 in section 4.1
corresponds to the product of the two messages ma2 and mc2 (the
dark grey arrows in the    gure). the backward message   2 corresponds
to the message mb2 (the light grey arrow in the    gure). indeed, the
decomposition of the marginal distribution p(ya) in (4.30) generalizes
that for the linear chain case in (4.14).

if g is not a tree, the message updates (4.29) are no longer guar-
anteed to return the exact marginals, nor are they guaranteed even to
converge, but we can still iterate them in an attempt to    nd a    xed
point. this procedure is called loopy belief propagation. to emphasize
the approximate nature of this procedure, we refer to the approximate

fig. 4.2 illustration of the correspondence between forward backward and belief propaga-
tion in linear chain graphs. see text for details.

326

id136

marginals that result from loopy bp as beliefs rather than as marginals,
and denote them by q(ys).

there is still the question of what schedule we use to iterate the
message updates. in the tree structured case, any propagation schedule
will converge to the correct marginal distributions, but this is not true
in the loopy case: rather, the schedule that is used to update messages
can a   ect not only the    nal answer from loopy bp but also whether the
algorithm converges at all. a simple choice that works well in practice
is to order the message updates randomly, for example, to shu   e the
factors via a random permutation, and then for each factor in turn,
send all of its outgoing and incoming messages via (4.29). however,
more sophisticated schedules can also be e   ective [35, 135, 152].

surprisingly, loopy bp can be seen as a variational method for infer-
ence, meaning that there exists an objective function over beliefs that
is approximately minimized by the iterative bp procedure. we give an
overview of this argument below; for more details, see several introduc-
tory papers [150, 158].

the general idea behind a variational algorithm is:
(1) de   ne a family of tractable approximations q and an objec-
tive function o(q) for q     q. each q could be either a
distribution whose marginals are easy to compute, like a
tree-structured distribution, or it could simply be a set of
approximate marginal distributions. if the latter strategy
is used, then the approximate marginals are often called
pseudomarginals, because they need not correspond to the
marginals of any joint distribution over y. the function o
should be designed to measure how well a tractable q     q
approximates p.

(2) find the    closest    approximation q
(3) use q

    to approximate the marginals of p.

    = minq   qo(q).

for example, suppose that we take q be the set of all possible distri-
butions over y, and we choose the objective function

(cid:3)
(cid:3)
o(q) = kl(q(cid:16)p)     log z

=    h(q)    

a

ya

q(ya)log   a(ya),

(4.33)
(4.34)

4.2 id136 in id114

327
   , we can use the
once we minimize this with respect to q to obtain q
    to approximate those of p. indeed, the solution to this
marginals of q
   ) =    log z. so
    = p with optimal value o(q
variational problem is q
solving this particular variational formulation is equivalent to perform-
ing exact id136. approximate id136 techniques can be devised
by changing the set q     for example, by requiring q to be fully factor-
ized     or by using a di   erent objective o. for example, the mean    eld
s qs(ys)
method arises by requiring q to be fully factorized, i.e., q(y) =
for some choice for qs, and    nding the factorized q that maximizes o(q)
as given by (4.34).

(cid:12)

with that background on variational methods,

let us see how
belief propagation can be understood in this framework. we make two
approximations. first, we approximate the id178 term h(q) of (4.34),
which as it stands is di   cult to compute. if q were a tree-structured
distribution, then its id178 could be written exactly as

hbethe(q) =    

q(ya)log q(ya) +

(di     1)q(yi)log q(yi),

(cid:3)

(cid:3)

a

ya

(cid:3)

(cid:3)

i

yi

(4.35)
where di is the degree of i, that is, the number of factors that depend
on yi. this follows from substituting the tree-structured factorization
(4.31) of the joint into the de   nition of id178. if q is not a tree, then
we can still take hbethe as an approximation to h to compute the exact
variational objective o. this yields the bethe free energy:
q(ya)log   a(ya)

obethe(q) =    hbethe(q)    

(cid:3)

(cid:3)

(4.36)

a

ya

the objective obethe depends on q only through its marginals, so rather
than optimizing it over all id203 distributions q, we can optimize
over the space of all marginal vectors. speci   cally, every distribution q
has an associated belief vector q, with elements qa;ya for each factor a
and assignment ya, and elements qi;yi for each variable i and assign-
ment yi. the space of all possible belief vectors has been called the
marginal polytope [150]. however, for intractable models, the marginal
polytope can have extremely complex structure.
this leads us to the second variational approximation made by
loopy bp, namely that the objective obethe is minimized instead over a

328

id136

relaxation of the marginal polytope. the relaxation is to require that
the beliefs be only locally consistent, that is, that
qa(ya) = qi(yi)    a, i     a.

(cid:3)

(4.37)

ya\yi

as a technical point, if a set of putative marginal distributions satis   es
(4.37), this does not imply that they are globally consistent, i.e., that
there exists a single joint q(y) that has those marginals. for this reason,
the distributions qa(ya) are also called pseudomarginals.
yedidia et al. [157] show that constrained stationary points of obethe
under the constraints (4.37) are    xed points of loopy bp. so we can
view the bethe energy obethe as an objective function that the loopy
bp    xed-point operations attempt to optimize.

this variational perspective provides new insight into the method
that would not be available if we thought of
it solely from the
message passing perspective. one of the most important insights is
that it shows how to use loopy bp to approximate log z. because
we introduced minq obethe(q) as an approximation to minq o(q), and
we know that minq o(q) = log z, then it seems reasonable to de   ne
log zbethe = minq obethe(q) as an approximation to log z. this will be
important when we discuss crf parameter estimation using bp in
section 5.4.2.

4.3

implementation concerns

in this section, we mention a few implementation techniques that
are important to practical id136 in crfs: sparsity and preventing
numerical under   ow.

first, it is often possible to exploit sparsity in the model to make
id136 more e   cient. two di   erent types of sparsity are relevant:
sparsity in the factor values, and sparsity in the features. first, about
the factor values, recall that in the linear-chain case, each of the forward
updates (4.6) and backward updates (4.9) requires o(m 2) time, that
is, quadratic time in the number of labels m. analogously, in general
crfs, an update of loopy bp in a model with pairwise factors requires
o(m 2) time. in some models, however, it is possible to implement

4.3 implementation concerns

329

id136 more e   ciently, because it is known a priori not all factor val-
ues (yt, yt   1) are feasible, that is, the factor   t(yt, yt+1,xt) is always 0
for many values yt, yt+1. in such cases, the computational cost of send-
ing a message can be reduced by implementing the message-passing
iterations using sparse matrix operations.

the second kind of sparsity that is useful is sparsity in the fea-
ture vectors. recall from (2.26) that computing the factors   c(xc,yc)
requires computing a dot product between the parameter vector   p
and and the vector of features fc = {fpk(yc,xc)|   p,   k}. often, many
elements of the vectors fc are zero. for example, natural language appli-
cations often involve binary indicator variables on word identity. in
this case, the time required to compute the factors   c can be greatly
improved using a sparse vector representation. in a similar fashion, we
can use sparsity to improve the time required to compute the likelihood
gradient, as we discuss in section 5.

a related trick, that will also speed up forward backward, is to tie
the parameters for certain subsets of transitions [24]. this has the e   ect
of reducing the e   ective size of the model   s transition matrix, lessening
the e   ect of the quadratic dependence of the size of the label set.

a second implementation concern that arises in id136 is avoiding
numerical under   ow. the probabilities involved in forward   backward
and belief propagation, i.e.,   t and msa are often too small to be repre-
sented within numerical precision (for example, in an id48   t decays
toward 0 exponentially fast in t). there are two standard approaches
to this common problem. one approach is to scale each of the vectors
  t and   t to sum to 1, thereby magnifying small values. this scaling
does not a   ect our ability to compute z(x) because it can be com-
puted as z(x) = p(y(cid:3)|x)   1
(cid:3)
t+1,xt)) for an arbitrary assign-
ment y(cid:3), where p(y(cid:3)|x)   1 is computed from the marginals using (4.31).
but in fact, there is actually a more e   cient method described by
rabiner [111] that involves saving each of the local scaling factors. in
any case, the scaling trick can be used in forward   backward or loopy
bp; in either case, it does not a   ect the    nal values of the beliefs.

t(  t(y

(cid:12)

(cid:3)
t, y

a second approach to preventing under   ow is to perform compu-
tations in the logarithmic domain, e.g., the forward recursion (4.6)

330

id136

becomes

log   t(j) =

log   t(j, i, xt) + log   t   1(i)

(cid:26)

(cid:27)

(cid:25)

i   s

,

(4.38)

where     is the operator a     b = log(ea + eb). at    rst, this does not
seem much of an improvement, since numerical precision is lost when
computing ea and eb. but     can be computed as

a     b = a + log(1 + eb   a) = b + log(1 + ea   b),

(4.39)

which can be much more numerically stable if we pick the version of
the identity with the smaller exponent.

at    rst, it would seem that the id172 approach is prefer-
able to the logarithmic approach, because the logarithmic approach
requires o(t m 2) calls to the special functions log and exp, which can
be computationally expensive. this observation is correct for id48s,
but not for crfs. in a crf, even when the id172 approach is
used, it is still necessary to call the exp function in order to compute
  t(yt, yt+1,xt), de   ned in (4.18). so in crfs, special functions can-
not be avoided. in the worst case, there are t m 2 of these   t values, so
the id172 approach needs t m 2 calls to special functions just as
the logarithmic domain approach does. however, there are some special
cases in which the id172 approach can yield a speedup, such
as when the transition features do not depend on the observations, so
that there are only m 2 distinct   t values.

5

parameter estimation

in this section we discuss how to estimate the parameters    = {  k}
of a crf. in the simplest and typical case, we are provided with
fully labeled independent data, but there has also been work in semi-
supervised learning with crfs, crfs with latent variables and crfs
for relational learning.

one way to train crfs is by maximum likelihood, that is, the
parameters are chosen such that the training data has highest prob-
ability under the model. in principle, this can be done in a manner
exactly analogous to id28, which should not be surprising
given the close relationship between these models that was described
in section 2. the main di   erence is computational: crfs tend to have
more parameters and more complex structure than a simple classi   er,
so training is correspondingly more expensive.

in tree structured crfs, the maximum likelihood parameters can be
found by a numerical optimization procedure that calls the id136
algorithms of section 4.1 as a subroutine. the id136 algorithms
are used to compute both the value of the likelihood and its gradient.
crucially, the likelihood is a convex function of the parameters, which
means that powerful optimization procedures are available that prov-
ably converge to the optimal solution.

331

332 parameter estimation

we begin by describing maximum likelihood training, both in the
linear chain case (section 5.1.1) and in the case of general graphical
structures (section 5.1.2), including the case of latent variables. we
also describe two general methods for speeding up parameter estima-
tion that exploit iid structure in the data: stochastic id119
(section 5.2) and multithreaded training (section 5.3).

for crfs with general structures, exact maximum likelihood train-
ing is intractable, so typically approximate procedures must be used.
at a high level, there are two strategies to dealing with this problem.
the    rst strategy is to approximate the likelihood with another
function that is easy to compute, which we call a surrogate likeli-
hood, and to optimize the surrogate function numerically. the second
strategy is an approximate marginal strategy that    plugs in    an
approximate id136 algorithm to compute approximate marginal
distributions wherever the maximum likelihood training algorithm
demands exact marginal distributions. some care must be used here,
because there can be interesting complications in the interaction
between approximate id136 and learning. we discuss these issues in
section 5.4.

5.1 maximum likelihood

5.1.1 linear-chain crfs

i=1, where each x(i) = {x(i)

in a linear-chain crf, the maximum likelihood parameters can be
determined using numerical optimization methods. we are given iid
training data d = {x(i),y(i)}n
} is
1 ,x(i)
a sequence of inputs, and each y(i) = {y
} is a sequence
(i)
(i)
2 , . . . y
t
of the desired predictions. to simplify the notation, we have assumed
that every training sequence x(i) has the same length t . usually, every
sequence will have a di   erent length     in other words, t will depend
on i. the discussion below can be extended to cover this situation in a
straightforward fashion.

2 , . . .x(i)

(i)
1 , y

t

parameter estimation is typically performed by penalized maximum
likelihood. because we are modeling the conditional distribution, the
following log likelihood, sometimes called the conditional log likelihood,

5.1 maximum likelihood

333

is appropriate:

n(cid:3)

i=1

(cid:8)(  ) =

log p(y(i)|x(i);   ).

(5.1)

to compute the maximum likelihood estimate, we maximize (cid:8)(  ), that
is, the estimate is     ml = sup   (cid:8)(  ).
one way to understand the conditional likelihood p(y|x;   ) is to
(cid:3)) to form a joint

imagine combining it with some arbitrary prior p(x;   
p(y,x). then considering the joint log likelihood

log p(y,x;   ) = log p(y|x;   ) + log p(x;   

(5.2)
(cid:3)) does not depend on the parameters    of
notice that the term p(x;   
the conditional distribution. if we do not need to estimate p(x), then
when computing the maximum likelihood estimate of   , we can simply
drop the second term from the maximization, which leaves (5.1).

),

(cid:3)

after substituting in the crf model (2.18) into the likelihood (5.1),

we get the following expression:

n(cid:3)

t(cid:3)

k(cid:3)

i=1

t=1

k=1

(cid:8)(  ) =

  kfk(y

(i)
t

, y

t )     n(cid:3)

(i)
t   1,x(i)

i=1

log z(x(i)),

(5.3)

it is often the case that we have a large number of parameters, e.g.,
several hundred thousand. to reduce over   tting, we use id173,
which is a penalty on weight vectors whose norm is too large. a com-
mon choice of penalty is based on the euclidean norm of    and on
a id173 parameter 1/2  2 that determines the strength of the
penalty. then the regularized log likelihood is

n(cid:3)

t(cid:3)

k(cid:3)

i=1

t=1

k=1

(cid:8)(  ) =

  kfk(y

(i)
t

, y

(i)
t   1,x(i)

t )     n(cid:3)

log z(x(i))     k(cid:3)

i=1

  2
k
2  2 .

k=1

(5.4)
the parameter   2 is a free parameter which determines how much to
penalize large weights. intuitively, the idea is to reduce the potential
for a small number of features to dominate the prediction. the nota-
tion for the regularizer is intended to suggest that id173 can

334 parameter estimation

also be viewed as performing maximum a posteriori (map) estima-
tion of   , if    is assigned a gaussian prior with mean 0 and covari-
ance   2i. determining the best id173 parameter can require
a computationally-intensive parameter sweep. fortunately, often the
accuracy of the    nal model is not sensitive to small changes in   2 (e.g.,
up to a factor of 10). the best value of   2 depends on the size of the
training set; for training sets such as those described in section 5.5, we
have often used a value like   2 = 10.

an alternative choice of id173 is to use the l1 norm instead
of the euclidean norm, which corresponds to an double exponential
prior on parameters [44]. this results in the following penalized likeli-
hood:

(cid:3)
(cid:8)

(  ) =

  kfk(y

(i)
t

, y

k(cid:3)

k=1

n(cid:3)
t(cid:3)
    n(cid:3)

t=1

i=1

t   1,x(i)
(i)
t )
k(cid:3)

|  k|,

log z(x(i))       

(5.5)

i=1

k=1

where    is a id173 parameter that needs to be tuned, analogous
to   2 for the l2 regularizer. this regularizer tends to encourage sparsity
in the learned parameters, meaning that most of the   k are 0. this
can be useful for performing feature selection, and also has theoretical
advantages [97]. in practice, models trained with the l1 regularizer tend
to be sparser but have roughly the same accuracy as models training
using the l2 regularizer [65]. a disadvantage of the l1 regularizer is
that it is not di   erentiable at 0, which somewhat complicates numerical
optimization [3, 44, 160].

in general, the function (cid:8)(  ) cannot be maximized in closed form,
so numerical optimization is used. the partial derivatives of (5.4) are

   (cid:8)
     k

=

fk(y

(i)
t

, y

t   1,x(i)
(i)
t )

n(cid:3)
t(cid:3)
t(cid:3)
    n(cid:3)

t=1

i=1

(cid:3)

i=1

t=1

y,y(cid:2)

(cid:3)

fk(y, y

,x(i)

t )p(y, y

(cid:3)|x(i))       k
  2 .

(5.6)

the    rst term can be interpreted as the expected value of fk under the
empirical distribution   p, which is de   ned as

5.1 maximum likelihood

335

n(cid:3)

i=1

  p(y,x) =

1
n

1{y=y(i)}1{x=x(i)}.

(5.7)

the second term, which arises from the derivative of log z(x), is the
expectation of fk under the model distribution p(y|x;   )  p(x). therefore,
at the unregularized maximum likelihood solution, when the gradient
is zero, these two expectations are equal. this pleasing interpretation is
a standard result about id113 in exponential
families.

to compute the likelihood (cid:8)(  ) and its derivative is where we require
the id136 techniques that we introduced in section 4. first, in
the likelihood, id136 is needed to compute the partition function
z(x(i)), which is a sum over all possible labellings. second, in the
derivatives, id136 is required to compute the marginal distributions
(cid:3)|x(i)). because both of these quantities depend on x(i), we will
p(y, y
need to run id136 once for each training instance every time the like-
lihood is computed. this is a di   erence from generative models, such
as the undirected generative models of section 2.1.1. undirected gener-
ative models can also be trained by maximum likelihood, but for those
models z depends only on the parameters, not on both the parameters
and the input. the requirement to run id136 n times for each like-
lihood computation is the motivation behind stochastic gradient ascent
methods (section 5.2).

(cid:1)

now we discuss how to optimize (cid:8)(  ). the function (cid:8)(  ) is con-
cave, which follows from the convexity of functions of the form g(x) =
i exp xi. convexity is extremely helpful for parameter estimation,
log
because it means that every local optimum is also a global optimum.
in addition, if a strictly concave regularizer is used, such as the l2 reg-
ularizer, then the objective function becomes strictly concave, which
implies that it has exactly one global optimum.

perhaps the simplest approach to optimize (cid:8) is steepest ascent along
the gradient (5.6), but this requires too many iterations to be practical.
newton   s method converges much faster because it takes into account
the curvature of the likelihood, but it requires computing the hessian,

336 parameter estimation

the matrix of all second derivatives. the size of the hessian is quadratic
in the number of parameters. since practical applications often use tens
of thousands or even millions of parameters, simply storing the full
hessian is not practical.

instead, techniques for optimizing (5.4) make approximate use
of second-order information. particularly successful have been quasi-
id77s such as bfgs [8], which compute an approximation
to the hessian from only the    rst derivative of the objective function.
a full k    k approximation to the hessian still requires quadratic
size, however, so a limited-memory version of bfgs is used, due to
byrd et al. [17]. conjugate gradient is another optimization technique
that also makes approximate use of second-order information and
has been used successfully with crfs. for a good introduction to
both limited-memory bfgs and conjugate gradient, see nocedal and
wright [100]. either can be thought of as a black-box optimization
routine that is a drop-in replacement for vanilla gradient ascent. when
such second-order methods are used, gradient-based optimization is
much faster than the original approaches based on iterative scaling
in la   erty et al. [63], as shown experimentally by several authors
[80, 92, 125, 153]. finally, trust region methods have recently been
shown to perform well on multinomial id28 [74], and may
work well for crfs as well.

all of these optimization algorithms     steepest descent, newton   s
method, quasi-id77s, conjugate gradient, and trust region
methods     are standard techniques for numerically optimizing nonlin-
ear functions. we apply them    o    the shelf    to optimize the regularized
likelihood of a crf. typically these algorithms require the ability to
calculate both the value and the gradient of the function that they opti-
mize. in our case the value is the likelihood (5.4) and the    rst-order
derivatives are given by (5.6). this is why in section 4 we discussed how
to compute the partition function z(x) in addition to the marginals.
finally, we discuss the computational cost of training linear chain
models. as we will see in section 4.1, the likelihood and gradient for
a single training instance can be computed by forward   backward in
time o(t m 2), where m is the number of labels and t the length of
the training instance. because we need to run forward   backward for

5.1 maximum likelihood

337

each training instance, each computation of the likelihood and gra-
dient requires o(t m 2n) time, so that the total cost of training is
o(t m 2n g), where g the number of gradient computations required
by the optimization procedure. unfortunately, g depends on the data
set and is di   cult to predict in advance. for batch l-bfgs on linear-
chain crfs, it is often but not always under 100. for many data
sets, this cost is reasonable, but if the number of states m is large,
or the number of training sequences n is very large, then this can
become expensive. depending on the number of labels, training crfs
can take anywhere from a few minutes to a few days; see section 5.5
for examples.

5.1.2 general crfs

parameter estimation for general crfs is essentially the same as for
linear chains, except that computing the model expectations requires
more general id136 algorithms. first, we discuss the fully-observed
case, in which the training and testing data are independent, and the
training data is fully observed. in this case the conditional log likeli-
hood, using the notation of section 2.4, is

(cid:3)

(cid:3)

k(p)(cid:3)

cp   c

  c   cp

k=1

(cid:8)(  ) =

  pkfpk(xc,yc)     log z(x).

(5.8)

the equations in this section do not explicitly sum over training
instances, because if a particular application happens to have iid train-
ing instances, they can be represented by disconnected components in
the graph g.

the partial derivative of the log likelihood with respect to a param-

eter   pk associated with a clique template cp is

(cid:3)

  c   cp

   (cid:8)
     pk

=

(cid:3)

(cid:3)

  c   cp

y(cid:2)

c

fpk(xc,yc)    

fpk(xc,y

(cid:3)
(cid:3)
c)p(y
c

|x).

(5.9)

the function (cid:8)(  ) has many of the same properties as in the linear-chain
case. first, the zero-gradient conditions can be interpreted as requiring
  c fpk(xc,yc) have the same
that the su   cient statistics fpk(x,y) =

(cid:1)

338 parameter estimation

expectations under the empirical distribution and under the model dis-
tribution. second, the function (cid:8)(  ) is concave, and can be e   ciently
maximized by second-order techniques such as conjugate gradient and
l-bfgs. finally, id173 is used just as in the linear-chain
case.

all of the discussion so far has assumed that the training data
contains the true values of all the label variables in the model. latent
variables are variables that are observed at neither training nor test
time. crfs with latent variables have been called hidden-state crfs
(hcrfs) in quattoni et al. [109, 110], which was one of the    rst
examples of latent variable crfs. for other early applications of
hcrfs, see [84, 138]. it is more di   cult to train crfs with latent
variables because the latent variables need to be marginalized out to
compute the likelihood.

suppose we have a crf with inputs x in which the output vari-
ables y are observed in the training data, but we have additional
variables w that are latent, so that the crf has the form:

p(y,w|x) =

1

z(x)

(cid:2)

(cid:2)

cp   c

  c   cp

  c(xc,wc,yc;   p).

(5.10)

a natural objective function to maximize during training is the
marginal likelihood

(cid:8)(  ) = log p(y|x) = log

p(y,w|x).

(5.11)

(cid:3)

w

(cid:1)
the    rst question is how even to compute the marginal likelihood (cid:8)(  ),
because if there are many variables w, the sum cannot be computed
w p(y,w|x)
directly. the key is to realize that we need to compute log
not for any possible assignment y, but only for the particular assign-
ment that occurs in the training data. this motivates taking the origi-
nal crf (5.10), and clamping the variables y to their observed values
in the training data, yielding a distribution over w:

(cid:2)

(cid:2)

p(w|y,x) =

1

z(y,x)

cp   c

  c   cp

  c(xc,wc,yc;   p),

(5.12)

where the id172 factor is

(cid:3)

(cid:2)

(cid:2)

w

cp   c

  c   cp

z(y,x) =

5.1 maximum likelihood

339

  c(xc,wc,yc;   p).

(5.13)

this new id172 constant z(y,x) can be computed by the same
id136 algorithm that we use to compute z(x). in fact, z(y,x) is
easier to compute, because it sums only over w, while z(x) sums over
both w and y. graphically, this amounts to saying that clamping the
variables y in the graph g can simplify the structure among w.

once we have z(y,x), the marginal likelihood can be computed as
p(y|x) =

  c(xc,wc,yc;   p) = z(y,x)
z(x) .

(5.14)

z(x)

1

(cid:3)

(cid:2)

(cid:2)

w

cp   c

  c   cp

now that we have a way to compute (cid:8), we discuss how to maximize it
with respect to   . maximizing (cid:8)(  ) can be di   cult because (cid:8) is no longer
convex in general (log-sum-exp is convex, but the di   erence of two
log-sum-exp functions might not be), so optimization procedures are
typically guaranteed to    nd only local maxima. whatever optimization
technique is used, the model parameters must be carefully initialized
in order to reach a good local maximum.

we discuss two di   erent ways to maximize (cid:8): directly using the
gradient, as in quattoni et al. [109]; and using em, as in mccallum
et al. [84]. (in addition, it is also natural to use stochastic gradient
descent here; see section 5.2.) to maximize (cid:8) directly, we need to cal-
culate its gradient. the simplest way to do this is to use the following
fact. for any function f(  ), we have

(cid:1)
which can be seen by applying the chain rule to log f and rearranging.
w p(y,w|x) yields
applying this to the marginal likelihood (cid:8)(  ) = log

(5.15)

df
d  

= f(  ) dlog f
d  

,

(cid:3)

1(cid:1)
w p(y,w|x)
(cid:3)
p(w|y,x)    
     pk

w

w

   (cid:8)
     pk

=

=

   

[p(y,w|x)]

     pk
[log p(y,w|x)].

(5.16)

(5.17)

340 parameter estimation

this is the expectation of the fully-observed gradient, where the expec-
tation is taken over w. this expression simpli   es to
|y,x)fk(yc,xc,w
(cid:3)
c)
|xc)fk(y
(cid:3)
c,xc,w

(cid:3)
(cid:3)
(cid:3)
(cid:3)

  c   cp
   

   (cid:8)
     pk

(cid:3)
p(w
c

(cid:3)
(cid:3)
c,y
c

(5.18)

(cid:3)
c).

p(w

w(cid:2)

=

c

  c   cp

c,y(cid:2)
w(cid:2)

c

c

this gradient requires computing two di   erent kinds of marginal proba-
|y,x), which
bilities. the    rst term contains a marginal id203 p(w(cid:3)
is exactly a marginal distribution of the clamped crf (5.12). the sec-
|xc), which is the same
ond term contains a di   erent marginal p(w(cid:3)
marginal id203 required in a fully-observed crf. once we have
computed the gradient, (cid:8) can be maximized by standard techniques
such as conjugate gradient. for bfgs, it has been our experience that
the memory-based approximation to the hessian can become confused
by violations of convexity, such as occur in latent-variable crfs. one
practical trick in this situation is to reset the hessian approximation
when that happens.

c,y(cid:3)

c

alternatively, (cid:8) can be optimized using expectation maximization
(em). at each iteration j in the em algorithm, the current parameter
vector   (j) is updated as follows. first, in the e-step, an auxiliary func-
tion q(w) is computed as q(w) = p(w|y,x;   (j)). second, in the m-step,
a new parameter vector   (j+1) is chosen as

  (j+1) = arg max
  (cid:2)

(cid:3)

q(w

)log p(y,w

(cid:3)|x;   

(cid:3)

).

(5.19)

(cid:3)

w(cid:2)

the direct maximization algorithm and the em algorithm are strikingly
similar. this can be seen by substituting the de   nition of q into (5.19)
and taking derivatives. the gradient is almost identical to the direct
gradient (5.18). the only di   erence is that in em, the distribution
p(w|y,x) is obtained from a previous,    xed parameter setting rather
than from the argument of the maximization. we are unaware of any
empirical comparison of em to direct optimization for latent-variable
crfs.

5.2 stochastic gradient methods

341

5.2 stochastic gradient methods

so far, all of the methods that we have discussed for optimizing the
likelihood work in a batch setting, meaning that they do not make
any change to the model parameters until they have scanned the entire
training set. if the training data consist of a large number of iid samples,
then this may seem wasteful. we may suspect that many di   erent
items in the training data provide similar information about the model
parameters, so that it should be possible to update the parameters after
seeing only a few examples, rather than sweeping through all of them.
stochastic id119 (sgd) is a simple optimization method
that is designed to exploit this insight. the basic idea is at every itera-
tion, to pick a training instance at random, and take a small step in the
direction given by the gradient for that instance only. in the batch set-
ting, id119 is generally a poor optimization method, because
the direction of steepest descent locally (that is, the negative gradient)
can point in a very di   erent direction than the optimum. so stochastic
gradient methods involve an interesting tradeo   : the directions of the
individual steps may be much better in l-bfgs than in sgd, but the
sgd directions can be computed much faster.

in order to keep the notation simple, we present sgd only for the
case of linear-chain crfs, but it can be easily used with any graphical
structure, as long as the training data are iid. the gradient of the
likelihood for a single training instance (x(i),y(i)) is

   (cid:8)i
     k

=

fk(y

(i)
t

, y

t   1,x(i)
(i)
t )

t(cid:3)
    t(cid:3)

t=1

(cid:3)

t=1

y,y(cid:2)

(cid:3)

fk(y, y

,x(i)

t )p(y, y

(cid:3)|x(i))       k
n   2 .

(5.20)

this is exactly the same as the full gradient (5.6), with two changes: the
(cid:1)
sum over training instances has been removed, and the id173
contains an additional factor of 1/n. these ensure that the batch gra-
i=1   (cid:8)i,
dient equals the sum of the per-instance gradients, i.e.,    (cid:8) =
where we use    (cid:8)i to denote the gradient for instance i.

n

342 parameter estimation

at each iteration m of sgd, we randomly select a training instance
(x(i),y(i)). then compute the new parameter vector   (m) from the old
vector   (m) by

  (m) =   (m   1) +   m   (cid:8)i(  (m   1)),

(5.21)

where   m > 0 is a step size parameter that controls how far the param-
eters move in the direction of the gradient. if the step size is too large,
then the parameters will swing too far in the direction of whatever
training instance is sampled at each iteration. if   m is too small, then
training will proceed very slowly, to the extent that in extreme cases,
the parameters may appear to have converged numerically when in fact
they are far from the minimum.

(cid:1)
m   2

(cid:1)
m   m =     and

we want   m to decrease as m increases, so that the optimization
algorithm converges to a single answer. classic convergence results
for stochastic approximation procedures [54, 115] provide the minimal
m <    , that is, the    should
requirements of
go to 0 for large m but not too quickly. the most common way to
meet these requirements is to select a step size schedule of a form like
  m     1/m or   m     1/
m. however, simply taking   m = 1/m is usu-
ally bad, because then the    rst few step sizes are too large. instead, a
common trick is to use a schedule like
1

   

(5.22)

  m =

  2(m0 + m) ,

where m0 is a free parameter that needs to be set. a suggestion for
setting this parameter, which is used in the crfsgd package of leon
bottou [13], is to sample a small subset of the training data and run
one pass of sgd over the subset with various    xed step sizes   . pick
    such that the resulting likelihood on the subset after one pass
the   
is highest, and choose m0 such that   0 =   

   .

stochastic id119 has also gone by the name of backprop-
agation in the neural network literature, and many tricks for tuning the
method have been developed over the years [66]. recently, there has
been renewed interest in advanced online optimization methods [27, 43,
126, 149], which also update parameters in an online fashion, but in
a more sophisticated way than simple sgd. vishwanathan et al. [149]
was the    rst application of stochastic gradient methods to crfs.

5.3 parallelism 343

the main disadvantage of stochastic gradient methods is that they
require tuning, unlike o   -the-shelf solvers such as conjugate gradient
and l-bfgs. stochastic gradient methods are also not useful in rela-
tional settings in which the training data are not iid, or on small data
sets. on appropriate data sets, however, stochastic gradient methods
can o   er considerable speedups.

5.3 parallelism

stochastic id119 speeds up the gradient computation by
computing it over fewer instances. an alternative way to speed up
the gradient computation is to compute the gradient over multiple
instances in parallel. because the gradient (5.6) is a sum over training
instances, it is easy to divide the computation into multiple threads,
where each thread computes the gradient on a subset of training
instances. if the crf implementation is run on a multicore machine,
then the threads will run in parallel, greatly speeding up the gradient
computation. this is a characteristic shared by many common machine
learning algorithms, as pointed out by chu et al. [22].

in principle, one could also distribute the gradient computation
across multiple machines, rather than multiple cores of the same
machine, but the overhead involved in transferring large parameter
vectors across the network can be an issue. a potentially promising
way to avoid this is to update the parameter vectors asynchronously.
an example of this idea is recent work on incorporating parallel com-
putation into stochastic gradient methods [64].

5.4 approximate training

all of the training methods that we have described so far, including
the stochastic and parallel gradient methods, assume that the graphical
structure of the crf is tractable, that is, that we can e   ciently com-
pute the partition function z(x) and the marginal distributions p(yc|x).
this is the case, for example, in linear chain and tree-structured crfs.
early work on crfs focused on these cases, both because of the
tractability of id136, and because this choice is very natural for
certain tasks such as sequence labeling tasks in nlp.

344 parameter estimation

when the graphical structure is more complex, then the marginal
distributions and the partition function cannot be computed tractably,
and we must resort to approximations. as described in section 4, there
is a large literature on approximate id136 algorithms. in the context
of crfs, however, there is a crucial additional consideration, which is
that the approximate id136 procedure is embedded within a larger
optimization procedure for selecting the parameters.

there are two general strategies for approximate training in crfs
[139]: a surrogate likelihood strategy in which we modify the objective
function that is used for training, and an approximate marginals strat-
egy in which we approximate the gradient. the    rst strategy involves
   nding a substitute for (cid:8)(  ) (such as the bp approximation (5.27)),
which we will call a surrogate likelihood that is easier to compute but
is still expected to favor good parameter settings. then the surrogate
likelihood can be optimized using a gradient-based method, in a similar
way to the exact likelihood. second, an approximate marginals strategy
means using a generic id136 algorithm to compute an approxima-
tion to the marginals p(yc|x), substituting the approximate marginals
for the exact marginals in the gradient (5.9), and performing a gradient
descent procedure using the resulting approximate gradients.

although surrogate likelihood and approximate marginal methods
are obviously closely related, they are distinct. usually a surrogate
likelihood method directly yields an approximate marginals method,
because just as the derivatives of log z(x) give the true marginal distri-
butions, the derivatives of an approximation to log z(x) can be viewed
as an approximation to the marginal distributions. these approximate
marginals are sometimes termed pseudomarginals [151]. however, the
reverse direction does not always hold: for example, there are certain
approximate marginal procedures that provably do not correspond to
the derivative of any likelihood function [131, 139].

the main advantage of a surrogate likelihood method is that having
an objective function can make it easier to understand the properties of
the method, both to human analysts and to the optimization procedure.
advanced optimization engines such as conjugate gradient and bfgs
require an objective function in order to operate. the advantage to the
approximate marginals viewpoint, on the other hand, is that it is more

5.4 approximate training

345

   exible. it is easy to incorporate arbitrary id136 algorithms, includ-
ing tricks such as early stopping of bp and mcmc. also, approximate
marginal methods    t well within a stochastic gradient framework.

there are aspects of the interaction between approximate infer-
ence and parameter estimation that are not completely understood. for
example, kulesza and pereira [60] present an example of a situation in
which the id88 algorithm interacts in a pathological fashion with
max-product belief propagation. surrogate likelihood methods, by con-
trast, do not seem to display this sort of pathology, as wainwright [151]
points out for the case of convex surrogate likelihoods.

to make this discussion more concrete, in the rest of this section, we
will discuss several examples of surrogate likelihood and approximate
marginal methods. we discuss surrogate likelihood methods based on
pseudolikelihood (section 5.4.1) and belief propagation (section 5.4.2)
and approximate gradient methods based on belief propagation (sec-
tion 5.4.2) and mcmc (section 5.4.3).

5.4.1 pseudolikelihood

one of the earliest surrogate likelihoods is the pseudolikelihood [9]. the
idea in pseudolikelihood is for the training objective to depend only on
conditional distributions over single variables. because the normalizing
constants for these distributions depend only on single variables, they
can be computed e   ciently. in the context of crfs, the pseudolikeli-
hood is

(cid:8)pl(  ) =

log p(ys|yn(s),x;   ).

(5.23)

(cid:3)

s   v

here the summation over s ranges over all output nodes in the graph,
and yn(s) are the values of the variables n(s) that are neighbors
of ys. (as in (5.8), we do not include the sum over training instances
explicitly.)
intuitively, one way to understand pseudolikelihood is that it
attempts to match the local conditional distributions p(ys|yn(s),x;   )
of the model distribution to the training data, and because of the con-
ditional independence assumptions of the model, the local conditional

346 parameter estimation

distributions are su   cient to specify a joint distribution. (this is sim-
ilar to the motivation behind a gibbs sampler.)

the parameters are estimated by maximizing the pseudolikelihood,
i.e., the estimates are     pl = max   (cid:8)pl(  ). typically, the maximization is
carried out by a second-order method such as limited-memory bfgs,
but in principle parallel computation or stochastic gradient can be
applied to the pseudolikelihood exactly in the same way as the full
likelihood. also, id173 can be used just as with maximum
likelihood.

there is no intention that the value of pseudolikelihood function (cid:8)pl
be a close approximation to the value of the true likelihood. rather,
the idea is for the maximum pseudolikelihood estimate     pl to match
the maximum likelihood estimate     ml. that is, the two functions are not
intended to coincide but the two maxima are. under certain conditions,
most notably that the model family is correct, it can be shown that
pseudolikelihood is asymptotically correct, i.e., that it will recover the
true parameters in the limit of an in   nite amount of data.

the motivation behind pseudolikelihood is computational e   ciency.
the pseudolikelihood can be computed and optimized without needing
to compute z(x) or the marginal distributions. although pseudolikeli-
hood has sometimes proved e   ective in nlp [146], more commonly the
performance of pseudolikelihood is poor [134], in an intuitively analo-
gous way that a gibbs sampler can mix slowly in sequential models.
in vision problems, a common criticism of pseudolikelihood is that it
   places too much emphasis on the edge potentials    [149]. an intuitive
explanation for this is that a model trained by pseudolikelihood can
learn to rely on the true values of the neighboring variables, but these
are not available at test time.

one can obtain better performance by performing a    blockwise   
version of pseudolikelihood in which the local terms involve conditional
probabilities of larger regions in the model. for example, in a linear-
chain crf, one could consider a per-edge pseudolikelihood:

(cid:8)epl(  ) =

log p(yt, yt+1|yt   1, yt+2,   ).

(5.24)

t   1(cid:3)

t=1

5.4 approximate training

347

(here we assume that the sequence is padded with dummy labels y0
and yt +1 so that the edge cases are correct.)

this blockwise version of pseudolikelihood is a special case of com-
posite likelihood [34, 75]. in composite likelihood, each of the indi-
vidual terms in the likelihood predicts not just a single variable or
a pair of variable, but a block of variables of arbitrary size that the
user can select. composite likelihood generalizes both standard pseu-
dolikelihood and the    blockwise    pseudolikelihood. there are general
theoretical results concerning asymptotic consistency and normality of
composite likelihood estimators. typically larger blocks lead to bet-
ter parameter estimates, both in theory and in practice. this allows
a tradeo    between training time and the quality of the resulting
parameters.

finally, the piecewise training method of sutton and mccallum
[134, 137] is related to composite likelihood, but is perhaps better
understood as a belief propagation method, so we defer it to the next
section.

5.4.2 belief propagation

the loopy belief propagation algorithm (section 4.2.2) can be used
within approximate crf training. this can be done within either the
surrogate likelihood or the approximate gradient perspectives.

in the approximate gradient algorithm, at every iteration of
training, we run loopy bp on the training input x, yielding a set
of approximate marginals q(yc) for each clique in the model. then
we approximate the true gradient (5.9) by substituting in the bp
marginals. this results in approximate partial derivatives

(cid:3)

  c   cp

      (cid:8)
     pk

=

(cid:3)

(cid:3)

  c   cp

y(cid:2)

c

fpk(xc,yc)    

fpk(xc,y

(cid:3)
c)q(y

(cid:3)
c).

these can be used to update the current parameter setting as

(t+1)
pk =   

(t)
pk +   

  

      (cid:8)
     pk

,

(5.25)

(5.26)

348 parameter estimation

where    > 0 is a step size parameter. the advantages of this setup are
that it is extremely simple, and is especially useful within an outer
stochastic gradient approximation.

more interestingly, however, it is also possible to use loopy bp
within a surrogate likelihood setup. to do this, we need to develop
some surrogate function for the true likelihood (5.8) which has the
property that the gradient of the surrogate likelihood are exactly the
approximate bp gradients (5.26). this may seem like a tall order,
but fortunately it is possible using the bethe free energy described
in section 4.2.2.

remember from that section that loopy belief propagation can be
viewed as an optimization algorithm, namely, one that minimizes the
objective function obethe(q) (4.36) over the set of all locally consistent
belief vectors, and that the minimizing value minq obethe(q) can be used
as an approximation to the partition function. substituting in that
approximation to the true likelihood (5.8) gives us, for a    xed belief
vector q, the approximate likelihood

(cid:8)bethe(  , q) =

log   c(xc,yc)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

cp   c
   

  c   cp

cp   c

+

s   y

q(yc)

  c(xc,yc)

q(yc)log

  c   cp
(1     di)q(ys)log q(ys).

(5.27)

then approximate training can be viewed as the optimization problem
max   minq (cid:8)bethe(  , q). this is a saddlepoint problem, in which we are
maximizing with respect to one variable (to    nd the best parameters)
and minimizing with respect to another (to solve the approximate infer-
ence problem). one approach to solve saddlepoint problems is coordi-
nate ascent, that is, to alternately minimize (cid:8)bethe with respect to q for
   xed    and take a gradient step to partially maximize (cid:8)bethe with respect
to    for    xed b. the    rst step (minimizing with respect to q) is just
running the loopy bp algorithm. the key point is that for the second
step (maximizing with respect to   ), the partial derivatives of (5.27)
with respect to a weight   k is exactly (5.26), as desired.

5.4 approximate training

349

alternatively, there is a di   erent surrogate likelihood that can also

be used. this is

  (cid:8)(  ; q) = log

(cid:28)(cid:12)

(cid:29)

(cid:12)
s   y q(ys)ds   1

  c   cp q(yc)

(cid:12)

cp   c

,

(5.28)

in other words, instead of the true joint likelihood, we use the product
over each clique   s approximate belief, dividing by the node beliefs to
avoid overcounting. the nice thing about this is that it is a direct
generalization of the true likelihood for tree-structured models, as can
be seen by comparing (5.28) with (4.31). this surrogate likelihood can
be justi   ed using a dual version of bethe energy that we have presented
here [91, 94]. when bp has converged, for the resulting belief vector
q, it can be shown that (cid:8)bethe(  , q) =   (cid:8)(  , q). this equivalence does not
hold in general for arbitrary values of q, e.g., if bp has not converged.
another surrogate likelihood method that is related to bp is the
piecewise estimator [137], in which the factors of the model are par-
titioned into tractable subgraphs that are trained independently. this
idea can work surprisingly well (better than pseudolikelihood) if the
local features are su   ciently informative. sutton and minka [139] dis-
cuss the close relationship between piecewise training and early stop-
ping of belief propagation.

5.4.3 id115

id115 (mcmc) id136 methods (section 4.2.1)
can be used within crf training in an approximate marginals frame-
work. once we have chosen a markov chain whose stationary distribu-
tion is p(y|x;   ), the algorithm is to the chain for a number of iterations
and use the resulting approximate marginals   p(y|x;   ) to approximate
the true marginals in the gradient (5.9).

in practice, however, mcmc methods have been less commonly
used in the context of crfs, because mcmc methods typically require
many iterations to reach convergence, and as we have emphasized, infer-
ence needs to be run for many di   erent parameter settings over the
course of training.
one possibility to overcome this di   culty is contrastive divergence
(cd) [50], in which the true marginals p(yc|x) in (5.9) are approximated

350 parameter estimation

by running an mcmc method for only a few iterations, where the initial
state of the markov chain (which is just an assignment to y) is set to
be the value of y in the training data. cd has been mostly applied to
latent variable models such as restricted id82s. while
in principle cd can be applied to crfs, we are unaware of much work
that does this.

another possibility is a more recent method called samplerank
[155], whose objective is that the learned parameters score pairs of
ys such that their sorted ranking obeys a given supervised ranking
(which is often speci   ed in terms of a    xed scoring function on y that
compares to true target values of y). approximate gradients may be
calculated from pairs of successive states of the mcmc sampler. like
cd, samplerank performs parameter updates on individual mcmc
steps rather than waiting for the chain to converge. experiments have
shown that models trained using samplerank can have substantially
better accuracy than cd [155].

in contrast to the approximate marginals framework that we have
been discussing, it is very di   cult to use an mcmc id136 method
within a surrogate likelihood framework, because it is notoriously dif-
   cult to obtain a good approximation to log z(x) given samples from
an mcmc method.

5.5

implementation concerns

to make the discussion of e   cient training methods more concrete, here
we give some examples of data sets from nlp in which crfs have been
successful. the idea is to give a sense of the scales of problem to which
crfs have been applied, including typical values for the number of
features and typical training times.

we describe three example tasks to which crfs have been applied.
the    rst example task is noun-phrase (np) chunking [120], in which the
problem is to    nd base noun phrases in text, such as the phrases    he   
and    the current account de   cit    in the sentence he reckons the current
account de   cit will narrow. the second task is named identity recog-
nition (ner) [121], the task of identifying proper names in text, such
as person names and company names. the    nal task is part-of-speech

5.5 implementation concerns

351

table 5.1. scale of typical crf applications in natural language processing.

task

parameters

np chunking
ner
id52

248471
187540
509951

observation
functions # sequences # positions labels time (s)

116731
119265
127764

8936
946
38219

211727
204567
912344

3
9
45

958s
4866s
325500s

tagging (pos), that is, labelling each word in a sentence with its part
of speech. the np chunking and pos data sets are derived from the
wsj id32 [82], while the ner data set consists of newswire
articles from reuters.

we will not go into detail about the features that we use, but they
include the identity of the current and previous word, pre   xes and
su   xes, and (for the named-entity and chunking tasks) automatically
generated part of speech tags and lists of common places and person
names. the feature set has a similar    avor to that described in the
named entity example in section 2.6. we do not claim that the feature
sets that we have used are optimal for these tasks, but still they should
be useful for understanding typical scale.

for each of these data sets, table 5.1 shows the number of param-
eters in the trained crf model, the size of the training set, in terms
of the total number of sequences and number of words, the number of
possible labels for each sequence position, and the training time. the
training times range from minutes in the best case to days in the worst
case. as can be expected from our previous discussion, the factor that
seems to most in   uence training time is the number of labels.

obviously the exact training time will depend heavily on details
of the implementation and hardware. for the examples in table 5.1,
we use the mallet toolkit on machines with a 2.4 ghz intel xeon
cpu, optimizing the likelihood using batch l-bfgs without using mul-
tithreaded or stochastic gradient training.

6

related work and future directions

in this section, we brie   y place crfs in the context of related lines of
research, especially that of id170, a general research area
which is concerned with extending classi   cation methods to complex
objects. we also describe relationships both to neural networks and
to a simpler sequence model called maximum id178 markov models
(memms). finally, we outline a few open areas for future work.

6.1 related work

6.1.1 id170

classi   cation methods provide established, powerful methods for pre-
dicting discrete outcomes. but in the applications that we have been
considering in this survey, we wish to predict more complex objects,
such as parse trees of natural
language sentences [38, 144], align-
ments between sentences in di   erent languages [145], and route plans
in mobile robotics [112]. each of these complex objects have internal
structure, such as the tree structure of a parse, and we should be able
to use this structure in order to represent our predictor more e   ciently.

352

6.1 related work

353

this general problem is called id170. just as the crf
likelihood generalizes id28 to predict arbitrary structures,
the    eld of id170 generalizes the classi   cation problem
to the problem of predicting structured objects.

id170 methods are essentially a combination of clas-
si   cation and graphical modeling, combining the ability to compactly
model multivariate data with the ability to perform prediction using
large sets of input features. crfs provide one way to do this, general-
izing id28, but other standard classi   cation methods can
be generalized to the id170 case as well. detailed infor-
mation about id170 methods is available in a recent
collection of research papers [5]. in this section, we give an outline and
pointers to some of these methods.

y   (x) for a set of weight vectors   y.

in order to explain more formally what a    structure    is, we review
the general setup of classi   cation. many classi   cation techniques can
be interpreted as learning a discriminant function f (y,x) over outputs
in a    nite set y     y, for example, y = {1,2, . . . c}. given a test input x,
    = arg maxy f (y,x). for example,
we predict the output and predict y
often the discriminant function is linear in some basis expansion   (x),
e.g., f (y,x) =   t
id170 follows this framework with one essential
di   erence. in id170, the set of possible outputs y is
extremely large, for example, the set of all ways to label the words
in a sentence with named-entity labels, as in section 2.6.1. clearly in
this situation it is not feasible to have a separate weight vector   y
for each y     y, i.e., for each of the possible ways to label a sequence.
so in id170 methods we add the restriction that the
discriminant function decomposes according to the output structure.
formally, in the structured case we write the output as a random vec-
tor y = (y1, y2, . . . yt ), and we require the discriminant to decompose
according to a set of parts ya, each of which is simply a subset of the
variables in y. we index the parts by a     {1,2, . . . a}. then the main
assumption is that the discriminant decomposes as

f (y,x) =

fa(ya,x).

(6.1)

a(cid:3)

a=1

354 related work and future directions

this discussion should be familiar from our discussion of undirected
models general crfs. for example, the log id203 of a general
crf can written in this form, because for the purposes of prediction at
test time we can ignore z(x). id170 methods all share
the property that the discriminant function decomposes according to
a set of parts. they di   er in how the parameters of the discriminant
function are estimated from data.

many types of id170 algorithms have been proposed.
the crf likelihood depend on summation over possible outputs, to
obtain partition function z(x) and to obtain marginal distributions
p(yi|x). most other id170 methods that do not use the
likelihood focus on maximization rather than summation. for example,
maximum-margin methods that are so successful for univariate clas-
si   cation have been generalized to the structured case, yielding the
structured id166 [2, 147] and the maximum margin markov network
[143]. the id88 update can also be generalized to structured
models [25]. the resulting algorithm is particularly appealing because
it is little more di   cult to implement than the algorithm for select-
ing y   . the online id88 update can also be made margin-aware,
yielding the mira algorithm [28].

another class of structured methods are search-based methods
[31, 32] in which a heuristic search procedure over outputs is assumed,
and learns a classi   er that predicts the next step in the search. this has
the advantage of    tting in nicely to many problems that are complex
enough to require performing search. it is also able to incorporate arbi-
trary id168s over predictions (i.e., ones that do not decompose
in the same way that the discriminant function does). finally, lecun
et al. [68] generalizes many prediction methods, including the ones
listed above, under the rubric of energy-based methods.

a general advantage of maximization- and search-based methods
is that they do not require summation over all con   gurations for the
partition function or for marginal distributions. there are certain com-
binatorial problems, such as matching and network    ow problems, in
which    nding an optimal con   guration is tractable, but summing over
con   gurations is not, for example, see taskar et al. [145]. for more
complex problems, neither summation nor maximization is tractable,

6.1 related work

355

so this advantage is perhaps not as signi   cant, although even in this
case maximization makes it easier to apply approximate search meth-
ods such as id125 and pruning. for example, see pal et al. [102]
for an example of the di   culties that can arise when trying to naively
incorporate id125 within crf training.

it is worth noting that although maximization-based training meth-
ods do not use the likelihood during parameter estimation, the resulting
parameters could still be interpreted as de   ning a factorized conditional
id203 distribution p(y|x), and hence a crf by de   nition 2.3.

perhaps the main advantage of probabilistic methods is that they
can incorporate latent variables in a natural way, by marginaliza-
tion. this can be useful, for example, in collective classi   cation meth-
ods [142]. for examples of structured models with latent variables, see
quattoni et al. [109] and mccallum et al. [84]. a particularly powerful
example of this is provided by bayesian methods, in which the model
parameters themselves are integrated out (section 6.2.1). that having
been said, recent work has proposed methods of incorporating latent
variables into id166s and structured id166s [36, 159].

the di   erences

in prediction accuracy between the various
id170 methods are not well understood. to date,
there has been little careful experimental comparison of structured
prediction methods across di   erent domains, although see keerthi
and sundararajan [53] for some experiments in this regard.1 we take
the view that the similarities between various id170
methods are more important than the di   erences. careful selection of
features has more e   ect on performance than the choice of structured
prediction algorithm.

6.1.2 neural networks

there are close relationships between neural networks and crfs,
in that both can be viewed as discriminatively trained probabilistic
models. neural networks are perhaps best known for their use in clas-
si   cation, but they can also be used to predict multiple outputs, for

1 an earlier study [99] appears to have been    awed. see keerthi and sundararajan [53] for
discussion.

356 related work and future directions

example, by using a shared latent representation [18], or by modeling
dependencies between outputs directly [67]. although neural networks
are typically trained using stochastic id119 (section 5.2), in
principle they can be trained using any of the other methods used for
crfs. the main di   erence between them is that neural networks rep-
resent the dependence between output variables using a shared latent
representation, while structured methods learn these dependences as
direct functions of the output variables.

because of this, it is easy to make the mistake of thinking that crfs
are convex and neural networks are not. this is inaccurate. a neural
network without a hidden layer is a linear classi   er that can be trained
e   ciently in a number of ways, while a crf with latent variables has
a non-convex likelihood (section 2.4). the correct way of thinking is:
in fully observed models, the likelihood is convex; in latent variable
models it is not.

so the main new insight of id170 models compared
to neural networks is: if you add connections among the nodes in the
output layer, and if you have a good set of features, then sometimes
you don   t need a hidden layer to get good performance. if you can
a   ord to leave out the hidden layer, then in practice you always want
to do so, because this avoids all of the problems with local minima. for
harder problems, however, one might expect that even after modeling
output structure, incorporating hidden states will still yield additional
bene   t. once hidden states are introduced into the model, whether it
be a neural network or a structured model, it seems to be inevitable
(at least given our current understanding of machine learning) that
convexity will be lost.

there is another sense in which even without hidden states, a crf
is like a neural network. for concreteness consider a linear-chain crf
p(y1, y2|x) over a sequence of length 2. the crf is a linear model,
by which we mean that for any two label assignments y = (y1, y2) and
2), the log odds log(p(y|x)/p(y(cid:3)|x)) are a linear function of
y(cid:3) = (y
(cid:3)
the parameters. however, the marginal distributions behave nonlin-
early. in other words, log(p(y1|x)/p(y
1|x)) is not a linear function of
(cid:3)
the parameters. this is because the variable y2 acts like a hidden vari-
able when computing the marginal distribution over y1. this viewpoint

(cid:3)
1, y

has been exploited to approximate crfs using per-position classi   ers
with an expanded feature set [71].

6.1 related work

357

6.1.3 memms, directed models, and label bias

linear-chain crfs were originally introduced as an improvement to the
maximum-id178 markov model (memm) [85], which is essentially a
markov model in which the transition probabilities are given by logistic
regression. formally, an memm is

t(cid:2)

t=1

(cid:3)

y(cid:2)

pmemm(y|x) =

p(yt|yt   1,x) =

zt(yt   1,x) =

p(yt|yt   1,x)
(cid:10)
k(cid:3)

1

zt(yt   1,x)

exp

(cid:10)
k(cid:3)

k=1
(cid:3)

(cid:11)

  kfk(yt, yt   1,xt)

(cid:11)

exp

  kfk(y

, yt   1,xt)

.

k=1

(6.2)

(6.3)

(6.4)

a similar idea can be extended to general directed graphs, in which the
distribution p(y|x) is expressed by a id110 in which each
local conditional distribution is a id28 model with input x
[117].

t

(cid:12)

in the linear-chain case, notice that the memm works out to have
the same form as the linear-chain crf (5.3) with the exception that in a
crf z(x) is a sum over sequences, whereas in a memm the analogous
t=1 zt(yt   1,x). this di   erence has important consequences.
term is
unlike crfs, maximum likelihood training of memms does not require
performing id136, because zt is just a simple sum over the labels at
a single position, rather than a sum over labels of an entire sequence.
this is an example of the general phenomenon that training of directed
models is less computationally demanding than undirected models.

there are theoretical di   culties with the memm model, however.
memms can exhibit the problem of label bias [63]. the label bias prob-
lem is essentially that future observations cannot a   ect the posterior
distribution over earlier states. to understand the label bias problem,
consider the backward recursion (4.9). in the case of an memm, this

358 related work and future directions

amounts to

(cid:3)

j   s

  t(i) =

p(yt+1 = j|yt = i, xt+1)  t+1(j).

(6.5)

unfortunately, this sum is always 1, regardless of the value of the cur-
rent label i. what this means is that the future observations provide
no information about the current state, which seems to lose a major
advantage of sequence modeling. to see this, assume for the sake of
induction that   t+1(j) = 1 for all j. then it is clear that the sum over
j in (6.5) collapses, and   t(i) = 1.

perhaps a more intuitive way to understand label bias is from the
perspective of id114. consider the graphical model of an
memm, shown in figure 6.1. by looking at the v-structures in the
graph, we can read o    the following independence assumptions: at
all time steps t, the label yt is marginally independent of the future
observations xt+1,xt+2, etc. this independence assumption is usually
strongly violated in sequence modeling, which explains why crfs can
have better performance than memms. also, this independence rela-
tion explains why   t(i) should always be 1. (in general, this corre-
spondence between graph structure and id136 algorithms is one of
main conceptual advantages of graphical modeling.) to summarize this
discussion, label bias is simply a consequence of explaining away.

there is a caveat here: we can always copy information from previ-
ous and future time steps into the feature vector xt, and this is common
in practice. this has the e   ect of adding arcs between (for example) xt
and xt+1. this explains why the performance gap in practice between
memms and crfs is not always as large as might be expected.

this graphical modeling view on label bias highlights an impor-
tant point. label bias is not caused by a model being directed or

y

x

fig. 6.1 graphical model of a maximum id178 markov model [85].

6.2 frontier areas

359

undirected. it is caused by the structure of the particular directed
model that is used in the memm. this point is forcefully corroborated
by berg-kirkpatrick et al. [6], which presents impressive experimental
results on various unsupervised learning tasks by using directed models
whose local conditional distributions have a log linear structure like the
memm does, but which avoid label bias because they have a generative
graphical structure, rather than the v-structures of figure 6.1.

finally, one might try a di   erent way to combine the advantages of
conditional training and directed models. one can imagine de   ning a
directed model p(y,x), perhaps a generative model, and then training
it by optimizing the resulting conditional likelihood p(y|x). in fact, this
procedure has a long history in the speech community, where it is called
maximum mutual information training [4]. naively, this might seem
to o   er a simpler training algorithm than crfs do, because directed
models are usually easier to train than undirected models. but in fact,
this approach does not obviate the need to perform id136 during
training. the reason is that computing the conditional likelihood p(y|x)
requires computing the marginal id203 p(x), which plays the same
role as z(x) in the crf likelihood. in fact, training can be more com-
plex in a directed model, because the model parameters are constrained
to be probabilities     constraints which can actually make the optimiza-
tion problem more di   cult.

6.2 frontier areas

finally, we describe a few open research areas that are related to crfs.
in all of the cases below, the research question is a special case of
a larger question for general id114, but there are special
additional considerations in conditional models that make the problem
more di   cult.

6.2.1 bayesian crfs

because of the large number of parameters in typical applications of
crfs, the models can be prone to over   tting. the standard way to
control this is using id173, as described in section 5.1.1. one
way that we motivated this procedure is as an approximation to a fully

360 related work and future directions

bayesian procedure. that is, instead of predicting the labels of a test-
ing instance x as y    = maxy p(y|x;     ), where      is a single parameter
estimate, in a bayesian method we would use the predictive distribu-
p(y|x;   )p(  |x(1),y(1), . . . ,x(n),y(n))d  . this integral
tion y    = maxy
over    needs to be approximated, for example, by mcmc.

(cid:30)

in general, it is di   cult to formulate e   cient bayesian methods
for undirected models; see [95, 96] for some of the few examples in
this regard. a few papers have specially considered approximate infer-
ence algorithms for bayesian crfs [107, 154, 161], but while these
methods are interesting, they do not seem to be useful at the scale
of current crf applications (e.g., those in table 5.1). even for linear
chain models, bayesian methods are not commonly in use for crfs,
primarily due to the computational demands. if all we want is the ben-
e   ts of model averaging, one may question whether simpler ensemble
learning techniques, such as id112, would give the same bene   t [141].
however, the bayesian perspective does have other potential bene   ts,
particularly when more complex, hierarchical priors are considered.

6.2.2 semi-supervised crfs

one practical di   culty in applying crfs is that training requires
obtaining true labels for potentially many sequences. this can be
expensive because it is more time consuming for a human labeller to
provide labels for sequence labelling than for simple classi   cation. for
this reason, it would be very useful to have techniques that can obtain
good accuracy given only a small amount of labeled data.
one strategy for achieving this goal is semi-supervised learning,
in which in addition to some fully-labelled data {(x(i),y(i))}n
i=1, the
data set is assumed to contain a large number of unlabelled instances
{x(j)}m
j=1, for which we observe only the inputs. however, unlike in
generative models, it is less obvious how to incorporate unlabelled data
into a conditional criterion, because the unlabelled data is a sample
from the distribution p(x), which in principle need have no relationship
to the crf p(y|x). in order to deal with this, several di   erent types
of id173 terms have been proposed that take the unlabelled
data into account, including id178 id173 [46, 52], generalized

6.2 frontier areas

361

expectation criteria [81], discriminative methods [70], posterior regular-
ization [39, 45], and measurement-based learning [73].

6.2.3 structure learning in crfs

all of the methods described in this survey assume that the structure
of the model has been decided in advance. it is natural to ask if we can
learn the structure of the model as well. as in id114 more
generally, this is a di   cult problem. in fact, bradley and guestrin [15]
point out an interesting complication that is speci   c to conditional
models. for a generative model p(x), maximum likelihood structure
learning can be performed e   ciently if the model is restricted to be
tree-structured, using the well-known chow-liu algorithm. in the con-
ditional case, when we wish to estimate the structure of p(y|x), the
analogous algorithm is more di   cult, because it requires estimating
marginal distributions of the form p(yu, yv|x), that is, we need to esti-
mate the e   ects of the entire input vector on every pair of output
variables. it is di   cult to estimate these distributions without knowing
the structure of the model to begin with.

acknowledgments

we thank kedar bellare, francine chen, gregory druck, benson
limketkai, david mimno, ray mooney, prakash nadkarni, oscar
t  ackstr  om, wenhao wang, vittorio ferrari, and three anonymous
reviewers for useful comments on earlier versions of this tutorial. a
previous version of this survey has appeared in sutton and mccal-
lum [136], and as part of charles sutton   s doctoral dissertation [132].

362

references

[1] s. m. aji and r. j. mceliece,    the generalized distributive law,    ieee trans-

actions on id205, vol. 46, no. 2, pp. 325   343, 2000.

[2] y. altun, i. tsochantaridis, and t. hofmann,    hidden markov support vector
machines,    in international conference on machine learning (icml), 2003.
[3] g. andrew and j. gao,    scalable training of l1-regularized id148,   

in international conference on machine learning (icml), 2007.

[4] l. r. bahl, p. f. brown, p. v. de souza, and r. l. mercer,    maximum mutual
information estimation of hidden markov model parameters for speech recogni-
tion,    in international conference on acoustics, speech, and signal processing
(icassp), vol. 11, pp. 49   52, 1986.

[5] g. h. bakir, t. hofmann, b. sch  olkopf, a. j. smola, b. taskar, and s. v. n.

vishwanathan, eds., predicting structured data. mit press, 2007.

[6] t. berg-kirkpatrick, a. bouchard-c  ot  e, j. denero, and d. klein,    painless
unsupervised learning with features,    in conference of the north american
chapter of the association for computational linguistics (hlt/naacl),
pp. 582   590.

[7] a. bernal, k. crammer, a. hatzigeorgiou, and f. pereira,    global discrim-
inative learning for higher-accuracy computational gene prediction,    plos
computational biology, vol. 3, no. 3, 2007.

[8] d. p. bertsekas, nonid135. athena scienti   c, 2nd ed., 1999.
[9] j. besag,    statistical analysis of non-lattice data,    the statistician, vol. 24,

no. 3, pp. 179   195, 1975.

[10] a. blake, p. kohli, and c. rother, eds., markov random fields for vision

and image processing. mit press, 2011.

363

364 references

[11] d. m. blei, a. y. ng, and m. i. jordan,    id44,    journal

of machine learning research, vol. 3, p. 993, 2003.

[12] p. blunsom and t. cohn,    discriminative word alignment with conditional
random    elds,    in international conference on computational linguistics and
annual meeting of the association for computational linguistics (coling-
acl), pp. 65   72, 2006.

[13] l. bottou,    stochastic id119 examples on toy problems,    2010.
[14] y. boykov and m.-p. jolly,    interactive graph cuts for optimal boundary &
region segmentation of objects in nd images,    in international conference on
id161 (iccv), vol. 1, pp. 105   112, 2001.

[15] j. k. bradley and c. guestrin,    learning tree conditional random    elds,    in

international conference on machine learning (icml), 2010.

[16] r. bunescu and r. j. mooney,    collective information extraction with rela-
tional markov networks,    in annual meeting of the association for computa-
tional linguistics (acl), 2004.

[17] r. h. byrd, j. nocedal, and r. b. schnabel,    representations of quasi-newton
matrices and their use in limited memory methods,    mathematical program-
ming, vol. 63, no. 2, pp. 129   156, 1994.

[18] r. caruana,    multitask learning,    machine learning, vol. 28, no. 1, pp. 41   75,

1997.

[19] r. caruana and a. niculescu-mizil,    an empirical comparison of supervised
learning algorithms using di   erent performance metrics,    technical report
tr2005-1973, cornell university, 2005.

[20] h. l. chieu and h. t. ng,    id39 with a maximum
id178 approach,    in conference on natural language learning (conll),
pp. 160   163, 2003.

[21] y. choi, c. cardie, e. rilo   , and s. patwardhan,    identifying sources of
opinions with conditional random    elds and extraction patterns,    in proceed-
ings of the human language technology conference/conference on empirical
methods in natural language processing (hlt-emnlp), 2005.

[22] c.-t. chu, s. k. kim, y.-a. lin, y. yu, g. bradski, a. y. ng, and k. oluko-
tun,    map-reduce for machine learning on multicore,    in advances in neural
information processing systems 19, pp. 281   288, mit press, 2007.

[23] s. clark and j. r. curran,    parsing the wsj using id35 and log-linear
models,    in proceedings of the meeting of the association for computational
linguistics (acl), pp. 103   110, 2004.

[24] t. cohn,    e   cient id136 in large conditional random    elds,    in european
conference on machine learning (ecml), pp. 606   613, berlin, germany,
september 2006.

[25] m. collins,    discriminative training methods for id48:
theory and experiments with id88 algorithms,    in conference on
empirical methods in natural language processing (emnlp), 2002.

[26] p. j. cowans and m. szummer,    a graphical model for simultaneous parti-
tioning and labeling,    in conference on arti   cial intelligence and statistics
(aistats), 2005.

[27] k. crammer, o. dekel, j. keshet, s. shalev-shwartz, and y. singer,    online
passive-aggressive algorithms,    journal of machine learning research, 2006.

references

365

[28] k. crammer and y. singer,    ultraconservative online algorithms for multi-
class problems,    journal of machine learning research, vol. 3, pp. 951   991,
january 2003.

[29] a. culotta, r. bekkerman, and a. mccallum,    extracting social networks
and contact information from email and the web,    in first conference on
email and anti-spam (ceas), mountain view, ca, 2004.

[30] a. culotta and a. mccallum,    con   dence estimation for information extrac-

tion,    in human language technology conference (hlt), 2004.

[31] h. daum  e iii, j. langford, and d. marcu,    search-based structured predic-

tion,    machine learning journal, 2009.

[32] h. daum  e iii and d. marcu,    learning as search optimization: approximate
large margin methods for id170,    in international conference
on machine learning (icml), bonn, germany, 2005.

[33] t. deselaers, b. alexe, and v. ferrari,    localizing objects while learning their

appearance,    in european conference on id161 (eccv), 2010.

[34] j. v. dillon and g. lebanon,    stochastic composite likelihood,    journal of

machine learning research, vol. 11, pp. 2597   2633, october 2010.

[35] g. elidan, i. mcgraw, and d. koller,    residual belief propagation: informed
scheduling for asynchronous message passing,    in conference on uncertainty
in arti   cial intelligence (uai), 2006.

[36] p. f. felzenszwalb, r. b. girshick, d. mcallester, and d. ramanan,    object
detection with discriminatively trained part based models,    ieee transac-
tions on pattern analysis and machine intelligence, 2010.

[37] j. finkel, t. grenager, and c. d. manning,    incorporating non-local infor-
mation into information extraction systems by id150,    in annual
meeting of the association for computational linguistics (acl), 2005.

[38] j. r. finkel, a. kleeman, and c. d. manning,    e   cient, feature-based, con-
ditional random    eld parsing,    in annual meeting of the association for
computational linguistics (acl/hlt), pp. 959   967, 2008.

[39] k. ganchev, j. graca, j. gillenwater, and b. taskar,    posterior regulariza-
tion for structured latent variable models,    technical report ms-cis-09-16,
university of pennsylvania department of computer and information science,
2009.

[40] a. e. gelfand and a. f. m. smith,    sampling-based approaches to calculating
marginal densities,    journal of the american statistical association, vol. 85,
pp. 398   409, 1990.

[41] s. geman and d. geman,    stochastic relaxation, gibbs distributions, and the
bayesian restoration of images,    ieee transactions on pattern analysis and
machine intelligence, vol. 6, pp. 721   741, 1984.

[42] n. ghamrawi and a. mccallum,    collective multi-label classi   cation,    in

conference on information and knowledge management (cikm), 2005.

[43] a. globerson, t. koo, x. carreras, and m. collins,    exponentiated gradient
algorithms for log-linear id170,    in international conference
on machine learning (icml), 2007.

[44] j. goodman,    exponential priors for maximum id178 models,    in proceed-
ings of the human language technology conference/north american chapter
of the association for computational linguistics (hlt/naacl), 2004.

366 references

[45] j. graca, k. ganchev, b. taskar, and f. pereira,    posterior vs parameter
sparsity in latent variable models,    in advances in neural information pro-
cessing systems 22, (y. bengio, d. schuurmans, j. la   erty, c. k. i. williams,
and a. culotta, eds.), pp. 664   672, 2009.

[46] y. grandvalet and y. bengio,    semi-supervised learning by id178 mini-
mization,    in advances in neural information processing systems (nips),
2004.

[47] m. l. gregory and y. altun,    using conditional random    elds to predict pitch
accents in conversational speech,    in annual meeting of the association for
computational linguistics (acl), pp. 677   683, 2004.

[48] a. gunawardana, m. mahajan, a. acero, and j. c. platt,    hidden conditional
random    elds for phone classi   cation,    in international conference on speech
communication and technology, 2005.

[49] x. he, r. s. zemel, and m. a. carreira-perpi  ni  an,    multiscale conditional
random    elds for image labelling,    in conference on id161 and
pattern recognition (cvpr), 2004.

[50] g. e. hinton,    training products of experts by minimizing contrastive diver-

gence,    neural computation, vol. 14, pp. 1771   1800, 2002.

[51] l. hirschman, a. yeh, c. blaschke, and a. valencia,    overview of biocre-
ative: critical assessment of information extraction for biology,    bmc bioin-
formatics, vol. 6, no. suppl 1, no. suppl 1, 2005.

[52] f. jiao, s. wang, c.-h. lee, r. greiner, and d. schuurmans,    semi-supervised
conditional random    elds for improved sequence segmentation and labeling,   
in joint conference of the international committee on computational lin-
guistics and the association for computational linguistics (coling/acl),
2006.

[53] s. s. keerthi and s. sundararajan,    crf versus id166-struct for sequence

labeling,    technical report, yahoo! research, 2007.

[54] j. kiefer and j. wolfowitz,    stochastic estimation of the maximum of a regres-
sion function,    annals of mathematical statistics, vol. 23, pp. 462   466, 1952.
[55] j.-d. kim, t. ohta, y. tsuruoka, y. tateisi, and n. collier,    introduction to
the bio-entity recognition task at jnlpba,    in international joint workshop
on natural language processing in biomedicine and its applications, pp. 70   75,
association for computational linguistics, 2004.

[56] p. kohli, l. ladick`y, and p. h. s. torr,    robust higher order potentials
for enforcing label consistency,    international journal of id161,
vol. 82, no. 3, no. 3, pp. 302   324, 2009.

[57] d. koller and n. friedman, probabilistic id114: principles and

techniques. mit press, 2009.

[58] f. r. kschischang, b. j. frey, and h.-a. loeliger,    factor graphs and the
sum-product algorithm,    ieee transactions on id205, vol. 47,
no. 2, no. 2, pp. 498   519, 2001.

[59] t. kudo, k. yamamoto, and y. matsumoto,    applying conditional random
   elds to japanese morphological analysis,    in proceedings of the conference
on empirical methods in natural language processing (emnlp), 2004.

[60] a. kulesza and f. pereira,    structured learning with approximate id136,   

in advances in neural information processing systems, 2008.

references

367

[61] s. kumar and m. hebert,    discriminative    elds for modeling spatial depen-
dencies in natural images,    in advances in neural information processing
systems (nips), 2003.

[62] s. kumar and m. hebert,    discriminative random    elds,    international jour-

nal of id161, vol. 68, no. 2, no. 2, pp. 179   201, 2006.

[63] j. la   erty, a. mccallum, and f. pereira,    conditional random    elds: prob-
abilistic models for segmenting and labeling sequence data,    international
conference on machine learning (icml), 2001.

[64] j. langford, a. smola, and m. zinkevich,    slow learners are fast,    in advances
in neural information processing systems 22, (y. bengio, d. schuurmans,
j. la   erty, c. k. i. williams, and a. culotta, eds.), pp. 2331   2339, 2009.

[65] t. lavergne, o. capp  e, and f. yvon,    practical very large scale crfs,   
in annual meeting of the association for computational linguistics (acl),
pp. 504   513, 2010.

[66] y. le cun, l. bottou, g. b. orr, and k.-r. m  uller,    e   cient backprop,   
in neural networks, tricks of the trade, lecture notes in computer science
lncs 1524, springer verlag, 1998.

[67] y. lecun, l. bottou, y. bengio, and p. ha   ner,    gradient-based learning
applied to document recognition,    proceedings of the ieee, vol. 86, no. 11,
pp. 2278   2324, november 1998.

[68] y. lecun, s. chopra, r. hadsell, r. marc   aurelio, and f.-j. huang,    a
tutorial on energy-based learning,    in predicting structured data, (g. bakir,
t. hofman, b. sch  olkopf, a. smola, and b. taskar, eds.), mit press, 2007.
[69] s. z. li, markov random field modeling in image analysis. springer-verlag,

2001.

[70] w. li and a. mccallum,    a note on semi-supervised learning using markov

random    elds,    2004.

[71] p. liang, h. daum  e iii, and d. klein,    structure compilation: trading struc-
ture for features,    in international conference on machine learning (icml),
pp. 592   599, 2008.

[72] p. liang and m. i. jordan,    an asymptotic analysis of generative, discrim-
inative, and pseudolikelihood estimators,    in international conference on
machine learning (icml), pp. 584   591, 2008.

[73] p. liang, m. i. jordan, and d. klein,    learning from measurements in expo-
nential families,    in international conference on machine learning (icml),
2009.

[74] c.-j. lin, r. c.-h. weng, and s. keerthi,    trust region id77s for
large-scale id28,    in interational conference on machine learn-
ing (icml), 2007.

[75] b. g. lindsay,    composite likelihood methods,    contemporary mathematics,

pp. 221   239, 1988.

[76] y. liu, j. carbonell, p. weigele, and v. gopalakrishnan,    protein fold recog-
nition using segmentation conditional random    elds (scrfs),    journal of
computational biology, vol. 13, no. 2, no. 2, pp. 394   406, 2006.

[77] d. g. lowe,    object recognition from local scale-invariant features,    in inter-
national conference on id161 (iccv), vol. 2, pp. 1150   1157, 1999.

368 references

[78] d. j. lunn, a. thomas, n. best, and d. spiegelhalter,    winbugs     a
bayesian modelling framework: concepts, structure, and extensibility,    statis-
tics and computing, vol. 10, no. 4, no. 4, pp. 325   337, 2000.

[79] d. j. c. mackay, id205, id136, and learning algorithms.

cambridge university press, 2003.

[80] r. malouf,    a comparison of algorithms for maximum id178 parameter
estimation,    in conference on natural language learning (conll), (d. roth
and a. van den bosch, eds.), pp. 49   55, 2002.

[81] g. mann and a. mccallum,    generalized expectation criteria for semi-
supervised learning of conditional random    elds,    in proceedings of associ-
ation of computational linguistics, 2008.

[82] m. p. marcus, b. santorini, and m. a. marcinkiewicz,    building a large anno-
tated corpus of english: the id32,    computational linguistics,
vol. 19, no. 2, no. 2, pp. 313   330, 1993.

[83] a. mccallum,    e   ciently inducing features of conditional random    elds,    in

conference on uncertainty in ai (uai), 2003.

[84] a. mccallum, k. bellare, and f. pereira,    a conditional random    eld for
discriminatively-trained    nite-state string id153,    in conference on
uncertainty in ai (uai), 2005.

[85] a. mccallum, d. freitag, and f. pereira,    maximum id178 markov models
for information extraction and segmentation,    in international conference on
machine learning (icml), pp. 591   598, san francisco, ca, 2000.

[86] a. mccallum and w. li,    early results for id39 with
conditional random    elds, feature induction and web-enhanced lexicons,    in
seventh conference on natural language learning (conll), 2003.

[87] a. mccallum, k. schultz, and s. singh,    factorie: probabilistic program-
ming via imperatively de   ned factor graphs,    in advances in neural informa-
tion processing systems (nips), 2009.

[88] a. mccallum and b. wellner,    conditional models of identity uncertainty
with application to noun coreference,    in advances in neural information
processing systems 17, (l. k. saul, y. weiss, and l. bottou, eds.), pp. 905   
912, cambridge, ma: mit press, 2005.

[89] r. j. mceliece, d. j. c. mackay, and j.-f. cheng,    turbo decoding as an
instance of pearl   s    belief propagation    algorithm,    ieee journal on selected
areas in communications, vol. 16, no. 2, no. 2, pp. 140   152, 1998.

[90] s. miller, j. guinness, and a. zamanian,    name tagging with word clus-
ters and discriminative training,    in hlt-naacl 2004: main proceedings,
(d. marcu, s. dumais, and s. roukos, eds.), pp. 337   342, boston, mas-
sachusetts, usa: association for computational linguistics, may 2   may 7
2004.

[91] t. p. minka,    the ep energy function and minimization schemes,    technical

report, 2001.

[92] t. p. minka,    a comparsion of numerical optimizers for id28,   

technical report, 2003.

[93] t. p. minka,    discriminative models, not discriminative training,    technical

report msr-tr-2005-144, microsoft research, october 2005.

references

369

[94] t. p. minka,    divergence measures and message passing,    technical report

msr-tr-2005-173, microsoft research, 2005.

[95] i. murray,    advances in id115 methods,    phd thesis,

gatsby computational neuroscience unit, university college london, 2007.

[96] i. murray, z. ghahramani, and d. j. c. mackay,    mcmc for doubly-
intractable distributions,    in uncertainty in arti   cial intelligence (uai),
pp. 359   366, auai press, 2006.

[97] a. y. ng,    feature selection, l1 vs. l2 id173, and rotational invari-

ance,    in international conference on machine learning (icml), 2004.

[98] a. y. ng and m. i. jordan,    on discriminative vs. generative classi   ers:
a comparison of id28 and naive bayes,    in advances in neu-
ral information processing systems 14, (t. g. dietterich, s. becker, and
z. ghahramani, eds.), pp. 841   848, cambridge, ma: mit press, 2002.

[99] n. nguyen and y. guo,    comparisons of sequence labeling algorithms and
extensions,    in international conference on machine learning (icml), 2007.
[100] j. nocedal and s. j. wright, numerical optimization. new york: springer-

verlag, 1999.

[101] s. nowozin and c. h. lampert,    id170 and learning in com-
puter vision,    foundations and trends in computer graphics and vision,
vol. 6, no. 3-4, no. 3-4, 2011.

[102] c. pal, c. sutton, and a. mccallum,    sparse forward-backward using mini-
mum divergence beams for fast training of conditional random    elds,    in inter-
national conference on acoustics, speech, and signal processing (icassp),
2006.

[103] j. pearl, probabilistic reasoning in intelligent systems: networks of plausible

id136. morgan kaufmann, 1988.

[104] f. peng, f. feng, and a. mccallum,    chinese segmentation and new word
detection using conditional random    elds,    in international conference on
computational linguistics (coling), pp. 562   568, 2004.

[105] f. peng and a. mccallum,    accurate information extraction from research
papers using conditional random    elds,    in human language technology con-
ference and north american chapter of the association for computational
linguistics (hlt-naacl), 2004.

[106] d. pinto, a. mccallum, x. wei, and w. b. croft,    table extraction using con-
ditional random    elds,    in acm sigir conference on research and devel-
opment in information retrieval, 2003.

[107] y. qi, m. szummer, and t. p. minka,    bayesian conditional random    elds,   

in conference on arti   cial intelligence and statistics (aistats), 2005.

[108] y. qi, m. szummer, and t. p. minka,    diagram structure recognition by
bayesian conditional random    elds,    in international conference on computer
vision and pattern recognition, 2005.

[109] a. quattoni, m. collins, and t. darrell,    conditional random    elds for object
recognition,    in advances in neural information processing systems (nips),
pp. 1097   1104, 2005.

[110] a. quattoni, s. wang, l.-p. morency, m. collins, and t. darrell,    hidden-
state conditional random    elds,    ieee transactions on pattern analysis and
machine intelligence, 2007.

370 references

[111] l. r. rabiner,    a tutorial on id48 and selected applications
in id103,    proceedings of the ieee, vol. 77, no. 2, no. 2, pp. 257   
286, 1989.

[112] n. ratli   , j. a. bagnell, and m. zinkevich,    maximum margin planning,    in

international conference on machine learning, july 2006.

[113] m. richardson and p. domingos,    markov logic networks,    machine learning,

vol. 62, no. 1   2, no. 1   2, pp. 107   136, 2006.

[114] s. riezler, t. king, r. kaplan, r. crouch, j. t. maxwell iii, and m. john-
son,    parsing the wall street journal using a lexical-functional grammar and
discriminative estimation techniques,    in proceedings of the annual meeting
of the association for computational linguistics, 2002.

[115] h. robbins and s. monro,    a stochastic approximation method,    annals of

mathematical statistics, vol. 22, pp. 400   407, 1951.

[116] c. robert and g. casella, monte carlo statistical methods.

springer,

2004.

[117] d. rosenberg, d. klein, and b. taskar,    mixture-of-parents maximum id178
markov models,    in conference on uncertainty in arti   cial intelligence
(uai), 2007.

[118] d. roth and w. yih,    integer id135 id136 for conditional
random    elds,    in international conference on machine learning (icml),
pp. 737   744, 2005.

[119] c. rother, v. kolmogorov, and a. blake,    grabcut: interactive foreground
extraction using iterated graph cuts,    acm transactions on graphics (sig-
graph), vol. 23, no. 3, no. 3, pp. 309   314, 2004.

[120] e. f. t. k. sang and s. buchholz,    introduction to the conll-2000 shared
task: chunking,    in proceedings of conll-2000 and lll-2000, 2000. see
http://lcg-www.uia.ac.be/   erikt/research/np-chunking.html.

[121] e. f. t. k. sang and f. d. meulder,    introduction to the conll-2003
shared task: language-independent id39,    in proceed-
ings of conll-2003, (w. daelemans and m. osborne, eds.), pp. 142   147,
edmonton, canada, 2003.

[122] s. sarawagi and w. w. cohen,    semi-markov conditional random    elds for
information extraction,    in advances in neural information processing sys-
tems 17, (l. k. saul, y. weiss, and l. bottou, eds.), pp. 1185   1192, cam-
bridge, ma: mit press, 2005.

[123] k. sato and y. sakakibara,    rna secondary structural alignment with

conditional random    elds,    bioinformatics, vol. 21, pp. ii237   242, 2005.

[124] b. settles,    abner: an open source tool for automatically tagging genes, pro-
teins, and other entity names in text,    bioinformatics, vol. 21, no. 14, no. 14,
pp. 3191   3192, 2005.

[125] f. sha and f. pereira,    id66 with conditional random    elds,    in
conference on human language technology and north american association
for computational linguistics (hlt-naacl), pp. 213   220, 2003.

[126] s. shalev-shwartz, y. singer, and n. srebro,    pegasos: primal estimated sub-
gradient solver for id166,    in international conference on machine learning
(icml), 2007.

references

371

[127] j. shotton, j. winn, c. rother, and a. criminisi,    textonboost: joint appear-
ance, shape and context modeling for mulit-class object recognition and seg-
mentation,    in european conference on id161 (eccv), 2006.

[128] p. singla and p. domingos,    discriminative training of markov logic net-
works,    in proceedings of the national conference on arti   cial intelligence,
pp. 868   873, pittsburgh, pa, 2005.

[129] f. k. soong and e.-f. huang,    a tree-trellis based fast search for    nding the
n-best sentence hypotheses in continuous id103,    in international
conference on acoustics, speech, and signal processing (icassp), 1991.

[130] d. h. stern, t. graepel, and d. j. c. mackay,    modelling uncertainty in the
game of go,    in advances in neural information processing systems 17, (l. k.
saul, y. weiss, and l. bottou, eds.), pp. 1353   1360, cambridge, ma: mit
press, 2005.

[131] i. sutskever and t. tieleman,    on the convergence properties of contrastive
divergence,    in conference on arti   cial intelligence and statistics (ais-
tats), 2010.

[132] c. sutton,    e   cient training methods for id49,    phd

thesis, university of massachusetts, 2008.

[133] c. sutton and a. mccallum,    collective segmentation and labeling of distant
entities in information extraction,    in icml workshop on statistical rela-
tional learning and its connections to other fields, 2004.

[134] c. sutton and a. mccallum,    piecewise training of undirected models,    in

conference on uncertainty in arti   cial intelligence (uai), 2005.

[135] c. sutton and a. mccallum,    improved dynamic schedules for belief propa-
gation,    in conference on uncertainty in arti   cial intelligence (uai), 2007.
[136] c. sutton and a. mccallum,    an introduction to conditional random    elds
for relational learning,    in introduction to statistical relational learning,
(l. getoor and b. taskar, eds.), mit press, 2007.

[137] c. sutton and a. mccallum,    piecewise training for id170,   

machine learning, vol. 77, no. 2   3, no. 2   3, pp. 165   194, 2009.

[138] c. sutton, a. mccallum, and k. rohanimanesh,    dynamic conditional
random    elds: factorized probabilistic models for labeling and segmenting
sequence data,    journal of machine learning research, vol. 8, pp. 693   723,
march 2007.

[139] c. sutton and t. minka,    local training and belief propagation,    technical

report tr-2006-121, microsoft research, 2006.

[140] c. sutton, k. rohanimanesh, and a. mccallum,    dynamic conditional
random    elds: factorized probabilistic models for labeling and segmenting
sequence data,    in international conference on machine learning (icml),
2004.

[141] c. sutton, m. sindelar, and a. mccallum,    reducing weight undertraining
in structured discriminative learning,    in conference on human language
technology and north american association for computational linguistics
(hlt-naacl), 2006.

372 references

[142] b. taskar, p. abbeel, and d. koller,    discriminative probabilistic models for
relational data,    in conference on uncertainty in arti   cial intelligence (uai),
2002.

[143] b. taskar, c. guestrin, and d. koller,    max-margin markov networks,    in
advances in neural information processing systems 16, (s. thrun, l. saul,
and b. sch  olkopf, eds.), cambridge, ma: mit press, 2004.

[144] b. taskar, d. klein, m. collins, d. koller, and c. manning,    max-margin
parsing,    in empirical methods in natural language processing (emnlp04),
2004.

[145] b. taskar, s. lacoste-julien, and d. klein,    a discriminative matching
approach to word alignment,    in conference on human language technol-
ogy and empirical methods in natural language processing (hlt-emnlp),
pp. 73   80, 2005.

[146] k. toutanova, d. klein, c. d. manning, and y. singer,    feature-rich part-of-

speech tagging with a cyclic dependency network,    in hlt-naacl, 2003.

[147] i. tsochantaridis, t. hofmann, t. joachims, and y. altun,    support vector
machine learning for interdependent and structured output spaces,    in inter-
ational conference on machine learning (icml), icml    04, 2004.

[148] p. viola and m. narasimhan,    learning to extract information from semi-
structured text using a discriminative id18,    in proceedings
of the acm sigir, 2005.

[149] s. v. n. vishwanathan, n. n. schraudolph, m. w. schmidt, and k. mur-
phy,    accelerated training of conditional random    elds with stochastic
meta-descent,    in international conference on machine learning (icml),
pp. 969   976, 2006.

[150] m. j. wainwright and m. i. jordan,    id114, exponential fami-
lies, and variational id136,    foundations and trends in machine learning,
vol. 1, no. 1-2, no. 1-2, pp. 1   305, 2008.

[151] m. j. wainwright,    estimating the wrong markov random    eld: bene   ts in the
computation-limited setting,    in advances in neural information processing
systems 18, (y. weiss, b. sch  olkopf, and j. platt, eds.), cambridge, ma: mit
press, 2006.

[152] m. j. wainwright, t. jaakkola, and a. s. willsky,    tree-based reparameteri-
zation framework for analysis of sum-product and related algorithms,    ieee
transactions on id205, vol. 45, no. 9, no. 9, pp. 1120   1146, 2003.
[153] h. wallach,    e   cient training of conditional random    elds,    m.sc. thesis,

university of edinburgh, 2002.

[154] m. welling and s. parise,    bayesian random    elds: the bethe-laplace approx-

imation,    in uncertainty in arti   cial intelligence (uai), 2006.

[155] m. wick, k. rohanimanesh, a. culotta, and a. mccallum,    samplerank:
learning preferences from atomic gradients,    in neural information process-
ing systems (nips) workshop on advances in ranking, 2009.

[156] m. wick, k. rohanimanesh, a. mccallum, and a. doan,    a discriminative
approach to ontology alignment,    in international workshop on new trends
in information integration (ntii), 2008.

references

373

[157] j. s. yedidia, w. t. freeman, and y. weiss,    constructing free energy approx-
imations and generalized belief propagation algorithms,    technical report
tr2004-040, mitsubishi electric research laboratories, 2004.

[158] j. s. yedidia, w. t. freeman, and y. weiss,    constructing free-energy approx-
imations and generalized belief propagation algorithms,    ieee transactions
on id205, vol. 51, no. 7, pp. 2282   2312, july 2005.

[159] c.-n. yu and t. joachims,    learning structural id166s with latent variables,   

in international conference on machine learning (icml), 2009.

[160] j. yu, s. v. n. vishwanathan, s. g  uunter, and n. n. schraudolph,    a quasi-
newton approach to nonsmooth id76 problems in machine
learning,    journal of machine learning research, vol. 11, pp. 1145   1200,
march 2010.

[161] y. zhang and c. sutton,    quasi-newton id115,    in

advances in neural information processing systems (nips), 2011.

