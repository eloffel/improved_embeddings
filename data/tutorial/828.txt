deep	
   learning	
   for	
   nlp	
   	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   (without	
   magic)	
   

richard	
   socher	
   and	
   christopher	
   manning	
   

stanford	
   university	
   
naacl	
   2013,	
   atlanta	
   

h0p://nlp.stanford.edu/courses/naacl2013/	
   

*with	
   a	
   big	
   thank	
   you	
   to	
   yoshua	
   bengio,	
   with	
   whom	
   we	
   

pargcipated	
   in	
   the	
   previous	
   acl	
   2012	
   version	
   of	
   this	
   tutorial	
   

deep learning 

most	
   current	
   machine	
   learning	
   works	
   
well	
   because	
   of	
   human-     designed	
   
representagons	
   and	
   input	
   features	
   

machine	
   learning	
   becomes	
   just	
   opgmizing	
   
weights	
   to	
   best	
   make	
   a	
      nal	
   predicgon	
   

ner	
   
	
   
	
   
	
   
srl 	
   

	
   

	
   

	
   

	
   id138	
   

	
   	
   	
   	
   parser	
   	
   

representagon	
   learning	
   a0empts	
   to	
   	
   
automagcally	
   learn	
   good	
   features	
   or	
   representagons	
   

deep	
   learning	
   algorithms	
   a0empt	
   to	
   learn	
   mulgple	
   levels	
   of	
   
representagon	
   of	
   increasing	
   complexity/abstracgon	
   

2	
   

a deep architecture 

mainly,	
   work	
   has	
   explored	
   deep	
   belief	
   networks	
   (dbns),	
   markov	
   
random	
   fields	
   with	
   mulgple	
   layers,	
   and	
   various	
   types	
   of	
   
mulgple-     layer	
   neural	
   networks	
   

output	
   layer	
   

here	
   predicgng	
   a	
   supervised	
   target	
   

	
   

hidden	
   layers	
   

these	
   learn	
   more	
   abstract	
   	
   
representagons	
   as	
   you	
   head	
   up	
   

input	
   layer	
   

3	
   

raw	
   sensory	
   inputs	
   (roughly)	
   

part	
   1.1:	
   the	
   basics	
   

five reasons to explore 
deep learning 

4	
   

#1 learning representations 

handcra^ing	
   features	
   is	
   gme-     consuming	
   

the	
   features	
   are	
   o^en	
   both	
   over-     speci   ed	
   and	
   incomplete	
   

the	
   work	
   has	
   to	
   be	
   done	
   again	
   for	
   each	
   task/domain/   	
   

we	
   must	
   move	
   beyond	
   handcra^ed	
   features	
   and	
   simple	
   ml	
   

humans	
   develop	
   representagons	
   for	
   learning	
   and	
   reasoning	
   

	
   our	
   computers	
   should	
   do	
   the	
   same	
   

deep	
   learning	
   provides	
   a	
   way	
   of	
   doing	
   this	
   

5	
   

#2 the need for distributed 

representations 

current	
   nlp	
   systems	
   are	
   incredibly	
   fragile	
   because	
   of	
   
their	
   atomic	
   symbol	
   representagons	
   

6	
   

crazy	
   senten@al	
   
complement,	
   such	
   as	
   for	
   
   likes	
   [(being)	
   crazy]   	
   

#2 the need for distributional & 

distributed representations 

learned	
   word	
   representagons	
   help	
   enormously	
   in	
   nlp	
   

they	
   provide	
   a	
   powerful	
   similarity	
   model	
   for	
   words	
   	
   

distribugonal	
   similarity	
   based	
   word	
   clusters	
   greatly	
   help	
   most	
   
applicagons	
   

+1.4%	
   f1	
   dependency	
   parsing	
   15.2%	
   error	
   reducgon	
   	
   (koo	
   &	
   
collins	
   2008,	
   brown	
   id91)	
   
+3.4%	
   f1	
   named	
   engty	
   recognigon	
   23.7%	
   error	
   reducgon	
   
(stanford	
   ner,	
   exchange	
   id91)	
   

distributed	
   representagons	
   can	
   do	
   even	
   be0er	
   by	
   represengng	
   
more	
   dimensions	
   of	
   similarity	
   

7	
   

	
   

#2 the need for distributed 

representations 

id91	
   

mulg-     	
   

id91	
   

c1	
   

c2	
   

c3	
   

input	
   

learning	
   features	
   that	
   are	
   not	
   mutually	
   exclusive	
   can	
   be	
   exponengally	
   
more	
   e   cient	
   than	
   nearest-     neighbor-     like	
   or	
   id91-     like	
   models	
   

8	
   

distributed representations deal with 
the curse of dimensionality 

generalizing	
   locally	
   (e.g.,	
   nearest	
   
neighbors)	
   requires	
   representagve	
   
examples	
   for	
   all	
   relevant	
   variagons!	
   
classic	
   solugons:	
   
       manual	
   feature	
   design	
   
       assuming	
   a	
   smooth	
   target	
   

funcgon	
   (e.g.,	
   linear	
   models)	
   

       kernel	
   methods	
   (linear	
   in	
   terms	
   
of	
   kernel	
   based	
   on	
   data	
   points)	
   
neural	
   networks	
   parameterize	
   and	
   
learn	
   a	
      similarity   	
   kernel	
   
	
   
	
   
	
   

9	
   

#3 unsupervised feature and 

weight learning  

today,	
   most	
   pracgcal,	
   good	
   nlp&	
   ml	
   methods	
   require	
   
labeled	
   training	
   data	
   (i.e.,	
   supervised	
   learning)	
   

but	
   almost	
   all	
   data	
   is	
   unlabeled	
   

most	
   informagon	
   must	
   be	
   acquired	
   unsupervised	
   

fortunately,	
   a	
   good	
   model	
   of	
   observed	
   data	
   can	
   really	
   help	
   you	
   
learn	
   classi   cagon	
   decisions	
   

10	
   

#4 learning multiple levels of 

representation 

biologically	
   inspired	
   learning	
   

the	
   cortex	
   seems	
   to	
   have	
   a	
   generic	
   
learning	
   algorithm	
   	
   
the	
   brain	
   has	
   a	
   deep	
   architecture	
   

task	
   1	
   output	
   

task	
   2	
   output	
   

task	
   3	
   output	
   

we	
   need	
   good	
   intermediate	
   representagons	
   
that	
   can	
   be	
   shared	
   across	
   tasks	
   
mulgple	
   levels	
   of	
   latent	
   variables	
   allow	
   
combinatorial	
   sharing	
   of	
   stagsgcal	
   strength	
   

insu   cient	
   model	
   depth	
   can	
   be	
   
exponengally	
   ine   cient	
   

11	
   
	
   

linguisgc	
   input	
   

#4 learning multiple levels 

of representation 

[lee	
   et	
   al.	
   icml	
   2009;	
   lee	
   et	
   al.	
   nips	
   2009]	
   
successive	
   model	
   layers	
   learn	
   deeper	
   intermediate	
   representagons	
   

	
   

12	
   

layer	
   3	
   

linguisgc	
   representagons	
   

high-     level	
   

layer	
   2	
   

layer	
   1	
   

handling the recursivity of human 
language  

human	
   sentences	
   are	
   composed	
   
from	
   words	
   and	
   phrases	
   

we	
   need	
   composigonality	
   in	
   our	
   
ml	
   models	
   	
   

recursion:	
   the	
   same	
   operator	
   
(same	
   parameters)	
   is	
   applied	
   
repeatedly	
   on	
   di   erent	
   
components	
   

zt   1	
   

zt	
   

zt+1	
   

xt   1	
   

xt	
   

xt+1	
   

s

vp

vp

np

np

a	
   small	
   crowd	
   
quietly	
   enters	
   
the	
   historic	
   

church
semantic	
   	
   
representations

a	
   small	
   
crowd

quietly	
   
enters

det.

adj.

np

n.

the

historic

church

13	
   

#5 why now? 

despite	
   prior	
   invesggagon	
   and	
   understanding	
   of	
   many	
   of	
   the	
   
algorithmic	
   techniques	
      	
   
before	
   2006	
   training	
   deep	
   architectures	
   was	
   unsuccessful	
   l   	
   

what	
   has	
   changed?	
   

       new	
   methods	
   for	
   unsupervised	
   pre-     training	
   have	
   been	
   

developed	
   (restricted	
   boltzmann	
   machines	
   =	
   rbms,	
   
autoencoders,	
   contrasgve	
   esgmagon,	
   etc.)	
   

       more	
   e   cient	
   parameter	
   esgmagon	
   methods	
   
       be0er	
   understanding	
   of	
   model	
   regularizagon	
   

deep learning models have already 
achieved impressive results for hlt 

neural	
   language	
   model	
   
[mikolov	
   et	
   al.	
   interspeech	
   2011]	
   
	
   
msr	
   mavis	
   speech	
   system	
   
[dahl	
   et	
   al.	
   2012;	
   seide	
   et	
   al.	
   	
   2011;	
   
following	
   mohamed	
   et	
   al.	
   2011]	
   

	
   

   the	
   algorithms	
   represent	
   the	
      rst	
   gme	
   a	
   
company	
   has	
   released	
   a	
   deep-     neural-     
networks	
   (dnn)-     based	
   speech-     recognigon	
   
algorithm	
   in	
   a	
   commercial	
   product.   	
   
15	
   
	
   

eval	
   wer	
   
model	
   \	
   wsj	
   asr	
   task	
   
17.2	
   
kn5	
   baseline	
   
16.9	
   
discriminagve	
   lm	
   
recurrent	
   nn	
   combinagon	
    14.4	
   

acous@c	
   model	
   &	
   
training	
   
gmm	
   40-     mix,	
   
bmmi,	
   swb	
   309h	
   

recog	
   
\	
   wer	
   
1-     pass	
   
   adapt	
   

rt03s	
   
fsh	
   
27.4	
   

hub5	
   
swb	
   
23.6	
   

dbn-     dnn	
   7	
   layer	
   
x	
   2048,	
   swb	
   309h	
   

1-     pass	
   
   adapt	
   

18.5	
   
(   33%)	
   

16.1	
   
(   32%)	
   

gmm	
   72-     mix,	
   
bmmi,	
   fsh	
   2000h	
   

k-     pass	
   
+adapt	
   

18.6	
   

17.1	
   

deep learn models have interesting 
performance characteristics 

deep	
   learning	
   models	
   can	
   now	
   be	
   very	
   fast	
   in	
   some	
   circumstances	
   

       senna	
   [collobert	
   et	
   al.	
   2011]	
   can	
   do	
   pos	
   or	
   ner	
   faster	
   than	
   
other	
   sota	
   taggers	
   (16x	
   to	
   122x),	
   using	
   25x	
   less	
   memory	
   

       wsj	
   pos	
   97.29%	
   acc;	
   conll	
   ner	
   89.59%	
   f1;	
   conll	
   chunking	
   94.32%	
   f1	
   

changes	
   in	
   compugng	
   technology	
   favor	
   deep	
   learning	
   

       in	
   nlp,	
   speed	
   has	
   tradigonally	
   come	
   from	
   exploigng	
   sparsity	
   
       but	
   with	
   modern	
   machines,	
   branches	
   and	
   widely	
   spaced	
   
memory	
   accesses	
   are	
   costly	
   
       uniform	
   parallel	
   operagons	
   on	
   dense	
   vectors	
   are	
   faster	
   
these	
   trends	
   are	
   even	
   stronger	
   with	
   mulg-     core	
   cpus	
   and	
   gpus	
   

	
   16	
   

17	
   

outline of the tutorial 
1.    the	
   basics	
   

1.    mogvagons	
   
2.    from	
   logisgc	
   regression	
   to	
   neural	
   networks	
   
3.    word	
   representagons	
   
4.    unsupervised	
   word	
   vector	
   learning	
   
5.    backpropagagon	
   training	
   
6.    learning	
   word-     level	
   classi   ers:	
   pos	
   and	
   ner	
   
7.    sharing	
   stagsgcal	
   strength	
   

2.    recursive	
   neural	
   networks	
   
3.    applicagons,	
   discussion,	
   and	
   resources	
   

18	
   

outline of the tutorial 
1.    the	
   basics	
   
2.    recursive	
   neural	
   networks	
   

1.    mogvagon	
   
2.    recursive	
   neural	
   networks	
   for	
   parsing	
   	
   
3.    opgmizagon	
   and	
   backpropagagon	
   through	
   structure	
   
4.    composigonal	
   vector	
   grammars:
5.    recursive	
   autoencoders:	
   
6.    matrix-     vector	
   id56s:	
    	
   
7.    recursive	
   neural	
   tensor	
   networks:	
    	
   sengment	
   analysis	
   

	
   parsing	
   
	
   paraphrase	
   detecgon	
   
	
   relagon	
   classi   cagon	
   

	
   
	
   

3.    applicagons,	
   discussion,	
   and	
   resources	
   

19	
   

outline of the tutorial 
1.    the	
   basics	
   
2.    recursive	
   neural	
   networks	
   
3.    applicagons,	
   discussion,	
   and	
   resources	
   

1.    assorted	
   speech	
   and	
   nlp	
   applicagons	
   
2.    deep	
   learning:	
   general	
   strategy	
   and	
   tricks	
   
3.    resources	
   (readings,	
   code,	
      )	
   
4.    discussion	
   

20	
   

part	
   1.2:	
   the	
   basics	
   

from id28 to 
neural nets 

21	
   

demystifying neural networks 

neural	
   networks	
   come	
   with	
   
their	
   own	
   terminological	
   
baggage	
   	
   

   	
   just	
   like	
   id166s	
   

	
   

but	
   if	
   you	
   understand	
   how	
   
logisgc	
   regression	
   or	
   maxent	
   
models	
   work	
   

then	
   you	
   already	
   understand	
   the	
   
operagon	
   of	
   a	
   basic	
   neural	
   
network	
   neuron!	
   

22	
   

a	
   single	
   neuron	
   

a	
   computagonal	
   unit	
   with	
   n	
   (3)	
   inputs	
   

and	
   1	
   output	
   

and	
   parameters	
   w,	
   b	
   

inputs	
   

acgvagon	
   
funcgon	
   

output	
   

bias	
   unit	
   corresponds	
   to	
   intercept	
   term	
   

from maxent classifiers to neural 
networks 

in	
   nlp,	
   a	
   maxent	
   classi   er	
   is	
   normally	
   wri0en	
   as:	
   

p(c | d,  ) =

exp   i fi(c,d)

i   
exp   i fi( "c,d)

i   

"c    c   

supervised	
   learning	
   gives	
   us	
   a	
   distribugon	
   for	
   datum	
   d	
   over	
   classes	
   in	
   c	
   
	
   
vector	
   form:	
   
	
   
such	
   a	
   classi   er	
   is	
   used	
   as-     is	
   in	
   a	
   neural	
   network	
   (   a	
   so^max	
   layer   )	
   

e  t f (c,d)
e  t f ( !c ,d)
!c   

p(c | d,  ) =

       o^en	
   as	
   the	
   top	
   layer:	
   	
   j	
   =	
   so^max(       x)	
   

but	
   for	
   now	
   we   ll	
   derive	
   a	
   two-     class	
   logisgc	
   model	
   for	
   one	
   neuron	
   
23	
   

from maxent classifiers to neural 
networks 

vector	
   form:	
   
	
   
make	
   two	
   class:	
   
	
    p(c1 | d,  ) =

p(c | d,  ) =

e  t f (c,d)
e  t f ( !c ,d)
!c   
e  t f (c1,d)

e  t f (c1,d) +e  t f (c2,d) =

e  t f (c1,d) +e  t f (c2,d)    

e  t f (c1,d)

e     t f (c1,d)
e     t f (c1,d)

1

=

1+e  t[ f (c2,d)    f (c1,d)] =

1

1+e     tx
= f(  tx)

for x = f (c1,d)     f (c2,d)

for	
   f(z)	
   =	
   1/(1	
   +	
   exp(   z)),	
   the	
   logisgc	
   funcgon	
      	
   a	
   sigmoid	
   non-     linearity.	
   

24	
   

this is exactly what a neuron 
computes 

hw,b(x) = f (wtx +b)
f (z) =

1
1+e   z

b:	
   we	
   can	
   have	
   an	
      always	
   on   	
   
feature,	
   which	
   gives	
   a	
   class	
   prior,	
   
or	
   separate	
   it	
   out,	
   as	
   a	
   bias	
   term	
   

25	
   

w,	
   b	
   are	
   the	
   parameters	
   of	
   this	
   neuron	
   

i.e.,	
   this	
   logisgc	
   regression	
   model	
   

a neural network = running several 
id28s at the same time 

if	
   we	
   feed	
   a	
   vector	
   of	
   inputs	
   through	
   a	
   bunch	
   of	
   logisgc	
   regression	
   
funcgons,	
   then	
   we	
   get	
   a	
   vector	
   of	
   outputs	
      	
   

but	
   we	
   don   t	
   have	
   to	
   decide	
   
ahead	
   of	
   <me	
   what	
   variables	
   
these	
   logis<c	
   regressions	
   are	
   
trying	
   to	
   predict!	
   

26	
   

a neural network = running several 
id28s at the same time 

   	
   which	
   we	
   can	
   feed	
   into	
   another	
   logisgc	
   regression	
   funcgon	
   

it	
   is	
   the	
   training	
   
criterion	
   that	
   will	
   direct	
   
what	
   the	
   intermediate	
   
hidden	
   variables	
   should	
   
be,	
   so	
   as	
   to	
   do	
   a	
   good	
   
job	
   at	
   predic<ng	
   the	
   
targets	
   for	
   the	
   next	
   
layer,	
   etc.	
   

27	
   

a neural network = running several 
id28s at the same time 

before	
   we	
   know	
   it,	
   we	
   have	
   a	
   mulglayer	
   neural	
   network   .	
   

28	
   

matrix notation for a layer 

we	
   have	
   	
   

	
   

	
   

a1 = f (w11x1 +w12x2 +w13x3 +b1)
a2 = f (w21x1 +w22x2 +w23x3 +b2)
etc.

in	
   matrix	
   notagon	
   

	
   

	
   

z =wx +b
a = f (z)

where	
   f	
   is	
   applied	
   element-     wise:	
   

f ([z1,z2,z3]) =[ f (z1), f (z2), f (z3)]

	
   

29	
   

a1	
   

a2	
   

a3	
   

w12	
   

b3	
   

how do we train the weights w? 
       for	
   a	
   single	
   supervised	
   layer,	
   we	
   train	
   just	
   like	
   a	
   maxent	
   model	
      	
   

we	
   calculate	
   and	
   use	
   error	
   derivagves	
   (gradients)	
   to	
   improve	
   
       online	
   learning:	
   stochasgc	
   gradient	
   descent	
   (sgd)	
   

       or	
   improved	
   versions	
   like	
   adagrad	
   (duchi,	
   hazan,	
   &	
   singer	
   2010)	
   

       batch	
   learning:	
   conjugate	
   gradient	
   or	
   l-     bfgs	
   

	
   

       a	
   mulglayer	
   net	
   could	
   be	
   more	
   complex	
   because	
   the	
   internal	
   

(   hidden   )	
   logisgc	
   units	
   make	
   the	
   funcgon	
   non-     convex	
      	
   just	
   as	
   
for	
   hidden	
   crfs	
   	
   	
   [qua0oni	
   et	
   al.	
   2005,	
   gunawardana	
   et	
   al.	
   2005]	
   
       but	
   we	
   can	
   use	
   the	
   same	
   ideas	
   and	
   techniques	
   	
   

       just	
   without	
   guarantees	
      	
   

       we	
      backpropagate   	
   error	
   derivagves	
   through	
   the	
   model	
   

30	
   

non-linearities: why they   re needed 
       for	
   logisgc	
   regression:	
   map	
   to	
   probabiliges	
   
       here:	
   funcgon	
   approximagon,	
   	
   
e.g.,	
   regression	
   or	
   classi   cagon	
   
       without	
   non-     lineariges,	
   deep	
   neural	
   networks	
   
can   t	
   do	
   anything	
   more	
   than	
   a	
   linear	
   transform	
   
       extra	
   layers	
   could	
   just	
   be	
   compiled	
   down	
   into	
   

a	
   single	
   linear	
   transform	
   

       probabilisgc	
   interpretagon	
   unnecessary	
   except	
   in	
   

the	
   boltzmann	
   machine/graphical	
   models	
   
       people	
   o^en	
   use	
   other	
   non-     lineariges,	
   such	
   as	
   

tanh,	
   as	
   we   ll	
   discuss	
   in	
   part	
   3	
   

31	
   

summary 
knowing the meaning of words! 

input	
   layer	
   =	
   input	
   training/test	
   vector	
   

you	
   now	
   understand	
   the	
   basics	
   and	
   the	
   relagon	
   to	
   other	
   models	
   
       neuron	
   =	
   logisgc	
   regression	
   or	
   similar	
   funcgon	
   
      
       bias	
   unit	
   =	
   intercept	
   term/always	
   on	
   feature	
   
       acgvagon	
   =	
   response	
   
       acgvagon	
   funcgon	
   is	
   a	
   logisgc	
   (or	
   similar	
      sigmoid   	
   nonlinearity)	
   
       backpropagagon	
   =	
   running	
   stochasgc	
   gradient	
   descent	
   backward	
   

layer-     by-     layer	
   in	
   a	
   mulglayer	
   network	
   

       weight	
   decay	
   =	
   regularizagon	
   /	
   bayesian	
   prior	
   

32	
   

effective deep learning became possible 
through unsupervised pre-training 

[erhan	
   et	
   al.,	
   jmlr	
   2010]	
   
	
   

(with	
   rbms	
   and	
   denoising	
   auto-     encoders)	
   

purely	
   supervised	
   neural	
   net	
   

with	
   unsupervised	
   pre-     training	
   

33	
   

0   9	
   handwri0en	
   digit	
   recognigon	
   error	
   rate	
   (mnist	
   data)	
   

part	
   1.3:	
   the	
   basics	
   

word representations 

34	
   

the standard word representation 
the	
   vast	
   majority	
   of	
   rule-     based	
   and	
   stagsgcal	
   nlp	
   work	
   regards	
   
words	
   as	
   atomic	
   symbols:	
   hotel, conference, walk 
in	
   vector	
   space	
   terms,	
   this	
   is	
   a	
   vector	
   with	
   one	
   1	
   and	
   a	
   lot	
   of	
   zeroes	
   

[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] 

dimensionality:	
   20k	
   (speech)	
      	
   50k	
   (ptb)	
      	
   500k	
   (big	
   vocab)	
      	
   13m	
   (google	
   1t)	
   

we	
   call	
   this	
   a	
      one-     hot   	
   representagon.	
   its	
   problem:	
   
  motel [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]  and 
  hotel  [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]  =  0	
   

35	
   

distributional similarity based 
representations 

you	
   can	
   get	
   a	
   lot	
   of	
   value	
   by	
   represengng	
   a	
   word	
   by	
   
means	
   of	
   its	
   neighbors	
   

   you	
   shall	
   know	
   a	
   word	
   by	
   the	
   company	
   it	
   keeps   	
   	
   

(j.	
   r.	
   firth	
   1957:	
   11)	
   

one	
   of	
   the	
   most	
   successful	
   ideas	
   of	
   modern	
   stagsgcal	
   nlp	
   
government debt problems turning into banking crises as has happened in 
         saying that europe needs unified banking regulation to replace the hodgepodge 

     	
   these	
   words	
   will	
   represent	
   banking	
        	
   

	
   

36	
   

you	
   can	
   vary	
   whether	
   you	
   use	
   local	
   or	
   large	
   context	
   
to	
   get	
   a	
   more	
   syntacgc	
   or	
   semangc	
   id91	
   

class-based (hard) and soft 
id91 word representations 

class	
   based	
   models	
   learn	
   word	
   classes	
   of	
   similar	
   words	
   based	
   on	
   
distribugonal	
   informagon	
   (	
   ~	
   class	
   id48)	
   
       brown	
   id91	
   (brown	
   et	
   al.	
   1992)	
   
       exchange	
   id91	
   (margn	
   et	
   al.	
   1998,	
   clark	
   2003)	
   
       desparsi   cagon	
   and	
   great	
   example	
   of	
   unsupervised	
   pre-     training	
   

so^	
   id91	
   models	
   learn	
   for	
   each	
   cluster/topic	
   a	
   distribugon	
   
over	
   words	
   of	
   how	
   likely	
   that	
   word	
   is	
   in	
   each	
   cluster	
   
       latent	
   semangc	
   analysis	
   (lsa/lsi),	
   random	
   projecgons	
   
       latent	
   dirichlet	
   analysis	
   (lda),	
   id48	
   id91	
   
	
   37	
   

neural id27s  
as a distributed representation 

similar	
   idea	
   

combine	
   vector	
   space	
   
semangcs	
   with	
   the	
   predicgon	
   of	
   
probabilisgc	
   models	
   (bengio	
   et	
   
al.	
   2003,	
   collobert	
   &	
   weston	
   
2008,	
   turian	
   et	
   al.	
   2010)	
   

in	
   all	
   of	
   these	
   approaches,	
   
including	
   deep	
   learning	
   models,	
   
a	
   word	
   is	
   represented	
   as	
   a	
   
dense	
   vector	
   

38	
   

	
   
	
   

	
   

	
   

linguis<cs	
   	
   =	
   

0.286	
   
0.792	
   
   0.177	
   
   0.107	
   
0.109	
   
   0.542	
   
0.349	
   
0.271	
   

neural id27s - 
visualization 

39	
   

stunning new result at this conference! 
mikolov, yih & zweig (naacl 2013) 

these	
   representagons	
   are	
   way	
   be0er	
   at	
   encoding	
   dimensions	
   of	
   
similarity	
   than	
   we	
   realized!	
   
       analogies	
   tesgng	
   dimensions	
   of	
   similarity	
   can	
   be	
   solved	
   quite	
   
well	
   just	
   by	
   doing	
   vector	
   subtracgon	
   in	
   the	
   embedding	
   space	
   
syntacgcally	
   
       xapple	
      	
   xapples	
      	
   xcar	
      	
   xcars	
      	
   xfamily	
      	
   xfamilies	
   	
   
       similarly	
   for	
   verb	
   and	
   adjecgve	
   morphological	
   forms	
   
semangcally	
   (semeval	
   2012	
   task	
   2)	
   
       xshirt	
      	
   xclothing	
      	
   xchair	
      	
   xfurniture	
   	
   

40	
   

stunning new result at this conference! 
mikolov, yih & zweig (naacl 2013) 

method	
   
lsa	
   320	
   dim	
   
id56	
   80	
   dim	
   
id56	
   320	
   dim	
   
id56	
   1600	
   dim	
   
method	
   
utd-     nb	
   (rink	
   &	
   h.	
   2012)	
   
lsa	
   640	
   
id56	
   80	
   
id56	
   1600	
   

syntax	
   %	
   correct	
   
16.5	
   [best]	
   
16.2	
   
28.5	
   
39.6	
   
seman@cs	
   spearm	
     	
   
0.230	
   [semeval	
   win]	
   
0.149	
   
0.211	
   
0.275	
   [new	
   sota]	
   

41	
   

advantages of the neural word 
embedding approach 

compared	
   to	
   a	
   method	
   like	
   lsa,	
   neural	
   word	
   embeddings	
   
can	
   become	
   more	
   meaningful	
   through	
   adding	
   supervision	
   
from	
   one	
   or	
   mulgple	
   tasks	
   
   discriminagve	
      ne-     tuning   	
   

for	
   instance,	
   sengment	
   is	
   usually	
   not	
   captured	
   in	
   unsupervised	
   
word	
   embeddings	
   but	
   can	
   be	
   in	
   neural	
   word	
   vectors	
   

we	
   can	
   build	
   representagons	
   for	
   large	
   linguisgc	
   units	
   

see	
   part	
   2	
   

42	
   

part	
   1.4:	
   the	
   basics	
   

unsupervised word vector 
learning 

43	
   

a neural network for learning word 
vectors         (collobert	
   et	
   al.	
   jmlr	
   2011)	
   

idea:	
   a	
   word	
   and	
   its	
   context	
   is	
   a	
   posigve	
   training	
   
sample;	
   a	
   random	
   word	
   in	
   that	
   same	
   context	
   gives	
   
a	
   negagve	
   training	
   sample:	
   

	
   	
   	
   	
   	
   	
   	
   cat	
   chills	
   on	
   a	
   mat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   cat	
   chills	
   jeju	
   a	
   mat	
   

similar:	
   implicit	
   negagve	
   evidence	
   in	
   contrasgve	
   
esgmagon,	
   (smith	
   and	
   eisner	
   2005)	
   

44	
   

a neural network for learning word 
vectors 

how	
   do	
   we	
   	
   formalize	
   this	
   idea?	
   ask	
   that	
   

score(cat	
   chills	
   on	
   a	
   mat)	
   >	
   score(cat	
   chills	
   jeju	
   a	
   mat)	
   

	
   

how	
   do	
   we	
   compute	
   the	
   score?	
   

       with	
   a	
   neural	
   network	
   
       each	
   word	
   is	
   associated	
   with	
   an	
   	
   

n-     dimensional	
   vector	
   

45	
   

id27 matrix 
      

inigalize	
   all	
   word	
   vectors	
   randomly	
   to	
   form	
   a	
   word	
   embedding	
   
matrix	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   |v|	
   
	
   

[	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ]	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
      

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   n	
   	
   

	
   

	
   l	
   	
   =	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   cat	
   	
   	
   	
   	
   	
   mat	
   	
      	
   

       these	
   are	
   the	
   word	
   features	
   we	
   want	
   to	
   learn	
   
       also	
   called	
   a	
   look-     up	
   table	
   

       conceptually	
   you	
   get	
   a	
   word   s	
   vector	
   by	
   le^	
   mulgplying	
   a	
   
one-     hot	
   vector	
   e	
   by	
   l:	
   	
   	
   	
   	
   x	
   =	
   le	
   

46	
   

word vectors as input to a neural 
network 
       score(cat	
   chills	
   on	
   a	
   mat)	
   	
   
       to	
   describe	
   a	
   phrase,	
   retrieve	
   (via	
   index)	
   the	
   corresponding	
   

vectors	
   from	
   l	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   cat	
   chills	
   on	
   	
   	
   a	
   	
   	
   mat	
   
       then	
   concatenate	
   them	
   to	
   5n	
   vector:	
   
       x	
   	
   =[	
   
	
   
       how	
   do	
   we	
   then	
   compute	
   score(x)?	
   
	
   
47	
   

	
   

	
   

	
   

	
   

	
   	
   	
   ]	
   

a single layer neural network 
       a	
   single	
   layer	
   was	
   a	
   combinagon	
   of	
   a	
   linear	
   
layer	
   and	
   a	
   nonlinearity:	
   

       the	
   neural	
   acgvagons	
   a	
   can	
   then	
   
be	
   used	
   to	
   compute	
   some	
   funcgon	
   
       for	
   instance,	
   the	
   score	
   we	
   care	
   about:	
   

48	
   

summary: feed-forward computation 
compugng	
   a	
   window   s	
   score	
   with	
   a	
   3-     layer	
   neural	
   
net:	
   s	
   =	
   score(cat	
   chills	
   on	
   a	
   mat)	
   

49	
   

cat	
   	
   	
   	
   	
   chills	
   	
   	
   	
   	
   	
   on	
   	
   	
   	
   	
   	
   	
   	
   	
   a	
   	
   	
   	
   	
   	
   	
   mat	
   

summary: feed-forward computation 
       s	
   	
   =	
   score(cat	
   chills	
   on	
   a	
   mat)	
   
       sc	
   =	
   score(cat	
   chills	
   jeju	
   a	
   mat)	
   
       idea	
   for	
   training	
   objecgve:	
   make	
   score	
   of	
   true	
   window	
   
larger	
   and	
   corrupt	
   window   s	
   score	
   lower	
   (ungl	
   they   re	
   
good	
   enough):	
   minimize	
   

	
   

       this	
   is	
   congnuous,	
   can	
   perform	
   sgd	
   

50	
   

training with id26 

assuming	
   cost	
   j	
   is	
   >	
   0,	
   it	
   is	
   simple	
   to	
   see	
   that	
   we	
   
can	
   compute	
   the	
   derivagves	
   of	
   s	
   and	
   sc	
   wrt	
   all	
   the	
   
involved	
   variables:	
   u,	
   w,	
   b,	
   x	
   

51	
   

	
   

training with id26 
       let   s	
   consider	
   the	
   derivagve	
   of	
   a	
   single	
   weight	
   wij	
   

       this	
   only	
   appears	
   inside	
   ai	
   
       for	
   example:	
   w23	
   is	
   only	
   	
   
used	
   to	
   compute	
   a2	
   

s	
   	
   	
    u2	
   

a1	
   

	
   	
   	
   	
   	
   	
   	
   	
   a2	
   

w23	
   

52	
   

	
   	
   	
   	
   	
   	
   	
   	
   x2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x3	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   +1	
   

x1	
   
	
   

training with id26 

derivagve	
   of	
   weight	
   wij:	
   

s	
   	
   	
    u2	
   

a1	
   

	
   	
   	
   	
   	
   	
   	
   	
   a2	
   

w23	
   

53	
   

	
   	
   	
   	
   	
   	
   	
   	
   x2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x3	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   +1	
   

x1	
   
	
   

training with id26 

derivagve	
   of	
   single	
   weight	
   wij	
   :	
   

local	
   error	
   

signal	
   

local	
   input	
   

signal	
   

54	
   

s	
   	
   	
    u2	
   

a1	
   

	
   	
   	
   	
   	
   	
   	
   	
   a2	
   

w23	
   

	
   	
   	
   	
   	
   	
   	
   	
   x2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x3	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   +1	
   

x1	
   
	
   

training with id26 
       from	
   single	
   weight	
   wij	
   to	
   full	
   w:	
   

       we	
   want	
   all	
   combinagons	
   of	
   

i	
   =	
   1,	
   2	
   and	
   j	
   =	
   1,	
   2,	
   3	
   

       solugon:	
   outer	
   product:	
   
where	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   is	
   the	
   	
   
   responsibility   	
   coming	
   from	
   	
   
each	
   acgvagon	
   a	
   

55	
   

s	
   	
   	
    u2	
   

a1	
   

	
   	
   	
   	
   	
   	
   	
   	
   a2	
   

w23	
   

	
   	
   	
   	
   	
   	
   	
   	
   x2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x3	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   +1	
   

x1	
   
	
   

training with id26 
       for	
   biases	
   b,	
   we	
   get:	
   

s	
   	
   	
    u2	
   

a1	
   

	
   	
   	
   	
   	
   	
   	
   	
   a2	
   

w23	
   

56	
   

	
   	
   	
   	
   	
   	
   	
   	
   x2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x3	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   +1	
   

x1	
   
	
   

training with id26 

that   s	
   almost	
   backpropagagon	
   

it   s	
   simply	
   taking	
   derivagves	
   and	
   using	
   the	
   chain	
   rule!	
   

	
   

remaining	
   trick:	
   we	
   can	
   re-     use	
   derivagves	
   computed	
   for	
   
higher	
   layers	
   in	
   compugng	
   derivagves	
   for	
   lower	
   layers	
   

	
   

example:	
   last	
   derivagves	
   of	
   model,	
   the	
   word	
   vectors	
   in	
   x	
   

57	
   

training with id26 
       take	
   derivagve	
   of	
   score	
   with	
   
respect	
   to	
   single	
   word	
   vector	
   
(for	
   simplicity	
   a	
   1d	
   vector,	
   
but	
   same	
   if	
   it	
   was	
   longer)	
   
       now,	
   we	
   cannot	
   just	
   take	
   
into	
   consideragon	
   one	
   ai	
   
because	
   each	
   xj	
   is	
   connected	
   
to	
   all	
   the	
   neurons	
   above	
   and	
   
hence	
   xj	
   in   uences	
   the	
   
overall	
   score	
   through	
   all	
   of	
   
these,	
   hence:	
   

58	
   

re-     used	
   part	
   of	
   previous	
   derivagve	
   

training with id26: 
softmax 

what	
   is	
   the	
   major	
   bene   t	
   of	
   deep	
   learned	
   word	
   vectors?	
   

ability	
   to	
   also	
   propagate	
   labeled	
   informagon	
   into	
   them,	
   	
   
via	
   so^max/maxent	
   and	
   hidden	
   layer:	
   

	
   

p(c | d,  ) =

e  t f (c,d)
e  t f ( !c ,d)
!c   

59	
   

c1	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   c2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   c3	
   	
   	
   
	
   

s	
   

a1	
   

	
   	
   	
   	
   	
   	
   	
   	
   a2	
   

	
   	
   	
   	
   	
   	
   	
   	
   x2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x3	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   +1	
   

x1	
   
	
   

part	
   1.5:	
   the	
   basics	
   

id26 training 

60	
   

back-prop 
       compute	
   gradient	
   of	
   example-     wise	
   loss	
   wrt	
   

parameters	
   	
   

       simply	
   applying	
   the	
   derivagve	
   chain	
   rule	
   wisely	
   

if	
   compugng	
   the	
   loss(example,	
   parameters)	
   is	
   o(n)	
   
computagon,	
   then	
   so	
   is	
   compugng	
   the	
   gradient	
   

      

61	
   

simple chain rule 

62	
   

multiple paths chain rule 

63	
   

multiple paths chain rule - general 

   	
   

64	
   

chain rule in flow graph 

   	
   

   	
   

flow	
   graph:	
   any	
   directed	
   acyclic	
   graph	
   

	
   node	
   =	
   computagon	
   result	
   
	
   arc	
   =	
   computagon	
   dependency	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   	
   =	
   successors	
   of	
   	
   

   	
   

65	
   

back-prop in multi-layer net 

h = sigmoid(vx)

   	
   
   	
   

66	
   

back-prop in general flow graph 

single	
   scalar	
   output	
   

   	
   

   	
   

1.    fprop:	
   visit	
   nodes	
   in	
   topo-     sort	
   order	
   	
   
2.    bprop:	
   

-         compute	
   value	
   of	
   node	
   given	
   predecessors	
   

	
   -     	
   inigalize	
   output	
   gradient	
   =	
   1	
   	
   
	
   -     	
   visit	
   nodes	
   in	
   reverse	
   order:	
   

	
   compute	
   gradient	
   wrt	
   each	
   node	
   using	
   	
   

	
   	
   	
   	
   	
   	
    	
   gradient	
   wrt	
   successors	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   	
   =	
   successors	
   of	
   	
   

   	
   

67	
   

automatic differentiation 

       the	
   gradient	
   computagon	
   can	
   
be	
   automagcally	
   inferred	
   from	
   
the	
   symbolic	
   expression	
   of	
   the	
   
fprop.	
   

       each	
   node	
   type	
   needs	
   to	
   know	
   
how	
   to	
   compute	
   its	
   output	
   and	
   
how	
   to	
   compute	
   the	
   gradient	
   
wrt	
   its	
   inputs	
   given	
   the	
   
gradient	
   wrt	
   its	
   output.	
   

       easy	
   and	
   fast	
   prototyping	
   

68	
   

part	
   1.6:	
   the	
   basics	
   

learning word-level classifiers: 
pos and ner 

69	
   

the model 

(collobert	
   &	
   weston	
   2008;	
   
collobert	
   et	
   al.	
   2011)	
   
       similar	
   to	
   word	
   vector	
   

learning	
   but	
   replaces	
   the	
   
single	
   scalar	
   score	
   with	
   a	
   
solmax/maxent	
   classi   er	
   

       training	
   is	
   again	
   done	
   via	
   

backpropagagon	
   which	
   gives	
   
an	
   error	
   similar	
   to	
   the	
   score	
   
in	
   the	
   unsupervised	
   word	
   
vector	
   learning	
   model	
   

70	
   

c1	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   c2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   c3	
   	
   	
   
	
   

a1	
   

	
   	
   	
   	
   	
   	
   	
   	
   a2	
   

s	
   

	
   	
   	
   	
   	
   	
   	
   	
   x2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x3	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   +1	
   

x1	
   
	
   

the model - training 
       we	
   already	
   know	
   the	
   so^max	
   classi   er	
   and	
   how	
   to	
   opgmize	
   it	
   
       the	
   interesgng	
   twist	
   in	
   deep	
   learning	
   is	
   that	
   the	
   input	
   features	
   
are	
   also	
   learned,	
   similar	
   to	
   learning	
   word	
   vectors	
   with	
   a	
   score:	
   

s	
   	
   	
    u2	
   

a1	
   

	
   	
   	
   	
   	
   	
   	
   	
   a2	
   

w23	
   

c1	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   c2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   c3	
   	
   	
   
	
   

a1	
   

	
   	
   	
   	
   	
   	
   	
   	
   a2	
   

s	
   

	
   	
   	
   	
   	
   	
   	
   	
   x2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x3	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   +1	
   

x1	
   
	
   

71	
   

	
   	
   	
   	
   	
   	
   	
   	
   x2	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x3	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   +1	
   

x1	
   
	
   

the secret sauce is the unsupervised 
pre-training on a large text collection 

state-     of-     the-     art*	
   

supervised	
   nn	
   

unsupervised	
   pre-     training	
   
followed	
   by	
   supervised	
   nn**	
   
	
   	
   	
   +	
   hand-     cra^ed	
   features***	
    97.29	
   

pos	
   
wsj	
   (acc.)	
   
97.24	
   
96.37	
   
97.20	
   

ner	
   
conll	
   (f1)	
   
89.31	
   
81.47	
   
88.87	
   

89.59	
   

*	
   representagve	
   systems:	
   pos:	
   (toutanova	
   et	
   al.	
   2003),	
   ner:	
   (ando	
   &	
   zhang	
   
2005)	
   
**	
   130,000-     word	
   embedding	
   trained	
   on	
   wikipedia	
   and	
   reuters	
   with	
   11	
   word	
   
window,	
   100	
   unit	
   hidden	
   layer	
      	
   for	
   7	
   weeks!	
      	
   then	
   supervised	
   task	
   training	
   
***features	
   are	
   character	
   su   xes	
   for	
   pos	
   and	
   a	
   gaze0eer	
   for	
   ner	
   
72	
   

supervised refinement of the 
unsupervised word representation helps 

pos	
   
wsj	
   (acc.)	
   
supervised	
   nn	
   
96.37	
   
nn	
   with	
   brown	
   clusters	
    96.92	
   
97.10	
   
fixed	
   embeddings*	
   
c&w	
   2011**	
   
97.29	
   

ner	
   
conll	
   (f1)	
   
81.47	
   
87.15	
   
88.87	
   
89.59	
   

*	
   same	
   architecture	
   as	
   c&w	
   2011,	
   but	
   word	
   embeddings	
   are	
   kept	
   constant	
   
during	
   the	
   supervised	
   training	
   phase	
   
**	
   c&w	
   is	
   unsupervised	
   pre-     train	
   +	
   supervised	
   nn	
   +	
   features	
   model	
   of	
   last	
   slide	
   

73	
   

part	
   1.7	
   

sharing statistical strength 

74	
   

id72 
       generalizing	
   be0er	
   to	
   new	
   
tasks	
   is	
   crucial	
   to	
   approach	
   
ai	
   

task 1  
output y1 

       deep	
   architectures	
   learn	
   

good	
   intermediate	
   
representagons	
   that	
   can	
   be	
   
shared	
   across	
   tasks	
   

       good	
   representagons	
   make	
   

sense	
   for	
   many	
   tasks	
   

75	
   

task 2 
output y2 

task 3  
output y3 

shared 
intermediate 
representation h 

raw input x 

combining multiple sources of 
evidence with shared embeddings 
       relagonal	
   learning	
   
       mulgple	
   sources	
   of	
   informagon	
   /	
   relagons	
   
       some	
   symbols	
   (e.g.	
   words,	
   wikipedia	
   entries)	
   shared	
   
       shared	
   embeddings	
   help	
   propagate	
   informagon	
   

among	
   data	
   sources:	
   e.g.,	
   id138,	
   xwn,	
   wikipedia,	
   
freebase,	
      	
   

76	
   

sharing statistical strength 
       besides	
   very	
   fast	
   predicgon,	
   the	
   main	
   advantage	
   of	
   

deep	
   learning	
   is	
   stagsgcal	
   

       potengal	
   to	
   learn	
   from	
   less	
   labeled	
   examples	
   because	
   

of	
   sharing	
   of	
   stagsgcal	
   strength:	
   
       unsupervised	
   pre-     training	
   &	
   mulg-     task	
   learning	
   
       semi-     supervised	
   learning	
        	
   

77	
   

semi-supervised learning 
       hypothesis:	
   p(c|x)	
   can	
   be	
   more	
   accurately	
   computed	
   using	
   

shared	
   structure	
   with	
   p(x)	
   	
   

purely	
   
supervised	
   

78	
   

semi-supervised learning 
       hypothesis:	
   p(c|x)	
   can	
   be	
   more	
   accurately	
   computed	
   using	
   

shared	
   structure	
   with	
   p(x)	
   	
   

semi-     	
   
supervised	
   

79	
   

deep autoencoders  

alternagve	
   to	
   contrasgve	
   unsupervised	
   word	
   learning	
   

       another	
   is	
   rbms	
   (hinton	
   et	
   al.	
   2006),	
   which	
   we	
   don   t	
   cover	
   today	
   

works	
   well	
   for	
      xed	
   input	
   representagons	
   

1.    de   nigon,	
   intuigon	
   and	
   variants	
   of	
   autoencoders	
   
2.    stacking	
   for	
   deep	
   autoencoders	
   
3.    why	
   do	
   autoencoders	
   improve	
   deep	
   neural	
   nets	
   so	
   much?	
   

80	
   

auto-encoders 
       mulglayer	
   neural	
   net	
   with	
   target	
   output	
   =	
   input	
   
       reconstrucgon=decoder(encoder(input))	
   

       probable	
   inputs	
   have	
   	
   

small	
   reconstrucgon	
   error	
   

	
   decoder	
   

	
   encoder	
   

81	
   

   	
   

	
   reconstrucgon	
   

	
   code=	
   latent	
   features	
   

   	
   

	
   input	
   

pca = linear manifold = linear auto-
encoder 

input	
   x,	
   0-     mean	
   
features=code=h(x)=w	
   x	
   
reconstrucgon(x)=wt	
   h(x)	
   =	
   wt	
   w	
   x	
   
w	
   =	
   principal	
   eigen-     basis	
   of	
   cov(x)	
   

linear	
   manifold	
   

reconstrucgon(x)	
   

reconstrucgon	
   error	
   vector	
   

x	
   

lsa	
   example:	
   
x	
   =	
   (normalized)	
   distribugon	
   
of	
   co-     occurrence	
   frequencies	
   

82	
   

the manifold learning hypothesis 
       examples	
   concentrate	
   near	
   a	
   lower	
   dimensional	
   

   manifold   	
   (region	
   of	
   high	
   density	
   where	
   small	
   changes	
   are	
   only	
   
allowed	
   in	
   certain	
   direcgons)	
   

83	
   

auto-encoders learn salient 
variations, like a non-linear pca 

minimizing	
   reconstrucgon	
   error	
   
forces	
   latent	
   representagon	
   of	
   	
   
   similar	
   inputs   	
   to	
   stay	
   on	
   	
   
manifold	
   

84	
   

auto-encoder variants 
       discrete	
   inputs:	
   cross-     id178	
   or	
   log-     likelihood	
   reconstrucgon	
   

criterion	
   (similar	
   to	
   used	
   for	
   discrete	
   targets	
   for	
   mlps)	
   

       prevengng	
   them	
   to	
   learn	
   the	
   idengty	
   everywhere:	
   

       undercomplete	
   (eg	
   pca):	
   	
   bo0leneck	
   code	
   smaller	
   than	
   input	
   
       sparsity:	
   penalize	
   hidden	
   unit	
   acgvagons	
   so	
   at	
   or	
   near	
   0	
   
	
   	
   	
   	
   [goodfellow	
   et	
   al	
   2009]	
   
       denoising:	
   predict	
   true	
   input	
   from	
   corrupted	
   input	
   
	
   	
   	
   	
   [vincent	
   et	
   al	
   2008]	
   
       contracgve:	
   force	
   encoder	
   to	
   have	
   small	
   derivagves	
   
	
   	
   	
   	
   [rifai	
   et	
   al	
   2011]	
   

85	
   

sparse autoencoder illustration for 
images 

	
   	
   	
   	
   natural	
   images	
   

learned	
   bases:	
   	
      edges   	
   

50

100

150

200

250

300

350

400

450

500

50

100

150

200

250

300

50

100

150

200

350

250

300

350

400

400

450

500

500

50

100

450
150

200

250

300

50

100

150

200
350

250

300

350

400

450

500

400

450

500

50

100

150

200

250

300

350

400

450

500

test	
   example 

    0.8 *                   + 0.3 *                     + 0.5 * 

86	
   

[a1,	
      ,	
   a64]	
   =	
   [0,	
   0,	
      ,	
   0,	
   0.8,	
   0,	
      ,	
   0,	
   0.3,	
   0,	
      ,	
   0,	
   0.5,	
   0]	
   	
   
(feature	
   representagon)	
   	
   

         0.8 *         36         +  0.3 *          42          + 0.5 *         63	
   

stacking auto-encoders 
       can	
   be	
   stacked	
   successfully	
   (bengio	
   et	
   al	
   nips   2006)	
   to	
   form	
   highly	
   

non-     linear	
   representagons	
   

87	
   

layer-wise unsupervised learning 

input 

    

88	
   

layer-wise unsupervised pre-training 

features 

input 

    

    

89	
   

layer-wise unsupervised pre-training 

? 
= 

    

input 

reconstruction 
of input 

features 

input 

90	
   

    

    

    

layer-wise unsupervised pre-training 

features 

input 

    

    

91	
   

layer-wise unsupervised pre-training 

more abstract 
features 
features 

input 

92	
   

    

    

    

layer-wise unsupervised pre-training 
layer-wise unsupervised learning 

? 
= 

    

reconstruction 
of features 

more abstract 
features 
features 

input 

93	
   

    
        

    

    

    

layer-wise unsupervised pre-training 

more abstract 
features 
features 

input 

94	
   

    

    

    

layer-wise unsupervised learning 

even more abstract 
features 

more abstract 
features 
features 

input 

95	
   

    

    

    

    

supervised fine-tuning 

output 
f(x)  six 

? 
= 

target 
y 

two! 

even more abstract 
features 

more abstract 
features 
features 

input 

96	
   

    

    

    

    

why is unsupervised pre-training 
working so well? 
       regularizagon	
   hypothesis:	
   	
   

       representagons	
   good	
   for	
   p(x)	
   
are	
   good	
   for	
   p(y|x)	
   	
   

       opgmizagon	
   hypothesis:	
   

       unsupervised	
   inigalizagons	
   start	
   
near	
   be0er	
   local	
   minimum	
   of	
   
supervised	
   training	
   error	
   
       minima	
   otherwise	
   not	
   
achievable	
   by	
   random	
   
inigalizagon	
   

erhan,	
   courville,	
   manzagol,	
   
vincent,	
   bengio	
   (jmlr,	
   2010)	
   

	
   97	
   

part	
   2	
   

recursive deep learning 

98	
   

building on word vector space models 

x2	
   
5	
   

4	
   

3	
   

2	
   

1	
   

germany	
   

1	
   
3	
   
france	
    2	
   
2.5	
   

1	
   
5	
   

1.1	
   
4	
   

monday	
   
tuesday	
   

9	
   
2	
   

9.5	
   
1.5	
   

	
   	
   	
   0	
   	
   	
   	
   	
   	
   	
   	
   1	
   	
   	
   	
   	
   	
   2	
   	
   	
   	
   	
   3	
   	
   	
   	
   	
   4	
   	
   	
   	
   	
   	
   5	
   	
   	
   	
   	
   6	
   	
   	
   	
   	
   7	
   	
   	
   	
   	
   8	
   	
   	
   	
   	
   	
   9	
   	
   	
   	
   	
   10	
   

x1	
   

the	
   country	
   of	
   my	
   birth	
   
	
   	
   the	
   place	
   where	
   i	
   was	
   born	
   

but	
   how	
   can	
   we	
   represent	
   the	
   meaning	
   of	
   longer	
   phrases?	
   
by	
   mapping	
   them	
   into	
   the	
   same	
   vector	
   space!	
   

99	
   

how should we map phrases into a 
vector space? 

use	
   principle	
   of	
   composigonality	
   
the	
   meaning	
   (vector)	
   of	
   a	
   sentence	
   
is	
   	
   determined	
   by	
   	
   
(1)   the	
   meanings	
   of	
   its	
   words	
   and	
   
(2)   the	
   rules	
   that	
   combine	
   them.	
   

x2	
   
5	
   

4	
   

3	
   

2	
   

1	
   

	
   	
   the	
   country	
   of	
   my	
   birth	
   

	
   	
   the	
   place	
   where	
   i	
   was	
   born	
   

germany	
   

france	
   

1	
   
5	
   

	
   	
   	
   0	
   	
   	
   	
   	
   	
   	
   	
   1	
   	
   	
   	
   	
   	
   2	
   	
   	
   	
   	
   	
   	
   3	
   	
   	
   	
   	
   	
   4	
   	
   	
   	
   	
   	
   5	
   	
   	
   	
   	
   	
   6	
   	
   	
   	
   	
   	
   7	
   	
   	
   	
   	
   	
   	
   8	
   	
   	
   	
   	
   	
   9	
   	
   	
   	
   	
   10	
   

monday	
   

tuesday	
   

x1	
   

1	
   
3.5	
   

5.5	
   
6.1	
   

2.5	
   
3.8	
   

0.4	
   
0.3	
   

2.1	
   
3.3	
   

7	
   
7	
   

4	
   
4.5	
   

2.3	
   
3.6	
   

	
   	
   the	
   	
   	
   	
   	
   	
   	
   country	
   	
   	
   	
   	
   	
   	
   of	
   	
   	
   	
   	
   	
   	
   	
   	
   my	
   	
   	
   	
   	
   	
   	
   birth	
   

models	
   in	
   this	
   secgon	
   
can	
   jointly	
   learn	
   parse	
   
trees	
   and	
   composigonal	
   
vector	
   representagons	
   

100	
   

semantic vector spaces 

vectors	
   	
   
represengng	
   
phrases	
   and	
   sentences	
   
that	
   do	
   not	
   ignore	
   word	
   order	
   
and	
   capture	
   semangcs	
   for	
   nlp	
   tasks	
   

	
   	
   	
   	
   
single	
   word	
   vectors	
   
       distribugonal	
   techniques	
   
       brown	
   clusters	
   
       useful	
   as	
   features	
   inside	
   

models,	
   e.g.	
   crfs	
   for	
   
ner,	
   etc.	
   

       cannot	
   capture	
   longer	
   

phrases	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   documents	
   vectors	
   

       bag	
   of	
   words	
   models	
   
       lsa,	
   lda	
   
       great	
   for	
   ir,	
   document	
   

      

exploragon,	
   etc.	
   
ignore	
   word	
   order,	
   no	
   
detailed	
   understanding	
   

recursive deep learning 
1.    mogvagon	
   
2.    recursive	
   neural	
   networks	
   for	
   parsing	
   	
   
3.    opgmizagon	
   and	
   backpropagagon	
   through	
   structure	
   
4.    composigonal	
   vector	
   grammars:
5.    recursive	
   autoencoders:	
   
	
   
6.    matrix-     vector	
   id56s:	
   
	
   
	
   
7.    recursive	
   neural	
   tensor	
   networks:	
   

	
   parsing	
   
	
   paraphrase	
   detecgon	
   
	
   relagon	
   classi   cagon	
   
	
   sengment	
   analysis	
   

102	
   

sentence parsing: what we want 

s	
   

vp	
   

pp	
   

np	
   

9	
   
1	
   

5	
   
3	
   

7	
   
1	
   

8	
   
5	
   

9	
   
1	
   

103	
   

	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   cat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   sat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   mat.	
   

np	
   

4	
   
3	
   

learn structure and representation 

5	
   
4	
   

s	
   

vp	
   

7	
   
3	
   

8	
   
3	
   

pp	
   

5	
   
2	
   

np	
   

3	
   
3	
   

np	
   

9	
   
1	
   

5	
   
3	
   

7	
   
1	
   

8	
   
5	
   

9	
   
1	
   

4	
   
3	
   

104	
   

	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   cat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   sat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   mat.	
   

id56s for 
structure prediction 

inputs:	
   two	
   candidate	
   children   s	
   representagons	
   
outputs:	
   
1.    the	
   semangc	
   representagon	
   if	
   the	
   two	
   nodes	
   are	
   merged.	
   
2.    score	
   of	
   how	
   plausible	
   the	
   new	
   node	
   would	
   be.	
   

1.3	
   

8	
   
3	
   

neural "
network"

8	
   
5	
   

3	
   
3	
   

105	
   

8	
   
3	
   

3	
   
3	
   

8	
   
5	
   

9	
   
1	
   

4	
   
3	
   

on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   mat.	
   

id56 definition 

score	
   	
   =	
   

1.3	
   

8	
   
3	
   

=	
   parent	
   

	
   

score	
   	
   =	
   	
   utp	
   
	
   
p	
   	
   =	
   	
   tanh(w	
   	
   	
   	
   	
   	
   	
   +	
   b),	
   
	
   
	
   
	
   
same	
   w	
   parameters	
   at	
   all	
   nodes	
   	
   
of	
   the	
   tree	
   

c1	
   
c2	
   

neural "
network"

8	
   
5	
   

3	
   
3	
   

c1	
   	
   	
   	
   	
   	
   c2	
   

106	
   

related work to socher et al. (icml 
2011) 
       pollack	
   (1990):	
   recursive	
   auto-     associagve	
   memories	
   
       previous	
   recursive	
   neural	
   networks	
   work	
   by	
   	
   

goller	
   &	
   k  chler	
   (1996),	
   costa	
   et	
   al.	
   (2003)	
   assumed	
   	
   
   xed	
   tree	
   structure	
   and	
   used	
   one	
   hot	
   vectors.	
   

       hinton	
   (1990)	
   and	
   bo0ou	
   (2011):	
   related	
   ideas	
   about	
   	
   
recursive	
   models	
   and	
   recursive	
   operators	
   as	
   smooth	
   	
   
versions	
   of	
   logic	
   operagons	
   

107	
   

parsing a sentence with an id56 

5	
   
2	
   

	
   3.1	
   

	
   0.3	
   

0	
   
1	
   

	
   0.1	
   

2	
   
0	
   

	
   0.4	
   

1	
   
0	
   

3	
   
3	
   

	
   2.3	
   

neural "
network"

neural "
network"

neural "
network"

neural "
network"

neural "
network"

9	
   
1	
   

5	
   
3	
   

7	
   
1	
   

8	
   
5	
   

9	
   
1	
   

4	
   
3	
   

	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   cat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   sat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   mat.	
   

108	
   

parsing a sentence 

2	
   
1	
   

	
   1.1	
   

neural "
network"

2	
   
0	
   

	
   0.4	
   

1	
   
0	
   

	
   0.1	
   

3	
   
3	
   

	
   2.3	
   

5	
   
2	
   

neural "
network"

neural "
network"

neural "
network"

9	
   
1	
   

5	
   
5	
   
3	
   
3	
   

7	
   
1	
   

8	
   
5	
   

9	
   
1	
   

4	
   
3	
   

	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   cat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   sat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   mat.	
   

109	
   

parsing a sentence 

2	
   
1	
   

	
   1.1	
   

neural "
network"

5	
   
2	
   

2	
   
0	
   

	
   0.1	
   

neural "
network"

8	
   
3	
   

	
   3.6	
   

neural "
network"

3	
   
3	
   

9	
   
1	
   

5	
   
5	
   
3	
   
3	
   

7	
   
1	
   

8	
   
5	
   

9	
   
1	
   

4	
   
3	
   

	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   cat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   sat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   mat.	
   

110	
   

parsing a sentence 

5	
   
4	
   

7	
   
3	
   

8	
   
3	
   

5	
   
2	
   

3	
   
3	
   

9	
   
1	
   

5	
   
5	
   
3	
   
3	
   

7	
   
1	
   

8	
   
5	
   

9	
   
1	
   

4	
   
3	
   

	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   cat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   sat	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   mat.	
   

111	
   

max-margin framework - details 
       the	
   score	
   of	
   a	
   tree	
   is	
   computed	
   by	
   	
   

8	
   
3	
   

1.3	
   
id56"

the	
   sum	
   of	
   the	
   parsing	
   decision	
   
scores	
   at	
   each	
   node.	
   

       similar	
   to	
   max-     margin	
   parsing	
   (taskar	
   et	
   al.	
   2004),	
   a	
   supervised	
   

max-     margin	
   objecgve	
   

8	
   
5	
   

3	
   
3	
   

	
   
	
   
       the	
   loss	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   penalizes	
   all	
   incorrect	
   decisions	
   
       structure	
   search	
   for	
   a(x)	
   was	
   maximally	
   greedy	
   

       instead:	
   beam	
   search	
   with	
   chart	
   

112	
   

id26 through structure 
      
       principally	
   the	
   same	
   as	
   general	
   backpropagagon	
   

introduced	
   by	
   goller	
   &	
   k  chler	
   (1996)	
   	
   

       two	
   di   erences	
   resulgng	
   from	
   the	
   tree	
   structure:	
   

       split	
   derivagves	
   at	
   each	
   node	
   

       sum	
   derivagves	
   of	
   w	
   from	
   all	
   nodes	
   

113	
   

bts: split derivatives at each node 
       during	
   forward	
   prop,	
   the	
   parent	
   is	
   computed	
   using	
   2	
   children	
   

8	
   
3	
   

8	
   
5	
   

c1	
   

3	
   
3	
   

c2	
   

p	
   	
   =	
   	
   tanh(w	
   	
   	
   	
   	
   	
   	
   +	
   b)	
   

c1	
   
c2	
   

       hence,	
   the	
   errors	
   need	
   to	
   be	
   computed	
   wrt	
   each	
   of	
   them:	
   

	
   
	
   

8	
   
5	
   

8	
   
3	
   

	
   

c1	
   

	
   

3	
   
3	
   

c2	
   

114	
   

	
   where	
   each	
   child   s	
   error	
   is	
   n-     dimensional	
   

bts: sum derivatives of all nodes 
       you	
   can	
   actually	
   assume	
   it   s	
   a	
   di   erent	
   w	
   at	
   each	
   node	
   
      

intuigon	
   via	
   example:	
   

if	
   take	
   separate	
   derivagves	
   of	
   each	
   occurrence,	
   we	
   get	
   same:	
   

      

115	
   

bts: optimization 
       as	
   before,	
   we	
   can	
   plug	
   the	
   gradients	
   into	
   a	
   
standard	
   o   -     the-     shelf	
   l-     bfgs	
   opgmizer	
   
       best	
   results	
   with	
   adagrad	
   (duchi	
   et	
   al,	
   2011):	
   	
   

       for	
   non-     congnuous	
   objecgve	
   use	
   subgradient	
   
method	
   (ratli   	
   et	
   al.	
   2007)	
   

116	
   

discussion: simple id56 
       good	
   results	
   with	
   single	
   matrix	
   id56	
   (more	
   later)	
   
       single	
   weight	
   matrix	
   id56	
   could	
   capture	
   some	
   

phenomena	
   but	
   not	
   adequate	
   for	
   more	
   complex,	
   
higher	
   order	
   composigon	
   and	
   parsing	
   long	
   sentences	
   

wscore

w

s
p

c1

c2

       the	
   composigon	
   funcgon	
   is	
   the	
   same	
   	
   

for	
   all	
   syntacgc	
   categories,	
   punctuagon,	
   etc	
   

solution: syntactically-untied id56 
       idea:	
   condigon	
   the	
   composigon	
   funcgon	
   on	
   the	
   

syntacgc	
   categories,	
      unge	
   the	
   weights   	
   

       allows	
   for	
   di   erent	
   composigon	
   funcgons	
   for	
   pairs	
   

of	
   syntacgc	
   categories,	
   e.g.	
   adv	
   +	
   adjp,	
   vp	
   +	
   np	
   

       combines	
   discrete	
   syntacgc	
   categories	
   with	
   

congnuous	
   semangc	
   informagon	
   

solution: cvg =  
pid18 + syntactically-untied id56 
       problem:	
   speed.	
   every	
   candidate	
   score	
   in	
   beam	
   

search	
   needs	
   a	
   matrix-     vector	
   product.	
   

       solugon:	
   compute	
   score	
   using	
   a	
   linear	
   combinagon	
   

of	
   the	
   log-     likelihood	
   from	
   a	
   simple	
   pid18	
   +	
   id56	
   
       prunes	
   very	
   unlikely	
   candidates	
   for	
   speed	
   
       provides	
   coarse	
   syntacgc	
   categories	
   of	
   the	
   
children	
   for	
   each	
   beam	
   candidate	
   

       composigonal	
   vector	
   grammars:	
   cvg	
   =	
   pid18	
   +	
   id56	
   

details: compositional vector 
grammar 
       scores	
   at	
   each	
   node	
   computed	
   by	
   combinagon	
   of	
   

pid18	
   and	
   su-     id56:	
   

       interpretagon:	
   factoring	
   discrete	
   and	
   congnuous	
   

parsing	
   in	
   one	
   model:	
   

       socher	
   et	
   al	
   (2013):	
   more	
   details	
   at	
   acl	
   

related work 
       resulgng	
   cvg	
   parser	
   is	
   related	
   to	
   previous	
   work	
   that	
   extends	
   pid18	
   

parsers	
   

       klein	
   and	
   manning	
   (2003a)	
   :	
   manual	
   feature	
   engineering	
   
       petrov	
   et	
   al.	
   (2006)	
   :	
   learning	
   algorithm	
   that	
   splits	
   and	
   merges	
   

syntacgc	
   categories	
   	
   

       lexicalized	
   parsers	
   (collins,	
   2003;	
   charniak,	
   2000):	
   describe	
   each	
   

category	
   with	
   a	
   lexical	
   item	
   

       hall	
   and	
   klein	
   (2012)	
   combine	
   several	
   such	
   annotagon	
   schemes	
   in	
   a	
   

factored	
   parser.	
   	
   

       cvgs	
   extend	
   these	
   ideas	
   from	
   discrete	
   representagons	
   to	
   richer	
   

congnuous	
   ones	
   

       hermann	
   &	
   blunsom	
   (2013):	
   combine	
   combinatory	
   categorial	
   

grammars	
   with	
   id56s	
   and	
   also	
   unge	
   weights,	
   see	
   upcoming	
   acl	
   2013	
   

experiments 
       standard	
   wsj	
   split,	
   labeled	
   f1	
   
       based	
   on	
   simple	
   pid18	
   with	
   fewer	
   states	
   
       fast	
   pruning	
   of	
   search	
   space,	
   few	
   matrix-     vector	
   products	
   
       3.8%	
   higher	
   f1,	
   20%	
   faster	
   than	
   stanford	
   parser	
   

parser	
   
stanford	
   pid18,	
   (klein	
   and	
   manning,	
   2003a)	
   
stanford	
   factored	
   (klein	
   and	
   manning,	
   2003b)	
   

factored	
   pid18s	
   (hall	
   and	
   klein,	
   2012)	
   
collins	
   (collins,	
   1997)	
   
ssn	
   (henderson,	
   2004)	
   
berkeley	
   parser	
   (petrov	
   and	
   klein,	
   2007)	
   
cvg	
   (id56)	
   (socher	
   et	
   al.,	
   acl	
   2013)	
   
cvg	
   (su-     id56)	
   (socher	
   et	
   al.,	
   acl	
   2013)	
   
charniak	
   -     	
   self	
   trained	
   (mcclosky	
   et	
   al.	
   2006)	
   
charniak	
   -     	
   self	
   trained-     reranked	
   (mcclosky	
   et	
   al.	
   2006)	
   

test,	
   all	
   sentences	
   

85.5	
   
86.6	
   

89.4	
   
87.7	
   
89.4	
   
90.1	
   
85.0	
   
90.4	
   
91.0	
   
92.1	
   

su-id56 analysis 
       learns	
   nogon	
   of	
   so^	
   head	
   words	
   

dt-     np	
   	
   
	
   
	
   
vp-     np	
   

analysis of resulting vector 
representations 

	
   

all	
   the	
      gures	
   are	
   adjusted	
   for	
   seasonal	
   variagons	
   
1.	
   all	
   the	
   numbers	
   are	
   adjusted	
   for	
   seasonal	
      uctuagons	
   
2.	
   all	
   the	
      gures	
   are	
   adjusted	
   to	
   remove	
   usual	
   seasonal	
   pa0erns	
   
knight-     ridder	
   wouldn   t	
   comment	
   on	
   the	
   o   er	
   
1.	
   harsco	
   declined	
   to	
   say	
   what	
   country	
   placed	
   the	
   order	
   
2.	
   coastal	
   wouldn   t	
   disclose	
   the	
   terms	
   
sales	
   grew	
   almost	
   7%	
   to	
   $unk	
   m.	
   from	
   $unk	
   m.	
   
1.	
   sales	
   rose	
   more	
   than	
   7%	
   to	
   $94.9	
   m.	
   from	
   $88.3	
   m.	
   
2.	
   sales	
   surged	
   40%	
   to	
   unk	
   b.	
   yen	
   from	
   unk	
   b.	
   
	
   
"

	
   

su-id56 analysis 
       can	
   transfer	
   semangc	
   informagon	
   from	
   
single	
   related	
   example	
   
       train	
   sentences:	
   

      he	
   eats	
   spaghe(cid:134)	
   with	
   a	
   fork.	
   	
   
      she	
   eats	
   spaghe(cid:134)	
   with	
   pork.	
   	
   

       test	
   sentences	
   	
   

      he	
   eats	
   spaghe(cid:134)	
   with	
   a	
   spoon.	
   	
   
      he	
   eats	
   spaghe(cid:134)	
   with	
   meat.	
   

su-id56 analysis 

labeling in id56s 
       we	
   can	
   use	
   each	
   node   s	
   
representagon	
   as	
   features	
   for	
   a	
   
solmax	
   classi   er:	
   

softmax"
layer"

np	
   

8	
   
3	
   

neural "
network"

       training	
   similar	
   to	
   model	
   in	
   part	
   1	
   with	
   
standard	
   cross-     id178	
   error	
   +	
   scores	
   

127	
   

scene parsing 

similar	
   principle	
   of	
   composigonality.	
   

       the	
   meaning	
   of	
   a	
   scene	
   image	
   is	
   
also	
   a	
   funcgon	
   of	
   smaller	
   regions,	
   	
   
       how	
   they	
   combine	
   as	
   parts	
   to	
   form	
   

larger	
   objects,	
   

      

	
   and	
   how	
   the	
   objects	
   interact.	
   

128	
   

algorithm for parsing images 

same	
   recursive	
   neural	
   network	
   as	
   for	
   natural	
   language	
   parsing!	
   	
   

(socher	
   et	
   al.	
   icml	
   2011)	
   

parsing	
   natural	
   scene	
   images
parsing	
   natural	
   scene	
   images

grass

people building

tree

129	
   

semantic	
   	
   
representations
features
segments

multi-class segmentation 

method	
   
pixel	
   crf	
   (gould	
   et	
   al.,	
   iccv	
   2009)	
   
classi   er	
   on	
   superpixel	
   features	
   
region-     based	
   energy	
   (gould	
   et	
   al.,	
   iccv	
   2009)	
   
local	
   labelling	
   (tighe	
   &	
   lazebnik,	
   eccv	
   2010)	
   
superpixel	
   mrf	
   (tighe	
   &	
   lazebnik,	
   eccv	
   2010)	
   
simultaneous	
   mrf	
   (tighe	
   &	
   lazebnik,	
   eccv	
   2010)	
   
recursive	
   neural	
   network	
   

130	
   

stanford	
   background	
   dataset	
   (gould	
   et	
   al.	
   2009)	
   

accuracy	
   

74.3	
   
75.9	
   
76.4	
   
76.9	
   
77.5	
   
77.5	
   
78.1	
   

recursive deep learning 
1.    mogvagon	
   
2.    recursive	
   neural	
   networks	
   for	
   parsing	
   	
   
3.    theory:	
   backpropagagon	
   through	
   structure	
   
4.    composigonal	
   vector	
   grammars:
5.    recursive	
   autoencoders:	
   
	
   
6.    matrix-     vector	
   id56s:	
   
	
   
	
   
7.    recursive	
   neural	
   tensor	
   networks:	
   

	
   parsing	
   
	
   paraphrase	
   detecgon	
   
	
   relagon	
   classi   cagon	
   
	
   sengment	
   analysis	
   

131	
   

semi-supervised recursive 
autoencoder 

       to	
   capture	
   sengment	
   and	
   solve	
   antonym	
   problem,	
   add	
   a	
   so^max	
   classi   er	
   	
   
       error	
   is	
   a	
   weighted	
   combinagon	
   of	
   reconstrucgon	
   error	
   and	
   cross-     id178	
   
       socher	
   et	
   al.	
   (emnlp	
   2011)	
   

reconstruction	
   error	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   cross-     id178	
   error

w(2)

w(1)

w(label)

132	
   

paraphrase detection 
       pollack	
   said	
   the	
   plaing   s	
   failed	
   to	
   show	
   that	
   merrill	
   

and	
   blodget	
   directly	
   caused	
   their	
   losses	
   

       basically	
   ,	
   the	
   plaing   s	
   did	
   not	
   show	
   that	
   omissions	
   

in	
   merrill   s	
   research	
   caused	
   the	
   claimed	
   losses	
   

       the	
   inigal	
   report	
   was	
   made	
   to	
   modesto	
   police	
   

december	
   28	
   

       it	
   stems	
   from	
   a	
   modesto	
   police	
   report	
   

133	
   

how to compare 
the meaning 
of two sentences? 

134	
   

unsupervised recursive autoencoders 
       similar	
   to	
   recursive	
   neural	
   net	
   but	
   instead	
   of	
   a	
   

supervised	
   score	
   we	
   compute	
   a	
   reconstrucgon	
   error	
   
at	
   each	
   node.	
   socher	
   et	
   al.	
   (emnlp	
   	
   2011)	
   

y2=f(w[x1;y1]	
   +	
   b)

y1=f(w[x2;x3]	
   +	
   b)

135	
   

x1

x2

x3

unsupervised unfolding rae 
       a0empt	
   to	
   encode	
   engre	
   tree	
   structure	
   at	
   each	
   node	
   

136	
   

recursive autoencoders for full 
sentence paraphrase detection 
       unsupervised	
   unfolding	
   rae	
   and	
   a	
   pair-     wise	
   sentence	
   

comparison	
   of	
   nodes	
   in	
   parsed	
   trees	
   

       socher	
   et	
   al.	
   (nips	
   2011)	
   

137	
   

recursive autoencoders for full 
sentence paraphrase detection 
       experiments	
   on	
   microso^	
   research	
   paraphrase	
   corpus	
   	
   
      

(dolan	
   et	
   al.	
   2004)	
   

method	
   
rus	
   et	
   al.(2008)	
   
mihalcea	
   et	
   al.(2006)	
   
islam	
   et	
   al.(2007)	
   
qiu	
   et	
   al.(2006)	
   	
   
fernando	
   et	
   al.(2008)	
   
wan	
   et	
   al.(2006)	
   
das	
   and	
   smith	
   (2009)	
   	
   
das	
   and	
   smith	
   (2009)	
   +	
   18	
   surface	
   features	
   
f.	
   bu	
   et	
   al.	
   (acl	
   2012):	
   string	
   re-     wrigng	
   kernel	
   
unfolding	
   recursive	
   autoencoder	
   (nips	
   2011)	
   

138	
   

acc.	
   
70.6	
   
70.3	
   
72.6	
   
72.0	
   
74.1	
   
75.6	
   
73.9	
   
76.1	
   
76.3	
   
76.8	
   

f1	
   
80.5	
   
81.3	
   
81.3	
   
81.6	
   
82.4	
   
83.0	
   
82.3	
   
82.7	
   
-     -     	
   
83.6	
   

recursive autoencoders for full 
sentence paraphrase detection 

139	
   

recursive deep learning 
1.    mogvagon	
   
2.    recursive	
   neural	
   networks	
   for	
   parsing	
   	
   
3.    theory:	
   backpropagagon	
   through	
   structure	
   
4.    composigonal	
   vector	
   grammars:
5.    recursive	
   autoencoders:	
   
	
   
6.    matrix-     vector	
   id56s:	
   
	
   
	
   
7.    recursive	
   neural	
   tensor	
   networks:	
   

	
   parsing	
   
	
   paraphrase	
   detecgon	
   
	
   relagon	
   classi   cagon	
   
	
   sengment	
   analysis	
   

140	
   

compositionality through recursive 
matrix-vector spaces 

p	
   	
   =	
   	
   tanh(w	
   	
   	
   	
   	
   	
   	
   +	
   b)	
   
	
   

c1	
   
c2	
   

       one	
   way	
   to	
   make	
   the	
   composigon	
   funcgon	
   more	
   powerful	
   	
   
	
   	
   	
   	
   	
   was	
   by	
   untying	
   the	
   weights	
   w	
   
       but	
   what	
   if	
   words	
   act	
   mostly	
   as	
   an	
   operator,	
   e.g.	
      very   	
   in	
   

	
   

	
   

	
   very	
   good	
   
       proposal:	
   a	
   new	
   composigon	
   funcgon	
   

	
   

141	
   

compositionality through recursive 
matrix-vector id56s 

p	
   	
   =	
   	
   tanh(w	
   	
   	
   	
   	
   	
   	
   +	
   b)	
   
	
   

c1	
   
c2	
   

p	
   	
   =	
   	
   tanh(w	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   +	
   b)	
   
	
   

c2c1	
   
c1c2	
   

142	
   

predicting sentiment distributions 
       good	
   example	
   for	
   non-     linearity	
   in	
   language	
   

143	
   

mv-id56 for relationship classification 

rela@onship	
   	
   

sentence	
   with	
   labeled	
   nouns	
   for	
   which	
   
to	
   predict	
   rela@onships	
   

cause-     
e   ect(e2,e1)	
   
	
   
engty-     
origin(e1,e2)	
   

message-     
topic(e2,e1)	
   
	
   
144	
   

avian	
   [in   uenza]e1	
   is	
   an	
   infecgous	
   
disease	
   caused	
   by	
   type	
   a	
   strains	
   of	
   the	
   
in   uenza	
   [virus]e2.	
   
the	
   [mother]e1	
   le^	
   her	
   nagve	
   [land]e2	
   
about	
   the	
   same	
   gme	
   and	
   they	
   were	
   
married	
   in	
   that	
   city.	
   
roadside	
   [a0racgons]e1	
   are	
   frequently	
   
advergsed	
   with	
   [billboards]e2	
   to	
   a0ract	
   
tourists.	
   

sentiment detection 
       sengment	
   detecgon	
   is	
   crucial	
   to	
   business	
   
intelligence,	
   stock	
   	
   trading,	
      	
   

145	
   

sentiment detection and bag-of-words 
models 
       most	
   methods	
   start	
   with	
   a	
   bag	
   of	
   words	
   
+	
   linguisgc	
   features/processing/lexica	
   

       but	
   such	
   methods	
   (including	
   (cid:136)-     idf)	
   can   t	
   
disgnguish:	
   
+	
   white	
   blood	
   cells	
   destroying	
   an	
   infecgon	
   
-     	
   an	
   infecgon	
   destroying	
   white	
   blood	
   cells	
   

146	
   

sentiment detection and bag-of-words 
models 
       sengment	
   is	
   that	
   sengment	
   is	
      easy   	
   
       detecgon	
   accuracy	
   for	
   longer	
   documents	
   ~90%	
   
       lots	
   of	
   easy	
   cases	
   (   	
   horrible   	
   	
   or	
      	
   awesome	
      )	
   
       for	
   dataset	
   of	
   single	
   sentence	
   movie	
   reviews	
   
(pang	
   and	
   lee,	
   2005)	
   	
   accuracy	
   never	
   reached	
   
above	
   80%	
   for	
   >7	
   years	
   

	
   

       harder	
   cases	
   require	
   actual	
   understanding	
   of	
   

negagon	
   and	
   its	
   scope	
   and	
   other	
   semangc	
   e   ects	
   

data: movie reviews 
stealing	
   harvard	
   doesn't	
   care	
   about	
   
cleverness,	
   wit	
   or	
   any	
   other	
   kind	
   of	
   
intelligent	
   humor.	
   

there	
   are	
   slow	
   and	
   repeggve	
   parts	
   
but	
   it	
   has	
   just	
   enough	
   spice	
   to	
   keep	
   it	
   
interesgng.	
   

148	
   

two missing pieces for improving 
sentiment 

1.   	
   composigonal	
   training	
   data	
   

2.   	
   be0er	
   composigonal	
   model	
   

1. new sentiment treebank  

1. new sentiment treebank  
       parse	
   trees	
   of	
   11,855	
   sentences	
   
       215,154	
   phrases	
   with	
   labels	
   
       allows	
   training	
   and	
   evaluagng	
   	
   
with	
   composigonal	
   informagon	
   

2. new compositional model 
       recursive	
   neural	
   tensor	
   network	
   
       more	
   expressive	
   than	
   any	
   other	
   id56	
   so	
   far	
   
      

idea:	
   allow	
   more	
   interacgons	
   of	
   vectors	
   

2. new compositional model 
       recursive	
   neural	
   tensor	
   network	
   

2. new compositional model 
       recursive	
   neural	
   tensor	
   network	
   

recursive neural tensor network 

experimental result on treebank 

experimental result on treebank 
       rntn	
   can	
   capture	
   x	
   but	
   y	
   
       rntn	
   accuracy	
   of	
   72%,	
   compared	
   to	
   mv-     id56	
   (65),	
   

binb	
   (58)	
   and	
   id56	
   (54)	
   

negation results 

negation results 
       most	
   methods	
   capture	
   that	
   negagon	
   o^en	
   makes	
   

things	
   more	
   negagve	
   (see	
   po0s,	
   2010)	
   

       analysis	
   on	
   negagon	
   dataset	
   

negation results 
       but	
   how	
   about	
   negagng	
   negagves?	
   
       posigve	
   acgvagon	
   should	
   increase!	
   

visualizing deep learning: word 
embeddings 

overview of id56 model variations 

       objecgve	
   funcgons	
   

       supervised	
   scores	
   for	
   structure	
   predic@on	
   
       classi   er	
   for	
   sen@ment,	
   rela@ons,	
   visual	
   objects,	
   logic	
   
       unsupervised	
   autoencoding	
   immediate	
   children	
   or	
   engre	
   tree	
   structure	
   

       composigon	
   funcgons	
   

       syntac@cally-     un@ed	
   weights	
   
       matrix	
   vector	
   id56	
   
       tensor-     based	
   models	
   

       tree	
   structures	
   

       cons@tuency	
   parse	
   trees	
   
       combinatory	
   categorical	
   grammar	
   trees	
   	
   
       dependency	
   parse	
   trees	
   
       fixed	
   tree	
   structures	
   (connecgons	
   to	
   id98s)	
   

162	
   

summary: recursive deep learning 

       recursive	
   deep	
   learning	
   can	
   predict	
   hierarchical	
   structure	
   and	
   classify	
   the	
   
       state-     of-     the-     art	
   performance	
   (all	
   with	
   code	
   on	
   www.socher.org)	
   

structured	
   output	
   using	
   composigonal	
   vectors	
   
       parsing	
   on	
   the	
   wsj	
   (java	
   code	
   soon)	
   
       sen@ment	
   analysis	
   on	
   mulgple	
   corpora	
   
       paraphrase	
   detec@on	
   with	
   unsupervised	
   id56s	
   
       rela@on	
   classi   ca@on	
   on	
   semeval	
   2011,	
   task8	
   
       object	
   detec@on	
   on	
   stanford	
   background	
   and	
   msrc	
   datasets	
   

parsing	
   natural	
   scene	
   images
parsing	
   natural	
   scene	
   images

grass

people building

tree

semantic	
   	
   
representations
features
segments

parsing	
   natural	
   language	
   sentences
parsing	
   natural	
   language	
   sentences

s

vp

vp

np

np

a	
   small	
   
crowd

quietly	
   
enters

det.

adj.

np

n.

the

historic

church

a	
   small	
   crowd	
   
quietly	
   enters	
   
the	
   historic	
   

church

semantic	
   	
   
representations
indices
words

163	
   

part 3 
1.    assorted	
   speech	
   and	
   nlp	
   applicagons	
   
2.    deep	
   learning:	
   general	
   strategy	
   and	
   tricks	
   
3.    resources	
   (readings,	
   code,	
      )	
   
4.    discussion	
   

164	
   

part	
   3.1:	
   applicagons	
   

assorted speech and nlp 
applications 

165	
   

existing nlp applications 
       language	
   modeling	
   (speech	
   recognigon,	
   machine	
   translagon)	
   
       word-     sense	
   learning	
   and	
   disambiguagon	
   
       reasoning	
   over	
   knowledge	
   bases	
   
       acousgc	
   modeling	
   
       part-     of-     speech	
   tagging	
   
       chunking	
   
       named	
   engty	
   recognigon	
   
       semangc	
   role	
   labeling	
   
       parsing	
   
       sengment	
   analysis	
   
       id141	
   
       quesgon-     answering	
   

166	
   

id38 

       predict	
   p(next	
   word	
   |	
   previous	
   word)	
   
       gives	
   a	
   id203	
   for	
   a	
   longer	
   sequence	
   
       applicagons	
   to	
   speech,	
   translagon	
   and	
   compression	
   
       computagonal	
   bo0leneck:	
   large	
   vocabulary	
   v	
   means	
   that	
   

compugng	
   the	
   output	
   costs	
   #hidden	
   units	
   x	
   |v|.	
   

167	
   

neural language model 
       bengio	
   et	
   al	
   nips   2000	
   

and	
   jmlr	
   2003	
      a	
   
neural	
   probabilis<c	
   
language	
   model   	
   
       each	
   word	
   represented	
   by	
   
a	
   distributed	
   congnuous-     
valued	
   code	
   

       generalizes	
   to	
   sequences	
   

of	
   words	
   that	
   are	
   
semangcally	
   similar	
   to	
   
training	
   sequences	
   

168	
   

recurrent neural net language 
modeling for asr 

	
   

      

	
   

	
   

[mikolov	
   et	
   al	
   2011]	
   
	
   bigger	
   is	
   be0er   	
   
	
   experiments	
   on	
   broadcast	
   
	
   news	
   nist-     rt04	
   

	
   perplexity	
   goes	
   from	
   
	
   140	
   to	
   102	
   

	
   paper	
   shows	
   how	
   to	
   
	
   train	
   a	
   recurrent	
   neural	
   net	
   
	
   with	
   a	
   single	
   core	
   in	
   a	
   few	
   
	
   days,	
   with	
   >	
   1%	
   absolute	
   
	
   improvement	
   in	
   wer	
   	
   

	
   
code:	
   http://www.fit.vutbr.cz/~imikolov/id56lm/!

169	
   
	
   

	
   	
   

application to statistical machine 
translation 

       schwenk	
   (naacl	
   2012	
   workshop	
   on	
   the	
   future	
   of	
   lm)	
   

       	
   41m	
   words,	
   arabic/english	
   bitexts	
   +	
   151m	
   english	
   from	
   ldc	
   

       perplexity	
   down	
   from	
   	
   71.1	
   (6	
   gig	
   back-     o   )	
   to	
   56.9	
   (neural	
   

model,	
   500m	
   memory)	
   

       +1.8	
   id7	
   score	
   (50.75	
   to	
   52.28)	
   
       can	
   take	
   advantage	
   of	
   longer	
   contexts	
   

       code:	
   http://lium.univ-lemans.fr/cslm/!

170	
   

learning multiple word vectors 
       tackles	
   problems	
   with	
   polysemous	
   words	
   

       can	
   be	
   done	
   with	
   both	
   standard	
   (cid:136)-     idf	
   based	
   	
   
methods	
   [reisinger	
   and	
   mooney,	
   naacl	
   2010]	
   

       recent	
   neural	
   word	
   vector	
   model	
   by	
   [huang	
   et	
   al.	
   acl	
   2012]	
   
learns	
   mulgple	
   prototypes	
   using	
   both	
   local	
   and	
   global	
   context	
   	
   

       state	
   of	
   the	
   art	
   	
   
correlagons	
   with	
   	
   
human	
   similarity	
   	
   
judgments	
   

171	
   

learning multiple word vectors 
       visualizagon	
   of	
   learned	
   word	
   vectors	
   from	
   	
   

huang	
   et	
   al.	
   (acl	
   2012)	
   
	
   

172	
   

common sense reasoning  
inside knowledge bases 
       quesgon:	
   can	
   neural	
   networks	
   learn	
   to	
   capture	
   logical	
   

id136,	
   set	
   inclusions,	
   part-     of	
   and	
   hypernym	
   relagonships?	
   

173	
   

neural networks for reasoning  
over relationships 
       higher	
   scores	
   for	
   each	
   	
   

triplet	
   t	
   =	
   (e1,r,e2)	
   	
   
indicate	
   that	
   engges	
   are	
   
more	
   likely	
   in	
   relagonship	
   
       training	
   uses	
   contrasgve	
   	
   

esgmagon	
   funcgon,	
   similar	
   	
   
to	
   word	
   vector	
   learning	
   

       ntn	
   scoring	
   funcgon:	
   

       cost:	
   

174	
   

	
   

accuracy of predicting true and false 
relationships 

       related	
   work	
   
       bordes,	
   weston,	
   

      

collobert	
   &	
   bengio,	
   	
   
aaai	
   2011)	
   
(bordes,	
   glorot,	
   
weston	
   &	
   bengio,	
   
aistats	
   2012)	
   

model	
   
distance	
   model	
   
hadamard	
   model	
   

standard	
   layer	
   model	
   (<ntn)	
   
bilinear	
   model	
   (<ntn)	
   
175	
   
neural	
   tensor	
   network	
   (chen	
   et	
   al.	
   2013)	
   

freebase	
    id138	
   

68.3	
   
80.0	
   

76.0	
   
84.1	
   
86.2	
   

61.0	
   
68.8	
   

85.3	
   
87.7	
   
90.0	
   

accuracy per relationship 

176	
   

part	
   3.2	
   

deep learning  
general strategy and tricks 

177	
   

general strategy 
1.    select	
   network	
   structure	
   appropriate	
   for	
   problem	
   
1.    structure:	
   single	
   words,	
      xed	
   windows	
   vs	
   recursive	
   

sentence	
   based	
   vs	
   bag	
   of	
   words	
   

2.    nonlinearity	
   

2.    check	
   for	
   implementagon	
   bugs	
   with	
   gradient	
   checks	
   
3.    parameter	
   inigalizagon	
   
4.    opgmizagon	
   tricks	
   
5.    check	
   if	
   the	
   model	
   is	
   powerful	
   enough	
   to	
   over   t	
   

if	
   not,	
   change	
   model	
   structure	
   or	
   make	
   model	
      larger   	
   
if	
   you	
   can	
   over   t:	
   regularize	
   

1.   
2.   

178	
   

non-linearities: what   s used 

	
   logisgc	
   (   sigmoid   )	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   tanh	
   

	
   
	
   
	
   
	
   
	
   
	
   
	
   
tanh	
   is	
   just	
   a	
   rescaled	
   and	
   shi^ed	
   sigmoid	
   
tanh	
   is	
   what	
   is	
   most	
   used	
   and	
   o^en	
   performs	
   best	
   for	
   deep	
   nets	
   
	
   
179	
   

tanh(z) = 2logistic(2z)    1

non-linearities: there are various 
other choices 

hard	
   tanh	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   so^	
   sign	
   
softsign(z) =
	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   recg   er	
   
rect(z) = max(z,0)

a
1+ a

	
   

	
   
	
   
       hard	
   tanh	
   similar	
   but	
   computagonally	
   cheaper	
   than	
   tanh	
   and	
   saturates	
   hard.	
   
      

[glorot	
   and	
   bengio	
   aistats	
   2010,	
   2011]	
   discuss	
   so^sign	
   and	
   recg   er	
   

180	
   

maxout network 
       a	
   very	
   recent	
   type	
   of	
   nonlinearity/network	
   
       goodfellow	
   et	
   al.	
   (2013)	
   

       where	
   	
   

       this	
   funcgon	
   too	
   is	
   a	
   universal	
   approximator	
   
       state	
   of	
   the	
   art	
   on	
   several	
   image	
   datasets	
   

181	
   

  ,  

gradient checks are awesome! 
       allows	
   you	
   to	
   know	
   that	
   there	
   are	
   no	
   bugs	
   in	
   your	
   neural	
   

network	
   implementagon!	
   

       steps:	
   

1.   
2.   

implement	
   your	
   gradient	
   
implement	
   a	
      nite	
   di   erence	
   computagon	
   by	
   looping	
   
through	
   the	
   parameters	
   of	
   your	
   network,	
   adding	
   and	
   
subtracgng	
   a	
   small	
   epsilon	
   (~10^-     4)	
   and	
   esgmate	
   derivagves	
   

3.    compare	
   the	
   two	
   and	
   make	
   sure	
   they	
   are	
   the	
   same	
   

182	
   

general strategy 
1.    select	
   appropriate	
   network	
   structure	
   
1.    structure:	
   single	
   words,	
      xed	
   windows	
   vs	
   recursive	
   

sentence	
   based	
   vs	
   bag	
   of	
   words	
   

2.    nonlinearity	
   

2.    check	
   for	
   implementagon	
   bugs	
   with	
   gradient	
   check	
   
3.    parameter	
   inigalizagon	
   
4.    opgmizagon	
   tricks	
   
5.    check	
   if	
   the	
   model	
   is	
   powerful	
   enough	
   to	
   over   t	
   

if	
   not,	
   change	
   model	
   structure	
   or	
   make	
   model	
      larger   	
   
if	
   you	
   can	
   over   t:	
   regularize	
   

1.   
2.   

183	
   

parameter initialization 
      

inigalize	
   hidden	
   layer	
   biases	
   to	
   0	
   and	
   output	
   (or	
   reconstrucgon)	
   
biases	
   to	
   opgmal	
   value	
   if	
   weights	
   were	
   0	
   (e.g.	
   mean	
   target	
   or	
   
inverse	
   sigmoid	
   of	
   mean	
   target).	
   
inigalize	
   weights	
   ~	
   uniform(-     r,r),	
   r	
   inversely	
   proporgonal	
   to	
   fan-     
in	
   (previous	
   layer	
   size)	
   and	
   fan-     out	
   (next	
   layer	
   size):	
   

      

	
   	
   	
   	
   	
   for	
   tanh	
   units,	
   and	
   4x	
   bigger	
   for	
   sigmoid	
   units	
   [glorot	
   aistats	
   2010]	
   
	
   
       pre-     training	
   with	
   restricted	
   boltzmann	
   machines	
   

184	
   

stochastic id119 (sgd) 
       gradient	
   descent	
   uses	
   total	
   gradient	
   over	
   all	
   examples	
   per	
   

update,	
   sgd	
   updates	
   a^er	
   only	
   1	
   or	
   few	
   examples:	
   

  t	
   =	
   learning	
   rate.	
   

       l	
   =	
   loss	
   funcgon,	
   zt	
   =	
   current	
   example,	
     	
   =	
   parameter	
   vector,	
   and	
   
       ordinary	
   gradient	
   descent	
   as	
   a	
   batch	
   method,	
   very	
   slow,	
   should	
   
never	
   be	
   used.	
   use	
   2nd	
   order	
   batch	
   method	
   such	
   as	
   lbfgs.	
   on	
   
large	
   datasets,	
   sgd	
   usually	
   wins	
   over	
   all	
   batch	
   methods.	
   on	
   
smaller	
   datasets	
   lbfgs	
   or	
   conjugate	
   gradients	
   win.	
   large-     batch	
   
lbfgs	
   extends	
   the	
   reach	
   of	
   lbfgs	
   [le	
   et	
   al	
   icml   2011].	
   

185	
   

learning rates 
       simplest	
   recipe:	
   keep	
   it	
      xed	
   and	
   use	
   the	
   same	
   for	
   all	
   

parameters.	
   

       collobert	
   scales	
   them	
   by	
   the	
   inverse	
   of	
   square	
   root	
   of	
   the	
   fan-     in	
   

of	
   each	
   neuron	
   

       be0er	
   results	
   can	
   generally	
   be	
   obtained	
   by	
   allowing	
   learning	
   
rates	
   to	
   decrease,	
   typically	
   in	
   o(1/t)	
   because	
   of	
   theoregcal	
   
convergence	
   guarantees,	
   e.g.,	
   
with	
   hyper-     parameters	
     0	
   and	
     	
   

       be0er	
   yet:	
   no	
   learning	
   rates	
   by	
   using	
   l-     bfgs	
   or	
   adagrad	
   (duchi	
   

et	
   al.	
   2011)	
   

	
   
186	
   

long-term dependencies  
and clipping trick 

  

      

in	
   very	
   deep	
   networks	
   such	
   as	
   recurrent	
   networks	
   (or	
   possibly	
   
recursive	
   ones),	
   the	
   gradient	
   is	
   a	
   product	
   of	
   jacobian	
   matrices,	
   
each	
   associated	
   with	
   a	
   step	
   in	
   the	
   forward	
   computagon.	
   this	
   
can	
   become	
   very	
   small	
   or	
   very	
   large	
   quickly	
   [bengio	
   et	
   al	
   1994],	
   
and	
   the	
   locality	
   assumpgon	
   of	
   gradient	
   descent	
   breaks	
   down.	
   	
   

       the	
   solugon	
      rst	
   introduced	
   by	
   mikolov	
   	
   is	
   to	
   clip	
   gradients	
   

to	
   a	
   maximum	
   value.	
   makes	
   a	
   big	
   di   erence	
   in	
   id56s	
   

	
   187	
   

general strategy 

1.   

2.   
3.   
4.   
5.   

structure:	
   single	
   words,	
      xed	
   windows	
   vs	
   recursive	
   sentence	
   based	
   vs	
   bag	
   of	
   words	
   
nonlinearity	
   

select	
   appropriate	
   network	
   structure	
   
1.   
2.   
check	
   for	
   implementagon	
   bugs	
   with	
   gradient	
   check	
   
parameter	
   inigalizagon	
   
opgmizagon	
   tricks	
   
check	
   if	
   the	
   model	
   is	
   powerful	
   enough	
   to	
   over   t	
   
1.   
2.   

if	
   not,	
   change	
   model	
   structure	
   or	
   make	
   model	
      larger   	
   
if	
   you	
   can	
   over   t:	
   regularize	
   

	
   
assuming	
   you	
   found	
   the	
   right	
   network	
   structure,	
   implemented	
   it	
   
correctly,	
   opgmize	
   it	
   properly	
   and	
   you	
   can	
   make	
   your	
   model	
   
over   t	
   on	
   your	
   training	
   data.	
   
	
   
now,	
   it   s	
   gme	
   to	
   regularize	
   

188	
   

prevent overfitting:  
model size and id173 
       simple	
      rst	
   step:	
   reduce	
   model	
   size	
   by	
   lower	
   number	
   of	
   units	
   

and	
   layers	
   and	
   other	
   parameters	
   

       standard	
   l1	
   or	
   l2	
   regularizagon	
   on	
   weights	
   	
   
       early	
   stopping:	
   use	
   parameters	
   that	
   gave	
   best	
   validagon	
   error	
   
       sparsity	
   constraints	
   on	
   hidden	
   acgvagons,	
   e.g.	
   add	
   to	
   cost:	
   	
   

       dropout	
   (hinton	
   et	
   al.	
   2012):	
   	
   

       randomly	
   set	
   50%	
   of	
   the	
   inputs	
   at	
   each	
   layer	
   to	
   0	
   
       at	
   test	
   gme	
   half	
   the	
   outgoing	
   weights	
   (now	
   twice	
   as	
   many)	
   
       prevents	
   co-     adaptagon	
   

189	
   

deep learning tricks of the trade 
       y.	
   bengio	
   (2012),	
      pracgcal	
   recommendagons	
   for	
   gradient-     

based	
   training	
   of	
   deep	
   architectures   	
   	
   
       unsupervised	
   pre-     training	
   
       stochasgc	
   gradient	
   descent	
   and	
   se(cid:134)ng	
   learning	
   rates	
   
       main	
   hyper-     parameters	
   

       learning	
   rate	
   schedule	
   &	
   early	
   stopping	
   	
   
       minibatches	
   
       parameter	
   inigalizagon	
   
       number	
   of	
   hidden	
   units	
   
       l1	
   or	
   l2	
   weight	
   decay	
   
       sparsity	
   regularizagon	
   

       debugging	
        	
   finite	
   di   erence	
   gradient	
   check	
   (yay)	
   
       how	
   to	
   e   ciently	
   search	
   for	
   hyper-     parameter	
   con   guragons	
   

190	
   

part	
   3.3:	
   resources	
   

resources: tutorials and code 

191	
   

related tutorials 
       see	
      neural	
   net	
   language	
   models   	
   scholarpedia	
   entry	
   
       deep	
   learning	
   tutorials:	
   h0p://deeplearning.net/tutorials	
   
       stanford	
   deep	
   learning	
   tutorials	
   with	
   simple	
   programming	
   

assignments	
   and	
   reading	
   list	
   h0p://deeplearning.stanford.edu/wiki/	
   

h0p://cseweb.ucsd.edu/~elkan/250b/learningmeaning.pdf	
   

       recursive	
   autoencoder	
   class	
   project	
   
       graduate	
   summer	
   school:	
   deep	
   learning,	
   feature	
   learning	
   
      
       more	
   reading	
   (including	
   tutorial	
   references):	
   
hjp://nlp.stanford.edu/courses/naacl2013/	
   	
   

h0p://www.ipam.ucla.edu/programs/gss2012/	
   
icml	
   2012	
   representagon	
   learning	
   tutorial	
   h0p://
www.iro.umontreal.ca/~bengioy/talks/deep-     learning-     tutorial-     2012.html	
   

192	
   

software 
      

      

theano	
   (python	
   cpu/gpu)	
   mathemagcal	
   and	
   deep	
   learning	
   
library	
   h0p://deeplearning.net/so^ware/theano	
   
       can	
   do	
   automagc,	
   symbolic	
   di   erengagon	
   
senna:	
   pos,	
   chunking,	
   ner,	
   srl	
   
       by	
   collobert	
   et	
   al.	
   h0p://ronan.collobert.com/senna/	
   
      
       3500	
   lines	
   of	
   c,	
   extremely	
   fast	
   and	
   using	
   very	
   li0le	
   memory	
   

state-     of-     the-     art	
   performance	
   on	
   many	
   tasks	
   

       recurrent	
   neural	
   network	
   language	
   model	
   
h0p://www.   t.vutbr.cz/~imikolov/id56lm/	
   
       recursive	
   neural	
   net	
   and	
   rae	
   models	
   for	
   paraphrase	
   detecgon,	
   

sengment	
   analysis,	
   relagon	
   classi   cagon	
   www.socher.org	
   

193	
   

software: what   s next 
       o   -     the-     shelf	
   id166	
   packages	
   are	
   useful	
   to	
   researchers	
   
from	
   a	
   wide	
   variety	
   of	
      elds	
   (no	
   need	
   to	
   understand	
   
rkhs).	
   

       one	
   of	
   the	
   goals	
   of	
   deep	
   learning:	
   build	
   o   -     the-     shelf	
   
nlp	
   classi   cagon	
   packages	
   that	
   are	
   using	
   as	
   training	
   
input	
   only	
   raw	
   text	
   (instead	
   of	
   features)	
   possibly	
   with	
   a	
   
label.	
   

194	
   

part	
   3.4:	
   

discussion 

195	
   

concerns 
       many	
   algorithms	
   and	
   variants	
   (burgeoning	
      eld)	
   

       hyper-     parameters	
   (layer	
   size,	
   regularizagon,	
   possibly	
   

learning	
   rate)	
   
       use	
   mulg-     core	
   machines,	
   clusters	
   and	
   random	
   
sampling	
   for	
   cross-     validagon	
   (bergstra	
   &	
   bengio	
   2012)	
   
       pre0y	
   common	
   for	
   powerful	
   methods,	
   e.g.	
   bm25,	
   lda	
   
       can	
   use	
   (mini-     batch)	
   l-     bfgs	
   instead	
   of	
   sgd	
   

196	
   

concerns 
       not	
   always	
   obvious	
   how	
   to	
   combine	
   with	
   exisgng	
   nlp	
   	
   
       simple:	
   add	
   word	
   or	
   phrase	
   vectors	
   as	
   features.	
   gets	
   
close	
   to	
   state	
   of	
   the	
   art	
   for	
   ner,	
   [turian	
   et	
   al,	
   acl	
   
2010]	
   
       integrate	
   with	
   known	
   problem	
   structures:	
   recursive	
   
and	
   recurrent	
   networks	
   for	
   trees	
   and	
   chains	
   
       your	
   research	
   here	
   	
   

197	
   

concerns 
       slower	
   to	
   train	
   than	
   linear	
   models	
   	
   

       only	
   by	
   a	
   small	
   constant	
   factor,	
   and	
   much	
   more	
   
compact	
   than	
   non-     parametric	
   (e.g.	
   n-     gram	
   models)	
   	
   
       very	
   fast	
   during	
   id136/test	
   gme	
   (feed-     forward	
   
pass	
   is	
   just	
   a	
   few	
   matrix	
   mulgplies)	
   

       need	
   more	
   training	
   data	
   

       can	
   handle	
   and	
   bene   t	
   from	
   more	
   training	
   data,	
   
suitable	
   for	
   age	
   of	
   big	
   data	
   (google	
   trains	
   neural	
   
nets	
   with	
   a	
   billion	
   connecgons,	
   [le	
   et	
   al,	
   icml	
   2012])	
   

198	
   

concerns 
       there	
   aren   t	
   many	
   good	
   ways	
   to	
   encode	
   prior	
   

knowledge	
   about	
   the	
   structure	
   of	
   language	
   into	
   deep	
   
learning	
   models	
   
       there	
   is	
   some	
   truth	
   to	
   this.	
   however:	
   
       you	
   can	
   choose	
   architectures	
   suitable	
   for	
   a	
   problem	
   
domain,	
   as	
   we	
   did	
   for	
   linguisgc	
   structure	
   
       you	
   can	
   include	
   human-     designed	
   features	
   in	
   the	
      rst	
   
layer,	
   just	
   like	
   for	
   a	
   linear	
   model	
   
       and	
   the	
   goal	
   is	
   to	
   get	
   the	
   machine	
   doing	
   the	
   learning!	
   

199	
   

concern: 
problems with model interpretability 
       no	
   discrete	
   categories	
   or	
   words,	
   everything	
   is	
   a	
   congnuous	
   
vector.	
   we   d	
   like	
   have	
   symbolic	
   features	
   like	
   np,	
   vp,	
   etc.	
   and	
   
see	
   why	
   their	
   combinagon	
   makes	
   sense.	
   	
   
       true,	
   but	
   most	
   of	
   language	
   is	
   fuzzy	
   and	
   many	
   words	
   have	
   so^	
   
relagonships	
   to	
   each	
   other.	
   also,	
   many	
   nlp	
   features	
   are	
   
already	
   not	
   human-     understandable	
   (e.g.,	
   concatenagons/
combinagons	
   of	
   di   erent	
   features).	
   	
   
       can	
   try	
   by	
   projecgons	
   of	
   weights	
   and	
   nearest	
   neighbors,	
   see	
   
part	
   2	
   

200	
   

concern: non-id76 
       can	
   inigalize	
   system	
   with	
   convex	
   learner	
   

       convex	
   id166	
   
       fixed	
   feature	
   space	
   

       then	
   opgmize	
   non-     convex	
   variant	
   (add	
   and	
   tune	
   learned	
   

features),	
   can   t	
   be	
   worse	
   than	
   convex	
   learner	
   

       not	
   a	
   big	
   problem	
   in	
   pracgce	
   (o^en	
   relagvely	
   stable	
   

performance	
   across	
   di   erent	
   local	
   opgma)	
   

	
   

201	
   

advantages 
       despite	
   a	
   small	
   community	
   in	
   the	
   intersecgon	
   of	
   deep	
   
learning	
   and	
   nlp,	
   already	
   many	
   state	
   of	
   the	
   art	
   results	
   
on	
   a	
   variety	
   of	
   language	
   tasks	
   

       o^en	
   very	
   simple	
   matrix	
   derivagves	
   (backprop)	
   for	
   
training	
   and	
   matrix	
   mulgplicagons	
   for	
   tesgng	
        	
   fast	
   
implementagon	
   

       fast	
   id136	
   and	
   well	
   suited	
   for	
   mulg-     core	
   cpus/gpus	
   

and	
   parallelizagon	
   across	
   machines	
   

202	
   

learning multiple levels of 
abstraction 
       the	
   big	
   payo   	
   of	
   deep	
   learning	
   

is	
   to	
   learn	
   feature	
   
representagons	
   and	
   higher	
   
levels	
   of	
   abstracgon	
   

       this	
   allows	
   much	
   easier	
   

generalizagon	
   and	
   transfer	
   
between	
   domains,	
   languages,	
   
and	
   tasks	
   

203	
   

the end 

204	
   

