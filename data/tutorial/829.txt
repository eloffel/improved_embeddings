dependency grammar and id33

joakim nivre

1 introduction

despite a long and venerable tradition in descriptive linguistics, dependency gram-
mar has until recently played a fairly marginal role both in theoretical linguistics
and in natural language processing. the increasing interest in dependency-based
representations in natural language parsing in recent years appears to be motivated
both by the potential usefulness of bilexical relations in disambiguation and by the
gains in ef   ciency that result from the more constrained parsing problem for these
representations.

in this paper, we will review the state of the art in dependency-based parsing,
starting with the theoretical foundations of dependency grammar and moving on
to consider both grammar-driven and data-driven methods for id33.
we will limit our attention to systems for id33 in a narrow sense,
i.e. systems where the analysis assigned to an input sentence takes the form of a
dependency structure. this means that we will not discuss systems that exploit
dependency relations for the construction of another type of representation, such
as the head-driven parsing models of collins (1997, 1999). moreover, we will
restrict ourselves to systems for full parsing, which means that we will not deal
with systems that produce a partial or underspeci   ed representation of dependency
structure, such as constraint grammar parsers (karlsson, 1990; karlsson et al.,
1995).

2 dependency grammar

although its roots may be traced back to p  an. ini   s grammar of sanskrit several cen-
turies before the common era (kruijff, 2002) and to medieval theories of grammar
(covington, 1984), dependency grammar has largely developed as a form for syn-
tactic representation used by traditional grammarians, especially in europe, and
particularly in classical and slavic domains (mel     cuk, 1988). this grammatical
tradition can be said to culminate with the seminal work of tesni`ere (1959), which

1

s
  
qq

 

 

 

 

 

 

 

 

"

 

 

 

 

 
np
  
hh
jj

nn

 

"
 
np
hh
  
nn
jj

 
vbd

q

q

q

q

"

q
vp
hh
 
np
hh
"
pp
hh
 
np
hh
  
jj

 
in

 

q

"

nns

q

q

q

q
pu

economic

news

had

little

effect

on

   nancial

markets

.

figure 1: constituent structure for english sentence from the id32

 

 

nmod

sbj

 

 

   

?
jj

economic

?
nn
news

vbd
had

p

 

obj

pmod

 

 

nmod

nmod

 

nmod

 

?
jj
little

 

   

 

 

?
nn
effect

?
in
on

?
jj

   nancial

?
nns

markets

?
pu
.

figure 2: dependency structure for english sentence from the id32

is usually taken as the starting point of the modern theoretical tradition of depen-
dency grammar.

this tradition comprises a large and fairly diverse family of grammatical theo-
ries and formalisms that share certain basic assumptions about syntactic structure,
in particular the assumption that syntactic structure consists of lexical elements
linked by binary asymmetrical relations called dependencies. thus, the common
formal property of dependency structures, as compared to representations based on
constituency is the lack of phrasal nodes. this can be seen by comparing the con-
stituency representation of an english sentence in figure 1, taken from the wall
street journal section of the id32 (marcus et al., 1993, 1994), to the
corresponding dependency representation in figure 2.

among the more well-known theories of dependency grammar, besides the
theory of structural syntax developed by tesni`ere (1959), we    nd word gram-

2

mar (wg) (hudson, 1984, 1990), functional generative description (fgd) (sgall
et al., 1986), dependency uni   cation grammar (dug) (hellwig, 1986, 2003),
meaning-text theory (mtt) (mel     cuk, 1988), and lexicase (starosta, 1988). in
addition, constraint-based theories of dependency grammar have a strong tradition,
represented by constraint dependency grammar (cdg) (maruyama, 1990; harper
and helzerman, 1995; menzel and schr  oder, 1998) and its descendant weighted
constraint dependency grammar (wcdg) (schr  oder, 2002), functional depen-
dency grammar (fdg) (tapanainen and j  arvinen, 1997; j  arvinen and tapanainen,
1998), largely developed from constraint grammar (cg) (karlsson, 1990; karls-
son et al., 1995), and    nally topological dependency grammar (tdg) (duchier
and debusmann, 2001), subsequently evolved into extensible dependency gram-
mar (xdg) (debusmann et al., 2004). a synthesis of dependency grammar and
categorial grammar is found in the framework of dependency grammar logic
(dgl) (kruijff, 2001).

we will make no attempt at reviewing all these theories here. instead, we will
try to characterize their common core of assumptions, centered upon the notion of
dependency, and discuss major points of divergence, such as the issue of projective
versus non-projective representations.

2.1 the notion of dependency

the fundamental notion of dependency is based on the idea that the syntactic struc-
ture of a sentence consists of binary asymmetrical relations between the words of
the sentence. the idea is expressed in the following way in the opening chapters
of tesni`ere (1959):

la phrase est un ensemble organis  e dont les   el  ements constituants sont
les mots.
[1.2] tout mot qui fait partie d   une phrase cesse par lui-
m  eme d     etre isol  e comme dans le dictionnaire. entre lui et ses voisins,
l   esprit aperc  oit des connexions, dont l   ensemble forme la charpente
de la phrase. [1.3] les connexions structurales   etablissent entre les
mots des rapports de d  ependance. chaque connexion unit en principe
un terme sup  erieur `a un terme inf  erieur.
[2.1] le terme sup  erieur
rec  oit le nom de r  egissant. le terme inf  erieur rec  oit le nom de subor-
donn  e. ainsi dans la phrase alfred parle [. . . ], parle est le r  egissant
et alfred le subordonn  e. [2.2] (tesni`ere, 1959, 11   13, emphasis in the
original)1

1english translation (by the author):    the sentence is an organized whole, the constituent ele-
ments of which are words. [1.2] every word that belongs to a sentence ceases by itself to be isolated
as in the dictionary. between the word and its neighbors, the mind perceives connections, the totality

3

in the terminology used in this paper, a dependency relation holds between a head
and a dependent. alternative terms in the literature are governor and regent for
head (cf. tesni`ere   s r  egissant) and modi   er for dependent (cf. tesni`ere   s subor-
donn  e).

criteria for establishing dependency relations, and for distinguishing the head
and the dependent in such relations, are clearly of central importance for depen-
dency grammar. such criteria have been discussed not only in the dependency
grammar tradition, but also within other frameworks where the notion of syntac-
tic head plays an important role, including all constituency-based frameworks that
subscribe to some version of x theory (chomsky, 1970; jackendoff, 1977). here
are some of the criteria that have been proposed for identifying a syntactic relation
between a head h and a dependent d in a construction c (zwicky, 1985; hudson,
1990):

1. h determines the syntactic category of c and can often replace c.

2. h determines the semantic category of c; d gives semantic speci   cation.

3. h is obligatory; d may be optional.

4. h selects d and determines whether d is obligatory or optional.

5. the form of d depends on h (agreement or government).

6. the linear position of d is speci   ed with reference to h.

it is clear that this list contains a mix of different criteria, some syntactic and
some semantic, and one may ask whether there is a single coherent notion of
dependency corresponding to all the different criteria. this has led some theo-
rists, such as hudson (1990), to suggest that the concept of head has a prototype
structure, i.e. that typical instances of this category satisfy all or most of the criteria
while more peripheral instances satisfy fewer. other authors have emphasized the
need to distinguish different kinds of dependency relations. according to mel     cuk
(1988), the word forms of a sentence can be linked by three types of dependencies:
morphological, syntactic and semantic. according to nikula (1986), we must dis-
tinguish between syntactic dependency in endocentric and exocentric constructions
(bloom   eld, 1933).

of which forms the structure of the sentence. [1.3] the structural connections establish dependency
relations between the words. each connection in principle unites a superior term and an inferior
term. [2.1] the superior term receives the name governor. the inferior term receives the name sub-
ordinate. thus, in the sentence alfred parle [. . . ], parle is the governor and alfred the subordinate.
[2.2]   

4

thus, in figure 2, the nmod relation holding between the noun markets and
the adjective    nancial is an endocentric construction, where the head can replace
the whole without disrupting the syntactic structure:

economic news had little effect on [   nancial] markets.

(1)

endocentric constructions may satisfy all the criteria listed above, although number
4 is usually considered less relevant, since dependents in endocentric constructions
are taken to be optional and not selected by their heads. by contrast, the pmod
relation holding between the preposition on and the noun markets is an exocentric
construction, where the head cannot readily replace the whole:

economic news had little effect on [markets].

(2)

exocentric constructions, by their de   nition, fail on criterion number 1, at least
with respect to subsitutability of the head for the whole, but they may satisfy the
remaining criteria. considering the rest of the relations exempli   ed in figure 2, the
sbj and obj relations are clearly exocentric, and the nmod relation from the noun
news to the adjective economic clearly endocentric, while the remaining nmod
relations (effect     little, effect     on) have a more unclear status.

the distinction between endocentric and exocentric constructions is also re-
lated to the distinction between head-complement and head-modi   er (or head-
adjunct) relations found in many contemporary syntactic theories, since head-
complement relations are exocentric while head-modi   er relations are endocentric.
many theories also recognize a third kind of relation, the head-speci   er relation,
typically exempli   ed by the determiner-noun relation, which is exocentric like the
head-complement relation, but where there is no clear selection of the dependent
element by the head.

the distinction between complements and modi   ers is often de   ned in terms
of valency, which is a central notion in the theoretical tradition of dependency
grammar. although the exact characterization of this notion differs from one theo-
retical framework to the other, valency is usually related to the semantic predicate-
argument structure associated with certain classes of lexemes, in particular verbs
but sometimes also nouns and adjectives. the idea is that the verb imposes re-
quirements on its syntactic dependents that re   ect its interpretation as a semantic
predicate. dependents that correspond to arguments of the predicate can be oblig-
atory or optional in surface syntax but can only occur once with each predicate
instance. by contrast, dependents that do not correspond to arguments can have
more than one occurrence with a single predicate instance and tend to be optional.
the valency frame of the verb is normally taken to include argument dependents,
but some theories also allow obligatory non-arguments to be included (sgall et al.,
1986).

5

the notion of valency will not play a central role in the present paper, but we
will sometimes use the terms valency-bound and valency-free to make a rough dis-
tinction between dependents that are more or less closely related to the semantic
interpretation of the head. returning to figure 2, the subject and the object would
normally be treated as valency-bound dependents of the verb had, while the adjec-
tival modi   ers of the nouns news and markets would be considered valency-free.
the prepositional modi   cation of the noun effect may or may not be treated as
valency-bound, depending on whether the entity undergoing the effect is supposed
to be an argument of the noun effect or not.

while head-complement and head-modi   er structures have a fairly straight-
forward analysis in dependency grammar, there are also many constructions that
have a relatively unclear status. this group includes constructions that involve
grammatical function words, such as articles, complementizers and auxiliary verbs,
but also structures involving prepositional phrases. for these constructions, there
is no general consensus in the tradition of dependency grammar as to whether they
should be analyzed as head-dependent relations at all and, if so, what should be
regarded as the head and what should be regarded as the dependent. for example,
some theories regard auxiliary verbs as heads taking lexical verbs as dependents;
other theories make the opposite assumption; and yet other theories assume that
verb chains are connected by relations that are not dependencies in the usual sense.
another kind of construction that is problematic for dependency grammar (as
for most theoretical traditions) is coordination. according to bloom   eld (1933),
coordination is an endocentric construction, since it contains not only one but sev-
eral heads that can replace the whole construction syntactically. however, this
characterization raises the question of whether coordination can be analyzed in
terms of binary asymmetrical relations holding between a head and a dependent.
again, this question has been answered in different ways by different theories
within the dependency grammar tradition.

in conclusion, the theoretical tradition of dependency grammar is united by the
assumption that an essential part of the syntactic structure of sentences resides in
binary asymmetrical relations holding between lexical elements. moreover, there
is a core of syntactic constructions for which the analysis given by different frame-
works agree in all important respects. however, there are also important differ-
ences with respect to whether dependency analysis is assumed to exhaust syntactic
analysis, and with respect to the analysis of certain types of syntactic construc-
tions. we will now turn to a discussion of some of the more important points of
divergence in this tradition.

6

2.2 varieties of dependency grammar

perhaps the most fundamental open question in the tradition of dependency gram-
mar is whether the notion of dependency is assumed to be not only necessary but
also suf   cient for the analysis of syntactic structure in natural language. this as-
sumption is not made in the theory of tesni`ere (1959), which is based on the three
complementary concepts of connection (connexion), junction (jonction) and trans-
fer (translation), where connection corresponds to dependency (cf. the quotation on
page 3) but where junction and transfer are other kinds of relations that can hold
between the words of a sentence. more precisely, junction is the relation that holds
between coordinated items that are dependents of the same head or heads of the
same dependent, while transfer is the relation that holds between a function word
or other element that changes the syntactic category of a lexical element so that it
can enter into different dependency relations. an example of the latter is the rela-
tion holding between the preposition de and pierre in the construction le livre de
pierre (pierre   s book), where the preposition de allows the proper name pierre to
modify a noun, a dependency relation otherwise reserved for adjectives. another
way in which theories may depart from a pure dependency analysis is to allow a
restricted form of constituency analysis, so that dependencies can hold between
strings of words rather than single words. this possibility is exploited, to different
degrees, in the frameworks of hellwig (1986, 2003), mel     cuk (1988) and hudson
(1990), notably in connection with coordination.

a second dividing line is that between mono-stratal and multi-stratal frame-
works, i.e. between theories that rely on a single syntactic representation and theo-
ries that posit several layers of representation. in fact, most theories of dependency
grammar, in contrast to frameworks for id33 that will be discussed
in section 3, are multi-stratal, at least if semantic representations are considered to
be a stratum of the theory. thus, in fgd (sgall et al., 1986) there is both an ana-
lytical layer, which can be characterized as a surface syntactic representation, and
a tectogrammatical layer, which can be regarded as a deep syntactic (or shallow
semantic) representation. in a similar fashion, mtt (mel     cuk, 1988) recognizes
both surface syntactic and deep syntactic representations (in addition to represen-
tations of deep id102, surface morphology, deep morphology and semantics).
by contrast, tesni`ere (1959) uses a single level of syntactic representation, the so-
called stemma, which on the other hand includes junction and transfer in addition
to syntactic connection.2 the framework of xdg (debusmann et al., 2004) can be
seen as a compromise in that it allows multiple layers of dependency-based linguis-
tic representations but requires that all layers, or dimensions as they are called in

2tesni`ere   s representations also include anaphors, which are described as supplementary seman-

tic connections without corresponding syntactic connections.

7

xdg, share the same set of nodes. this is in contrast to theories like fgd, where
e.g. function words are present in the analytical layer but not in the tectogrammat-
ical layer.

the different requirements of xdg and fgd point to another issue, namely
what can constitute a node in a dependency structure. although most theories
agree that dependency relations hold between lexical elements, rather than phrases,
they can make different assumptions about the nature of these elements. the most
straightforward view is that the nodes of the dependency structure are simply the
word forms occurring in the sentence, which is the view adopted in most parsing
systems based on dependency grammar. but it is also possible to construct depen-
dency structures involving more abstract entities, such as lemmas or lexemes, es-
pecially in deeper syntactic representations. another variation is that the elements
may involve several word forms, constituting a dissociate nucleus (nucl  eus dis-
soci  e) in the terminology of tesni`ere (1959), or alternatively correspond to smaller
units than word forms, as in the morphological dependencies of mel     cuk (1988).

a fourth dimension of variation concerns the inventory of speci   c dependency
types posited by different theories, i.e. functional categories like sbj, obj and
nmod that are used to label dependency arcs in the representation in figure 2.
broadly speaking, most theories either assume a set of more surface-oriented gram-
matical functions, such as subject, object, adverbial, etc., with a more or less elab-
orate subclassi   cation, or a set of more semantically oriented role types, such as
agent, patient, goal, etc., belonging to the tradition of case roles or thematic roles
(fillmore, 1968; jackendoff, 1972; dowty, 1989).3 multi-stratal theories often
combine the two relation types. thus, fgd (sgall et al., 1986) uses grammatical
functions in the analytical layer and semantic roles in the tectogrammatical layer.
an alternative scheme of representation, which is found in mtt (mel     cuk, 1988),
is to use numerical indices for valency-bound dependents to re   ect a canonical
ordering of arguments (argument 1, 2, 3, etc.) and to use descriptive labels only
for valency-free dependents. finally, it is also possible to use unlabeled depen-
dency structures, although this is more common in practical parsing systems than
in linguistic theories.

there are several open issues in dependency grammar that have to do with
formal properties of representations. since a dependency representation consists
of lexical elements linked by binary asymmetrical relations, it can be de   ned as
a labeled directed graph, where the set of nodes (or vertices) is the set of lexi-
cal elements (as de   ned by the particular framework), and the set of labeled arcs

3the notion of a semantic role can be traced back to p  an. ini   s k  anaka theory (misra, 1966), which
is sometimes also seen as the earliest manifestation of dependency grammar. the notion of a gram-
matical function also has a long history that extends at least to the work of appolonius dyscolus in
the second century of the common era (robins, 1967).

8

represent dependency relations from heads to dependents. in order to provide a
complete syntactic analysis of a sentence, the graph must also be connected so that
every node is related to at least one other node (mel     cuk, 1988). again, we refer
to figure 2 as an illustration of this representation, where the nodes are the word
tokens of the sentence (annotated with parts-of-speech) and the arcs are labeled
with grammatical functions.4

given this general characterization, we may then impose various additional
conditions on these graphs. two basic constraints that are assumed in most ver-
sions of dependency grammar are the single-head constraint, i.e. the assumption
that each node has at most one head, and the acyclicity constraint, i.e. the assump-
tion that the graph should not contain cycles. these two constraints, together with
connectedness, imply that the graph should be a rooted tree, with a single root node
that is not a dependent of any other node. for example, the representation in fig-
ure 2 is a rooted tree with the verb had as the root node. although these constraints
are assumed in most versions of dependency grammar, there are also frameworks
that allow multiple heads as well as cyclic graphs, such as wg (hudson, 1984,
1990). another issue that arises for multi-stratal theories is whether each level of
representation has its own set of nodes, as in most theories, or whether they only
de   ne different arc sets on top of the same set of nodes, as in xdg (debusmann
et al., 2004).

however, the most important and hotly debated issues concerning formal rep-
resentations have to do with the relation between dependency structure and word
order. according to tesni`ere (1959), dependency relations belong to the structural
order (l   ordre structural), which is different from the linear order (l   ordre lin  eaire)
of a spoken or written string of words, and structural syntax is based on the re-
lations that exist between these two dimensions. most versions of dependency
grammar follow tesni`ere in assuming that the nodes of a dependency structure are
not linearly ordered in themselves but only in relation to a particular surface real-
ization of this structure. a notable exception to this generalization is fgd, where
the representations of both the analytical layer and the tectogrammatical layer are
linearly ordered in order to capture aspects of information structure (sgall et al.,
1986). in addition, there are frameworks, such as tdg (duchier and debusmann,
2001), where the linear order is represented by means of a linearly ordered de-
pendency structure, the linear precedence (lp) tree, while the proper dependency
representation, the immediate dominance (id) tree, is unordered.

4there seems to be no general consensus in the literature on dependency grammar as to whether
the arcs representing dependency relations should be drawn pointing from heads to dependents or
vice versa (or indeed with arrowheads at all). we have chosen to adopt the former alternative, both
because it seems to be the most common representation in the literature and because it is consistent
with standard practice in id207.

9

however, whether dependency relations introduce a linear ordering or not,
there may be constraints relating dependency structures to linear realizations. the
most well-known example is the constraint of projectivity,    rst discussed by lecerf
(1960), hays (1964) and marcus (1965), which is related to the contiguity con-
straint for constituent representations. a dependency graph satis   es the constraint
of projectivity with respect to a particular linear order of the nodes if, for every
arc h     d and node w, w occurs between h and d in the linear order only if w
is dominated by h (where dominates is the re   exive and transitive closure of the
arc relation). for example, the representation in figure 2 is an example of a pro-
jective dependency graph, given the linear order imposed by the word order of the
sentence.

the distinction between projective and non-projective dependency grammar
often made in the literature thus refers to the issue of whether this constraint is
assumed or not. broadly speaking, we can say that whereas most practical sys-
tems for id33 do assume projectivity, most dependency-based lin-
guistic theories do not. more precisely, most theoretical formulations of depen-
dency grammar regard projectivity as the norm but also recognize the need for non-
projective representations of certain linguistic constructions, e.g. long-distance de-
pendencies (mel     cuk, 1988; hudson, 1990). it is also often assumed that the con-
straint of projectivity is too rigid for the description of languages with free or    ex-
ible word order.

some multi-stratal theories allow non-projective representations in some layers
but not in others. for example, fgd assumes that tectogrammatical representations
are projective while analytical representations are not (sgall et al., 1986). similarly,
tdg (duchier and debusmann, 2001) assume projectivity for lp trees but not for
id trees. sometimes a weaker condition called planarity is assumed, which allows
a node w to occur between a head h and a dependent d without being dominated by
h only if w is a root (sleator and temperley, 1993).5 further relaxations of these
constraints are discussed in kahane et al. (1998) and yli-jyr  a (2003).

the points of divergence considered up till now have all been concerned with
aspects of representation. however, as mentioned at the end of the previous sec-
tion, there are also a number of points concerning the substantive linguistic analysis
where different frameworks of dependency grammar make different assumptions,
in the same way as theories differ also within other traditions. we will limit our-
selves to a brief discussion of two such points.

the    rst point concerns the issue of syntactic versus semantic heads. as noted
in section 2.1, the criteria for identifying heads and dependents invoke both syn-

5this constraint is related to but not equivalent to the standard notion of planarity in id207

(see, e.g., grimaldi, 2004).

10

tactic and semantic properties. in many cases, these criteria give the same result,
but in others they diverge. a typical example is found in so-called case marking
prepositions, exempli   ed in the following sentence:

i believe in the system.

(3)

according to syntactic criteria, it is natural to treat the preposition in as a depen-
dent of the verb believe and as the head of the noun system. according to semantic
criteria, it is more natural to regard system as a direct dependent of believe and to
treat in as a dependent of system (corresponding to a case marking af   x in some
other languages).6 most versions of dependency grammar treat the preposition as
the head of the noun, but there are also theories that make the opposite assump-
tion. similar considerations apply to many constructions involving one function
word and one content word, such as determiner-noun and complementizer-verb
constructions. an elegant solution to this problem is provided by the theory of
tesni`ere (1959), according to which the function word and the content word form
a dissociate nucleus (nucl  eus dissoci  e), united by a relation of transfer (transla-
tion). in multi-stratal theories, it is possible to treat the function word as the head
only in more surface-oriented layers. for example, to return to example (3), fgd
would assume that the preposition takes the noun as a dependent in the analytical
layer, but in the tectogrammatical layer the preposition would be absent and the
noun would instead depend directly on the verb.

the second point concerns the analysis of coordination, which presents prob-
lems for any syntactic theory but which seems to be especially hard to reconcile
with the idea that syntactic constructions should be analyzed in terms of binary
head-dependent relations. consider the following example:

they operate ships and banks.

(4)

it seems clear that the phrase ships and banks functions as a direct object of the
verb operate, but it is not immediately clear how this phrase can be given an inter-
nal analysis that is compatible with the basic assumptions of dependency analysis,
since the two nouns ships and banks seem to be equally good candidates for being
heads. one alternative is to treat the conjunction as the head, as shown in figure 3
(top), an analysis that may be motivated on semantic grounds and is adopted in
fgd. another alternative, advocated by mel     cuk (1988), is to treat the conjunction
as the head only of the second conjunct and analyze the conjunction as a dependent
of the    rst conjunct, as shown in figure 3 (bottom). the arguments for this anal-
ysis are essentially the same as the arguments for an asymmetric right-branching

6a third alternative is to treat both in and system as dependents of believe, since it is the verb that

selects the preposition and takes the noun as an argument.

11

 

 

obj

sbj

p

 

cj

 

cj

 

?
prp
they

 

 

 

 

 

vbp
operate

?
nns
ships

?
cc
and

?
nns
banks

?
pu
.

p

 

 

sbj

obj

co

cj

 

?
prp
they

 

 

vbp
operate

 

 

?
nns
ships

 

 

 

?
cc
and

?
nns
banks

?
pu
.

figure 3: two analyses of coordination

analysis in constituency-based frameworks. a third option is to give up a pure
dependency analysis and allow a limited form of phrase structure, as in wg (hud-
son, 1990). a fourth and    nal variant is the analysis of tesni`ere (1959), according
to which both ships and banks are dependents of the verb, while the conjunction
marks a relation of junction (jonction) between the two nouns.

3 parsing with dependency representations

so far, we have reviewed the theoretical tradition of dependency grammar, focusing
on the common core of assumptions as well as major points of divergence, rather
than on individual instantiations of this tradition. we will now turn to what is the
main topic of this paper, namely the computational implementation of syntactic
analysis based on dependency representations, i.e. representations involving lexical
nodes, connected by dependency arcs, possibly labeled with dependency types.

such implementations may be intimately tied to the development of a particular
theory, such as the plain system based on dug (hellwig, 1980, 2003) or the
fdg parsing system (tapanainen and j  arvinen, 1997; j  arvinen and tapanainen,
1998). on the whole, however, the connections between theoretical frameworks
and computational systems are often rather indirect for dependency-based analysis,
probably more so than for theories and parsers based on constituency analysis. this
may be due to the relatively lower degree of formalization of dependency grammar
theories in general, and this is also part of the reason why the topic of this section

12

is described as parsing with dependency representations, rather than parsing with
dependency grammar.

in discussing dependency-based systems for syntactic parsing, we will follow
carroll (2000) and distinguish two broad types of strategy, the grammar-driven
approach and the data-driven approach, although these approaches are not mutu-
ally exclusive. we will conclude the paper with a brief discussion of some of the
potential advantages of using dependency representations in syntactic parsing.

3.1 grammar-driven id33

the earliest work on parsing with dependency representations was intimately tied
to formalizations of dependency grammar that were very close to context-free
grammar, such as the proposals of hays (1964) and gaifman (1965). in the formu-
lation of gaifman (1965) a dependency system contains three sets of rules:7

1. li: rules of the form x(y1          yi     yi+1          yn), where i may equal 0 and/or
n, which say that the category x may occur with categories y1, . . . , yn as
dependents, in the order given (with x in position    ).

2. lii: rules giving for every category x the list of words belonging to it

(where each word may belong to more than one category).

3. liii: a rule giving the list of all categories the occurrence of which may

govern a sentence.

a sentence consisting of words w1, . . ., wn is analyzed by assigning to it a sequence
of categories x1, . . ., xn and a relation of dependency d between words such that
the following conditions hold (where d    is the transitive closure of d):

1. for no wi, d   (wi, wi).

2. for every wi, there is at most one wj such that d(wi, wj).

3. if d   (wi, wj) and wk is between wi and wj, then d   (wk, wj).

4. the whole set of word occurrences is connected by d.

5. if w1, . . . , wi are left dependents and wi+1, . . . , wn right dependents of some
word, and x1, . . . , xi, xi+1, . . . , xn are the categories of w1, . . . , wi, wi+1,
. . . , wn, then x(x1          xi     xi+1          xn) is a rule of li.

6. the word occurrence wi that governs the sentence belongs to a category

listed in liii.

7the formulation of hays (1964) is slightly different but equivalent in all respects.

13

gaifman remarks that 1   4 are general structure requirements that can be made on
any relation de   ned on a    nite linearly ordered set whether it is a set of categories
or not, while 5   6 are requirements which relate the relation to the speci   c gram-
mar given by the three sets of rules li   liii. referring back to the discussion of
graph conditions in section 2.2, we may    rst of all note that gaifman de   nes de-
pendency relations to hold from dependent to head, rather than the other way round
which is more common in the recent literature. secondly, we see that condition 2
corresponds to the single-head constraint and condition 3 to the projectivity con-
straint. conditions 1, 2 and 4 jointly entail that the graph is a rooted tree, which
is presupposed in condition 6. finally, it should be pointed out that this kind of
dependency system only gives an unlabeled dependency analysis, since there are
no dependency types used to label dependency relations.

gaifman (1965) proves several equivalence results relating his dependency sys-
tems to context-free grammars. in particular, he shows that the two systems are
weakly equivalent, i.e. that they both characterize the class of context-free lan-
guages. however, he also shows that whereas any dependency system can be con-
verted to a strongly equivalent context-free grammar (modulo a speci   c mapping
between dependency trees and context-free parse trees), the inverse construction is
only possible for a restricted subset of context-free grammar (roughly grammars
where all productions are lexicalized).

these results have been invoked to explain the relative lack of interest in depen-
dency grammar within natural language processing for the subsequent twenty-   ve
years or so, based on the erroneous conclusion that dependency grammar is only a
restricted variant of context-free grammar (j  arvinen and tapanainen, 1998).8 this
conclusion is erroneous simply because the results only concern the speci   c ver-
sion of dependency grammar formalized by hays and gaifman, which for one
thing is restricted to projective dependency structures. however, it also worth em-
phasizing that with the increasing importance of problems like robustness and dis-
ambiguation, issues of (limited) generative capacity have lost some of their signi   -
cance in the context of natural language parsing. nevertheless, it seems largely true
to say that, except for isolated studies of dependency grammar as an alternative to
context-free grammar as the basis for transformational grammar (robinson, 1970),
dependency grammar has played a marginal role both in syntactic theory and in
natural language parsing until fairly recently.

the close relation to context-free grammar in the formalization of dependency
grammar by hays and gaifman means that essentially the same parsing methods

8a similar development seems to have taken place with respect to categorial grammar after the
weak equivalence of a restricted type of categorial grammar with context-free grammar was proven
by bar-hillel et al. (1960).

14

can be used for both types of system. hence, the parsing algorithm outlined in hays
(1964) is a bottom-up id145 algorithm very similar to the cky
algorithm proposed for context-free parsing at about the same time (kasami, 1965;
younger, 1967). the use of id145 algorithms that are closely
connected to context-free parsing algorithms such as cky and earley   s algorithm
(earley, 1970) is a prominent trend also in more recent grammar-driven approaches
to id33. one example is the link grammar parser of sleator and
temperley (1991, 1993), which uses a id145 algorithm imple-
mented as a top-down recursive algorithm with memoization to achieve parsing in
o(n3) time. link grammar is not considered an instance of dependency grammar
by its creators, and it departs from the traditional view of dependency by using
undirected links, but the representations used in link grammar parsing are similar
to dependency representations in that they consist of words linked by binary rela-
tions. other examples include a modi   cation of the cky algorithm (holan et al.,
1997) and an earley-type algorithm with left-corner    ltering in the prediction step
(lombardo and lesmo, 1996; barbero et al., 1998).

a common property of all frameworks that implement id33 as a
form of lexicalized context-free parsing is that they are restricted to the derivation
of projective dependency structures, although some of the frameworks allow post-
processing that may introduce non-projective structures (sleator and temperley,
1991, 1993). many of these frameworks can be subsumed under the notion of
bilexical grammar introduced by eisner (2000). in eisner   s formulation, a bilexical
grammar consists of two elements:

1. a vocabulary v of terminal symbols (words), containing a distinguished

symbol root.

2. for each word w     v , a pair of deterministic    nite-state automata lw and

rw. each automaton accepts some regular subset of v    .

the language l(g) de   ned by a bilexical dependency grammar g is de   ned as
follows:

1. a dependency tree is a rooted tree whose nodes are labeled with words from
v , and where the root node is labeled with the special symbol root. the
children of a node are ordered with respect to each other and the node itself,
so that the node has both left children that precede it and right children that
follow it.

2. a dependency tree is grammatical according to g iff for every word token
w that appears in the tree, lw accepts the (possibly empty) sequence of w   s

15

left children (from right to left), and rw accepts the sequence of w   s right
children (from left to right).

3. a string x     v     is generated by g with analysis y if y is a grammatical
dependency tree according to g and listing the node labels of y in in   x order
yields the string x followed by root; x is called the yield of y.

4. the language l(g) is the set of all strings generated by g.

the general parsing algorithm proposed by eisner for bilexical grammar is again a
id145 algorithm, which proceeds by linking spans (strings where
roots occur either leftmost or rightmost or both) instead of constituents, thereby
reducing the time complexity from o(n5) to o(n3). more precisely, the running
time is o(n3g3t), where g is an upper bound on the number of possible senses
(lexical entries) of a single word, and t is an upper bound on the number of states
of a single automaton.

eisner shows how the framework of bilexical grammar, and the cubic-time
parsing algorithm, can be modi   ed to capture a number of different frameworks
and approaches such as milward   s (mono)lexical dependency grammar (milward,
1994), alshawi   s head automata (alshawi, 1996), sleator and temperley   s link
grammar (sleator and temperley, 1991, 1993), and eisner   s own probabilistic de-
pendency models that will be discussed below in section 3.2 (eisner, 1996b,a).

the second main tradition in grammar-driven id33 is based on
the notion of eliminative parsing, where sentences are analyzed by successively
eliminating representations that violate constraints until only valid representations
remain. one of the    rst parsing systems based on this idea is the cg framework
(karlsson, 1990; karlsson et al., 1995), which uses underspeci   ed dependency
structures represented as syntactic tags and disambiguated by a set of constraints
intended to exclude ill-formed analyses. in cdg (maruyama, 1990), this idea is
extended to complete dependency structures by generalizing the notion of tag to
pairs consisting of a syntactic label and an identi   er of the head node. this kind of
representation is fundamental for many different approaches to dependency pars-
ing, since it provides a way to reduce the parsing problem to a tagging or classi-
   cation problem. typical representatives of this tradition are the extended cdg
framework of harper and helzerman (1995) and the fdg system (tapanainen and
j  arvinen, 1997; j  arvinen and tapanainen, 1998), where the latter is a development
of cg that combines eliminative parsing with a non-projective dependency gram-
mar inspired by tesni`ere (1959).

in the eliminative approach, parsing is viewed as a id124 prob-
lem, where any analysis satisfying all the constraints of the grammar is a valid

16

analysis. id124 in general is np complete, which means that spe-
cial care must be taken to ensure reasonable ef   ciency in practice. early versions
of this approach used procedures based on local consistency (maruyama, 1990;
harper et al., 1995), which attain polynomial worst case complexity by only con-
sidering local information in the application of constraints. in the more recently
developed tdg framework (duchier, 1999, 2003), the problem is confronted head-
on by using constraint programming to solve the satisfaction problem de   ned by
the grammar for a given input string. the tdg framework also introduces several
levels of representation (cf. section 2.2), arguing that constraints can be simpli   ed
by isolating different aspects of the grammar such as immediate dominance (id)
and linear precedence (lp) and have constraints that relate different levels to each
other (duchier and debusmann, 2001; debusmann, 2001). this view is taken to
its logical extension in the most recent version of the framework called extensible
dependency grammar (xdg), where any number of levels, or dimensions, can be
de   ned in the grammatical framework (debusmann et al., 2004)

from the point of view of parsing unrestricted text, parsing as constraint satis-
faction can be problematic in two ways. first, for a given input string, there
may be no analysis satisfying all constraints, which leads to a robustness prob-
lem. secondly, there may be more than one analysis, which leads to a problem
of disambiguation. menzel and schr  oder (1998) extends the cdg framework of
maruyama (1990) with graded, or weighted, constraints, by assigning a weight w
(0.0     w     1.0) to each constraint indicating how serious the violation of this
constraint is (where 0.0 is the most serious). in this extended framework, later
developed into wcdg (schr  oder, 2002), the best analysis for a given input string
is the analysis that minimizes the total weight of violated constraints. while early
implementations of this system used an eliminative approach to parsing (menzel
and schr  oder, 1998), the more recent versions instead use a transformation-based
approach, which successively tries to improve the analysis by transforming one
solution into another guided by the observed constraint violations in the current
solution. one advantage of this heuristic approximation strategy is that it can be
combined with arbitrarily complex constraints, whereas standard eliminative pro-
cedures usually require constraints to be binary for ef   ciency reasons (foth et al.,
2004).

so far, we have distinguished two main trends in grammar-driven dependency
parsing. the    rst is based on a formalization of dependency grammar that is closely
related to context-free grammar, and therefore usually restricted to projective de-
pendency structures, using standard techniques from context-free parsing to obtain
good ef   ciency in the presence of massive ambiguity, in particular dynamic pro-
gramming or memoization. the second is based on a formalization of dependency
grammar in terms of constraints, not necessarily limited to projective structures,

17

where parsing is naturally viewed as a id124 problem which can
be addressed using eliminative parsing methods, although the exact parsing prob-
lem is often intractable.

in addition to these two traditions, which both involve fairly complex grammars
and parsing algorithms, there is a third tradition which is based on a simpler notion
of dependency grammar together with a deterministic parsing strategy (possibly
with limited backtracking). as in other parsing paradigms, the study of determin-
istic parsing can be motivated either by a wish to model human sentence processing
or by a desire to make syntactic parsing more ef   cient (or possibly both). accord-
ing to covington (2001), these methods have been known since the 1960   s without
being presented systematically in the literature. the fundamental parsing strat-
egy comes in different versions but we will concentrate here on the left-to-right
(or incremental) version, which is formulated in the following way by covington
(2001):

accept words one by one starting at the beginning of the sentence, and
try linking each word as head or dependent of every previous word.

this parsing strategy is compatible with many different grammar formulations. all
that is required is that a grammar g de   nes a boolean function fg that, for any two
words w1 and w2, returns true if w1 can be the head of w2 according to g (and
false) otherwise.9 covington (2001) demonstrates how this parsing strategy can
be used to produce dependency structures satisfying different conditions such as
uniqueness (single head) and projectivity simply by imposing different constraints
on the linking operation. covington has also shown in previous work how this
parsing strategy can be adapted to suit languages with free,    exible or rigid word
order (covington, 1990a,b, 1994). the time complexity of covington   s algorithm
is o(n2) in the deterministic version.

the parsing algorithm proposed by nivre (2003), which will be discussed in
section 3.2, can be derived as a special case of covington   s algorithm, although we
will not give this formulation here, and the very    rst experiments with this algo-
rithm used a simple grammar of the kind presupposed by covington to perform
unlabeled id33 (nivre, 2003). a similar approach can be found
in obrebski (2003), although this system is nondeterministic and derives a com-
pact representation of all permissible dependency trees in the form of a directed
acyclic graph. yet another framework that shows af   nities with the determinis-
tic grammar-driven approach is that of kromann (2004), although it is based on a

9in this formulation, the parsing strategy is limited to unlabeled dependency graphs. in principle,
it is possible to perform labeled id33 by returning a set of permissible dependency
types instead of true, but this makes the process nondeterministic in general.

18

more sophisticated notion of grammar called discontinuous grammar. parsing in
this framework is essentially deterministic but subject to repair mechanisms that
are associated with local cost functions derived from the grammar.

before we close the discussion of grammar-driven id33, we
should also mention the work of o   azer (2003), which is an extended    nite-state
approach to id33 similar to the cascaded partial parsers used for
constituency-based parsing by abney (1996) and roche (1997). o   azer   s system
allows violable constraints for robust parsing and uses total link length to rank
alternative analyses, as proposed by lin (1996).

3.2 data-driven id33

as for natural language parsing in general, the    rst attempts at data-driven depen-
dency parsing were also grammar-driven in that they relied on a formal dependency
grammar and used corpus data to induce a probabilistic model for disambigua-
tion. thus, carroll and charniak (1992) essentially use a pid18 model, where
the context-free grammar is restricted to be equivalent to a hays/gaifman type
dependency grammar. they report experiments trying to induce such a probabilis-
tic grammar using unsupervised learning on an arti   cially created corpus but with
relatively poor results.

a more successful and more in   uential approach was developed by eisner
(1996a,b), who de   ned several probabilistic models for id33 and
evaluated them using supervised learning with data from the wall street journal
section of the id32. in later work, eisner (2000) has shown how these
models can be subsumed under the general notion of a bilexical grammar (bg),
which means that parsing can be performed ef   ciently as discussed in section 3.1.
eisner (2000) de   nes the notion of a weighted bilexical grammar (wbg) in terms
of bg as follows (cf. section 3.1):

1. a weighted dfa a is a deterministic    nite automaton that associates a real-
valued weight with each arc and each    nal state. each accepting path through
a is assigned a weight, namely the sum of all arc weights on the path and the
weight of the    nal state. each string x accepted by a is assigned the weight
of its accepting path.

2. a wbg g is a bg in which all the automata lw and rw are weighted dfas.
the weight of a dependency tree y under g is de   ned as the sum, over all
word tokens w in y, of the weight with which lw accepts w   s sequence of
left children plus the weight with which rw accepts w   s sequence of right
children.

19

eisner (1996b) presents three different probabilistic models for dependency pars-
ing, which can be reconstructed as different weighting schemes within the frame-
work of wbg. however, the    rst two models (models a and b) require that we dis-
tinguish between an underlying string x     v    , described by the wbg, and a sur-
face string x   , which results from a possibly nondeterministic, possibly weighted
   nite-state transduction r on x. the surface string x    is then grammatical with
analysis (y, p) if y is a grammatical dependency tree whose yield x is transduced
to x    along an accepting path p in r. to avoid the distinction between underlying
strings and surface strings, we will restrict our attention to model c, which was
found to perform signi   cantly better than the other two models in the experiments
reported in eisner (1996a).

first of all, it should be pointed out that all the models in eisner (1996b) in-
volve part-of-speech tags, in addition to word tokens and (unlabeled) dependency
relations, and de   ne the joint id203 of the words, tags and dependency links.
model c is de   ned as follows:

p (tw(1), . . ., tw(n), links) =

n
y
i=1

p (lc(i) | tw(i))p (rc(i) | tw(i))

(5)

where tw(i) is the ith tagged word, and lc(i) and rc(i) are the left and right chil-
dren of the ith word, respectively. the id203 of generating each child is con-
ditioned on the tagged head word and the tag of the preceding child (left children
being generated from right to left):

p (lc(i) | tw(i)) =

m
y
j=1

p (tw(lcj(i)) | t(lcj   1(i)), tw(i))

p (rc(i) | tw(i)) =

m
y
j=1

p (tw(rcj(i)) | t(rcj   1(i)), tw(i))

(6)

(7)

where lcj(i) is the jth left child of the ith word and t(lcj   1(i)) is the tag of the
preceding left child (and analogously rcj(i) and t(rcj   1(i)) for right children).
this model can be implemented in the wbg framework by letting the automata
lw and rw have weights on their arcs corresponding to the log of the probabil-
ity of generating a particular left or right child given the tag of the preceding
child.
in this way, the weight assigned to a dependency tree t will be the log
of p (tw(1), . . ., tw(n), links) as de   ned above (eisner, 2000).

eisner   s work on data-driven id33 has been in   uential in two
ways. first, it showed that generative probabilistic modeling and supervised learn-
ing could be applied to dependency representations to achieve a parsing accuracy
comparable to the best results reported for constituency-based parsing at the time,

20

although the evalutation metrics used in the two cases are not strictly comparable.
secondly, it showed how these models could be coupled with ef   cient parsing tech-
niques that exploit the special properties of dependency structures. the importance
of the second aspect can be seen in recent work by mcdonald et al. (2005), apply-
ing discriminative estimation methods to probabilistic id33. thanks
to the more ef   cient parsing methods offered by eisner   s methods for bilexical
parsing, training can be performed without pruning the search space, which is im-
possible for ef   ciency reasons when using lexicalized constituency representations
with comparable lexical dependencies.

collins et al. (1999) apply the generative probabilistic parsing models of collins
(1997, 1999) to id33, using data from the prague dependency tree-
bank. this requires preprocessing to transform dependency structures into    at
phrase structures for the training phase and postprocessing to extract dependency
structures from the phrase structures produced by the parser. the parser of char-
niak (2000) has been adapted and applied to the prague dependency treebank in a
similar fashion, although this work remains unpublished.

samuelsson (2000) proposes a probabilistic model for dependency grammar
that goes beyond the models considered so far by incorporating labeled depen-
dencies and allowing non-projective dependency structures. in this model, depen-
dency representations are generated by two stochastic processes: one top-down
process generating the tree structure y and one bottom-up process generating the
surface string x given the tree structure, de   ning the joint id203 as p (x, y) =
p (y)p (x|y). samuelsson suggests that the model can be implemented using a
bottom-up id145 approach, but the model has unfortunately never
been implemented and evaluated.

another probabilistic approach to id33 that incorporates labeled
dependencies is the stochastic cdg parser of wang and harper (2004), which ex-
tends the cdg model with a generative probabilistic model. parsing is performed
in two steps, which may be tightly or loosely integrated, where the    rst step as-
signs to each word a set of superarvs, representing constraints on possible heads
and dependents, and where the second step determines actual dependency links
given the superarv assignment. although the basic model and parsing algorithm
is limited to projective structures, the system allows non-projective structures for
certain wh-constructions. the system has been evaluated on data from the wall
street journal section of the id32 and achieves state-of-the-art perfor-
mance using a dependency-based evaluation metric (wang and harper, 2004).

the    rst step in the parsing model of wang and harper (2004) can be seen as a
kind of id55, which has turned out to be a crucial element in many recent
approaches to statistical parsing, e.g. in ltag (joshi and sarkar, 2003; banga-
lore, 2003) and id35 (clark and curran, 2004; curran and clark, 2004). a similar

21

two-step process is used in the statistical dependency parser of bangalore (2003),
which uses elementary ltag trees as supertags in order to derive a dependency
structure in the second step. id55 is performed using a standard id48
trigram tagger, while dependency structures are derived using a heuristic determin-
istic algorithm that runs in linear time. another data-driven dependency parser
based on id55 is nasr and rambow (2004), where supertags are derived
from a lexicalized extended context-free grammar and the most probable analysis
is derived using a modi   ed version of the cky algorithm.

most of the systems described in this section are based on a formal dependency
grammar in combination with a generative probabilistic model, which means that
parsing conceptually consists in the derivation of all analyses that are permissible
according to the grammar and the selection of the most probable analysis according
to the generative model. this is in contrast to recent work based on purely discrim-
inative models of inductive learning in combination with a deterministic parsing
strategy, methods that do not involve a formal grammar.

the deterministic discriminative approach was    rst proposed by kudo and
matsumoto (2000, 2002) and yamada and matsumoto (2003), using support vector
machines (vapnik, 1995) to train classi   ers that predict the next action of a deter-
ministic parser constructing unlabeled dependency structures. the parsing algo-
rithm used in these systems implements a form of id132 with three
possible parse actions that apply to two neighboring words referred to as target
nodes:10

1. a shift action adds no dependency construction between the target words wi
and wi+1 but simply moves the point of focus to the right, making wi+1 and
wi+2 the new target words.

2. a right action constructs a dependency relation between the target words,
adding the left node wi as a child of the right node wi+1 and reducing the
target words to wi+1, making wi   1 and wi+1 the new target words.

3. a left action constructs a dependency relation between the target words,
adding the right node wi+1 as a child of the left node wi and reducing the
target words to wi, making wi   1 and wi the new target words.

the parser processes the input from left to right repeatedly as long as new depen-
dencies are added, which means that up to n     1 passes over the input may be
required to construct a complete dependency tree, giving a worst case time com-
plexity of o(n2), although the worst case seldom occurs in practice. the features

10the parsers described in kudo and matsumoto (2000, 2002) are applied to japanese, which is

assumed to be strictly head-   nal, which means that only the actions shift and right are required.

22

used to predict the next parse action are the word forms and part-of-speech tags of
the target words, of their left and right children, and of their left and right string
context (in the reduced string). yamada and matsumoto (2003) evaluate the system
using the standard data set from the wall street journal section of the penn tree-
bank and shows that deterministic discriminative id33 can achieve
an accuracy that is close to the state-of-the-art with respect to dependency accuracy.
further improvements with this model are reported in isozaki et al. (2004).

the framework of inductive id33,    rst presented in nivre et al.
(2004) and more fully described in nivre (2005), has many properties in common
with the system of yamada and matsumoto (2003), but there are three differences.
the    rst and most important difference is that the system of nivre et al. (2004)
constructs labeled dependency representations, i.e. representations where depen-
dency arcs are labeled with dependency types. this also means that dependency
type information can be exploited in the feature model used to predict the next
parse action. the second difference is that the algorithm proposed in nivre (2003)
is a head-driven arc-eager algorithm that constructs a complete dependency tree in
a single pass over the data. the third and    nal difference is that nivre et al. (2004)
use memory-based learning to induce classi   ers for predicting the next parsing
action based on conditional features, whereas yamada and matsumoto (2003) use
support vector machines. however, as pointed out by kudo and matsumoto (2002),
in a deterministic discriminative parser the learning method is largely independent
of the rest of the system.

4 the case for id33

as noted several times already, dependency-based syntactic representations have
played a fairly marginal role in the history of linguistic theory as well as that of
natural language processing. saying that there is increasing interest in dependency-
based approaches to syntactic parsing may therefore not be saying very much, but
it is hard to deny that the notion of dependency has become more prominent in the
literature on syntactic parsing during the last decade or so.

in conclusion, it therefore seems appropriate to ask what are the potential bene-
   ts of using dependency-based representations in syntactic parsing, as opposed to
the more traditional representations based on constituency. according to coving-
ton (2001), id33 offers three prima facie advantages:

    dependency links are close to the semantic relationships needed for the next
stage of interpretation; it is not necessary to    read off    head-modi   er or
head-complement relations from a tree that does not show them directly.

23

    the dependency tree contains one node per word. because the parser   s job is
only to connect existing nodes, not to postulate new ones, the task of parsing
is in some sense more straightforward. [...]

    id33 lends itself to word-at-a-time operation, i.e., parsing
by accepting and attaching words one at a time rather than by waiting for
complete phrases. [...]

to this it is sometimes added that dependency-based parsing allows a more ade-
quate treatment of languages with variable word order, where discontinuous syn-
tactic constructions are more common than in languages like english (mel     cuk,
1988; covington, 1990b). however, this argument is only plausible if the formal
framework allows non-projective dependency structures, which is not the case for
most dependency parsers that exist today.

for us, the    rst two advantages identi   ed by covington seem to be the most
important. having a more constrained representation, where the number of nodes
is    xed by the input string itself, should enable conceptually simpler and compu-
tationally more ef   cient methods for parsing. at the same time, it is clear that
a more constrained representation is a less expressive representation and that de-
pendency representations are necessarily underspeci   ed with respect to certain as-
pects of syntactic structure. for example, as pointed out by mel     cuk (1988), it
is impossible to distinguish in a pure dependency representation between an ele-
ment modifying the head of a phrase and the same element modifying the entire
phrase. however, this is precisely the kind of ambiguity that is notoriously hard to
disambiguate correctly in syntactic parsing anyway, so it might be argued that this
kind of underspeci   cation is actually bene   cial. and as long as the syntactic rep-
resentation encodes enough of the structural relations that are relevant for semantic
interpretation, then we are only happy if we can constrain the problem of deriving
these representations.

in general, there is a tradeoff between the expressivity of syntactic represen-
tations and the complexity of syntactic parsing, and we believe that dependency
representations provide a good compromise in this respect. they are less expres-
sive than most constituency-based representations, but they compensate for this
by providing a relatively direct encoding of predicate-argument structure, which is
relevant for semantic interpretation, and by being composed of bilexical relations,
which are bene   cial for disambiguation. in this way, dependency structures are
suf   ciently expressive to be useful in natural language processing systems and at
the same time suf   ciently restricted to allow full parsing with high accuracy and
ef   ciency. at least, this seems like a reasonable working hypothesis.

24

references

abney, s. (1996). partial parsing via    nite-state cascades. journal of natural

language engineering 2: 337   344.

alshawi, h. (1996). head automata and bilingual tiling: translation with minimal
representations. proceedings of the 34th annual meeting of the association for
computational linguistics (acl), pp. 167   176.

bangalore, s. (2003). localizing dependencies and id55. in bod, r., scha,
r. and sima   an, k. (eds), data-oriented parsing, csli publications, university
of chicago press, pp. 283   298.

bar-hillel, y., gaifman, c. and shamir, e. (1960). on categorial and phrase-

structure grammars. bulletin of the research council of israel 9f: 1   16.

barbero, c., lesmo, l., lombardo, v. and merlo, p. (1998). integration of syn-
tactic and lexical information in a hierarchical dependency grammar.
in ka-
hane, s. and polgu`ere, a. (eds), proceedings of the workshop on processing of
dependency-based grammars (acl-coling), pp. 58   67.

bloom   eld, l. (1933). language. the university of chicago press.

carroll, g. and charniak, e. (1992). two experiments on learning probabilistic
dependency grammars from corpora, technical report tr-92, department of
computer science, brown university.

carroll, j. (2000). statistical parsing. in dale, r., moisl, h. and somers, h. (eds),

handbook of natural language processing, marcel dekker, pp. 525   543.

charniak, e. (2000). a maximum-id178-inspired parser. proceedings of the first
meeting of the north american chapter of the association for computational
linguistics (naacl), pp. 132   139.

chomsky, n. (1970). remarks on nominalization. in jacobs, r. and rosenbaum,

p. s. (eds), readings in english transformational grammar, ginn and co.

clark, s. and curran, j. r. (2004). parsing the wsj using id35 and log-linear
models. proceedings of the 42nd annual meeting of the association for com-
putational linguistics (acl), pp. 104   111.

collins, m. (1997). three generative, lexicalised models for statistical parsing.
proceedings of the 35th annual meeting of the association for computational
linguistics (acl), pp. 16   23.

25

collins, m. (1999). head-driven statistical models for natural language parsing.

phd thesis, university of pennsylvania.

collins, m., haji  c, j., brill, e., ramshaw, l. and tillmann, c. (1999). a statis-
tical parser for czech. proceedings of the 37th meeting of the association for
computational linguistics (acl), pp. 505   512.

covington, m. a. (1984). syntactic theory in the high middle ages. cambridge

university press.

covington, m. a. (1990a). a dependency parser for variable-word-order lan-

guages, technical report ai-1990-01, university of georgia.

covington, m. a. (1990b). parsing discontinuous constituents in dependency

grammar. computational linguistics 16: 234   236.

covington, m. a. (1994). discontinuous id33 of free and    xed
word order: work in progress, technical report ai-1994-02, university of geor-
gia.

covington, m. a. (2001). a fundamental algorithm for id33. pro-

ceedings of the 39th annual acm southeast conference, pp. 95   102.

curran, j. r. and clark, s. (2004). the importance of id55 for wide-
coverage id35 parsing. proceedings of the 20th international conference on
computational linguistics (coling), pp. 282   288.

debusmann, r. (2001). a declarative grammar formalism for dependency gram-

mar, master   s thesis, computational linguistics, universit  at des saarlandes.

debusmann, r., duchier, d. and kruijff, g.-j. m. (2004). extensible dependency
grammar: a new methodology. proceedings of the workshop on recent ad-
vances in dependency grammar, pp. 78   85.

dowty, d. (1989). on the semantic content of the notion of    thematic role   . in
chierchia, g., partee, b. h. and turner, r. (eds), properties, types and meaning.
volume ii: semantic issues, reider, pp. 69   130.

duchier, d. (1999). axiomatizing id33 using set constraints. pro-

ceedings of the sixth meeting on mathematics of language, pp. 115   126.

duchier, d. (2003). con   guration of labeled trees under lexicalized constraints

and principles. research on language and computation 1: 307   336.

26

duchier, d. and debusmann, r. (2001). topological dependency trees: a
constraint-based account of linear precedence. proceedings of the 39th annual
meeting of the association for computational linguistics (acl), pp. 180   187.

earley, j. (1970). en ef   cient context-free parsing algorithm. communications of

the acm 13: 94   102.

eisner, j. m. (1996a). an empirical comparison of id203 models for depen-
dency grammar, technical report ircs-96-11, institute for research in cogni-
tive science, university of pennsylvania.

eisner, j. m. (1996b). three new probabilistic models for id33:
an exploration. proceedings of the 16th international conference on computa-
tional linguistics (coling), pp. 340   345.

eisner, j. m. (2000). bilexical grammars and their cubic-time parsing algorithms.
in bunt, h. and nijholt, a. (eds), advances in probabilistic and other parsing
technologies, kluwer, pp. 29   62.

fillmore, c. j. (1968). the case for case. in bach, e. w. and harms, r. t. (eds),

universals in linguistic theory, holt, rinehart and winston, pp. 1   88.

foth, k., daum, m. and menzel, w. (2004). a broad-coverage parser for german

based on defeasible constraints. proceedings of konvens 2004, pp. 45   52.

gaifman, h. (1965). dependency systems and phrase-structure systems. informa-

tion and control 8: 304   337.

grimaldi, r. p. (2004). discrete and combinatorial mathematics.

5th edn,

addison-wesley.

harper, m. p. and helzerman, r. a. (1995). extensions to constraint depen-
dency parsing for spoken language processing. computer speech and language
9: 187   234.

harper, m. p., helzermann, r. a., zoltowski, c. b., yeo, b. l., chan, y., steward,
t. and pellom, b. l. (1995). implementation issues in the development of the
parsec parser. software: practice and experience 25: 831   862.

hays, d. g. (1964). dependency theory: a formalism and some observations.

language 40: 511   525.

hellwig, p. (1980). plain     a program system for dependency analysis and for
in bolc, l. (ed.), representation and

simulating natural language id136.
processing of natural language, hanser, pp. 195   198.

27

hellwig, p. (1986). dependency uni   cation grammar. proceedings of the 11th
international conference on computational linguistics (coling), pp. 195   
198.

hellwig, p. (2003). dependency uni   cation grammar. in agel, v., eichinger, l. m.,
eroms, h.-w., hellwig, p., heringer, h. j. and lobin, h. (eds), dependency and
valency, walter de gruyter, pp. 593   635.

holan, t., kubo  n, v. and pl  atek, m. (1997). a prototype of a id131 for
czech. fifth conference on applied natural language processing, pp. 147   154.

hudson, r. a. (1984). word grammar. blackwell.

hudson, r. a. (1990). english word grammar. blackwell.

isozaki, h., kazawa, h. and hirao, t. (2004). a deterministic word dependency
analyzer enhanced with preference learning. proceedings of the 20th interna-
tional conference on computational linguistics (coling), pp. 275   281.

jackendoff, r. (1972). semantic interpretation in generative grammar. mit

press.

jackendoff, r. s. (1977). x syntax: a study of phrase structure. mit press.

j  arvinen, t. and tapanainen, p. (1998). towards an implementable dependency
grammar. in kahane, s. and polgu`ere, a. (eds), proceedings of the workshop
on processing of dependency-based grammars, pp. 1   10.

joshi, a. and sarkar, a. (2003). id34s and their application to
statistical parsing. in bod, r., scha, r. and sima   an, k. (eds), data-oriented
parsing, csli publications, university of chicago press, pp. 253   281.

kahane, s., nasr, a. and rambow, o. (1998). pseudo-projectivity: a polyno-
mially parsable non-projective dependency grammar. proceedings of the 36th
annual meeting of the association for computational linguistics and the 17th
international conference on computational linguistics, pp. 646   652.

karlsson, f. (1990). constraint grammar as a framework for parsing running text.
in karlgren, h. (ed.), papers presented to the 13th international conference on
computational linguistics (coling), pp. 168   173.

karlsson, f., voutilainen, a., heikkil  a, j. and anttila, a. (eds) (1995). constraint
grammar: a language-independent system for parsing unrestricted text. mou-
ton de gruyter.

28

kasami, t. (1965). an ef   cient recognition and syntax algorithm for context-free
languages, technical report af-crl-65-758, air force cambridge research
laboratory.

kromann, m. t. (2004). optimality parsing and local cost functions in discontin-
uous grammar. electronic notes of theoretical computer science 53: 163   179.

kruijff, g.-j. m. (2001). a categorial-modal logical architecture of informa-
tivity: dependency grammar logic and information structure. phd thesis,
charles university.

kruijff, g.-j. m. (2002). formal and computational aspects of dependency gram-

mar: history and development of dg, technical report, esslli-2002.

kudo, t. and matsumoto, y. (2000). japanese dependency structure analysis based
on support vector machines. proceedings of the joint sigdat conference on
empirical methods in natural language processing and very large corpora
(emnlp/vlc), pp. 18   25.

kudo, t. and matsumoto, y. (2002).

japanese dependency analysis using cas-
caded chunking. proceedings of the sixth workshop on computational lan-
guage learning (conll), pp. 63   69.

lecerf, y. (1960). programme des con   its, mod`ele des con   its. bulletin bimestriel

de l   atala 1(4): 11   18, 1(5): 17   36.

lin, d. (1996). on the structural complexity of natural language sentences. pro-
ceedings of the 16th international conference on computational linguistics
(coling), pp. 729   733.

lombardo, v. and lesmo, l. (1996). an earley-type recognizer for dependency
grammar. proceedings of the 16th international conference on computational
linguistics (coling), pp. 723   728.

marcus, m. p., santorini, b. and marcinkiewicz, m. a. (1993). building a large
annotated corpus of english: the id32. computational linguistics
19: 313   330.

marcus, m. p., santorini, b., marcinkiewicz, m. a., macintyre, r., bies, a., fer-
guson, m., katz, k. and schasberger, b. (1994). the id32: annotat-
ing predicate-argument structure. proceedings of the arpa human language
technology workshop, pp. 114   119.

29

marcus, s. (1965). sur la notion de projectivit  e. zeitschrift f  ur mathematische

logik und grundlagen der mathematik 11: 181   192.

maruyama, h. (1990). structural disambiguation with constraint propagation. pro-
ceedings of the 28th meeting of the association for computational linguistics
(acl), pittsburgh, pa, pp. 31   38.

mcdonald, r., crammer, k. and pereira, f. (2005). online large-margin training of
dependency parsers. proceedings of the 43rd annual meeting of the association
for computational linguistics (acl), pp. 91   98.

mel     cuk, i. (1988). dependency syntax: theory and practice. state university of

new york press.

menzel, w. and schr  oder, i. (1998). decision procedures for id33
using graded constraints. in kahane, s. and polgu`ere, a. (eds), proceedings of
the workshop on processing of dependency-based grammars, pp. 78   87.

milward, d. (1994). dynamic dependency grammar. linguistics and philosophy

17: 561   605.

misra, v. n. (1966). the descriptive technique of panini. mouton.

nasr, a. and rambow, o. (2004). a simple string-rewriting formalism for depen-
dency grammar. proceedings of the workshop on recent advances in depen-
dency grammar, pp. 25   32.

nikula, h. (1986). dependensgrammatik. liber.

nivre, j. (2003). an ef   cient algorithm for projective id33.

in
van noord, g. (ed.), proceedings of the 8th international workshop on parsing
technologies (iwpt), pp. 149   160.

nivre, j. (2005). inductive id33 of natural language text. phd

thesis, v  axj  o university.

nivre, j., hall, j. and nilsson, j. (2004). memory-based id33. in
ng, h. t. and riloff, e. (eds), proceedings of the 8th conference on computa-
tional natural language learning (conll), pp. 49   56.

obrebski, t. (2003). id33 using dependency graph. in van noord,
g. (ed.), proceedings of the 8th international workshop on parsing technologies
(iwpt), pp. 217   218.

30

o   azer, k. (2003). id33 with an extended    nite-state approach.

computational linguistics 29: 515   544.

robins, r. h. (1967). a short history of linguistics. longman.

robinson, j. j. (1970). dependency structures and transformational rules. lan-

guage 46: 259   285.

roche, e. (1997). parsing with    nite state transducers. in roche, e. and schabes,

y. (eds), finite-state language processing, mit press, pp. 241   281.

samuelsson, c. (2000). a statistical theory of dependency syntax. proceedings of

the 18th international conference on computational linguistics (coling).

schr  oder, i. (2002). natural language parsing with graded constraints. phd

thesis, hamburg university.

sgall, p., haji  cov  a, e. and panevov  a, j. (1986). the meaning of the sentence in its

pragmatic aspects. reidel.

sleator, d. and temperley, d. (1991). parsing english with a link grammar, tech-
nical report cmu-cs-91-196, carnegie mellon university, computer science.

sleator, d. and temperley, d. (1993). parsing english with a link grammar. third

international workshop on parsing technologies (iwpt), pp. 277   292.

starosta, s. (1988). the case for lexicase: an outline of lexicase grammatical

theory. pinter publishers.

tapanainen, p. and j  arvinen, t. (1997). a non-projective dependency parser.
proceedings of the 5th conference on applied natural language processing,
pp. 64   71.

tesni`ere, l. (1959).   el  ements de syntaxe structurale. editions klincksieck.

vapnik, v. n. (1995). the nature of statistical learning theory. springer.

wang, w. and harper, m. p. (2004). a statistical constraint dependency grammar
(cdg) parser.
in keller, f., clark, s., crocker, m. and steedman, m. (eds),
proceedings of the workshop in incremental parsing: bringing engineering and
cognition together (acl), pp. 42   29.

yamada, h. and matsumoto, y. (2003). statistical dependency analysis with sup-
port vector machines. in van noord, g. (ed.), proceedings of the 8th interna-
tional workshop on parsing technologies (iwpt), pp. 195   206.

31

yli-jyr  a, a. (2003). multiplanarity     a model for dependency structures in tree-
banks. in nivre, j. and hinrichs, e. (eds), proceedings of the second workshop
on treebanks and linguistic theories (tlt), v  axj  o university press, pp. 189   
200.

younger, d. h. (1967). recognition and parsing of context-free languages in time

n3. information and control 10: 189   208.

zwicky, a. m. (1985). heads. journal of linguistics 21: 1   29.

32

