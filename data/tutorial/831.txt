a survey on id14 and

id33

by

avishek dan

(roll no. 113050011)

and

janardhan singh
(roll no. 10305067)

under the guidance of

prof. pushpak bhattacharyya

abstract

semantics is a    eld of natural language processing concerned with extracting meaning from
a sentence. id14 takes the initial steps in extracting meaning from text by
giving generic labels or roles to the words of the text. the meaning of this small set of labels
can be assumed to be understood by the machine. to help semantic extraction the relationship
between the words in a text needs to be understood at the syntactic level. dependency grammar
and parsing give binary relationships between the words of a text giving clues to their semantic
relations. this document attempts to give a brief survey on these two important    elds concerned
with semantic extraction from text. id14 task was surveyed till the year
2010 while concepts of id33 were covered upto 2008.

ii

table of contents

1 id14
1.1 semantic roles .
.
1.2 lexical resources .

.

. .

. .

. .

. .

. .

1.5 minipar

principle-based parser

1.2.1
1.2.2
1.2.3 verbnet
.
1.2.4 id138 .

.
.
. .
framenet
.
propbank .
.
.
. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 link parser based on link grammar . . . . . . . . . . . . . . . . . . . . . . .
1.4 link grammar
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.1 links and linking requirements . . . . . . . . . . . . . . . . . . . . .
1.4.2 connectors and formulae
. . . . . . . . . . . . . . . . . . . . . . . .
1.4.3 disjunctive form . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.4 grammar rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.1
. . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.2 generating principles . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.3
filtering principles . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.6 automatic id14 . . . . . . . . . . . . . . . . . . . . . . . .
features for frame element labeling . . . . . . . . . . . . . . . . . . .
1.6.1
features for frame element boundary identi   cation . . . . . . . . . . .
1.6.2
id203 estimation of a single role . . . . . . . . . . . . . . . . . .
1.6.3
1.6.4
id203 estimation of all the roles in the sentence . . . . . . . . . .
1.6.5 generalizing lexical semantics . . . . . . . . . . . . . . . . . . . . . .
1.7 extensions to automatic srl . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.8 semi-supervised id14 . . . . . . . . . . . . . . . . . . . .
1.8.1 learning method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.8.2
projecting annotations . . . . . . . . . . . . . . . . . . . . . . . . . .
1.9 statistical method for unl relation label generation . . . . . . . . . . . . .
feature generation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.10 overview of unl system at geta . . . . . . . . . . . . . . . . . . . . . . .
1.10.1 universal word resources . . . . . . . . . . . . . . . . . . . . . . . .
1.10.2 the enconversion and deconversion process
. . . . . . . . . . . . . .
1.10.3 graph to tree conversion . . . . . . . . . . . . . . . . . . . . . . . .

1.9.1
1.9.2 training .
1.9.3 testing .

1.7.1 other work .

.
.

.
.

.
.

.

1
1
2
2
3
4
4
4
5
5
5
6
6
7
7
7
8
8
9
9
10
10
11
11
12
12
12
15
15
15
16
16
16
16
16
17

iii

17
17

19
20
20
22
22
22
22

25
25
26
28
28
28
29
29
30

1.10.4 deployment .
.

1.11 summary .

.

.

.

.

. .
.
.

. .
.
.

. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .

2 dependency grammar and id33

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1 robinson   s axioms
2.2 projective and non-projective dependency structures
. . . . . . . . . . . . . .
2.3 id33 techniques . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1 data-based dependency parser . . . . . . . . . . . . . . . . . . . . . .
2.3.2 transition-based id33 . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4 summary .

.

.

.

.

.

.

.

.

.

3 techniques for corpus based learning

. .
.

. .
.

3.1.1 description .
3.1.2 observations .

3.1 transformation-based error-driven learning . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 statistical dependency analysis . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.1
3.2.2 algorithm . .
3.2.3 classi   cation .
.

parsing actions . .

3.3 summary .

. .
.
.

. .

. .

.

.

.

.

.

iv

chapter 1

id14

id14 is the task of assigning semantic roles to the constituents of the sen-
tence.

in recent years, we have seen successful deployment of domain speci   c semantic extraction
systems. the challenge is to move from domain speci   c systems to domain independent and
robust systems. this is now possible because of the development of large semantic databases
(corpora) and progress in machine learning algorithms. most of these databases are hand-tagged
corpora developed by large groups of people. although, many rule-based techniques like link
parser, minipar and lexical functional grammar have been traditionally used for this task,
success has also been achieved by statistical techniques. one of the foundational works on
this ground was done by jurafsky and gildea(2002) [dd02]. this work uses lexical resources
like id138 and framenet. statistical techniques are applied on these semantic databases for
semantic extraction. the idea is to train supervised classi   ers on these corpora that can be used
to automatically tag vast amount of unseen text with shallow semantic information.

semantic role labelers are commonly developed using a supervised learning paradigm where
a classi   er learns to predict role labels based on features extracted from annotated training data.
the creation of resources that document the realization of semantic roles in example sentences
such as framenet [bfl98b] and propbank [kp02] has greatly facilitated the development of
learning algorithms capable of automatically analyzing the role semantic structure of input sen-
tences.

the section starts by describing semantic roles. it then discusses about the different lexical
resources that can be used for id14. it then describes the different semantic
role labeling techniques.

1.1 semantic roles

in linguistic theory, semantic roles are one of the oldest classes of constructs. the paninian
karaka theory is probably one of the oldest works in this    eld[dd02]. a lot of variety in seman-
tic roles exist today. the semantic roles could be domain-speci   c or generic. fillmore[bfl98a]
gives a hierarchical classi   cation of semantic roles. the framenet project was based on these
framenet roles given by fillmore. we will look at framenet later in the chapter. let   s look at
some examples of semantic roles.

consider an example from the cognition domain in    gure 3.1

1

figure 1.1: id14 example

figure 1.2: id14 example

here, the semantic roles judge, evaluee and reason are speci   c to the cognition domain

when a judgment is made. in the framenet hierarchy, these roles fall in the judgment frame.

for an example with more generic roles, consider the sentence in    gure 1.2.

1.2 lexical resources

this section looks at some of the useful lexical resources used for id14.

1.2.1 framenet
a frame is a structure used to de   ne the semantic meaning of a word. it is a generalizable
concept with recurring frame elements that are recognized intuitively. frame elements are the
elements which make up a frame. framenet currently has about 170,000 manually annotated
sentences providing a unique training dataset for id14 [bfl98b]. in framenet
dataset, the sentences are arranged in a hierarchical order with each frame referring to a concept.
frames at the higher level refer to a more generic concept while frames at the lower level refer
to more speci   c concepts. figure 1.3 [dd02] gives the structure of frames in the framenet.

figure 1.3 that every frame has invoking predicates attached to it. these are the verbs and
some nouns of english that invoke the concept, referred to, by the frame they are attached
to. sentences that have these predicates would have constructs that play the role given by the
frame elements of the invoked frame. for example, in    gure 3.1 the predicate blame invokes the
judgment frame and other constructs in the sentence play the invoked semantic roles. in that
example:

2

figure 1.3: sample domains and frames in framenet

(she) plays the role (judge), (the government) plays the role (evaluee), (for failing to do

enough to help) plays the role (reason),

figure 1.4 shows how the transportation frame is inherited by the driving frame which in
turn is inherited by the riding frame. it can be seen that the frame elements are inherited and
they become more and more speci   c down the order. the scenes on the other hand, do not have
such a relationship.

1.2.2 propbank
propbank[kp03] is another important lexical resource for id14. propbank
is a proposition bank in which sentences are annotated with verbal propositions and their ar-
guments. it was proposed by martha palmer et. al. it is similar to framenet but differs in two
major ways[dut11]:

1. all the verbs in the corpus are annotated.

2. all arguments to a verb must be syntactic constituents. a standard set of argument labels

have been de   ned for this purpose.

verbs are annotated with coarse grained senses and with in   ectional information. in   ection
describes how does the form of the verb modify with change in tense, person, aspect, mood,
number and other grammatical categories [inf11]. the arguments to a verb include:

1. core argument labels from arg0 to arg5. each of them have a speci   c meaning like

that of the karakas in paninian karaka theory.

2. all arguments to a verb must be syntactic constituents. a standard set of argument labels

have been de   ned for this purpose. eg argm-adv: general purpose modi   er label.

3

figure 1.4: frame inheritance

1.2.3 verbnet
verbnet[sch05] is a hierarchical lexical resource that organizes english verbs into different
classes based on the verbal classi   cation of levin (1993). each verbal class takes different
thematic roles and certain syntactic constraints that describe their super   cial behavior. verbnets
hierarchical verb classes establish a set of possible thematic roles and a set of possible syntactic
realizations. verbnet contains mappings to other resources such as framenet and id138.
figure 1.5 shows a verbnet class with it thematic roles and various frames.

verbnet can be highly effective in choosing the correct number of roles for a verb.

1.2.4 id138
id138[fel98] is a large lexical database of english. nouns, verbs, adjectives and adverbs are
grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. synsets
are interlinked by means of conceptual-semantic and lexical relations. the result is a network
of meaningfully related words and concepts.

1.3 link parser based on link grammar

link parser uses link grammar[st95] to give semantic roles to words in the form of relation
between pair of words. the pair forms a grammatical relation. this grammatical relation
de   nes the role of one word with respect to the other. the word whose role is de   ned modi   es

4

figure 1.5: verbnet hit class

the other word, the governing word.

1.4 link grammar

link grammar contains set of words which act as its terminal symbols, a set of relations which
de   ne the links between a pair of words and a set of linking requirement which are the properties
of the words. the linking requirements of words are stored in a dictionary.

1.4.1 links and linking requirements
link is the connection or relation between two words. linking requirements of a word de   ne
the roles the word can play. it also de   nes about the word it can be linked with. example: the
word    cat   

    can be a subject (s)
    can be an object (o)
    will have a determiner (d)

1.4.2 connectors and formulae
the symbols s, o and d represent connectors of the words. the connectors de   ne what the
word can be and what the word needs. connectors end with    +    or    -    which indicates the
direction of its connecting word.    +    means the word it will be connected to is in the right
of it and    -    means the word is to the left of it. the connectors at the opposite ends must
match (+ -). a linking requirements is represented by a formula of connectors combined by

5

figure 1.6: the roles    cat    can play

binary operators    &    and    or   . let c and d be two connectors of a word. then c & d would
mean the linking requirement of the word is that both connectors c and d must be connected
to their complement connectors. c or d means only one of c and d needs to get connected to
a corresponding complementary connector.

1.4.3 disjunctive form
when formula of each word is represented in disjunctive form it has a set of disjuncts associated
with it. a disjunct has two ordered lists of connectors -

1. left list - connectors with    -    mark with it.

2. right list - connectors with    +    mark with it.

a disjunct looks like ((l1,l2,l3,l4,,ln)(r1,r2,r3,r4,,rm)). to convert a formula into its
disjunctive form we need to    nd all the valid permutations of connectors. by    valid    it means
that connectors separated by    &    must be present in the list and among connectors separated by
   or    only one should be present in the list. some examples are shown in table1.1

1.4.4 grammar rules
link grammar contains rules which put constraints on the formation of relations among the
words of the given sentence. these rules are:

1. planarity - links between the words, when all of the links are drawn above the words,

will not cross.

2. connectivity - all the words should have a connection.

3. satisfaction - the linking requirement of each word is satis   ed by the provided links.

(a) ordering: when the left connectors of a formula are traversed from left to right, the
words to which they connect proceed from near to far. when the right connectors of
a formula are traversed from left to right, the words to which they connect proceed
from far to near.

(b) exclusion: no two links may connect the same pair of words.

6

word formulae
sit
a
dog

s- & (o+ or b+)
d+
{@a-} & (d-) &
{b+} & (s+ or o-)

black a+

disjunctive form
((s)(o)), ((s)(b))
(()(d))
((d) (s)), ((d,o) ()),
((d) (s,b)), ((d,o) (b)), ((a,d) (s)),
((a,d,o) ()), ((a,d) (s,b)), ((a,d,o) (b))
(()(a))

table 1.1: disjunctive form

1.5 minipar

minipar is a principle-based english parser. it represents its grammar as a network where the
nodes represent grammatical categories and the links represent types of syntactic (dependency)
relationships. minipar uses grammatical principle instead of grammatical rules which act as
constraints associated with the nodes and links.[roy11]

1.5.1 principle-based parser
principle-based grammars[lin94] use principles like government-binding (gb) theory. let us
consider a sentence in passive voice: the ice-cream was eaten. if a rule-based parser handles
this passive voice while    nding the object of the action    eat    it would use an if-then rule:
if
the
this is a shallow approach. the basic idea of principle-based parser is to replace this shal-
low approach with a much deeper and explanatory set of principles. this set of principles is
classi   ed into generators and filters.

(subject be-verb + ed no object)
make the subject the object

1.5.2 generating principles
generating principles produce possible structures of a given sentence. this class includes:

    x-bar theory: this theory describes the basic shapes of tree allowed in the language. in
natural language there are roughly two forms of tree: function-argument form, like a verb
begins in a verb phrase and argument-function form where x ends in a xp.

    movement principle: movement principle says that any phrase can be moved anywhere.
this would create new possibilities, though this might violate other principles. example:
john likes ice-cream. this can changed after moving to: ice-cream, john likes.

    binding theory: binding theory speci   es how pronouns can be bound to their antecedents.
multiple mappings of one pronoun to different antecedents would create new possible
structures.
example: john thinks he likes ice-cream.
   he    may refer to john or some other person.(two possibilities).
he thinks john likes ice-cream.
   he    refers surely some other person except john.(one possibility)

7

1.5.3 filtering principles
filters remove possible structures which fail some given constraints.

    case filter: case theory speci   es that every noun phrase should be assigned a case. the
subject is given a nominative case and direct objects are given accusative case. if a phrase
tree fails to satisfy that the tree is an invalid one.
example: it is likely that john will win.
john: nominative case. this is a valid sentence structure.
it is likely john to win.
no case for john. this is an invalid sentence structure.

    theta criterion: theta theory determines the number of arguments a verb needs and
assigns thematic roles to its arguments. this brings in the concepts of transitive and
intransitive verbs. transitive verbs should specify the agent and patient of the action.
if any verb requires two objects it must specify the thematic roles of both. one such
example is the verb give which demands what to give and whom to give.
example: john put the book on the shelf.
   put    has two objects book, thing to put, and shelf, place to put. this is valid.
john put the book.
   put    has only objects book, thing to put ,not the place to put. this is invalid.

    empty category principle and locality theory:empty category principle says that
all traces must be properly governed. the trace should not be too far away from its
antecedent.
example: who do you think likes mary?
w hoi do you think [tilikesmary]?
here the trace ti is governed by its antecedent whoi. this is valid.
w hoi do you think that [tilikesmary]?
here the trace ti far away from its antecedent whoi. this is invalid.

given the generator-   lter model, the simplest way build a parser is to cascade the principles

in a sequence.

1.6 automatic id14

[dd02] this is a statistical technique of id14, in which, a statistical classi   er
is trained over a corpora of sentences for the id14(srl) task. two sets
of experiments were described by jurafsky and gildea, which were conducted by them on the
framenet corpus. in the    rst set, inputs to the system were a sentence, a target word, a frame
and the frame element boundaries labeled by hand. the outputs were the frame element labels.
in the second set of experiments, inputs to the system included just a sentence, a target word
and a frame. the system now performed the dual task of frame element boundary identi   cation
and frame element labeling. again the outputs were the frame element labels.

8

figure 1.7: parse tree path example.

1.6.1 features for frame element labeling
the sentence is    rst parsed by a constituent parser to obtain a parse tree. then following features
are derived:

phrase type for every constituent of the sentence its phrase type can be determined by

the constituent parse.

governing category a noun phrase(np) can be directly a constituent of a sentence(s)
or a verb phrase(vp). this is used as a feature for only nps giving a strong indication if it is
used as subject or object of the verb.

parse tree path a parse tree path of a constituent of a sentence is the path of the con-
stituent from the target word in the constituent parse tree which includes the intermediate nodes
and the arrow directions. figure 1.7 shows an example path between verb eat and noun phrase
he as: v b     v p     s     np

position the position feature indicates whether the constituent occurs before or after the

predicate invoking the frame.

voice the voice of the sentence is a feature. active or passive voice is identi   ed with the

help of 10 passive identifying patterns.

head word the head words of each constituent acts as a very useful feature.

1.6.2 features for frame element boundary identi   cation
similar features are also used for frame element boundary identi   cation namely head word,
parse tree path and target word.

9

1.6.3 id203 estimation of a single role
in order to automatically label the semantic role of a constituent, we wish to estimate a prob-
ability distribution indicating how likely the constituent is to    ll each possible role, given the
features described above and the predicate, or target word, t:

p(r|h, pt,gov, position,voice,t)

this could be done by direct counts as:

p(r|h, pt,gov, position,voice,t) = count(r,h,pt,gov,position,voice,t)
count(h,pt,gov,position,voice,t)

but there were on an average 34 sentences per target word in the framenet dataset used.
in general, the data is sparse for estimating the id155 of a label given all the
above features together. this is because, a particular combination of the above six features
along with the target word occurs rarely in the dataset. to overcome this, conditional proba-
bilities of the frame element labels given subsets of above features like p(role | targetword),
p(role|path,targetword) etc. are computed. these are combined using different strategies like
equal linear interpolation, em linear interpolation, geometric mean, back-off linear interpo-
lation and back-off geometric mean. this strategy gives a signi   cant improvement in perfor-
mance over the baseline approach of directly estimating the id155 of the labels
given all the six features in the conditioning set.

1.6.4 id203 estimation of all the roles in the sentence
if we assume that the roles played by the different constituents of a sentence are independent of
each other then the id203 estimation of the previous section is enough to label the roles.
but, it is trivial to note that this is just a simplifying assumption. if we relax this assumption,
then we have to compute role assignment over the entire sentence r   :

r    = argmaxr1...n p(r1...n|t, f1...n)

here, f1...n are the set of features as discussed above. applying bayes rule, removing ele-
ments not contributing to the argmax computation and assuming that features fi are independent
of each other given the target word t, we get the following equation:
r    = argmax{r1...n}p(r1...n|t)   i p( fi|ri,t)

applying bayes rule again and removing constant part of numerator (not contributing to

argmax) we get:

r    = argmax{r1...n}p(r1...n|t)   i

p(ri| fi,t)
p(ri|t)

the frame element identi   cation part can be incorporated in the above evaluation as follows:

r    = argmax{r1...n}p(r1...n|t)   i

p(ri| fi, f ei,t).p( f ei| fi))

p(ri|t)

10

1.6.5 generalizing lexical semantics
the head word feature is observed to be the most useful. but due to large vocabulary of english,
training on all possible head words is infeasible. hence, to generalize our training from a small
set of head words to other head words three techniques viz. automatic id91, id64
and making head word hierarchy using id138 is described.

1.7 extensions to automatic srl

[pwh+04][pwh+05] extend on this basic work. in the [pwh+04], one vs rest support vector
machines(id166) are trained for automatic id14. the features used extend
on the basic features combining features like:

    named entities in constituents named entities (person, organization, loca-
tion, percent, money, time, date) using identi-finder (bikel et al,1999) added
as 7 binary features

    head word pos part of speech tag of the headword
    verb id91 predicate id91 to counter unknown predicates
    partial path to avoid data sparsity parse tree paths in partial forms
    verb sense information word sense of the predicate
    head word of prepositional phrases replacing the preposition with the    rst noun as the

head word

    first and last word/pos in constituent found to be discriminative
    ordinal constituent position to avoid false positives of elements far away from predi-

cate

    constituent tree distance finer way of depicting an existing feature
    constituent relative features features of parents and siblings
    temporal cue words temporal words not captured by ner
    dynamic class content hypotheses of at most two previous nodes belonging to the same

tree

some of these features were given by [shwa03]
[pwh+05] extend their own work by combining parses obtained from semantic parsers

trained using different syntactic views like charniak parser and mini-par dependency parser.

[hy10] model word spans using an 80 state id48 model. taking the hidden states as
features in addition to the previous features an improvement is achieved in open domain srl.

11

1.7.1 other work
[arr09] present an unsupervised argument identi   cation algorithm for srl. using an unsu-
pervised parser which generates unlabeled parse tree and pos tag annotation the algorithm is
able to achieve 56% precision on the argument identi   cation task.

1.8 semi-supervised id14

supervised learning methods deliver reasonably good performance. however, the reliance on
labeled training data, which is both dif   cult and highly expensive to produce, presents a major
obstacle to the widespread application of id14 across different languages and
text genres. even for english, despite the substantial annotation effort involved in the creation
of framenet, the numbers of annotated instances vary greatly across lexical items. also labeled
data is scarce for individual predicates.

a better alternative is to use semi-supervised methods that make use of a small number of
manually labeled training instances to annotate a large number of unlabeled instances which are
similar to the training instances. whereas manually labeled data are expensive to create, unla-
beled data are often readily available in large quantities. the latter approach aims to improve
the performance of a supervised id14 system by enlarging its training set
with automatically inferred annotations of unlabeled sentences. the key idea of this approach
is to    nd novel instances for classi   er training based on their similarity to manually labeled
seed instances. the underlying assumption is that sentences that are similar in their lexical ma-
terial and syntactic structure are likely to share a frame semantic analysis. the annotation of an
unlabeled sentence can therefore be inferred from a suf   ciently similar labeled sentence.

for example, seed sentence:
[lee]agent punched[john]victim[in the eye]body part.
unlabeled sentences:
bill will punch me in the face. i punched her hard in the head.
the unlabeled sentences are both structurally and semantically similar to the seed sentence.
now, in order to use these new sentences as training data we must somehow infer their semantic
roles. we can probably guess that constituents in the same syntactic position must have the same
semantic role, especially if they refer to the same concept (e.g., body parts) and thus label in the
face and in the head with the role body part. analogously, bill and i would be labeled as agent
and me and her as victim.

[bill]agentwill punch[me]victim[in the face]body part.
[i]agent punched[her]victimhard[in the head]body part.

1.8.1 learning method
this method needs a small seed corpus that has been manually annotated but not on a scale that
is suf   cient for high-performance supervised learning. for each sentence in the seed corpus, a
number of similar sentences are selected from an unlabeled expansion corpus. these are auto-
matically annotated by projecting relevant semantic role information from the labeled sentence.
the similarity between two sentences is compared by measuring whether their arguments have
a similar structure and whether they express related meanings (concepts). the seed corpus is
then enlarged with the k most similar unlabeled sentences to form the expanded corpus.

12

figure 1.8: labeled dependency graph

lemma
blood
vein
again

gramrole

subj

iobj through

mod

semrole

fluid
path
   

table 1.2: predicate-argument structure for the verb course in figure 1.8

extracting predicate-argument structures

the method operates over labeled dependency graphs. figure 1.8 shows the labeled dependency
graph for the sentence we can feel the blood coursing through our veins again. the frame is
fluidic motion and the roles are    uid, path and the verb, which is called the frame evoking
element (fee).

directed edges (without dashes) represent dependency relations between words, edge labels
denote types of grammatical relations (e.g. subj, aux). the verbs, i.e. frame evoking ele-
ments, in the seed and unlabeled corpora are represented by their predicate-argument structure.
speci   cally, the direct dependents of the predicate course (e.g. blood or again in figure 1.8) and
their grammatical roles (e.g. subj, mod) are recorded. prepositional nodes are collapsed, i.e.,
the prepositions object and a composite grammatical role are recorded (like iobj through,
where iobj stands for prepositional object and through for the preposition itself). for each
argument node, the semantic roles it carries are recorded, if any. all surface word forms are
lemmatized.

an example of the argument structure information we obtain for the predicate course 1.8 is

shown in table 1.2.

measuring similarity

for each frame evoking verb in the seed corpus, a labeled predicate-argument representation
similar to table 1.2 is created. all sentences from the unlabeled corpus containing the same verb
is then extracted. since a verb may invoke different frames with different roles and predicate-
argument structure, all the extracted sentences are not suitable instances for adding to the train-

13

ing data. therefore only sentences resembling the seed annotations must be selected.

the idea used for selection is that verbs appearing in similar syntactic and semantic con-
texts will behave similarly in the way they relate to their arguments. estimating the similarity
between two predicate-argument structures amounts to    nding the highest-scoring alignment
between them, i.e., given a labeled predicate-argument structure pl with m arguments and an
unlabeled predicate-argument structure pu with n arguments,    nd and score all possible align-
ments between these arguments. an alignment is an injective function

where

   : m       {1, ...,n}

m       {1, ...,m}

thus we choose a subset of arguments from the labeled predicate-argument structure and map
each of the arguments to some argument of the unlabeled predicate-argument structure on a
one-one basis. it can be seen that the alignment function allows for partial alignments, i.e. there
can be partial alignment on both sides.

each alignment    is scored using a similarity function sim(  ) de   ned as:

(cid:16)

(cid:17)

(cid:16)

a.syn

gl
i,gu

  (i)

+ sem

wl
i,wu

  (i)

(cid:17)    b
(cid:17)

(cid:16)

   
i  m  

and

where syn(gl

i,gu

  (i)) denotes the syntactic similarity between grammatical roles gl

i and gu

  (i)

sem((wl

i,wu

  (i)) denotes the similarity between the head words wl

i and wu

  (i).

i,gu

the goal is then to    nd an alignment such that the similarity function is maximized. this
optimization problem is a generalized version of the linear assignment problem. the best align-
ment crucially depends on estimating the syntactic and semantic similarity between the argu-
ments. syn(gl
  (i)) is set to 1 if the relations are identical, set to some a <= 1 if the relations
are of same type but different subtype and to 0 otherwise. the semantic similarity is measured
with a semantic space model. the meaning of each word is represented by a vector of its co-
occurrences with neighboring words. the cosine of the angle of the vectors representing wl
and wu quanti   es their similarity. the parameter a counterbalances the importance of syntactic
and semantic information, while the parameter b acts as a threshold for the lowest acceptable
similarity value for an alignment between two arguments is possible.

figure 1.9 graphically illustrates the alignment projection problem. here the aim is to
project the semantic role information from the seed blood coursing through our veins again onto
the unlabeled sentence adrenalin was still coursing through her veins. the predicate course has
three arguments in the labeled sentence and four in the unlabeled sentence. there are 73 pos-
sible alignments in this example. each alignment is scored by taking the sum of the similarity
scores of the individual alignment pairs (e.g. between blood and be, vein and still and so on).
in this example, the highest scoring alignment is between blood and adrenalin, vein and vein,
and again and still, whereas be is left unaligned. note that only vein and blood carry semantic
roles which are projected onto adrenalin and vein, respectively.

14

figure 1.9: labeled dependency graph

1.8.2 projecting annotations
once the best alignment between seed sentence and unlabeled sentence is obtained, the roles
are projected, resulting in a labeled sentence. these projections form the expansion corpus. for
each seed sentence, k most similar neighbors are added to the training data. the parameter k
controls the trade-off between annotation con   dence and expansion size.

1.9 statistical method for unl relation label generation

nguyen et al. [ni06] presents a statistical technique for relation generation. to the besy of my
knowledge, this is the only attempt at unl generation using statistcial techniques. this method
only tackles the simpler problem of generating relation labels given the source and destination
phrases. the system extracts various features representing the source and destination phrases
and the relationships between them. the training step    nds the id155 of each
relation given the feature values. the system is described in detail in the following sections.

1.9.1 feature generation
the features used are similar to the ones described in gildea et al. [dd02]. these are enumer-
ated below.

    phrase type: pos tag of the source and destination phrase. the charniak parser tags is

used.

    head word: this is the root of the phrase in the syntactic parse.
    voice: this is only applicable to verbs. for other features, the value is unspeci   ed.
    dependency path: this is the path from the source phrase to the destination phrase in the

dependency parse tree.

    syntactic cross path: this is the path from the source phrase to the destination phrase in

the syntactic parse tree.

15

1.9.2 training
the number of relations for each feature vector is counted in the training data to estimate the
id155 of each relation given the feature vector in the test data. since data is
sparse, the probabilities are also calculated for individual features.

1.9.3 testing
the probabilities are combined through linear interpolation to get the    nal id203 estima-
tion. the predicted relation is an argmax over the probabilitiy values. the system achieves a
best accuracy of 73.2% when tested on data supplied by the undl foundation.

1.10 overview of unl system at geta

this section describes the unl system developed at geta, france by etienne blanc [bla08][bla11].unl
is an arti   cial language based on semantic graphs. it stems from the pivot language of one of
the atlas ii, a japanese-english machine translation (mt) system. unl is intended for much
broader applications than just machine translation. unl was created and is maintained by the
former leader of the atlas team, hiroshi uchida. the link between unl and various natural
languages are the responsibilities of the    language centers   . geta is the language center for
french. unl has huge potential for dissipating information on the internet. the more dif   cult
task of converting from the source language to unl (enconversion) needs to be done once,
irrespective of the number of target languages. the comparatively easier task of deconversion
from the well structured non ambiguous unl graph to natural language (deconversion) has to
be done for each target language.

the unl system was developed at geta as part of the ariane machine translation system.
the system incorporates a french enconverter and deconverter.
in addition, a unl graph
editor is provided. sixteen languages such as english, chinese, arabic, russian, hindi are
linked to french through universal word dictionaries.

1.10.1 universal word resources
the system uses three universal word resources

    the unl knowledge base which constitutes a network structure where uws are inter-

connected through relations of unl.

    the uw++ dictionary developed at geta
    the uw dictionary developed at cfilt, iit bombay having entries for indian languages.

1.10.2 the enconversion and deconversion process
the process consists of three major steps:

    analysis: the analysis steps elaborates a so called    multilevel    tree. this tree re   ects
the syntagmatic structure of the input sentence, but bears 3 levels of information : the
morphologic, syntactic and semantic ones.

16

    transfer: the transfer step is mainly lexical : it translates the source words (uws) into
the target french words (lemmas). the unl-french dictionary is used for this step. the
morphologic and syntactic informations are no more relevant in the target language..

    generation: the generation step builds up the multilevel target tree, and    nally the target

text.

1.10.3 graph to tree conversion
ariane translation system uses a tree structure for unl. when translating from unl graphs
produced by other systems that uses a graph structure, the deconversion    rst process converts
the graph into a tree using various rewrite rules. some of the conversion rules are outlined
below:

    nodes with multiple parents: in this case, the relation of the child with one of the parents

is reversed. for example, obj(parent, child) becomes invobj(child, parent)

    closed circuit: circuits do not exist in an unl graph if directionality is considered. but
if in the corresponding undirected graph, a closed circuit exists, a node is duplicated to
create an undirected graph.

    compound uws: compound uws are replaced in the main graph with the corresponding

subgraphs.

1.10.4 deployment
the system is available as a mobile phone application and also through a web based interface.
interactive modes of lexical transfer and execution with trace is possible.
in the interactive
mode for lexical transfer, the possible french equivalents for a uw is displayed and the user
has the option to select from the option. if no equivalent is found, the user may enter one, and
the dictionary is automatically augmented.

the author also speaks about the possibilities of applying unl in speech mt in the future.

1.11 summary

id14 is a fast developing area of research. it is a shallow level representation
of semantics and has applications in large number of natural language processing tasks. these
include the semantic web, information extraction, id123 etc. with advances in
both rule based and statistical techniques id14 is a rapidly developing area
in nlp.

17

18

chapter 2

dependency grammar and dependency
parsing

dependency grammar is a theory that de   nes how the words in a sentence are connected to each
other. the basic idea is that in a sentence all but one word is dependent on some other word.
the word which is independent is usually the main verb of the sentence. consider the following
example :

sentence: a man sleeps.

the main verb in the sentence is sleeps which is independent of all the other words and gives
the central idea of the sentence. if the sentence is about a sleep, then there should be an agent
or a subject of this action. thus the word man is the agent or the subject who is sleeping and
it depends on the word sleeps. we can also say that man    lls the verb-argument frame for the
verb sleeps. now, in the sentence we are not talking about some speci   c man but about some
inde   nite man. this is denoted by the article a. thus the word a is dependent on the word man,
or in other words, modi   es the word man. thus, the dependency relations coming out of this
simple relation is given in 2.1.

figure 2.1: dependency relation

19

figure 2.2: projective example

2.1 robinson   s axioms

robinson(1970) [rob70] described four basic axioms for dependency grammar which govern
the well-formedness of dependency structure. it says that in a dependency structure:

1. one and only one element is independent.

2. all others depend directly on some element.

3. no element directly depends on more than one element.

4. if a depends directly on b and some element c intervenes between them (in the linear
order of the string), then c depends directly on a or b or some other intervening element.

the    rst axiom has the result that the dependency structure has a root node. the second
axiom allows the dependency structure to be a single connected component. the third axiom is
also called the single-head property says that every element can have at most one parent. thus,
the dependency structure can be a tree or a graph and different formalisms exist for both of
them. the fourth axiom is the projective property, but many formalisms do not obey this axiom.

2.2 projective and non-projective dependency structures

a dependency structure is said to be projective, if it follows the fourth axiom of robinson. a
projective structure essentially means that no two edges of the graph cut-across each other. let
us look at an example 2.2.

as seen in the    gure, no edges of the graph here cut-across each other. algorithm designing
for projective dependency structures is easier. but the problem with projective formalisms is,
that languages with relatively free word order do not follow projectivity constraint. an example
of a non-projective dependency structure is given in    gure 2.3.

as can be seen, this sentence has a projective version, as seen in    gure 2.4 if we modify the

word order.

these three examples have been taken adapted from [niv08].

20

figure 2.3: non-projective example

figure 2.4: projective version of the non-projective example

21

2.3 id33 techniques

id33 is the method of parsing sentences into dependency structures. in this
section, we look at some of the prevalent id33 techniques.

2.3.1 data-based dependency parser
data-driven id33 uses machine learning from linguistic data to parse new sen-
tences.
in this report the supervised approaches will be discussed. the sentences used for
machine learning are annotated with their correct dependency structures. the goal is to learn
a good predictor of the dependency tree of a sentence given an input sentence. a model for
this is m where m = (t, p, h), where t is the given set of constraints that helps in forming the
structures for the sentence, p is a set of parameters to be learned from data and h is a    xed
parsing algorithm.

2.3.2 transition-based id33
transition-based parsing system parameterizes a model to learn to predict the next transition
given the input sentence and the parse history. the dependency trees are predicted using a
greedy, deterministic algorithm.

a transition system generally contains a set of states, a set of rules to de   ne transition of one
state to another, an initial state and a set of    nal states. a simple stack-based transition system
which uses a form of id132 will be explained. a con   guration would be de   ned
as a triple of stack, input buffer and a set of dependency arcs. a formal de   nition is:
given a set of dependency types r, an input sentence s=w0w1...wn, the con   guration c of the
sentence s is de   ned as c = (  ,  ,a), where

       is a stack of words wi
       is the input buffer
    a is a set of dependency arcs (wi,r,w j)     vsrvs.
a con   guration c contains partially processed words in the stack, remaining words to be
processed in the buffer and the partially constructed dependency tree in the form of dependency
arcs in the set a.    and    are represented in the form of lists. the stack    has its top at the right.
initial con   guration: c0(s) = ([w0]  , [w1,w2, ,wn]  ,  ).
terminal con   guration: cm(s) = (  , []  ,a) for any    and a.
make w0=root and push it to stack and we reach con   guration c0(s).

2.4 summary

dependency grammar is one of the earliest language grammars which has regained it   s im-
portance with applications in information extraction and various natural language processing
tasks. a wide variety of dependency formalisms exist but most of them follow the basic axioms
of robinson. dependency parsers like the stanford dependency parser and the xle parser

22

are now available and the accuracy of these parsers is improving rapidly with the use of both
statistical and rule-based techniques.

23

24

chapter 3

techniques for corpus based learning

manual encoding of linguistic information have often been challenged by annotated corpus
based learning methods as a technique for providing linguistic knowledge to a system. many
rule based systems suffer from low recall rates due to linguistic knowledge acquisition bottle-
neck. automatic linguistic information extraction from text corpus can overcome this and thus
help create robust and highly accurate natural language processing systems. this chapter inves-
tigates various statistical and rule based techniques for learning from annotated corpus. with
the availability of large quantities of corpus, corpus learning techniques have become a viable
option. some successful application of corpus-based techniques are in building parts-of-speech
taggers, empirically assigning probabilities to grammar rules, computing statistical measures of
lexical association, id51 and machine translation.

3.1 transformation-based error-driven learning

although corpus-based approaches can successfully model various linguistic information, the
linguistic insight into the model behavior is often lost in huge quantities of statistical data. for
example, a hidden markov model based parts of speech tagger tags words based on a model
uses a huge database of p(tag |previous tags) and p(word |tag) as the prediction model which
lack any linguistic intuition. this kind of indirect modeling can make it dif   cult to analyze,
understand and improve the ability of these approaches to model underlying linguistic behavior.
the point to be noted here is that unlike rule-based systems, corpus-based methods succeed
without capturing the true complexities of knowledge. they achieve this due to the fact that
complex linguistic phenomena can often be indirectly observed through simple epiphenomena.

consider the following sentences for example.
    the frog kept an eye on the    y.
    the pilot will not    y the aircraft.

the word    y can be pos-tagged in the above sentences without linguistic insight into phrase
structure by observing that a word that lies one or two words to the right of a modal verb is a
verb whereas a word following a determiner is a noun.

as demonstrated above, simple stochastic id165 taggers can obtain very high accuracy
simply by observing the phenomenon in small neighborhoods without recourse to the underly-
ing linguistic structure. the work by brill [bri95] identi   es the problems associated with such

25

figure 3.1: transformation-based error-driven learning

blind dependence on corpus statistics. the understanding of the model generated by corpus
based methods is central to detecting when the approximation model deviates from the actual
linguistic phenomenon.

the author proposes a new approach called transformation-based error-driven learning. in
this approach, the linguistic information that is learned is represented in a concise and compre-
hensive form. thus it can be used to discover how learnt models can be coupled with the under-
lying linguistic phenomenon. the method has been applied to problems such as part-of-speech
tagging, prepositional phrase attachment disambiguation, syntactic parsing, letter-to-sound gen-
eration and id103.

3.1.1 description
figure 3.1 illustrates the working of transformation-based error-driven learning. the unanno-
tated text is    rst fed to an initial-state annotator. the initial-state annotator can range from a
naive system that randomly annotates the text to a manually designed annotation framework.
for example, the initial-state annotator for a parts-of-speech (pos) tagger may assign the same
or random pos tag to all words or it may assign the most common tag for each word. similarly
for syntactic parsing, the initial-state annotations may range from the output of a sophisticated
parser to random tree structure with random nonterminal labels.

the annotated text is then compared to the truth. a manually annotated corpus serves as
reference for truth. the goal is to learn transformations that takes the annotation closer to the

26

figure 3.2: bracketing rewrite rule

truth. the transformations are written in the form of rewrite rules. in addition, the transforma-
tion are also ordered in the sequence in which they may be applied. a transformation essentially
consists of two components

    a rewrite rule, which speci   es the effect of applying the transformation
    a triggering environment, which speci   es the conditions under which the transformation

may be applied

for example, in the id52 example taken earlier, the following transformation may

be applied

    rewrite rule: change the tag from verb to noun
    triggering environment: the previous word is a determiner
in case of parsing, the rewrite rule may be in the form of bracketing. figure 3.2 illustrates
such a rule. here a, b and c are either terminals or nonterminals. such a transformation may
be applied to the initial-state parse tree ((john eats) rice) to obtain (john (eats rice)).

the above technique is essentially a greedy algorithm where at each step we choose the
transformation that reduces the error by the greatest extent. for this purpose, a set of possible
transformations is prede   ned. the algorithm terminates when no transformation can reduce
the error further. the author also suggests simulated annealing and look-ahead window based
learning as alternative methods. figure 3.3 shows the operation of the technique. the space of
transformations in this case is t1 to t4. the transformation t2 achieves the largest decrease in
error so it is chosen as the    rst transformation and then t3 is chosen. the algorithm terminates
after this since no further reduction in error can be achieved.

this algorithm can be applied to many different problems by specifying the following.
    the initial state annotator
    the set of transformations
    the objective function to be optimized

the order of application of transformations is important in many cases. it is also important
to consider whether while applying a transformation, all triggering environments should be
identi   ed    rst and then the rewrite rules are applied or the application can be done one by one.

27

figure 3.3: example of terminaton-based error-driven learning

3.1.2 observations
the work also compares this learning method with id90 and shows that decision tree
classi   cations are a subset of the valid transformations using this method. however, the method
is free from the data sparsity and over-   tting problems associated with id90 since each
transformation is applied on the entire training data set.

in addition, unlike id90, the effect of transformations at each step are re   ected on
the data before the next transformation is applied. thus the later transformations can make a
more informed decision.

3.2 statistical dependency analysis

the task of    nding word-word dependencies is closely related to the task of generating relations
between words. the work by [ym99] uses a bottom up technique for generating word-word
dependencies. features are extracted from the parsed annotated training data and are used to
learn id166 classi   ers for dependency generation.

3.2.1 parsing actions
a parsing iteration is performed on the sentence from left to right. at any time, a parsing action
is applicable to two neighboring words also called target nodes. the following parsing actions
are applicable to the input sentences.

28

figure 3.4: example of shift action showing states before and after the action

figure 3.5: example of right action showing states before and after the action

    shift: no dependency can be constructed between the nodes. the point of focus shifts

right by one unit.

    right: a dependency is constructed between the nodes with the left node becoming the
child of the right node. the right frontier focus point is unchanged. ]item left: a depen-
dency is constructed between the nodes with the right node becoming the child of the left
node. the left frontier focus point is unchanged.

figures 3.4 and 3.5 show the results of shift and right actions applied to target nodes. the
actions right and left may be applied only when all dependencies of the node that becomes the
child has been resolved. once a node becomes a child, it is removed from further consideration.

3.2.2 algorithm
the work describes an iterative algorithm for constructing the parse tree based on the above
parser actions. the algorithm iteratively scans the sentence from left to right, choosing parsing
actions from the contextual information around a node and the applying the selected parser
action. after each action, the target nodes shift right by one unit. the algorithm terminates
when only the root node is left or no more dependencies can be constructed. the choice of
parser action is taken by training support vector machines. this is discussed in the next section.

3.2.3 classi   cation
support vector machines (id166s) are trained on the features extracted. the choice of id166s is
made due to the following advantages over other classi   ers.

    high generalization performance in high dimensional feature space: since id166s opti-
mize based on maximum margin strategy, it theoretically guarantees low generalization
error.

29

    learning with combination of features using polynomial id81s: kernel func-

tions allow id166s to deal with non-linear classi   cation.

since the choice of parser action is a multi class problem, three binary classi   ers are con-
structed taking the actions pairwise: left vs. right, shift vs. left and right vs. shift. majority
voting among the three classi   ers gives the chosen action.

the left context is de   ned as the indexes on the left of the target nodes and the right context
is de   ned as the indexes on the right of the target nodes. the features used are of the form (p,
k, v) where p denotes the position with respect to the target nodes (negative values denote left
context, positive values denote right context), k denotes the type of the feature and v denotes its
value. the types of features used for this task are discussed below.

    pos: part of speech tag string
    lex: word string
    ch-l-pos: pos tag string of the child node modifying to the parent node from left side
    ch-l-lex: word string of the child node node modifying to the parent node from left side
    ch-r-pos: pos tag string of the child node modifying to the parent node from right side
    ch-r-lex: word string of the child node modifying to the parent node from right side

for example, in    gure 3.6, some of the features extracted are
    (-1, pos, nns): the word to the left of    of    has a pos tag nns
    (-1, lex, sellers): the word to the left of    of    is sellers
    (-1, ch-r-lex, the): the word modifying word to the left of    of    is the
the authors note that the task of training the id166 is quadratic to cubic in the number of
training examples. thus to reduce the training cost, the data set is divided into groups on the
basis of the pos tag of the left target node. thus a id166 is constructed for each pos tag of the
left node and the appropriate id166 is chosen during dependency generation.

a id81 is used to allow non linear combination of features. the id81

chosen for this task is

.

3.3 summary

(cid:48)(cid:48)

(cid:48)

(x

.x

+ 1)2

the chapter presented two methods for corpus based learning. the    rst work provides a gen-
eral architecture for developing a system which learns better rules through error analysis and
feedback. the other work describes features that can be used for    nding dependencies between
words. these works provide strong motivation for an automated hybrid system.

30

figure 3.6: example of contextual information

31

32

bibliography

[arr09] o. abend, r. reichart, and a. rappoport. unsupervised argument identi   cation
in proceedings of the joint conference of the 47th
for id14.
annual meeting of the acl and the 4th international joint conference on natural
language processing of the afnlp: volume 1-volume 1, pages 28   36. association
for computational linguistics, 2009.

[bfl98a] collin f. baker, charles j. fillmore, and john b. lowe. the berkeley framenet
project. in coling-acl    98: proceedings of the conference, pages 86   90, mon-
treal, canada, 1998.

[bfl98b] collin f baker, charles j fillmore, and john b lowe. the berkeley framenet
in proceedings of the 17th international conference on computational
project.
linguistics-volume 1, pages 86   90. association for computational linguistics,
1998.

[bla08]

etienne blanc. le langage unl au geta, november 2008.

[bla11]

[bri95]

[dd02]

[dut11]

[fel98]

[hy10]

etienne blanc. the lexical database of the french unl development environ-
ment, february 2011.

eric brill. transformation-based error-driven learning and natural language pro-
cessing: a case study in part-of-speech tagging. computational linguistics,
21(4):543   565, 1995.

gildea daniel and jurafsky daniel. automatic labeling of semantic roles.
computational linguistics, 2002.

in

subhajit dutta. semantics extraction from text, stage 1 report. master   s thesis,
iit bombay, 2011.

christiane fellbaum. id138: an electronic lexical database. bradford books,
1998.

f. huang and a. yates. open-domain id14 by modeling word
in proceedings of the 48th annual meeting of the association for com-
spans.
putational linguistics, pages 968   978. association for computational linguistics,
2010.

[inf11]

in   ection. http://en.wikipedia.org/wiki/inflection, october 2011.

33

[kp02]

[kp03]

[lin94]

[ni06]

paul kingsbury and martha palmer. from treebank to propbank. in proceedings of
the 3rd international conference on language resources and evaluation (lrec-
2002), pages 1989   1993. citeseer, 2002.

p. kingsbury and m. palmer. propbank: the next level of treebank. in proceedings
of treebanks and lexical theories, 2003.

d. lin. principar: an ef   cient, broad-coverage, principle-based parser. in proceed-
ings of the 15th conference on computational linguistics-volume 1, pages 482   488.
association for computational linguistics, 1994.

dat pt nguyen and mitsuru ishizuka. a statistical approach for universal network-
ing language-based id36. in proc. int. conf. on research, innovation
and vision for the future, pages 153   160. citeseer, 2006.

[niv08]

j. nivre. sorting out id33. advances in natural language process-
ing, pages 16   27, 2008.

[pwh+04] s. pradhan, w. ward, k. hacioglu, j. martin, and d. jurafsky. shallow semantic
parsing using support vector machines. in proceedings of hlt/naacl, pages 233   
240, 2004.

[pwh+05] s. pradhan, w. ward, k. hacioglu, j.h. martin, and d. jurafsky. semantic role
labeling using different syntactic views. in proceedings of the 43rd annual meet-
ing on association for computational linguistics, pages 581   588. association for
computational linguistics, 2005.

[rob70]

[roy11]

[sch05]

j.j. robinson. dependency structures and transformational rules. language, pages
259   285, 1970.

gourab roy. semantics extraction from text, stage 2 report. master   s thesis, iit
bombay, 2011.

karin kipper schuler. verbnet: a broad-coverage, comprehensive verb lexicon.
phd thesis, university of pennsylvania, 2005.

[shwa03] m. surdeanu, s. harabagiu, j. williams, and p. aarseth. using predicate-argument
structures for information extraction. in proceedings of the 41st annual meeting
on association for computational linguistics-volume 1, pages 8   15. association
for computational linguistics, 2003.

[st95]

[ym99]

d.d.k. sleator and d. temperley. parsing english with a link grammar. arxiv
preprint cmp-lg/9508004, 1995.

hiroyasu yamada and yuji matsumoto. statistical dependency analysis with sup-
port vector machines. machine learning, 34(1-3):151   175, 1999.

34

