information extraction: theory 
and practice

ronen feldman
computer science department
bar-ilan university, israel

feldman@cs.biu.ac.il

outline

  introduction to text 

mining

  information extraction

  entity extraction
  relationship extraction
  ke approach

  ie languages

  ml approaches to ie

  id48

  id2
  evaluation

  link detection

  introduction to ld
  architecture of ld 

systems

  9/11
  link analysis demos

1

background

  rapid proliferation of 

information available in 
digital format

  people have less time to 
absorb more information

the information landscape

problem

lack of tools to handle 

unstructured data

unstructured

(textual)

80%

structured
(databases)

20%

2

find documents

display information

matching the query

relevant to the query 

actual information buried 
inside documents

extract information from 
within the documents

long lists of documents

aggregate over entire 
collection

id111

input

documents
documents

output

patterns
patterns

connections
connections

profiles
profiles

trends
trends

seeing the forest for the trees

3

let id111 do the legwork for you

id111
id111

find material
find material

readread

understand
understand

consolidate
consolidate

absorb / act
absorb / act

what is unique in id111?

  feature extraction.
  very large number of features that 
represent each of the documents.

  the need for background knowledge.
  even patterns supported by small number 

of document may be significant.

  huge number of patterns, hence need for 

visualization, interactive exploration.

4

document types

  structured documents

  output from cgi

  semi-structured documents

  seminar announcements
  job listings
  ads

  free format documents

  news
  scientific papers

text representations

  character trigrams
  words
  linguistic phrases
  non-consecutive phrases
  frames
  scripts
  role annotation
  parse trees

5

the 100,000 foot picture

development
evironment

intelligent auto-tagging

(c) 2001, chicago tribune. 
visit the chicago tribune on the internet at 
http://www.chicago.tribune.com/ 
distributed by knight ridder/tribune 
information services. 
by stephen j. hedges and cam simpson 

      . 

the finsbury park mosque is the center of 
radical muslim activism in england. through 
its doors have passed at least three of the men 
now held on suspicion of terrorist activity in 
france, england and belgium, as well as one 
algerian man in prison in the united states. 

``the mosque's chief cleric, abu hamza al-
masri lost two hands fighting the soviet 
union in afghanistan and he advocates the 
elimination of western influence from 
muslim countries. he was arrested in london 
in 1999 for his alleged involvement in a 
yemen bomb plot, but was set free after 
yemen failed to produce enough evidence to 
have him extradited. .''

      

<facility>finsbury park mosque</facility>

<country>england</country>
<country>france </country>

<country>england</country>

<country>belgium</country>

<country>united states</country>

<person>abu hamza al-masri</person>

<personpositionorganization>
<offlen offset="3576" length=   33" />
<person>abu hamza al-masri</person>
<position>chief cleric</position>
<organization>finsbury park mosque</organization>
</personpositionorganization>

<city>london</city>

<personarrest>
<offlen offset="3814" length="61" />
<person>abu hamza al-masri</person>
<location>london</location>
<date>1999</date>
<reason>his alleged involvement in a yemen bomb            

plot</reason>

</personarrest>

6

tagging a document

bc-dynegy-enron-offer-update5
dynegy may offer at least $8 bln to acquire 
enron (update5)
by george stein
sourcec.2001 bloomberg news
body

      . 

``dynegy has to act fast,'' said roger 
hamilton, a money manager with john 
hancock advisers inc., which sold its enron 
shares in recent weeks. ``if enron can't get 
financing and its bonds go to junk, they lose
counterparties and their marvelous business 
vanishes.''

moody's investors service lowered its 

rating on enron's bonds to ``baa2'' and 
standard & poor's cut the debt to ``bbb.'' in 
the past two weeks.

      

<company>dynegy inc</company>

<person>roger hamilton</person>

<company>john hancock advisers inc. </company>

<personpositioncompany>
<offlen offset="3576" length="63" />
<person>roger hamilton</person>
<position>money manager</position>
<company>john hancock advisers inc.</company>
</personpositioncompany>

<company>enron corp</company>

<company>moody's investors service</company>

<creditrating>
<offlen offset="3814" length="61" />
<company_source>moody's investors service</company_source>
<company_rated>enron corp</company_rated>
<trend>downgraded</trend> <rank_new>baa2</rank_new>
<__type>bonds</__type>
</creditrating>

intelligent auto-tagging

compensatory glomerular growth after unilateral nephrectomy is vegf 
dependent.

flyvbjerg a, schrijvers bf, de vriese as, tilton rg, rasch r.

medical department m, medical research laboratories, institute of 
experimental clinical research, aarhus university, dk-8000 aarhus, 
denmark. 
various growth factors and cytokines have been implicated in different 
forms of kidney enlargement. vascular endothelial growth factor (vegf) 
is essential for normal renal development and plays a role in diabetic 
glomerular enlargement. to explore a possible role for vegf in 
compensatory renal changes after uninephrectomy, we examined the effect 
of a neutralizing vegf-antibody (vegf-ab) on glomerular volume and 
kidney weight in mice treated for 7 days. serum and kidney insulin-like 
growth factor i (igf-i) levels were measured, since igf-i has been 
implicated in the pathogenesis of compensatory renal growth, and vegf 
has been suggested to be a downstream mediator of igf-i. placebo-treated 
uninephrectomized mice displayed an early transient increase in kidney 
igf-i concentration and an increase in glomerular volume and kidney 
weight. in vegf-ab-treated uninephrectomized animals, increased 
glomerular volume was abolished, whereas renal hypertrophy was 
partially blocked. furthermore, the renal effects of vegf-ab
administration were seen without affecting the renal igf-i levels. in 
conclusion, these results demonstrate that compensatory glomerular
growth after uninephrectomy is vegf dependent.

<category>biomedical</ category >

<gene>vegf</gene>
<disease/syndrome>

kidney enlargement</disease/syndrome>
<gene>vascular endothelial growth factor, 

vegf</gene>

<disease/syndrome>diabetic glomerular

enlargement </ disease/syndrome>

<gene-disease/syndrome>

<gene>vascular endothelial growth factor, 
vegf</gene>
<disease/syndrome>diabetic glomerular
enlargement </disease/syndrome>

</gene-disease/syndrome >

<gene>vegf</gene>
<gene> insulin-like growth factor i, igf-i</gene>

<gene>igf-i</gene>

<gene> vegf </gene>

<gene>igf-i</gene>
<gene>igf-i</gene>
<phenotype>increase in glomerular volume and 
kidney weight</ phenotype >

<gene-phenotype>

<gene>igf-i</gene>
<phenotype>increase in glomerular volume 

and kidney weight</phenotype >
</gene- phenotype >

<gene>igf-i</gene>
<gene> vegf </gene>

7

a tagged news document

example

phosphorylation

template

{

kinase

protein

some kinase

x

      bes1 is phosphorylated and 
appears to be destabilized by the 
glycogen synthase kinase-3 
(gsk-3) bin2      

kinase

glycogen synthase
kinase-3 (gsk-3) bin2

protein

bes1

phosphorylation

8

leveraging content investment

any type of content

    unstructured textual content (current focus)

    structured data; audio; video (future) 

in any format

    documents; pdfs; e-mails; articles; etc

       raw    or categorized

    formal; informal; combination

from any source

    www; file systems; news feeds; etc.

    single source or combined sources

the tagging process (a unified 
view)

r
r
lle
lle
o
o
r
r
t
t
n
n
o
o
 c
 c
g
g
in
in
g
g
g
g
a
a
t
t

9

id111 horizontal applications

events  

detections & 
notifications(cid:0)

crm analysis(cid:0)

market  

research(cid:0)

competitive   
intelligence(cid:0)

enterprise portals

(eip)

id111
id111

lead generation(cid:0)

ip 

management (cid:0)

content 

uniformity

content
management

new products
offerings

id111: vertical applications

legal

chemical

global 2000

biotech

pharma

insurance

publishing

financial
services

defense

10

information extraction

what is information extraction?

  ie does not indicate which documents need to be read 

by a user, it rather extracts pieces of information that 
are salient to the user's needs. 

  links between the extracted information and the 

original documents are maintained to allow the user to 
reference context.

  the kinds of information that systems extract vary in 

detail and reliability. 

  named entities such as persons and organizations 

can be extracted with reliability in the 90th percentile 
range, but do not provide attributes, facts, or events 
that those entities have or participate in.  

11

relevant ie definitions

  entity: an object of interest such as a person 

or organization.

  attribute: a property of an entity such as its 

name, alias, descriptor, or type.

  fact: a relationship held between two or more 

entities such as position of a person in a 
company.

  event: an activity involving several entities 

such as a terrorist act, airline crash, 
management change, new product 
introduction.

example (entities and facts)

 

fletcher maddox, former dean of the 
ucsd business school, announced the 
formation of la jolla genomatics together 
with his two sons. la jolla genomatics will 
release its product geninfo in june 1999. 
geninfo is a turnkey system to assist 
biotechnology researchers in keeping up 
with the voluminous literature in all 
aspects of their field. 

  dr. maddox will be the firm's ceo. his 

son, oliver, is the chief scientist and 
holds patents on many of the algorithms 
used in geninfo. oliver's brother, 
ambrose, follows more in his father's 
footsteps and will be the cfo of l.j.g. 
headquartered in the maddox family's 
hometown of la jolla, ca.

locations:  artifacts:  dates: 

organizations: 

persons:  
fletcher maddox  ucsd business school  la jolla 
dr. maddox 
oliver 
oliver 
ambrose 
maddox 
 

la jolla genomatics 
la jolla genomatics 
l.j.g. 
  
  

ca 
  
  
  
  

ucsd business 
school 
la jolla genomatics 
la jolla genomatics 
la jolla genomatics 

employee_of  organization 
employee_of 
employee_of 
employee_of 
employee_of 

person 
fletcher 
maddox 
fletcher 
maddox 
oliver 
ambrose 
organization 
artifact  product_of 
geninfo 
la jolla genomatics 
product_of 
location   location_of  organization 
la jolla 
ca 
 

location_of  la jolla genomatics 
location_of  la jolla genomatics 

geninfo 
geninfo 
  
  
  
  

june 1999 
  
  
  
  
  

12

events and attributes

company-formation_event:  
company: 
la jolla genomatics 
principals:  fletcher maddox 

oliver 
ambrose 
  
  

date: 
capital: 

release-event:  

company:  la jolla genomatics 
product:  geninfo 
date: 
cost: 
 

june 1999 
  

name: 

descriptor: 

fletcher maddox 
maddox 
former dean of the ucsd 
business school 
his father 
the firm's ceo 
person 
category: 
name: 
oliver 
descriptor:  his son 

category: 
name: 
descriptor:  oliver's brother 

chief scientist 
person 
ambrose 

the cfo of l.j.g. 
person 
ucsd business school 
  

category: 
name: 
descriptor: 
category:  organization 
name: 

la jolla genomatics 
l.j.g. 
  

descriptor: 
category:  organization 
name: 
descriptor: 
category: 
name: 
descriptor: 
category: 
name: 
descriptor: 
category: 
 

geninfo 
its product 
artifact 
la jolla 
the maddox family's hometown 
location 
ca 
  
location 

ie accuracy by information type

information 

accuracy

type
entities

90-98%

attributes

80%

facts

events

60-70%

50-60%

13

unstructured text

police are investigating a robbery that occurred at the 7-

eleven store located at 2545 little river turnpike in the 

lincolnia area about 12:30 am friday.  a 24 year old 

alexandria area employee was approached by two men who 

demanded money.  she relinquished an undisclosed amount 

of cash and the men left.  no one was injured.  they were 

described as black, in their mid twenties, both were five 

feet nine inches tall, with medium builds, black hair and 

clean shaven.  they were both wearing black pants and 

black coats.  anyone with information about the incident 

or the suspects involved is asked to call police at (703) 555-

5555.

structured (desired) information

crime
abduction

   
robbery

robbery

address
8700 block of 
little river 
turnpike,
   
7- eleven 
store located 
at 2545 little 
river turnpike,

7- eleven 
store located 
at 5624 ox 
road,

town
annandal
e

time
11:30 pm

day
sunda
y

   
lincolnia

   
12:45 am

   
friday

fairfax

3:00 am

friday

14

muc conferences

conference

muc 1

muc 2

muc 3

muc 4

muc 5

muc 6

muc 7

year

1987

1989

1991

1992

1993

1995

1997

topic

naval operations

naval operations

terrorist activity

terrorist activity

joint venture and micro 
electronics
management changes

spaces vehicles and missile 
launches

the ace evaluation 

  the ace program is dedicated to the challenge of extracting 

content from human language.  this is a fundamental capability 
that the ace program addresses with a basic research effort that
is directed to master first the extraction of    entities   , then the 
extraction of    relations    among these entities, and finally the 
extraction of    events    that are causally related sets of relations.  

  after two years of research on the entity detection and tracking
task, top systems have achieved a capability that is useful by 
itself and that, in the context of the ace edt task, successfully 
captures and outputs well over 50 percent of the value at the 
entity level.  

  here value is defined to be the benefit derived by successfully 
extracting the entities, where each individual entity provides a
value that is a function of the entity type (i.e.,    person   , 
   organization   , etc.) and level (i.e.,    named   ,    unnamed   ).  thus 
each entity contributes to the overall value through the 
incremental value that it provides.

15

ace entity detection & tracking 
evaluation  -- 2/2002

fac
loc
gpe
org
per

0.3
0.6

11.8

24.3

63.0

goal:  extract entities. each 
entity is assigned a value.  
this value is a function of its 
type and level.  this value 
is gained when the entity is 
successfully detected.  this 
value is lost when an entity 
is missed, spuriously 
detected, or 
mischaracterized. 

table of entity values

per

org

gpe

loc

fac

nam

1

0.5

0.25

0.1

0.05

nom

0.2

0.1

0.05

0.02

0.01

pro

0.04

0.02

0.01

0.004

0.002

100

human performance ~80

90

)

%
n

 

i
(
 
t
s
o
c

 
/
 

e
u

l

 

a
v
d
e
z
i
l

a
m
r
o
n

e
u

l

 

a
v
m
u
m
x
a
m
o
t
 

 

i

e
v

i
t
a

l

e
r

average 
of top 4 
systems

0.0
0.1
9.1

11.3

40.6

80

70

60

50

40

30

20

10

0

act value

max value

miss  36%,  false alarm   22%, type error   6%

applications of information 
extraction

  routing of information
  infrastructure for ir and for 

categorization (higher level features)

  event based summarization.
  automatic creation of databases and 

knowledge bases.

16

where would ie be useful?

  semi-structured text
  generic documents like news articles.
  most of the information in the document 

is centered around a set of easily 
identifiable entities.

approaches for building ie 
systems

  knowledge engineering approach

  rules are crafted by linguists in cooperation with 

domain experts.

  most of the work is done by inspecting a set of 

relevant documents.

  can take a lot of time to fine tune the rule set.
  best results were achieved with kb based ie 

systems.

  skilled/gifted developers are needed.
  a strong development environment is a must! 

17

approaches for building ie 
systems

  automatically trainable systems

  the techniques are based on pure statistics and 

almost no linguistic knowledge

  they are language independent
  the main input is an annotated corpus
  need a relatively small effort when building the 
rules, however creating the annotated corpus is 
extremely laborious.

  huge number of training examples is needed in 

order to achieve reasonable accuracy.  

  hybrid approaches can utilize the user input in the 

development loop. 

a basic ie system

18

architecture of a full ie system

components of ie system

19

the extraction engine

why is ie difficult?

  different languages

  morphology is very easy in english, much harder in german and 

hebrew.

  identifying word and sentence boundaries is fairly easy in 

european language, much harder in chinese and japanese.

  some languages use orthography (like english) while others (like 

hebrew, arabic etc) do no have it.

  different types of style

  scientific papers
  newspapers
  memos
  emails
  speech transcripts
  type of document

  tables
  graphics
  small messages vs. books

20

morphological analysis

  easy 

  english, japanese
  listing all inflections of a word is a real possibility

  medium

  french spanish
  a simple morphological component adds value.

  difficult

  german, hebrew, arabic 
  a sophisticated morphological component is a 

must!

using vocabularies

     size doesn   t matter   

  large lists tend to cause more mistakes
  examples:

  said as a person name (male)
  alberta as a name of a person (female)

  it might be better to have small domain 

specific dictionaries

21

id52

  pos can help to reduce ambiguity, and 

to deal with all caps text.

  however

  it usually fails exactly when you need it
  it is domain dependent, so to get the best 
results you need to retrain it on a relevant 
corpus.

  it takes a lot of time to prepare a training 

corpus.

a simple pos strategy

  use a tag frequency table to determine 

the right pos.
  this will lead to elimination of rare senses.

  the overhead is very small
  it improve accuracy by a small 

percentage. 

  compared to full pos it provide similar 

boost to accuracy.

22

dealing with proper names

  the problem

  impossible to enumerate
  new candidates are generated all the time
  hard to provide syntactic rules

  types of proper names

  people
  companies
  organizations
  products
  technologies
  locations (cities, states, countries, rivers, 

mountains)

comparing rb systems with ml 
based systems

wall street 
journal
muc6
muc7

transcribed 
speech
hub4

rule based

id48

96.4

93.7

93

90.4

90.3

90.6

23

building a rb proper name 
extractor

  a lexicon is always a good start
  the rules can be based on the lexicon and on:
  the context (preceding/following verbs or nouns)
  id157

  companies: capital* [,] inc, capital* corporation..
  locations: capital* lake, capital* river 

  capitalization
  list structure

  after the creation of an initial set of rules

  run on the corpus
  analyze the results
  fix the rules and repeat   

  this process can take around 2-3 weeks and result in performance of 

between 85-90% break even.

  better performance can be achieved with more effort (2-3 months) and 

then performance can get to 95-98%

introduction to id48s for ie

24

motivation

  we can view the named entity extraction as a 
classification problem, where we classify each 
word as belonging to one of the named entity 
classes or to the no-name class. 

  one of the most popular techniques for 

dealing with classifying sequences is id48. 

  example of using id48 for another nlp 

classification task is that of part of speech 
tagging (church, 1988 ; weischedel et. al., 
1993). 

what is id48?

  id48 (hidden markov model) is a finite state 

automaton with stochastic state transitions and 
symbol emissions (rabiner 1989). 

  the automaton models a probabilistic 

generative process. 

  in this process a sequence of symbols is 

produced by starting in an initial state, 
transitioning to a new state, emitting a symbol 
selected by the state and repeating this 
transition/emission cycle until a designated 
final state is reached.

25

disadvantage of id48

  the main disadvantage of using an id48 
for information extraction is the need for 
a large amount of training data. i.e., a 
carefully tagged corpus. 

  the corpus needs to be tagged with all 
the concepts whose definitions we want 
to learn. 

notational conventions

  t = length of the sequence of observations (training 

set) 

  n = number of states in the model 
  qt = the actual state at time t
  s = {s1,...sn} (finite set of possible states)
  v =  {o1,...om} (finite set of observation symbols)
     = {  i} =  {p(q1 = si)} starting probabilities
  a = {aij}=p(qt+1= si | qt = sj) transition probabilities
  b = {bi(ot)} = {p(ot | qt = si)} emission probabilities

26

the classic problems related 
to id48s

  find p( o |    ): the id203 of an 

observation sequence given the id48 
model. 

  find the most likely state trajectory given 

   and o. 

  adjust    = (  , a, b) to maximize p( o |   

).

calculating p( o |    )

  the most obvious way to do that would be to 

enumerate every possible state sequence of length t 
(the length of the observation sequence). let q = 
q1,...qt , then by assuming independence between 
the states we have 
  p(o|q,   ) = 
      
a  
  p(q|  ) = 

qop

ob
q

   

   

  
)

=

qq
i
i

=
1

=
1

=
1

(

)

(

q
1

,

+
1

|

t

t

t

1

i

i

i

i

i

i

i

  by using id47 we have 

  p(o,q|  ) = p(o|q,   ) p(q|  )

  finally the main problem with this is that we need to 
do 2tnt multiplications, which is certainly not feasible 
even for a modest t like 10.

27

the forward-backward algorithm

  in order to solve that we use the forward-backward 

algorithm this is far more efficient.  the forward part is 
based on the computation of terms called the alpha 
terms. we define the alpha values as follows, 

)

i

1

=

)(
i

    
ob
1
i
   
   
   

  
+
1

)(
j

=

t

  

t

)(
obai
t

(

ij

j

   
   
   

)

+
1

op

(

|

  
)

=

)(
ia
t

(
n
   
=
1
i
n
   
=
1
i

efficient way.

  we can compute the alpha values inductively in a very 

  this calculation requires just n2t multiplications. 

the backward phase

  in a similar manner we can define a 

backward variable called beta that 
computes the id203 of a partial 
observation sequence from t+1 to t. the 
beta variable will also be computed 
inductively but in a backward fashion. 

  

t

  

t

=

1)(
i
n

)(
i

   =
=
1

j

oba
ij
t

(

i

  
)
+
1

t

)(
j

+
1

28

solution for the second problem

  our main goal is to find the    optimal    state sequence. we will do that by 

maximizing the probabilities of each state individually.

  we will start by defining a set of gamma variables the measure the 

probabilities that at time t we are at state si.

  

t

)(
i

=

    
)(
i
t
(
|
op

)(
i
  
)

t

=

    

)(
i

t

t

)(
i

    

)(
i

)(
i

n
   
=
1
i

measure.

  the denominator is used just to make gamma a true id203 

t

t

  now we can find the best state at each time slot in a local fashion.

q

=

)](
i

max[
arg
      
1

  
t
ni

  the main problem with this approach is that the optimization is done 

t

locally, and not on the whole sequence of states. this can lead either to 
a local maximum or even to an invalid sequence. in order to solve that 
problem we use a well known id145 algorithm called the 
viterbi algorithm.

the viterbi algorithm

  intuition

  compute the most likely sequence starting with the 

empty observation sequence; use this result to compute 
the most likely sequence with an output sequence of 
length one; recurse until you have the most likely 
sequence for the entire sequence of observations. 

  algorithmic details

  the delta variables compute the highest id203 of a 

partial sequence up to time t that ends in state si. the 
psi variables enables us to accumulate the best 
sequence as we move along the time slices. 

  1. initialization: 

1

    
i
  
0

)(
i
)(
i

=
=

1

ob
1
i

(

)

29

viterbi (cont).

  recursion: 

  

t

)(
j

=

[
  
   
1

t

)(
obai
t

(

ij

i

]

)

1

max
      
ni
arg
max
      
1
ni

)(
j

  

[
  
1   
  termination: 

=

t

t

)(
ai

]ij

*

p

=

max
      
ni

])(
i
  reconstruction: 

[
t  

1

q

*
t

=

arg
      
1

max
ni

[
  

t

])(
i

  for t = t-1,t-2,...,1.

sequence, , solves problem 2. 

q   

=

*
t

t

*
1 ,
qq l

*
2

*
tq

)

( *
q
+
+
1
1
t
the resulting 

viterbi (example)

30

the just research id48

  each id48 extracts just one field of a given document. 

if more fields are needed, several id48s need to be 
constructed.

  the id48 takes the entire document as one 

observation sequence.

  the id48 contains two classes of states, background 
states and target states. the background states emit 
words in which are not interested, while the target 
states emit words that constitute the information to be 
extracted.

  the state topology is designed by hand and only a few 

transitions are allowed between the states.

possible id48 topologies

background

state

initial state

final state

initial state

background

state

final state

target
state

prefix states

suffix states

target
state

31

a more general id48 
architecture

experimental evaluation

acquiring
company

acquired 
company

abbreviation 
of acquired 
company

price of 
acquisition

status of
acquisition

30.9%

48.1%

40.1%

55.3%

46.7%

speaker

location

start time

end time

71.1%

83.9%

99.1%

59.5%

32

bbn   s identifinder

  an ergodic bigram model.
  each named class has a separate region in 

the id48.

  the number of states in each nc region is 

equal to |v|. each word has its own state.
  rather then using plain words, extended 

words are used. an extended word is a pair 
<w,f>, where f is a feature of the word w.

bbn   s id48 architecture

33

possible word features

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.

2 digit number (01)
4 digit number (1996)
alphanumeric string (a34-24)
digits and dashes (12-16-02)
digits and slashes (12/16/02)
digits and comma (1,000)
digits and period (2.34)
any other number (100)
all capital letters (clf)
capital letter and a period (m.)
first word of a sentence (the)
initial letter of the word is capitalized (albert)
word in lower case (country)
all other words and tokens (;)

statistical model

  the design of the formal model is done in 

levels. 

  at the first level we have the most accurate 
model, which require the largest amount of 
training data. 

  at the lower levels we have back-off models 
that are less accurate but also require much 
smaller amounts of training data. 

  we always try to use the most accurate model 
possible given the amount of available training 
data.

34

computing state transition 
probabilities

  when we want to analyze formally the 
id203 of annotating a given word 
sequence with a set of name classes, we need 
to consider three different statistical models:
  a model for generating a name class
  a model to generate the first word in a name class
  a model to generate all other words (but the first 

word) in a name class

computing the probabilities : 
details

  the model to generate a name class depends on the 
previous name class and on the word that precedes 
the name class; this is the last word in the previous 
name class and we annotate it by w-1. so formally this 
amounts to p(nc | nc-1,w-1).

  the model to generate the first word in a name class 
depends on the current name class and the previous 
name class and hence is p(<w,f>first| nc, nc-1). 

  the model to generate all other words within the same 
name class depends on the previoues word (within the 
same name class) and the current name class, so 
formally it is p(<w,f>| <w,f>-1, nc). 

35

the actual computation

ncp

(

|

nc

,

w

   
1

   
1

)

=

,

(
ncc
(
ncc

fwp

(

,

<

>

first

|

nc

,

nc

   
1

fwp

(

,

<

<>
|

fw

,

>

   
1

,

nc

)

)

   
1

w
   
1
)
<

,
nc
   
1
,
w
   
1
>
(
,
fwc
(
ncc
<>
(
,
,
fwc
<
,
(
fwc

=

<

=

)

nc

)

   
1

nc

,
nc
>
   
1
nc

,
)
   
1
,
nc
)

first
,
,
fw
>
,

   
1

)

c(<w,f>,<w,f>-1,nc), counts the number of times that we 
have the pair <w,f> after the pair <w,f>-1 and they both are 
tagged by the name class nc.

modeling unknown words

  the main technique is to create a new entity called 

unknown (marked _unk_), and create statistics for 
that new entity. all words that were no seen before are 
mapped to _unk_. 

  split the collection into 2 even parts, and each time 
use one part for training and one part as a hold out 
set. the final statistics is the combination of the results 
from the two runs. 

  the statistics needs to be collected for 3 different 

classes of cases: _unk_ and then a known word (|v| 
cases), a known word and then _unk_ and two 
consecutive _unk_ words. this statistics is collected 
for each name class.

36

name class back-off models

  the full model take into account both the previous 
name class and the previous word (p(nc| nc-1,w-1)
  the first back-off  model takes into account just the 

previous name class (p((nc| nc-1)).

  the next back-off model would just estimate the 

id203 of seeing the name class based on the 
distribution of the various name classes (p(nc)).
  finally, we use a uniform distribution between all 

names classes (1/(n+1), where n is number of the 
possible name classes)

first word back-off  models

  the full model takes into account the current name 
class and the previous name class (p(<w,f>first| nc, 
nc-1)).

  the first back-off model  takes into account just the 

current name class (p(<w,f>first| nc)).

  the next back-off model, breaks the <w,f> pair and 

just uses multiplication of two independent events 
given the current word class (p(w|nc)p(f|nc))

  the next back-off model is a uniform distribution 

between all pairs of words and features (              , 
where f# is the # of possible word features)

fv

|#

|

1
||

37

rest of the words back-off  
models

  the full model takes into account the current name 
class and the previous word (p(<w,f>|<w,f>-1, nc)).
  the first back-off model  takes into account just the 

current name class (p(<w,f>| nc)).

  the next back-off model, breaks the <w,f> pair and 

just uses multiplication of two independent events 
given the current word class (p(w|nc)p(f|nc))

  the next back-off model is a uniform distribution 
between all pairs of words and features (            , 
where f# is the # of possible word features)

fv

1
||

|#

|

combining all the models

  the actual id203 is a combination of the different 
models. each model gets a different weight based on 
the amount of training available to that model.

  lets assume we have 4 models (one full model, and 3 

back-off models), and we are trying to estimate the 
id203 of p(x|y).  let p1 be id203 of the 
event according to the full model, and p2, p3, p4 ate 
the back-off models respectively.

  the weights are computed based on a lambda 
parameter that is based on each model and it 
immediate back-off model. for instance   1 will adjust 
the wait between the full model and the first back-off 
1
model.
)(#
y
)(
ybc

)
(
yc
(
)
ybc

=  

   
      
   

   
      
   

+

   

1

1

38

analysis

  where c(y) the count of event y according to the full 

model, and  bc(y) is the count of event y according to 
the back-off model. #(y) is the number of unique 
outcomes of y. 

  lambda has two desirable properties

  if the full model and the back-off model both have the 

same support for event y, then lambda will be 0 and we 
will use just the full model.

  if the possible outcomes of y are distributed uniformly 
then the weight of lambda will be close to 0 since there 
is low confidence in the back-off model.

=  

1

   

   
      
   

)(
yc
)(
ybc

   
      
   

1
)(#
y
)(
ybc

1

+

example

  we want to compute the id203 of p(   bank    |    river   ,    not-a-name   ). 
lets assume that river appears with 3 different words in the not-a-name   
name class, and in total there are 9 different occurrences of river with 
any of the 3 words.

=  

1

       
1
   
   

0
9

=   =

1

3
4

3
4

   
   
   
   
   
   
   
   
   

1

1

+

3
9

   
   
   
   
   
   

  so we will use the full model (p1) with 0.75, and the other back-off 

models with 0.25. we then compute   2 which computes the weight of the 
first back-off model (p2) against the other back-off models, and finally   3 
which is the weight of the second back-off model (p3) against the last 
back-off model. so to sum up, the id203 of p(x|y) would be: 

  p(x|y) =   1 * p1(x|y) + (1 -   1) * (  2 * p2(x|y) + (1 -   2) * (  3 * p3(x|y) 

+ (1 -   3) * p4(x|y)))

39

using different modalities of 
text

  mixed case: abu sayyaf carried out an attack on a south western beach resort 
on may 27, seizing hostages including three americans. they are still holding a 
missionary couple, martin and gracia burnham, from wichita, kansas, and claim 
to have beheaded the third american, guillermo sobero, from corona, california. 
mr. sobero's body has not been found.

  upper case: abu sayyaf carried out an attack on a south 

western beach resort on may 27, seizing hostages including 
three americans. they are still holding a missionary couple, 
martin and gracia burnham, from wichita, kansas, and claim to 
have beheaded the third american, guillermo sobero, from 
corona, california. mr sobero's body has not been found.

  snor: abu sayyaf carried out an attack on a south western 

beach resort on may twenty seven seizing hostages including 
three americans they are still holding a missionary couple 
martin and gracia burnham from wichita kansas and claim to 
have beheaded the third american guillermo sobero  from 
corona california mr soberos body has not been found.

experimental evaluation (muc 
7)

modality

language

rule based

id48

mixed case

english

96.4%

94.9%

upper case

english

snor

english

mixed case

spanish

89%

74%

93%

93.6%

90.7%

90%

40

how much data is needed to 
train an id48?

number of 

english

spanish

tagged words

23,000

60,000

85,000

130,000

230,000

650,000

na

91.5%

91.9%

92.8%

93.1%

94.9%

88.6%

89.7%

na

90.5%

91.2%

na

limitations of the model

  the context which is used for deciding on the type of 

each word is just the word the precedes the current 
word. in many cases, such a limited context may 
cause classification errors.

  as an example consider the following text fragment 

   the turkish company, birgen air, was using the 
plane to fill a charter commitment to a german 
company,   . the token that precedes birgen is a 
comma, and hence we are missing the crucial clue 
company which is just one token before the comma. 
  due to the lack of this hint, the indentifinder system 

classified birgen air as a location rather than as a 
company. one way to solve this problem is to 
augment the model with another token when the 
previous token is a punctuation mark.

41

example - input

<document>
<type>news</type>
<id> 4 ryan daughters tied to cash ( thu feb 13, 9:49 am )</id>
<title> 4 ryan daughters tied to cash </title>
<date> thu feb 13, 9:49 am </date>
<source> yahoo-news </source>
<body> by matt o'connor, tribune staff reporter. tribune staff reporter ray gibson contributed to this report <p> four of 

former gov. george ryan's daughters shared in almost 10,000 in secret payments from sen. phil gramm's presidential 
campaign in the mid-1990s, according to testimony wednesday in federal court. <p> 

alan drazek, who said he was brought in at ryan's request as a point man for the gramm campaign in illinois, testified he 

was told by scott fawell, ryan's chief of staff, or richard juliano, fawell's top aide, to cut the checks to the women. 
according to court records made public wednesday, ryan's daughter, lynda pignotti, now known as lynda fairman, 
was paid a combined 5,950 in 1995 by the gramm campaign in four checks laundered through drazek's business, 
american management resources.

<p>
<p>
in 1996, individual checks went to ryan daughters nancy coghlan, who received 1,725, and joanne barrow and julie r. 

koehl, who each pocketed 1,000, the records showed.

<p>
<p>
a source said all four daughters had been given immunity from prosecution by federal authorities and testified before the 

grand jury investigating fawell as part of the operation safe road probe.

<p>
<p> full story at chicago tribune <p>
<p>

<p> </body>
</document>

example - output

full story at <location>baltimore sun</location>
by <person>matt o ' connor</person> , <organization>tribune</organization> staff 
reporter . tribune staff reporter <person>ray gibson</person> contributed to this report 

four of former gov . <person>george ryan</person> ' s daughters shared in almost 

<money>10 , 000</money> in secret payments from sen . <person>phil 
gramm</person> ' s presidential campaign in the <date>mid - 1990 s</date> , according to 
testimony <date>wednesday</date> in federal court . 

<person>alan drazek</person> , who said he was brought in at 

<organization>ryan</organization> ' s request as a point man for the gramm campaign 
in <location>illinois</location> , testified he was told by <person>scott 
fawell</person> , ryan ' s chief of staff , or <person>richard juliano</person> , 
<person>fawell</person> ' s top aide , to cut the checks to the women . according to court 
records made public <date>wednesday</date> , ryan ' s daughter , <person>lynda 
pignotti</person> , now known as <person>lynda fairman</person> , was paid a 
combined <percent>5</percent> , 950 in <date>1995</date> by the gramm campaign 
in four checks laundered through drazek ' s business , <organization>american 
management resources</organization> . 

in <date>1996</date> , individual checks went to ryan daughters <person>nancy 

coghlan</person> , who received <money>1 , 725</money> , and <person>joanne 
barrow and julie r . koehl</person> , who each <money>pocketed 1 , 000</money> , the 
records showed . 

a source said all four daughters had been given immunity from prosecution by federal authorities and 

testified before the grand jury investigating fawell as part of the operation safe road probe . 

full story at <organization>chicago tribune</organization> 

42

training: ace + muc => muc

r/p

person

organization

date

time

location

money

percent

muc7

91.9/85.5

91.1/93.7

90.9/76.6

76.4/77.6

90.7/91.3

97.6/82.1

93.7/40.54

ace+muc7

84.9/88.6

83.1/95.9

59/89.5

68.6/92.5

77.7/91.7

86.6/82.1

50/29.6

results with our new algorithm

)

%

(
 

e
r
u
s
a
e
m
-
f

100

90

80

70

60

10.7 14.6 19.8 27

36.7 49.9 68

92.5 126 171 233 317 432 588 801 1090 1484 2019 2749 3741

amount of training (kwords) 
logarithmic scale - each tick 
multiplies the amount by 7/6

date

location

organization

person

date-nymble

loc-nymble

org-nymble

currently available
amount of training

43

some useful resources

  linguistic data consortium (ldc)

  lexicons
  annotated corpora (text and speech)

  new mexico state university

  gazetteers 
  many lists of names
  lexicons for different languages

  various web sources

  cia world fact book
  hoovers
  sec, nasdaq (list of public company names)
  us census data
  private web sites (like arabic, persian, pakistani names)

ie in biotech

44

two levels of processing

  out-of-the-box entity and relationship tagging
  identifies entities and categorizes them according to 

vocabularies (i.e. genes, tissues,    )

  identifies relationships based on verbs (and/or verb 

phrases)

  identifies co-occurrence relationships

  advanced tagging

  enables a complete discovery process (identify 

entities that are not in any vocabulary)

  add constraints, conditions and semantics for more 

granular relationship identification

45

46

advanced tagging

  enables a complete discovery process 

(identify entities that are not in any 
vocabulary)

  add constraints, conditions and 

semantics for more granular relationship 
identification

47

advanced tagging

enable a discovery process:

  tag any tissue, even if it does not exist in 

any vocabulary.

example:  

any noun-phrase that starts with    kinase    is probably 
a gene/protein

advanced tagging

  enable a deeper, more accurate tagging 

process

example:
   intralysosomal accumulation of 

glycogen affects function of 
skeletal muscle   

na  ve result: <glycogen> affects <function>
correct result: <intralysosomal accumulation of 
glycogen> affects <function of skeletal muscle>

48

advanced tagging

  enables constraints, conditions and 

semantics for more granular relationship 
identification

  example:
tag a <causality relationship> between 

<gene-mutation> and <disease>

user supplied 
gene vocabulary

user supplied 

disease vocabulary

result sample:
mutations that disrupt the 
dna-binding domain of the t-
box gene, tbx3, have been 
demonstrated to cause ums
(ulnar-mammary syndrome) 

49

developing ie rules

the process        graphically
graphically
the process 

learning 
the domain

domain 
expert

making the 

rules

improving 
the code

nlp 

developer

   internal   

testing

   external   
feedback

final 

rulebook

client

50

the dial language

declarative information analysis language

  prolog (logic programming)- based (   rules   )
  unique pattern-matching capabilities
  enhanced by c++ - based procedural elements
  dial is implemented in c++
  dial code is compiled to c++ functions, from 

which an executive dll is produced

dial rules and rulebooks

rulebook

formulation of real-world concepts as sets of 
dial predicates (   program   ) that match and 
extract their occurrences in texts.

concepts
  entities        basic    object types 
  relationships     specific connection or 

occurrence involving two or more entities: (a 
   fact    or an    event   ).

51

entities and relationships- (examples)

  financial news domain

entities:  company, product, person (executive)
relationships: company headquarter, merger

  sports news domain

entities: athletes, sports teams
relationships : match result

  genomics domain

entities: gene, protein, disease    

  html pages parsing / analysis

entities: url (links within the page)

dial predicates and rules

each predicate has several definitions (rules)
each rule may have:
  a series of pattern matching elements
  a set of constraints that the matched 

patterns must obey

  a set of assignments of the predicate   s 

parameters and/or actions concerning 
external variables / data structures

52

pattern matching elements
  an explicit token (string) : e.g.    announces   
  a wordclass     a predefined set of phrases that 

share a common semantic function. 

example : 
wordclass wcresignation = resignation retirement 

departure ;

  (another) predicate call
  flow control operators: cut, local and global 

consumption (@!, @>, @%)

constraints
used for carrying out    on-the-fly    boolean checks 
on relevant pattern matching elements

example :

verify(inwc(p,@wcannounce))

means that the p pattern matching element must 
be a member of the wordclass wcannounce

53

a full rule     an example

predicate toplevel companyname ( string 

company ) ;

companyname(c) :-
capwords -> d 
wccompanyext->e 
[ "." ]->f 

verify (! firstin 
(d,@wccompnamenonstarters ) ) 
@! @>

{ c = d+e+f ; } ;
example of an instance: 
crown central petroleum corp.

natural language processing    
challenges and solutions

why nlp is tricky     an example

personleftposition (fired) event :
a company fired a person (an executive)

example:
banco mercantil del per   yesterday
fired the embezzler, pilar gonzalez

54

the    na  ve    approach

the pattern:

possible_company     followed by
   fired    (or a similar word - wordclass)    
followed by
possible_person_name

the problem

a    real    sentence matching this very same rule:

alabama power's new gas-fired electric 

generating facility at plant barry.

the syntactical function of    fired    in the sentence 

must be a verb !

55

id66 in ie

  only core constituents are extracted
  no attempt is made at full parses
  relevant prepositional attachments are 

extracted.
  i saw the man with a telescope

  only adverbials related to location and time 

are processed, others are ignored.

  quantifiers, modals, and propositional 

attitudes are ignored, or treated in a simplified 
way. 

why not full parsing?

  full parsing for ie was tried in:

  sri tacitus system (muc 3)
  nyu proteus (muc-6)

  main issues:

  slow (combinatorial explosion of possible 

parses) 

  erroneous 
  a simple predicate-argument structure is 

needed.

56

coreference

  the general problem is related to co referential 

relations between expressions
  whole     part relationship
  containment relationship (set/subset)

  a simplest version is to find which noun 

phrases refer to the same entity

  an even more restricted version is to limit it 

just to proper names.

  example:

  the president, george bush, george w. bush, or 

even    w   , all refer to the same entity.

easy and hard in coreference

     mohamed atta, a suspected leader of the 

hijackers, had spent time in  belle glade, fla., 
where a crop-dusting business is located. atta
and other  middle eastern men came to south 
florida crop care nearly every weekend for  
two months. 

     will lee, the firm's general manager, said the 

men asked repeated  questions about the 
crop-dusting business. he said the questions 
seemed "odd,"  but he didn't find the men
suspicious until after the sept. 11 attack.   

57

the    simple    coreference

  proper names

  ibm,    international business machines   , big blue
  osama bin ladin, bin ladin, usama bin laden. 

(note the variations)

  definite noun phrases

  the giant computer manufacturer, the company, 

the owner of over 600,000 patents

  pronouns

  it, he , she, we   ..

coreference example

granite systems provides service resource management 

(srm) software for communication service providers with 
wireless, wireline, optical and packet technology networks. 
utilizing granite' xng system, carriers can manage 
inventories of network resources and capacity, define 
network configurations, order and schedule resources and 
provide a database of record to other operational support 
systems (osss). an award-winning company, including an 
inc. 500 company in 2000 and 2001, granite systems
enables clients including at&t wireless, kpn belgium, 
colt telecom, atg and verizon to eliminate resource 
redundancy, improve network reliability and speed service 
deployment. founded in 1993, the company is 
headquartered in manchester, nh with offices in denver, 
co; miami, fl; london, u.k.; nice, france; paris, france; 
madrid, spain; rome, italy; copenhagen, denmark; and 
singapore.

58

a ke approach to corefernce

  mark each noun phrase with the following:

  type (company, person, location)
  singular vs. plural
  gender (male, female, neutral)
  syntactic (name, pronoun, definite/indefinite)

  for each candidate

  find accessible antecedents

  each antecedent has a different scope 

  proper names   s scope is the whole document
  definite clauses   s scope is the preceding paragraphs
  pronouns might be just the previous sentence, or the same 

paragraph.

  filter by consistency check
  order by dynamic syntactic preference

filtering antecedents

  george bush will not match    she   , or    it   
  george bush can not be an antecedent 

of    the company    or    they   

  using a sort hierarchy we can use 

background information to be smarter
  example:    the big automaker is planning to 
get out the car business. the company feels 
that it can never longer make a profit 
making cars.   

59

autoslog (riloff, 1993) 

  creates extraction patterns from annotated 

texts (nps).

  uses sentence analyzer (circus, lehnert, 

1991) to identify clause boundaries and 
syntactic constituents (subject, verb, direct 
object, prepositional phrase)

  it then uses heuristic templates to generate 

extraction patterns 

example templates

template
<subj> passive-verb
<subj> aux noun
active-verb <dobj>
noun prep <np>
active-verb prep <np>
passive-verb prep <np>

example
<victim> was murdered
<victim> was victim
bombed <target>
bomb against <target>
killed with <instrument>
was aimed at <target>

60

autoslog-ts (riloff, 1996)

  it took 8 hours to annotate 160 documents, 

and hence probably a week to annotate 1000 
documents.

  this bottleneck is a major problem for using ie 

in new domains.

  hence there is a need for a system that can 

generate ie patterns from un-annotated 
documents.

  autoslog-ts is such a system. 

autoslog ts

s: world trade center

v: was bombed
pp: by terrorists

autoslog heuristics

extraction
patterns

<w> was killed
bombed by <y>

sentence analyzer

ep
<x> was bobmed
bombed by <y>
<w> was killed
<z> saw

rel %
 87%
84%
63%
49%

sentence analyzer

extraction patterns

<w> was killed
<x> was bombed
bombed by <y>

<z> saw

61

top 24 extraction patterns

<subj> exploded

murder of <np>

assassination of <np>

<subj> was killed

<subj> was kidnapped

attack on <np>

<subj> was injured

exploded in <np>

death of <np>

<subj> took place

caused <dobj>

claimed <dobj>

<subj> was wounded

<subj> occured

<subj> was loctated

took place on <np>

responsibility for <np>

occurred on <np>

was wounded in <np>

destroyed <dobj>

<subj> was murdered

one of <np>

<subj> kidnapped

exploded on <np>

evaluation

  data set: 1500 docs from muc-4 (772 

relevant)

  autoslog generated 1237 patterns which 

were manually filtered to 450 in 5 hours.

  autoslog-ts generated 32,345 patterns, after 

discarding singleton patterns, 11,225 were 
left.

  rank(ep) = 
  the user reviewed the the top 1970 patterns 

2

and selected 210 of them in 85 minutes.

log

freq

rel

freq

   
freq

  autoslog achieved better recall, while 
autoslog-ts achieved better precision.

62

learning dictionaries by id64 
(riloff and jones, 1999)

  learn dictionary entries (semantic lexicon) and 

extraction patterns simultaneously. 

  use untagged text as a training source for 

learning.

  start with a set of seed lexicon entries and 
using mutual id64 learn extraction 
patterns and more lexicon entries.

mutual id64 algorithm

  using autoslog generate all possible extraction 

patterns.
apply patterns to the corpus and save results to 
epdata
seid113x = seed words

 

 

  cat_eplist = {}
1. score all extraction patterns in epdata
2. best_ep = highest scoring pattern
3. add best_ep to cat_eplist
4. add best_ep   s extractions to seid113x
5. goto 1

63

meta id64 process

 

candidate extraction patterns 

and their extractions 

seed words 

initialize 

select best_ep

permanent semantic 

lexicon 

temporary semantic 

lexicon 

category ep list 

add 5 best 
nps 

add best_ep   s
extractions

sample extraction patterns

www location
offices in <x>
facilities in <x>
operations in <x>
operates in <x>
seminars in <x>
activities in <x>
consulting in <x>
outlets in <x>
customers in <x>
distributors in <x>
services in <x>
expanded into <x>

www company
owned by <x>
<x> employed 
<x> is distributor
<x> positioning
motivated <x>
sold to <x>
devoted to <x>
<x> thrive
message to <x>
<x> request information
<x> has positions
offices of <x>

terrorism location
living in <x>
traveled to <x>
become in <x>
sought in <x>
presidents of <x>
parts of <x>
to enter <x>
ministers of <x>
part in <x>
taken in <x>
returned to <x>
process in <x>

64

experimental evaluation

iter 1

iter 10

iter 20

iter 30

iter 40

iter 50

web 
company

web 
location

web title

terr. 
location

terr. 
weapon

5/5 
(1)

5/5 
(1)

0/1 
(0)

5/5 
(1)

4/4 
(1)

25/32 
(.78)

46/50
(.92)

22/31 
(.71)

32/50 
(.64)

31/44 
(.70)

52/65 
(.80)

72/113 
(.64)

86/163 
(.53)

95/206 
(.46)

88/100 
(.88)

129/150 
(.86)

163/200 
(.82)

191/250 
(.76)

63/81 
(.78)

86/131 
(.66)

101/181 
(.56)

107/231 
(.46)

66/100 
(.66)

100/150 
(.67)

127/200 
(.64)

158/250 
(.63)

68/94 
(.72)

85/144 
(.59)

101/194 
(.52)

124/244 
(.51)

semi automatic approach

smart tagging

65

66

67

68

auditing environments

fixing recall and precision 
problems

69

auditing events

auditing a personpositioncompany
event 

70

updating the taxonomy and the 
thesaurus with new entities 

using background information

  often the analyst would like to use background 

information about entities, which is not available in the 
documents.

  this background information enables the analyst to 

perform filtering, id91 and to automatically color 
entities in various colors. 
  examples:

  cia world factbook
  hoovers
  sic codes 

  a database gateway can create virtual nodes in the 

taxonomy based on properties stored in the database. 

71

using the cia world fact book

using hoovers

72

link detection

the importance of analysis

   analysis is the key to the successful use of 

information; it transforms raw data into 
intelligence. it is the fourth of five stages in the 
intelligence process: collection, evaluation, 
collation, analysis, dissemination. without the 
ability to perform effective and useful analysis, 
the intelligence process is reduced to a simple 
storage and retrieval system for effectively 
unrelated data   

the intelligence analysts training manual

scotland yard, london 

73

what is link analysis? (law 
enforcement context)

link analysis is the graphic portrayal of 

investigative data, done is a manner to 
facilitate the understanding of large 
amounts of data, and particularly to allow 
investigators to develop possible 
relationships between individuals that 
otherwise would be hidden by the mass 
of data obtained.

coady, 1985 

generalized definition

link analysis is the graphic portrayal of 

extracted/derived data, done is a manner 
to facilitate the understanding of large 
amount of data, and particularly to allow 
analysts to develop possible 
relationships between entities that 
otherwise would be hidden by the mass 
of data obtained.

74

characteristics of criminal networks

  huge size (many nodes, relatively small 

numbers of links)

  incompleteness

  not all    real    nodes and links will be 

observed.

  fuzzy boundaries

  organizations are often interlaced

  dynamic

  each link strength has distribution over time

notions of centrality(node)

  degree

  # of other nodes to which it is directly linked.

  betweeness

  # of geodesics (shortest paths between 2 other nodes) which pass

through it.

  closeness

  minimizing the maximum of the minimal path length to other nodes in 

the network. (the central nodes have a minimal radius).

  center of gravity
  point strength

  increase in the number of maximal connected subcomponents upon 

removal of the node.

  business

  how busy is each node in transmitting information

75

applications of centrality

  targeting

  betweeness
  business

  identification of network vulnerability

  betweeness
  point strength
  business

equivalence

  substitutability

  objects a, b of category c are structurally equivalent if, for any 
relation m and any object x of c, amx iff bmx and xma iff xmb.

  stochastic equivalence

  given a stochastic multigraph x, actors i and j are stochastically 

equivalent iff the id203 of any event concerning x is unchanged 
by interchanging actors i and j. 

  two network nodes are stochastically equivalent if the probabilities of 

them being related to any other individual are the same.

  role equivalence

  in a network x, a and b are role equivalent if there exist an 

automorphism f of x which maps a into b and b into a, and which is 
link preserving (f(a) = b, f(b) = a; f(c) is linked to f(d) iff c is linked to 
d).

  two nodes can be role equivalent even if they are in two 

disconnected components. obviously they are not stochastically 
equivalent.

76

applications of equivalence

  assessment of network vulnerability

  whether a target individual has a substitute 

or not.

  detecting aliases

  no link joining them directly
  many paths of length 2 connecting them

  role equivalence can be used for 

template matching. building models and 
seeking a match within the link graph.

weak ties

  weak ties are the ties which add most 

to the efficiency of communication within 
the network. they usually connect 
cliques to the outside world. 

  detecting activity of the weak ties usually 

signals a critical operation.

  eliminating weak ties may hurt the 

network the most.

77

how to derive relationships?

  wayne baker and robert faulkner looked at archival data to 

derive relationship data. 
  the data they used to analyze illegal price-fixing networks were 

mostly court documents and sworn testimony. this data included 
accounts of observed interpersonal relationships from various 
witnesses.

  bonnie erickson (erickson, 1981) talk about trusted prior contacts 

for the effective functioning of a secret society. 
  the 19 hijackers appeared to have come from a network that had 

formed while they were completing terrorist training in afghanistan. 

  many were school mates, some had lived together for years, and 

others were related by kinship ties. 

  deep trusted ties, that were not easily visible to outsiders, wove this 

terror network together.

a complete link detection system

78

types of link detection questions:

  who is central in the organization?
  which 3 individuals    removal or incapacitation 

would sever this drug-supply network?

  what role or roles does specific individual 

appear to be playing in a given organization?

  which communication channels without a 
terrorist organization are worth monitoring?

  what significant changes have taken place in 

the supply operation of a given organization 
since this time last year?

spring graph of people co 
occurrence graph

79

visualizing relationship between 
companies

link analysis in the 9/11 
context (krebs 2001)

80

analysis (krebs 2001)

  many on the same flight were more than two steps 

away from each other. a strategy for keeping cell 
members distant from each other, and from other 
cells, minimizes damage to the network if a cell 
member is captured or otherwise compromised. 

  "those who were trained to fly didn't know the others. 

one group of people did not know the other group." 
  usama bin laden 

average path length

contacts

contacts + shortcuts

4.75
2.79

81

trusted prior contacts

82

so how was the    work    done?

  special meetings were held to connect distant 

parts of the network.

  these meetings created transitory shortcuts 

(watts, 1999).

  one of the known meetings was held in las 

vegas.

  as a results, 6 shortcuts were added to the 

network and it reduced the average path 
length by 40%.

  all pilots formed a small clique!  

measuring network parameters

degrees
0.361

mohammad 
atta
marwan al-
shehhi
hani
hanjour

essid sami
ben kemais
nawaf
alhazmi
ramzi bin 
al-shibh

0.295

0.213

0.18

0.18

0.164

ziad jarrah

0.164

betweeness

closeness

mohammad 
atta
essid sami
ben kemais
zacarias
moussaoui

nawaf
alhazmi
hani
hanjour
djamal
beghal

marwan al-
shehhi

0.588

0.252

0.232

0.154

0.126

0.105

0.088

mohammad 
atta
marwan al-
shehhi
hani
hanjour

nawaf
alhazmi
ramzi bin 
al-shibh
zacarias
moussaoui
essid sami
ben kemais

0.587

0.466

0.445

0.442

0.436

0.436

0.433

83

ranking the terrorists

building networks

84

example

another example

85

drilling down

person-organization-person

86

which person connects wal-mart 
and kinko   s?

conclusions

  information extraction is a mature technology 
that can create a solid foundation for real text 
mining.

  entity extraction can be performed with very 

high accuracy (~95% breakeven).

  entity extraction can be implemented either by 

using rule based approaches or by using 
machine learning approaches (id48 or tbr)
  link detection is a very effective method to 

uncover indirect relationships between entities.

87

future work

  machine learning approaches for 

relationship extraction

  hybrid approaches (rules based + 
machine learning) for relationship 
extraction

  visual authoring environments for rule 

based methods

  visual environment for defining complex 

link detection queries.

references i

 

 

 

 

 

 

 

 

 

 

 

 

anand t. and kahn g., 1993. opportunity explorer: navigating large databases using knowledge 
discovery templates. in proceedings of the 1993 workshop on knowledge discovery in databases.
appelt, douglas e., jerry r. hobbs, john bear, david israel, and mabry tyson, 1993. ``fastus: a 
finite-state processor for information extraction from real-world text'', proceedings. ijcai-93, 
chambery, france, august 1993. 
appelt, douglas e., jerry r. hobbs, john bear, david israel, megumi kameyama, and mabry tyson, 
1993a. ``the sri muc-5 jv-fastus information extraction system'', proceedings, fifth message 
understanding conference (muc-5), baltimore, maryland, august 1993. 
bookstein a., klein s.t. and raita t., 1995. clumping properties of content-bearing words. in 
proceedings of sigir   95.
brachman r. j., selfridge p. g., terveen l. g.,  altman b., borgida a., halper f., kirk t., lazar a., 
mcguinness d. l., and resnick l. a., 1993. integrated support for data archaeology. international 
journal of intelligent and cooperative information systems 2(2):159-185.
brill e., 1995. transformation-based error-driven learning and natural language processing: a case study 
in part-of-speech tagging, computational linguistics, 21(4):543-565
church k.w. and hanks p., 1990. word association norms, mutual information, and id69, 
computational linguistics, 16(1):22-29
cohen w. and singer y., 1996. context sensitive learning methods for text categorization. in 
proceedings of sigir   96.
cohen. w., "compiling prior knowledge into an explicit bias". working notes of the 1992 aaai spring 
symposium on knowledge assimilation. stanford, ca, march 1992.
daille b., gaussier e. and lange j.m., 1994. towards automatic extraction of monolingual and bilingual 
terminology, in proceedings of the international conference on computational linguistics, coling   94, 
pages 515-521.
dumais, s. t., platt, j., heckerman, d. and sahami, m. inductive learning algorithms and representations 
for text categorization. proceedings of the seventh international conference on information and 
knowledge management (cikm   98), 148-155, 1998.
dunning t., 1993. accurate methods for the statistics of surprise and coincidence, computational 
linguistics, 19(1).

88

references ii

 

 

 

 

 

 

 

 

 

 

 

 

 

 

feldman r. and dagan i., 1995. kdt     knowledge discovery in texts. in proceedings of the first international conference on 
knowledge discovery, kdd-95.
feldman r., and hirsh h., 1996. exploiting background information in knowledge discovery from text. journal of intelligent 
information systems. 1996.
feldman r., aumann y., amir a., kl  sgen w. and zilberstien a., 1997. maximal association rules: a new tool for mining for 
keyword co-occurrences in document collections, in proceedings of the 3rd international conference on knowledge 
discovery, kdd-97,  newport beach, ca.
feldman r., rosenfeld b., stoppi j., liberzon y. and schler, j., 2000.    a framework for specifying explicit bias for revision of 
approximate information extraction rules   . kdd 2000: 189-199.
fisher d., soderland s., mccarthy j., feng f. and lehnert w., "description of the umass systems as used for muc-6," in 
proceedings of the 6th message understanding conference, november, 1995, pp. 127-140.
frantzi t.k., 1997. incorporating context information for the extraction of terms. in preceedings of acl-eacl   97. 
frawley w. j., piatetsky-shapiro g., and matheus c. j., 1991. knowledge discovery in databases: an overview. in g. 
piatetsky-shapiro and w. j. frawley, editors, knowledge discovery in databases, pages 1-27, mit press.
grishman r., the role of syntax in information extraction, in: advances in text processing: tipster program phase ii, morgan 
kaufmann, 1996.
hahn, u.; and schnattinger, k. 1997.  deep knowledge discovery from natural language texts. in proceedings of the 3rd 
international conference of knowledge discovery and data mining, 175-178.
hayes ph. 1992. intelligent high-volume processing using shallow, domain-specific techniques. text-basedintelligent
systems: current research and practice in information extraction and retrieval. new jersey, p.227-242.
hayes, p.j. and weinstein, s.p. construe: a system for content-based indexing of a database of news stories. second 
annual conference on innovative applications of artificial intelligence, 1990.
hobbs, j. (1986), resolving pronoun references, in  b.  j.  grosz,  k.  sparck jones,  &  b.  l.  webber (eds.), readings in 
natural language processing (pp. 339-352),  los  altos,  ca:  morgan  kaufmann publishers, inc.
hobbs, jerry r., douglas e. appelt, john bear, david israel, megumi kameyama, and mabry tyson, ``fastus: a system for 
extracting information from text'', proceedings, human language technology, princeton, new jersey, march 1992, pp. 133-
137.
hobbs, jerry r., mark stickel, douglas appelt, and paul martin, 1993, ``interpretation as abduction'', artificial intelligence, vol. 
63, nos. 1-2, pp. 69-142. also published as sri international artificial intelligence center technical note 499, december 1990.

references iii

 

 

 

 

 

 

 

 

 

 

 

 

 

 

hull d., 1996. id30 algorithms - a case study for detailed evaluation. journal of the american society for information 
science. 47(1):70-84. 
ingria, r. & stallard, d., a computational mechanism  for  pronominal  reference, proceedings, 27th  annual  meeting  of  the  
association  for computational linguistics, vancouver, 1989.
cowie j., and lehnert w., "information extraction," communications of the association of computing machinery, vol. 39 (1), pp. 
80-91.
joachims, t. text categorization with support vector machines: learning with many relevant features. proceedings of 
european conference on machine learning (ecml   98), 1998
justeson j. s. and katz s. m., 1995. technical terminology: some linguistic properties and an algorithm for identification in 
text. in natural language engineering, 1(1):9-27.
kl  sgen w., 1992. problems for knowledge discovery in databases and their treatment in the statistics interpreter 
explora. international journal for intelligent systems, 7(7):649-673.
lappin,  s. &  mccord,  m.,  a  syntactic filter  on  pronominal  anaphora  for  slot  grammar, proceedings, 28th annual meeting 
of the association for  computational  linguistics,  university  of pittsburgh,  association  for  computational linguistics, 1990.
larkey, l. s. and croft, w. b. 1996. combining classi   ers in text categorization. in proceedings of sigir-96, 19th 
acminternational conference on research and development in information retrieval (zurich, ch, 1996), pp. 289   297.
lehnert, wendy, claire cardie, david fisher, ellen riloff, and robert williams, 1991. ``description of the circus system as 
used for muc-3'', proceedings, third message understanding conference (muc-3), san diego, california, pp. 223-233. 
lent b., agrawal r. and srikant r., 1997. discovering trends in text databases. in proceedings of the 3rd international 
conference on knowledge discovery, kdd-97,  newport beach, ca.
lewis, d. d. 1995a. evaluating and optmizing autonomous text classi   cation systems. in proceedings of sigir-95, 18th 
acminternational conference on research and development in information retrieval (seattle, us, 1995), pp. 246   254.
lewis, d. d. and hayes, p. j. 1994. guest editorial for the special issue on text categorization. acmt ransactions on 
information systems 12, 3, 231.
lewis, d. d. and ringuette, m. 1994. a comparison of two learning algorithms for text categorization. in proceedings of sdair-
94, 3rd annual symposium on document analysis and information retrieval (las vegas, us, 1994), pp. 81   93.
lewis, d. d., schapire, r. e., callan, j. p., and papka, r. 1996. training algorithms for linear text classi   ers. in proceedings of 
sigir-96, 19th acminternational conference on research and development in information retrieval (z  urich, ch, 1996), pp. 
298   306.

89

references iv

 

 

 

 

 

 

 

 

 

 

 

 

 

 

lin d. 1995. university of manitoba: description of the pie system as used for muc-6 . in proceedings of the sixth conference 
on message understanding (muc-6), columbia, maryland.  
piatetsky-shapiro g. and frawley w. (eds.), 1991. knowledge discovery in databases, aaai press, menlo park, ca.
rajman m. and besan  on r., 1997. id111: natural language techniques and id111 applications. in proceedings 
of the seventh ifip 2.6 working conference on database semantics (ds-7), chapam & hall ifip proceedings serie. leysin, 
switzerland, oct 7-10, 1997.
riloff ellen and lehnert wendy, information extraction as a basis for high-precision text classification, acm transactions on 
information systems (special issue on text categorization) or also umass-te-24, 1994.
sebastiani f. machine learning in automated text categorization, acm computing surveys, vol. 34, number 1, pages 1-47, 
2002.
sheila tejada, craig a. knoblock and steven minton, learning object identification rules for information integration, 
information systems vol. 26, no. 8, 2001, pp. 607-633. 
soderland s., fisher d., aseltine j., and lehnert w., "issues in inductive learning of domain-specific text extraction rules," 
proceedings of the workshop on new approaches to learning for natural language processing at the fourteenth international 
joint conference on artificial intelligence, 1995.
srikant r. and agrawal r., 1995. mining generalized association rules. in proceedings of the 21st vldb conference, zurich, 
swizerland.
sundheim, beth, ed., 1993. proceedings, fifth message understanding conference (muc-5), baltimore, maryland, august 
1993. distributed by morgan kaufmann publishers, inc., san mateo, california. 
tipster text program (phase i), 1993. proceedings, advanced research projects agency, september 1993. 
vapnik, v., estimation of dependencies based on data [in russian], nauka, moscow, 1979. (english translation: springer 
verlag, 1982.)
vapnik, v., the nature of statistical learning theory, springer-verlag, 1995.
yang, y. and chute, c. g. 1994. an example-based mapping method for text categorization and retrieval. acmt ransactions
on information systems 12, 3, 252   277.
yang, y. and liu, x. 1999. a re-examination of text categorization methods. in proceedings of sigir-99, 22nd acm 
international conference on research and development in information retrieval (berkeley, us, 1999), pp. 42   49.

90

