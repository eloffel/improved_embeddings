foundations and trends r(cid:1) in
information retrieval
vol. 2, no. 3 (2008) 137   213
c(cid:1) 2008 c. zhai
doi: 10.1561/1500000008

statistical language models for information

retrieval a critical review

chengxiang zhai

university of illinois at urbana-champaign, 201 n. goodwin, urbana, il
61801, usa, czhai@cs.uiuc.edu

abstract

statistical language models have recently been successfully applied
to many information retrieval problems. a great deal of recent work
has shown that statistical language models not only lead to superior
empirical performance, but also facilitate parameter tuning and open
up possibilities for modeling nontraditional retrieval problems. in gen-
eral, statistical language models provide a principled way of model-
ing various kinds of retrieval problems. the purpose of this survey is
to systematically and critically review the existing work in applying
statistical language models to information retrieval, summarize their
contributions, and point out outstanding challenges.

1

introduction

the goal of an information retrieval (ir) system is to rank documents
optimally given a query so that relevant documents would be ranked
above nonrelevant ones. in order to achieve this goal, the system must
be able to score documents so that a relevant document would ideally
have a higher score than a nonrelevant one.

clearly the retrieval accuracy of an ir system is directly determined
by the quality of the scoring function adopted. thus, not surprisingly,
seeking an optimal scoring function (retrieval function) has always been
a major research challenge in information retrieval. a retrieval function
is based on a retrieval model, which formalizes the notion of relevance
and enables us to derive a retrieval function that can be computed to
score and rank documents.

over the decades, many di   erent types of retrieval models have been
proposed and tested. a great diversity of approaches and methodology
has developed, but no single uni   ed retrieval model has proven to be
most e   ective. indeed,    nding the single optimal retrieval model has
been and remains a long-standing challenge in information retrieval
research.

138

139

the    eld has progressed in two di   erent ways. on the one hand,
theoretical models have been proposed often to model relevance
through id136s; representative models include the logic models
[27, 111, 115] and the id136 network model [109]. however, these
models, while theoretically interesting, have not been able to directly
lead to empirically e   ective models, even though heuristic instantia-
tions of them can be e   ective. on the other hand, there have been many
empirical studies of models, including many variants of the vector space
model [89, 90, 91, 96] and probabilistic models [26, 51, 80, 83, 110, 109].
the vector-space model with heuristic tf-idf weighting and docu-
ment length id172 has traditionally been one of the most e   ec-
tive retrieval models, and it remains quite competitive as a state of
the art retrieval model. the popular bm25 (okapi) retrieval function
is very similar to a tf-idf vector space retrieval function, but it is
motivated and derived from the 2-poisson probabilistic retrieval model
[84, 86] with heuristic approximations. bm25 is one of the most robust
and e   ective retrieval functions. another e   ective retrieval model is
divergence from randomness which is based on probabilistic justi   ca-
tions for several term weighting components [1].

while both vector space models and bm25 rely on heuristic design
of retrieval functions, an interesting class of probabilistic models called
id38 approaches to retrieval have led to e   ective retrieval
functions without much heuristic design. in particular, the query like-
lihood retrieval function [80] with dirichlet prior smoothing [124]
has comparable performance to the most e   ective tf-idf weighting
retrieval functions including bm25 [24]. due to their good empiri-
cal performance and great potential of leveraging statistical estima-
tion methods, the id38 approaches have been attracting
much attention since ponte and croft   s pioneering paper published in
acm sigir 1998 [80]. many variations of the basic language mod-
eling approach have since been proposed and studied, and language
models have now been applied to multiple retrieval tasks such as cross-
lingual retrieval [54], distributed ir [95], expert    nding [25], passage
retrieval [59], web search [47, 76], genomics retrieval [129], topic track-
ing [41, 53, 99], and subtopic retrieval [122].

140 introduction

this survey is to systematically review this development of the
id38 approaches. we will survey a wide range of retrieval
models based on id38 and attempt to make connections
between this new family of models and traditional retrieval models.
we will summarize the progress we have made so far in these models
and point out remaining challenges to be solved in order to further
increase their impact.

the survey is written for readers who have already had some basic
knowledge about information retrieval. readers with no prior knowl-
edge about information retrieval will    nd it more comfortable to read
an ir textbook (e.g., [29, 63])    rst before reading this survey. the read-
ers are also assumed to have already had some basic knowledge about
id203 and statistics such as maximum likelihood estimator, but
a reader should still be able to follow the high-level discussion in the
survey even without such background.

the rest of the survey is organized as follows. in section 2, we review
the very    rst generation of language models which are computationally
as e   cient as any other existing retrieval model. the success of these
early models has stimulated many follow-up studies and extensions of
language models for retrieval. in section 3, we review work that aims
at understanding why these language models are e   ective and why
they can be justi   ed based on relevance. in section 4, we review work
on extending and improving the basic id38 approach.
feedback is an important component in an ir system, but it turns
out that there is some di   culty in supporting feedback with the    rst
generation basic id38 approach. in section 5, we review
several lines of work on developing and extending language models to
support feedback (particularly pseudo feedback). they are among the
most e   ective language models for retrieval. in section 6, we further
review a wide range of applications of language models to di   erent spe-
cial retrieval tasks where a standard language model is often extended
or adapted to better    t a speci   c application. finally, in section 7, we
brie   y review some work on developing general theoretical frameworks
to facilitate systematic applications of language models to ir. we
summary the survey and discuss future research directions in section 8.

2

the basic id38 approach

in this section, we review the basic id38 approach (often
called the query likelihood scoring method) which represents the very
   rst generation of language models applied to information retrieval.
extensions of these models are reviewed in the next few sections.

2.1 ponte and croft   s pioneering work

the id38 approach was    rst introduced by ponte and
croft in their sigir 98 paper [80]. in this work, they proposed a new
way to score a document, later often called the query likelihood scoring
method. the basic idea behind the new approach is simple:    rst esti-
mate a language model for each document, and then rank documents by
the likelihood of the query according to the estimated language model
of each document. this new method was shown to be quite e   ective.
they called this approach    id38 approach    due to the
use of language models in scoring.

the term language model refers to a probabilistic model of text (i.e.,
it de   nes a id203 distribution over sequences of words). before it
was applied to retrieval, it had already been used successfully in related

141

142 the basic id38 approach

areas such as id103 [39] and machine translation [11]. in
these applications, language models are used to assess what kind of
word sequences are more typical according to language usages, and
inject the right bias accordingly into a id103 system or
machine translation system to prefer an output sequence of words with
high id203 according to the language model.

in the basic id38 approach proposed by ponte and
croft, the query is assumed to be a sample of words drawn according
to a language model estimated based on a document (i.e., a document
language model). we will then ask the question: which document lan-
guage model gives our query the highest id203? documents can
thus be ranked based on the likelihood of generating the query using
the corresponding document model. intuitively, if a document language
model gives the query a high id203, the query words must have
high probabilities according to the document language model, which
further means that the query words occur frequently in the document.
formally, the general idea of the query likelihood retrieval function
can be described as follows. let q be a query and d a document. let
  d be a language model estimated based on document d. we de   ne
the score of document d with respect to query q as the conditional
id203 p(q|  d). that is,

score(q, d) = p(q|  d).

(2.1)

clearly in order to use such a model to score documents, we must
solve two problems: (1) how to de   ne   d? (2) how to estimate   d based
on document d? note that the de   nition of   d is quite critical as it
would completely determine how we model the query, thus essentially
determining the query representation to be adopted.

in ponte and croft   s paper, the model   d has not been explicitly
de   ned, but the    nal retrieval function derived suggests that the model
is a multiple bernoulli model. formally, let v = {w1, . . . , w|v |} be the
vocabulary of the language of our documents. we can de   ne a binary
random variable xi     {0,1} for each word wi to indicate whether word
wi is present (xi = 1) or absent (xi = 0) in the query. thus model   d
would have precisely |v | parameters, i.e.,   d = {p(xi = 1|d)}i   [1,|v |],
which can model presence and absence of all the words in the query.

2.1 ponte and croft   s pioneering work

143

according to this model, the query likelihood can be written as:

p(q|  d) =

p(xi = 1|d)

(1     p(xi = 1|d)),

(cid:1)

wi   q

(cid:1)

wi /   q

where the    rst product is for words in the query and the second words
not occurring in the query. computationally, the retrieval problem now
boils down to estimating all the |v | parameters (i.e., p(xi = 1|d))
based on d; di   erent ways to estimate the parameters would lead to
di   erent retrieval functions.

in order to estimate the multiple bernoulli model   d, we would
assume that document d is a sample of   d. if we are to interpret d as
a single bit vector representing the presence and absence of each word,
we would not be able to capture term frequency (tf) since a bernoulli
model only models the presence and absence of a word rather than how
many times a word occurs. to capture tf, we can treat each word wi
in d as a sample from our model where only wi has shown up and
all other words are absent. thus according to the maximum likelihood
(ml) estimator, p(xi = 1|d) is equal to the relative frequency of word
wi in d, i.e.,

p(xi = 1|d) =

c(wi, d)

|d|

,

where c(wi, d) is the count of word wi in d and |d| is the length of
d (i.e., the total word counts). this derivation is discussed in detail
in [69].

one problem with this ml estimator is that an unseen word in docu-
ment d would get a zero id203, making all queries containing an
unseen word have zero id203 p(q|  d). this is clearly undesirable.
more importantly, since a document is a very small sample for our
model, the ml estimate is generally not accurate. so an important
problem we have to solve is to smooth the ml estimator so that we
do not assign zero id203 to unseen words and can improve the
accuracy of the estimated language model in general.

in ponte and croft   s model, they set the id203 of an unseen
word to that of the word in the whole collection of documents. intui-
tively, this is to say that if we do not observe a word in the document,

144 the basic id38 approach

we would assume that the id203 of such a word is the same as
the id203 of seeing the word in any document in the whole collec-
tion. this ensures that none of the words in the collection would get a
zero id203. to further improve the robustness of smoothing, they
also heuristically take the geometric mean of the ml estimate and the
average term frequency in all other documents in the collection [80].

ponte and croft   s work makes two important contributions in study-
ing retrieval models: first, it introduces a new e   ective probabilistic
ranking function based on query likelihood with smoothed estimate
of model parameters. while the previous probabilistic models (e.g.,
[20, 83]) have failed to directly lead to an empirically e   ective retrieval
function due to the di   culty in estimating parameters,1 this new query
likelihood retrieval model makes the parameter estimation problem eas-
ier to solve (see section 3 for more discussion about this). second, it
connects the di   cult problem of text representation and term weighting
in ir with the id38 techniques that have been well-
studied in other application areas such as statistical machine transla-
tion and id103, making it possible to exploit various kinds
of id38 techniques to address the representation problem.
such a connection was actually recognized in some early work, but no
previous work has looked into the problem of how to estimate such a
model accurately. for example, wong and yao [114] proposed to use
a multinomial model to represent a document, but they just used the
ml estimator and did not further study the estimation problem.

2.2 bbn and twenty-one in trec-7

at about the same time and apparently independent of ponte and
croft   s work, two trec-7 participating groups, bbn [70] and twenty-
one [34], have used the same idea of scoring documents with query-
likelihood but with a slightly di   erent de   nition of the actual model.
instead of using multiple bernoulli, these two groups used a multi-
nomial distribution model, which is more commonly called a unigram
language model. such a model directly models the counts of terms and

1 the relevance model [55] to be discussed later can be regarded as a technique to solve the
parameter estimation problem in these classic probabilistic models.

2.2 bbn and twenty-one in trec-7

145

is more common in other applications such as id103 than
the multiple bernoulli; the latter was more popular for retrieval and
was already used in an earlier probabilistic retrieval model [83]. both
groups have achieved very good empirical performance in the trec-7
evaluation using their new models.
speci   cally, these two groups de   ne   d as a unigram language
model or multinomial word distribution, i.e.,   d = {p(wi|d)}i   [1,|v |],
where p(wi|d) is the id203 of word wi. note that as in the
previous section, we use   d to denote a id203 distribution
and p(w|d) to denote the id203 of a word according to the
(cid:2)|v |
distribution   d. however, unlike in multiple bernoulli, where our
constraint is p(xi = 1|d) + p(xi = 0|d) = 1, here our constraint is
p(wi|d) = 1. according to such a model, the likelihood of a query

i=1

q = q1...qm, where qi is a query word, would be
p(qi|d).

p(q|  d) =

m(cid:1)

i=1

for example, a document language model   d might assign a proba-
bility of 0.1 to the word    computer    and 0.05 to the word    virus    (i.e.,
p(computer|d) = 0.1, p(virus|d) = 0.05). if our query q is    computer
virus,    we would have p(q|  d) = 0.1     0.05 = 0.005. thus intuitively,
the more frequently a query word occurs in document d, the higher
the query likelihood would be for d, capturing the basic tf retrieval
heuristic [24].

as in the case of multiple bernoulli, the retrieval problem is now
reduced to the problem of estimating the language model   d (i.e.,
p(w|d) for each word w). once again, the issue of smoothing the ml
estimate is critical. in both groups    work,   d is smoothed by interpo-
lating the ml estimate with a background language model estimated
using the entire collection:

p(w|d) = (1       )

|d| +   p(w|c),
c(w, d)

where p(w|c) is a collection (background) language model estimated
based on word counts in the entire collection and        [0,1] is a smooth-
ing parameter.

146 the basic id38 approach

the two groups used a slightly di   erent formula for estimating
p(w|c); bbn used the normalized total counts of a word in the collec-
tion while twenty-one used the normalized total number of documents
containing a word [34, 70]. the general idea is similar, though, and it
is also similar to what ponte and croft used in their estimate of   d.

these two groups also went beyond the basic query likelihood scor-
ing formula to introduce a document prior p(d) using bayes formula,
thus suggesting that we essentially score a document based on the con-
ditional id203 p(d|q):

p(d|q) =

p(q|d)p(d)

p(q)

    p(q|d)p(d).

(2.2)

note that in this derivation, we have not speci   ed how to interpret
p(q|d). one way is to interpret it as p(q|  d), which would give us pre-
cisely the basic query likelihood scoring formula originally introduced
in [80]. however, other interpretations may also be possible (e.g., the
translation model [4]).

the document prior p(d) (which should be distinguished from
p(  d)) can be useful for introducing additional retrieval criteria to
favor documents with certain features, and indeed has been explored
in [47, 49, 58]. this prior presumably can also be added to the query
likelihood formula proposed by ponte and croft. thus this formulation
is a more general formulation of the basic id38 approach
than the query likelihood retrieval function proposed by ponte and
croft. a similar formulation was also given in [75], where ng also dis-
cussed other issues including how to estimate the smoothing parameter
with pseudo feedback.

there are two additional points worth mentioning: first, the bbn
group presented their model as a hidden markov model (id48) [82];
the id48 makes smoothing more explicit and also has the potential to
accommodate more sophisticated models, particularly combining di   er-
ent representations [70]. second, the twenty-one group revealed the
connection of their language model with traditional retrieval heuris-
tics such as tf-idf weighting [96] and document length id172
[34, 30], which o   ers an intuitive justi   cation for this new retrieval

2.3 variants of the basic id38 approach

147

model. a more general derivation of this connection can be found
in [124].

2.3 variants of the basic id38 approach

the basic id38 approach (i.e., the query likelihood scor-
ing method) can be instantiated in di   erent ways by varying (1)   d
(e.g., multiple bernoulli or multinomial), (2) estimation methods of   d
(e.g., di   erent smoothing methods), or (3) the document prior p(d).
indeed, this has led to many variants of this basic model, which we now
brie   y review.

although the original work by ponte and croft used the multi-
ple bernoulli model, it has not been as popular as the multinomial
model. one reason may be because the latter can capture the term
frequency in documents (as well as the query) more naturally than the
former; indeed, the multiple bernoulli model clearly ignores query term
frequencies and is also somewhat unnatural to incorporate tf in the
documents. note that both models make some independence assump-
tion about term occurrences, but their assumptions are di   erent. in
multiple bernoulli model, the presence/absence of a term is assumed
to be independent of that of other terms, whereas in multinomial model,
every word occurrence is assumed to be independent, including the mul-
tiple occurrences of the same term. since once an author starts to use
a term in a document, the author tends to use the term again, treating
multiple occurrences of the same term as independent can cause    over
counting    of the occurrences of a term. recently some new models (e.g.,
dirichlet compound multinomial [62]) have been proposed to address
this problem and model word burtiness; they may potentially lead to
better retrieval models. the multiple bernoulli model and multinomial
model were compared empirically earlier in the context of text catego-
rization [64] and later in the context of query likelihood for retrieval
[52, 97]. the current conclusions seem to be that multinomial model is
better, but more evaluation is needed to make more de   nitive conclu-
sions.

recently, a poisson model was proposed as an alternative for the
query likelihood retrieval function [65] and some promising results

148 the basic id38 approach

have been achieved. one potential advantage of multiple bernoulli over
multinomial is the possibility of naturally smoothing the model for each
term independently (because each term is treated as an independent
event),2 which provides    exibility for optimizing smoothing at a per-
term basis, while multinomial can naturally capture term frequencies in
the query, which are ignored in multiple bernoulli. poisson model can
accommodate both    exible smoothing and modeling term frequencies,
making it a very interesting model to further study.

another variation is to relax the independence assumptions made in
the basic model to capture some limited dependency such as through
bigram language models. we will review this line of work and other
extensions in section 4.

estimation of   d is quite critical for this family of models, and a
particularly important issue is how to smooth the maximum likelihood
estimate which assigns zero id203 to unseen words. many di   er-
ent smoothing methods have been used. in addition to those mentioned
earlier in our discussion, there are many other smoothing methods that
can be applied (see, e.g., [17, 45]). zhai and la   erty [124] empirically
compared three di   erent smoothing methods, including jelinek-mercer
(   xed coe   cient interpolation) [40], dirichlet prior [61], and absolute
discounting [74], on several standard test collections. most of these
smoothing methods end up interpolating the original maximum likeli-
hood estimate with the collection background language model in some
way. despite this similarity, di   erent smoothing methods can perform
di   erently. it was found that in general, dirichlet prior smoothing works
the best, especially for keyword queries (nonverbose queries). the rea-
son may be because it adjusts the amount of smoothing according to
the length of a document in a reasonable way (longer documents get
less smoothing). furthermore, interpolation-based smoothing methods
all work better than backo    smoothing methods [17, 44], even though
the latter works well for id103, which is likely due to the
lack of an idf e   ect in backo    smoothing. this point will be further
elaborated in section 3.2.

2 one can also let the dirichlet prior smoothing parameter    take a term-speci   c value   i
for term wi to achieve term-speci   c smoothing, for multinomial, but this is not as natural
as in the case of multiple bernoulli.

2.3 variants of the basic id38 approach

149

the dirichlet prior smoothing method can be derived by using
bayesian estimation (instead of ml estimation) with a dirichlet con-
jugate prior [61, 125], and the formula is as follows:

p(w|d) =

c(w, d) +   p(w|c)

|d| +   

,

where p(w|c) is a background (collection) language model and    is a
smoothing parameter, which can be interpreted as the total number
of pseudo counts of words introduced through the prior. the dirichlet
prior smoothing method has now become a very popular smoothing
method for smoothing language models in an ir task.

the study by zhai and la   erty has also shown that retrieval per-
formance can be quite sensitive to the setting of smoothing parameters
and suggested that smoothing plays two di   erent roles in the query
likelihood retrieval formula, an issue we will further discuss later.

all these smoothing methods discussed so far are simple in the sense
that di   erent documents are smoothed using the same collection lan-
guage model. intuitively, each document can have its own    reference
language model    for smoothing, and there has been work done in this
direction. we will review this line of work in section 4.

the document prior p(d) can naturally incorporate any static rank-
ing preferences of documents (i.e., ranking preferences independent of
a query) such as id95 scores or other document features. in this
line of the work, kraaij and coauthors [47] successfully leveraged this
prior to implement an interesting web search heuristic for named page
   nding. their idea is to prefer pages with shorter urls since an entry
page tends to have a shorter url. they used some training data to
estimate the prior p(d) based on url lengths, and showed that this
prior can improve search performance signi   cantly [47]. li and croft
[58] studied how to leverage the document prior p(d) to implement
time-related preferences in retrieval so that a document with a more
recent time would be preferred. this strategy has been shown to be
e   ective for a particular set of    recency queries.    in a study by kurland
and lee [49], a id95 score computed using induced links between
documents based on document similarity has been used as a prior to
improve retrieval accuracy. in [132] priors to capture document quality

150 the basic id38 approach

are shown to be e   ective for improving the accuracy of the top-ranked
documents in ad hoc web search.

2.4 summary

in this section, we reviewed the basic id38 approach,
which is roughly characterized by the use of query likelihood for scor-
ing and simple smoothing methods based on a background collection
language model. such a basic approach (especially with dirichlet prior
smoothing) has been shown to be as e   ective as well-tuned existing
retrieval models such as pivoted length id172 and bm25 [24].
retrieval functions in this basic id38 approach can gener-
ally be computed as e   ciently as any standard tf-idf retrieval model
with the aid of an inverted index.3

3 this point will be further elaborated in section 3.2.

3

understanding query likelihood scoring

although the query likelihood retrieval method has performed well
empirically, there were questions raised regarding its foundation as
a retrieval model, particularly its connection with the key notion in
retrieval     relevance [98]. indeed, none of the early work has provided a
rigorous treatment of the language model   d, nor has it provided a solid
connection between query likelihood and relevance. thus it is unclear
how we should interpret   d: is it a model for documents or for queries?
one may also question whether such models have just happened to
perform well, but without any solid relevance-based foundation.

3.1 relevance-based justi   cation for query likelihood

the super   cial justi   cation based on equation (2.2) suggests the fol-
lowing relevance-based interpretation: suppose that there is precisely
one relevant document, and the observed query has been    generated   
using that relevant document. we can then use the bayes    rule to infer
which document is    that relevant document    based on the observed
query. this leads to equation (2.2), which boils down to scoring with
p (q|d) under the assumption of a uniform prior p(d). unfortunately,

151

152 understanding query likelihood scoring

such a    single relevant document    formulation raises many questions
as discussed in [98].

to better understand the retrieval foundation of the query likelihood
method, la   erty and zhai [51] o   ered a more general relevance-based
derivation of the query likelihood method. speci   cally, they show that
the query likelihood retrieval function can be justi   ed in a similar way
as the classical probabilistic retrieval model based on the id203
ranking principle [85].
the starting point of the derivation is the id155
p(r = 1|q, d) (r     {0,1} is a binary relevance random variable) which
is the id203 that document d is relevant to query q. the proba-
bility ranking principle provides a justi   cation for ranking documents
for a query based on this id155. this is equivalent
to ranking documents based on the odds ratio, which can be further
transformed using bayes    rule:

o(r = 1|q, d) =

(3.1)
at this point, we may decompose the joint id203 p (q, d|r)

.

p(r = 1|q, d)
p(r = 0|q, d)

    p(q, d|r = 1)
p(q, d|r = 0)

in two di   erent ways:

(1) document generation: p(q, d|r) = p(d|q, r)p(q|r), and
(2) query generation: p(q, d|r) = p(q|d, r)p(d|r).

with document generation, we have

o(r = 1|q, d)     p(d|q, r = 1)
p(d|q, r = 0)

.

thus the ranking is equivalent to the ranking given by the clas-
sical robertson   sparck-jones probabilistic model
if we de   ne
p (d|q, r) as multiple bernoulli models [51]: see [88] for an in-depth
discussion of other variants such as multinomial and poisson as well as
the relationship between di   erent variants.

[83]

with query generation, we have

o(r = 1|q, d)     p(q|d, r = 1)
p(q|d, r = 0)

p(r = 1|d)
p(r = 0|d)

.

3.1 relevance-based justi   cation for query likelihood

153
if we make the assumption that p(q|d, r = 0) = p(q|r = 0) (i.e., the
distribution of    nonrelevant queries    does not depend on the particular
document, which is not a very strong assumption), we obtain

o(r = 1|q, d)     p(q|d, r = 1)

p(r = 1|d)
p(r = 0|d)

.

the term p(r=1|d)
p(r=0|d) can be interpreted as a prior of relevance on a
document, which can be estimated based on additional information
such as the links pointing to d in a hypertext collection. without such
extra knowledge, we may assume that this term is the same across all
the documents, which gives the following relation [51]1

o(r = 1|q, d)     p(q|d, r = 1),

which provides a relevance-based justi   cation for the query likelihood
scoring method.
note that the query likelihood derived above, i.e., p(q|d, r), has an
additional relevance variable r in the id155, which is
essential to obtain a relevance-based justi   cation for query likelihood
scoring and gives a clear interpretation of the   d mentioned earlier.
speci   cally, it suggests that the id203 p(q|d) used in all the query
likelihood scoring methods should be interpreted as p(q|d, r = 1),
which intuitively means the id203 that a user who likes document
d would pose query q.

clearly this does not mean that the user can only like one docu-
ment, thus there is no concern about the    single relevant document.   
furthermore, this also suggests that   d is really a model for what kind
of queries would be posed if a user wants to retrieve document d. when
we estimate   d using a document d as observation, we are essentially
using words in d to approximate the queries a user might pose to
retrieve d, which is reasonable but not ideal. for example, anchor text
describing a link to document d can be a better approximation of the
queries a user might pose to retrieve d, and can thus be leveraged to
improve the estimate of   d as explored in [73].

1 a stronger assumption p(r, d) = p(r)p(d) has been used in [51] to derive this relation.

154 understanding query likelihood scoring

3.2 query likelihood, smoothing, and tf-idf weighting

in another line of work attempting to understand why query like-
lihood scoring is e   ective as a retrieval method, zhai and la   erty
studied the robustness of query likelihood scoring and examined how
retrieval performance is a   ected by di   erent strategies for smoothing
[121, 124, 126]. through comparing several di   erent smoothing meth-
ods, they have observed: (1) retrieval performance is sensitive to the
setting of smoothing parameters and the choice of smoothing methods;
(2) the sensitive patterns are di   erent for keyword queries (all words
are content-carrying keywords) and verbose queries (queries are sen-
tences describing the information need, thus contain many common
nondiscriminative words).

the    rst observation suggests that while heuristic term weighting
in traditional retrieval models has been replaced with language model
estimation (particularly smoothing) in the query likelihood approach,
we have not been able to escape from the need for heuristic tuning of
parameters since nonoptimal smoothing can degrade retrieval perfor-
mance signi   cantly. however, compared with tf-idf weighting param-
eters, a smoothing parameter is more meaningful from the view point
of statistical estimation. indeed, completely automatic tuning of the
smoothing parameters is shown to be possible in [125] and the perfor-
mance with automatic parameter setting is comparable to the optimal
performance achieved through manual tuning.

the second observation suggests that smoothing plays two dis-
tinct roles in the query likelihood scoring methods: one obvious role
is to address the data sparseness issue (since a document is a small
sample) and improve the accuracy of the estimated language model;
the other nonobvious role is to model the noisy (nondiscriminative)
words in the query. it is conjectured that it is this second role that
has caused the di   erent sensitivity patterns for keyword and verbose
queries; indeed since the modeling of noise in queries is much more
critical for verbose queries than keyword queries, it is not surpris-
ing that additional smoothing is often needed (for the second role) to
achieve optimal performance for verbose queries than keyword queries
as observed in [126].

3.2 query likelihood, smoothing, and tf-idf weighting

155

this second role of smoothing is also closely related to a general
connection between smoothing with a background language model and
the idf e   ect in the query likelihood scoring formula. in [124], it is
shown that if we smooth a document language model with a general
smoothing scheme where an unseen word w in document d would have
a id203 proportional to the id203 of the word given by a
collection language model (i.e. p(w|d) =   dp(w|c) with a parameter
  d to control the amount of smoothing), the query likelihood scoring
function can be rewritten as follows:
ps(qi|d)
  d p(qi|c)

   
    + m log   d +

   
    (cid:5)

log p(q|d) =

log p(qi|c),

m(cid:5)

log

i:c(qi,d)>0

i=1

where ps(qi|d) is the smoothed id203 of a seen query word qi and
m is the query length.

since the last term does not a   ect ranking, it can be ignored for
ranking. as a result, we see that the formula essentially involves a
sum of term weights over all the matched query terms in the doc-
ument, just as in any other traditional retrieval function. moreover,
each matched term contributes a tf-idf like weight. in this sense,
the query likelihood retrieval function simply o   ers an alternative way
of implementing tf-idf weighting and document length normaliza-
tion heuristics. in particular, the idf e   ect is achieved through having
p(qi|c) in the denominator of the weighting function. this means that
through smoothing, we implicitly penalize words that are common in
the collection (with high p(qi|c)). this also explains why we can model
the noise in the query through more aggressive smoothing. see [125] for
more discussion about this.

the equation above also shows that computing the query like-
lihood scoring function using any
smoothing method based on a
collection language model is as e   cient as computing a traditional
retrieval function such as the pivoted length id172 function
of the vector space model [96] with an inverted index. indeed, the
query likelihood retrieval function with several di   erent smoothing
methods has been implemented in this way in the lemur toolkit
(http://www.lemurproject.org/), which is the main retrieval toolkit

156 understanding query likelihood scoring

currently available for experimenting with id38 retrieval
approaches.

these understandings provide an empirical explanation for why the
query likelihood retrieval function is reasonable from retrieval perspec-
tive in the sense that it can be regarded as just another way of imple-
menting tf-idf weighting and document length id172. they
also suggest that the implementation of idf heuristic in this approach
is not as direct as in a traditional model, leading some researchers to
have explored alternative ways to incorporate idf [35].

4

improving the basic id38

approach

in section 2, we restricted the discussion to the family of models that
use simple smoothing methods based on a background language model;
their e   ciency is comparable to any traditional tf-idf model. in this
section, we review some improvements to the basic id38
approach, which often outperform the basic approach, but also tend to
demand signi   cantly more computation than the basic approach. all
these improvements remain in the family of query-likelihood scoring,
which distinguishes them from the other models to be reviewed in the
next section.

4.1 beyond unigram models

a natural extension of the basic query likelihood method is to go
beyond unigram language models. unlike unigram language models
where the occurrences of words are assumed to be completely indepen-
dent (an assumption obviously not holding), these models can capture
some dependency between words.

for example, song and croft [97] have studied using bigram and
trigram language models. in a bigram language model, the generation

157

158 improving the basic id38 approach

of a current word would be dependent on the previous word generated,
thus it can potentially capture the dependency of adjacent words (e.g.,
phrases). speci   cally, the query likelihood would be
p(qi|qi   1, d),

p(q|d) = p(q1|d)

m(cid:1)

where p(qi|qi   1, d) is the id155 of generating qi after
we have just generated qi   1.

i=2

such id165 models capture dependency based on word positions.
other work has attempted to capture dependency based on gram-
mar structures [28, 72, 100, 102, 101]. in all these approaches, the
retrieval formula eventually boils down to some combination of scores
from matching units larger than single words (e.g., bigrams, head-
modi   er pairs, or collocation pairs). while these approaches have
mostly shown bene   t of capturing dependencies, the improvement
tends to be insigni   cant or at least not so signi   cant as some other
extensions that can achieve some kind of pseudo feedback e   ect. (these
other extensions will be reviewed in the next section.) one reason for
these nonexciting results may be because as we move to more complex
models to capture dependency, our data becomes even more sparse,
making it di   cult to obtain accurate estimation of the model. the
general observation on these models is consistent with what researchers
have observed on some early e   ort on applying natural language pro-
cessing techniques to improve indexing, notably phrase-based indexing
[23, 56, 104, 120].

a more successful retrieval model that can capture limited depen-
dencies is the markov random field model proposed in [68]. this model
is a general discriminative model where arbitrary features can be com-
bined in a retrieval function. in most of the applications of such a
model, the features are typically the scores of a document with respect
to a query using an existing retrieval function such as the query like-
lihood, thus the markov random field model essentially serves as a
way to combine multiple scoring strategies and scoring with multiple
representations. in particular, it has been shown that one can com-
bine unigram id38 scoring with bigram scoring as well
as scoring based on word collocations within a small window of text.

4.2 cluster-based smoothing and document expansion

159

such a combination achieves better retrieval accuracy than using only
unigram scoring [68].

4.2 cluster-based smoothing and document expansion

smoothing every document with the same collection language model is
intuitively not optimal since we essentially assume that all the unseen
words in di   erent documents would have similar probabilities. ideally,
we should use some document-dependent    augmented text data    that
can more accurately re   ect the content of the document under con-
sideration. with such reasoning, several researchers have attempted
to exploit the corpus structure to achieve such document-speci   c
smoothing.

the work in this line can be grouped into two categories: (1) cluster
documents and smooth a document with the cluster containing the
document. (2) for each document, obtain the most similar documents
in the collection and then smooth the document with the obtained
   neighbor documents.   

in liu and croft [60], documents are clustered using a cosine
similarity measure, and each document is smoothed with the cluster
containing the document by interpolating the original maximum likeli-
hood estimate p(w|d) with a cluster language model p(w|cluster),
which is further smoothed by interpolating itself with a collection
language model p(w|c). such a model is shown to outperform the
baseline smoothing method that only uses the collection language
model for smoothing. however, the improvement is mostly insigni   cant.
one possible reason may be because the two roles of smoothing have
been mixed, thus if the parameters are not set appropriately, then
smoothing using cluster-based language model may actually end up
penalizing terms common in the cluster due to the idf e   ect of
smoothing, thus lowering the scores of documents matching terms
in the cluster. in kurland and lee [48], the authors presented a
general strategy for exploiting the cluster structure to achieve an
e   ect similar to smoothing document language models with clus-
ter language models; document language models are not explicitly
smoothed with a cluster language model, but a document is scored

160 improving the basic id38 approach

based on a weighted combination of its regular query likelihood score
with the likelihood of the query given the clusters containing the
document.

a soft id91 strategy has been adopted to smooth document
language models through using the id44 (lda)
model to do id91 [113]. with this model, we allow a document to
be in multiple topics (roughly like document clusters, but characterized
by unigram language models) with some uncertainties. thus smoothing
of a document can involve an interpolation of potentially many clusters;
this is di   erent from [60], where just one cluster is used for smoothing.
results reported in [113] are quite encouraging.

a problem with smoothing a document using a cluster is that the
cluster is not necessarily a good representation of similar documents
to the document to be smoothed. this is clearly the case when the
document is at the boundary of the cluster. to address this problem,
tao and others [106] proposed to construct a document-speci   c    neigh-
borhood    in the document space, essentially to form a cluster for each
document with the document at the center of the cluster. intuitively,
such a neighborhood contains the documents that are most similar to
the document, thus serves well for smoothing. to further improve the
robustness of the smoothing method, the authors assign weights to the
neighbors based on a cosine similarity measure so that a document
farther away would contribute less to smoothing. they then use the
probabilistic neighborhood to smooth the count of a word by interpo-
lating the original count in the document with a weighted sum of counts
of the word in the neighbor documents to obtain a smoothed count for
each word. such smoothed counts thus represent an    expanded docu-
ment,    and are then used as if they were the true counts of the words in
the document for further smoothing with a collection language model.
experiment results show that such a document expansion method not
only outperforms the baseline simple smoothing method (i.e., with only
a collection language model), but also outperforms the cluster-based
smoothing method proposed in [60]. moreover, it can be combined with
pseudo feedback to further improve performance [106].

in [93], this neighborhood-based document expansion method is fur-
ther extended to allow for smoothing with remotely related documents

4.3 parsimonious language models

161

through probabilistic propagation of term counts. this new smoothing
method is shown to outperform the simple smoothing methods using a
collection language model. it also achieves consistently better precision
in the top-ranked documents than both cluster-based and document
expansion smoothing methods. but interestingly, it has a worse mean
average precision than the latter, indicating room for further research
to improve this smoothing method.

a general optimization framework, which covers mostly all the work
mentioned above as special cases, is proposed in [67]. in this work,
a general objective function is explicitly de   ned for smoothing lan-
guage models over graph structures, thus o   ering a general principled
framework for smoothing. several new smoothing methods have been
derived using the framework with some outperforming the state of the
methods [67].

4.3 parsimonious language models

all the methods for smoothing discussed so far end up interpolating
counts of words in various documents, thus the estimated document
language model generally assigns high probabilities to frequent words
including stop words. from retrieval perspective, we would like our
model to be more discriminative (i.e., idf heuristic). while smoothing
with a collection language model can achieve the needed discrimination
indirectly, one may also attempt to do it more directly. motivated by
this reasoning, a    distillation    strategy with a two-component mixture
model was proposed in [121], where a query or a document is assumed
to be generated from a mixture model involving two components: one
is a    xed background (collection) language model and the other a topic
language model to be estimated. if we estimate the topic language
model by    tting such a two-component mixture model to some text
sample (e.g., query or document), the common words would be easily
   explained    by the background model; as a result, the estimated topic
model would be more discriminative and tend to assign high proba-
bilities to content-carrying words which do not have high probabilities
according to the background model. the query distillation experiments
in [121] have shown positive results from using the distillation strategy.

162 improving the basic id38 approach

such a distillation strategy was further generalized in [35] to be used
in all stages of retrieval, including indexing stage, query stage, and feed-
back stage. in all cases, the basic idea is to use a background language
model to    factor out    the nondiscriminative    background words    from
a language model. the authors call such language models parsimo-
nious language models. unfortunately, such parsimonious models have
not shown signi   cant improvement in retrieval accuracy, though they
can be useful for reducing the index size [35]. this result appears to
be counter-intuitive. there could be two possible explanations: (1) the
current id38 retrieval approach already has some    built-
in    idf heuristic (i.e., through interpolation with a collection language
model), so making the estimated model more discriminative would not
add more bene   t. (2) there may be complicated interactions between
smoothing and term weighting, so with a more discriminative language
model, we may need to adjust smoothing accordingly as well. further
study of such models is needed to understand their potential better.

4.4 full bayesian query likelihood

in all the work we have discussed so far, we estimate   d using a point
estimator, which means we obtain our best guess of   d. intuitively,
there are uncertainties associated with our estimate, and our estimate
may not be accurate. a potentially better method is thus to consider
this uncertainty and use the posterior distribution of   d (i.e., p(  d|d))
to compute the query likelihood. such a full bayesian treatment was
proposed and studied in zaragoza and others [119].

their new scoring function is

(cid:8)

p(q|d) =

p(q|  d)p(  d|d)d  d.

the regular query likelihood scoring formula can be seen as a special
case of this more general query likelihood when we assume that p(  d|d)
is entirely concentrated at one single point.

although the integral looks intimidating, it actually has a closed
form solution when we use a conjugate prior for computing the pos-
terior distribution p(  d|d), making it relatively e   cient to compute

4.5 translation model

163

this likelihood. indeed, the scoring formula is not much more expensive
than a scoring formula using simple smoothing [119].

unfortunately, empirical evaluation shows that this new model,
while theoretically very interesting, does not outperform the simple
query likelihood function signi   cantly. however, when this new model
is combined with linear interpolation smoothing, the performance is
better than any other combinations of existing smoothing methods.
this may suggest that the new model cannot model the query noise
very well, thus it can be substantially improved when it is combined
with the linear interpolation smoothing to obtain the extra smoothing
needed for modeling query noise. as the authors pointed out, it would
be interesting to further study how to model the query noise using a
full bayesian model.

4.5 translation model

the work mentioned so far is all essentially based on the same query
likelihood scoring function which performs the retrieval task through
exact keyword matching in a way similar to a traditional retrieval
model. in order to allow inexact matching of semantically related words
and address the issues of synonym and polysemy, berger and la   erty
proposed a very important extension to the basic exact matching query
likelihood function by allowing the query likelihood to be computed
based on a translation model of the form p(u|v), which gives the prob-
ability that word v can be    semantically translated    to word u [4].

formally, in this new model, the query likelihood is computed in

the following way:

p(q|d) =

m(cid:1)

(cid:5)

w   v

i=1

p(qi|w)p(w|d),

where p(qi|w) is the id203 of    translating    word w into qi. this
translation model can be understood by imagining a user who likes
document d would formulate a query in two steps. in the    rst, the user
would sample a word from document d; in the second, the user would
   translate    the word into possibly another di   erent but semantically
related word.

164 improving the basic id38 approach

it is easy to see that if p(qi|w) only allows a word to be trans-
lated into itself, we would recover the simple exact matching query
likelihood. in general, of course, p(qi|w) would allow us to translate
w to other semantically related words by giving those other words a
nonzero id203. this enables us to score a document by counting
the matches between a query word and a di   erent but semantically
related word in the document.
a major challenge here is how to obtain the translation model
p(qi|w). the best training data for estimating this translation model
would be many relevance judgments for all the documents. unfortu-
nately we generally do not have such training data available. as an
approximation, berger and la   erty used a heuristic method to gener-
ate some synthetic query-document pairs for training the translation
model. using this method, they have shown that the translation model
can improve retrieval performance signi   cantly over the baseline exact
matching query likelihood [4].

an alternative way of estimating the translation model based on
document titles was proposed in [42], which has also been shown to be
e   ective. furthermore, id138 and co-occurrences of words have been
exploited to de   ne the translation model p(qi|w) in [14], and improve-
ment of performance is observed.

another challenge in using such a model in a practical system is
how to improve the scoring e   ciency as we now have to consider many
other words for possible matchings with each query word. indeed, eval-
uation of this method in trec-8 has revealed that there are signi   cant
challenges in handling e   ciently the large number of parameters and
scoring all the documents [5].

despite these challenges, the translation model provides a principled
way of achieving    semantic smoothing    and enables semantic matching
of related words. it thus makes an important contribution in extending
the basic query likelihood retrieval model. such a model has later been
used successfully in applying language models to cross-lingual informa-
tion retrieval [118].

the cluster-based query likelihood method proposed in [48] can also
be regarded as a form of a translation model where the whole document
is    translated    into a query as a single unit through a set of clusters,

4.6 summary

165

giving the following query likelihood formula:

p(q|d) =

p(q|gi)p(gi|d),

(cid:5)

gi   g

where gi is a cluster of documents and g is a pre-constructed set of
clusters of documents in the collection. this method has shown some
improvement over the simple query likelihood method when combined
with the simple query likelihood method, but does not perform well
alone. since the translation of a document into a cluster gi causes loss
of information, matching based on the clusters alone may not be dis-
criminative enough to distinguish relevant documents from nonrelevant
ones, even though such a matching can potentially increase recall due
to the allowed inexact matching of terms. this may explain why such
methods alone often do not perform well, but they would perform much
better when they are combined with a basic model that can supply the
needed word-level discrimination. similar observations have also been
made in [38] where probabilistic id45 (plsi) was
used to learn a lower dimension representation of text in terms of prob-
abilistic topics. plsi will be discussed further in section 6.8.

4.6 summary

in this section, we reviewed a number of models that all attempted to
extend the basic query likelihood retrieval method in various ways.
they are often substantially more expensive to compute than the
basic model. many of the extensions have not really led to signi   -
cant improvement over the basic model. given their complexity and
the relative insigni   cant improvement (compared with models to be
reviewed in the next section), most of these models have not found
widespread applications. however, some document-speci   c smoothing
methods have been shown to improve performance signi   cantly, and
the computation can often be done o   ine at the indexing stage. so it
is still feasible to use these methods in a large-scale application system.
also, the translation model has later been applied to cross-lingual ir
tasks successfully.

5

query models and feedback in language models

in the query likelihood retrieval model, we are mostly concerned with
estimating a document language model   d. one limitation of this scor-
ing method is that it is unnatural and hard to support feedback mainly
because the query is assumed to be a sample of a language model, thus
it is unclear how we should interpret any expanded/modi   ed query.
to address this problem, a new scoring strategy based on computing
the kullback   leibler (kl) divergence of a document language model
and a query language model has been introduced. the kl-divergence
retrieval model can naturally support feedback by casting it as a prob-
lem of estimating a query language model (or relevance model) based
on feedback information. in this section, we review this development.

5.1 di   culty in supporting feedback with query likelihood

feedback is an important technique to improve retrieval accuracy. both
relevance feedback and pseudo feedback have been well-supported in
traditional models (e.g., rocchio [87] for the vector space model and
term re-weighting for the classical probabilistic model [83]). naturally,
in the early days when the query likelihood scoring method was intro-
duced, people also explored feedback [70, 75, 79].

166

5.2 kullback   leibler divergence retrieval model

167

however, unlike in the traditional models where feedback can be
naturally accommodated, in the query likelihood retrieval method, it is
rather awkward to support feedback. the problem is caused by the fact
that in all the query likelihood retrieval methods, the query is regarded
as a sample of some kind of language model. thus it would not make
much sense to talk about improving the query by adding additional
terms and/or adjusting weights of those terms as done in the vector
space model [87], nor is it natural to use the feedback documents from
a user or from pseudo feedback to improve our estimate of models (i.e.,
improving   d) as done in the classical probabilistic model [83].

due to this di   culty, early work on achieving feedback using the
query likelihood scoring method tends to be quite heuristic, and the
techniques used are often not as elegant as the query likelihood method
itself. for example, in [79], terms with high probabilities in the feedback
documents but low probabilities in the collection are selected using a
ratio approach as additional query terms. while this method generally
performs well (similarly to rocchio [87]), the ratio approach is concep-
tually restricted to the view of query as a set of terms, so it cannot
be applied to the more general case when the query is considered as
a sequence of terms in order to incorporate the frequency information
of a query term. also, the in   uence of feedback cannot be controlled
through term weighting; a term is either added to the query or not.
similar strategies for heuristic expansion of queries were also studied
in miller and others [70] and [75]. but in all these approaches, it is
no longer conceptually clear how to interpret the expanded query in a
probabilistic way.

several studies [31, 32, 75, 125] have used feedback documents to
optimize the smoothing parameter or query term re-weighting. while
these methods do not cause conceptual inconsistency, they also do
not achieve full bene   t of feedback due to the limited use of feedback
information.

5.2 kullback   leibler divergence retrieval model

the di   culty in supporting feedback with query likelihood scoring has
motivated the development of a more general family of probabilistic

168 query models and feedback in language models

similarity models called kullback   leibler (kl) divergence retrieval
model [50, 123]. in this model, we de   ne two di   erent language mod-
els, one for a query (  q) and one for a document (  d). that is, we
will assume that the query is a sample observed from a query language
model   q (which presumably represents a user   s information need),
while the document is a sample from a document language model   d
(which represents the topic/content of a document). we can then use
the kl-divergence of these two models to measure how close they are
to each other and use their distance (indeed, negative distance) as a
score to rank documents. this way, the closer the document model is
to the query model, the higher the document would be ranked.

intuitively, such a scoring strategy is very similar to the vector-space
model except that we now have probabilistic representation of text and
work with a probabilistic distance/similarity function. formally, the
score of a document d with respect to a query q is given by:

s(d, q) =    d(  q||  d)

(cid:5)
=    
(cid:5)

=

w   v

p(w|  q)log

p(w|  q)
p(w|  d)
w   v
p(w|  q)log p(w|  d)    

(5.1)

(5.2)

p(w|  q)log p(w|  q)

(5.3)

(cid:5)

w   v

(cid:2)
since the last term is query id178 and does not a   ect ranking of
documents, ranking based on negative kl-divergence is the same as
w   v p(w|  q)log p(w|  d).1
ranking based on negative cross id178
with this model, the retrieval task is reduced to two subtasks    
estimating   q and   d, respectively. the estimation of document model
  d is similar to that in the query likelihood retrieval model, but the esti-
mation of query model   q o   ers interesting opportunities of leveraging
feedback information to improve retrieval accuracy. speci   cally,

1 note that although the two ways to rank documents are equivalent in the ad hoc retrieval
setting, where we always compare documents for the same query and can thus ignore
any document-independent constant, the kl-divergence and cross id178 would generate
quite di   erent results when we compare scores across di   erent queries as in the case of
   ltering or topic detection and tracking [46]. speci   cally, one measure may generate scores
more comparable across queries than the other, depending on whether including the query
id178 makes sense. for a detailed analysis of this di   erence and alternative ways of
normalizing scores, see [46].

5.3 estimation of query models

169

feedback information can be exploited to improve our estimate of   q.
such a feedback method is called model-based feedback in [123].

on the surface, the kl-divergence model appears to be quite dif-
ferent from the query likelihood method. however, it turns out that it
is easy to show that the kl-divergence model covers the query likeli-
hood method as a special case when we use the empirical query word
distribution to estimate   q [50], i.e.,
p(w|  q) =

c(w, q)

.

|q|

in this sense, the kl-divergence model is a generalization of the query
likelihood scoring method with the additional advantage of supporting
feedback more naturally.

this kl-divergence retrieval model was    rst proposed in [50] within
a risk minimization retrieval framework, which introduces the con-
cept of query language model (in additional to the document language
model) and models the retrieval problem as a statistical decision prob-
lem [50, 121, 127]. however, kl-divergence had previously been used
for distributed information retrieval [117].

by truncating the query model   q to keep only high id203
words and renormalizing it, we can score a kl-divergence model e   -
ciently. indeed, we may rewrite the scoring function in the same way as
in the case of query likelihood to obtain a scoring formula essentially
involving a sum over the terms with nonzero probabilities for both   q
and   d [76]:

score(d, q) rank=

+ log   d.

(5.4)

(cid:5)

w   d

p(w|  q)log

ps(w|  d)
  dp(w|c)

thus the generalization of query likelihood as kl-divergence does
not really incur much extra computational overhead; it is generally
regarded as a state-of-the-art retrieval model based on language
modeling.

5.3 estimation of query models

with the kl-divergence retrieval model, feedback can be achieved thro-
ugh re-estimating the query model   q based on feedback information.

170 query models and feedback in language models

several methods have been proposed to perform such model-based feed-
back in the pseudo feedback setting. interestingly, the relevance feed-
back setting appears to have not attracted that much attention, though
these methods can presumably also be applied to relevance feedback.
however, sometimes these methods may need to be adapted appropri-
ately to handle relevance feedback; in particular, feedback based on
only negative information (i.e., nonrelevant information) remains chal-
lenging with the kl-divergence retrieval model and additional heuris-
tics may need to be used [112]. this is in contrast to the document-
generation probabilistic models such as the robertson   sparck jones
model [83] which can naturally use negative examples to improve the
estimate of the nonrelevant document model.

5.3.1 model-based feedback

zhai and la   erty [123] proposed two methods for estimating an
improved query model   q using feedback documents. both methods
follow the basic idea of interpolating an existing query model (e.g.,
one estimated based on the empirical query word distribution) with an
estimated feedback topic model.

speci   cally, let   q be the current query model and   f be a feedback
topic model estimated based on (positive) feedback documents f =
{d1, . . . , dn}. the updated new query model   (cid:3)

q is given by

p(w|  (cid:3)

q) = (1       )p(w|  q) +   p(w|  f ),

where        [0,1] is a parameter to control the amount of feedback. the
two methods di   er in the way of estimating   f with f .

one approach uses a two-component mixture model to    t the
feedback documents where one component is a    xed background lan-
guage model p(w|c) estimated using the collection and the other is an
unknown, to-be-discovered topic model p(w|  f ). essentially, the words
in f are assumed to fall into two kinds: (1) background words (to
be explained by p(w|c)) and (2) topical words (to be explained by
p(w|  f )). by    tting such a mixture model to the data, we can    factor
out    the background words and obtain a discriminative topic model
which would assign high probabilities to words that are common in the

5.3 estimation of query models

171

feedback documents but not common in the collection (thus not well
explained by p(w|c)). the log-likelihood function is

log p(f|  f ) =

c(w, f )log((1       )p(w|  f ) +   p(w|c)),

(cid:5)

w   v

where        [0,1] is a parameter indicating how much weight is put on
the background model, which can be interpreted as the amount of back-
ground words we would like to factor out. the topic model   f can be
obtained by using the ml estimator, which can be computed using the
id83 [123].

the other approach proposed in [123] uses an idea similar to roc-
chio in the vector space model [87] and assumes that   f is a language
model that is very close to the language model of every document in
the feedback document set f , but far away from the collection language
model which can be regarded as an approximation of nonrelevant lan-
guage model. the distance between language models is measured using
kl-divergence. the problem of computing   f boils down to solving the
following optimization problem:

    f = arg min

  

1
n

d(  ||  i)       d(  ||  c),

where   i is the language model estimated using document di     f ,   c
is the background collection language model p(w|c), and        [0,1) is
a parameter to control the distance between the estimated   f and the
background model   c. this optimization problem has an analytical
solution:

(cid:9)

p(w|    f )     exp

1
1       

1
n

log p(w|  i)     1

(cid:10)
1        log p(w|c)

.

both the mixture model method and the divergence minimization
method are shown to be quite e   ective for pseudo feedback with per-
formance comparable to or better than rocchio [123]. however, both
methods (especially divergence minimization) are also shown to be sen-
sitive to parameter settings.

there has been some follow-up work on improving the robustness of
the mixture model feedback method [107, 108]. in tao and zhai [107],

n(cid:5)

i=1

n(cid:5)

i=1

172 query models and feedback in language models

the mixture model was extended to better integrate the original query
model with the feedback documents and to allow each feedback doc-
ument to potentially contribute di   erently to the estimated feedback
topic language model. the extended model is shown to be relatively
more robust than the original model, but the model is still quite sensi-
tive to the number of documents used for feedback [107]. moreover, due
to the use of several priors, this new model has more prior parameters
that need to be set manually with little guidance.

in tao and zhai

[108], these prior parameters were eliminated
through a regularized em algorithm, and a more robust pseudo feed-
back model is established. indeed, it has been shown that with no
parameter tuning, the model delivers comparable performance to a
well-tuned baseline pseudo feedback model.

the main ideas introduced in this new model and estimation
method are the following: (1) each feedback document is allowed to
have a potentially di   erent amount of noisy words, and the amount
of noise is estimated with no need of manual tuning. this makes it
more robust with respect to the number of documents used for pseudo
feedback. (2) the interpolation of the original query model with the
feedback model is implemented by treating the original query model as
a prior in a bayesian estimation framework. this makes the interpola-
tion more meaningful and o   ers the opportunity to dynamically change
the interpolation weights during the estimation process. (3) the param-
eter estimation process (em algorithm) is carefully regularized so that
we would start with the original query model and gradually enrich it
with additional words picked up from the feedback documents. such
id173 ensures that the estimated model stays close to the orig-
inal query. (4) this gradual enrichment process stops when    su   cient   
new words have been picked up by the em algorithm, where    su   -
cient    roughly corresponds to reaching a balance between the original
query model and the new topic model picked up from the feedback
documents (i.e., interpolation with a 0.5 weight).

a di   erent approach to improving robustness of pseudo feedback is
presented in collins-thompson and callan [19], where the idea is to
perform sampling over both the feedback documents and the query
to generate alternative sets of feedback documents and alternative

5.3 estimation of query models

173

query variants. feedback models obtained from each alternative set
can then be combined to improve the robustness of the estimated feed-
back model. experiments using a variant of the relevance model [55] as
the baseline feedback method show that the proposed sampling method
can improve the robustness of feedback even though not necessarily the
accuracy of feedback.

5.3.2 markov chain query model estimation

another approach to estimating a query model is to iteratively mine
the entire corpus by following a markov chain formed by documents
and terms [50]. the basic idea of this approach is to exploit term co-
occurrences to learn a translation model t(u|v) which can be expected
to capture the semantic relations between words in the sense that t(u|v)
would give a high id203 to word u if it is semantically related to
word v. speci   cally, we can imagine a surfer iteratively following a
markov chain of the form w0     d0     w1     d1 . . ., where wi is a word
and di a document, and the transition id203 from a document
to a word is given by the document language model p(w|d), while the
transition id203 from a word to a document is assumed to be the
posterior id203 p(d|w)     p(w|d)p(d). when visiting a word, the
surfer is further assumed to stop at the word with id203 1       
which is a parameter to be empirically set. the translation probabil-
ity t(u|v) can then be de   ned as the id203 of stopping at u if
the surfer starts with v. clearly, the same markov chain can also be
exploited to compute other translation probabilities such as t(d1|d2) or
t(d|u) without much modi   cation.

once we have such a translation model, we can assume that a user
has an information need characterized by a query model   q, and the
user has formulated the current query q through sampling a word
from   q and then    translating    it to a query word in q according to
the translation model. given the observed q, we can then compute
the posterior id203 of a word being selected from   q and use this
id203 to estimate   q:

p(w|  q)     m(cid:5)

i=1

t(qi|w)p(w|u),

174 query models and feedback in language models
where p(w|u) is our prior id203 that a word w would have been
chosen by user u; it can be set to the collection language model p(w|c)
with no additional knowledge.

intuitively, this model exploits global co-occurrences of words to
expand a query and obtain an enriched query language model. how-
ever, while such a global expansion has been shown to be e   ective, the
expansion is much more e   ective if the markov chain is restricted to
going through the top-ranked documents for a query [50]. thus the
method can also be regarded as a way to perform pseudo feedback
with language models. the observation that local co-occurrence analy-
sis is more e   ective than global co-occurrence analysis is also consistent
with a study of a traditional retrieval model [116]. intuitively, this is
because the local documents (i.e., documents close to the query) can
prevent noisy words from being picked from feedback due to distracting
co-occurrences.

in collins-thompson and callan [18], such a markov chain expan-
sion method has been extended to include multiple types of term asso-
ciations, such as co-occurrences in an external corpus, co-occurrences in
top-ranked search results, term associations obtained from an external
resource (e.g., id138). while the expansion accuracy is not better
than a strong baseline expansion method, such a massive expansion
strategy is shown to be more robust.

5.3.3 relevance model

the work reviewed so far on using language models for ir is rooted at
the query likelihood retrieval method. in 2001, another very interesting
language model, called relevance model was developed by lavrenko and
croft [55]. the motivation for this model comes from the di   culty in
estimating model parameters in the classical probabilistic model when
we do not have relevance judgments.

the classical probabilistic model can be obtained by using the
same derivation as discussed in section 3.1 and taking the document-
generation decomposition of the joint id203 p(q, d|r):

o(r|q, d)     p(d|q, r = 1)
p(d|q, r = 0)

.

(5.5)

5.3 estimation of query models

175

we see that our main tasks are to estimate two document models,
one for relevant documents (i.e., p(d|q, r = 1)) and one for nonrele-
vant documents (i.e., p(d|q, r = 0)). if we assume a multiple bernoulli
model for p(d|q, r), we will obtain precisely the binary indepen-
dence model proposed by robertson and sparck jones [83] and further
studied by others (e.g., [20, 110]). the model parameters can be esti-
mated by using some examples of relevant and nonrelevant documents,
making this an attractive model for relevance feedback.

however, when we do not have relevance judgments, it would be
di   cult to estimate the parameters. croft and harper [20] studied this
problem and introduced two approximations: (1) the nonrelevant doc-
ument model p(d|q, r = 0) can be estimated by assuming all the doc-
uments in the collection to be nonrelevant. (2) the relevant document
model p(d|q, r = 1) is assumed to give a constant id203 to all the
query words. using these assumptions, they showed that this classical
probabilistic model would lead to a scoring formula with idf weighting
for matched terms. this is indeed a very interesting derivation and pro-
vides some probabilistic justi   cation of idf. however, while the    rst
assumption is reasonable, the second is clearly an over-simpli   cation.
a more reasonable approximation may be to use some top-ranked docu-
ments as an approximation of relevant documents, i.e., follow the idea
of pseudo relevance feedback. this is essentially the idea behind the
relevance model work [55].

in the relevance model, a multinomial model is used to model a
document, thus we can capture the term frequency naturally. (previ-
ously, 2-poisson mixture models had been proposed as a member of the
classical probabilistic models to model term frequency, and an approx-
imation of that model has led to the e   ective bm25 retrieval function
[84].) using multinomial distribution, we have

(cid:11)n
(cid:11)n
i=1 p(di|q, r = 1)
i=1 p(di|q, r = 0)

,

(5.6)

o(r|q, d)    
where document d = d1       dn.

since p(di|q, r = 0) can be reasonably approximated by p(di|c)
(i.e., collection language model), the main challenge is to estimate
p(di|q, r = 1), which captures word occurrences in relevant documents

176 query models and feedback in language models

and is called a relevance model. in [55], the authors proposed two
methods for estimating such a relevance model, both based on the idea
of using the top-ranked documents to approximate relevant documents
to estimate the relevance model p(w|q, r = 1).
in model 1, they essentially use the query likelihood p(q|d) as a
weight for document d and take a weighted average of the id203
of word w given by each document language model. clearly only the
top ranked documents matter because other documents would have a
very small or zero weight. formally, the formula is as follows:

p(w|q, r = 1)     p(w, q|r = 1)

(cid:5)
(cid:5)

  d     

  d     

   

=

p(  d)p(w|  d)p(q|  d)

p(  d)p(w|  d)

p(qi|  d),

m(cid:1)

i=1

(5.7)
(5.8)

(5.9)

where    represents the set of smoothed document models in the col-
lection. p(  d) can be set to uniform.

in model 2, they compute the association between each word and
the query using documents containing both query terms and the word
as    bridges.    the strongly associated words are then assigned high
probabilities in the relevance model. formally,
p(w|q, r = 1)     p(q|w, r = 1)p(w)

(5.10)

= p(w)

p(qi|  d)p(  d|w),

(5.11)

m(cid:1)

(cid:5)

  d     

i=1

where p(  d|w) can be computed as2

p(  d|w)    

(cid:2)
p(w|  d)p(  d)
  d      p(w|  d)p(  d)

.

once again, we see that the document models that give query words
high probabilities dominate the computation. thus this model intu-
itively also assigns high probabilities to words that occur frequently in
documents that match the query well.
2 the formula given in [55] is p(mi|w) = p(w|mi)p(w)/p(mi), which is probably meant to
be p(mi|w) = p(w|mi)p(mi)/p(w); mi is the same as   d.

5.3 estimation of query models

177

while both models can be potentially computed over the entire
space of empirical document models, in the experiments reported in
[55], the authors restricted the computation to the top 50 documents
returned for each query. this not only improves the e   ciency, but also
improves the robustness of the estimated model as we are at a lower
risk of including some distracting document models. indeed, as shown
in [55], including more documents can be potentially harmful. this
is the same observation as in [50], all suggesting that these models
are essentially alternative ways of implementing pseudo feedback with
language models.

both versions of the relevance model are shown to be quite e   ective
[55]. the relevance model has also later been applied to other tasks
such as cross-lingual ir [54].

although relevance model was motivated by the classical proba-
bilistic model, it can clearly be regarded as a way to estimate the
query language model. conceptually, there appears to be little di   er-
ence between relevance model and query model, both are to model what
a user is interested in. that is, we may view p(w|q, r = 1) as the same
as p(w|  q) discussed in section 5.2. indeed, in some later studies [52],
it was shown that scoring with the kl-divergence function works bet-
ter than scoring with the classical probabilistic model for the relevance
model.

5.3.4 structured query models

sometimes a query may be represented in multiple ways or semi-
structured so that it has multiple    elds. for example, in the case of
multiple representations, one representation may be based on unigrams
and another may be based on word associations extracted from some
domain resources [2]. in the trec genomics track, gene queries are
examples of queries with multiple    elds: a gene query often consists of
two    elds, one containing the name of a gene (usually a phrase) and one
with a set of symbols [128]. in all these cases, using one single query lan-
guage model to represent the query appears to an over-simpli   cation
as it does not allow us to    exibly put di   erent weights on di   erent
representations of    elds.

178 query models and feedback in language models

a common solution to these problems is to de   ne the query model
as a mixture model, which was done in [2] for combining multiple
sources of knowledge about id183 and in [128] for assign-
ing di   erent weights to di   erent    elds of a gene query. speci   cally,
let q = {q1, . . . , qk} be a query with k    elds or representations. the
mixture query model is de   ned as

k(cid:5)

p(w|  q) =

  ip(w|  qi),

i=1

where p(w|  qi) is a query model corresponding to    eld or representation
qi, and   i is the corresponding weight.
in [128], a pseudo feedback algorithm is proposed to expand each
p(w|  qi) and estimate   i simultaneously. the basic idea is to use each
   eld (qi) to de   ne a prior on   qi and    t the mixture model to a set
of feedback documents in the same way as    tting the two-component
mixture model for model-based feedback discussed in section 5.3.1.
such semi-structured query model is shown to be e   ective.

5.4 summary

in this section, we discussed how feedback (particularly pseudo feed-
back) can be performed with language models. as a generalization of
query likelihood scoring, the kl-divergence retrieval model has now
been established as the state-of-the-art approach for using language
models to rank documents. it supports all kinds of feedback through
estimating a query language model based on feedback information. we
reviewed several di   erent approaches to improving the estimation of
a query language model by using word co-occurrences in the corpus.
although some approaches are meant to work on the entire corpus, they
tend to work much better when restricting the estimation to using only
the top-ranked documents. thus it is fair to say that all these meth-
ods are essentially di   erent ways to implement the traditional pseudo
feedback heuristic with language models. among all the methods, the
two-component mixture model [108, 123] and the relevance model [55]
appear to be most e   ective and robust and also are computationally
feasible.

6

language models for special retrieval tasks

although most work on language models deals with the standard mono-
lingual ad hoc search task, there has also been a lot of research on
applying language models to many other special retrieval tasks, includ-
ing cross-lingual retrieval, distributed ir, expert    nding, personalized
search, modeling redundancy, subtopic retrieval, and topic mining,
among others. in this section, we will review this line of work.

6.1 cross-lingual information retrieval

a major challenge in cross-lingual ir is to cross the language barrier
in some way, typically involving translating either the query or the
document from one language to the other. the translation model dis-
cussed in section 4.5 can be naturally applied to solve this problem by
de   ning the translation id203 p(u|v) on terms in the two di   er-
ent languages involved in the retrieval task. for example, u may be a
word in chinese and v a word in english. in such a case, p(u|v) would
give us the id203 that u is a good translation of english word v in
chinese; intuitively it captures the semantic association between words
in di   erent languages.

179

180 language models for special retrieval tasks

xu and co-authors [118] applied this idea to cross-lingual ir, and
proposed the following cross-lingual query likelihood retrieval function:

(cid:12)
  p(qi|cs) + (1       )

(cid:13)
p(qi|w)p(w|d)

,

(cid:5)

w   vt

p(q|d) =

m(cid:1)

i=1

where cs is the collection in the source language (i.e., the language of
the query q), vt is the vocabulary set of the target language (i.e., the
language of the document d), and    is a smoothing parameter.
as in the translation model for monolingual ad hoc retrieval, a major
challenge here is to estimate the translation id203 p(qi|w). in
[118], the authors experimented with several options, including using a
bilingual word list, parallel corpora, and a combination of them. the
cross-lingual query likelihood retrieval function has been shown to be
quite e   ective, achieving over 85% performance of monolingual retrieval
baseline.

in another line of work on applying language models to clir,
lavrenko and co-authors [54] adapted the relevance model (model 1) in
two ways to perform clir, both based on the kl-divergence scoring
function. the document language model   d is estimated in a normal
way, thus it assigns probabilities to words in the target language. their
main idea is to adapt relevance model so that we can start with a query
q in the source language to estimate a query model   q that can assign
probabilities to words in the target language. this way, the query model
and the document model can be compared using the kl-divergence
scoring function since they are now in the same (target) language.

their    rst method is to leverage a parallel corpus where documents
in the source language are paired with translations of them in the tar-
get language. in this case, the document model   d in their relevance
model can be generalized to include two separate models, one for each
language. that is,   d = (  s
d is the model for the source
document and   t
d the model for the corresponding target document.
with this setup, the relevance model can be generalized in a straight-
forward way to give the following id203 of word wt in the target
language according to the query model   q:
p(  d)p(wt|  t
d)

p(wt|  q) =

d), where   s

p(qi|  s
d).

(cid:5)

m(cid:1)

d,   t

  d     

i=1

6.2 distributed information retrieval

181

d and   t

d has enabled the crossing of the language

the pairing of   s
barrier.
their second method is to leverage a bilingual dictionary to induce a
translation model p(ws|wt ) and use this translation model to convert
the document language model p(wt|d), which is in the target lan-
guage, to a document language model for the source language p(ws|d).
that is,

p(wt|  q) =

=

p(  d)p(wt|  d)

p(  d)p(wt|  d)

(6.1)

(6.2)

p(qi|w)p(w|  d).

m(cid:1)
m(cid:1)

i=1

p(qi|  d)
(cid:5)

w   v t

i=1

(cid:5)
(cid:5)

  d     

  d     

this time, the translation model p(qi|w) has enabled the crossing of
the language barrier.

these models have been shown to achieve very good retrieval per-

formance (90%   95% of a strong monolingual baseline).

6.2 distributed information retrieval

language models have also been applied to perform distributed ir. the
task of distributed ir can often be decomposed into two subtasks: (1)
resource selection; and (2) result fusion. language models have been
applied to both tasks.

for resource selection, which is to select the most promising col-
lections to search based on a query, the general idea is to treat each
collection as a special    document    and apply standard language mod-
els to rank collections. in an early work by xu and croft [117], the
authors cluster the documents to form topical clusters. each cluster is
then treated as one coherent subcollection, which is then used to esti-
mate a topic language model. the kl-divergence between the empirical
query word distribution and the estimated topic language model is then
used to select the most promising topical collections for further query-
ing. such a id91 method is shown to be e   ective for collection
selection [117].

182 language models for special retrieval tasks

in [95], the authors proposed a id38 framework for
resource selection and result fusion. in this framework, documents in
each collection are scored using regular query likelihood retrieval func-
tion but smoothed with the background language model corresponding
to the collection. as a result, the scores of documents in di   erent collec-
tions are strictly speaking not comparable because of the use of di   erent
background language model for smoothing. a major contribution of the
work [95] is to derive an adjustment strategy that can ensure that the
scores of all the documents would be comparable after adjustment.

speci   cally, let d be a document in collection ci. in general, we
score the document for query q with query likelihood and rank docu-
ments based on p(q|d, ci). the likelihood is conditioned on ci because
of smoothing, thus directly merging results based on their query like-
lihood scores p(q|d, ci) would be problematic since the scores may
not be comparable. thus they use probabilistic rules to derive a nor-
malized form of the likelihood denoted as p(q|d), which can then be
used as scores of documents for the purpose of result fusion. they
show that ranking based on p(q|d) is equivalent to ranking based on
p(q|d,ci)
  p(ci|q)+1, where    is a parameter to be empirically set. thus when we
merge the results, we just need to divide the original score p(q|d, ci)
by the normalizer   p(ci|q) + 1, which can be computed using bayes   
rule and the likelihood of the query given collection ci (i.e., p(q|ci)).
their experiment results show that this id38 approach is
e   ective for distributed ir and outperforms a state-of-the-art method
(i.e., cori) [95].

6.3 structured document retrieval and combining

representations

most retrieval models are designed to work on a bag of words repre-
sentation of a document, but ignore any structure of a document. in
reality, a document often has both intra-document structures (e.g., title
vs. body) and inter-document structures (e.g., hyperlinks and topical
relations), which can be potentially leveraged to improve search accu-
racy. this is especially true in xml retrieval and web search. it is also
common that one may obtain multiple text representations of the same

6.3 structured document retrieval and combining representations

183

document, which should be combined to improve retrieval accuracy. in
all these problems, we can assume that a document d has k parts or
text representations d = {d1, . . . , dk}, and our goal is to rank such
documents with consideration of the known structure of the document.
in ogilvie and callan [78], the authors have extended the basic
query likelihood to address this problem. their approach allows dif-
ferent parts of a document or di   erent representations of a document
to be combined with di   erent weights. speci   cally, the    generation   
process of a query given a document is assumed to consist of two steps:
in the    rst step, a part di is selected from the structured document
d according to a selection id203 p(di|d). in the second, a query
is generated using the selected part di. thus, the query likelihood is
given by

m(cid:1)
m(cid:1)

i=1

p(q|d) =

=

p(qi|d)
k(cid:5)

p(qi|dj)p(dj|d).

(6.3)

(6.4)

i=1

j=1

in [78], such a two-step generation process was not explicitly given,
but their model implies such a generation process. the    part selection
id203    p(di|d) is denoted by   i in [78]; it can be interpreted as
the weight assigned to di and can be set based on prior knowledge or
estimated using training data. how to implement such a model e   -
ciently was discussed in length in [77]. experiment results show that
this id38 approach to combining multiple representations
is e   ective. language models have also been applied to xml retrieval
by other researchers [33].

a general probabilistic propagation framework was proposed in [92]
to combine probabilistic content-based retrieval models including lan-
guage models with link structures (e.g., hyperlinks). results show that
the propagation framework can improve ranking accuracy over pure
content-based scoring. while the framework is not speci   c to language
models, it was shown in [92] that the performance is much better if
the content-based scores can be interpreted as probabilities of rele-
vance. another general (nonprobabilistic) propagation framework was

184 language models for special retrieval tasks

proposed in [81] which has been shown to be e   ective for improving
web search through both score propagation and term count propa-
gation. how to integrate such propagation frameworks with language
models more tightly remains an interesting future research question.

6.4 personalized and context-sensitive search

in personalized search, we would like to use more user information
to better infer a user   s information need. with language models, this
means we would like to estimate a better query language model with
more user information. in [94, 105], the authors proposed several esti-
mation methods for estimating a query language model based on
implicit feedback information, including the previous queries and click-
throughs of a user. these methods are shown to be e   ective for improv-
ing search accuracy for a new related query.

in [94], implicit feedback within a single search session is consid-
ered. this is to simulate a scenario when the initial search results were
not satisfactory to the user, so the user would reformulate the query
potentially multiple times. the feedback information available consists
of the previous queries and the snippets of viewed documents (i.e.,
clickthrough information). given the user   s current query, the ques-
tion is how to use such feedback information to improve the estimate
of the query language model   q. in [94], four di   erent methods were
proposed to solve this problem, all essentially leading to some inter-
polation of many unigram language models estimated using di   erent
feedback information, respectively. di   erent methods mainly di   er in
the way to assign weights to di   erent types of information (e.g., queries
vs. snippets). experiment results show that using the history informa-
tion, especially the snippets of viewed documents, can improve search
accuracy for the current query. it is also shown to be bene   cial to use a
dynamic interpolation coe   cient similar to dirichlet prior smoothing.
in [105], implicit feedback using the entire search history of a user
is considered. since in this setup, there is potentially noisy informa-
tion in the search history, it is important to    lter out such noise when
estimating the query language model. the idea presented in [105] for
solving this problem is the following: first, each past query is treated

6.5 expert finding

185

as a unit and represented by the snippets of the top-ranked search
results. second, the search results (snippets) of the current query are
used to assign a weight to each past query based on the similarity
between the search results of the past query and those of the current
one. the weighting helps    lter out any noisy information in the history.
finally, the query language model is estimated as a weighted combina-
tion of unigram language models for each past query. the second step
is implemented by using a mixture model with each past query con-
tributing a component language model to    t the current search results.
the em algorithm is used to compute the ml estimate so that we can
obtain optimal weights for all the past queries. intuitively, the weight
on each past query indicates how well the search results of that query
can explain the current search results, i.e., similarity between that past
query and the current query. evaluation shows that such a language
modeling approach to query estimation based on search history can
improve performance substantially [105].

6.5 expert finding

the task of expert    nding as set up in the trec enterprise track is the
following: given a list of names and emails of candidate experts and text
collections where their expertise may be mentioned, retrieve experts
with expertise on a given topic (described by keywords). language
models have been applied to this task with reasonable success [3, 25].
in [25], a general probabilistic model is presented for solving this
problem with an analogous derivation to the one given in [51]. speci   -
cally, three random variables are introduced: (1) t for topic; (2) c for
a candidate expert; (3) r     {0,1} for relevance. the goal is to rank the
candidates according to the id155 p(r = 1|t, c). fol-
lowing the derivation in [51], the authors derived two families of models
corresponding to two di   erent ways of factoring the joint id203
p(t, c|r), either as p(t|r, c)p(c|r), which is called topic generation
model or p(c|t, r)p(t|r), which is called candidate generation model.
they also proposed three techniques to improve the estimation of mod-
els: (1) a mixture model for modeling the candidate mentions, which
can e   ectively assign di   erent weights to di   erent representations of an

186 language models for special retrieval tasks

expert; (2) topic expansion to enrich topic representation; (3) email-
based candidate prior to prefer candidates with many email mentions.
these techniques are shown to be empirically e   ective.

in [3], the authors proposed two di   erent topic generation models
for expert    nding. in both models (indeed, also in most other studies of
expert    nding with the trec enterprise track setup), the documents
where mentions of a candidate and terms describing a topic co-occur
serve as    bridges    to assess the associations between a candidate and a
topic. however, the two models di   er in the way a topic is    generated   
from a candidate. in model 1, a topic is generated by generating each
word in the topic independently, thus the generation of two words of
the topic can potentially be going through a di   erent document, and a
bridge document only needs to match the candidate and one topic word
well. in model 2, the whole topic is generated together using the same
document as a bridge, thus requiring the bridge document to match
both the candidate and the entire topic well. thus intuitively model
2 appears to more reasonable than model 1. indeed, empirical experi-
ments also show that model 2 outperforms model 1 [3]. this is a case
where some analysis of the assumptions made in designing language
models can help assess the e   ectiveness of a model for retrieval.

6.6 modeling redundancy and novelty

a basic task in many applications is to measure the redundancy
between two documents; the purpose is often to remove or reduce the
redundancy in the documents. language models can be used to natu-
rally solve this problem.

for example, in [121, 122], a simple two-component mixture model
is used to measure the redundancy (or equivalently novelty) of a docu-
ment d2 with respect to another document d1. the idea is to assume
that the redundancy of d1 with respect to d2 corresponds to how well
we can predict the content of d1 using a language model estimated
based on d2. intuitively, if d1 is very similar to d2, then we should
expect the model based on d2 to predict d1 well (i.e., give high prob-
ability to d1), whereas if they are quite di   erent, the model would not
predict d1 well.

formally, let   d2 be a language model estimated using d2, we de   ne

6.7 predicting query di   culty

187

the redundancy of d1 with respect to d2 as

(cid:5)
log p(d1|  ,   d2)

     

= arg max

  

= arg max

  

w   v

c(w, d1)log(  p(w|  d2) + (1       )p(w|c)),

(6.5)

(6.6)

where p(w|c) is a background collection language model.
essentially, this is to let the background model and   d2 to compete
for explaining d1, and           [0,1] indicates the relative    competitive-
ness    of   d2 to the background model, thus intuitively captures the
redundancy.       can be computed using the em algorithm. the novelty
can be de   ned as 1          .

a similar but slightly more sophisticated three-component mixture
model was proposed in [131] in order to capture novelty in information
   ltering.

note that the redundancy/novelty captured in this way is asymmet-
ric in the sense that if we switch the roles of d1 and d2, the redundancy
value would be di   erent. this is reasonable as in general, redundancy is
asymmetric (considering a case where one document is part of another
document). another way of measuring redundancy/novelty with lan-
guage models is to compute the cross-id178 between two document
language models to obtain asymmetric similarities [49].

6.7 predicting query di   culty

yet another use of language models is to predict query di   culty [21].
the idea is to compare the query model and the collection language
model and a query would be assumed to be di   cult if its query model
is close to the collection language model. the assumption made here
is that a discriminative query tends to be easier and the discrimina-
tiveness of a query can be measured by the kl-divergence of the query
model and the collection model.

speci   cally, a measure called    query clarity    is de   ned in [21] as

follows:

clarity(q) =

(cid:5)

w   v

p(w|  q)log

p(w|  q)
p(w|c)

,

188 language models for special retrieval tasks

where   q is usually an expanded query model using any feedback-based
query model estimation method (e.g., mixture model [123] or relevance
model [55]). positive correlation between the clarity scores and retrieval
accuracy has been observed [21].

6.8 subtopic retrieval

the subtopic retrieval task represents an interesting retrieval task
because it requires modeling the dependency of relevance of individ-
ual documents [122]. given a topic query, the task of subtopic retrieval
is to retrieve documents that can cover as many subtopics of the topic
as possible. if we are to solve the problem with a traditional retrieval
model, we likely would have a great deal of redundancy in the top
ranked documents. as a result, although most top-ranked documents
may be relevant to the query, they may all cover just one subtopic, thus
we do not end up having a high coverage of subtopics.

intuitively, we may solve this problem by attempting to remove
the redundancy in the search results, hoping that by avoiding covering
already covered subtopics, we will have a higher chance of covering new
subtopics quickly. this is precisely the idea explored in [122], where the
authors used the novelty measure discussed in section 6.6 in combina-
tion with the query likelihood relevance scoring to iteratively select
the best document that is both relevant and di   erent from the already
picked documents, a strategy often called maximal marginal relevance
(mmr) ranking [16].

in [121], topic models (plsa [38] and lda [9]) (to be discussed
in more detail in section 6.9) are applied to model the underlying
subtopics and a kl-divergence retrieval function is then applied to
rank documents based on subtopic representation. this method has
not worked as well as the mmr method reported in [122], but it may
be possible to combine such a subtopic representation with word-level
representation to improve the performance.

6.9 topic mining

the probabilistic latent semantic analysis (plsa) model was intro-
duced by hofmann in [37, 38] as a model for analyzing and extracting

6.9 topic mining

189

the latent topics in text documents. in [38], hofmann has shown that
using the latent topics discovered by plsa to represent documents
can improve retrieval performance (called probabilistic latent semantic
indexing, or plsi). later, many di   erent extensions of this model have
been proposed, mostly for the purpose of mining (extracting) latent
topics in text and revealing interesting topical patterns (e.g., temporal
topical trends).

the basic idea of plsa is to assume that each word in a document
is generated from a    nite mixture model with k multinomial component
models (i.e., k unigram language models). formally, let d be a docu-
ment and   1, . . . ,   k be k multinomial distributions over words, repre-
senting k latent topics. associated with d we have a document-speci   c
topic selection id203 distribution pd(i) (i = 1, . . . , k), which indi-
cates the id203 of selecting   i to generate a word in the document.
the log-likelihood of document d is then

(cid:5)

w   v

(cid:14) k(cid:5)

(cid:15)
pd(i)p(w|  i)

,

i=1

log p(d) =

c(w, d)log

where v is the vocabulary set, c(w, d) is the count of word w in d.
plsa can be estimated using the standard maximum likelihood estima-
tor (with the id83 [22]) to obtain
parameter values for   i and pd(i). clearly, if k = 1, plsa degenerates
to the simple unigram language model. plsa is generally used to    t
a set of documents. since the topic models   i are tied across the doc-
uments, they can capture clusters of words that co-occur with each
other, and help discover interesting latent topics in a collection of text.
there are two problems with plsa: (1) it is not really a generative
model because the topic selection id203 is de   ned in a document-
speci   c way; (2) it has many parameters, making it hard to    nd a global
maximum in parameter estimation. to address these limitations, blei
and co-authors [9] proposed id44 (lda) as an
extension of plsa. the main idea is to de   ne pd(i) in a    generative   
way by drawing the distribution pd(i) from a dirichlet distribution.
this not only gives us a generative model that can be used to sample
   future documents,    but also reduces the number of parameters sig-
ni   cantly. however, the estimation of plsa is no longer as simple as

190 language models for special retrieval tasks

using the standard em algorithm, and tends to be computationally
much more expensive [9, 71]. many extensions of lda have since been
proposed to model coordinated data, hierarchical topics, and temporal
topic patterns (see e.g., [7, 8, 10, 57, 103]).

although lda is advantageous over plsa as a generative model,
for the purpose of mining topics, it is unclear whether regularizing
the topic choices with a parametric dirichlet distribution is advanta-
geous; intuitively, this makes the estimated pd(i) less discriminative.
plsa has also been extended in several studies mostly to accommo-
date a topic hierarchy [36], incorporate context variables such as time
and location [66], and analyze sentiments [65]. in [130], a background
topic is introduced to plsa to make the extracted topic models more
focusing on the content words rather than the common words in the
collection.

6.10 summary

in this section, we reviewed a wide spectrum of work on using language
models for di   erent kinds of special retrieval tasks. since the central
topic of this review paper is ad hoc retrieval, we have intentionally
focused on applications of language models in unsupervised settings
and left out work on using language models in supervised learning set-
tings (where labeled training data is needed) because the latter, which
includes tasks such as text categorization, information    ltering, and
topic tracking and detection, is better reviewed through comparing
the generative language models with many other competing supervised
learning methods, notably discriminative models.

7

unifying di   erent language models

in addition to the study of individual retrieval models using lan-
guage modeling, there has also been some work attempting to estab-
lish a general formal framework to unify di   erent language models and
facilitate systematic explorations of language models in information
retrieval. we have seen from previous sections that with language
models we may rank documents using three di   erent strategies: (1)
query likelihood (i.e., computing p(q|d) or p(q|  d)); (2) document
likelihood ratio (i.e., computing p(d|q, r = 1)/p(d|q, r = 0)); and
(3) kl-divergence (i.e., computing d(  q||  d)). the    rst two can be
derived from the same ranking criterion p(r = 1|q, d) as shown in
[51], thus they naturally follow the id203 ranking principle [85].
an interesting question is whether we can further unify the third one
(i.e., kl-divergence scoring) with the    rst two. one major advantage of
unifying these di   erent scoring methods is that we will obtain a general
retrieval framework that can serve as a road map to explore variations
of language models systematically.

191

192 unifying di   erent language models

7.1 risk minimization

a major work in this line is the risk minimization framework [50, 121,
127]. the basic idea of this framework is to formalize the retrieval
problem generally as a decision problem with bayesian decision theory,
which provides a solid theoretical foundation for thinking about prob-
lems of action and id136 under uncertainty [6]. language models
are introduced into the framework as models for the observed data,
particularly the documents and queries.
speci   cally, we assume that we observe the user u, the query q,
the document source s, and the collection of documents c. we view
a query as being the output of some probabilistic process associated
with the user u, and similarly, we view a document as being the output
of some probabilistic process associated with an author or document
source s. a query (document) is the result of choosing a model, and
then generating the query (document) using that model.

the system is supposed to choose an optimal action to take in
response to these observations. an action corresponds to a possible
response of the system to a query. we can represent all actions by
a = {(di,   i)}, where di     c is a subset of c (results) and   i        is
some presentation strategy.
in bayesian decision theory, to each such action ai = (di,   i)     a
there is associated a loss l(ai,   , f (u), f (s)), which in general depends
upon all of the parameters of our model,        (  q,{  i}n
i=1) as well as any
relevant user factors f (u) and document source factors f (s).
in this framework, the expected risk of action ai is given by
r(di,   i|u,q,s,c) =

l(di,   i,   , f (u), f (s))p(  |u,q,s,c) d  ,

(cid:8)

  

where the posterior distribution is given by
p(  |u,q,s,c)     p(  q|q,u)

n(cid:1)

p(  i|di,s).

the bayes decision rule is then to choose the action a    with the

i=1

least expected risk:

a   

= (d   ,      

) = arg min
d,  

r(d,   |u,q,s,c).

7.2 generative relevance

193

that is, to select d    and present d    with strategy      .
the risk minimization framework provides a general way to frame a
retrieval problem using language models. with di   erent instantiations
of the query model   q and document models   i, it can accommodate
potentially many di   erent language models, while the id168 can
be instantiated in di   erent ways to re   ect di   erent retrieval/ranking
criteria. it has been shown in [121, 127] that the risk minimization
framework covers many existing retrieval models as special cases and
can serve as a roadmap for exploring new models. for example, the
id203 ranking principle (thus both the query likelihood scoring
method and the document likelihood ratio scoring method) can be
derived by making the independent loss assumption1 and de   ning the
id168 based on the relevance status of a document. the kl-
divergence scoring function can be justi   ed by de   ning the loss of
returning a document based on the kl-divergence of its language model
and the query language model.

in general, we do not have to make the independent loss assumption
and can de   ne the id168 over the entire ranked list of documents.
thus in the risk minimization framework, it is possible to go beyond
independent relevance to capture redundancy between documents (e.g.,
as done in [122]), which is hard to capture when the retrieval problem
is formulated as computing a score based on matching one query with
one document. see section 6.8 for more discussion of using language
models to solve the subtopic retrieval problem.

the optimization setup of the risk minimization is quite general and
o   ers potential for combining language models with the line of work on
learning to rank (e.g., [12, 43] and their recent extensions [13, 15]).

7.2 generative relevance

another important work is the generative relevance framework devel-
oped in lavrenko   s thesis [52]. in [52], the following generative relevance

1 the independent loss assumption says that the loss of returning a document in response
to a query is independent of returning other documents. this assumption clearly does
not hold in reality because redundant documents do not have independent loss. thus the
id203 ranking principle has the limitation of not being able to model redundancy
among the search results.

194 unifying di   erent language models

hypothesis was proposed:

generative relevance hypothesis: for a given information need, queries
expressing that need and documents relevant to that need can be
viewed as independent random samples from the same underlying gen-
erative model.

lavrenko developed three di   erent retrieval functions under this
hypothesis (i.e., query likelihood, document likelihood, and kl-
divergence) and proposed a general technique called kernel-based
allocation for estimating various kinds of language models [52]. the
generative relevance hypothesis has two important implications from
the perspective of deriving retrieval models: (1) it naturally accommo-
dates matching of queries and documents even if they are in di   erent
languages (as in the case of cross-lingual retrieval) or in di   erent media
(e.g., text queries on images). (2) it makes it possible to estimate and
improve a relevant document language model based on examples of
queries and vice versa. conceptually, the generative relevance frame-
work can be regarded as a special case of risk minimization when doc-
ument models and query models are assumed to be in the same space.

8

summary and outlook

8.1 language models vs. traditional retrieval models

it has been a long-standing challenge in ir research to develop robust
and e   ective retrieval models. as a new generation of probabilistic
retrieval models, id38 approaches have several advan-
tages over traditional retrieval models such as the vector-space model
and the classical probabilistic retrieval model:

first, these language models generally have a good statistical foun-
dation. this makes it possible to leverage many established statistical
estimation methods to set parameters in a retrieval function as demon-
strated in [108, 125]. following rigorous statistical modeling also forces
any assumptions to be made explicit. a good understanding of such
assumptions often helps diagnose the weakness and strength of a model
and thus better interpret experiment results.

second, they provide a principled way to address the critical issue
of text representation and term weighting. the issue of term weight-
ing has long been recognized as critical, but before id38
approaches were proposed, this issue had been traditionally addressed
mostly in a heuristic way. language models, multinomial unigram
language models particular, can incorporate term frequencies and

195

196 summary and outlook

document length id172 naturally into a probabilistic model.
while such connection has also been made in the classic probabilis-
tic retrieval model (e.g., [83]), the estimation of parameters was not
addressed as seriously as in the language models.

third,

language models can often be more easily adapted to
model various kinds of complex and special retrieval problems than
traditional models as discussed in section 6. the bene   t has largely
come from the availability of many well-understood statistical models
such as    nite mixture models, which can often be estimated easily by
using the em algorithm.

however, the id38 approaches also have some de   -

ciencies as compared with traditional models:

first, there is a lack of explicit discrimination in most of the lan-
guage models developed so far. for example, in the query likelihood
retrieval function, the idf e   ect is achieved through smoothing the
document language model with a background model. while this can
be explained by modeling the noise in the query, it seems to be a
rather unnatural way to penalize matching common words, at least as
compared with the traditional tf-idf weighting. such a lack of dis-
crimination is indeed a general problem with all generative models as
they are designed to describe what the data looks like rather than how
the data di   ers.

second, the language models have been found to be less robust
than the traditional tf-idf model in some cases and can perform
poorly or be very sensitive to parameter setting. for example, the feed-
back methods proposed in [123] are shown to be sensitive to parameter
setting, whereas a traditional method such as rocchio appears to be
more robust. this may be the reason why language models have not
yet been able to outperform well-tuned full-   edged traditional methods
consistently and convincingly in trec evaluation. in particular, bm25
term weighting coupled with rocchio feedback remains a strong base-
line which is at least as competitive as any id38 approach
for many tasks.

third, some sophisticated language models can be computationally
expensive (e.g., the translation model), which may limit their uses in
large-scale retrieval applications.

8.2 summary of research progress

197

it should be noted that these relative advantages and disadvantages
are based on a quite vague distinction between language models and
classical probabilistic retrieval models. indeed, the boundary is not very
clear. conceptually, any probabilistic model of text can be called a
language model. in this sense, the classical probabilistic retrieval model
such as the robertson   sparck jones model is certainly also a language
model (i.e., multiple bernoulli). however, for historical reasons, the
term language model tends to be used to refer to either the use of a
multinomial model (or a higher order id165 model such as bigram
or trigram language model) or the query likelihood retrieval function
and its generalization kl-divergence retrieval function. thus readers
should be careful about the vague distinction of the so-called language
models from other (traditional) probabilistic models.

8.2 summary of research progress

since the pioneering work by ponte and croft [80], a lot of progress has
been made in studying the id38 approaches to ir, which
we brie   y reviewed in this survey. the following is an incomplete list
of some of the most important developments:

    framework and justi   cation for using lms for ir have been
established: the query likelihood retrieval method has been
shown to be a well-justi   ed model according to the proba-
bility ranking principle [51]. general frameworks such as the
risk minimization framework [50, 121, 127] and the generative
relevance framework [52] o   er road maps for systematically
applying language models to retrieval problems.
    many e   ective models have been developed and they often
work well for multiple tasks:

    the kl-divergence retrieval model

[50, 52, 123],
which covers the query likelihood retrieval model,
has been found to be a solid and empirically e   ec-
tive retrieval model that can easily incorporate many
advanced language models; many methods have been

198 summary and outlook

developed to improve estimation of query language
models.

    dirichlet prior smoothing has been recognized as
an e   ective smoothing method for retrieval
[124].
the kl-divergence retrieval model combined with
dirichlet prior smoothing represents the state-of-
the-art baseline method for the id38
approaches to ir.

    the translation model proposed in [4] enables han-
dling polysemy and synonyms in a principled way
with a great potential for supporting semantic infor-
mation retrieval.

    the relevance model [52, 55] o   ers an elegant solution
to the estimation problem in the classical proba-
bilistic retrieval model as well as serves as an e   ec-
tive feedback method for the kl-divergence retrieval
model.

    mixture unigram language models have been shown
to be very powerful and can be useful for many
purposes such as pseudo feedback [123], improving
model discriminativeness [35], and modeling redun-
dancy [122, 131].

    it has been shown that completely automatic tuning of
parameters is possible for both nonfeedback retrieval [125]
and pseudo feedback [108].
    lms can be applied to virtually any retrieval task with great
potential for modeling complex ir problems (as surveyed in
section 6).

for practitioners who want to apply language models in spe-
ci   c applications, the kl-divergence retrieval function combined with
dirichlet prior smoothing for estimating document language models
and either relevance model or mixture model for estimating query lan-
guage models can be highly recommended. these methods have all been
implemented in the lemur toolkit (http://www.lemurproject.org/).

8.3 future directions

199

8.3 future directions

despite much progress has been made in applying language models to
ir, there are still many challenges to be solved to fully develop the
potential of such models. the following is a list of some interesting
opportunities for future research:

challenge 1 : develop an e   cient, robust and e   ective language model
for ad hoc retrieval that can (1) optimize retrieval parameters auto-
matically, (2) perform as well as or better than well-tuned traditional
retrieval methods with pseudo feedback (e.g., bm25 with rocchio), and
(3) be computed as e   ciently as traditional retrieval methods. would
some kind of language model eventually replace the currently popular
bm25 and rocchio? how to implement idf more explicitly in a lan-
guage modeling approach may be an important issue to further study.
relaxing the assumption that the same words occur independently in
a document (e.g., by using the dirichlet compound model) may also
be necessary to capture tf id172 more accurately.

challenge 2 : demonstrate consistent and substantial improvement by
going beyond unigram language models. while there has been some
e   ort in this direction, the empirical performance improvement of the
more sophisticated models over the simple models tends to be insignif-
icant. this is consistent with what has been observed in traditional
retrieval models. would we ever be able to achieve signi   cant improve-
ment over the unigram language models by using higher-order id165
models or capturing limited syntactic/semantic dependencies among
words? as we go beyond unigram language models, reliable estimation
of the model becomes more challenging due to the problem of data
sparseness. thus developing better estimation techniques (e.g., those
that can lead to optimal weighting of phrases conditioned on weight-
ing of single words) may be critical for making more progress in this
direction.

challenge 3 : develop language models to support personalized
search. using more user information and a user   s search context to
better infer a user   s information need is essential for optimizing search
accuracy. this is especially important when the search results are not

200 summary and outlook

satisfactory and the user would reformulate the query many times. how
can we use language models to accurately represent a user   s interest and
further incorporate such knowledge into a retrieval model? detailed
analysis of user actions (e.g., skipping some results and viewing others,
deleting query terms but adding them back later, recurring interests
vs. adhoc information needs) may be necessary to obtain an accurate
representation of a user   s information need.
challenge 4 : develop language models that can support    life-time
learning.    one important advantage of language models is the potential
bene   t from improved estimation of the models based on additional
training data. as a search engine is being used, we will be able to
collect a lot of implicit feedback information such as clickthroughs.
how can we develop language models that can learn from all such
feedback information from all the users to optimize retrieval results
for future queries? from the viewpoint of personalized search, how
can we leverage many users of a system to improve performance for
a particular user (i.e., supporting collaborative search)? translation
models appear to be especially promising in this direction, and they
are complementary with the recently developed discriminative models
for learning to rank documents such as ranknet [12] and ranking id166
[43]. it should be extremely interesting to study how to combine these
two complementary approaches.
challenge 5 : develop language models that can model document
structures and subtopics. most existing work on studying retrieval mod-
els, including work on language models, has assumed a simple bag-of-
words representation of text. while such a representation ensures that
the model would work for any text, in a speci   c application, docu-
ments often have certain structures that can be potentially exploited
to improve search accuracy. for example, often it is some part of a
long document that is relevant. how can we model potentially di   erent
subtopics in a single document and match only the relevant part of a
document with a query? mixture models and id48
may be promising in this direction.
challenge 6 : generalize language models to support ranking of both
unstructured and structured data. traditionally, structured data and

8.3 future directions

201

unstructured data (text) have been managed in di   erent ways with
structured data mainly handled through a relational database while
unstructured data through an information retrieval system, leading to
two di   erent research communities (i.e., the database community and
the information retrieval community). recently, however, the bound-
ary between the two communities seems to become vague. first, as
exploratory search on databases becomes more and more popular
on the web, db researchers are now paying much attention to the
problem of ranking structured data in a database. the information
needs to be satis   ed are very similar to those in a retrieval system.
second, some database    elds may contain long text (e.g., abstracts of
research surveys), while most text documents also have some struc-
tured meta-data (e.g., authors, dates). thus a very interesting ques-
tion is whether we can generalize language models to develop uni   ed
probabilistic models for searching/ranking both structured data and
unstructured data. the inex initiative (http://inex.is.informatik.uni-
duisburg.de/) has stimulated a lot of research in developing xml
retrieval models (i.e., semi-structured data retrieval models), but we
are still far from a uni   ed model for unstructured, semi-structured,
and structured data.

challenge 7 : develop language models for hypertext retrieval. as an
abstract representation, the web can be regarded a hypertext col-
lection. language models developed so far have not explicitly incor-
porated hyperlinks and the associated anchor text into the model.
how can we use id38 to develop a hypertext retrieval
model for web search? how should we de   ne a generative model for
hypertext?

challenge 8 : develop/extend language models for retrieval with
complex information needs. language models are natural for mod-
eling topical relevance. but in many retrieval applications, a user   s
information need consists of multiple dimensions of preferences with
topical relevance being only one of them. other factors such as read-
ability, genre, and sentiment may also be important. how can we
use language models to capture such nontopical aspects? how can we
develop or extend language models to optimize ranking of documents

202 summary and outlook

based on multiple factors? in this direction, recent work has shown
that the learning-to-rank approaches are quite promising, thus again
it would be very interesting to study how to combine the language
modeling approaches (generative approaches) with the learning-to-rank
approaches (discriminative approaches).

acknowledgments

i want to thank jamie callan, editor of foundations and trends in ir,
for the opportunity to write this review, for his technical feedback about
the review, and for his periodical reminders which ensured progress in
my writing. i also want to thank him for encouraging me to give a
tutorial on this topic at several conferences. this survey is largely a
written elaboration of the tutorial. i am very grateful to john la   erty
for supervising my dissertation on this topic. the conceptual framework
and a substantial part of this survey are based on my dissertation work.
i want to thank donald metzler and two anonymous reviewers for their
many useful comments and suggestions on how to improve this survey.
(naturally, any errors that remain are solely my own responsibility.)
i also wants to thank many peer researchers, especially w. bruce croft,
stephen robertson, victor lavrenko, rong jin, tao tao, david a.
evans, and wessel kraaij, for their useful discussions on various issues
about the topic covered in this survey. i give special thanks to james
finlay and mike casey of now publishers for their help and support
in preparing the    nal version of this review.

my own research work covered in this survey is supported in part
by the advanced research and development activity in information

203

204 acknowledgments

technology (arda) under its statistical id38 for
information retrieval research program, by the national science
foundation under a career grant
(iis-0347933), by google,
microsoft, and ibm through their research programs, and by the alfred
p. sloan foundation through a research fellowship. any opinions,    nd-
ings, and conclusions, or recommendations expressed in this paper are
those of the author and do not necessarily re   ect the views of the
sponsors.

references

[1] g. amati and c. j. v. rijsbergen,    probabilistic models of information
retrieval based on measuring the divergence from randomness,    acm trans-
actions on information system, vol. 20, pp. 357   389, 2002.

[2] j. bai, j.-y. nie, g. cao, and h. bouchard,    using query contexts in infor-

mation retrieval,    in proceedings of acm sigir 2007, pp. 15   22, 2007.

[3] k. balog, l. azzopardi, and m. de rijke,    formal models for expert    nding

in enterprise corpora,    in proceedings of sigir-06, 2006.

[4] a. berger and j. la   erty,    information retrieval as statistical translation,    in
proceedings of the 1999 acm sigir conference on research and develop-
ment in information retrieval, pp. 222   229, 1999.

[5] a. l. berger and j. d. la   erty,    the weaver system for document retrieval,   

in proceedings of trec 1999, 1999.

[6] j. berger, statistical decision theory and bayesian analysis. springer-verlap,

1985.

[7] d. blei, t. gri   ths, m. jordan, and j. tenenbaum,    hierarchical topic models
and the nested chinese restaurant process,    in neural information processing
systems (nips) 16, 2003.

[8] d. blei and j. la   erty,    correlated topic models,    in proceedings of nips

   05: advances in neural information processing systems 18, 2005.

[9] d. blei, a. ng, and m. jordan,    id44,    journal of

machine learning research, vol. 3, pp. 993   1022, 2003.

[10] d. m. blei and j. d. la   erty,    dynamic topic models,    in proceedings of the

23rd international conference on machine learning, pp. 113   120, 2006.

[11] p. f. brown, j. cocke, s. a. d. pietra, v. j. d. pietra, f. jelinek,
j. d. la   erty, r. l. mercer, and p. s. roossin,    a statistical approach to

205

206 references

machine translation,    computational linguistics, vol. 16, no. 2, pp. 79   85,
1990.

[12] c. burges, t. shaked, e. renshaw, a. lazier, m. deeds, n. hamilton, and
g. hullender,    learning to rank using id119,    in icml    05: pro-
ceedings of the 22nd international conference on machine learning, pp. 89   
96, usa, new york, ny: acm, 2005.

[13] c. j. c. burges, r. ragno, and q. v. le,    learning to rank with nonsmooth
cost functions,    in proceedings of nips 2006, (b. scholkopf, j. c. platt, and
t. ho   man, eds.), pp. 193   200, 2006.

[14] g. cao, j.-y. nie, and j. bai,    integrating word relationships into language
models,    in proceedings of the 2005 acm sigir conference on research and
development in information retrieval, pp. 298   305, 2005.

[15] z. cao, t. qin, t.-y. liu, m.-f. tsai, and h. li,    learning to rank: from
pairwise approach to listwise approach,    in proceedings of icml 2007, volume
227 of acm international conference proceeding sereies, (z. ghahramani,
ed.), pp. 129   136, 2007.

[16] j. carbonell and j. goldstein,    the use of mmr, diversity-based reranking for
reordering documents and producing summaries,    in proceedings of sigir   98,
pp. 335   336, 1998.

[17] s. f. chen and j. goodman,    an empirical study of smoothing techniques for

id38,    technical report tr-10-98, harvard university, 1998.

[18] k. collins-thompson and j. callan,    id183 using random walk

models,    in proceedings of acm cikm 2005, pp. 704   711, 2005.

[19] k. collins-thompson and j. callan,    estimation and use of uncertainty in
pseudo-relevance feedback,    in proceedings of acm sigir 2007, pp. 303   310,
2007.

[20] w. b. croft and d. harper,    using probabilistic models of document retrieval
without relevance information,    journal of documentation, vol. 35, pp. 285   
295, 1979.

[21] s. cronen-townsend, y. zhou, and w. b. croft,    predicting query perfor-
mance,    in proceedings of the 25th annual international acm sigir confer-
ence on research and development in information retrieval (sigir 2002),
pp. 299   306, august 2002.

[22] a. p. dempster, n. m. laird, and d. b. rubin,    maximum likelihood from
incomplete data via the em algorithm,    journal of royal statistical society
b, vol. 39, pp. 1   38, 1977.

[23] d. a. evans and c. zhai,    noun-phrase analysis in unrestricted text for infor-

mation retrieval,    in proceedings of acl 1996, pp. 17   24, 1996.

[24] h. fang, t. tao, and c. zhai,    a formal study of information retrieval heuris-
tics,    in proceedings of the 2004 acm sigir conference on research and
development in information retrieval, pp. 49   56, 2004.

[25] h. fang and c. zhai,    probabilistic models for expert    nding,    in proceedings

of ecir 2007, pp. 418   430, 2007.

[26] n. fuhr,    probabilistic models in information retrieval,    the computer jour-

nal, vol. 35, pp. 243   255, 1992.

references

207

[27] n. fuhr,    language models and uncertain id136 in information retrieval,   
in proceedings of the id38 and ir workshop, pp. 6   11, may
31   june 1 2001.

[28] j. gao, j.-y. nie, g. wu, and g. cao,    dependence language model for
information retrieval,    in sigir    04: proceedings of the 27th annual interna-
tional acm sigir conference on research and development in information
retrieval, pp. 170   177, usa, new york, ny: acm, 2004.

[29] d. grossman and o. frieder, information retrieval: algorithms and heuristics.

springer, 2004.

[30] d. hiemstra,    a probabilistic justi   cation for using tf x idf term weighting
in information retrieval,    international journal on digital libraries, vol. 3,
pp. 131   139, 2000.

[31] d. hiemstra,    using language models for information retrieval,    phd thesis,

university of twente, 2001.

[32] d. hiemstra,    term-speci   c smoothing for the id38 approach
to information retrieval: the importance of a query term,    in proceedings of
acm sigir 2002, pp. 35   41, 2002.

[33] d. hiemstra,    statistical language models for intelligent xml retrieval,    in

intelligent search on xml data, pp. 107   118, 2003.

[34] d. hiemstra and w. kraaij,    twenty-one at trec-7: ad-hoc and cross-
language track,    in proceedings of seventh text retrieval conference (trec-
7), pp. 227   238, 1998.

[35] d. hiemstra, s. robertson, and h. zaragoza,    parsimonious language models
for information retrieval,    in sigir    04: proceedings of the 27th annual inter-
national acm sigir conference on research and development in information
retrieval, pp. 178   185, usa, new york, ny: acm, 2004.

[36] t. hofmann,    the cluster-abstraction model: unsupervised learning of topic
hierarchies from text data,    in proceedings of ijcai    99, pp. 682   687, 1999.
[37] t. hofmann,    probabilistic latent semantic analysis,    in proceedings of uai

1999, pp. 289   296, 1999.

[38] t. hofmann,    probabilistic id45,    in proceedings of acm

sigir   99, pp. 50   57, 1999.

[39] f. jelinek, statistical methods for id103. mit press, 1997.
[40] f. jelinek and r. mercer,    interpolated estimation of markov source param-
eters from sparse data,    in pattern recognition in practice, (e. s. gelsema
and l. n. kanal, eds.), pp. 381   402, 1980.

[41] h. jin, r. schwartz, s. sista, and f. walls,    topic tracking for radio, tv broad-
cast, and newswire,    in proceedings of the darpa broadcast news workshop,
pp. 199   204, 1999.

[42] r. jin, a. g. hauptmann, and c. zhai,    title language model for information

retrieval,    in proceedings of acm sigir 2002, pp. 42   48, 2002.

[43] t. joachims,    optimizing search engines using clickthrough data,    in pro-

ceedings of the acm kdd 2002, pp. 133   142, 2002.

[44] s. m. katz,    estimation of probabilities from sparse data for the language
model component of a speech recognizer,    ieee transactions on acoustics,
speech and signal processing, vol. assp-35, pp. 400   401, 1987.

208 references

[45] r. kneser and h. ney,    improved backing-o    for m-gram id38,   
in proceedings of the ieee international conference on acoustics, speech and
signal processing, pp. 181   184, 1995.

[46] w. kraaij,    variations on id38 for information retrieval,    phd

thesis, university of twente, 2004.

[47] w. kraaij, t. westerveld, and d. hiemstra,    the importance of prior proba-
bilities for entry page search,    in proceedings of acm sigir 2002, pp. 27   34,
2002.

[48] o. kurland and l. lee,    corpus structure, language models, and ad hoc infor-
mation retrieval,    in sigir    04: proceedings of the 27th annual international
conference on research and development in information retrieval, pp. 194   
201, acm press, 2004.

[49] o. kurland and l. lee,    id95 without hyperlinks: structural re-ranking
using links induced by language models,    in sigir    05: proceedings of the 28th
annual international acm sigir conference on research and development
in information retrieval, pp. 306   313, usa, new york, ny: acm, 2005.

[50] j. la   erty and c. zhai,    document language models, query models, and risk
minimization for information retrieval,    in proceedings of sigir   01, pp. 111   
119, september 2001.

[51] j. la   erty and c. zhai,    probabilistic relevance models based on document
and query generation,    in id38 and information retrieval,
(w. b. croft and j. la   erty, eds.), pp. 1   6, kluwer academic publishers,
2003.

[52] v. lavrenko,    a generative theory of relevance,    phd thesis, university of

massachusetts, amherst, 2004.

[53] v. lavrenko, j. allan, e. deguzman, d. laflamme, v. pollard, and
s. thomas,    relevance models for topic detection and tracking,    in proceed-
ings of the second international conference on human language technology
research, pp. 115   121, san francisco, ca, usa: morgan kaufmann publish-
ers inc., 2002.

[54] v. lavrenko, m. choquette, and w. b. croft,    cross-lingual relevance mod-

els,    in proceedings of acm sigir 2002, pp. 175   182, 2002.

[55] v. lavrenko and w. b. croft,    relevance-based language models,    in pro-

ceedings of sigir   01, pp. 120   127, september 2001.

[56] d. d. lewis,    representation and learning in information retrieval,    technical

report 91-93, university of massachusetts, 1992.

[57] w. li and a. mccallum,    pachinko allocation: dag-structured mixture mod-
els of topic correlations,    in icml    06: proceedings of the 23rd international
conference on machine learning, pp. 577   584, 2006.

[58] x. li and w. b. croft,    time-based language models,    in cikm    03: proceed-
ings of the twelfth international conference on information and knowledge
management, pp. 469   475, usa, new york, ny: acm, 2003.

[59] x. liu and w. b. croft,    passage retrieval based on language models,    in
cikm    02: proceedings of the eleventh international conference on informa-
tion and knowledge management, pp. 375   382, usa, new york, ny: acm,
2002.

references

209

[60] x. liu and w. b. croft,    cluster-based retrieval using language models,   
in sigir    04: proceedings of the 27th annual international conference on
research and development in information retrieval, pp. 186   193, acm press,
2004.

[61] d. mackay and l. peto,    a hierarchical dirichlet language model,    natural

language engineering, vol. 1, pp. 289   307, 1995.

[62] r. e. madsen, d. kauchak, and c. elkan,    modeling word burstiness using
the dirichlet distribution,    in icml    05: proceedings of the 22nd international
conference on machine learning, pp. 545   552, usa, new york, ny: acm,
2005.

[63] c. manning, p. raghavan, and h. schutze, introduction to information

retrieval. cambridge university press, 2008.

[64] a. mccallum and k. nigam,    a comparison of event models for naive bayes
text classi   cation,    in aaai-1998 learning for text categorization workshop,
pp. 41   48, 1998.

[65] q. mei, h. fang, , and c. zhai,    a study of poisson query generation model for
information retrieval,    in proceedings of the 30th annual international acm
sigir conference on research and development in information retrieval,
pp. 319   326, 2007.

[66] q. mei and c. zhai,    a mixture model for contextual id111,    in pro-

ceedings of kdd    06, pp. 649   655, 2006.

[67] q. mei, d. zhang, and c. zhai,    a general optimization framework for
smoothing language models on graph structures,    in sigir    08: proceedings
of the 31st annual international acm sigir conference on research and
development in information retrieval, pp. 611   618, usa, new york, ny:
acm, 2008.

[68] d. metzler and w. b. croft,    a markov random    eld model for term depen-
dencies,    in proceedings of the 2005 acm sigir conference on research and
development in information retrieval, pp. 472   479, 2005.

[69] d. metzler, v. lavrenko, and w. b. croft,    formal multiple-bernoulli models
for id38,    in sigir    04: proceedings of the 27th annual interna-
tional acm sigir conference on research and development in information
retrieval, pp. 540   541, usa, new york, ny: acm, 2004.

[70] d. h. miller, t. leek, and r. schwartz,    a hidden markov model informa-
tion retrieval system,    in proceedings of the 1999 acm sigir conference on
research and development in information retrieval, pp. 214   221, 1999.

[71] t. minka and j. la   erty,    expectation-propagation for the generative aspect

model,    in proceedings of the uai 2002, pp. 352   359, 2002.

[72] r. nallapati and j. allan,    capturing term dependencies using a lan-
guage model based on sentence trees,    in cikm    02: proceedings of the
eleventh international conference on information and knowledge manage-
ment, pp. 383   390, usa, new york, ny: acm, 2002.

[73] r. nallapati, b. croft, and j. allan,    relevant query feedback in statistical
id38,    in cikm    03: proceedings of the twelfth international
conference on information and knowledge management, pp. 560   563, usa,
new york, ny: acm, 2003.

210 references

[74] h. ney, u. essen, and r. kneser,    on structuring probabilistic dependen-
cies in stochastic id38,    computer speech and language, vol. 8,
pp. 1   38, 1994.

[75] k. ng,    a maximum likelihood ratio information retrieval model,    in pro-
ceedings of the eighth text retrieval conference (trec-8), (e. voorhees
and d. harman, eds.), pp. 483   492, 2000.

[76] p. ogilvie and j. callan,    experiments using the lemur toolkit,    in proceed-

ings of the 2001 trec conference, 2002.

[77] p. ogilvie and j. callan,    using language models for    at text queries in xml
retrieval,    in proceedings of the initiative for the evaluation of xml retrieval
workshop (inex 2003), 2003.

[78] p. ogilvie and j. p. callan,    combining id194s for known-

item search,    in proceedings of acm sigir 2003, pp. 143   150, 2003.

[79] j. ponte,    a id38 approach to information retrieval,    phd the-

sis, university of massachusetts at amherst, 1998.

[80] j. ponte and w. b. croft,    a id38 approach to information

retrieval,    in proceedings of the acm sigir   98, pp. 275   281, 1998.

[81] t. qin, t.-y. liu, x.-d. zhang, z. chen, and w.-y. ma,    a study of relevance
propagation for web search,    in proceedings of sigir 2005, pp. 408   415, 2005.
[82] l. r. rabiner,    a tutorial on id48,    proceedings of the

ieee, vol. 77, pp. 257   285, 1989.

[83] s. robertson and k. sparck jones,    relevance weighting of search terms,   
journal of the american society for information science, vol. 27, pp. 129   
146, 1976.

[84] s. robertson and s. walker,    some simple e   ective approximations to
the 2-poisson model for probabilistic weighted retrieval,    in proceedings of
sigir   94, pp. 232   241, 1994.

[85] s. e. robertson,    the id203 ranking principle in ir,    journal of docu-

mentation, vol. 33, pp. 294   304, december 1977.

[86] s. e. robertson, s. walker, k. sparck jones, m. m. hancock-beaulieu, and
m. gatford,    okapi at trec-3,    in the third text retrieval conference
(trec-3), (d. k. harman, ed.), pp. 109   126, 1995.

[87] j. rocchio,    relevance feedback in information retrieval,    in the smart
retrieval system: experiments in automatic document processing, pp. 313   
323, prentice-hall inc., 1971.

[88] t. roelleke and j. wang,    a parallel derivation of probabilistic information
retrieval models,    in sigir    06: proceedings of the 29th annual interna-
tional acm sigir conference on research and development in information
retrieval, pp. 107   114, new york, ny, usa: acm, 2006.

[89] g. salton, automatic text processing: the transformation, analysis and

retrieval of information by computer. addison-wesley, 1989.

[90] g. salton and m. mcgill, introduction to modern information retrieval.

mcgraw-hill, 1983.

[91] g. salton, c. s. yang, and c. t. yu,    a theory of term importance in auto-
matic text analysis,    journal of the american society for information science,
vol. 26, pp. 33   44, jan   feb 1975.

references

211

[92] a. shakery and c. zhai,    a probabilistic relevance propagation model for

hypertext retrieval,    in proceedings of cikm 2006, pp. 550   558, 2006.

[93] a. shakery and c. zhai,    smoothing document language models with proba-
bilistic term count propagation,    information retrieval, vol. 11, pp. 139   164,
2008.

[94] x. shen, b. tan, and c. zhai,    context-sensitive information retrieval using

implicit feedback,    in proceedings of sigir 2005, pp. 43   50, 2005.

[95] l. si, r. jin, j. p. callan, and p. ogilvie,    a id38 framework
for resource selection and results merging,    in proceedings of cikm 2002,
pp. 391   397, 2002.

[96] a. singhal, c. buckley, and m. mitra,    pivoted document length normaliza-
tion,    in proceedings of the 1996 acm sigir conference on research and
development in information retrieval, pp. 21   29, 1996.

[97] f. song and w. b. croft,    a general

for information
retrieval,    in proceedings of the 1999 acm sigir conference on research
and development in information retrieval, pp. 279   280, 1999.

language model

[98] k. sparck jones, s. robertson, d. hiemstra, and h. zaragoza,    language
modeling and relevance,    in id38 for information retrieval,
(w. b. croft and j. la   erty, eds.), pp. 57   72, 2003.

[99] m. spitters and w. kraaij,    language models for topic tracking,    in language

modeling for information retrieval, pp. 95   124, 2003.

[100] m. srikanth and r. srihari,    biterm language models for document retrieval,   
in sigir    02: proceedings of the 25th annual international acm sigir con-
ference on research and development in information retrieval, pp. 425   426,
usa, new york, ny: acm, 2002.

[101] m. srikanth and r. srihari,    exploiting syntactic structure of queries in
a id38 approach to ir,    in cikm    03: proceedings of the
twelfth international conference on information and knowledge manage-
ment, pp. 476   483, usa, new york, ny: acm, 2003.

[102] m. srikanth and r. srihari,    incorporating query term dependencies in lan-
guage models for document retrieval,    in sigir    03: proceedings of the 26th
annual international acm sigir conference on research and development
in informaion retrieval, pp. 405   406, usa, new york, ny: acm, 2003.

[103] m. steyvers, p. smyth, m. rosen-zvi, and t. gri   ths,    probabilistic author-
topic models for information discovery,    in proceedings of kdd   04, pp. 306   
315, 2004.

[104] t. strzalkowski and b. vauthey,    information retrieval using robust natural
language processing,    in proceedings of the 30th annual meeting on asso-
ciation for computational linguistics, pp. 104   111, morristown, nj, usa:
association for computational linguistics, 1992.

[105] b. tan, x. shen, and c. zhai,    mining long-term search history to improve

search accuracy,    in kdd, pp. 718   723, 2006.

[106] t. tao, x. wang, q. mei, and c. zhai,    language model information retrieval
with document expansion,    in proceedings of the main conference on human
language technology conference of the north american chapter of the asso-
ciation of computational linguistics, pp. 407   414, morristown, nj, usa:
association for computational linguistics, 2006.

212 references

[107] t. tao and c. zhai,    mixture id91 model for pseudo feedback in infor-
mation retrieval,    in proceedings of the 2004 meeting of the international
federation of classi   cation societies, spriner, 2004.

[108] t. tao and c. zhai,    regularized estimation of mixture models for robust
pseudo-relevance feedback,    in proceedings of acm sigir 2006, pp. 162   169,
2006.

[109] h. turtle and w. b. croft,    evaluation of an id136 network-based retrieval
model,    acm transactions on information systems, vol. 9, pp. 187   222, july
1991.

[110] c. j. van rijbergen,    a theoretical basis for the use of co-occurrence data in

information retrieval,    journal of documentation, pp. 106   119, 1977.

[111] c. j. van rijsbergen,    a non-classical logic for information retrieval,    the

computer journal, vol. 29, no. 6, pp. 481   485, 1986.

[112] x. wang, h. fang, and c. zhai,    improve retrieval accuracy for di   cult
queries using negative feedback,    in cikm    07: proceedings of the sixteenth
acm conference on conference on information and knowledge management,
pp. 991   994, usa, new york, ny: acm, 2007.

[113] x. wei and w. bruce croft,    lda-based document models for ad-hoc
retrieval,    in sigir    06: proceedings of the 29th annual international acm
sigir conference on research and development in information retrieval,
pp. 178   185, usa, new york, ny: acm, 2006.

[114] s. k. m. wong and y. y. yao,    a id203 distribution model for informa-
tion retrieval,    information processing and management, vol. 25, pp. 39   53,
1989.

[115] s. k. m. wong and y. y. yao,    on modeling information retrieval with proba-
bilistic id136,    acm transactions on information systems, vol. 13, pp. 69   
99, 1995.

[116] j. xu and w. b. croft,    id183 using local and global document

analysis,    in proceedings of the sigir   96, pp. 4   11, 1996.

[117] j. xu and w. b. croft,    cluster-based language models for distributed

retrieval,    in proceedings of the sigir   99, pp. 254   261, 1999.

[118] j. xu, r. weischedel, and c. nguyen,    evaluating a probabilistic model for
cross-lingual information retrieval,    in sigir    01: proceedings of the 24th
annual international acm sigir conference on research and development
in information retrieval, pp. 105   110, usa, new york, ny: acm, 2001.

[119] h. zaragoza, d. hiemstra, and m. e. tipping,    bayesian extension to the
language model for ad hoc information retrieval,    in proceedings of acm
sigir 2003, pp. 4   9, 2003.

[120] c. zhai,    fast statistical parsing of noun phrases for document indexing,    in
5th conference on applied natural language processing (anlp-97), pp. 312   
319, march 31   april 3 1997.

[121] c. zhai,    risk minimization and id38 in text retrieval,    phd

thesis, carnegie mellon university, 2002.

[122] c. zhai, w. w. cohen, and j. la   erty,    beyond independent relevance: meth-
ods and id74 for subtopic retrieval,    in proceedings of acm
sigir   03, pp. 10   17, august 2003.

references

213

[123] c. zhai and j. la   erty,    model-based feedback in the id38
approach to information retrieval,    in tenth international conference on
information and knowledge management (cikm 2001), pp. 403   410, 2001.
[124] c. zhai and j. la   erty,    a study of smoothing methods for language models
applied to ad hoc information retrieval,    in proceedings of acm sigir   01,
pp. 334   342, september 2001.

[125] c. zhai and j. la   erty,    two-stage language models for information

retrieval,    in proceedings of acm sigir   02, pp. 49   56, august 2002.

[126] c. zhai and j. la   erty,    a study of smoothing methods for language models
applied to information retrieval,    acm transactions on information systems,
vol. 2, pp. 179   214, 2004.

[127] c. zhai and j. la   erty,    a risk minimization framework for information

retrieval,    information processing management, vol. 42, pp. 31   55, 2006.

[128] c. zhai, x. lu, x. ling, a. velivelli, x. wang, h. fang, and a. shakery,
   uiuc/musc at trec 2005 genomics track,    in proceedings of trec
2005, 2005.

[129] c. zhai, t. tao, h. fang, and z. shang,    improving the robustness of lan-
guage models     uiuc trec 2003 robust and genomics experiments,    in
proceedings of trec 2003, pp. 667   672, 2003.

[130] c. zhai, a. velivelli, and b. yu,    a cross-collection mixture model for compar-
ative text minning,    in proceeding of the 10th acm sigkdd international
conference on knowledge discovery in data mining, pp. 743   748, 2004.

[131] y. zhang, j. callan, and t. minka,    redundancy detection in adaptive    lter-

ing,    in proceedings of sigir   02, pp. 81   88, august 2002.

[132] y. zhou and w. b. croft,    document quality models for web ad hoc retrieval,   
in cikm    05: proceedings of the 14th acm international conference on infor-
mation and knowledge management, pp. 331   332, usa, new york, ny: acm,
2005.

